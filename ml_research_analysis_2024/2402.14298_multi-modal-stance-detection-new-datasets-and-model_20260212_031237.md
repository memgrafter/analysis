---
ver: rpa2
title: 'Multi-modal Stance Detection: New Datasets and Model'
arxiv_id: '2402.14298'
source_url: https://arxiv.org/abs/2402.14298
tags:
- stance
- multi-modal
- detection
- prompt
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting public opinion
  from social media posts that combine text and images, a common format in today's
  platforms like Twitter. The authors create five new datasets spanning different
  domains, each consisting of text-image pairs labeled for stance toward specific
  targets.
---

# Multi-modal Stance Detection: New Datasets and Model

## Quick Facts
- **arXiv ID:** 2402.14298
- **Source URL:** https://arxiv.org/abs/2402.14298
- **Reference count:** 29
- **Primary result:** TMPT model achieves macro F1-scores over 75% on five new multi-modal stance detection datasets

## Executive Summary
This paper addresses the challenge of detecting public opinion from social media posts that combine text and images, a common format in today's platforms like Twitter. The authors create five new datasets spanning different domains, each consisting of text-image pairs labeled for stance toward specific targets. To tackle this task, they propose a Targeted Multi-modal Prompt Tuning framework (TMPT) that leverages target information to guide both textual and visual prompt tuning in pre-trained models, enabling effective multi-modal stance feature learning. Experiments show that TMPT significantly outperforms existing methods in both in-target and zero-shot multi-modal stance detection, with the best models achieving macro F1-scores over 75% across most datasets. The approach also generalizes well to different pre-trained models and fusion strategies.

## Method Summary
The paper proposes a Targeted Multi-modal Prompt Tuning framework (TMPT) that incorporates target information into prompt tuning for both text and image modalities. The framework first encodes text and images separately, then applies target-aware prompt tuning to enhance stance feature learning. The target information is integrated through a target-aware mechanism that helps the model focus on stance-related features specific to each target. The tuned representations are then fused using a multi-modal fusion module before classification. The authors evaluate their approach across five newly created datasets with varying domains and targets, demonstrating superior performance compared to existing baselines.

## Key Results
- TMPT achieves macro F1-scores over 75% across most datasets
- The model significantly outperforms existing methods in both in-target and zero-shot multi-modal stance detection
- TMPT demonstrates strong generalization across different pre-trained models and fusion strategies
- The best performance is achieved with 7 visual prompt tokens, though this varies by image type

## Why This Works (Mechanism)
The effectiveness of TMPT stems from its targeted prompt tuning approach that incorporates target-specific information into the feature learning process. By explicitly conditioning the prompt tuning on target information, the model can learn stance features that are more discriminative for specific targets rather than learning generic stance representations. This targeted approach allows the model to capture nuanced stance patterns that vary across different targets. The multi-modal fusion component effectively combines complementary information from text and images, with each modality providing different cues about stance (text for explicit opinions, images for implicit context and sentiment).

## Foundational Learning

**Multi-modal learning**: Combining information from multiple modalities (text, image, audio, video) is essential for understanding complex social media posts where stance is expressed through both words and visual content. Quick check: Verify the model can process and integrate both text and image features effectively.

**Prompt tuning**: A parameter-efficient fine-tuning method that adds trainable prompt tokens to pre-trained models, allowing adaptation to new tasks with minimal parameter updates. Quick check: Confirm that prompt tuning achieves comparable performance to full fine-tuning while using fewer parameters.

**Stance detection**: The task of identifying whether a text expresses agreement, disagreement, or neutrality toward a specific target or topic. Quick check: Ensure the model correctly classifies stance across different target domains.

**Zero-shot learning**: Evaluating model performance on targets not seen during training, testing the model's ability to generalize stance detection capabilities to new targets. Quick check: Validate that the model maintains reasonable performance on unseen targets.

## Architecture Onboarding

**Component map:** Text encoder -> Target-aware prompt tuning -> Image encoder -> Target-aware prompt tuning -> Multi-modal fusion -> Classification

**Critical path:** The critical path involves encoding text and images, applying target-aware prompt tuning to both modalities, fusing the resulting representations, and making the final stance classification. The target-aware prompt tuning is the distinguishing component that enables effective stance feature learning.

**Design tradeoffs:** The paper trades parameter efficiency (through prompt tuning) for potential performance limitations compared to full fine-tuning. The choice of 7 visual prompt tokens represents a balance between expressiveness and overfitting risk. The multi-modal fusion strategy (likely late fusion based on typical approaches) trades early interaction opportunities for simpler implementation.

**Failure signatures:** Performance degradation on targets with subtle stance expressions, reduced accuracy on image-only or text-only components, and potential overfitting to specific target-stance patterns when training data is limited for certain target-stance combinations.

**3 first experiments:**
1. Baseline evaluation: Compare TMPT against text-only and image-only baselines to quantify the value of multi-modal information
2. Prompt token ablation: Systematically vary the number of visual prompt tokens (1, 3, 5, 7, 9) to identify the optimal configuration
3. Target information ablation: Evaluate model performance with and without target information in the prompt tuning to validate the contribution of target-aware mechanisms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do audio and video modalities affect stance detection performance compared to text and image-only approaches?
- Basis in paper: [inferred] The paper mentions that the current version of the data does not consider audio modality and video information, and this is identified as an issue to explore in the future.
- Why unresolved: The paper only considers text and image modalities for stance detection, leaving the impact of audio and video modalities unexplored.
- What evidence would resolve it: Experimental results comparing stance detection performance using text-image, text-audio, text-video, and text-image-audio-video modalities on the same dataset would resolve this question.

### Open Question 2
- Question: What is the optimal number of visual prompt tokens for different types of images (e.g., person, events, words, memes, mixed)?
- Basis in paper: [explicit] The paper conducts an experiment analyzing the impact of the number of visual prompt tokens and finds that 7 tokens yield the best overall performance. It also performs an error analysis on different image types, showing varying model performance across categories.
- Why unresolved: The paper only tests a fixed number of visual prompt tokens (7) across all image types, without optimizing for specific image categories.
- What evidence would resolve it: Experimental results showing the optimal number of visual prompt tokens for each image type category (person, events, words, memes, mixed) would resolve this question.

### Open Question 3
- Question: How does the performance of multi-modal stance detection vary across different cultural contexts and languages?
- Basis in paper: [inferred] The paper creates datasets based on Twitter data, which is primarily in English, and focuses on specific targets (e.g., Donald Trump, Joe Biden, Russia, Ukraine). It does not explore how stance detection performance might vary across different cultural contexts or languages.
- Why unresolved: The paper does not investigate the generalizability of the multi-modal stance detection approach to different cultural contexts or languages beyond the English-speaking Twitter dataset.
- What evidence would resolve it: Experimental results comparing multi-modal stance detection performance on datasets from different cultural contexts and languages would resolve this question.

## Limitations

- Performance metrics are measured only on newly created datasets, which may not fully represent real-world complexity or distribution shifts
- The paper lacks evaluation on existing multi-modal stance detection benchmarks for direct comparison
- Datasets' diversity and potential biases (e.g., political leaning in US elections, religious focus in COVID-19) are not thoroughly analyzed

## Confidence

- **High confidence:** The claim that TMPT "significantly outperforms existing methods" is supported by the experimental results presented, though the comparison pool is limited to specific baselines rather than the broader stance detection literature.
- **Medium confidence:** The assertion that TMPT "generalizes well to different pre-trained models and fusion strategies" has medium confidence, as the paper demonstrates performance across some variants but doesn't exhaustively explore the design space.
- **Medium confidence:** The reported macro F1-scores over 75% are credible based on the methodology, but the limited evaluation scope prevents higher confidence.

## Next Checks

1. Evaluate TMPT on established multi-modal datasets from other domains to verify cross-domain generalization beyond the five created datasets
2. Conduct ablation studies isolating the impact of target-aware prompt tuning versus other architectural choices to validate the claimed contribution of target information integration
3. Perform robustness testing with noisy or adversarial image-text pairs to assess real-world deployment viability, as social media content often contains misleading or low-quality multimodal posts