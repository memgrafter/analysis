---
ver: rpa2
title: Doing Experiments and Revising Rules with Natural Language and Probabilistic
  Reasoning
arxiv_id: '2402.06025'
source_url: https://arxiv.org/abs/2402.06025
tags:
- human
- rule
- should
- rules
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for probabilistic inference of natural
  language hypotheses in online settings, integrating Large Language Models (LLMs)
  with Sequential Monte Carlo Samplers (SMC-S). The approach interleaves belief updates
  with experiment design under information-theoretic criteria.
---

# Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning

## Quick Facts
- arXiv ID: 2402.06025
- Source URL: https://arxiv.org/abs/2402.06025
- Reference count: 40
- Key outcome: Model achieves 6.55/8 predictive posterior accuracy vs humans at 5.26/8 on Zendo tasks

## Executive Summary
This paper introduces a method for online probabilistic inference over natural language hypotheses, integrating Large Language Models (LLMs) with Sequential Monte Carlo Samplers (SMC-S) and information-theoretic active learning. The approach interleaves belief updates with experiment design, using an LLM to propose hypothesis revisions and candidate experiments. Tested on Zendo-style and ActiveACRE tasks, the model outperforms both humans and baselines, particularly when incorporating fuzzy probabilistic rules. The method demonstrates how LLMs can enable tractable inference over infinitely large hypothesis spaces while maintaining computational efficiency.

## Method Summary
The method combines LLM-based hypothesis revision with SMC-S for online inference over natural language hypotheses. It uses an LLM to propose revisions to low-likelihood particles in the sampler, avoiding the need to maintain all possible hypotheses. Active experiment selection employs information gain criteria, with candidate experiments proposed by an LLM and evaluated under the approximate posterior. The approach incorporates fuzzy rules with noise parameters (ϵ, δ) to represent probabilistic rather than deterministic relationships. This enables the model to better match human reasoning patterns while maintaining computational tractability through focused LLM calls only on particles needing revision.

## Key Results
- Achieves 6.55/8 predictive posterior accuracy on Zendo tasks vs humans at 5.26/8
- Online LLM-SMC-S outperforms both human participants and baseline methods
- Fuzzy rules better explain human behavior patterns (R² = .57) compared to hard rules (R² = .10)
- InfoGain active learning significantly improves performance over random or LLM-only experiment selection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Using an LLM to propose revisions to hypotheses in an SMC-S framework allows tractable online inference over an infinitely large hypothesis space.
- **Mechanism**: The method tracks a small pool of particles and calls the LLM to revise only the lowest-likelihood ones, focusing computational effort where it matters most.
- **Core assumption**: The LLM can generate relevant neighboring hypotheses given a current hypothesis and experimental outcome.
- **Evidence anchors**:
  - [abstract]: "uses an LLM proposal distribution to allow tractable inference over natural language strings"
  - [section 2.1]: "But calling an LLM to perturb every single particle is expensive, and unnecessary for hypotheses that already explain the data well."

### Mechanism 2
- **Claim**: Active experiment selection via information gain, with candidate experiments proposed by an LLM, leads to more efficient identification of the true hypothesis than random or LLM-only selection.
- **Mechanism**: The LLM generates diverse candidate experiments, which are then evaluated under the approximate posterior from SMC-S using expected information gain.
- **Core assumption**: The LLM can generate experiments that cover the space of possibilities relevant to the current hypotheses.
- **Evidence anchors**:
  - [section 2.2]: "We take the best experiment proposed by the LLM, as measured under the approximate posterior from SMC-S"
  - [section 3]: "substituting InfoGain with alternative methods significantly degrades model performance"

### Mechanism 3
- **Claim**: Allowing fuzzy (probabilistic) rules in the hypothesis space better explains human behavior patterns even when the true rules are deterministic.
- **Mechanism**: Each hypothesis includes noise parameters θ = (ϵ, δ) controlling false-positive and false-negative rates, allowing representation of rules that only partly hold.
- **Core assumption**: Human inference is better characterized by considering fuzzy rules than hard deterministic rules, even when underlying rules are deterministic.
- **Evidence anchors**:
  - [section 3]: "the model explains 57% of the variation in this more fine-grained measurement of human accuracy (R2 = .57)" with fuzzy rules vs "R2 = .10" with hard rules

## Foundational Learning

- **Concept**: Bayesian optimal experiment design
  - Why needed here: Provides the theoretical framework for alternating between experimentation and belief updating, defining the information gain objective for experiment selection.
  - Quick check question: What is the objective function maximized when selecting the next experiment in Bayesian optimal experiment design?

- **Concept**: Sequential Monte Carlo Samplers (SMC-S)
  - Why needed here: Enables online approximate inference by maintaining a weighted set of hypothesis particles that evolve as new data arrives, avoiding intractable full posterior computation.
  - Quick check question: How does SMC-S approximate a posterior distribution using a finite set of weighted particles?

- **Concept**: Importance sampling vs online inference
  - Why needed here: Distinguishes the proposed online approach from batch methods, showing why online inference can outperform when hypotheses need to evolve incrementally.
  - Quick check question: What is the key difference between batch importance sampling and the online inference approach used here?

## Architecture Onboarding

- **Component map**: Experiment → Outcome → Hypothesis revision (LLM + SMC-S) → New experiment selection (InfoGain + LLM) → Repeat
- **Critical path**: Experiment → Outcome → Hypothesis revision (LLM + SMC-S) → New experiment selection (InfoGain + LLM) → Repeat
- **Design tradeoffs**:
  - Number of particles vs computational cost: More particles give better posterior approximation but require more LLM calls
  - Fuzzy vs hard rules: Fuzzy rules better explain human data but may reduce accuracy on deterministic tasks
  - LLM calls per iteration: More calls improve hypothesis diversity but increase cost and potential for redundancy
- **Failure signatures**:
  - Poor human-model fit suggests hypothesis space or inference method doesn't capture human reasoning patterns
  - Low accuracy indicates the method isn't effectively identifying the true hypothesis
  - High computational cost with marginal gains suggests inefficient particle management or experiment selection
- **First 3 experiments**:
  1. Implement the SMC-S algorithm with a fixed set of hand-crafted hypotheses to verify the inference mechanics work before adding LLM components
  2. Add LLM-based hypothesis revision to test whether the proposal distribution can effectively explore the hypothesis space
  3. Implement active experiment selection and compare InfoGain vs random vs LLM-only experiment selection to verify the importance of the information gain criterion

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond those inherent in the research itself. The limitations section addresses key unresolved issues regarding computational efficiency, generalization to complex domains, and the balance between fuzzy and hard rule representations.

## Limitations
- Reliance on LLM-generated hypothesis revisions introduces uncertainty about performance on tasks requiring very different hypothesis structures than training data
- Computational cost scales linearly with particle count, potentially limiting real-time applications
- Method assumes LLM can generate meaningful neighboring hypotheses, which may not hold for all hypothesis spaces
- Evaluation focuses on specific task domains (Zendo, ActiveACRE) without broader generalization testing

## Confidence
- **High confidence**: SMC-S framework correctly implements online approximate inference; information gain criterion is theoretically sound for active experiment selection; fuzzy rule formulation with noise parameters is mathematically coherent
- **Medium confidence**: LLM proposal distribution effectively explores hypothesis space in practice; method's superior performance over humans and baselines generalizes beyond specific evaluation tasks; fuzzy rules genuinely better explain human reasoning rather than overfitting
- **Low confidence**: Computational efficiency gains are substantial enough to justify approach over simpler alternatives; method scales effectively to significantly larger hypothesis spaces or more complex domains

## Next Checks
1. Test the method on a broader range of hypothesis spaces, including those with discontinuous or highly non-linear relationships, to evaluate the LLM's ability to generate meaningful revisions in more challenging contexts.
2. Compare the computational cost per iteration (LLM calls, inference time) against the accuracy gains to establish whether the trade-off is favorable for practical applications.
3. Conduct ablation studies removing the LLM components to isolate their contribution, testing whether performance benefits come from the LLM's reasoning capabilities or from the active inference framework itself.