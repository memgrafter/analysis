---
ver: rpa2
title: 'DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models'
arxiv_id: '2408.01933'
source_url: https://arxiv.org/abs/2408.01933
tags:
- disease
- clinical
- note
- reasoning
- diagnosis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DiReCT, a dataset and benchmark for evaluating
  the diagnostic reasoning and interpretability of large language models (LLMs) in
  clinical settings. DiReCT contains 511 clinical notes annotated by physicians, detailing
  the diagnostic reasoning process from observations to final diagnosis.
---

# DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models

## Quick Facts
- arXiv ID: 2408.01933
- Source URL: https://arxiv.org/abs/2408.01933
- Reference count: 23
- Key outcome: Significant gap between LLM diagnostic reasoning and human doctors, with accuracy, completeness, and faithfulness metrics highlighting the need for improved clinical reasoning models.

## Executive Summary
This paper introduces DiReCT, a dataset and benchmark for evaluating the diagnostic reasoning and interpretability of large language models (LLMs) in clinical settings. DiReCT contains 511 clinical notes annotated by physicians, detailing the diagnostic reasoning process from observations to final diagnosis, along with a diagnostic knowledge graph for reasoning support. Experiments with leading LLMs reveal significant performance gaps compared to human doctors, particularly in finding essential information in long text and generating faithful explanations. The results highlight the need for AI models that can perform reliable and interpretable reasoning in clinical environments.

## Method Summary
DiReCT provides a benchmark for diagnostic reasoning using LLMs on clinical notes from the MIMIC-IV dataset. The dataset includes physician-annotated reasoning paths and a knowledge graph based on clinical guidelines. The baseline method employs three LLM-based modules: narrowing-down (predicts disease category), perception (extracts observations), and reasoning (iteratively traverses the knowledge graph). Models are evaluated on diagnostic accuracy, completeness of extracted observations, and faithfulness of explanations. Experiments compare various LLMs with and without external knowledge, using an LLM-based automatic evaluation engine for matching predictions to ground-truth annotations.

## Key Results
- LLMs show significantly lower diagnostic accuracy compared to human doctors on the DiReCT benchmark
- Models struggle with extracting complete observations from clinical notes, particularly in identifying key information
- Faithfulness of explanations is notably low, indicating difficulties in establishing accurate observation-diagnosis correspondences
- Performance improves with external knowledge (diagnostic knowledge graph) but remains substantially below human performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The knowledge graph grounds LLM reasoning in structured diagnostic criteria, enabling step-by-step entailment from observations to diagnoses.
- **Mechanism:** Each supporting edge in the graph maps a premise to a diagnosis; annotations link observations to these premises. The model iteratively verifies premises using observations, ensuring consistency with clinical guidelines.
- **Core assumption:** Premises in the knowledge graph are complete, mutually exclusive, and sufficient for diagnosis.
- **Evidence anchors:**
  - [abstract] "Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs."
  - [section 3.2] "Our knowledge graphs K = {Ki}i is a collection of graph Ki for disease category i. Ki is based on the diagnosis criteria in existing guidelines."
  - [corpus] Weak evidence; only one related work uses knowledge graphs (MedCoT-RAG), but not in the exact entailment-tree form.
- **Break condition:** If premises are missing or incomplete in the graph, the model cannot proceed through the entailment chain.

### Mechanism 2
- **Claim:** Decomposing the diagnosis into a sequence of intermediate diagnoses allows the model to perform multi-hop reasoning.
- **Mechanism:** The baseline module uses the knowledge graph to predict a disease category, then iteratively selects child diagnoses from the current node, verifying each with observations. This mimics how clinicians narrow down possibilities.
- **Core assumption:** Clinical notes contain sufficient observable evidence to support at least one path through the knowledge graph.
- **Evidence anchors:**
  - [section 3.4] "our reasoning module V iteratively and greedily identifies the next step's diagnosis (i.e., dt+1) from {dn}n, making a rationalization for each deduction."
  - [section 4] "an overview of our baseline with three LLM-based modules narrowing-down, perception, and reasoning."
  - [corpus] Weak evidence; related works (e.g., MedCoT-RAG) use chain-of-thought but not explicit graph-guided stepwise reasoning.
- **Break condition:** If observations are ambiguous or missing, the iterative step fails and reasoning halts.

### Mechanism 3
- **Claim:** Fine-grained physician annotations linking observations to diagnoses and rationales enable quantitative evaluation of faithfulness.
- **Mechanism:** Annotations provide (observation, rationale, diagnosis) triples for each supporting edge. Automatic evaluation matches predicted observations and rationales against these triples, measuring both completeness and faithfulness.
- **Core assumption:** Ground-truth annotations are consistent and unambiguous, enabling reliable automatic matching.
- **Evidence anchors:**
  - [abstract] "DiReCT contains 511 clinical notes annotated by physicians, detailing the diagnostic reasoning process from observations to final diagnosis."
  - [section 3.3] "The annotation process was carried out by 9 clinical physicians and subsequently verified for accuracy and completeness by three senior medical experts."
  - [section 5.3] "We randomly pick out 100 samples from DiReCT and their prediction by GPT-4 ... to assess the consistency of our automated metrics to evaluate the observational and explanatory performance ... Three physicians joined this experiment."
  - [corpus] Moderate evidence; similar annotation approaches in MedCoT-RAG and ER-REASON, but DiReCT's triple-level granularity is unique.
- **Break condition:** If annotators disagree on mappings, automated matching accuracy degrades.

## Foundational Learning

- **Concept:** Entailment trees and multi-evidence reasoning
  - **Why needed here:** Clinical diagnosis often requires combining multiple observations and rules; simple QA or single-hop inference is insufficient.
  - **Quick check question:** Can the model trace a diagnosis back through intermediate nodes in the knowledge graph using observations?

- **Concept:** Graph traversal algorithms (BFS/DFS with constraints)
  - **Why needed here:** The reasoning module must navigate the diagnostic knowledge graph efficiently while checking premises against observations.
  - **Quick check question:** Does the traversal correctly stop when a leaf diagnosis is reached or when no observations satisfy a premise?

- **Concept:** Automated metric design for interpretability
  - **Why needed here:** Faithful explanations require both correct observations and rationales; standard accuracy is insufficient.
  - **Quick check question:** Do Obscomp and Expall scores decrease when the model hallucinates or omits critical observations?

## Architecture Onboarding

- **Component map:** Clinical note → narrowing-down → perception → iterative reasoning → prediction + explanation
- **Critical path:** Clinical note → narrowing-down → perception → iterative reasoning → prediction + explanation
- **Design tradeoffs:**
  - Using a knowledge graph provides interpretability but limits flexibility if the graph is incomplete.
  - Iterative greedy reasoning is simpler than beam search but may miss optimal paths.
  - LLM-based automatic evaluation is fast but may misjudge subtle differences in rationales.
- **Failure signatures:**
  - High Obscomp but low Expall → model finds observations but fails to connect them to correct diagnoses.
  - Low Accdiag but high Obscomp → model extracts correct observations but misclassifies the disease.
  - All metrics low → clinical notes lack sufficient observable evidence for the given knowledge graph.
- **First 3 experiments:**
  1. Run baseline on a small subset (e.g., 10 samples) with graph input; verify Accdiag > 0.5.
  2. Remove the knowledge graph; check if diagnostic accuracy drops sharply.
  3. Manually inspect 5 predictions; verify that observed observations match ground truth and rationales are meaningful.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of LLMs on DiReCT vary when using more complex diagnostic knowledge graphs that include inter-diagnostic relationships and multi-disease cases?
- **Basis in paper:** [inferred] The paper mentions that DiReCT currently only considers one PDD and omits inter-diagnostic relationships due to complexity. It also suggests that DiReCT can be extended to more challenging settings by removing the knowledge graph.
- **Why unresolved:** The current DiReCT dataset and evaluation do not include these complex scenarios, and the paper only mentions this as a potential future direction without providing results.
- **What evidence would resolve it:** An extended version of DiReCT that includes cases with multiple diagnoses and complex inter-diagnostic relationships, along with performance evaluations of LLMs on this extended dataset.

### Open Question 2
- **Question:** What specific architectural or training modifications could improve LLMs' ability to align their diagnostic reasoning with human doctors, particularly in identifying key observations and establishing accurate observation-diagnosis correspondences?
- **Basis in paper:** [explicit] The paper highlights a significant gap between LLMs' reasoning ability and that of human doctors, especially in finding essential information in long text and generating faithful explanations. It also mentions that current models struggle with associating premises and text in clinical notes, which are often superficially different though semantically consistent.
- **Why unresolved:** The paper identifies the problem but does not propose or test specific solutions to improve this alignment.
- **What evidence would resolve it:** Development and evaluation of LLM architectures or training methods specifically designed to improve alignment with human diagnostic reasoning, tested on the DiReCT benchmark.

### Open Question 3
- **Question:** How does the performance of LLMs on DiReCT change when incorporating chain-of-thought reasoning or other advanced prompting techniques?
- **Basis in paper:** [inferred] The paper mentions that their baseline method may not use optimal prompts or chain-of-thought reasoning, and suggests this as a limitation.
- **Why unresolved:** The paper uses a simple baseline method without exploring more advanced prompting techniques, leaving the potential benefits of these techniques unknown.
- **What evidence would resolve it:** Experiments comparing the performance of LLMs on DiReCT using various prompting techniques, including chain-of-thought reasoning, and analysis of how these techniques affect diagnostic accuracy and reasoning faithfulness.

## Limitations
- The knowledge graph completeness remains unverified; missing premises may silently limit model performance without clear failure signals.
- The iterative greedy reasoning approach may not explore alternative diagnostic paths, potentially missing correct diagnoses when early choices are suboptimal.
- Automated metric reliability depends heavily on the consistency of physician annotations and the matching capability of the LLM-based evaluation engine.

## Confidence
- Knowledge graph-grounded entailment reasoning: Medium confidence
- Iterative multi-hop reasoning approach: Medium-Low confidence
- Automated faithfulness evaluation: Medium confidence

## Next Checks
1. Conduct ablation studies removing subsets of knowledge graph premises to quantify performance degradation and identify critical missing edges.
2. Compare greedy reasoning with beam search (width=3) on a validation subset to measure potential accuracy gains from exploring alternative paths.
3. Perform inter-annotator agreement analysis on 50 samples to quantify annotation consistency and assess impact on automated metric reliability.