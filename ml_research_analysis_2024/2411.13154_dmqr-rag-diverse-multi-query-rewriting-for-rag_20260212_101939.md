---
ver: rpa2
title: 'DMQR-RAG: Diverse Multi-Query Rewriting for RAG'
arxiv_id: '2411.13154'
source_url: https://arxiv.org/abs/2411.13154
tags:
- rewriting
- retrieval
- query
- documents
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes DMQR-RAG, a framework for improving retrieval-augmented
  generation (RAG) by using diverse multi-query rewriting strategies. The key insight
  is that different rewriting strategies can retrieve more diverse and relevant documents
  than single-query approaches.
---

# DMQR-RAG: Diverse Multi-Query Rewriting for RAG

## Quick Facts
- arXiv ID: 2411.13154
- Source URL: https://arxiv.org/abs/2411.13154
- Reference count: 4
- Key outcome: DMQR-RAG framework improves RAG by using diverse multi-query rewriting strategies, achieving 14.46% increase in P@5 on FreshQA and outperforming RAG-Fusion

## Executive Summary
This paper proposes DMQR-RAG, a framework that improves retrieval-augmented generation by using diverse multi-query rewriting strategies. The core insight is that different rewriting strategies can retrieve more diverse and relevant documents than single-query approaches. The framework employs four rewriting strategies based on different levels of information (equality, expansion, and reduction) and includes an adaptive strategy selection method that chooses optimal rewrites for each query. Experiments demonstrate significant improvements in document retrieval and end-to-end response quality across academic and industrial datasets.

## Method Summary
DMQR-RAG generates multiple rewritten queries using four strategies: GQR (goal-based rewriting), KWR (knowledge-based rewriting), PAR (pseudo-answer rewriting), and CCE (context-based rewriting). These strategies operate at different information levels to increase document retrieval diversity. An adaptive selection method uses lightweight prompting and few-shot learning to dynamically choose the most suitable rewriting strategies for each specific query, reducing unnecessary rewrites while maintaining performance. The framework integrates with Bing search as the retriever and BAAI-BGE as the reranker.

## Key Results
- 14.46% increase in P@5 on FreshQA dataset compared to single-query rewriting methods
- Outperforms RAG-Fusion across multiple datasets with significant improvements in document retrieval
- Adaptive strategy selection reduces the number of rewrites by ~40% while maintaining or improving performance
- Consistent improvements in end-to-end response quality metrics (EM, F1 scores, accuracy)

## Why This Works (Mechanism)

### Mechanism 1
Diverse multi-query rewriting increases document retrieval diversity by leveraging different levels of information. The framework generates multiple rewritten queries using four strategies that operate at different information levels (equality, expansion, reduction). Each rewritten query retrieves different documents, increasing the chance of capturing relevant information. Core assumption: Queries with different information quantities retrieve diverse documents that together cover more relevant content than single-query approaches.

### Mechanism 2
Adaptive rewriting strategy selection improves performance by choosing optimal strategies for each query. A lightweight prompting and few-shot learning approach dynamically selects the most suitable rewriting strategies for each specific query, reducing unnecessary rewrites while maintaining performance. Core assumption: Different query types benefit from different rewriting strategies, and the model can accurately identify which strategies work best for each query.

### Mechanism 3
Information expansion through pseudo-answer rewriting captures semantic patterns that aid document retrieval. PAR generates pseudo-answers that, while potentially inaccurate, are semantically aligned with correct answers and capture relevant response patterns, helping retrieve more pertinent documents. Core assumption: Pseudo-answers provide additional semantic context that bridges the gap between user queries and relevant documents.

## Foundational Learning

- Concept: Information retrieval and ranking fundamentals
  - Why needed here: Understanding how different query formulations affect document retrieval and ranking is essential for grasping why diverse rewriting strategies improve performance
  - Quick check question: What is the relationship between query diversity and document recall in information retrieval systems?

- Concept: Large language model prompting techniques
  - Why needed here: The framework relies heavily on prompt engineering to guide LLMs in rewriting queries and selecting strategies, requiring understanding of prompt effectiveness
  - Quick check question: How does few-shot learning in prompts improve the accuracy of rewriting strategy selection?

- Concept: Evaluation metrics for retrieval and generation systems
  - Why needed here: Understanding metrics like H@5, P@5, EM, F1 is crucial for interpreting experimental results and comparing against baselines
  - Quick check question: What is the difference between hit rate and precision metrics in evaluating retrieval systems?

## Architecture Onboarding

- Component map:
  User query → LLMs (multiple rewriting strategies) → Rewritten queries → Retriever (Bing) → Retrieved documents → Reranker (BAAI-BGE) → Top-K documents → LLM response generator → Final answer
  Strategy pool: {RSGQR, RSKWR, RSPAR, RSCCE, ...}
  Adaptive selector: Lightweight prompting + few-shot learning

- Critical path: Query → Rewriting strategies → Document retrieval → Reranking → Response generation
- Design tradeoffs:
  - Number of rewrites vs. noise introduction: More rewrites increase diversity but also noise
  - Strategy selection complexity vs. performance gain: More sophisticated selection improves results but increases computational overhead
  - Retrieval source choice: Using Bing ensures timeliness but limits control over retrieval parameters

- Failure signatures:
  - Performance degradation when too many rewrites are applied (excessive noise)
  - Strategy selection failures when queries don't fit expected patterns
  - Retrieval failures when rewritten queries are too far from original intent

- First 3 experiments:
  1. Baseline comparison: Run original query vs. single-query rewriting vs. DMQR-RAG on AmbigNQ dataset to verify performance improvements
  2. Strategy ablation: Test each rewriting strategy individually on FreshQA to identify which contribute most to performance
  3. Adaptive selection validation: Compare fixed strategy set vs. adaptive selection on complex queries to measure noise reduction benefits

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of DMQR-RAG scale with larger language models compared to smaller ones? The paper shows that DMQR-RAG works well with both GPT-4 and smaller models like Llama3-8B and Qwen2-7B, but does not provide a systematic comparison of performance scaling with model size.

### Open Question 2
How does the adaptive rewriting selection method perform in real-time scenarios with varying query types and user intents? The paper mentions that the adaptive rewriting selection method is designed to handle diverse queries, but does not provide extensive evaluation in real-time scenarios with varying query types and user intents.

### Open Question 3
What are the potential limitations of the information-based rewriting strategies in handling highly complex or ambiguous queries? The paper presents four rewriting strategies based on different levels of information, but does not explicitly discuss their limitations in handling highly complex or ambiguous queries.

## Limitations
- Effectiveness heavily depends on quality of rewriting strategies and adaptive selection mechanism, both relying on prompt engineering details not fully disclosed
- Use of Bing search as retriever introduces external dependencies affecting reproducibility
- Lacks direct corpus evidence supporting some core claims about information diversity and pseudo-answer effectiveness
- Evaluation focuses primarily on retrieval metrics without extensive analysis of response quality or user satisfaction in real-world applications

## Confidence
- High Confidence: Diverse multi-query rewriting improves document retrieval diversity (supported by experimental results showing consistent performance improvements)
- Medium Confidence: Adaptive strategy selection method's effectiveness demonstrated through performance metrics, but reproducibility concerns due to lack of detailed prompt implementation
- Low Confidence: Pseudo-answer rewriting captures semantic patterns for better retrieval (lacks direct corpus evidence, relies primarily on observed performance improvements)

## Next Checks
1. Implement and test exact prompts for GQR, KWR, PAR, and CCE strategies to verify reproducibility of described rewriting behaviors
2. Conduct analysis of retrieved document diversity using intrinsic measures to directly validate whether different rewriting strategies retrieve truly diverse documents
3. Deploy framework on live system with user queries to measure end-to-end performance improvements and assess practical user experience gains