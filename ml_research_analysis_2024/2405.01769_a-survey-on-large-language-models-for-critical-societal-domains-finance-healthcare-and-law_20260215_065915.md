---
ver: rpa2
title: 'A Survey on Large Language Models for Critical Societal Domains: Finance,
  Healthcare, and Law'
arxiv_id: '2405.01769'
source_url: https://arxiv.org/abs/2405.01769
tags:
- arxiv
- legal
- llms
- language
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey examines large language models (LLMs) in three critical\
  \ societal domains\u2014finance, healthcare, and law\u2014that require professional\
  \ expertise, sensitive data, and strict compliance. It reviews specialized LLMs,\
  \ performance benchmarks, and key challenges like data scarcity, multimodal document\
  \ handling, and ethical risks."
---

# A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law

## Quick Facts
- arXiv ID: 2405.01769
- Source URL: https://arxiv.org/abs/2405.01769
- Reference count: 40
- LLMs in finance, healthcare, and law require domain expertise, multimodal handling, and ethical safeguards for real-world deployment

## Executive Summary
This survey examines large language models (LLMs) in three critical societal domains—finance, healthcare, and law—that require professional expertise, sensitive data, and strict compliance. It reviews specialized LLMs, performance benchmarks, and key challenges like data scarcity, multimodal document handling, and ethical risks. Evaluations show GPT-4 and domain-tuned models excel at basic tasks, but struggle with complex reasoning and multi-modal contexts. The paper highlights the need for better explainability, fairness, robustness, and domain-specific datasets to meet real-world standards. It also stresses the importance of interdisciplinary collaboration and ethical safeguards to ensure safe and effective LLM deployment in high-stakes settings.

## Method Summary
The paper conducts a comprehensive survey of LLMs in finance, healthcare, and law domains, examining specialized models, benchmarks, and challenges. It reviews approaches including pre-training on general corpora followed by domain-specific instruction fine-tuning, multimodal document processing incorporating visual and spatial features, and ethical considerations around explainability and fairness. The methodology involves analyzing existing financial NLP tasks (sentiment analysis, information extraction), medical NLP benchmarks (clinical information extraction, medical QA), and legal NLP applications (judgment prediction, text classification), while identifying gaps in current evaluation frameworks and proposing future research directions.

## Key Results
- GPT-4 and domain-tuned models like FinMA achieve strong performance on basic financial and medical tasks but struggle with complex reasoning
- Multimodal LLMs incorporating visual features (FinTral, BioViL) show improved performance on document interpretation tasks
- Current work rarely addresses explainability and fairness comprehensively, with ethical risks including hallucinations and bias in high-stakes decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs in finance, healthcare, and law benefit from domain-specific instruction tuning that improves accuracy in specialized tasks.
- Mechanism: Pre-training on general corpora followed by instruction fine-tuning on domain-specific datasets (e.g., FinMA, PMC-LLAMA) enhances performance on tasks requiring complex reasoning and domain knowledge.
- Core assumption: Domain-specific instruction data effectively bridges the knowledge gap between general LLMs and specialized tasks.
- Evidence anchors:
  - [abstract]: "Recent works scale the model sizes up and conduct instruction fine-tuning, with the evaluation covering broader sets of financial tasks."
  - [section]: "FinMA (Xie et al., 2023) is an open-sourced financial LLM built from instruction fine-tuning on LlaMA... achieves comparable performance and sometimes surpasses the proprietary models like GPT-3.5."
  - [corpus]: Weak - no direct citations for instruction tuning efficacy in corpus neighbors.
- Break Condition: If instruction data fails to cover diverse real-world scenarios, or if domain-specific reasoning exceeds current instruction tuning capabilities.

### Mechanism 2
- Claim: Multimodal document handling is critical for LLM performance in finance, healthcare, and law due to the prevalence of complex, non-textual data.
- Mechanism: Incorporating visual and spatial features alongside text representations enhances LLM performance on tasks involving tables, charts, and images (e.g., FinTral, BioViL).
- Core assumption: Visual and spatial information significantly contributes to accurate interpretation and reasoning in these domains.
- Evidence anchors:
  - [abstract]: "Documents with visually rich content and complex layouts are common in financial domains... incorporation of visual and spatial features in the representation of text can enhance the performance of LLMs."
  - [section]: "FinTral (Bhatia et al., 2024) is a series of multimodal LLMs... This allows the authors to explore multimodal contexts, and to include visual reasoning tasks such as question answering over charts and graphs."
  - [corpus]: Weak - corpus neighbors focus on graph ML and privacy, not multimodal document handling.
- Break Condition: If visual and spatial features are not properly integrated or if models fail to correlate information across modalities.

### Mechanism 3
- Claim: Explainability and fairness are essential for LLM deployment in high-stakes domains due to the potential for significant real-world consequences.
- Mechanism: Developing methods to elucidate LLM decision-making processes and mitigate biases ensures transparency and trust in critical applications (e.g., legal judgment prediction, medical diagnosis).
- Core assumption: Users in these domains require clear understanding of LLM reasoning and assurance of unbiased outcomes.
- Evidence anchors:
  - [abstract]: "Requirement for Explainability and Fairness... Developing LLM-based applications that offer transparent reasoning and minimize bias is vital for any real-world deployment in these domains."
  - [section]: "Current work in academic settings rarely addresses these considerations in a comprehensive and systematic way. Issues such as fairness, accountability for misguided financial advice, and the ethical implications of AI-driven decision-making demand rigorous attention."
  - [corpus]: Weak - corpus neighbors focus on graph ML safety and privacy, not explainability and fairness in high-stakes domains.
- Break Condition: If explainability methods fail to provide meaningful insights or if bias mitigation strategies are ineffective.

## Foundational Learning

- Concept: Domain-specific knowledge acquisition
  - Why needed here: Finance, healthcare, and law require deep understanding of specialized concepts, terminologies, and regulations.
  - Quick check question: Can a general LLM accurately interpret a complex financial report or medical diagnosis without domain-specific knowledge?

- Concept: Multimodal data processing
  - Why needed here: Documents in these domains often contain text, tables, charts, and images that must be interpreted together.
  - Quick check question: How would an LLM handle a financial report with embedded charts and tables without multimodal processing capabilities?

- Concept: Ethical considerations in AI
  - Why needed here: The high-stakes nature of these domains demands transparency, fairness, and accountability in AI decision-making.
  - Quick check question: What are the potential consequences of biased or unexplainable AI decisions in finance, healthcare, or law?

## Architecture Onboarding

- Component map: Data Ingestion -> Feature Extraction -> Model Architecture -> Training Pipeline -> Evaluation Framework -> Deployment Interface
- Critical path: Data Ingestion → Feature Extraction → Model Architecture → Training Pipeline → Evaluation Framework → Deployment Interface
- Design tradeoffs:
  - Model size vs. efficiency: Larger models may offer better performance but require more computational resources.
  - General vs. domain-specific knowledge: Balancing breadth of knowledge with depth of expertise in specific domains.
  - Explainability vs. accuracy: More interpretable models may sacrifice some accuracy, while highly accurate models may be less transparent.
- Failure signatures:
  - Inaccurate or biased outputs: Indicates issues with training data, model architecture, or bias mitigation strategies.
  - Poor performance on multimodal tasks: Suggests problems with feature extraction or integration of visual and spatial information.
  - Lack of user trust: May result from insufficient explainability or perceived unfairness in AI decision-making.
- First 3 experiments:
  1. Evaluate LLM performance on a financial sentiment analysis task using both general and domain-specific instruction tuning.
  2. Assess the impact of multimodal document handling on LLM accuracy in interpreting complex financial reports.
  3. Test the effectiveness of explainability methods in providing clear insights into LLM decision-making processes for legal judgment prediction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically evaluate the robustness of LLMs against adversarial attacks in finance, healthcare, and law domains?
- Basis in paper: [explicit] The paper discusses various adversarial attacks on LLMs in §6.1.2 (Robustness) and mentions the need for robustness evaluation.
- Why unresolved: The paper acknowledges the importance of robustness but doesn't provide a concrete methodology for systematic evaluation across the three domains.
- What evidence would resolve it: A framework for evaluating LLM robustness against domain-specific adversarial attacks, including metrics and datasets for finance, healthcare, and law.

### Open Question 2
- Question: What are the most effective techniques for mitigating hallucinations in LLMs when applied to high-stakes domains like finance, healthcare, and law?
- Basis in paper: [explicit] The paper discusses hallucination as a major ethical concern in §6.1.2 and mentions various mitigation techniques.
- Why unresolved: The paper identifies hallucination as a critical issue but doesn't provide a comprehensive comparison of mitigation techniques across the three domains.
- What evidence would resolve it: A comparative study of hallucination mitigation techniques, including their effectiveness and limitations in finance, healthcare, and law applications.

### Open Question 3
- Question: How can we ensure fairness and equity in LLM applications across diverse demographic groups in finance, healthcare, and law?
- Basis in paper: [explicit] The paper discusses bias and fairness as key ethical considerations in §6.1.2 and mentions domain-specific examples.
- Why unresolved: The paper highlights the importance of fairness but doesn't provide concrete strategies for ensuring fairness across diverse demographic groups in the three domains.
- What evidence would resolve it: A framework for evaluating and mitigating bias in LLM applications, including domain-specific metrics and techniques for promoting fairness and equity.

## Limitations
- The paper relies heavily on existing benchmarks without validating their adequacy for real-world deployment in high-stakes domains
- Proposed solutions lack detailed implementation guidance or empirical validation for the identified challenges
- Ethical considerations section provides limited concrete frameworks for systematically addressing explainability and fairness

## Confidence

High: The identification of domain-specific LLMs and their general capabilities (GPT-4, FinMA, BioViL)

Medium: The characterization of key challenges (data scarcity, multimodal handling, ethical risks)

Low: The proposed solutions and their effectiveness in addressing real-world deployment challenges

## Next Checks

1. Replicate LLM performance on at least one domain-specific benchmark (e.g., financial sentiment analysis using FinPH) to verify reported capabilities
2. Test multimodal document handling by evaluating an LLM on a complex financial report with tables and charts to assess the claimed performance benefits
3. Conduct a bias audit on LLM predictions across different demographic groups in a legal judgment prediction task to validate the ethical considerations discussed