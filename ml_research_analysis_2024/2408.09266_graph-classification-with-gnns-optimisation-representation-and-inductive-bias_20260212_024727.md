---
ver: rpa2
title: 'Graph Classification with GNNs: Optimisation, Representation and Inductive
  Bias'
arxiv_id: '2408.09266'
source_url: https://arxiv.org/abs/2408.09266
tags:
- graph
- pooling
- nodes
- graphs
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the implicit inductive bias in Graph Neural
  Networks (GNNs) for graph classification tasks, focusing on how different global
  pooling mechanisms affect the learned representations. The authors argue that the
  representation power of GNNs, often analyzed via Weisfeiler-Lehman tests, does not
  fully capture the optimization dynamics and inductive biases during training.
---

# Graph Classification with GNNs: Optimisation, Representation and Inductive Bias

## Quick Facts
- arXiv ID: 2408.09266
- Source URL: https://arxiv.org/abs/2408.09266
- Reference count: 40
- Primary result: Attention-based pooling in GNNs leads to preference for closely-connected substructures, while average pooling focuses on node presence regardless of connectivity

## Executive Summary
This paper investigates the implicit inductive biases in Graph Neural Networks (GNNs) for graph classification, focusing on how different global pooling mechanisms affect learned representations. The authors argue that traditional analyses based on Weisfeiler-Lehman tests don't fully capture optimization dynamics and inductive biases during training. Through theoretical analysis and empirical validation, they demonstrate that attention-based pooling leads GNNs to prefer closely-connected substructures as discriminative features, while average pooling focuses on node presence regardless of connectivity. These findings have practical implications for incorporating domain knowledge in graph classification tasks.

## Method Summary
The authors introduce a synthetic dataset of grid-graphs with planted subgraphs to study how GNNs learn to distinguish graphs based on subgraph presence. They theoretically prove that attention-based pooling leads to preference for closely-connected substructures, while average pooling focuses on node presence. The analysis examines the optimization dynamics and inductive biases during training, showing how different pooling mechanisms influence what features GNNs prioritize. Empirical validation is conducted on both synthetic and real-world datasets to verify these theoretical findings.

## Key Results
- Attention-based pooling consistently classifies graphs based on subgraph connectivity
- Average pooling does not consider connectivity, focusing only on node presence
- The choice of pooling mechanism significantly impacts the inductive bias of GNNs
- Theoretical analysis shows equivalence between optimization dynamics and inductive bias

## Why This Works (Mechanism)
The mechanism works because different pooling methods create distinct optimization landscapes that guide GNN training toward different local minima. Attention-based pooling emphasizes the importance of connected substructures by weighting node features based on their relationships, while average pooling treats all nodes equally regardless of connectivity. This leads to different feature representations that capture either structural connectivity patterns or mere node presence.

## Foundational Learning

1. **Weisfeiler-Lehman Test** - A graph isomorphism test used to analyze GNN representation power. Why needed: Provides baseline for understanding what graph structures GNNs can distinguish. Quick check: Can GNNs distinguish between graphs with different subgraph patterns?

2. **Global Pooling Mechanisms** - Methods for aggregating node features into graph-level representations. Why needed: Critical for graph classification tasks. Quick check: How do different pooling methods affect final classification?

3. **Inductive Bias** - The tendency of learning algorithms to prioritize certain hypotheses over others. Why needed: Explains why different architectures learn different features. Quick check: What features does the model prefer given a specific pooling method?

## Architecture Onboarding

Component map: Input Graphs -> GNN Layers -> Global Pooling -> Classification Head

Critical path: Graph features extracted by GNN layers are aggregated via pooling mechanism, then classified. The pooling step is crucial as it determines what structural information is preserved.

Design tradeoffs: Attention pooling captures connectivity but is computationally more expensive. Average pooling is simpler but may lose important structural information.

Failure signatures: Models may overfit to spurious correlations when pooling doesn't capture relevant structural patterns. Performance drops when test graphs have different connectivity patterns than training data.

First experiments:
1. Train with synthetic grid-graphs to observe pooling behavior
2. Ablation study removing attention weights
3. Compare performance across different graph densities

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes optimal GNN features, which may not hold in practice
- Real-world dataset experiments show mixed results, limiting generalizability
- Findings may not extend to deeper GNN architectures or different message passing schemes

## Confidence
- High confidence in theoretical analysis of pooling mechanisms' biases
- Medium confidence in practical implications for real-world datasets
- Medium confidence in synthetic dataset as representative testbed

## Next Checks
1. Test the proposed framework on larger-scale real-world datasets with varying graph characteristics (size, density, heterophily)
2. Extend the theoretical analysis to deeper GNN architectures and different message passing schemes
3. Conduct controlled experiments varying optimization hyperparameters to assess their impact on the emergence of inductive biases