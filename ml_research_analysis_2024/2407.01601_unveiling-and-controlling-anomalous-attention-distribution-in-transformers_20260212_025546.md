---
ver: rpa2
title: Unveiling and Controlling Anomalous Attention Distribution in Transformers
arxiv_id: '2407.01601'
source_url: https://arxiv.org/abs/2407.01601
tags:
- attention
- element
- elements
- first
- waiver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the anomalous attention phenomenon in Transformers
  where the first element receives disproportionately high attention. The authors
  propose that this occurs due to a "waiver phenomenon" where the model reduces certain
  elements' internal values to absorb excess attention without affecting their contribution
  to information.
---

# Unveiling and Controlling Anomalous Attention Distribution in Transformers

## Quick Facts
- arXiv ID: 2407.01601
- Source URL: https://arxiv.org/abs/2407.01601
- Reference count: 29
- Key outcome: This paper investigates the anomalous attention phenomenon in Transformers where the first element receives disproportionately high attention. The authors propose that this occurs due to a "waiver phenomenon" where the model reduces certain elements' internal values to absorb excess attention without affecting their contribution to information. They identify two selection strategies for waiver elements: positional-encoding-based (using learnable positional encoding) and feature-distribution-within-elements-based (based on non-mixed distribution patterns). Experiments with Llama3-8B and Bert-Large demonstrate that by modifying structured mask matrices or positional encodings, the model can be made to concentrate attention on artificially designated waiver elements, confirming their hypothesis.

## Executive Summary
This paper investigates the anomalous attention distribution phenomenon in Transformers where the first element consistently receives disproportionate attention across different architectures. The authors propose that this occurs through a "waiver phenomenon" where the model strategically reduces internal values of certain elements to absorb excess attention while maintaining their information contribution. They identify two distinct selection strategies for waiver elements based on architectural differences: positional-encoding-based for models with global attention, and feature-distribution-within-elements-based for models with causal attention. Through experiments with Llama3-8B and Bert-Large, the authors demonstrate that by modifying structured mask matrices or positional encodings, arbitrary elements can be designated as waiver elements, confirming their hypothesis about the underlying mechanism.

## Method Summary
The authors analyze attention patterns in Llama3-8B and Bert-Large models using Wikipedia text samples (100 samples > 1024 characters). They investigate the anomalous attention phenomenon by examining the relationship between attention weights and L2 norms of vectors in the Value matrix. To test controllability, they modify structured mask matrices for Llama3-8B and positional encodings for Bert-Large, observing how these changes affect attention distribution. The experiments involve visualizing attention patterns across multiple transformer layers and analyzing how the model responds to artificial modifications of the attention mechanism. The study focuses on understanding why the first element receives high attention and whether this behavior can be controlled to designate arbitrary elements as attention sinks.

## Key Results
- The first element receives disproportionately high attention across multiple transformer architectures due to a "waiver phenomenon" where the model reduces internal values to absorb excess attention
- Two distinct strategies for selecting waiver elements: positional-encoding-based (using learnable positional encoding) and feature-distribution-within-elements-based (based on non-mixed distribution patterns)
- By modifying structured mask matrices or positional encodings, the model can be controlled to concentrate attention on artificially designated waiver elements, confirming the hypothesis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The anomalous attention to the first element is caused by the model reducing internal values of certain elements to absorb excess attention without affecting their information contribution.
- Mechanism: The model uses a "waiver phenomenon" where it strategically reduces the L2 norm of vectors in the Value (V) matrix for specific elements, allowing them to absorb disproportionate attention weights while maintaining minimal impact on the weighted sum calculation.
- Core assumption: The model can learn to identify which elements should become waiver elements based on their relationship patterns with other elements in the sequence.
- Evidence anchors:
  - [abstract] "we analyze such a phenomenon from the perspective of waiver phenomenon, which involves reducing the internal values of certain elements in the sequence, allowing them to absorb excess attention without affecting their contribution to information"
  - [section] "we posit that the observed phenomenon is influenced by the attention pattern... the model uses learnable positional encoding to distinguish the waiver option elements"
  - [corpus] Weak - no direct corpus evidence for the waiver mechanism specifically
- Break condition: If the model cannot effectively reduce L2 norms of V matrix vectors without degrading overall performance, or if the softmax constraint cannot be satisfied with the remaining elements.

### Mechanism 2
- Claim: The model selects waiver elements using two distinct strategies based on architectural differences: positional-encoding-based and feature-distribution-within-elements-based.
- Mechanism: For models with causal attention (like Llama3-8B), the model identifies elements with non-mixed distribution patterns in their embedding vectors and reduces their V matrix norms. For models with global attention (like Bert-Large), the model uses learnable positional encodings where the first and last positions have disproportionately large L2 norms to identify waiver elements.
- Core assumption: Different transformer architectures lead to different selection mechanisms for waiver elements based on their attention patterns and positional encoding schemes.
- Evidence anchors:
  - [abstract] "we have found that the selection of waiver elements by the model can be categorized into two methods: positional-encoding-based and feature-distribution-within-elements-based"
  - [section] "We observe that in the meta-llama/Meta-Llama-3-8B... when the index of the first token is 128000... the model assigns high attention to the first element from the first layer and significantly lowers the L2 norm of the corresponding vector in the V matrix"
  - [section] "we found that after changing the first token, the model still exhibited the waiver phenomenon on the first element in the third layer. Additionally, in the google-bert/bert-large-cased-whole-word-masking (Bert-Large)... this waiver phenomenon not only appeared on the first element but also on the last element"
  - [corpus] Moderate - related work on attention sinks and Big Bird provides context but doesn't directly confirm the two-strategy mechanism
- Break condition: If models don't show the distinct selection patterns described, or if both strategies fail to explain waiver element selection across different architectures.

### Mechanism 3
- Claim: The model can be controlled to designate arbitrary elements as waiver options by modifying structured mask matrices or positional encodings.
- Mechanism: By adjusting the causal attention mask to prevent certain elements from being mixed through attention weighting, those elements exhibit non-mixed distribution and become waiver options. Alternatively, by replacing positional encoding vectors, other positions can inherit the waiver characteristics of first/last positions.
- Core assumption: The model's waiver selection is dependent on observable distributional patterns that can be artificially manipulated through architectural modifications.
- Evidence anchors:
  - [abstract] "we designed methods that can arbitrarily control whether an element becomes a waiver element based on the mechanisms of these two selection strategies"
  - [section] "We aim to adjust the feature distribution of element embedding vectors to control whether an element becomes a waiver option... In causal attention, we can control how the attention weighting mechanism affects a particular element by adjusting the structured mask matrix"
  - [section] "This strategy involves replacing the positional encoding embedding vectors corresponding to other positions with those corresponding to the first and last positions"
  - [corpus] Weak - no direct corpus evidence for the controllability experiments described
- Break condition: If modifying mask matrices or positional encodings doesn't result in the expected attention concentration on designated waiver elements.

## Foundational Learning

- Concept: Softmax normalization and its constraints
  - Why needed here: Understanding how softmax forces attention weights to sum to 1 is crucial for grasping why the model needs waiver elements to absorb excess attention
  - Quick check question: What happens to attention weights when one element receives very high attention in softmax, and how does this constrain the distribution?

- Concept: Vector norms and their relationship to attention mechanisms
  - Why needed here: The L2 norm reduction in V matrix vectors is the core mechanism by which waiver elements absorb attention without affecting information contribution
  - Quick check question: How does reducing the L2 norm of a vector affect its contribution when multiplied by a large attention weight?

- Concept: Positional encoding and attention patterns in different transformer variants
  - Why needed here: Different transformer architectures (causal vs global attention) use different mechanisms for selecting waiver elements, requiring understanding of these architectural differences
  - Quick check question: How do causal attention and global attention differ in their treatment of position dependencies, and how might this affect waiver element selection?

## Architecture Onboarding

- Component map: Query/Key/Value matrices in self-attention -> softmax function -> weighted sum with V matrix -> positional encoding schemes -> structured mask matrices that control attention patterns. The waiver phenomenon specifically involves manipulation of the V matrix vectors and their L2 norms.
- Critical path: The attention weight calculation (QK^T / √dk) → softmax → weighted sum with V matrix. The waiver mechanism intervenes by modifying V matrix norms or attention patterns before the weighted sum.
- Design tradeoffs: Using waiver elements allows the model to maintain attention distribution stability but may create bottlenecks where too much information flows through a single element. The tradeoff is between distribution stability and potential information compression.
- Failure signatures: If the model fails to reduce V matrix norms appropriately, attention may become unstable or collapse. If positional encodings don't create sufficient distributional differences, waiver element identification may fail. Excessive attention concentration on waiver elements may degrade overall model performance.
- First 3 experiments:
  1. Verify the L2 norm reduction in V matrix vectors for the first element across multiple transformer layers and architectures
  2. Test the controllability by modifying the structured mask matrix and observing whether designated elements become new attention sinks
  3. Replace positional encoding vectors for intermediate positions and measure whether those positions begin attracting disproportionate attention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the waiver phenomenon affect attention distribution in different model sizes and architectures beyond Llama3-8B and Bert-Large?
- Basis in paper: [inferred] The paper only tests on Llama3-8B and Bert-Large models, but the phenomenon is described as "prevalent across Transformer-based models"
- Why unresolved: The study is limited to two specific models, and the authors acknowledge the need for further investigation in different architectures
- What evidence would resolve it: Experiments on a diverse range of Transformer models (different sizes, attention patterns, positional encodings) showing consistent or varying waiver phenomena

### Open Question 2
- Question: What is the precise mechanism by which positional encoding influences the selection of waiver elements in global attention models?
- Basis in paper: [explicit] The paper identifies positional-encoding-based selection but does not fully explain the underlying mechanism
- Why unresolved: While the paper observes that positional encoding affects waiver element selection, it does not provide a detailed explanation of how this occurs
- What evidence would resolve it: Detailed analysis of how positional encoding interacts with attention mechanisms to influence waiver element selection, possibly through controlled experiments varying positional encoding properties

### Open Question 3
- Question: How does the waiver phenomenon impact model performance in downstream tasks, and can it be leveraged for performance optimization?
- Basis in paper: [explicit] The paper mentions that understanding the phenomenon is crucial for techniques like KV cache compression and infinite extrapolation, but does not explore performance impacts
- Why unresolved: The study focuses on explaining the phenomenon rather than its practical implications for model performance
- What evidence would resolve it: Empirical studies comparing model performance with and without waiver elements in various tasks, and exploring methods to optimize performance by controlling waiver element selection

## Limitations
- The proposed "waiver phenomenon" mechanism relies heavily on interpretation of attention patterns and vector norms without direct causal evidence
- The two-strategy framework (positional-encoding-based vs feature-distribution-within-elements-based) may be an oversimplification that doesn't generalize to all transformer variants
- Controllability experiments demonstrate manipulation is possible but don't prove the original model specifically employs these strategies for waiver selection

## Confidence
- **High Confidence**: The observation that the first element consistently receives disproportionate attention across multiple transformer architectures is well-supported by empirical evidence and reproducible across different models.
- **Medium Confidence**: The general concept that the model reduces V matrix norms to absorb excess attention is supported by observed correlations, but the specific "waiver phenomenon" terminology and interpretation as an intentional selection strategy requires further validation.
- **Low Confidence**: The two distinct selection strategies (positional-encoding-based and feature-distribution-within-elements-based) are based on pattern observations across limited architectures and may not generalize to all transformer variants.

## Next Checks
1. **Ablation Study of V Matrix Norm Reduction**: Systematically disable the ability to reduce V matrix norms for the first element (through parameter freezing or regularization) and observe whether the anomalous attention pattern persists. This would directly test whether V norm reduction is necessary for the phenomenon.

2. **Cross-Architecture Generalization Test**: Apply the controllability experiments (mask matrix and positional encoding modifications) to a diverse set of transformer architectures beyond Llama3-8B and Bert-Large, including encoder-only, decoder-only, and encoder-decoder models, to validate whether the two-strategy framework generalizes.

3. **Intervention Analysis with Gradient Attribution**: Use gradient-based attribution methods to determine whether changes to the first element's V matrix vectors directly influence the attention weights assigned to that element, providing stronger causal evidence for the proposed mechanism rather than mere correlation.