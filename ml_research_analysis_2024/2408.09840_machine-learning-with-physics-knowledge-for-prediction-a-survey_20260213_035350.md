---
ver: rpa2
title: 'Machine Learning with Physics Knowledge for Prediction: A Survey'
arxiv_id: '2408.09840'
source_url: https://arxiv.org/abs/2408.09840
tags:
- learning
- neural
- machine
- networks
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively reviews methods for integrating machine
  learning with physics knowledge for predictive modeling, focusing on partial differential
  equations. It examines techniques across three levels: incorporating physics knowledge
  through objective functions, structured models, and data augmentation; leveraging
  multi-task, meta, and contextual learning as data-driven approaches; and applying
  these methods to industrial applications.'
---

# Machine Learning with Physics Knowledge for Prediction: A Survey

## Quick Facts
- arXiv ID: 2408.09840
- Source URL: https://arxiv.org/abs/2408.09840
- Reference count: 40
- One-line primary result: Comprehensive survey of methods for integrating physics knowledge with machine learning for predictive modeling, focusing on PDEs and covering PINNs, neural operators, latent variable models, and data-driven approaches.

## Executive Summary
This survey comprehensively reviews methods for integrating machine learning with physics knowledge for predictive modeling, focusing on partial differential equations. It examines techniques across three levels: incorporating physics knowledge through objective functions, structured models, and data augmentation; leveraging multi-task, meta, and contextual learning as data-driven approaches; and applying these methods to industrial applications. The survey covers physics-informed neural networks, neural operators, latent variable models for dynamical systems, system identification, and physics-inspired model invariances. It provides a taxonomy for incorporating physical priors and discusses open-source libraries and datasets.

## Method Summary
The survey employs a comprehensive literature review methodology, synthesizing research across physics-informed machine learning approaches including physics-informed neural networks (PINNs), neural operators, latent variable models, and system identification techniques. The methodology categorizes approaches based on how they incorporate physics knowledge: through objective functions (like PINNs), structured predictive models (like DeepONets), data augmentation, and data-driven approaches such as multi-task, meta-, and contextual learning. The survey analyzes these methods through theoretical frameworks, practical implementations, and application domains, while also providing resources for reproducibility through open-source libraries and datasets.

## Key Results
- Physics-informed neural networks constrain model solutions toward physically plausible regions through physics-informed loss terms, improving generalization
- Operator learning methods enable generalization across different instances of physical systems without solving PDEs from scratch for each new configuration
- Latent variable models provide interpretable representations of system dynamics, enabling more efficient learning and prediction through simplified coordinate systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating physics knowledge through objective functions (e.g., PINNs) constrains model solutions toward physically plausible regions, reducing the search space and improving generalization.
- Mechanism: The physics-informed loss term penalizes violations of the governing equations and boundary conditions, effectively adding a regularization that biases the model toward solutions satisfying the underlying physics.
- Core assumption: The governing equations accurately represent the physical system, and the loss weighting balances physics and data terms appropriately.
- Evidence anchors:
  - [abstract]: "These methods have attracted significant interest due to their potential impact on advancing scientific research and industrial practices by improving predictive models with small- or large-scale datasets and expressive predictive models with useful inductive biases."
  - [section 3.3]: "Using these loss functions, learning PINNs can be considered unsupervised, as no data is required. However, physical knowledge is embedded in the loss definition through the specification of the PDE operator La in the residual term and provides a supervision signal during training."
  - [corpus]: "Physics-Informed Machine Learning for Grade Prediction in Froth Flotation" - weak evidence, no direct mechanistic description available.
- Break Condition: Incorrect physics models or poor loss balancing can lead to underfitting or solutions that satisfy the equations but not the true system behavior.

### Mechanism 2
- Claim: Operator learning methods (e.g., DeepONets, neural operators) enable generalization across different instances of the physical system without solving the PDE from scratch for each new configuration.
- Mechanism: By learning a mapping between function spaces, these models can predict solutions for new parameters or boundary conditions based on learned patterns from multiple training instances.
- Core assumption: The underlying physics is consistent across the training instances, and the model architecture can capture the necessary invariances.
- Evidence anchors:
  - [abstract]: "The second considers data as physics knowledge, which motivates looking at multi-task, meta, and contextual learning as an alternative approach to incorporating physics knowledge in a data-driven fashion."
  - [section 3.4]: "Addressing these contemporary issues of learning PDEs, directly learning the solution operator of the PDE as a map between infinite-dimensional spaces has gained traction in recent years."
  - [corpus]: "Replication Study: Enhancing Hydrological Modeling with Physics-Guided Machine Learning" - weak evidence, no specific operator learning details provided.
- Break Condition: Insufficient diversity in training data or model architecture limitations can prevent effective generalization to unseen configurations.

### Mechanism 3
- Claim: Latent variable models (e.g., Koopman theory, neural processes) provide interpretable representations of system dynamics, enabling more efficient learning and prediction.
- Mechanism: These models identify a coordinate system where the dynamics become simpler (e.g., linear in Koopman theory), reducing the complexity of the learning problem.
- Core assumption: An appropriate latent representation exists that simplifies the dynamics while capturing the essential system behavior.
- Evidence anchors:
  - [abstract]: "The survey has two parts. The first considers incorporating physics knowledge on an architectural level through objective functions, structured predictive models, and data augmentation. The second considers data as physics knowledge, which motivates looking at multi-task, meta, and contextual learning as an alternative approach to incorporating physics knowledge in a data-driven fashion."
  - [section 3.5]: "Latent variable models (LVM) refers to the assumption that an observed set of measurements are a consequence of an unobserved latent process. Much of statistics, signal processing and machine learning reduce to problems of this form."
  - [corpus]: "Meta-Learning and Knowledge Discovery based Physics-Informed Neural Network for Remaining Useful Life Prediction" - weak evidence, no detailed latent variable model discussion.
- Break Condition: Incorrect choice of latent representation or insufficient training data can lead to poor approximation of the true dynamics.

## Foundational Learning

- Concept: Ordinary Differential Equations (ODEs) and Partial Differential Equations (PDEs)
  - Why needed here: Many physical systems are naturally described by differential equations, and understanding their structure is crucial for designing physics-informed models.
  - Quick check question: What is the key difference between an ODE and a PDE in terms of the number of independent variables involved?

- Concept: System Identification
  - Why needed here: Learning the parameters of physical systems from data is a core task in physics-informed machine learning, bridging the gap between data and models.
- Quick check question: In system identification, what is the relationship between the Markov assumption and the factorization of the joint distribution p(Y, X | W, Î¸)?

- Concept: Function Approximation with Neural Networks
  - Why needed here: Neural networks serve as flexible function approximators in many physics-informed models, from PINNs to operator learning architectures.
  - Quick check question: What is the key advantage of using automatic differentiation in training physics-informed neural networks?

## Architecture Onboarding

- Component map:
  - Data input modules (context sets, boundary conditions, parameters)
  - Physics knowledge encoding (objective functions, model structures, invariances)
  - Neural network architectures (PINNs, DeepONets, Koopman-based models, etc.)
  - Training and optimization components (sampling strategies, loss balancing, etc.)
  - Evaluation and uncertainty quantification modules

- Critical path:
  1. Define the physical system and governing equations
  2. Choose appropriate physics knowledge encoding (objective functions, model structures, or data-driven approaches)
  3. Design the neural network architecture based on the chosen encoding
  4. Implement training with physics-informed losses and appropriate sampling strategies
  5. Evaluate model performance and uncertainty

- Design tradeoffs:
  - Model expressiveness vs. physical plausibility
  - Computational efficiency vs. accuracy
  - Data requirements vs. inductive bias strength
  - Interpretability vs. predictive performance

- Failure signatures:
  - Poor convergence during training (indicates issues with loss balancing or sampling)
  - High prediction errors in extrapolation regimes (suggests insufficient physics incorporation)
  - Unstable or non-physical predictions (indicates incorrect physics encoding or model architecture)

- First 3 experiments:
  1. Implement a simple PINN for a 1D heat equation with known analytical solution to verify basic functionality
  2. Compare different sampling strategies (uniform vs. quasi-Monte Carlo) for a PINN solving Poisson's equation
  3. Implement a DeepONet for learning the solution operator of a simple linear PDE across multiple parameter instances

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to balance multi-task learning and meta-learning for physics-informed prediction tasks?
- Basis in paper: [explicit] The paper discusses both multi-task learning and meta-learning approaches, noting that multi-task learning optimizes source and target tasks jointly while meta-learning considers source tasks only during pre-training for computationally efficient learning on target tasks.
- Why unresolved: The paper presents both approaches as viable but doesn't provide a framework for determining when one approach is superior to the other, or how to optimally combine them for different physics-informed learning scenarios.
- What evidence would resolve it: Systematic empirical comparisons across different physics domains showing performance trade-offs, and theoretical analysis of when each approach is more suitable based on characteristics like data availability, task similarity, and computational constraints.

### Open Question 2
- Question: How can physics-informed models effectively handle the reality gap between simulated data and real-world measurements?
- Basis in paper: [explicit] The paper mentions that real-world data might be effectively augmented with simulated data from physical models, but this decision introduces a reality gap that needs to be carefully handled during model learning.
- Why unresolved: While the paper acknowledges this challenge, it doesn't provide specific methodologies for bridging this gap or determining the optimal balance between simulated and real data sources.
- What evidence would resolve it: Empirical studies demonstrating effective techniques for domain adaptation or data weighting strategies, along with quantitative measures of performance improvements when handling the reality gap.

### Open Question 3
- Question: What is the most effective way to incorporate uncertainty quantification in physics-informed machine learning models?
- Basis in paper: [explicit] The paper notes that there is a need for more research on uncertainty quantification, especially for industrial applications operating in data-scarce environments, and highlights neural processes as promising approaches.
- Why unresolved: The paper identifies the importance of uncertainty quantification but doesn't provide a comprehensive framework for how to best incorporate it into different physics-informed learning approaches or how to validate these uncertainty estimates.
- What evidence would resolve it: Comparative studies of different uncertainty quantification methods across various physics domains, validation techniques for uncertainty estimates, and demonstration of how uncertainty quantification improves decision-making in practical applications.

## Limitations
- The field remains actively evolving with many methods being relatively new and their long-term performance characteristics not fully understood
- Effectiveness of approaches can be highly dependent on specific problem domain, data quality, and computational resources available
- Challenges exist in systematically benchmarking methods across different applications and the need for more standardized datasets and evaluation metrics

## Confidence
- **High Confidence**: The survey accurately represents the current state of research in physics-informed machine learning and provides a valuable taxonomy for understanding different approaches to incorporating physics knowledge
- **Medium Confidence**: Claims regarding effectiveness in improving reliability, robustness, and generalization are generally supported by examples provided, but more extensive empirical validation across diverse applications would strengthen these claims
- **Low Confidence**: Some speculative applications and future directions discussed may not yet be fully validated or may face unforeseen challenges in practical implementation

## Next Checks
1. **Empirical Benchmarking**: Conduct systematic benchmarking comparing performance of different physics-informed machine learning methods (PINNs, neural operators, Koopman-based models) on standardized problems with varying complexity and data availability
2. **Robustness Analysis**: Investigate robustness to noise, model misspecification, and distribution shifts; analyze impact on prediction accuracy and uncertainty quantification
3. **Scalability Assessment**: Evaluate computational efficiency and scalability for large-scale problems or high-dimensional systems; identify bottlenecks and explore potential optimizations