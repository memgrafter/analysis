---
ver: rpa2
title: 'TACT: Advancing Complex Aggregative Reasoning with Information Extraction
  Tools'
arxiv_id: '2406.03618'
source_url: https://arxiv.org/abs/2406.03618
tags:
- table
- text
- tact
- language
- pandas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TACT evaluates LLMs on aggregative reasoning tasks by combining
  information from text and structured tables to answer complex numerical instructions.
  It leverages the InstructIE dataset, transforming instances into challenging queries
  that require both information extraction and computation.
---

# TACT: Advancing Complex Aggregative Reasoning with Information Extraction Tools

## Quick Facts
- arXiv ID: 2406.03618
- Source URL: https://arxiv.org/abs/2406.03618
- Reference count: 40
- Primary result: Current LLMs struggle with complex aggregative reasoning tasks, achieving <38% accuracy on TACT

## Executive Summary
TACT introduces a benchmark for evaluating LLMs on complex numerical reasoning tasks that require information extraction from text and tables. The benchmark transforms instances from the InstructIE dataset into challenging queries requiring both information extraction and computation. By decomposing the problem into table-generation, Pandas command-generation, and execution subtasks, the authors reveal substantial headroom for improvement. Their IE as a tool approach, which implements each subtask as a separate tool with few-shot prompting, improves performance by up to 12% over standard prompting methods.

## Method Summary
TACT evaluates LLMs on aggregative reasoning by combining information from text and structured tables to answer complex numerical instructions. The method leverages the InstructIE dataset, transforming instances into challenging queries that require both information extraction and computation. The approach decomposes the problem into three subtasks: table-generation (extracting numerical information from text to create structured tables), Pandas command-generation (generating executable commands based on the extracted data), and execution (running the commands to produce final answers). Each subtask is implemented as a separate "tool" with few-shot prompting, allowing targeted optimization and reducing compounding errors.

## Key Results
- Current models struggle significantly on TACT tasks, with accuracies below 38%
- IE as a tool approach improves performance by up to 12% over standard prompting
- The decomposition reveals substantial headroom in each subtask, particularly table generation and command generation
- Gemini-1.0-Ultra achieves highest performance at 38%, while smaller models like Gemma-7b and Mistral-7b show moderate improvements with few-shot learning

## Why This Works (Mechanism)

### Mechanism 1: Task Decomposition
- **Claim**: Decomposing TACT into three subtasks enables targeted improvement by isolating weak model components
- **Mechanism**: Each subtask is implemented as a separate "tool" with few-shot prompting, allowing focused optimization
- **Core assumption**: Model performance on each subtask is independent enough that optimizing them separately will improve overall performance
- **Evidence anchors**: Abstract mentions "substantial headroom in each subtask" and section 4 describes the three-task decomposition

### Mechanism 2: Few-Shot Prompting
- **Claim**: Using few-shot prompting as a tool implementation method improves performance compared to standard prompting techniques
- **Mechanism**: Few-shot examples provide context for the model to learn task structure and expected output format
- **Core assumption**: The model can effectively learn from few examples without extensive fine-tuning
- **Evidence anchors**: Abstract mentions improvement over existing prompting techniques; section 5.1 discusses semantic similarity results

### Mechanism 3: Tool-Based Information Extraction
- **Claim**: Treating information extraction as a tool rather than a monolithic reasoning task enables better handling of complex numerical reasoning
- **Mechanism**: Breaking down the reasoning process into discrete steps (extract → transform → execute) allows the model to focus on one aspect at a time
- **Core assumption**: LLMs perform better when complex tasks are broken into simpler, sequential steps
- **Evidence anchors**: Abstract mentions "tools for each of the above steps"; section 3 describes the sequential invocation approach

## Foundational Learning

- **Concept**: Pandas data manipulation
  - Why needed here: TACT requires generating and executing Pandas commands to answer complex numerical queries
  - Quick check question: What is the Pandas command to filter rows where 'Crate Size' equals 'Medium' and sum the 'Weight (kg)' column?

- **Concept**: Information extraction from text
  - Why needed here: TACT involves extracting numerical information from unstructured text to populate tables
  - Quick check question: Given the text "The warehouse floor was a Tetris puzzle of wooden crates... twelve large crates... each a hefty 25 kilograms", what table structure would capture this information?

- **Concept**: Semantic similarity metrics
  - Why needed here: TACT evaluation uses semantic similarity (SentenceT5) to compare generated tables with gold tables
  - Quick check question: What is the difference between semantic similarity and lexical similarity in the context of table generation?

## Architecture Onboarding

- **Component map**: Text preprocessing → Table generation tool → Pandas command generation tool → Python interpreter → Final answer
- **Critical path**: Table generation → Pandas command generation → Execution
- **Design tradeoffs**: Few-shot prompting vs. fine-tuning; tool decomposition vs. monolithic approach; semantic vs. lexical evaluation metrics
- **Failure signatures**: 
  - Table generation fails → No valid Pandas commands can be generated
  - Pandas command generation fails → Execution errors or incorrect answers
  - Execution fails → Syntax errors in generated commands
- **First 3 experiments**:
  1. Compare few-shot vs. zero-shot performance on table generation task
  2. Test different semantic similarity thresholds for acceptable table generation
  3. Evaluate error propagation by deliberately introducing errors at each subtask stage

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of IE as a tool approach vary with different types of numerical reasoning tasks (e.g., arithmetic operations vs. counting)?
- **Basis in paper**: Explicit - The paper discusses improvement in performance using IE as a tool approach and mentions different types of instructions like "Calculate" and "Count"
- **Why unresolved**: The paper provides an overall improvement but does not specify how the approach performs across different types of numerical reasoning tasks
- **What evidence would resolve it**: Detailed performance metrics of IE as a tool approach for each type of numerical reasoning task

### Open Question 2
- **Question**: What are the specific challenges faced by smaller models like Gemma-7b and Mistral-7b in table generation and Pandas command generation?
- **Basis in paper**: Explicit - The paper mentions that smaller models show moderate improvements with few-shot learning and have lower performance in table validity
- **Why unresolved**: The paper does not delve into the specific reasons for the lower performance of smaller models in these tasks
- **What evidence would resolve it**: Analysis of error patterns and specific difficulties encountered by smaller models in generating tables and Pandas commands

### Open Question 3
- **Question**: How does the semantic similarity between generated and gold tables correlate with the final answer accuracy in TACT tasks?
- **Basis in paper**: Explicit - The paper evaluates table generation using semantic similarity and mentions that despite high semantic similarity, lexical similarity remains low
- **Why unresolved**: The paper does not establish a direct correlation between semantic similarity of tables and the accuracy of the final answers
- **What evidence would resolve it**: Statistical analysis showing the relationship between table semantic similarity scores and final answer accuracy

### Open Question 4
- **Question**: What are the potential improvements in IE as a tool approach if the tools for table generation and Pandas command generation are enhanced?
- **Basis in paper**: Explicit - The paper suggests that there is significant potential for enhancing overall task effectiveness through the refinement of IE tools
- **Why unresolved**: The paper does not explore specific enhancements or their impact on the performance of IE as a tool approach
- **What evidence would resolve it**: Comparative studies showing performance improvements with enhanced IE tools

## Limitations

- Substantial performance gap between top models (38% accuracy) and others (<22% accuracy) suggests methodology may be less important than model selection
- Relatively small dataset size (124 examples) raises questions about generalization to broader reasoning tasks
- Semantic similarity metric may not fully capture semantic equivalence needed for accurate table generation, as lexical similarity remains low despite high semantic similarity scores

## Confidence

- **High confidence**: The decomposition approach for TACT tasks is well-grounded and demonstrates measurable improvements over baseline methods
- **Medium confidence**: The effectiveness of few-shot prompting as implemented in the IE as a tool framework shows promise but requires more extensive validation
- **Low confidence**: The semantic similarity metric's ability to fully capture table generation quality remains uncertain, particularly given the gap between semantic and lexical similarity measures

## Next Checks

1. **Cross-dataset validation**: Test the IE as a tool approach on additional numerical reasoning datasets beyond TACT to assess generalizability and robustness of the methodology

2. **Error propagation analysis**: Conduct systematic ablation studies to quantify how errors at each subtask stage impact final answer accuracy, and whether error correction mechanisms can be implemented

3. **Semantic similarity refinement**: Compare multiple semantic similarity metrics (beyond SentenceT5) and evaluate their correlation with human judgment of table generation quality to ensure the evaluation methodology accurately captures semantic equivalence