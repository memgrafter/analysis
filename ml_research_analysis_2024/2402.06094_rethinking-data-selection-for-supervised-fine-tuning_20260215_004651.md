---
ver: rpa2
title: Rethinking Data Selection for Supervised Fine-Tuning
arxiv_id: '2402.06094'
source_url: https://arxiv.org/abs/2402.06094
tags:
- data
- instances
- instruction
- responses
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper questions the common approach of selecting high-quality
  or diverse data for supervised fine-tuning (SFT) of large language models. Instead,
  it proposes that data selection should focus on demonstrations that reflect human-like
  interactions, hypothesizing that detailed responses are more helpful.
---

# Rethinking Data Selection for Supervised Fine-Tuning

## Quick Facts
- arXiv ID: 2402.06094
- Source URL: https://arxiv.org/abs/2402.06094
- Authors: Ming Shen
- Reference count: 40
- Primary result: Selecting instances with long responses improves SFT performance more than quality/diversity selection or full dataset training

## Executive Summary
This paper challenges conventional wisdom about data selection for supervised fine-tuning (SFT) of large language models. Rather than focusing on data quality or diversity, the author proposes that selecting instances with long, detailed responses better captures human-like conversational patterns. Experiments across multiple SFT datasets (Alpaca, WizardLM, Dolly) show that this simple heuristic consistently outperforms baseline strategies. The key insight is that SFT primarily teaches style and conversational formatting rather than new knowledge, making the mimicry of human interaction patterns more valuable than traditional quality metrics.

## Method Summary
The study finetunes LLaMA-2-7B on subsets of SFT data selected by response length, comparing against baselines (full dataset, random, quality-based, diversity-based). Data selection strategies include response length ranking, K-Means clustering for diversity, and ChatGPT quality scoring. The models are evaluated using GPT-4 pairwise comparisons with bias mitigation, benchmark tasks (LongForm, ELI5, ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, GSM8K, BBHA, VG1, VG2), and human inspection of winning/losing pairs.

## Key Results
- Selecting top 1K longest responses from Alpaca dataset outperforms full dataset training
- The response length heuristic works across multiple SFT datasets (Alpaca, WizardLM, Dolly)
- GPT-4 judge analysis shows preference for detailed responses even when content quality is similar
- Human inspection confirms that longer responses provide richer conversational patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Longer responses contain more detailed information mimicking human conversation
- **Core assumption**: Pretraining covers most knowledge; SFT teaches conversational style
- **Evidence**: GPT-4 prefers detailed responses in 33/50 pairs with similar content quality
- **Break condition**: Artificial padding without substantive content

### Mechanism 2
- **Claim**: SFT is superficial, focusing on style over knowledge acquisition
- **Core assumption**: Knowledge comes from pretraining, not fine-tuning
- **Evidence**: Paper frames SFT as "style learning" rather than knowledge transfer
- **Break condition**: Insufficient pretraining for target domain

### Mechanism 3
- **Claim**: Detailed responses provide richer training signals for response organization
- **Core assumption**: More detailed responses contain more useful conversational patterns
- **Evidence**: Preference for detailed responses in pairwise comparisons
- **Break condition**: Tasks requiring concise rather than detailed responses

## Foundational Learning

- **Concept**: Pretraining vs. Fine-tuning distinction
  - Why needed: Core argument relies on understanding SFT teaches style, not knowledge
  - Quick check: If a model knows Paris is France's capital from pretraining, what is SFT's primary role for capital city questions?

- **Concept**: Data quality vs. data relevance for style learning
  - Why needed: Challenges conventional wisdom that higher quality is always better
  - Quick check: Why might a lower "quality" but more detailed response be more valuable for SFT than a higher "quality" brief response?

- **Concept**: Human evaluation bias in LLM assessment
  - Why needed: Evaluation relies on GPT-4, which may have verbosity bias
  - Quick check: How does the paper attempt to mitigate position bias in GPT-4 pairwise comparisons?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Model training -> Evaluation -> Analysis
- **Critical path**: Data selection → Model fine-tuning → Evaluation → Analysis
- **Design tradeoffs**: Simple heuristic vs. complex metrics; computational cost vs. accuracy
- **Failure signatures**: Performance degrades on concise response tasks; evaluation shows strong verbosity bias
- **First 3 experiments**: 
  1. Compare top 1K longest vs. full dataset using same evaluation protocol
  2. Test heuristic robustness across different SFT datasets
  3. Evaluate whether long-response models generate more detailed outputs by human inspection

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do specific linguistic features (politeness markers, discourse markers, structured formats) impact SFT effectiveness?
- **Basis**: Paper mentions these features but only investigates response length
- **Why unresolved**: Acknowledges more features exist but doesn't explore their impact
- **Evidence needed**: Experiments comparing models trained on datasets selected by different linguistic features

### Open Question 2
- **Question**: What is the optimal selection size for long responses across different SFT datasets?
- **Basis**: Different optimal sizes found for Alpaca (1K) vs. WizardLM (6%)
- **Why unresolved**: Only specific sizes tested, not systematic investigation
- **Evidence needed**: Varying selection sizes across datasets to identify optimal sizes

### Open Question 3
- **Question**: How does instruction type distribution in selected instances affect generalization performance?
- **Basis**: Analysis shows 70% of selected instructions are "write/generate/create/compose"
- **Why unresolved**: Doesn't investigate correlation between instruction type distribution and task-specific performance
- **Evidence needed**: Experiments analyzing instruction type distribution and evaluating on various task types

## Limitations

- Evaluation relies heavily on GPT-4 judge, which may have bias toward verbose responses
- Results may not generalize beyond LLaMA-2-7B architecture and tested datasets
- Assumes pretraining provides sufficient knowledge, which may not hold for specialized domains

## Confidence

- **High Confidence**: Empirical finding that response length selection consistently outperforms baselines across multiple datasets
- **Medium Confidence**: Mechanism explanation about mimicry of human conversational style
- **Medium Confidence**: Broader claim that quality/diversity metrics are less important for SFT

## Next Checks

1. Conduct human evaluation on stratified sample of response pairs to verify GPT-4 preferences align with human judgment
2. Test response length heuristic on domain-specific SFT datasets where knowledge acquisition is more critical
3. Evaluate scalability of the strategy across different model sizes (smaller and larger than LLaMA-2-7B)