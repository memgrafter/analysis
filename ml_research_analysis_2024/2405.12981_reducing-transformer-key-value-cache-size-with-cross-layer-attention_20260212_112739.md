---
ver: rpa2
title: Reducing Transformer Key-Value Cache Size with Cross-Layer Attention
arxiv_id: '2405.12981'
source_url: https://arxiv.org/abs/2405.12981
tags:
- attention
- cache
- layer
- learning
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cross-Layer Attention (CLA), a method to
  reduce the memory footprint of transformer-based language models by sharing key/value
  activations across layers. CLA is inspired by Multi-Query Attention (MQA) and Grouped-Query
  Attention (GQA), which reduce memory usage by sharing key/value heads across query
  heads within a layer.
---

# Reducing Transformer Key-Value Cache Size with Cross-Layer Attention

## Quick Facts
- arXiv ID: 2405.12981
- Source URL: https://arxiv.org/abs/2405.12981
- Reference count: 14
- Primary result: CLA achieves 2× reduction in KV cache size while maintaining accuracy when combined with MQA

## Executive Summary
This paper introduces Cross-Layer Attention (CLA), a method to reduce the memory footprint of transformer-based language models by sharing key/value activations across layers. CLA extends Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) by also sharing key/value heads across adjacent layers, enabling further memory savings. Extensive pretraining experiments on 1B- and 3B-parameter models demonstrate that CLA provides a Pareto improvement in accuracy/memory tradeoffs compared to MQA and GQA. The best results are achieved with a sharing factor of 2 and when used in conjunction with MQA.

## Method Summary
CLA computes key/value projections for only a subset of layers in the model, with attention blocks in layers without key/value projections reusing the KV activations from previous layers. This extends the concept of MQA/GQA, which reduce memory by sharing KV heads within a layer, to also sharing across layers. The method groups consecutive layers to share the same KV projections, reducing storage requirements by a factor equal to the sharing factor. Models are trained on the SlimPajama dataset using AdamW optimizer with sequence length 2048, batch size 2048, and learning rate 3e-4.

## Key Results
- CLA with sharing factor 2 achieves 2× reduction in KV cache size while maintaining nearly the same accuracy as unmodified MQA
- Uniform sharing patterns (pairs of consecutive layers) perform better than concentrated sharing at beginning/end
- CLA is most effective when combined with MQA, providing a Pareto improvement in accuracy-memory tradeoffs
- Models using CLA benefit from higher learning rates than comparable non-CLA models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLA reduces KV cache memory by sharing key/value projections across multiple transformer layers
- Mechanism: Traditional transformers require 2 · nquery · dhead elements per token for KV caching. CLA reduces this to 2 · nquery · dhead / S elements per token by sharing KV projections across S consecutive layers
- Core assumption: Shared KV activations from earlier layers remain relevant for attention computations in subsequent layers
- Evidence anchors: [abstract], [section], [corpus: weak]
- Break condition: If shared KV activations become stale or irrelevant for later layers, accuracy will degrade significantly

### Mechanism 2
- Claim: CLA achieves Pareto improvement by maintaining accuracy while reducing memory
- Mechanism: Careful selection of which layers share KV activations minimizes accuracy loss while maximizing memory savings. Shared KV projections force layers to communicate through compressed representations
- Core assumption: Consecutive layers have similar information needs, making KV sharing between them less harmful than sharing across distant layers
- Evidence anchors: [abstract], [section], [corpus: weak]
- Break condition: If model architecture changes significantly, the optimal sharing pattern may change

### Mechanism 3
- Claim: CLA interacts beneficially with MQA/GQA by compounding memory savings
- Mechanism: MQA/GQA reduces KV heads within a layer while CLA reduces layers that compute KV heads. Combined effect multiplies memory reduction: (nquery / ngroup) × (1 / sharing_factor)
- Core assumption: Memory savings from MQA/GQA and CLA are independent and multiplicative
- Evidence anchors: [abstract], [section], [corpus: moderate]
- Break condition: If MQA/GQA already uses minimal KV heads, additional CLA savings may be negligible or harmful

## Foundational Learning

- Concept: Transformer attention mechanism and KV cache
  - Why needed here: Understanding how attention works and why KV caching is necessary for efficient decoding is fundamental to grasping CLA's purpose
  - Quick check question: Why do we need to cache KV activations during autoregressive decoding instead of recomputing them?

- Concept: Multi-Query Attention (MQA) and Grouped-Query Attention (GQA)
  - Why needed here: CLA builds directly on these techniques, so understanding their mechanisms and tradeoffs is essential
  - Quick check question: How does MQA reduce KV cache size compared to standard Multi-Head Attention?

- Concept: Pareto efficiency in model design
  - Why needed here: The paper claims CLA provides a Pareto improvement, meaning better accuracy-memory tradeoffs
  - Quick check question: What does it mean for one model configuration to Pareto-dominate another?

## Architecture Onboarding

- Component map: Query projection layer -> (shared) Key/value projection layer -> Attention computation -> Output projection layer -> Layer norm
- Critical path: 1) Compute query projections in each layer 2) For layers with KV projections, compute and cache KV 3) For layers without KV projections, reuse cached KV from designated sharing layer 4) Perform attention computation 5) Apply output projection and layer norm
- Design tradeoffs: Memory vs. accuracy (higher sharing factors reduce memory more but may hurt accuracy), uniform vs. non-uniform sharing (uniform performs better), integration with MQA/GQA (works best when combined)
- Failure signatures: Accuracy degradation (if sharing factor too high or pattern suboptimal), training instability (if layer norm parameters not properly separated), memory leaks (if KV cache not properly shared/reused)
- First 3 experiments: 1) Implement CLA with sharing factor 2 on small model and verify 2× memory reduction 2) Compare perplexity of CLA2 vs. baseline MQA at same memory budget 3) Test different sharing patterns to verify optimal patterns

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following areas remain unexplored:
- Performance on sequence lengths longer than 2048 tokens
- Optimal learning rates at scales larger than 3B parameters
- Interaction with retrieval-augmented language models

## Limitations
- Limited ablation studies on sharing factor optimization and sharing patterns
- Experiments only conducted on 1B- and 3B-parameter models, leaving uncertainty about scalability
- No exploration of CLA performance on tasks requiring long-range dependencies versus short-range tasks

## Confidence
- CLA provides Pareto improvements: High confidence (extensive pretraining experiments)
- Optimal sharing factor of 2: Medium confidence (limited ablation studies)
- CLA works best with MQA: Medium confidence (some evidence but not extensively tested)
- Uniform sharing patterns perform better: Medium confidence (limited pattern ablation)

## Next Checks
1. Implement CLA with varying sharing factors (1-8) on a 1B-parameter model to empirically verify the optimal sharing factor identified in the paper
2. Test CLA on models larger than 3B parameters to assess scalability and whether Pareto improvements hold at larger scales
3. Compare CLA performance on tasks requiring long-range dependencies versus short-range tasks to understand when cross-layer KV sharing is most beneficial