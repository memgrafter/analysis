---
ver: rpa2
title: Are Large Language Models True Healthcare Jacks-of-All-Trades? Benchmarking
  Across Health Professions Beyond Physician Exams
arxiv_id: '2406.11328'
source_url: https://arxiv.org/abs/2406.11328
tags:
- medical
- empec
- chinese
- questions
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMPEC, a large-scale healthcare knowledge
  benchmark in traditional Chinese, covering 157,803 exam questions across 124 subjects
  and 20 healthcare professions. The benchmark aims to evaluate the performance of
  large language models (LLMs) beyond physician exams, including underrepresented
  professions like optometrists and audiologists.
---

# Are Large Language Models True Healthcare Jacks-of-All-Trades? Benchmarking Across Health Professions Beyond Physician Exams

## Quick Facts
- arXiv ID: 2406.11328
- Source URL: https://arxiv.org/abs/2406.11328
- Reference count: 40
- One-line primary result: EMPEC benchmark reveals current LLMs struggle with specialized healthcare professions despite strong overall performance

## Executive Summary
This paper introduces EMPEC, a large-scale healthcare knowledge benchmark in traditional Chinese covering 157,803 exam questions across 124 subjects and 20 healthcare professions. The benchmark aims to evaluate LLM performance beyond physician exams, including underrepresented professions like optometrists and audiologists. Through extensive experiments on 17 LLMs, including both general and medical-specific models, the study reveals that while leading models like GPT-4 achieve over 75% accuracy, they struggle with specialized fields and alternative medicine. Surprisingly, general-purpose LLMs outperformed medical-specific models, and incorporating EMPEC's training data significantly enhanced performance.

## Method Summary
The study evaluates 17 large language models on the EMPEC benchmark, which contains 157,803 multiple-choice questions across 124 subjects and 20 healthcare professions in traditional Chinese. The dataset is split into training (149,603), validation (200), and test (8,000) sets. Models are tested through zero-shot prompting using specific templates, with accuracy measured based on correct answer selection. Additionally, the Qwen1.5-7B model is fine-tuned on EMPEC training data for 3 epochs with learning rate 1e-4 on 2 A100 GPUs using vLLM 0.7.2 with greedy decoding to assess the impact of domain-specific training.

## Key Results
- General-purpose LLMs (GPT-4, GPT-3.5) outperform medical-specific models (HuatuoGPT2, MMedLM2, MedGPT) on healthcare benchmarks
- Leading models achieve over 75% accuracy but struggle significantly with specialized professions like Dental Technicians and Traditional Chinese Medicine
- Fine-tuning on EMPEC training data improves performance by nearly 11% for the Qwen1.5-7B model
- Traditional-to-Simplified Chinese conversion has negligible impact on model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** EMPEC provides a more robust evaluation of LLMs in healthcare by covering 20 professions rather than just physicians.
- **Mechanism:** By expanding beyond physician-focused benchmarks, EMPEC tests model knowledge across a broader range of healthcare roles, exposing gaps in understanding for underrepresented professions like dentists and optometrists.
- **Core assumption:** Broader profession coverage correlates with better real-world healthcare applicability.
- **Evidence anchors:**
  - [abstract]: "EMPEC goes beyond the commonly assessed professions such as Physicians and Nurses, to include occupations like Optometrist and Audiologist often overlooked in previous assessments."
  - [section]: "EMPEC provides a thorough evaluation of knowledge and expertise of 20 healthcare professions."
  - [corpus]: Weak - no direct evidence that broader coverage improves real-world performance.
- **Break condition:** If model performance on EMPEC does not correlate with performance in real healthcare tasks.

### Mechanism 2
- **Claim:** Fine-tuning on EMPEC training data significantly improves model performance on healthcare tasks.
- **Mechanism:** Training on EMPEC's diverse question set provides models with a richer representation of healthcare knowledge across professions, improving their ability to answer related queries.
- **Core assumption:** Domain-specific fine-tuning on diverse medical data improves model performance on similar tasks.
- **Evidence anchors:**
  - [abstract]: "incorporating EMPEC's training data significantly enhanced performance."
  - [section]: "our fine-tuned Qwen1.5-7B model achieves 61% accuracy, nearly 11% better than the Chat counterpart."
  - [corpus]: Weak - no evidence that EMPEC fine-tuning generalizes beyond the test set.
- **Break condition:** If fine-tuning on EMPEC does not improve performance on unseen healthcare tasks.

### Mechanism 3
- **Claim:** Traditional Chinese to Simplified Chinese conversion has negligible impact on model performance.
- **Mechanism:** Models exhibit robust linguistic versatility, maintaining performance across character variants due to similar co-occurrence patterns in both forms.
- **Core assumption:** Linguistic robustness across character variants implies broader applicability.
- **Evidence anchors:**
  - [abstract]: "The transition from traditional to simplified Chinese characters had a negligible impact on model performance, indicating robust linguistic versatility."
  - [section]: "We assume that the character difference in traditional and simplified Chinese does not affect the model's performance as the co-occurrence relations of these characters are similar in the two kinds of Chinese."
  - [corpus]: Weak - no evidence that this robustness generalizes to other linguistic variations.
- **Break condition:** If performance degrades significantly with other linguistic variations.

## Foundational Learning

- **Concept:** Healthcare profession diversity
  - **Why needed here:** Understanding the range of healthcare roles is crucial for interpreting EMPEC's coverage and significance.
  - **Quick check question:** Why is it important to include professions like optometrists and audiologists in healthcare benchmarks?

- **Concept:** Benchmark design and evaluation
  - **Why needed here:** Knowledge of how benchmarks are constructed and evaluated is essential for understanding EMPEC's methodology and results.
  - **Quick check question:** How does stratified sampling ensure balanced representation across professions in EMPEC?

- **Concept:** Domain adaptation in LLMs
  - **Why needed here:** Understanding domain adaptation is key to interpreting the results of medical-specific versus general LLMs.
  - **Quick check question:** Why might medical-specific LLMs underperform general LLMs on EMPEC despite domain adaptation?

## Architecture Onboarding

- **Component map:** Data collection → Pre-processing → Dataset creation → Model testing → Analysis → Fine-tuning → Re-evaluation
- **Critical path:** EMPEC dataset creation → Zero-shot evaluation of 17 LLMs → Fine-tuning Qwen1.5-7B → Performance analysis
- **Design tradeoffs:** EMPEC prioritizes breadth (20 professions) over depth (fewer questions per profession), potentially limiting its ability to deeply assess specific roles.
- **Failure signatures:** Poor performance on EMPEC may indicate overfitting to physician-centric data or lack of domain-specific training.
- **First 3 experiments:**
  1. Evaluate a baseline model (random guess) on EMPEC to establish a performance floor.
  2. Test a general-purpose LLM on EMPEC to compare with physician-focused benchmarks.
  3. Fine-tune a general-purpose LLM on EMPEC training data and re-evaluate to measure improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of specialized healthcare knowledge (e.g., Dental Technicians, Optometrists, TCM Practitioners) are most challenging for current LLMs to master?
- Basis in paper: [explicit] The paper states that LLMs struggle with specialized fields and alternative medicine, particularly in professions like Dental Technicians, Optometrists, and TCM Practitioners.
- Why unresolved: The paper identifies the struggle but does not delve into the specific types of knowledge or question formats that pose the greatest challenge.
- What evidence would resolve it: Detailed analysis of error patterns and question types where LLMs consistently fail for specialized professions.

### Open Question 2
- Question: What specific training strategies or data augmentations could improve the performance of medical domain-specific LLMs on comprehensive healthcare benchmarks like EMPEC?
- Basis in paper: [inferred] The paper notes that medical domain LLMs (HuatuoGPT2, MMedLM2, MedGPT) underperform compared to general-purpose models, suggesting that current domain adaptation methods are insufficient.
- Why unresolved: The paper does not explore alternative training approaches or data strategies to address this performance gap.
- What evidence would resolve it: Experimental results comparing different training strategies (e.g., multi-task learning, curriculum learning, data augmentation) on EMPEC performance.

### Open Question 3
- Question: How does the performance of LLMs on healthcare benchmarks correlate with their ability to handle real-world clinical scenarios and patient interactions?
- Basis in paper: [explicit] The paper emphasizes the need to expand benchmarks beyond physician exams to better assess LLMs' applicability in real-world healthcare scenarios.
- Why unresolved: The paper focuses on exam-based evaluation but does not investigate the correlation between exam performance and real-world clinical competence.
- What evidence would resolve it: Comparative studies evaluating LLM performance on both exam benchmarks and real-world clinical case simulations or patient interaction scenarios.

## Limitations

- EMPEC focuses exclusively on Chinese-language exams, limiting generalizability to other languages and clinical contexts
- The study relies on multiple-choice questions which may not fully capture the complexity of real-world healthcare scenarios
- No investigation into whether EMPEC performance correlates with actual clinical task performance

## Confidence

- General-purpose LLMs outperform medical-specific models: **High**
- EMPEC reveals gaps in physician-centric benchmarks: **Medium**
- Traditional-to-simplified Chinese conversion has negligible impact: **Medium**
- Fine-tuning on EMPEC data improves performance: **High**
- Broader profession coverage improves real-world healthcare applicability: **Low**

## Next Checks

1. Test whether EMPEC performance correlates with actual clinical task performance by having models complete realistic healthcare scenarios beyond multiple-choice questions.

2. Evaluate the same models on physician-focused benchmarks like MedQA to determine if the general-purpose advantage persists across different healthcare evaluation frameworks.

3. Conduct ablation studies by removing specific professions from EMPEC to identify which professional domains contribute most to the observed performance patterns and gaps.