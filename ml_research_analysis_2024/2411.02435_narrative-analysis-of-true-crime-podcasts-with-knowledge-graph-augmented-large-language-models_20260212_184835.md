---
ver: rpa2
title: Narrative Analysis of True Crime Podcasts With Knowledge Graph-Augmented Large
  Language Models
arxiv_id: '2411.02435'
source_url: https://arxiv.org/abs/2411.02435
tags:
- knowledge
- information
- graph
- narrative
- graphrag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work applies knowledge graph-augmented large language models
  (KGLLMs) to analyze the true crime podcast Serial. The KGLLM combines GraphRAG for
  knowledge graph construction with GPT-4o-mini for querying, outperforming standard
  LLMs and RAG on question-answering tasks across 36 queries with superior comprehensiveness,
  directness, diversity, and empowerment scores.
---

# Narrative Analysis of True Crime Podcasts With Knowledge Graph-Augmented Large Language Models

## Quick Facts
- arXiv ID: 2411.02435
- Source URL: https://arxiv.org/abs/2411.02435
- Reference count: 40
- Key outcome: KGLLMs outperform standard LLMs on narrative analysis tasks for Serial podcast with superior comprehensiveness, directness, diversity, and empowerment scores

## Executive Summary
This work applies knowledge graph-augmented large language models (KGLLMs) to analyze the true crime podcast Serial. The KGLLM combines GraphRAG for knowledge graph construction with GPT-4o-mini for querying, demonstrating superior performance on question-answering tasks across 36 queries. When tested with adversarial prompts designed to induce hallucinations, the KGLLM showed greater robustness than baseline models. The KGLLM also produced more detailed topic summaries compared to classical methods like BERTopic, better capturing narrative nuances including public sentiment and cultural identity aspects of the case.

## Method Summary
The study preprocessed the Serial podcast transcript into a structured format with episode numbers, timestamps, and speaker labels. A knowledge graph was constructed using GraphRAG with chunk size set to 600 tokens and GPT-4o-mini as the base model. The KGLLM's performance was evaluated by testing question-answering tasks, adversarial prompts, topic modeling, sentiment analysis, and hearsay detection against baseline models including RAG and standalone GPT approaches.

## Key Results
- KGLLM outperformed standard LLMs on question-answering tasks across 36 queries with superior comprehensiveness, directness, diversity, and empowerment scores
- KGLLM demonstrated greater robustness to adversarial prompts, refusing to give definitive answers despite suggestive prompts
- KGLLM produced more detailed topic summaries compared to classical methods like BERTopic, better capturing narrative nuances including public sentiment and cultural identity aspects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KGLLMs outperform standard LLMs on narrative tasks due to their ability to encode relational information from knowledge graphs
- Mechanism: GraphRAG uses an LLM to extract entities and relations from text, then clusters them hierarchically, creating a knowledge graph that augments the LLM's ability to reason about relationships between entities in the narrative
- Core assumption: The hierarchical clustering of the knowledge graph captures meaningful relationships between entities in the narrative that are relevant to answering questions
- Evidence anchors:
  - [abstract]: "Our results indicate that KGLLMs outperform LLMs on a variety of metrics, are more robust to adversarial prompts, and are more capable of summarizing the text into topics."
  - [section]: "GraphRAG automates the knowledge graph construction which may be biased by our understanding of the story and reduces the amount of human labor required."
- Break condition: If the hierarchical clustering algorithm fails to identify meaningful relationships between entities, or if the LLM's entity and relation extraction is inaccurate

### Mechanism 2
- Claim: KGLLMs are more robust to adversarial prompts because they can distinguish between information from the narrative and external information
- Mechanism: The knowledge graph stores information extracted from the narrative, allowing the KGLLM to cross-reference external information against the graph. If the external information conflicts with the graph, the KGLLM can refuse to provide a definitive answer
- Core assumption: The knowledge graph accurately represents the information in the narrative, and the KGLLM can effectively compare external information against the graph
- Evidence anchors:
  - [abstract]: "When tested with adversarial prompts designed to induce hallucinations, the KGLLM demonstrated greater robustness than baseline models, refusing to give definitive answers despite suggestive prompts."
  - [section]: "Our most successful attempt at deceiving GraphRAG included more suggestive information in the prompt... all four models accept the hammer's presence in the podcast. Although the hammer never appears in the podcast, the models accept this information and use it in their response in a convincing manner. While GraphRAG uses this false information, it appears to be more measured in its response when compared to the other models."
- Break condition: If the knowledge graph is inaccurate or incomplete, or if the KGLLM's comparison mechanism fails

### Mechanism 3
- Claim: KGLLMs can capture more nuanced and relevant topics in narrative analysis compared to classical topic modeling methods
- Mechanism: The knowledge graph provides a richer representation of the relationships between entities in the narrative, allowing the KGLLM to identify more subtle themes and connections
- Core assumption: The knowledge graph accurately captures the relationships between entities in the narrative, and the KGLLM can effectively use this information to identify topics
- Evidence anchors:
  - [abstract]: "The KGLLM also produced more detailed topic summaries compared to classical methods like BERTopic, better capturing narrative nuances including public sentiment and cultural identity aspects of the case."
  - [section]: "GraphRAG provides a more comprehensive account of the discovery of Hae's body. It detects thematic keywords such as 'Murder Investigation' and lists a more precise location of where Hae's body was found ('Leakin Park' instead of 'woods')."
- Break condition: If the knowledge graph fails to capture meaningful relationships between entities, or if the KGLLM's topic modeling mechanism is ineffective

## Foundational Learning

- Concept: Knowledge graphs
  - Why needed here: Knowledge graphs provide a structured representation of the relationships between entities in the narrative, which is crucial for KGLLMs to reason about the narrative and answer questions
  - Quick check question: What are the key components of a knowledge graph, and how do they relate to the entities and relationships in a narrative?

- Concept: Adversarial prompting
  - Why needed here: Adversarial prompting is used to test the robustness of KGLLMs to hallucinations and external information that conflicts with the narrative
  - Quick check question: What are the different types of adversarial prompts, and how can they be used to assess the robustness of a KGLLM?

- Concept: Topic modeling
  - Why needed here: Topic modeling is used to summarize the narrative into a smaller collection of topics, which helps to understand the higher-level themes and connections in the story
  - Quick check question: What are the different approaches to topic modeling, and how do they differ in their ability to capture nuanced and relevant topics in a narrative?

## Architecture Onboarding

- Component map: Input text data -> Preprocessing (entity/relation extraction) -> Knowledge graph construction (GraphRAG) -> Query processing (local/global retrieval) -> Output generation

- Critical path: 1. Preprocess the text data 2. Construct the knowledge graph using GraphRAG 3. Query the knowledge graph using local and global retrieval mechanisms 4. Generate answers to questions and topic summaries

- Design tradeoffs:
  - Accuracy vs. efficiency: More accurate entity and relation extraction may require more computational resources
  - Complexity vs. interpretability: A more complex knowledge graph may capture more nuanced relationships, but it may be harder to interpret
  - Robustness vs. flexibility: A more robust KGLLM may be less flexible in handling new or unexpected information

- Failure signatures: Inaccurate answers to questions, inability to identify relevant topics, hallucinations or inconsistencies in the output

- First 3 experiments:
  1. Compare the accuracy of answers to questions generated by KGLLM and standard LLM on a small set of questions about the Serial podcast
  2. Test the robustness of KGLLM to adversarial prompts by providing external information that conflicts with the narrative
  3. Evaluate the ability of KGLLM to identify relevant topics in the narrative compared to classical topic modeling methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do adversarial prompts affect the hierarchical structure of knowledge graphs constructed by GraphRAG?
- Basis in paper: [explicit] The paper discusses adversarial prompting in Section 4.2 and mentions GraphRAG's hierarchical knowledge graph in Section 3.2.2
- Why unresolved: The paper tests GraphRAG's robustness to adversarial prompts but doesn't analyze how these prompts might alter the knowledge graph's structure or clustering
- What evidence would resolve it: Systematic analysis comparing knowledge graph structures before and after exposure to adversarial prompts, examining changes in hierarchical clustering and node/edge distributions

### Open Question 2
- Question: What is the optimal balance between local and global search in GraphRAG for different types of narrative analysis tasks?
- Basis in paper: [explicit] Section 4.1 shows GraphRAG local search outperformed global search on 110 out of 144 evaluations, but the paper doesn't explore optimal task-specific configurations
- Why unresolved: The paper provides comparative results but doesn't investigate how to optimize the local/global search balance for specific query types or narrative analysis objectives
- What evidence would resolve it: Controlled experiments testing various local/global search ratios across different narrative analysis tasks, with performance metrics for each combination

### Open Question 3
- Question: How does the temporal ordering of evidence presentation in narratives affect knowledge graph construction and subsequent LLM reasoning?
- Basis in paper: [inferred] Section 1.1 mentions that the Serial podcast doesn't progress chronologically, and Section 5 suggests exploring dimensionality reduction methods to incorporate temporal dynamics
- Why unresolved: The paper acknowledges temporal ordering as important but doesn't investigate how narrative chronology affects KG construction or LLM performance on temporal reasoning tasks
- What evidence would resolve it: Comparative studies analyzing knowledge graphs and LLM performance on chronologically-ordered versus non-chronological narrative presentations of the same events

## Limitations
- The study's evaluation relies on subjective human judgments for metrics like comprehensiveness, directness, diversity, and empowerment, which introduces potential rater bias
- The adversarial prompt testing, while showing KGLLM's robustness, used only a single crafted prompt scenario
- The claim that KGLLMs capture more nuanced topics than BERTopic is based on a single case study (Serial podcast) without broader validation across different narrative types or domains

## Confidence
- High confidence: KGLLM outperforms standard LLMs on the specific question-answering tasks tested with Serial podcast data
- Medium confidence: KGLLM demonstrates greater robustness to adversarial prompts, though this is based on limited prompt variations
- Medium confidence: Topic modeling comparisons with BERTopic, as these involve subjective assessments of narrative nuance capture

## Next Checks
1. **Cross-domain validation**: Apply the KGLLM approach to multiple narrative types (fiction, news articles, historical accounts) to test generalizability beyond the true crime podcast domain
2. **Objective metric development**: Create automated, objective metrics for evaluating KGLLM performance in topic modeling and question-answering, reducing reliance on subjective human judgments
3. **Comprehensive adversarial testing**: Design and test a broader range of adversarial prompts including temporal inconsistencies, character contradictions, and mixed-format inputs to thoroughly evaluate KGLLM robustness