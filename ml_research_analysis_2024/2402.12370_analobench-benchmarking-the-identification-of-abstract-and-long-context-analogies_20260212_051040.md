---
ver: rpa2
title: 'AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies'
arxiv_id: '2402.12370'
source_url: https://arxiv.org/abs/2402.12370
tags:
- analogies
- sentence
- story
- stories
- analogical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ANALO BENCH, a benchmark designed to evaluate
  the ability of language models (LMs) to identify abstract and long-context analogies
  in natural language stories. The benchmark consists of two tasks: (1) selecting
  the most analogous story from a small set of candidates, and (2) identifying the
  top 10 most analogous stories from a larger story bank.'
---

# AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies

## Quick Facts
- **arXiv ID:** 2402.12370
- **Source URL:** https://arxiv.org/abs/2402.12370
- **Reference count:** 40
- **Primary result:** Language models show degraded performance on longer-context analogies compared to humans, with GPT-4 achieving 60.7% accuracy on 30-sentence stories versus human performance of 73.3%

## Executive Summary
AnaloBench introduces a benchmark for evaluating language models' ability to identify abstract and long-context analogies in natural language stories. The benchmark consists of two tasks: selecting the most analogous story from a small set of candidates, and identifying the top 10 most analogous stories from a larger story bank. The benchmark includes 340 high-quality, human-written analogies, with each story elaborated to varying lengths (1, 10, and 30 sentences) using GPT-4. Experiments conducted on a broad range of models, including proprietary (e.g., GPT-4, Claude-v2) and open-source models (e.g., LLaMA2, Tulu2), reveal that while scaling LMs leads to improved performance on shorter stories, the benefits of scaling diminish significantly for longer stories.

## Method Summary
The authors developed AnaloBench to evaluate analogical reasoning in language models through two tasks: (1) a 4-way classification task where models select the most analogous story from four candidates, and (2) a retrieval task where models identify the top 10 most analogous stories from a bank of 100 stories. The benchmark uses 340 human-written analogy pairs, with each story elaborated to three different lengths (1, 10, and 30 sentences) using GPT-4. The elaboration process involves three rounds of GPT-4 refinement to ensure coherence and maintain the original analogy structure. Models are evaluated using accuracy for the classification task and Recall@10 for the retrieval task, with model scores aggregated from 10 attempts per question.

## Key Results
- GPT-4 achieves 89.1% accuracy on 1-sentence stories but only 60.7% on 30-sentence stories
- Human performance shows a smaller degradation, from 96.0% on 1-sentence stories to 73.3% on 30-sentence stories
- Open-source models show more severe performance drops with increasing context length compared to proprietary models
- Retrieval performance (Recall@10) is generally lower than classification accuracy, indicating greater difficulty in ranking analogies

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on abstract analogies rather than surface-level similarities, requiring models to identify deep structural parallels between stories. By using human-written analogy pairs and varying story lengths through controlled elaboration, the benchmark isolates the challenge of maintaining analogical reasoning across different context sizes while avoiding confounding factors from synthetic analogy generation.

## Foundational Learning
- **Analogical reasoning:** Understanding structural similarities between different domains is crucial for evaluating models' abstract thinking capabilities
- **Context window limitations:** Models struggle to maintain coherence and identify patterns across longer sequences, affecting their ability to recognize analogies
- **Retrieval evaluation:** Recall@10 metric provides insight into models' ability to rank relevant items, complementing classification accuracy
- **Scaling effects:** Larger models show improved performance on simpler tasks but diminishing returns on complex reasoning tasks

## Architecture Onboarding

**Component map:** Human-written stories -> GPT-4 elaboration -> Task formulation (classification/retrieval) -> Model evaluation

**Critical path:** Story elaboration → Task formulation → Model inference → Performance evaluation

**Design tradeoffs:** The benchmark prioritizes abstract reasoning over surface-level matching, uses controlled story elaboration to isolate context length effects, and balances evaluation difficulty between classification and retrieval tasks.

**Failure signatures:** Performance degradation with increasing story length, lower retrieval accuracy compared to classification, and greater difficulty for open-source models versus proprietary models.

**3 first experiments:**
1. Evaluate baseline models on surface-level analogy tasks to establish performance floors
2. Test model performance with and without explicit instructions to focus on abstract structure
3. Compare performance using human-written elaborations versus GPT-4 elaborations

## Open Questions the Paper Calls Out
None

## Limitations
- GPT-4 elaboration process may introduce artifacts that bias results or fail to preserve original analogy structure
- Evaluation protocol with 10 attempts per question may not fully capture model capabilities
- Human performance comparison limited to 1-sentence stories creates apples-to-oranges comparison
- Focus on abstract scenarios limits generalizability to real-world applications

## Confidence
- **High:** Performance degradation on longer contexts is consistently observed across multiple models and story lengths
- **Medium:** Claim about abstract analogies being more challenging requires careful interpretation of human performance data
- **Medium:** Assertion that scaling laws break down for longer contexts may be influenced by factors beyond model size

## Next Checks
1. Replicate main experiments using human-written elaborations of all story lengths to isolate GPT-4 elaboration effects
2. Conduct human evaluations on all story lengths (1, 10, and 30 sentences) for direct model comparison
3. Test whether performance on longer contexts improves with prompts focusing on abstract structural elements