---
ver: rpa2
title: 'LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training'
arxiv_id: '2406.16554'
source_url: https://arxiv.org/abs/2406.16554
tags:
- arxiv
- expert
- training
- experts
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLaMA-MoE, a method for constructing Mixture-of-Experts
  (MoE) models from existing dense language models like LLaMA-2-7B. The key innovation
  lies in splitting the Feed-Forward Networks (FFNs) of the dense model into multiple
  experts, followed by continual pre-training to restore language capabilities.
---

# LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training

## Quick Facts
- arXiv ID: 2406.16554
- Source URL: https://arxiv.org/abs/2406.16554
- Authors: Tong Zhu; Xiaoye Qu; Daize Dong; Jiacheng Ruan; Jingqi Tong; Conghui He; Yu Cheng
- Reference count: 10
- Primary result: LLaMA-MoE-3.5B significantly outperforms dense models with similar activation parameters

## Executive Summary
This paper introduces LLaMA-MoE, a method for constructing Mixture-of-Experts (MoE) models from existing dense language models like LLaMA-2-7B. The key innovation lies in splitting the Feed-Forward Networks (FFNs) of the dense model into multiple experts, followed by continual pre-training to restore language capabilities. The authors explore different expert construction methods and data sampling strategies for efficient training. LLaMA-MoE models, particularly the 3.5B variant, significantly outperform dense models with similar activation parameters on various downstream tasks.

## Method Summary
The authors partition the FFN parameters of a pre-trained dense model (LLaMA-2-7B) into multiple experts using random, clustering-based, or importance-based splitting methods. After partitioning, they apply re-scaling to preserve model capacity and implement a gate network for expert routing. The MoE model then undergoes continual pre-training on the SlimPajama dataset with optimized data sampling weights. The resulting LLaMA-MoE models are evaluated on downstream tasks to compare performance against dense baselines with similar activation parameters.

## Key Results
- LLaMA-MoE-3.5B models significantly outperform dense models with similar activation parameters on downstream tasks
- Non-overlapping random splitting of FFN parameters achieves the best performance among expert construction methods
- Static data sampling weights lead to faster convergence and better performance compared to dynamic weights
- LLaMA-MoE models exhibit expert specialization, with different experts showing preferences for specific domains

## Why This Works (Mechanism)

### Mechanism 1
Reusing pre-trained dense model parameters as MoE experts provides a performance and efficiency boost compared to training from scratch. The dense model has already learned general language representations during pre-training, which can be leveraged when repurposed as experts in an MoE architecture.

### Mechanism 2
Independent random partitioning of FFN neurons into experts, followed by re-scaling, achieves the best performance. Randomly splitting the FFN neurons into non-overlapping sets for each expert creates diverse expert specializations while preserving overall model capacity.

### Mechanism 3
Static data sampling weights, optimized for the MoE model, lead to faster convergence and better performance compared to dynamic weights. By assigning static weights to different data domains based on their importance for the MoE model, the model can focus on the most relevant data and converge faster.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how MoE works is crucial for grasping the paper's approach of converting a dense model to MoE.
  - Quick check question: In an MoE model with 8 experts and top-2 routing, what fraction of the total parameters are activated for each input token?

- Concept: Feed-Forward Networks (FFNs) in Transformers
  - Why needed here: The paper's MoE construction method involves partitioning the FFNs of the dense model into experts.
  - Quick check question: What are the main components of an FFN in a standard Transformer, and how do they process the input?

- Concept: Continual pre-training
  - Why needed here: After converting the dense model to MoE, the paper uses continual pre-training to restore and enhance the model's language capabilities.
  - Quick check question: How does continual pre-training differ from standard pre-training, and why is it necessary in this case?

## Architecture Onboarding

- Component map: Dense model (LLaMA-2-7B) -> FFN partitioning -> MoE model (LLaMA-MoE) -> Expert routing -> Continual pre-training -> Downstream tasks
- Critical path: FFN partitioning -> Expert routing -> Continual pre-training
- Design tradeoffs:
  - Partitioning strategy (random vs. importance-based) affects expert diversity and specialization
  - Number of experts vs. activation ratio impacts model capacity and efficiency
  - Static vs. dynamic data sampling weights affects convergence speed and stability
- Failure signatures:
  - Poor downstream task performance indicates issues with expert construction or continual pre-training
  - High training loss or slow convergence suggests problems with data sampling or model architecture
  - Expert imbalance (some experts rarely activated) may require pruning or adjusting the routing mechanism
- First 3 experiments:
  1. Implement FFN partitioning with random splitting and compare downstream task performance to the dense model
  2. Experiment with different numbers of experts and activation ratios to find the optimal configuration
  3. Compare static and dynamic data sampling strategies to determine their impact on convergence and final performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LLaMA-MoE models change when experts are partitioned at different transformer layers, particularly focusing on deeper layers versus shallow layers? The paper identifies that deep layers show more routing preferences than shallow layers, suggesting potential for further performance improvements through targeted expert partitioning.

### Open Question 2
What is the optimal reference loss to use for dynamic data sampling weight construction, and how does the choice of reference model affect downstream task performance? The paper notes that the choice of reference loss is "very tricky" and leaves it for future work, as different reference losses led to different sampling weight behaviors.

### Open Question 3
Can LLaMA-MoE models be further improved by introducing additional pre-training domains specifically tailored to bridge the gap between current pre-training data and downstream tasks with large expert selection distance? The paper identifies that some downstream tasks have large expert selection distances from current pre-training data, suggesting a potential performance bottleneck.

## Limitations

- Limited experimental scope focusing on specific configurations and English-language tasks
- Fixed number of experts (8) and activation ratios without comprehensive ablation studies
- Data sampling strategy optimization performed once and fixed without exploring dynamic adaptation during training
- Computational cost analysis only considers activation parameters rather than total FLOPs or wall-clock training time

## Confidence

*High Confidence:* The core methodology of partitioning FFN parameters and using continual pre-training to create MoE models is well-established and the implementation details are clearly specified.

*Medium Confidence:* Claims about faster convergence and superior performance compared to training from scratch are based on limited comparisons and could benefit from more extensive baselines.

*Low Confidence:* Generalizability of findings across different base model architectures and scalability to larger model sizes remains uncertain.

## Next Checks

1. **Architecture Transferability Test:** Implement LLaMA-MoE construction using a different dense base model (e.g., OPT or Mistral) to verify that the non-overlapping random splitting strategy consistently outperforms other partitioning methods across architectures.

2. **Scaling Analysis:** Conduct experiments scaling the method to larger dense models (e.g., LLaMA-2-13B or 70B) to identify whether the optimal number of experts and activation ratio remain constant or require adjustment with model size.

3. **Dynamic Sampling Evaluation:** Replace the static data sampling weights with a dynamic strategy that adapts during training, comparing both convergence speed and final task performance against the static approach to validate the paper's claims about static weight effectiveness.