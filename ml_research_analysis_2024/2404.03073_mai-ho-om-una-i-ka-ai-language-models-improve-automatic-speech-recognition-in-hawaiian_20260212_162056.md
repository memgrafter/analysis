---
ver: rpa2
title: "Mai Ho'om\u0101una i ka 'Ai: Language Models Improve Automatic Speech Recognition\
  \ in Hawaiian"
arxiv_id: '2404.03073'
source_url: https://arxiv.org/abs/2404.03073
tags:
- hawaiian
- whisper
- text
- data
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving automatic speech
  recognition (ASR) for Hawaiian, a low-resource language, by incorporating large
  amounts of independent text data into a foundation model. The core method involves
  training an external language model (LM) on approximately 1.5 million words of Hawaiian
  text and using it to rescore the ASR outputs.
---

# Mai Ho'omāuna i ka 'Ai: Language Models Improve Automatic Speech Recognition in Hawaiian

## Quick Facts
- **arXiv ID**: 2404.03073
- **Source URL**: https://arxiv.org/abs/2404.03073
- **Reference count**: 0
- **Primary result**: Incorporating an external Hawaiian language model to rescore Whisper ASR outputs improves WER by 2.9-5.9% relative.

## Executive Summary
This paper addresses the challenge of improving automatic speech recognition (ASR) for Hawaiian, a low-resource language, by incorporating large amounts of independent text data into a foundation model. The core method involves training an external language model (LM) on approximately 1.5 million words of Hawaiian text and using it to rescore the ASR outputs. The primary result is a small but significant improvement in word error rate (WER) when ASR outputs are rescored with the Hawaiian LM, compared to using the foundation model without an external LM. The study highlights the potential of leveraging all available data, including text corpora, to enhance ASR systems for underrepresented languages.

## Method Summary
The method involves training a character-level RNN language model on ~1.5M words of Hawaiian text, then using it to rescore Whisper ASR outputs by combining their log probabilities with a weight α. The best-performing configuration uses large Whisper models with α=0.5, achieving modest WER improvements over baseline.

## Key Results
- Large Whisper models (large, large-v2) outperform smaller models in zero-shot Hawaiian ASR.
- LM rescoring with α=0.5 reduces WER by 2.9-5.9% relative compared to baseline Whisper.
- Larger LM training corpora correlate with lower perplexity but not statistically significant WER improvements.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rescoring Whisper outputs with a Hawaiian LM improves WER because the LM compensates for Whisper's lack of explicit language modeling of Hawaiian.
- Mechanism: Whisper uses an implicit LM trained on non-Hawaiian data. By combining its log probability with a dedicated Hawaiian LM's log probability (weighted by α), the rescoring step biases the beam search toward Hawaiian-likely sequences.
- Core assumption: The Hawaiian LM can reliably estimate the probability of Hawaiian character sequences, even with modest training data (~1.5M words).
- Evidence anchors:
  - [abstract]: "We train an external language model (LM) on ∼1.5M words of Hawaiian text. We then use the LM to rescore Whisper..."
  - [section]: "We therefore propose to incorporate an explicit Hawaiian LM into Whisper by using rescoring."
  - [corpus]: Weak - no direct neighbor evidence for rescoring efficacy; neighbors focus on general ASR rescoring trends.
- Break condition: If the LM's perplexity is too high or its character distribution diverges significantly from spoken Hawaiian, rescoring may hurt rather than help.

### Mechanism 2
- Claim: Larger Whisper models transfer better to Hawaiian zero-shot because they have more parameters to capture language-agnostic speech patterns.
- Mechanism: Bigger models (large, large-v2) have richer representations learned from diverse audio, allowing them to generalize to unseen languages without fine-tuning.
- Core assumption: The Whisper architecture's zero-shot transfer ability holds across typologically diverse languages, even without target-language text.
- Evidence anchors:
  - [section]: "The best results (Figure 1, left panel) were obtained for the large and large-v2 models."
  - [section]: "Focusing on the best two models (Figure 1, right panel), we find no significant difference between large and large-v2 in terms of zero-shot WER..."
  - [corpus]: Weak - no direct neighbor evidence for model size impact; neighbors focus on rescoring and model comparison, not size scaling.
- Break condition: If the language is too dissimilar from training data (e.g., tonal vs. non-tonal), larger models may not compensate for lack of fine-tuning.

### Mechanism 3
- Claim: More training text improves LM quality (lower perplexity), which correlates with better WER after rescoring.
- Mechanism: Increasing the size of the LM training corpus provides more exposure to Hawaiian character n-grams, leading to better probability estimates and improved rescoring.
- Core assumption: The relationship between LM training data size and WER improvement is monotonic and statistically detectable.
- Evidence anchors:
  - [section]: "We observe a negative correlation between the number of words in the training set and the LM's validation perplexity..."
  - [section]: "Correlations involving WER did not reach significance, apparently because of large variance in the WER results."
  - [corpus]: Weak - no neighbor evidence for LM data scaling; neighbors focus on rescoring and model choice, not data size effects.
- Break condition: If the additional text is not representative of the target domain (e.g., old orthography vs. modern speech), perplexity may decrease but WER may not improve.

## Foundational Learning

- Concept: Beam search decoding
  - Why needed here: Whisper uses beam search to generate hypotheses; rescoring requires understanding how multiple candidates are scored and selected.
  - Quick check question: What is the default beam size in Whisper's Hugging Face implementation, and how does rescoring modify the scoring formula?

- Concept: Language model perplexity
  - Why needed here: Perplexity measures how well an LM predicts held-out text; it's used to select the best checkpoint and correlates with rescoring quality.
  - Quick check question: How is perplexity computed for a character-level RNN LM, and why is lower perplexity better for rescoring?

- Concept: Zero-shot transfer
  - Why needed here: The study evaluates Whisper's ability to transcribe a language it was never fine-tuned on; understanding this concept is key to interpreting results.
  - Quick check question: What distinguishes zero-shot transfer from few-shot or fine-tuning in the context of ASR foundation models?

## Architecture Onboarding

- Component map: Whisper (encoder-decoder ASR model) -> external RNN LM (character-level) -> rescoring layer (weighted log probability combination) -> WER evaluation on curated test set
- Critical path: ASR output generation -> LM rescoring -> final transcript selection -> WER calculation
- Design tradeoffs: Using a large Whisper model improves zero-shot WER but increases compute cost; a more complex LM (e.g., transformer) could improve perplexity but requires more data and training time
- Failure signatures: WER plateaus or worsens with rescoring (α too high); rescoring has no effect (LM perplexity too high or poorly matched to speech data)
- First 3 experiments:
  1. Compare zero-shot WER of tiny vs. base vs. small Whisper models on the test set to establish baseline scaling
  2. Train a small LM on a subset of data and evaluate rescoring impact for α ∈ {0, 0.25, 0.5}
  3. Vary α in finer increments (e.g., 0.1, 0.2) around the best value to observe sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal amount of labeled Hawaiian audio data needed to significantly improve Whisper's performance for Hawaiian ASR, beyond what is achievable with text-based LM rescoring?
- Basis in paper: [explicit] The paper mentions ongoing efforts to gather more labeled Hawaiian data and suggests fine-tuning Whisper on this data could correct phonological errors.
- Why unresolved: The study did not have access to large amounts of labeled Hawaiian audio for fine-tuning, so the impact of additional labeled data on Whisper's performance remains untested.
- What evidence would resolve it: Training and evaluating Whisper on varying amounts of labeled Hawaiian audio data to determine the point of diminishing returns in WER improvement.

### Open Question 2
- Question: How do self-supervised learning methods (e.g., wav2vec 2.0, HuBERT) compare to Whisper with LM rescoring for low-resource languages like Hawaiian?
- Basis in paper: [inferred] The paper discusses the potential of using unlabeled audio data and mentions self-supervised learning as an approach, but does not compare it to the Whisper+LM method.
- Why unresolved: The study focused on Whisper and LM rescoring, not on comparing different self-supervised learning approaches for Hawaiian.
- What evidence would resolve it: Training and evaluating self-supervised models on Hawaiian audio data and comparing their performance to Whisper with LM rescoring.

### Open Question 3
- Question: How does the performance of Whisper with LM rescoring for Hawaiian generalize to other low-resource languages with different phonological and orthographic characteristics?
- Basis in paper: [explicit] The paper acknowledges that Hawaiian may not be representative of other low-resource languages and suggests empirical testing is needed.
- Why unresolved: The study only evaluated the approach on Hawaiian, so its applicability to other languages is unknown.
- What evidence would resolve it: Applying the Whisper+LM rescoring method to other low-resource languages and comparing the results to the Hawaiian case.

### Open Question 4
- Question: What is the impact of using larger and more sophisticated language models (e.g., transformers, multilingual LMs) on the performance of Whisper for low-resource languages?
- Basis in paper: [inferred] The paper mentions the possibility of training better LMs for Hawaiian and the use of transformers, but does not explore this in depth.
- Why unresolved: The study used a relatively simple RNN-based LM, so the potential benefits of larger LMs are untested.
- What evidence would resolve it: Training and evaluating Whisper with different types and sizes of LMs for Hawaiian and other low-resource languages.

## Limitations

- The improvement in WER through LM rescoring is modest (2.9-5.9% relative) and may not scale to languages with scarcer resources.
- The optimal α value (0.5) was not rigorously established, as only three values were tested.
- The small test set (57 pairs, 1,120 words) raises concerns about overfitting and inflated confidence in the observed improvements.

## Confidence

- **High Confidence**: The finding that larger Whisper models (large, large-v2) outperform smaller ones in zero-shot Hawaiian ASR is well-supported by direct comparison across model sizes and consistent with known scaling properties of foundation models.
- **Medium Confidence**: The claim that LM rescoring improves WER is supported by experimental results, but the effect size is modest and the optimal α value is not rigorously established.
- **Low Confidence**: The hypothesis that increasing the size of the LM training corpus will monotonically improve WER is weakly supported, as the correlation between corpus size and WER did not reach significance in the study.

## Next Checks

1. Reproduce WER improvements across multiple random seeds: Run the full ASR + rescoring pipeline at least three times with different random seeds to assess the stability of the observed WER improvements and compute confidence intervals.
2. Test additional α values and LM architectures: Evaluate the rescoring performance for α ∈ {0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9} and compare against a transformer-based LM to determine if the current configuration is near-optimal.
3. Validate on an external Hawaiian test set: Apply the best-performing model (large Whisper + LM rescoring) to a held-out or independently curated Hawaiian speech dataset to confirm generalization beyond the original test set.