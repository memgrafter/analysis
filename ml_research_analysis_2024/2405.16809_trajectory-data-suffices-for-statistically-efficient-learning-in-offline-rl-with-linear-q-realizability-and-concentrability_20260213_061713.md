---
ver: rpa2
title: "Trajectory Data Suffices for Statistically Efficient Learning in Offline RL\
  \ with Linear $q^\u03C0$-Realizability and Concentrability"
arxiv_id: '2405.16809'
source_url: https://arxiv.org/abs/2405.16809
tags:
- traj
- lemma
- holds
- such
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies offline reinforcement learning (RL) under linear
  $q^\pi$-realizability and concentrability, where the goal is to learn a near-optimal
  policy from a fixed dataset of trajectories. The key question is whether such learning
  is possible with sample complexity independent of the state space size.
---

# Trajectory Data Suffices for Statistically Efficient Learning in Offline RL with Linear $q^π$-Realizability and Concentrability

## Quick Facts
- arXiv ID: 2405.16809
- Source URL: https://arxiv.org/abs/2405.16809
- Reference count: 40
- Key outcome: With trajectory data, offline RL under linear $q^π$-realizability and concentrability can achieve sample complexity poly(d,H,C_conc)/ε^2, independent of state space size.

## Executive Summary
This paper addresses offline reinforcement learning under linear $q^π$-realizability and concentrability assumptions, demonstrating that trajectory data enables statistically efficient learning with sample complexity independent of state space size. The key insight is that linear MDPs can approximate linearly $q^π$-realizable MDPs when using trajectory data, enabling techniques from online RL to be applied. The proposed method uses a sophisticated optimization framework that considers all possible "skipping" mechanisms over low-range states, combined with careful confidence sets and value function approximations. While the statistical efficiency is established, computational efficiency remains an open question.

## Method Summary
The method solves an optimization problem over all possible "guesses" G for skipping mechanisms, using trajectory data to construct least-squares targets and estimate linear MDP parameters. The learner outputs a greedy policy with respect to the estimated action-values. The approach relies on transforming trajectory data into least-squares targets that enable bounded noise estimation of linear MDP parameters, then optimizing over all possible skipping mechanisms to find the one that yields the tightest estimators and highest policy value.

## Key Results
- Trajectory data enables offline RL with sample complexity poly(d,H,C_conc)/ε^2, independent of state space size
- Linear MDP approximation works when skipping low-range states (range ≤ α/√2d)
- Optimization over all possible skipping mechanisms ensures the true MDP structure is recovered
- Statistical efficiency is established, but computational efficiency remains open

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear MDPs can approximate linearly qπ-realizable MDPs when using trajectory data.
- Mechanism: By "skipping" over low-range states, the MDP transitions directly from a state to a future state τ, summing rewards along the skipped path. This skipping preserves the ability to compute action-values linearly.
- Core assumption: The true guess ¯G is used to define the skipping mechanism.
- Evidence anchors:
  - [abstract] "The main tool that makes this result possible is due to Weisz et al. [2023], who demonstrate that linear MDPs can be used to approximate linearly qπ-realizable MDPs."
  - [section 4.1] "Our learner relies on the observation due to Weisz et al. [2023] that linearly qπ-realizable MDPs are linear MDPs, as long as they contain no low-range states."
  - [corpus] Weak evidence: only one related paper found, and it focuses on policy evaluation, not optimization.
- Break condition: If the true guess ¯G is not used (e.g., an incorrect G is chosen), the skipping mechanism may discard important states, breaking the approximation.

### Mechanism 2
- Claim: Trajectory data enables estimation of the linear MDP parameters with bounded error.
- Mechanism: Each full trajectory can be transformed into a least-squares target Eτ∼F j G,h+1 [rj h:τ-1 + f(sj τ)], which has the desired properties: bounded noise and expectation equal to ⟨φ(s,a),ρ π h(f)⟩ for all (s,a) ∈ Sh × A.
- Core assumption: The data is collected by a single policy π 0 and consists of full-length trajectories.
- Evidence anchors:
  - [section 4.2] "Full trajectory data (Assumption 2) makes this possible. Each full length trajectory traj j = (sj t,a j t,r j t)t∈ [H+1]j ∈ [n] can be used to create the following least-squares target..."
  - [corpus] No direct evidence; the mechanism relies on internal consistency of the paper.
- Break condition: If the data consists of individual transitions rather than full trajectories, the least-squares targets cannot be constructed, breaking the estimation.

### Mechanism 3
- Claim: Optimization over all possible "guesses" G ensures that the true guess ¯G is considered and its parameters are accurately estimated.
- Mechanism: The learner solves an optimization problem over all G ∈ G, defining confidence sets ΘG,h that include the true parameters ψh(π⋆ G) with high probability. By selecting the G that yields the tightest estimators and highest policy value, the learner effectively recovers the true MDP structure.
- Core assumption: The true guess ¯G is feasible (passes the tightness constraint Eq. (14)) and the confidence sets are constructed correctly.
- Evidence anchors:
  - [section 4.3] "The challenge is that the true guess ¯G that Lemma 4.2 relies upon is not known to the learner. This means that the learner is not given any explicit information of what states to 'skip over'. To overcome this, we design a learner to output the policy π ′..."
  - [section 4.4] "Optimization Problem 1... optimizes over all G ∈ G, which can be seen as an optimization over all possible 'modified MDPs' with different skipping mechanisms."
  - [corpus] No direct evidence; the mechanism relies on internal consistency of the paper.
- Break condition: If the optimization problem rejects ¯G due to overly conservative confidence sets, or if the search space G is too restrictive, the true MDP structure may not be recovered.

## Foundational Learning

- Concept: Linear qπ-realizability
  - Why needed here: The assumption that the action-value function of every policy is linear with respect to a given d-dimensional feature function is the foundation for the linear MDP approximation and the least-squares estimation.
  - Quick check question: Can you explain why linear qπ-realizability is strictly weaker than the linear MDP assumption?

- Concept: Concentrability
  - Why needed here: This assumption bounds the extent to which the state-action distribution of any policy can veer off the data distribution, ensuring that the data covers the relevant state-action space.
  - Quick check question: How does concentrability differ from the λmin lower bound condition used in other works?

- Concept: Trajectory data vs. individual transitions
  - Why needed here: Trajectory data allows the construction of least-squares targets with the desired properties, while individual transitions do not.
  - Quick check question: Why can't we use individual transitions to simulate the skipping mechanisms required for the linear MDP approximation?

## Architecture Onboarding

- Component map: Data ingestion (trajectory data) -> Skipping mechanism (true guess ¯G) -> Least-squares estimation -> Optimization over G -> Policy output
- Critical path:
  1. Define the true guess ¯G and approximate range function range ¯G.
  2. Transform trajectory data into least-squares targets using the skipping mechanism.
  3. Estimate parameters ρπ h(f) for each stage h using least-squares regression.
  4. Construct confidence sets ΘG,h for each G ∈ G and h ∈ [H].
  5. Solve Optimization Problem 1 to select the best G and parameter estimates.
  6. Output the greedy policy π ′.
- Design tradeoffs:
  - Statistical efficiency vs. computational efficiency: The learner achieves poly(d,H,Cconc,1/ε) sample complexity but may be computationally inefficient due to the optimization over all G ∈ G.
  - Tightness of confidence sets vs. feasibility: Overly tight confidence sets may reject the true guess ¯G, while overly loose sets may lead to inaccurate estimates.
- Failure signatures:
  - If the true guess ¯G is not feasible (fails the tightness constraint), the optimization may select a suboptimal G.
  - If the confidence sets are too large, the greedy policy π ′ may not be near-optimal.
  - If the data does not cover the state-action space well (low concentrability), the estimation error may be large.
- First 3 experiments:
  1. Generate synthetic data from a linearly qπ-realizable MDP with known parameters. Run the learner and check if the output policy achieves near-optimal value.
  2. Vary the concentrability coefficient Cconc and observe its effect on the sample complexity and estimation error.
  3. Compare the performance of the learner with and without trajectory data (using only individual transitions) to verify the importance of trajectory data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the sample complexity bound in Theorem 1 optimal, or can it be improved?
- Basis in paper: [explicit] The paper states "Another limitation of our work is that we are not sure if our statistical rate in Theorem 1 is optimal. Showing a matching lower bound or improving the rate is left for future work."
- Why unresolved: The paper proves an upper bound but does not establish whether this bound is tight or if better rates are achievable under the same assumptions.
- What evidence would resolve it: Either a matching lower bound that matches the poly(d,H,Cconc,1/ε) sample complexity, or an algorithm achieving better sample complexity with a matching upper bound proof.

### Open Question 2
- Question: Can the optimization problem at the heart of the learner be solved computationally efficiently?
- Basis in paper: [explicit] "One limitation of this work is that we are not aware of any computationally efficient implementation of Optimization Problem 1, which is at the heart of our learner. As such, it is left as an open problem whether computationally efficient learning is possible in the setting we considered."
- Why unresolved: While the paper establishes statistical efficiency, it does not provide a computationally efficient algorithm to solve the optimization problem, leaving computational feasibility uncertain.
- What evidence would resolve it: Either a computationally efficient algorithm that solves Optimization Problem 1 with provable guarantees, or a computational lower bound showing that solving this optimization is intractable under standard complexity assumptions.

### Open Question 3
- Question: Does the trajectory data requirement extend to more general offline data settings?
- Basis in paper: [inferred] The paper's key result relies on trajectory data, but it's unclear whether this requirement is fundamental or if similar results could be achieved with other offline data structures.
- Why unresolved: The paper shows trajectory data suffices but doesn't investigate whether the result holds for other offline data formats or if trajectory data is strictly necessary for this level of statistical efficiency.
- What evidence would resolve it: Either a proof that similar sample complexity bounds can be achieved with other offline data formats, or a lower bound showing that trajectory data is fundamentally required to achieve the stated efficiency.

## Limitations
- Computational efficiency of the optimization problem remains open, with potentially exponential complexity in searching over all possible "guesses" G
- Reliance on trajectory data collected by a single policy π 0 may limit applicability in scenarios requiring diverse data collection
- Theoretical results don't establish whether the sample complexity bound is optimal or can be improved

## Confidence
- High Confidence: The statistical efficiency claim (poly(d,H,C_conc)/ε^2 sample complexity) is supported by rigorous theoretical analysis and proofs in the paper.
- Medium Confidence: The mechanism of using trajectory data to construct least-squares targets and skipping low-range states is internally consistent but relies on specific assumptions about data collection and MDP structure.
- Low Confidence: The practical computational efficiency and scalability of the optimization problem for real-world applications remain uncertain without empirical validation.

## Next Checks
1. Implement the optimization problem on synthetic MDPs with varying dimensions (d) and horizons (H) to empirically verify the stated sample complexity bound.
2. Test the algorithm's sensitivity to the concentrability coefficient C_conc by generating datasets with controlled coverage and measuring the impact on policy performance.
3. Compare the performance of the proposed method with existing offline RL algorithms on benchmark problems to assess practical efficiency and effectiveness.