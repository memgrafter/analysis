---
ver: rpa2
title: 'SimulTron: On-Device Simultaneous Speech to Speech Translation'
arxiv_id: '2406.02133'
source_url: https://arxiv.org/abs/2406.02133
tags:
- translation
- simultron
- speech
- s2st
- decoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SimulTron is an on-device, real-time speech-to-speech translation
  (S2ST) system that achieves BLEU scores of 51.2 on the Conversational Spanish-English
  dataset and 14.7 on the MuST-C English-Spanish dataset, while maintaining low latency
  suitable for mobile deployment. It uses a causal conformer encoder, wait-k attention,
  and a streaming vocoder based on the Translatotron architecture, optimized for efficient
  streaming operation on devices like the Pixel 7 Pro.
---

# SimulTron: On-Device Simultaneous Speech to Speech Translation

## Quick Facts
- **arXiv ID**: 2406.02133
- **Source URL**: https://arxiv.org/abs/2406.02133
- **Reference count**: 0
- **Primary result**: On-device simultaneous speech-to-speech translation achieving BLEU scores of 51.2 (Spanish-English) and 14.7 (English-Spanish) with low latency suitable for mobile deployment.

## Executive Summary
SimulTron is the first demonstrated real-time speech-to-speech translation system capable of on-device inference. It achieves high translation quality while maintaining low latency through a causal conformer encoder, wait-k attention mechanism, and streaming vocoder optimized for mobile deployment. The system supports adjustable fixed delays and can be quantized to 8-bit precision for further efficiency gains. Evaluated on Pixel 7 Pro, it demonstrates practical feasibility for real-world cross-lingual communication applications.

## Method Summary
SimulTron employs a streaming Translatotron architecture with a causal conformer encoder (16 layers, 256 dim) for efficient speech encoding, wait-k attention decoder (6 layers, 678 dim LSTM) for controlled latency-quality tradeoff, and streaming MelGAN vocoder for real-time waveform synthesis. The system processes 16kHz audio through a mel frontend, uses post-training dynamic range quantization to int8 for efficiency, and achieves real-time factor below 1.0 on mobile devices. Key design choices include causal self-attention with 65-frame left context and adjustable k parameter (150 frames ≈ 3s delay) for the wait-k mechanism.

## Key Results
- Achieves BLEU score of 51.2 on Conversational Spanish-English dataset
- Achieves BLEU score of 14.7 on MuST-C English-Spanish dataset
- Maintains real-time factor below 1.0 on Pixel 7 Pro with latency as low as 0.6s

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Causal conformer encoder enables streaming without violating temporal dependencies.
- **Mechanism**: Uses causal self-attention with fixed left context window (65 frames) and 1D depth-wise convolution, ensuring outputs depend only on past/current input frames.
- **Core assumption**: Fixed left context window captures sufficient contextual information for accurate speech representation.
- **Evidence anchors**: Paper specifies causal self-attention with 65-frame context and causal layers throughout pipeline.
- **Break condition**: If left context window too small to capture relevant phonetic/prosodic cues, translation quality degrades.

### Mechanism 2
- **Claim**: Wait-k attention balances latency and translation quality by controlling source context used.
- **Mechanism**: Decoder attends to 'k' most recent encoded source frames, introducing fixed delay that trades latency for improved accuracy.
- **Core assumption**: Fixed delay acceptable for real-time communication and chosen k values provide sufficient context.
- **Evidence anchors**: Paper describes wait-k mechanism with k=150 frames (3s delay) and shows BLEU scores for different k values.
- **Break condition**: If k too small, translation quality suffers; if k too large, latency becomes unacceptable.

### Mechanism 3
- **Claim**: Post-training quantization reduces model size and latency while maintaining acceptable quality.
- **Mechanism**: Dynamic range quantization converts 32-bit weights to 8-bit integers, reducing memory footprint and computational cost.
- **Core assumption**: Quantization preserves essential information in model weights without significant quality degradation.
- **Evidence anchors**: Paper reports latency reduction by factor of 2 and size reduction by factor of 3.7-3.1 for quantized models.
- **Break condition**: If quantization introduces significant noise, translation quality degrades beyond acceptable levels.

## Foundational Learning

- **Concept**: Causal attention mechanisms
  - **Why needed**: Ensures model only attends to past and present input frames, crucial for real-time streaming without violating temporal causality.
  - **Quick check**: What is the maximum number of past frames the causal conformer encoder can attend to, and how is this determined?

- **Concept**: Wait-k policy in simultaneous translation
  - **Why needed**: Controls tradeoff between latency and translation quality by determining source context used before generating each target frame.
  - **Quick check**: How does changing the 'k' value affect both latency and BLEU score in the SimulTron system?

- **Concept**: Post-training quantization
  - **Why needed**: Enables deployment on resource-constrained mobile devices by reducing model size and computational requirements while maintaining reasonable translation quality.
  - **Quick check**: What are the typical latency and size reductions achieved through dynamic range quantization in the SimulTron system?

## Architecture Onboarding

- **Component map**: Audio input → Mel frontend → Causal conformer encoder → Wait-k attention decoder → Causal PostNet → Streaming vocoder → Audio output

- **Critical path**: Raw audio → 80-bin mel spectrograms (25ms frames, 10ms steps) → 16-layer causal conformer (8 heads, 256 dim, 65-frame context) → 6-layer LSTM decoder with wait-k attention → Causal convolutional PostNet → MelGAN vocoder → Synthesized speech

- **Design tradeoffs**:
  - Latency vs. quality: Larger k values improve BLEU scores but increase initial delay
  - Model size vs. quality: Smaller decoder dimensions reduce memory but may lower BLEU scores
  - Quantization vs. quality: Int8 quantization reduces size/latency but may introduce quantization noise

- **Failure signatures**:
  - High latency: k value too large, model too big for single-threaded execution, quantization errors
  - Poor translation quality: k value too small, insufficient left context in encoder, quantization noise
  - Audio artifacts: Streaming vocoder issues, quantization errors, insufficient model capacity

- **First 3 experiments**:
  1. Measure BLEU score and latency for different k values (50, 100, 150) on validation set to find optimal latency-quality tradeoff
  2. Compare model size, latency, and BLEU score for different decoder configurations (768/6, 512/6, 256/6) to determine best resource-quality balance
  3. Evaluate impact of post-training quantization on both model size/latency and translation quality (BLEU score, MOS) to assess feasibility of quantized deployment

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several areas remain unexplored based on the limitations and future work discussion.

## Limitations

- Lack of comparative analysis with other state-of-the-art S2ST systems makes it difficult to assess relative performance
- No detailed ablation studies on critical design choices like left context window sizes or k value effects across diverse datasets
- Evaluation focuses primarily on BLEU and MOS without exploring other important metrics like word error rate or semantic similarity
- Results based on specific hardware (Pixel 7 Pro) may not generalize to other mobile devices

## Confidence

- **High Confidence**: Real-time S2ST on mobile devices claim (supported by RTF < 1.0 and Pixel 7 Pro deployment)
- **Medium Confidence**: BLEU scores (51.2 and 14.7) and latency-quality tradeoff claims (supported by methodology but lack comparative analysis)
- **Low Confidence**: Quantization benefits (size/latency improvements reported but no BLEU scores for quantized models)

## Next Checks

1. **Cross-Dataset Validation**: Evaluate SimulTron on additional S2ST datasets (CVSS, CoVoST) with varying language pairs to verify 51.2 BLEU score robustness and assess performance across diverse conditions.

2. **Quantization Quality Assessment**: Implement int8 quantization pipeline and measure BLEU scores, MOS, and audio quality metrics (PESQ, STOI) for both floating-point and quantized models to empirically validate quality preservation.

3. **Latency-Quality Tradeoff Analysis**: Systematically vary k parameter (50, 100, 150, 200 frames) and measure corresponding BLEU scores, MOS, and RTF values to create comprehensive tradeoff curve and identify optimal operating points.