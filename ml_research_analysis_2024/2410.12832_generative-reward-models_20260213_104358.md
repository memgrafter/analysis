---
ver: rpa2
title: Generative Reward Models
arxiv_id: '2410.12832'
source_url: https://arxiv.org/abs/2410.12832
tags:
- reward
- reasoning
- arxiv
- preference
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generative Reward Models (GenRM), a unified
  framework combining Reinforcement Learning from Human Feedback (RLHF) and Reinforcement
  Learning from AI Feedback (RLAIF). The authors address the challenge that synthetic
  preference labels generated by LLMs may not align well with human preference judgments.
---

# Generative Reward Models

## Quick Facts
- arXiv ID: 2410.12832
- Source URL: https://arxiv.org/abs/2410.12832
- Authors: Dakota Mahan; Duy Van Phung; Rafael Rafailov; Chase Blagden; Nathan Lile; Louis Castricato; Jan-Philipp Fränken; Chelsea Finn; Alon Albalak
- Reference count: 20
- Primary result: Generative Reward Models (GenRM) achieve comparable in-distribution accuracy to Bradley-Terry models while significantly outperforming them (10-45%) on out-of-distribution tasks.

## Executive Summary
This paper introduces Generative Reward Models (GenRM), a unified framework combining Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF). The authors address the challenge that synthetic preference labels generated by LLMs may not align well with human preference judgments. Their core method, GenRM, uses an iterative approach to train an LLM on self-generated reasoning traces, improving the alignment of synthetic preferences with human judgments. Experiments show that zero-shot LLM-based judgments under-perform Bradley-Terry reward models on in-distribution tasks (9-36% lower accuracy), while GenRM achieves comparable in-distribution accuracy and significantly outperforms them on out-of-distribution tasks (10-45% higher accuracy).

## Method Summary
GenRM replaces the traditional Bradley-Terry reward modeling approach with a more general preference modeling objective that doesn't assume a point-wise reward estimate or specific preference distribution parameterization. The method uses an iterative approach similar to Self-Taught Reasoner (STaR) to train an LLM on self-generated reasoning traces. The framework includes variants with and without chain-of-thought reasoning (CoT-GenRM) and different training objectives including supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and a rationalizer approach that uses external rationale generation. The models are trained on preference data from UltraFeedback and UltraInteract datasets and evaluated on both in-distribution held-out splits and out-of-distribution RewardBench tasks.

## Key Results
- Zero-shot LLM judgments under-perform Bradley-Terry reward models on in-distribution tasks (9-36% lower accuracy)
- GenRM achieves comparable in-distribution accuracy to Bradley-Terry models while significantly outperforming them on out-of-distribution tasks (10-45% higher accuracy)
- GenRM surpasses LLM-as-a-judge performance on both in-distribution (9-31% higher) and out-of-distribution tasks (2-6% higher)
- Chain-of-thought prompting in GenRM significantly improves performance on new prompts and tasks over the base GenRM approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GenRM improves alignment by iteratively training an LLM on self-generated reasoning traces, which better match human preference judgments compared to zero-shot LLM judgments.
- Mechanism: The self-generated reasoning traces capture the latent preferences humans use when judging responses. By training on these traces, the model learns to mimic human decision-making processes rather than just memorizing outcome patterns.
- Core assumption: Self-generated reasoning traces are a good proxy for human reasoning when judging responses.
- Evidence anchors: [abstract] "GenRM uses an iterative approach to train an LLM on self-generated reasoning traces, improving the alignment of synthetic preferences with human judgments." [section 4] "We begin with an LLM πϕ acting as a zero-shot judge in an RLAIF setting... Using a dataset of user preferences, we adopt a STaR-like methodology (Zelikman et al., 2022) to align the LLM with user choices, effectively training it to function as a reward model."
- Break condition: If the self-generated reasoning becomes systematically biased or diverges from actual human reasoning patterns, the alignment will degrade.

### Mechanism 2
- Claim: The generative approach outperforms Bradley-Terry models on out-of-distribution tasks because it captures preference distributions more flexibly without assuming a specific reward parameterization.
- Mechanism: Bradley-Terry models use a fixed linear predictor architecture that may not generalize well to novel contexts. The generative approach can adapt its preference distribution modeling to new distributions through its flexible language modeling capabilities.
- Core assumption: The Bradley-Terry model's architecture limitations are responsible for its OOD performance degradation.
- Evidence anchors: [abstract] "GenRM achieves in-distribution accuracy comparable to Bradley-Terry models, while significantly outperforming them on out-of-distribution tasks (between 10-45%)." [section 3] "The core contribution of our work is that we replace the Bradley-Terry reward modelling approach with a strictly more general preference modelling objective p(yw ≻ yl|x) that does not assume a point-wise reward estimate, a special model architecture, or a specific preference distribution parameterization as in Eq. 1."
- Break condition: If OOD performance differences are due to dataset characteristics rather than model architecture, the claim about generative flexibility would be weakened.

### Mechanism 3
- Claim: Chain-of-thought reasoning improves reward modeling accuracy by encouraging more logical, step-wise decision-making that better captures human preference judgments.
- Mechanism: The CoT-GenRM variant prompts the model to provide intermediate reasoning before making preference judgments, which forces the model to engage in more deliberate evaluation similar to human judgment processes.
- Core assumption: Chain-of-thought reasoning leads to more accurate preference judgments than direct classification.
- Evidence anchors: [section 5.1] "Interestingly we observe that the STaR-Rationalizer model matches the performance of the STaR-DPO model on both datasets, significantly out-performing the STaR-SFT approach, which uses the same training objective." [section 5.4] "Our results from the previous sections show that using COT prompting to induce reasoning in the evaluator model can significantly improve performance on new prompts and tasks over the base GenRM approach."
- Break condition: If the additional reasoning introduces noise or bias that degrades judgment accuracy, or if direct classification proves equally effective with less computational overhead.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding the baseline RLHF pipeline is essential to appreciate why GenRM offers improvements and how it modifies the standard approach.
  - Quick check question: What are the three stages of the standard RLHF pipeline and how does GenRM modify the second stage?

- Concept: Preference Modeling and Bradley-Terry Models
  - Why needed here: The paper contrasts GenRM with Bradley-Terry models, so understanding how preference modeling works and its limitations is crucial.
  - Quick check question: How does the Bradley-Terry model parameterize preference distributions, and what are its key assumptions?

- Concept: Self-Taught Reasoner (STaR) and Iterative Bootstrapping
  - Why needed here: GenRM uses STaR-like methodology for iterative training, so understanding this approach is essential for implementing the algorithm.
  - Quick check question: How does the STaR approach work for improving reasoning capabilities, and how is it adapted for preference modeling in GenRM?

## Architecture Onboarding

- Component map: LLaMa-3.1-8B-Instruct -> Preference generation modules (GenRM/CoT-GenRM with STaR) -> Training (SFT/DPO/STaR) -> Evaluation (UltraFeedback held-out/RrewardBench)
- Critical path: Data → LLM judge (with or without CoT) → Preference generation → Training (SFT/DPO/STaR) → Evaluation on in-distribution and OOD tasks
- Design tradeoffs: Direct GenRM is faster but less accurate than CoT-GenRM; STaR-based approaches require multiple iterations but may generalize better; Bradley-Terry models are simpler but less flexible for OOD tasks
- Failure signatures: Zero-shot LLM judgments underperforming Bradley-Terry models (9-36% lower accuracy); STaR-SFT models showing no improvement over base LLM; GenRM models failing to generalize to RewardBench tasks
- First 3 experiments:
  1. Compare zero-shot LLM-as-a-judge performance vs Bradley-Terry reward models on held-out UltraFeedback data to establish baseline performance gap
  2. Train GenRM (no CoT) on UltraFeedback and evaluate on both UltraFeedback and RewardBench to measure in-distribution vs OOD performance
  3. Train CoT-GenRM with STaR-DPO approach on UltraFeedback and compare performance to GenRM across all evaluation sets to measure impact of reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do generative reward models perform on multimodal tasks beyond text?
- Basis in paper: [inferred] The paper discusses potential future work in adapting GenRM to multimodal feedback, mentioning that VLMs have already proven to provide reasonable feedback in a zero-shot setting for text-to-image generation tasks.
- Why unresolved: The current study focuses solely on text-based preference modeling. While the authors mention the potential for multimodal applications, they do not provide empirical results or analysis for tasks involving images, audio, or other non-textual data.
- What evidence would resolve it: Empirical studies comparing the performance of GenRM on multimodal tasks (e.g., text-to-image generation) against traditional reward models or human evaluators, demonstrating generalization capabilities and alignment with human preferences across different modalities.

### Open Question 2
- Question: What is the impact of data selection strategies on the performance of iterative training methods in RLAIF?
- Basis in paper: [explicit] The authors mention that data selection has proven critical for model performance in pretraining and task-specific fine-tuning, but selecting optimal samples for iterative training methods in RLAIF remains an understudied issue with large potential for progress.
- Why unresolved: The study does not explore different data selection strategies for iterative training methods. It is unclear how the choice of samples affects the model's ability to learn and generalize, especially in the context of RLAIF.
- What evidence would resolve it: Comparative experiments evaluating the performance of GenRM using different data selection strategies (e.g., active learning, diversity sampling) for iterative training methods, demonstrating the impact on model performance and generalization.

### Open Question 3
- Question: How robust are generative reward models to adversarial attacks or "reward hacking"?
- Basis in paper: [explicit] The authors suggest that evaluating the "reward hacking" issues with Generative RMs is a promising direction for future work, especially given their results on out-of-distribution generalization.
- Why unresolved: The study does not assess the robustness of GenRM to adversarial inputs or attempts to exploit the reward function. Understanding the model's vulnerability to such attacks is crucial for real-world applications.
- What evidence would resolve it: Experiments exposing GenRM to adversarial examples or "reward hacking" attempts, measuring the model's ability to maintain alignment with human preferences and resist manipulation.

## Limitations

- The approach relies heavily on specific datasets (UltraFeedback, UltraInteract, RewardBench) which may limit generalizability to other domains or task types
- The iterative bootstrapping approach may amplify biases present in the training data and is sensitive to initialization
- The computational cost of generating reasoning traces and performing multiple training iterations may limit practical applicability

## Confidence

- **High Confidence**: In-distribution performance claims (GenRM matches Bradley-Terry accuracy), basic algorithmic description, and dataset usage
- **Medium Confidence**: Out-of-distribution generalization improvements (10-45% gains), effectiveness of chain-of-thought reasoning, and the STaR-based training methodology
- **Low Confidence**: Claims about why specific mechanisms work (e.g., whether reasoning traces truly capture human judgment processes), the scalability of the approach to larger models, and the robustness of results across different LLM architectures

## Next Checks

1. **Distribution Shift Analysis**: Systematically analyze the covariate and concept shift between UltraFeedback and RewardBench datasets to determine if performance differences are due to genuine generalization capabilities or dataset artifacts

2. **Ablation Studies**: Conduct controlled experiments removing key components (reasoning traces, iterative training, STaR methodology) to isolate which elements contribute most to the performance gains and verify the mechanism claims

3. **Cross-Domain Generalization**: Evaluate GenRM on entirely different types of preference data (e.g., image generation, code completion) to test whether the approach generalizes beyond text-based conversational tasks and to identify potential domain-specific limitations