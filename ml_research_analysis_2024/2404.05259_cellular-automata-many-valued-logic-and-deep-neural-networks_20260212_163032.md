---
ver: rpa2
title: Cellular automata, many-valued logic, and deep neural networks
arxiv_id: '2404.05259'
source_url: https://arxiv.org/abs/2404.05259
tags:
- networks
- network
- relu
- logic
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a theory linking deep neural networks, cellular
  automata (CA), and many-valued (MV) logic. The core problem addressed is learning
  the logical rules governing CA behavior from evolution traces using neural networks.
---

# Cellular automata, many-valued logic, and deep neural networks

## Quick Facts
- arXiv ID: 2404.05259
- Source URL: https://arxiv.org/abs/2404.05259
- Authors: Yani Zhang; Helmut Bölcskei
- Reference count: 40
- One-line primary result: Deep ReLU networks can learn and express the logical rules of cellular automata through many-valued logic formulas

## Executive Summary
This paper establishes a novel theoretical connection between cellular automata (CA), many-valued (MV) logic, and deep neural networks. The authors demonstrate that cellular automata can be characterized as logical machines operating in Łukasiewicz MV logic, and that their behavior can be learned and represented by deep ReLU networks. The work provides a comprehensive framework for understanding CA as MV logic functions, shows how these functions can be realized by neural networks, and presents an algorithm to extract the underlying logical formulas from trained networks.

## Method Summary
The approach leverages the McNaughton theorem to establish that functions in Łukasiewicz MV logic correspond to continuous piecewise linear functions with rational coefficients, which can be realized by ReLU networks. The method involves: (1) training an RNN on CA evolution traces to learn the transition function with integer weights and rational biases in Q_k; (2) converting the trained ReLU network to a σ-network for bounded outputs; and (3) systematically extracting DMV terms from each neuron using an inductive decomposition algorithm, then composing these to form the overall formula.

## Key Results
- Every CA transition function can be realized by a deep ReLU network with rational coefficients
- The dynamical behavior of CA can be emulated by recurrent neural networks
- An algorithm exists to extract the MV logic formulas underlying CA from trained neural networks
- The extracted formulas are functionally equivalent to the original CA behavior when proper interpolation is achieved

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep ReLU networks can learn the logical rules of cellular automata by interpolating their transition functions into continuous piecewise linear functions with rational coefficients.
- Mechanism: The McNaughton theorem establishes that functions in Łukasiewicz MV logic are exactly the continuous piecewise linear functions with rational coefficients. Since ReLU networks naturally realize such functions, training a ReLU network on CA evolution traces will produce a network whose structure and weights encode the underlying MV formula.
- Core assumption: The CA transition function can be linearly interpolated to a continuous piecewise linear function with rational coefficients.
- Evidence anchors:
  - [abstract]: "This is accomplished by first establishing a novel connection between CA and Łukasiewicz propositional logic."
  - [section]: "Specifically, we show that many-valued (MV) logic, specifically Łukasiewicz propositional logic, constitutes a suitable language for characterizing general CA as logical machines."
  - [corpus]: Weak - corpus does not contain specific evidence about interpolation to rational coefficients.
- Break condition: If the CA transition function requires irrational coefficients for interpolation, the correspondence with MV logic breaks down.

### Mechanism 2
- Claim: Recurrent neural networks can emulate the dynamical behavior of cellular automata by embedding the ReLU network that realizes the CA transition function.
- Mechanism: An RNN maintains a hidden state that stores the states of neighboring cells. At each time step, the ReLU network computes the next state of the current cell based on its own state and those of its neighbors, then updates the hidden state accordingly.
- Core assumption: The CA neighborhood structure can be mapped to an RNN hidden state vector that captures the relevant spatial context.
- Evidence anchors:
  - [abstract]: "Finally, we show that the dynamical behavior of CA can be realized by recurrent neural networks."
  - [section]: "Theorem 3.7... There exists an RNN that maps every configuration over the cellular space to the next configuration."
  - [corpus]: Missing - corpus does not provide specific evidence about RNN implementation details.
- Break condition: If the CA neighborhood is too large or irregular, the RNN hidden state design may become impractical.

### Mechanism 3
- Claim: DMV formulas underlying CA behavior can be extracted from trained ReLU networks by converting them to σ-networks and applying a systematic decomposition algorithm.
- Mechanism: ReLU neurons are converted to σ-neurons (bounded between 0 and 1) to ensure their outputs correspond to values in the MV algebra. The σ-network is then decomposed layer by layer, with each neuron's function expressed as a combination of MV operations (⊕, ⊙, ¬) using the inductive lemma. The resulting MV terms are algebraically composed to yield the overall formula.
- Core assumption: Each σ-neuron can be expressed as a combination of MV operations, and the compositional structure of the network corresponds to compositions in MV algebra.
- Evidence anchors:
  - [abstract]: "A corresponding algorithm together with a software implementation is provided."
  - [section]: "We now discuss how DMV formulae can be read out from the network Φf... Applying this algorithm... indeed recovers the MV term."
  - [corpus]: Missing - corpus does not provide specific evidence about the extraction algorithm details.
- Break condition: If the network contains non-linearities or operations outside the MV algebra, the extraction algorithm may fail.

## Foundational Learning

- Concept: Łukasiewicz Many-Valued Logic
  - Why needed here: It provides the logical framework that corresponds to general cellular automata with arbitrary state sets, extending Boolean logic to handle more than two truth values.
  - Quick check question: What operation in Łukasiewicz logic corresponds to the CA state update rule when states are in [0,1]?

- Concept: McNaughton Theorem
  - Why needed here: It establishes the precise correspondence between continuous piecewise linear functions with rational coefficients and formulas in Łukasiewicz logic, which is the foundation for connecting CA to ReLU networks.
  - Quick check question: What are the two necessary and sufficient conditions for a function to have an associated MV term according to the McNaughton theorem?

- Concept: Continuous Piecewise Linear Functions
  - Why needed here: ReLU networks naturally realize such functions, and the McNaughton theorem shows these are exactly the functions that can be expressed in Łukasiewicz logic.
  - Quick check question: Why do ReLU networks always realize continuous piecewise linear functions?

## Architecture Onboarding

- Component map:
  - CA simulation: Grid of cells with state values in [0,1]
  - Transition function: ReLU network with integer weights and biases in Q_k
  - RNN wrapper: Hidden state vector storing neighbor states, affine transformation for state update
  - Formula extraction: σ-network conversion, layer-by-layer decomposition, MV term composition

- Critical path:
  1. Define CA parameters (dimension, neighborhood, state set)
  2. Generate training data from CA evolution traces
  3. Train ReLU network to interpolate transition function with integer weights/biases in Q_k
  4. Wrap trained network in RNN structure for CA emulation
  5. Convert ReLU network to σ-network for formula extraction
  6. Apply decomposition algorithm to extract MV formula

- Design tradeoffs:
  - Integer vs. rational weights: Integer weights ensure correspondence with MV logic but may limit expressiveness
  - Network depth: Deeper networks can represent more complex CA but are harder to interpret
  - Neighborhood size: Larger neighborhoods require larger hidden states in RNN but capture more context

- Failure signatures:
  - Poor interpolation: Training loss remains high, indicating the network cannot accurately represent the transition function
  - Extraction failure: σ-network conversion produces incorrect results or decomposition algorithm fails to terminate
  - RNN instability: Hidden state values diverge or produce nonsensical outputs during CA emulation

- First 3 experiments:
  1. Binary CA with simple rule (e.g., XOR-like): Verify that the extracted formula matches the known Boolean expression
  2. 3-state totalistic CA: Test the full pipeline from training to formula extraction with non-binary states
  3. Multi-dimensional CA: Extend the RNN implementation to 2D or 3D cellular spaces and verify correct dynamical behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DMV formula extraction algorithm be extended to handle non-interpolating ReLU networks, where the network output does not match the CA transition function on all points of the state set?
- Basis in paper: [explicit] The paper assumes interpolation (Equation 25) and states "this, of course, requires that the RNN being trained have 'seen' all possible combinations of neighborhood states"
- Why unresolved: The paper explicitly focuses on the interpolation case and does not address what happens when this assumption fails
- What evidence would resolve it: Experiments showing the algorithm's behavior on networks that only approximate rather than perfectly interpolate CA transition functions

### Open Question 2
- Question: How does the choice of n-simplex subdivision in Lemma 4.1 affect the complexity and interpretability of the extracted DMV formulae?
- Basis in paper: [explicit] "the set of n-simplices underlying the linear pieces pj constructed in the proof of Lemma 4.1 not being unique" and Example 4.1 shows different subdivisions lead to "functionally different" but equivalent formulae
- Why unresolved: The paper acknowledges non-uniqueness but does not analyze how different subdivisions impact formula complexity or practical interpretability
- What evidence would resolve it: Systematic comparison of extracted formulae across different subdivision strategies with complexity metrics

### Open Question 3
- Question: Can the RNN-based CA identification approach handle partially observed evolution traces where some cell states are missing or corrupted?
- Basis in paper: [inferred] The paper assumes complete knowledge of the cellular space, state set, and neighborhood set, and that training sequences are "sufficiently long and their initial configurations exhibit sufficient richness"
- Why unresolved: Real-world CA observation data may contain missing values or noise, but the paper does not address robustness to such imperfections
- What evidence would resolve it: Experiments testing the algorithm's performance on evolution traces with various levels of missing data or noise corruption

## Limitations

- The paper assumes exact interpolation between CA states and continuous values, which may not hold for complex CA with irregular behavior
- The extraction algorithm details and implementation are not fully specified, limiting reproducibility
- The framework focuses on CA with well-defined neighborhoods and state sets, potentially limiting applicability to more complex or stochastic systems

## Confidence

- **High confidence**: The theoretical framework connecting CA transition functions to MV logic via continuous piecewise linear functions is rigorously established through the McNaughton theorem
- **Medium confidence**: The claim that trained ReLU networks can be systematically converted to extract DMV formulas, as the algorithm details are briefly described but implementation specifics are missing
- **Low confidence**: The practical feasibility of the RNN wrapper implementation for arbitrary CA dimensions and neighborhood structures, as the paper lacks concrete architectural specifications and empirical validation

## Next Checks

1. **Empirical verification of formula extraction**: Implement the σ-network conversion and decomposition algorithm for a simple 1D binary CA (e.g., Rule 90), extract the DMV formula, and verify it reproduces the CA behavior on all possible neighborhood configurations.

2. **RNN wrapper validation**: For a 2D CA with Moore neighborhood, implement the RNN structure as described in Theorem 3.7, train it on CA evolution data, and verify it correctly emulates the CA dynamics across multiple time steps.

3. **Scalability test**: Apply the full pipeline to a larger CA (e.g., 3D with larger state set), measuring both the accuracy of the trained network in approximating the transition function and the complexity of the extracted DMV formula.