---
ver: rpa2
title: 'Language Model Cascades: Token-level uncertainty and beyond'
arxiv_id: '2404.10136'
source_url: https://arxiv.org/abs/2404.10136
tags:
- deferral
- language
- uncertainty
- large
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing deferral rules
  for language model cascades, where a smaller model handles easy instances and a
  larger model handles hard ones. The authors show that simple sequence-level uncertainty
  measures, such as Chow-Sum and Chow-Average, suffer from length bias issues, either
  over- or under-emphasizing outputs based on their lengths.
---

# Language Model Cascades: Token-level uncertainty and beyond

## Quick Facts
- arXiv ID: 2404.10136
- Source URL: https://arxiv.org/abs/2404.10136
- Reference count: 40
- Primary result: Token-level uncertainty through quantiles and learned post-hoc deferral rules significantly improves cost-quality tradeoffs in language model cascades

## Executive Summary
This paper addresses the challenge of designing deferral rules for language model cascades, where a smaller model handles easy instances and a larger model handles hard ones. The authors show that simple sequence-level uncertainty measures, such as Chow-Sum and Chow-Average, suffer from length bias issues, either over- or under-emphasizing outputs based on their lengths. To mitigate this, they propose exploiting token-level uncertainty through quantiles and learned post-hoc deferral rules. Experiments on various NLP benchmarks with FLAN-T5 models demonstrate that incorporating token-level uncertainty and intermediate embeddings from the larger model significantly improves cost-quality tradeoffs.

## Method Summary
The authors propose a framework for efficient language model cascades that uses a small model for initial inference and defers to a larger model when uncertainty is high. They first identify length bias in traditional sequence-level uncertainty measures (Chow-Sum and Chow-Average), then introduce Chow-Quantile using token-level uncertainty captured through various quantiles. Finally, they develop learned post-hoc deferral rules that combine quantile features with embeddings from both models to optimize the cost-quality tradeoff. The approach is evaluated on multiple NLP tasks including classification, QA, and translation using FLAN-T5 models.

## Key Results
- Chow-Sum and Chow-Average suffer from length bias, over-deferring long sequences or short sequences respectively
- Chow-Quantile with token-level uncertainty consistently outperforms fixed aggregation methods across all tasks
- Learned post-hoc deferral rules using quantile features and intermediate embeddings provide additional performance gains
- The proposed method achieves better cost-quality tradeoffs than baselines across multiple NLP benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Simple sequence-level uncertainty measures like Chow-Sum and Chow-Average suffer from length bias issues, causing over- or under-deferral based on output length.
- **Mechanism:** These measures aggregate per-token probabilities across the entire sequence, but longer sequences have more opportunities for compounding errors in probability estimates. Chow-Sum tends to defer longer sequences because they accumulate more negative log-probabilities, while Chow-Average over-corrects by favoring shorter sequences.
- **Core assumption:** The length bias problem exists because language models produce imperfect probability estimates for each token, and these errors compound multiplicatively (or additively in log space) across sequence length.
- **Evidence anchors:**
  - [abstract] "simple sequence-level uncertainty measures, such as Chow-Sum and Chow-Average, suffer from length bias issues, either over- or under-emphasizing outputs based on their lengths"
  - [section] "Chow-Sum computes the aggregate sequence-level log-probability. A natural variant is the average of the per-token log-probabilities. This is equivalently the length normalized log-probability, or the log-perplexity"
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.342, average citations=0.0." - This indicates moderate relevance in the literature, though citations are low suggesting this is a relatively novel insight
- **Break condition:** If language models produced perfectly calibrated per-token probabilities, the length bias would disappear and simple aggregation would work effectively.

### Mechanism 2
- **Claim:** Token-level uncertainty captured through quantiles provides richer information than simple aggregation, leading to better deferral decisions.
- **Mechanism:** Instead of aggregating all token uncertainties into a single score, using quantiles (especially minimum and high-percentile values) captures different aspects of uncertainty distribution. The minimum quantile identifies problematic tokens like repetitions or unknown tokens, while high percentiles capture uncertainty in shorter sequences.
- **Core assumption:** Different quantiles capture complementary sources of uncertainty information that simple aggregation methods miss.
- **Evidence anchors:**
  - [abstract] "We propose to exploit the richer token-level uncertainty information implicit in generative LMs. We argue that na¨ıve predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties"
  - [section] "The discussion in §3.2 suggests there is value in considering the following generalization of the maximal sequence probability: squant(x, α) = quantileα(...)"
  - [corpus] Weak - the corpus shows related work on cascades but doesn't specifically address quantile-based uncertainty measures
- **Break condition:** If the distribution of per-token uncertainties were uniform or if all tokens contributed equally to sequence quality, quantile-based approaches would offer no advantage over simple aggregation.

### Mechanism 3
- **Claim:** Learned post-hoc deferral rules using quantile features and intermediate embeddings significantly outperform fixed aggregation strategies.
- **Mechanism:** By training a model to combine quantile features with embeddings from both the small and large models, the system can learn task-specific patterns for when deferral is beneficial. Intermediate embeddings from the larger model provide additional information without requiring full model inference.
- **Core assumption:** The relationship between uncertainty features and deferral decisions is complex enough to require learned models rather than hand-crafted rules.
- **Evidence anchors:**
  - [abstract] "We further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff"
  - [section] "The above target labels exploit information from the large model during training. Importantly, we cannot directly use such information during inference, as it would require querying the large model"
  - [corpus] "Gatekeeper: Improving Model Cascades Through Confidence Tuning" and "Cascade-Aware Training of Language Models" suggest that learned routing mechanisms are an active area of research
- **Break condition:** If the deferral decision boundary were simple and linear in the feature space, a learned model would provide no advantage over carefully designed fixed rules.

## Foundational Learning

- **Concept:** Language model probability estimation and token-level uncertainty
  - Why needed here: The entire cascade deferral mechanism relies on understanding how language models assign probabilities to tokens and how these probabilities relate to uncertainty
  - Quick check question: If a language model assigns probability 0.9 to each token in a 10-token sequence, what is the probability of the entire sequence? What if the probabilities vary between 0.5 and 0.95?

- **Concept:** Cascading and adaptive inference
  - Why needed here: The paper's core contribution is about designing deferral rules for cascades, which requires understanding the cascade framework and cost-quality tradeoffs
  - Quick check question: In a two-model cascade with costs c1 and c2 (where c1 << c2), what is the average cost if 30% of examples are deferred? How does this compare to always using the large model?

- **Concept:** Quantile statistics and their interpretation
  - Why needed here: The paper uses quantiles of per-token probability distributions as features for deferral decisions
  - Quick check question: For a sequence with per-token probabilities [0.9, 0.95, 0.8, 0.99, 0.7], what are the 0th, 0.5, and 1.0 quantiles? How would each quantile capture different aspects of uncertainty?

## Architecture Onboarding

- **Component map:** Input prompt -> Small model inference -> Deferral rule module -> (If defer) Large model inference -> Output
- **Critical path:** 1) Input prompt → Small model inference 2) Extract per-token probabilities and embeddings 3) Compute quantile features and other statistics 4) Apply deferral rule (learned or fixed) 5) If defer, run large model; otherwise use small model output 6) Return final output
- **Design tradeoffs:** Fixed vs. learned deferral rules (simplicity vs. performance); feature complexity (more features improve performance but increase overhead); intermediate embedding usage (more information vs. latency)
- **Failure signatures:** High deferral rate with minimal quality improvement (rule too conservative); low deferral rate despite poor small model performance (rule too aggressive); length bias in deferral decisions (need better normalization); overfitting in learned deferral rules (poor validation performance)
- **First 3 experiments:** 1) Implement and compare Chow-Sum vs Chow-Average deferral on a small translation dataset to observe length bias 2) Add quantile features (0th and 1.0) to the deferral rule and measure improvement on classification tasks 3) Train a simple MLP deferral rule using quantile features and compare against fixed rules on multiple datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RLHF fine-tuning affect the findings on token-level uncertainty and cascading performance?
- Basis in paper: [inferred] The authors mention that models with RLHF fine-tuning become uncalibrated (Figure 8 in OpenAI 2023) and suggest it would be interesting to see how various fine-tuning steps affect the findings.
- Why unresolved: The paper does not conduct experiments with RLHF fine-tuned models to empirically validate how this impacts token-level uncertainty measures and deferral rule performance.
- What evidence would resolve it: Experiments comparing cascading performance using Chow-Quantile, Post-Hoc-Quantile, and Post-Hoc-Embed methods on both pre-trained and RLHF fine tuned FLAN-T5 models across multiple NLP tasks, showing changes in AUC-DF and deferral curve shapes.

### Open Question 2
- Question: How do different decoder-only architectures (like GPT vs FLAN-T5 encoder-decoder) perform under the proposed cascading strategies?
- Basis in paper: [explicit] The authors state they focused on FLAN-T5 instruction-tuned Encoder-Decoder models and believe the insights should generalize to Decoder-only architectures, but suggest it would be interesting to evaluate the proposed approaches for such architectures.
- Why unresolved: The paper only evaluates on encoder-decoder models and does not empirically test whether token-level uncertainty measures and post-hoc deferral rules transfer to decoder-only architectures.
- What evidence would resolve it: Direct comparison of Chow-Quantile, Post-Hoc-Quantile, and Post-Hoc-Embed performance on GPT models vs FLAN-T5 models for the same NLP tasks, with analysis of differences in deferral curves and AUC-DF scores.

### Open Question 3
- Question: How does the choice of quantile values in Chow-Quantile affect performance across different NLP tasks, and can we learn optimal quantile weights?
- Basis in paper: [explicit] The authors observe that different quantiles capture complementary uncertainty information (e.g., Chow-Quantile-0 captures repetitions and unknown tokens, Chow-Quantile-0.8 captures uncertainty in shorter predictions) and that there is no fixed quantile that works well across all tasks.
- Why unresolved: While the paper proposes learning a post-hoc deferral rule using quantiles as features, it does not explore whether the model learns to weight certain quantiles more heavily for specific task types or if there are patterns in optimal quantile selection.
- What evidence would resolve it: Analysis of learned weights for different quantiles across tasks in the Post-Hoc-Quantile model, identifying which quantiles are most important for classification vs generation vs translation tasks, and whether task-specific quantile selection could improve performance.

## Limitations

- The approach relies on quality labels or proxy metrics for training learned deferral rules, which may not be available in all practical scenarios
- Performance depends on the quality of per-token probability estimates, which can be problematic for models with poor calibration or generation pathologies
- Using intermediate embeddings from the larger model adds computational overhead that may offset some of the efficiency gains from cascading

## Confidence

**High Confidence**: The existence and characterization of length bias in Chow-Sum and Chow-Average measures - directly observable from mathematical formulation and empirical demonstrations.

**Medium Confidence**: The superiority of Chow-Quantile over fixed aggregation methods and the effectiveness of learned post-hoc deferral rules - experimental results are compelling but depend on specific conditions and availability of training data.

**Low Confidence**: The claim that incorporating intermediate embeddings from the larger model provides a significant additional boost - requires the larger model to be available during training, which may not be practical, and the trade-off between cost and benefit is not thoroughly analyzed.

## Next Checks

1. **Length Bias Verification**: Implement Chow-Sum and Chow-Average deferral rules on a controlled synthetic dataset where sequence quality is independent of length. Measure whether deferral rates correlate with sequence length as predicted, and quantify the magnitude of this bias across different language models and sequence length distributions.

2. **Quantile Feature Ablation**: Conduct controlled experiments removing each quantile feature (0th, 0.4, 0.8, 1.0) from the Chow-Quantile approach to determine which quantiles contribute most to performance improvements. This will validate whether all proposed quantiles are necessary or if a subset captures the majority of the benefit.

3. **Training Data Dependency Analysis**: Evaluate the post-hoc deferral rules when trained on different quality proxy metrics (e.g., model confidence, cross-entropy, heuristic scores) rather than ground truth labels. This will assess the practical applicability of learned rules in scenarios where quality labels are expensive or unavailable.