---
ver: rpa2
title: Disentangled Hyperbolic Representation Learning for Heterogeneous Graphs
arxiv_id: '2406.10367'
source_url: https://arxiv.org/abs/2406.10367
tags:
- graph
- heterogeneous
- node
- information
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Dis-H2GCN, a method for disentangling structural
  and semantic information in heterogeneous graph representation learning. The key
  idea is to separately learn structural and semantic representations using hyperbolic
  graph convolutional networks and mutual information constraints.
---

# Disentangled Hyperbolic Representation Learning for Heterogeneous Graphs

## Quick Facts
- arXiv ID: 2406.10367
- Source URL: https://arxiv.org/abs/2406.10367
- Reference count: 40
- Key outcome: Dis-H2GCN achieves up to 18.01% improvement in node classification and 8.08% improvement in link prediction on five real-world datasets

## Executive Summary
This paper proposes Dis-H2GCN, a method for disentangling structural and semantic information in heterogeneous graph representation learning. The key innovation is to separately learn structural and semantic representations using hyperbolic graph convolutional networks with mutual information constraints. Experiments on five real-world datasets demonstrate state-of-the-art performance, with the method showing significant improvements over baseline approaches in both node classification and link prediction tasks while also demonstrating benefits in training efficiency and robustness.

## Method Summary
Dis-H2GCN learns structural and semantic embeddings separately using hyperbolic graph convolutional networks (HGCN B and he-HGCN B) and then disentangles them through mutual information minimization and discriminator training. The method first learns pure structural embeddings by converting the heterogeneous graph to a homogeneous graph, then learns semantic embeddings via type-based hyperbolic message passing with contrastive learning. The disentangling module uses mutual information estimation to ensure independence between structural and semantic representations, which are finally fused using Möbius addition for downstream tasks.

## Key Results
- Achieves up to 18.01% improvement in node classification over baseline methods
- Obtains up to 8.08% improvement in link prediction performance
- Demonstrates better training efficiency and robustness against perturbations compared to existing approaches

## Why This Works (Mechanism)

### Mechanism 1
Separating structural and semantic features into distinct hyperbolic embeddings improves downstream task performance. The method learns pure structural embeddings by converting to a homogeneous graph, then learns semantic embeddings via type-based hyperbolic message passing with contrastive learning, and finally disentangles them using mutual information minimization and discriminator training. This assumes structural and semantic features are statistically independent and can be effectively separated using the proposed constraints.

### Mechanism 2
Hyperbolic geometry better captures power-law distributions and hierarchical structures in real-world graphs compared to Euclidean spaces. The model embeds all components in hyperbolic spaces (Poincaré ball) to match the exponential expansion property of power-law distributed graphs. This assumes real-world heterogeneous graphs exhibit power-law distributions in both global structure and individual edge types.

### Mechanism 3
Contrastive learning with type-based message propagation captures unique semantic patterns of different edge types. The method creates perturbed views of the input graph and uses contrastive learning to maximize mutual information between original and perturbed views, while message passing is performed separately for each edge type in hyperbolic spaces. This assumes different edge types have distinct message patterns that can be learned through type-specific parameters and contrastive learning.

## Foundational Learning

- **Concept**: Poincaré ball model of hyperbolic geometry
  - Why needed here: The method uses the Poincaré ball for all hyperbolic operations including distance calculation, Möbius addition, and exponential/logarithmic maps.
  - Quick check question: Can you explain why the Poincaré ball is preferred over the Lorentz model for this application?

- **Concept**: Mutual information and its estimation
  - Why needed here: The disentangling module uses mutual information minimization to ensure structural and semantic embeddings are independent.
  - Quick check question: How does the CLUB upper bound estimation work for calculating mutual information between structural and semantic embeddings?

- **Concept**: Graph convolutional networks in hyperbolic spaces
  - Why needed here: Both HGCN B and he-HGCN B are hyperbolic GCN variants with specialized operations for the Poincaré ball.
  - Quick check question: What are the key differences between hyperbolic linear transformation and standard Euclidean linear transformation?

## Architecture Onboarding

- **Component map**: Graph input → Structural module (HGCN B) → Structural embeddings; Graph input → Semantic module (he-HGCN B) → Semantic embeddings; Structural and semantic embeddings → Disentangling module → Final embeddings; Final embeddings → Downstream tasks

- **Critical path**: Graph input → Structural module (HGCN B) → Structural embeddings; Graph input → Semantic module (he-HGCN B) → Semantic embeddings; Structural and semantic embeddings → Disentangling module → Final embeddings; Final embeddings → Downstream tasks (node classification or link prediction)

- **Design tradeoffs**: Separating structural and semantic learning increases model complexity but improves disentanglement; Hyperbolic geometry matches data distribution but adds computational overhead; Contrastive learning improves semantic embeddings but requires additional training

- **Failure signatures**: Poor performance on downstream tasks (check if disentanglement constraints are too strong or too weak); Training instability (verify hyperbolic operations and curvature initialization); Slow convergence (adjust contrastive learning temperature or discriminator parameters)

- **First 3 experiments**: 1) Train Dis-H2GCN on a small heterogeneous graph (e.g., DBLP) with default parameters and verify both structural and semantic embeddings improve over baseline; 2) Remove the disentangling module and compare performance to confirm its contribution; 3) Switch from hyperbolic to Euclidean spaces and measure performance degradation to validate the geometric choice

## Open Questions the Paper Calls Out

### Open Question 1
How does the disentangling module's performance scale with increasing graph size and complexity? The paper evaluates Dis-H2GCN on five real-world datasets but doesn't explicitly explore scalability. Systematic experiments with graphs of increasing size and complexity, measuring disentanglement effectiveness and overall performance, would resolve this.

### Open Question 2
Can the disentangling approach be extended to other types of graph data, such as dynamic graphs or knowledge graphs? The paper focuses on heterogeneous static graphs, but the disentangling concept might be applicable to other graph types. Experiments applying Dis-H2GCN or a similar disentangling approach to dynamic graphs or knowledge graphs would resolve this.

### Open Question 3
How does the choice of hyperbolic space curvature impact the disentanglement performance and overall model effectiveness? The paper mentions that curvatures are initialized as -1 and kept trainable, but doesn't explore the impact of different curvature values. Experiments varying the initial curvature values and analyzing their impact would resolve this.

## Limitations
- Claims about disentangling structural and semantic features rely on theoretical assumptions about independence that aren't directly validated in real-world graphs
- Hyperbolic geometry advantages are asserted based on power-law distribution observations without systematic ablation studies in Euclidean spaces across all datasets
- Mutual information estimation method (CLUB upper bound) may introduce approximation errors that affect disentanglement quality

## Confidence

- **High Confidence**: Performance improvements over baselines on node classification and link prediction tasks are well-documented with multiple datasets and metrics
- **Medium Confidence**: The mechanism of separating structural and semantic learning into distinct modules is supported by experimental results, though theoretical necessity could be further explored
- **Low Confidence**: The specific advantage of hyperbolic geometry over Euclidean spaces is not conclusively demonstrated, as ablation studies in Euclidean space are limited

## Next Checks

1. **Ablation Study**: Remove the disentangling module entirely and compare performance to determine if separation of structural and semantic features is essential for observed improvements

2. **Geometry Comparison**: Implement the same model architecture using Euclidean spaces instead of hyperbolic spaces and evaluate performance degradation across all five datasets to quantify geometric advantage

3. **Feature Independence Analysis**: After training, compute and analyze the correlation between structural and semantic embeddings to empirically validate whether mutual information constraints successfully achieve independence