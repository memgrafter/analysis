---
ver: rpa2
title: Sequential Signal Mixing Aggregation for Message Passing Graph Neural Networks
arxiv_id: '2409.19414'
source_url: https://arxiv.org/abs/2409.19414
tags:
- ssma
- graph
- aggregation
- datasets
- neighbor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limited performance of sum-based aggregators
  in Graph Neural Networks (GNNs) despite their theoretical advantages. The authors
  propose Sequential Signal Mixing Aggregation (SSMA), which treats neighbor features
  as 2D discrete signals and sequentially convolves them to enhance neighbor mixing.
---

# Sequential Signal Mixing Aggregation for Message Passing Graph Neural Networks

## Quick Facts
- arXiv ID: 2409.19414
- Source URL: https://arxiv.org/abs/2409.19414
- Authors: Mitchell Keren Taraday; Almog David; Chaim Baskin
- Reference count: 40
- Primary result: Proposes SSMA, a plug-and-play aggregation module that significantly improves GNN performance by enhancing neighbor feature mixing, achieving state-of-the-art results on multiple benchmarks.

## Executive Summary
This paper addresses the limited performance of sum-based aggregators in Graph Neural Networks (GNNs) despite their theoretical advantages. The authors propose Sequential Signal Mixing Aggregation (SSMA), which treats neighbor features as 2D discrete signals and sequentially convolves them to enhance neighbor mixing. SSMA has a provably polynomial representation size of O(n²d) and efficiently generalizes the DeepSets polynomial to multidimensional features. The method includes practical adjustments for stability and scalability. Extensive experiments show that SSMA significantly improves performance when integrated into various GNN architectures across multiple benchmarks, achieving state-of-the-art results in many settings.

## Method Summary
SSMA is a plug-and-play aggregation module for Message Passing Graph Neural Networks that enhances neighbor feature mixing through 2D circular convolution. The method encodes neighbor features as 2D discrete signals, applies FFT-based circular convolution, and compresses the result through an MLP. SSMA provably achieves polynomial representation size O(n²d) compared to exponential for sum-based approaches, while maintaining permutation invariance. The authors integrate SSMA into multiple GNN architectures (GCN, GAT, GATv2, GIN, GraphGPS, PNA) and demonstrate consistent performance improvements across diverse benchmarks.

## Key Results
- SSMA achieves state-of-the-art performance on multiple graph classification benchmarks including TU datasets (MUTAG, PTC-MR, PROTEINS) and OGB datasets
- Significant improvements on datasets lacking node/edge features where traditional aggregation methods struggle
- Demonstrated effectiveness in long-range graph tasks on LRGB datasets (peptides-func, peptides-struct)
- Provides theoretical guarantees with polynomial representation size O(n²d) compared to exponential for sum-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSMA improves neighbor mixing by performing 2D circular convolution over neighbor features
- Mechanism: SSMA treats neighbor features as 2D discrete signals and sequentially convolves them, creating a higher-order mixing effect where each element of the output depends on all neighbors through product terms
- Core assumption: Mixing of features from distinct neighbors is critical for downstream task performance
- Evidence anchors:
  - [abstract] "SSMA treats the neighbor features as 2D discrete signals and sequentially convolves them, inherently enhancing the ability to mix features attributed to distinct neighbors"
  - [section 3] "sum-based aggregators fail to 'mix' features belonging to distinct neighbors"
  - [corpus] Weak evidence - related papers focus on efficient aggregation but not specifically on neighbor mixing
- Break condition: If downstream tasks do not require mixing of neighbor features, the convolution-based approach may not provide significant benefits

### Mechanism 2
- Claim: SSMA provides efficient generalization of DeepSets polynomial to multidimensional features
- Mechanism: Encodes each d-dimensional feature as a polynomial and reduces the problem to scalar case, then uses circular convolution to compute coefficients
- Core assumption: DeepSets polynomial construction can be extended to vector features through polynomial encoding
- Evidence anchors:
  - [section 4.2] "The key idea underlying our answer to this question is to encode each feature vector as another polynomial"
  - [section 4.1] "DeepSets polynomial coefficients can be efficiently computed and directly utilized as a multiset representation"
  - [corpus] No direct evidence - related papers focus on different aggregation approaches
- Break condition: If the polynomial encoding does not preserve the separation properties needed for the specific application

### Mechanism 3
- Claim: SSMA achieves polynomial representation size O(n²d) compared to exponential for sum-based approaches
- Mechanism: Uses 2D circular convolution with fixed evaluation points (roots of unity) to compute polynomial coefficients efficiently
- Core assumption: The number of separators needed for vector features grows polynomially with n and d
- Evidence anchors:
  - [abstract] "SSMA has a provably polynomial representation size of O(n²d)"
  - [section 4.2] "the number of separators is m = m1 × m2 = (n + 1)(n(d − 1) + 1) ∈ O(n²d)"
  - [corpus] Weak evidence - related papers focus on different aggregation strategies
- Break condition: If the polynomial construction does not provide sufficient separation power for the target application

## Foundational Learning

- Concept: Circular convolution theorem and FFT
  - Why needed here: SSMA implements 2D circular convolution using FFT for computational efficiency
  - Quick check question: How does circular convolution relate to polynomial multiplication and why is FFT useful here?

- Concept: Multiset representation and permutation invariance
  - Why needed here: SSMA must produce invariant representations of neighbor features regardless of neighbor order
  - Quick check question: