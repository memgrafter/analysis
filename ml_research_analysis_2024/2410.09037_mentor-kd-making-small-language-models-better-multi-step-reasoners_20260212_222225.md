---
ver: rpa2
title: 'Mentor-KD: Making Small Language Models Better Multi-step Reasoners'
arxiv_id: '2410.09037'
source_url: https://arxiv.org/abs/2410.09037
tags:
- reasoning
- distillation
- mentor
- student
- mentor-kd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mentor-KD, a knowledge distillation framework
  that improves small language models' multi-step reasoning ability by leveraging
  an intermediate-sized mentor model. The mentor fine-tuned on task-specific data
  augments training sets with additional rationales and provides soft labels, addressing
  limitations of insufficient and low-quality distillation sets from black-box LLM
  teachers.
---

# Mentor-KD: Making Small Language Models Better Multi-step Reasoners

## Quick Facts
- **arXiv ID**: 2410.09037
- **Source URL**: https://arxiv.org/abs/2410.09037
- **Reference count**: 23
- **Key outcome**: Mentor-KD improves small language models' multi-step reasoning ability using task-specific mentor models, achieving up to 2% better average accuracy than baselines.

## Executive Summary
Mentor-KD introduces a knowledge distillation framework that addresses limitations in distilling reasoning capabilities from large language models to small ones. The key innovation is using an intermediate-sized mentor model, fine-tuned on task-specific data, to augment training sets with additional rationales and provide soft labels. This approach overcomes the problem of insufficient and low-quality distillation sets that arise when directly using black-box LLM teachers. Experiments across arithmetic, commonsense, logical, and symbolic reasoning tasks demonstrate that Mentor-KD outperforms existing reasoning distillation baselines and shows robustness in low-resource scenarios where teacher-generated samples are limited.

## Method Summary
Mentor-KD works by first collecting Chain-of-Thought annotations from an LLM teacher using Zero-shot-CoT prompting, then filtering these annotations to retain only those where predictions match ground truth. An intermediate-sized mentor model is fine-tuned on this filtered data, which enables it to generate additional CoT rationales and soft labels for distillation. The student model is then trained using both fine-tuning on the combined CoT annotations and soft label distillation from the mentor model. This approach addresses the problem of limited and noisy distillation sets from black-box LLM teachers by leveraging a task-specific mentor to generate higher-quality rationales and probability distributions.

## Key Results
- Mentor-KD outperforms existing reasoning distillation baselines by up to 2% average accuracy across multiple reasoning task types
- The approach demonstrates robustness in low-resource scenarios, maintaining performance even with fewer teacher-generated samples
- Larger mentor models generally lead to better student performance, though even the smallest mentor outperforms baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Task-specific mentor models generate higher-quality CoT rationales than black-box LLM teachers by learning task-specific reasoning patterns through fine-tuning on filtered teacher annotations. The core assumption is that task-specific fine-tuning concentrates the mentor model's power toward the target reasoning ability. Evidence is weak as no direct corpus comparison of rationale quality exists. The approach breaks if the mentor model lacks sufficient capacity to learn from the filtered data.

### Mechanism 2
Mentor models provide soft labels that transfer internal knowledge from larger models through probability distributions over answer choices, allowing students to mimic predictive behavior rather than just copying rationales. The core assumption is that soft labels capture richer information than hard labels. Evidence is weak as no corpus evidence supports the effectiveness of soft label transfer. The approach breaks if temperature hyperparameters are set too high, causing soft labels to become uninformative.

### Mechanism 3
Mentor-KD is robust in low-resource scenarios by reducing dependency on LLM-generated data, as mentor models can generate substantial correct reasoning samples even when LLM teachers provide limited data. The core assumption is that mentor models can generalize from filtered teacher data to generate new correct rationales. Evidence is weak as no corpus evidence shows low-resource generalization. The approach breaks if initial teacher-generated data is too sparse for the mentor to learn meaningful patterns.

## Foundational Learning

- **Concept: Knowledge distillation fundamentals** - Needed because Mentor-KD builds on standard KD principles but extends them to reasoning tasks with CoT. Quick check: What is the difference between hard label and soft label distillation?
- **Concept: Chain-of-Thought prompting** - Needed because the entire framework depends on generating and distilling multi-step reasoning. Quick check: How does CoT prompting differ from standard few-shot prompting?
- **Concept: Data augmentation strategies** - Needed because mentor models generate additional training samples to complement limited teacher data. Quick check: What are the risks of over-augmenting training data in KD?

## Architecture Onboarding

- **Component map**: Teacher LLM -> Filtered CoT annotations -> Mentor model fine-tuning -> Mentor-generated rationales and soft labels -> Student model training
- **Critical path**: 1) Collect CoT annotations from teacher LLM, 2) Filter annotations where predictions match ground truth, 3) Fine-tune mentor model on filtered data, 4) Generate additional CoT rationales and soft labels from mentor, 5) Combine all data for student training, 6) Train student with joint rationale and soft label distillation
- **Design tradeoffs**: Mentor size vs training cost (larger mentors may generate better data but increase computational overhead), number of augmented samples vs noise (more augmentation can help but may introduce errors), temperature setting for soft labels (higher temperatures create softer distributions but may lose signal)
- **Failure signatures**: Student performance worse than baseline (likely indicates mentor is generating poor-quality rationales), no improvement in low-resource settings (mentor may not be generalizing well from limited data), overfitting on training data (too many mentor-generated samples without sufficient diversity)
- **First 3 experiments**: 1) Compare student performance with and without mentor-generated soft labels, 2) Test different numbers of mentor-augmented rationales per question, 3) Evaluate student performance on low-resource subsets of the data

## Open Questions the Paper Calls Out
1. How does the choice of mentor model architecture (encoder-decoder vs. decoder-only) impact the effectiveness of Mentor-KD in reasoning distillation?
2. What is the optimal number of augmented CoT rationales per question for maximizing student model performance without introducing noise?
3. How does the size of the mentor model affect the quality and diversity of the augmented distillation sets in Mentor-KD?
4. Can Mentor-KD be effectively applied to other reasoning strategies beyond Chain-of-Thought, such as program-guided reasoning or retrieval-based reasoning?

## Limitations
- Limited empirical validation of rationale quality - no direct corpus evidence comparing mentor vs teacher rationale quality
- Soft label effectiveness unvalidated - no ablation study isolating the impact of soft labels versus hard labels
- Low-resource robustness claims need stronger support - experiments only show results down to 100 samples per task, not truly extreme low-resource scenarios

## Confidence
- **High confidence**: Overall framework design and superiority over baseline distillation methods on tested datasets
- **Medium confidence**: Claim that mentor models provide substantially better rationales than teacher models
- **Low confidence**: Robustness claims in extreme low-resource scenarios (tested only down to 100 samples)

## Next Checks
1. Implement automated metrics to directly compare the quality of rationales generated by mentor models versus teacher models on the same set of questions
2. Conduct experiments isolating the contribution of soft label distillation by comparing student performance with only rationale fine-tuning, only soft label distillation, and the full Mentor-KD approach
3. Evaluate Mentor-KD performance on datasets reduced to <10 samples per task to determine the minimum viable dataset size for the approach to remain effective