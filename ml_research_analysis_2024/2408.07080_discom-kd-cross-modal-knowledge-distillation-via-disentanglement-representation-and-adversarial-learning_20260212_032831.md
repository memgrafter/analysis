---
ver: rpa2
title: 'DisCoM-KD: Cross-Modal Knowledge Distillation via Disentanglement Representation
  and Adversarial Learning'
arxiv_id: '2408.07080'
source_url: https://arxiv.org/abs/2408.07080
tags:
- knowledge
- cross-modal
- distillation
- learning
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DisCoM-KD, a novel framework for cross-modal
  knowledge distillation that explicitly models different types of per-modality information
  to transfer knowledge from multi-modal data to single-modal classifiers. Unlike
  traditional teacher/student paradigms, DisCoM-KD simultaneously learns all single-modal
  classifiers by effectively combining disentanglement representation learning with
  adversarial domain adaptation to extract domain-invariant, domain-informative, and
  domain-irrelevant features for each modality.
---

# DisCoM-KD: Cross-Modal Knowledge Distillation via Disentanglement Representation and Adversarial Learning

## Quick Facts
- arXiv ID: 2408.07080
- Source URL: https://arxiv.org/abs/2408.07080
- Authors: Dino Ienco; Cassio Fraga Dantas
- Reference count: 37
- Key outcome: DisCoM-KD framework achieves F1-scores of 47.69, 80.03, and 92.86 on SUNRGBD, EuroSat-MS-SAR, and TRISTAR datasets respectively

## Executive Summary
DisCoM-KD introduces a novel framework for cross-modal knowledge distillation that explicitly models different types of per-modality information to transfer knowledge from multi-modal data to single-modal classifiers. Unlike traditional teacher/student paradigms, DisCoM-KD simultaneously learns all single-modal classifiers by effectively combining disentanglement representation learning with adversarial domain adaptation to extract domain-invariant, domain-informative, and domain-irrelevant features for each modality. The method was evaluated on three standard multi-modal benchmarks (SUNRGBD, EuroSat-MS-SAR, and TRISTAR) and compared against recent state-of-the-art knowledge distillation frameworks.

## Method Summary
DisCoM-KD is a cross-modal knowledge distillation framework that extracts three per-modality embeddings: modality-invariant, modality-informative, and modality-irrelevant representations. The architecture uses two parallel modality branches, each with a modality-specific encoder, modality-invariant encoder, projection head, modality classifiers, and task classifier. The framework employs adversarial learning via Gradient Reversal Layer to enforce modality-invariance and orthogonality constraints to ensure complementary information extraction. Training involves optimizing five loss terms: classification loss, adversarial loss, modality classification loss, auxiliary task loss, and orthogonality constraint, using Adam optimizer with learning rate 1e-4 for 300 epochs.

## Key Results
- Achieved F1-scores of 47.69, 80.03, and 92.86 on SUNRGBD, EuroSat-MS-SAR, and TRISTAR datasets respectively
- Consistently outperforms competitors in both cross-modal and multi-modal knowledge distillation scenarios
- Eliminates need to train each student model separately and avoids arbitrary choices of modalities for teacher training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disentanglement learning enables simultaneous extraction of complementary feature types (invariant, informative, irrelevant) for each modality.
- Mechanism: The framework uses a two-encoder architecture per modality: one for modality-specific features (informative + irrelevant) and one for modality-invariant features, with a projection head ensuring consistent dimensionality.
- Core assumption: Different types of features can be explicitly separated and still contribute meaningfully to the downstream task.
- Evidence anchors:
  - [abstract] "explicitly models different types of per-modality information with the aim to transfer knowledge from multi-modal data to a single-modal classifier"
  - [section] "extracts three per-modality embeddings zinv, zin f, zirr referred as modality-invariant, modality-informative and modality-irrelevant representation"
- Break condition: If the three feature types cannot be cleanly separated or if combining them degrades performance.

### Mechanism 2
- Claim: Adversarial learning with GRL enforces modality-invariant representations by making them indistinguishable across modalities.
- Mechanism: A modality classifier with GRL is applied to the modality-invariant embeddings, encouraging the model to extract features that cannot be classified by modality.
- Core assumption: Making representations modality-invariant improves cross-modal knowledge transfer.
- Evidence anchors:
  - [abstract] "adversarial domain adaptation to simultaneously extract, foreach modality, domain-invariant, domain-informative and domain-irrelevant features"
  - [section] "we use an adversarial training strategy, implemented via Gradient Reversal Layer (GRL) [2] over the modality invariant (zinv) representations"
- Break condition: If modality invariance actually harms task performance or if the adversarial term destabilizes training.

### Mechanism 3
- Claim: Orthogonality constraints ensure complementary information across different feature types within each modality.
- Mechanism: The loss function explicitly enforces orthogonality between modality-invariant/informative and informative/irrelevant representations using dot product normalization.
- Core assumption: Forcing representations to be orthogonal prevents redundancy and ensures each captures distinct aspects of the data.
- Evidence anchors:
  - [abstract] "explicitly models different types of per-modality information"
  - [section] "we implement a double disentanglement process, enforcing orthogonality [12] between modality-invariant (zinv) and informative (zin f) representations, as well as between modality-informative (zin f) and irrelevant (zirr) embeddings"
- Break condition: If orthogonality constraints prevent the model from capturing necessary correlations or if they create training instability.

## Foundational Learning

- Concept: Knowledge Distillation fundamentals
  - Why needed here: DisCoM-KD builds upon knowledge distillation principles but modifies the paradigm significantly
  - Quick check question: What is the traditional teacher/student paradigm in knowledge distillation and how does DisCoM-KD differ from it?

- Concept: Disentangled representation learning
  - Why needed here: The framework explicitly separates different types of information for each modality
  - Quick check question: What are the three types of per-modality representations extracted by DisCoM-KD and what distinguishes them?

- Concept: Adversarial domain adaptation
  - Why needed here: Used to enforce modality-invariance in the learned representations
  - Quick check question: How does the Gradient Reversal Layer (GRL) work to make representations modality-invariant?

## Architecture Onboarding

- Component map: Two parallel modality branches, each with: modality-specific encoder, modality-invariant encoder, projection head, modality classifiers, task classifier. Plus global auxiliary classifiers.
- Critical path: Forward pass through both modality branches → extraction of three representations per modality → classification → loss computation → backpropagation through all components
- Design tradeoffs: Simultaneous learning of all single-modal classifiers vs. sequential teacher/student approach; complexity of disentanglement vs. potential performance gains
- Failure signatures: Degraded performance compared to baselines, unstable training (oscillating losses), poor modality invariance (modality classifier easily distinguishes representations)
- First 3 experiments:
  1. Implement and test the two-encoder architecture for a single modality with only the task loss (Lcl)
  2. Add the adversarial modality classifier (Ladv) and verify that modality-invariant representations become less distinguishable
  3. Add the orthogonality constraint (L⊥) and verify that the three representations become more distinct as measured by their dot products

## Open Questions the Paper Calls Out

- Question: How can DisCoM-KD be extended to handle more than two modalities simultaneously?
- Basis in paper: [explicit] The paper states "Our current process has only been assessed on cross-modal distillation tasks involving no more than two modalities. Extending DisCoM-KD to manage more than two modalities at once remains an open question."
- Why unresolved: The current framework architecture and loss functions are designed for two modalities, and it's unclear how to modify the adversarial term and disentanglement process to handle multiple modalities.
- What evidence would resolve it: A successful implementation of DisCoM-KD extended to three or more modalities with improved performance over existing multi-modal distillation methods would resolve this question.

## Limitations

- The exact implementation details of the gradient reversal layer (GRL) and its integration with the modality classifier are not fully specified
- The evaluation relies on only three datasets, which may limit generalizability to other cross-modal scenarios
- The computational complexity of the two-encoder architecture per modality is not thoroughly discussed

## Confidence

**High**: The framework's novel approach of combining disentanglement representation learning with adversarial domain adaptation for cross-modal knowledge distillation

**Medium**: The effectiveness of the three representation types (invariant, informative, irrelevant) and their orthogonality constraints in improving cross-modal transfer

**Medium**: The reported performance improvements over baseline methods across all three benchmark datasets

## Next Checks

1. **Implement a simplified ablation**: Remove the orthogonality constraint and measure its impact on performance to validate whether this component is truly essential or if simpler disentanglement would suffice.

2. **Test modality invariance rigorously**: Evaluate how well the modality-invariant representations actually generalize by testing them on completely unseen modalities or through modality transfer experiments beyond the standard evaluation protocol.

3. **Analyze computational overhead**: Measure the actual training time and memory requirements of DisCoM-KD compared to simpler baselines to assess the practical trade-offs of the more complex architecture.