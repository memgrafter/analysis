---
ver: rpa2
title: Relational Graph Convolutional Networks for Sentiment Analysis
arxiv_id: '2404.13079'
source_url: https://arxiv.org/abs/2404.13079
tags:
- graph
- text
- data
- dataset
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes leveraging Relational Graph Convolutional Networks
  (RGCNs) for sentiment analysis, combining them with pre-trained language models
  (BERT/RoBERTa) to capture complex relationships in text data. RGCNs excel at modeling
  diverse relationships between nodes in a graph, making them suitable for sentiment
  analysis where words and documents have various types of connections.
---

# Relational Graph Convolutional Networks for Sentiment Analysis

## Quick Facts
- arXiv ID: 2404.13079
- Source URL: https://arxiv.org/abs/2404.13079
- Authors: Asal Khosravi, Zahed Rahmati, Ali Vefghi
- Reference count: 24
- One-line primary result: RGCNs with pre-trained language models achieve 70.59% accuracy on balanced Amazon reviews and 91.17% on imbalanced version

## Executive Summary
This paper proposes using Relational Graph Convolutional Networks (RGCNs) for sentiment analysis by constructing a heterogeneous graph with word-word, word-document, and document-document edges. The approach combines RGCNs with pre-trained language models (BERT/RoBERTa) to capture complex relationships in text data. RGCNs excel at modeling diverse relationships between nodes, making them suitable for sentiment analysis where words and documents have various types of connections. The authors demonstrate effectiveness on Amazon and Digikala review datasets, showing RGCNs outperform traditional GCNs and pre-trained language models alone.

## Method Summary
The method constructs a heterogeneous graph from text corpora with three edge types: word-word connections weighted by PMI, word-document connections weighted by TF-IDF, and document-document connections weighted by Jaccard similarity. Pre-trained language models (BERT/RoBERTa) extract contextualized embeddings for each node, with document nodes using the CLS vector and word nodes using min-pooling over document contexts. A two-layer RGCN processes these embeddings using relation-specific weight matrices, followed by a softmax classifier for sentiment labels. The model is trained in a transductive manner using the entire graph structure.

## Key Results
- RGCN with RoBERTa achieves 70.59% accuracy on balanced Amazon dataset
- RGCN with RoBERTa achieves 91.17% accuracy on imbalanced Amazon dataset
- RGCNs outperform traditional GCNs and pre-trained language models alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RGCNs outperform GCNs on sentiment analysis because they capture diverse edge types between words and documents.
- Mechanism: RGCN uses relation-specific weight matrices (W_r) for each edge type, enabling different message transformations per relationship (e.g., PMI for word-word, TF-IDF for word-doc, Jaccard for doc-doc).
- Core assumption: Different semantic relationships benefit from distinct transformation functions rather than a shared GCN kernel.
- Evidence anchors:
  - [abstract] "Relational Graph Convolutional Networks (RGCNs) offer interpretability and flexibility by capturing dependencies between data points represented as nodes in a graph."
  - [section] "The difference between the RGCN method and the GCN method is that the GCN method operates on graphs with one type of edge, while the RGCN method operates on graphs with multiple types of edges and considers a different processing channel for each type of edge."
  - [corpus] Weak evidence: no cited RGCN vs GCN comparisons found in neighbor titles.
- Break condition: If the dataset edges are homogeneous or if all edges are treated as single relation type.

### Mechanism 2
- Claim: Pre-trained language model embeddings (BERT/RoBERTa) provide strong node feature initialization for RGCN.
- Mechanism: Text tokens are mapped to contextualized embeddings; document nodes use CLS vector, word nodes use min-pool over document contexts.
- Core assumption: Pre-trained LM embeddings encode rich semantic context that improves downstream graph message passing.
- Evidence anchors:
  - [section] "We first create an initial representation vector, using the pre-trained BERT model and the pre-trained RoBERTa model for each of the nodes (documents and words) in the graph."
  - [section] "Using formula 7, we consider the representation vector '[CLS]' as the representation vector of the document Dd."
  - [corpus] Weak evidence: no neighbor papers compare LM init vs random init in RGCNs.
- Break condition: If graph topology dominates over node features or if pre-training domain mismatches dataset.

### Mechanism 3
- Claim: Transductive training on the full corpus graph yields better performance than pure inductive learning for text classification.
- Mechanism: All documents and words are embedded in a single large graph; training uses labels only on train set nodes but full graph structure guides message passing.
- Core assumption: Graph structure (word co-occurrences, doc similarities) contains useful inductive biases for sentiment prediction.
- Evidence anchors:
  - [section] "Our model is trained in a transductive manner because, according to the transductive logic, it uses the entire graph structure to obtain embeddings."
  - [section] "The graph is constructed at the text corpus level, i.e., a new graph is created over the entire text data."
  - [corpus] Weak evidence: no neighbor papers discuss transductive vs inductive trade-offs in RGCNs.
- Break condition: If new documents arrive frequently, requiring inductive updates.

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: RGCN extends GCN; understanding neighbor aggregation is prerequisite to grasping relation-specific transformations.
  - Quick check question: In a GCN, how is the new representation of a node computed from its neighbors?

- Concept: Heterogeneous graphs and edge typing
  - Why needed here: The method constructs a graph with word-word, word-doc, doc-doc edges, each requiring distinct weights.
  - Quick check question: What distinguishes a heterogeneous graph from a homogeneous one?

- Concept: Pre-trained language models (BERT/RoBERTa)
  - Why needed here: These models provide contextualized embeddings used as initial node features before RGCN layers.
  - Quick check question: How does BERT's CLS token differ from other token embeddings?

## Architecture Onboarding

- Component map: Text preprocessing → Graph construction (nodes + typed edges) → LM feature extraction → RGCN layers (2-layer) → Softmax classifier
- Critical path:
  1. Build directed weighted heterogeneous graph.
  2. Compute node embeddings via BERT/RoBERTa.
  3. Feed embeddings into RGCN with relation-specific weight matrices.
  4. Stack 2 RGCN layers, apply ReLU.
  5. Output logits to Softmax for sentiment labels.
- Design tradeoffs:
  - More RGCN layers vs. oversmoothing risk.
  - Sparse vs. dense adjacency matrices for scalability.
  - LM choice (BERT vs. RoBERTa) vs. embedding quality and compute cost.
- Failure signatures:
  - Training loss stalls early: check graph connectivity and embedding quality.
  - Validation accuracy diverges from training: possible overfitting, reduce RGCN layers or add dropout.
  - Extremely low accuracy: verify edge weights and graph construction logic.
- First 3 experiments:
  1. Replace RGCN with plain GCN and compare validation accuracy.
  2. Swap BERT embeddings for random init and measure impact on convergence.
  3. Toggle transductive vs. inductive mode (mask edges for new docs) to test robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RGCNs on text classification tasks compare when using different types of relationships (co-occurrence, similarity, frequency) or when combining them in different ways?
- Basis in paper: [explicit] The paper constructs a heterogeneous graph with three types of relationships (co-occurrence, similarity, frequency) and uses different weighting metrics for each (PMI, Jaccard, TF-IDF).
- Why unresolved: The paper does not provide a detailed analysis of the individual or combined effects of these different relationship types on the performance of RGCNs.
- What evidence would resolve it: A systematic comparison of RGCN performance using only one type of relationship, or different combinations of the three, would clarify their relative contributions.

### Open Question 2
- Question: How does the performance of RGCNs on text classification tasks change when using different pre-trained language models (e.g., BERT, RoBERTa, ParsBERT) or when fine-tuning these models?
- Basis in paper: [explicit] The paper uses BERT, RoBERTa, and ParsBERT as pre-trained language models for feature extraction.
- Why unresolved: The paper does not explore the impact of different pre-trained language models or fine-tuning on the performance of RGCNs.
- What evidence would resolve it: A comparison of RGCN performance using different pre-trained language models, with and without fine-tuning, would elucidate the optimal choice for various text classification tasks.

### Open Question 3
- Question: How does the performance of RGCNs on text classification tasks change when using different graph construction methods or when incorporating additional information into the graph?
- Basis in paper: [inferred] The paper constructs a heterogeneous graph using co-occurrence, similarity, and frequency relationships. The paper also mentions the possibility of using dynamic edges or incorporating additional information into the graph.
- Why unresolved: The paper does not explore the impact of different graph construction methods or the incorporation of additional information on the performance of RGCNs.
- What evidence would resolve it: A comparison of RGCN performance using different graph construction methods, or incorporating additional information such as sentiment lexicons or domain-specific knowledge, would reveal the optimal approach for various text classification tasks.

## Limitations

- The empirical comparison with pure GCNs is not directly supported by corpus evidence, making performance advantage uncertain
- The transductive training approach may not generalize to streaming or evolving text corpora with new documents
- Results are limited to two review datasets (Amazon and Digikala), potentially not capturing full diversity of sentiment analysis tasks

## Confidence

**High Confidence**: The core methodology of combining pre-trained language models with RGCNs for sentiment analysis is well-established and technically sound. The graph construction approach using PMI, TF-IDF, and Jaccard metrics for different edge types is clearly specified and implementable.

**Medium Confidence**: The reported performance improvements over baselines are plausible given the architecture, but the lack of direct RGCN vs GCN comparison in the literature and absence of ablation studies creates uncertainty about the true source of gains. The transductive training benefits are theoretically sound but not empirically validated against inductive alternatives.

**Low Confidence**: The generalizability of results to other sentiment analysis domains and languages remains unproven. The scalability claims for large corpora are not empirically tested, and the impact of different edge weighting schemes is not systematically explored.

## Next Checks

1. **RGCN vs GCN Direct Comparison**: Implement and evaluate both RGCN and plain GCN architectures on the same datasets, using identical pre-trained embeddings and hyperparameters, to isolate the impact of relation-specific weight matrices on sentiment classification performance.

2. **Inductive vs Transductive Evaluation**: Create a train/validation/test split where test documents are completely unseen during training, and compare performance against the transductive approach. This will reveal whether the method can handle new documents or is fundamentally limited to static corpora.

3. **Ablation of Language Model Components**: Train RGCN models with three different initialization strategies: (a) pre-trained BERT/RoBERTa embeddings, (b) randomly initialized embeddings, and (c) embeddings from a smaller, domain-specific pre-trained model. This will quantify the contribution of large pre-trained models versus the graph structure itself.