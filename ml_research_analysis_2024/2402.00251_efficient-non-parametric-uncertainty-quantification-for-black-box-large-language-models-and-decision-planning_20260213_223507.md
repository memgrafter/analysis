---
ver: rpa2
title: Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language
  Models and Decision Planning
arxiv_id: '2402.00251'
source_url: https://arxiv.org/abs/2402.00251
tags:
- actions
- arxiv
- user
- language
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses uncertainty quantification for black-box large
  language models (LLMs) in decision planning, aiming to mitigate the hallucination
  problem. The proposed method efficiently estimates point-wise dependencies between
  user prompts and decisions using a single inference, without access to token logits.
---

# Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning

## Quick Facts
- arXiv ID: 2402.00251
- Source URL: https://arxiv.org/abs/2402.00251
- Authors: Yao-Hung Hubert Tsai; Walter Talbott; Jian Zhang
- Reference count: 14
- One-line primary result: Point-wise dependency estimation enables uncertainty quantification for black-box LLMs without token access, with step-by-step planning achieving F1 score of 0.164

## Executive Summary
This paper addresses uncertainty quantification for black-box large language models (LLMs) in decision planning, aiming to mitigate hallucination problems. The proposed method efficiently estimates point-wise dependencies between user prompts and decisions using a single inference without access to token logits. This enables statistical interpretation of decision trustworthiness through conformal prediction thresholds. The approach is applied to a decision-making agent that generates actions based on user requests and historical actions, seeking user input when multiple high-dependency actions are identified.

## Method Summary
The method uses point-wise dependency neural estimation to quantify uncertainty in LLM-generated decisions. A neural network directly models the ratio p(a,x)/(p(a)p(x)) between actions and user prompts using separate GRUs and fully connected layers, computing dependency scores through inner products. Conformal prediction establishes statistical thresholds for these scores, filtering actions below the threshold. The decision-making agent operates in a step-by-step planning mode, using the estimated dependencies to either execute a single high-confidence action or request user selection among multiple candidates.

## Key Results
- Step-by-step planning outperforms all-at-once generation with F1 score of 0.164
- Mean precision, recall, and F1 scores of 0.134, 0.212, and 0.164 respectively
- Increasing the threshold for estimated point-wise dependency enhances mean precision
- Method works with black-box LLMs without requiring access to token logits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Efficiently estimates point-wise dependency between user prompts and decisions using single inference without token logits
- Mechanism: Uses relative predictive coding to estimate p(a,x)/(p(a)p(x)) ratio with neural network processing inputs through GRUs and fully connected layers
- Core assumption: Point-wise dependency provides meaningful correlation measure where >1 indicates positive correlation
- Evidence anchors: [abstract] "efficiently estimating point-wise dependencies... without access to token logits"; [section] "relative predictive coding method... to estimate point-wise dependency r(a,x) = p(a,x)/(p(a)p(x))"
- Break condition: Neural network fails to learn correct density ratio, making uncertainty quantification unreliable

### Mechanism 2
- Claim: Step-by-step planning outperforms all-at-once generation in F1 score
- Mechanism: Sequential action generation with dependency filtering, leveraging context from previously selected actions
- Core assumption: Contextually constrained actions are more appropriate than independently generated ones
- Evidence anchors: [abstract] "step-by-step planning outperforms all-at-once generation"; [section] "step-by-step decision planning outperforms all-at-once decision planning in terms of F1 score"
- Break condition: Previous action context doesn't meaningfully constrain next action space

### Mechanism 3
- Claim: Higher dependency thresholds enhance mean precision in action selection
- Mechanism: Conformal prediction establishes statistical thresholds, discarding low-dependency actions to reduce false positives
- Core assumption: Higher dependency scores indicate more likely true/correct actions
- Evidence anchors: [abstract] "increasing threshold... enhances mean precision"; [section] "increasing threshold leads to improved F1 score... mean precision increases while mean recall decreases"
- Break condition: Biased dependency estimation or poor threshold selection eliminates true actions

## Foundational Learning

- Concept: Point-wise dependency estimation
  - Why needed here: Core statistical measure quantifying relationship between prompts and actions without token logits
  - Quick check question: If r(a,x) = 0.8, what does this tell us about the relationship between action a and user prompt x?

- Concept: Conformal prediction
  - Why needed here: Statistically grounded method for setting dependency score thresholds
  - Quick check question: What is the relationship between the non-conformity score and estimated point-wise dependency?

- Concept: Gated Recurrent Units (GRUs)
  - Why needed here: Processes sequential LLM outputs to capture contextual information
  - Quick check question: Why might GRUs be preferred over simple feedforward networks for sequential processing?

## Architecture Onboarding

- Component map: Mistral-7B-Instruct-v0.1 -> Point-wise dependency neural estimator -> Conformal prediction wrapper -> Decision-making agent

- Critical path:
  1. User provides prompt
  2. LLM generates all possible actions in single inference
  3. Dependency estimator computes scores for all action-prompt pairs
  4. Conformal prediction applies threshold to filter actions
  5. Agent either executes single action or requests user selection
  6. Selected action appended to prompt history for next iteration

- Design tradeoffs:
  - Single inference vs multiple generations: Trades potential richness for computational efficiency
  - Exact match vs semantic similarity: Strict criteria may understate practical utility
  - Threshold selection: Higher thresholds improve precision but reduce recall and increase user interactions

- Failure signatures:
  - Consistently low dependency scores indicate inappropriate action generation
  - High variance in scores suggests estimator instability
  - User repeatedly selecting below-threshold actions indicates threshold too high

- First 3 experiments:
  1. Compare F1 scores for step-by-step vs all-at-once planning on held-out test data
  2. Sweep different dependency thresholds (0.0, 1.0, 1.627) and measure precision-recall tradeoff
  3. Test method with different LLM backend to verify black-box compatibility

## Open Questions the Paper Calls Out
- Question: How does the point-wise dependency method perform compared to existing black-box uncertainty quantification methods with proprietary LLMs like GPT-4?
- Question: How does the agent's performance change with different pre-trained LLMs (Llama 2, GPT-4) or varying model sizes?
- Question: How well does the uncertainty quantification generalize to other decision-making domains beyond smart home scenarios?

## Limitations
- Point-wise dependency estimator accuracy is critical and untested for failure modes
- Synthetic smart home dataset may not capture real-world complexity
- Exact match evaluation criteria may underestimate practical utility
- No comparison with other established black-box uncertainty quantification methods

## Confidence

*High Confidence Claims:*
- Point-wise dependency estimation method can be implemented with relative predictive coding, GRUs, and fully connected layers
- Conformal prediction can provide statistical thresholds for action selection
- Synthetic dataset and evaluation methodology are clearly specified

*Medium Confidence Claims:*
- Single-inference dependency estimation effectiveness for uncertainty quantification
- Step-by-step planning superiority over all-at-once generation (based on synthetic data)
- Precision-recall tradeoff behavior with different dependency thresholds

## Next Checks

1. **Estimator Robustness Test**: Validate point-wise dependency estimator on held-out test set with varying prompt-action correlation levels to assess distinguishing ability

2. **Real-World Dataset Evaluation**: Apply method to real-world decision planning dataset (task-oriented dialogue or robot planning) to verify synthetic data generalization

3. **Semantic Evaluation**: Implement semantic similarity-based evaluation metric alongside exact match to assess identification of functionally equivalent actions