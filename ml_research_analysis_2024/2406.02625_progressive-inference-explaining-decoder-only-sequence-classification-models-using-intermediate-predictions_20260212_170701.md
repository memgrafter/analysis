---
ver: rpa2
title: 'Progressive Inference: Explaining Decoder-Only Sequence Classification Models
  Using Intermediate Predictions'
arxiv_id: '2406.02625'
source_url: https://arxiv.org/abs/2406.02625
tags:
- attributions
- input
- predictions
- intermediate
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Progressive Inference, a framework for explaining
  the predictions of decoder-only sequence classification models. The key insight
  is that intermediate predictions from these models can be interpreted as predictions
  on masked versions of the input, leveraging the causal attention mechanism.
---

# Progressive Inference: Explaining Decoder-Only Sequence Classification Models Using Intermediate Predictions

## Quick Facts
- arXiv ID: 2406.02625
- Source URL: https://arxiv.org/abs/2406.02625
- Reference count: 40
- Primary result: Progressive Inference framework provides significantly better attributions for decoder-only models compared to prior XAI techniques, with up to 10.3% improvement in AUC for activation study and 57.5% reduction for inverse activation study.

## Executive Summary
This paper introduces Progressive Inference, a framework for explaining decoder-only sequence classification models by leveraging their intermediate predictions. The key insight is that due to the causal attention mechanism, intermediate predictions at each position can be interpreted as predictions on masked versions of the input. Two methods are proposed: Single-Pass Progressive Inference (SP-PI) computes attributions by taking differences between consecutive intermediate predictions with minimal computational overhead, while Multi-Pass Progressive Inference (MP-PI) performs multiple inference passes with randomly masked inputs to compute SHAP-like attributions more efficiently than Kernel SHAP.

## Method Summary
The Progressive Inference framework computes input attributions for decoder-only sequence classification models by interpreting intermediate predictions as predictions on masked inputs. SP-PI calculates attributions by taking differences between consecutive intermediate predictions, requiring only a single forward pass. MP-PI uses multiple inference passes with randomly masked inputs and connects to Kernel SHAP to compute SHAP-like attributions by finding an optimal mask sampling distribution that approximates the Shapley distribution. The framework is evaluated on diverse text classification tasks using fine-tuned GPT-2 and Llama-2 models.

## Key Results
- SP-PI and MP-PI provide significantly better attributions compared to prior XAI techniques on activation and inverse activation studies
- MP-PI achieves up to 10.3% improvement in AUC for activation study compared to baseline methods
- MP-PI achieves 57.5% reduction in AUC for inverse activation study, demonstrating superior attribution quality
- SP-PI requires minimal computational overhead while MP-PI provides higher quality SHAP-like attributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoder-only models' intermediate predictions can be interpreted as predictions on masked versions of the input due to the causal attention mechanism.
- Mechanism: When a decoder-only model processes input tokens sequentially, each intermediate prediction at position i only depends on tokens up to that position. This allows treating the intermediate prediction as if it were the model's output on a masked input where only tokens up to position i are active.
- Core assumption: The causal attention mechanism ensures that each intermediate prediction only attends to preceding tokens, making the interpretation valid.
- Evidence anchors:
  - [abstract]: "Due to the causal attention mechanism, these intermediate predictions only depend on the tokens seen before the inference point, allowing us to obtain the model's prediction on a masked input sub-sequence, with negligible computational overheads."
  - [section]: "Due to the causal attention mechanism, we can intuitively view ⃗ p1, ⃗ p2, ..., ⃗ pn as representing the predictions of the model on the masked inputs [t1, m, ..., m], [t1, t2, ..., m], ...,[t1, t2, ..., tn] respectively."
- Break condition: If the model uses cross-attention or other mechanisms that allow future tokens to influence current predictions, this interpretation breaks down.

### Mechanism 2
- Claim: Single-Pass Progressive Inference (SP-PI) computes attributions by taking differences between consecutive intermediate predictions, requiring minimal computational overhead.
- Mechanism: SP-PI calculates the attribution for each feature by subtracting the model's prediction before seeing that feature from the prediction after seeing it. This difference represents the marginal contribution of that feature to the final prediction.
- Core assumption: The difference between consecutive predictions accurately captures the feature's contribution to the model's decision.
- Evidence anchors:
  - [abstract]: "Single-Pass Progressive Inference (SP-PI) computes attributions by taking differences between consecutive intermediate predictions, requiring minimal computational overhead."
  - [section]: "SP-PI computes attribution for the ith feature by taking the difference between successive intermediate predictions as follows: ϕi = pc i − pc i−1"
- Break condition: If the model's predictions change non-linearly or if intermediate predictions are noisy, the simple difference may not accurately represent feature importance.

### Mechanism 3
- Claim: Multi-Pass Progressive Inference (MP-PI) uses multiple inference passes with randomly masked inputs to compute higher quality attributions that approximate SHAP values.
- Mechanism: MP-PI samples various masked versions of the input, runs inference on each, and collects intermediate predictions. It then uses these predictions to solve a weighted regression problem that approximates SHAP values by finding a mask sampling distribution that matches the Shapley distribution.
- Core assumption: By sampling masks according to the optimized distribution, the intermediate predictions can approximate the model's behavior on all possible feature coalitions.
- Evidence anchors:
  - [abstract]: "Multi Pass-Progressive Inference (MP-PI) performs multiple inference passes with randomly masked inputs and connects to Kernel SHAP to compute SHAP-like attributions."
  - [section]: "MP-PI uses intermediate predictions from multiple masked versions of the input to compute higher quality attributions... MP-PI uses a connection with Kernel SHAP to compute SHAP-like attributions more efficiently compared to Kernel SHAP."
- Break condition: If the model's predictions on masked inputs don't accurately reflect the contribution of features, or if the sampling distribution can't be optimized effectively, the SHAP-like attributions may be inaccurate.

## Foundational Learning

- Concept: Causal attention mechanism in decoder-only Transformers
  - Why needed here: This is the fundamental property that enables interpreting intermediate predictions as predictions on masked inputs.
  - Quick check question: What ensures that the prediction at position i only depends on tokens at or before position i?

- Concept: SHAP values and game-theoretic attribution
  - Why needed here: MP-PI connects to Kernel SHAP, and understanding SHAP values is crucial for grasping how MP-PI computes attributions.
  - Quick check question: What are the key axiomatic properties that SHAP values satisfy?

- Concept: Feature attribution methods and perturbation-based approaches
  - Why needed here: Progressive Inference is a perturbation-based method, and understanding the landscape of attribution methods helps contextualize its novelty.
  - Quick check question: How do perturbation-based methods differ from gradient-based methods in computing feature attributions?

## Architecture Onboarding

- Component map: Input processing layer -> Causal attention mechanism -> Intermediate prediction collection (SP-PI) -> Multiple mask sampling and inference (MP-PI) -> Attribution computation

- Critical path: 1. Input tokenization and model initialization 2. Single forward pass to collect intermediate predictions (SP-PI) 3. For MP-PI: Mask sampling → Masked input creation → Forward pass → Intermediate prediction collection (repeated B times) 4. Attribution computation from intermediate predictions

- Design tradeoffs:
  - SP-PI: Minimal computational overhead but may provide less accurate attributions due to considering only one coalition per feature
  - MP-PI: Higher computational cost but provides SHAP-like attributions by considering multiple coalitions per feature
  - Choice of mask sampling distribution significantly impacts MP-PI's performance

- Failure signatures:
  - SP-PI: May miss important features that only contribute in specific coalitions
  - MP-PI: Poor mask sampling distribution leads to suboptimal approximations of SHAP values
  - Both: If the model's predictions on masked inputs don't accurately reflect feature contributions

- First 3 experiments:
  1. Implement SP-PI on a simple GPT-2 model and verify that intermediate predictions match masked input predictions on toy examples
  2. Implement MP-PI with uniform mask sampling and compare attributions to Kernel SHAP on small datasets
  3. Optimize the mask sampling distribution for MP-PI and evaluate the impact on attribution quality compared to uniform sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the approximation error in Eqn. 4 vary with prediction position, length of the input, and the model being used?
- Basis in paper: [explicit] "The approximation error in Eqn. 4 can vary with prediction position, length of the input and the model being used."
- Why unresolved: The paper acknowledges this variation but does not provide quantitative analysis or empirical measurements of the approximation error under different conditions.
- What evidence would resolve it: Systematic experiments measuring the approximation error across different model architectures, input lengths, and prediction positions would provide concrete data on the error's behavior and magnitude.

### Open Question 2
- Question: How sensitive are the MP-PI attributions to the optimized probability distribution P' found through Eqn. 16?
- Basis in paper: [explicit] "A key component of our proposed MP-PI method is finding an optimal P' that results in the distribution of intermediate samples resembling the Shapley distribution."
- Why unresolved: While the paper mentions optimizing P' to minimize distance from P*, it doesn't quantify how sensitive the resulting attributions are to variations in P' or how close the actual distribution of intermediate samples gets to the Shapley distribution.
- What evidence would resolve it: Empirical studies varying P' slightly from the optimal solution and measuring the resulting changes in attribution quality would reveal the sensitivity. Additionally, measuring the actual distribution of intermediate samples against P* would show how well the optimization performs.

### Open Question 3
- Question: What is the impact of using different mask tokens (m) on the quality of attributions computed by SP-PI and MP-PI?
- Basis in paper: [inferred] The paper uses '...' as the mask token but doesn't discuss how different mask tokens might affect the model's predictions on masked inputs or the resulting attributions.
- Why unresolved: The choice of mask token could influence how the model handles masked positions, potentially affecting the validity of interpreting intermediate predictions as predictions on masked inputs. This impact is not explored in the paper.
- What evidence would resolve it: Experiments using different mask tokens (e.g., random tokens, special mask tokens) and comparing the resulting attribution quality would reveal how sensitive the methods are to this choice and whether certain tokens lead to more reliable attributions.

## Limitations

- Limited generalizability to encoder-decoder or encoder-only models: The framework's core assumption relies on the causal attention mechanism in decoder-only models, which may not translate to other architectures.
- Mask sampling distribution optimization: While the paper mentions optimizing the mask sampling distribution P' to approximate Shapley values, the specific optimization procedure and its effectiveness across different datasets and model sizes is not thoroughly explored.
- Computational overhead in practice: Although MP-PI is described as "efficient" compared to Kernel SHAP, the actual computational overhead for larger models or datasets is not clearly quantified.

## Confidence

**High confidence**: The core mechanism of Progressive Inference (interpreting intermediate predictions as masked input predictions due to causal attention) and the SP-PI method are well-founded and clearly explained. The experimental results showing improvements over prior XAI techniques are robust within the tested domains.

**Medium confidence**: The MP-PI method's connection to Kernel SHAP and its ability to approximate SHAP values efficiently is conceptually sound but may require careful tuning of the mask sampling distribution. The paper provides theoretical grounding but limited practical guidance on optimization.

**Low confidence**: Claims about the framework's applicability to "diverse text classification tasks" are supported by experiments on seven datasets, but the paper doesn't address potential limitations when scaling to more complex tasks or different domains (e.g., multi-modal inputs, longer sequences).

## Next Checks

1. **Mask sampling distribution sensitivity analysis**: Systematically evaluate how different mask sampling distributions P' affect MP-PI's attribution quality and computational efficiency across multiple datasets and model sizes.

2. **Cross-architecture validation**: Test Progressive Inference on encoder-decoder and encoder-only models to assess the framework's generalizability beyond decoder-only architectures.

3. **Scaling study**: Evaluate the computational overhead and attribution quality of both SP-PI and MP-PI methods on larger models (e.g., GPT-3, LLaMA-2 70B) and longer sequences to quantify practical limitations.