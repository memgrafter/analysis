---
ver: rpa2
title: 'Reducing Bias in Deep Learning Optimization: The RSGDM Approach'
arxiv_id: '2409.15314'
source_url: https://arxiv.org/abs/2409.15314
tags:
- algorithm
- learning
- rsgdm
- sgdm
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses bias and lag issues in exponential moving\
  \ average (EMA)-based gradient estimation in deep learning optimizers, particularly\
  \ in SGDM. The proposed RSGDM algorithm introduces differential correction by estimating\
  \ gradient changes (\u0394gt) via EMA and incorporating this term into the momentum\
  \ update, thereby reducing bias and lag."
---

# Reducing Bias in Deep Learning Optimization: The RSGDM Approach

## Quick Facts
- arXiv ID: 2409.15314
- Source URL: https://arxiv.org/abs/2409.15314
- Reference count: 27
- Primary result: RSGDM achieves 0.14% higher test accuracy than SGDM on CIFAR-10 and 0.57% higher on CIFAR-100

## Executive Summary
This paper addresses bias and lag issues in exponential moving average (EMA)-based gradient estimation in deep learning optimizers, particularly in SGDM. The proposed RSGDM algorithm introduces differential correction by estimating gradient changes (Δgt) via EMA and incorporating this term into the momentum update, thereby reducing bias and lag. Experiments on CIFAR-10 and CIFAR-100 with ResNet18 and ResNet50 models show that RSGDM achieves 0.14% higher test accuracy than SGDM on CIFAR-10 and 0.57% higher on CIFAR-100, with consistent accuracy improvements in later training epochs. RSGDM maintains the same hyperparameter count as SGDM, making it easy to implement without increasing tuning complexity.

## Method Summary
RSGDM is a modification of the SGDM optimizer that addresses bias and lag in EMA-based gradient estimation. The algorithm maintains three buffers: mt (standard momentum term), zt (differential buffer tracking gradient changes), and nt (combined estimate). The momentum buffer mt is updated using standard EMA of gradients. The differential buffer zt tracks EMA of gradient differences Δgt = gt - gt-1. The final estimate nt combines both terms as nt = mt + β * zt. The algorithm uses the same β hyperparameter for both EMA operations, requiring no additional hyperparameter tuning. The combined estimate nt is then used for parameter updates in the standard SGD manner.

## Key Results
- RSGDM achieves 0.14% higher test accuracy than SGDM on CIFAR-10 with ResNet18
- RSGDM achieves 0.57% higher test accuracy than SGDM on CIFAR-100 with ResNet50
- Accuracy improvements are consistent across later training epochs, demonstrating reduced lag
- RSGDM maintains the same hyperparameter count as SGDM, requiring no additional tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RSGDM reduces bias in gradient estimation by incorporating differential correction through Δgt.
- Mechanism: RSGDM maintains the standard momentum term (mt) and adds a differential term (zt) that tracks the change in gradients between consecutive iterations. The final estimate (nt) combines both terms, where the differential correction term compensates for the bias introduced by the exponential moving average in mt.
- Core assumption: The gradient difference Δgt captures meaningful information about the true gradient trend that can be used to correct the biased EMA estimate.
- Evidence anchors:
  - [abstract] "RSGDM algorithm introduces differential correction by estimating gradient changes (Δgt) via EMA and incorporating this term into the momentum update, thereby reducing bias and lag."
  - [section] "The RSGDM algorithm performs exponential moving averages on both gt and Δgt, and corrects the former using the moving average of the latter."
- Break condition: If gradient changes are uncorrelated with the direction needed to correct the bias, or if Δgt becomes noisy and dominates the correction term.

### Mechanism 2
- Claim: RSGDM mitigates lag by reducing the influence of historical gradients through differential correction.
- Mechanism: The differential term zt captures recent gradient changes, allowing nt to respond more quickly to shifts in gradient direction. By weighting the correction term with β, RSGDM reduces the weight of outdated gradient information while preserving stability.
- Core assumption: Recent gradient changes are more informative about current gradient trends than older gradients, and this information can be captured through Δgt.
- Evidence anchors:
  - [section] "To address this situation, we propose the RSGDM algorithm, which uses the differential (change) of the gradient to correct the estimated value mt. Intuitively, it can be understood in this way: if the gradient is increasing and the differential estimate is also greater than 0, then this correction term plays a role in accelerating convergence."
  - [section] "It can be seen that the bias term ξ of the RSGDM algorithm lacks the influence of the historical gradient g1, and because β is less than 1, |ξ̄| ≤ |ξ|. In summary, we can conclude that the RSGDM algorithm has a smaller bias compared to the SGDM algorithm and is less influenced by historical gradients (mitigating lag)."
- Break condition: If the gradient changes are highly volatile, causing the differential correction to introduce more noise than it removes.

### Mechanism 3
- Claim: RSGDM maintains the same hyperparameter count as SGDM, making it easy to implement without increasing tuning complexity.
- Mechanism: RSGDM reuses the existing β hyperparameter from SGDM for both the momentum term and the differential correction term, requiring no additional hyperparameter tuning.
- Core assumption: A single β value can effectively balance both the momentum smoothing and the differential correction smoothing.
- Evidence anchors:
  - [abstract] "RSGDM maintains the same hyperparameter count as SGDM, making it easy to implement without increasing tuning complexity."
  - [section] "The RSGDM algorithm does not introduce additional hyperparameters compared to the SGDM algorithm, which does not increase the burden of tuning parameters when training our model."
- Break condition: If a single β value cannot optimally balance both smoothing operations, requiring separate hyperparameters for momentum and differential correction.

## Foundational Learning

- Concept: Exponential Moving Average (EMA)
  - Why needed here: Understanding EMA is crucial because both SGDM and RSGDM use it to estimate gradients, and RSGDM's improvement relies on understanding how EMA introduces bias and lag.
  - Quick check question: What is the formula for EMA and how does the decay factor β affect the weighting of past values?

- Concept: Momentum in Optimization
  - Why needed here: Momentum is the foundation of SGDM, and RSGDM builds upon this concept by adding differential correction while maintaining the momentum mechanism.
  - Quick check question: How does momentum help with convergence in stochastic optimization, and what are its limitations?

- Concept: Gradient Descent Variants
  - Why needed here: RSGDM is a variant of SGDM, so understanding the landscape of gradient descent optimization algorithms helps contextualize where RSGDM fits and its advantages.
  - Quick check question: What are the key differences between SGD, SGDM, and adaptive methods like Adam?

## Architecture Onboarding

- Component map: Gradients -> Momentum buffer (mt) -> Differential buffer (zt) -> Combined update (nt) -> Parameter update
- Critical path:
  1. Compute gradients from mini-batch
  2. Update momentum buffer: mt = β * mt-1 + (1-β) * gt
  3. Compute gradient difference: Δgt = gt - gt-1
  4. Update differential buffer: zt = β * zt-1 + (1-β) * Δgt
  5. Combine estimates: nt = mt + β * zt
  6. Update parameters: θt = θt-1 - α * nt
- Design tradeoffs:
  - Pros: Reduces bias and lag, maintains hyperparameter count, easy to implement
  - Cons: Additional memory for differential buffer, potential noise amplification from gradient differences
  - Alternative considered: Using separate β values for momentum and differential terms, but rejected to maintain simplicity
- Failure signatures:
  - High variance in updates: Differential term may be amplifying noise
  - Convergence stalls: Differential correction may be too conservative or too aggressive
  - Worse performance than SGDM: Implementation bug or inappropriate β value
- First 3 experiments:
  1. CIFAR-10 with ResNet18: Compare training curves and final accuracy against SGDM baseline
  2. CIFAR-100 with ResNet50: Test generalization on more classes with deeper network
  3. Ablation study: Remove differential term (run pure SGDM) to quantify improvement from correction mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RSGDM scale with different datasets beyond CIFAR-10 and CIFAR-100, particularly in more complex and diverse image classification tasks?
- Basis in paper: [explicit] The paper mentions that future work will focus on extending the applicability of RSGDM to a broader range of datasets and problem domains.
- Why unresolved: The experiments conducted are limited to CIFAR-10 and CIFAR-100 datasets. There is no evidence of RSGDM's effectiveness on more complex datasets or different domains.
- What evidence would resolve it: Conducting experiments on a variety of datasets such as ImageNet, medical imaging datasets, or other complex image classification tasks to compare RSGDM's performance with SGDM and other optimizers.

### Open Question 2
- Question: What are the theoretical guarantees of convergence for RSGDM compared to SGDM and other optimizers like Adam?
- Basis in paper: [inferred] The paper discusses the bias and lag issues in SGDM and claims RSGDM addresses these issues, but does not provide a formal proof of convergence or compare it theoretically with other optimizers.
- Why unresolved: The paper lacks a rigorous theoretical analysis comparing the convergence properties of RSGDM with other optimizers.
- What evidence would resolve it: Providing a mathematical proof or theoretical analysis demonstrating the convergence guarantees of RSGDM and comparing them with those of SGDM, Adam, and other optimizers.

### Open Question 3
- Question: How does RSGDM perform in non-image classification tasks such as natural language processing, time series analysis, or reinforcement learning?
- Basis in paper: [explicit] The paper suggests that the principles underlying RSGDM may inspire novel optimization algorithms for other complex machine learning tasks beyond image recognition.
- Why unresolved: The experiments are solely focused on image classification tasks. There is no evidence of RSGDM's effectiveness in other domains.
- What evidence would resolve it: Implementing and evaluating RSGDM in tasks such as natural language processing, time series prediction, or reinforcement learning to assess its performance and generalizability.

### Open Question 4
- Question: How sensitive is RSGDM to the choice of hyperparameters compared to SGDM, and what is the impact of hyperparameter tuning on its performance?
- Basis in paper: [explicit] The paper states that RSGDM does not introduce additional hyperparameters compared to SGDM, but does not discuss the sensitivity to hyperparameter choices or the impact of tuning.
- Why unresolved: The paper does not provide an analysis of hyperparameter sensitivity or the effects of tuning on RSGDM's performance.
- What evidence would resolve it: Conducting hyperparameter sensitivity analysis and ablation studies to determine how different hyperparameter settings affect RSGDM's performance compared to SGDM.

## Limitations

- Limited experimental validation - only tested on two datasets with two model architectures
- No ablation studies to isolate the contribution of the differential correction versus other factors
- Exact implementation details of the differential correction term are not fully specified in the paper

## Confidence

- Mechanism 1 (bias reduction through differential correction): Medium
- Mechanism 2 (lag mitigation through reduced historical influence): Medium
- Mechanism 3 (same hyperparameter count): High

## Next Checks

1. Implement the exact RSGDM algorithm based on the paper's description and verify the differential correction term calculation matches the intended design.
2. Conduct ablation studies comparing RSGDM with variants: (a) without differential correction (pure SGDM), (b) with separate β values for momentum and differential terms.
3. Test RSGDM across a broader range of tasks including NLP benchmarks and different network architectures to assess generalizability beyond CIFAR image classification.