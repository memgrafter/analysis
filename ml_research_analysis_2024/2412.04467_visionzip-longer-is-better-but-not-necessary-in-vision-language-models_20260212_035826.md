---
ver: rpa2
title: 'VisionZip: Longer is Better but Not Necessary in Vision Language Models'
arxiv_id: '2412.04467'
source_url: https://arxiv.org/abs/2412.04467
tags:
- tokens
- visionzip
- visual
- token
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VisionZip addresses visual token redundancy in vision-language
  models by selecting a subset of highly informative tokens before feeding them to
  the language model. It uses attention-based dominant token selection and contextual
  token merging to preserve most visual information while drastically reducing token
  count.
---

# VisionZip: Longer is Better but Not Necessary in Vision Language Models

## Quick Facts
- arXiv ID: 2412.04467
- Source URL: https://arxiv.org/abs/2412.04467
- Reference count: 40
- Primary result: Achieves 95% of baseline performance using only 10% of visual tokens

## Executive Summary
VisionZip addresses visual token redundancy in vision-language models by selecting a subset of highly informative tokens before feeding them to the language model. It uses attention-based dominant token selection and contextual token merging to preserve most visual information while drastically reducing token count. The method achieves up to 95% of baseline performance using only 10% of visual tokens, reduces prefilling time by 8x, and enables a 13B model to run faster than a 7B model while outperforming it. VisionZip is training-free, broadly applicable, and improves efficiency without sacrificing accuracy.

## Method Summary
VisionZip is a training-free method that reduces visual token redundancy in vision-language models by selecting and merging informative tokens before they enter the language model. The approach identifies dominant tokens with high attention scores from the vision encoder and merges similar remaining tokens, preserving most image information while reducing token count. VisionZip can operate in two modes: training-free (no fine-tuning required) or VisionZip‡ (fine-tuning the projector layer to improve alignment). The method is architecture-agnostic and works with various vision encoders including CLIP and SigLIP.

## Key Results
- Achieves up to 95% of baseline performance using only 10% of visual tokens
- Reduces prefilling time by 8x and enables faster inference than smaller models
- Maintains >90% accuracy across multiple benchmarks (TextVQA, GQA, VQAv2) while reducing tokens by up to 90%

## Why This Works (Mechanism)

### Mechanism 1
VisionZip achieves high performance by selecting and merging dominant visual tokens before they enter the language model. The method identifies tokens with high attention scores from the vision encoder and merges similar remaining tokens, preserving most image information while reducing token count. This works because tokens with high attention scores contain most of the important image information, and merging similar tokens preserves semantic content without significant loss.

### Mechanism 2
VisionZip's text-agnostic approach outperforms text-relevant methods because it avoids feature misalignment. By selecting tokens based on vision encoder attention rather than text-visual cross-attention, VisionZip captures tokens that actually contain aggregated image information rather than tokens semantically related to text. This works because vision encoders pre-group information into proxy tokens that may not be text-relevant but contain the actual image information.

### Mechanism 3
The softmax function in attention mechanisms creates an "attention sink" effect that concentrates information in fewer tokens over deeper layers. The gradient of the softmax function (∂softmax(zi)/∂zi = softmax(zi) · (1 - softmax(zi))) amplifies differences between high and low attention scores, causing information to concentrate in dominant tokens. This mathematical property creates positive feedback loops that reinforce existing attention patterns.

## Foundational Learning

- **Concept: Attention mechanisms in transformers**
  - Why needed here: Understanding how VisionZip selects dominant tokens requires knowledge of how attention scores are computed and interpreted in transformer models.
  - Quick check question: What does the attention score matrix represent in a transformer layer, and how is it computed from query and key vectors?

- **Concept: Token merging based on similarity**
  - Why needed here: VisionZip merges similar tokens using dot product similarity on key values, which is crucial for understanding how contextual tokens are created.
  - Quick check question: How does averaging tokens based on similarity preserve semantic information while reducing token count?

- **Concept: Vision-language model architecture**
  - Why needed here: Understanding the overall VLM architecture (vision encoder → projector → LLM) is essential for grasping where VisionZip fits and why it's effective.
  - Quick check question: In a typical VLM, what are the three main components and how do they interact to process visual and textual information?

## Architecture Onboarding

- **Component map:** Vision encoder (CLIP/SigLIP) → Dominant token selection → Contextual token merging → Projector (fine-tuned if using VisionZip‡) → LLM
- **Critical path:** 1) Input image to vision encoder 2) Extract attention scores from -2 layer 3) Select K dominant tokens based on attention 4) Merge remaining tokens by similarity 5) Concatenate dominant and contextual tokens 6) Project to LLM embedding space 7) Input to LLM for processing
- **Design tradeoffs:** Token count vs. performance: Lower token count reduces computation but may reduce accuracy; Training-free vs. fine-tuning: Training-free mode is simpler but fine-tuning the projector can recover some performance loss; Dominant vs. contextual tokens: Balancing between information density and coverage of small details
- **Failure signatures:** Performance degradation when token count is too low (below ~64 tokens); Memory issues if VisionZip is not properly integrated before projector; Incompatibility with models that don't use CLS tokens (requires modified selection algorithm)
- **First 3 experiments:** 1) Apply VisionZip to LLaVA-1.5 with 576→192 tokens and measure accuracy on GQA benchmark 2) Compare VisionZip training-free mode vs. VisionZip‡ fine-tuned mode on TextVQA 3) Test VisionZip on a non-CLS encoder like SigLIP to verify generalization

## Open Questions the Paper Calls Out

### Open Question 1
What are the fundamental architectural or training mechanisms in vision encoders that cause attention to concentrate on a small subset of tokens, and can this phenomenon be mitigated at the encoder level? The paper analyzes attention distribution across layers but doesn't explore alternative architectural designs or training strategies that could prevent this concentration of attention.

### Open Question 2
How does the performance of VisionZip vary across different types of visual tasks, particularly those requiring fine-grained object detection or detailed scene understanding? The paper demonstrates effectiveness on general benchmarks but doesn't specifically address tasks requiring high-resolution detail or precise localization.

### Open Question 3
What is the optimal strategy for determining the number of dominant and contextual tokens to retain for different types of images or tasks? The paper presents fixed token count configurations but doesn't explore adaptive strategies based on image content or task requirements.

### Open Question 4
How does VisionZip perform in multi-modal tasks that require cross-attention between multiple images or between images and other modalities beyond text? The paper mentions applicability to video understanding but doesn't provide detailed analysis of cross-modal attention scenarios.

## Limitations

- Architecture dependency on attention concentration properties of specific vision encoders
- Assumes presence of CLS token, limiting compatibility with some encoder architectures
- Task-dependent optimal token reduction ratio without systematic determination method
- Limited analysis of long-tail performance degradation at extreme compression levels

## Confidence

- **High confidence**: Computational efficiency gains (8x prefilling speedup, faster inference than smaller models) while maintaining >90% of baseline performance
- **Medium confidence**: Softmax gradient explanation for attention concentration and claims about text-relevant methods failing due to feature misalignment
- **Medium confidence**: Dominant token selection outperforming text-relevant selection methods across broader range of visual reasoning tasks

## Next Checks

1. Apply VisionZip to a diverse set of vision encoders beyond CLIP and SigLIP (e.g., DINOv2, OpenCLIP variants) to verify robustness across different attention concentration patterns.

2. Develop and validate a systematic method for determining optimal token counts for new tasks by analyzing attention distribution patterns and performance degradation curves across multiple benchmark datasets.

3. Conduct ablation studies to identify which specific visual features (objects, relationships, spatial layouts) are most affected by token reduction, and whether these losses correlate with performance drops on specific question types in visual reasoning benchmarks.