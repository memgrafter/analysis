---
ver: rpa2
title: Stochastic Modified Flows for Riemannian Stochastic Gradient Descent
arxiv_id: '2402.03467'
source_url: https://arxiv.org/abs/2402.03467
tags:
- stochastic
- riemannian
- such
- where
- rsgd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper establishes rigorous weak error bounds for Riemannian\
  \ stochastic gradient descent (RSGD) compared to two continuous-time limits: the\
  \ deterministic Riemannian gradient flow and a diffusion process termed the Riemannian\
  \ stochastic modified flow (RSMF). The authors prove that RSGD with learning rate\
  \ \u03B7 approximates the gradient flow with error O(\u03B7), and the RSMF (which\
  \ accounts for noise) with error O(\u03B7\xB2), under conditions on the retraction\
  \ map, manifold geometry, and gradient estimator."
---

# Stochastic Modified Flows for Riemannian Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2402.03467
- Source URL: https://arxiv.org/abs/2402.03467
- Reference count: 14
- Key outcome: RSGD approximates gradient flow with O(η) error and RSMF with O(η²) error

## Executive Summary
This paper establishes rigorous weak error bounds for Riemannian stochastic gradient descent (RSGD) by comparing it to two continuous-time limits: the deterministic Riemannian gradient flow and the Riemannian stochastic modified flow (RSMF). The authors prove that RSGD with learning rate η approximates the gradient flow with error O(η), and the RSMF with error O(η²). This provides the first quantified convergence rates for RSGD and demonstrates that the RSMF better captures the stochastic dynamics of RSGD in the small learning rate regime.

## Method Summary
The paper analyzes RSGD as a discrete-time approximation of continuous-time processes on Riemannian manifolds. Using tools from stochastic differential geometry, the authors prove weak error bounds by comparing RSGD trajectories to solutions of the Riemannian gradient flow ODE and the RSMF SDE. The analysis requires bounded curvature, appropriate retraction maps, and regularity conditions on the noise. The RSMF includes both drift and diffusion terms that capture the mean behavior and random fluctuations of RSGD, enabling higher-order approximation than the deterministic gradient flow.

## Key Results
- RSGD with learning rate η approximates the Riemannian gradient flow with error O(η)
- The RSMF captures both mean behavior and random fluctuations of RSGD with error O(η²)
- RSMF provides more accurate continuous-time approximation than deterministic gradient flow in small learning rate regime

## Why This Works (Mechanism)

### Mechanism 1
RSGD approximates the Riemannian gradient flow with error O(η) because each RSGD step using a retraction map is a first-order Euler discretization of the continuous gradient flow ODE. The retraction provides a computationally efficient approximation to the exponential map.

Core assumption: The retraction map is a uniform first-order retraction and the manifold has bounded curvature.

Break condition: If the retraction map deviates significantly from the exponential map or if manifold curvature is unbounded, the O(η) approximation error bound would fail.

### Mechanism 2
The Riemannian stochastic modified flow (RSMF) captures both mean behavior and random fluctuations of RSGD with O(η²) error by including diffusion terms that account for stochastic noise in RSGD.

Core assumption: The manifold has regularity 4 and the noise satisfies certain integrability conditions.

Break condition: If the manifold regularity condition is violated or if the noise is too heavy-tailed, the O(η²) error bound would not hold.

### Mechanism 3
The RSMF provides a more accurate continuous-time approximation of RSGD than the deterministic gradient flow in the small learning rate regime by including both a drift term that corrects for discretization bias and a diffusion term that accounts for stochastic noise.

Core assumption: The noise in RSGD has sufficient regularity and the manifold geometry is well-behaved.

Break condition: If the noise is not sufficiently regular or if the manifold geometry is too complex, the higher-order approximation may break down.

## Foundational Learning

- Concept: Riemannian manifolds and their geometry
  - Why needed here: The paper deals with optimization on manifolds, requiring understanding of tangent spaces, exponential maps, Riemannian gradients, and curvature
  - Quick check question: What is the relationship between the Riemannian gradient and the Euclidean gradient on a submanifold of R^N?

- Concept: Stochastic differential equations (SDEs) on manifolds
  - Why needed here: The RSMF is an SDE on the manifold, and the analysis requires knowledge of how SDEs behave on curved spaces
  - Quick check question: How does the Stratonovich integral differ from the Itô integral when defining SDEs on manifolds?

- Concept: Retraction maps and their properties
  - Why needed here: RSGD uses retraction maps instead of the more expensive exponential maps, and the approximation error depends on the quality of this approximation
  - Quick check question: What are the key properties that a retraction map must satisfy to be a "uniform first-order retraction"?

## Architecture Onboarding

- Component map: RSGD algorithm -> Retraction maps -> Gradient flow ODE -> RSMF SDE -> Error analysis
- Critical path: 1) Verify retraction map properties and manifold regularity, 2) Implement RSGD with chosen retraction, 3) Implement RSMF SDE solver, 4) Perform error analysis to verify O(η) and O(η²) bounds
- Design tradeoffs: Retraction map choice trades computational efficiency against approximation quality; compact vs. non-compact manifolds affect regularity requirements; higher moment conditions on noise enable tighter error bounds
- Failure signatures: Poor retraction map choice leads to RSGD convergence failure and error bound violations; violated manifold regularity causes RSMF non-existence or error bound failures; heavy-tailed noise prevents achievement of weak error bounds
- First 3 experiments: 1) Implement RSGD on a simple manifold (e.g., sphere) with different retraction maps, verify O(η) approximation to gradient flow, 2) Solve RSMF SDE for the same problem, compare with RSGD to verify O(η²) improvement, 3) Test on a non-compact manifold (e.g., hyperbolic space) with noise, verify that regularity conditions hold and error bounds are achieved

## Open Questions the Paper Calls Out

### Open Question 1
Can the order 2 approximation result (Theorem 5.1) be extended to non-compact manifolds like hyperbolic space that do not satisfy the regularity conditions in Definition 4.3? The paper notes that hyperbolic space is popular for embedding hierarchical structures in machine learning, but the required regularity conditions are not satisfied due to exponential volume growth.

### Open Question 2
Can the diffusion approximation results be extended to Riemannian manifolds with unbounded curvature, such as the upper half-plane with the Fisher-Rao metric used in statistical manifolds? The geometry is similar to hyperbolic space considered in Section 6.3, but the authors do not investigate whether regularity conditions are satisfied.

### Open Question 3
Can the stochastic modified flow (RSMF) be used to derive optimal hyperparameter schedules for Riemannian SGD, similar to the Euclidean case? While diffusion approximations have been instrumental in deriving optimal hyperparameter schedules in the Euclidean setting, the authors do not investigate whether this approach can be extended to the Riemannian setting using the RSMF.

## Limitations
- Analysis relies on bounded sectional curvature and bounded geometry assumptions, excluding important applications like large-scale neural networks with unbounded parameter norms
- RSMF formulation involves an infinite-dimensional noise coefficient G that is difficult to interpret or implement in practice
- Weak error bounds apply only to specific test functions and may not capture strong convergence properties needed for some applications

## Confidence
High confidence in O(η) gradient flow approximation error bound (follows directly from first-order nature of retraction map under bounded curvature)
Medium confidence in O(η²) RSMF approximation error (requires more stringent regularity conditions on manifold and noise)
Medium confidence in practical relevance (theoretical framework provides insights but may be challenging to implement directly)

## Next Checks
1. Implement numerical experiments comparing RSGD, gradient flow, and RSMF on simple manifolds (e.g., spheres, hyperbolic spaces) to verify theoretical error bounds hold empirically for various test functions.

2. Analyze computational complexity and stability of solving the RSMF SDE numerically, particularly for the infinite-dimensional noise coefficient G, to assess practical implementability.

3. Extend analysis to adaptive variants of RSGD (e.g., Riemannian Adam) where noise structure is more complex, to determine if similar weak error bounds can be established under modified assumptions.