---
ver: rpa2
title: 'J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution'
arxiv_id: '2403.19259'
source_url: https://arxiv.org/abs/2403.19259
tags:
- reference
- resolution
- dataset
- object
- relations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents J-CRe3, a Japanese dialogue dataset for real-world
  reference resolution. The dataset contains egocentric video and dialogue audio of
  real-world conversations between two people acting as a master and an assistant
  robot at home.
---

# J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution

## Quick Facts
- arXiv ID: 2403.19259
- Source URL: https://arxiv.org/abs/2403.19259
- Reference count: 0
- A Japanese dialogue dataset containing egocentric video and dialogue audio with crossmodal annotations for reference resolution tasks

## Executive Summary
This paper introduces J-CRe3, a novel Japanese dialogue dataset designed for real-world reference resolution tasks. The dataset captures conversations between two people acting as a master and an assistant robot in home environments, providing egocentric video footage and dialogue audio. It includes comprehensive annotations linking spoken phrases to object bounding boxes in video frames, covering both direct and indirect reference types including predicate-argument structures and bridging references. The authors also develop and evaluate an experimental model to establish baseline performance for multimodal reference resolution tasks.

## Method Summary
The J-CRe3 dataset was created by recording 15 dialogue sessions between two participants in home environments, with one acting as a master and the other as an assistant robot. The recordings include egocentric video from wearable cameras and audio of the conversations. Annotations were performed to create crossmodal links between phrases in the utterances and object bounding boxes in the video frames. The annotation process captured various reference types including direct references (pronouns, demonstratives), indirect references (predicate-argument structures), and bridging references (anaphoric relations requiring inference). The dataset includes 8,041 utterances with reference annotations and corresponding video frames with object annotations.

## Key Results
- The experimental model achieved F-scores of approximately 0.8 for textual reference resolution tasks
- Text-to-object reference resolution showed lower performance with recall around 0.4
- The dataset successfully captures both direct and indirect reference relations in real-world conversational contexts
- Crossmodal annotations link 8,041 utterances with object bounding boxes across video frames

## Why This Works (Mechanism)
The dataset's effectiveness stems from capturing real-world conversational dynamics in naturalistic home environments where reference resolution naturally occurs. By using egocentric video from the perspective of the assistant robot, the dataset preserves the visual context that humans use during conversation. The comprehensive annotation scheme captures not just simple pronoun resolution but also more complex linguistic phenomena like bridging references and predicate-argument structures, which are essential for understanding real conversational references.

## Foundational Learning

1. **Crossmodal reference resolution** - The ability to link linguistic expressions to visual objects across different modalities
   - Why needed: Essential for developing AI systems that can understand and respond to natural language in visual environments
   - Quick check: Can the model correctly identify which objects are being referred to in ambiguous situations?

2. **Bridging references** - Anaphoric relations that require inference beyond simple coreference
   - Why needed: Real conversations often use indirect references that cannot be resolved through direct matching
   - Quick check: Does the model understand that "the coffee" might refer to "the coffee I just mentioned making"?

3. **Predicate-argument structures** - The syntactic relationships between verbs and their arguments in reference resolution
   - Why needed: Many references depend on understanding the roles entities play in actions or states
   - Quick check: Can the model distinguish between "put the book on the table" and "the book on the table"?

## Architecture Onboarding

Component map: **Raw video/audio -> Feature extraction -> Reference detection -> Crossmodal linking -> Evaluation**

Critical path: The most computationally intensive step is the crossmodal linking, which requires processing both visual features from video frames and linguistic features from dialogue simultaneously to establish correspondences between spoken references and visual objects.

Design tradeoffs: The dataset uses controlled scenarios with scripted roles rather than fully spontaneous conversations, trading ecological validity for controlled data collection. This enables systematic annotation but may miss some natural conversational phenomena.

Failure signatures: The model's significantly lower recall (0.4) for text-to-object resolution compared to textual reference resolution (F-score 0.8) indicates it struggles with genuine crossmodal understanding, potentially relying on textual patterns rather than visual context.

Three first experiments:
1. Evaluate model performance on distinguishing between visually similar objects mentioned in conversation
2. Test model robustness to changes in lighting, camera angle, and object occlusion in the video frames
3. Assess whether the model can handle references to objects not currently visible but mentioned in prior conversation

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset size is relatively small with only 15 dialogue sessions, limiting statistical power and generalizability
- Conversations are controlled scenarios with participants acting roles rather than spontaneous natural dialogue
- Human annotation introduces subjectivity, particularly for complex reference types like bridging references
- The experimental model shows significant performance gaps between textual and crossmodal reference resolution

## Confidence

| Claim | Confidence |
|-------|------------|
| Dataset construction methodology | Medium |
| Model performance claims | Low |
| Real-world applicability of dataset | Medium |

## Next Checks

1. Conduct inter-annotator agreement studies on a subset of the dataset to quantify annotation reliability, particularly for complex reference types like bridging references and predicate-argument structures

2. Test the experimental model on a held-out test set with conversations involving different participants, topics, and home environments to assess generalization beyond the training data

3. Perform ablation studies to determine which modalities (text, video, audio) contribute most to model performance and identify whether the model relies on spurious correlations rather than genuine reference understanding