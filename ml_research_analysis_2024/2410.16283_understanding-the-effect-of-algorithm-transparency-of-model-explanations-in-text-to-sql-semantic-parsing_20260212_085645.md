---
ver: rpa2
title: Understanding the Effect of Algorithm Transparency of Model Explanations in
  Text-to-SQL Semantic Parsing
arxiv_id: '2410.16283'
source_url: https://arxiv.org/abs/2410.16283
tags:
- trust
- participants
- explanations
- explanation
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how varying levels of algorithm transparency
  in model explanations affect user trust and ability to identify correct vs. incorrect
  predictions in text-to-SQL semantic parsing.
---

# Understanding the Effect of Algorithm Transparency of Model Explanations in Text-to-SQL Semantic Parsing

## Quick Facts
- arXiv ID: 2410.16283
- Source URL: https://arxiv.org/abs/2410.16283
- Reference count: 40
- Primary result: Medium-transparency explanations strike a balance between user comprehension and cognitive load in text-to-SQL semantic parsing tasks

## Executive Summary
This study investigates how varying levels of algorithm transparency in model explanations affect user trust and ability to identify correct versus incorrect predictions in text-to-SQL semantic parsing. The researchers designed three explanation approaches with different transparency levels and conducted a human-subject study with 97 non-technical participants. They found that medium-transparency explanations enabled participants to gradually improve performance over time and showed the least change in trust after the study, while overly complex high-transparency explanations could confuse non-technical users.

## Method Summary
The researchers fine-tuned a T5-base language model on the Spider dataset for text-to-SQL semantic parsing and designed three explanation approaches: low transparency (model prediction only), medium transparency (feature attribution + confidence score), and high transparency (step-by-step SQL execution details). They recruited 97 non-technical participants and conducted a web-based study where participants evaluated AI predictions using these different explanation interfaces, measuring accuracy in recognizing correct/incorrect predictions and trust using Propensity to Trust and Jian Scale questionnaires.

## Key Results
- Medium-transparency explanations strike a good balance, enabling participants to gradually learn the model's decision-making process
- Medium-transparency participants showed the least changes in trust before and after the study
- No significant impact on overall accuracy across transparency levels, but participants were less successful in identifying incorrect predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Medium-transparency explanations strike a balance between user comprehension and cognitive load.
- Mechanism: By exposing a moderate amount of model decision-making detail (feature attribution + confidence score), users can engage meaningfully without being overwhelmed, leading to better performance and trust calibration.
- Core assumption: Non-technical users can process feature attribution explanations without specialized background.
- Evidence anchors:
  - [abstract] "medium-transparency explanations strike a good balance"
  - [section] "medium-transparency explanations were more effective, enabling participants to gradually learn to understand the model's decision-making process"
  - [corpus] Weak - corpus lacks direct evidence about feature attribution effectiveness for non-technical users
- Break condition: If feature attribution explanations require SQL knowledge to interpret, the balance is lost.

### Mechanism 2
- Claim: High-transparency explanations can be counterproductive for non-technical users.
- Mechanism: Excessive information about model decision-making processes (step-by-step SQL execution) overwhelms users without technical backgrounds, leading to confusion and reduced performance.
- Core assumption: Users without SQL programming skills cannot effectively process detailed SQL execution explanations.
- Evidence anchors:
  - [abstract] "overly complex explanations can confuse non-technical users"
  - [section] "high-transparency explanations offer more detailed insights into the AI's decision-making process, they may have been more persuasive... but the huge amount of information... may be cognitively too much for them"
  - [corpus] Weak - corpus lacks specific evidence about SQL complexity effects
- Break condition: If users can access interactive tools to explore SQL execution step-by-step, the information may become more digestible.

### Mechanism 3
- Claim: Explanation transparency level affects trust calibration differently than performance.
- Mechanism: While medium-transparency explanations improve performance through balanced information, they also minimize trust changes because users find the explanations sufficiently reasonable without being overwhelming.
- Core assumption: Trust changes are influenced by the perceived reasonableness and comprehensibility of explanations.
- Evidence anchors:
  - [abstract] "medium-transparency participant group... showed the least changes in trust before and after the study"
  - [section] "the explanations provided might have been sufficiently reasonable, leading to a more balanced level of trust after the study"
  - [corpus] Weak - corpus lacks evidence about trust calibration mechanisms
- Break condition: If trust is primarily determined by model accuracy rather than explanation quality, transparency level effects may be minimal.

## Foundational Learning

- Concept: Algorithm transparency levels
  - Why needed here: The study compares three levels of explanation detail, requiring understanding of what constitutes low, medium, and high transparency
  - Quick check question: What information is included in medium-transparency explanations that's not in low-transparency ones?

- Concept: Feature attribution in NLP models
  - Why needed here: Medium-transparency explanations use feature attribution (LERG-S) to highlight important input features
  - Quick check question: How does LERG-S differ from traditional feature attribution methods like LIME?

- Concept: Trust measurement scales
  - Why needed here: The study uses Propensity to Trust and Jian Scale to measure dispositional and learned trust
  - Quick check question: Why are two different trust measures used instead of one?

## Architecture Onboarding

- Component map: Question → Database view → Model prediction → Explanation generation → User judgment → Feedback
- Critical path: Question → Database view → Model prediction → Explanation generation → User judgment → Feedback
- Design tradeoffs:
  - Information vs. cognitive load: More detailed explanations improve understanding but may overwhelm users
  - Explanation generation speed vs. quality: Step-by-step explanations are more informative but computationally expensive
  - Study realism vs. control: Using actual SQL queries provides realism but requires database setup
- Failure signatures:
  - Users spending very little time on high-transparency explanations (overwhelm)
  - Users spending excessive time on low-transparency explanations (confusion)
  - No correlation between explanation detail and performance improvement
- First 3 experiments:
  1. A/B test showing time spent on explanations across transparency levels
  2. Correlation analysis between feature attribution clarity and correct judgment rates
  3. Trust change measurement before/after interaction with each transparency level

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do counterfactual questions specifically improve users' ability to identify incorrect AI predictions in text-to-SQL semantic parsing tasks?
- Basis in paper: [explicit] The authors propose counterfactual questions as a potential solution in their discussion section, suggesting it could help users cross-check the consistency of AI predictions against modified questions.
- Why unresolved: The paper only mentions this as a theoretical solution without testing it. No empirical data exists on whether counterfactual questions would actually improve performance.
- What evidence would resolve it: A controlled study comparing user performance with and without counterfactual questions as explanations, measuring accuracy in identifying correct vs. incorrect predictions.

### Open Question 2
- Question: What is the optimal balance between explanation complexity and cognitive load for non-technical users in complex AI tasks?
- Basis in paper: [inferred] The study found medium-transparency explanations performed best, but didn't determine if this is the absolute optimum or if further refinement could yield better results.
- Why unresolved: The medium-transparency level was defined arbitrarily and may not represent the true optimum. Different user groups might require different levels of transparency.
- What evidence would resolve it: Systematic testing of multiple explanation complexity levels across different user demographics and task types to identify the optimal balance point.

### Open Question 3
- Question: How can real-time trust calibration mechanisms be implemented effectively in AI systems to prevent both over-trust and under-trust?
- Basis in paper: [explicit] The authors discuss the need for dynamic explanation types that calibrate human-AI trust as a potential solution in their discussion section.
- Why unresolved: The paper identifies this as a research need but doesn't propose specific mechanisms or test any implementation strategies.
- What evidence would resolve it: Development and testing of specific trust calibration algorithms that adjust explanations based on detected trust levels, with measurable improvements in appropriate trust behavior.

### Open Question 4
- Question: What are the long-term effects of different explanation transparency levels on user learning and adaptation in complex AI tasks?
- Basis in paper: [inferred] The study only examined performance over a single session of 30 tasks, but didn't investigate how explanations affect learning curves over extended periods.
- Why unresolved: The paper observed some learning effects within the study period but didn't examine whether these effects persist or compound over longer usage periods.
- What evidence would resolve it: Longitudinal studies tracking user performance and trust levels across multiple sessions or weeks of interaction with different explanation types.

## Limitations
- The participant pool consisted of non-technical users without SQL programming skills, limiting generalizability to technical audiences.
- Results may not transfer to other domains or models beyond the specific Spider dataset and T5-base architecture used.
- The study measured trust changes but did not investigate long-term effects of explanation transparency on user behavior or model adoption.

## Confidence

- **High confidence**: The finding that medium-transparency explanations strike a balance between user comprehension and cognitive load is well-supported by multiple evidence anchors and the observed pattern of least trust changes among participants.
- **Medium confidence**: The claim that high-transparency explanations overwhelm non-technical users is supported but relies heavily on abstract statements rather than detailed user feedback or interaction patterns.
- **Low confidence**: The mechanism linking explanation transparency to trust calibration lacks direct evidence from the corpus and requires more detailed investigation of how users process and interpret different explanation levels.

## Next Checks
1. Conduct a follow-up study with technical participants who have SQL programming skills to determine if high-transparency explanations are more effective for this audience.
2. Implement a within-subjects design where participants interact with all three transparency levels to control for individual differences in trust propensity and learning ability.
3. Add eye-tracking or interaction time metrics to quantify how users engage with different explanation levels and identify specific points of confusion or disengagement.