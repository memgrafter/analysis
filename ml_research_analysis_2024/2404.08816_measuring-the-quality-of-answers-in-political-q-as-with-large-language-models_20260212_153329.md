---
ver: rpa2
title: Measuring the Quality of Answers in Political Q&As with Large Language Models
arxiv_id: '2404.08816'
source_url: https://arxiv.org/abs/2404.08816
tags:
- answers
- questions
- answer
- government
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new method for measuring answer quality in
  political Q&A sessions using large language models. The authors operationalize answer
  quality as the ease and accuracy with which an answer can be identified among random
  candidates given the question's text.
---

# Measuring the Quality of Answers in Political Q&As with Large Language Models

## Quick Facts
- arXiv ID: 2404.08816
- Source URL: https://arxiv.org/abs/2404.08816
- Reference count: 5
- Primary result: A self-supervised LLM method measures answer quality in political Q&A by semantic similarity, revealing party-based disparities in answer relevance

## Executive Summary
This paper introduces a novel method for assessing answer quality in political Q&A sessions using large language models. The authors operationalize answer quality as the ease and accuracy with which an answer can be identified among random candidates given the question's text. They implement this using a self-supervised model trained on Canadian House of Commons Question Period data, measuring semantic similarity between questions and answers. The analysis reveals that while some answers show weak semantic connections to questions, most are at least moderately relevant—far exceeding what would be expected from random replies.

## Method Summary
The methodology uses a self-supervised approach to measure answer quality by fine-tuning a pre-trained Sentence-BERT model on Canadian parliamentary Q&A data. The model is trained to recognize the correct answer among random candidates using Multiple Negatives Ranking Loss, with semantic similarity measured via cosine similarity between question and answer embeddings. The corpus includes 58,343 exchanges from 2006-2021, with 5% used for training, 1% for validation, and 94% for inference. The quality metric is based on how easily and accurately the correct answer can be identified from a set of randomly drawn candidates given the question text.

## Key Results
- Most answers show moderate semantic relevance to their questions, significantly exceeding random baseline
- Significant correlation found between answer quality and party affiliation of MPs asking questions
- Government backbenchers, third-party MPs, and those ideologically closer to government receive more relevant answers
- Self-supervised approach successfully measures answer quality without requiring human-labeled data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic similarity between questions and answers can be measured by how easily the correct answer can be identified among random candidates.
- Mechanism: The paper operationalizes answer quality as the ease and accuracy of recognizing the correct answer from a random set of candidates given the question text. This mirrors semantic search, where relevance is measured by similarity between query and document embeddings.
- Core assumption: Relevant answers have unique semantic patterns that distinguish them from random answers, making them more easily identifiable.
- Evidence anchors:
  - [abstract] "We measure the quality of an answer based on how easily and accurately it can be recognized in a random set of candidate answers given the question's text."
  - [section 4] "Drawing a parallel with semantic search, we measure an answer's relevance based on how easily and accurately it can be recognized from a set of candidates randomly drawn from the corpus of observed answers based on the question's text."
- Break condition: If answers are deliberately evasive or use generic language that could apply to many questions, this mechanism breaks down as the semantic patterns become indistinguishable from random answers.

### Mechanism 2
- Claim: Fine-tuning a pre-trained BERT model on political Q&A data creates embeddings that capture domain-specific semantic relationships.
- Mechanism: The model uses transfer learning, starting with a pre-trained Sentence-BERT model and fine-tuning it on 5% of the observed questions and answers. This adapts general language patterns to the specific context of parliamentary exchanges.
- Core assumption: Politicians answer questions in ways that follow consistent semantic patterns within the political discourse domain, even when being evasive.
- Evidence anchors:
  - [section 5.3] "We use themulti-qa-mpnet-base-cos-v1 model... We fine-tune the model on five percent of the exchanges in our corpus"
- Break condition: If political discourse uses highly idiosyncratic language patterns or if evasion tactics create semantic patterns completely different from normal discourse, fine-tuning may not capture meaningful relationships.

### Mechanism 3
- Claim: Cosine similarity between question and answer embeddings provides a continuous measure of answer quality.
- Mechanism: The fine-tuned model produces embeddings for questions and answers, and cosine similarity between these embeddings serves as the quality metric. Higher similarity indicates more relevant answers.
- Core assumption: Semantic similarity correlates with relevance in political Q&A contexts, even when answers are not completely straightforward.
- Evidence anchors:
  - [section 5.1] "We use the cosine similarity to measure the resemblance between question and answer embeddings"
- Break condition: If cosine similarity fails to distinguish between genuinely relevant answers and evasive but semantically related responses, the measure becomes unreliable.

## Foundational Learning

- Concept: Semantic search and information retrieval
  - Why needed here: The methodology directly builds on semantic search principles to measure answer quality
  - Quick check question: Can you explain how semantic search differs from keyword-based search and why it's relevant for measuring answer quality?

- Concept: Transfer learning in NLP
  - Why needed here: The model uses pre-trained embeddings and fine-tunes them on domain-specific data
  - Quick check question: What are the advantages and potential risks of using transfer learning for this application?

- Concept: Cosine similarity and vector space models
  - Why needed here: The quality metric relies on measuring similarity between high-dimensional embeddings
  - Quick check question: How does cosine similarity capture semantic relationships differently from Euclidean distance?

## Architecture Onboarding

- Component map: Question text → Question encoder → Answer text → Answer encoder → Cosine similarity → Quality score
- Critical path: Question text → Question encoder → Answer text → Answer encoder → Cosine similarity → Quality score
- Design tradeoffs: Self-supervised learning avoids human labeling costs but may miss nuanced quality aspects; pre-training speeds up training but may introduce domain bias
- Failure signatures: Low correlation between similarity scores and human judgments; model performs well on training data but poorly on validation data; similarity scores cluster around certain values regardless of actual answer quality
- First 3 experiments:
  1. Test correlation between cosine similarity and human-labeled answer quality on a small validation set
  2. Compare pre-trained model performance vs. fine-tuned model on a held-out test set
  3. Analyze whether similarity scores differ significantly between random answer pairs and actual question-answer pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we distinguish between low-quality answers that result from poor question framing versus deliberate evasion or obfuscation?
- Basis in paper: [explicit] The authors acknowledge that poorly framed questions can affect answer quality and discuss the need to "look past these political tactics" to assess whether answers are meaningful.
- Why unresolved: The paper's methodology measures semantic similarity but doesn't fully account for how question quality impacts this measure. The authors note this is an indirect relationship but don't propose a solution for disentangling these effects.
- What evidence would resolve it: A systematic comparison of question-answer pairs where the same question is answered by different speakers, or where different questions receive the same answer, could help isolate the effects of question quality from answer quality.

### Open Question 2
- Question: How does the quality of follow-up questions affect the quality of subsequent answers in parliamentary Q&A sessions?
- Basis in paper: [inferred] The authors mention that follow-up questions are particularly challenging in unstructured settings and suggest this warrants further investigation.
- Why unresolved: The current methodology focuses on individual question-answer pairs and doesn't capture the dynamic nature of parliamentary exchanges where follow-up questions often probe initial answers.
- What evidence would resolve it: Analysis of sequences of related questions and answers, tracking how answer quality changes in response to follow-up questions compared to initial questions, would provide insights into the effectiveness of follow-up questioning.

### Open Question 3
- Question: How does the timing of questions within a parliamentary session (e.g., first vs. last questions) affect the quality of answers received?
- Basis in paper: [inferred] The paper describes the structured nature of Question Period with predetermined order but doesn't examine how position in the sequence affects answer quality.
- Why unresolved: While the paper analyzes party affiliation and legislative context, it doesn't consider temporal factors within individual sessions that might influence answer quality.
- What evidence would resolve it: Statistical analysis comparing answer quality across different positions in the question sequence within individual Question Period sessions, controlling for party affiliation and other factors, would reveal whether timing affects answer quality.

## Limitations
- Methodology assumes semantic similarity directly correlates with answer quality, which may not hold for evasive responses
- Fine-tuning uses only 5% of corpus, potentially limiting ability to capture nuanced political discourse patterns
- Absence of human-labeled validation data prevents direct mapping of similarity scores to human judgments

## Confidence
- **High Confidence**: The methodology of using semantic similarity as a proxy for answer quality and the correlation between party affiliation and answer quality
- **Medium Confidence**: The effectiveness of the self-supervised fine-tuning approach for capturing political discourse patterns
- **Low Confidence**: The generalizability of findings beyond the Canadian parliamentary context and whether semantic similarity truly captures all dimensions of answer quality

## Next Checks
1. **Human Validation Study**: Conduct a small-scale human evaluation where independent raters assess answer quality using the three-category taxonomy (Full Reply, Intermediate Reply, Non-Reply) on a sample of 500 question-answer pairs, then compare these ratings to the model's cosine similarity scores to establish construct validity.

2. **Cross-Domain Transfer Test**: Apply the fine-tuned model to Question Period data from another parliamentary system (e.g., UK House of Commons or Australian Parliament) to test whether the semantic patterns learned are transferable or specific to the Canadian context.

3. **Evasion Detection Experiment**: Manually identify a set of known evasive answers from the corpus and analyze whether these receive lower cosine similarity scores than straightforward answers, testing whether the semantic similarity metric can detect evasion tactics.