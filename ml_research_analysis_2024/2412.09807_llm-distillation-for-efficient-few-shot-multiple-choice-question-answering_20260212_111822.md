---
ver: rpa2
title: LLM Distillation for Efficient Few-Shot Multiple Choice Question Answering
arxiv_id: '2412.09807'
source_url: https://arxiv.org/abs/2412.09807
tags:
- data
- generation
- generated
- json
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel framework that leverages large language
  models (LLMs) for few-shot multiple-choice question answering (MCQA). The approach
  uses LLMs to generate synthetic MCQA datasets and assign probability scores to answer
  choices, then fine-tunes a smaller encoder-only model (DeBERTa-v3-base) using these
  generated data and soft labels via distillation loss.
---

# LLM Distillation for Efficient Few-Shot Multiple Choice Question Answering

## Quick Facts
- arXiv ID: 2412.09807
- Source URL: https://arxiv.org/abs/2412.09807
- Authors: Patrick Sutanto; Joan Santoso; Esther Irawati Setiawan; Aji Prasetya Wibawa
- Reference count: 40
- Primary result: Improves MMLU MCQA accuracy from 28.9% to 39.3% using LLM distillation

## Executive Summary
This paper proposes a novel framework that leverages large language models (LLMs) for few-shot multiple-choice question answering (MCQA). The approach uses LLMs to generate synthetic MCQA datasets and assign probability scores to answer choices, then fine-tunes a smaller encoder-only model (DeBERTa-v3-base) using these generated data and soft labels via distillation loss. Experiments on the MMLU benchmark show that the proposed method improves accuracy from 28.9% to 39.3%, achieving over 10% absolute gain compared to a 5-shot baseline. The method enables strong few-shot MCQA performance with compact models, approaching the performance of significantly larger models like LLaMA-7B and Flan-T5-250M.

## Method Summary
The proposed framework uses LLM-generated data and soft labels to train a smaller encoder-only model through knowledge distillation. Specifically, the method employs Llama-3.1-8B-Instruct to generate synthetic MCQA examples (using either JSON or decomposed generation methods) and score the answer choices with probability distributions. These scores serve as soft labels during training of DeBERTa-v3-base using a combination of cross-entropy loss and distillation loss. The approach addresses the challenge of limited labeled data in few-shot settings by leveraging LLM capabilities to create augmented training data while maintaining efficiency through model compression via distillation.

## Key Results
- Achieves 39.3% accuracy on MMLU MCQA, improving from 28.9% 5-shot baseline
- Outperforms significantly larger models including LLaMA-7B and Flan-T5-250M
- Demonstrates the effectiveness of LLM-driven data generation combined with distillation
- Shows that decomposed generation method provides more robust data generation than direct JSON generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-driven data generation combined with distillation enables effective few-shot MCQA performance with compact models
- Mechanism: LLMs generate synthetic MCQA datasets with associated probability scores, which are used to train a smaller encoder-only model through knowledge distillation
- Core assumption: LLM-generated data, despite potential noise, provides valuable training signals when combined with soft-label distillation
- Evidence anchors:
  - [abstract] "Our approach utilizes LLMs to create MCQA data which contains questions and choices, and to assign probability scores to the generated choices. We then use the generated data and LLM-assigned scores to finetune a smaller and more efficient encoder-only model"
  - [section] "We then use the LLM to give a probability score to the generated answer choices, providing soft labels that are incorporated into the student model's training through a distillation loss"
  - [corpus] Weak evidence - no direct corpus references found
- Break condition: If LLM-generated data quality is too poor or if the distillation process fails to effectively transfer knowledge

### Mechanism 2
- Claim: The decomposed generation method provides more robust data generation than direct JSON generation
- Mechanism: Breaking down data generation into separate stages (question, positive answer, negative answer) avoids parsing issues and ensures higher data yield
- Core assumption: The decomposed approach, despite potentially lower individual data quality, yields more usable training data overall
- Evidence anchors:
  - [section] "Our second approach termed the decomposed generation method, breaks down the MCQA data generation process into three distinct stages: question generation, positive answer generation, and negative answer generation... This decomposition eliminates the need for complex parsing of LLM output, which can be prone to errors when enforcing structured formats like JSON"
  - [section] "The decomposed approach offers a more robust and efficient alternative" (C.5)
  - [corpus] Weak evidence - no direct corpus references found
- Break condition: If the decomposed method fails to generate diverse and relevant data or if the increased data volume doesn't compensate for potential quality loss

### Mechanism 3
- Claim: LLM distillation with soft labels improves robustness to noisy labels in generated data
- Mechanism: Using the full probability distribution from LLM scoring rather than hard labels allows the student model to learn a smoother probability distribution over answer choices
- Core assumption: Soft labels from LLM scoring provide a more nuanced and robust training signal than hard labels
- Evidence anchors:
  - [section] "We leverage the LLM scores to guide the training of the encoder model through distillation loss... We define the distillation loss as Ldistill = LCE (p, Ë†p) where p represents the soft target probabilities derived from the LLM scores"
  - [section] "Using a temperature of 0 leads to a significant performance drop, highlighting the importance of soft-label distillation for mitigating the impact of noise in the generated data" (C.2)
  - [corpus] Weak evidence - no direct corpus references found
- Break condition: If the temperature hyperparameter is set too high or too low, or if the LLM scoring is too noisy

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: To transfer knowledge from the large LLM to the smaller encoder-only model
  - Quick check question: What is the key difference between hard labels and soft labels in the context of knowledge distillation?

- Concept: Few-Shot Learning
  - Why needed here: The method aims to achieve strong MCQA performance with limited labeled data
  - Quick check question: How does few-shot learning differ from traditional supervised learning in terms of data requirements?

- Concept: Encoder-Only Models
  - Why needed here: The target model architecture is an encoder-only model (DeBERTa-v3-base)
  - Quick check question: What are the key differences between encoder-only models and encoder-decoder models in terms of architecture and typical use cases?

## Architecture Onboarding

- Component map:
  Llama-3.1-8B-Instruct -> Decomposed Generation Pipeline -> DeBERTa-v3-base -> Distillation Loss

- Critical path:
  1. Generate synthetic MCQA data using LLM (either JSON or decomposed method)
  2. Score generated data using LLM to obtain probability distributions over answer choices
  3. Fine-tune DeBERTa-v3-base using the generated data and distillation loss

- Design tradeoffs:
  - JSON vs. decomposed generation: JSON may yield higher quality data but with lower parsing success rates, while decomposed generation ensures higher data yield but potentially with more noise
  - Temperature hyperparameter: Higher temperatures encourage more diverse data generation but may also introduce more noise

- Failure signatures:
  - Low parsing success rate in JSON generation
  - Poor performance on validation set despite high training accuracy
  - Significant performance gap between generated data and real data

- First 3 experiments:
  1. Generate 1024 MCQA examples using the decomposed method and evaluate the impact on DeBERTa-v3-base performance compared to the 5-shot baseline
  2. Compare the performance of DeBERTa-v3-base when trained with and without distillation on JSON-generated data
  3. Investigate the effect of varying the number of negative examples generated per question in the decomposed method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-driven data generation and distillation scale with the size and diversity of the initial few-shot dataset?
- Basis in paper: [inferred] The paper uses a fixed 5-shot dataset and observes performance improvements, but does not explore how performance varies with different sizes or compositions of the initial dataset.
- Why unresolved: The study focuses on demonstrating the effectiveness of the approach with a 5-shot baseline, leaving the impact of varying the initial dataset unexplored.
- What evidence would resolve it: Experiments varying the number of initial examples (e.g., 1, 3, 10, 20 shots) and the diversity of topics covered in the initial dataset, measuring performance gains from data generation and distillation.

### Open Question 2
- Question: What is the optimal balance between data generation quality and quantity for few-shot MCQA tasks?
- Basis in paper: [explicit] The paper mentions that the decomposed method generates more data but with potentially lower quality, while the JSON method generates higher-quality data but with lower success rates.
- Why unresolved: While the paper shows that both methods improve performance, it does not investigate the trade-off between generating more data versus generating higher-quality data.
- What evidence would resolve it: Controlled experiments comparing performance using datasets with varying quality and quantity, generated using different strategies, to identify the optimal balance.

### Open Question 3
- Question: How does the proposed approach generalize to other NLP tasks beyond MCQA, particularly those involving longer contexts or more complex answer formats?
- Basis in paper: [explicit] The paper acknowledges limitations in handling longer contexts and suggests future work to extend the approach to other tasks.
- Why unresolved: The experiments are confined to MCQA tasks with relatively short contexts, leaving the applicability to other tasks unexplored.
- What evidence would resolve it: Applying the LLM data generation and distillation framework to tasks like extractive question answering, text summarization, or long-form document classification, and evaluating performance improvements.

## Limitations
- Limited evaluation scope - only tested on MMLU benchmark with 5-shot setup
- No comparison against other state-of-the-art few-shot MCQA methods
- Lack of ablation studies on critical components (data generation quality vs. distillation)
- No analysis of how the method performs across different domains or question types

## Confidence
- High Confidence: Achieves 39.3% accuracy on MMLU, representing a 10% absolute improvement over 5-shot baseline; Knowledge distillation with soft labels provides robustness benefits; Decomposed generation method ensures higher data yield compared to JSON parsing
- Medium Confidence: LLM-generated data combined with distillation enables effective few-shot MCQA performance; The approach approaches performance of significantly larger models (LLaMA-7B, Flan-T5-250M); Soft-label distillation mitigates noise in generated data
- Low Confidence: The specific mechanisms by which decomposed generation improves robustness; The optimal temperature settings for balancing data diversity and quality; Generalizability of results to other MCQA benchmarks or domains

## Next Checks
- Check 1: Replication with disclosed hyperparameters - Run a faithful reproduction using the exact prompt templates and temperature settings to verify if the 39.3% MMLU accuracy is achievable
- Check 2: Cross-domain generalization test - Evaluate the method on a different MCQA benchmark (e.g., RACE, DREAM) to assess whether the performance gains generalize beyond MMLU
- Check 3: Ablation study on data generation methods - Compare performance when using: (a) real 5-shot examples only, (b) LLM-generated data with hard labels, (c) LLM-generated data with soft labels