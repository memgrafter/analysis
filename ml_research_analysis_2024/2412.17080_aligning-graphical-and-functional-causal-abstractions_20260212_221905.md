---
ver: rpa2
title: Aligning Graphical and Functional Causal Abstractions
arxiv_id: '2412.17080'
source_url: https://arxiv.org/abs/2412.17080
tags:
- abstraction
- causal
- variables
- graphical
- exists
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a rigorous connection between two major\
  \ frameworks for causal abstraction: graphical abstractions (specifically Cluster\
  \ DAGs) and functional abstractions (specifically \u03B1-abstractions). The authors\
  \ prove that bijective L2-consistent \u03B1-abstractions with all variables as relevant\
  \ (R=V) are equivalent to Cluster DAGs, meaning they describe the same set of abstracted\
  \ models."
---

# Aligning Graphical and Functional Causal Abstractions

## Quick Facts
- arXiv ID: 2412.17080
- Source URL: https://arxiv.org/abs/2412.17080
- Authors: Willem Schooltink; Fabio Massimo Zennaro
- Reference count: 35
- Primary result: Bijective L2-consistent α-abstractions with R=V are equivalent to Cluster DAGs; Partial Cluster DAGs extend expressiveness.

## Executive Summary
This paper establishes a rigorous connection between two major frameworks for causal abstraction: graphical abstractions (Cluster DAGs) and functional abstractions (α-abstractions). The authors prove that bijective L2-consistent α-abstractions with all variables as relevant are equivalent to Cluster DAGs, meaning they describe the same set of abstracted models. They extend this result by introducing Partial Cluster DAGs (PCDAGs), which allow for more expressive abstractions by enabling the removal of certain variables while preserving their confounding effects. These results enable the transfer of results and methods between frameworks, suggesting that PCDAGs can serve as a practical foundation for designing and validating consistent causal abstractions.

## Method Summary
The authors prove equivalence results between graphical and functional causal abstraction frameworks through a series of lemmas and theorems. They align L2-consistency notions across frameworks, establish that bijective L2-consistent α-abstractions with R=V uniquely determine CDAG structure, and extend this to PCDAGs for more expressive abstractions. The proofs leverage properties of surjective and bijective range mappings, interventional distribution agreement, and confounding edge preservation. The paper also demonstrates an equivalence between α-abstractions and constructive τ-abstractions, further strengthening the bridge between graphical and functional approaches.

## Key Results
- Bijective L2-consistent α-abstractions with R=V are equivalent to Cluster DAGs
- Partial Cluster DAGs (PCDAGs) describe a larger set of L2-consistent α-abstractions than standard CDAGs
- PCDAGs allow for more expressive abstractions by enabling variable removal while preserving confounding effects
- An equivalence is established between α-abstractions and constructive τ-abstractions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: L2-consistency implies L1-consistency through null intervention.
- Mechanism: The null intervention (do(∅)) in L2-consistency enforces agreement on marginal and joint distributions, which by Bayes' theorem yields conditional distribution agreement, satisfying L1-consistency.
- Core assumption: All range maps in the abstraction are surjective, ensuring pre-images exist for all values.
- Break condition: If range maps are not surjective, the pre-image α⁻¹ may not exist for some values, breaking the marginal-to-conditional transfer.

### Mechanism 2
- Claim: Bijective range maps preserve distributional (in)equalities between base and abstracted models.
- Mechanism: Surjectivity ensures that any inequality in the abstracted model has a corresponding inequality in the base model; bijectivity additionally guarantees that equalities are preserved.
- Core assumption: The abstraction is L2-consistent and all range maps α_V are bijective.
- Break condition: If range maps are merely surjective (not bijective), some equalities in the abstracted model may not correspond to equalities in the base model, violating graphical consistency.

### Mechanism 3
- Claim: PCDAGs generalize CDAGs by allowing variables to be excluded from clusters while preserving their confounding effects.
- Mechanism: Rule 2B of PCDAGs explicitly captures confounding edges introduced when shared parents are dropped, enabling more expressive abstractions without losing causal consistency.
- Core assumption: The abstracted model must maintain L2-consistency even when some variables are excluded from clusters.
- Break condition: If Rule 2B is omitted, confounding effects of dropped variables may be lost, breaking consistency between base and abstracted models.

## Foundational Learning

- Concept: Structural Causal Models (SCMs) and their underlying DAG representation
  - Why needed here: The paper's alignment between graphical and functional abstractions relies on understanding how SCMs encode causal relationships via DAGs and interventions.
  - Quick check question: In an SCM, if variable X is a direct cause of Y, what edge exists in the underlying DAG?

- Concept: Pearl's Causal Hierarchy (L1 observational, L2 interventional, L3 counterfactual)
  - Why needed here: The paper defines and aligns consistency notions across L1 and L2, with L2-consistency being the key property for causal abstractions.
  - Quick check question: Can a query at L2 be reduced to L1 without additional assumptions? Why or why not?

- Concept: Do-calculus and identifiability of causal queries
  - Why needed here: Graphical consistency is evaluated through identifiability of causal queries across models, which requires understanding do-calculus.
  - Quick check question: What does it mean for a causal query to be "identifiable" in the context of do-calculus?

## Architecture Onboarding

- Component map:
  - Base SCM (M) → Graphical abstraction (CDAG/PCDAG) → Functional abstraction (α-abstraction)
  - Key components: variable clustering, range mappings, intervention preservation, confounding edge handling

- Critical path:
  1. Define base SCM with DAG structure
  2. Construct PCDAG via clustering rules (including Rule 2B for partial clustering)
  3. Define bijective α-abstraction with R=V
  4. Verify L2-consistency via interventional distribution agreement

- Design tradeoffs:
  - Bijectivity vs. surjectivity: Bijective mappings ensure full preservation of equalities but may be restrictive; surjective mappings are more flexible but may lose some consistency guarantees.
  - Complete vs. partial clustering: CDAGs require all variables to be clustered (R=V), while PCDAGs allow dropping variables but must preserve their confounding effects.

- Failure signatures:
  - Loss of independent predictability or intervenability when using CDAGs for certain abstractions
  - Inconsistent interventional distributions between base and abstracted models
  - Missing confounding edges when using partial clustering without Rule 2B

- First 3 experiments:
  1. Implement Example 1: Create SCM with smoking, air pollution, cancer, and breathing variables; test CDAG vs. PCDAG abstraction with Z dropped
  2. Verify Mechanism 1: Construct L2-consistent abstraction and check if L1-consistency holds via null intervention
  3. Test Mechanism 2: Create bijective vs. merely surjective abstraction and compare preservation of distributional equalities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the expressiveness of Partial Cluster DAGs (PCDAGs) compare to other abstraction frameworks like T-abstraction or constructive τ-abstractions in terms of modeling real-world causal systems?
- Basis in paper: [explicit] The paper compares PCDAGs to α-abstractions and mentions T-abstraction as a linearized version, but does not provide a comprehensive comparison with other frameworks.
- Why unresolved: The paper focuses on aligning graphical and functional abstractions but does not explore how PCDAGs perform relative to other frameworks in practical applications or complex scenarios.
- What evidence would resolve it: Empirical studies comparing PCDAGs to T-abstraction and constructive τ-abstractions on benchmark causal datasets, or theoretical proofs establishing their relative expressiveness.

### Open Question 2
- Question: What are the computational implications of using PCDAGs for causal abstraction learning algorithms, particularly in high-dimensional or large-scale systems?
- Basis in paper: [inferred] The paper suggests PCDAGs can be a practical foundation for designing and validating abstractions, but does not address computational efficiency or scalability.
- Why unresolved: The paper establishes theoretical properties of PCDAGs but does not explore their algorithmic implementation or performance in practice.
- What evidence would resolve it: Runtime complexity analysis of PCDAG-based algorithms, empirical comparisons with existing abstraction learning methods on large-scale datasets, or case studies demonstrating scalability.

### Open Question 3
- Question: How do assumptions like faithfulness and L2-consistency affect the applicability of PCDAGs in real-world causal inference tasks where these assumptions may be violated?
- Basis in paper: [explicit] The paper assumes faithfulness and L2-consistency throughout, but acknowledges these may not hold in practical applications.
- Why unresolved: The paper does not investigate the robustness of PCDAGs to violations of these assumptions or provide guidance on handling such cases.
- What evidence would resolve it: Simulation studies testing PCDAGs under assumption violations, theoretical bounds on error propagation when assumptions are relaxed, or extensions of the framework to handle non-faithful or non-L2-consistent scenarios.

### Open Question 4
- Question: Can the alignment between graphical and functional abstractions be extended to include higher-order causal queries (e.g., counterfactuals) beyond the L1 and L2 layers discussed in the paper?
- Basis in paper: [explicit] The paper focuses on aligning L1 and L2 consistency but explicitly limits its scope to these layers.
- Why unresolved: The paper does not explore whether the equivalence between CDAGs/PCDAGs and α-abstractions holds for counterfactual consistency or other higher-order queries.
- What evidence would resolve it: Proofs or counterexamples showing whether the equivalence extends to counterfactuals, or empirical studies demonstrating the performance of PCDAGs in counterfactual reasoning tasks.

### Open Question 5
- Question: What are the practical implications of the bijective range mapping assumption in α-abstractions for applications where variable ranges are continuous or high-dimensional?
- Basis in paper: [inferred] The paper relies on bijective range mappings for theoretical results but does not address their feasibility in continuous or high-dimensional settings.
- Why unresolved: The paper does not discuss how to handle cases where bijective mappings are impractical or impossible, such as in continuous or high-dimensional variable spaces.
- What evidence would resolve it: Examples of PCDAGs applied to continuous or high-dimensional systems, theoretical extensions of the framework to relax the bijective assumption, or empirical studies showing the impact of this assumption on real-world applications.

## Limitations
- The bijective requirement in α-abstractions may be too restrictive for many real-world applications, particularly with continuous or high-dimensional variables
- The framework assumes faithfulness in SCMs, which may not hold in practical applications
- Computational complexity of verifying PCDAGs in practice, particularly for high-dimensional systems, remains unexplored

## Confidence
- High confidence in the core equivalence between bijective L2-consistent α-abstractions with R=V and Cluster DAGs
- Medium confidence in the Partial Cluster DAG extension due to its novelty and need for empirical validation
- Low confidence in the practical scalability and computational efficiency of PCDAG-based algorithms

## Next Checks
1. Implement PCDAGs on benchmark causal discovery datasets to empirically verify that they capture more valid abstractions than CDAGs while maintaining consistency
2. Test the practical expressivity limits of bijective α-abstractions by attempting to construct non-bijective but L2-consistent alternatives for known causal systems
3. Evaluate the computational complexity of PCDAG verification algorithms and compare against CDAG verification for systems of increasing size and complexity