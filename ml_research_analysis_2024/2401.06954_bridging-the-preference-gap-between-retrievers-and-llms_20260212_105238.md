---
ver: rpa2
title: Bridging the Preference Gap between Retrievers and LLMs
arxiv_id: '2401.06954'
source_url: https://arxiv.org/abs/2401.06954
tags:
- faze
- bridge
- context
- llms
- clan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates a preference gap between retrievers and
  LLMs in retrieval-augmented generation (RAG) systems, where retrievers rank information
  for human readability while LLMs have different preferences for optimal performance.
  The authors propose a novel bridge model (BGM) that sits between frozen retrievers
  and frozen LLMs, transforming retrieved information into an LLM-friendly format
  through a sequence-to-sequence architecture.
---

# Bridging the Preference Gap between Retrievers and LLMs
## Quick Facts
- arXiv ID: 2401.06954
- Source URL: https://arxiv.org/abs/2401.06954
- Reference count: 10
- Primary result: BGM bridge model significantly improves RAG performance across four datasets, achieving up to 35.64 EM score on HotpotQA

## Executive Summary
This paper addresses a fundamental mismatch in retrieval-augmented generation (RAG) systems where retrievers rank information for human readability while large language models (LLMs) have different preferences for optimal performance. The authors propose a novel bridge model (BGM) that transforms retrieved information into an LLM-friendly format through sequence-to-sequence architecture. BGM sits between frozen retrievers and frozen LLMs, effectively bridging the preference gap through a combination of supervised learning and reinforcement learning training approaches.

The research demonstrates that directly using retrieved passages in RAG systems can lead to suboptimal LLM performance due to this preference gap. By introducing BGM as an intermediate component, the system achieves significant improvements across multiple datasets and tasks. The approach combines synthesized silver passage sequences for supervised learning with downstream task performance as reward for reinforcement learning, creating a robust training framework that adapts retriever outputs to better suit LLM requirements.

## Method Summary
The authors propose a bridge model (BGM) that transforms retrieved information into an LLM-friendly format through a sequence-to-sequence architecture. BGM is trained using a combination of supervised learning with synthesized silver passage sequences and reinforcement learning with downstream task performance as reward. The model sits between frozen retrievers and frozen LLMs, effectively bridging the preference gap by transforming retrieved information to better suit LLM preferences for optimal performance.

## Key Results
- BGM significantly outperforms strong baselines including GTR retrievers and reranking models
- Achieved up to 35.64 EM score on HotpotQA compared to 25.80 for GTR retriever baseline
- Demonstrated consistent improvements across four datasets (NQ, HotpotQA, Email, Book)
- Showed effectiveness of combining supervised learning and reinforcement learning training approaches

## Why This Works (Mechanism)
The preference gap between retrievers and LLMs arises because retrievers are optimized for human readability and relevance ranking, while LLMs have different preferences for information presentation that optimize their performance. BGM effectively transforms the retrieved information into a format that better aligns with LLM preferences, acting as an adapter layer that doesn't require retraining the retriever or LLM components.

## Foundational Learning
- **Retrieval-augmented generation (RAG)**: Systems that combine information retrieval with text generation - needed to understand the baseline architecture being improved
- **Sequence-to-sequence models**: Architectures that transform input sequences to output sequences - needed to understand how BGM adapts retrieved information
- **Supervised learning with silver data**: Training using synthetic or generated labels - needed to understand the training methodology
- **Reinforcement learning with task rewards**: Training that optimizes for downstream performance metrics - needed to understand the performance-driven training component
- **Frozen model training**: Training a model while keeping other components fixed - needed to understand the practical deployment constraints
- **Preference alignment**: Adjusting system outputs to match target model preferences - needed to understand the core problem being solved

## Architecture Onboarding
Component map: Retriever -> BGM -> LLM
Critical path: Query -> Retriever -> BGM transformation -> LLM generation -> Output
Design tradeoffs: Introducing BGM adds computational overhead but significantly improves performance; requires careful balance between supervised and reinforcement learning components
Failure signatures: Poor performance indicates either retriever failure to retrieve relevant information or BGM failure to transform information appropriately
First experiments:
1. Test BGM on single dataset with baseline retriever to establish baseline improvement
2. Evaluate performance with only supervised learning component vs. only reinforcement learning component
3. Test different sequence-to-sequence architectures for BGM to identify optimal design

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability concerns beyond tested datasets (NQ, HotpotQA, Email, Book)
- Potential biases from synthesized silver passage sequences in training data
- Computational overhead impact on real-time deployment scenarios not quantified

## Confidence
- Claim: Bridging preference gap is crucial for effective RAG systems -> High
- Claim: Specific BGM architectural choices are optimal -> Medium
- Claim: Approach generalizes to complex domains -> Medium

## Next Checks
1. Test BGM on additional datasets from diverse domains to assess generalizability beyond current experimental scope
2. Conduct ablation studies to quantify contribution of supervised learning vs. reinforcement learning components
3. Measure computational overhead and latency introduced by BGM in real-time RAG applications to evaluate practical deployment feasibility