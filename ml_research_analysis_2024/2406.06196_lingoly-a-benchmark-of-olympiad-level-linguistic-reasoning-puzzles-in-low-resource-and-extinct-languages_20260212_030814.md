---
ver: rpa2
title: 'LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource
  and Extinct Languages'
arxiv_id: '2406.06196'
source_url: https://arxiv.org/abs/2406.06196
tags:
- language
- dataset
- scores
- questions
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LINGOLY benchmark evaluates advanced reasoning abilities in
  large language models using challenging Linguistic Olympiad puzzles in low-resource
  and extinct languages. It consists of 1,133 problems across 6 formats and 5 difficulty
  levels, covering over 90 languages to minimize data contamination.
---

# LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages

## Quick Facts
- **arXiv ID:** 2406.06196
- **Source URL:** https://arxiv.org/abs/2406.06196
- **Reference count:** 40
- **Primary result:** Top models achieve only 46.3% exact match accuracy on challenging linguistic reasoning puzzles in low-resource and extinct languages

## Executive Summary
LINGOLY is a benchmark designed to evaluate advanced reasoning abilities in large language models using Linguistic Olympiad puzzles. The benchmark consists of 1,133 problems across 6 formats and 5 difficulty levels, covering over 90 languages to minimize data contamination. The evaluation uses exact match accuracy and improvement over a no-context baseline to distinguish reasoning from memorization. Results show the benchmark is highly challenging, with top models achieving only 46.3% accuracy and performance dropping significantly on harder problems. Large closed models typically outperform open models, and language resource availability correlates with performance, indicating that true multi-step out-of-domain reasoning remains difficult for current language models.

## Method Summary
The benchmark evaluates 11 state-of-the-art LLMs on 1,133 linguistic reasoning puzzles from the UK Linguistics Olympiad. Models are tested using zero-shot prompting with puzzle sheets containing preamble, context, and questions. Performance is measured using exact match accuracy and improvement over a no-context baseline (∆N C) to control for memorization. The dataset covers 6 problem formats and 5 difficulty levels across more than 90 low-resource and extinct languages. The benchmark is implemented in Python and available on GitHub and Hugging Face, with code for dataset loading, benchmarking, and analysis.

## Key Results
- Top models achieve only 46.3% exact match accuracy on the benchmark
- Performance drops to 38.7% on advanced difficulty levels
- Improvement over no-context baseline is 28.8%, indicating partial control of memorization
- Large closed models outperform open models, and language resource availability correlates with performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Linguistic Olympiad puzzles test reasoning rather than knowledge by providing all necessary information within the problem context
- **Mechanism:** Each puzzle sheet contains a preamble with language background, context with example translations, and questions that can be solved through pattern recognition and deductive reasoning using only the provided examples
- **Core assumption:** The puzzles are designed such that no external knowledge of the target language is required to solve them
- **Evidence anchors:**
  - [abstract] "Each of the puzzles can be solved through a combination of deductive and analogical reasoning"
  - [section 3.1] "As a guiding principle, we preserve the original text of puzzles, making adaptations only for machine readability"
  - [corpus] Weak evidence - the corpus neighbors focus on linguistic reasoning with LLMs but don't directly address the sufficiency of context in Olympiad puzzles
- **Break condition:** If a puzzle requires external knowledge of the language's grammar, cultural context, or speaker community that isn't provided in the examples, the reasoning mechanism breaks down

### Mechanism 2
- **Claim:** Using low-resource and extinct languages minimizes data contamination and memorization
- **Mechanism:** Languages with few speakers and limited online presence are unlikely to appear in LLM training data, reducing the probability that models can answer based on memorized patterns rather than reasoning
- **Core assumption:** The training data for LLMs has limited coverage of the languages used in the benchmark
- **Evidence anchors:**
  - [abstract] "The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination"
  - [section 3.2] "Languages tested range from very high resource (e.g. Dutch, 25 million native speakers), to very low-resource (e.g. Yawalapiti, <10 native speakers)"
  - [corpus] Strong evidence - the corpus shows multiple papers addressing data contamination in linguistic reasoning benchmarks
- **Break condition:** If a language appears frequently in web data or has been specifically included in multilingual training datasets, the contamination protection fails

### Mechanism 3
- **Claim:** Comparing model performance to a no-context baseline controls for memorization
- **Mechanism:** By evaluating models on the same questions with and without the context examples, we can measure the improvement attributable to using the provided information versus recalling memorized answers
- **Core assumption:** The no-context condition still includes the preamble, which might contain sufficient information for some answers
- **Evidence anchors:**
  - [abstract] "We assess performance with both direct accuracy and comparison to a no-context baseline to further control for memorisation"
  - [section 4.1] "For some scoring function S, and model responses r we define ∆NC to be the improvement in model score between the No Context baseline and the Full prompt"
  - [corpus] Moderate evidence - the corpus shows similar approaches to controlling for memorization in other reasoning benchmarks
- **Break condition:** If the preamble contains enough information to answer questions without the context, the baseline becomes ineffective at distinguishing reasoning from memorization

## Foundational Learning

- **Concept:** Pattern recognition in morphological, phonological, and syntactic structures
  - **Why needed here:** The puzzles require identifying grammatical patterns from limited examples to make translations
  - **Quick check question:** Given "cat" → "cats", "dog" → "dogs", what is the pattern for pluralization in English?

- **Concept:** Analogical reasoning for language generalization
  - **Why needed here:** Once patterns are identified, they must be applied to new words or structures not seen in the examples
  - **Quick check question:** If "book" → "buk" and "look" → "luk" in Language X, what would "cook" likely be?

- **Concept:** Instruction following and task decomposition
  - **Why needed here:** Complex puzzles often require multiple steps and careful attention to specific formatting requirements
  - **Quick check question:** If asked to "Translate these Yawalapiti words into English and then into Dutch," what are the two distinct tasks?

## Architecture Onboarding

- **Component map:** Data pipeline → Model inference → Response formatting → Scoring engine → Analysis dashboard
- **Critical path:** Question loading → Prompt construction → Model API call → Response validation → Score calculation
- **Design tradeoffs:** Exact match scoring ensures linguistic correctness but is harsh on partially correct answers; open models are more accessible but closed models perform better
- **Failure signatures:** Zero scores on computational or monolingual formats suggest instruction-following issues; consistent errors on high-resource languages suggest memorization
- **First 3 experiments:**
  1. Run the benchmark with one open and one closed model to compare performance distributions
  2. Test the no-context baseline on a subset of questions to validate the memorization control
  3. Evaluate chain-of-thought prompting on a small sample to assess potential performance impact

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we effectively separate reasoning from memorization in language model evaluation, particularly for tasks that draw on background knowledge?
- **Basis in paper:** explicit - The paper discusses the challenge of distinguishing between reasoning and memorization in benchmarks, noting that "the combination of these features is difficult to achieve in practice since memorisation and contamination may reduce the necessity of reasoning."
- **Why unresolved:** Current evaluation methods like comparing to a no-context baseline help control for memorization but don't fully solve the problem. The paper shows that even with these controls, models may still leverage memorized knowledge rather than true reasoning.
- **What evidence would resolve it:** Developing more sophisticated evaluation frameworks that can reliably detect when models are reasoning versus recalling memorized patterns, perhaps through systematic testing of model behavior on systematically varied tasks.

### Open Question 2
- **Question:** What specific linguistic patterns or reasoning strategies are most challenging for current language models, and how can we design benchmarks to better target these weaknesses?
- **Basis in paper:** explicit - The paper identifies that "true multi-step out-of-domain reasoning remains a challenge for current language models," particularly on harder problems where "even the top model only achieved 38.7% accuracy."
- **Why unresolved:** While the paper demonstrates that current models struggle with linguistic reasoning tasks, it doesn't identify which specific types of reasoning (e.g., morphological transformations vs. semantic mappings) are most problematic or why.
- **What evidence would resolve it:** Detailed error analysis across different linguistic domains and reasoning types, combined with targeted testing of model capabilities on simplified versions of challenging tasks.

### Open Question 3
- **Question:** How does instruction-following ability impact language model performance on complex reasoning tasks, and can this be improved independently of core reasoning capabilities?
- **Basis in paper:** explicit - The paper notes that "effective instruction following was a limiting factor in performance, with open models erring more than closed models," suggesting that auxiliary tasks like complex instruction following can "disproportionately impact smaller models."
- **Why unresolved:** The paper identifies instruction-following as a key differentiator but doesn't explore whether improving this skill would lead to better reasoning performance or if it's a separate bottleneck.
- **What evidence would resolve it:** Experiments comparing model performance on reasoning tasks with varying instruction complexity, and testing whether improving instruction-following through fine-tuning leads to better reasoning outcomes.

## Limitations
- Exact match scoring may be overly harsh on models producing partially correct or semantically equivalent answers
- The no-context baseline may be ineffective if the preamble contains sufficient information for some question types
- The benchmark's controlled nature may not fully capture the breadth of real-world linguistic reasoning tasks
- Open-vocabulary formats show poor performance from closed models despite their strong overall results

## Confidence
- **High Confidence (8/10):** The claim that LINGOLY is a challenging benchmark is well-supported by empirical results showing low performance across all tested models
- **Medium Confidence (6/10):** The assertion that low-resource and extinct languages minimize data contamination is reasonable but depends on assumptions about training data composition
- **Low Confidence (4/10):** The claim that LINGOLY specifically tests reasoning rather than knowledge is difficult to fully validate due to the fuzzy boundary between reasoning and knowledge application

## Next Checks
1. **Baseline Effectiveness Analysis:** Conduct systematic analysis of the no-context baseline by testing it on a stratified sample of questions across all formats and difficulty levels to measure what percentage can be answered using only preamble information.
2. **Alternative Scoring Validation:** Implement a more lenient scoring system that accepts semantically equivalent answers and compare results to exact match scoring to quantify the impact of overly strict evaluation criteria.
3. **Cross-Benchmark Comparison:** Test the same models on other linguistic reasoning benchmarks and natural language inference tasks to assess whether LINGOLY's difficulty profile is consistent with other reasoning evaluations or represents a unique challenge.