---
ver: rpa2
title: 'DCAST: Diverse Class-Aware Self-Training Mitigates Selection Bias for Fairer
  Learning'
arxiv_id: '2409.20126'
source_url: https://arxiv.org/abs/2409.20126
tags:
- bias
- samples
- class
- cast
- hierarchy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diverse Class-Aware Self-Training (DCAST)
  to address selection bias in machine learning. Selection bias, often caused by uneven
  data representation, leads to unfairness in model predictions.
---

# DCAST: Diverse Class-Aware Self-Training Mitigates Selection Bias for Fairer Learning

## Quick Facts
- arXiv ID: 2409.20126
- Source URL: https://arxiv.org/abs/2409.20126
- Authors: Yasin I. Tepeli; Joana P. Gonçalves
- Reference count: 40
- Primary result: DCAST outperforms conventional self-training and six domain adaptation techniques across eleven datasets, showing significant improvements in prediction accuracy, especially for higher-dimensional datasets.

## Executive Summary
This paper introduces DCAST (Diverse Class-Aware Self-Training), a model-agnostic semi-supervised learning framework that addresses selection bias in machine learning. Selection bias, caused by uneven data representation, leads to unfairness in model predictions. DCAST promotes sample diversity and class-awareness to counter confirmation bias in conventional self-training. The method achieves superior performance across eleven datasets compared to conventional self-training and six domain adaptation techniques, particularly excelling with higher-dimensional data and neural network models.

## Method Summary
DCAST is a semi-supervised learning framework that iteratively augments labeled training data with pseudo-labeled samples from an unlabeled set. It operates through a self-training loop with two key modifications: class-aware sample selection that ensures balanced representation across classes, and an optional diversity module that clusters candidate samples to promote diversity in the pseudo-labeled set. The method uses discriminative embeddings for distance calculations to better capture class-relevant diversity. DCAST is evaluated against conventional self-training and six domain adaptation techniques across eleven datasets, with hierarchy bias used to induce class-specific multivariate bias for evaluation purposes.

## Key Results
- DCAST outperforms conventional self-training and six domain adaptation techniques across eleven datasets
- Significant improvements in prediction accuracy, especially for higher-dimensional datasets
- DCAST effectively mitigates bias induced by hierarchy bias, random subsampling, Dirichlet, and joint bias
- Best results achieved when paired with neural networks, though method is model-agnostic
- Class-aware and diversity-guided components contribute to superior bias mitigation performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class-aware self-training counters confirmation bias by balancing pseudo-label selection across classes.
- Mechanism: At each iteration, the algorithm selects a fixed number of pseudo-labeled samples per class (sc = s × class_ratio(c)), ensuring that no single class dominates the training set. This prevents the model from over-confidently predicting one class and reinforces underrepresented classes.
- Core assumption: The underlying population distribution is balanced across classes or can be approximated by maintaining class ratios in the pseudo-labeled set.
- Evidence anchors:
  - [abstract] "promotes sample diversity to counter confirmation bias of conventional self-training while leveraging unlabeled samples for an improved representation of the underlying population"
  - [section] "sample selection is done separately per class as follows... select the top sc × d samples from the unlabeled set with confidence or prediction probability larger than t"
- Break condition: If class imbalance is severe in the unlabeled data or if class_ratio(c) is zero for some classes, the method may fail to mitigate bias.

### Mechanism 2
- Claim: Diversity-guided clustering reduces overconfidence bias by selecting pseudo-labeled samples that are dissimilar to each other.
- Mechanism: After selecting candidate samples based on confidence, DCAST clusters them (sc clusters per class) and picks the most confidently predicted sample from each cluster. This ensures that the pseudo-labeled set is both high-confidence and diverse, reducing the chance of reinforcing a narrow, biased subset of the feature space.
- Core assumption: Diverse samples across the candidate set correspond to a more representative subset of the true data distribution.
- Evidence anchors:
  - [abstract] "promotes sample diversity to counter confirmation bias of conventional self-training"
  - [section] "Diversity Module... identifies sc clusters using hierarchical clustering... selects the most confidently predicted sample from each cluster"
- Break condition: If the embedding space is not discriminative enough, clustering may group dissimilar samples together or fail to separate them, weakening the diversity benefit.

### Mechanism 3
- Claim: Using discriminative embeddings for distance calculation captures class-relevant diversity better than raw features.
- Mechanism: Instead of using Euclidean distances in the original feature space, DCAST computes distances in the discriminative embedding space (e.g., hidden layer activations for neural networks or leaf distributions for random forests). This focuses diversity on samples that are distinct in the model's learned representation, which is more relevant for classification.
- Core assumption: The discriminative embedding preserves meaningful class separation and can be used to measure diversity relevant to the prediction task.
- Evidence anchors:
  - [section] "Diversity is assessed based on pairwise sample distances, calculated using a specific sample vector representation or embedding... Preferably, DCAST uses discriminative embeddings based on the learnt model M(i)"
  - [section] "For models without discriminative embeddings, such as SVM or LR, DCAST uses the original feature vector representation"
- Break condition: If the model is not trained well or the embedding is not discriminative, distances in this space may not reflect true diversity, leading to poor pseudo-label selection.

## Foundational Learning

- Concept: Selection bias
  - Why needed here: The paper's goal is to mitigate unfairness caused by unrepresentative training data; understanding how selection bias manifests is key to designing mitigation strategies.
  - Quick check question: What happens to model performance if the training set over-represents one class or feature subspace?

- Concept: Semi-supervised learning (self-training)
  - Why needed here: DCAST is a semi-supervised method that iteratively adds pseudo-labeled samples to the training set; knowing how self-training works is essential to grasp how DCAST modifies it.
  - Quick check question: In self-training, how are pseudo-labeled samples typically selected, and what is a common pitfall of this approach?

- Concept: Domain adaptation
  - Why needed here: The paper compares DCAST to domain adaptation methods that adapt models across distribution shifts; understanding these methods helps contextualize DCAST's novelty.
  - Quick check question: How do importance weighting and subspace alignment methods differ in their approach to handling distribution shift?

## Architecture Onboarding

- Component map: Labeled train set (XL, YL) -> Base model training -> Prediction on unlabeled set (XU) -> Class-aware sample selection -> Optional diversity clustering -> Pseudo-labeled set augmentation -> Repeat until stopping criteria -> Best model Mbest by validation performance

- Critical path:
  1. Train base model on labeled data
  2. Predict probabilities on unlabeled data
  3. Select sc × d high-confidence candidates per class
  4. If d > 1, cluster candidates and select one per cluster
  5. Add selected samples to labeled set
  6. Repeat until stopping criteria
  7. Return best model by validation performance

- Design tradeoffs:
  - Diversity strength d vs. convergence speed: Higher d increases diversity but may slow convergence or require more candidates
  - Confidence threshold t vs. sample inclusion: Higher t ensures high-quality pseudo-labels but may exclude useful samples
  - Model choice: Non-regularized models (e.g., neural nets) adapt more to unlabeled data, while regularized ones (e.g., RF, LR) may resist bias mitigation

- Failure signatures:
  - Class imbalance in pseudo-labeled set: Indicates class-awareness module not working
  - Low diversity in selected samples: Suggests clustering or embedding is not capturing meaningful differences
  - No improvement over supervised learning: Could mean regularization is too strong or unlabeled data is not helpful

- First 3 experiments:
  1. Run DCAST with d=1 (CAST) on a binary dataset with induced hierarchy bias; compare to supervised learning on biased and unbiased labeled sets
  2. Run DCAST with d=10 on the same setup; observe if diversity improves performance
  3. Run DCAST with d=100 and monitor cluster formation; check if it over-diversifies and hurts accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of diversity strength parameter d in DCAST affect its bias mitigation performance across different datasets and model types?
- Basis in paper: [explicit] The paper discusses that diversity strength d influences the extent of bias mitigation, with larger values resulting in improved robustness. However, the optimal value of d likely depends on the dataset characteristics and model type.
- Why unresolved: The paper only evaluates a limited set of diversity strength values (d={1,10,100}) and does not provide a comprehensive analysis of how d affects performance across different scenarios.
- What evidence would resolve it: A systematic evaluation of DCAST with a wider range of diversity strength values (e.g., d={1,2,5,10,20,50,100}) across various datasets and model types would provide insights into the optimal choice of d for different situations.

### Open Question 2
- Question: How does the use of regularization in models like RF and LR impact the effectiveness of DCAST in mitigating bias?
- Basis in paper: [explicit] The paper suggests that regularization could restrict model adaptation and limit the contribution of unlabeled samples in RF and LR models, potentially affecting the performance of DCAST.
- Why unresolved: The paper does not provide a detailed analysis of how regularization interacts with DCAST's bias mitigation strategies.
- What evidence would resolve it: A comparative study of DCAST's performance with and without regularization across different model types and datasets would shed light on the impact of regularization on bias mitigation.

### Open Question 3
- Question: How does DCAST's performance compare to other semi-supervised learning methods specifically designed for bias mitigation?
- Basis in paper: [inferred] The paper compares DCAST to conventional self-training and several domain adaptation techniques, but does not include other semi-supervised learning methods that are explicitly designed for bias mitigation.
- Why unresolved: The comparison with existing semi-supervised learning methods would provide a more comprehensive understanding of DCAST's strengths and weaknesses in the context of bias mitigation.
- What evidence would resolve it: An evaluation of DCAST against other semi-supervised learning methods designed for bias mitigation, such as those mentioned in the introduction (e.g., P3SVM, Co-training), would provide valuable insights into its relative performance.

## Limitations
- Hyperparameter values for neural network architecture and logistic regression regularization are unspecified
- Clustering implementation details in the diversity module are underspecified
- Evaluation focuses on accuracy metrics without fairness-specific metrics
- Lacks ablation studies to isolate contributions of class-awareness versus diversity modules

## Confidence
- High confidence: DCAST outperforms conventional self-training across datasets
- Medium confidence: Diversity module improves performance for higher-dimensional datasets
- Medium confidence: DCAST is model-agnostic and works best with neural networks

## Next Checks
1. Perform ablation studies to quantify the individual contributions of class-awareness and diversity modules to overall performance.
2. Evaluate DCAST using fairness-specific metrics (e.g., demographic parity, equal opportunity) in addition to accuracy to directly measure bias mitigation.
3. Test DCAST on datasets with known severe class imbalance to assess robustness when class_ratio(c) approaches zero for minority classes.