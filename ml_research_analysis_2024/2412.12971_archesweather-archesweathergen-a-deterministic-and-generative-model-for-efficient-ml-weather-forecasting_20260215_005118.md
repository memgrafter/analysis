---
ver: rpa2
title: 'ArchesWeather & ArchesWeatherGen: a deterministic and generative model for
  efficient ML weather forecasting'
arxiv_id: '2412.12971'
source_url: https://arxiv.org/abs/2412.12971
tags:
- deterministic
- weather
- ensemble
- archesweathergen
- lead
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a two-stage methodology to leverage deterministic
  ML weather models for efficient generative probabilistic weather forecasting. First,
  it introduces ArchesWeather, a transformer-based deterministic model that improves
  upon Pangu-Weather by replacing 3D local attention with global cross-level attention,
  achieving competitive RMSE scores with much lower computational budget.
---

# ArchesWeather & ArchesWeatherGen: a deterministic and generative model for efficient ML weather forecasting

## Quick Facts
- arXiv ID: 2412.12971
- Source URL: https://arxiv.org/abs/2412.12971
- Authors: Guillaume Couairon; Renu Singh; Anastase Charantonis; Christian Lessig; Claire Monteleoni
- Reference count: 23
- Key outcome: ArchesWeatherGen achieves state-of-the-art probabilistic weather forecasting with 45 V100-days training and 1 minute per 15-day ensemble on A100 GPU

## Executive Summary
This paper presents a two-stage methodology for efficient probabilistic weather forecasting. First, ArchesWeather improves upon Pangu-Weather by replacing 3D local attention with global cross-level attention, achieving competitive RMSE scores with significantly lower computational cost. Second, ArchesWeatherGen trains a flow-matching diffusion model on residuals (ERA5 minus ArchesWeather predictions), producing physically-consistent ensemble forecasts that surpass IFS ENS and NeuralGCM on WeatherBench headline variables in probabilistic metrics while requiring minimal training resources.

## Method Summary
The methodology follows a two-stage approach: (1) Train a deterministic ArchesWeather transformer with cross-level attention to predict expected weather states using ERA5 data (1979-2018 training), and (2) Compute normalized residuals between ERA5 and deterministic predictions, then train a flow-matching model on these residuals to generate ensemble members. The generative model is fine-tuned on out-of-distribution 2019 data and uses noise scaling to mitigate underdispersion caused by deterministic model overfitting. The final forecast combines deterministic predictions with sampled residuals to produce probabilistic ensemble forecasts.

## Key Results
- ArchesWeather achieves competitive deterministic RMSE with much lower computational budget than Pangu-Weather
- ArchesWeatherGen surpasses IFS ENS and NeuralGCM on all WeatherBench headline variables (except NeuralGCM's geopotential) in probabilistic metrics
- Training requires only ~45 V100-days and generates 15-day trajectories at 1 minute per member on A100 GPU
- Ensemble forecasts maintain physical consistency while providing calibrated uncertainty estimates

## Why This Works (Mechanism)

### Mechanism 1
Cross-Level Attention (CLA) improves forecasting skill by allowing vertical interactions across all pressure levels. CLA uses column-wise attention along the vertical dimension, enabling information sharing between distant atmospheric layers, unlike local 3D attention which restricts interactions to neighboring levels. This works because atmospheric phenomena at different pressure levels can benefit from direct vertical information exchange rather than being limited to local interactions. If vertical atmospheric interactions are indeed local and short-range, CLA would add unnecessary computational overhead without performance gains.

### Mechanism 2
Residual modeling with flow matching improves ensemble forecasting by learning to generate physically consistent perturbations. A deterministic model first predicts the expected weather state, then a generative flow matching model learns to generate residuals (differences between true states and predictions), creating ensemble members that maintain physical consistency. This works because the residual distribution is simpler and more amenable to generative modeling than modeling the full weather state distribution. If residuals are not simpler than full states or if the deterministic model's predictions are too biased, the generative model cannot learn meaningful perturbations.

### Mechanism 3
OOD fine-tuning and noise scaling mitigate underdispersion caused by overfitting in the deterministic model. Fine-tuning the generative model on out-of-distribution data (year 2019) and scaling the input noise variance during sampling helps match the residual distribution seen at test time. This works because overfitting in the deterministic model causes residuals at test time to have higher variance than training residuals, leading to underdispersed ensembles. If the deterministic model does not overfit or if the test distribution differs significantly from the training distribution, these strategies may not improve ensemble dispersion.

## Foundational Learning

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: The paper builds on Swin Transformers and modifies attention patterns for weather forecasting.
  - Quick check question: How does local 3D attention differ from global cross-level attention in terms of receptive field and parameter count?

- Concept: Diffusion models and flow matching
  - Why needed here: The generative component uses flow matching, a variant of diffusion models, to learn the residual distribution.
  - Quick check question: What is the key difference between denoising in DDPM and flow matching in terms of the noise schedule and training objective?

- Concept: Probabilistic forecasting metrics (CRPS, Brier Score, spread-skill ratio)
  - Why needed here: The paper evaluates generative models using ensemble metrics that assess both accuracy and dispersion.
  - Quick check question: Why is a spread-skill ratio of 1 considered ideal for ensemble forecasts?

## Architecture Onboarding

- Component map:
  ERA5 data (normalized) -> ArchesWeather deterministic model (Cross-Level Attention) -> Deterministic prediction -> Residual computation -> Flow matching model (denoising) -> Sampled residuals -> Ensemble forecast (deterministic + residuals)

- Critical path:
  1. Preprocess ERA5 data to 1.5Â° resolution and normalize
  2. Train deterministic ArchesWeather model (MSE loss, 24h lead time)
  3. Compute normalized residuals using deterministic predictions
  4. Train flow matching model on residuals (denoising loss)
  5. Generate ensemble forecasts via iterative denoising
  6. Evaluate using ensemble metrics (fCRPS, fES, Brier Score, spread-skill ratio)

- Design tradeoffs:
  - Local 3D attention vs. global Cross-Level Attention: Parameter efficiency vs. vertical information exchange
  - Deterministic + generative vs. pure generative: Training efficiency vs. potential for more complex distributions
  - OOD fine-tuning vs. more training data: Mitigation of overfitting vs. potential for better base model

- Failure signatures:
  - Underdispersion: Spread-skill ratio < 1, indicating ensemble members are too similar
  - Overfitting: Higher test RMSE than training RMSE in deterministic model
  - Physically inconsistent samples: High CRPS despite good Ensemble Mean RMSE

- First 3 experiments:
  1. Train ArchesWeather with and without Cross-Level Attention on a small subset of ERA5 to verify vertical interaction benefits.
  2. Train a flow matching model on synthetic residuals (e.g., Gaussian noise added to deterministic predictions) to validate the residual modeling approach.
  3. Evaluate the impact of OOD fine-tuning by training the generative model with and without 2019 data and comparing ensemble dispersion metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does training deterministic models on less data to improve generative model performance lead to better overall forecast skill?
- Basis in paper: [inferred] from discussion of overfitting and data allocation trade-offs in section 4.6
- Why unresolved: The paper acknowledges this as a potential optimization but does not experimentally explore different data splits between deterministic and generative model training.
- What evidence would resolve it: Systematic experiments varying the training data allocation between deterministic and generative models, measuring impact on both deterministic RMSE and ensemble metrics.

### Open Question 2
- Question: Would fine-tuning the generative model on its own auto-regressive rollouts improve multi-step forecast consistency compared to the Markovian assumption?
- Basis in paper: [inferred] from discussion of Markovian approximation limitations and multi-step fine-tuning effects in section 4.6
- Why unresolved: The paper found multi-step fine-tuning of deterministic models degraded generative performance, but suggests fine-tuning the generative model itself could help, without testing this approach.
- What evidence would resolve it: Experiments comparing generative model performance with and without fine-tuning on its own rollouts, measuring metrics like spread-skill ratio and trajectory consistency at longer lead times.

### Open Question 3
- Question: Would using initial condition perturbations in addition to our noise scaling approach further improve ensemble dispersion and forecast skill?
- Basis in paper: [explicit] from discussion of IC perturbations in section 4.6 comparing to GenCast methodology
- Why unresolved: The paper chose not to use IC perturbations, showing our noise scaling and OOD fine-tuning solved under-dispersion, but acknowledges IC perturbations could provide additional uncertainty information.
- What evidence would resolve it: Experiments adding IC perturbations to our generative model and comparing ensemble metrics (spread-skill ratio, CRPS) against our current approach with noise scaling.

## Limitations
- The novel Cross-Level Attention mechanism lacks direct ablation studies comparing it against alternative vertical attention designs
- The effectiveness of residual modeling depends critically on the assumption that deterministic model residuals are simpler to model than full distributions
- OOD fine-tuning addresses underdispersion but may introduce other biases that aren't fully characterized

## Confidence

- **High confidence** in deterministic forecasting results (RMSE comparisons against Pangu-Weather and other baselines are well-established)
- **Medium confidence** in the two-stage generative methodology, as the residual modeling approach is novel and lacks extensive ablation studies
- **Medium confidence** in ensemble metrics, though the paper acknowledges the challenge of evaluating generative weather models

## Next Checks

1. **Ablation Study**: Systematically compare Cross-Level Attention against local 3D attention with varying window sizes and against alternative global attention mechanisms to isolate the specific contribution of vertical interactions.

2. **Residual Complexity Analysis**: Quantitatively compare the complexity of modeling full weather states versus residuals by training flow matching models directly on full states and measuring training stability, convergence speed, and final performance.

3. **Long-horizon Ensemble Consistency**: Evaluate ensemble forecasts beyond 24 hours (e.g., 5-15 day trajectories) to assess whether physical consistency is maintained over longer lead times and whether underdispersion worsens with forecast horizon.