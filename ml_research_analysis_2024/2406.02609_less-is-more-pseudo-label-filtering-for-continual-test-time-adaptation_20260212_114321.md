---
ver: rpa2
title: 'Less is More: Pseudo-Label Filtering for Continual Test-Time Adaptation'
arxiv_id: '2406.02609'
source_url: https://arxiv.org/abs/2406.02609
tags:
- thresholds
- domain
- pseudo-labels
- threshold
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continual test-time adaptation
  (CTTA), where a pre-trained model must adapt to a sequence of unknown target domains
  during testing without access to source data. The key issue is that existing methods
  using pseudo-labels for self-training suffer from noise accumulation over time,
  leading to poor adaptation.
---

# Less is More: Pseudo-Label Filtering for Continual Test-Time Adaptation

## Quick Facts
- arXiv ID: 2406.02609
- Source URL: https://arxiv.org/abs/2406.02609
- Reference count: 40
- Primary result: Proposed Pseudo Labeling Filter (PLF) achieves 14.8% error rate on CIFAR-10-C, outperforming state-of-the-art CTTA methods

## Executive Summary
This paper addresses the challenge of continual test-time adaptation (CTTA) where pre-trained models must adapt to unknown target domains during testing without source data access. The key problem is that existing pseudo-label based methods suffer from noise accumulation over time, degrading adaptation quality. The authors propose Pseudo Labeling Filter (PLF), which introduces adaptive thresholding principles and Class Prior Alignment to filter low-quality pseudo-labels and encourage diverse predictions across classes. PLF achieves significant improvements on CIFAR-10-C, CIFAR-100-C, and ImageNet-C datasets.

## Method Summary
PLF introduces three principles for adaptive pseudo-label filtering: low-level initialization, positive correlation to model confidence, and class-specific thresholds. The method uses Self-Adaptive Thresholding combining Exponential Moving Average (EMA) and Exponential Decay (ED) to adjust filtering thresholds based on confidence trends. Class Prior Alignment (CPA) computes class-specific weights to encourage diverse predictions and reduce error accumulation. The approach operates within a Mean Teacher framework, filtering pseudo-labels before they're used in self-training to maintain high-quality adaptation across continual domain shifts.

## Key Results
- Achieves 14.8% error rate on CIFAR-10-C (vs 16.2% baseline)
- Outperforms state-of-the-art CTTA methods on CIFAR-10-C, CIFAR-100-C, and ImageNet-C
- Demonstrates effectiveness of adaptive thresholding and class prior alignment
- Shows robust performance across 15 corruptions at 5 severity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive thresholding based on EMA and ED maintains stable pseudo-label quality during domain shifts
- Mechanism: EMA tracks recent confidence trends while ED adjusts thresholds when confidence drops across domains, ensuring thresholds stay positively correlated with model confidence
- Core assumption: Model confidence exhibits measurable fluctuations during domain adaptation that can be tracked and used to adjust filtering thresholds
- Evidence anchors:
  - [abstract] "Based on these principles, we design Self-Adaptive Thresholding to filter pseudo-labels."
  - [section 4.2] "We utilize Exponential Moving Average (EMA) Na et al. [2023] and Exponential Decay (ED) strategy on the confidence scores of unlabeled data to ensure a positive correlation between thresholds and the model confidence"

### Mechanism 2
- Claim: Class-specific thresholds improve pseudo-label filtering by accounting for varying difficulty across classes
- Mechanism: Computes separate EMA and ED thresholds for each class based on their individual confidence patterns, then normalizes them relative to the global threshold
- Core assumption: Different classes exhibit different learning difficulties and confidence patterns during domain adaptation
- Evidence anchors:
  - [abstract] "vary in different classes" as one of the three principles
  - [section 4.2] "Considering that the difficulty of class learning varies across domain transformations, we propose a class-balanced regularization objective"

### Mechanism 3
- Claim: Class Prior Alignment encourages diverse predictions across all classes to reduce error accumulation
- Mechanism: Computes class-specific weights based on the ratio between pseudo-label distribution and model predictions, then uses this to regularize the loss function
- Core assumption: Imbalanced pseudo-label distribution across classes contributes to error accumulation during adaptation
- Evidence anchors:
  - [abstract] "we introduce a Class Prior Alignment (CPA) method to encourage the model to make diverse predictions for unknown domain samples"
  - [section 4.3] "CPA encourages assigning larger weights to predictions with fewer pseudo-labels and smaller weights to predictions with more pseudo-labels"

## Foundational Learning

- Concept: Test-Time Adaptation (TTA) and its limitations
  - Why needed here: The paper builds on TTA by extending it to continual scenarios (CTTA), so understanding the baseline is crucial
  - Quick check question: What is the key difference between TTA and CTTA, and why does this difference matter for pseudo-label filtering?

- Concept: Pseudo-labeling and its challenges in domain adaptation
  - Why needed here: The entire method is built around improving pseudo-label quality, so understanding how pseudo-labels work and their typical failure modes is essential
  - Quick check question: Why do pseudo-labels tend to accumulate errors during continual adaptation, and how does this impact model performance?

- Concept: Mean Teacher framework and self-training
  - Why needed here: The method uses a Mean Teacher structure for adaptation, so understanding this framework is necessary to grasp how the filtering integrates
  - Quick check question: How does the Mean Teacher framework work, and what role do pseudo-labels play in its self-training process?

## Architecture Onboarding

- Component map: Pre-trained model (student) -> EMA-based teacher model -> Self-Adaptive Thresholding module -> Pseudo-label filtering -> Class Prior Alignment module -> Loss computation -> Student model update

- Critical path:
  1. Teacher generates predictions on unlabeled data
  2. Self-Adaptive Thresholding computes adaptive thresholds
  3. Pseudo-labels are filtered based on thresholds
  4. Class Prior Alignment computes class weights
  5. Losses are computed and combined
  6. Student model is updated

- Design tradeoffs:
  - Fixed vs. adaptive thresholds: Fixed thresholds are simpler but less effective across varying domains; adaptive thresholds require more computation but maintain better performance
  - Global vs. class-specific thresholds: Global thresholds are computationally cheaper but may miss class-specific patterns; class-specific thresholds provide better granularity but increase complexity

- Failure signatures:
  - Thresholds become too conservative, filtering out too many potentially correct pseudo-labels
  - Thresholds become too lenient, allowing noisy pseudo-labels to pass through
  - Class weights become imbalanced, favoring certain classes over others
  - EMA/ED parameters are poorly tuned, leading to threshold instability

- First 3 experiments:
  1. Test the method on CIFAR-10-C with a fixed threshold baseline to verify that adaptive thresholding improves performance
  2. Evaluate the impact of removing Class Prior Alignment to quantify its contribution to overall performance
  3. Test the method on a more challenging dataset (CIFAR-100-C or ImageNet-C) to verify scalability and robustness to increased class complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Pseudo Labeling Filter (PLF) method perform in more complex datasets with a larger number of categories, and what are the limitations of the method in such scenarios?
- Basis in paper: [inferred] The paper mentions that the effectiveness of PLF may decrease in more complex datasets with a larger number of categories due to the model's reduced ability to learn about category confidence
- Why unresolved: The paper does not provide experimental results or a detailed analysis of PLF's performance on datasets with a large number of categories
- What evidence would resolve it: Experimental results comparing PLF's performance on datasets with varying numbers of categories, along with an analysis of the relationship between dataset complexity and PLF's effectiveness

### Open Question 2
- Question: What are the potential implications of using an initially low threshold in the Pseudo Labeling Filter (PLF) method, and how does this approach affect the method's performance over extended periods of continual test-time adaptation?
- Basis in paper: [explicit] The paper states that the role of an initially low threshold acts only with forward continuous domains and becomes more limited as the continual test-time adaptation (CTTA) process lengthens
- Why unresolved: The paper does not provide a detailed analysis of the long-term effects of using an initially low threshold in PLF
- What evidence would resolve it: Long-term experimental results demonstrating the effects of using an initially low threshold in PLF over extended periods of CTTA, along with an analysis of the trade-offs between short-term and long-term performance

### Open Question 3
- Question: How does the Pseudo Labeling Filter (PLF) method compare to other state-of-the-art methods in terms of computational efficiency, and what are the trade-offs between performance and computational cost?
- Basis in paper: [inferred] The paper mentions that the proposed method outperforms state-of-the-art approaches on CIFAR-10-C, CIFAR-100-C, and ImageNet-C datasets, but does not provide a detailed comparison of computational efficiency
- Why unresolved: The paper does not provide a detailed analysis of the computational efficiency of PLF compared to other methods
- What evidence would resolve it: A comprehensive comparison of the computational efficiency of PLF and other state-of-the-art methods, including runtime analysis and memory usage, along with an evaluation of the trade-offs between performance and computational cost

## Limitations

- Limited evaluation on real-world domain shift scenarios beyond synthetic corruption datasets
- No detailed analysis of computational overhead compared to baseline methods
- Paper does not provide ablation studies on the relative importance of each filtering principle
- Performance may degrade in datasets with very large number of categories

## Confidence

- High confidence in the general effectiveness of adaptive thresholding
- Medium confidence in the specific mechanisms of Class Prior Alignment
- Low confidence in long-term error accumulation behavior beyond tested corruption sequences

## Next Checks

1. **Ablation Study Validation**: Remove each component (EMA, ED, CPA) individually and test on CIFAR-10-C to verify their individual contributions to performance improvements
2. **Real-World Domain Shift Test**: Evaluate the method on a dataset with naturally occurring domain shifts (e.g., Office-31 or DomainNet) rather than synthetic corruptions to assess practical applicability
3. **Long-Term Error Accumulation Analysis**: Run adaptation for extended sequences (100+ corruption severity levels) to test whether the filtering mechanism prevents error accumulation over very long adaptation periods