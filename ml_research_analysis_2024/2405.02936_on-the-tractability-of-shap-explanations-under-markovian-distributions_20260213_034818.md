---
ver: rpa2
title: On the Tractability of SHAP Explanations under Markovian Distributions
arxiv_id: '2405.02936'
source_url: https://arxiv.org/abs/2405.02936
tags:
- shap
- language
- markovian
- markov
- given
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the computational complexity of SHAP explanations
  under Markovian feature distributions. The authors show that computing SHAP scores
  for weighted automata and disjoint DNFs (which includes decision trees) is tractable
  under the Markovian assumption, in contrast to previous results showing intractability
  under feature independence assumptions.
---

# On the Tractability of SHAP Explanations under Markovian Distributions

## Quick Facts
- arXiv ID: 2405.02936
- Source URL: https://arxiv.org/abs/2405.02936
- Reference count: 40
- Key outcome: Computing SHAP scores for weighted automata and disjoint DNFs is tractable under Markovian feature distributions, in contrast to intractability under feature independence assumptions

## Executive Summary
This paper establishes polynomial-time computability of SHAP (SHapley Additive ex Planations) scores for weighted automata and disjoint DNFs (including decision trees) under Markovian feature distributions. Previous work showed SHAP computation to be intractable under feature independence assumptions, but this paper demonstrates that a more structured Markovian assumption enables efficient computation. The key insight is reformulating SHAP score calculation as operations over languages and seq2seq languages represented by weighted automata and weighted transducers. This enables the use of efficient algorithms for these operations, leading to polynomial-time computation of SHAP scores for the specified model classes.

## Method Summary
The paper provides constructive algorithmic proofs showing that SHAP scores can be computed in polynomial time for weighted automata and disjoint DNFs under Markovian distributions. The method involves reformulating SHAP score calculation as a sequence of operations (product, partition constant, projection) over languages and seq2seq languages, then constructing weighted automata and weighted transducers that implement these operations efficiently. For weighted automata, the algorithm constructs three key machines: one representing the model, one computing the seq2seq language for probability distributions, and one for feature attribution. For disjoint DNFs, the approach reduces the problem to weighted automata through sequentialization of boolean variables.

## Key Results
- SHAP computation for weighted automata under Markovian distributions is in FP (deterministic polynomial time)
- SHAP computation for disjoint DNFs under Markovian distributions is in FP through reduction to weighted automata
- The tractability results extend beyond the restrictive feature independence assumption that previously limited SHAP computation
- Polynomial-time algorithms are provided for the key operations needed in SHAP computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Computing SHAP scores for weighted automata and disjoint DNFs is tractable under Markovian feature distributions.
- Mechanism: The paper constructs polynomial-time algorithms that implement key operations (product, partition constant, projection) on languages/seq2seq languages represented by weighted automata and weighted transducers. These operations are then composed to compute SHAP scores efficiently.
- Core assumption: The feature distribution is Markovian (i.e., each feature's distribution depends only on the previous feature), and the distribution is polynomial-time computable.
- Evidence anchors:
  - [abstract]: "We show that, under the Markovian assumption, computing the SHAP score for the class of Weighted automata, Disjoint DNFs and Decision Trees can be performed in polynomial time"
  - [section 2.2]: Defines Markovian distributions and polynomial-time computability
  - [corpus]: Weak evidence - related papers focus on computational complexity but don't directly address Markovian distributions
- Break condition: If the feature distribution is not Markovian, or if the distribution parameters cannot be efficiently queried, the tractability result no longer holds.

### Mechanism 2
- Claim: The SHAP score formula can be reformulated in terms of operations over languages/seq2seq languages.
- Mechanism: The paper reformulates the SHAP score calculation as a sequence of operations (product, partition constant, projection) over languages/seq2seq languages. This reformulation enables the use of efficient algorithms for these operations.
- Core assumption: The reformulation accurately represents the original SHAP score calculation.
- Evidence anchors:
  - [section 3.1]: Provides the reformulation of SHAP1 and SHAP2 in terms of language operators
  - [section 3.2.1]: Lemma 3.4 provides the formal reformulation
  - [corpus]: Weak evidence - related papers focus on SHAP computation but don't address this specific reformulation
- Break condition: If the reformulation introduces errors or approximations, the tractability result may not hold for the original SHAP score.

### Mechanism 3
- Claim: Weighted automata and weighted transducers can be constructed in polynomial time to represent the required languages/seq2seq languages.
- Mechanism: The paper provides constructive algorithms to build weighted automata and weighted transducers that represent the languages/seq2seq languages needed for SHAP score computation. These constructions leverage the structure of Markovian distributions and the properties of weighted automata/transducers.
- Core assumption: The weighted automata and weighted transducers can be constructed efficiently, and their size remains polynomial in the input instance.
- Evidence anchors:
  - [section 3.2.2]: Lemma 3.5 provides the constructive algorithms for building the required machines
  - [section 3.2.2]: Discusses the construction of weighted automata for Markovian distributions
  - [corpus]: Weak evidence - related papers focus on SHAP computation but don't address this specific construction
- Break condition: If the construction algorithms are not efficient, or if the resulting machines are too large, the tractability result may not hold.

## Foundational Learning

- Concept: Weighted Automata (WAs) and Weighted Transducers (WTs)
  - Why needed here: The paper uses WAs and WTs to represent languages and seq2seq languages, which are then used to compute SHAP scores efficiently.
  - Quick check question: Can you explain the difference between a weighted automaton and a weighted transducer, and give an example of when each would be used?

- Concept: Markovian Distributions
  - Why needed here: The tractability result relies on the assumption that the feature distribution is Markovian, meaning each feature's distribution depends only on the previous feature.
  - Quick check question: What is the formal definition of a Markovian distribution, and how does it differ from an independent feature distribution?

- Concept: Shapley Value and SHAP Score
  - Why needed here: The paper is concerned with computing the SHAP score, which is based on the Shapley value from cooperative game theory. Understanding the SHAP score is crucial for understanding the problem being solved.
  - Quick check question: Can you explain the intuition behind the Shapley value and how it relates to feature importance in machine learning models?

## Architecture Onboarding

- Component map:
  - Weighted Automata (WAs) and Weighted Transducers (WTs): Represent languages and seq2seq languages
  - Markovian Distribution: The assumed feature distribution
  - SHAP Score Calculation: The target computation, reformulated in terms of language operations
  - Language Operations: Product, partition constant, and projection operators implemented efficiently for WAs/WTs

- Critical path:
  1. Construct WAs/WTs representing the required languages/seq2seq languages
  2. Apply the language operations (product, partition constant, projection) to compute the SHAP score
  3. Combine the results to obtain the final SHAP score

- Design tradeoffs:
  - Using WAs/WTs allows for efficient computation of the SHAP score under the Markovian assumption, but may not generalize to other distributions
  - The reformulation of the SHAP score in terms of language operations is elegant but may be less intuitive than the original formulation

- Failure signatures:
  - If the Markovian assumption is violated, the tractability result no longer holds
  - If the weighted automata/transducers cannot be constructed efficiently, the algorithm may not run in polynomial time
  - If the language operations are not implemented correctly, the computed SHAP score may be incorrect

- First 3 experiments:
  1. Implement a simple weighted automaton and verify that it correctly represents a given language
  2. Implement the product, partition constant, and projection operators for weighted automata and verify their correctness
  3. Construct a weighted automaton representing a Markovian distribution and verify that it can be used to compute SHAP scores efficiently

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the tractability results for SHAP under Markovian distributions be extended to higher-order Markov models (n-gram models) beyond the 1st-order Markovian distributions considered in this paper?
- Basis in paper: Explicit - The paper states in the conclusion that "by revisiting algorithms designed to generate WTs that compute the seq2seq languages g(1)w,P, g(2)w,i,P (lemma 3.5), the algorithmic construction described in this article can be easily extended to adapt to higher-order markovian distributions, e.g. n-gram models (Fink, 2014), provided the order of the distribution is of reasonably small size."
- Why unresolved: The paper only mentions this as a possibility for future work without providing concrete algorithms or complexity bounds for the higher-order case.
- What evidence would resolve it: A formal proof showing that SHAP(WA, MARKOV) remains in FP for higher-order Markovian distributions, along with an explicit construction of the required weighted automata/transducers for the higher-order case.

### Open Question 2
- Question: Can the tractability of SHAP computation be extended to other model classes beyond weighted automata and disjoint DNFs under the Markovian assumption?
- Basis in paper: Explicit - The paper states in the conclusion "In feature research, we aim at exploring the possibility to extend the tractability of SHAP explanations for other families of models under the Markovian assumption. An interesting family to be considered as a natural extension is the class of Deterministic Decomposable Circuits whose SHAP score computation under the feature independence assumption has been proven to be in FP (Arenas et al., 2023)."
- Why unresolved: While the paper identifies Deterministic Decomposable Circuits as a potential next target, it does not provide any analysis or results for this or other model classes.
- What evidence would resolve it: A formal proof establishing the complexity of SHAP computation for Deterministic Decomposable Circuits (or other model classes) under Markovian distributions, along with the construction of efficient algorithms if the problem is tractable.

### Open Question 3
- Question: How do the SHAP explanations computed under the Markovian assumption compare in terms of faithfulness and utility to those computed under feature independence or other distributional assumptions?
- Basis in paper: Inferred - The paper focuses on establishing computational tractability of SHAP under Markovian distributions but does not address the quality or usefulness of the resulting explanations compared to other approaches.
- Why unresolved: The paper is primarily theoretical and does not include any empirical evaluation or comparison of the explanations generated under different assumptions.
- What evidence would resolve it: An empirical study comparing the explanations generated by SHAP under Markovian, feature independence, and other distributional assumptions across various real-world datasets and model classes, measuring metrics like faithfulness, stability, and utility for model debugging and feature selection.

## Limitations

- The tractability results are strictly limited to Markovian distributions, which represent a narrow subset of possible feature distributions
- The constructive algorithms for weighted transducers are described at a high level without complete formal specifications
- The reduction from d-DNF to WA SHAP computation assumes specific encoding schemes that may not generalize to all model classes

## Confidence

- High confidence in the theoretical framework and reduction proofs for the stated model classes under Markovian distributions
- Medium confidence in the practical implementability of the weighted transducer constructions due to incomplete algorithmic details
- Low confidence in the extensibility of these results to non-Markovian distributions or more complex model classes

## Next Checks

1. **Algorithmic Completeness Verification**: Implement the complete algorithm for computing SHAP scores for weighted automata under Markovian distributions and verify it runs in polynomial time for increasing input sizes, measuring against the stated complexity bound O(poly(size(A), |Î£|, |w|))

2. **Distribution Generalizability Test**: Systematically evaluate the algorithm's behavior when the Markovian assumption is slightly violated (e.g., using k-order Markov distributions with small k) to quantify the sensitivity to the distributional assumption

3. **Cross-Model Class Validation**: Implement the d-DNF to WA reduction and verify that SHAP scores computed through the WA representation match those computed through direct enumeration for small, verifiable d-DNF instances