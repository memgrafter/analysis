---
ver: rpa2
title: 'Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large
  Language Models Using Only Attention Maps'
arxiv_id: '2407.07071'
source_url: https://arxiv.org/abs/2407.07071
tags:
- lookback
- lens
- decoding
- heads
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses contextual hallucinations in large language
  models, where models generate content inconsistent with provided input context despite
  having access to correct facts. The authors propose Lookback Lens, a method that
  detects hallucinations by analyzing attention weights between context tokens and
  newly generated tokens.
---

# Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps

## Quick Facts
- arXiv ID: 2407.07071
- Source URL: https://arxiv.org/abs/2407.07071
- Authors: Yung-Sung Chuang; Linlu Qiu; Cheng-Yu Hsieh; Ranjay Krishna; Yoon Kim; James Glass
- Reference count: 29
- Primary result: Attention-based hallucination detection method that transfers across models without retraining

## Executive Summary
This paper addresses contextual hallucinations in large language models, where models generate content inconsistent with provided input context despite having access to correct facts. The authors propose Lookback Lens, a method that detects hallucinations by analyzing attention weights between context tokens and newly generated tokens. Specifically, they compute a lookback ratio for each attention head, measuring the ratio of attention on context versus generated tokens. A linear classifier trained on these features achieves comparable performance to richer detectors using hidden states or text-based entailment models. The method transfers across tasks and models, enabling a detector trained on a 7B model to work on a 13B model without retraining. Applied to mitigation, Lookback Lens Guided Decoding reduces hallucinations by up to 9.6% on the XSum summarization task. The approach leverages attention maps for interpretability and model transferability.

## Method Summary
Lookback Lens extracts attention weights from transformer models during generation and computes lookback ratios that measure the proportion of attention allocated to context tokens versus newly generated tokens. These ratios are calculated for each attention head and layer, forming feature vectors that are fed into a logistic regression classifier to predict hallucination probability. The method is trained on annotated datasets (CNN/DM for summarization, Natural Questions for QA) where responses are labeled as hallucinated or non-hallucinated using GPT-4o. For cross-model transfer, a linear regression model maps attention heads from larger models to smaller ones based on training data from the smaller model. The mitigation approach samples multiple chunk candidates at each decoding step and selects the one with the lowest predicted hallucination probability.

## Key Results
- Linear classifier on lookback ratio features achieves AUROC above 0.85 on both CNN/DM and Natural Questions datasets
- Lookback Lens transfers from 7B to 13B models without retraining, demonstrating cross-model applicability
- Lookback Lens Guided Decoding reduces hallucinations by up to 9.6% on XSum summarization task
- Outperforms text-based entailment models while being simpler than methods using full hidden states

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextual hallucinations are detectable by analyzing the ratio of attention weights on context tokens versus newly generated tokens.
- Mechanism: For each attention head at each decoding step, compute lookback ratio as the proportion of attention allocated to context tokens versus generated tokens. Hallucinations correlate with lower lookback ratios.
- Core assumption: Attention maps reflect the model's grounding in provided context, and deviations from this grounding manifest as reduced attention to context tokens.
- Evidence anchors:
  - [abstract] "We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations."
  - [section 2.1] "We propose a simple feature called lookback ratio, which is computed as the ratio of attention weights on the given context versus the newly generated tokens."
  - [corpus] Weak - no direct citations about attention ratios for hallucination detection found in related papers.
- Break condition: If attention weights become uniformly distributed across all tokens regardless of context relevance, or if the model relies heavily on position-based attention patterns rather than content-based ones.

### Mechanism 2
- Claim: A linear classifier on lookback ratio features achieves comparable performance to richer detectors using hidden states or text-based entailment models.
- Mechanism: Concatenate lookback ratios across all heads and layers to form feature vectors, then train a logistic regression classifier to predict hallucination probability.
- Core assumption: The distribution of lookback ratios across heads and layers contains sufficient information to distinguish hallucinated from factual content.
- Evidence anchors:
  - [abstract] "We find that a linear classifier based on these lookback ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model."
  - [section 2.3] "We find that the Lookback Lens achieves slightly better performance than the hidden states-based classifier and significantly outperforms the NLI models."
  - [corpus] Weak - while related work uses hidden states, this specific claim about attention-based simplicity outperforming complexity is novel.
- Break condition: If hallucination patterns are encoded in non-attention-based mechanisms like MLP activations or residual connections, or if the hallucination type changes such that attention patterns no longer correlate with factuality.

### Mechanism 3
- Claim: Lookback Lens can transfer across models without retraining by mapping attention head spaces between different model sizes.
- Mechanism: Use linear regression to map attention heads from larger model (13B) to smaller model (7B) based on training data from the smaller model.
- Core assumption: Attention head behaviors are sufficiently correlated between models of different sizes that a linear mapping can preserve the hallucination detection capability.
- Evidence anchors:
  - [abstract] "The lookback ratio-based detector -- Lookback Lens -- is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model."
  - [section 4] "Since the total numbers of attention heads are different in 7B and 13B models... we use a linear regression model to map the heads from the 13B model to the heads in 7B model."
  - [corpus] Weak - cross-model transfer using attention maps is not well-established in the literature.
- Break condition: If the attention mechanisms fundamentally differ between models (e.g., different attention patterns due to architectural changes), or if the hallucination generation process is too model-specific for the mapping to generalize.

## Foundational Learning

- Concept: Transformer attention mechanism and multi-head attention
  - Why needed here: The entire method relies on extracting and interpreting attention weights from transformer models
  - Quick check question: What is the shape of the attention matrix output by a single attention head for a sequence of length N?

- Concept: Logistic regression and linear classification
  - Why needed here: The Lookback Lens uses a simple linear classifier on extracted features
  - Quick check question: What is the mathematical form of the decision boundary in logistic regression?

- Concept: Cross-model transfer learning
  - Why needed here: The method claims to work across different model sizes without retraining
  - Quick check question: What are the key challenges in transferring detectors between models of different architectures?

## Architecture Onboarding

- Component map: Data preprocessing -> Attention extraction -> Lookback ratio computation -> Classifier prediction -> Candidate selection during decoding
- Critical path: Attention extraction → Lookback ratio computation → Classifier prediction → Candidate selection during decoding
- Design tradeoffs:
  - Simplicity vs performance: Using only attention maps vs hidden states
  - Transferability vs accuracy: Linear mapping vs retraining
  - Computational cost: Sampling multiple candidates vs greedy decoding
- Failure signatures:
  - Poor detection performance: Attention patterns don't correlate with hallucinations
  - Failed transfer: Linear mapping doesn't preserve detection capability
  - Slow decoding: Sampling too many candidates or high computational overhead
- First 3 experiments:
  1. Verify lookback ratios correlate with hallucination detection on small dataset
  2. Compare detection performance against baseline methods (hidden states, NLI)
  3. Test cross-model transfer by training on 7B and evaluating on 13B without retraining

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but based on the limitations and discussion, several implicit questions emerge:

### Open Question 1
- Question: Can the Lookback Lens approach be extended to detect hallucinations in zero-shot or few-shot scenarios where minimal training data is available?
- Basis in paper: [inferred] The paper discusses the transferability of the Lookback Lens across tasks and models, suggesting potential for broader applicability. However, it does not explicitly explore zero-shot or few-shot settings.
- Why unresolved: The paper focuses on scenarios where labeled data is available for training, but real-world applications often require detecting hallucinations without extensive annotated datasets. This leaves open the question of whether the attention-based features alone are sufficient for zero-shot detection.
- What evidence would resolve it: Experiments demonstrating the effectiveness of the Lookback Lens in zero-shot or few-shot settings, with comparisons to existing methods that do not require labeled data.

### Open Question 2
- Question: How does the Lookback Lens perform in detecting hallucinations in multimodal models that process both text and images?
- Basis in paper: [explicit] The paper focuses exclusively on text-based models and does not address multimodal scenarios. The attention mechanism described is specific to text tokens.
- Why unresolved: With the increasing prevalence of multimodal models, it is unclear whether the attention-based approach can be adapted to handle cross-modal attention patterns and detect hallucinations in image-text contexts.
- What evidence would resolve it: Results from applying the Lookback Lens to multimodal models, showing its ability to detect hallucinations in tasks that involve both text and images.

### Open Question 3
- Question: Can the Lookback Lens be integrated with other interpretability methods to provide a more comprehensive understanding of hallucination mechanisms?
- Basis in paper: [inferred] The paper highlights the interpretability of attention maps but does not explore integration with other interpretability techniques such as feature importance analysis or causal interventions.
- Why unresolved: While the Lookback Lens provides insights into attention patterns, combining it with other methods could reveal deeper mechanisms behind hallucinations and improve detection accuracy.
- What evidence would resolve it: Studies that combine the Lookback Lens with other interpretability methods, demonstrating enhanced detection performance or deeper insights into hallucination causes.

## Limitations

- Model dependency: The approach relies heavily on attention mechanisms that may not generalize across different transformer architectures
- Task generalization: Effectiveness may vary significantly across different task types and domains
- Computational overhead: Lookback Lens Guided Decoding requires sampling multiple candidates, potentially increasing inference time

## Confidence

- Detection Effectiveness: High confidence - experimental results show AUROC scores above 0.85 on benchmark datasets
- Cross-Model Transfer: Medium confidence - demonstrated on one model pair but needs validation across more diverse architectures
- Hallucination Mitigation: Medium confidence - modest improvements (up to 9.6%) show promise but may vary by task and parameters

## Next Checks

1. **Cross-Architecture Transfer Validation**: Test the Lookback Lens detector on models with fundamentally different attention mechanisms (e.g., Longformer, BigBird, or models with multi-query attention) to validate whether the attention-based detection generalizes beyond standard transformer architectures.

2. **Long-Context Performance Evaluation**: Evaluate detection and mitigation performance on tasks requiring long-context reasoning (e.g., book summarization, multi-document QA) to assess whether attention patterns remain informative for hallucination detection in extended contexts where attention may become less focused.

3. **Real-Time Application Feasibility**: Implement and benchmark Lookback Lens Guided Decoding in a real-time application setting, measuring both the improvement in factual accuracy and the computational overhead across different sequence lengths and candidate sampling strategies to determine practical deployment viability.