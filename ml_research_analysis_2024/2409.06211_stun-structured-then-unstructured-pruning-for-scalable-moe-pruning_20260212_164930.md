---
ver: rpa2
title: 'STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning'
arxiv_id: '2409.06211'
source_url: https://arxiv.org/abs/2409.06211
tags:
- pruning
- experts
- unstructured
- expert
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STUN (Structured-Then-Unstructured Pruning),
  a novel approach for pruning Mixture-of-Experts (MoE) models that outperforms both
  structured and unstructured pruning methods individually. The key insight is that
  MoE expert pruning can serve as an effective first phase, followed by unstructured
  pruning within individual experts, achieving higher overall sparsity with minimal
  performance loss.
---

# STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning

## Quick Facts
- arXiv ID: 2409.06211
- Source URL: https://arxiv.org/abs/2409.06211
- Reference count: 35
- Primary result: Achieves 40% sparsity on Snowflake Arctic with nearly no performance loss, requiring only one H100 GPU and two hours

## Executive Summary
This paper introduces STUN (Structured-Then-Unstructured Pruning), a novel approach for pruning Mixture-of-Experts (MoE) models that outperforms both structured and unstructured pruning methods individually. The key insight is that MoE expert pruning can serve as an effective first phase, followed by unstructured pruning within individual experts, achieving higher overall sparsity with minimal performance loss. STUN addresses the computational challenge of scaling MoE pruning by reducing expert pruning complexity from O(k·n/√n) to O(1) through behavioral similarity clustering based on router weights.

## Method Summary
STUN combines expert-level pruning with unstructured pruning to achieve higher sparsity in MoE models while maintaining performance. The method first identifies and removes redundant experts through a novel O(1) clustering algorithm that groups experts based on behavioral similarity using router weights. After expert pruning, unstructured pruning is applied within the remaining experts to achieve fine-grained sparsity. This two-phase approach preserves the high-kurtosis weight distribution needed for effective unstructured pruning while significantly reducing model size and inference cost.

## Key Results
- STUN achieves 40% sparsity on Snowflake Arctic (480B, 128 experts) with nearly no performance loss
- Maintains GSM8K accuracy where state-of-the-art unstructured pruning fails completely
- Shows 20-30 percentage point accuracy gaps over unstructured pruning alone on NLU tasks like ARC-challenge and HellaSwag at high sparsity levels
- Requires only one H100 GPU and two hours for pruning Snowflake Arctic
- Outperforms both structured pruning and unstructured pruning individually across multiple MoE models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert pruning reduces redundancy before unstructured pruning, preserving model robustness.
- Mechanism: Coarse pruning removes entire experts, maintaining the high-kurtosis weight distribution needed for effective unstructured pruning.
- Core assumption: Unstructured pruning works best when weight distributions have high kurtosis (many outliers).
- Evidence anchors:
  - [abstract] "Our distinction is to introduce a new class of pruning– structured-then-unstructured pruning– and demonstrate its significant advantages for MoEs, surpassing the performance of either method alone."
  - [section 4.3] "We argue that expert-level pruning does not reduce kurtosis, thereby preserving the network's resilience to unstructured pruning."
  - [corpus] Weak evidence - no direct citation found for kurtosis preservation claim in corpus.

### Mechanism 2
- Claim: The O(1) expert pruning algorithm approximates combinatorial expert selection without exhaustive search.
- Mechanism: Uses 1st-order Taylor approximation to rank experts within clusters, selecting representatives that minimize reconstruction loss.
- Core assumption: Experts within clusters have similar behavior, so selecting one representative captures cluster importance.
- Evidence anchors:
  - [section 4.2.3] "Therefore, the expert closest to θ̄i within each cluster has the highest priority to be retained."
  - [section 4.2.2] "Given cluster mapping c which maps an expert to a set of similarly behaving experts, we assign the value P(Ei|Sk−1)"
  - [corpus] Weak evidence - no direct citation found for Taylor approximation effectiveness in expert pruning.

### Mechanism 3
- Claim: STUN adapts better to MoEs with many small experts versus few large experts.
- Mechanism: More experts provide greater flexibility for removing redundant experts while maintaining performance.
- Core assumption: Small experts can be more specialized and redundant experts are easier to identify.
- Evidence anchors:
  - [section 5.2.3] "The performance gap between STUN and unstructured pruning increases as the MoE has more experts with small sizes (from (c) to (a))."
  - [section 4.2] "Our key contribution is drastically reducing the number of GPU calls to O(1), without compromising the performance."
  - [corpus] Moderate evidence - MoE-Inference-Bench paper confirms performance evaluation of MoE models but doesn't specifically address expert size adaptation.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how MoE layers select and combine experts is fundamental to understanding why pruning works differently than in dense models
  - Quick check question: How does an MoE layer decide which experts to activate for a given input token?

- Concept: Structured vs unstructured pruning tradeoffs
  - Why needed here: The paper's key contribution is combining both methods, so understanding their individual limitations is crucial
  - Quick check question: Why does unstructured pruning typically outperform structured pruning in dense models?

- Concept: Kurtosis and weight distribution properties
  - Why needed here: The paper argues that preserving kurtosis is key to successful unstructured pruning after expert pruning
  - Quick check question: How does high kurtosis in weight distributions relate to a model's ability to tolerate unstructured pruning?

## Architecture Onboarding

- Component map: Router layer -> Expert layers -> STUN pipeline (Expert pruning → Unstructured pruning) -> Clustering module
- Critical path:
  1. Router weights analysis to compute pairwise similarities
  2. Agglomerative clustering to group similar experts
  3. Expert pruning using Taylor approximation and selective reconstruction
  4. Unstructured pruning within remaining experts

- Design tradeoffs:
  - Expert size vs number: More small experts give more pruning flexibility but increase routing complexity
  - Clustering granularity: Finer clusters give better pruning but increase computation
  - Sparsity levels: Higher expert sparsity reduces model size but may hurt performance more

- Failure signatures:
  - Performance degradation on generative tasks (GSM8K) indicates expert pruning removed too many useful experts
  - Unstructured pruning failing after expert pruning suggests kurtosis was reduced too much
  - Clustering producing singleton clusters indicates similarity metric isn't capturing expert behavior

- First 3 experiments:
  1. Run clustering on a small MoE (8 experts) with different λ1, λ2 values to verify expert similarity detection
  2. Test expert pruning alone at 20% sparsity on Mixtral-8x7B to validate baseline performance
  3. Apply unstructured pruning after expert pruning and measure kurtosis change to verify robustness preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the behavior similarity between experts change across different layers and training stages in MoE models?
- Basis in paper: [explicit] The paper discusses leveraging behavioral similarity between experts through router weight analysis and clustering, but does not explore how this similarity evolves during training or varies across different layers.
- Why unresolved: The clustering approach relies on static router weights, but MoE models are dynamic with expert selection varying by input. Understanding temporal and spatial variations in expert behavior could improve clustering effectiveness.
- What evidence would resolve it: Analysis showing how expert behavioral similarity matrices change across training epochs and different layers, and whether dynamic clustering during training improves pruning performance.

### Open Question 2
- Question: What is the theoretical limit of combined structured-then-unstructured pruning for MoE models, and how does it scale with the number of experts?
- Basis in paper: [inferred] The paper demonstrates effectiveness up to 40-65% sparsity for models with 8-128 experts, but does not establish theoretical bounds or scaling properties as expert counts increase dramatically in modern MoEs.
- Why unresolved: While practical results are shown, there is no theoretical framework explaining why combining structured and unstructured pruning outperforms either alone, or how this relationship changes with expert count.
- What evidence would resolve it: A theoretical analysis connecting expert specialization, redundancy, and pruning limits, plus empirical validation across MoEs with varying expert counts (e.g., 8, 64, 128, 256, 512 experts).

### Open Question 3
- Question: How does STUN's performance on generative tasks compare to its performance on discriminative tasks, and what architectural factors contribute to this difference?
- Basis in paper: [explicit] The paper emphasizes GSM8K as particularly challenging for unstructured pruning while STUN maintains performance, but does not provide comprehensive analysis of why generative tasks are harder or what architectural properties affect this.
- Why unresolved: The paper shows STUN works well on GSM8K but doesn't explain the underlying mechanisms that make generative tasks more sensitive to pruning or how different MoE architectures affect this sensitivity.
- What evidence would resolve it: Detailed comparison of pruning sensitivity across multiple generative and discriminative tasks, with analysis of how expert specialization patterns and routing mechanisms differ between task types.

## Limitations

- Expert Pruning Approximation Quality: The O(1) expert pruning relies on behavioral clustering and Taylor approximation to avoid combinatorial search, but the approximation quality across diverse MoE architectures remains uncertain.
- Kurtosis Preservation Claims: The paper argues that expert pruning preserves weight distribution kurtosis necessary for effective unstructured pruning, but lacks direct empirical validation of this critical assumption.
- Generalization to Different MoE Architectures: The method is primarily validated on decoder-only models, with untested performance on encoder-decoder architectures or different routing mechanisms.

## Confidence

- High Confidence: STUN outperforms unstructured pruning alone on generative tasks (GSM8K) and NLU tasks at high sparsity levels (40%).
- Medium Confidence: The computational efficiency claim (O(1) vs O(k·n/√n)) is supported by the algorithmic description.
- Low Confidence: The theoretical justification for kurtosis preservation and its critical role in unstructured pruning success.

## Next Checks

1. Measure weight distribution kurtosis before and after expert pruning on multiple MoE architectures to verify the kurtosis preservation assumption.
2. Compare expert clustering results based on router weights versus actual activation patterns on held-out data to quantify approximation error.
3. Apply STUN to encoder-decoder MoE models and models with different routing mechanisms to test generalization beyond decoder-only architectures.