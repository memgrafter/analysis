---
ver: rpa2
title: 'Beyond Metrics: Evaluating LLMs'' Effectiveness in Culturally Nuanced, Low-Resource
  Real-World Scenarios'
arxiv_id: '2406.00343'
source_url: https://arxiv.org/abs/2406.00343
tags:
- sentiment
- llms
- negative
- message
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates seven leading large language models (LLMs)
  on sentiment analysis of a multilingual, code-mixed WhatsApp dataset containing
  Swahili, English, and Sheng. Models were assessed using F1 scores and qualitative
  analysis of their generated explanations.
---

# Beyond Metrics: Evaluating LLMs' Effectiveness in Culturally Nuanced, Low-Resource Real-World Scenarios

## Quick Facts
- arXiv ID: 2406.00343
- Source URL: https://arxiv.org/abs/2406.00343
- Reference count: 33
- Evaluated seven leading LLMs on sentiment analysis of multilingual WhatsApp messages containing Swahili, English, and Sheng

## Executive Summary
This study evaluates seven leading large language models (LLMs) on sentiment analysis of a multilingual, code-mixed WhatsApp dataset containing Swahili, English, and Sheng. The evaluation combines F1 scores with qualitative analysis of model explanations to assess performance in culturally nuanced, low-resource real-world scenarios. While Mistral-7b and Mixtral-8x7b achieved the highest F1 scores, all models except GPT-4 and GPT-4-Turbo struggled with linguistic and contextual nuances, particularly in non-English contexts. The findings demonstrate that quantitative metrics alone are insufficient for evaluating LLM performance in culturally diverse, low-resource settings, highlighting the need for human-centered evaluation approaches.

## Method Summary
The researchers evaluated seven LLMs including Mistral-7b, Mixtral-8x7b, and GPT variants on a dataset of 100 multilingual WhatsApp messages containing Swahili, English, and Sheng code-mixing. Models were assessed using standard F1 scores for sentiment analysis alongside qualitative analysis of their generated explanations. The evaluation framework combined quantitative metrics with human-centered approaches to examine model performance in culturally nuanced contexts. The study specifically focused on low-resource language settings and examined how well models handle linguistic mixing, cultural references, and contextual understanding in real-world communication scenarios.

## Key Results
- Mistral-7b and Mixtral-8x7b achieved highest F1 scores but struggled with linguistic and contextual nuances
- GPT-4 and GPT-4-Turbo demonstrated superior multilingual understanding and consistent predictions aligned with human expectations
- All models showed inconsistent handling of cultural nuances, even in English contexts

## Why This Works (Mechanism)
The study's mechanism for evaluating LLM effectiveness combines quantitative performance metrics with qualitative assessment of model explanations. This dual approach captures both statistical accuracy and the reasoning process behind model predictions, revealing limitations that pure metrics miss. By using a real-world, multilingual dataset with code-mixing, the evaluation tests models' ability to handle authentic communication patterns rather than idealized, monolingual text. The human-centered analysis component examines whether model explanations are relevant, coherent, and demonstrate understanding of cultural context, providing insights into the models' actual reasoning capabilities beyond numerical scores.

## Foundational Learning
- Multilingual sentiment analysis: Understanding how models handle multiple languages in single texts - needed to evaluate performance in code-mixed environments where languages blend naturally
- Cultural context in NLP: Recognizing cultural references, idioms, and social norms in text - needed because sentiment often depends on cultural understanding rather than literal meaning
- Code-mixing patterns: Identifying and processing language alternation within and across utterances - needed since real-world multilingual communication often involves fluid language switching
- Human-centered evaluation: Assessing model explanations for coherence and relevance - needed to move beyond metrics and understand model reasoning quality
- Low-resource language processing: Handling languages with limited training data - needed for evaluating performance in underrepresented linguistic contexts
- Qualitative HCI methods: Using human-centered approaches to evaluate AI systems - needed to capture nuanced aspects of model performance that quantitative metrics miss

## Architecture Onboarding
Component map: WhatsApp dataset -> Preprocessing -> LLM inference -> F1 score calculation -> Qualitative explanation analysis -> Human evaluation
Critical path: Dataset preparation → Model inference → Sentiment classification → Explanation generation → Human evaluation of explanations
Design tradeoffs: Quantitative metrics vs. qualitative understanding, computational cost vs. evaluation depth, model selection vs. comprehensive testing
Failure signatures: Inconsistent predictions across similar inputs, irrelevant or nonsensical explanations, inability to handle code-mixing, cultural misinterpretation
First experiments: 1) Run all models on a small validation subset, 2) Compare model explanations qualitatively, 3) Test model consistency on perturbed inputs

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size of only 100 multilingual messages may limit generalizability
- Subjective nature of assessing model explanations and cultural understanding introduces potential bias
- Focus on specific language combination (Swahili, English, Sheng) may not translate to other language pairs
- Computational resources and API access constraints influenced which models were tested

## Confidence
- GPT-4 superior multilingual understanding: Medium confidence (limited sample size, potential selection bias)
- All models struggle with cultural nuances: High confidence (consistent observation across methods)
- Quantitative metrics insufficient alone: High confidence (demonstrated value of combined approach)

## Next Checks
1. Expand dataset to 500+ messages across additional low-resource languages and code-mixing scenarios to test generalizability
2. Conduct blind evaluations with multiple human annotators from target cultural communities to validate qualitative assessment
3. Test same models on parallel sentiment analysis tasks in different communication contexts (social media, customer service, etc.) to determine if performance patterns hold across domains