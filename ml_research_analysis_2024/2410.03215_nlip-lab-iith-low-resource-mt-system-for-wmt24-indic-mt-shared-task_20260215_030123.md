---
ver: rpa2
title: NLIP_Lab-IITH Low-Resource MT System for WMT24 Indic MT Shared Task
arxiv_id: '2410.03215'
source_url: https://arxiv.org/abs/2410.03215
tags:
- indicrasp
- translation
- data
- english
- indic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a low-resource machine translation system for
  four Indic languages (Assamese, Khasi, Mizo, Manipuri) in the WMT24 shared task.
  The authors explore fine-tuning pre-trained models (IndicRASP and IndicRASP Seed)
  that use alignment-augmented objectives for 22 Indian languages.
---

# NLIP_Lab-IITH Low-Resource MT System for WMT24 Indic MT Shared Task

## Quick Facts
- arXiv ID: 2410.03215
- Source URL: https://arxiv.org/abs/2410.03215
- Reference count: 10
- This paper presents a low-resource machine translation system for four Indic languages (Assamese, Khasi, Mizo, Manipuri) in the WMT24 shared task

## Executive Summary
This paper presents a low-resource machine translation system for four Indic languages in the WMT24 shared task. The authors explore fine-tuning pre-trained models (IndicRASP and IndicRASP Seed) that use alignment-augmented objectives for 22 Indian languages. Their primary approach uses language-specific fine-tuning, achieving chrF2 scores of 50.6, 42.3, 54.9, and 66.3 for English→Assamese, English→Khasi, English→Mizo, and English→Manipuri respectively. They also experiment with multilingual training, language grouping by script similarity, and layer freezing techniques. Results show that language-specific fine-tuning outperforms multilingual approaches, with bilingual models initialized from multilingual weights showing further improvements. Script-based language grouping provides modest gains, while layer freezing underperforms full fine-tuning.

## Method Summary
The paper fine-tunes pre-trained IndicRASP models (trained on 22 Indic languages with alignment augmentation) for four low-resource language pairs. The approach uses language-specific fine-tuning with hyperparameters including learning rate of 3e-5, dropout rate of 0.3, batch size of 512 tokens, and Adam optimizer. Experiments compare bilingual vs multilingual fine-tuning, script-based language grouping, and layer freezing techniques. The IndicNECorp1.0 dataset provides 50k parallel sentences for English-Assamese and English-Mizo, 24k for English-Khasi, and 21.6k for English-Manipuri, supplemented with monolingual data.

## Key Results
- Language-specific fine-tuning achieves chrF2 scores of 50.6 (Assamese), 42.3 (Khasi), 54.9 (Mizo), and 66.3 (Manipuri)
- Bilingual models outperform multilingual approaches by +4.1 to +7.7 chrF2 points
- Script-based language grouping shows modest improvements of +1.4 to +3.3 chrF2 over standalone multilingual models
- Layer freezing underperforms full fine-tuning in low-resource settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific fine-tuning on alignment-augmented pre-trained models outperforms multilingual approaches for low-resource Indic languages.
- Mechanism: Pre-trained models (IndicRASP and IndicRASP Seed) trained on 22 Indic languages with alignment augmentation objectives capture cross-lingual patterns. Fine-tuning on specific language pairs allows the model to specialize while retaining useful pre-trained knowledge.
- Core assumption: Pre-trained embeddings capture generalizable linguistic patterns that transfer to low-resource scenarios when properly fine-tuned.
- Evidence anchors:
  - [abstract] "Our primary approach uses language-specific fine-tuning, achieving chrF2 scores of 50.6, 42.3, 54.9, and 66.3 for English→Assamese, English→Khasi, English→Mizo, and English→Manipuri respectively"
  - [section] "Bilingual models perform better than multilingual models, showing a +4.1 and +7.7 chrF2 score improvement for English to Manipuri and English to Khasi, respectively"
- Break condition: When the pre-trained model's coverage doesn't include the target language, or when fine-tuning data is insufficient to specialize the model.

### Mechanism 2
- Claim: Script-based language grouping provides modest improvements over standalone multilingual models.
- Mechanism: Languages sharing similar scripts (Bengali vs Latin) have overlapping vocabularies and linguistic properties. Training them together exploits these shared features while maintaining distinct representations.
- Core assumption: Script similarity correlates with vocabulary overlap and transferable linguistic patterns.
- Evidence anchors:
  - [section] "Language grouping and layer freezing are effective techniques for preserving pre-trained knowledge and mitigating the challenges of multilinguality"
  - [section] "We observe that script-based language grouping shows improvements over a standalone multilingual model with +1.6, +0.3, +3.3, and +1.4 for English to Assamese, Khasi, Mizo, and Manipuri, respectively"
- Break condition: When languages grouped by script have fundamentally different linguistic structures, or when vocabulary overlap is minimal despite script similarity.

### Mechanism 3
- Claim: Layer freezing of encoder components preserves pre-trained knowledge better than full fine-tuning in low-resource settings.
- Mechanism: Freezing encoder weights prevents catastrophic forgetting of pre-trained cross-lingual patterns while allowing adaptation of decoder components to specific language pairs.
- Core assumption: Encoder representations contain the most valuable pre-trained cross-lingual knowledge, while decoder components are more task-specific.
- Evidence anchors:
  - [section] "We explored layer-freezing approaches for IndicTrans2 Distilled and IndicRASP Seed models"
  - [section] "We observe that freezing only the encoder yields better chrF2 scores compared to freezing both the embedding and the encoder"
- Break condition: When the target language differs significantly from pre-training languages, making frozen encoder representations less useful.

## Foundational Learning

- Concept: Alignment augmentation objectives
  - Why needed here: The pre-trained models use alignment augmentation (Lin et al., 2020) which aligns embeddings closer using bilingual dictionaries, crucial for cross-lingual transfer in low-resource settings
  - Quick check question: What is the primary objective of alignment augmentation in pre-training multilingual models?

- Concept: Transfer learning in low-resource MT
  - Why needed here: The paper leverages pre-trained models on 22 Indic languages to bootstrap translation for 4 low-resource languages with limited parallel data
  - Quick check question: Why is transfer learning particularly important for low-resource machine translation compared to high-resource scenarios?

- Concept: Multilingual model limitations (curse of multilinguality)
  - Why needed here: The paper explicitly addresses how multilingual training can underperform bilingual approaches and explores solutions like language grouping
  - Quick check question: What is the "curse of multilinguality" and how does it affect translation quality in multilingual models?

## Architecture Onboarding

- Component map: Pre-trained models (IndicRASP/IndicRASP Seed) -> Fine-tuning approach selection -> Hyperparameter tuning -> Evaluation
- Critical path: Pre-trained model → Fine-tuning strategy selection → Hyperparameter tuning (learning rate, dropout, warmup) → Evaluation on test set
- Design tradeoffs:
  - Bilingual vs Multilingual: Bilingual gives better performance (+4.1 to +7.7 chrF2) but requires more models; multilingual is more scalable but suffers from performance degradation
  - Layer freezing: Preserves pre-trained knowledge but may limit adaptation; full fine-tuning allows more adaptation but risks forgetting
  - Language grouping: Script-based grouping shows modest gains but requires careful language selection
- Failure signatures:
  - Poor performance on languages not in pre-training coverage
  - Degradation when applying multilingual approaches to very dissimilar languages
  - Catastrophic forgetting when fine-tuning with too high learning rates
  - Overfitting on small datasets with complex architectures
- First 3 experiments:
  1. Fine-tune IndicRASP Seed on English→Assamese with language-specific approach, baseline learning rate 3e-5, batch size 512 tokens
  2. Compare bilingual vs multilingual fine-tuning on English→Khasi to quantify performance gap
  3. Test layer freezing (encoder only) vs full fine-tuning on English→Manipuri to measure knowledge preservation effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alignment-augmented pre-trained models compare to standard multilingual models for low-resource Indic languages?
- Basis in paper: [explicit] The paper specifically explores fine-tuning IndicRASP models that use alignment-augmentation objectives, showing improved performance over standard models
- Why unresolved: While the paper shows positive results, a comprehensive comparison with other pre-training approaches and their impact on different language pairs remains unexplored
- What evidence would resolve it: Direct comparisons between alignment-augmented models and other pre-training approaches (like mBART, XLM-R) across all four language pairs, measuring both translation quality and training efficiency

### Open Question 2
- Question: What is the optimal strategy for utilizing monolingual data in low-resource Indic MT systems?
- Basis in paper: [inferred] The paper acknowledges limitations in not using monolingual data, which could potentially improve performance through techniques like back-translation
- Why unresolved: The paper does not explore monolingual data utilization, leaving open questions about the effectiveness of different approaches like back-translation, self-training, or data selection strategies
- What evidence would resolve it: Experiments comparing various monolingual data utilization techniques (back-translation, self-training, data selection) and their impact on translation quality across different resource levels

### Open Question 3
- Question: How does language grouping based on script similarity compare to other language grouping strategies for multilingual MT?
- Basis in paper: [explicit] The paper explores script-based language grouping but notes it provides only modest improvements
- Why unresolved: The paper only considers script similarity for grouping, without exploring other linguistic or typological similarities that might yield better results
- What evidence would resolve it: Comparative experiments using different grouping strategies (by language family, typology, or shared linguistic features) to determine which yields optimal performance for multilingual Indic MT systems

## Limitations

- The paper lacks detailed methodology for incorporating monolingual data, which is mentioned as leveraged but not specified in the training procedure
- The reported improvements from language grouping (1.4-3.3 chrF2 points) are modest and may not generalize to other language pairs outside the tested script-based groupings
- Layer freezing underperforms full fine-tuning, contradicting the initial hypothesis that it would better preserve pre-trained knowledge

## Confidence

- **High Confidence**: The finding that language-specific fine-tuning outperforms multilingual approaches (+4.1 to +7.7 chrF2 improvement) is well-supported by direct comparisons and aligns with established transfer learning principles.
- **Medium Confidence**: The modest improvements from script-based language grouping (1.4-3.3 chrF2 points) are observed but the mechanism is less clearly established, and the effect size may not justify the added complexity.
- **Medium Confidence**: The conclusion that layer freezing underperforms full fine-tuning is supported by results but contradicts the initial hypothesis and the broader literature on knowledge preservation in low-resource settings.

## Next Checks

1. **Ablation study**: Run experiments with and without monolingual data incorporation to quantify its actual contribution to the reported performance gains.
2. **Generalization test**: Apply the script-based grouping strategy to a different set of language pairs (e.g., Devanagari script languages) to verify if the modest improvements generalize beyond the tested cases.
3. **Hyperparameter sensitivity**: Systematically vary learning rates and batch sizes for the layer freezing approach to determine if the underperformance is due to suboptimal hyperparameters rather than the technique itself.