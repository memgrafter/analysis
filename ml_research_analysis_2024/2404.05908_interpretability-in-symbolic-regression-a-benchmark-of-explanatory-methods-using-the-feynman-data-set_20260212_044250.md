---
ver: rpa2
title: 'Interpretability in Symbolic Regression: a benchmark of Explanatory Methods
  using the Feynman data set'
arxiv_id: '2404.05908'
source_url: https://arxiv.org/abs/2404.05908
tags:
- regression
- methods
- symbolic
- data
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of evaluating and comparing explanatory
  methods for regression models, particularly symbolic regression, by proposing a
  benchmark framework using synthetic physics equations. The core method involves
  creating a unified experimental design that evaluates different regression models
  (from white-box to black-box) with multiple feature importance explanatory methods
  using various quality and robustness measures.
---

# Interpretability in Symbolic Regression: a benchmark of Explanatory Methods using the Feynman data set

## Quick Facts
- arXiv ID: 2404.05908
- Source URL: https://arxiv.org/abs/2404.05908
- Authors: Guilherme Seidyo Imai Aldeia; Fabricio Olivetti de Franca
- Reference count: 40
- Primary result: Proposed benchmark framework evaluates explanatory methods for regression models using synthetic physics equations, showing that symbolic regression models can achieve competitive accuracy with appropriate explanations.

## Executive Summary
This paper introduces a benchmark framework for evaluating explanatory methods in regression models, particularly focusing on symbolic regression. The study uses the Feynman dataset of 100 physics equations to create a standardized testing environment where ground-truth feature importance is known. Through comprehensive experiments with multiple regression models (from interpretable to black-box) and various explanation methods, the authors demonstrate that symbolic regression can achieve competitive accuracy while providing meaningful explanations. The framework, implemented as the open-source iirsBenchmark module, provides a systematic approach to evaluating explanation quality beyond simple prediction accuracy.

## Method Summary
The authors created a unified benchmark framework that wraps multiple regression and explainer methods under a common interface, enabling standardized evaluation across different approaches. The framework uses synthetic data generated from 100 physics equations, allowing quantitative comparison of explanation methods against ground-truth feature importance. The benchmark implements grid search for optimal hyperparameters, trains models on synthetic data, applies various explanation methods, and calculates multiple quality and robustness metrics. The iirsBenchmark module is made publicly available with scripts for execution and post-processing, enabling reproducibility and future extensions.

## Key Results
- Partial Effects and SHAP emerged as the most robust explanation models across different regression approaches
- Integrated Gradients showed instability specifically with tree-based models like XGB and Random Forest
- Symbolic regression methods (ITEA and Operon) achieved competitive accuracy while providing interpretable explanations
- Model accuracy proved essential for reliable explanations - inaccurate models produced unreliable explanations regardless of the explainer used

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified benchmark design enables fair comparison across regression and explanation methods.
- Mechanism: The iirsBenchmark wraps multiple regression and explainer methods under a common interface, standardizing evaluation metrics and experimental flow. This eliminates variability from inconsistent implementations.
- Core assumption: All wrapped methods correctly implement their respective algorithms and return outputs in expected formats.
- Evidence anchors:
  - [abstract] "This benchmark is publicly available for further experiments."
  - [section] "The python module also provides a script to execute the experiments and all post-processing scripts."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.523, average citations=0.0. Top related titles: Enhancing Symbolic Regression and Universal Physics-Informed Neural Networks with Dimensional Analysis...
- Break Condition: If any wrapped method diverges from expected behavior or interface, cross-method comparisons become invalid.

### Mechanism 2
- Claim: Ground-truth physical equations enable quantitative evaluation of explanation quality.
- Mechanism: Feynman data set provides synthetic data from known physics equations, allowing calculation of exact feature importance (e.g., partial derivatives) as ground-truth for comparison.
- Core assumption: The synthetic data generation process accurately reflects the true underlying mathematical relationships.
- Evidence anchors:
  - [abstract] "Experiments were performed using 100 physics equations with different interpretable and non-interpretable regression methods"
  - [section] "For the overall results, we use the Feynman benchmark, introduced in [56], containing 100 equations from mechanical, electromagnetism, and quantum physics"
  - [corpus] "Rediscovering the Lunar Equation of the Centre with AI Feynman via Embedded Physical Biases" - indicates related use of Feynman dataset
- Break Condition: If the synthetic data generation introduces noise or approximation errors, ground-truth comparisons lose validity.

### Mechanism 3
- Claim: Robustness measures detect unreliable explanations even when prediction accuracy is high.
- Mechanism: Stability, infidelity, and Jaccard stability metrics measure explanation sensitivity to small perturbations in input data, revealing instability not captured by accuracy metrics alone.
- Core assumption: Small perturbations in input data should not drastically change feature importance rankings for a reliable explainer.
- Evidence anchors:
  - [abstract] "we observed that Partial Effects and SHAP were the most robust explanation models, with Integrated Gradients being unstable only with tree-based models"
  - [section] "We will analyze the results by measuring the prediction error of each regression model, followed by the quality measures of the explainer models"
  - [corpus] "A Comparison of Recent Algorithms for Symbolic Regression to Genetic Programming" - suggests comparative evaluation approaches
- Break Condition: If the perturbation methodology itself introduces bias or unrealistic data transformations, robustness measures may misidentify stable explainers as unstable.

## Foundational Learning

- Concept: Feature importance attribution in regression models
  - Why needed here: Core evaluation metric for explanation methods
  - Quick check question: What is the difference between local and global feature importance explanations?

- Concept: Symbolic regression and its representation methods
  - Why needed here: Two SR methods (ITEA, Operon) are key comparison points
  - Quick check question: How does Interaction-Transformation representation constrain the search space compared to expression trees?

- Concept: Robustness metrics for explanation methods
  - Why needed here: Critical evaluation criteria beyond prediction accuracy
  - Quick check question: Why might a highly accurate model still produce unstable explanations?

## Architecture Onboarding

- Component map:
  - iirsBenchmark -> Regression wrappers -> Explainer wrappers -> Evaluation metrics
  - Regression wrappers: Operon, ITEA, XGB, RF, MLP, SVM, k-NN, Linear, Lasso, Decision Tree
  - Explainer wrappers: Partial Effects, SHAP, SAGE, Permutation Importance, Morris Sensitivity, Integrated Gradients, LIME, ELA, Random Importance
  - Data generators: Feynman equations, GP benchmarks
  - Evaluation metrics: Cosine similarity, NMSE, Stability, Infidelity, Jaccard Stability

- Critical path:
  1. Generate synthetic data from Feynman equations
  2. Grid search optimal hyperparameters for each regressor
  3. Train models on training data
  4. Apply explainers to generate feature importance
  5. Calculate robustness and quality metrics
  6. Aggregate and visualize results

- Design tradeoffs:
  - Computational cost vs. explanation quality (SHAP/SAGE more expensive but potentially more accurate)
  - Model complexity vs. interpretability (SR methods vs. linear models)
  - Local vs. global explanations (different use cases and evaluation criteria)

- Failure signatures:
  - Explainer takes orders of magnitude longer than others → potential implementation issue
  - Correlation between model accuracy and explanation quality drops significantly → potential explanation method bias
  - Robustness metrics show high variance across similar data points → potential explainer instability

- First 3 experiments:
  1. Run benchmark with only Linear and Lasso regressors + Partial Effects explainer to verify baseline functionality
  2. Add Operon and ITEA + SHAP explainer to test SR methods integration
  3. Add all explainers with XGB regressor to stress test computational scaling and interface consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dimensionality of input features affect the quality and robustness of feature importance explanations in symbolic regression?
- Basis in paper: [inferred] The authors mention plans to investigate the role of dimensionality in future work, suggesting this remains an open question
- Why unresolved: The paper focuses on datasets with relatively low feature counts (median around 10 features) and doesn't systematically explore how explanation quality scales with dimensionality
- What evidence would resolve it: Experiments comparing explanation quality across datasets with systematically varied dimensionality, showing how measures like cosine similarity and NMSE change as feature count increases

### Open Question 2
- Question: What is the optimal balance between model accuracy and interpretability in symbolic regression, and how does this trade-off vary across different domains?
- Basis in paper: [explicit] The authors note that interpretable models sometimes present less accurate models when compared to black-box models, but also show SR can achieve competitive accuracy
- Why unresolved: While the paper demonstrates SR can achieve good accuracy, it doesn't provide a systematic framework for quantifying the interpretability-accuracy trade-off or how this varies by domain
- What evidence would resolve it: A comprehensive study across multiple domains measuring both prediction accuracy and various interpretability metrics for different regression approaches

### Open Question 3
- Question: How can we develop more robust evaluation methodologies for feature importance explanations that account for the imperfections in both the prediction model and the explainer?
- Basis in paper: [explicit] The authors discuss the challenge of evaluating explanations given imperfect prediction models and note this as an open question in the field
- Why unresolved: Current evaluation methods struggle to distinguish between errors due to poor prediction models versus errors due to poor explanation methods
- What evidence would resolve it: Development and validation of new metrics that can isolate and measure the separate contributions of prediction model quality and explainer quality to overall explanation error

### Open Question 4
- Question: What are the limitations of using synthetic physics equations as ground truth for evaluating explanatory methods, and how can we develop more diverse and challenging benchmarks?
- Basis in paper: [explicit] The authors acknowledge they only tested problems from the physics domain and plan to add more diverse regression problems in future work
- Why unresolved: The current benchmark may not adequately represent the diversity of real-world regression problems across different domains
- What evidence would resolve it: Creation and validation of a diverse benchmark suite spanning multiple domains (economics, biology, social sciences, etc.) with varying levels of non-linearity and feature interactions

## Limitations
- The benchmark relies on synthetic physics equations which may not capture real-world data complexity and noise
- Computational cost of certain explanation methods (SHAP, SAGE) limits scalability to larger datasets
- Focus on regression tasks means results may not directly transfer to classification problems
- Limited evaluation of model-specific explanations that leverage internal model structures

## Confidence
- **High confidence**: The benchmark framework design and implementation (standardized interface, reproducible experimental setup)
- **Medium confidence**: Relative performance rankings of explanation methods (results depend on synthetic data characteristics)
- **Medium confidence**: Robustness findings for specific explainer-regressor combinations (computational limitations prevented exhaustive testing)
- **Low confidence**: Generalization of results to non-physics domains and real-world noisy data

## Next Checks
1. **Real-world validation**: Apply the benchmark framework to at least two non-physics datasets with known feature importance patterns to test generalizability beyond synthetic data
2. **Computational scaling test**: Measure execution times and memory usage for all explainer methods on progressively larger datasets (10K, 100K, 1M samples) to identify practical implementation limits
3. **Cross-domain comparison**: Run the same experiments with a classification dataset and corresponding explanation methods to determine if the robustness patterns observed in regression transfer to classification tasks