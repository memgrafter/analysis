---
ver: rpa2
title: 'ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing
  Collaborative AI Systems'
arxiv_id: '2409.01392'
source_url: https://arxiv.org/abs/2409.01392
tags:
- video
- image
- workflow
- should
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ComfyBench, a benchmark for evaluating LLM-based
  agents' ability to autonomously design collaborative AI systems in ComfyUI. The
  authors propose ComfyAgent, a multi-agent framework that generates workflows using
  code representation and step-by-step refinement.
---

# ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems

## Quick Facts
- arXiv ID: 2409.01392
- Source URL: https://arxiv.org/abs/2409.01392
- Reference count: 40
- Primary result: ComfyAgent achieves 55.5% pass rate and 32.5% resolve rate on ComfyBench, comparable to o1-preview

## Executive Summary
This paper introduces ComfyBench, a benchmark for evaluating LLM-based agents' ability to autonomously design collaborative AI systems in ComfyUI. The authors propose ComfyAgent, a multi-agent framework that generates workflows using code representation and step-by-step refinement. Experiments show ComfyAgent achieves comparable performance to o1-preview with 55.5% pass rate and 32.5% resolve rate on ComfyBench, significantly outperforming other agents. However, it only resolves 15% of creative tasks, indicating LLMs still have limitations in autonomously designing complex collaborative systems.

## Method Summary
ComfyAgent is a multi-agent framework that generates workflows in ComfyUI using code representation. The system employs a Planner (PlanAgent) that iteratively updates plans based on memory state, selects actions, and coordinates with specialized agents (CombineAgent, AdaptAgent, RetrieveAgent, RefineAgent). The code representation converts workflows to a Python-like syntax that LLMs can better understand. The framework uses a shared memory system (History, Reference, Workspace) and performs step-by-step refinement with error checking after each modification. ComfyBench evaluates agents across 200 tasks categorized as vanilla, complex, and creative.

## Key Results
- ComfyAgent achieves 55.5% pass rate and 32.5% resolve rate on ComfyBench
- Performance is comparable to o1-preview and significantly surpasses other agents
- Only 15% of creative tasks are resolved, highlighting limitations in handling novel workflow combinations
- Code representation enables better LLM understanding compared to native JSON or graph representations

## Why This Works (Mechanism)

### Mechanism 1
Code representation of workflows enables LLM-based agents to better understand complex collaborative AI systems compared to native JSON or graph representations. The code representation uses a restricted subset of Python-like syntax that provides Turing completeness, rich semantic information, and natural compatibility with LLMs' code generation capabilities, while avoiding the context limitations and redundant information present in JSON representations. Core assumption: LLMs can effectively parse and reason about structured code representations of workflows. Break condition: If the code representation becomes too complex or the LLMs fail to maintain semantic equivalence during conversion.

### Mechanism 2
The multi-agent framework decomposes complex workflow generation tasks into manageable subtasks that can be handled by specialized agents working in coordination. The framework uses PlanAgent for global planning, RetrieveAgent for knowledge acquisition, CombineAgent for workflow integration, AdaptAgent for parameter modification, and RefineAgent for error correction, with a shared memory system that maintains history, references, and workspace state. Core assumption: Task decomposition through specialized agents leads to better performance than monolithic approaches. Break condition: If agent coordination becomes too complex or if the memory system cannot effectively share relevant information.

### Mechanism 3
The step-by-step refinement process with iterative planning and action execution allows the system to gradually build correct workflows while minimizing error accumulation. The PlanAgent iteratively updates plans based on current memory state, selects appropriate actions, and uses RefineAgent to check and correct errors after each modification before updating the workspace. Core assumption: Incremental refinement with error checking at each step prevents the accumulation of errors that would make later corrections more difficult. Break condition: If the refinement process becomes too computationally expensive or if errors compound faster than they can be corrected.

## Foundational Learning

- **Directed Acyclic Graph (DAG) representation of workflows**: ComfyUI workflows are naturally represented as DAGs with nodes as processing components and directed edges as information flow. Why needed: Forms the foundation for understanding how workflows are structured and executed. Quick check: What are the two main elements that constitute a workflow in ComfyUI and how do they relate to each other?

- **Stable Diffusion pipeline architecture**: Understanding the basic components (VAEs, CLIPs, KSamplers, etc.) and their roles in image/video generation is essential for comprehending how workflows are constructed. Why needed: Essential for understanding how different nodes interact. Quick check: What is the typical sequence of operations in a basic text-to-image generation pipeline in ComfyUI?

- **Multi-agent system coordination patterns**: The system relies on multiple agents working together through a shared memory architecture. Why needed: Understanding how different agents can specialize in different aspects of workflow generation. Quick check: How does the PlanAgent coordinate with other agents to ensure the workflow generation process stays on track?

## Architecture Onboarding

- **Component map**: ComfyAgent consists of Memory (History, Reference, Workspace), Planner (PlanAgent), and Actions (CombineAgent, AdaptAgent, RetrieveAgent, RefineAgent). The system uses code representation for workflows and interacts with ComfyUI through parsers that convert between code and JSON formats.

- **Critical path**: PlanAgent initializes task, selects action → corresponding agent processes action and updates memory → RefineAgent checks and corrects errors → PlanAgent updates plan and selects next action → repeat until task completion.

- **Design tradeoffs**: Code representation vs JSON (semantic richness vs native compatibility), multi-agent complexity vs monolithic simplicity, iterative refinement vs batch processing, specialized agents vs generalist approach.

- **Failure signatures**: Workflow execution failures indicate syntax/semantic errors, incorrect output indicates planning/adaptation failures, performance degradation suggests memory coordination issues, task incompletion points to planning limitations.

- **First 3 experiments**:
  1. Test code representation conversion: Create simple workflows in JSON, convert to code representation, verify round-trip conversion maintains semantic equivalence
  2. Test single agent baseline: Implement basic workflow generation using only PlanAgent without other agents to establish baseline performance
  3. Test memory system isolation: Verify History, Reference, and Workspace components correctly store and retrieve information independently before integrating into full system

## Open Questions the Paper Calls Out

### Open Question 1
Can ComfyAgent handle tasks requiring novel combinations of nodes not present in the curriculum workflows? Basis: The paper notes that creative tasks cannot be solved by directly imitating curriculum workflows, and ComfyAgent only resolves 15% of creative tasks. Why unresolved: The current ComfyAgent framework relies heavily on existing workflows for learning. What evidence would resolve it: Experiments testing ComfyAgent on tasks requiring entirely new node combinations not present in training data.

### Open Question 2
Would fine-tuning ComfyAgent on ComfyBench significantly improve its performance on creative tasks? Basis: The authors note that well-designed fine-tuning methods could potentially elevate agents to an expert level on ComfyBench, but they are quite expensive and thus not involved in their work. Why unresolved: The paper only uses zero-shot prompting and retrieval-augmented generation without any fine-tuning. What evidence would resolve it: Performance comparisons between fine-tuned and non-fine-tuned versions of ComfyAgent across all task categories.

### Open Question 3
How would ComfyAgent perform on other visual programming platforms beyond ComfyUI? Basis: The paper focuses exclusively on ComfyUI, though it discusses broader applications of LLM-based agents for designing collaborative AI systems. Why unresolved: The framework's architecture is tailored to ComfyUI's specific node structure and JSON representation. What evidence would resolve it: Successful adaptation of ComfyAgent to other visual programming platforms like Node-RED or Unreal Engine Blueprints.

## Limitations

- Evaluation relies heavily on GPT-4o as the judge, introducing potential bias in determining pass/resolve rates
- Comparison with o1-preview is limited to the ComfyBench dataset and may not generalize to other domains
- Code representation's effectiveness assumes consistent semantic preservation across complex workflow transformations, which wasn't extensively validated

## Confidence

- **High confidence**: The basic methodology of using code representation for workflows and the ComfyBench benchmark creation
- **Medium confidence**: The comparative performance results between ComfyAgent and other agents
- **Low confidence**: The generalizability of the code representation approach to non-ComfyUI workflows and the scalability of the multi-agent system to more complex tasks

## Next Checks

1. **Ablation study**: Test ComfyAgent performance with individual agents removed to quantify each component's contribution to overall success rates
2. **Cross-domain validation**: Apply the code representation and multi-agent framework to a different workflow system (e.g., LangChain or similar) to test generalizability
3. **Judge bias analysis**: Compare evaluation results using multiple judges (GPT-4o, Claude, human experts) to assess consistency and potential bias in the pass/resolve rate measurements