---
ver: rpa2
title: 'AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents'
arxiv_id: '2403.12835'
source_url: https://arxiv.org/abs/2403.12835
tags:
- npyt
- clipped
- motion
- npy8-12-new
- npyh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AnySkill introduces a hierarchical framework for learning open-vocabulary
  physical interaction skills in virtual agents. It combines a low-level controller
  trained via imitation learning to encode atomic actions, with a high-level policy
  that selects and composes these actions using CLIP-based image-text similarity rewards.
---

# AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents

## Quick Facts
- arXiv ID: 2403.12835
- Source URL: https://arxiv.org/abs/2403.12835
- Authors: Jieming Cui; Tengyu Liu; Nian Liu; Yixin Zhu; Siyuan Huang
- Reference count: 40
- Key outcome: Hierarchical framework combining low-level atomic actions with high-level CLIP-based policy achieves 6.16/10 success rate vs. ~4-5/10 baselines

## Executive Summary
AnySkill introduces a hierarchical framework for learning open-vocabulary physical interaction skills in virtual agents. It combines a low-level controller trained via imitation learning to encode atomic actions, with a high-level policy that selects and composes these actions using CLIP-based image-text similarity rewards. This enables agents to generate natural, physically plausible motions for novel textual instructions, including dynamic object interactions, without manual reward engineering. The method significantly outperforms existing approaches in both qualitative user studies and quantitative CLIP similarity scores.

## Method Summary
AnySkill employs a two-level hierarchical approach: a shared low-level controller trained via GAIL encodes diverse atomic actions into a latent space, while a high-level policy trained for each instruction composes these actions using CLIP similarity between rendered images and text as rewards. The framework leverages 93 motion clips from CMU and SFU motion capture databases, re-targeted to a humanoid skeleton, and text descriptions from HumanML3D and BABEL datasets. The high-level policy uses image-based rewards to enable object interaction without manual reward engineering, and early termination prevents policy collapse during training.

## Key Results
- 6.16/10 success rate in user studies vs. ~4-5/10 for baselines
- 24.18 CLIP similarity score vs. ~21-22 for baselines
- Enables natural object interactions without manual reward engineering
- Works with open-vocabulary text instructions not seen during training

## Why This Works (Mechanism)

### Mechanism 1
The hierarchical decomposition of low-level atomic actions and high-level policy enables open-vocabulary skill learning. A shared low-level controller trained via GAIL encodes diverse atomic actions into a latent space. A high-level policy selects and composes these actions using CLIP similarity rewards, allowing adaptation to novel textual instructions without manual reward engineering. The core assumption is that the latent space captures sufficient diversity to compose novel skills, and CLIP similarity correlates with meaningful motion-text alignment.

### Mechanism 2
Image-based rewards via CLIP enable object interaction without manual reward engineering. The high-level policy uses CLIP similarity between rendered images of the agent and the text description as a reward signal. This image-based representation naturally encodes the entire environment, facilitating interaction with dynamic objects without needing object-specific reward crafting. The core assumption is that CLIP features capture sufficient environmental context for object interaction, and the similarity metric aligns with desired interaction outcomes.

### Mechanism 3
Early termination prevents policy collapse into local minima during training. The training environment resets with 80% probability after eight successive reductions in CLIP similarity or when the agent's head height falls below 15cm. This prevents the policy from getting stuck in poor performance regions. The core assumption is that the policy can escape poor local minima through environmental resets, and the conditions for reset indicate failure modes worth escaping.

## Foundational Learning

- Concept: Reinforcement learning with image-based rewards
  - Why needed here: The high-level policy learns through trial and error using CLIP similarity between rendered images and text as reward, requiring understanding of RL principles and reward shaping.
  - Quick check question: How does the agent receive feedback during training, and what role does the CLIP similarity play in this feedback loop?

- Concept: Generative Adversarial Imitation Learning (GAIL)
  - Why needed here: The low-level controller is trained using GAIL to encode atomic actions from unlabeled motion data, requiring understanding of adversarial learning and imitation learning.
  - Quick check question: What is the role of the discriminator in GAIL, and how does it help the controller learn physically plausible motions?

- Concept: Vision-Language Models (VLMs) and CLIP architecture
  - Why needed here: The method relies on CLIP to provide image-text similarity rewards, requiring understanding of how VLMs encode visual and textual information into comparable feature spaces.
  - Quick check question: How does CLIP encode images and text into a shared embedding space, and why does this enable cross-modal similarity measurement?

## Architecture Onboarding

- Component map:
  - Low-level controller (πL) -> Motion encoder (E) -> Latent space Z -> Discriminator (D) -> Atomic action
  - High-level policy (πH) -> CLIP model -> Renderer -> Image-text similarity reward
  - State projection MLP -> CLIP image features -> Training efficiency

- Critical path: Motion clip → Encoder E → Latent space Z → Low-level controller πL → Atomic action → Simulation → Rendered image → CLIP features → High-level policy πH → New latent action selection

- Design tradeoffs:
  - Using CLIP similarity as reward vs. manually crafted rewards: More flexible and generalizable but potentially noisier and less precise for specific tasks
  - Image-based rewards vs. state-based rewards: Natural object interaction but computationally expensive rendering
  - Hierarchical vs. monolithic policy: Better decomposition and modularity but requires coordination between levels

- Failure signatures:
  - Low-level controller failure: Generated motions don't match reference motions, lack physical plausibility, or don't cover required action diversity
  - High-level policy failure: Policy doesn't improve CLIP similarity, gets stuck in local minima, or produces physically implausible sequences
  - CLIP similarity failure: Similarity scores don't correlate with task success, or policy exploits CLIP weaknesses rather than learning meaningful skills

- First 3 experiments:
  1. Train low-level controller on CMU dataset and verify it can reproduce reference motions with physical plausibility
  2. Test CLIP similarity between rendered agent images and text descriptions for simple, unambiguous instructions
  3. Train high-level policy on a single simple instruction (e.g., "raise arm") and verify it can compose atomic actions to achieve the task

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications emerge from its discussion of limitations and future work directions.

## Limitations

- The method shows limitations with prolonged actions and vague descriptions, suggesting image-based rewards provide insufficient directional guidance for extended tasks.
- Performance sensitivity to the quality and diversity of the low-level atomic action dataset is not systematically explored.
- Computational and memory costs of scaling to more complex environments or higher-frequency policies are not fully evaluated.

## Confidence

- Open-vocabulary skill learning through hierarchical decomposition: Medium
- CLIP-based image rewards enabling object interaction without manual engineering: Medium
- Early termination preventing policy collapse: Low
- Qualitative and quantitative performance improvements over baselines: High

## Next Checks

1. **Latent Space Diversity Analysis**: Systematically evaluate whether the learned latent space contains sufficient action diversity by testing the low-level controller's ability to interpolate between and extrapolate from existing atomic actions, measuring both physical plausibility and task coverage.

2. **CLIP Reward Alignment Study**: Conduct controlled experiments comparing CLIP similarity scores with human judgments of motion-text correspondence across a wide range of instructions, including edge cases and adversarial examples that might exploit CLIP's weaknesses.

3. **Early Termination Parameter Sensitivity**: Perform ablation studies varying the reset probability (80%), similarity reduction threshold (8 successive reductions), and head height threshold (15cm) to determine their impact on final policy performance and identify optimal parameter ranges for different instruction complexities.