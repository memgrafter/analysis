---
ver: rpa2
title: A Closer Look at Classification Evaluation Metrics and a Critical Reflection
  of Common Evaluation Practice
arxiv_id: '2404.16958'
source_url: https://arxiv.org/abs/2404.16958
tags:
- macro
- metric
- prevalence
- class
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically examines common evaluation metrics for multi-class
  classification. Starting from the intuitive concepts of bias and prevalence, the
  author defines five metric properties (monotonicity, class sensitivity, class decomposability,
  prevalence invariance, and chance correction) and analyzes popular metrics including
  Accuracy, Macro Recall, Macro Precision, Macro F1, Weighted F1, Kappa, and Matthews
  Correlation Coefficient (MCC).
---

# A Closer Look at Classification Evaluation Metrics and a Critical Reflection of Common Evaluation Practice

## Quick Facts
- arXiv ID: 2404.16958
- Source URL: https://arxiv.org/abs/2404.16958
- Authors: Juri Opitz
- Reference count: 34
- Key outcome: Critical examination of multi-class classification metrics reveals prevalence invariance and chance correction as crucial properties, with Macro Recall identified as having all five desirable properties.

## Executive Summary
This paper provides a comprehensive analysis of evaluation metrics for multi-class classification, starting from fundamental concepts of bias and prevalence. The author defines five desirable properties for evaluation metrics and systematically analyzes popular metrics including Accuracy, Macro Recall, Macro Precision, Macro F1, Weighted F1, Kappa, and MCC. The analysis reveals that Macro Recall satisfies all five properties and is prevalence-invariant, while other metrics have various limitations. The paper also identifies ambiguity in the definition of "macro F1" and conducts a survey of recent shared tasks, finding that metric selection is often not well-justified. The author concludes with recommendations for more informed and transparent metric selection in classification tasks.

## Method Summary
The study employs theoretical analysis to examine classification evaluation metrics. The author starts by defining fundamental concepts of bias and prevalence in classification tasks, then establishes five desirable metric properties: monotonicity, class sensitivity, class decomposability, prevalence invariance, and chance correction. These properties are systematically evaluated against common metrics through mathematical proofs and examples. Additionally, the author conducts a survey of recent shared tasks to examine real-world metric selection practices and identifies ambiguities in metric definitions.

## Key Results
- Macro Recall has all five desirable properties and is prevalence-invariant, making it a strong candidate for macro evaluation
- Accuracy and Weighted F1 are prevalence-dependent, while Kappa and MCC are non-monotonic
- Ambiguity exists in the definition of "macro F1", with two distinct metrics identified
- Survey of shared tasks reveals poor justification for metric selection, with "macro F1" being the most frequent choice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prevalence calibration transforms prevalence-dependent metrics into prevalence-invariant ones by equalizing class frequencies
- Mechanism: By scaling the confusion matrix columns with λ such that all classes have equal prevalence, the metric becomes independent of the original class distribution
- Core assumption: Class prevalence is the only important difference across datasets that affects metric scores
- Evidence anchors:
  - [abstract] "It also offers three intuitive interpretations: Drawing an item from a random class, Bookmaker metric and prevalence-calibrated Accuracy."
  - [section] "Given two random items drawn from two random classes, bT b seems to measure the chance that the classifier randomly predicts the same label"
  - [corpus] Weak evidence - no direct corpus support for prevalence calibration mechanism

### Mechanism 2
- Claim: Macro metrics are class-sensitive because they measure performance across all classes rather than just overall accuracy
- Mechanism: By averaging class-specific scores, macro metrics ensure that performance on rare classes is not overshadowed by performance on frequent classes
- Core assumption: Users expect metrics to show classifier performance equally across all classes
- Evidence anchors:
  - [abstract] "However, what is meant with phrases like 'imbalanced data' or 'macro' is rarely made explicit"
  - [section] "A 'macro' metric needs to be sensitive to classes, or else it could not yield a 'balanced measurement' for 'classes having different sizes'"
  - [corpus] Weak evidence - corpus contains related papers but no direct support for class sensitivity mechanism

### Mechanism 3
- Claim: Chance correction provides interpretable baselines by normalizing scores against random performance
- Mechanism: By subtracting the expected score from random guessing, chance correction contextualizes metric scores relative to a known baseline
- Core assumption: Users need to understand how well a classifier performs compared to random guessing
- Evidence anchors:
  - [abstract] "Two simple 'baseline' classifiers are: Predicting classes uniformly randomly, or based on observed prevalence"
  - [section] "A macro metric can be expected to show robustness against any such chance classifier and be chance corrected, assigning a clear and comparable baseline score"
  - [corpus] Weak evidence - corpus contains related papers but no direct support for chance correction mechanism

## Foundational Learning

- Concept: Confusion matrix construction
  - Why needed here: All metric calculations are based on confusion matrix entries
  - Quick check question: Given predictions [A, B, A] and true labels [A, A, B], what are the confusion matrix entries for a 2-class problem?

- Concept: Prevalence vs bias
  - Why needed here: These concepts distinguish between how often classes appear vs how often the classifier predicts each class
  - Quick check question: In a 3-class problem with 100 examples per class, if a classifier always predicts class 1, what are the prevalence and bias vectors?

- Concept: Harmonic vs arithmetic mean
  - Why needed here: F1-score uses harmonic mean, which affects how macro F1 is calculated differently from macro precision/recall
  - Quick check question: For precision=0.8 and recall=0.5, what is F1-score and how does it differ from their arithmetic mean?

## Architecture Onboarding

- Component map: Confusion matrix builder → metric calculator → property validator → comparison engine
- Critical path: Confusion matrix generation → metric computation → property verification → result presentation
- Design tradeoffs: Precision vs recall balance (F1-score) vs individual metric reporting; prevalence invariance vs prevalence sensitivity; chance correction vs raw performance
- Failure signatures: Non-monotonic metrics (incorrect score increase with errors); prevalence-dependent metrics (scores change with class distribution); non-decomposable metrics (cannot report per-class performance)
- First 3 experiments:
  1. Create confusion matrices for perfect, random, and always-wrong classifiers and verify metric scores match theoretical expectations
  2. Test prevalence calibration on imbalanced datasets and verify metric invariance
  3. Compare metric rankings on real datasets to identify which metrics produce similar or divergent results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise definition of "macro F1" used in different shared tasks and papers?
- Basis in paper: [explicit] The paper identifies ambiguity in the definition of "macro F1", noting two distinct metrics - the arithmetic mean of class-wise F1 scores and the harmonic mean of macro Precision and Recall.
- Why unresolved: Different papers and shared tasks use the term "macro F1" without clear definition, leading to potential misinterpretation and inconsistent results.
- What evidence would resolve it: Explicitly defining the formula for "macro F1" in each paper and shared task, or using distinct names for the two different metrics.

### Open Question 2
- Question: How does the choice of evaluation metric affect the selection of winning systems in shared tasks?
- Basis in paper: [explicit] The paper demonstrates that different metrics can lead to different rankings of systems, and that the choice of metric can influence which system is considered "best".
- Why unresolved: The impact of metric selection on shared task outcomes is not well understood, and the paper does not provide a comprehensive analysis of this issue.
- What evidence would resolve it: Conducting a systematic study of shared task results using different metrics and analyzing the impact on system rankings and winner selection.

### Open Question 3
- Question: What are the practical implications of using prevalence-calibrated metrics in real-world applications?
- Basis in paper: [inferred] The paper discusses prevalence calibration as a way to achieve prevalence invariance, but does not explore its practical implications in real-world scenarios.
- Why unresolved: The paper focuses on theoretical analysis of metrics, but does not provide guidance on how to apply these concepts in practical applications.
- What evidence would resolve it: Empirical studies comparing the performance of calibrated and non-calibrated metrics in real-world classification tasks, and analyzing the impact on decision-making and system evaluation.

## Limitations

- Theoretical focus: The analysis is primarily theoretical, with limited empirical validation across diverse real-world classification tasks
- Sample size: The survey of shared tasks is based on a relatively small sample and may not represent the full diversity of metric usage patterns
- Domain specificity: The paper does not address how metric selection might vary across different application domains or task types

## Confidence

- Metric property analysis: High
- Survey findings: Medium
- Recommendations: Medium-High

## Next Checks

1. Empirical validation: Test the identified metric properties across diverse multi-class classification datasets with varying class distributions and difficulty levels.

2. Community survey: Conduct a broader survey of researchers to understand how they select evaluation metrics and what properties they prioritize for different classification tasks.

3. Metric comparison: Systematically compare metric rankings on benchmark datasets to identify practical differences in metric behavior and their impact on model selection.