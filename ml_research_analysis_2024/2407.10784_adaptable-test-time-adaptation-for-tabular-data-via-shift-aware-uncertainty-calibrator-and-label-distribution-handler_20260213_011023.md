---
ver: rpa2
title: 'AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty
  Calibrator and Label Distribution Handler'
arxiv_id: '2407.10784'
source_url: https://arxiv.org/abs/2407.10784
tags:
- distribution
- label
- tabular
- data
- shifts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdapTable, a test-time adaptation method
  designed for tabular data. It addresses the challenge of distribution shifts in
  tabular domains, where existing TTA methods struggle due to entangled covariate
  and concept shifts, and label distribution shifts.
---

# AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler

## Quick Facts
- arXiv ID: 2407.10784
- Source URL: https://arxiv.org/abs/2407.10784
- Authors: Changhun Kim; Taewon Kim; Seungyeon Woo; June Yong Yang; Eunho Yang
- Reference count: 40
- One-line primary result: Achieves up to 16% improvement on HELOC dataset by calibrating predictions using graph neural networks and correcting for label distribution shifts

## Executive Summary
This paper introduces AdapTable, a test-time adaptation method designed for tabular data that addresses the challenge of distribution shifts in tabular domains. The method operates in two stages: first, it calibrates model predictions using a shift-aware uncertainty calibrator that captures complex feature shifts via graph neural networks; second, it adjusts predictions to match the target label distribution using a label distribution handler. Experiments on six tabular datasets demonstrate AdapTable's effectiveness, achieving up to 16% improvement on the HELOC dataset, outperforming baselines across various distribution shifts and common corruptions.

## Method Summary
AdapTable is a test-time adaptation method for tabular data that operates in two stages without requiring parameter updates. The first stage uses a shift-aware uncertainty calibrator with graph neural networks to capture per-sample temperature based on feature shift trends and correlations. The second stage employs a label distribution handler that estimates and corrects for target label distribution shifts using online debiased estimation. The method is model-agnostic and designed to handle the unique challenges of tabular data where covariate, concept, and label distribution shifts are often entangled.

## Key Results
- Achieves up to 16% improvement on HELOC dataset compared to source model
- Outperforms baselines across various distribution shifts and common corruptions on six tabular datasets
- Demonstrates effectiveness on both synthetic and real-world tabular data scenarios
- Shows robustness to label distribution shifts while maintaining calibration quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The shift-aware uncertainty calibrator uses per-sample temperature scaling informed by graph neural networks to correct overconfident predictions.
- Mechanism: The calibrator treats each column as a graph node, builds a shift trend graph using feature differences between current and source batches, applies GNN message passing to capture inter-feature correlations, and outputs a per-sample temperature. This temperature is used to scale logits before softmax, reducing confidence for samples with high uncertainty.
- Core assumption: Graph neural networks can effectively model column-wise shift trends and their correlations to reflect input uncertainty.
- Evidence anchors:
  - [abstract]: "Our shift-aware uncertainty calibrator utilizes graph neural networks to assign per-sample temperature for each model prediction. By treating each column as a graph node, it captures not only individual feature shifts but also complex patterns across features."
  - [section 3.2]: "Our shift-aware uncertainty calibrator utilizes graph neural networks to assign per-sample temperature for each model prediction. By treating each column as a graph node, it captures not only individual feature shifts but also complex patterns across features."
  - [corpus]: Weak; no direct citation of GNN usage for tabular calibration in neighbors, suggesting novelty.
- Break condition: If feature shifts are independent and uncorrelated, the GNN's ability to capture complex patterns may be unnecessary, and simpler per-feature scaling could suffice.

### Mechanism 2
- Claim: The label distribution handler estimates and corrects for target label distribution shift to improve adaptation performance.
- Mechanism: The handler computes debiased target label estimates by dividing predicted probabilities by source label distribution, then smooths these with an online estimator that incorporates batch history. Final predictions are adjusted by combining calibrated probabilities with distributionally aligned predictions.
- Core assumption: Model predictions are biased toward the source label distribution under distribution shift, and correcting this bias improves accuracy.
- Evidence anchors:
  - [abstract]: "it adjusts predictions to match the target label distribution using a label distribution handler."
  - [section 3.3]: "we propose a simple yet effective estimator... defined as: p̄i(y|xti) = p̃t(y|xti) + norm( p̃t(y|xti) pt(y)/ps(y) ) / 2"
  - [corpus]: Weak; label distribution shift is mentioned in neighbors but no detailed mechanism for tabular data.
- Break condition: If the target label distribution is highly dynamic or non-stationary, the online estimator may lag, causing misalignment.

### Mechanism 3
- Claim: Two-stage uncertainty calibration improves calibration by first using GNN-based per-sample temperature, then refining with batch-relative uncertainty quantiles.
- Mechanism: Stage one computes per-sample temperature Ti using GNN. Stage two recalculates temperature Ti using quantiles of the uncertainty δi within the batch, applying high/low quantile rules to amplify or reduce temperature.
- Core assumption: Relative uncertainty within a batch provides meaningful signals for recalibration beyond absolute shift trends.
- Evidence anchors:
  - [section 3.3]: "This two-stage uncertainty calibration comprehensively evaluates the current batch and estimates relative uncertainty using st, gϕ, and Ti."
  - [section 3.3]: "We then measure the quantiles for each instance xi using δi within the current batch and recalibrate the original probability with Ti"
  - [corpus]: Missing; no neighbor discusses two-stage calibration for tabular data.
- Break condition: If batch size is too small, quantile estimates become unreliable, breaking the recalibration step.

## Foundational Learning

- Concept: Distribution shifts in tabular data often involve both covariate and concept shifts entangled with label distribution shifts.
  - Why needed here: Explains why existing TTA methods fail; highlights need for label distribution correction.
  - Quick check question: What is the difference between covariate shift and concept shift in tabular data?

- Concept: Graph neural networks for tabular feature correlation modeling.
  - Why needed here: GNN captures inter-feature shift patterns, which is central to the uncertainty calibrator.
  - Quick check question: How does a GNN help model feature correlations in a tabular shift trend graph?

- Concept: Online estimation and smoothing of label distributions.
  - Why needed here: Enables dynamic adaptation to evolving target label distributions during inference.
  - Quick check question: What role does the smoothing factor α play in the online label distribution estimator?

## Architecture Onboarding

- Component map: Pre-trained classifier fθ -> Shift-aware uncertainty calibrator gϕ -> Label distribution handler -> Final predictions
- Critical path:
  1. Compute shift trend graph from current batch.
  2. Pass through GNN to get per-sample temperature.
  3. Apply two-stage temperature scaling.
  4. Estimate target label distribution online.
  5. Combine calibrated predictions with distribution correction.
  6. Output final predictions.
- Design tradeoffs:
  - Model-agnosticism vs. need for post-training calibrator.
  - Batch size impacts GNN message passing efficiency and quantile reliability.
  - Smoothing factor α trades off responsiveness vs. stability in label distribution estimation.
- Failure signatures:
  - Overconfident predictions persist → GNN not capturing shifts or batch too small.
  - Calibration noise → quantile thresholds too extreme or batch statistics unstable.
  - Label distribution drift → smoothing factor α too low or batch history insufficient.
- First 3 experiments:
  1. Run calibrator alone on synthetic shift data; verify temperature increases with shift magnitude.
  2. Apply label handler alone; verify correction toward target label distribution in imbalanced settings.
  3. Combine both on small tabular dataset; check macro F1 improvement over source model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AdapTable be extended to effectively handle input covariate shifts and concept shifts in tabular data, beyond its current focus on label distribution shifts?
- Basis in paper: [inferred] The paper acknowledges that AdapTable primarily focuses on label distribution shifts and suggests that further exploration is needed to assess its effectiveness in handling input covariate shifts or concept shifts.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on how AdapTable performs when dealing with covariate shifts or concept shifts, which are also significant challenges in tabular data.
- What evidence would resolve it: Experimental results comparing AdapTable's performance on datasets with varying levels of covariate and concept shifts, alongside modifications to the model to specifically address these shifts.

### Open Question 2
- Question: What is the impact of using different encoding types (e.g., one-hot, label, embedding) for categorical features on AdapTable's performance in tabular data?
- Basis in paper: [explicit] The paper mentions that different encoding types do not play a significant role in terms of accuracy, as noted in Grinsztajn et al. [20].
- Why unresolved: While the paper states that encoding types do not significantly impact accuracy, it does not provide experimental evidence or a detailed analysis of how different encoding methods affect AdapTable's performance.
- What evidence would resolve it: Comparative experiments using AdapTable with various encoding methods for categorical features across multiple datasets, measuring performance metrics such as accuracy, F1 score, and calibration.

### Open Question 3
- Question: Can AdapTable be adapted to handle time-series tabular data, where temporal dependencies and trends are crucial?
- Basis in paper: [inferred] The paper does not discuss the applicability of AdapTable to time-series data, which presents unique challenges such as temporal correlations and trends.
- Why unresolved: The current framework of AdapTable does not explicitly account for temporal dependencies, and the paper does not provide insights or experiments on how it might be extended to handle time-series data.
- What evidence would resolve it: Development and evaluation of a time-series extension of AdapTable, tested on datasets with temporal components, to assess improvements in performance and robustness over standard methods.

## Limitations

- The specific GNN architecture and hyperparameters for the shift-aware uncertainty calibrator are not fully specified, making faithful reproduction challenging.
- The exact implementation details of the focal loss and calibration loss used during calibrator training are not provided.
- The two-stage calibration process's effectiveness depends heavily on batch size, as small batches can lead to unreliable quantile estimates and unstable label distribution estimates.

## Confidence

**High Confidence**: The general framework of using a GNN-based uncertainty calibrator followed by label distribution correction is well-articulated and theoretically sound. The mechanism for per-sample temperature scaling and the online label distribution estimator are clearly described.

**Medium Confidence**: The specific implementation details for the GNN architecture, loss functions, and hyperparameter choices are not fully specified. While the overall approach is clear, these details could significantly impact performance.

**Low Confidence**: The paper does not provide sufficient details on how to handle edge cases such as highly dynamic or non-stationary target label distributions, or how to determine optimal batch sizes for reliable quantile estimates.

## Next Checks

1. **Calibrator Isolation Test**: Implement and test the shift-aware uncertainty calibrator alone on synthetic tabular data with known covariate shifts. Verify that the GNN correctly assigns higher temperatures to samples with larger feature shifts.

2. **Label Distribution Handler Validation**: Create a controlled experiment with imbalanced source and target label distributions. Test the label distribution handler's ability to estimate and correct for the target distribution, measuring the JS divergence between estimated and true distributions.

3. **End-to-End Integration on Small Dataset**: Combine both components and evaluate on a small tabular dataset (e.g., a subset of HELOC). Compare macro F1 scores against the source model and a simple baseline that applies temperature scaling without GNN or label distribution correction.