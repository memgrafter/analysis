---
ver: rpa2
title: 'Puzzle Solving using Reasoning of Large Language Models: A Survey'
arxiv_id: '2402.11291'
source_url: https://arxiv.org/abs/2402.11291
tags:
- puzzles
- reasoning
- language
- llms
- puzzle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey explores Large Language Models' (LLMs) puzzle-solving
  capabilities, categorizing puzzles into rule-based (deterministic and stochastic)
  and rule-less (riddles, programming, and commonsense reasoning) types. It examines
  methods like prompting techniques (Chain-of-Thought, Tree-of-Thought), neuro-symbolic
  approaches (logic rule generation), and fine-tuning.
---

# Puzzle Solving using Reasoning of Large Language Models: A Survey

## Quick Facts
- arXiv ID: 2402.11291
- Source URL: https://arxiv.org/abs/2402.11291
- Reference count: 19
- Key outcome: LLMs show improved puzzle-solving with prompting techniques like CoT and ToT, but still fall short of human-level performance, especially in complex rule-less puzzles.

## Executive Summary
This survey explores how Large Language Models tackle various puzzle types, categorizing them into rule-based (deterministic and stochastic) and rule-less (riddles, programming, commonsense reasoning) puzzles. The authors examine prompting techniques, neuro-symbolic approaches, and fine-tuning methods, finding that while these strategies improve reasoning capabilities, significant gaps remain between LLM and human performance. The study highlights the need for richer datasets and novel strategies to advance LLMs' puzzle-solving proficiency, particularly for complex and ambiguous puzzle types.

## Method Summary
The survey systematically reviews puzzle-solving approaches by categorizing puzzles into rule-based and rule-less types, then evaluating various LLM methodologies including Chain-of-Thought, Tree-of-Thought, neuro-symbolic translation, and fine-tuning. It analyzes performance across multiple benchmarks and datasets, identifying strengths and limitations of each approach while proposing future research directions to address current gaps in LLM puzzle-solving capabilities.

## Key Results
- Prompting techniques like Chain-of-Thought and Tree-of-Thought significantly improve LLM puzzle-solving by providing structured reasoning pathways.
- Neuro-symbolic approaches effectively leverage external solvers for rule-based puzzles but remain underexplored for rule-less puzzles.
- A persistent performance gap exists between LLMs and human reasoning, particularly for stochastic puzzles and rule-less challenges requiring commonsense knowledge.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dividing puzzles into rule-based and rule-less categories clarifies the distinct reasoning demands placed on LLMs, enabling targeted methodological approaches.
- Mechanism: The categorization separates deterministic games, which require logical deduction and strategy within fixed rules, from stochastic games, which demand probabilistic inference and risk management, and from rule-less puzzles, which require flexible thinking and real-world knowledge.
- Core assumption: Puzzles can be effectively distinguished by the type of reasoning they require, and this distinction correlates with the performance of different LLM methods.
- Evidence anchors:
  - [abstract]: "This survey leverages a unique taxonomy—dividing puzzles into rule-based and rule-less categories—to critically assess LLMs through various methodologies..."
  - [section]: "We distinguish puzzles by their reliance on formal rules or broader world knowledge accompanied by general inferential skills..."
- Break condition: If puzzles exhibit overlapping characteristics across categories, the taxonomy may fail to guide method selection effectively.

### Mechanism 2
- Claim: Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) improve LLM puzzle-solving by structuring intermediate reasoning steps.
- Mechanism: These methods provide explicit reasoning pathways, with CoT offering step-wise explanations and ToT exploring multiple branching paths, enabling the model to handle complex puzzles more effectively than simple input-output prompts.
- Core assumption: LLMs benefit from guided reasoning structures that mimic human problem-solving processes, particularly for puzzles requiring multi-step logic.
- Evidence anchors:
  - [abstract]: "Through a critical review of relevant datasets and benchmarks, we assess LLMs’ performance, identifying significant challenges in complex puzzle scenarios."
  - [section]: "Prompting strategies that provide intermediate reasoning steps are pivotal in enhancing the puzzle-solving capabilities of language models."
- Break condition: If the added complexity of methods like ToT leads to increased computational costs without proportional gains in accuracy, their utility may be limited.

### Mechanism 3
- Claim: Neuro-symbolic approaches that translate natural language puzzles into formal representations (e.g., logic rules or code) enable the use of external solvers, leveraging LLM strengths in language understanding.
- Mechanism: LLMs generate formal representations from puzzle descriptions, which are then solved by symbolic solvers or code execution engines, combining linguistic comprehension with formal reasoning capabilities.
- Core assumption: LLMs can accurately interpret puzzle descriptions and generate correct formal representations that external tools can process effectively.
- Evidence anchors:
  - [section]: "The primary approach involves using LLMs to generate logic rules from the puzzle’s natural language and subsequently solve it using a symbolic solver."
  - [corpus]: Weak—neuro-symbolic methods are noted as underutilized in puzzle benchmarks, with no studies found on translating rule-less puzzles into code.
- Break condition: If the translation step introduces errors or if the external solvers cannot handle the complexity of the generated representations, the overall method may fail.

## Foundational Learning

- Concept: Taxonomy of puzzle types (rule-based vs. rule-less)
  - Why needed here: Understanding the puzzle categorization is essential for selecting appropriate LLM methods and evaluating performance across different reasoning challenges.
  - Quick check question: What are the key differences between deterministic and stochastic games in the context of LLM puzzle-solving?

- Concept: Prompting techniques (Chain-of-Thought, Tree-of-Thought, etc.)
  - Why needed here: These techniques are central to improving LLM reasoning capabilities, and understanding their mechanisms helps in applying them effectively to different puzzle types.
  - Quick check question: How does Tree-of-Thought differ from Chain-of-Thought in structuring reasoning pathways?

- Concept: Neuro-symbolic integration
  - Why needed here: Combining LLM language understanding with formal reasoning systems can enhance puzzle-solving, especially for rule-based puzzles with clear logical structures.
  - Quick check question: Why might neuro-symbolic approaches be particularly suitable for rule-based deterministic puzzles?

## Architecture Onboarding

- Component map:
  - Taxonomy Engine: Classifies puzzles into rule-based (deterministic/stochastic) and rule-less categories
  - Method Selector: Chooses appropriate LLM methods (prompting, neuro-symbolic, fine-tuning) based on puzzle type
  - Prompt Generator: Creates prompts incorporating techniques like CoT, ToT, or self-consistency
  - Solver Interface: Connects to external symbolic solvers or code execution engines for neuro-symbolic approaches
  - Evaluator: Assesses LLM performance using benchmarks and metrics specific to each puzzle category

- Critical path:
  1. Input puzzle → Taxonomy Engine classifies puzzle type
  2. Method Selector chooses appropriate methods
  3. Prompt Generator creates tailored prompts
  4. LLM processes prompts and generates solution
  5. If neuro-symbolic, Solver Interface translates and solves
  6. Evaluator compares solution to ground truth

- Design tradeoffs:
  - Prompt complexity vs. computational efficiency (e.g., ToT requires more LLM invocations than CoT)
  - Generalization vs. specialization (fine-tuning improves performance on specific tasks but may reduce adaptability)
  - Language understanding vs. formal reasoning (neuro-symbolic approaches leverage both but depend on accurate translation)

- Failure signatures:
  - Poor performance on stochastic puzzles may indicate insufficient probabilistic reasoning capabilities.
  - Inability to solve rule-less riddles suggests limitations in commonsense knowledge or abstract reasoning.
  - High computational costs with minimal accuracy gains may signal inefficient method selection.

- First 3 experiments:
  1. Apply Chain-of-Thought prompting to a Sudoku puzzle and measure accuracy improvement over simple prompting.
  2. Use Tree-of-Thought on a Rubik's Cube puzzle and compare success rates and computational costs to Chain-of-Thought.
  3. Implement a neuro-symbolic approach by translating a logic puzzle into Answer Set Programming and solving it with an external solver, then evaluate the accuracy of the LLM's translation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to LLMs would enable them to achieve human-level performance on rule-less puzzles like riddles and commonsense reasoning tasks?
- Basis in paper: [explicit] The survey explicitly states that "Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference" and "there is a notable performance gap between LLMs and human levels" for riddles and commonsense reasoning puzzles.
- Why unresolved: Despite various prompting methods and fine-tuning approaches being tested, the fundamental gap in reasoning capabilities remains. The paper identifies this as a key challenge but doesn't propose specific architectural solutions beyond current methodologies.
- What evidence would resolve it: A systematic study comparing LLM architectures (transformer variants, memory mechanisms, neuro-symbolic hybrids) on rule-less puzzles, showing which architectural features correlate with human-level performance and why current architectures fall short.

### Open Question 2
- Question: How can neuro-symbolic approaches be effectively extended from rule-based to rule-less puzzles, particularly for translating natural language riddles and commonsense reasoning problems into formal representations?
- Basis in paper: [explicit] The paper notes that "While neuro-symbolic approaches have been applied to puzzle translation into logic rules, we have found no studies on transforming puzzles from natural language into code" and this "suggests a potential area for future exploration."
- Why unresolved: Current neuro-symbolic methods focus on structured rule-based puzzles but haven't been adapted to handle the ambiguity and metaphorical language in rule-less puzzles. The paper identifies this as an underutilized approach.
- What evidence would resolve it: Development and evaluation of neuro-symbolic frameworks that can parse and formalize the linguistic complexity of riddles and commonsense scenarios, demonstrating improved performance over pure language model approaches.

### Open Question 3
- Question: What are the fundamental limitations of current prompting methods (CoT, ToT, XoT) that prevent them from achieving optimal performance on complex stochastic puzzles like card games and social deduction games?
- Basis in paper: [inferred] The survey shows that while prompting methods show promise, they still struggle with stochastic environments. For instance, "GPT-4 improved mine identification but struggled to complete boards" in Minesweeper, and fine-tuning LLMs "falter in complex settings that require extensive multi-step reasoning."
- Why unresolved: The paper identifies performance issues but doesn't deeply analyze the theoretical limitations of prompting approaches when dealing with uncertainty, hidden information, and probabilistic reasoning.
- What evidence would resolve it: A comprehensive analysis of why prompting methods fail on stochastic puzzles, potentially through ablation studies that isolate the reasoning components that break down under uncertainty, and theoretical frameworks explaining these limitations.

## Limitations

- Data Coverage Uncertainty: The survey identifies significant gaps in puzzle datasets and benchmarks, particularly for stochastic and rule-less puzzle categories, limiting generalizability of findings.
- Methodological Scope Limitations: The analysis focuses primarily on zero-shot and few-shot learning scenarios, potentially overlooking performance improvements achievable through task-specific fine-tuning.
- Performance Gap Ambiguity: The survey identifies a persistent performance gap between LLMs and human reasoning but lacks specific quantitative metrics defining this gap across different puzzle categories.

## Confidence

- High Confidence: The taxonomy-based categorization of puzzles and the identification of prompting techniques as effective methods are well-supported by multiple references.
- Medium Confidence: The analysis of performance disparities across puzzle categories is supported by literature review but would benefit from more quantitative validation.
- Low Confidence: The survey's recommendations for future research directions based on literature gaps are more speculative than empirically validated.

## Next Checks

1. **Dataset Coverage Analysis**: Conduct a quantitative audit of available puzzle datasets across all identified categories, measuring representation gaps and identifying specific puzzle types lacking adequate benchmarks.

2. **Method Performance Benchmarking**: Implement a standardized benchmark comparing Chain-of-Thought, Tree-of-Thought, and neuro-symbolic approaches across multiple puzzle types using consistent metrics.

3. **Human-LLM Performance Gap Quantification**: Design experiments to measure specific performance gaps between LLMs and human solvers across different puzzle complexity levels, using standardized metrics to quantify the disparity.