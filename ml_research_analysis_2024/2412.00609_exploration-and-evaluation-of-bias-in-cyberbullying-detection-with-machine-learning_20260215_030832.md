---
ver: rpa2
title: Exploration and Evaluation of Bias in Cyberbullying Detection with Machine
  Learning
arxiv_id: '2412.00609'
source_url: https://arxiv.org/abs/2412.00609
tags:
- dataset
- cyberbullying
- data
- datasets
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates bias in cyberbullying detection through
  machine learning models, focusing on the impact of differing definitions and data
  collection methods. The research uses three popular cyberbullying datasets to evaluate
  model generalizability through cross-dataset testing, revealing significant performance
  drops when models are applied to unseen data.
---

# Exploration and Evaluation of Bias in Cyberbullying Detection with Machine Learning

## Quick Facts
- arXiv ID: 2412.00609
- Source URL: https://arxiv.org/abs/2412.00609
- Reference count: 18
- Primary result: Significant performance drops in cross-dataset evaluation, with average 0.222 decrease in Macro F1 Score

## Executive Summary
This study investigates bias in cyberbullying detection through machine learning models, focusing on the impact of differing definitions and data collection methods. The research uses three popular cyberbullying datasets to evaluate model generalizability through cross-dataset testing, revealing significant performance drops when models are applied to unseen data. Key findings show an average decrease of 0.222 in Macro F1 Score across experiments, highlighting the importance of dataset curation and cross-dataset testing. The study identifies issues with automated dataset expansion methods and emphasizes that many cyberbullying datasets are only useful for their original context.

## Method Summary
The study evaluates cyberbullying detection models using three datasets collected through different methods: traditional keyword filtering, human labeling, and Dynamic Query Expansion (DQE). Models are trained using stratified K-fold cross-validation with early stopping, then tested on unseen datasets to measure generalizability. The primary evaluation metric is Macro F1 Score, with CatBoost and XGBoost classifiers tested using various vectorization approaches. The research systematically compares cross-validation performance to cross-dataset performance to identify sources of bias and generalization failure.

## Key Results
- Average 0.222 decrease in Macro F1 Score when models are applied to unseen datasets
- Cross-validation performance consistently overstates real-world applicability
- Models trained on DQE-expanded datasets show particular susceptibility to bias
- Vocabulary mismatch and definitional differences significantly impact cross-dataset performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic Query Expansion (DQE) introduces bias by labeling new tweets with the same class as the query tweet, regardless of actual content.
- Mechanism: DQE queries for new tweets using top keywords from each class, then assigns the same label as the originating tweet. This creates a self-reinforcing loop where keyword co-occurrence drives labeling, not semantic understanding.
- Core assumption: The keywords most associated with a class in the training data are reliable indicators of that class in new, unseen tweets.
- Evidence anchors: [abstract] "An emphasis is made on the impact of dataset expansion methods, which utilize current data points to fetch and label new ones." [section] "The method, while removing the burden on human labelers, has an apparent downside and may result in a model approximating the DQE function, rather than the human labelers."

### Mechanism 2
- Claim: Differing definitions of cyberbullying across datasets cause poor cross-dataset generalization.
- Mechanism: Each dataset uses a different operational definition of cyberbullying, leading to fundamentally different labeling functions (H1 vs H2). Models trained on one definition struggle when applied to data labeled under another.
- Core assumption: The labeling function H used to create the training data is consistent with the labeling function H' used in the test data.
- Evidence anchors: [abstract] "The bias introduced from differing definitions of cyberbullying and from data collection is discussed in detail." [section] "A significant challenge in this field is the poor cross-dataset performance, where performance declines substantially when models are applied to datasets different from those on which they were trained."

### Mechanism 3
- Claim: Lexicon-based data collection overemphasizes explicit language and misses nuanced cyberbullying.
- Mechanism: Using predefined lists of offensive terms (lexicons) to collect tweets creates a dataset biased toward explicit language, while subtle forms of cyberbullying go undetected. This leads to models that overfit to explicit language patterns.
- Core assumption: Offensive words in a lexicon are reliable indicators of cyberbullying across all contexts and datasets.
- Evidence anchors: [abstract] "A less explicit form of bias is the bias from data collection methods. Data collection methods used to curate cyberbullying datasets frequently rely on a lexicon of keywords...to obtain tweets that include offensive words." [section] "This reliance tends to overfit models to the specific language patterns of a single dataset, making them less effective on datasets where abusive language may be more nuanced or context-dependent."

## Foundational Learning

- Concept: Cross-validation methodology
  - Why needed here: The study uses stratified K-fold cross-validation to evaluate model performance and select hyperparameters before cross-dataset testing.
  - Quick check question: What is the purpose of using stratified K-fold cross-validation instead of simple train-test split in this study?

- Concept: Macro F1 Score calculation
  - Why needed here: The study uses Macro F1 Score as the primary evaluation metric because it accounts for both classes and is less affected by class imbalance than accuracy.
  - Quick check question: How does Macro F1 Score differ from Micro F1 Score, and why is it more appropriate for imbalanced cyberbullying datasets?

- Concept: Out-of-vocabulary (OOV) terms impact
  - Why needed here: The study investigates whether OOV terms explain performance drops in cross-dataset evaluation through correlation tests.
  - Quick check question: If a test dataset contains 30% OOV terms relative to the training vocabulary, what would you expect to happen to model performance?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Vectorization -> Model training -> Cross-validation -> Cross-dataset evaluation

- Critical path:
  1. Load and preprocess dataset
  2. Convert labels to binary Cyberbullying/Not Cyberbullying
  3. Apply stratified K-fold cross-validation with early stopping
  4. Select top-performing models based on Macro F1 Score
  5. Train selected models on full training dataset
  6. Evaluate on unseen datasets
  7. Compare cross-dataset performance to cross-validation performance

- Design tradeoffs:
  - Simple bag-of-words vs. contextual embeddings: The study uses traditional vectorizers for interpretability, but this limits detection of nuanced language.
  - Binary vs. multi-class classification: Converting to binary simplifies the problem but loses granularity in cyberbullying types.
  - Lexicon-based vs. context-based data collection: Lexicon methods are efficient but create bias toward explicit language.

- Failure signatures:
  - High variance in cross-validation scores (std > 0.01) indicates overfitting
  - Large drop (>0.2) in Macro F1 Score during cross-dataset evaluation indicates poor generalization
  - Correlation between OOV ratio and performance drop is weak or negative, suggesting other factors dominate

- First 3 experiments:
  1. Replicate the cross-validation on Dataset 1 using only tweets from original labeling (exclude DQE samples) and compare performance to full dataset.
  2. Train a model on Dataset 2 and test on Dataset 3, then manually analyze misclassified tweets to identify whether failures are due to definitional differences or linguistic patterns.
  3. Implement a simple context-aware classifier (e.g., using word embeddings) on Dataset 1 and evaluate whether it generalizes better than the bag-of-words approach to Dataset 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between automated dataset expansion methods and human labeling to minimize bias while maintaining dataset size?
- Basis in paper: [explicit] The paper discusses the Dynamic Query Expansion (DQE) method and its potential to introduce bias, estimating that 25,358 samples from Dataset 1 were created via DQE.
- Why unresolved: The paper highlights the bias introduced by DQE but doesn't provide a clear threshold or methodology for determining when automated expansion becomes problematic.
- What evidence would resolve it: Controlled experiments comparing model performance using varying proportions of DQE-generated data versus human-labeled data, with systematic measurement of bias indicators.

### Open Question 2
- Question: How do different cyberbullying definitions impact model generalizability across datasets with varying labeling schemes?
- Basis in paper: [explicit] The paper discusses multiple cyberbullying definitions and their impact on labeling processes, noting that models struggle when trained on one definition and tested on another.
- Why unresolved: While the paper demonstrates performance drops in cross-dataset testing, it doesn't isolate the specific impact of definitional differences versus other factors like data collection methods.
- What evidence would resolve it: Systematic experiments training models on datasets with different definitions but similar data collection methods, and vice versa, to isolate the impact of definitional differences.

### Open Question 3
- Question: What alternative data collection strategies beyond lexicon-based methods could improve cross-dataset performance in cyberbullying detection?
- Basis in paper: [inferred] The paper critiques lexicon-based methods for over-emphasizing explicit language and causing overfitting to specific datasets.
- Why unresolved: The paper identifies the limitations of current methods but doesn't propose or test alternative data collection strategies.
- What evidence would resolve it: Comparative studies of model performance using different data collection approaches (e.g., context-based sampling, active learning, or unsupervised clustering) across multiple datasets.

## Limitations

- Small sample size: The study is limited to three datasets, primarily focused on English-language Twitter data
- Theoretical bias attribution: The paper identifies potential bias sources but does not experimentally isolate their individual contributions
- Missing modern approaches: The study does not explore transformer-based models that might mitigate some identified biases

## Confidence

- High Confidence: The empirical findings of significant performance drops (average 0.222 decrease in Macro F1 Score) during cross-dataset evaluation are well-supported by the experimental results.
- Medium Confidence: The attribution of performance drops to specific bias sources (DQE methods, lexicon-based collection, definitional differences) is reasonable but not definitively proven, as the study does not isolate these factors experimentally.
- Low Confidence: The generalizability of findings to other social media platforms, languages, or cyberbullying definitions beyond the three studied datasets.

## Next Checks

1. Conduct controlled experiments isolating the impact of DQE methods by comparing models trained on manually-labeled subsets versus DQE-expanded subsets from the same dataset.
2. Replicate the cross-dataset evaluation using transformer-based models (e.g., BERT, DeBERTa) to assess whether contextual embeddings reduce the observed generalization gap.
3. Perform a systematic analysis of definitional variance by having multiple annotators label a common set of tweets using different cyberbullying definitions, then measure inter-annotator agreement and its correlation with model performance.