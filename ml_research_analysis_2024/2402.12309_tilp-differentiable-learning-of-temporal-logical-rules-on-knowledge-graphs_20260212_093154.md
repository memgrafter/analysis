---
ver: rpa2
title: 'TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs'
arxiv_id: '2402.12309'
source_url: https://arxiv.org/abs/2402.12309
tags:
- temporal
- rule
- rules
- learning
- logical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TILP, a differentiable framework for learning
  temporal logical rules on temporal knowledge graphs. The method uses constrained
  random walks and incorporates temporal features like recurrence, order, interval,
  and duration.
---

# TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs

## Quick Facts
- arXiv ID: 2402.12309
- Source URL: https://arxiv.org/abs/2402.12309
- Authors: Siheng Xiong; Yuan Yang; Faramarz Fekri; James Clayton Kerce
- Reference count: 10
- One-line primary result: TILP achieves comparable performance to state-of-the-art embedding-based methods while providing interpretable rule-based explanations for temporal link prediction.

## Executive Summary
This paper introduces TILP, a differentiable framework for learning temporal logical rules on temporal knowledge graphs. The method uses constrained random walks and incorporates temporal features like recurrence, order, interval, and duration. TILP learns attention-based rule confidences and temporal feature distributions. Experiments on WIKIDATA12k and YAGO11k datasets show TILP achieves comparable performance to state-of-the-art embedding-based methods while providing interpretable rule-based explanations. TILP outperforms baselines in scenarios with few training samples, biased data, and time-shifted evaluation, demonstrating robustness and flexibility.

## Method Summary
TILP addresses temporal link prediction on temporal knowledge graphs through a differentiable learning framework. The method consists of two phases: first, learning temporal logical rules using constrained random walks that satisfy both Markovian (predicate and temporal relations) and non-Markovian (temporal relations between body intervals) constraints; second, applying these rules with learned attention-based confidences and temporal feature scores. The framework extracts rules by exploring paths in the knowledge graph while respecting temporal constraints, then learns attention vectors for predicates and temporal relations to estimate rule confidence. Temporal features including recurrence, order, interval, and duration are modeled using continuous distributions and evaluated using both facts and constrained random walks as evidence.

## Key Results
- TILP achieves comparable performance to state-of-the-art embedding-based methods (e.g., TimePlex) while providing interpretable rule-based explanations
- TILP outperforms baselines in scenarios with few training samples, biased data, and time-shifted evaluation
- The framework addresses limitations of existing methods by handling interval-based temporal relations and learning relative temporal patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The constrained random walk mechanism enables efficient exploration of temporal logical rules by combining Markovian and non-Markovian constraints.
- Mechanism: The framework performs random walks under Markovian constraints (predicates and temporal relations between query and body intervals) and then filters results based on non-Markovian constraints (temporal relations between body intervals). This two-stage approach ensures efficiency while maintaining constraint satisfaction.
- Core assumption: Temporal logical rules can be decomposed into Markovian and non-Markovian components, where the Markovian part can be handled through adjacency matrix operations.
- Evidence anchors:
  - [section]: "Since a successful random walk should satisfy both classes of constraints, we first perform random walk under Markovian ones, and then filter out the results according to non-Markovian ones."
  - [abstract]: "By designing a constrained random walk mechanism and the introduction of temporal operators, we ensure the efficiency of our model."

### Mechanism 2
- Claim: Attention-based confidence estimation for constraints creates a joint and robust learning process while reducing model parameters.
- Mechanism: Instead of estimating confidence for every single rule, the framework creates attention vectors for every type of constraint (predicates and temporal relations) that are reused across different rules. These attention vectors are learned through RNN-based mapping functions.
- Core assumption: Constraint confidence is independent of specific entity substitutions and depends primarily on the target predicate and rule length.
- Evidence anchors:
  - [section]: "Instead of estimating confidence for every single rule, we create attention vectors for every type of constraints, and re-use them in different rules. Sharing confidence of constraints creates a joint and robust learning process, and largely reduces model parameters."
  - [abstract]: "We present temporal features modeling in tKG, e.g., recurrence, temporal order, interval between pair of relations, and duration, and incorporate it into our learning process."

### Mechanism 3
- Claim: Temporal feature modeling extends the framework's capabilities beyond simple temporal relations to capture continuous distributions and additional evidence.
- Mechanism: The framework models temporal features including recurrence, temporal order, relation pair intervals, and duration using continuous distributions (Gaussian, exponential). These features are evaluated using both facts and constrained random walks as evidence.
- Core assumption: Temporal patterns in knowledge graphs follow predictable distributions that can be captured through parametric models.
- Evidence anchors:
  - [section]: "To address these limitations, we introduce temporal features modeling where extra evidences and continuous distribution measurements are involved."
  - [corpus]: "DeepDFA: Injecting Temporal Logic in Deep Learning for Sequential Subsymbolic Applications" - weak connection as this paper focuses on temporal logic in neural networks but doesn't specifically address knowledge graph reasoning.

## Foundational Learning

- Concept: Temporal Knowledge Graphs (tKGs)
  - Why needed here: Understanding tKGs is fundamental to grasping why temporal reasoning is challenging and why TILP's approach is necessary.
  - Quick check question: What distinguishes a temporal knowledge graph from a static knowledge graph, and why does this distinction matter for link prediction?

- Concept: Random Walks on Graphs
  - Why needed here: The constrained random walk mechanism is central to TILP's operation, so understanding standard random walks is prerequisite.
  - Quick check question: How does a standard random walk differ from a constrained random walk, and what additional information does the latter need to track?

- Concept: Attention Mechanisms in Neural Networks
  - Why needed here: The attention-based confidence estimation is a key innovation, requiring understanding of how attention works in neural architectures.
  - Quick check question: How does an attention mechanism help in learning which constraints are more important for rule confidence?

## Architecture Onboarding

- Component map: Query -> Constrained random walk -> Rule extraction -> Attention learning -> Temporal feature modeling -> Combined scoring -> Output
- Critical path: Query → Constrained random walk → Rule application → Temporal feature evaluation → Combined scoring → Output
- Design tradeoffs:
  - Efficiency vs. completeness in random walk exploration
  - Parameter sharing (attention vectors) vs. specificity to individual rules
  - Continuous temporal modeling vs. discrete temporal relations
  - Interpretability (logical rules) vs. performance (embedding methods)
- Failure signatures:
  - Poor performance on rare relations → Attention vectors not capturing specific patterns
  - Slow inference → Random walk exploration too exhaustive
  - Overfitting on training data → Temporal feature distributions too specific
  - Inconsistent explanations → Rule confidence estimation unstable
- First 3 experiments:
  1. Implement the constrained random walk on a small synthetic tKG to verify path generation
  2. Test attention learning on a simplified rule set to ensure confidence estimation works
  3. Validate temporal feature modeling on a dataset with known temporal patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would TILP perform on temporal knowledge graphs with higher temporal granularity (e.g., monthly or daily intervals) compared to the yearly resolution used in the experiments?
- Basis in paper: [explicit] The paper mentions that "for datasets with higher granularity, we would expect improved performance due to more precise temporal relations" and discusses experiments using a yearly resolution on WIKIDATA12k and YAGO11k datasets.
- Why unresolved: The experiments only tested yearly resolution, so empirical evidence for higher temporal granularity is lacking.
- What evidence would resolve it: Running TILP on datasets with monthly or daily temporal granularity and comparing performance metrics (MRR, hit@1, hit@10) to the yearly resolution results.

### Open Question 2
- Question: What is the impact of varying the maximum rule length parameter (currently set to 5) on TILP's performance and computational efficiency?
- Basis in paper: [explicit] The paper states "The maximum rule length is set to 5 for both datasets" but doesn't explore how performance changes with different maximum lengths.
- Why unresolved: The paper doesn't investigate the trade-off between longer rules (potentially more expressive) and computational cost.
- What evidence would resolve it: Systematic experiments varying the maximum rule length (e.g., 3, 5, 7, 10) and measuring both performance and computational efficiency.

### Open Question 3
- Question: How does TILP's performance compare to state-of-the-art methods on other temporal knowledge graph datasets beyond WIKIDATA12k and YAGO11k?
- Basis in paper: [explicit] The paper only evaluates on WIKIDATA12k and YAGO11k datasets, though it mentions these are "standard" benchmarks.
- Why unresolved: The results may be specific to these particular datasets and their characteristics.
- What evidence would resolve it: Testing TILP on additional temporal knowledge graph datasets (e.g., ICEWS, GDELT with temporal extensions) and comparing results to other state-of-the-art methods.

## Limitations
- Scalability concerns with constrained random walks for large temporal knowledge graphs
- Dependence on parametric distributions for temporal features may struggle with irregular or non-stationary temporal patterns
- The assumption that temporal constraints can be cleanly decomposed into Markovian and non-Markovian components may not hold for complex temporal patterns

## Confidence
- Constrained random walk mechanism: High
- Attention-based confidence estimation: High
- Temporal feature modeling: Medium
- Comparative performance claims: Medium

## Next Checks
1. Test constrained random walk scalability on a large-scale temporal knowledge graph (e.g., GDELT) to identify performance bottlenecks and assess whether the Markovian/non-Markovian decomposition remains effective.

2. Validate temporal feature modeling on a dataset with known irregular temporal patterns (e.g., social media event data) to evaluate the framework's robustness to non-standard temporal distributions.

3. Implement ablation studies removing the temporal feature modeling component to quantify its contribution to overall performance and determine whether the additional complexity is justified across different dataset characteristics.