---
ver: rpa2
title: What Makes Cryptic Crosswords Challenging for LLMs?
arxiv_id: '2412.09012'
source_url: https://arxiv.org/abs/2412.09012
tags:
- clue
- wordplay
- definition
- answer
- cryptic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how well three popular LLMs (Gemma2, LLaMA3,
  and ChatGPT) solve cryptic crossword clues. The authors compare zero-shot performance
  across two datasets and find that ChatGPT performs best (11.4% accuracy), but all
  models still fall well short of human solvers.
---

# What Makes Cryptics Challenging for LLMs?

## Quick Facts
- arXiv ID: 2412.09012
- Source URL: https://arxiv.org/abs/2412.09012
- Authors: Abdelrahman Sadallah; Daria Kotova; Ekaterina Kochmar
- Reference count: 19
- Key outcome: ChatGPT achieves 11.4% accuracy on cryptic crossword clues, outperforming Gemma2 and LLaMA3, but all models fall well short of human solvers

## Executive Summary
This paper investigates why current LLMs struggle with cryptic crossword clues by evaluating three popular models (Gemma2, LLaMA3, and ChatGPT) on two datasets. The authors find that while ChatGPT performs best at 11.4% accuracy, all models remain far below human performance. Through decomposition into subtasks—definition extraction, wordplay type detection, and explanation generation—the study reveals that models perform significantly better on definition extraction (up to 41.2% for ChatGPT) than on identifying wordplay types or generating coherent explanations. The research concludes that LLMs cannot effectively decompose the composite task of solving cryptic clues independently and suggests future work on chain-of-thought prompting, curriculum learning, and mixture-of-experts approaches.

## Method Summary
The study evaluates three LLMs (Gemma2, LLaMA3, ChatGPT) using zero-shot prompting on two cryptic crossword datasets: Guardian (28,476 examples) and Times for the Times (1,000 examples). Models are tested with base prompts ("you are a cryptic crosswords expert") and all-inclusive prompts containing additional definition information. The research decomposes the task into subtasks: definition extraction, wordplay type detection, and explanation generation, using exact match accuracy as the primary metric. Experiments employ greedy sampling with temperature 0 and focus on interpretability by analyzing model outputs for each component of cryptic clue solving.

## Key Results
- ChatGPT achieves highest accuracy at 11.4% on full clue solving, significantly outperforming open-source models
- Models perform substantially better on definition extraction (up to 41.2% for ChatGPT) than on full clue solving
- LLMs struggle with wordplay type detection and providing coherent explanations for their answers
- All models show significant performance gaps compared to human solvers on this complex linguistic reasoning task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can achieve better performance on definition extraction than on full clue solving because the definition is explicitly provided in the clue and can be treated as a retrieval task rather than a generation task.
- Mechanism: When the definition is explicitly included in the clue, models can more easily identify it as a synonym of the answer, leveraging their pre-trained knowledge of word relationships.
- Core assumption: The definition is usually positioned at the beginning or end of the clue and is a direct synonym of the answer.
- Evidence anchors:
  - [abstract]: "Models achieve better results on definition extraction (up to 41.2% for ChatGPT) but struggle with identifying wordplay types and providing coherent explanations."
  - [section 5.2.1]: "All models show higher results in the definition extraction task. One reason for this could be that the definition is explicitly included in the clue, making the task a matter of repeating part of the clue, which is generally easier than generating new words as an answer."
- Break condition: If the definition is not positioned at the beginning or end of the clue, or if it's not a direct synonym of the answer, this mechanism would fail.

### Mechanism 2
- Claim: Adding more information to the prompt (such as definitions, explanations, and examples) improves model performance on wordplay type detection.
- Mechanism: Providing additional context and examples helps models understand the specific characteristics of each wordplay type, enabling better classification.
- Core assumption: Models can effectively utilize additional information to improve their understanding of wordplay types.
- Evidence anchors:
  - [section 5.2.2]: "The results show that adding the definition for the wordplay and providing a model with the answer do not significantly improve the model's ability to extract the wordplay type except for Gemma, which has a performance increase of 10%."
  - [section 5.2.2]: "Providing more information in the other prompts helped the model predict other types."
- Break condition: If the additional information is not relevant or is too complex for the model to process effectively, this mechanism would fail.

### Mechanism 3
- Claim: ChatGPT outperforms open-source models on cryptic crossword solving due to its superior pre-training and instruction-tuning.
- Mechanism: ChatGPT's training data and fine-tuning process have likely exposed it to a wider variety of linguistic patterns and puzzle-solving strategies, enabling better performance on complex tasks.
- Core assumption: ChatGPT has been exposed to more diverse training data and has undergone more extensive fine-tuning for language understanding tasks.
- Evidence anchors:
  - [abstract]: "ChatGPT performs best (11.4% accuracy), but all models still fall well short of human solvers."
  - [section 5.1]: "We can see that ChatGPT outperforms the open-source models."
- Break condition: If the open-source models undergo similar training and fine-tuning processes, this performance gap may narrow.

## Foundational Learning

- Concept: Wordplay types and their characteristics
  - Why needed here: Understanding the different types of wordplay is crucial for solving cryptic crossword clues and for designing experiments to test model performance.
  - Quick check question: Can you list the five major wordplay types mentioned in the paper and provide an example of each?

- Concept: Prompt engineering techniques
  - Why needed here: Effective prompting is essential for eliciting the desired responses from LLMs and for improving their performance on complex tasks.
  - Quick check question: How does the "all inclusive prompt" differ from the "base prompt" in terms of the information provided to the model?

- Concept: Evaluation metrics for NLP tasks
  - Why needed here: Understanding how to measure and compare model performance is crucial for assessing the effectiveness of different approaches and for drawing meaningful conclusions from experimental results.
  - Quick check question: What metric is used to evaluate model performance on the cryptic crossword solving task, and why is it appropriate for this task?

## Architecture Onboarding

- Component map: Data preprocessing -> Model selection -> Prompt engineering -> Inference -> Evaluation
- Critical path: 1. Data preprocessing: Prepare the datasets and parse the clues 2. Model selection: Choose the appropriate LLMs for the task 3. Prompt engineering: Design effective prompts for each task 4. Inference: Generate model outputs using the chosen prompts 5. Evaluation: Compare model performance using appropriate metrics
- Design tradeoffs: Model size vs. inference speed: Larger models may perform better but require more computational resources and time; Prompt complexity vs. model understanding: More complex prompts may provide more information but could also confuse the model; Dataset size vs. annotation quality: Larger datasets may provide more diverse examples but may require less detailed annotations
- Failure signatures: Low accuracy on full clue solving but high accuracy on definition extraction: The model may struggle with the wordplay component of the clue; Over-prediction of certain wordplay types: The model may have a bias towards certain types of wordplay or may not have enough examples of other types; Inconsistent outputs across different runs: The model may be sensitive to slight variations in the prompt or may have difficulty with the task
- First 3 experiments: 1. Compare model performance on full clue solving vs. definition extraction to assess the relative difficulty of each task 2. Test the impact of adding more information to the prompt on wordplay type detection performance 3. Analyze the models' explanations for their answers to identify common errors and areas for improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning LLMs on cryptic crossword datasets significantly improve their performance beyond zero-shot prompting?
- Basis in paper: [explicit] The paper mentions that Sadallah et al. (2024) explicitly fine-tuned open-source LLMs for cryptic crossword solving, while this study used only zero-shot prompting with pre-trained models.
- Why unresolved: This paper focuses on zero-shot performance and interpretability of LLMs' reasoning rather than training/fine-tuning approaches.
- What evidence would resolve it: Direct comparison of fine-tuned vs. zero-shot performance on the same dataset using the same evaluation metrics.

### Open Question 2
- Question: How would LLMs perform on full cryptic crossword grids with cross-referencing information versus single clues?
- Basis in paper: [inferred] The paper explicitly states they focus on solving one clue at a time rather than in a grid context, and acknowledges this is different from real-world solving scenarios.
- Why unresolved: The paper deliberately chose to study single-clue solving to isolate interpretability, avoiding the complexity of grid-based solving.
- What evidence would resolve it: Experimental comparison of LLM performance on single clues versus full grids, measuring accuracy and reasoning quality.

### Open Question 3
- Question: Would curriculum learning approaches that progressively introduce more complex wordplay types improve LLM performance on cryptic crosswords?
- Basis in paper: [explicit] The paper mentions that Rozner et al. (2021) achieved performance increases using curriculum learning with T5 models, and suggests this direction is worth exploring with LLMs.
- Why unresolved: The paper did not implement curriculum learning, instead focusing on zero-shot performance and auxiliary task analysis.
- What evidence would resolve it: Experimental results comparing standard training versus curriculum-based training on cryptic crossword datasets, measuring both accuracy and convergence speed.

## Limitations
- The study uses relatively small evaluation datasets (1,000 examples for main test set), which may not capture full diversity of cryptic crossword constructions
- Focus on zero-shot prompting without exploring chain-of-thought or few-shot approaches leaves open question of whether better prompting could significantly improve results
- Evaluation relies heavily on exact match accuracy for answers, which may be overly strict for a task where partial understanding of clue components could still demonstrate meaningful progress

## Confidence
- High confidence: The finding that all models perform poorly on full clue solving (11.4% max accuracy) is well-supported by the experimental results and methodology.
- Medium confidence: The claim that ChatGPT outperforms open-source models is supported but doesn't explore underlying reasons or whether this gap would persist with different prompting strategies.
- Medium confidence: The observation that models perform better on definition extraction than full solving is demonstrated, but the mechanism explanation relies on inference rather than direct testing.

## Next Checks
1. Test whether few-shot prompting with examples of successfully solved clues improves performance beyond the current zero-shot results, particularly for wordplay type detection.
2. Conduct an ablation study on the all-inclusive prompt to determine which specific components (definition information, examples, etc.) contribute most to performance improvements.
3. Evaluate models using a more granular metric that rewards partial credit for correctly identifying clue components (definition, wordplay type) even when the final answer is incorrect.