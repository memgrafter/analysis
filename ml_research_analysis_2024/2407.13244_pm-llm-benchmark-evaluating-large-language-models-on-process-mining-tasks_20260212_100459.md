---
ver: rpa2
title: 'PM-LLM-Benchmark: Evaluating Large Language Models on Process Mining Tasks'
arxiv_id: '2407.13244'
source_url: https://arxiv.org/abs/2407.13244
tags:
- process
- llms
- mining
- tasks
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PM-LLM-Benchmark provides the first comprehensive benchmark for
  evaluating large language models (LLMs) on process mining tasks, focusing on two
  implementation paradigms (direct insights and code generation) and including process-mining-specific
  and process-specific domain knowledge. Using LLM-as-a-Judge evaluation strategy,
  the benchmark assesses 25+ LLMs including commercial (GPT-4, Claude) and open-source
  (Qwen2, Llama3, Mixtral) models.
---

# PM-LLM-Benchmark: Evaluating Large Language Models on Process Mining Tasks

## Quick Facts
- arXiv ID: 2407.13244
- Source URL: https://arxiv.org/abs/2407.13244
- Reference count: 30
- 25+ LLMs evaluated on process mining tasks with commercial models achieving mean scores of 7.6-8.4/10

## Executive Summary
PM-LLM-Benchmark introduces the first comprehensive benchmark for evaluating large language models on process mining tasks, focusing on domain knowledge and two implementation paradigms: direct insights and code generation. Using an LLM-as-a-Judge evaluation strategy, the benchmark assesses commercial models (GPT-4, Claude) and open-source models (Qwen2, Llama3, Mixtral) on tasks including process discovery, anomaly detection, and hypothesis generation. Results show commercial and big open-source models achieve mean scores of 7.6-8.4/10, while smaller models demonstrate varying capabilities with quantized models experiencing significant performance degradation.

## Method Summary
The benchmark employs a comprehensive evaluation framework using LLM-as-a-Judge methodology, where advanced LLMs (like GPT-4) score responses from evaluated models on a 1-10 scale. It tests 25+ LLMs across two paradigms: direct insights and code generation, focusing on process-mining-specific and process-specific domain knowledge. The benchmark includes tasks like process discovery, anomaly detection, and hypothesis generation, with evaluation designed to assess both domain knowledge and practical implementation capabilities.

## Key Results
- Commercial and big open-source models achieve mean scores of 7.6-8.4/10 on process mining tasks
- Small models (≤8GB) achieve adequate scores (4.6-6.5) for simpler tasks
- Tiny models (≤4GB) remain inadequate for most process mining tasks
- Process model generation and understanding remain challenging for smaller models
- Quantization severely impacts LLM performance in process mining tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-as-Judge evaluation enables scalable, customizable assessment of open-ended process mining answers without requiring ground truth.
- Mechanism: By having a high-performing LLM evaluate the textual outputs of other LLMs, the benchmark can assess complex, open-ended tasks that lack definitive answers.
- Core assumption: The judge LLM is sufficiently more capable than the evaluated LLMs to provide reliable, unbiased scoring.
- Evidence anchors:
  - [abstract] "Using LLM-as-Judge evaluation strategy, the benchmark assesses 25+ LLMs"
  - [section] "As the output of LLMs for the proposed prompts is textual, we propose to use an advanced LLM (for instance, gpt-4o-20240513) as judge [28], assigning a score from 1.0 (minimum) to 10.0 (maximum) to each answer."
  - [corpus] Weak - corpus contains related work on LLM evaluation but not direct evidence for this specific mechanism
- Break condition: When the judge LLM is not significantly more capable than the evaluated LLMs, leading to biased or unreliable scoring.

### Mechanism 2
- Claim: The benchmark's focus on domain knowledge (process-mining-specific and process-specific) distinguishes it from general-purpose LLM benchmarks.
- Mechanism: By including prompts that require specific process mining domain knowledge, the benchmark can more accurately assess an LLM's practical utility for process mining tasks rather than just general language understanding.
- Core assumption: Process mining tasks require specialized domain knowledge that cannot be adequately assessed by general-purpose benchmarks.
- Evidence anchors:
  - [abstract] "focusing on domain knowledge (process-mining-specific and process-specific)"
  - [section] "Moreover, as the goal of process mining is to assist data-driven decision-making, we aim to assess how much the LLM is able to identify biases starting from the event data."
  - [corpus] Weak - corpus contains related work on process mining with LLMs but limited evidence on domain-specific knowledge assessment
- Break condition: When general-purpose LLM benchmarks sufficiently cover process mining domain knowledge, making this specialization unnecessary.

### Mechanism 3
- Claim: The benchmark's two implementation paradigms (direct insights and code generation) provide comprehensive assessment of LLM capabilities for process mining.
- Mechanism: By testing both direct answer provision and code generation for process mining tasks, the benchmark captures different practical approaches to implementing process mining analyses with LLMs.
- Core assumption: Both direct insight provision and code generation are valid and useful implementation paradigms for process mining with LLMs.
- Evidence anchors:
  - [abstract] "focusing on two implementation paradigms (direct insights and code generation)"
  - [section] "Our benchmark measures how much an LLM is knowledgeable and capable in process mining. The capability is measured in the correct interpretation and production of different process mining artifacts"
  - [corpus] Weak - corpus contains related work on LLM implementation paradigms but not direct evidence for this specific dual-paradigm approach
- Break condition: When one paradigm becomes clearly superior for process mining tasks, making the dual-paradigm approach redundant.

## Foundational Learning

- Concept: Process mining domain knowledge
  - Why needed here: The benchmark specifically assesses process mining domain knowledge, so understanding what constitutes this knowledge is essential
  - Quick check question: What are the main types of process mining analyses that an LLM should be able to perform?

- Concept: LLM evaluation methodologies
  - Why needed here: The benchmark uses LLM-as-Judge evaluation, requiring understanding of this approach and its limitations
  - Quick check question: What are the potential biases when using LLMs as judges for evaluating other LLMs?

- Concept: Prompt engineering for LLMs
  - Why needed here: The benchmark uses carefully crafted prompts to elicit process mining responses from LLMs
  - Quick check question: How do you balance providing sufficient context in a prompt while keeping it within context window limits?

## Architecture Onboarding

- Component map: Prompt repository -> LLM execution layer -> LLM-as-Judge evaluation layer -> Result aggregation/reporting
- Critical path: Prompt execution → LLM response generation → LLM-as-Judge evaluation → Score aggregation → Result reporting
- Design tradeoffs: Using static prompts vs. dynamic prompt generation; choosing evaluation granularity (task-level vs. category-level); balancing comprehensiveness vs. execution time
- Failure signatures: Inconsistent scoring across judge LLMs; timeouts due to context window limitations; evaluation bias toward verbose responses
- First 3 experiments:
  1. Execute a single prompt with a commercial LLM and verify the response format and content
  2. Use an LLM-as-Judge to evaluate a known-good and known-bad answer to establish baseline scoring behavior
  3. Run the full benchmark on a small subset of LLMs to verify end-to-end execution and identify potential issues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific factors that contribute to the performance differences between quantized and non-quantized versions of the same LLM model in process mining tasks?
- Basis in paper: [inferred] The paper mentions that "aggressive quantization has a severe impact on LLMs' abilities in the proposed benchmark" but does not provide precise explanations for this phenomenon.
- Why unresolved: The paper acknowledges the impact of quantization on performance but does not delve into the specific technical reasons behind this observation.
- What evidence would resolve it: Detailed comparative studies analyzing the effects of different quantization levels on LLM performance in process mining tasks, including memory usage, inference speed, and task-specific accuracy metrics.

### Open Question 2
- Question: How can the evaluation biases in LLM-as-Judge be effectively mitigated to ensure fair and accurate assessment of LLM performance in process mining tasks?
- Basis in paper: [explicit] The paper discusses the challenges of using LLM-as-Judge, including potential biases such as the "egocentric bias" and the difficulty in following arbitrary evaluation directives.
- Why unresolved: While the paper identifies these biases, it does not provide concrete solutions or methodologies to address them comprehensively.
- What evidence would resolve it: Development and validation of new evaluation frameworks or techniques that minimize biases in LLM-as-Judge, possibly through cross-validation with human evaluators or the use of multiple diverse judge models.

### Open Question 3
- Question: What are the key challenges in developing dynamic event data generation techniques that incorporate semantic anomalies and root causes for LLM benchmarking in process mining?
- Basis in paper: [explicit] The paper suggests that current simulation solutions lack the semantic understanding needed to generate data suitable for process mining assessment and proposes using LLMs for this purpose.
- Why unresolved: The paper highlights the need for such techniques but does