---
ver: rpa2
title: In-Context Learning for Extreme Multi-Label Classification
arxiv_id: '2401.12178'
source_url: https://arxiv.org/abs/2401.12178
tags:
- infer
- retrieve
- dspy
- calls
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extreme multi-label classification
  (XMC) where language models (LMs) struggle to handle thousands of classes. The proposed
  method, Infer-Retrieve-Rank (IReRa), uses a frozen retriever combined with two in-context
  learning modules to efficiently tackle XMC problems.
---

# In-Context Learning for Extreme Multi-Label Classification

## Quick Facts
- **arXiv ID**: 2401.12178
- **Source URL**: https://arxiv.org/abs/2401.12178
- **Reference count**: 7
- **Primary result**: State-of-the-art results on job vacancy datasets using frozen retriever and two in-context learning modules without fine-tuning

## Executive Summary
This paper introduces Infer-Retrieve-Rank (IReRa), a novel approach for extreme multi-label classification (XMC) that leverages frozen retrievers combined with two in-context learning modules. The method addresses the challenge of classifying inputs into thousands of possible labels without requiring fine-tuning of language models. IReRa achieves state-of-the-art results on three job vacancy datasets and competitive performance on a biomedical dataset, using only tens of labeled examples for optimization.

## Method Summary
IReRa implements a three-step program using the DSPy framework: an Infer module generates relevant queries from input documents using an LM, a frozen retriever finds labels matching these queries, and a Rank module re-ranks the retrieved labels using another LM. The entire system is optimized through DSPy's declarative programming model, requiring no fine-tuning of underlying models and only tens of labeled examples. The approach uses different LMs for each module (e.g., Llama-2-7b-chat for Infer, GPT-4 for Rank) and a frozen retriever, making it easily applicable to new tasks without prompt engineering.

## Key Results
- State-of-the-art performance on three job vacancy datasets (HOUSE, TECH, TECHWOLF)
- Meaningful traction on biomedical task (BioDEX) without fine-tuning
- Achieves competitive results using only ~50 labeled examples
- No prompt engineering required, easily applicable to new tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The frozen retriever becomes more flexible when the LM learns in-context to predict relevant queries.
- Mechanism: The Infer module uses an LM to predict queries from the input document, which are then passed to a frozen retriever. This allows the retriever to handle inputs that are not similar to its training data by leveraging the LM's ability to generate relevant queries.
- Core assumption: The LM can effectively predict queries that are semantically related to the labels, even if the retriever was not trained on those specific inputs.
- Evidence anchors:
  - [abstract]: "The key insights of Infer–Retrieve–Rankis that such a frozen retriever can be made much more flexible if the LM learns in-context how to predict relevant queries and interpret the retrieved results."
  - [section]: "The underlying LMs, retriever, and prompts are considered hyperparameters of theIReRa program, which can be tuned automatically or easily configured."

### Mechanism 2
- Claim: The two-step in-context learning approach (Infer and Rank) allows the system to achieve state-of-the-art results without fine-tuning.
- Mechanism: The Infer module generates queries, the frozen retriever retrieves relevant labels, and the Rank module re-ranks the retrieved labels using another LM. This multi-step approach leverages the strengths of LMs in understanding context and re-ranking without requiring model updates.
- Core assumption: The LMs used in the Infer and Rank modules can effectively understand the input and re-rank the retrieved labels without being fine-tuned on the specific task.
- Evidence anchors:
  - [abstract]: "Unlike prior work, our proposed solution requires no finetuning, is easily applicable to new tasks, alleviates prompt engineering, and requires only tens of labeled examples."
  - [section]: "Our program attains state-of-the-art results on the job vacancy datasets, and gets meaningful traction on the harder biomedical task—without finetuning, without prompt engineering, and by using only≈50 labeled examples."

### Mechanism 3
- Claim: The DSPy programming model allows for modular and declarative specification of the program, enabling automatic optimization.
- Mechanism: DSPy separates the program logic from the optimization process, allowing the system to be configured with different LMs and retrievers and optimized towards specific datasets using only a few labeled examples.
- Core assumption: The DSPy framework can effectively optimize the program's components by bootstrapping from a minimal seed-prompt and a few labeled examples.
- Evidence anchors:
  - [abstract]: "We implement this program using the DSPy programming model, which specifies in-context systems in a declarative manner, and use DSPy optimizers to tune it towards specific datasets by bootstrapping only tens of few-shot examples."
  - [section]: "Because our program consists of two in-context modules, we propose to bootstrap them sequentially."

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The system relies on LMs to perform tasks without fine-tuning, using only a few examples in the prompt.
  - Quick check question: What is the difference between in-context learning and fine-tuning, and why is in-context learning advantageous in this scenario?

- Concept: Multi-label classification
  - Why needed here: The task involves assigning multiple labels to each input, which is more complex than single-label classification.
  - Quick check question: How does multi-label classification differ from multi-class classification, and what are the challenges associated with it?

- Concept: Retriever-augmented generation
  - Why needed here: The system uses a frozen retriever to find relevant labels based on the queries generated by the LM.
  - Quick check question: What is the role of the retriever in this system, and how does it interact with the LM?

## Architecture Onboarding

- Component map:
  - Infer module (LM) -> Retrieve module (frozen retriever) -> Rank module (LM) -> Final ranked labels

- Critical path:
  1. Input document is processed by the Infer module to generate queries.
  2. Queries are passed to the frozen retriever to retrieve relevant labels.
  3. Retrieved labels are re-ranked by the Rank module.
  4. Final ranked labels are returned as the output.

- Design tradeoffs:
  - Using a frozen retriever reduces the need for fine-tuning but may limit flexibility if the retriever's training data is not representative of the input distribution.
  - Relying on in-context learning avoids fine-tuning but may require more LM calls per input, increasing inference costs.
  - The DSPy framework simplifies optimization but may have limitations in handling complex program structures.

- Failure signatures:
  - If the Infer module generates irrelevant queries, the retriever will return unrelated labels.
  - If the Rank module fails to re-rank the labels effectively, the final output may not be optimal.
  - If the DSPy optimization does not converge, the system may not achieve the desired performance.

- First 3 experiments:
  1. Test the Infer module alone to ensure it generates meaningful queries.
  2. Test the Retrieve module with the queries to verify it returns relevant labels.
  3. Test the Rank module to ensure it improves the ranking of the retrieved labels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Infer-Retrieve-Rank scale with different sizes of training data beyond the 10 unlabeled and ~50 labeled examples used in the experiments?
- Basis in paper: [explicit] The paper mentions using 10 unlabeled training examples and ~50 labeled validation examples for bootstrapping, but doesn't explore the effect of varying these numbers on performance.
- Why unresolved: The paper focuses on demonstrating effectiveness with minimal data but doesn't systematically investigate how performance changes with different dataset sizes.
- What evidence would resolve it: Experiments varying the number of training and validation examples (e.g., 5/25, 20/100, 50/250) and measuring the resulting performance changes would provide insight into scalability and data efficiency.

### Open Question 2
- Question: How robust is the optimization process across different language models and retrievers, and what are the trade-offs in performance vs. cost?
- Basis in paper: [inferred] The paper mentions that the choice of LMs and retrievers can be configured, but doesn't extensively explore the impact of different model combinations on performance and cost.
- Why unresolved: While the paper demonstrates effectiveness with specific model choices, it doesn't provide a comprehensive analysis of how different combinations of LMs and retrievers affect the overall performance and computational requirements.
- What evidence would resolve it: Systematic experiments replacing the current LMs and retrievers with alternatives (e.g., different sizes of Llama, other retrievers) and measuring both performance and cost metrics would clarify the trade-offs and robustness of the approach.

### Open Question 3
- Question: How does the performance of Infer-Retrieve-Rank compare to fine-tuned methods when more labeled data is available?
- Basis in paper: [explicit] The paper states that Infer-Retrieve-Rank achieves state-of-the-art results with minimal labeled data, but doesn't directly compare its performance to fine-tuned methods when larger amounts of labeled data are available.
- Why unresolved: The paper emphasizes the efficiency of Infer-Retrieve-Rank with limited data but doesn't explore whether it can maintain its advantage or if fine-tuned methods surpass it with more data.
- What evidence would resolve it: Experiments comparing Infer-Retrieve-Rank to fine-tuned methods using increasing amounts of labeled data (e.g., 100, 1000, 10000 examples) would reveal the scalability and potential limitations of the approach.

## Limitations

- The generalizability of the approach to domains outside of job vacancies and biomedical applications remains uncertain.
- The paper lacks detailed analysis of failure cases and ablation studies to understand the contribution of each component.
- The assertion that only tens of labeled examples are needed is not fully validated through systematic studies of minimum requirements.

## Confidence

- **High Confidence**: The IReRa framework's design is sound, leveraging the strengths of LMs in understanding context and re-ranking without requiring model updates. The use of DSPy for modular and declarative specification is a valid approach.
- **Medium Confidence**: The claim of achieving state-of-the-art results on the job vacancy datasets is supported by the reported metrics, but the comparison with other methods is not comprehensive. The paper lacks detailed analysis of the optimization process and the impact of hyperparameters.
- **Low Confidence**: The assertion that the method requires only tens of labeled examples for effective optimization is not fully validated. The paper does not provide a systematic study of the minimum number of examples needed or the impact of example quality on performance.

## Next Checks

1. **Domain Generalization Test**: Evaluate the IReRa framework on a diverse set of datasets from different domains to assess its generalizability and identify potential limitations in handling out-of-distribution inputs.

2. **Ablation Study**: Conduct a detailed ablation study to quantify the contribution of each component (Infer, Retrieve, Rank) and the impact of the DSPy optimization process. This will help identify the critical factors for the framework's success.

3. **Optimization Analysis**: Perform a systematic analysis of the DSPy optimization process, including the impact of the number and quality of labeled examples, the choice of hyperparameters, and the convergence behavior. This will provide insights into the framework's scalability and robustness.