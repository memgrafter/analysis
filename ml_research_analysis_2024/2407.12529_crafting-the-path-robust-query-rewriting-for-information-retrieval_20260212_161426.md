---
ver: rpa2
title: 'Crafting the Path: Robust Query Rewriting for Information Retrieval'
arxiv_id: '2407.12529'
source_url: https://arxiv.org/abs/2407.12529
tags:
- query
- rewriting
- information
- retrieval
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CRAFTING THE PATH is a structured query rewriting method for information
  retrieval that addresses the problem of factual inaccuracies in LLM-generated queries.
  The method involves three steps: Query Concept Comprehension, Query Type Identification,
  and Expected Answer Extraction, which together craft query-related information necessary
  for finding relevant passages.'
---

# Crafting the Path: Robust Query Rewriting for Information Retrieval

## Quick Facts
- arXiv ID: 2407.12529
- Source URL: https://arxiv.org/abs/2407.12529
- Authors: Ingeol Baek; Jimin Lee; Joonho Yang; Hwanhee Lee
- Reference count: 33
- Primary result: Structured query rewriting method that reduces factual inaccuracies and improves retrieval performance, especially in less familiar domains

## Executive Summary
CRAFTING THE PATH is a structured query rewriting method designed to improve information retrieval by reducing factual inaccuracies in LLM-generated queries. The method employs a three-step process—Query Concept Comprehension, Query Type Identification, and Expected Answer Extraction—to craft precise, focused queries for retrieval. Experimental results demonstrate superior performance over previous methods like query2doc and query2expand, with higher FActScore and better accuracy in retrieval-augmented generation scenarios, while also achieving lower latency.

## Method Summary
CRAFTING THE PATH improves information retrieval by rewriting user queries through a structured, three-step LLM process. First, it comprehends the query concept to enrich background context. Second, it identifies the query type to filter irrelevant information. Third, it extracts the expected answer format, generating "None" when uncertain to avoid fabrication. The rewritten queries are then used with both dense and sparse retrievers, showing robust performance especially when the model lacks prior knowledge of the query topic.

## Key Results
- Outperforms previous methods (query2doc, query2cot, query2expand) in retrieval-augmented generation scenarios
- Achieves 10% higher FActScore, indicating fewer factual inaccuracies in generated queries
- Demonstrates 7.3% less latency compared to baseline methods while maintaining superior accuracy

## Why This Works (Mechanism)

### Mechanism 1
The three-step structured rewriting process reduces factual errors by decoupling query understanding, type specification, and answer extraction. Each step has a distinct semantic role—Query Concept Comprehension provides background, Query Type Identification filters irrelevant information, and Expected Answer Extraction specifies target information—preventing conflation of unrelated inference paths that cause errors in one-shot rewriting. Core assumption: Factually incorrect generations stem primarily from conflating multiple inference goals in a single LLM call rather than from model knowledge gaps.

### Mechanism 2
Reducing dependency on internal LLM knowledge improves performance in less familiar domains. By explicitly generating "None" when uncertain, the method avoids fabricating plausible but incorrect information, shifting reliance to retrieval of external knowledge rather than model parameter knowledge. Core assumption: Model's parametric knowledge is unreliable or incomplete for many queries, especially in specialized or out-of-distribution domains.

### Mechanism 3
Structured rewriting yields shorter, more focused queries, reducing latency without sacrificing performance. By separating concerns into three concise steps, the method avoids verbose or redundant generation common in free-form rewriting, resulting in lower context length and faster LLM calls. Core assumption: Latency is dominated by LLM generation length, not by the retrieval or processing steps.

## Foundational Learning

- **Concept**: Retrieval-Augmented Generation (RAG)
  - Why needed here: The paper's motivation and evaluation hinge on improving retrieval to support RAG systems; understanding the distinction between parametric knowledge and retrieved knowledge is critical.
  - Quick check question: In a RAG system, what is the role of the retrieval component relative to the LLM's parametric knowledge?

- **Concept**: Dense vs. Sparse Retrieval
  - Why needed here: The experiments compare both retrieval paradigms, and the method is designed to work with both; understanding their differences clarifies performance expectations.
  - Quick check question: How does a dense retriever's use of embeddings differ from a sparse retriever's term matching in processing rewritten queries?

- **Concept**: Query Expansion/Rewriting Objectives
  - Why needed here: The paper's novelty lies in shifting from generating pseudo-documents to structuring queries for retrieval; understanding typical objectives helps evaluate the contribution.
  - Quick check question: What is the primary difference between query rewriting methods like Q2D and the structured approach in CRAFTING THE PATH?

## Architecture Onboarding

- **Component map**: Input query → Query Concept Comprehension → Query Type Identification → Expected Answer Extraction → Concatenated rewritten query → Dense/Sparse Retriever → (Optional) RAG generation
- **Critical path**: 1. LLM call for rewriting (single-shot with structured prompt) 2. Query concatenation and formatting 3. Retrieval (dense or sparse) 4. (Optional) RAG generation step
- **Design tradeoffs**: Structured rewriting vs. free-form: Better factual accuracy but may lose nuance; Three-step vs. one-step: Lower latency but more engineering complexity; Relying on "None" generation: Reduces errors but may weaken queries if overused
- **Failure signatures**: Retrieval performance drops when Step 1 misinterprets query intent; Factual errors increase if Step 3 fabricates information instead of marking "None"; Latency unexpectedly high if LLM generates verbose output in any step
- **First 3 experiments**: 1. Ablation study: Remove each step in turn and measure retrieval impact 2. Closed-book QA setting: Test rewriting when model lacks internal knowledge 3. Latency comparison: Measure generation time vs. query length across methods

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CRAFTING THE PATH compare to other methods when the rewriting model lacks knowledge about the query topic? Basis: The paper mentions superior performance in less familiar domains but lacks detailed comparison in knowledge-deficient scenarios. What evidence would resolve it: Detailed experimental analysis comparing performance on queries from domains not well-represented in the pre-training data of the rewriting model.

### Open Question 2
What is the impact of the length of the generated query on the retrieval performance? Basis: The paper suggests shorter query length correlates with superior performance but does not investigate the relationship explicitly. What evidence would resolve it: Experimental study varying query length while keeping other factors constant and measuring corresponding retrieval performance.

### Open Question 3
How does CRAFTING THE PATH perform in multi-hop reasoning scenarios where the answer requires information from multiple passages? Basis: The paper evaluates on HotpotQA but does not provide detailed analysis of multi-hop performance. What evidence would resolve it: Detailed analysis of performance on HotpotQA, specifically focusing on questions requiring information from multiple passages.

## Limitations
- The method's performance may degrade with highly ambiguous or multi-faceted queries where information spans multiple domains
- Scalability and latency under high-throughput, real-time retrieval workloads have not been demonstrated
- Results may be sensitive to the specific LLM models used and may not generalize to other languages or model architectures

## Confidence
- High confidence: Structured rewriting reduces factual inaccuracies (supported by 10% higher FActScore and qualitative analysis)
- Medium confidence: Better performance in less familiar domains (supported by experiments but compared against older models)
- Medium confidence: Latency reduction claim (7.3% less, based on controlled experiments but may vary with hardware)

## Next Checks
1. Test CRAFTING THE PATH with diverse LLMs (including newer, larger models) to assess robustness and generalizability
2. Systematically evaluate performance on rare or ambiguous queries where information is scattered across multiple domains
3. Benchmark the method in a high-throughput, real-time retrieval system to measure latency, throughput, and resource utilization under realistic workloads