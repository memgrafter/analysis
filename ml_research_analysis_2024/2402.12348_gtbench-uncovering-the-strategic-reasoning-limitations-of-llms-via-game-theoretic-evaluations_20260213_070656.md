---
ver: rpa2
title: 'GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic
  Evaluations'
arxiv_id: '2402.12348'
source_url: https://arxiv.org/abs/2402.12348
tags:
- llms
- opponent
- games
- reasoning
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GTBench, a language-driven environment for
  evaluating the strategic reasoning abilities of large language models (LLMs) in
  game-theoretic tasks. GTBench comprises 10 widely recognized games across a comprehensive
  taxonomy, including complete vs.
---

# GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations

## Quick Facts
- arXiv ID: 2402.12348
- Source URL: https://arxiv.org/abs/2402.12348
- Reference count: 40
- Primary result: Introduces GTBench, a benchmark revealing LLMs fail in complete/deterministic games but perform competitively in probabilistic scenarios, with code-pretraining benefiting strategic reasoning.

## Executive Summary
This paper introduces GTBench, a language-driven environment for evaluating the strategic reasoning abilities of large language models (LLMs) in game-theoretic tasks. GTBench comprises 10 widely recognized games across a comprehensive taxonomy, including complete vs. incomplete information, dynamic vs. static, and probabilistic vs. deterministic scenarios. The authors conduct experiments to characterize the game-theoretic reasoning of LLMs and perform LLM-vs.-LLM competitions. Key findings include: (1) LLMs exhibit distinct behaviors in different gaming scenarios, failing in complete and deterministic games but remaining competitive in probabilistic scenarios; (2) Most open-source LLMs are less competitive than commercial LLMs in complex games, but the recently released Llama-3-70b-Instruct performs comparably; (3) Code-pretraining benefits strategic reasoning, while advanced reasoning methods like Chain-of-Thought and Tree-of-Thought do not always help. The study provides insights into the game-theoretic properties of LLMs, such as equilibrium and Pareto efficiency in repeated games.

## Method Summary
GTBench is a language-driven environment for evaluating LLMs' strategic reasoning in game-theoretic tasks. It includes 10 games spanning complete/incomplete information, dynamic/static, and probabilistic/deterministic scenarios. The environment consists of three components: an Environment (game hosting and execution), a Prompt Adapter (converting observations to prompts and extracting actions), and Participants (LLM-driven agents with various reasoning methods). Experiments compare LLM performance against conventional solvers (MCTS, Random Agent) and other LLMs using Normalized Relative Advantage (NRA) and Elo Rating metrics. The study tests different LLM architectures and reasoning methods (Prompt, CoT, SC-CoT, ToT) across game types.

## Key Results
- LLMs fail in complete and deterministic games, achieving NRA of -1 against MCTS, but remain competitive in probabilistic scenarios
- Code-pretrained models (CodeLlama-34b-Instruct) significantly outperform chat LLMs in strategic reasoning tasks
- Advanced reasoning methods (CoT, SC-CoT, ToT) do not consistently improve LLM game performance and may worsen results for ordinary LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GTBench reveals distinct LLM behaviors across game-theoretic taxonomies (complete vs. incomplete, deterministic vs. probabilistic).
- Mechanism: The benchmark uses 10 diverse games to expose LLMs' strategic reasoning limits by contrasting their performance in different game types.
- Core assumption: LLM failures in complete and deterministic games stem from inability to match MCTS performance in these scenarios.
- Evidence anchors:
  - [abstract] "LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios"
  - [section 4.1] "LLM-driven agents achieve NRA as -1, meaning that LLM agents can barely win even a single match" against MCTS in complete/deterministic games
  - [corpus] GTBench includes both deterministic games (Tic-Tac-Toe, Connect-4) and probabilistic games (Kuhn Poker, Liar's Dice)
- Break condition: If game complexity exceeds LLM reasoning capacity or MCTS becomes impractical, the contrast may not hold.

### Mechanism 2
- Claim: Code-pretraining significantly benefits LLM strategic reasoning compared to general chat LLMs.
- Mechanism: Code-pretrained models like CodeLlama-34b-Instruct outperform larger chat LLMs (Llama-2-70b-chat) in game-theoretic tasks.
- Core assumption: Code-pretraining enhances logical and mathematical reasoning capabilities needed for strategic games.
- Evidence anchors:
  - [section 4.3] "CodeLlama-34b-Instruct achieves comparable results as GPT-4, and significantly outperforms Llama-2-70b-chat"
  - [table 2] Code-pretrained LLMs have higher average NRA in deterministic and probabilistic games than chat LLMs
  - [corpus] Recent studies show code-pretraining improves logical reasoning (Madaan et al., 2022)
- Break condition: If reasoning tasks don't require logical/mathematical skills, the advantage may diminish.

### Mechanism 3
- Claim: Advanced reasoning methods (CoT, SC-CoT, ToT) don't always improve LLM game performance.
- Mechanism: While powerful LLMs benefit from advanced reasoning, ordinary LLMs may suffer from increased reasoning errors.
- Core assumption: Advanced reasoning methods introduce complexity that can lead to mistakes in ordinary LLMs.
- Evidence anchors:
  - [section 4.3] "Advanced reasoning methods, such as Chain-of-Thought (CoT), Self-Consistent CoT (SC-CoT), Tree-of-Thought (ToT) are not always helpful"
  - [table 3] CoT reasoning improves results for GPT-3.5-turbo but worsens performance for CodeLlama-34b-Instruct
  - [appendix A8] Different CoT prompts yield varying performances, all worse than naive Prompt Agent
- Break condition: If prompts are optimized for specific tasks or models, the negative effect may not occur.

## Foundational Learning

- Concept: Game-theoretic taxonomy (complete/incomplete, dynamic/static, probabilistic/deterministic)
  - Why needed here: Understanding these distinctions explains why LLMs perform differently across game types
  - Quick check question: What's the key difference between complete and incomplete information games?

- Concept: Nash equilibrium and regret minimization
  - Why needed here: These concepts help characterize LLM strategies and measure how close they are to optimal play
  - Quick check question: How does regret value indicate proximity to Nash equilibrium?

- Concept: Elo rating system
  - Why needed here: Provides standardized skill comparison between LLMs in competitive settings
  - Quick check question: How is expected score calculated in the Elo system?

## Architecture Onboarding

- Component map: Environment -> Prompt Adapter -> Participant
- Critical path: Observation -> Prompt Adapter -> LLM reasoning -> Action -> Environment execution -> Score calculation
- Design tradeoffs: Unified interfaces enable extensibility but may limit game-specific optimizations; reasoning methods add complexity but can improve performance
- Failure signatures: LLM agents achieving NRA of -1 against MCTS in complete/deterministic games; high completion rates (>90%) indicating proper prompt configuration
- First 3 experiments:
  1. Test LLM vs. Random Agent across all games to establish baseline performance
  2. Compare LLM performance in complete vs. incomplete information games
  3. Evaluate impact of reasoning methods (Prompt, CoT, SC-CoT, ToT) on game performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different reasoning methods, such as Chain-of-Thought, Self-Consistent Chain-of-Thought, and Tree-of-Thought, affect the strategic reasoning performance of large language models (LLMs) in game-theoretic tasks?
- Basis in paper: [explicit] The paper mentions that code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help.
- Why unresolved: The paper suggests that advanced reasoning methods do not always improve performance, but it does not provide a detailed analysis of how each method affects different types of games or LLMs with different capabilities.
- What evidence would resolve it: A comprehensive study comparing the performance of LLMs using different reasoning methods across various game-theoretic tasks, with a focus on identifying the specific conditions under which each method is beneficial.

### Open Question 2
- Question: What are the specific error patterns that LLMs exhibit when engaging in game-theoretic tasks, and how can these errors be mitigated?
- Basis in paper: [explicit] The paper identifies five prevalent mistake patterns observed across different games: Misinterpretation, Factual Inaccuracies, Overconfidence, Calculation Mistakes, and Endgame Misdetection.
- Why unresolved: The paper provides examples of these error patterns but does not offer a detailed analysis of their frequency or propose strategies for mitigating them.
- What evidence would resolve it: A detailed error analysis that quantifies the frequency of each error pattern and explores potential strategies for reducing these errors, such as improved prompt engineering or model fine-tuning.

### Open Question 3
- Question: How do the game-theoretic properties of LLMs, such as Nash Equilibrium and Pareto Efficiency, vary across different types of games and LLMs?
- Basis in paper: [explicit] The paper characterizes the game-theoretic properties of LLMs, such as equilibrium and Pareto Efficiency in repeated games.
- Why unresolved: The paper provides some insights into these properties but does not offer a comprehensive analysis of how they vary across different game types and LLMs.
- What evidence would resolve it: A systematic study that evaluates the Nash Equilibrium and Pareto Efficiency of LLMs across a wide range of game-theoretic tasks, with a focus on identifying the factors that influence these properties.

## Limitations
- The study's findings are limited by the specific game selection in GTBench and may not generalize to all game-theoretic scenarios
- Performance evaluation depends heavily on prompt engineering quality, which varies across games and LLMs
- The computational constraints of LLM inference may artificially limit performance in time-sensitive game scenarios

## Confidence
- **High Confidence**: LLMs struggle in complete and deterministic games while performing better in probabilistic scenarios; code-pretrained models outperform chat models in strategic reasoning
- **Medium Confidence**: Advanced reasoning methods don't consistently improve performance; commercial LLMs generally outperform open-source models
- **Low Confidence**: Generalization of strategic reasoning limitations to broader cognitive capabilities; claims about equilibrium and Pareto efficiency properties need further validation

## Next Checks
1. **Cross-architecture validation**: Test the same GTBench games with additional model families (Mistral, Gemma, etc.) to verify if the complete game failure pattern holds across different LLM architectures.

2. **Prompt engineering robustness**: Systematically vary prompt templates and reasoning strategies across all games to determine if performance gaps between game types can be reduced through better prompt engineering.

3. **Computational resource impact**: Evaluate how varying inference time limits and token budgets affect LLM performance in both complete and probabilistic games to understand if resource constraints drive the observed limitations.