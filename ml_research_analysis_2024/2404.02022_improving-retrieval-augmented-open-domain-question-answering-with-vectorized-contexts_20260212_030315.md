---
ver: rpa2
title: Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized
  Contexts
arxiv_id: '2404.02022'
source_url: https://arxiv.org/abs/2404.02022
tags:
- contexts
- encoder
- language
- training
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to enhance open-domain question
  answering (ODQA) by incorporating a small encoder to process long contexts via cross-attention
  with a large language model. The approach enables the model to handle up to 10k
  tokens of context without significant increases in computational cost.
---

# Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts

## Quick Facts
- **arXiv ID:** 2404.02022
- **Source URL:** https://arxiv.org/abs/2404.02022
- **Authors:** Zhuo Chen; Xinyu Wang; Yong Jiang; Pengjun Xie; Fei Huang; Kewei Tu
- **Reference count:** 9
- **One-line primary result:** Method improves ODQA by 2-4 EM points on TriviaQA and NQ by encoding additional contexts via cross-attention without proportional computational cost increase

## Executive Summary
This paper introduces a method to enhance open-domain question answering (ODQA) by incorporating a small encoder to process long contexts via cross-attention with a large language model. The approach enables the model to handle up to 10k tokens of context without significant increases in computational cost. Experiments show consistent improvements over a strong baseline across two held-in, four held-out, and two in-context learning (ICL) settings, achieving gains such as 45.74 → 48.24 EM on TriviaQA and 37.16 → 39.11 EM on NQ. The method maintains competitive runtime and scales well with context length, though performance gains are smaller in ICL settings without explicit context.

## Method Summary
The method fine-tunes a large language model (Bloomz-1b7) with an additional small encoder (BERT-base-uncased) for context encoding. Additional contexts retrieved from a knowledge base are encoded into dense vectors by the small encoder, projected to match the task model's hidden dimension, and incorporated via cross-attention in each layer. The training uses a two-phase strategy: initially freezing the encoder to preserve pre-trained weights, then jointly training both models. This allows processing up to 10k tokens of context while maintaining computational efficiency comparable to the baseline.

## Key Results
- Improves Exact Match (EM) on TriviaQA from 45.74 to 48.24
- Improves EM on Natural Questions from 37.16 to 39.11
- Shows consistent improvements across two held-in, four held-out, and two ICL settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The encoder allows the model to cover up to 10k tokens of context without increasing computational cost proportionally.
- **Mechanism:** A small encoder model (BERT-base-uncased) encodes additional contexts independently, producing dense vector representations. These vectors are projected to match the hidden dimension of the large task model and incorporated via cross-attention in each layer.
- **Core assumption:** Cross-attention can effectively integrate dense context vectors into the task model's processing without disrupting its learned representations.
- **Evidence anchors:** [abstract] "the length of context that the model can cover increases from 2k (in text form) to a maximum of 10k (in dense form, which is condensed by the encoder)."
- **Break condition:** If the projector fails to align the encoder and task model hidden spaces, the cross-attention will not effectively integrate the additional context information.

### Mechanism 2
- **Claim:** Cross-attention mechanism selectively attends to relevant parts of the encoded context vectors.
- **Mechanism:** In each layer, the output from the previous layer (or embeddings in the first layer) acts as "query" to attend to the encoded context vectors ("key" and "value"). This allows the model to dynamically focus on the most relevant information from the additional contexts.
- **Core assumption:** The cross-attention mechanism can effectively identify and utilize relevant information from the dense context vectors.
- **Evidence anchors:** [section] "To incorporate the information stored in kadd we add a cross-attention module, where representations of additional contexts hkv serve as 'key' and 'value', followed by an MLP."
- **Break condition:** If the cross-attention mechanism fails to properly attend to relevant information, the additional context vectors will not improve the model's performance.

### Mechanism 3
- **Claim:** The training strategy of freezing the encoder initially and then training it after a few steps prevents random gradients from disrupting the pre-trained encoder's utility.
- **Mechanism:** The encoder is initialized with pre-trained BERT weights and kept frozen during the initial training steps. After a few steps (e.g., one epoch), the encoder is unfrozen and trained jointly with the task model.
- **Core assumption:** Pre-trained encoder weights are useful for encoding additional contexts, and random gradients can disrupt these useful representations if applied too early.
- **Evidence anchors:** [section] "Therefore, we design two strategies of training... In the first few training steps (e.g., one epoch), ϕ is kept frozen to prevent random gradients from breaking its well-pre-trained parameters."
- **Break condition:** If the encoder is trained from the beginning with random gradients, its pre-trained utility for encoding information may be broken, leading to worse performance.

## Foundational Learning

- **Concept:** Cross-attention mechanism
  - Why needed here: To integrate the dense context vectors encoded by the small encoder into the large task model's processing.
  - Quick check question: What are the roles of "query", "key", and "value" in a cross-attention mechanism?

- **Concept:** Pre-trained language models
  - Why needed here: The encoder is initialized with pre-trained BERT weights, which are useful for encoding additional contexts.
  - Quick check question: Why is it beneficial to initialize the encoder with pre-trained weights rather than training it from scratch?

- **Concept:** Gradient descent and backpropagation
  - Why needed here: The model is trained using gradient descent, and the training strategy aims to prevent random gradients from disrupting the pre-trained encoder's utility.
  - Quick check question: How can random gradients during the initial training steps potentially disrupt the pre-trained encoder's utility?

## Architecture Onboarding

- **Component map:** Task model -> Projector -> Cross-attention module -> Self-attention and MLP modules
- **Critical path:**
  1. Encode additional contexts using the small encoder.
  2. Project the encoded vectors to match the task model's hidden dimension.
  3. In each layer of the task model, use cross-attention to incorporate the projected vectors.
  4. Process the output through self-attention and MLP modules.
- **Design tradeoffs:** Using a small encoder reduces computational cost but may limit the model's ability to encode complex contexts. Freezing the encoder initially preserves its pre-trained utility but may slow down the overall training process.
- **Failure signatures:** If the projector fails to align the hidden spaces, the cross-attention will not effectively integrate the additional context information. If the cross-attention mechanism fails to properly attend to relevant information, the additional context vectors will not improve the model's performance.
- **First 3 experiments:**
  1. Train the model with a frozen encoder and evaluate its performance on held-in, held-out, and ICL settings.
  2. Train the model with an unfrozen encoder from the beginning and compare its performance to the frozen encoder setting.
  3. Experiment with different numbers of additional contexts (e.g., 5k vs. 10k) and evaluate the impact on performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method's performance scale with larger language models beyond the 1B7 model tested?
- Basis in paper: [inferred] The paper notes that only 1B7 models with a 110M encoder were tested due to limited computing resources, suggesting potential for larger models.
- Why unresolved: The paper does not provide experimental results or theoretical analysis for scaling the method to larger language models like 7B or 70B.
- What evidence would resolve it: Conducting experiments with larger language models and comparing their performance with the current model would provide insights into the scalability of the method.

### Open Question 2
- Question: What are the specific reasons for the relatively modest performance improvement in the ICL setting without contexts compared to the baseline?
- Basis in paper: [explicit] The paper notes that the method exhibits modest performance improvement in the ICL setting without contexts, with only slight improvements compared to the baseline.
- Why unresolved: The paper suggests potential reasons such as the cross-attention mechanism being unsuitable for modeling the relationship between context and ICL samples without contexts, but does not provide a detailed analysis or experimental evidence.
- What evidence would resolve it: Conducting a detailed analysis of the cross-attention mechanism's performance in the ICL setting without contexts and comparing it with other mechanisms would provide insights into the reasons for the modest performance improvement.

### Open Question 3
- Question: How does the proposed method's performance compare to other context compression techniques like AutoCompressors in terms of efficiency and effectiveness?
- Basis in paper: [inferred] The paper mentions related work on context compression techniques like AutoCompressors but does not provide a direct comparison with the proposed method.
- Why unresolved: The paper does not provide experimental results or theoretical analysis comparing the proposed method with other context compression techniques in terms of efficiency and effectiveness.
- What evidence would resolve it: Conducting experiments comparing the proposed method with other context compression techniques in terms of efficiency and effectiveness would provide insights into the relative performance of the methods.

## Limitations
- The paper lacks explicit computational cost analysis to verify runtime and memory usage comparisons between the baseline and proposed method across different context lengths.
- Performance gains in in-context learning (ICL) settings are notably smaller (2-3 EM points) compared to fine-tuning settings (2-4 EM points), suggesting context-dependent effectiveness.
- Specific training strategy details (exact number of steps to freeze the encoder, learning rate schedules) are not fully specified, which could impact reproducibility and performance.

## Confidence

- **High Confidence**: The mechanism of using cross-attention to incorporate dense context vectors is well-supported by the method description and experimental results. The consistent improvements across multiple datasets provide strong empirical evidence for the core claims.
- **Medium Confidence**: The claim about maintaining computational efficiency is plausible based on theoretical advantages of dense vector processing, but lacks direct empirical validation through runtime measurements or memory analysis.
- **Low Confidence**: The specific training strategy details (exact number of steps to freeze the encoder, learning rate schedules) are not fully specified, which could impact reproducibility and performance.

## Next Checks
1. **Computational Efficiency Validation**: Conduct runtime and memory usage comparisons between the baseline model and the proposed method across different context lengths (1k, 5k, 10k) to empirically verify the computational cost claims.

2. **Cross-Attention Mechanism Analysis**: Perform ablation studies removing the cross-attention layers to quantify their exact contribution to performance improvements, and analyze attention weight distributions to verify selective context utilization.

3. **Training Strategy Sensitivity**: Systematically vary the encoder freezing duration (0, 1, 2, 3 epochs) and learning rate schedules to determine the optimal training strategy and test the robustness of the two-phase approach.