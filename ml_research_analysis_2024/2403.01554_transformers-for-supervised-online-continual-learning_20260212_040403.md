---
ver: rpa2
title: Transformers for Supervised Online Continual Learning
arxiv_id: '2403.01554'
source_url: https://arxiv.org/abs/2403.01554
tags:
- learning
- data
- online
- transformer
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel method for online continual learning
  that combines the strengths of transformer models and stochastic gradient descent
  (SGD) training. The key idea is to use a transformer architecture that is explicitly
  conditioned on recent observations while simultaneously training it online with
  SGD, following the procedure introduced with Transformer-XL.
---

# Transformers for Supervised Online Continual Learning

## Quick Facts
- arXiv ID: 2403.01554
- Source URL: https://arxiv.org/abs/2403.01554
- Authors: Jorg Bornschein; Yazhe Li; Amal Rannen-Triki
- Reference count: 21
- Primary result: Achieves 70% average accuracy on CLOC using pre-extracted features from MAE ViT-L, nearly doubling previous state-of-the-art results

## Executive Summary
This paper introduces a transformer-based approach for online continual learning that combines in-context learning with parametric SGD training. The method explicitly conditions transformers on recent observations while training them online, leveraging the strengths of both rapid adaptation and sustained improvement over long sequences. By incorporating replay streams, the approach maintains benefits of multi-epoch training while adhering to sequential protocols.

## Method Summary
The method proposes two transformer architectures: a 2-token approach using input and label tokens separately, and a privileged information transformer (pi-transformer) with label projections in attention keys/values. Both architectures use online training with Transformer-XL style KV caching and replay streams for chronological replay of previously observed examples. The model processes chunks of 100 examples at a time, updating parameters via AdamW optimizer with learning rates of 1e-5 for transformers and 3e-4 for feature extractors.

## Key Results
- Achieves 70% average accuracy on CLOC using pre-extracted MAE ViT-L features
- Reaches 67% accuracy when learning feature extractor from scratch
- Nearly doubles previous state-of-the-art performance on this large-scale geo-localization benchmark

## Why This Works (Mechanism)

### Mechanism 1: In-Context Learning for Fast Adaptation
- Claim: The transformer's attention window enables rapid adaptation to sudden changes in the data sequence by conditioning on the C most recent observations.
- Core assumption: Recent observations contain sufficient information to make accurate predictions for the current time step, and attention mechanism can effectively extract relevant patterns.
- Evidence anchors: Abstract mentions explicitly conditioning transformers on recent observations; section discusses combining in-context learning with parametric learning.

### Mechanism 2: Parametric Learning for Sustained Improvement
- Claim: Online training with SGD enables continuous improvement over long sequences beyond the attention window.
- Core assumption: Gradient updates are informative and lead to improvements in generalization across the entire sequence.
- Evidence anchors: Abstract discusses combining in-context learning with parametric learning via SGD; section highlights sustained progress over long sequence lengths.

### Mechanism 3: Replay for Multi-Epoch Training
- Claim: Replay allows benefits of multi-epoch training while adhering to sequential protocol of online learning.
- Core assumption: Replayed examples are representative of overall data distribution and provide useful information for improving future performance.
- Evidence anchors: Abstract mentions incorporating replay to maintain multi-epoch training benefits; section describes replay-streams for chronological replay.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: Transformer's ability to attend to and process tokens as context is crucial for enabling in-context learning and fast adaptation.
  - Quick check question: How does self-attention in transformers allow processing of sequential data and learning from context?

- Concept: Online learning and continual learning
  - Why needed here: Paper focuses on supervised online continual learning where model must adapt to non-stationary data streams.
  - Quick check question: What are key challenges in online continual learning compared to traditional batch learning?

- Concept: Experience replay and buffer management
  - Why needed here: Replay maintains benefits of multi-epoch training while adhering to sequential protocol of online learning.
  - Quick check question: How does experience replay help mitigate catastrophic forgetting in continual learning?

## Architecture Onboarding

- Component map: Feature extractor -> Transformer encoder (pi-transformer or 2-token) -> Online training loop with SGD -> Replay streams

- Critical path: 1) Extract features from input images using chosen feature extractor, 2) Process features through transformer encoder conditioning on recent observations via self-attention, 3) Generate predictions based on transformer's output, 4) Compute prediction loss and perform gradient updates, 5) Store processed features and labels in replay buffer

- Design tradeoffs: Attention window size vs. computational cost and long-term dependency capture; Number of replay streams vs. memory usage and overfitting risk; Model depth/width vs. expressivity and overfitting risk

- Failure signatures: Poor performance on new tasks (insufficient in-context learning); Degradation over time (insufficient parametric learning); Overfitting to replayed examples (excessive replay)

- First 3 experiments: 1) Implement pi-transformer with fixed feature extractor on synthetic Split-EMNIST dataset, 2) Compare pi-transformer vs 2-token approach on same dataset varying attention window and replay streams, 3) Evaluate pi-transformer on CLOC with pre-extracted features varying model depth, width, and learning rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relative contribution of in-context learning versus parametric learning in transformer's performance on CLOC?
- Basis in paper: [explicit] Experiments disabling gradient updates at various points and comparing pi-transformers with/without attention show contribution of each mechanism.
- Why unresolved: Experiments provide insights but don't quantify exact contribution or analyze variation across model architectures/sequence lengths.
- What evidence would resolve it: Systematic ablation study varying attention window, replay streams, and model architecture while measuring performance at different sequence positions.

### Open Question 2
- Question: How does choice of feature extractor impact transformer's ability to learn in-context and parametric representations?
- Basis in paper: [explicit] Shows MAE ViT-L features substantially better than ResNet-50 features; hypothesizes better representations for in-context learning.
- Why unresolved: Doesn't investigate specific properties of different feature extractors that make them more/less suitable.
- What evidence would resolve it: Analyze learned representations of different feature extractors and correlate properties with transformer's performance.

### Open Question 3
- Question: How does transformer's performance on non-stationary data generalize to other domains and tasks beyond image geo-localization?
- Basis in paper: [inferred] Focuses on CLOC benchmark showing strong performance but generalizability to other domains not explicitly explored.
- Why unresolved: Paper doesn't investigate performance on other types of non-stationary data or tasks.
- What evidence would resolve it: Evaluate transformer on other non-stationary benchmarks and analyze factors influencing performance.

## Limitations

- Evaluation limited to CLOC benchmark and synthetic datasets without comparison to broader set of online continual learning methods
- No ablations isolating contributions of in-context learning versus replay mechanisms
- Scalability to very long sequences (millions of steps) and robustness to distribution shifts beyond piecewise-stationary scenarios untested

## Confidence

**High confidence** in technical implementation details of transformer architectures and training procedures
**Medium confidence** in claim of achieving state-of-the-art results given limited comparison set
**Low confidence** in assertion that approach successfully combines complementary strengths without evidence isolating mechanism contributions

## Next Checks

1. Conduct ablation study varying attention window size C and replay stream count E to isolate effects of in-context learning versus replay on final performance

2. Evaluate proposed approach against broader range of online continual learning methods including non-transformer alternatives like experience replay with standard neural networks

3. Test method's performance on longer sequences (1M+ steps) and more complex distribution shifts (smooth gradual changes, cyclic patterns) with systematic sensitivity analysis of hyperparameters