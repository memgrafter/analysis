---
ver: rpa2
title: 'Cottention: Linear Transformers With Cosine Attention'
arxiv_id: '2409.18747'
source_url: https://arxiv.org/abs/2409.18747
tags:
- attention
- cosine
- softmax
- memory
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cottention, a novel attention mechanism that
  replaces the softmax operation with cosine similarity to achieve linear memory complexity
  with respect to sequence length. The key innovation is decoupling the normalization
  step from matrix multiplication in the attention computation, enabling efficient
  processing of longer sequences.
---

# Cottention: Linear Transformers With Cosine Attention

## Quick Facts
- arXiv ID: 2409.18747
- Source URL: https://arxiv.org/abs/2409.18747
- Reference count: 40
- Key outcome: Cosine attention achieves linear memory complexity by replacing softmax with cosine similarity and leveraging matrix multiplication associativity

## Executive Summary
This paper introduces Cottention, a novel attention mechanism that replaces the softmax operation with cosine similarity to achieve linear memory complexity with respect to sequence length. The key innovation is decoupling the normalization step from matrix multiplication in the attention computation, enabling efficient processing of longer sequences. The authors demonstrate that Cottention can be reformulated as a recurrent neural network with a finite hidden state, allowing for constant memory usage during inference. Experimental results on BERT and GPT-J models show comparable performance to softmax attention while significantly reducing memory requirements.

## Method Summary
The method replaces standard softmax attention with cosine similarity by normalizing query and key matrices before computing similarity scores. This allows rearranging the attention computation from (Q·K^T)·V to Q·(K^T·V), achieving d² memory complexity instead of s². A learned stabilization constant m controls normalization magnitude to prevent training instability. The mechanism is implemented with a custom CUDA kernel for efficiency and can be reformulated as an RNN with a finite hidden state of size d².

## Key Results
- Achieves linear memory scaling with sequence length, unlike quadratic scaling of softmax attention
- Maintains comparable performance to softmax attention on BERT (GLUE) and GPT-J (perplexity) benchmarks
- Demonstrates stable training with learned stabilization constant that decays over training
- Enables constant memory inference through RNN reformulation with finite hidden state

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cosine attention achieves linear memory complexity by replacing softmax with cosine similarity and leveraging the associative property of matrix multiplication
- Mechanism: The attention computation is rearranged from (Q·K^T)·V to Q·(K^T·V), where the latter requires only d² memory instead of s². This rearrangement is valid because cosine similarity decouples the normalization step from the matrix multiplication
- Core assumption: The associative property of matrix multiplication allows the computation order to be rearranged without changing the result
- Evidence anchors:
  - [abstract]: "By leveraging the properties of cosine similarity and rearranging the attention equation, Cottention achieves native linear memory complexity"
  - [section]: "Removing the dimension subscripts for brevity, cosine attention can achieve d² memory complexity in the bidirectional case by rearranging the computation order: [Q·K^T]·V = Q·[K^T·V]"
  - [corpus]: "Softmax Attention has a quadratic time complexity, which becomes prohibitive to run at long contexts, even with highly optimized GPU kernels"
- Break condition: The rearrangement fails when an attention mask is applied in the causal case, as the mask prevents the direct application of the associative property

### Mechanism 2
- Claim: The RNN reformulation enables constant memory usage during inference by maintaining a finite hidden state
- Mechanism: The cumulative sum operation in cosine attention can be interpreted as maintaining a hidden state that accumulates key-value outer products over time. This hidden state has fixed size d², independent of sequence length
- Core assumption: The hidden state captures all necessary information from previous tokens through the accumulated key-value outer products
- Evidence anchors:
  - [abstract]: "We demonstrate that Cottention can be reformulated as a recurrent neural network (RNN) with a finite hidden state, allowing for constant memory usage during inference"
  - [section]: "Cosine attention can be reformulated as a recurrent neural network (RNN) by interpreting the cumulative sum operation in the forward pass as a recurrent computation"
  - [corpus]: "Softmax Attention has a quadratic time complexity, which becomes prohibitive to run at long contexts"
- Break condition: The finite hidden state may become insufficient for very long sequences if the accumulated information becomes too complex or if the positional encodings don't properly distinguish between distant tokens

### Mechanism 3
- Claim: The stabilization constant m helps prevent training instability by controlling the normalization magnitude
- Mechanism: A learned scalar m (passed through sigmoid) divides the value matrix by s^m, where s is the sequence length. This prevents the similarity matrix from having row sums that are too large, which would cause instability
- Core assumption: The similarity matrix from cosine attention can have row sums up to s, while softmax attention has row sums of 1, requiring additional stabilization
- Evidence anchors:
  - [abstract]: "We address the stability issues encountered in previous works without the need for additional constraints or modifications"
  - [section]: "To address this issue, we propose a stabilized formulation of cosine attention, given as: CosAttention(Q, K, V) = 1/(sσ(m)) ⊙ Sim(Q, K) · V"
  - [corpus]: "The learned scalar m for each attention head adds a small number of parameters to the model, equal to (num_heads) ∗ (num_layers)"
- Break condition: If the initialization of m is not appropriate or if the training dynamics require different stabilization levels, the model may still experience instability

## Foundational Learning

- Concept: Matrix multiplication associativity
  - Why needed here: Understanding that (A·B)·C = A·(B·C) is crucial for grasping why the computation can be rearranged to achieve linear memory complexity
  - Quick check question: If A is 1000×500, B is 500×500, and C is 500×300, which computation uses less memory: (A·B)·C or A·(B·C)?

- Concept: L2 normalization and cosine similarity
  - Why needed here: The core of cosine attention is replacing softmax with cosine similarity, which requires understanding how L2 normalization affects the attention weights
  - Quick check question: What is the range of cosine similarity values, and how does this differ from softmax outputs?

- Concept: Recurrent neural networks and hidden states
  - Why needed here: The RNN reformulation shows how cosine attention can maintain a constant memory footprint by treating the cumulative sum as a recurrent operation
  - Quick check question: In a standard RNN, what is the typical size of the hidden state relative to the input sequence length?

## Architecture Onboarding

- Component map: Query/Key/Value projections -> L2 normalization -> Cosine similarity computation -> Cumulative sum operation -> Output projection
- Critical path: The custom CUDA kernel that computes the cumulative sum-based attention, as this replaces the standard matrix multiplication and determines the overall efficiency gains
- Design tradeoffs: Linear complexity vs. potential loss of expressiveness compared to softmax attention. The stabilization constant adds parameters but improves training stability. The custom CUDA kernel improves performance but adds implementation complexity.
- Failure signatures: Training instability without the stabilization constant, performance degradation on tasks requiring complex attention patterns, memory usage that doesn't scale as expected with sequence length.
- First 3 experiments:
  1. Implement the naive PyTorch version (provided in Appendix A) and verify it produces the same outputs as the CUDA kernel
  2. Measure memory usage vs. sequence length on a small BERT model to confirm linear scaling
  3. Train a small model with and without the stabilization constant to observe the impact on training stability

## Open Questions the Paper Calls Out

- **Optimal stabilization constant initialization**: The paper mentions that the initialization of m=0.5 worked well, but suggests there may be room for improvement and leaves exploration of optimal initialization strategies to future work.

- **Performance on larger models**: The paper states that they applied cosine attention to relatively smaller models and that the application to larger models was not investigated, leaving this for future research.

- **Matrix factorization opportunities**: The paper mentions that cosine attention decouples the computation of Q, K, and V, opening up new possibilities for matrix factorization that could not be explored with softmax attention due to the QK^T constraint.

## Limitations

- The RNN reformulation with finite hidden state may become insufficient for very long sequences as accumulated information grows complex
- Custom CUDA kernel implementation is critical for performance but details are limited in the paper
- Stabilization mechanism effectiveness depends on appropriate initialization of the learned scalar m

## Confidence

- **High confidence**: The theoretical foundation of replacing softmax with cosine similarity to achieve linear memory complexity is well-established. The matrix multiplication rearrangement and its memory implications are mathematically sound.
- **Medium confidence**: The RNN reformulation as a recurrent computation with finite hidden state is plausible based on the cumulative sum interpretation, but the practical implications for very long sequences remain uncertain without extensive empirical validation.
- **Medium confidence**: The stabilization constant mechanism addresses a real issue in cosine attention, and the empirical evidence of m decaying during training supports its effectiveness. However, the sensitivity analysis and exploration of alternative stabilization approaches is limited.

## Next Checks

1. **Hidden state capacity test**: Implement a controlled experiment varying sequence length from 100 to 100,000 tokens while monitoring task performance and memory usage. Track whether performance degrades at extreme sequence lengths despite linear memory scaling, which would indicate limitations in the finite hidden state's capacity to capture long-range dependencies.

2. **Stabilization sensitivity analysis**: Systematically vary the initialization range of the stabilization constant m (e.g., 0.1, 0.5, 1.0, 2.0) and measure training stability metrics including gradient norms, loss curves, and final task performance across multiple random seeds. This would quantify the robustness of the stabilization mechanism.

3. **CUDA kernel validation**: Implement the naive PyTorch version from Appendix A alongside the CUDA kernel and create a comprehensive test suite comparing outputs across various input sizes, attention masks, and edge cases. Measure the actual performance gain and verify that the kernel handles all corner cases correctly, particularly for causal attention with non-standard mask patterns.