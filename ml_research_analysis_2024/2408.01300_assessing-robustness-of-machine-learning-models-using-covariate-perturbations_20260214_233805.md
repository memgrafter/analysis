---
ver: rpa2
title: Assessing Robustness of Machine Learning Models using Covariate Perturbations
arxiv_id: '2408.01300'
source_url: https://arxiv.org/abs/2408.01300
tags:
- perturbations
- variables
- data
- variable
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework to assess the robustness of machine
  learning models using covariate perturbations. It introduces separate strategies
  for perturbing numeric (raw and adaptive) and categorical variables (pseudo-distance
  method) to evaluate how model predictions change under small perturbations.
---

# Assessing Robustness of Machine Learning Models using Covariate Perturbations

## Quick Facts
- arXiv ID: 2408.01300
- Source URL: https://arxiv.org/abs/2408.01300
- Reference count: 0
- One-line primary result: XGB model shows highest robustness to small perturbations despite larger train-test performance gap

## Executive Summary
This paper introduces a framework to assess machine learning model robustness using covariate perturbations. The authors propose separate strategies for perturbing numeric (raw and adaptive) and categorical variables (pseudo-distance method) to evaluate how model predictions change under small perturbations. A robustness metric, ArPPV (average root perturbed prediction volatility), is defined to compare models on their stability. The methodology is demonstrated on the Taiwan credit dataset, comparing GLM, XGB, and FFNN models, with XGB found to be most robust to small perturbations despite a larger gap between train and test performance.

## Method Summary
The framework assesses model robustness by perturbing each observation in the test dataset K times within a local neighborhood defined by the data envelope. For numeric variables, perturbations use Gaussian noise with either global standard deviation (raw) or local adaptive scaling based on quantile-based bucketing. For categorical variables, perturbations use a pseudo-distance method that defines similarity based on average response impact. The rPPV (root perturbed prediction volatility) metric captures prediction variability under perturbations, while ArPPV (average rPPV) provides a single robustness score for model comparison. Local diagnosis tools identify high-volatility regions using PSI, diagnostic trees, and partial dependence plots.

## Key Results
- XGB model demonstrates highest robustness to small perturbations despite larger train-test performance gap
- Adaptive perturbations outperform raw perturbations in maintaining local data structure
- Categorical variables show varying sensitivity across different perturbation strategies
- High-rPPV regions can be identified and linked to specific variables and model behaviors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework isolates model instability from sensitivity by comparing deviations of perturbed predictions from original predictions rather than from true labels.
- Mechanism: By computing rPPV as the root mean square of (ŷᵢₖ - ŷᵢ), the metric captures only the variability in model output under small input changes, not model bias or error. This separation allows robustness assessment without conflating prediction quality with prediction stability.
- Core assumption: Small perturbations should not cause large changes in prediction if the model is robust, regardless of whether the original prediction was accurate.
- Evidence anchors:
  - [abstract]: "summaries of perturbations to assess and compare model robustness across different scenarios"
  - [section 2.2]: "While summarizing the variability in ŷᵢₖ, it is possible to summarize the K perturbations by their deviation from the original prediction (ŷᵢₖ - ŷᵢ) or by their deviation from the true response (ŷᵢₖ - yᵢ)"
  - [corpus]: Weak; no direct match found in neighboring papers. This is a unique formulation not covered in standard adversarial robustness literature.
- Break condition: If the model exhibits high sensitivity to input changes in regions where the underlying relationship is genuinely sharp or non-smooth, this metric will conflate sensitivity with instability.

### Mechanism 2
- Claim: The pseudo-distance metric enables principled perturbation of categorical variables by defining similarity based on average response impact.
- Mechanism: Categorical levels are considered "close" if their average response values are similar. This allows perturbations that stay within a meaningful local neighborhood, respecting the data envelope and avoiding arbitrary jumps between unrelated categories.
- Core assumption: Categorical levels with similar average response are likely to represent similar subpopulations, making transitions between them small-scale and interpretable.
- Evidence anchors:
  - [section 3.2.1]: "We define a pseudo-distance between levels of categorical variables based on their average impact on the response... If two levels of a categorical variable are very similar then their average impact on the response should not be very different."
  - [section 5.3]: "Table 5-5. Average response for each level of a categorical variables 'EDUCATION' and 'MARRIAGE' defining the similarity among the levels."
  - [corpus]: Moderate; the paper "Unifying Adversarial Perturbation for Graph Neural Networks" discusses perturbation strategies but not categorical-specific distance metrics.
- Break condition: If categorical levels have similar average responses but represent fundamentally different subpopulations (e.g., due to confounding), the pseudo-distance may incorrectly suggest small-scale transitions.

### Mechanism 3
- Claim: Adaptive perturbation strategies preserve the local structure of the data by scaling noise according to local density.
- Mechanism: Instead of applying uniform noise scaled by global standard deviation, adaptive perturbations scale noise by the local spread within quantile-based buckets. This ensures conservative perturbations in dense regions and larger perturbations in sparse regions, maintaining the relative uncertainty structure.
- Core assumption: Local variability is a better indicator of appropriate perturbation scale than global variability, especially in non-uniform distributions.
- Evidence anchors:
  - [section 3.1.2]: "We make the perturbations adaptive by adjusting the noise scale σⱼ to σ̃ᵢⱼ where each observation 'i' has its own scale based on the spread of nearby points."
  - [section 5.1]: "Figure 5. Comparison of robustness of models to increasing perturbations on numeric variables. (Left): Raw perturbations, (Right): Adaptive perturbations."
  - [corpus]: Weak; no direct matches found. This is a novel approach not covered in standard perturbation literature.
- Break condition: If the quantile bucketing is too coarse or the local spread estimate is unstable, the adaptive scaling may introduce artifacts or fail to capture the true local structure.

## Foundational Learning

- Concept: Multivariate Gaussian perturbations with correlation structure
  - Why needed here: To generate realistic perturbations that respect the existing relationships between numeric variables, avoiding unrealistic combinations that distort the data envelope.
  - Quick check question: How would you generate perturbations that maintain the correlation structure of a dataset with three correlated variables?

- Concept: Partial Dependence Plots (PDP) for interpreting model behavior
  - Why needed here: To distinguish between model sensitivity (expected sharp transitions) and lack of robustness (unexpected instability) by comparing rPPV patterns with PDP marginal effects.
  - Quick check question: What would a high rPPV region with a smooth PDP transition indicate about model behavior?

- Concept: Population Stability Index (PSI) for distribution comparison
  - Why needed here: To identify which variables contribute most to volatility by comparing the distribution of observations with high rPPV against the rest of the data.
  - Quick check question: How would you interpret a PSI value of infinity for a categorical variable?

## Architecture Onboarding

- Component map: Data preprocessing -> Perturbation engine -> Model evaluation -> Robustness metrics -> Local diagnosis
- Critical path: 1. Prepare test dataset with both numeric and categorical variables 2. For each observation, generate K perturbations (separately for numeric and categorical) 3. Obtain model predictions on original and perturbed observations 4. Compute rPPV for each observation 5. Calculate ArPPV to compare models 6. Perform local diagnosis on high-rPPV observations
- Design tradeoffs:
  - Budget selection: Too low → insufficient perturbation; too high → testing sensitivity rather than robustness
  - Perturbation strategy: Correlated vs independent (numeric); pseudo-distance vs shuffling (categorical)
  - Metric choice: ArPPV (average view) vs max-based metrics (worst-case view)
- Failure signatures:
  - ArPPV increases linearly with budget for all models → test may be too coarse
  - One model consistently shows much higher ArPPV → potential overfitting or instability
  - High rPPV regions don't align with high-importance variables → possible model sensitivity issues
- First 3 experiments:
  1. Compare ArPPV of GLM, XGB, and FFNN on Taiwan credit dataset with 2% budget using raw perturbations on numeric variables only
  2. Repeat experiment with adaptive perturbations and observe changes in model ranking
  3. Add categorical perturbations using pseudo-distance method and assess impact on overall ArPPV

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different summarization metrics (e.g., mean, median, max) compare in their ability to distinguish between model sensitivity and lack of robustness?
- Basis in paper: [explicit] The paper states that different summarization metrics capture different notions of robustness and discusses using mean square, absolute maximum, absolute mean, and absolute median as alternatives to ArPPV.
- Why unresolved: The paper only compares a few summarization metrics (absolute mean, absolute maximum, root mean square) on the Taiwan credit dataset, without exploring the full range of options or their effectiveness in distinguishing sensitivity from lack of robustness.
- What evidence would resolve it: Systematic experiments comparing model rankings and sensitivity/robustness identification across multiple summarization metrics on diverse datasets and model types.

### Open Question 2
- Question: How does the choice of perturbation budget affect the trade-off between capturing true model sensitivity versus falsely identifying lack of robustness?
- Basis in paper: [explicit] The paper discusses that ArPPV increases with budget and that robustness tests should be performed at low budgets to avoid conflating sensitivity with lack of robustness, but does not provide quantitative guidelines.
- Why unresolved: While the paper mentions that low budgets are preferred, it does not establish specific thresholds or provide a framework for determining the optimal budget that balances sensitivity detection and robustness assessment.
- What evidence would resolve it: Empirical studies showing how varying budget levels impact the detection of true sensitivity versus false positives for lack of robustness across different model types and data distributions.

### Open Question 3
- Question: How effective is the pseudo-distance method for categorical variables compared to other approaches in maintaining data envelope and capturing true variable relationships?
- Basis in paper: [explicit] The paper introduces the pseudo-distance method as an alternative to random shuffling for categorical variables, stating it ensures conservative local perturbations, but acknowledges it may not capture true closeness as well as subject-matter expertise.
- Why unresolved: The paper only compares the pseudo-distance method to random shuffling on one dataset and does not evaluate its effectiveness in preserving data envelope or capturing true variable relationships compared to other possible approaches.
- What evidence would resolve it: Comparative studies of the pseudo-distance method against alternative categorical perturbation approaches (e.g., expert-defined distance metrics, clustering-based methods) on multiple datasets, evaluating both data envelope preservation and relationship capture.

## Limitations
- The framework focuses exclusively on in-distribution perturbations without addressing out-of-distribution scenarios or distributional shift robustness.
- The choice of budget parameter (2%) and K=100 perturbations appears somewhat arbitrary without sensitivity analysis.
- The pseudo-distance metric for categorical variables lacks comparison with alternative perturbation strategies.

## Confidence

- High confidence: The separation of robustness assessment from model accuracy through rPPV metric formulation
- Medium confidence: The effectiveness of pseudo-distance metric for categorical perturbation in preserving semantic similarity
- Medium confidence: The superiority of adaptive perturbations over raw perturbations in maintaining local structure
- Low confidence: The generalizability of findings to datasets with different characteristics or model architectures

## Next Checks

1. Conduct ablation study comparing pseudo-distance method with alternative categorical perturbation strategies (e.g., uniform shuffling, embedding-based distance)
2. Perform sensitivity analysis on budget parameter (2%) and K=100 perturbations to determine stability of model rankings
3. Validate framework on additional datasets with different dimensionalities and variable types to assess generalizability of robustness conclusions