---
ver: rpa2
title: Comparison of parallel SMC and MCMC for Bayesian deep learning
arxiv_id: '2402.06173'
source_url: https://arxiv.org/abs/2402.06173
tags:
- mcmc
- parallel
- carlo
- methods
- monte
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper compares parallel implementations of sequential Monte\
  \ Carlo (SMC\u2225) and Markov chain Monte Carlo (MCMC\u2225) methods for Bayesian\
  \ deep learning, focusing on their scalability and convergence properties. Both\
  \ methods are designed to provide consistent (asymptotically unbiased) posterior\
  \ estimates but differ in computational complexity and communication overhead."
---

# Comparison of parallel SMC and MCMC for Bayesian deep learning

## Quick Facts
- arXiv ID: 2402.06173
- Source URL: https://arxiv.org/abs/2402.06173
- Authors: Xinzhu Liang, Joseph M. Lukens, Sanjaya Lohani, Brian T. Kirby, Thomas A. Searles, Xin Qiu, Kody J. H. Law
- Reference count: 40
- The paper compares parallel implementations of sequential Monte Carlo (SMC∥) and Markov chain Monte Carlo (MCMC∥) methods for Bayesian deep learning, showing SMC∥ achieves O(1) complexity while MCMC∥ has O(log P) complexity due to initialization bias.

## Executive Summary
This paper presents a theoretical and empirical comparison of parallel implementations of sequential Monte Carlo (SMC∥) and Markov chain Monte Carlo (MCMC∥) methods for Bayesian deep learning. Both methods are designed to provide asymptotically unbiased posterior estimates but differ in their computational complexity and communication requirements. The authors demonstrate that SMC∥ achieves constant time complexity O(1) in the number of parallel processes, while MCMC∥ exhibits logarithmic scaling O(log P) due to initialization bias requiring warm-up samples that scale with log(P). Systematic experiments on MNIST, CIFAR, and IMDb datasets reveal that both methods perform comparably when run for sufficient wall-clock time, though both suffer from catastrophic non-convergence if chains are not run long enough. The choice between methods depends on the trade-off between communication overhead (higher for SMC∥) and warm-up requirements (higher for MCMC∥).

## Method Summary
The paper compares two parallel Bayesian inference methods: SMC∥ and MCMC∥. SMC∥ uses P parallel SMC samplers with N particles each, employing adaptive tempering schedules and particle selection strategies to approximate the posterior distribution. MCMC∥ uses P parallel MCMC chains (HMC or pCN kernels) that require warm-up samples scaling with log(P) to overcome initialization bias. Both methods are designed to provide consistent (asymptotically unbiased) posterior estimates. The theoretical analysis shows SMC∥ achieves O(1) complexity in the number of parallel processes, while MCMC∥ has O(log P) complexity due to initialization bias. The experiments evaluate test accuracy, negative log-likelihood, mean squared error, and effective sample size across multiple datasets including MNIST, CIFAR, and IMDb, using both CPU and GPU implementations.

## Key Results
- SMC∥ achieves O(1) complexity scaling with parallel processes while MCMC∥ exhibits O(log P) scaling due to initialization bias
- Both methods produce comparable results to serial implementations when run for sufficient wall-clock time
- Both SMC∥ and MCMC∥ suffer from catastrophic non-convergence if chains are not run long enough, with test accuracy degrading to 10% (random guessing) in failure cases
- SMC∥ requires more communication overhead but offers communication-free parallelism, while MCMC∥ eliminates communication costs but requires log(P) warm-up samples

## Why This Works (Mechanism)
The paper demonstrates that both SMC∥ and MCMC∥ can achieve asymptotically unbiased posterior estimates through different mechanisms. SMC∥ leverages particle diversity across parallel chains and adaptive tempering to explore the posterior efficiently, with communication overhead for particle exchange being the primary cost. MCMC∥ exploits the independence of parallel chains to achieve parallelism, but requires initialization bias correction through log(P) warm-up samples to ensure convergence to the true posterior. The O(1) scaling of SMC∥ arises from its ability to parallelize the particle filter structure, while MCMC∥'s O(log P) scaling reflects the cost of overcoming initialization bias in high-dimensional spaces.

## Foundational Learning
**Sequential Monte Carlo (SMC)**: A particle-based inference method that uses sequential importance sampling and resampling to approximate posterior distributions. Needed to understand the SMC∥ algorithm's particle dynamics and tempering approach. Quick check: Verify that particle weights sum to 1 after each resampling step.

**Markov Chain Monte Carlo (MCMC)**: A class of algorithms that generate samples from probability distributions by constructing a Markov chain with the desired stationary distribution. Needed to understand the MCMC∥ algorithm's convergence properties and initialization bias. Quick check: Monitor trace plots for stationarity and autocorrelation time for mixing.

**Initialization Bias in Parallel MCMC**: The phenomenon where parallel chains initialized from different starting points require additional warm-up samples proportional to log(P) to overcome initial bias and converge to the true posterior. Needed to understand why MCMC∥ has O(log P) complexity. Quick check: Compare ESS per second for different warm-up sample counts to identify optimal trade-off.

**Parallel Tempering**: A technique where multiple chains run in parallel at different "temperatures" to facilitate exploration of multimodal distributions. Needed to understand the adaptive tempering schedule in SMC∥. Quick check: Verify temperature ladder spacing follows geometric progression and monitor swap acceptance rates.

## Architecture Onboarding

**Component Map**: Data → Model Architecture → Parallel Inference Engine (SMC∥/MCMC∥) → Posterior Samples → Evaluation Metrics

**Critical Path**: Data preprocessing → Model initialization → Parallel inference execution → Convergence diagnostics → Posterior analysis

**Design Tradeoffs**: SMC∥ offers better theoretical scaling (O(1)) but higher communication overhead; MCMC∥ eliminates communication costs but requires log(P) warm-up samples. The choice depends on available computational resources and network architecture.

**Failure Signatures**: Catastrophic non-convergence manifests as test accuracy dropping to random guessing (10% for classification), ESS approaching zero, and trace plots showing no mixing. Monitor these metrics to detect failures early.

**First Experiments**:
1. Run SMC∥ and MCMC∥ on MNIST with P=4, N=100, measuring wall-clock time, test accuracy, and ESS to establish baseline performance
2. Vary P from 1 to 64 while keeping N constant to empirically verify O(1) vs O(log P) scaling claims
3. Test both methods on Australian Credit dataset with logistic regression to validate performance on smaller-scale problems

## Open Questions the Paper Calls Out

**Open Question 1**: What is the optimal strategy for selecting M (number of MCMC steps per tempering stage) in SMC, and how does it vary with problem complexity and dimension? The paper notes that "It is not clear how to select Mmin" and shows empirical behavior of MSE with varying M and N but lacks a theoretical framework for optimal M selection.

**Open Question 2**: How does the communication overhead in SMC∥ scale with the number of processors P and particles N, and what architectural optimizations could minimize this bottleneck? The paper only tests communication overhead on a specific HPC setup without providing a general scaling model or exploring architectural solutions like hierarchical communication.

**Open Question 3**: Under what conditions do SMC∥ and MCMC∥ exhibit catastrophic non-convergence, and can early warning metrics predict when chains are "too short"? The paper identifies the problem but doesn't provide quantitative criteria for identifying "long enough" chains or explore diagnostic metrics for predicting impending non-convergence.

## Limitations
- Results are primarily validated on specific architectures (CNNs for vision tasks, LSTM for IMDb) limiting generalizability to other model families
- Communication overhead experiments conducted on specialized HPC infrastructure, making results difficult to reproduce on cloud-based distributed systems
- The paper doesn't provide theoretical bounds on minimum chain lengths required for different problem classes to avoid catastrophic non-convergence

## Confidence
- **High**: Theoretical complexity analysis showing O(1) vs O(log P) scaling differences
- **Medium**: Empirical comparison results across multiple benchmark datasets (MNIST, CIFAR, IMDb)
- **Medium**: Claims about communication overhead trade-offs between SMC∥ and MCMC∥

## Next Checks
1. **Scalability verification**: Test the O(1) scaling claim of SMC∥ with larger P values (>128) on a cloud-based distributed system to confirm theoretical predictions hold beyond HPC environments.

2. **Architecture robustness**: Validate the comparative results using different neural network architectures (ResNet, Transformer) to assess method performance across varied model families.

3. **Convergence diagnostics**: Implement automated monitoring of ESS and autocorrelation time to establish minimum runtime thresholds for avoiding catastrophic non-convergence across different dataset sizes.