---
ver: rpa2
title: 'Parrot: Multilingual Visual Instruction Tuning'
arxiv_id: '2406.02539'
source_url: https://arxiv.org/abs/2406.02539
tags:
- multilingual
- parrot
- visual
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multilingual erosion in multimodal
  large language models (MLLMs), where their ability to handle non-English languages
  degrades after visual-text alignment training due to imbalanced, English-centric
  datasets. To solve this, the authors propose PARROT, a method that uses textual
  guidance to align visual tokens at the language level.
---

# Parrot: Multilingual Visual Instruction Tuning

## Quick Facts
- arXiv ID: 2406.02539
- Source URL: https://arxiv.org/abs/2406.02539
- Reference count: 40
- Primary result: Introduces PARROT method that dynamically converts English-biased visual features into language-specific embeddings, achieving state-of-the-art performance on multilingual multimodal tasks

## Executive Summary
This paper addresses the problem of multilingual erosion in multimodal large language models (MLLMs), where their ability to handle non-English languages degrades after visual-text alignment training due to imbalanced, English-centric datasets. To solve this, the authors propose PARROT, a method that uses textual guidance to align visual tokens at the language level. PARROT employs a Mixture-of-Experts (MoE) module that dynamically converts English-biased visual features into language-specific embeddings based on the input language.

## Method Summary
PARROT introduces a novel approach to multilingual visual instruction tuning by addressing the challenge of language imbalance in multimodal datasets. The method uses a Mixture-of-Experts (MoE) architecture that dynamically transforms English-centric visual features into language-specific embeddings through textual guidance. This allows the model to maintain multilingual capabilities while performing visual-text alignment tasks. The approach is evaluated on a newly introduced benchmark covering 6 languages with 12,000 questions, demonstrating significant improvements particularly in Turkish and Arabic while maintaining competitive performance across other languages.

## Key Results
- PARROT achieves state-of-the-art performance on both multilingual and multimodal tasks
- Outperforms models like Qwen2-VL and LLaVA-OneVision in multiple languages
- Shows significant improvements in Turkish and Arabic performance
- Demonstrates effective multilingual alignment with minimal multilingual data requirements

## Why This Works (Mechanism)
PARROT works by addressing the fundamental challenge of language imbalance in multimodal training data through dynamic feature transformation. The MoE module serves as a language-specific adapter that converts generic visual features into language-appropriate representations based on the input language context. This approach preserves the visual understanding capabilities while adapting the language-specific components, allowing the model to maintain strong performance across different languages without requiring extensive multilingual visual data.

## Foundational Learning
- Mixture-of-Experts (MoE): A neural network architecture where multiple expert networks are used, and a gating network determines which experts to use for each input - needed for dynamic language-specific feature transformation; quick check: verify gating mechanism effectively routes based on language
- Visual-Text Alignment: The process of aligning visual and textual representations in multimodal models - needed to ensure coherent understanding across modalities; quick check: test alignment quality on cross-modal retrieval tasks
- Multilingual Erosion: Degradation of non-English language capabilities in models trained on English-centric data - needed to understand the problem being solved; quick check: measure performance drop when training on English-only data
- Textual Guidance: Using language context to guide feature transformation - needed for language-specific adaptation; quick check: validate guidance effectiveness through ablation studies
- Token Alignment: The process of matching visual tokens with corresponding language representations - needed for coherent multimodal understanding; quick check: verify alignment accuracy across different language pairs

## Architecture Onboarding

**Component Map:** Input Text -> Language Detection -> MoE Gating Network -> Expert Networks -> Language-Specific Visual Embeddings -> Multimodal Processing

**Critical Path:** The most critical path in PARROT is the text-to-language detection to MoE gating mechanism, which determines how visual features are transformed based on the input language. This pathway must be highly accurate to ensure proper language-specific adaptation of visual features.

**Design Tradeoffs:** The authors trade model complexity (through the MoE architecture) for improved multilingual performance, accepting additional parameters and computational overhead to achieve better language-specific visual understanding. This contrasts with simpler approaches that might use language-specific adapters or fine-tuning.

**Failure Signatures:** Potential failure modes include incorrect language detection leading to wrong expert selection, MoE gating network confusion between similar languages, and visual feature transformation that doesn't properly preserve semantic information across languages.

**First Experiments:**
1. Test MoE gating network accuracy on language identification tasks
2. Measure visual feature preservation quality before and after MoE transformation
3. Evaluate language-specific performance on simple VQA tasks to validate individual expert effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation covers only 6 languages, limiting generalizability to truly multilingual scenarios
- Focus primarily on VQA-style tasks leaves unclear benefits for other multimodal applications
- Heavy reliance on textual guidance mechanism without extensive validation of its necessity

## Confidence
- State-of-the-art claims: Medium
- Performance improvements in Turkish/Arabic: Medium
- Generalizability to other languages: Low
- Effectiveness of textual guidance mechanism: Medium

## Next Checks
1. Expand evaluation to include languages with different linguistic families and writing systems (e.g., Chinese, Hindi, Arabic script variants) to assess true multilingual generalization.

2. Perform extensive ablation studies to isolate the contribution of the MoE module versus the textual guidance mechanism, including training variants without either component.

3. Test PARROT on diverse multimodal tasks beyond VQA, such as image captioning, visual reasoning, and cross-modal retrieval, to evaluate the breadth of its multilingual capabilities.