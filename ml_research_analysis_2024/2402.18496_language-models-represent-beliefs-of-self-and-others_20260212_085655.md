---
ver: rpa2
title: Language Models Represent Beliefs of Self and Others
arxiv_id: '2402.18496'
source_url: https://arxiv.org/abs/2402.18496
tags:
- believes
- does
- belief
- story
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the Theory of Mind (ToM) capabilities of Large
  Language Models (LLMs) by investigating their internal representations of beliefs.
  The authors discover that it is possible to linearly decode the belief status of
  various agents from the perspectives of both the protagonist and the oracle through
  neural activations of language models.
---

# Language Models Represent Beliefs of Self and Others

## Quick Facts
- arXiv ID: 2402.18496
- Source URL: https://arxiv.org/abs/2402.18496
- Authors: Wentao Zhu; Zhining Zhang; Yizhou Wang
- Reference count: 40
- This study discovers that LLMs develop internal representations encoding both self and others' beliefs, which can be linearly decoded and manipulated to alter Theory of Mind performance.

## Executive Summary
This paper investigates Theory of Mind capabilities in Large Language Models by exploring their internal representations of beliefs. The authors demonstrate that it is possible to linearly decode the belief status of various agents from both protagonist and oracle perspectives through neural activations. By manipulating these representations using activation intervention, they observe dramatic changes in the models' ToM performance. The findings extend to diverse social reasoning tasks, suggesting these representations capture fundamental aspects of social reasoning.

## Method Summary
The study employs linear and multinomial probing on attention head activations to decode belief status from two perspectives: the protagonist and the oracle. The researchers use Mistral-7B-Instruct and DeepSeek-LLM-7B-Chat models with the BigToM dataset for social reasoning tasks. They implement activation intervention to manipulate model behavior by steering internal representations along identified directions. Cross-task analysis evaluates the generalization of these belief directions across different social reasoning tasks with varying causal inference patterns.

## Key Results
- Attention heads in middle layers can accurately capture belief status with over 80% validation accuracy
- Activation intervention along identified directions dramatically changes model ToM performance
- Belief representation directions generalize across different social reasoning tasks with varying causal inference patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs develop internal representations that encode both self and others' beliefs through attention head activations, which can be linearly decoded using probes.
- Mechanism: Attention heads in middle layers learn to represent belief states by projecting belief-relevant features into a linearly separable space. These representations are shaped by the causal structure of the input stories and the model's prediction task.
- Core assumption: Attention head activations contain sufficient information to linearly separate belief states of different agents, and this separability is learned during training.
- Evidence anchors: "it is possible to linearly decode the belief status of various agents from the perspectives of both the protagonist and the oracle through neural activations"; "a large number of attention heads can accurately capture the oracle's belief status" and "a specific group of attention heads in the middle layers exhibits remarkably better performance, achieving over 80% validation accuracy"

### Mechanism 2
- Claim: Manipulating internal belief representations through activation intervention directly alters the model's Theory of Mind reasoning performance.
- Mechanism: By steering attention head activations toward specific directions identified by probing (e.g., +TpFo), the model's output probability distribution shifts, causing it to select different answer choices in ToM tasks.
- Core assumption: The identified directions correspond to meaningful belief representations and not to arbitrary features; intervention strength is sufficient to override the model's default behavior.
- Evidence anchors: "By manipulating these representations, we observe dramatic changes in the models' ToM performance"; "we find that intervention towards this direction (+TpFo) remarkably changes the model performance, effectively improving the overall ToM reasoning capabilities"

### Mechanism 3
- Claim: The same belief representation directions generalize across different social reasoning tasks with varying causal inference patterns.
- Mechanism: Belief representations capture common causal variables (desires, percepts, beliefs) that are shared across tasks, allowing directions identified in one task to influence performance in others.
- Core assumption: All social reasoning tasks fundamentally require inference about belief states, even when they are implicit rather than explicit.
- Evidence anchors: "our findings extend to diverse social reasoning tasks that involve different causal inference patterns"; "the directions identified in one task do generalize to others, suggesting that these directions might encapsulate a more universal function as belief representations"

## Foundational Learning

- Concept: Linear probing and its limitations
  - Why needed here: Understanding how to extract interpretable features from model activations and what the results actually tell us about internal representations
  - Quick check question: What's the difference between a linear probe and a non-linear probe, and when might each be appropriate?

- Concept: Activation intervention techniques
  - Why needed here: Knowing how to manipulate model behavior by steering internal representations and understanding the mechanics of this process
  - Quick check question: How does changing the intervention strength α affect the model's output, and what does this tell us about the robustness of the belief representations?

- Concept: Theory of Mind task structures and causal inference patterns
  - Why needed here: Understanding the different types of social reasoning tasks and why certain belief representations would generalize across them
  - Quick check question: What are the key differences between Forward Belief, Forward Action, and Backward Belief tasks in terms of what they require the model to infer?

## Architecture Onboarding

- Component map: Story → Attention head activations → Belief representations → Linear classifier → Model behavior
- Critical path: Story → Attention head activations → Belief representations → Linear classifier → Model behavior
- Design tradeoffs:
  - Linear vs. non-linear probing: Simpler but potentially less expressive vs. more complex but harder to interpret
  - Intervention strength: Too weak to have effect vs. too strong to be stable
  - Task specificity: Task-specific directions vs. generalizable directions
- Failure signatures:
  - Probing accuracy near chance level
  - Intervention causing random output changes
  - Directions identified in one task failing to generalize
- First 3 experiments:
  1. Train linear probes on attention head activations to classify belief states and identify top-performing heads
  2. Apply activation intervention along identified directions and measure changes in ToM task performance
  3. Test generalization of intervention directions across different social reasoning tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language models develop distinct internal representations for self-belief versus other agents' beliefs, and if so, how do these representations evolve during training?
- Basis in paper: The paper discovers that it is possible to linearly decode the belief status of various agents from the perspectives of both the protagonist and the oracle through neural activations of language models, indicating the existence of internal representations of self and others' beliefs.
- Why unresolved: While the paper identifies that such representations exist and can be manipulated, it does not explore the developmental process of these representations during the training of language models.
- What evidence would resolve it: Detailed analysis of the internal representations at different stages of training, showing how and when the models begin to differentiate between self-belief and others' beliefs.

### Open Question 2
- Question: How do the identified belief representations in language models generalize to more complex social reasoning tasks that involve multiple agents and conflicting beliefs?
- Basis in paper: The findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.
- Why unresolved: The paper primarily focuses on tasks with a single protagonist and a clear causal event. It does not address scenarios with multiple agents and more intricate belief conflicts.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the belief representations in tasks involving multiple agents with conflicting beliefs, showing whether the representations can handle more complex social dynamics.

### Open Question 3
- Question: What is the impact of manipulating belief representations on the overall coherence and consistency of language models' responses in unrelated tasks?
- Basis in paper: The paper shows that manipulation of these representations significantly affects the model's social reasoning performances and explores the influence of intervention along the identified belief directions on unrelated tasks.
- Why unresolved: The study notes that activation intervention along the identified ToM directions does not significantly change the model performance on unrelated tasks, but it does not investigate the broader implications on coherence and consistency.
- What evidence would resolve it: Comprehensive evaluation of language models' responses across a wide range of unrelated tasks before and after belief representation manipulation, assessing any changes in coherence and consistency.

## Limitations

- The analysis is based on abstract and introduction-level information, limiting validation of mechanisms described
- Technical uncertainty around activation intervention implementation details and key hyperparameters
- Generalization claims across diverse social reasoning tasks require more extensive empirical validation

## Confidence

**High Confidence**: The fundamental premise that LLMs develop internal representations of beliefs that can be decoded through linear probing. This is supported by established techniques in interpretability research and the theoretical framework of attention mechanisms.

**Medium Confidence**: The claim that manipulating these representations through activation intervention directly alters ToM performance. While the mechanism is plausible and aligns with other intervention studies, the specific implementation details and robustness of the approach are unclear.

**Low Confidence**: The assertion that these representations generalize across diverse social reasoning tasks with different causal inference patterns. This claim requires more extensive empirical validation across a broader range of tasks and distributions.

## Next Checks

1. **Probe Robustness Analysis**: Conduct cross-validation studies with varying probe architectures (linear vs. non-linear) and regularization techniques to establish the stability of belief representations across different model states and input distributions.

2. **Intervention Ablation Study**: Systematically vary intervention strength (α) and direction selection methods across multiple tasks to determine the sensitivity of performance changes and identify optimal intervention parameters.

3. **Distribution Shift Evaluation**: Test the generalization claims by evaluating the belief representations and intervention effectiveness on out-of-distribution social reasoning tasks, including those with different narrative structures and causal patterns.