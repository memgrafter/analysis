---
ver: rpa2
title: 'HOLMES: to Detect Adversarial Examples with Multiple Detectors'
arxiv_id: '2405.19956'
source_url: https://arxiv.org/abs/2405.19956
tags:
- adversarial
- examples
- detectors
- holmes
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of detecting adversarial examples
  in deep neural networks (DNNs) without modifying the original models. The proposed
  method, HOLMES, trains multiple binary detectors using logits (confidence scores)
  from the DNN's last hidden layer as features.
---

# HOLMES: to Detect Adversarial Examples with Multiple Detectors

## Quick Facts
- arXiv ID: 2405.19956
- Source URL: https://arxiv.org/abs/2405.19956
- Reference count: 40
- One-line primary result: HOLMES achieves over 99% detection rate for various adversarial attacks on MNIST, CIFAR-10, and ImageNet datasets.

## Executive Summary
This paper proposes HOLMES, a novel approach to detect adversarial examples in deep neural networks without modifying the original models. The method trains multiple binary detectors using logits from the DNN's last hidden layer as features. Two diversification strategies are employed: training dedicated detectors per classification and using top-k confidence scores. HOLMES demonstrates high detection rates (>99%) across various adversarial attacks on standard datasets, with low false positive rates and compatibility with different DNN architectures.

## Method Summary
HOLMES trains multiple binary detectors using logits from the DNN's last hidden layer as features to distinguish adversarial from benign examples. The method employs two diversification strategies: training dedicated detectors for each classification and training detectors with top-k confidence scores. These detectors are evaluated using three consensus policies (Any, Major, All) to determine the final decision. The approach is compatible with any DNN architecture since it only requires logits and doesn't modify the original model.

## Key Results
- Achieves over 99% detection rate for various adversarial attacks on MNIST, CIFAR-10, and ImageNet datasets
- Demonstrates transferability to unseen attacks with consistent performance
- Maintains low false positive rates across all tested scenarios
- Shows robustness against adaptive attacks with minimal performance degradation

## Why This Works (Mechanism)

### Mechanism 1
Adversarial examples have lower maximum and variance in logits than benign examples. By training binary classifiers on logits, HOLMES can distinguish adversarial examples because the statistical distribution of logits differs significantly. Core assumption: The confidence score distribution of adversarial examples is consistently more concentrated and has lower maximum values than benign examples. Break condition: If adversarial generation techniques evolve to maintain higher confidence scores and variance similar to benign examples.

### Mechanism 2
Multiple diverse detectors are harder to bypass than a single detector. HOLMES trains dedicated detectors for each classification and detectors with top-k logits to create diversity. Attackers must bypass multiple detectors simultaneously. Core assumption: Each detector captures different aspects of the logit distribution, making it difficult for a single adaptive attack to fool all detectors. Break condition: If attackers can develop a single adaptive attack that fools all detector types simultaneously.

### Mechanism 3
HOLMES is compatible with any DNN architecture since it only requires logits. HOLMES operates externally to the DNN, using the last hidden layer outputs (logits) as features, making it architecture-agnostic. Core assumption: Logits from the last hidden layer are consistently available and meaningful across different DNN architectures. Break condition: If certain DNN architectures do not produce interpretable logits or if the last hidden layer is not accessible.

## Foundational Learning

- Concept: Adversarial examples and their generation methods (FGSM, JSMA, CW attacks)
  - Why needed here: Understanding how adversarial examples are created is crucial for developing effective detection mechanisms.
  - Quick check question: What are the three main distance metrics used to measure perturbations in adversarial examples?

- Concept: Deep neural network architecture and logits
  - Why needed here: HOLMES relies on logits from the last hidden layer as features for detection, so understanding their role is essential.
  - Quick check question: What is the relationship between logits and softmax probabilities in a DNN?

- Concept: Binary classification and detector training
  - Why needed here: HOLMES uses binary classifiers to distinguish adversarial from benign examples based on logit features.
  - Quick check question: How does the choice of training data affect the performance of a binary detector?

## Architecture Onboarding

- Component map: DNN model -> Logit extraction layer -> Multiple binary detectors -> Consensus policy module -> Output decision layer
- Critical path: 1. Input image passes through DNN, 2. Logits extracted from last hidden layer, 3. Each detector processes logits independently, 4. Consensus policy determines final decision, 5. Output: benign or adversarial classification
- Design tradeoffs: Detector diversity vs. computational cost, Strictness of consensus policy vs. false positive rates, Number of detectors vs. detection accuracy
- Failure signatures: High false positive rates indicate over-sensitive detectors, Low detection rates suggest insufficient detector diversity, Inconsistent results across consensus policies indicate detector disagreement
- First 3 experiments: 1. Test HOLMES with single detector on MNIST dataset using CW-L2 attacks, 2. Add dedicated detectors per class and measure improvement in detection rates, 3. Test different consensus policies (Any, Major, All) and compare false positive rates

## Open Questions the Paper Calls Out

### Open Question 1
How does the number of detectors in HOLMES affect its detection performance and computational efficiency? The paper only evaluates HOLMES with three detectors and doesn't systematically study the impact of varying the number of detectors.

### Open Question 2
Can the diversification strategies used in HOLMES be applied to other detection methods to improve their performance against adversarial examples? The paper doesn't explore their applicability to other detection methods.

### Open Question 3
What is the optimal balance between the sensitivity of HOLMES (detecting more adversarial examples) and its specificity (minimizing false positives) for different application scenarios? The paper doesn't provide guidance on how to choose the optimal policy for specific scenarios.

## Limitations
- Performance claims based on standard attack methods may not hold against more sophisticated adaptive attacks
- System's performance on datasets with natural noise patterns (medical imaging, satellite data) is untested
- Architectural flexibility claim lacks systematic testing across different DNN types

## Confidence

Detection performance claims: Medium - Strong results shown but limited attack diversity tested
Architecture compatibility: Low - Theoretical claim with minimal empirical validation
Adaptive attack robustness: Medium - Some adaptive attack testing but methodology unclear
Transferability claims: Low - Insufficient evidence for cross-dataset and cross-model transferability

## Next Checks

1. Test HOLMES against state-of-the-art adaptive attacks (P-RGF, HPSO) with varying noise budgets and multi-step generation processes
2. Evaluate performance on noisy datasets (medical images, satellite imagery) with varying signal-to-noise ratios to assess false positive rates in realistic conditions
3. Implement cross-architecture validation using different DNN types (CNNs, ResNets, Transformers) to verify compatibility claims across diverse model families