---
ver: rpa2
title: Looped ReLU MLPs May Be All You Need as Practical Programmable Computers
arxiv_id: '2410.09375'
source_url: https://arxiv.org/abs/2410.09375
tags:
- relu-mlp
- data
- step
- arxiv
- operation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that a 23-layer looped ReLU MLP can function as
  a universal programmable computer, challenging the belief that only advanced architectures
  like Transformers can perform complex computational tasks. The key insight is that
  by carefully constructing a ReLU MLP with 23 layers, it can implement the single-instruction
  SUBLEQ (subtract and branch if less than or equal to zero) operation, which is Turing-complete.
---

# Looped ReLU MLPs May Be All You Need as Practical Programmable Computers

## Quick Facts
- arXiv ID: 2410.09375
- Source URL: https://arxiv.org/abs/2410.09375
- Reference count: 40
- Primary result: A 23-layer looped ReLU-MLP can function as a universal programmable computer, challenging the belief that only advanced architectures like Transformers can perform complex computational tasks

## Executive Summary
This paper demonstrates that a carefully constructed 23-layer looped ReLU-MLP can emulate a universal programmable computer by implementing the SUBLEQ instruction, which is Turing-complete. This challenges the conventional wisdom that sophisticated architectures like Transformers are necessary for complex computational tasks. The authors show that by decomposing fundamental operations (memory access, arithmetic, conditional branching) into modular MLP components, simple neural networks can achieve computational universality. Furthermore, using low-rank decomposition techniques, the parameter count can be reduced to O(n log n), making each forward pass more efficient than the O(n²) complexity of looped Transformers.

## Method Summary
The authors construct a 23-layer ReLU-MLP architecture that implements the SUBLEQ instruction through modular components: memory read/write operations, addition/subtraction functions, and conditional branching mechanisms. The state vector contains registers, memory, and instructions, with data represented in {±1} format for efficient address vector operations. While the naive implementation would require O(n²) parameters, the authors employ low-rank decomposition to reduce this to O(n log n). The construction proves that looped ReLU-MLPs can execute any program that a Turing-complete computer can run, with the efficiency advantage stemming from the reduced parameter count and computational complexity.

## Key Results
- A 23-layer looped ReLU-MLP can implement the SUBLEQ instruction, proving it can function as a universal programmable computer
- Using low-rank decomposition, the parameter count can be reduced from O(n²) to O(n log n)
- Looped ReLU-MLPs achieve O(n log n) time complexity, outperforming looped Transformers at O(n²) for programmable computing tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A 23-layer looped ReLU-MLP can emulate a universal programmable computer by implementing the SUBLEQ instruction.
- Mechanism: The paper constructs a ReLU-MLP architecture that performs SUBLEQ operations through modular components: reading/writing memory, addition/subtraction, and conditional branching. By looping this architecture, it can execute any program that a Turing-complete computer can run.
- Core assumption: The SUBLEQ instruction is Turing-complete and can be implemented using only ReLU activation functions.
- Evidence anchors:
  - [abstract] "by carefully constructing a ReLU MLP with 23 layers, it can implement the single-instruction SUBLEQ (subtract and branch if less than or equal to zero) operation, which is Turing-complete"
  - [section 4] "Theorem 4.1 (Looped ReLU-MLP as Programmable Computer)"
- Break condition: If ReLU-MLP cannot efficiently implement conditional branching or memory operations, the construction fails.

### Mechanism 2
- Claim: The computational efficiency of looped ReLU-MLP (O(n log n)) surpasses looped Transformers (O(n²)) for programmable computing tasks.
- Mechanism: Using low-rank decomposition techniques, the paper reduces parameter count from O(n²) to O(n log n), making each forward pass more efficient than the quadratic complexity of looped Transformers.
- Core assumption: Low-rank decomposition can effectively approximate the required weight matrices without losing computational capability.
- Evidence anchors:
  - [section 6.1] "we show that our model only requires O(n log n) number of parameters by low-rank decomposition"
  - [section 6.2] "our looped ReLU-MLP takes O(n log n) time complexity, while looped Transformer takes O(n²)"
- Break condition: If low-rank decomposition introduces numerical instability or cannot approximate the required operations accurately.

### Mechanism 3
- Claim: Simple neural network modules like ReLU-MLPs have untapped potential for complex computations and may not require sophisticated architectures like Transformers.
- Mechanism: The paper demonstrates that fundamental operations (read, write, arithmetic, branching) can be implemented using basic ReLU-MLP layers, suggesting that complex architectures may be unnecessary for certain computational tasks.
- Core assumption: The expressive power of ReLU-MLPs extends beyond traditional approximation tasks to algorithmic computation.
- Evidence anchors:
  - [abstract] "This indicates simple modules have stronger expressive power than previously expected and have not been fully explored"
  - [section 6.3] "Our research has confirmed that the capabilities of the ReLU-MLP are on par with Transformers when it comes to constructing programmable computers"
- Break condition: If certain algorithmic operations cannot be efficiently implemented using ReLU-MLP without requiring architectural innovations.

## Foundational Learning

- Concept: Turing completeness and computational universality
  - Why needed here: The paper's central claim relies on understanding that implementing SUBLEQ makes the system Turing-complete, equivalent to a programmable computer
  - Quick check question: What does it mean for a system to be Turing-complete, and why does implementing SUBLEQ achieve this?

- Concept: Neural network universal approximation theory
  - Why needed here: The paper builds on the fact that ReLU-MLPs are universal approximators, but extends this to algorithmic computation rather than just function approximation
  - Quick check question: How does the universal approximation property of ReLU-MLPs differ from their ability to perform algorithmic computations like SUBLEQ?

- Concept: Low-rank matrix decomposition and computational complexity
  - Why needed here: The efficiency argument depends on understanding how low-rank decomposition reduces parameter count and computational complexity
  - Quick check question: Why does decomposing an n×n matrix into n×log(n) matrices reduce the computational complexity from O(n²) to O(n log n)?

## Architecture Onboarding

- Component map: Memory read → Arithmetic operations → Conditional branching → Memory write → Program counter update → Loop back to memory read
- Critical path: The most critical path is the SUBLEQ execution sequence: read instruction → read memory operands → perform subtraction → conditional branching → write back result. Each of these steps requires specific MLP layers to function correctly.
- Design tradeoffs: The design trades architectural simplicity for layer count (23 layers vs. 9 layers for Transformers). The use of ReLU activation limits expressiveness but provides computational efficiency. The choice of {±1} data representation simplifies address vector operations but requires conversion for arithmetic.
- Failure signatures: If any of the fundamental operations (read, write, addition, subtraction, branching) cannot be implemented correctly, the entire construction fails. Numerical instability in low-rank decomposition would also cause failure. Inefficient implementation of any component could make the approach impractical.
- First 3 experiments:
  1. Implement a single-layer ReLU-MLP to verify it can perform basic read operations from memory to registers
  2. Build a 7-layer MLP to test subtraction and conditional branching functionality
  3. Construct the full 23-layer architecture to verify SUBLEQ instruction execution with simple test programs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the number of layers required for ReLU-MLP to function as a programmable computer be further reduced below 23?
- Basis in paper: [explicit] The paper proves that a 23-layer ReLU-MLP can emulate a programmable computer, but suggests this may not be the minimal construction
- Why unresolved: The authors acknowledge their construction may not be the smallest possible and leave this as future work
- What evidence would resolve it: Demonstrating a functional programmable computer using fewer than 23 layers of ReLU-MLP, or proving a lower bound on the number of layers required

### Open Question 2
- Question: Can ReLU-MLPs achieve in-context learning capabilities similar to Transformers?
- Basis in paper: [inferred] The paper notes that while ReLU-MLPs can function as programmable computers, it remains unclear whether they can perform complex tasks like in-context learning
- Why unresolved: The paper focuses on proving computational equivalence but doesn't explore the emergent capabilities that make Transformers effective for tasks like in-context learning
- What evidence would resolve it: Demonstrating successful in-context learning on a suite of tasks using looped ReLU-MLPs, or proving theoretical limitations preventing this capability

### Open Question 3
- Question: What is the minimal parameter requirement for a ReLU-MLP to perform specific computational tasks?
- Basis in paper: [explicit] The paper shows that using low-rank decomposition, parameter requirements can be reduced from O(n²) to O(n log n), but suggests further optimization is possible
- Why unresolved: The authors propose using their methodology to identify minimal model requirements for specific tasks but haven't implemented this approach
- What evidence would resolve it: Developing a systematic framework to determine minimal parameter requirements for various computational tasks, validated through empirical testing

## Limitations

- The construction relies on low-rank decomposition approximations whose numerical stability at scale remains unverified
- While the paper proves theoretical feasibility, the practical parameter count and runtime efficiency gains over Transformers require empirical validation on real workloads
- The extension from 1-bit to multi-bit integers is mentioned but not demonstrated

## Confidence

- High confidence: The SUBLEQ instruction is Turing-complete, and the modular construction approach is mathematically sound
- Medium confidence: The low-rank decomposition can achieve O(n log n) complexity while maintaining computational accuracy
- Low confidence: The practical efficiency gains over Transformers in real-world applications

## Next Checks

1. Implement the full 23-layer architecture and verify SUBLEQ execution on progressively complex test programs, measuring numerical stability
2. Benchmark the practical parameter count and runtime performance against looped Transformers on standard computational benchmarks
3. Validate the multi-bit integer extension by implementing a working prototype that can handle d-bit integers beyond the theoretical proof