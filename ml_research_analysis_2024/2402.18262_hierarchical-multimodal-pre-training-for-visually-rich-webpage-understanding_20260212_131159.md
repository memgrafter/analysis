---
ver: rpa2
title: Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding
arxiv_id: '2402.18262'
source_url: https://arxiv.org/abs/2402.18262
tags:
- html
- structure
- visual
- pre-training
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WebLM, a multimodal pre-training framework
  for understanding visually rich webpages. WebLM addresses the limitations of previous
  models that rely solely on HTML text and structure by incorporating hierarchical
  visual features extracted from webpage screenshots.
---

# Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding

## Quick Facts
- arXiv ID: 2402.18262
- Source URL: https://arxiv.org/abs/2402.18262
- Authors: Hongshen Xu; Lu Chen; Zihan Zhao; Da Ma; Ruisheng Cao; Zichen Zhu; Kai Yu
- Reference count: 40
- Primary result: WebLM achieves state-of-the-art performance on WebSRC and SWDE datasets by incorporating hierarchical visual features from webpage screenshots

## Executive Summary
This paper introduces WebLM, a multimodal pre-training framework designed to understand visually rich webpages by integrating text, structure, and visual information. Unlike previous models that rely solely on HTML text and structure, WebLM incorporates hierarchical visual features extracted from webpage screenshots, aligned with HTML DOM nodes. The model uses a unified Transformer architecture with three pre-training tasks (MLM, TSP, VMD) to learn cross-modal relationships. Experiments show WebLM significantly outperforms previous state-of-the-art models on both Web-based Structural Reading Comprehension (WebSRC) and Structured Web Data Extraction (SWDE) datasets.

## Method Summary
WebLM is a multimodal pre-training framework that processes HTML structure and content tokens separately, aligns them with hierarchical visual features extracted from webpage screenshots, and models cross-modal interactions using three pre-training tasks. The model is pre-trained on 6 million webpages for 300K steps using AdamW optimizer, then fine-tuned on WebSRC (Web-based Structural Reading Comprehension) and SWDE (Structured Web Data Extraction) datasets. The architecture uses a unified Transformer with separate embeddings for structure tokens, content tokens, tag information, and visual features, with 2D position embeddings for bounding box information.

## Key Results
- WebLM-base achieves 72.14% exact match and 79.67% F1 score on WebSRC development set
- WebLM-large achieves 78.40% exact match and 84.24% F1 score on WebSRC development set
- WebLM-base outperforms previous state-of-the-art models by 3.75% EM and 5.22% F1 on WebSRC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating HTML structure into visual feature extraction improves cross-modal alignment
- Mechanism: By aligning HTML DOM nodes with their corresponding regions in webpage screenshots, the model can extract hierarchical visual features that respect the semantic structure of the webpage
- Core assumption: There is a reliable visual alignment between HTML nodes and regions in the webpage screenshot
- Evidence anchors: [abstract] "Instead of processing document images as unified natural images, WebLM integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents"

### Mechanism 2
- Claim: Separate structure and content tokens accelerate information flow within each modality
- Mechanism: By placing structure tokens and text tokens in two separate lists and processing them separately, the model can better capture intra-modal relationships without interference from the other modality
- Core assumption: The structure and content modalities have distinct information patterns that can be better modeled separately
- Evidence anchors: [section] "Separate structure tokens from content tokens. ... This approach not only retains full document structure information by preserving the structure tokens but also accelerates information flow within each modality"

### Mechanism 3
- Claim: Pre-training tasks designed for inter-modal interactions improve the model's ability to understand webpages
- Mechanism: The Tree Structure Prediction (TSP) task models the semantic structure of webpages by predicting relationships between structure and content tokens. The Visual Misalignment Detection (VMD) task enhances visual robustness by detecting noise in the visual features
- Core assumption: Learning inter-modal interactions during pre-training transfers to downstream webpage understanding tasks
- Evidence anchors: [abstract] "we propose several pre-training tasks to model the interaction among text, structure, and image modalities effectively"

## Foundational Learning

- Concept: Multimodal pre-training
  - Why needed here: Webpage understanding requires integrating information from text, structure, and visual modalities. Multimodal pre-training allows the model to learn cross-modal relationships on large-scale unlabeled data
  - Quick check question: What are the three main modalities that WebLM integrates for webpage understanding?

- Concept: Hierarchical structure in documents
  - Why needed here: Webpages have a hierarchical structure (page → section → region → element) that carries semantic information. Modeling this structure helps the model understand the layout and relationships between different parts of the webpage
  - Quick check question: How does WebLM use the hierarchical structure of HTML to improve visual feature extraction?

- Concept: Cross-modal alignment
  - Why needed here: Aligning textual elements with their corresponding visual regions enables the model to learn the relationship between content and appearance. This alignment is crucial for tasks that require understanding both the text and layout of a webpage
  - Quick check question: How does WebLM align HTML nodes with regions in the webpage screenshot?

## Architecture Onboarding

- Component map:
  - Input layer: HTML parsing → Separate structure/content tokens → Extract hierarchical visual features → Align visual features with HTML nodes
  - Embedding layers: Token embedding + Tag embedding + Image embedding + 2D position embedding → Sum to construct input embeddings
  - Visual encoder: ResNeXt-FPN backbone for hierarchical visual features with RoS pooling
  - Transformer encoder: Unified Transformer architecture for cross-modal interaction
  - Pre-training tasks: MLM (15% masking) → TSP (tree relationship prediction) → VMD (visual misalignment detection)

- Critical path:
  1. Parse HTML into DOM tree and extract structure and content tokens
  2. Extract hierarchical visual features from webpage screenshot using ResNeXt-FPN
  3. Align visual features with HTML nodes based on bounding box information
  4. Construct input embeddings by summing token, tag, image, and position embeddings
  5. Feed input embeddings into Transformer encoder
  6. Apply pre-training tasks to learn cross-modal relationships

- Design tradeoffs:
  - Separating structure and content tokens improves intra-modal information flow but may slow down cross-modal interactions
  - Using hierarchical visual features captures multi-granularity information but requires reliable visual alignment
  - Pre-training on large-scale web data improves performance but requires significant computational resources

- Failure signatures:
  - Poor performance on visually complex webpages (Compare and Table types) due to inadequate visual feature extraction
  - Suboptimal results on SWDE due to noise in generated screenshots from HTML rendering
  - Slow convergence during pre-training due to imbalance in pre-training tasks

- First 3 experiments:
  1. Verify visual alignment: Extract visual features for a few HTML nodes and check if they correspond to the correct regions in the screenshot
  2. Test TSP task: Sample node pairs from HTML and check if the model can correctly predict their tree relationships
  3. Evaluate VMD task: Add noise to visual features and check if the model can detect the misalignment

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the traditional sense, but the limitations section and discussion point to several areas for future research, including the need for better visual alignment mechanisms and exploration of different pre-training task designs.

## Limitations
- Dependence on reliable visual alignment between HTML nodes and screenshot regions, which may be noisy or unavailable in practice
- Computational cost of pre-training on large-scale web data (6 million webpages, 300K steps)
- Potential suboptimal performance on webpages with complex visual layouts that are difficult to capture with hierarchical visual features

## Confidence
- Hierarchical visual feature extraction: Medium confidence - theoretically sound but dependent on reliable visual alignment
- Separate structure/content processing: Medium confidence - improves intra-modal flow but may slow cross-modal interactions
- Inter-modal pre-training tasks: Medium confidence - effective for learning cross-modal relationships but task design impacts transfer

## Next Checks
1. Verify visual alignment by extracting visual features for sample HTML nodes and manually checking correspondence with screenshot regions
2. Implement ablation study where structure and content tokens are processed together to quantify impact on cross-modal interaction
3. Test model robustness on webpages with varying visual complexity and structural noise to assess VMD task effectiveness