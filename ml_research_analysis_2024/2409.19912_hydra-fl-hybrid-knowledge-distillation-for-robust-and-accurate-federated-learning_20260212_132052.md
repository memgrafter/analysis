---
ver: rpa2
title: 'HYDRA-FL: Hybrid Knowledge Distillation for Robust and Accurate Federated
  Learning'
arxiv_id: '2409.19912'
source_url: https://arxiv.org/abs/2409.19912
tags:
- data
- attack
- moon
- distillation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of knowledge distillation
  (KD)-based federated learning (FL) to model poisoning attacks, which causes a phenomenon
  called "attack amplification." In benign settings, KD improves model performance,
  but in adversarial settings, it unknowingly aligns local models with poisoned global
  models, leading to accuracy degradation. To address this, the authors propose HYDRA-FL,
  a hybrid KD framework that applies KD loss at both the final and a shallow layer
  via an auxiliary classifier, reducing the final layer's KD loss.
---

# HYDRA-FL: Hybrid Knowledge Distillation for Robust and Accurate Federated Learning

## Quick Facts
- **arXiv ID:** 2409.19912
- **Source URL:** https://arxiv.org/abs/2409.19912
- **Reference count:** 40
- **Primary result:** HYDRA-FL significantly improves accuracy in attack scenarios while maintaining comparable performance in benign settings for KD-based FL

## Executive Summary
This paper investigates the vulnerability of knowledge distillation (KD)-based federated learning to model poisoning attacks, which cause a phenomenon called "attack amplification." While KD improves model performance in benign settings, it unknowingly aligns local models with poisoned global models in adversarial settings, leading to accuracy degradation. The authors propose HYDRA-FL, a hybrid KD framework that applies KD loss at both the final and a shallow layer via an auxiliary classifier, reducing the final layer's KD loss. HYDRA-FL is adapted to FedNTD and MOON, two KD-based FL algorithms, and evaluated on MNIST, CIFAR10, and CIFAR100 datasets.

## Method Summary
HYDRA-FL introduces a hybrid knowledge distillation approach for federated learning that addresses attack amplification by combining shallow and diminished final-layer distillation. The framework adds an auxiliary classifier at a shallow layer, applying KD-loss between this auxiliary classifier's output and the server's output. The final layer KD-loss coefficient is reduced by dividing it by a diminishing factor b. This dual-alignment strategy protects against poisoning while preserving knowledge transfer benefits. HYDRA-FL is adapted to FedNTD and MOON, using their respective architectures and loss functions with modified KD components.

## Key Results
- HYDRA-FL significantly improves accuracy in attack scenarios while maintaining comparable performance in benign settings
- On CIFAR10 with α=0.1, HYDRA-FL achieves 60.1% accuracy in attack scenarios, outperforming both MOON (58.8%) and FedAvg (57.8%)
- HYDRA-FL demonstrates robustness across multiple datasets (MNIST, CIFAR10, CIFAR100) and different levels of data heterogeneity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Shallow distillation at an intermediate layer reduces the influence of poisoned global model representations on local models.
- **Mechanism:** HYDRA-FL introduces an auxiliary classifier at a shallow layer, applying KD-loss between the auxiliary classifier's output/representation and the server's output/representation. This alignment occurs before deeper layers, which are more susceptible to adversarial manipulation.
- **Core assumption:** Shallow layers capture fundamental features less affected by model poisoning than final-layer outputs.
- **Evidence anchors:** [abstract] "HYDRA-FL introduces KD-loss at a shallow layer via an auxiliary classifier and reduces the KD-loss impact at the final layer."

### Mechanism 2
- **Claim:** Reducing the KD-loss coefficient at the final layer mitigates attack amplification while maintaining benign performance.
- **Mechanism:** HYDRA-FL divides the KD loss coefficient β by a diminishing factor b, reducing the weight of final-layer alignment between client and server models.
- **Core assumption:** Attack amplification occurs primarily through strong final-layer KD alignment with poisoned models.
- **Evidence anchors:** [abstract] "HYDRA-FL introduces KD-loss at a shallow layer via an auxiliary classifier and reduces the KD-loss impact at the final layer."

### Mechanism 3
- **Claim:** The hybrid approach balances robustness and accuracy by combining shallow and diminished final-layer distillation.
- **Mechanism:** HYDRA-FL maintains a reduced final-layer KD-loss while adding shallow-layer distillation, creating a dual-alignment strategy that protects against poisoning while preserving knowledge transfer benefits.
- **Core assumption:** Complete removal of final-layer KD-loss harms benign performance, while retaining some provides necessary benefits.
- **Evidence anchors:** [abstract] "Unlike traditional KD methods that apply KD-loss only at the final layer, HYDRA-FL introduces KD-loss at a shallow layer via an auxiliary classifier and reduces the KD-loss impact at the final layer."

## Foundational Learning

- **Concept:** Knowledge Distillation (KD) in machine learning
  - **Why needed here:** Understanding how KD transfers knowledge from teacher to student models is fundamental to grasping HYDRA-FL's approach to mitigating attack amplification.
  - **Quick check question:** What is the primary objective of knowledge distillation, and how does it typically work in neural networks?

- **Concept:** Federated Learning (FL) and data heterogeneity
  - **Why needed here:** HYDRA-FL addresses data heterogeneity challenges in FL while defending against model poisoning attacks.
  - **Quick check question:** How does data heterogeneity affect model performance in federated learning, and what are common approaches to address it?

- **Concept:** Model poisoning attacks in federated learning
  - **Why needed here:** HYDRA-FL's primary motivation is defending against model poisoning attacks that cause attack amplification in KD-based FL.
  - **Quick check question:** What distinguishes model poisoning attacks from data poisoning attacks, and why are model poisoning attacks considered more potent in FL?

## Architecture Onboarding

- **Component map:** Base model (client and server) -> Auxiliary classifier (shallow layer) -> Loss function components (Cross-entropy loss, Diminished KD loss at final layer, Shallow distillation loss) -> Aggregation rule (Trimmed Mean defense)

- **Critical path:** 1. Server distributes global model to selected clients 2. Clients compute local updates using HYDRA-FL loss function 3. Clients send local updates to server 4. Server applies Trimmed Mean aggregation to filter malicious updates 5. Server updates global model and redistributes

- **Design tradeoffs:**
  - Shallow vs. deep auxiliary classifier placement: Shallower layers provide more fundamental feature learning but may reduce the effectiveness of knowledge transfer
  - Diminishing factor b: Higher values reduce attack amplification but may impact benign performance
  - γ coefficient: Controls the importance of shallow distillation relative to other loss components

- **Failure signatures:**
  - Performance degradation in benign settings when b is too large
  - Ineffective defense against sophisticated attacks targeting shallow layers
  - Overfitting to local data when shallow distillation is too strong

- **First 3 experiments:**
  1. **Baseline comparison:** Implement FedAvg, MOON, and FedNTD with the same datasets and settings to establish performance baselines
  2. **Shallow layer placement:** Test HYDRA-FL with auxiliary classifiers at different shallow layers (after first vs. second convolutional layer) to determine optimal placement
  3. **Diminishing factor optimization:** Vary the diminishing factor b to find the optimal balance between attack robustness and benign performance across different heterogeneity levels (α = 0.05, 0.1, 0.5)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does HYDRA-FL perform when adapted to other KD-based FL techniques beyond FedNTD and MOON?
- **Basis in paper:** [explicit] The authors mention that HYDRA-FL is a generic framework adaptable to any FL algorithm, but only test it on FedNTD and MOON.
- **Why unresolved:** The paper does not provide empirical evidence of HYDRA-FL's performance on other KD-based FL techniques.
- **What evidence would resolve it:** Experiments applying HYDRA-FL to other KD-based FL techniques like FedMD, FedBE, or other contrastive learning-based methods, comparing their performance in both benign and attack settings.

### Open Question 2
- **Question:** What is the optimal depth for the auxiliary classifier in HYDRA-FL, and how does it vary across different model architectures?
- **Basis in paper:** [explicit] The authors explore different shallow layers for auxiliary classifiers in MOON, but the analysis is limited to two specific positions.
- **Why unresolved:** The paper does not provide a comprehensive study on the impact of auxiliary classifier placement across various depths and model architectures.
- **What evidence would resolve it:** Systematic experiments varying the auxiliary classifier position across different depths in various model architectures, measuring the impact on accuracy and robustness.

### Open Question 3
- **Question:** How does HYDRA-FL perform under data poisoning attacks, compared to model poisoning attacks?
- **Basis in paper:** [explicit] The paper focuses on model poisoning attacks and does not explore the performance of HYDRA-FL under data poisoning attacks.
- **Why unresolved:** The paper does not provide any empirical evidence of HYDRA-FL's robustness against data poisoning attacks.
- **What evidence would resolve it:** Experiments applying HYDRA-FL under various data poisoning attack scenarios, comparing its performance to model poisoning attacks and baseline techniques.

## Limitations
- **Shallow layer selection criteria:** The paper does not provide clear criteria for selecting the optimal shallow layer position for auxiliary classifiers, which could significantly impact performance across different model architectures
- **Hyperparameter sensitivity:** The choice of diminishing factor b and γ coefficient appears empirical, with limited analysis of their sensitivity to different attack types or data distributions
- **Defense transferability:** While HYDRA-FL shows promise against specific poisoning attacks, its effectiveness against more sophisticated adaptive attacks targeting the shallow distillation mechanism remains untested

## Confidence
- **High confidence:** The core mechanism of shallow distillation reducing attack amplification effects (supported by multiple experiments across datasets)
- **Medium confidence:** The claim that HYDRA-FL maintains comparable benign performance while improving attack robustness (performance gains vary across datasets and attack scenarios)
- **Low confidence:** The generalizability of HYDRA-FL to different model architectures beyond the tested CNN configurations (no experiments with transformers or other architectures)

## Next Checks
1. **Adaptive attack testing:** Implement white-box attacks that specifically target the shallow auxiliary classifier to evaluate HYDRA-FL's robustness against more sophisticated adversaries
2. **Architectural generalization:** Test HYDRA-FL with different backbone architectures (e.g., ResNet, Vision Transformer) to assess its applicability beyond the current CNN-based models
3. **Hyperparameter optimization:** Conduct systematic ablation studies varying b and γ across a wider range to identify optimal configurations for different heterogeneity levels and attack types