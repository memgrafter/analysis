---
ver: rpa2
title: 'GePBench: Evaluating Fundamental Geometric Perception for Multimodal Large
  Language Models'
arxiv_id: '2412.21036'
source_url: https://arxiv.org/abs/2412.21036
tags:
- geometric
- perception
- visual
- zhang
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GePBench, a benchmark designed to evaluate
  geometric perception in multimodal large language models (MLLMs). The benchmark
  focuses on tasks involving understanding geometric shapes, structures, and spatial
  relationships, which are foundational for higher-level visual reasoning.
---

# GePBench: Evaluating Fundamental Geometric Perception for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2412.21036
- Source URL: https://arxiv.org/abs/2412.21036
- Reference count: 40
- State-of-the-art models like GPT-4o and Claude-3.5-Sonnet achieve as low as 17.5% accuracy on size-related tasks, below random guessing

## Executive Summary
This paper introduces GePBench, a benchmark designed to evaluate geometric perception capabilities in multimodal large language models (MLLMs). The benchmark focuses on fundamental visual reasoning skills involving geometric shapes, structures, and spatial relationships through 80K images and 285K multiple-choice questions across six aspects: existence, counting, location, size, reference, and relationships. Evaluations on 20 state-of-the-art models reveal significant deficiencies in geometric perception, with top models performing below random guessing on certain tasks. The study also introduces LLaVA-GeP, a model trained with GePBench data that demonstrates notable improvements in tasks requiring spatial awareness and abstract visual understanding, highlighting the critical role of foundational geometric perception in enabling advanced multimodal applications.

## Method Summary
GePBench uses a data synthesis engine to generate structured textual descriptions of geometric figures, which are rendered into images and paired with multiple-choice questions. The benchmark evaluates 20 state-of-the-art MLLMs through zero-shot evaluation using standardized prompts. Additionally, LLaVA-GeP is trained using GePBench data through a two-stage process: feature alignment followed by visual instruction tuning. The evaluation covers six geometric perception aspects with both easy and hard splits, using a last-occurrence-of-option-letter scheme for answer parsing.

## Key Results
- State-of-the-art models show significant deficiencies in geometric perception, with GPT-4o achieving only 17.5% accuracy on size-related tasks
- Models trained with GePBench data demonstrate substantial improvements on downstream tasks requiring spatial awareness
- Visual encoders with higher resolution improve detail recognition but compromise spatial accuracy
- Mixed encoders underperform in geometric tasks due to increased complexity of positional embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured textual descriptions ensure precise alignment between geometric figures and their corresponding multiple-choice questions.
- Mechanism: The data synthesis engine generates structured textual descriptions that capture shape attributes and spatial relationships, which are then rendered into figures and used to create questions with ground truth answers.
- Core assumption: The structured textual description accurately represents the geometric properties and relationships in the figure.
- Evidence anchors:
  - [abstract]: "Our dataset is constructed using a specialized data synthesis engine that generates structured textual descriptions, which are then translated into geometric figures."
  - [section]: "To construct a comprehensive dataset for evaluating geometric perception, we begin by compiling a pool of commonly encountered basic geometric shapes... From this pool, we randomly sample shapes and assign them fundamental geometric attributes, i.e., size, position, and orientation."
  - [corpus]: Weak - no direct corpus evidence on the precision of this alignment mechanism.
- Break condition: If the rendering process introduces distortions or the question generation fails to accurately capture the structured description.

### Mechanism 2
- Claim: The inclusion of visual noise improves model robustness by simulating real-world conditions.
- Mechanism: The rendering process incorporates visual noise (Gaussian noise, salt-and-pepper noise, Perlin noise) with a probability of 0.5 to simulate real-world image degradation.
- Core assumption: Models trained on noisy data will generalize better to real-world images with noise.
- Evidence anchors:
  - [abstract]: "To align with real-world conditions where noise might exist in collected images, we incorporate some visual noise into the figures with a probability of 0.5."
  - [section]: "These include Gaussian noise for the background, random disturbance and salt-and-pepper noise on shape outlines, and Perlin noise (Perlin, 1985) for closed shapes."
  - [corpus]: Weak - no direct corpus evidence on the effectiveness of this noise simulation.
- Break condition: If the noise level is too high, making the figures unrecognizable, or too low, providing no benefit.

### Mechanism 3
- Claim: Training on GePBench data improves downstream performance on tasks requiring spatial awareness and abstract visual understanding.
- Mechanism: The LLaVA-GeP model is trained with GePBench data, which enhances its geometric perception capabilities, leading to improved performance on tasks like MME-Perception and MM-Vet.
- Core assumption: Geometric perception is a foundational skill that transfers to higher-level visual reasoning tasks.
- Evidence anchors:
  - [abstract]: "we show that models trained with GePBench data demonstrate substantial improvements on a wide range of benchmark tasks, highlighting the critical role of geometric perception in enabling advanced multimodal applications."
  - [section]: "The trained model, LLaVA-GeP-7B, is evaluated on a wide range of tasks... The results, visualized in Figure 6, show general improvements across the evaluated tasks. Notably, tasks demanding spatial awareness, abstract visual understanding and scientific diagram comprehension, such as MME-Perception, MM-Vet, ScienceQA-Image, exhibit considerable gains."
  - [corpus]: Weak - no direct corpus evidence on the transferability of geometric perception skills.
- Break condition: If the improvements are only seen on tasks closely related to geometric perception and not on more general visual reasoning tasks.

## Foundational Learning

- Concept: Geometric perception as a foundational skill for visual reasoning.
  - Why needed here: The paper emphasizes that geometric perception is a crucial, foundational ability that is often overlooked in current MLLM research, despite its importance for higher-level semantic tasks.
  - Quick check question: Can you explain why geometric perception is considered a foundational skill for visual reasoning in MLLMs?
- Concept: Data synthesis for controlled evaluation.
  - Why needed here: GePBench uses a data synthesis engine to generate controlled datasets, allowing for precise evaluation of geometric perception without the variability and noise present in real-world images.
  - Quick check question: What are the advantages of using a data synthesis engine for creating evaluation datasets?
- Concept: Multimodal model architecture and training.
  - Why needed here: Understanding the architecture and training process of MLLMs is essential for interpreting the results and understanding how improvements in geometric perception can be achieved.
  - Quick check question: Can you describe the typical components of a multimodal large language model and how they work together?

## Architecture Onboarding

- Component map: Data synthesis engine -> Rendering module -> Question generation pipeline -> MLLM
- Critical path: Generate structured textual description → Render figure → Generate question-answer pair → Evaluate MLLM response
- Design tradeoffs:
  - Noise inclusion: Adding noise improves robustness but may also introduce unnecessary complexity
  - Question difficulty: Balancing easy and hard questions to provide a comprehensive evaluation
  - Model size vs. performance: Larger models generally perform better but require more computational resources
- Failure signatures:
  - Low accuracy on size and location tasks: Indicates deficiencies in spatial awareness
  - High variance in performance across different aspects: Suggests that the model is not uniformly learning geometric perception
  - Poor generalization to noisy images: Implies that the model is not robust to real-world conditions
- First 3 experiments:
  1. Evaluate a simple MLLM (e.g., BLIP2) on GePBench to establish a baseline
  2. Test the impact of different visual encoders (e.g., CLIP, OpenCLIP, SigLIP) on geometric perception performance
  3. Assess the transferability of geometric perception skills by training a model on GePBench data and evaluating it on downstream tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific visual encoder architectures impact the performance of MLLMs on geometric perception tasks, and what architectural modifications could enhance spatial awareness?
- Basis in paper: [inferred] The paper discusses the impact of different visual encoders on geometric perception performance, noting that higher resolution can improve detail recognition but may compromise spatial accuracy. It also mentions that mixed encoders underperform in geometric tasks due to increased complexity of positional embeddings.
- Why unresolved: While the paper provides insights into how different visual encoders affect performance, it does not explore specific architectural modifications that could enhance spatial awareness. The study highlights the limitations of current encoders but does not propose concrete solutions or modifications.
- What evidence would resolve it: Conducting experiments with modified visual encoder architectures that prioritize spatial awareness, such as incorporating attention mechanisms specifically designed for spatial relationships, could provide evidence. Additionally, comparing the performance of these modified encoders against standard ones on geometric perception tasks would offer insights into their effectiveness.

### Open Question 2
- Question: To what extent does the inclusion of geometric perception tasks in MLLM training datasets improve performance on downstream applications requiring spatial reasoning?
- Basis in paper: [explicit] The paper introduces LLaVA-GeP, a model trained with GePBench data, which demonstrates improvements in tasks requiring spatial awareness and abstract visual understanding. This suggests that incorporating geometric perception tasks into training can enhance performance on related downstream applications.
- Why unresolved: While the paper shows that training with geometric perception data improves performance, it does not quantify the extent of improvement across a wide range of downstream applications. The study focuses on specific benchmarks but does not provide a comprehensive analysis of how these improvements translate to real-world scenarios.
- What evidence would resolve it: Conducting a large-scale evaluation of models trained with geometric perception tasks across diverse downstream applications, such as medical image analysis and fossil classification, would provide evidence. Comparing the performance of these models against those trained without such tasks would quantify the impact on spatial reasoning capabilities.

### Open Question 3
- Question: What are the limitations of current MLLM architectures in handling complex geometric relationships, and how can these limitations be addressed to improve geometric perception?
- Basis in paper: [inferred] The paper highlights that current MLLMs struggle with geometric perception tasks, particularly those involving complex relationships and spatial awareness. It suggests that the design of visual encoders, which prioritize invariance to transformations, may hinder precise spatial perception.
- Why unresolved: The paper identifies limitations in current MLLM architectures but does not propose specific strategies to address these issues. It acknowledges the need for advancements in foundational geometric understanding but does not explore potential architectural changes or training methodologies to overcome these limitations.
- What evidence would resolve it: Developing and testing new MLLM architectures that incorporate mechanisms for better handling of geometric relationships, such as graph-based representations or specialized attention layers, could provide evidence. Evaluating these architectures on geometric perception benchmarks would demonstrate their effectiveness in addressing current limitations.

## Limitations

- Data synthesis approach creates artificial evaluation environment that may not capture real-world geometric perception complexity
- Focus on multiple-choice questions may not comprehensively assess open-ended geometric reasoning capabilities
- Limited exploration of specific architectural modifications to enhance spatial awareness in MLLMs

## Confidence

- **High Confidence**: MLLMs show significant deficiencies in geometric perception tasks, particularly for size and location aspects
- **Medium Confidence**: Geometric perception is a foundational skill that transfers to higher-level visual reasoning tasks
- **Medium Confidence**: LLaVA-GeP trained on GePBench data shows improvements on downstream tasks

## Next Checks

1. Evaluate GePBench-trained models on real-world datasets containing natural images with geometric elements to assess generalization beyond synthetic data.

2. Conduct ablation studies comparing models trained on GePBench data versus models trained on other visual reasoning datasets to isolate the specific benefits of geometric perception training.

3. Test model performance on open-ended geometric reasoning tasks rather than only multiple-choice questions to better understand the depth of geometric understanding achieved.