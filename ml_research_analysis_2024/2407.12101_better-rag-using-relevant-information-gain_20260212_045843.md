---
ver: rpa2
title: Better RAG using Relevant Information Gain
arxiv_id: '2407.12101'
source_url: https://arxiv.org/abs/2407.12101
tags:
- dartboard
- retrieval
- diversity
- passages
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of retrieval augmented generation
  (RAG) systems where limited context windows require balancing relevance and diversity
  in retrieved passages to avoid redundancy. The authors propose Dartboard, a retrieval
  algorithm that maximizes relevant information gain by modeling retrieval as a probabilistic
  guessing game, naturally encouraging diversity without explicitly trading off relevance
  and diversity.
---

# Better RAG using Relevant Information Gain

## Quick Facts
- arXiv ID: 2407.12101
- Source URL: https://arxiv.org/abs/2407.12101
- Authors: Marc Pickett; Jeremy Hartman; Ayan Kumar Bhowmick; Raquib-ul Alam; Aditya Vempaty
- Reference count: 28
- Primary result: Novel Dartboard algorithm maximizes relevant information gain in RAG systems, achieving state-of-the-art performance on RGB benchmark

## Executive Summary
This paper addresses a fundamental challenge in Retrieval Augmented Generation (RAG) systems: balancing relevance and diversity when retrieving passages within limited context windows. Traditional approaches struggle with redundancy, often retrieving multiple similar passages that collectively provide little additional information. The authors propose Dartboard, a retrieval algorithm that reframes the problem as a probabilistic guessing game, where the goal is to maximize the expected information gain from each additional passage. This approach naturally encourages diversity without requiring explicit trade-offs between relevance and diversity metrics.

Dartboard's key innovation lies in its ability to model retrieval as an information-theoretic process, where each retrieved passage is evaluated based on its contribution to reducing uncertainty about the answer. By maximizing relevant information gain, the algorithm implicitly balances relevance and diversity - passages that provide novel, complementary information are favored over redundant ones. The method demonstrates significant improvements on the RGB benchmark, achieving state-of-the-art performance in both retrieval quality (measured by NDCG) and end-to-end question answering accuracy.

## Method Summary
The Dartboard algorithm models retrieval as a probabilistic guessing game where each retrieved passage provides information about the correct answer. The core objective is to maximize relevant information gain, which measures how much uncertainty about the answer is reduced by adding a new passage to the retrieval context. The algorithm iteratively selects passages that maximize this expected information gain, naturally balancing relevance (how well the passage answers the question) and diversity (how much new information it provides). This is achieved through a scoring function that combines relevance scores with a penalty for redundancy, computed based on the information-theoretic distance between candidate passages and already-retrieved content. The method operates within fixed context windows by prioritizing passages that contribute the most unique information, making it particularly effective for scenarios with limited retrieval budgets.

## Key Results
- Dartboard achieves state-of-the-art performance on the RGB benchmark across multiple metrics
- Significant improvements in retrieval quality with higher NDCG scores compared to baseline methods
- Demonstrates superior end-to-end question answering accuracy while maintaining efficient retrieval within context window constraints

## Why This Works (Mechanism)
The mechanism works by reframing retrieval as an information-theoretic optimization problem. Instead of treating relevance and diversity as separate objectives requiring explicit trade-offs, Dartboard models the retrieval process as a guessing game where each passage provides evidence to reduce uncertainty about the answer. The algorithm computes the expected information gain from each candidate passage - essentially measuring how much it would reduce the entropy of the answer distribution if added to the current retrieval set. This creates a natural incentive for diversity because redundant passages provide little additional information gain. The scoring function balances the immediate relevance of a passage with its long-term contribution to reducing uncertainty, allowing the system to make globally optimal retrieval decisions within the context window constraints.

## Foundational Learning

**Information Gain Theory**
- Why needed: Provides the mathematical foundation for measuring how much new information each passage contributes
- Quick check: Verify that information gain calculations properly account for redundancy between passages

**Probabilistic Guessing Games**
- Why needed: Enables the reframing of retrieval as an uncertainty reduction problem rather than a simple ranking task
- Quick check: Confirm that the guessing game formulation correctly models the sequential nature of retrieval

**Entropy and KL Divergence**
- Why needed: Essential for quantifying the information content and redundancy between passages
- Quick check: Validate that divergence measures properly capture semantic similarity between passages

**Context Window Optimization**
- Why needed: Ensures the algorithm operates effectively within practical constraints of fixed retrieval budgets
- Quick check: Test that the algorithm makes optimal decisions when forced to select from limited candidate sets

## Architecture Onboarding

Component Map:
Query Encoder -> Passage Retriever -> Dartboard Selector -> Context Pool -> Generator

Critical Path:
The critical path flows from the query through the passage retriever to the Dartboard selector, which makes the final selection of passages for the context pool. The generator then uses this context to produce the final answer.

Design Tradeoffs:
- Information gain vs. computational complexity: The probabilistic calculations add overhead but provide better diversity
- Context window size vs. answer quality: Smaller windows force harder selection decisions but improve efficiency
- Model complexity vs. interpretability: The information-theoretic approach is principled but harder to debug than simpler ranking methods

Failure Signatures:
- If information gain calculations are poorly calibrated, the system may over-prioritize diversity at the expense of relevance
- In cases where passages are highly redundant, the algorithm might fail to find enough unique information to answer questions
- Performance may degrade significantly if the passage retriever provides low-quality candidates

Three First Experiments:
1. Compare retrieval quality (NDCG) with and without the information gain optimization
2. Measure diversity of retrieved passages using automatic metrics and human evaluation
3. Evaluate end-to-end QA performance across different context window sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental evaluation relies solely on the RGB benchmark, limiting generalizability to other domains
- Comparison pool focuses on recent diversity-aware methods while omitting established baseline approaches
- Theoretical foundation is elegant but lacks thorough empirical validation across diverse datasets

## Confidence

| Claim | Confidence |
|-------|------------|
| State-of-the-art performance on RGB benchmark | Medium |
| Natural diversity encouragement through information gain | Medium |
| Effective balancing of relevance and diversity | Medium |

## Next Checks

1. Cross-dataset evaluation: Test Dartboard on established QA benchmarks like Natural Questions, TriviaQA, or WebQuestions to assess performance outside the RGB corpus.

2. Ablation analysis: Systematically evaluate the contribution of key components (e.g., the guessing game formulation, the information gain objective) by comparing against simplified variants of the algorithm.

3. Qualitative diversity assessment: Conduct human evaluation of retrieved passages to verify that the model achieves meaningful diversity beyond what is captured by automatic metrics, particularly in cases where relevance and diversity might conflict.