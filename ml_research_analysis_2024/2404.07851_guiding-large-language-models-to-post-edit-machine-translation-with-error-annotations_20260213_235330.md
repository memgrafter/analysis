---
ver: rpa2
title: Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations
arxiv_id: '2404.07851'
source_url: https://arxiv.org/abs/2404.07851
tags:
- translation
- feedback
- error
- language
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether smaller open-source large language\
  \ models (LLaMA-2 7B/13B) can effectively post-edit machine translation (MT) outputs\
  \ when guided by external feedback, as an alternative to relying on the largest\
  \ proprietary models. The authors explore prompting and fine-tuning strategies using\
  \ various granularities of feedback\u2014generic, score-based, and fine-grained\
  \ error annotations from both human (MQM) and automatic sources (InstructScore,\
  \ xCOMET)."
---

# Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations

## Quick Facts
- **arXiv ID**: 2404.07851
- **Source URL**: https://arxiv.org/abs/2404.07851
- **Reference count**: 40
- **Primary result**: Smaller open-source LLMs (7B/13B) can effectively post-edit MT outputs when guided by external error feedback, with fine-tuning significantly enhancing their ability to exploit fine-grained annotations.

## Executive Summary
This paper investigates whether smaller open-source large language models can effectively post-edit machine translation outputs when guided by external feedback, as an alternative to relying on the largest proprietary models. The authors explore prompting and fine-tuning strategies using various granularities of feedback—generic, score-based, and fine-grained error annotations from both human (MQM) and automatic sources (InstructScore, xCOMET). Experiments across Chinese-English, English-German, and English-Russian show that zero- and few-shot prompting improves translation quality (BLEU, COMET, TER) over original MT, though the advantage of fine-grained feedback is unclear in prompting settings. Fine-tuning the models on error-annotated data, however, significantly enhances their ability to exploit fine-grained feedback, yielding larger improvements in automatic metrics and human evaluation. Human annotators confirm that fine-tuned models not only fix targeted errors but also produce more natural, less translationese output. The findings demonstrate that effective MT post-editing is achievable with smaller open models, reducing dependence on costly human annotations.

## Method Summary
The authors investigate post-editing MT outputs using LLaMA-2 models (7B and 13B) guided by external feedback. They employ three feedback types: generic, score-based, and fine-grained error annotations (human-annotated via MQM and automatic via InstructScore/xCOMET). The approach includes zero-shot and few-shot prompting with these feedback types, followed by instruction fine-tuning using LoRA on error-annotated translations from MQM and DEMETR datasets. Models are evaluated on held-out test sets using automatic metrics (BLEU, TER, COMET) and human evaluation across three language pairs: Chinese-English, English-German, and English-Russian.

## Key Results
- Zero- and few-shot prompting with any form of feedback improves TER, BLEU, and COMET scores over original MT outputs.
- Fine-tuning on error-annotated data significantly enhances the ability to exploit fine-grained feedback, yielding larger improvements in automatic metrics.
- Human evaluation confirms that fine-tuned models not only fix targeted errors but also produce more natural, less translationese output.
- Fine-grained feedback shows unclear advantages over generic feedback in prompting settings but demonstrates clear benefits when models are fine-tuned.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting smaller LLMs with external error feedback improves translation quality beyond original MT outputs.
- Mechanism: The model leverages contextual information (source text, MT output, and feedback) to perform targeted edits, correcting identified errors while potentially improving fluency.
- Core assumption: LLMs possess sufficient reasoning and rewriting capabilities to interpret feedback and apply corrections even without extensive fine-tuning.
- Evidence anchors:
  - [abstract] "prompting LLMs to post-edit MT improves TER, BLEU and COMET scores"
  - [section 5] "we observe a marginal improvement when post-editing with any form of feedback in zero-shot settings"
- Break condition: Feedback is ambiguous, incorrect, or requires deep contextual understanding beyond the model's reasoning capacity.

### Mechanism 2
- Claim: Fine-tuning on error-annotated data enables models to more effectively exploit fine-grained feedback.
- Mechanism: Instruction fine-tuning aligns the model's behavior with feedback interpretation, teaching it to recognize and correct specific error types based on annotated examples.
- Core assumption: The fine-tuning dataset is representative and diverse enough to generalize across unseen errors.
- Evidence anchors:
  - [abstract] "Fine-tuning helps integrate fine-grained feedback more effectively and further improves translation quality"
  - [section 6.1] "fine-tuning using fine-grained feedback consistently yields superior translation quality compared to fine-tuning with generic feedback"
- Break condition: Overfitting to the fine-tuning data, domain mismatch between training and test data, or insufficient fine-tuning data diversity.

### Mechanism 3
- Claim: Smaller open-source LLMs can achieve post-editing performance comparable to larger proprietary models when properly guided.
- Mechanism: Instruction-following capabilities and few-shot learning compensate for parameter count differences, allowing smaller models to perform well with appropriate prompting and fine-tuning strategies.
- Core assumption: Model architecture and pre-training data quality are sufficient for the task, regardless of size.
- Evidence anchors:
  - [abstract] "We argue that it is also worth exploring to what extent LLMs of more moderate size (e.g., 7B, 13B) can perform post-editing"
  - [section 5] "This trend suggests that few-shot learning helps bridge the performance gap between model sizes for MT post-editing"
- Break condition: Task complexity exceeds the model's inherent reasoning capacity, or prompt templates are not optimized for smaller model characteristics.

## Foundational Learning

- Concept: Multidimensional Quality Metrics (MQM) framework
  - Why needed here: Provides the annotation schema for identifying and categorizing translation errors that serve as feedback for the LLM.
  - Quick check question: What are the three severity levels in MQM, and how do they differ from error types?

- Concept: Zero-shot and few-shot prompting
  - Why needed here: These techniques allow the model to perform post-editing without requiring extensive fine-tuning, making the approach more practical and scalable.
  - Quick check question: How does few-shot prompting differ from zero-shot prompting in terms of performance and resource requirements?

- Concept: Fine-tuning with instruction following
  - Why needed here: This technique teaches the model to better interpret and act on specific feedback, improving the effectiveness of fine-grained error annotations.
  - Quick check question: What is the difference between fine-tuning for a specific task versus instruction fine-tuning?

## Architecture Onboarding

- Component map:
  - Data pipeline: MQM dataset → error annotation extraction → instruction template generation
  - Model pipeline: LLaMA-2 base model → prompt generation → decoding with temperature=0 → evaluation
  - Training pipeline: QLoRA fine-tuning → validation monitoring → early stopping
  - Evaluation pipeline: Automatic metrics (BLEU, TER, COMET) + human evaluation

- Critical path: Error annotation → instruction template → model prompting/fine-tuning → evaluation metrics → analysis
- Design tradeoffs:
  - Model size vs. cost: 7B vs 13B models show diminishing returns with few-shot learning
  - Annotation quality vs. availability: Human MQM vs. automatic xCOMET/InstructScore
  - Prompt complexity vs. model capability: Fine-grained feedback may overwhelm smaller models

- Failure signatures:
  - Metrics plateau or degrade after fine-tuning (overfitting)
  - Human evaluation shows no improvement despite automatic metric gains (translationese issues)
  - Fine-grained feedback shows no advantage over generic feedback (model cannot exploit detail)

- First 3 experiments:
  1. Zero-shot prompting with generic feedback on a small subset (50 examples) to establish baseline capability
  2. Few-shot prompting (10 examples) with MQM feedback to test feedback utilization
  3. Fine-tuning on 1000 error-annotated examples with fine-grained feedback, then evaluating on held-out set

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following questions emerge from the discussion:

### Open Question 1
- Question: How does the performance of post-editing change when using different instruction tuning datasets beyond DEMETR and MQM?
- Basis in paper: The authors use DEMETR and MQM for instruction fine-tuning but acknowledge that "our work highlights the effectiveness of using external feedback to resolve errors in translations" while noting "the scarcity of high-quality feedback remains a significant challenge."
- Why unresolved: The paper only explores two specific datasets for fine-tuning. Other potential sources of error-annotated MT data could yield different results in terms of post-editing quality and model adaptation.
- What evidence would resolve it: Experiments comparing post-editing performance using various error-annotated datasets (e.g., different language pairs, domains, or annotation methodologies) would show whether the observed improvements are dataset-specific or generalizable.

### Open Question 2
- Question: What is the impact of fine-tuning on the model's ability to handle translations with multiple error spans per sentence?
- Basis in paper: The authors note that "the original MQM dataset only has one error span per sentence" and observe that "fine-tuning results overall show greater improvements, especially for TER, considering that the original MQM dataset only has one error span per sentence."
- Why unresolved: The current experiments use data where each sentence contains exactly one error. Real-world translations often contain multiple errors, and it's unclear how well the fine-tuned models would perform in such scenarios.
- What evidence would resolve it: Testing the fine-tuned models on translations containing multiple error spans per sentence and measuring improvements in automatic and human evaluation metrics would reveal whether the models can effectively handle more complex error patterns.

### Open Question 3
- Question: How does the model's post-editing performance vary across different error types when fine-tuned on multilingual versus bilingual data?
- Basis in paper: The authors compare bilingual and multilingual fine-tuning approaches, finding that "the multilingual approach mostly outperforms the bilingual one" and conduct an error analysis showing that "all of the errors are addressed in a balanced manner" without fine-tuning, but "fine-tuning best fixes the targeted error span."
- Why unresolved: While the paper shows that fine-tuning improves overall performance and addresses errors more effectively, it doesn't provide a detailed breakdown of how different error types (e.g., accuracy vs. fluency) are handled differently by bilingual versus multilingual fine-tuning.
- What evidence would resolve it: A detailed error analysis comparing the performance of bilingual and multilingual fine-tuned models on different error types (accuracy, fluency, terminology, etc.) would reveal whether certain error types benefit more from multilingual training or if the improvement is uniform across error categories.

## Limitations
- The study uses relatively small evaluation sets (1,000 instances per language pair) and single reference translations, which may not fully capture quality improvements or reveal edge cases.
- The paper lacks ablation studies on instruction template variations and doesn't analyze which specific error types benefit most from different feedback granularities.
- Automatic metrics show modest gains (1-2 BLEU points), suggesting potential ceiling effects or inherent limitations in the post-editing task when applied to already reasonable MT outputs.

## Confidence
- **High Confidence**: Smaller LLMs can improve MT quality through post-editing with external feedback. The consistent metric improvements across multiple language pairs and feedback types, combined with human evaluation support, provide strong evidence for this claim.
- **Medium Confidence**: Fine-tuning significantly enhances the ability to exploit fine-grained feedback. While the results show clear improvements, the exact mechanisms by which fine-tuning enables better feedback utilization remain somewhat underspecified, and the optimal fine-tuning duration and dataset size are not thoroughly explored.
- **Low Confidence**: Fine-grained feedback provides clear advantages over generic feedback in prompting settings. The paper itself notes this advantage is "unclear" in prompting, with human evaluation showing limited distinction, suggesting the benefit may be more pronounced in fine-tuned models rather than prompting scenarios.

## Next Checks
1. **Ablation on Feedback Granularity**: Run controlled experiments comparing generic vs. fine-grained feedback on identical error types to isolate whether the observed improvements stem from feedback detail level or from the fine-tuning process itself.

2. **Error Type Analysis**: Analyze which specific MQM error categories (e.g., lexical, syntactic, fluency) show the most improvement with fine-grained feedback to identify whether the approach is biased toward certain error types.

3. **Cross-Lingual Generalization Test**: Evaluate the fine-tuned models on a held-out language pair (e.g., train on En-De/En-Ru, test on En-Fr) to assess whether the feedback interpretation capabilities generalize beyond the training languages.