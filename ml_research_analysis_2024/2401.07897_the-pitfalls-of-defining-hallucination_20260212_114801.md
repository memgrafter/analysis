---
ver: rpa2
title: The Pitfalls of Defining Hallucination
arxiv_id: '2401.07897'
source_url: https://arxiv.org/abs/2401.07897
tags:
- output
- input
- text
- arxiv
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hallucination in text generation lacks a rigorous, unified definition,
  hindering reliable evaluation. This paper proposes a logical classification system
  based on classical entailment relations between structured input and generated output,
  distinguishing overgeneration, undergeneration, logical independence, and contradiction.
---

# The Pitfalls of Defining Hallucination

## Quick Facts
- arXiv ID: 2401.07897
- Source URL: https://arxiv.org/abs/2401.07897
- Reference count: 10
- Primary result: Hallucination in text generation lacks a rigorous, unified definition, hindering reliable evaluation

## Executive Summary
This paper addresses the lack of a rigorous, unified definition of hallucination in natural language generation. It proposes a logical classification system based on classical entailment relations between structured input and generated output, distinguishing overgeneration, undergeneration, logical independence, and contradiction. The framework provides a clearer theoretical foundation for analyzing and mitigating hallucination, though it faces limitations with vague text, pragmatic phenomena, and open-ended LLM use cases.

## Method Summary
The paper proposes a logical classification system for hallucination based on classical entailment relations between structured input and generated text. It defines four categories: overgeneration (output too strong), undergeneration (output too weak), logical independence (output unrelated to input), and contradiction (output negates input). The framework maps existing analyses of hallucination onto these categories and suggests extending the approach with belief-desire-intention logic for open-ended LLM applications where no structured input exists.

## Key Results
- Hallucination lacks a rigorous, unified definition, hindering reliable evaluation
- A logical classification based on entailment relations clarifies different types of hallucination
- The framework applies to data-text NLG but requires adaptation for open-ended LLM use cases
- Extending the framework with BDI logic may handle strategic omissions and half-truths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A logical classification based on entailment relations between input and output structures clarifies the different types of hallucination.
- Mechanism: The paper defines four categories (overgeneration, undergeneration, logical independence, contradiction) by examining whether the input logically implies the output, vice versa, neither, or whether they contradict. This classification maps directly onto classical entailment, making it amenable to automated NLI-based evaluation.
- Core assumption: Classical first-order logic can model the semantic relations between structured input and generated text, and NLI systems can approximate the entailment relation accurately.
- Evidence anchors:
  - [abstract]: "This paper proposes a logical classification system based on classical entailment relations between structured input and generated output..."
  - [section 3]: "Let us step back and ask what Logical Consequence... relations can exist between input and output, assuming a classical logic."
  - [corpus]: Weak, no corpus evidence directly supports this claim; relies on logical inference modeling.
- Break condition: If the generated text is too ambiguous or vague, or if NLI systems cannot capture the entailment accurately, the classification breaks down.

### Mechanism 2
- Claim: Distinguishing types of hallucination improves error identification and mitigation strategies.
- Mechanism: By separating intrinsic hallucination (contradiction) from extrinsic (logically independent but not contradictory), the framework allows targeted mitigation: one addresses factual contradictions, the other addresses unsupported elaborations.
- Core assumption: Different types of hallucination require different mitigation strategies, and clear classification enables that differentiation.
- Evidence anchors:
  - [section 2]: "Ji et al. (2023)...distinguished between intrinsic hallucination and extrinsic hallucination..."
  - [section 3]: "Splitting error type 3 in two...3a. (3) and input |= ¬ output...3b. (3) and input |= ¬ output."
  - [corpus]: No direct corpus evidence; inference from proposed classification structure.
- Break condition: If annotators or automated systems cannot reliably distinguish between these categories, the benefit of differentiation is lost.

### Mechanism 3
- Claim: Extending the entailment framework with belief-desire-intention (BDI) logic handles open-ended LLM outputs where no structured input exists.
- Mechanism: BDI logic allows modeling what the system believes, desires, and intends to communicate, thus capturing strategic omissions ("withholding information") and half-truths that classical entailment misses.
- Core assumption: In open-ended generation, the "input" is implicitly defined by the system's beliefs/desires, and BDI logic can formalize this relation.
- Evidence anchors:
  - [abstract]: "The framework applies directly to data-text NLG...but requires adaptation for open-ended LLM use cases...suggesting extensions via belief-desire-intention logics."
  - [section 5]: "BDI logics add to our analytical arsenal because they allow us to reason about the inferences that a hearer will draw from an utterance..."
  - [corpus]: Weak, speculative extension with no empirical validation yet.
- Break condition: If BDI formalization is too complex or unverifiable, the extension will not improve practical hallucination detection.

## Foundational Learning

- Concept: Classical entailment and first-order logic
  - Why needed here: The entire classification relies on determining whether input |= output, output |= input, neither, or contradiction, which are fundamental logical relations.
  - Quick check question: Given input A ∧ B and output A, does input |= output hold? (Yes)

- Concept: Natural Language Inference (NLI)
  - Why needed here: The paper proposes using NLI systems to approximate the entailment relation computationally for evaluation.
  - Quick check question: If NLI predicts contradiction between input and output, which category does that map to? (Category 3b: contradiction)

- Concept: Pragmatics and implicature
  - Why needed here: The paper notes that literal semantic analysis can misclassify metaphorical or contextually appropriate utterances; pragmatic reasoning is necessary for accurate classification.
  - Quick check question: If the input says "temperature > 220°C" and the output says "It's warm today," is this a hallucination? (No, but requires pragmatic interpretation)

## Architecture Onboarding

- Component map: Structured input → Generator → Generated text → Entailment analyzer (NLI) → Classification engine (logical categories) → Output label (over/under-generation, independence, contradiction) → Optional BDI layer for open-ended cases
- Critical path: Structured input → Generator → Generated text → Entailment analyzer → Classification → Evaluation; BDI extension adds: Generated text → BDI reasoning → Strategic omission/half-truth detection
- Design tradeoffs: Classical entailment is precise but limited to structured inputs; BDI extension handles open-ended generation but is more complex and harder to automate; NLI approximation is practical but can be error-prone
- Failure signatures: Ambiguous text leads to misclassification; NLI errors propagate into wrong categories; BDI logic is too speculative to operationalize reliably
- First 3 experiments:
  1. Apply NLI to a set of generated texts with known entailment relations and check classification accuracy
  2. Annotate a corpus of data-text NLG outputs with the four categories and compare human vs. NLI-based classification
  3. Test BDI-based classification on open-ended LLM outputs for strategic omissions and half-truths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed logical classification of hallucination be reliably implemented in real-world NLG systems using current NLP technology?
- Basis in paper: explicit - "This synthesis can underpin a computational metric... provided the 'follows from' relation can be computationally modeled using Natural Language Inference"
- Why unresolved: The paper notes that NLI systems currently struggle with ambiguity, vagueness, and cross-format inference, and that additional challenges exist for pragmatic reasoning and handling non-classical entailment scenarios.
- What evidence would resolve it: Empirical evaluation of the classification system applied to diverse NLG datasets using state-of-the-art NLI models, including error analysis of classification failures and performance metrics.

### Open Question 2
- Question: How can the logical framework be extended to handle open-ended LLM applications where input-output relationships are ill-defined?
- Basis in paper: explicit - "Deﬁning hallucination is even more problematic in open-ended LLM applications... because, in such applications, it would be difﬁcult to say what input... such an LLM is expressing"
- Why unresolved: The paper suggests BDI logics as a potential framework but notes that no existing NLI system could implement this, and acknowledges the inherent difficulty of assessing truth and informativeness in subjective domains.
- What evidence would resolve it: Development and evaluation of BDI-based hallucination detection methods applied to open-ended LLM outputs, with comparison to human judgments across diverse application domains.

### Open Question 3
- Question: What is the relationship between hallucination and pragmatic phenomena like implicature, presupposition, and metaphor in NLG?
- Basis in paper: explicit - "Pragmatic reasoning should always be applied... including phenomena such as ambiguity and vagueness... including the crucial distinction between what a sentence asserts and what an utterance of the sentence communicates"
- Why unresolved: The paper identifies pragmatic phenomena as important limitations but doesn't propose specific methods for integrating pragmatic analysis into the logical framework, noting that this requires understanding what an utterance communicates beyond literal meaning.
- What evidence would resolve it: Annotation studies comparing pragmatic-aware versus literal interpretations of NLG outputs, and development of computational models that can distinguish literal from pragmatic content.

## Limitations

- The framework struggles with vague or underspecified input, which is common in many NLG scenarios
- Handling pragmatic phenomena like implicature and metaphor remains a significant challenge
- The BDI extension for open-ended generation remains largely theoretical with no empirical validation
- The framework lacks direct empirical validation on real datasets, relying instead on logical analysis

## Confidence

- High confidence: The logical framework for classifying entailment relations (overgeneration, undergeneration, independence, contradiction) is well-defined and internally consistent
- Medium confidence: The mapping of existing hallucination analyses to the logical framework is reasonable, though some edge cases may not fit cleanly
- Low confidence: The BDI extension for open-ended generation and the practical utility of the framework for real-world evaluation without significant adaptation

## Next Checks

1. **NLI-based classification validation**: Apply the logical classification to a benchmark dataset of data-text NLG outputs, using state-of-the-art NLI systems to determine entailment relations, and measure classification accuracy against human annotations.

2. **Vagueness handling benchmark**: Create a test suite of input-output pairs with varying degrees of vagueness, and evaluate how well the logical classification handles these cases compared to human judgment, documenting specific failure modes.

3. **BDI framework prototyping**: Implement a prototype BDI-based reasoning system for open-ended generation, and test it on a small corpus of LLM outputs to assess whether it can identify strategic omissions and half-truths that classical entailment misses.