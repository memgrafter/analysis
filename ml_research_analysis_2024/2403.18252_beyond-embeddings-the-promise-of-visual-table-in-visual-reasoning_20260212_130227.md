---
ver: rpa2
title: 'Beyond Embeddings: The Promise of Visual Table in Visual Reasoning'
arxiv_id: '2403.18252'
source_url: https://arxiv.org/abs/2403.18252
tags:
- visual
- description
- object
- knowledge
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Visual Table, a novel form of visual representation
  tailored for visual reasoning. Visual tables are structured hierarchical text descriptions
  of visual scenes, including scene descriptions and object-centric descriptions covering
  categories, attributes, and knowledge.
---

# Beyond Embeddings: The Promise of Visual Table in Visual Reasoning

## Quick Facts
- arXiv ID: 2403.18252
- Source URL: https://arxiv.org/abs/2403.18252
- Authors: Yiwu Zhong; Zi-Yuan Hu; Michael R. Lyu; Liwei Wang
- Reference count: 40
- Primary result: Visual tables outperform CLIP embeddings across 11 visual reasoning benchmarks

## Executive Summary
This paper introduces Visual Table, a novel hierarchical text representation for visual reasoning that combines scene descriptions, object attributes, and instance-level knowledge in JSON format. Unlike traditional visual embeddings, visual tables are directly interpretable by both humans and large language models (LLMs). The authors collect a dataset of 61K visual table annotations using foundation models and train a generator model to produce these representations. Extensive experiments demonstrate that visual tables significantly outperform previous structural and text-based representations, enhancing state-of-the-art multimodal LLMs across diverse visual reasoning benchmarks.

## Method Summary
The method involves collecting visual table annotations on COCO images using foundation models (GPT-4V) with detailed prompts, then training a generator model based on LLaVA-1.5 architecture. The generator uses CLIP ViT-L/14@336px as the visual encoder, Vicuna-13B as the LLM, and a two-layer MLP connector. Training occurs in three stages: visual-language alignment, instruction fine-tuning, and supervised fine-tuning on the visual table annotations. The generated visual tables are then used as inputs to multimodal LLMs for visual reasoning tasks across 11 benchmarks including MM-Vet, LLaVA-Bench, and MMMU.

## Key Results
- Visual tables outperform CLIP embeddings on 11 diverse visual reasoning benchmarks