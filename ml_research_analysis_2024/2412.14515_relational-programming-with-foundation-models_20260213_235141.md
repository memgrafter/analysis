---
ver: rpa2
title: Relational Programming with Foundation Models
arxiv_id: '2412.14515'
source_url: https://arxiv.org/abs/2412.14515
tags:
- image
- query
- gpt-4
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Vieira, a declarative framework for programming
  with foundation models. The key idea is to treat foundation models as stateless
  functions with relational inputs and outputs, enabling their seamless integration
  with logic programs and supporting neuro-symbolic and multi-modal applications.
---

# Relational Programming with Foundation Models

## Quick Facts
- arXiv ID: 2412.14515
- Source URL: https://arxiv.org/abs/2412.14515
- Reference count: 20
- Primary result: Declarative framework achieving comparable or better accuracy than competitive baselines across 9 benchmark tasks

## Executive Summary
This paper introduces Vieira, a declarative framework for programming with foundation models by treating them as stateless functions with relational inputs and outputs. The framework enables seamless integration of foundation models with logic programs, supporting neuro-symbolic and multi-modal applications. Vieira is implemented by extending the Scallop compiler with a foreign interface that supports 12 foundation models including GPT, CLIP, and SAM.

## Method Summary
The Vieira framework extends the Scallop compiler to treat foundation models as foreign predicates (FPs) and foreign attributes (FAs) within a Datalog-based language. The framework uses probabilistic soft logic to handle uncertainty from foundation model outputs, computing similarity scores through cosine similarity for tensors. Foundation models are invoked through a plugin system, and results are processed through logical rules combined with the probabilistic reasoning layer. The approach is evaluated zero-shot or few-shot on 9 benchmark tasks spanning language, vision, and structured/vector databases.

## Key Results
- Vieira programs are concise and can incorporate modern foundation models
- Achieves comparable or better accuracy than competitive baselines across 9 benchmark tasks
- Demonstrates strong performance in date reasoning, kinship reasoning, math reasoning, question answering, compositional visual question answering, object tagging, and image generation/editing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Declarative relational programming unifies foundation models by treating them as stateless functions with relational inputs and outputs
- Mechanism: Foundation models are wrapped as foreign predicates (FPs) and foreign attributes (FAs) in Vieira's logic programming language, enabling composition with logical rules and structured data
- Core assumption: Foundation models can be treated as pure functions that map relational inputs to relational outputs
- Evidence anchors:
  - [abstract] "treats foundation models as stateless functions with relational inputs and outputs"
  - [section] "Our key insight is that foundation models are stateless functions with relational inputs and outputs"
  - [corpus] Weak - the corpus focuses on graph foundation models and knowledge graphs, not declarative frameworks for foundation models
- Break Condition: Foundation models require maintaining internal state across invocations or produce outputs that cannot be structured as relations

### Mechanism 2
- Claim: Probabilistic soft logic enables uncertainty handling and soft joins for foundation model outputs
- Mechanism: Vieira's probabilistic soft logic layer uses soft equality operators on tensors and provenance tracking to compute similarity scores and rank results
- Core assumption: Foundation model outputs can be meaningfully compared using similarity metrics like cosine similarity
- Evidence anchors:
  - [abstract] "soft-logic operations produce probabilities as well" and "soft-eq operator (˜=) on Tensors derives cosine-similarity"
  - [section] "Soft-logic operations produce probabilities as well. For instance, the soft-eq operator (˜=) on Tensors derives cosine-similarity between tensors"
  - [corpus] Weak - corpus discusses graph transformers and message passing but not probabilistic soft logic for foundation models
- Break Condition: Foundation model outputs are categorical or discrete in ways that cannot be meaningfully compared through similarity metrics

### Mechanism 3
- Claim: Foreign interface enables extensibility by allowing plugins for new foundation models
- Mechanism: Vieira extends the Scallop compiler with a plugin framework where new foundation models can be added as FPs and FAs
- Core assumption: New foundation models can be integrated by implementing the foreign predicate/attribute interface
- Evidence anchors:
  - [abstract] "We implement Vieira by extending the Scallop compiler with a foreign interface that supports foundation models as plugins"
  - [section] "We implement Vieira by extending the Scallop compiler with a foreign interface that supports foundation models as plugins"
  - [corpus] Weak - corpus discusses graph foundation models but not foreign interface extensibility
- Break Condition: New foundation models require fundamentally different interaction patterns than those currently supported

## Foundational Learning

- **Logic Programming**: Understanding Horn clauses, relations, and logical inference
  - Why needed here: Vieira's core language is based on Datalog and requires understanding how to write rules that combine foundation model outputs
  - Quick check question: What is the difference between a fact and a rule in Datalog?

- **Probabilistic Reasoning**: Understanding soft logic, probability distributions, and uncertainty quantification
  - Why needed here: Vieira uses probabilistic soft logic to handle uncertainty from foundation model outputs and perform soft joins
  - Quick check question: How does soft equality differ from hard equality in logic programming?

- **Foreign Function Interface**: Understanding how to wrap external systems as logic predicates
  - Why needed here: Foundation models are integrated through the foreign interface as FPs and FAs
  - Quick check question: What is the difference between a foreign predicate and a foreign attribute in Vieira?

## Architecture Onboarding

- **Component Map**:
  Vieira Language -> Foreign Interface Layer -> Foundation Model Plugins -> Probabilistic Soft Logic Engine -> Scallop Compiler Backend

- **Critical Path**:
  1. Parse Vieira program
  2. Translate foreign predicates/attributes to model invocations
  3. Execute logical rules with foundation model outputs
  4. Apply probabilistic reasoning
  5. Generate final results

- **Design Tradeoffs**:
  - **Extensibility vs Performance**: Plugin framework allows easy integration but may incur overhead from model invocations
  - **Declarative vs Imperative**: Declarative approach enables reasoning but may be less intuitive than imperative programming
  - **Probabilistic vs Deterministic**: Probabilistic reasoning handles uncertainty but may produce less precise results

- **Failure Signatures**:
  - **Model Invocation Failures**: Foreign predicates return errors or timeouts
  - **Logical Inconsistency**: Rules produce contradictory results
  - **Performance Degradation**: Large number of model invocations causes slow execution

- **First 3 Experiments**:
  1. Implement a simple GPT-based text completion example using @gpt attribute
  2. Create a CLIP-based image classification example using @clip attribute
  3. Combine both to build a question-answering system that uses GPT for reasoning and CLIP for image understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Vieira's performance scale when handling larger datasets or more complex reasoning tasks beyond the evaluated benchmarks?
- Basis in paper: [inferred] The paper evaluates Vieira on 9 benchmark tasks but doesn't explore performance on larger datasets or more complex reasoning tasks.
- Why unresolved: The current evaluation focuses on specific datasets with limited complexity. Scaling to real-world applications would require testing on larger, more diverse datasets.
- What evidence would resolve it: Systematic evaluation of Vieira on larger datasets, including stress tests with increasing data complexity and task difficulty.

### Open Question 2
- Question: What is the impact of different foundation model choices on Vieira's accuracy and efficiency?
- Basis in paper: [explicit] The paper mentions using 12 foundation models but doesn't provide a comprehensive comparison of their impact on different tasks.
- Why unresolved: While the paper shows that Vieira can incorporate various foundation models, it doesn't analyze how model selection affects performance or efficiency trade-offs.
- What evidence would resolve it: Systematic benchmarking of Vieira's performance using different combinations of foundation models for each task, measuring both accuracy and computational efficiency.

### Open Question 3
- Question: How does Vieira's probabilistic reasoning framework handle uncertainty in real-world applications with noisy or incomplete data?
- Basis in paper: [explicit] The paper mentions Vieira's probabilistic soft logic capabilities but doesn't explore its behavior with real-world uncertainty scenarios.
- Why unresolved: The evaluation uses clean benchmark datasets, but real-world applications often involve noisy or incomplete data where uncertainty management is crucial.
- What evidence would resolve it: Testing Vieira on datasets with varying levels of noise and incompleteness, measuring how well its probabilistic framework handles uncertainty compared to deterministic approaches.

## Limitations
- Framework's extensibility depends heavily on foreign interface implementation which may not generalize to all foundation model types
- Probabilistic soft logic assumes foundation model outputs can be meaningfully compared using similarity metrics, which may not hold for all applications
- Performance trade-offs between declarative and imperative approaches are not fully characterized, particularly for large-scale applications

## Confidence
- **High Confidence**: The core mechanism of treating foundation models as stateless functions with relational inputs and outputs is well-supported by the implementation and experimental results across 9 benchmark tasks
- **Medium Confidence**: The extensibility claims through the foreign interface are supported by the current implementation supporting 12 models, but the generalizability to arbitrary new models remains to be fully validated
- **Medium Confidence**: The probabilistic soft logic approach for handling uncertainty shows promise in the experimental results, but its effectiveness across diverse application domains needs broader validation

## Next Checks
1. **Cross-Model Generalization Test**: Implement and evaluate Vieira with at least 3 additional foundation models not mentioned in the original paper to verify the extensibility claims of the foreign interface

2. **Performance Benchmarking**: Conduct systematic performance comparison between Vieira and equivalent imperative implementations on computationally intensive tasks to quantify the overhead of the declarative approach

3. **Uncertainty Handling Validation**: Test Vieira's probabilistic reasoning capabilities on tasks with known uncertainty distributions to validate the effectiveness of soft logic operations in quantifying and handling model uncertainty