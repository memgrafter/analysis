---
ver: rpa2
title: Towards a More Complete Theory of Function Preserving Transforms
arxiv_id: '2410.11038'
source_url: https://arxiv.org/abs/2410.11038
tags:
- network
- function
- epochs
- more
- preserving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces R2R, a set of function-preserving transforms
  that extend residual connections to network morphism operations. The core method
  introduces two transforms: R2WiderR (for widening networks) and R2DeeperR (for deepening
  networks) that maintain network functionality while allowing architectural changes.'
---

# Towards a More Complete Theory of Function Preserving Transforms

## Quick Facts
- arXiv ID: 2410.11038
- Source URL: https://arxiv.org/abs/2410.11038
- Reference count: 40
- Primary result: Introduces R2R, function-preserving transforms extending residual connections to network morphism operations

## Executive Summary
This paper introduces R2R, a set of function-preserving transforms that extend residual connections to network morphism operations. The core method introduces two transforms: R2WiderR (for widening networks) and R2DeeperR (for deepening networks) that maintain network functionality while allowing architectural changes. Experiments on Cifar-10 show that R2R achieves competitive performance with Net2Net and Network Morphism, reaching validation accuracies around 69-72% across different transformation types.

## Method Summary
R2R uses symmetric parameter initialization with residual connections to ensure function preservation when expanding network width or depth. The method allows training of larger networks starting from pre-trained models without performance degradation. The approach adapts residual connections to account for reshaped volumes when applying R2WiderR operations, maintaining the original network's function while enabling architectural changes.

## Key Results
- R2R achieves competitive performance with Net2Net and Network Morphism on CIFAR-10
- Validation accuracies reach approximately 69-72% across different transformation types
- Demonstrates faster training convergence when used during training, reducing computational costs while maintaining similar final performance

## Why This Works (Mechanism)

### Mechanism 1
When widening a layer, R2WiderR pads the intermediate volume with two identical halves (xi_L and xi_R). Newly introduced convolution kernels are initialized such that their contributions cancel when computing the next layer's output. For the next layer, the kernel applied to the left half is the negative of the kernel applied to the right half. This works under the assumption that non-linearities maintain channel independence.

### Mechanism 2
R2DeeperR introduces an identity function by creating a residual block initialized to zero. The method uses zero initialization where two new layers are added such that their combined effect is zero. A residual connection then adds the input back, creating an identity function. This requires the non-linearity in the final layer of the added block to have a fixed point at zero (σ(0) = 0).

### Mechanism 3
R2R's compatibility with residual connections enables function preservation in architectures where previous FPTs couldn't be applied. When applying R2WiderR to a layer that's part of a residual connection, the method adapts the residual path to account for the widened volume. The residual connection is modified to include contributions from both new halves in a way that preserves the original function.

## Foundational Learning

- **Function-preserving transforms (FPTs)**: Operations that change network architecture while maintaining the same function representation. Needed to understand why R2R is valuable - it allows architectural changes without retraining from scratch. Quick check: If a network computes f(x) = 2x + 1, what must a function-preserving transform ensure about the new network's computation?

- **Residual connections**: Skip connections that add the input to the output of a layer or block. Needed because R2R specifically addresses incorporating residual connections into FPTs. Quick check: In a residual block where y = F(x) + x, if F is initialized to output zero, what function does the block compute?

- **Symmetric initialization**: Initializing parameters such that their combined effect cancels out. Needed because R2R relies on symmetric initialization to ensure widened components cancel out, preserving the original function. Quick check: If two layers are initialized such that their combined output is always zero, and a residual connection adds the input back, what function does this compute?

## Architecture Onboarding

- **Component map**: R2WiderR -> Parameter Initialization -> Residual Adaptation -> Function Preservation Check -> Training; R2DeeperR -> Zero Initialization -> Residual Connection Handler -> Training
- **Critical path**: Widen/Deepen → Parameter Initialization → Residual Adaptation → Function Preservation Check → Training
- **Design tradeoffs**: Symmetric vs. asymmetric initialization (symmetric ensures function preservation but may limit learning diversity); Zero initialization scale (too large causes instability; too small may not break symmetry effectively); Residual connection complexity (simple identity vs. parameterized reshaping functions)
- **Failure signatures**: Function not preserved (check symmetry breaking in initialization, non-linearity compatibility); Training instability (check initialization scale relative to existing weights); Performance degradation (check residual connection adaptation, symmetry breaking effectiveness)
- **First 3 experiments**: 1) Basic function preservation test: Apply R2WiderR to a simple linear network and verify outputs match before/after; 2) Residual compatibility test: Apply R2WiderR to a network with residual connections and verify function preservation; 3) Training stability test: Apply R2R with varying initialization scales and observe training behavior

## Open Questions the Paper Calls Out

### Open Question 1
Can the symmetry breaking property of R2WiderR be theoretically proven to occur after a specific number of training iterations under certain data conditions? The authors had preliminary mathematical reasoning but did not complete the formal proof or specify exact conditions under which symmetry breaking occurs.

### Open Question 2
How do the hyper-parameter interactions (learning rate, weight decay, initialization scale, optimizer choice) affect the stability and performance of FPT operations across different network architectures? The experiments showed hyper-parameter tuning was necessary, but no systematic study of these interactions was conducted.

### Open Question 3
Can learned initializations for FPT operations outperform the symmetric initialization methods used in R2R and Net2Net? The paper uses heuristic symmetric initializations but does not explore whether meta-learned initializations could provide better performance or faster convergence.

## Limitations

- Theoretical foundations rely heavily on specific non-linearity properties that may not hold across all network architectures
- Symmetric initialization may introduce constraints on the network's ability to learn diverse representations during training
- Experiments are limited to CIFAR-10, a relatively simple dataset, which may not fully capture the method's behavior on more complex tasks

## Confidence

- **High Confidence**: The core mechanism of symmetric initialization for function preservation is well-established and theoretically sound
- **Medium Confidence**: The experimental results show competitive performance, but the limited dataset scope reduces generalizability confidence
- **Low Confidence**: The long-term stability of the symmetric initialization during extended training remains unclear

## Next Checks

1. **Extended Dataset Testing**: Validate R2R on more complex datasets (e.g., ImageNet, COCO) to assess scalability and robustness beyond CIFAR-10
2. **Training Dynamics Analysis**: Monitor the evolution of symmetry in initialized parameters during extended training to identify potential breaking points
3. **Architecture Diversity Testing**: Apply R2R to various network architectures (e.g., Transformers, CNNs with different depths) to evaluate its versatility and limitations across different model types