---
ver: rpa2
title: Solving Multi-Model MDPs by Coordinate Ascent and Dynamic Programming
arxiv_id: '2407.06329'
source_url: https://arxiv.org/abs/2407.06329
tags:
- policy
- cadp
- policies
- algorithm
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CADP, an algorithm for solving Multi-Model
  Markov Decision Processes (MMDPs) that combines coordinate ascent with dynamic programming.
  CADP improves upon existing methods by using adjustable model weights to guarantee
  monotone policy improvements and converge to a local maximum.
---

# Solving Multi-Model MDPs by Coordinate Ascent and Dynamic Programming

## Quick Facts
- arXiv ID: 2407.06329
- Source URL: https://arxiv.org/abs/2407.06329
- Reference count: 12
- The paper introduces CADP, an algorithm for solving Multi-Model Markov Decision Processes (MMDPs) that combines coordinate ascent with dynamic programming. CADP improves upon existing methods by using adjustable model weights to guarantee monotone policy improvements and converge to a local maximum.

## Executive Summary
This paper addresses the challenge of solving Multi-Model Markov Decision Processes (MMDPs), where the true MDP model is unknown and drawn from a set of possible models. The authors introduce CADP (Coordinate Ascent Dynamic Programming), an algorithm that iteratively adjusts model weights and applies dynamic programming to find better policies. CADP guarantees monotone policy improvements by conditioning model weights on the current state and time, ensuring convergence to a local maximum. Theoretical analysis proves that CADP never performs worse than prior algorithms like WSU, while empirical results show significant performance improvements across multiple benchmark problems.

## Method Summary
CADP solves MMDPs by combining coordinate ascent with dynamic programming. The algorithm iteratively adjusts model weights to reflect the posterior probability of each model given the current state and time, then applies dynamic programming to optimize the policy for each state sequentially. Unlike previous methods that use static prior weights, CADP's dynamic weights ensure monotone policy improvements by leveraging the Markov property. The method maintains computational efficiency by reusing value functions from previous iterations rather than recomputing from scratch. Theoretical analysis proves that CADP converges to a local maximum with finite termination, while empirical results demonstrate superior performance compared to existing methods across multiple benchmark domains.

## Key Results
- CADP guarantees monotone policy improvements by dynamically adjusting model weights based on current state and time
- Theoretical analysis proves CADP never performs worse than prior algorithms like WSU and converges to a local maximum
- Empirical results show CADP significantly outperforms existing methods on benchmark problems, achieving the best or near-best returns while considering Markov policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adjustable model weights ensure monotone policy improvements by conditioning probabilities on the current state and time.
- Mechanism: Instead of using static prior weights λ, CADP computes dynamic weights bπ_t,m(s) that reflect the posterior probability of model m given the agent is in state s at time t under policy π. These weights are updated using (10), leveraging the Markov property to avoid history dependence.
- Core assumption: The posterior distribution over models given the current state can be accurately computed via (10) without needing full belief tracking.
- Evidence anchors:
  - [abstract]: "The main innovation of CADP compared with earlier algorithms is to take the coordinate ascent perspective to adjust model weights iteratively to guarantee monotone policy improvements to a local maximum."
  - [section 4.1]: Definition 4.1 and update rule (10) for model weights.
- Break condition: If the model space M is continuous or extremely large, exact computation of bπ_t,m(s) becomes intractable, breaking the monotone improvement guarantee.

### Mechanism 2
- Claim: Combining coordinate ascent with dynamic programming avoids the computational explosion of pure coordinate ascent while preserving the linearity of the objective in each coordinate.
- Mechanism: CADP iteratively optimizes each policy component π_t by solving a linear problem over actions, reusing value functions qπ_t,m from the previous iteration rather than recomputing from scratch. This is implemented in Algorithm 1.
- Core assumption: The linearity of ρ(π) in each π_t (Corollary 4.2) allows exact one-dimensional optimization without sacrificing global structure.
- Evidence anchors:
  - [section 4.3]: "While the coordinate ascent scheme outlined above is simple and theoretically appealing, it is computationally inefficient. The computational inefficiency arises because computing the weights b and value functions q necessary in (13) requires one to update the entire dynamic program."
  - [section 4.2]: Proof of linearity in Corollary 4.2.
- Break condition: If the policy space is continuous or very high-dimensional, the linear subproblem may still be expensive, and CADP's iteration count could become prohibitive.

### Mechanism 3
- Claim: The non-decreasing return property ensures finite termination and convergence to a local maximum.
- Mechanism: Theorem 5.1 shows that each CADP iteration produces a policy π^n with ρ(π^n) ≥ ρ(π^{n-1}). Since the policy space is finite, this monotone sequence must terminate at a fixed point where no single-state improvement is possible.
- Core assumption: The policy improvement in each coordinate is sufficient to guarantee overall return improvement; simultaneous multi-state improvements are not needed for convergence.
- Evidence anchors:
  - [section 5]: Theorem 5.1 and Corollary 5.2 establishing finite termination.
  - [section 5]: "With a finite number of policies, the algorithm must eventually terminate."
- Break condition: If the policy improvement step size is too small or if floating-point precision causes ties, the algorithm might loop indefinitely in practice despite theoretical guarantees.

## Foundational Learning

- Concept: Multi-model MDPs (MMDPs) and their relationship to robust and soft-robust MDPs.
  - Why needed here: CADP solves MMDPs; understanding the model class is essential to grasp why adjustable weights and coordinate ascent are necessary.
  - Quick check question: What distinguishes an MMDP from a standard MDP in terms of the objective and policy class?

- Concept: Policy gradient and its linearity in the context of MMDPs.
  - Why needed here: CADP leverages the policy gradient theorem to derive the update rule (13); understanding this links the algorithm to broader RL theory.
  - Quick check question: How does the policy gradient in MMDPs differ from that in standard MDPs in terms of the gradient expression?

- Concept: Coordinate ascent and its application to non-convex optimization.
  - Why needed here: CADP is framed as coordinate ascent over the policy vector; knowing how coordinate ascent works explains why CADP converges and how it differs from full gradient methods.
  - Quick check question: Why does coordinate ascent converge even when the objective is non-convex?

## Architecture Onboarding

- Component map: CADP consists of two main loops: (1) the outer loop (Algorithm 2) that iterates until convergence, and (2) the inner loop (Algorithm 1) that performs a backward dynamic programming sweep to optimize the current policy coordinate. Supporting components include the weight computation (10) and the value function update (4).

- Critical path: The bottleneck is the weight update (10) and value function update (4) in each iteration; these dominate runtime. The outer loop terminates when ρ(π^n) = ρ(π^{n-1}).

- Design tradeoffs: CADP trades increased per-iteration computation for guaranteed monotone improvement and convergence to a local maximum. This is preferable to methods that may cycle or require many more iterations (e.g., pure coordinate ascent).

- Failure signatures: Slow convergence or cycling may indicate numerical instability in weight updates, ill-conditioned model distributions, or insufficient precision in value function updates.

- First 3 experiments:
  1. Run CADP on a tiny MMDP (e.g., 2 states, 2 actions, 2 models) and verify that ρ increases each iteration and terminates in finite steps.
  2. Compare the runtime and return of CADP vs. WSU on a medium-size MMDP (e.g., Riverswim) to confirm the theoretical improvement claim.
  3. Test CADP with a randomized initial policy and with WSU-initialized policy to confirm that initialization does not affect the final return.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas warrant further investigation based on the analysis and results presented.

## Limitations

- Theoretical guarantees rely on finite policy spaces and tractable posterior weight computation, which may not hold for large or continuous problems
- Performance is sensitive to initialization, though the monotone improvement property mitigates this concern
- Computational efficiency claims are based on moderate-sized problems; scaling to very large MMDPs remains untested

## Confidence

- **High confidence**: The monotone improvement property and finite termination guarantee (Theorem 5.1) - these are rigorously proven with clear assumptions.
- **Medium confidence**: Empirical performance claims - while results show consistent improvement over baselines, the comparison is limited to specific benchmark problems and may not generalize to all MMDP settings.
- **Medium confidence**: Computational efficiency claims - the paper argues CADP avoids pure coordinate ascent's computational explosion, but actual runtime scaling with problem size is not thoroughly characterized.

## Next Checks

1. **Scalability test**: Implement CADP on incrementally larger MMDPs (doubling states/actions/models) and measure both return improvement and runtime scaling to empirically validate computational efficiency claims.

2. **Initialization robustness**: Systematically test CADP with diverse initial policies (random, WSU, MVP, uniform) across multiple random seeds to quantify sensitivity and verify the monotone improvement property holds regardless of starting point.

3. **Model space stress test**: Create MMDPs with continuous or extremely large model spaces to evaluate whether the weight update equation (10) remains tractable and whether the monotone improvement property breaks down when exact posterior computation is infeasible.