---
ver: rpa2
title: 'Less is More: Selective Reduction of CT Data for Self-Supervised Pre-Training
  of Deep Learning Models with Contrastive Learning Improves Downstream Classification
  Performance'
arxiv_id: '2410.14524'
source_url: https://arxiv.org/abs/2410.14524
tags:
- pre-training
- dataset
- learning
- downstream
- slices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of using all CT slices for contrastive
  pre-training in medical imaging, which can lead to performance degradation due to
  high slice similarity. The authors propose selective reduction of CT datasets to
  improve contrastive pre-training.
---

# Less is More: Selective Reduction of CT Data for Self-Supervised Pre-Training of Deep Learning Models with Contrastive Learning Improves Downstream Classification Performance

## Quick Facts
- arXiv ID: 2410.14524
- Source URL: https://arxiv.org/abs/2410.14524
- Reference count: 34
- Primary result: Reducing CT datasets to 10% using HASH method improves contrastive pre-training AUC scores from 0.775 to 0.830 for COVID-19, 0.968 to 0.978 for OrganSMNIST, and 0.727 to 0.831 for Brain classification

## Executive Summary
This study addresses the challenge of using all CT slices for contrastive pre-training, which can degrade performance due to high slice similarity within CT volumes. The authors propose selective reduction of CT datasets using four similarity-based approaches (SSIM, MI, DeepNet, HASH) compared to two baseline methods. Using SwAV contrastive learning on PET-CT and LIDC pre-training datasets, they evaluate performance on three downstream classification tasks. Results show that reducing datasets to 10% using the HASH method with Hamming distance threshold 6 leads to significant performance gains while reducing pre-training time by up to 9x. The findings generalize to other contrastive methods but not to masked autoencoders.

## Method Summary
The method involves selectively reducing CT datasets before contrastive pre-training by removing highly similar slices within each volume. The authors compare six reduction strategies: two baseline methods (ALL using all slices, EveryN taking every Nth slice) and four similarity-based approaches (SSIM, MI, DeepNet, HASH). The HASH method computes 64-bit hashes from compressed images and removes slices with Hamming distance below threshold 6. Pre-trained models using SwAV contrastive learning are then fine-tuned on downstream classification tasks (COVID-19, OrganSMNIST, Brain hemorrhage) with supervised learning for 10 epochs followed by full fine-tuning.

## Key Results
- Reducing datasets to 10% using HASH-6 method improves downstream classification performance across all tasks
- COVID-19 classification AUC improves from 0.775 to 0.830
- OrganSMNIST multi-class accuracy improves from 0.968 to 0.978
- Brain hemorrhage classification AUC improves from 0.727 to 0.831
- Pre-training time is reduced by up to 9x (538 h to 62 h on PET-CT, 280 h to 27 h on LIDC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing highly similar CT slices improves contrastive learning by reducing false negative pairs.
- Mechanism: CT slices within the same volume are very similar. When all slices are used, two augmented views of one slice may be less similar than two different slices, confusing the model's positive/negative pair discrimination.
- Core assumption: The similarity between two different CT slices can exceed the similarity between two augmented views of the same slice.
- Evidence anchors: [abstract]: "We hypothesize that the similarity of medical images hinders the success of contrastive learning in the medical imaging domain." [section 1]: "We derive our hypothesis from the fact that CT datasets have very low variance compared to natural image datasets due to the high similarity of the CT slices."

### Mechanism 2
- Claim: HASH-based reduction finds slices that maximize intra-volume diversity while maintaining sufficient sample count.
- Mechanism: HASH computes 64-bit hashes from compressed images and removes slices with Hamming distance below threshold 6, ensuring remaining slices are sufficiently dissimilar within each volume.
- Core assumption: Hamming distance on compressed 9Ã—8 images correlates with perceptual similarity relevant for contrastive learning.
- Evidence anchors: [section 2.1]: "The HASH similarity is based on the comparison of hash values derived from each image... The Hamming distance between the two hashes of images x and y measures the similarity." [section 3.2]: "We found that the HASH approach with a Hamming distance threshold of six (HASH-6) performs best."

### Mechanism 3
- Claim: Reduced datasets speed up contrastive pre-training without sacrificing downstream performance.
- Mechanism: Fewer slices mean fewer positive/negative pairs to compute during pre-training, reducing computational cost while the selected diverse slices maintain learning quality.
- Core assumption: Computational savings from fewer slices directly translate to faster pre-training with maintained or improved downstream results.
- Evidence anchors: [section 3.4]: "The pre-training time is reduced from 538 h to 62 h and from 280 h to 27 h on the PET-CT and LIDC datasets, respectively." [section 3.1]: "Pre-training is up to nine times faster due to the dataset reduction."

## Foundational Learning

- Concept: Contrastive learning objective and positive/negative pair discrimination
  - Why needed here: The entire paper builds on understanding why slice similarity affects contrastive learning performance
  - Quick check question: In contrastive learning, what distinguishes a positive pair from a negative pair, and why does slice similarity matter for this distinction?

- Concept: Mutual information and structural similarity metrics for image comparison
  - Why needed here: The paper evaluates multiple similarity measures (SSIM, MI, DeepNet, HASH) for dataset reduction
  - Quick check question: How does structural similarity index (SSIM) differ from mutual information (MI) in measuring image similarity?

- Concept: Transfer learning and downstream task evaluation
  - Why needed here: The paper's main contribution is improving downstream classification performance through better pre-training
  - Quick check question: Why are classification tasks commonly used as benchmarks for evaluating self-supervised pre-training methods?

## Architecture Onboarding

- Component map: Load CT volumes -> extract slices -> apply similarity-based reduction -> feed to SwAV/MoCo/SparK -> ResNet50 encoder -> contrastive learning head (SwAV clustering, MoCo dictionary, or SparK reconstruction) -> fine-tune on downstream classification tasks -> compute AUC/F1 scores

- Critical path: 1. Implement HASH similarity calculation and dataset reduction 2. Set up SwAV contrastive pre-training with reduced datasets 3. Fine-tune pre-trained models on downstream classification tasks 4. Evaluate and compare against baseline ALL method

- Design tradeoffs:
  - Dataset reduction vs. information loss: Reducing to 10% improves performance but may lose rare but important cases
  - Similarity threshold selection: Hamming distance 6 works best but may need adjustment for different CT protocols
  - Computational cost vs. performance: Faster pre-training vs. potential generalization issues from reduced diversity

- Failure signatures:
  - No performance improvement after reduction: Indicates slice similarity wasn't the limiting factor
  - Degraded performance after reduction: Suggests too much information was removed or wrong slices were selected
  - Unstable training after reduction: May indicate insufficient positive pairs or batch size issues

- First 3 experiments:
  1. Compare ALL vs. EveryN (10% reduction) on SwAV with PET-CT -> verify basic hypothesis
  2. Implement HASH similarity and test different thresholds (3, 6, 12) on SwAV -> find optimal threshold
  3. Test MoCo with HASH-6 reduction vs. ALL on LIDC -> validate generalizability to other contrastive methods

## Open Questions the Paper Calls Out

- How do different self-supervised pre-training methods perform on CT segmentation tasks when using the HASH-6 dataset reduction approach?
- What is the optimal mask ratio for the SparK masked autoencoder method when applied to CT images?
- How does the optimal similarity threshold for dataset reduction vary across different CT datasets with varying slice thickness and anatomical variation?

## Limitations
- Generalizability limited to CT imaging with specific slice similarity characteristics
- Dataset representativeness may not capture full diversity of clinical CT imaging
- Optimal Hamming distance threshold may require adjustment for different CT scanners and protocols

## Confidence
- High confidence: The core finding that selective reduction improves contrastive pre-training performance is well-supported by experimental results
- Medium confidence: The generalizability to other contrastive learning methods (MoCo) is demonstrated, but failure to generalize to masked autoencoders suggests method-specific limitations
- Medium confidence: Computational speedup claims are supported but implementation details affecting actual performance are not fully specified

## Next Checks
1. Test the selective reduction approach on non-CT medical imaging modalities (MRI, X-ray) to assess generalizability beyond CT-specific slice similarity patterns
2. Evaluate the approach on CT datasets from multiple clinical centers with different scanners and protocols to assess robustness to imaging variability
3. Develop and validate an adaptive method for determining optimal reduction thresholds based on dataset characteristics rather than using fixed Hamming distance values