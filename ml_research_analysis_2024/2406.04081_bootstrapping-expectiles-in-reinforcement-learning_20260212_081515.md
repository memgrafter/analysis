---
ver: rpa2
title: Bootstrapping Expectiles in Reinforcement Learning
arxiv_id: '2406.04081'
source_url: https://arxiv.org/abs/2406.04081
tags:
- robust
- expectile
- learning
- algorithm
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ExpectRL, a method that replaces the classic
  L2 loss with an expectile loss in reinforcement learning algorithms, particularly
  in the context of twin critic approaches like TD3. The core idea is to introduce
  pessimism by bootstrapping expectiles instead of expectations, which helps control
  overestimation bias and improve robustness.
---

# Bootstrapping Expectiles in Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.04081
- Source URL: https://arxiv.org/abs/2406.04081
- Authors: Pierre Clavier; Emmanuel Rachelson; Erwan Le Pennec; Matthieu Geist
- Reference count: 40
- Key outcome: ExpectRL outperforms classic TD3 with twin critics in Mujoco environments, particularly reducing variance and improving worst-case performance

## Executive Summary
This paper introduces ExpectRL, a method that replaces the classic L2 loss with an expectile loss in reinforcement learning algorithms, particularly in the context of twin critic approaches like TD3. The core idea is to introduce pessimism by bootstrapping expectiles instead of expectations, which helps control overestimation bias and improve robustness. The method is shown to outperform classic TD3 with twin critics in Mujoco environments, particularly in terms of reducing variance and improving worst-case performance. ExpectRL is also competitive with state-of-the-art robust RL algorithms when combined with domain randomization. Additionally, an automatic mechanism for selecting the expectile value (AutoExpectRL) is proposed, further enhancing the method's effectiveness.

## Method Summary
ExpectRL modifies the critic training objective in TD3 by replacing the L2 loss with an expectile loss, effectively bootstrapping a pessimistic estimate of the value function. The method uses multiple expectile parameters (α values) and a bandit algorithm to automatically select the optimal pessimism level. When combined with domain randomization, ExpectRL samples from the full uncertainty set while maintaining pessimistic value estimates, providing robustness to environmental variations. The approach maintains the twin critic architecture of TD3 but with expectile-specific heads for different α values.

## Key Results
- ExpectRL outperforms classic TD3 with twin critics in Mujoco environments, particularly reducing variance and improving worst-case performance
- The method is competitive with state-of-the-art robust RL algorithms when combined with domain randomization
- AutoExpectRL, the automatic expectile selection mechanism, further enhances performance by adapting the degree of pessimism to environment-specific uncertainty characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing L2 loss with expectile loss in TD3's critic training controls overestimation bias by bootstrapping a pessimistic estimate of the value function.
- Mechanism: The expectile loss with α < 0.5 weights underestimation more heavily than overestimation, effectively computing a lower quantile of the next-state value distribution instead of the mean. This creates a pessimistic Bellman backup that mitigates the positive bias inherent in standard Q-learning.
- Core assumption: The next-state value distribution is skewed such that lower quantiles better approximate the true expected return under function approximation error.
- Evidence anchors:
  - [abstract] "By replacing the L2 loss with a more general expectile loss for the critic" and "introduces pessimism by bootstrapping expectiles instead of expectations"
  - [section] "ExpectRL is equivalent to bootstrapping the expectile and not the expectation of the Q value"
  - [corpus] Weak support; no direct citations found, but related to "Mitigating Preference Hacking in Policy Optimization with Pessimism" and "Moderate Actor-Critic Methods: Controlling Overestimation Bias via Expectile Loss"
- Break condition: If the Q-function approximation error is symmetric or underestimates true values, expectile bootstrapping may introduce harmful pessimism leading to suboptimal policies.

### Mechanism 2
- Claim: Auto-tuning the expectile parameter α using a bandit algorithm adapts the degree of pessimism to environment-specific uncertainty characteristics.
- Mechanism: Each bandit arm corresponds to a different α value. The algorithm tracks performance improvements across episodes and adjusts selection probabilities, effectively performing model selection without retraining separate networks. This allows dynamic adjustment between exploration and exploitation based on observed reward signals.
- Core assumption: Different environments require different levels of pessimism for optimal performance, and reward signals provide sufficient information to distinguish effective α values.
- Evidence anchors:
  - [section] "We employ a bandit algorithm, specifically the Exponentially Weighted Average Forecasting algorithm" and "This approach can be seen as a form of model selection"
  - [corpus] Weak support; "Model-based Offline Reinforcement Learning with Lower Expectile Q-Learning" suggests expectile methods are effective but doesn't confirm bandit-based auto-tuning
- Break condition: If environment uncertainty is uniform across tasks or reward signals are too noisy to distinguish effective α values, the bandit may converge to suboptimal parameters.

### Mechanism 3
- Claim: Combining expectile bootstrapping with domain randomization (DR) provides robustness to environmental variations by sampling from the full uncertainty set while maintaining pessimistic value estimates.
- Mechanism: DR samples trajectories from multiple environments in the uncertainty set, exposing the agent to diverse scenarios. Expectile bootstrapping ensures value estimates remain conservative across this variability, leading to policies that perform well on worst-case environments rather than just the nominal one.
- Core assumption: The uncertainty set adequately represents the range of environmental variations the agent will encounter during deployment, and pessimistic estimates across this set improve worst-case performance.
- Evidence anchors:
  - [section] "By extending expectile bootstrapping (ExpectRL) with sampling from the entire uncertainty set using domain randomization (DR), our approach bolsters robustness"
  - [corpus] Weak support; "Federated Reinforcement Learning in Heterogeneous Environments" suggests multi-environment training helps but doesn't confirm expectile-DRT combination
- Break condition: If the uncertainty set is too large or poorly specified, the agent may become overly conservative, sacrificing average performance for robustness to scenarios that never occur.

## Foundational Learning

- Concept: Bellman operator and its contraction property
  - Why needed here: ExpectRL relies on the expectile Bellman operator being a contraction mapping, which guarantees convergence to a fixed point
  - Quick check question: What mathematical property ensures that iterative application of the Bellman operator converges to the optimal value function?

- Concept: Risk measures and coherence properties
  - Why needed here: Expectiles are coherent risk measures, which is crucial for the theoretical connection between expectile bootstrapping and robust RL
  - Quick check question: What are the four properties that define a coherent risk measure, and how do expectiles satisfy them?

- Concept: Function approximation error and overestimation bias
  - Why needed here: The paper addresses overestimation bias inherent in Q-learning with neural network function approximation
  - Quick check question: Why does temporal difference learning with function approximation tend to overestimate action values, and how does this affect policy optimization?

## Architecture Onboarding

- Component map: Environment -> Actor network with shared body and four α-specific heads -> Critic network with four expectile-specific heads -> Bandit algorithm for α selection -> Replay buffer -> Target networks
- Critical path: Environment interaction → Store transition → Sample batch → Update critics (four expectile losses) → Update actor (using selected α) → Update target networks → Bandit update
- Design tradeoffs: Using four separate heads allows different pessimism levels per α but increases memory usage; using a single network with shared parameters reduces parameters but may limit expressivity
- Failure signatures: If one α head consistently underperforms, it may indicate environment mismatch; if all heads underperform similarly, the architecture or learning rates may be incorrect
- First 3 experiments:
  1. Implement basic TD3 with one critic and L2 loss as baseline
  2. Add four expectile heads to critic, test with fixed α values to identify best performer
  3. Add bandit algorithm to auto-select α, verify it converges to previously identified best α value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the expectile bootstrapping method be theoretically extended to risk-seeking RL scenarios where α > 1/2, and what would be the implications for convergence and stability?
- Basis in paper: [explicit] The paper mentions that for α > 1/2, the Expectile Bellman operator is not a contraction, and there are no theoretical convergence guarantees for risk-seeking RL.
- Why unresolved: The paper only focuses on risk-averse RL scenarios (α < 1/2) and does not explore the potential of expectile bootstrapping in risk-seeking scenarios. The theoretical analysis of the Expectile Bellman operator for α > 1/2 is left open.
- What evidence would resolve it: A theoretical analysis proving or disproving the convergence and stability of the Expectile Bellman operator for α > 1/2 would resolve this question. Empirical results demonstrating the performance of expectile bootstrapping in risk-seeking RL scenarios would also provide valuable insights.

### Open Question 2
- Question: How does the AutoExpectRL algorithm perform in combination with domain randomization (DR) when uncertainty parameters change between trajectories, and what modifications would be necessary to adapt the bandit algorithm for this setting?
- Basis in paper: [explicit] The paper mentions that AutoExpectRL cannot be used with DR as uncertainty parameters change between trajectories, making it difficult for the algorithm to distinguish between the effects of different expectile values and the effects of changing uncertainty parameters.
- Why unresolved: The paper does not explore the potential of AutoExpectRL in combination with DR, leaving the question of its performance and necessary modifications unanswered.
- What evidence would resolve it: Empirical results comparing the performance of AutoExpectRL with and without DR would provide insights into the effectiveness of the algorithm in the presence of changing uncertainty parameters. Theoretical analysis of the bandit algorithm's behavior in the context of DR would also be valuable.

### Open Question 3
- Question: How does the choice of the uncertainty set in the expectile formulation affect the performance of ExpectRL, and can alternative uncertainty sets be designed to provide more interpretable and effective robustness guarantees?
- Basis in paper: [explicit] The paper mentions that the uncertainty set defined by expectiles is not very interpretable, and other algorithms with a more interpretable set and similar experimental results would be interesting.
- Why unresolved: The paper does not explore alternative uncertainty sets or their impact on the performance of ExpectRL, leaving the question of how to design effective and interpretable uncertainty sets unanswered.
- What evidence would resolve it: Empirical results comparing the performance of ExpectRL with different uncertainty sets would provide insights into the impact of the choice of uncertainty set on robustness and performance. Theoretical analysis of alternative uncertainty sets and their properties would also be valuable.

## Limitations

- The method requires careful hyperparameter tuning of the expectile parameter α, which the bandit approach addresses but doesn't eliminate
- Performance improvements are primarily demonstrated in continuous control environments; discrete action spaces may require different considerations
- The computational overhead of maintaining multiple expectile heads and the bandit algorithm could be prohibitive for resource-constrained applications

## Confidence

- Mechanism 1: High confidence - well-established connection between expectiles and risk-sensitive learning
- Mechanism 2: Medium confidence - empirical approach with limited theoretical analysis
- Mechanism 3: Medium confidence - combination with DR shows promise but lacks ablation studies
- Overall: Medium-High confidence in core expectile bootstrapping mechanism, Medium confidence in AutoExpectRL and DR combination

## Next Checks

1. Conduct ablation studies comparing AutoExpectRL against a grid of fixed α values to quantify the benefit of automatic selection
2. Test the method on environments with different reward distributions (sparse rewards, multimodal rewards) to verify robustness claims
3. Analyze the variance reduction empirically by comparing the distribution of returns between TD3 and ExpectRL across multiple seeds