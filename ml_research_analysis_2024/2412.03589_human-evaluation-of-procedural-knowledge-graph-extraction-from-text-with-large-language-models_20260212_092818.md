---
ver: rpa2
title: Human Evaluation of Procedural Knowledge Graph Extraction from Text with Large
  Language Models
arxiv_id: '2412.03589'
source_url: https://arxiv.org/abs/2412.03589
tags:
- knowledge
- human
- evaluation
- procedural
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes using large language models (LLMs) to extract
  procedural knowledge from text and populate a knowledge graph (KG) based on a predefined
  ontology. An iterative prompt engineering approach, utilizing chain-of-thought prompting,
  is employed to extract steps, actions, objects, equipment, and temporal information
  from procedural text.
---

# Human Evaluation of Procedural Knowledge Graph Extraction from Text with Large Language Models

## Quick Facts
- arXiv ID: 2412.03589
- Source URL: https://arxiv.org/abs/2412.03589
- Authors: Valentina Anita Carriero; Antonia Azzini; Ilaria Baroni; Mario Scrocca; Irene Celino
- Reference count: 40
- Primary result: Human evaluators generally rated LLM-extracted procedural knowledge quality positively but were more doubtful about its usefulness

## Executive Summary
This study explores using large language models (LLMs) to extract procedural knowledge from text and populate a knowledge graph based on a predefined ontology. The researchers employed an iterative chain-of-thought prompting approach to extract steps, actions, objects, equipment, and temporal information from procedural text. A user study with 180 participants evaluated the perceived quality and usefulness of the LLM-extracted procedural knowledge. While evaluators generally rated the quality of the extracted knowledge positively, they expressed more skepticism about its usefulness. The study also found some evidence of bias against AI-generated content, with evaluators being slightly more critical of LLM-extracted knowledge compared to human annotator-generated knowledge.

## Method Summary
The researchers used an iterative prompt engineering approach with chain-of-thought prompting to extract procedural knowledge from text. The process involved two sequential steps: first generating semi-structured step descriptions, then converting them to RDF format for knowledge graph construction. The system extracted steps, actions, objects, equipment, and temporal information from procedural text. Human evaluation was conducted through a crowdsourcing platform with 180 participants who assessed the quality and usefulness of the extracted knowledge graphs. An A/B testing approach was used to investigate potential bias by comparing evaluations of LLM-extracted knowledge versus human annotator-generated knowledge.

## Key Results
- Evaluators generally rated the quality of LLM-extracted procedural knowledge positively
- Evaluators expressed doubt about the usefulness of the extracted knowledge
- Evidence of evaluator bias against AI-generated content, with lower ratings for LLM-extracted knowledge compared to human annotations on 5 out of 22 evaluation questions
- Low inter-rater agreement among human annotators suggests the task is inherently subjective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative Chain-of-Thought (CoT) prompting improves LLM procedural knowledge extraction quality.
- Mechanism: Decomposing the global extraction task into two sequential steps—first generating semi-structured step descriptions, then converting them to RDF—reduces cognitive load on the LLM and produces more accurate outputs.
- Core assumption: Complex reasoning tasks benefit from decomposition rather than single-step prompting.
- Evidence anchors:
  - The paper shows that "we tried a Chain-of-Thought (CoT) prompting approach, i.e. decomposing the global problem into intermediate steps (prompts) and solve each of them sequentially, because this significantly improves the ability of LLMs to perform complex reasoning tasks"
  - Results show that GPT 4o "perfectly mimics our conventions for local names" and produces "complete" KGs with all extracted entities
- Break condition: If intermediate outputs are low quality, errors will compound in the second step, making the final KG incorrect.

### Mechanism 2
- Claim: Human evaluators show slight bias against LLM-generated procedural knowledge compared to human annotator output.
- Mechanism: When evaluators are told annotations come from an LLM rather than an expert human, they tend to give slightly lower quality ratings on certain dimensions, even when the actual content is identical.
- Core assumption: People have implicit negative attitudes toward AI-generated content that affects their judgment.
- Evidence anchors:
  - "we performed an A/B testing to check for any difference in people judgment, if they are told that the extraction task was executed by a LLM rather than an expert human annotator"
  - "data displays only a few statistically significant differences between the two groups, in relation to items Q2, Q3, CQ1, CQ4 and U4 (p-values of the Kruskal-Wallis test between 0.007 and 0.04); in all those cases, the evaluators gave lower ratings to the LLM with respect to the expert human annotator"
- Break condition: If evaluators are explicitly trained to focus on content quality rather than source, the bias may disappear.

### Mechanism 3
- Claim: Procedural knowledge extraction is inherently subjective, making definitive ground truth impossible.
- Mechanism: Different human annotators produce varying but valid step identifications and action-object mappings, creating ambiguity that prevents establishing absolute correctness standards.
- Core assumption: Procedural steps extracted from natural language text involve interpretation rather than objective fact extraction.
- Evidence anchors:
  - "The number of annotated steps, tools, and actions varied quite a lot, thus the level of agreement of people in executing such a task was very low (no significant inter-rater agreement indicator)"
  - "the step identification is quite different from an entity recognition or relation extraction task, because it involves the extraction of entire sentences, thus our goal is somehow more 'subjective' and it is not amenable to have an actual 'ground truth'"
- Break condition: If procedures are formalized with strict syntax or if annotators receive extensive training, agreement might improve enough to establish reliable ground truth.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: The task requires complex reasoning about procedural text, including identifying steps, actions, objects, and temporal information, which benefits from decomposition into manageable subtasks
  - Quick check question: What are the two main steps in the CoT approach used for procedural knowledge extraction in this paper?

- Concept: Knowledge Graph (KG) construction from text
  - Why needed here: The system must transform unstructured procedural text into a structured KG using predefined ontology, requiring understanding of both text extraction and semantic representation
  - Quick check question: What are the main entity types and relationships defined in the procedural ontology used in this study?

- Concept: Human bias in AI evaluation
  - Why needed here: The study specifically investigates whether telling evaluators that annotations come from an LLM versus a human affects their judgment of quality and usefulness
  - Quick check question: What statistical test was used to determine if there were significant differences between LLM and human annotator evaluations?

## Architecture Onboarding

- Component map: Input → LLM with CoT prompts → Semi-structured output → RDF conversion → Knowledge Graph → Human evaluation
- Critical path: Text processing → Step identification → Entity annotation → RDF generation → Quality assessment
- Design tradeoffs: Single complex prompt vs. multi-step decomposition (chosen: decomposition for better quality), including implicit entities vs. only explicit mentions (chosen: include implicit), providing full ontology vs. example mapping (chosen: example for simplicity)
- Failure signatures: Low inter-rater agreement suggests task ambiguity; inconsistent step identification indicates poor prompt instructions; missing RDF triples suggests conversion prompt issues
- First 3 experiments:
  1. Test zero-shot, one-shot, and few-shot prompting for step extraction to compare quality
  2. Validate CoT approach by running P1 and P2 separately and checking intermediate output quality
  3. Run A/B test with evaluators on identical LLM output labeled as human vs. AI to confirm bias exists

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on human perception rather than objective correctness measures
- Task is inherently subjective with low inter-rater agreement among human annotators
- Evaluation sample size (n=180) may not be sufficient to detect subtle effects
- Only one domain (WikiHow procedures) was tested, limiting generalizability

## Confidence

- **High confidence**: The existence of evaluator bias against LLM-generated content (statistically significant differences observed in 5 out of 22 questions)
- **Medium confidence**: The effectiveness of chain-of-thought prompting for procedural extraction (supported by qualitative observations but lacking direct comparison to baseline methods)
- **Low confidence**: The generalizability of findings to other domains or procedural text types (only one domain tested)

## Next Checks
1. Conduct A/B testing with identical content labeled differently to quantify the magnitude of evaluator bias and determine if it persists with evaluator training
2. Implement automated precision/recall metrics against a small set of hand-verified ground truth extractions to supplement subjective human evaluation
3. Test the extraction pipeline across multiple procedural domains (medical, technical, culinary) to assess domain transfer capability and identify domain-specific limitations