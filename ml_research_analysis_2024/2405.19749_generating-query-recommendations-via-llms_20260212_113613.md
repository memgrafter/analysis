---
ver: rpa2
title: Generating Query Recommendations via LLMs
arxiv_id: '2405.19749'
source_url: https://arxiv.org/abs/2405.19749
tags:
- query
- recommendations
- system
- queries
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GQR (Generative Query Recommendation), a novel
  approach to query recommendation that leverages Large Language Models (LLMs) without
  requiring training or fine-tuning. GQR uses carefully designed prompts with example
  query-recommendation pairs to instruct the LLM to generate relevant recommendations
  for a new query.
---

# Generating Query Recommendations via LLMs

## Quick Facts
- arXiv ID: 2405.19749
- Source URL: https://arxiv.org/abs/2405.19749
- Authors: Andrea Bacciu; Enrico Palumbo; Andreas Damianou; Nicola Tonellotto; Fabrizio Silvestri
- Reference count: 40
- Primary result: GQR improves NDCG@10 by ~4% and user preference by ~59% compared to existing methods

## Executive Summary
This paper introduces GQR (Generative Query Recommendation), a novel approach that leverages Large Language Models to generate query recommendations without requiring training or fine-tuning. By using carefully designed prompts with example query-recommendation pairs, GQR enables LLMs to generate relevant recommendations for new queries through in-context learning. An enhanced version, RA-GQR, further improves performance by dynamically composing prompts using similar queries retrieved from logs. Experiments demonstrate significant improvements over traditional methods across multiple datasets.

## Method Summary
The approach prompts a pre-trained LLM with example query-recommendation pairs and a new query, using in-context learning to generate recommendations without requiring fine-tuning. RA-GQR enhances this by retrieving similar queries from logs using FAISS and embedding models to create more relevant in-context examples. The system uses SCS and NDCG@10 for evaluation, measuring recommendation specificity and ranking quality respectively.

## Key Results
- GQR improves NDCG@10 by approximately 4% compared to existing methods
- RA-GQR further improves NDCG@10 by ~11% and ~6% on Robust04 and ClueWeb09B datasets respectively
- Blind user study shows GQR achieves ~59% user preference improvement over existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GQR generates relevant query recommendations without requiring query logs or fine-tuning.
- Mechanism: The system leverages a pre-trained LLM's language generation capabilities by prompting it with example query-recommendation pairs and a new query. The LLM uses in-context learning to generate recommendations based on the patterns shown in the examples.
- Core assumption: The LLM has been pre-trained on diverse text data that includes enough query-like patterns and recommendation structures to generalize to new, unseen queries.
- Evidence anchors:
  - [abstract]: "GQR uses an LLM as its foundation and does not require to be trained or fine-tuned to tackle the query recommendation problem."
  - [section]: "Essentially, in that example, we build a prompt composed of two pairs of query recommendations, followed by the user query and a ‘recommendations’ token."
  - [corpus]: Weak evidence; no directly comparable prior work found in neighbors.

### Mechanism 2
- Claim: RA-GQR improves performance by dynamically composing prompts using similar queries retrieved from logs.
- Mechanism: RA-GQR retrieves past user queries similar to the current query using a pre-trained embedding model and FAISS. These retrieved queries are used as in-context examples in the prompt, making the examples more relevant to the current query's topic.
- Core assumption: Query logs contain useful patterns and semantic similarities that can guide the LLM to generate better recommendations for the current query.
- Evidence anchors:
  - [section]: "RA-GQR dynamically composes its prompt by retrieving similar queries from query logs."
  - [section]: "This approach aims to provide GQR with relevant in-context learning examples."
  - [corpus]: Weak evidence; no directly comparable prior work found in neighbors.

### Mechanism 3
- Claim: The use of simplified clarity score (SCS) and NDCG@10 effectively measures the quality of generated recommendations.
- Mechanism: SCS measures the specificity of a query by comparing the language model of the query to that of the document collection. NDCG@10 measures the ranking quality of the recommended queries in terms of their ability to retrieve relevant documents.
- Core assumption: A higher SCS indicates a more specific query that leads to more coherent search results, and NDCG@10 is a valid measure of recommendation effectiveness.
- Evidence anchors:
  - [section]: "SCS relies on calculating a relevance model based on the query and compares it to the relevance model generated by the entire corpus using Kullback–Leibler Divergence."
  - [section]: "Our reference effectiveness metric is the Normalized Discounted Cumulative Gain at cutoff 10 (NDCG@10)."
  - [corpus]: Weak evidence; no directly comparable prior work found in neighbors.

## Foundational Learning

- Concept: Large Language Models (LLMs) and in-context learning
  - Why needed here: Understanding how LLMs can perform tasks without fine-tuning by using example prompts is central to how GQR works.
  - Quick check question: What is the difference between fine-tuning an LLM and using in-context learning with prompts?

- Concept: Query recommendation systems and their traditional approaches
  - Why needed here: Knowing the limitations of traditional query recommendation systems (e.g., need for query logs, cold-start problem) helps appreciate the innovation of GQR.
  - Quick check question: What are the main challenges faced by traditional query recommendation systems that GQR aims to solve?

- Concept: Evaluation metrics for information retrieval (IR)
  - Why needed here: Understanding metrics like SCS and NDCG@10 is essential for interpreting the experimental results and assessing the performance of GQR.
  - Quick check question: How does the simplified clarity score (SCS) differ from other query performance predictors in IR?

## Architecture Onboarding

- Component map:
  Prompt Generator -> LLM -> RA-GQR Module (optional) -> Evaluation Module

- Critical path:
  1. Generate prompt with example pairs and new query
  2. Input prompt to LLM and generate recommendations
  3. Evaluate recommendations using SCS and NDCG@10

- Design tradeoffs:
  - Using a larger LLM may improve recommendation quality but increase cost and latency
  - Including more example pairs in the prompt may help the LLM but could exceed token limits
  - Using RA-GQR can improve performance but requires access to query logs and adds complexity

- Failure signatures:
  - LLM generates irrelevant or nonsensical recommendations
  - SCS and NDCG@10 scores are low or inconsistent across runs
  - RA-GQR fails to retrieve relevant similar queries

- First 3 experiments:
  1. Test GQR with a small set of example pairs and a simple query to verify basic functionality
  2. Evaluate GQR on a held-out test set and compare SCS and NDCG@10 scores to baseline methods
  3. Implement RA-GQR and test its performance improvement over GQR on a dataset with query logs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM architectures (e.g., GPT, BLOOM, OPT) compare in performance for query recommendation tasks, and what architectural features contribute most to success?
- Basis in paper: [explicit] The paper compares GPT-3 and BLOOM, noting that GPT-3 performs better. It also references other LLMs like OPT and BLOOM in related work.
- Why unresolved: The paper only tests two specific LLMs (GPT-3 and BLOOM) and doesn't provide a comprehensive comparison across different architectures or analyze which architectural features are most important.
- What evidence would resolve it: A systematic comparison of multiple LLM architectures (e.g., GPT, BLOOM, OPT, LLaMA) on the same query recommendation tasks, analyzing performance differences and architectural characteristics.

### Open Question 2
- Question: How does the performance of generative query recommendation systems scale with increasing prompt size and diversity, and what is the optimal balance between prompt length and quality?
- Basis in paper: [explicit] The paper uses 10 examples in its prompt and mentions that the LLM is prompted with examples of queries and recommendations. It also discusses the importance of prompt design.
- Why unresolved: The paper uses a fixed number of examples (10) and doesn't explore how performance changes with different prompt sizes or the trade-off between prompt length and quality.
- What evidence would resolve it: Experiments varying the number of examples in prompts, analyzing performance changes, and determining the optimal prompt length for different query types or domains.

### Open Question 3
- Question: Can generative query recommendation systems effectively handle multi-lingual queries, and how does performance vary across different languages?
- Basis in paper: [inferred] The paper mentions LLMs being trained on diverse text data and references multi-lingual models like BLOOM, suggesting potential for multi-lingual applications.
- Why unresolved: The paper only tests on English datasets (Robust04, ClueWeb09B, AOL) and doesn't explore multi-lingual capabilities or performance differences across languages.
- What evidence would resolve it: Testing the GQR system on multi-lingual datasets, comparing performance across different languages, and analyzing any language-specific challenges or optimizations needed.

### Open Question 4
- Question: How does the inclusion of additional context (e.g., user history, search session information) impact the performance of generative query recommendation systems beyond using only the current query?
- Basis in paper: [explicit] The paper notes that GQR doesn't use user information or past interactions except the current query, but also introduces RA-GQR which uses similar past queries from logs.
- Why unresolved: While RA-GQR uses similar past queries, the paper doesn't explore richer contextual information like full user search history or session context.
- What evidence would resolve it: Experiments incorporating various types of user context (search history, session information, user profiles) into the generative query recommendation process and measuring performance improvements.

## Limitations

- The exact prompt examples used in experiments are not provided, limiting reproducibility
- Performance improvements are demonstrated only on specific English datasets (Robust04, ClueWeb09B)
- User study sample size is relatively small (14 participants) and may not capture broader user preferences

## Confidence

**High Confidence**: The core claim that LLMs can generate query recommendations without training or fine-tuning is well-supported by the experimental results and aligns with established in-context learning capabilities.

**Medium Confidence**: The performance improvements reported (4% NDCG@10 for GQR, 11% for RA-GQR) are statistically significant within the tested datasets but may not generalize across all query recommendation scenarios.

**Low Confidence**: The scalability claims and real-world applicability remain uncertain due to limited testing scope and lack of discussion on computational costs and latency considerations.

## Next Checks

1. Reproduce with varying prompt examples: Test GQR's performance using different sets of example query-recommendation pairs to verify the robustness of the approach and identify optimal prompt structures.

2. Cross-domain evaluation: Evaluate GQR on datasets from different domains (e.g., e-commerce, social media) to assess generalizability beyond the tested TREC collections.

3. Resource consumption analysis: Measure and report the computational resources, latency, and cost implications of using GQR and RA-GQR at scale to understand practical deployment constraints.