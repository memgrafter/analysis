---
ver: rpa2
title: 'Matching Problems to Solutions: An Explainable Way of Solving Machine Learning
  Problems'
arxiv_id: '2406.15662'
source_url: https://arxiv.org/abs/2406.15662
tags:
- data
- problem
- algorithm
- domain
- requirements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Machine Learning (ML) problem-solving workbench
  that helps domain specialists explore the ML solution space. The workbench captures
  ML problem-solving knowledge and embodies it in a tool to assist with problem formulation,
  solution design, and refinement.
---

# Matching Problems to Solutions: An Explainable Way of Solving Machine Learning Problems

## Quick Facts
- **arXiv ID**: 2406.15662
- **Source URL**: https://arxiv.org/abs/2406.15662
- **Reference count**: 21
- **Primary result**: Presents a workbench that helps domain specialists explore ML solution space through transparent, explainable algorithm family ranking based on domain requirements and data characteristics.

## Executive Summary
This paper introduces a Machine Learning problem-solving workbench designed to assist domain specialists in exploring and selecting appropriate ML algorithm families without requiring deep ML expertise. The system captures ML problem-solving knowledge and embodies it in a tool that supports problem formulation, solution design, and refinement. The workbench uses a heuristic matching function that ranks algorithm families based on how well they satisfy domain requirements and data characteristics, making the selection process transparent and interpretable.

## Method Summary
The workbench implements a heuristic matching function that ranks ML algorithm families by combining domain requirement importance with algorithm property weights through a weighted sum of satisfaction functions. Domain problems are mapped to computational requirements, which are then evaluated against algorithm family properties using satisfaction functions that return 0-1 scores. The system normalizes these scores to produce a ranked list of suitable algorithm families, with transparency maintained through explicit data representation rather than opaque procedural code.

## Key Results
- The workbench captures ML problem-solving knowledge and embodies it in a transparent tool for domain specialists
- A heuristic matching function identifies appropriate ML algorithm families based on domain requirements and data characteristics
- The system allows domain experts to understand and contribute to the solution process through explainable rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The matching function ranks algorithm families by combining domain requirement importance with algorithm property weights
- Mechanism: Uses weighted sum of satisfaction functions where weights are products of "how much the domain expert cares" and "how much the algorithm property matters", then normalizes across all candidates
- Core assumption: Domain experts can accurately express importance via linguistic values (Not, Could, Should, Must) that map to numeric weights
- Evidence anchors:
  - [abstract] "a heuristic matching function that helps identify the ML algorithm family that is most appropriate for the domain problem at hand, given the domain (expert) requirements, and the characteristics of the training data."
  - [section] "Wi ≡ [how much does the domain expert care about Prop P b,i] × [how much the corresponding algorithm property (ies) matters]"
- Break condition: If domain experts cannot reliably map their subjective priorities to consistent numeric weights, the ranking becomes arbitrary and loses explainability

### Mechanism 2
- Claim: Problem requirements map to multiple algorithm properties, allowing fine-grained filtering
- Mechanism: Each domain requirement maps to one or more selection criteria (e.g., "adaptability with incremental change" → Incrementality and Evolutivity), with satisfaction functions returning 0-1 scores
- Core assumption: The mapping table between requirements and algorithm properties is complete and accurate
- Evidence anchors:
  - [abstract] "the main ML solution artefacts, as well as a heuristic matching function"
  - [section] "Some domain requirements may map to several algorithm family properties. This is due to two factors: ... Genuine part-whole relationships between algorithm properties and domain requirements."
- Break condition: If the mapping table omits critical properties or over-associates unrelated ones, the function will misrank algorithm families, producing suboptimal or misleading suggestions

### Mechanism 3
- Claim: Transparency and explainability are built-in by representing solution knowledge as explicit data rather than procedural code
- Mechanism: The workbench stores algorithm families, their properties, and requirement mappings as structured data, with simple query and scoring methods; no opaque optimization or auto-ML black box
- Core assumption: Structured data with clear semantics is more interpretable than learned models or procedural search
- Evidence anchors:
  - [abstract] "embodied it in a ML problem solving workbench to helps domain specialists who are not ML experts to explore the ML solution space."
  - [section] "Our approach encodes problem solving expertise in transparent data (ML solution artefacts and their properties), with simple and transparent processes (querying, scoring, etc.)."
- Break condition: If the data representation grows too complex or the scoring functions become opaque, the system loses the explainability advantage and domain experts cannot audit or modify recommendations

## Foundational Learning

- Concept: Heuristic matching functions and weighted scoring
  - Why needed here: The workbench must rank algorithm families without exhaustive search or opaque ML models; simple scoring makes the process transparent
  - Quick check question: If an algorithm satisfies 80% of requirements with importance weights summing to 0.8, and another satisfies 70% with weights summing to 1.0, which ranks higher after normalization?

- Concept: Mapping between domain and computational requirements
  - Why needed here: Domain experts think in business terms (e.g., "auditability"), but algorithms are selected based on computational properties (e.g., "explainability"). A translation layer is essential
  - Quick check question: If "interpretability" maps to algorithm property "Interpretable," and a domain expert sets "how much you care" to "Must," what weight multiplier is applied in the satisfaction function?

- Concept: Handling multi-valued and fuzzy properties
  - Why needed here: Real-world requirements and algorithm properties are not binary; tolerance levels, accuracy ranges, and incremental capability require graded scoring
  - Quick check question: How does the system score an algorithm with "High tolerance to noise" when the requirement is "Low tolerance to noise"?

## Architecture Onboarding

- Component map: DomainProblem + DomainRequirementValue → ComputationalRequirementValues → SelectionCriterion + SelectionCriterionValueSet → Satisfaction functions → MatchingFunction (weighted sum + normalization) → Ranked list → BPMN-extended processing chain templates → Executable workflows

- Critical path:
  1. Load domain problem and requirements
  2. Map to computational requirements
  3. Evaluate data properties
  4. Compute satisfaction scores for each algorithm family
  5. Normalize and rank results
  6. Present ranked list with explanations

- Design tradeoffs:
  - Transparency vs. completeness: More detailed properties improve accuracy but complicate explainability
  - Static mapping vs. learned adaptation: Fixed mappings are interpretable but may miss novel problem-solution fits
  - Linguistic weights vs. numeric scores: Easier for users but introduces rounding and interpretation variance

- Failure signatures:
  - All algorithm families score near zero → Mismatch between problem framing and available algorithm properties
  - Rankings change drastically with small weight tweaks → Weights are not well-calibrated; users may distrust results
  - No algorithm satisfies a "Must" requirement → Constraint is too strict or mapping is incomplete

- First 3 experiments:
  1. Create a toy problem (e.g., "binary classification with small, labeled, noisy data, accuracy >80%, explainability required") and verify the workbench returns a ranked list including decision trees and logistic regression
  2. Modify weights to make explainability a "Must" and confirm that opaque models (e.g., deep neural nets) drop out of the top results
  3. Add a requirement for "incremental learning" and check that online learning algorithms (e.g., incremental SVM) rise in ranking when data volume is marked as "high"

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the matching function between ML problem requirements and algorithm families be validated in a controlled experiment?
- Basis in paper: [explicit] Section 6.3 outlines a validation protocol involving experimental subjects proficient in ML algorithm families and a dataset of real ML problems from literature
- Why unresolved: The paper states that the authors are currently working on finalizing the experimental parameters and protocols, including the size of the dataset and the number of participants
- What evidence would resolve it: A completed experimental study with a statistically significant sample size, demonstrating high correlation between the matching function's rankings and human experts' rankings of algorithm families for various ML problems

### Open Question 2
- Question: What is the optimal representation of ML problem requirements and algorithm family properties for effective matching?
- Basis in paper: [inferred] The paper discusses the importance of representing ML problems and solutions, including domain problems, ML problems, and solution artifacts. However, it does not explicitly address the optimal representation for effective matching
- Why unresolved: The paper focuses on the design of the matching function and its validation, but does not delve into the specifics of the representation itself
- What evidence would resolve it: Comparative studies evaluating the effectiveness of different representations of ML problem requirements and algorithm family properties in terms of the accuracy and efficiency of the matching process

### Open Question 3
- Question: How can the crowd-sourcing platform for selecting algorithm families be optimized to ensure high-quality contributions?
- Basis in paper: [explicit] Section 6.2 mentions the use of a crowd-sourcing platform called "icontributetoml" to gather values for selection criteria of algorithm families, but acknowledges the challenge of ensuring quality contributions due to the inability to select users
- Why unresolved: The paper does not provide specific details on how to optimize the crowd-sourcing platform for quality control
- What evidence would resolve it: Implementation and evaluation of different quality control mechanisms, such as user reputation systems, peer review processes, or automated validation checks, to ensure the reliability of contributions to the crowd-sourcing platform

## Limitations

- The accuracy and completeness of the mapping between domain requirements and algorithm properties is a critical uncertainty that could significantly impact the system's effectiveness
- The linguistic-to-numeric weight mapping introduces subjectivity that may not be consistently interpreted across different domain experts
- The system's effectiveness depends heavily on the quality of the underlying mappings and property definitions, which are not fully detailed in the available information

## Confidence

- **High confidence**: The conceptual approach of using weighted satisfaction functions for algorithm ranking is sound and well-documented
- **Medium confidence**: The transparency and explainability benefits are valid but depend on maintaining simplicity in the representation and scoring mechanisms
- **Medium confidence**: The mapping between domain requirements and algorithm properties is crucial but its accuracy cannot be verified without access to the complete mapping table

## Next Checks

1. **Mapping Table Validation**: Test the completeness and accuracy of the domain requirement to algorithm property mapping by having multiple ML experts review and score a diverse set of domain problems, then compare the suggested algorithm families against their expert recommendations

2. **Weight Sensitivity Analysis**: Conduct experiments where the same domain problem is evaluated with different expert weight assignments to determine how sensitive the ranking is to subjective importance ratings and whether small changes produce unreasonable results

3. **Real-World Problem Testing**: Apply the workbench to three actual domain problems from different industries (e.g., medical diagnosis, fraud detection, recommendation systems) and evaluate whether the top-ranked algorithm families align with accepted best practices in those domains