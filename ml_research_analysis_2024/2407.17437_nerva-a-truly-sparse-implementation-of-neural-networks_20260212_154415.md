---
ver: rpa2
title: 'Nerva: a Truly Sparse Implementation of Neural Networks'
arxiv_id: '2407.17437'
source_url: https://arxiv.org/abs/2407.17437
tags:
- nerva
- sparsity
- sparse
- accuracy
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Nerva is a C++ neural network library that uses Intel\u2019s MKL\
  \ sparse matrix operations to implement truly sparse models, eliminating the need\
  \ for binary masks. Experiments on CIFAR-10 with an MLP show that Nerva achieves\
  \ equivalent accuracy to PyTorch while offering substantial speedups at high sparsity\
  \ levels."
---

# Nerva: a Truly Sparse Implementation of Neural Networks

## Quick Facts
- arXiv ID: 2407.17437
- Source URL: https://arxiv.org/abs/2407.17437
- Reference count: 40
- Nerva achieves equivalent accuracy to PyTorch while offering substantial speedups at high sparsity levels on CIFAR-10

## Executive Summary
Nerva is a C++ neural network library that leverages Intel’s MKL sparse matrix operations to implement truly sparse models without binary masks. By using compressed sparse row (CSR) format for weight matrices, Nerva eliminates redundant computation of zero-valued weights during both training and inference. Experiments on CIFAR-10 with an MLP demonstrate that Nerva maintains equivalent accuracy to PyTorch while achieving significant reductions in training time and memory usage at high sparsity levels.

## Method Summary
The paper compares Nerva with PyTorch on CIFAR-10 using an MLP with layer sizes [3072, 1024, 512, 10] and ReLU activations. Nerva implements truly sparse training using Intel MKL’s sparse matrix operations in CSR format, while PyTorch uses dense weights with binary masks. Both implementations use identical training hyperparameters including Xavier initialization, ER sparsity distribution, 100 epochs, SGD with momentum=0.9, and learning rate scheduling based on sparsity. The comparison measures training and inference runtime, memory usage, and test accuracy across multiple sparsity levels from 0.5 to 0.999.

## Key Results
- At 99% sparsity, Nerva achieves 4× speedup in training time compared to PyTorch with masks
- Inference time improves significantly at higher sparsity levels (95% and above)
- Memory usage reduced by 49× at 99% sparsity compared to dense implementation
- Accuracy remains equivalent between Nerva and PyTorch across all tested sparsity levels

## Why This Works (Mechanism)

### Mechanism 1
Sparse matrix operations in MKL reduce FLOPs and runtime linearly with sparsity. MKL's CSR format stores only non-zero entries and their indices, allowing matrix multiplications to skip computations for zero entries and leading to proportional runtime reduction. The overhead of sparse matrix formats is outweighed by savings from avoiding multiplications with zeros.

### Mechanism 2
Eliminating binary masks removes redundant computation of masked weights. PyTorch with masks still computes dense matrix products and then applies masks, wasting FLOPs. Nerva avoids these entirely by using sparse weights from the start, eliminating the cost of maintaining and applying masks.

### Mechanism 3
Sparse training yields equivalent accuracy to dense training when sparsity is below a critical threshold. The sparse connectivity pattern preserves the expressive capacity of the network, and random sparsity with sufficient density maintains gradient flow and convergence properties.

## Foundational Learning

- Concept: Sparse matrix formats (CSR, CSC)
  - Why needed here: Understanding CSR is essential to reason about memory layout and computational savings in Nerva
  - Quick check question: In CSR format, how many integers are stored per non-zero entry beyond the value itself?

- Concept: Linear algebra operations (SpMM, SpMV)
  - Why needed here: Core of Nerva's speed gains; engineers must know when sparse multiplication is beneficial
  - Quick check question: For a sparse matrix with density d, how do SpMM operations scale with d compared to dense?

- Concept: Neural network training loop mechanics
  - Why needed here: To modify backpropagation and optimization to handle sparse weight matrices
  - Quick check question: In backpropagation, which matrix operation becomes sparse when weights are sparse?

## Architecture Onboarding

- Component map: C++ core with MKL sparse ops -> Python bindings via Pybind11 -> MLP, dense, sparse layers, batch norm, dropout, SGD -> CSR storage for sparse weights -> Dense fallback for low sparsity

- Critical path: 1. Build MLP graph 2. Convert to sparse where beneficial 3. Execute forward/backward via MKL 4. Update weights with momentum handling

- Design tradeoffs: Sparse ops require CSR conversion overhead; dense ops are simpler but memory-heavy. Python interface adds latency but increases accessibility. CPU-only implementation limits performance on GPUs.

- Failure signatures: Runtime stalls (likely CSR conversion or memory allocation). Accuracy drop (sparsity too high or poor initialization). Memory bloat (sparse matrices not compressed correctly).

- First 3 experiments: 1. Train dense MLP on CIFAR-10; verify correctness. 2. Switch to sparse with 50% density; measure speedup and accuracy. 3. Sweep sparsity from 10% to 99%; plot accuracy vs runtime.

## Open Questions the Paper Calls Out

- Question: How does the runtime scaling of Nerva compare to PyTorch on GPUs, and does sparse GPU implementation achieve comparable speedups to CPU sparse operations?
  - Basis in paper: The paper notes plans to report GPU results in the future and questions whether sparse GPU implementation can compete with dense GPU
  - Why unresolved: The paper only presents CPU experiments and does not evaluate GPU performance
  - What evidence would resolve it: Benchmarking Nerva and PyTorch on GPUs with varying sparsity levels and model sizes, comparing training and inference times to dense GPU implementations

- Question: Does truly sparse training with Nerva provide any accuracy advantages over masked training at high sparsity levels, or are observed differences due to implementation discrepancies?
  - Basis in paper: The paper notes that Nerva outperforms PyTorch in accuracy at high sparsity levels (99%+), but the authors are unsure if this is due to sparse training advantages or implementation differences
  - Why unresolved: The paper does not identify the cause of the accuracy difference between Nerva and PyTorch at high sparsity levels
  - What evidence would resolve it: Carefully controlled experiments comparing Nerva and PyTorch implementations, ensuring identical hyperparameters and initialization, or theoretical analysis of sparse training dynamics

## Limitations

- Experiments are limited to CIFAR-10 MLP workloads on CPU only
- No evaluation of GPU performance or large transformer-scale models
- Very high sparsity levels (>99.5%) and small matrix sizes are not quantified
- Only one sparsity distribution scheme (ER) is tested across all experiments

## Confidence

- Runtime scaling claims at 80-99% sparsity: **High** (directly measured wall-clock time)
- Memory savings: **Medium** (CSR overhead is well understood but per-layer storage varies)
- Accuracy preservation generality: **Low** (only one dataset, model, and sparsity distribution examined)

## Next Checks

1. Replicate the runtime vs. sparsity curve on a different MLP architecture (e.g., 784-512-256-10) to test architectural sensitivity
2. Profile the per-layer runtime contribution to confirm that reductions scale linearly with density
3. Vary the random seed for sparsity pattern generation and measure accuracy variance at 99% sparsity