---
ver: rpa2
title: Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning
  Attacks
arxiv_id: '2410.18210'
source_url: https://arxiv.org/abs/2410.18210
tags:
- safety
- fine-tuning
- parameters
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how fine-tuning attacks can compromise multilingual
  large language models (LLMs) across languages, even when training data is in just
  one language. Using adversarial instruction-following examples, the authors show
  that safety alignment can be broken not only in the language used for fine-tuning
  but also in other languages, without degrading multilingual capabilities.
---

# Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attacks

## Quick Facts
- arXiv ID: 2410.18210
- Source URL: https://arxiv.org/abs/2410.18210
- Authors: Samuele Poppi; Zheng-Xin Yong; Yifei He; Bobbie Chern; Han Zhao; Aobo Yang; Jianfeng Chi
- Reference count: 24
- Primary result: Fine-tuning attacks on multilingual LLMs compromise safety across languages, even when training data is in just one language

## Executive Summary
This paper explores how fine-tuning attacks can compromise multilingual large language models (LLMs) across languages, even when training data is in just one language. Using adversarial instruction-following examples, the authors show that safety alignment can be broken not only in the language used for fine-tuning but also in other languages, without degrading multilingual capabilities. They introduce Safety Information Localization (SIL), a method to identify language-agnostic safety-related parameters. Their findings show that modifying only 20% of model weights via SIL is sufficient to cause safety violations across all tested languages. They also confirm the "alternative pathways" hypothesis—freezing safety parameters does not prevent fine-tuning attacks—and demonstrate that attack vectors remain effective even after models are adapted to new languages. This work highlights critical vulnerabilities in multilingual LLM safety and offers new insights into defending against cross-lingual fine-tuning attacks.

## Method Summary
The authors conduct experiments on multilingual LLMs (Qwen-2-7B-Instruct and Llama-3.1-8B-Instruct) by fine-tuning them with harmful instruction-following data in one language and evaluating safety violations across all tested languages. They introduce Safety Information Localization (SIL), which uses gradient-based importance scoring to identify language-agnostic safety-related parameters. The SIL method identifies the top 20% most important parameters that, when modified, cause safety violations. They demonstrate the effectiveness of SIL through stitching experiments, where localized parameters from fine-tuned models are transferred back to safety-aligned models, causing them to jailbreak. Additionally, they test the alternative pathways hypothesis by freezing SIL-identified parameters during fine-tuning attacks to show that new safety-bypassing pathways are created.

## Key Results
- Fine-tuning with harmful examples in one language (e.g., English) causes safety violations across all tested languages
- Safety-related parameters are language-agnostic and shared across languages in multilingual LLMs
- Only 20% of parameters identified by SIL are sufficient to cause safety violations when stitched back to original models
- Freezing safety-related parameters does not prevent fine-tuning attacks due to alternative pathways being created

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety-related parameters in multilingual LLMs are language-agnostic, enabling cross-lingual jailbreaking through fine-tuning attacks.
- Mechanism: The safety alignment information is stored in shared parameters across languages. Fine-tuning attacks modify these shared parameters in one language, causing safety violations in other languages.
- Core assumption: The safety knowledge is not language-specific but rather stored in language-agnostic parameters that are shared across all languages.
- Evidence anchors:
  - [abstract] "we hypothesize that safety-related information is language-agnostic and propose a new method termed Safety Information Localization (SIL) to identify the safety-related information in the model parameter space"
  - [section 3.3] "there exists a language-agnostic safety parameters within multilingual safety-aligned LLMs, and fine-tuning attacks (in Section 2.2) update these parameters and thus produce harmful behaviors across different languages"
  - [corpus] "Average neighbor FMR=0.455" (weak evidence, only 25 related papers found)

### Mechanism 2
- Claim: Stitching localized safety-related parameters from fine-tuned models back to safety-aligned models can jailbreak them.
- Mechanism: The Safety Information Localization (SIL) method identifies the parameters modified during fine-tuning attacks. By transferring these parameters back to the original model, the jailbreak effect is replicated without needing to retrain.
- Core assumption: The parameters identified by SIL are sufficient to cause safety violations when transferred.
- Evidence anchors:
  - [section 3.2] "We introduce the stitching operation, which uses the binary mask γSIL-k to make the safety-aligned pretrained model unsafe"
  - [section 3.2] "we observe that using only 20% of the parameters selected by SIL can already undo the safety alignment of LLMs"
  - [corpus] "Found 25 related papers" (limited related work on this specific stitching approach)

### Mechanism 3
- Claim: Freezing safety-related parameters does not prevent fine-tuning attacks due to alternative pathways being created.
- Mechanism: When safety-related parameters are frozen, the fine-tuning attack creates new pathways through other parameters that achieve the same jailbreaking effect. SIL can identify these new parameters.
- Core assumption: The model can find alternative routes to bypass safety constraints even when primary safety parameters are frozen.
- Evidence anchors:
  - [abstract] "Furthermore, we provide evidence to the alternative pathways hypothesis for why freezing safety-related parameters does not prevent fine-tuning attacks"
  - [section 4.1] "we will be able to localize this new pathway using SIL...we demonstrate the two aforementioned properties in Table 3 and Table 4, thus confirming the alternative pathways hypothesis"
  - [corpus] "Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models" (limited evidence for alternative pathways)

## Foundational Learning

- Concept: Fine-tuning attacks on LLMs
  - Why needed here: The paper's core contribution is understanding how fine-tuning attacks work across languages in multilingual LLMs
  - Quick check question: What is the minimum number of harmful examples needed to successfully jailbreak a safety-aligned LLM through fine-tuning?

- Concept: Safety alignment in LLMs
  - Why needed here: Understanding what safety alignment is and how it can be compromised is essential to grasp the paper's significance
  - Quick check question: What is the primary goal of safety alignment in LLMs?

- Concept: Multilingual model architecture
  - Why needed here: The paper's focus on cross-lingual effects requires understanding how multilingual LLMs handle different languages
  - Quick check question: How do multilingual LLMs typically represent knowledge across different languages?

## Architecture Onboarding

- Component map: Multilingual LLM (Qwen-2-7B-Instruct or Llama-3.1-8B-Instruct) -> Fine-tuning attack mechanism -> Safety evaluation pipeline -> SIL localization method -> Stitching operation
- Critical path: 1) Fine-tune the multilingual LLM with harmful data in one language, 2) Evaluate safety violation rates across all languages, 3) Apply SIL to identify safety-related parameters, 4) Stitch these parameters back to the original model, 5) Verify the jailbreak effect persists
- Design tradeoffs: The SIL method trades parameter sparsity (using only 20% of parameters) for effectiveness, while the fine-tuning attack trades computational resources (single epoch) for cross-lingual impact. The choice of automatic evaluators versus human evaluation represents a tradeoff between scalability and accuracy
- Failure signatures: If cross-lingual violation rates don't increase after fine-tuning, if SIL localization doesn't identify effective parameters, if stitched models don't jailbreak, or if freezing safety parameters prevents attacks (contradicting the alternative pathways hypothesis)
- First 3 experiments:
  1. Fine-tune Qwen-2-7B-Instruct with 100 harmful English examples and measure violation rates across all 9 tested languages
  2. Apply SIL to the English-fine-tuned model and create stitched models, then compare their violation rates to the original fine-tuned model
  3. Freeze the SIL-identified safety parameters and attempt the fine-tuning attack again to verify alternative pathways are created

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms or architectural features in multilingual LLMs enable the cross-lingual transfer of safety-related parameters?
- Basis in paper: [explicit] The paper demonstrates that safety-related parameters are shared across languages, but doesn't explain the underlying mechanisms that allow this sharing.
- Why unresolved: The study identifies the phenomenon but does not investigate the technical reasons behind parameter sharing across languages.
- What evidence would resolve it: Detailed architectural analysis or experiments showing how specific components (e.g., attention heads, feed-forward layers) contribute to cross-lingual safety parameter sharing.

### Open Question 2
- Question: How effective are alternative defense strategies beyond freezing safety parameters in preventing cross-lingual fine-tuning attacks?
- Basis in paper: [explicit] The paper confirms that freezing safety parameters fails to prevent attacks but does not explore other defensive approaches.
- Why unresolved: The study focuses on demonstrating the vulnerability rather than developing or testing mitigation techniques.
- What evidence would resolve it: Experiments testing various defense mechanisms such as differential privacy, adversarial training, or parameter masking across multiple languages.

### Open Question 3
- Question: Does the effectiveness of cross-lingual fine-tuning attacks vary across different language families or based on linguistic similarity?
- Basis in paper: [explicit] The study tests nine languages but does not analyze whether attack effectiveness correlates with linguistic relationships between languages.
- Why unresolved: The experimental design tests multiple languages but lacks comparative analysis of attack success rates based on language family relationships.
- What evidence would resolve it: Systematic experiments comparing attack success rates across languages grouped by linguistic families (e.g., Romance, Indo-European, Sino-Tibetan) and measuring correlation with linguistic distance metrics.

## Limitations

- The use of automatic evaluators (Llama-Guard-3 and Llama-3.1-405B) rather than human evaluation introduces potential brittleness in safety violation detection
- The 20% parameter threshold for SIL appears effective but may vary across model architectures
- The translation quality of BeaverTails examples across 9 languages is not explicitly validated, which could affect cross-lingual transfer patterns

## Confidence

**High Confidence:** The core finding that fine-tuning attacks in one language can cause safety violations across all tested languages is well-supported by experimental evidence. The demonstration that SIL can identify effective safety-related parameters and that stitching these parameters causes jailbreaking is robust.

**Medium Confidence:** The claim about alternative pathways being created when safety parameters are frozen is supported but could benefit from more mechanistic analysis. The 20% parameter threshold for SIL effectiveness is demonstrated but may not generalize to all multilingual models.

**Low Confidence:** The assumption that automatic evaluators provide reliable safety violation detection across all 9 languages without human validation.

## Next Checks

1. **Human Evaluation Validation:** Conduct human evaluation studies to verify that automatic evaluator-detected safety violations correspond to actual harmful outputs across all tested languages.

2. **Parameter Threshold Sensitivity:** Systematically vary the SIL parameter threshold (e.g., 10%, 20%, 30%, 40%) to determine the minimum effective fraction and assess sensitivity to this hyperparameter.

3. **Cross-Model Generalization:** Test SIL localization and stitching effectiveness on a different multilingual architecture (e.g., Mistral or BLOOM models) to validate that safety-related parameters are indeed language-agnostic across model families.