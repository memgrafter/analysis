---
ver: rpa2
title: Length Desensitization in Direct Preference Optimization
arxiv_id: '2409.06411'
source_url: https://arxiv.org/abs/2409.06411
tags:
- length
- ld-dpo
- optimization
- preference
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies length sensitivity in Direct Preference Optimization
  (DPO) and proposes LD-DPO to mitigate verbose responses. DPO's gradients are inversely
  proportional to likelihood, causing longer responses to be disproportionately favored.
---

# Length Desensitization in Direct Preference Optimization

## Quick Facts
- arXiv ID: 2409.06411
- Source URL: https://arxiv.org/abs/2409.06411
- Reference count: 40
- This paper identifies length sensitivity in Direct Preference Optimization (DPO) and proposes LD-DPO to mitigate verbose responses.

## Executive Summary
This paper identifies a fundamental issue in Direct Preference Optimization (DPO) where gradients are inversely proportional to likelihood, causing longer responses to be disproportionately favored during training. This leads to verbose outputs that don't necessarily reflect human preferences. The authors propose Length Desensitization DPO (LD-DPO), which reparameterizes the likelihood calculation to decouple verbosity preference from the reward, thereby smoothing optimization. Through extensive experiments on Llama2-13B, Llama3-8B, and Qwen2-7B models, LD-DPO achieves 10-40% shorter responses while improving performance on multiple benchmarks including MT-Bench, AlpacaEval 2, and reasoning tasks.

## Method Summary
LD-DPO modifies the standard DPO algorithm by reparameterizing the likelihood calculation to reduce length sensitivity. The key innovation is decomposing the likelihood of longer responses into public-length and excessive portions, then applying a hyperparameter α to attenuate the impact of the excessive portion on the optimization direction. The method maintains the other human-like preferences while eliminating verbosity induced by excessive length. The authors demonstrate that the optimal α value depends on model capability, with stronger models showing less length sensitivity and requiring different α settings.

## Key Results
- LD-DPO achieves 10-40% shorter responses compared to standard DPO
- Maintains or improves performance on MT-Bench, AlpacaEval 2, Arena-Hard, and ProofWriter benchmarks
- Optimal α hyperparameter depends on model capability, with stronger models showing less length sensitivity
- Outperforms DPO and baseline methods while maintaining human-like preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO's gradients are inversely proportional to likelihood, causing longer responses to be disproportionately favored
- Mechanism: The partial derivatives of DPO's loss function with respect to chosen and rejected responses are inversely proportional to their respective likelihood values. Since likelihood decreases with sequence length, longer responses receive disproportionately larger gradient updates
- Core assumption: The likelihood of a response is calculated as the product of conditional probabilities of each token, which decreases rapidly with increasing sequence length
- Evidence anchors:
  - "According to Eq.7, we know that if len(yw) > len(yl), then it is highly likely that πθ(yw|x) < πθ(yl|x), so the language model tends to generate the longer response yw after DPO"
  - "∂LDP O(X1; X2)/∂X1 / ∂LDP O(X1; X2)/∂X2 = X2/X1 = πθ(yl|x)/πθ(yw|x)"

### Mechanism 2
- Claim: LD-DPO mitigates length sensitivity by reparameterizing likelihood to decouple verbosity preference from the reward
- Mechanism: LD-DPO decomposes the likelihood of longer responses into public-length and excessive portions, then applies a hyperparameter α to reduce the impact of the excessive portion on the optimization direction
- Core assumption: The verbosity preference caused by excessively long responses can be decoupled from other human-like preferences without losing alignment quality
- Evidence anchors:
  - "In LD-DPO, our objective is to attenuate the sensitivity of DPO by eliminating the verbosity preferences induced by the excessively long portions, while concurrently maintaining the other preferences"
  - "The excessive portion is further broken down into verbosity preference (due to excess length) and other preferences"

### Mechanism 3
- Claim: Model capability correlates inversely with length sensitivity during DPO training
- Mechanism: More capable models are better at capturing genuine human preferences rather than being influenced by text length, requiring less aggressive length desensitization
- Core assumption: Model capability affects the balance between length preference and substantive content preference in the optimization process
- Evidence anchors:
  - "the length sensitivity during DPO training exhibits a negative correlation with the underlying model capability"
  - "The models selected for our experiments vary in their capabilities and, consequently, in their length sensitivity during the DPO process"

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding the standard RLHF pipeline is essential to grasp how DPO differs from traditional RLHF approaches
  - Quick check question: What are the three stages of the standard RLHF pipeline, and how does DPO simplify the final stage?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the baseline algorithm being improved upon, so understanding its mechanics and limitations is crucial
  - Quick check question: How does DPO reparameterize the reward function compared to traditional RLHF methods?

- Concept: Likelihood and sequence length relationship
  - Why needed here: The core mechanism of length sensitivity relies on understanding how sequence length affects likelihood calculations
  - Quick check question: Why does the likelihood of a response decrease as the sequence length increases?

## Architecture Onboarding

- Component map:
  Input -> Likelihood reparameterization with hyperparameter α -> Desensitized optimization direction

- Critical path:
  1. Compute public length lp = min(lw, ll)
  2. Apply likelihood modification: πθ(y|x) → ˆπθ(y|x) using Eq. 12
  3. Calculate optimization gradients using modified likelihood
  4. Update model parameters based on gradients

- Design tradeoffs:
  - α too high: Length sensitivity remains, verbose responses persist
  - α too low: Risk of losing genuine human preferences, potential performance degradation
  - Computational overhead: Minimal additional computation beyond standard DPO

- Failure signatures:
  - Responses remain verbose despite LD-DPO training
  - Model performance degrades significantly on reasoning tasks
  - Inconsistent α values needed across similar model sizes

- First 3 experiments:
  1. Compare average response length between DPO and LD-DPO with α=0.5 on a small dataset
  2. Test LD-DPO with different α values (0.1, 0.5, 0.9) on a single model to find optimal setting
  3. Verify length sensitivity reduction by plotting predicted probability differences vs. length differences before and after LD-DPO

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the length sensitivity coefficient γ vary across different model architectures (e.g., decoder-only vs encoder-decoder models)?
- Basis in paper: [inferred] The paper discusses γ as a metric for length sensitivity and shows it varies with model capability using decoder-only LLMs.
- Why unresolved: The analysis is limited to decoder-only models (Llama2, Llama3, Qwen2). Encoder-decoder models like T5 or BART might exhibit different length sensitivity patterns due to their architectural differences in handling sequence generation.
- What evidence would resolve it: Experimental results comparing γ across diverse model architectures including encoder-decoder models on the same preference optimization tasks.

### Open Question 2
- Question: What is the relationship between training data characteristics (e.g., average response length, length variance) and the optimal α hyperparameter in LD-DPO?
- Basis in paper: [explicit] The paper notes that the optimal α depends on model capability and observes different α values work best for different models, but doesn't analyze how data characteristics affect this choice.
- Why unresolved: The paper uses UltraFeedback dataset but doesn't investigate how its specific length distribution impacts the optimal α value or whether different datasets would require different α settings.
- What evidence would resolve it: Systematic experiments varying dataset characteristics (length distribution, variance) and measuring how optimal α changes across these variations.

### Open Question 3
- Question: Does LD-DPO's length desensitization generalize to other preference optimization objectives beyond the Bradley-Terry formulation used in DPO?
- Basis in paper: [inferred] The paper focuses on DPO's specific optimization objective and its length sensitivity, but doesn't explore whether similar length issues exist in alternative preference learning frameworks.
- Why unresolved: The theoretical analysis is specific to DPO's log-likelihood ratio formulation. Other preference optimization methods (like KTO or ORPO) might have different mathematical properties that either mitigate or exacerbate length sensitivity.
- What evidence would resolve it: Comparative analysis of length sensitivity across multiple preference optimization algorithms, measuring implicit rewards and optimization gradients for each method.

## Limitations
- The optimal α hyperparameter depends on model capability but the relationship remains empirical rather than theoretically grounded
- Primary metric of "length-controlled win rate" against GPT-4-1106-preview is computationally expensive and may not fully capture quality trade-offs
- Analysis relies heavily on the UltraFeedback dataset without investigating how effectiveness varies across different preference dataset characteristics

## Confidence

**High Confidence**: The identification of length sensitivity as a fundamental issue in DPO's gradient calculation. The mathematical derivation showing that DPO gradients are inversely proportional to likelihood, combined with empirical observations of verbose outputs, provides strong evidence for this core claim.

**Medium Confidence**: The effectiveness of LD-DPO in reducing response length while maintaining or improving performance. While the experimental results are compelling across multiple models and benchmarks, the dependency on the α hyperparameter and the limited exploration of its optimal selection methodology reduce confidence in practical deployment.

**Low Confidence**: The claim about the inverse correlation between model capability and length sensitivity. While the paper observes this pattern across the tested models, the mechanistic explanation for why more capable models are less sensitive to length differences remains speculative rather than empirically validated.

## Next Checks

1. **Ablation Study on α Selection**: Conduct a systematic study to determine if there's a predictable relationship between model capability metrics (parameter count, pre-training compute, downstream performance) and the optimal α value, rather than using fixed empirical values.

2. **Dataset Diversity Analysis**: Test LD-DPO across multiple preference datasets with varying characteristics (e.g., different distributions of length differences between chosen/rejected responses) to assess robustness and identify potential failure modes.

3. **Long-form Quality Assessment**: Design experiments specifically evaluating whether the 10-40% length reduction affects the quality of long-form responses in domains requiring detailed explanations, to validate that the length reduction doesn't sacrifice substantive content.