---
ver: rpa2
title: 'DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific
  Factors'
arxiv_id: '2406.11427'
source_url: https://arxiv.org/abs/2406.11427
tags:
- speech
- text
- diffusion
- length
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DiTTo-TTS, a Diffusion Transformer-based text-to-speech
  (TTS) system that achieves state-of-the-art performance without relying on domain-specific
  factors like phonemes and durations. The authors demonstrate that DiTTo-TTS outperforms
  U-Net architectures, achieves superior results with variable-length modeling and
  a speech length predictor, and benefits from aligned text-speech embeddings.
---

# DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors

## Quick Facts
- arXiv ID: 2406.11427
- Source URL: https://arxiv.org/abs/2406.11427
- Reference count: 40
- This paper presents DiTTo-TTS, a Diffusion Transformer-based text-to-speech (TTS) system that achieves state-of-the-art performance without relying on domain-specific factors like phonemes and durations.

## Executive Summary
This paper introduces DiTTo-TTS, a novel text-to-speech system that leverages Diffusion Transformers (DiT) instead of traditional U-Net architectures. The authors demonstrate that DiTTo-TTS achieves superior performance without requiring domain-specific factors such as phonemes and durations. By implementing variable-length modeling with a speech length predictor and aligning text-speech embeddings through joint modeling, the system shows significant improvements in naturalness, intelligibility, and speaker similarity. The model is trained on 82K hours of multilingual data and achieves state-of-the-art results while offering 4.6x faster inference and 3.84x smaller model size compared to autoregressive baselines.

## Method Summary
DiTTo-TTS is a diffusion-based TTS system that operates in latent space using a neural audio codec (Mel-V AE). The architecture consists of a text encoder (SpeechT5 or ByT5), a speech length predictor, and DiT blocks with cross-attention for text conditioning. Unlike previous works, it uses variable-length modeling with a speech length predictor to avoid padding artifacts, and achieves semantic alignment between text and speech embeddings through joint modeling. The system is trained on 82K hours of multilingual speech-transcript data across 9 languages without any domain-specific factors.

## Key Results
- Base model outperforms autoregressive state-of-the-art models with 4.6x faster inference and 3.84x smaller model size
- Variable-length modeling with speech length prediction significantly improves performance over fixed-length approaches
- Semantic alignment between text and speech embeddings through joint modeling enhances TTS performance
- Achieves or exceeds baseline performance in naturalness, intelligibility, and speaker similarity across English-only and multilingual evaluations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Diffusion Transformer architecture with minimal modifications outperforms U-Net in TTS tasks without domain-specific factors.
- **Mechanism:** DiT provides better temporal modeling and sequence alignment capabilities compared to U-Net's spatial-focused architecture. The addition of long skip connections and global adaptive layer normalization (AdaLN) enhances feature propagation and conditioning effectiveness.
- **Core assumption:** Temporal alignment in speech can be effectively learned through transformer-based attention mechanisms without explicit phoneme or duration modeling.
- **Evidence anchors:**
  - [abstract] "we demonstrate that the Diffusion Transformer (DiT) (Peebles & Xie, 2023) is more suitable for TTS tasks than U-Net (Ronneberger et al., 2015) architectures"
  - [section] "we demonstrate that the Diffusion Transformer (DiT) (Peebles & Xie, 2023) is more suitable for TTS tasks than U-Net (Ronneberger et al., 2015) architectures"
  - [corpus] Weak - related papers focus on DiT applications but don't directly compare to U-Net for TTS
- **Break condition:** If attention mechanisms cannot effectively learn temporal alignment without explicit alignment signals, performance would degrade significantly.

### Mechanism 2
- **Claim:** Variable-length modeling with speech length prediction significantly improves performance over fixed-length approaches.
- **Mechanism:** Instead of padding to fixed lengths (which introduces noise and computational waste), the model learns to predict total speech length from text input, enabling more efficient and accurate generation without padding artifacts.
- **Core assumption:** The total speech length for a given text can be accurately predicted from the text representation alone, enabling variable-length generation without explicit duration modeling.
- **Evidence anchors:**
  - [abstract] "we show that modeling variable speech length improves performance compared to the fixed length modeling used in previous works"
  - [section] "modeling variable speech length improves performance compared to the fixed length modeling used in previous works (Gao et al., 2023; Lovelace et al., 2024)"
  - [corpus] Weak - related works focus on different aspects of TTS but don't address variable-length modeling specifically
- **Break condition:** If speech length prediction becomes inaccurate for complex or out-of-domain text, the model would struggle with variable-length generation.

### Mechanism 3
- **Claim:** Semantic alignment between text and speech embeddings through joint modeling significantly enhances TTS performance.
- **Mechanism:** When text and speech encoders are jointly trained or fine-tuned to align their representations, cross-attention mechanisms can more effectively condition speech generation on text input without explicit alignment signals.
- **Core assumption:** Closer proximity between text and speech representations in embedding space leads to better cross-attention learning and more accurate text-to-speech alignment.
- **Evidence anchors:**
  - [abstract] "conditions like semantic alignment in speech latent representations are key to further enhancement"
  - [section] "we explore the crucial conditions for effective latent representation in LDM-based TTS models, including semantic alignment"
  - [corpus] Weak - related papers mention semantic alignment but don't specifically address joint text-speech encoder training
- **Break condition:** If cross-attention mechanisms cannot effectively utilize aligned embeddings, performance gains from semantic alignment would not materialize.

## Foundational Learning

- **Concept:** Diffusion models and denoising processes
  - **Why needed here:** Understanding how diffusion models progressively transform noise into speech data is fundamental to grasping DiTTo-TTS architecture and training process
  - **Quick check question:** What is the primary difference between forward and reverse processes in diffusion models?

- **Concept:** Transformer architectures and attention mechanisms
  - **Why needed here:** DiTTo-TTS relies heavily on transformer blocks with cross-attention for text conditioning, requiring understanding of how attention mechanisms process sequential data
  - **Quick check question:** How does cross-attention differ from self-attention in transformer architectures?

- **Concept:** Latent diffusion models and autoencoder integration
  - **Why needed here:** The paper operates in latent space using a neural audio codec, requiring understanding of how latent representations capture essential features while reducing computational costs
  - **Quick check question:** Why might operating in latent space be more efficient than working directly with raw audio data?

## Architecture Onboarding

- **Component map:** Text Encoder (SpeechT5 or ByT5) → Text Embeddings → DiT Blocks with Cross-Attention → Denoising → Generated Speech Latents → Audio Codec Decoder → Raw Speech Output

- **Critical path:** Text → Text Encoder → DiT Blocks (with Cross-Attention) → Denoising → Generated Speech Latents → Audio Codec Decoder → Raw Speech Output

- **Design tradeoffs:**
  - Using latent representations trades some audio fidelity for computational efficiency
  - Eliminating domain-specific factors simplifies training but requires more sophisticated alignment learning
  - Variable-length modeling improves performance but adds complexity to the prediction pipeline

- **Failure signatures:**
  - Poor intelligibility indicates issues with text-speech alignment or latent quality
  - Speaker similarity problems suggest conditioning effectiveness issues
  - Slow inference may indicate suboptimal model scaling or diffusion step configuration

- **First 3 experiments:**
  1. Test basic DiT architecture with fixed-length modeling to establish baseline performance
  2. Implement speech length predictor and evaluate variable-length generation improvements
  3. Compare joint vs. separate text-speech encoder training to measure semantic alignment impact

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of DiTTo-TTS scale with even larger datasets beyond 82K hours, and what is the optimal balance between data size and model complexity?
- **Basis in paper:** [explicit] The paper discusses scaling with 82K hours of training data and 790M parameters, but does not explore beyond this scale.
- **Why unresolved:** The paper focuses on the effectiveness of DiTTo-TTS at a specific scale, but does not investigate the upper limits of data and model size scaling.
- **What evidence would resolve it:** Experiments with significantly larger datasets (e.g., 500K+ hours) and correspondingly larger models (e.g., 2B+ parameters) to measure performance improvements and identify diminishing returns.

### Open Question 2
- **Question:** Can the speech length predictor in DiTTo-TTS be further improved to achieve better alignment between text and generated speech, particularly in multilingual settings?
- **Basis in paper:** [explicit] The paper introduces a speech length predictor but notes its performance is crucial for alignment and may vary across languages.
- **Why unresolved:** The current speech length predictor shows good results but could potentially be optimized further, especially for handling diverse linguistic structures in multilingual contexts.
- **What evidence would resolve it:** Comparative studies of different speech length prediction architectures (e.g., transformer-based vs. non-transformer) and their impact on alignment quality across multiple languages.

### Open Question 3
- **Question:** How does the choice of latent representation (e.g., Mel-V AE vs. other codecs) impact the overall performance and efficiency of DiTTo-TTS, and what are the trade-offs between compression ratio and speech quality?
- **Basis in paper:** [explicit] The paper compares Mel-V AE with EnCodec and DAC, highlighting Mel-V AE's superior performance, but does not exhaustively explore other latent representations.
- **Why unresolved:** While Mel-V AE is shown to be effective, the paper does not investigate the full spectrum of potential latent representations and their impact on the model's capabilities.
- **What evidence would resolve it:** Systematic evaluation of various latent representations (e.g., different autoencoder architectures, quantization methods) and their effects on speech quality, intelligibility, and model efficiency.

## Limitations

- **Generalization across languages:** While the paper reports multilingual performance, the evaluation focuses primarily on English, with only limited testing on other languages. The claim that the model performs well across all 9 training languages is based on very sparse evidence, particularly regarding pronunciation accuracy and language-specific prosody.

- **Trade-off between speed and quality:** The reported 4.6x faster inference comes at the cost of using only 25 diffusion steps. While this is standard practice, the paper doesn't provide systematic ablation studies showing how quality degrades with fewer steps, or whether the claimed speed advantage holds across different quality thresholds.

- **Absence of fine-tuning data:** The claim of "outperforming state-of-the-art" autoregressive models is based on comparing to models trained with much more data (100K+ hours for some baselines). The paper doesn't address whether these results would hold if baselines were given similar data scales, making the comparison potentially unfair.

## Confidence

- **High:** Base architecture effectiveness, variable-length modeling benefits, semantic alignment importance
- **Medium:** Speed advantages, multilingual capabilities, comparison with baselines
- **Low:** Cross-lingual performance consistency, long-term stability, robustness to out-of-domain text

## Next Checks

1. **Ablation study on diffusion steps:** Systematically evaluate speech quality at 25, 50, 100, and 200 steps to quantify the trade-off between inference speed and naturalness, verifying whether the 4.6x speed claim holds when quality is held constant.

2. **Cross-lingual robustness testing:** Conduct detailed evaluations of the multilingual model on each of the 9 training languages separately, measuring not just overall MOS but also language-specific metrics like pronunciation accuracy, accent preservation, and prosody naturalness.

3. **Controlled baseline comparison:** Retrain the best autoregressive baseline (VALL-E X) with the same 82K hours of data used for DiTTo-TTS to establish a fair comparison, measuring whether the performance gap persists when training data is equalized.