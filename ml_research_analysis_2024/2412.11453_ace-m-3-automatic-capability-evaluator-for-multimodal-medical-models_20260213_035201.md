---
ver: rpa2
title: 'ACE-$M^3$: Automatic Capability Evaluator for Multimodal Medical Models'
arxiv_id: '2412.11453'
source_url: https://arxiv.org/abs/2412.11453
tags:
- response
- score
- evaluation
- criterion
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ACE-M3, an open-source multimodal evaluation
  model for medical large language models. The model uses a branch-merge architecture
  with three specialized evaluators for expression, medical knowledge correctness,
  and patient question relevance, combined with a conclusion model.
---

# ACE-$M^3$: Automatic Capability Evaluator for Multimodal Medical Models

## Quick Facts
- arXiv ID: 2412.11453
- Source URL: https://arxiv.org/abs/2412.11453
- Authors: Xiechi Zhang; Shunfan Zheng; Linlin Wang; Gerard de Melo; Zhu Cao; Xiaoling Wang; Liang He
- Reference count: 40
- Primary result: 82.71% accuracy on multimodal data, outperforming GPT-4-Turbo by 5.3%

## Executive Summary
This paper introduces ACE-M3, an open-source multimodal evaluation model for medical large language models. The model uses a branch-merge architecture with three specialized evaluators for expression, medical knowledge correctness, and patient question relevance, combined with a conclusion model. To improve efficiency, the authors propose an Efficient-RTDPO training strategy that freezes lower LLM layers and uses reward tokens for direct preference optimization. The model is trained on a carefully curated instruction dataset collected from GPT-3.5-Turbo with reliable medical evaluation criteria. Experiments show ACE-M3 achieves 82.71% accuracy on multimodal data and 72.59% on text-only data, outperforming existing models like GPT-4-Turbo by 5.3% in conclusion evaluation accuracy.

## Method Summary
ACE-M3 is an automatic capability evaluator for multimodal medical models that processes both image and text inputs. It employs a branch-merge architecture consisting of three specialized sub-domain evaluation models (Expression, Medical Knowledge Correctness, Patient Question Relevance) and a conclusion evaluation model. The model is trained using an Efficient-RTDPO strategy that freezes lower LLM layers to save computation time while maintaining accuracy through reward token-based direct preference optimization. The training dataset is synthetically generated using GPT-3.5-Turbo with reliable medical evaluation criteria, verified through a two-step process including format checking and human sampling content verification.

## Key Results
- Achieves 82.71% accuracy on multimodal medical data and 72.59% on text-only data
- Outperforms GPT-4-Turbo by 5.3% in conclusion evaluation accuracy
- Demonstrates strong correlation with human judgments and reduced position bias compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
The branch-merge architecture enables both detailed multi-criteria evaluation and a unified final score by aggregating three specialized sub-evaluators. Three separate evaluation models handle Expression (EXP), Medical Knowledge Correctness (MKC), and Patient Question Relevance (PQR). Their outputs are concatenated with the original inputs and fed into a conclusion model that produces the final evaluation. Combining specialized sub-evaluations yields more accurate and nuanced overall assessments than a single monolithic model.

### Mechanism 2
Efficient-RTDPO training strategy saves computation time without sacrificing evaluation accuracy by freezing lower LLM layers and applying reward tokens for direct preference optimization. Lower layers of the LLM are frozen during training to reduce parameters updated per batch. RTDPO prepends reward tokens ([Good]/[Bad]) to positive/negative samples, steering the model toward more accurate evaluations. Freezing lower layers captures general linguistic features less relevant to domain-specific evaluation, so freezing them retains performance while speeding training.

### Mechanism 3
The synthetic instruction dataset collected from GPT-3.5-Turbo with reliable medical evaluation criteria provides high-quality training signals that correlate strongly with human judgments. The dataset is constructed using detailed criteria covering Expression, Medical Knowledge Correctness, and Patient Question Relevance. Two-step verification (format check + sampling content verification) ensures reliability. LLM-generated evaluations, when guided by rigorous criteria and verified, can approximate human judgments at scale.

## Foundational Learning

- Concept: Multimodal instruction tuning
  - Why needed here: ACE-M3 must process both image and text inputs and generate coherent evaluations, requiring joint vision-language modeling
  - Quick check question: What mechanism connects visual features to the LLM's embedding space in ACE-M3?

- Concept: Preference optimization (DPO/RTDPO)
  - Why needed here: Direct optimization toward human-aligned evaluation preferences improves model accuracy beyond standard supervised learning
  - Quick check question: How does RTDPO differ from vanilla DPO in ACE-M3's training?

- Concept: Evaluation metrics design (beyond ROUGE/BLEU)
  - Why needed here: Medical evaluations require domain-specific criteria (e.g., factual accuracy, empathy) that lexical overlap metrics cannot capture
  - Quick check question: Which three major sub-domain criteria does ACE-M3 evaluate?

## Architecture Onboarding

- Component map: Vision encoder (CLIP/ViT-L/14) → projection matrix → LLM token embeddings → three branch evaluators (EXP, MKC, PQR) → conclusion model → final score

- Critical path: Image → visual features → projected embeddings → LLM → branch evaluators → conclusion model → final evaluation

- Design tradeoffs:
  - Branch-merge vs. single monolithic evaluator: better granularity vs. higher complexity
  - Freezing lower layers: faster training vs. potential accuracy loss (mitigated by RTDPO)
  - Synthetic dataset: scalable training data vs. potential LLM bias

- Failure signatures:
  - Incorrect output formatting → LLaVA models fail on image-text data
  - Low accuracy on one modality → check frozen layers or vision encoder choice
  - Position bias → swap response positions and compare scores

- First 3 experiments:
  1. Train with all layers unfrozen → measure accuracy vs. training time
  2. Replace RTDPO with vanilla DPO → compare evaluation accuracy
  3. Use domain-specific encoders (PubMedCLIP, BiomedCLIP) → measure accuracy improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ACE-M3's performance compare to other specialized medical MLLM evaluation models beyond PandaLM, particularly on multimodal data?
- Basis in paper: The paper mentions that PandaLM is used for text-only comparison due to lack of instruction fine-tuned multimodal evaluation models, suggesting a gap in direct multimodal comparisons
- Why unresolved: The paper only compares ACE-M3 against PandaLM for text-only data, and lacks comparison against other potential multimodal evaluation models that may have been developed but are not mentioned
- What evidence would resolve it: Experimental results showing ACE-M3's performance against a broader range of specialized medical MLLM evaluation models on multimodal datasets

### Open Question 2
- Question: What is the impact of ACE-M3's bias towards longer responses on its evaluation accuracy for different types of medical questions (e.g., yes/no vs. open-ended)?
- Basis in paper: The paper discusses verbosity bias, noting ACE-M3 prefers longer responses, but doesn't explore how this affects evaluation accuracy across different question types
- Why unresolved: While the paper identifies verbosity bias, it doesn't investigate whether this preference impacts the model's accuracy differently for various medical question formats
- What evidence would resolve it: Analysis of ACE-M3's evaluation accuracy across different medical question types (yes/no, short answer, open-ended) when comparing longer vs. shorter responses

### Open Question 3
- Question: How would ACE-M3's performance change if different vision encoders were used, particularly domain-specific encoders like BiomedCLIP vs. general encoders like CLIP?
- Basis in paper: The paper mentions that BiomedCLIP and PubMedCLIP offer certain improvements over CLIP, indicating that encoder choice impacts performance
- Why unresolved: The paper only tests a few specific encoders (CLIP, PubMedCLIP, BiomedCLIP) and doesn't explore the full range of possible vision encoders or their relative performance
- What evidence would resolve it: Comprehensive benchmarking of ACE-M3 using various vision encoders, including domain-specific and general-purpose options, to quantify performance differences

## Limitations
- Model architecture details and exact implementation of MedLlama22 base model are not fully specified
- Efficient-RTDPO training strategy lacks complete methodological detail including specific hyperparameter settings
- Synthetic dataset construction process does not provide sufficient transparency about potential biases

## Confidence

**High Confidence Claims:**
- The branch-merge architecture concept and its general effectiveness in multimodal evaluation
- The need for domain-specific medical evaluation criteria beyond standard metrics
- The general advantage of freezing lower LLM layers for computational efficiency

**Medium Confidence Claims:**
- The specific 82.71% accuracy on multimodal data (dependent on exact dataset splits and evaluation protocols)
- The 5.3% improvement over GPT-4-Turbo (requires independent verification of baseline comparisons)
- The effectiveness of RTDPO vs. vanilla DPO (implementation details matter significantly)

**Low Confidence Claims:**
- The exact contribution of each architectural component to overall performance
- The generalizability of results to other medical domains beyond the evaluated benchmarks
- The long-term stability and robustness of the model across different clinical scenarios

## Next Checks

1. **Architecture Ablation Study**: Conduct controlled experiments to isolate the contribution of the branch-merge architecture versus a monolithic evaluator, measuring both accuracy and computational efficiency.

2. **Human Expert Validation**: Commission independent medical professionals to evaluate ACE-M3's assessments against gold-standard human judgments across diverse medical scenarios, particularly focusing on edge cases.

3. **Cross-Domain Generalization**: Test ACE-M3 on medical datasets from different specialties and geographical regions to assess whether the model's strong performance generalizes beyond the training distribution.