---
ver: rpa2
title: 'MoE-I$^2$: Compressing Mixture of Experts Models through Inter-Expert Pruning
  and Intra-Expert Low-Rank Decomposition'
arxiv_id: '2411.01016'
source_url: https://arxiv.org/abs/2411.01016
tags:
- pruning
- layer
- experts
- arxiv
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MoE-I2, a two-stage compression framework for
  Mixture-of-Experts (MoE) large language models (LLMs) that addresses the high deployment
  costs of these models. The framework consists of inter-expert pruning using layer-wise
  genetic search and block-wise KT-Receptive Field, followed by intra-expert low-rank
  decomposition with non-uniform ranks based on expert importance.
---

# MoE-I$^2$: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition

## Quick Facts
- arXiv ID: 2411.01016
- Source URL: https://arxiv.org/abs/2411.01016
- Reference count: 8
- Reduces expert parameters by >50% while maintaining performance with up to 1.28× speedup and 51% memory savings

## Executive Summary
This paper introduces MoE-I2, a two-stage compression framework for Mixture-of-Experts (MoE) large language models that addresses their high deployment costs. The framework first applies inter-expert pruning using layer-wise genetic search with block-wise KT-Receptive Field to identify and remove less important experts, followed by intra-expert low-rank decomposition with non-uniform ranks based on expert importance. The method achieves significant parameter reduction (>50%) while maintaining or even improving zero-shot performance across nine classification tasks and two perplexity benchmarks, outperforming existing approaches like EEP and Wanda.

## Method Summary
MoE-I2 is a two-stage compression framework that first performs inter-expert pruning using layer-wise genetic search combined with block-wise KT-Receptive Field to efficiently identify experts to remove. The second stage applies intra-expert low-rank decomposition with non-uniform ranks determined by expert importance. Layer importance is calculated through loss degradation analysis, guiding both the non-uniform pruning ratios and the rank assignments for decomposition. The compressed model is then fine-tuned using LoRA for performance recovery. The approach is evaluated on three MoE models (Mixtral-8×7B, Qwen1.5-MoE-A2.7B, DeepSeek-V2-Lite) across nine zero-shot tasks, demonstrating superior performance compared to existing methods while achieving substantial parameter reduction and inference speedups.

## Key Results
- Achieves >50% reduction in expert parameters while maintaining or improving zero-shot accuracy
- Delivers up to 1.28× inference speedup and 51% memory savings
- Outperforms EEP and Wanda methods in both perplexity and zero-shot accuracy metrics
- Maintains performance across diverse MoE architectures (14.3B, 16B, and 47B parameter models)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-uniform layer-wise pruning improves performance by targeting parameter importance
- Mechanism: The framework analyzes each MoE layer's importance by measuring average loss degradation when pruning individual experts within that layer. This creates a layer-wise importance score (Ii), which then determines how many experts to prune in each layer rather than using uniform pruning ratios across all layers.
- Core assumption: Layer importance measured through loss degradation correlates with the layer's contribution to model performance and can guide effective pruning decisions
- Evidence anchors:
  - [abstract]: "we analyze the importance of each layer and propose the Layer-wise Genetic Search and Block-wise KT-Reception Field with the non-uniform pruning ratio"
  - [section]: "The layer importance of i-th layer, denoted by Ii, is defined as the average loss degradation by removing individual experts within this layer"
  - [corpus]: Weak - no direct corpus evidence supporting this specific importance analysis mechanism
- Break condition: If layer importance scores don't accurately reflect actual performance impact, or if the loss degradation measurement doesn't capture long-range dependencies effectively

### Mechanism 2
- Claim: Layer-wise genetic search with block-wise KT-Receptive Field reduces search complexity while finding better pruning combinations
- Mechanism: Instead of brute-force search across all possible expert combinations (which is computationally prohibitive for models with 60-64 experts per layer), genetic search efficiently explores the solution space by maintaining a population of candidate combinations, selecting based on loss, and generating offspring through mutation. The block-wise KT-Receptive Field extends the search scope from single layers to T consecutive layers, enabling better global optimization.
- Core assumption: Genetic search can effectively approximate the optimal expert pruning combinations while significantly reducing search time compared to brute-force methods
- Evidence anchors:
  - [section]: "we leverage Genetic Search (Grefenstette, 1993; Alam et al., 2020) with KT-Receptive Field methods to enhance search efficiency and concurrently identify the least impactful combinations of experts on a more global scale"
  - [section]: "If 25% of experts need to be pruned, (Lu et al., 2024) needs to traverse C60^15 and C64^16 times for each layer respectively, which is unacceptable in terms of time consumption"
  - [corpus]: Weak - no direct corpus evidence supporting this specific genetic search mechanism
- Break condition: If genetic search gets stuck in local optima or if the fitness function (loss calculation) doesn't accurately represent true model performance

### Mechanism 3
- Claim: Non-uniform expert decomposition with varying ranks preserves performance while achieving higher compression
- Mechanism: After inter-expert pruning, the framework analyzes individual expert importance and assigns non-uniform ranks (Rij) based on importance scores. More important experts receive higher ranks (more parameters retained), while less important experts get lower ranks. This targeted low-rank decomposition within each expert allows for fine-grained compression while maintaining performance.
- Core assumption: Expert importance measured through loss degradation correlates with the expert's contribution to model performance and can guide effective rank assignment for decomposition
- Evidence anchors:
  - [abstract]: "Second, in the intra-expert decomposition stage, we apply the low-rank decomposition to further compress the parameters within the remaining experts"
  - [section]: "we measure the importance of each expert and assign non-uniform ranks accordingly"
  - [corpus]: Weak - no direct corpus evidence supporting this specific importance-based rank assignment mechanism
- Break condition: If expert importance scores don't accurately reflect actual performance impact, or if low-rank decomposition causes excessive information loss for important experts

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture and dynamic routing
  - Why needed here: Understanding how MoE models activate only a subset of experts per input token is crucial for grasping why pruning and compression strategies are different from dense models
  - Quick check question: In a typical MoE layer with 8 experts where 2 are activated, what percentage of parameters are active during inference?

- Concept: Low-rank decomposition and singular value decomposition (SVD)
  - Why needed here: The intra-expert compression stage uses low-rank decomposition to further reduce parameters within remaining experts after pruning
  - Quick check question: If a weight matrix has dimensions 1000×1000 and we apply SVD with rank 500, how many parameters does the decomposed representation have compared to the original?

- Concept: Genetic algorithms and optimization
  - Why needed here: The inter-expert pruning stage uses genetic search to efficiently find optimal expert combinations to prune without exhaustive search
  - Quick check question: What are the three main operations in a genetic algorithm (selection, crossover/mutation, and what else)?

## Architecture Onboarding

- Component map:
  - Layer Importance Analysis -> Inter-Expert Pruning -> Intra-Expert Decomposition -> Fine-tuning Stage -> Evaluation Pipeline

- Critical path: Layer Importance Analysis → Inter-Expert Pruning → Intra-Expert Decomposition → Fine-tuning → Evaluation
- Design tradeoffs: 
  - Search efficiency vs. solution quality (genetic search vs. brute-force)
  - Compression ratio vs. performance preservation (uniform vs. non-uniform pruning/ranks)
  - Model size reduction vs. inference speedup (structured vs. unstructured pruning)
- Failure signatures:
  - Significant performance degradation after pruning/decomposition
  - Slow convergence or poor results from genetic search
  - Inconsistent importance scores across different calibration datasets
  - Memory errors during low-rank decomposition due to rank assignment issues
- First 3 experiments:
  1. Run Layer Importance Analysis on a small MoE model (like Mixtral-8×7B) with calibration data to verify importance score distribution matches expectations
  2. Test Inter-Expert Pruning with genetic search (K=3, T=3) on a single layer to confirm it finds better combinations than random pruning
  3. Apply Intra-Expert Decomposition with non-uniform ranks on a single expert to verify rank assignment preserves more important parameters

## Open Questions the Paper Calls Out
None

## Limitations
- The paper's claims about performance preservation through non-uniform layer-wise pruning and expert importance analysis lack strong empirical validation, with only weak corpus evidence supporting these mechanisms
- The genetic search approach, while computationally efficient, may converge to suboptimal solutions without rigorous comparison to baseline search methods
- The effectiveness of non-uniform low-rank decomposition relies heavily on accurate importance scoring, which is not thoroughly validated across diverse model architectures and tasks

## Confidence
- High confidence: The general framework architecture and two-stage compression approach
- Medium confidence: The effectiveness of genetic search for expert pruning
- Medium confidence: The performance preservation claims with 50% expert parameter reduction
- Low confidence: The specific mechanisms of layer importance analysis and expert decomposition without more detailed ablation studies

## Next Checks
1. Conduct ablation studies comparing uniform vs. non-uniform pruning ratios to quantify the actual benefit of layer importance analysis
2. Implement and compare genetic search against brute-force and random search baselines to verify computational efficiency claims
3. Test the compression framework on additional MoE architectures (e.g., GLaM, Switch Transformers) to assess generalizability beyond the three models evaluated