---
ver: rpa2
title: 'Leveraging Biases in Large Language Models: "bias-kNN'''' for Effective Few-Shot
  Learning'
arxiv_id: '2401.09783'
source_url: https://arxiv.org/abs/2401.09783
tags:
- bias-knn
- biases
- language
- learning
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes a novel method called "bias-kNN" to improve
  few-shot learning in large language models (LLMs) by leveraging their inherent biases.
  Instead of trying to minimize or correct these biases, the method harnesses them
  as primary features for k-nearest neighbors (kNN) classification, supplemented with
  gold labels.
---

# Leveraging Biases in Large Language Models: "bias-kNN'' for Effective Few-Shot Learning

## Quick Facts
- arXiv ID: 2401.09783
- Source URL: https://arxiv.org/abs/2401.09783
- Reference count: 0
- Key outcome: bias-kNN leverages LLM biases as features for kNN classification, outperforming conventional in-context learning in few-shot scenarios

## Executive Summary
This paper introduces "bias-kNN," a novel method that leverages inherent biases in large language models (LLMs) to improve few-shot learning performance. Instead of treating biases as a problem to be solved, the approach harnesses them as discriminative features for k-nearest neighbors (kNN) classification, supplemented with gold labels. Evaluations across diverse text classification datasets and GPT-2 model sizes demonstrate that bias-kNN consistently outperforms conventional in-context learning, achieving higher minimum accuracy and greater stability with as few as 3 labeled samples.

The method transforms the challenge of LLM biases into an advantage by using biased probability outputs as features in a kNN framework. This approach not only improves classification accuracy but also demonstrates robustness to variations in templates and verbalizers, suggesting it can simplify the process of selecting optimal prompts. The research presents a unique perspective on bias in LLMs, showing how systematic biases can be repurposed as useful signals for sample inference in few-shot learning scenarios.

## Method Summary
The bias-kNN method works by first generating biased probability outputs from an LLM for each input sample using templates and verbalizers. These biased outputs serve as features for kNN retrieval, where the k most similar samples are identified based on cosine distance. The final label prediction is determined through majority voting over the gold labels of the retrieved neighbors. This approach effectively combines the discriminative power of biased features with the correctness of labeled data, leading to improved performance in few-shot settings. The method was evaluated across six text classification datasets using different GPT-2 model sizes, demonstrating consistent improvements over conventional in-context learning.

## Key Results
- Bias-kNN achieves higher minimum accuracy and greater stability compared to zero-shot learning and raw in-context learning across all classification tasks
- Using GPT-2-large with just 3 labeled samples, bias-kNN outperforms traditional methods in few-shot scenarios
- The approach demonstrates robustness to variations in templates and verbalizers, reducing sensitivity to prompt engineering choices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bias-kNN method leverages the inherent label bias in LLM outputs to create discriminative features that improve classification accuracy in few-shot settings.
- Mechanism: By using the biased probability outputs from the LLM as features for kNN classification, the method captures directional differences between classes that are obscured in traditional decision boundaries. This transforms the bias from a problem into a signal.
- Core assumption: The bias in LLM outputs is systematic and consistent enough across similar samples to be useful as a feature for nearest neighbor retrieval.
- Evidence anchors:
  - [abstract] "This approach capitalizes on the biased outputs, harnessing them as primary features for kNN and supplementing with gold labels."
  - [section] "As depicted in Figure 1, biases in LLMs frequently result in lower probabilities for verbalizers, causing dense clusters and category overlaps. However, the evident directionality differences between categories hint at an intriguing opportunity: utilizing biased outputs together with Nearest Neighbor methods to enhance sample inference."
  - [corpus] Weak evidence; neighboring papers discuss bias in LLMs but not specifically using bias as features for kNN.

### Mechanism 2
- Claim: Supplementing kNN with gold labels mitigates the noise introduced by using biased features, leading to more stable and accurate predictions.
- Mechanism: The kNN retrieval finds the k most similar samples based on biased features, but the final label is determined by majority vote over gold labels, combining the discriminative power of bias with the correctness of labeled data.
- Core assumption: The gold-labeled samples in the kNN datastore are representative and accurate enough to correct or reinforce the decisions made based on biased features.
- Evidence anchors:
  - [abstract] "...harnessing them as primary features for kNN and supplementing with gold labels."
  - [section] "The definitive label ypred for the input sample is then ascertained through a majority vote."
  - [corpus] Weak evidence; neighboring papers focus on bias mitigation or detection, not on using gold labels to supplement biased features in kNN.

### Mechanism 3
- Claim: The method reduces sensitivity to template and verbalizer selection by making the classification decision based on kNN retrieval over multiple samples rather than relying on a single model prediction.
- Mechanism: By aggregating over k nearest neighbors, the method averages out the variability introduced by different templates and verbalizers, leading to more robust performance across prompt variations.
- Core assumption: The bias patterns are somewhat consistent across different templates and verbalizers, so that nearest neighbors can still be meaningfully retrieved even with prompt variations.
- Evidence anchors:
  - [abstract] "...demonstrates robustness across a spectrum of samples, templates and verbalizers."
  - [section] "Moreover, as depicted in Figure 4, altering templates can lead to variability in the performance of bias-kNN. However, there’s a notable enhancement in performance when m > 2."
  - [corpus] Weak evidence; neighboring papers do not discuss robustness to template and verbalizer variations in the context of kNN methods.

## Foundational Learning

- Concept: k-Nearest Neighbors (kNN) algorithm
  - Why needed here: The method relies on kNN to retrieve similar samples based on biased features and aggregate their labels.
  - Quick check question: How does kNN determine the "nearest" neighbors, and what distance metric is typically used?

- Concept: Bias in large language models
  - Why needed here: Understanding how biases manifest in LLM outputs is crucial to leveraging them as features.
  - Quick check question: What are the common types of biases in LLMs, and how do they affect probability distributions?

- Concept: Prompt engineering with templates and verbalizers
  - Why needed here: The method uses templates and verbalizers to structure inputs for the LLM, and its robustness to these is a key feature.
  - Quick check question: How do templates and verbalizers influence the LLM's output probabilities?

## Architecture Onboarding

- Component map: Input text -> Template + Verbalizer -> LLM -> Biased probabilities -> kNN retrieval -> k nearest samples -> Majority vote over gold labels -> Final prediction

- Critical path:
  1. Input text → Template + Verbalizer → LLM → Biased probabilities
  2. Biased probabilities → kNN retrieval → k nearest samples
  3. k nearest samples → Majority vote over gold labels → Final prediction

- Design tradeoffs:
  - Using biased features vs. calibrated features: biased features may be more discriminative but noisier
  - k value in kNN: larger k increases stability but may reduce sensitivity to local patterns
  - Number of labeled samples (m): more samples improve robustness but increase storage and computation

- Failure signatures:
  - High variance in predictions across different prompt templates or verbalizers
  - Degradation in performance when k is too small or too large
  - Sensitivity to the choice of distance metric in kNN

- First 3 experiments:
  1. Compare bias-kNN performance using biased features vs. calibrated features on a small dataset.
  2. Evaluate the effect of different k values on prediction stability and accuracy.
  3. Test robustness by varying templates and verbalizers and measuring performance variance.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to GPT-2 models and specific text classification tasks
- Relatively small number of trials and limited hyperparameter exploration (e.g., k values, distance metrics)
- Assumption that bias patterns remain sufficiently consistent across prompt variations may not generalize to all LLM architectures or task types

## Confidence
- High: Core claim that bias-kNN consistently outperforms in-context learning is well-supported by reported results
- Medium: Method's robustness to template and verbalizer variations is demonstrated but not thoroughly validated
- Medium: Claim that bias-kNN simplifies prompt engineering is supported but not systematically explored

## Next Checks
1. Test bias-kNN with larger k values and alternative distance metrics (e.g., Euclidean, Manhattan) to assess sensitivity to retrieval parameters
2. Evaluate performance on additional model architectures (e.g., GPT-3, BERT) to verify generalizability beyond GPT-2
3. Conduct systematic ablation studies on template and verbalizer variations to quantify the method's robustness across the full prompt engineering space