---
ver: rpa2
title: The use of large language models to enhance cancer clinical trial educational
  materials
arxiv_id: '2412.01955'
source_url: https://arxiv.org/abs/2412.01955
tags:
- trial
- clinical
- pmid
- page
- trials
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated using GPT-4 to generate patient-friendly
  educational content from clinical trial informed consent forms. Zero-shot learning
  was used to create trial summaries, and one-shot learning was used to develop multiple-choice
  questions.
---

# The use of large language models to enhance cancer clinical trial educational materials

## Quick Facts
- arXiv ID: 2412.01955
- Source URL: https://arxiv.org/abs/2412.01955
- Reference count: 0
- GPT-4 can generate readable clinical trial summaries and accurate multiple-choice questions from informed consent forms, but requires human oversight to catch hallucinations

## Executive Summary
This study demonstrates that GPT-4 can effectively generate patient-friendly educational materials from clinical trial informed consent forms without extensive fine-tuning. Using zero-shot learning for summaries and one-shot learning for multiple-choice questions, the approach produces readable and comprehensive trial information that potentially improves patient understanding and interest in clinical trials. While the system shows promise for scalable educational content generation, hallucinations and inaccuracies require ongoing human oversight to ensure safety.

## Method Summary
The study used 91 interventional cancer clinical trial informed consent forms from ClinicalTrials.gov, applying zero-shot learning with GPT-4 to generate trial summaries based on basic elements of informed consent. For multiple-choice questions, one-shot learning was employed using in-context examples. Two prompting approaches were tested: direct summarization and sequential extraction-then-summarization. Patient surveys and crowdsourced annotations (via DiagnosUs) evaluated the generated materials' effectiveness.

## Key Results
- GPT-4-generated summaries were readable and comprehensive, potentially improving patient understanding
- Multiple-choice questions demonstrated high accuracy and agreement with crowdsourced annotators
- Hallucinations occurred in approximately 5-10% of cases, requiring human oversight
- Patient survey responses showed improved understanding and interest in clinical trials

## Why This Works (Mechanism)

### Mechanism 1
- GPT-4 can generate readable and comprehensive clinical trial summaries from informed consent forms using straightforward prompting methods.
- The model leverages zero-shot learning to directly summarize complex ICF text into plain language while preserving key trial elements required for informed consent.
- Core assumption: The model's training data includes sufficient exposure to medical consent-style text, enabling it to generalize to ICF summarization without fine-tuning.
- Evidence anchors: GPT4-generated summaries were both readable and comprehensive; explored two approaches to generating trial summaries.

### Mechanism 2
- GPT-4 can generate accurate multiple-choice questions that assess patient understanding of trial-specific details.
- Using one-shot learning, GPT-4 is provided with expert-written question-answer pairs and the target ICF text, then generates additional MCQAs that align with the ICF content.
- Core assumption: The provided in-context examples are representative enough for the model to generalize question generation to new trials.
- Evidence anchors: Multiple-choice questions demonstrated high accuracy and agreement with crowdsourced annotators; adopted in-context learning method.

### Mechanism 3
- The sequential prompting approach (extraction then summarization) reduces hallucinations compared to direct summarization.
- By first extracting relevant text sections based on consent elements and then summarizing only that extracted text, the model is constrained to factual content present in the ICF.
- Core assumption: Constraining the model's input to pre-identified relevant sections limits its ability to fabricate information.
- Evidence anchors: Sequential approach found to reduce inaccuracies; hallucinations tended to occur for topics not described in the given ICF.

## Foundational Learning

- **Concept: Zero-shot and one-shot learning in LLMs**
  - Why needed here: Understanding these learning paradigms explains how GPT-4 can perform summarization and question generation without extensive fine-tuning on clinical trial data
  - Quick check question: What is the difference between zero-shot and one-shot learning in the context of this study?

- **Concept: Informed consent elements and their importance**
  - Why needed here: The study's prompts are based on key elements of informed consent; understanding these elements is crucial for evaluating summary quality
  - Quick check question: Name three key elements that must be included in an informed consent summary according to the Revised Common Rule

- **Concept: Hallucination and inaccuracy in LLM outputs**
  - Why needed here: The study identifies hallucinations as a key risk; understanding this phenomenon is essential for implementing appropriate human oversight
  - Quick check question: What is a hallucination in the context of LLM outputs, and why is it particularly problematic for medical applications?

## Architecture Onboarding

- **Component map**: ClinicalTrials.gov API → PDF text extraction (PyMuPDF) → ICF text preprocessing → GPT-4 API calls for summarization and MCQ generation with prompt templates → Clinician review → Patient survey → Crowdsourced annotation (DiagnosUs)
- **Critical path**: ICF retrieval → Prompt generation → GPT-4 response → Human review → Patient survey
- **Design tradeoffs**: Simplicity vs. accuracy (direct summarization is simpler but may hallucinate more than sequential extraction); Automation vs. oversight (fully automated generation is faster but requires human review to catch errors); Generic vs. trial-specific (generic prompts work across trials but may miss trial-specific nuances)
- **Failure signatures**: Missing key consent elements in summaries; MCQAs with incorrect answers or unanswerable questions; Low agreement between human readers and GPT-4-assigned answers; Hallucinations when ICF lacks adequate coverage of requested topics
- **First 3 experiments**: Compare direct vs. sequential prompting approaches on a small set of ICFs to measure hallucination rates; Generate MCQAs for a single ICF and evaluate with a small group of annotators to establish baseline accuracy; Test multi-LLM verification by having GPT-4o, Claude, and Gemini answer the same MCQAs and compare agreement with GPT-4's answers

## Open Questions the Paper Calls Out

### Open Question 1
- How can we systematically reduce hallucinations in LLM-generated clinical trial summaries while maintaining readability and comprehensiveness?
- Basis: The paper identifies hallucinations as a key challenge, particularly when ICFs lack adequate content for requested topics, and notes that even the sequential prompting approach only partially mitigates this issue.
- Why unresolved: Current prompting methods show improvement but still produce hallucinations in approximately 5-10% of cases, and there's no established framework for balancing accuracy with patient-friendly language.
- What evidence would resolve it: Comparative studies testing different prompting strategies (e.g., chain-of-thought prompting, knowledge retrieval augmentation) with quantitative hallucination rates and readability scores across diverse ICF types.

### Open Question 2
- What is the optimal balance between automated LLM generation and human oversight for clinical trial educational materials to ensure safety while maximizing efficiency?
- Basis: The paper emphasizes that implementation requires a "human-in-the-loop" to avoid misinformation risks, but doesn't specify what level of human review is necessary or how to scale it effectively.
- Why unresolved: There's tension between the need for thorough human review (time-consuming) and the goal of creating diverse, scalable educational resources, with no established guidelines for appropriate oversight levels.
- What evidence would resolve it: Empirical studies comparing different oversight models (full review, spot-checking, post-publication monitoring) with metrics on error rates, time efficiency, and patient outcomes.

### Open Question 3
- How do patient preferences and comprehension vary across different formats and levels of detail in LLM-generated clinical trial educational materials?
- Basis: The patient survey showed generally positive responses but had substantial drop-off for non-BROADBAND trials, suggesting potential variations in effectiveness across different trial types or presentation formats.
- Why unresolved: The study only evaluated five trial summaries with a small sample size, and didn't explore personalization options or test different formats (e.g., video, interactive, tiered detail levels).
- What evidence would resolve it: Large-scale A/B testing of multiple formats and detail levels with diverse patient populations, measuring comprehension, engagement, and preference patterns.

## Limitations
- Prompt engineering transparency: The study does not provide the exact prompt templates used for zero-shot and one-shot learning approaches
- Generalizability beyond cancer trials: The approach was validated on cancer clinical trials only; effectiveness for other disease areas remains untested
- Human oversight requirements: While the study notes hallucinations require human oversight, it does not quantify the extent of human review needed for production deployment

## Confidence
- **High Confidence**: GPT-4 can generate readable and comprehensive clinical trial summaries from informed consent forms; GPT-4 can generate accurate multiple-choice questions that assess patient understanding of trial-specific details; The sequential prompting approach reduces hallucinations compared to direct summarization
- **Medium Confidence**: Generated materials improve patients' understanding and interest in clinical trials (based on small-scale patient surveys); Multi-LLM verification effectively catches hallucinations and errors (method described but limited validation)
- **Low Confidence**: Implementation with minimal trial-specific engineering is feasible at scale (not tested beyond the 91 trials); The approach generalizes to non-cancer clinical trials (not evaluated)

## Next Checks
1. **Prompt reproducibility test**: Implement the study's approach using only the information provided in the paper, then compare outputs against the published examples to assess reproducibility.
2. **Multi-disease validation**: Apply the same methodology to clinical trials from different therapeutic areas (e.g., cardiovascular, neurology) to evaluate generalizability.
3. **Human review burden quantification**: Measure the time and expertise required for human review of LLM-generated materials across a larger sample size to establish practical implementation requirements.