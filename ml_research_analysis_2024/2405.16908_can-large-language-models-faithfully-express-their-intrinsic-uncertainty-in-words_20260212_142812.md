---
ver: rpa2
title: Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in
  Words?
arxiv_id: '2405.16908'
source_url: https://arxiv.org/abs/2405.16908
tags:
- uncertainty
- arxiv
- answer
- confidence
- decisiveness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method for evaluating whether large language
  models (LLMs) faithfully communicate their uncertainty in natural language. The
  authors formalize this as measuring the gap between the model's intrinsic confidence
  in its assertions and the decisiveness with which these assertions are conveyed
  in the response.
---

# Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?

## Quick Facts
- arXiv ID: 2405.16908
- Source URL: https://arxiv.org/abs/2405.16908
- Authors: Gal Yona; Roee Aharoni; Mor Geva
- Reference count: 18
- Key outcome: LLMs perform poorly at faithfully conveying uncertainty, often answering decisively even when uncertain, and failing to align hedging expressions with actual confidence levels.

## Executive Summary
This paper introduces a framework for evaluating whether large language models faithfully communicate their intrinsic uncertainty when answering questions. The authors formalize this as measuring the gap between a model's intrinsic confidence in its assertions and the decisiveness with which these assertions are conveyed in natural language. Using a judge LLM to quantify both decisiveness and confidence, they evaluate several leading models (GPT-3.5, GPT-4, Gemini variants) on knowledge-intensive question answering tasks. Results show that models struggle to align their hedging expressions with actual uncertainty levels, answering decisively even when uncertain, though some prompting methods show modest improvements in faithfulness.

## Method Summary
The method uses a "judge" LLM (Gemini Ultra) to evaluate faithfulness by quantifying both the decisiveness of assertions in responses (based on hedging expressions) and the model's intrinsic confidence (by checking consistency across multiple sampled answers). Faithfulness is calculated as the gap between these two scores, with the conditional mean faithful generation (cMFG) providing a single metric for model comparison. The framework is applied to knowledge-intensive question answering datasets (PopQA and Natural Questions) using multiple prompting methods including standard decoding and uncertainty-focused approaches.

## Key Results
- Leading LLMs (GPT-3.5, GPT-4, Gemini variants) show poor faithfulness in expressing uncertainty, with high decisiveness scores even when confidence is low
- Standard prompting methods yield cMFG scores around 0.3-0.4, indicating weak alignment between expressed and intrinsic uncertainty
- Uncertainty-focused prompting methods provide modest improvements (cMFG up to 0.63-0.7) but still show weak correlation between decisiveness and confidence
- Models fail to reliably use hedging expressions when uncertain, often answering decisively despite low confidence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Faithful response uncertainty can be quantified as the gap between the decisiveness of an assertion and the model's intrinsic confidence in it.
- **Mechanism:** By defining a faithfulness score that penalizes both excessive hedging (low decisiveness with high confidence) and lack of hedging (high decisiveness with low confidence), the evaluation captures whether the model's language accurately reflects its internal uncertainty.
- **Core assumption:** The model's intrinsic confidence can be approximated by the consistency of its responses across multiple samples.
- **Evidence anchors:** [abstract] "We formalize faithful response uncertainty based on the gap between the model's intrinsic confidence in the assertions it makes and the decisiveness by which they are conveyed." [section 2] "faithfulnessM (R; Q) ≡ 1−(1/|A(R)|)ΣA∈A(R)|dec(A; R, Q) − confM (A)|"

### Mechanism 2
- **Claim:** A judge LLM can reliably score both the decisiveness of an assertion and the model's intrinsic confidence.
- **Mechanism:** By prompting a capable LLM (Gemini Ultra) with few-shot examples to extract assertions and their decisiveness, and to judge consistency between sampled answers, we obtain numeric scores that correlate with human judgment.
- **Core assumption:** The judge LLM's perception of decisiveness and contradiction aligns with human perception.
- **Evidence anchors:** [section 3] "For decisiveness (Eq. 1), we randomly sample 100 model answers... Fig. 2 shows for each hedging expression the mean decisiveness score versus the distribution of perceived probabilities humans assigned to it." [section 3] "For confidence (Eq. 2), we compare the confidence scores for 100 randomly selected examples... We observe a high correlation of 0.97 between the two scores."

### Mechanism 3
- **Claim:** Standard decoding and prompting methods fail to align linguistic decisiveness with intrinsic confidence.
- **Mechanism:** Experiments show that even when models are prompted to express uncertainty, their hedging expressions are not well-correlated with their actual confidence levels, as evidenced by low cMFG scores.
- **Core assumption:** The cMFG metric effectively captures the alignment (or lack thereof) between expressed uncertainty and intrinsic uncertainty.
- **Evidence anchors:** [section 5] "For the non-Vanilla baselines, there is a small increase in cMFG, with maximal scores reaching 0.63 for GPT-4 and 0.7 and Gemini-Ultra." [section 5] "Importantly, however, the correlation between decisiveness and confidence is weak; see Fig. 4."

## Foundational Learning

- **Concept:** Aleatoric vs. epistemic uncertainty
  - **Why needed here:** The paper focuses on aleatoric (model) uncertainty, distinguishing it from epistemic (data) uncertainty which arises from ambiguous questions. Understanding this distinction is crucial for interpreting the results and limitations.
  - **Quick check question:** What type of uncertainty does the paper focus on measuring, and how does it differ from uncertainty arising from ambiguous questions?

- **Concept:** Hedging expressions and perceived decisiveness
  - **Why needed here:** The paper uses hedging expressions to quantify linguistic uncertainty, and relies on a judge LLM to assign perceived decisiveness scores. Understanding how hedging works and how it can be quantified is essential for understanding the methodology.
  - **Quick check question:** How does the paper quantify the decisiveness of an assertion, and what role do hedging expressions play in this quantification?

- **Concept:** Consistency-based confidence estimation
  - **Why needed here:** The paper estimates a model's intrinsic confidence by examining the consistency of its responses across multiple samples. Understanding this approach and its limitations is important for interpreting the results.
  - **Quick check question:** How does the paper estimate a model's intrinsic confidence, and what are the potential limitations of this approach?

## Architecture Onboarding

- **Component map:** Judge LLM (Gemini Ultra) -> Decisiveness scoring -> Confidence scoring -> Faithfulness calculation -> cMFG metric
- **Critical path:** (1) Generate responses using prompting method, (2) Extract assertions and score decisiveness using judge LLM, (3) Sample additional answers and extract assertions, (4) Judge consistency between original and sampled assertions using judge LLM, (5) Compute faithfulness metrics
- **Design tradeoffs:** Using a judge LLM for scoring is flexible and doesn't require model access but may introduce biases. Sampling multiple answers for confidence estimation is more robust than single-answer approaches but is computationally expensive. Focusing on aleatoric uncertainty simplifies the evaluation but limits generalizability.
- **Failure signatures:** Low cMFG scores indicate poor alignment between expressed and intrinsic uncertainty. High disagreement between judge LLM and human scores on decisiveness indicates potential bias in the judge. High variance in confidence scores across samples indicates poor calibration or sensitivity to sampling.
- **First 3 experiments:**
  1. **Experiment 1:** Evaluate standard decoding (Vanilla) on a small subset of PopQA to confirm that models answer decisively despite uncertainty (expect low faithfulness).
  2. **Experiment 2:** Test the judge LLM's scoring reliability by having it score a set of manually crafted examples with known decisiveness levels (expect high correlation with human scores).
  3. **Experiment 3:** Compare confidence scores obtained via consistency sampling vs. direct prompting of the model to express its confidence (expect moderate correlation but different distributions).

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but raises several important considerations:
- How the framework can be extended to other methods of quantifying uncertainty beyond consistency-based confidence estimation
- Whether faithfulness relates to other metrics of uncertainty communication such as calibration or verbalized confidence
- How the framework performs on different types of language models beyond the large models evaluated

## Limitations

- The framework focuses exclusively on aleatoric (model) uncertainty and does not address epistemic uncertainty arising from ambiguous or underspecified questions
- Judge LLM scoring may introduce systematic biases that are not fully characterized or validated against human judgment
- Confidence estimation through sampling is computationally expensive and may not capture all aspects of model uncertainty

## Confidence

- **High:** The core methodology for quantifying faithfulness (MFG) is well-specified and grounded in measurable quantities (decisiveness and confidence scores)
- **Medium:** The experimental results showing poor faithfulness across multiple models are robust but may be influenced by judge LLM biases
- **Low:** The generalizability of results to other uncertainty types and the effectiveness of proposed prompting methods for improving faithfulness

## Next Checks

1. **Judge LLM Validation:** Conduct human evaluations on a subset of responses to validate whether the judge LLM's scores for decisiveness and confidence align with human perception, particularly for nuanced hedging expressions.

2. **Alternative Confidence Estimation:** Compare the consistency-based confidence estimation approach with alternative methods such as direct prompting of the model to express its confidence or using calibration datasets to estimate confidence.

3. **Epistemic Uncertainty Extension:** Extend the evaluation framework to handle epistemic uncertainty by designing prompts that explicitly ask models to identify and communicate when questions are ambiguous or underspecified.