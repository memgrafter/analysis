---
ver: rpa2
title: 'Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search'
arxiv_id: '2408.10635'
source_url: https://arxiv.org/abs/2408.10635
tags:
- player
- players
- score
- state
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STRATEGIST introduces a bi-level tree search framework that combines
  high-level strategy learning via LLM self-improvement with low-level policy refinement
  through Monte Carlo Tree Search. The approach uses population-based self-play simulations
  to optimize strategies without training data.
---

# Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search

## Quick Facts
- arXiv ID: 2408.10635
- Source URL: https://arxiv.org/abs/2408.10635
- Reference count: 40
- Primary result: Achieves human-level performance in strategic games using bi-level tree search without training data

## Executive Summary
STRATEGIST introduces a bi-level tree search framework that combines high-level strategy learning via LLM self-improvement with low-level policy refinement through Monte Carlo Tree Search. The approach uses population-based self-play simulations to optimize strategies without training data. Evaluated on Game of Pure Strategy (GOPS) and Resistance: Avalon, STRATEGIST achieves comparable performance to human players and outperforms traditional RL methods, other LLM-based agents, and pre-existing baselines. The framework demonstrates effective integration between high-level strategy improvement and low-level policy refinement, with the ability to learn sophisticated mixed strategies that rival human-level play.

## Method Summary
STRATEGIST employs a bi-level tree search architecture where LLMs search and update high-level strategies (as text) at the top level, which are then refined and executed by low-level Monte Carlo Tree Search (MCTS). The framework uses population-based self-play for feedback, conducting round-robin tournaments among top strategies and using win-rates as performance signals. Strategies are improved through a modular approach using an idea queue that stores candidate improvements, which are applied incrementally to existing strategies. This allows testing of specific improvements without confounding factors and enables reuse of successful ideas across different strategies. The framework operates without requiring human-generated training data, instead relying on self-play simulations for performance evaluation.

## Key Results
- Achieves human-level performance in Game of Pure Strategy (GOPS) and Resistance: Avalon
- Outperforms traditional RL methods and other LLM-based agents in strategic game settings
- Demonstrates effective integration between high-level strategy improvement and low-level policy refinement
- Learns sophisticated mixed strategies that rival human-level play without training data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The bi-level tree search architecture enables efficient strategy improvement by decoupling high-level strategy search from low-level policy refinement.
- **Mechanism:** STRATEGIST maintains a hierarchical structure where the LLM searches over high-level strategy abstractions (text representations) at the top level, while MCTS refines these into executable policies at the bottom level. This separation allows the LLM to focus on strategic principles without being overwhelmed by action-space complexity.
- **Core assumption:** High-level strategy abstractions are sufficient for guiding low-level policy refinement without requiring direct training data.
- **Evidence anchors:**
  - [abstract] "Our approach leverages LLMs to search and update high-level strategies (as text), which are then refined and executed by low-level Monte Carlo Tree Search (MCTS)."
  - [section 2.2] "We abstract and 'parameterize' the policy ϕ as a value heuristic (VH) σ := v : S → R|N | that estimates the expected cumulative returns for each player at a given state"
  - [corpus] Weak - no corpus evidence specifically addressing the bi-level separation claim
- **Break condition:** If high-level strategy abstractions cannot adequately capture the strategic nuances needed for low-level policy refinement, or if MCTS cannot effectively execute the abstracted strategies.

### Mechanism 2
- **Claim:** Population-based self-play provides effective feedback for strategy improvement without requiring human-generated data.
- **Mechanism:** The framework conducts round-robin tournaments among top strategies, using win-rates as performance signals. This evolutionary approach creates robust strategies that can handle diverse opponent behaviors while eliminating the need for extensive training datasets.
- **Core assumption:** Self-play simulations can generate sufficiently diverse and challenging opponent policies to drive meaningful strategy improvement.
- **Evidence anchors:**
  - [section 2.4] "Specifically, we use population based self-play, where we conduct round-robin games among the top ten strategies generated and use the average performance as the strategy score"
  - [section 3.5] "We benchmark our feedback method (population based self-play) against (1) an LLM-critic (Madaan et al., 2024) and (2) trajectory feedback from a fixed opponent policy, with results in Table 4 showing superior performance"
  - [corpus] Weak - limited corpus evidence on population-based self-play effectiveness
- **Break condition:** If self-play population becomes too homogeneous, leading to overfitting, or if win-rate signals become too noisy to provide useful feedback.

### Mechanism 3
- **Claim:** Modular improvement via idea queue enables efficient exploration of strategy space while maintaining interpretability.
- **Mechanism:** The framework generates improvement ideas stored in a priority queue, which are then applied incrementally to existing strategies. This modular approach allows testing of specific improvements without confounding factors and enables reuse of successful ideas across different strategies.
- **Core assumption:** Strategy improvements can be decomposed into modular components that can be tested and reused independently.
- **Evidence anchors:**
  - [section 2.3.3] "A key feature of our framework is the use of an idea queue to modularize the search process. By refining strategies incrementally rather than globally, we avoid confounding factors and ensure interpretability of changes."
  - [section 3.2] "Our results, presented in Table 2, use the same feedback method (simulation-based population self-play) while varying the improvement methods, with a constant number of new strategies generated across all methods. Figure 12 (right) shows the gameplay scores of these strategies on GOPS."
  - [corpus] Weak - limited corpus evidence on modular improvement effectiveness
- **Break condition:** If strategy improvements are not modularizable, or if the overhead of managing the idea queue outweighs the benefits of modular exploration.

## Foundational Learning

- **Concept:** Value function approximation
  - Why needed here: STRATEGIST abstracts policies as value heuristics that estimate expected returns, enabling the LLM to reason about state quality rather than specific actions
  - Quick check question: Can you explain how a value heuristic differs from a direct policy function in terms of input/output and computational requirements?

- **Concept:** Monte Carlo Tree Search (MCTS)
  - Why needed here: MCTS refines the high-level value heuristics into executable policies by exploring action sequences and updating value estimates based on simulated outcomes
  - Quick check question: What are the four main phases of MCTS and how does each contribute to policy refinement?

- **Concept:** Upper Confidence Bound (UCB) exploration
  - Why needed here: The framework uses UCB sampling to balance exploration of new ideas versus exploitation of proven improvements in both strategy and idea selection
  - Quick check question: How does the UCB formula balance exploration vs exploitation, and what happens if the exploration constant is set too high or too low?

## Architecture Onboarding

- **Component map:** Strategy tree (T) -> Idea queue (Q) -> LLM modules -> MCTS executor -> Self-play simulations -> Tree/queue updates
- **Critical path:** Strategy selection → Idea generation → Strategy implementation → Self-play evaluation → Tree/queue updates → Repeat
- **Design tradeoffs:**
  - Bi-level vs unified search: Bi-level enables more efficient exploration but adds complexity
  - Population-based vs single-opponent feedback: Population provides robustness but requires more computation
  - Modular vs global improvements: Modular enables interpretability but may miss global optima
- **Failure signatures:**
  - Strategy tree stagnation: Limited diversity in generated strategies
  - Idea queue saturation: Queue fills with low-quality or redundant ideas
  - Self-play convergence: Population becomes too homogeneous
  - LLM generation failures: Unable to generate meaningful improvements
- **First 3 experiments:**
  1. Implement basic strategy tree with random strategy generation and MCTS execution
  2. Add self-play feedback loop with simple win-rate evaluation
  3. Integrate idea queue and modular improvement mechanism

## Open Questions the Paper Calls Out
1. How does the performance of STRATEGIST scale with the number of self-play simulations and the size of the strategy tree?
2. Can STRATEGIST be effectively applied to non-adversarial environments, such as single-agent tasks or cooperative multi-agent games?
3. How does the modularization of strategy improvement through an idea queue impact the interpretability and transferability of learned strategies?

## Limitations
- Limited evidence of scalability to more complex games with larger state spaces
- Computational efficiency claims relative to traditional RL methods not fully analyzed
- Performance in non-adversarial environments not experimentally validated

## Confidence
**High-confidence claims:** The bi-level architecture effectively separates strategy learning from policy execution, as evidenced by consistent performance improvements across both game environments (Medium confidence). The population-based self-play feedback mechanism provides meaningful performance signals without human data, demonstrated by superior results compared to fixed opponent baselines (Medium confidence).

**Medium-confidence claims:** The modular improvement approach via idea queue enables efficient exploration of strategy space while maintaining interpretability (Low confidence - limited ablation studies). The framework's ability to learn sophisticated mixed strategies that rival human-level play is demonstrated but requires more extensive human comparison studies (Low confidence - limited human player data).

**Low-confidence claims:** The framework's scalability to more complex games beyond the evaluated domains (Very low confidence - no experiments on games with larger state spaces). The computational efficiency claims relative to traditional RL methods (Very low confidence - no detailed computational complexity analysis provided).

## Next Checks
1. **Ablation Study:** Remove the idea queue mechanism and compare strategy improvement rates to quantify the modular improvement benefit.
2. **Human Comparison:** Conduct systematic comparisons against human players across multiple skill levels to validate human-level performance claims.
3. **Scalability Test:** Apply the framework to a more complex game with larger state spaces to evaluate architectural scalability and identify potential bottlenecks.