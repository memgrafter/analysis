---
ver: rpa2
title: 'Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language
  Models'
arxiv_id: '2403.06199'
source_url: https://arxiv.org/abs/2403.06199
tags:
- language
- arxiv
- visual
- multimodal
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates Multimodal Small Language Models (MSLMs)
  and introduces Mipha, a family of efficient multimodal assistants with 1.7B to 3B
  parameters. The authors explore the design space of MSLMs, focusing on three key
  components: visual representation, language models, and optimization strategies.'
---

# Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language Models

## Quick Facts
- arXiv ID: 2403.06199
- Source URL: https://arxiv.org/abs/2403.06199
- Authors: Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang
- Reference count: 40
- One-line primary result: Mipha-3B outperforms LLaVA-1.5-13B on 8 out of 11 benchmarks despite having only 23% of its parameter count

## Executive Summary
This paper introduces Mipha, a family of efficient multimodal assistants with 1.7B to 3B parameters that challenge the dominance of large multimodal models. The authors systematically explore the design space of Multimodal Small Language Models (MSLMs), focusing on visual representation, language models, and optimization strategies. Their key finding is that MSLMs can outperform state-of-the-art large models like LLaVA-1.5-13B without requiring additional training data, simply through more efficient design choices.

The study reveals three counterintuitive insights: increasing image resolution isn't always beneficial, fine-tuning both visual and language components is crucial (contrary to previous findings for large models), and instruction tuning isn't essential for MSLMs. Mipha-3B achieves the highest performance in 8 out of 11 benchmarks, demonstrating that parameter efficiency and careful architectural choices can rival the capabilities of much larger models.

## Method Summary
The authors employ a two-stage training pipeline for Mipha: pretraining to align vision features with text embedding, followed by visual instruction tuning. They use SigLIP visual backbone (384px), Phi-2 language model, and full-parameter fine-tuning of both components. The model is trained on LCS-558K for pretraining and 665K samples for instruction tuning, using existing multimodal datasets without additional data collection.

## Key Results
- Mipha-3B outperforms LLaVA-1.5-13B on 8 out of 11 benchmarks
- Achieves highest performance despite having only 23% of LLaVA-1.5-13B's parameter count
- Demonstrates that MSLMs can rival capabilities of much larger MLLMs
- Visual resolution scaling is not universally beneficial for MSLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Visual resolution scaling is not universally beneficial for MSLMs.
- **Mechanism:** Increasing image resolution does not guarantee better performance; optimal resolution depends on the task and model scale.
- **Core assumption:** MSLMs have fewer active neurons than MLLMs, requiring more expressive visual features for effective comprehension.
- **Evidence anchors:**
  - [abstract] "Increasing image resolution is not a silver bullet... In some benchmarks, images with a resolution of 224 pixels outperform those with 448 pixels."
  - [section] "Our empirical results indicate that higher image resolution does not guarantee improved performance for MSLMs... a 336px resolution CLIP model excelled in others."
  - [corpus] Found related work on visual prompting and understanding, but none directly contradicting this claim.
- **Break condition:** If model architecture or task requirements change significantly, the resolution-performance relationship may differ.

### Mechanism 2
- **Claim:** Fine-tuning both the visual backbone and language model is crucial for MSLM performance.
- **Mechanism:** MSLMs require fine-tuning of both components to achieve optimal performance, contrary to previous findings for MLLMs.
- **Core assumption:** SLMs have less pre-trained data and fewer parameters, making visual feature expressiveness critical for image understanding.
- **Evidence anchors:**
  - [abstract] "we underscore the importance of simultaneously finetuning both the visual backbone and the language model for MSLMs."
  - [section] "Our study indicates that fine-tuning the vision encoder can actually enhance the performance of MSLMs across all evaluated benchmarks."
  - [corpus] No direct contradictions found in related literature.
- **Break condition:** If the language model is significantly larger or pre-trained on more data, this relationship might change.

### Mechanism 3
- **Claim:** Instruction tuning is not essential for MSLMs.
- **Mechanism:** Base language models can perform comparably to instruction-tuned models in MSLMs, especially when using RLHF-based methods.
- **Core assumption:** The visual instruction tuning process itself provides sufficient instruction-following capability.
- **Evidence anchors:**
  - [abstract] "our analysis reveals that instruction tuning—be it through supervised fine-tuning or reinforcement learning from human feedback (RLHF)—is not essential."
  - [section] "The comparative analysis across all 8 benchmarks shows minimal variance among the four configurations tested."
  - [corpus] No strong contradictions found in related work.
- **Break condition:** If the base language model is particularly weak at instruction following, this might not hold.

## Foundational Learning

- **Concept:** Visual feature extraction and embedding
  - **Why needed here:** Understanding how visual backbones convert images into embeddings is crucial for designing effective MSLMs.
  - **Quick check question:** What is the difference between grid features and patch features in visual backbones?
- **Concept:** Language model fine-tuning techniques
  - **Why needed here:** Different fine-tuning approaches (base, SFT, RLHF) have varying impacts on MSLM performance.
  - **Quick check question:** How does RLHF differ from SFT in terms of training data and objectives?
- **Concept:** Parameter-efficient fine-tuning methods
  - **Why needed here:** LoRA and similar techniques can significantly reduce training costs while maintaining performance.
  - **Quick check question:** What is the main difference between LoRA and full-parameter fine-tuning?

## Architecture Onboarding

- **Component map:** SigLIP visual backbone → Vision-Language Projector (MLP) → Phi-2 language model → Output
- **Critical path:** Image → Visual features → Text embedding space → Language model → Answer generation
- **Design tradeoffs:**
  - Higher resolution vs. computational efficiency
  - Fine-tuning all parameters vs. using parameter-efficient methods
  - Base vs. instruction-tuned language models
- **Failure signatures:**
  - Poor visual understanding: Check visual backbone and resolution
  - Weak instruction following: Check language model and fine-tuning strategy
  - High computational cost: Consider parameter-efficient fine-tuning
- **First 3 experiments:**
  1. Compare base vs. instruction-tuned language models on a small benchmark
  2. Test different image resolutions (224px, 336px, 448px) on visual understanding tasks
  3. Evaluate full-parameter vs. LoRA fine-tuning on a subset of the training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the advantage of scaling up image resolution vary depending on the specific multimodal task or benchmark?
- Basis in paper: [explicit] The paper mentions that higher image resolutions do not always lead to better performance for MSLMs, and that increasing resolution is not a silver bullet.
- Why unresolved: The paper provides some examples where higher resolution did not improve performance, but does not provide a comprehensive analysis of how the effect of image resolution scaling varies across different tasks or benchmarks.
- What evidence would resolve it: A detailed study examining the performance of MSLMs with varying image resolutions across a wide range of multimodal tasks and benchmarks, identifying patterns in which tasks benefit from higher resolution and which do not.

### Open Question 2
- Question: How does the performance of MSLMs compare to MLLMs when both are trained on the same amount of data and computational resources?
- Basis in paper: [inferred] The paper compares the performance of MSLMs and MLLMs, but does not directly compare them when trained under identical conditions.
- Why unresolved: The comparison is made between models of different sizes and trained on different amounts of data, making it difficult to isolate the impact of model size and data quantity on performance.
- What evidence would resolve it: A controlled experiment where MSLMs and MLLMs of similar sizes are trained on the same amount and type of data, using the same computational resources, and their performance is compared across a range of benchmarks.

### Open Question 3
- Question: What is the optimal balance between the size of the language model and the visual representation backbone in MSLMs for achieving the best performance?
- Basis in paper: [inferred] The paper discusses the design space of MSLMs, including the choice of language models and visual backbones, but does not provide a systematic study of the optimal balance between these components.
- Why unresolved: The paper focuses on the overall performance of MSLMs without delving into the specific contribution of each component to the final results.
- What evidence would resolve it: A series of experiments where the size of the language model and visual backbone are varied independently while keeping other factors constant, and the performance of the resulting MSLMs is evaluated across multiple benchmarks to identify the optimal balance.

## Limitations
- The study's benchmarking is limited to existing datasets that may not reflect real-world deployment scenarios
- Comparison with LLaVA-1.5-13B is based on standard benchmarks that may not capture all practical use cases
- Focus on relatively small parameter models (1.7B-3B) limits generalizability to larger-scale deployments
- The impact of model architecture variations on the findings is not fully explored

## Confidence

- **High Confidence**: The findings on visual resolution scaling (Mechanism 1) are well-supported by systematic experiments across multiple benchmarks
- **Medium Confidence**: The claim about fine-tuning both visual backbone and language model (Mechanism 2) is supported by experimental results but needs validation across different model scales
- **Medium Confidence**: The assertion that instruction tuning is not essential (Mechanism 3) is based on controlled experiments but requires additional validation across different base language models and tasks

## Next Checks

1. **Cross-Domain Robustness Test**: Evaluate Mipha's performance on out-of-distribution datasets and real-world multimodal scenarios to verify the robustness of the claimed advantages over LLaVA-1.5-13B.

2. **Fine-tuning Strategy Ablation**: Systematically test the impact of different fine-tuning strategies (full-parameter vs. LoRA) on both visual backbone and language model components across varying model scales to validate Mechanism 2.

3. **Instruction Tuning Dependency Analysis**: Conduct a comprehensive study comparing base, SFT, and RLHF-tuned language models across diverse instruction-following tasks to further validate the necessity claims in Mechanism 3.