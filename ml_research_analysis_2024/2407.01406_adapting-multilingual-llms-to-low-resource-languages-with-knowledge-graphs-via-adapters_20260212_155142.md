---
ver: rpa2
title: Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs
  via Adapters
arxiv_id: '2407.01406'
source_url: https://arxiv.org/abs/2407.01406
tags:
- language
- adapters
- knowledge
- languages
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the integration of graph knowledge from linguistic
  ontologies into multilingual Large Language Models (LLMs) using adapters to improve
  performance for low-resource languages (LRLs) in sentiment analysis (SA) and named
  entity recognition (NER). Building upon successful parameter-efficient fine-tuning
  techniques, such as K-ADAPTER and MAD-X, we propose a similar approach for incorporating
  knowledge from multilingual graphs, connecting concepts in various languages with
  each other through linguistic relationships, into multilingual LLMs for LRLs.
---

# Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters

## Quick Facts
- arXiv ID: 2407.01406
- Source URL: https://arxiv.org/abs/2407.01406
- Reference count: 17
- Parameter-efficient adapters trained on ConceptNet triples improve SA and NER for eight low-resource languages.

## Executive Summary
This paper investigates integrating graph knowledge from linguistic ontologies into multilingual LLMs for low-resource languages (LRLs) using parameter-efficient adapters. The authors propose fine-tuning language-specific adapters on data extracted from ConceptNet, aiming to enable knowledge transfer across languages covered by the knowledge graph. They compare various fine-tuning objectives, including standard MLM, MLM with full-word masking, and MLM with targeted masking, to analyze their effectiveness in learning and integrating the extracted graph data. Through empirical evaluation on language-specific tasks, they assess how structured graph knowledge affects the performance of multilingual LLMs for LRLs in sentiment analysis and named entity recognition.

## Method Summary
The method uses parameter-efficient adapters trained on ConceptNet triples to inject multilingual semantic relations into frozen mBERT weights. Language adapters use bottleneck architectures with MLM-style objectives to learn to map ConceptNet linguistic relations into natural language predicates and fill masked positions. The study explores different fine-tuning objectives (MLM, FLM, TLM) for training these adapters, with TLM chosen for NER tasks. AdapterFusion is optionally used to combine Wikipedia and ConceptNet adapters through attention-like Key, Value, and Query matrices. Task adapters are then fine-tuned on labeled SA or NER data while keeping language adapters frozen.

## Key Results
- Parameter-efficient adapters trained on ConceptNet triples improve sentiment analysis performance for low-resource languages.
- Targeted MLM (TLM) masking improves named entity recognition performance by forcing the model to predict only words not part of the relation predicate.
- AdapterFusion enables dynamic combination of factual and commonsense knowledge during inference, though results vary by language pair.

## Why This Works (Mechanism)

### Mechanism 1
Parameter-efficient adapters trained on ConceptNet triples allow LRLs to benefit from multilingual semantic relations without full model fine-tuning. The adapters use bottleneck architectures with MLM-style objectives to inject multilingual commonsense knowledge into frozen mBERT weights. The adapters learn to map ConceptNet linguistic relations (e.g., "RelatedTo", "IsA") into natural language predicates and fill masked positions accordingly. This works because mBERT can generalize the relational knowledge encoded in ConceptNet when the training objective aligns with masked token prediction over multilingual text. Break condition: If mBERT's pre-trained representations are too distant from the ConceptNet relation space, the adapters will fail to converge or transfer effectively.

### Mechanism 2
Fusion of Wikipedia and ConceptNet adapters enables dynamic combination of factual and commonsense knowledge during inference. AdapterFusion learns Key, Value, and Query matrices per layer to attend to the most relevant adapter (Wikipedia vs ConceptNet) conditioned on the current context, preserving both knowledge sources without destructive interference. This works because the contextual attention over adapters can learn to activate the correct knowledge source for each downstream task token. Break condition: If both adapters encode conflicting signals, the fusion weights may oscillate or settle on a suboptimal mixture.

### Mechanism 3
Targeted MLM (TLM) masking improves NER performance by forcing the model to predict only words not part of the relation predicate, thereby learning entity-linking patterns across languages. During adapter training, TLM masks tokens that are not in the ConceptNet relation list with 50% probability, compelling the adapter to learn cross-lingual entity associations. This works because NER benefits more from entity-word prediction than from full sentence reconstruction, which is captured better by selective masking. Break condition: If masked tokens are too ambiguous or lack sufficient context, the adapter may memorize surface patterns rather than semantic links.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (adapter modules)
  - Why needed here: Full fine-tuning mBERT on LRL data is computationally expensive and risks catastrophic forgetting; adapters add small trainable modules that preserve the base model.
  - Quick check question: What is the reduction factor used for bottleneck adapters in this study?
    - Answer: 16.

- Concept: Masked Language Modeling (MLM) objective
  - Why needed here: MLM is the standard pre-training objective adapted to learn graph knowledge by masking tokens in ConceptNet-derived sentences.
  - Quick check question: Which masking variant was found optimal for NER in this work?
    - Answer: Targeted MLM (TLM).

- Concept: AdapterFusion dynamic routing
  - Why needed here: Combining Wikipedia (factual) and ConceptNet (commonsense) knowledge requires a non-destructive way to blend their signals per token.
  - Quick check question: What attention-like components does AdapterFusion use?
    - Answer: Key, Value, and Query matrices per layer.

## Architecture Onboarding

- Component map: frozen mBERT -> language adapters (ConceptNet/Wikipedia) -> (optional AdapterFusion) -> task adapters -> output heads

- Critical path: 1. Extract ConceptNet triples â†’ natural language sentences; 2. Train language adapter (MLM/FLM/TLM) on extracted data; 3. Stack task adapter on frozen language adapter; 4. Fine-tune task adapter on labeled SA or NER data; 5. (Optional) Add AdapterFusion between base and task adapter

- Design tradeoffs: Full fine-tuning vs adapters (computational cost vs risk of forgetting); MLM vs TLM objectives (general language understanding vs task-specific entity learning); single vs fused adapters (simplicity vs richer knowledge mix)

- Failure signatures: Adapter loss plateaus early (ConceptNet sentences too noisy or domain-mismatched); task adapter overfits quickly (labeled data too small relative to model capacity); fusion weights collapse (conflicting knowledge signals in Wikipedia/ConceptNet adapters)

- First 3 experiments: 1. Train a ConceptNet language adapter with standard MLM on Maltese data and evaluate on SA; 2. Replace MLM with TLM for NER task adapters on Uyghur; 3. Enable AdapterFusion between Wikipedia and ConceptNet adapters for Nepali NER and compare to single-adapter baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of objective function for training language adapters on graph knowledge affect downstream task performance across different low-resource languages? The paper compares MLM, FLM, and TLM but doesn't deeply analyze why certain objectives perform better for specific languages or tasks. Systematic experiments comparing additional objective functions across multiple low-resource languages and tasks would resolve this.

### Open Question 2
What is the impact of knowledge graph data quantity on language adapter performance, and is there a threshold below which additional data provides diminishing returns? The paper notes LRLs have fewer ConceptNet and Wikipedia resources but doesn't systematically vary data quantities to determine optimal thresholds. Experiments varying graph data amounts and augmentation techniques would address this.

### Open Question 3
How do language adapters based on different knowledge sources (ConceptNet vs. Wikipedia) complement each other, and can their fusion be optimized for specific low-resource language pairs? The paper explores AdapterFusion but doesn't analyze which language pairs benefit most from fusion or develop language-specific fusion strategies. Detailed analysis of knowledge source complementarity and development of language-specific fusion strategies would resolve this.

## Limitations
- The self-designed TLM objective lacks published implementation details, making it difficult to reproduce the claimed NER gains.
- No direct comparisons to full fine-tuning baselines make it hard to quantify the true cost-benefit tradeoff of adapters.
- The reliance on ConceptNet's automatically extracted triples introduces noise, especially for morphologically rich LRLs.

## Confidence

- Core mechanism (parameter-efficient adapters learning from graph knowledge): High
- TLM-specific gains for NER: Medium
- AdapterFusion results: Low

## Next Checks

1. Implement and test the TLM objective independently to verify it produces meaningfully different masking behavior than MLM and FLM, and that this difference translates to NER performance gains.

2. Run an ablation study comparing adapter-based fine-tuning to full fine-tuning on a subset of LRLs to quantify parameter efficiency vs. performance trade-offs.

3. Evaluate the robustness of the approach by introducing controlled noise into the ConceptNet triples and measuring degradation in downstream SA and NER tasks.