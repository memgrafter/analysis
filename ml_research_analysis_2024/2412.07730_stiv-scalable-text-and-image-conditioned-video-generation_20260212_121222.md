---
ver: rpa2
title: 'STIV: Scalable Text and Image Conditioned Video Generation'
arxiv_id: '2412.07730'
source_url: https://arxiv.org/abs/2412.07730
tags:
- video
- image
- generation
- training
- frame
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STIV introduces a unified video generation model capable of both
  text-to-video (T2V) and text-image-to-video (TI2V) tasks using a single Diffusion
  Transformer (DiT) architecture. It integrates image conditioning through frame replacement
  and employs joint image-text conditional classifier-free guidance for seamless multi-task
  learning.
---

# STIV: Scalable Text and Image Conditioned Video Generation

## Quick Facts
- arXiv ID: 2412.07730
- Source URL: https://arxiv.org/abs/2412.07730
- Reference count: 40
- One-line primary result: STIV achieves state-of-the-art VBench T2V score of 83.1 and VBench I2V score of 90.1 using a unified Diffusion Transformer architecture.

## Executive Summary
STIV introduces a unified video generation model capable of both text-to-video (T2V) and text-image-to-video (TI2V) tasks using a single Diffusion Transformer (DiT) architecture. It integrates image conditioning through frame replacement and employs joint image-text conditional classifier-free guidance for seamless multi-task learning. The approach addresses scaling challenges with stability techniques (e.g., QK-norm, sandwich-norm) and training efficiency measures (e.g., MaskDiT, AdaFactor). At 8.7B parameters and 512 resolution, STIV achieves state-of-the-art VBench T2V score of 83.1, surpassing models like CogVideoX-5B and Pika, and VBench I2V score of 90.1. The framework also extends to video prediction, frame interpolation, multi-view generation, and long video generation, demonstrating strong versatility and performance across tasks.

## Method Summary
STIV is built on a Diffusion Transformer architecture that processes spatial and temporal latent embeddings through stacked blocks with spatial-temporal attention. The model uses frame replacement to integrate image conditions by replacing the first noised video latent frame with an un-noised image condition latent during training and inference. Joint Image-Text Classifier-Free Guidance (JIT-CFG) combines text and image conditions with a single guidance scale to enhance performance while addressing motion staleness. The training follows a progressive recipe starting from T2I to initialize T2V, which then initializes the STIV model at both low and high resolutions. Stability techniques (QK-norm, sandwich-norm) and efficiency measures (MaskDiT, AdaFactor) enable scaling to 8.7B parameters.

## Key Results
- Achieves VBench T2V score of 83.1, surpassing CogVideoX-5B and Pika
- Achieves VBench I2V score of 90.1, outperforming ImageFX and I2VGEN
- Demonstrates state-of-the-art performance across multiple video generation tasks including prediction, interpolation, multi-view, and long video generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frame replacement enables seamless integration of image conditions into DiT architecture.
- Mechanism: The first noised video latent frame is replaced with the un-noised image condition latent during training, and this un-noised latent is used for the first frame at each TI2V diffusion step during inference.
- Core assumption: The DiT architecture can effectively propagate the image-conditioned first frame through stacked spatial-temporal attention layers.
- Evidence anchors:
  - [abstract]: "Our framework integrates image condition into a Diffusion Transformer (DiT) through frame replacement..."
  - [section 2.3.1]: "During training, we replace the noised first frame latent with the un-noised latent of the image condition..."
  - [corpus]: Weak - no corpus evidence directly supporting this mechanism.
- Break condition: If the DiT architecture cannot effectively propagate the image-conditioned first frame through stacked spatial-temporal attention layers, the frame replacement strategy would fail.

### Mechanism 2
- Claim: Joint Image-Text Classifier-Free Guidance (JIT-CFG) enhances both T2V and TI2V performance while addressing motion staleness.
- Mechanism: JIT-CFG modifies velocity estimates by combining unconditional, image-only, and joint text-image conditions with a single guidance scale, shifting probability mass toward higher likelihood regions.
- Core assumption: The modified velocity estimates effectively balance text and image conditioning strengths.
- Evidence anchors:
  - [abstract]: "incorporating text conditioning via a joint image-text conditional classifier-free guidance"
  - [section 3.4.2]: "We introduce aJoint Image-Text Classifier-Free Guidance (JIT-CFG)approach, which leverages both text and image conditions."
  - [corpus]: Weak - no corpus evidence directly supporting this mechanism.
- Break condition: If the single guidance scale cannot effectively balance text and image conditioning strengths, JIT-CFG would fail to enhance performance or address motion staleness.

### Mechanism 3
- Claim: Progressive training recipe enables efficient scaling from T2I to T2V to STIV models.
- Mechanism: The T2I model is first trained to initialize the T2V model, which then initializes the STIV model at both low and high resolutions, incorporating interpolated RoPE embeddings.
- Core assumption: Lower resolution models can effectively initialize higher resolution models when combined with interpolated RoPE embeddings.
- Evidence anchors:
  - [abstract]: "culminating in a simple and scalable text-image-conditioned video generation method"
  - [section 2.4]: "We employ a progressive training recipe as illustrated in Figure 4."
  - [corpus]: Weak - no corpus evidence directly supporting this mechanism.
- Break condition: If lower resolution models cannot effectively initialize higher resolution models, even with interpolated RoPE embeddings, the progressive training recipe would fail.

## Foundational Learning

- Concept: Diffusion Transformers (DiTs)
  - Why needed here: STIV is built on a DiT architecture, which is central to its video generation capabilities.
  - Quick check question: What is the key difference between a DiT and a traditional U-Net architecture used in video generation?

- Concept: Classifier-Free Guidance (CFG)
  - Why needed here: JIT-CFG is an extension of CFG that incorporates both text and image conditions.
  - Quick check question: How does CFG improve the quality of generated images in diffusion models?

- Concept: Rotary Positional Embeddings (RoPE)
  - Why needed here: RoPE is used in STIV to handle relative temporal and spatial relationships.
  - Quick check question: What is the advantage of using 2D RoPE for spatial attention and 1D RoPE for temporal attention?

## Architecture Onboarding

- Component map:
  VAE encoder -> DiT blocks (with spatial-temporal attention) -> Cross-attention (text) -> Frame replacement (image) -> JIT-CFG -> Output video

- Critical path:
  Input frames → VAE encoder → DiT blocks (with spatial-temporal attention) → Cross-attention (text) → Frame replacement (image) → JIT-CFG → Output video

- Design tradeoffs:
  - Using frame replacement vs. cross-attention for image conditioning
  - Single vs. dual guidance scales in JIT-CFG
  - Progressive training vs. training from scratch

- Failure signatures:
  - Motion staleness in high-resolution videos
  - Poor semantic alignment with text prompts
  - Inconsistent background or subject across frames

- First 3 experiments:
  1. Test frame replacement with a simple T2V model to verify basic image conditioning works
  2. Implement JIT-CFG with single guidance scale and compare against separate image/text CFG
  3. Train a small STIV model with progressive initialization from T2I to verify the scaling approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of JIT-CFG compare to using separate guidance scales for text and image conditions?
- Basis in paper: [explicit] The paper mentions that using two separate guidance scales would require three forward passes, increasing inference cost, and states that "overall, SIT-CFG does not show significant advantages compared to JIT-CFG."
- Why unresolved: The paper only conducted a grid search on one dataset (MSRVTT) and found no significant advantages, but did not explore other datasets or scenarios where separate scales might be beneficial.
- What evidence would resolve it: Systematic comparison of JIT-CFG versus SIT-CFG across multiple video generation datasets and diverse prompt types, measuring both quality metrics and computational efficiency.

### Open Question 2
- Question: What is the optimal strategy for handling high-resolution models to prevent motion staleness without relying on image condition dropout?
- Basis in paper: [inferred] The paper observes that high-resolution models (512) produce static frames and introduces image condition dropout as a solution, suggesting this is a known scaling challenge.
- Why unresolved: The paper presents image condition dropout as an effective but somewhat ad hoc solution, without exploring alternative architectural or training modifications that might address the root cause.
- What evidence would resolve it: Investigation of alternative approaches such as modified attention mechanisms, different conditioning strategies, or architectural changes that maintain motion quality at high resolutions without requiring dropout.

### Open Question 3
- Question: How does the progressive training recipe scale beyond the demonstrated T2I → T2V → STIV pipeline for other video generation tasks?
- Basis in paper: [explicit] The paper describes a progressive training pipeline from T2I to T2V to STIV and mentions potential applications like video prediction and frame interpolation.
- Why unresolved: The paper only demonstrates this specific progression and mentions extensions to other tasks, but does not provide systematic study of how the progressive approach generalizes to different video generation modalities.
- What evidence would resolve it: Empirical evaluation of progressive training applied to different task sequences (e.g., T2I → video prediction → frame interpolation) and comparison with direct training approaches for each task.

## Limitations

- Limited empirical validation of core mechanisms (frame replacement, JIT-CFG, progressive training) with comparative analysis against alternatives
- Single guidance scale in JIT-CFG lacks systematic comparison with dual guidance scale approaches
- Frame replacement mechanism effectiveness depends on untested assumptions about DiT architecture propagation capabilities

## Confidence

- High Confidence: Claims about achieving state-of-the-art VBench scores (83.1 for T2V, 90.1 for I2V) are well-supported by quantitative metrics and competitive comparisons
- Medium Confidence: Claims about model versatility across multiple video generation tasks are supported by experimental results but lack detailed ablations for each task
- Low Confidence: Claims about the effectiveness of core mechanisms (frame replacement, JIT-CFG, progressive training) lack sufficient empirical validation and comparative analysis against alternatives

## Next Checks

1. **Frame Replacement Propagation Test**: Conduct ablation studies comparing frame replacement against cross-attention-based image conditioning, measuring how well image conditions propagate through multiple frames in both low and high-resolution settings.

2. **JIT-CFG Guidance Scale Analysis**: Systematically compare single guidance scale JIT-CFG against dual guidance scale approaches, measuring performance trade-offs in text fidelity, image preservation, and motion quality across different guidance scale values.

3. **Progressive Training Scaling Validation**: Test the progressive training approach across multiple model size increments (not just T2I→T2V→STIV) and document failure cases where initialization from lower-resolution models doesn't transfer effectively, particularly for high-resolution video generation.