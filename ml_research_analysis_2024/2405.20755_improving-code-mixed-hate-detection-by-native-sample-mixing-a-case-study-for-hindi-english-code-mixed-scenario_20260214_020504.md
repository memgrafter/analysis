---
ver: rpa2
title: 'Improving code-mixed hate detection by native sample mixing: A case study
  for Hindi-English code-mixed scenario'
arxiv_id: '2405.20755'
source_url: https://arxiv.org/abs/2405.20755
tags:
- hate
- code-mixed
- samples
- native
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the effectiveness of native language samples
  in improving code-mixed hate detection. The authors propose using native hate samples
  alongside code-mixed samples to train multilingual language models (MLMs).
---

# Improving code-mixed hate detection by native sample mixing: A case study for Hindi-English code-mixed scenario

## Quick Facts
- arXiv ID: 2405.20755
- Source URL: https://arxiv.org/abs/2405.20755
- Authors: Debajyoti Mazumder; Aakash Kumar; Jasabanta Patro
- Reference count: 40
- Key outcome: Adding native language hate samples to code-mixed training data significantly improves hate detection performance, with MLMs achieving F1 scores up to 0.63, and native-only trained MLMs reaching F1 0.60 on code-mixed test data.

## Executive Summary
This paper investigates whether adding native language samples to code-mixed hate detection training data can improve model performance. The authors propose a novel approach of mixing Hindi and English hate samples with Hindi-English code-mixed data during training. Experiments on a Hindi-English code-mixed hate dataset demonstrate that this native sample mixing strategy significantly improves multilingual language model (MLM) performance, with the best model achieving an F1 score of 0.63. Notably, MLMs trained solely on native samples could detect code-mixed hate nearly as well (F1 score: 0.60), suggesting that the code-mixed text's Hindi-dominant nature enables effective transfer learning.

## Method Summary
The study uses three datasets: a Hindi-English code-mixed hate dataset (4575 tweets), a Hindi hate dataset (4754 samples), and an English hate dataset (5852 samples). Three experimental setups were tested: code-mixed only, code-mixed plus native samples in equal ratio, and code-mixed plus native samples in code-mixed ratio. Five MLMs (mBERT, XLM, XLM-R, IndicBERT, MuRIL) were fine-tuned using these training sets with standard hyperparameters (AdamW optimizer, learning rate 2e-5, batch size 32, sequence length 248). Statistical classifiers (SVM, Random Forest, Naive Bayes) were also tested for comparison. Attention visualization was used to analyze model behavior, and error analysis examined performance on sarcastic and subjective hate.

## Key Results
- MLMs trained with native sample mixing achieved F1 scores up to 0.63, significantly outperforming models trained on code-mixed data alone.
- MLMs trained solely on native samples achieved F1 score of 0.60 on code-mixed test data, nearly matching the performance of models trained on code-mixed data.
- Attention visualization revealed that native samples helped MLMs focus on hate-emitting words in code-mixed contexts.
- Statistical models performed worse when native samples were added, likely due to vocabulary expansion issues.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Native samples improve code-mixed hate detection by helping MLMs focus attention on hate-emitting words.
- Mechanism: When trained on native samples, the multilingual embeddings capture semantic patterns associated with hate in the native language. This knowledge transfers to code-mixed contexts, where the model learns to attend to corresponding words or phrases even when mixed with another language.
- Core assumption: Multilingual language models can generalize semantic representations learned from monolingual contexts to code-mixed scenarios.
- Evidence anchors:
  - [abstract]: "visualisation of attention scores revealed that, when native samples were included in training, MLMs could better focus on the hate emitting words in the code-mixed context"
  - [section 6.1]: "We observed that after native sample mixing, the MLMs gave high scores to the hate-emitting words when they appeared in code-mixed contexts"
- Break condition: If the code-mixed data contains hate expressions that are unique to the code-mixing context and don't exist in native samples (e.g., sarcasm, cultural references specific to code-mixing), this mechanism would fail.

### Mechanism 2
- Claim: MLMs trained solely on native samples can detect code-mixed hate because the code-mixed text is predominantly in one language with a minority of words from another.
- Mechanism: Since the code-mixed data is mostly Hindi-dominant (39% non-dominant words), the MLMs can leverage their strong Hindi language representations to identify hate, even when some English words are present.
- Core assumption: The linguistic overlap between code-mixed and native samples is sufficient for the model to generalize hate detection capabilities.
- Evidence anchors:
  - [abstract]: "MLMs trained with native samples alone observed to be detecting code-mixed hate to a large extent"
  - [section 6.3]: "The code-mixed hate dataset is majorly Hindi dominant with nearly 39 percent non-dominant (majorly English) words. Therefore, when MLMs were trained with Hindi samples, they could capture the code-mixed hate."
- Break condition: If the code-mixed data becomes more balanced between languages or if hate is expressed predominantly in the minority language, this mechanism would fail.

### Mechanism 3
- Claim: The vocabulary of statistical models changes unfavorably when native samples are added, causing performance degradation.
- Mechanism: Statistical models use n-grams as features. Adding native samples introduces new monolingual contexts that change the lexical distribution of n-grams, which statistical models rely on for probability distributions and lexical dissimilarity.
- Core assumption: Statistical models are sensitive to changes in lexical distribution and n-gram patterns.
- Evidence anchors:
  - [abstract]: "On combining the native samples, while the statistical models performed worse, many MLMs reported significant improvements"
  - [section 6.1]: "We believe this is because statistical models took n-grams as features, whereas MLMs took token embeddings for the same. The vocabulary in statistical models accommodated more Hindi and English words after we added the native/monolingual samples to the code-mixed training set."
- Break condition: If the native samples were carefully selected to match the code-mixed distribution or if the statistical model uses a vocabulary that's robust to such changes, this mechanism might not apply.

## Foundational Learning

- Concept: Multilingual Language Models (MLMs) and their cross-lingual capabilities
  - Why needed here: Understanding how MLMs can transfer knowledge from one language to another is crucial for grasping why native samples help with code-mixed detection.
  - Quick check question: How do MLMs handle multiple languages in a single model, and what enables them to generalize across languages?

- Concept: Attention mechanisms in transformer models
  - Why needed here: The paper's key finding about attention focusing on hate-emitting words requires understanding how attention works in transformers.
  - Quick check question: What does it mean when a model "attends" to certain words, and how can this be visualized?

- Concept: Code-mixing complexity metrics (CMI and burstiness)
  - Why needed here: Understanding how code-mixing complexity is measured helps interpret why certain approaches work better than others.
  - Quick check question: What do CMI and burstiness scores tell us about the nature of code-mixed text, and why are these metrics relevant for model selection?

## Architecture Onboarding

- Component map: Data preparation -> Model training with different training sets -> Evaluation on code-mixed test set -> Analysis of results

- Critical path: Data preparation → Model training with different training sets → Evaluation on code-mixed test set → Analysis of results

- Design tradeoffs:
  - Using statistical models vs. MLMs: Statistical models are simpler but less effective with code-mixed data
  - Adding transformer layers vs. fine-tuning: Adding layers provides more parameter space but doesn't always improve performance
  - Equal label ratio vs. code-mixed ratio: No significant difference in performance, suggesting robustness to label unbalancing

- Failure signatures:
  - Poor performance on sarcastic or subjective hate (as noted in the error analysis)
  - Degradation of statistical model performance when native samples are added
  - High fluctuation in F1 scores for MLMs with transformer layers

- First 3 experiments:
  1. Train a statistical classifier (e.g., Naive Bayes) on code-mixed data only, then on code-mixed + native data, and compare F1 scores.
  2. Fine-tune mBERT on native Hindi samples only, then evaluate on code-mixed test set to verify the 0.60 F1 score claim.
  3. Train MuRIL on code-mixed data, then incrementally add native samples in batches, and observe the change in F1 score to replicate the experiment in Figure 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of native sample mixing vary across different language pairs beyond Hindi-English?
- Basis in paper: [explicit] The authors state "In this work, we have evaluated our hypothesis only for a single code-mixed environment (Hindi-English)" and suggest future work should "evaluate the idea of native sample mixing for other code-mixed environments"
- Why unresolved: The current study is limited to Hindi-English due to the authors' linguistic expertise and availability of annotated corpora. The effectiveness of native sample mixing for other language pairs remains untested.
- What evidence would resolve it: Experimental results from code-mixed hate detection studies using native sample mixing for other language pairs (e.g., Spanish-English, Arabic-French) would demonstrate generalizability.

### Open Question 2
- Question: Does the performance of native sample mixing differ when applied to tasks beyond binary hate classification, such as multi-class or sequence labeling tasks?
- Basis in paper: [explicit] The authors mention "We have kept the architecture and the task setup simple and basic" and suggest future work could "report further the applicability of the idea of native language mixing" across "many task set-ups"
- Why unresolved: The current study only uses binary classification. The effectiveness of native sample mixing for more complex task formulations is unknown.
- What evidence would resolve it: Comparative experiments showing native sample mixing performance on multi-class hate detection, sarcasm detection, or sequence labeling tasks would reveal task-specific effectiveness.

### Open Question 3
- Question: What specific linguistic features captured by native samples contribute most to improved code-mixed hate detection performance?
- Basis in paper: [inferred] The authors observe that native samples improve focus on hate-emitting words and cultural context, but don't analyze which specific features drive this improvement
- Why unresolved: While the study demonstrates that native samples help, it doesn't identify which aspects (lexical, syntactic, semantic, cultural) are most important
- What evidence would resolve it: Feature ablation studies or attention mechanism analysis isolating the contribution of different linguistic features (taboo words, cultural references, syntactic patterns) would identify key drivers of performance improvement.

## Limitations

- The study is limited to a single code-mixed language pair (Hindi-English) and may not generalize to other language pairs.
- The native samples were not curated to match the code-mixed distribution, potentially affecting statistical model performance.
- The error analysis is qualitative and focuses mainly on sarcastic and subjective hate, without systematic quantification of error types.

## Confidence

**High Confidence**: The finding that MLMs outperform statistical models for code-mixed hate detection (F1 scores ranging from 0.58-0.63 vs 0.55-0.56) is well-supported by experimental results. The observation that native samples help MLMs focus attention on hate-emitting words is also well-validated through visualization.

**Medium Confidence**: The claim that MLMs trained solely on native samples can detect code-mixed hate nearly as well (F1 score 0.60) is based on experimental evidence but may not generalize to datasets with different code-mixing patterns or languages.

**Low Confidence**: The assertion that adding transformer layers doesn't significantly improve performance is based on limited experimentation with only one model architecture. The error analysis suggesting poor performance on sarcastic or subjective hate is qualitative and would benefit from more systematic evaluation.

## Next Checks

1. **Cross-dataset validation**: Test the native sample mixing approach on a different code-mixed hate dataset (e.g., from a different platform or language pair) to verify generalizability beyond the Hindi-English Twitter corpus used in this study.

2. **Controlled native sample selection**: Repeat the experiments with native samples that are carefully curated to match the code-mixed distribution in terms of vocabulary overlap and linguistic patterns, to isolate whether the performance degradation of statistical models is due to native sample addition or vocabulary expansion.

3. **Error type analysis**: Systematically categorize and quantify the types of errors made by models trained with and without native samples, particularly focusing on sarcastic, subjective, and culturally-specific hate expressions to validate the qualitative error analysis presented in the paper.