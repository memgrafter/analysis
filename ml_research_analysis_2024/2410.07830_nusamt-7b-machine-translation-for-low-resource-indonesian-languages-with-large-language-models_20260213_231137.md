---
ver: rpa2
title: 'NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages with
  Large Language Models'
arxiv_id: '2410.07830'
source_url: https://arxiv.org/abs/2410.07830
tags:
- data
- should
- languages
- language
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of machine translation for low-resource
  Indonesian languages like Balinese and Minangkabau, which lack sufficient parallel
  corpora. The authors propose NusaMT-7B, a fine-tuned LLM approach that combines
  continued pre-training on monolingual data, supervised fine-tuning, data cleaning
  with an LLM-based preprocessor, and backtranslation.
---

# NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages with Large Language Models

## Quick Facts
- arXiv ID: 2410.07830
- Source URL: https://arxiv.org/abs/2410.07830
- Reference count: 30
- Outperforms state-of-the-art models by up to +6.69 spBLEU for translations into Balinese and Minangkabau

## Executive Summary
This paper addresses the challenge of machine translation for low-resource Indonesian languages like Balinese and Minangkabau, which lack sufficient parallel corpora. The authors propose NusaMT-7B, a fine-tuned LLM approach that combines continued pre-training on monolingual data, supervised fine-tuning, data cleaning with an LLM-based preprocessor, and backtranslation. In the FLORES-200 benchmark, NusaMT-7B outperforms state-of-the-art models by up to +6.69 spBLEU for translations into Balinese and Minangkabau, though it underperforms by up to -3.38 spBLEU for higher-resource languages. The results demonstrate that fine-tuned LLMs with high-quality, cleaned datasets can significantly improve translation quality for low-resource languages, supporting linguistic preservation and cross-cultural communication.

## Method Summary
NusaMT-7B fine-tunes LLaMA2-7B through a multi-stage process. First, continued pre-training on monolingual data exposes the model to linguistic structures of low-resource languages. Second, supervised fine-tuning trains the model on parallel sentence pairs. Third, an LLM-based preprocessor (using GPT-4o mini) cleans noisy parallel sentences through few-shot prompting to determine alignment and improve sentence quality. Finally, backtranslation generates synthetic parallel data by translating monolingual target language text back into the source language. The approach uses LoRA with rank 16, trained for 3 epochs with learning rate 0.002 using bfloat16 precision.

## Key Results
- Outperforms state-of-the-art models by up to +6.69 spBLEU for translations into Balinese and Minangkabau
- Underperforms by up to -3.38 spBLEU for higher-resource languages
- Data cleaning reduces dataset size significantly (e.g., from 31k to 22.4k for min↔id) while maintaining or improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued pre-training on monolingual data improves LLM performance for low-resource languages.
- Mechanism: Exposing the LLM to linguistic structures and vocabulary of the target language during pre-training helps the model learn the language's specific characteristics.
- Core assumption: Monolingual data contains sufficient linguistic patterns to teach the model about the target language.
- Evidence anchors:
  - [abstract] "leveraging the pretrained LLaMA2-7B, our approach integrates continued pre-training on monolingual data"
  - [section 3.1] "Most pretrained LLMs are unfamiliar with low-resource languages, making continued pre-training in the target low-resource language essential for teaching the LLM the linguistic principles of these previously unseen languages"
- Break condition: If the monolingual data is too small or not representative of the language's full linguistic diversity, continued pre-training may not be effective.

### Mechanism 2
- Claim: An LLM-based data preprocessor can effectively clean noisy parallel sentences and improve translation quality.
- Mechanism: The LLM cleaner uses few-shot prompting to determine if two sentences share the same meaning and clean the sentences to improve alignment.
- Core assumption: GPT-4o mini is capable of accurately identifying aligned sentence pairs and cleaning them without introducing new errors.
- Evidence anchors:
  - [abstract] "our approach integrates... an LLM-based data cleaner to reduce noise in parallel sentences"
  - [section 3.2] "We propose an LLM data preprocessor tasked with (1) determining if two sentences share the same underlying meaning and, if so, (2) cleaning the parallel sentences to improve sentence alignment"
- Break condition: If the LLM cleaner is not accurate enough, it may incorrectly filter out good sentence pairs or fail to clean noisy ones, potentially degrading translation quality.

### Mechanism 3
- Claim: Backtranslation can generate additional training data and improve translation quality for low-resource languages.
- Mechanism: The model translates monolingual target language data back into the source language, creating synthetic parallel sentence pairs that are then filtered and cleaned.
- Core assumption: The model's translations are accurate enough to create useful synthetic data for training.
- Evidence anchors:
  - [abstract] "NusaMT-7B incorporates... synthetic data generation"
  - [section 3.3] "Backtranslation is a self-training method used to generate additional training data for SFT. It is a data-efficient method to augment new parallel sentence pairs and generate additional training data on more diverse linguistic structures and contexts"
- Break condition: If the backtranslated sentences contain errors or do not accurately represent the target language, the synthetic data may not be useful for training and could even harm performance.

## Foundational Learning

- Concept: Continued pre-training on monolingual data
  - Why needed here: LLMs are typically pretrained on high-resource languages and need exposure to low-resource languages to learn their linguistic patterns.
  - Quick check question: What is the purpose of continued pre-training on monolingual data in the NusaMT-7B approach?

- Concept: Supervised fine-tuning (SFT)
  - Why needed here: SFT allows the model to learn the task of translation by training on parallel sentence pairs.
  - Quick check question: What is the role of supervised fine-tuning in the NusaMT-7B approach?

- Concept: Backtranslation
  - Why needed here: Backtranslation generates additional training data by translating monolingual target language data back into the source language.
  - Quick check question: How does backtranslation contribute to the training of NusaMT-7B?

## Architecture Onboarding

- Component map: Pre-trained LLM (LLaMA2-7B) → Continued pre-training on monolingual data → Supervised fine-tuning (SFT) on parallel data → LLM-based data preprocessor for cleaning parallel sentences → Backtranslation for synthetic data generation → Final model

- Critical path: Pre-trained LLM → Continued pre-training → SFT → Data cleaning → Backtranslation → Final model

- Design tradeoffs:
  - Using a smaller model (7B parameters) instead of a larger one to save on computational resources, but potentially sacrificing some performance
  - Relying on an external LLM (GPT-4o mini) for data cleaning, which introduces a dependency but leverages the LLM's capabilities
  - Generating synthetic data through backtranslation, which can introduce errors but provides more training data

- Failure signatures:
  - Poor performance on low-resource languages despite the proposed methods
  - High computational cost due to the large model size and multiple training stages
  - Inaccuracies in the LLM cleaner's sentence alignment and cleaning

- First 3 experiments:
  1. Train a baseline model using only SFT on parallel data without any of the proposed methods.
  2. Add continued pre-training on monolingual data and evaluate the impact on translation quality.
  3. Introduce the LLM cleaner and backtranslation, and assess the combined effect on performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of NusaMT-7B scale with additional monolingual pretraining data across different low-resource Indonesian languages?
- Basis in paper: [inferred] The paper mentions that monolingual pretraining is essential but is limited by GPU resources, suggesting uncertainty about the optimal amount of data needed.
- Why unresolved: The paper uses Komodo-7B-base with limited GPU resources and does not fully assess the impact of varying amounts of monolingual data on performance.
- What evidence would resolve it: Experiments varying the quantity and diversity of monolingual pretraining data while measuring performance on multiple low-resource languages.

### Open Question 2
- Question: Can the LLM-based data cleaning method maintain or improve performance when applied to other low-resource language pairs beyond Balinese and Minangkabau?
- Basis in paper: [inferred] The paper shows significant performance gains from data cleaning for Balinese but notes that ablation studies were only conducted on Balinese directions.
- Why unresolved: The effectiveness of the LLM cleaner is demonstrated for Balinese but not validated across other low-resource languages or language pairs.
- What evidence would resolve it: Applying the LLM cleaning method to parallel corpora for other low-resource languages and measuring performance changes.

### Open Question 3
- Question: How does NusaMT-7B's performance compare to NLLB-54B-MOE when trained on equivalent datasets for low-resource Indonesian languages?
- Basis in paper: [explicit] The paper acknowledges not benchmarking against NLLB-54B-MOE, NLLB's largest model, due to computational constraints.
- Why unresolved: The paper only compares against smaller NLLB variants and does not evaluate the largest model in the NLLB family.
- What evidence would resolve it: Direct benchmarking of NusaMT-7B against NLLB-54B-MOE using identical training datasets for Indonesian low-resource languages.

### Open Question 4
- Question: What is the minimum viable dataset size for effective low-resource translation using fine-tuned LLMs?
- Basis in paper: [inferred] The paper demonstrates that cleaning reduces dataset size significantly (e.g., from 31k to 22.4k for min↔id) while maintaining or improving performance, supporting the LIMA hypothesis.
- Why unresolved: While the paper shows that smaller, cleaner datasets can improve performance, it does not establish the minimum effective dataset size for various low-resource scenarios.
- What evidence would resolve it: Systematic experiments reducing dataset sizes while measuring performance thresholds where translation quality degrades significantly.

## Limitations

- Mixed performance across resource levels (underperforms by up to -3.38 spBLEU on higher-resource languages)
- Critical dependency on GPT-4o mini for data cleaning
- Insufficient ablation studies to isolate contribution of each component

## Confidence

- **High confidence**: The general methodology of using continued pre-training for low-resource languages is well-supported by the results and aligns with established practices in the field
- **Medium confidence**: The specific implementation details of the LLM cleaner and backtranslation pipeline are insufficiently described to fully validate the approach
- **Low confidence**: The claim that this approach generalizes well beyond the specific Indonesian languages tested, given the mixed performance across different resource levels

## Next Checks

1. **Ablation study**: Systematically remove each component (continued pre-training, data cleaning, backtranslation) individually and measure the impact on translation quality for both low-resource and high-resource language pairs to isolate the contribution of each mechanism.

2. **Open-source cleaner evaluation**: Replace GPT-4o mini with an open-source LLM (such as Llama-2-chat or Mistral) for the data cleaning step and compare results to assess whether the approach depends on proprietary models.

3. **Cross-linguistic generalization**: Evaluate the fine-tuned model on low-resource language pairs outside the Indonesian language family (such as African or indigenous American languages) to test whether the approach generalizes beyond closely related languages with similar linguistic features.