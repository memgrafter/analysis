---
ver: rpa2
title: 'Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey
  of Speculative Decoding'
arxiv_id: '2401.07851'
source_url: https://arxiv.org/abs/2401.07851
tags:
- decoding
- speculative
- speedup
- methods
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first comprehensive survey on Speculative
  Decoding, a novel decoding paradigm for accelerating Large Language Model (LLM)
  inference. Speculative Decoding mitigates high inference latency by efficiently
  drafting multiple future tokens and verifying them in parallel using the target
  LLM, enabling simultaneous decoding of multiple tokens per step.
---

# Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding

## Quick Facts
- arXiv ID: 2401.07851
- Source URL: https://arxiv.org/abs/2401.07851
- Reference count: 40
- Primary result: EAGLE achieves highest speedup of 1.8× to 2.4× over autoregressive decoding in greedy settings

## Executive Summary
This survey presents the first comprehensive overview of Speculative Decoding, a novel paradigm for accelerating Large Language Model (LLM) inference. The paper systematically categorizes existing research into drafting strategies (independent vs self-drafting) and verification methods (greedy, speculative sampling, and token tree), introducing Spec-Bench - a comprehensive benchmark covering six tasks. Experiments show EAGLE achieving the highest speedup of 1.8× to 2.4× over autoregressive decoding in greedy settings, while PLD excels in tasks with high input-output similarity. The paper identifies key challenges including balancing speculation accuracy and drafting efficiency, applying Speculative Decoding in batched inference scenarios, and integrating with other acceleration techniques.

## Method Summary
The survey evaluates six open-source Speculative Decoding methods (EAGLE, SpS, Medusa, PLD, REST, Lookahead) on Vicuna-7B at FP16 precision using Spec-Bench, a comprehensive benchmark consisting of six tasks with 80 samples each. The evaluation uses a single NVIDIA 3090 GPU with batch size 1, measuring speedup ratio compared to autoregressive decoding in tokens per second with mean over three runs. The paper formalizes the Speculative Decoding paradigm, categorizing methods based on their drafting strategies and verification approaches, and provides a systematic framework for understanding and comparing different techniques.

## Key Results
- EAGLE achieves the highest speedup of 1.8× to 2.4× over autoregressive decoding in greedy settings
- PLD excels in tasks with high input-output similarity, achieving 2.4× speedup in summarization
- Speedup diminishes with higher sampling temperatures and FP32 precision
- Spec-Bench provides comprehensive evaluation across six diverse tasks (multi-turn conversation, translation, summarization, question answering, mathematical reasoning, retrieval-augmented generation)

## Why This Works (Mechanism)

### Mechanism 1
LLMs are memory-bound, meaning the bottleneck is data transfer from High-Bandwidth Memory (HBM) to on-chip cache, not arithmetic computation. Speculative Decoding drafts tokens in parallel during these idle memory transfer periods, utilizing otherwise wasted computational resources.

### Mechanism 2
Drafted tokens are verified in parallel by the target LLM. Only tokens that meet the verification criterion are accepted, ensuring the output matches the quality of autoregressive decoding.

### Mechanism 3
The acceleration effect primarily hinges on the acceptance rate of drafted tokens at each step, which is influenced by the draft quality, verification criteria, and the behavior alignment between the drafter and the target LLM.

## Foundational Learning

- **Autoregressive decoding**: The baseline method that Speculative Decoding aims to accelerate. *Quick check*: What is the main limitation of autoregressive decoding in LLM inference?

- **Memory-bound computation**: The core observation that enables Speculative Decoding's speedup. *Quick check*: What is the primary bottleneck in LLM inference according to the paper?

- **Verification criterion**: The mechanism by which Speculative Decoding ensures output quality. *Quick check*: How does Speculative Decoding ensure that the output matches the quality of autoregressive decoding?

## Architecture Onboarding

- **Component map**: Input sequence -> Draft model (drafter) -> Target LLM -> Verification criterion -> Correction strategy -> Output sequence
- **Critical path**: Drafting → Verification → Acceptance/Correction → Output
- **Design tradeoffs**:
  - Draft model size vs. drafting efficiency: Larger drafters are more accurate but slower
  - Verification strictness vs. acceptance rate: Stricter verification ensures quality but reduces speedup
  - Speculation length vs. parallelism: Longer speculation allows more parallelism but increases rejection chance
- **Failure signatures**:
  - Low acceptance rate: Indicates poor drafting or overly strict verification
  - Slow verification: Indicates target LLM is not efficient at verifying drafts
  - Output quality degradation: Indicates verification criterion is not strict enough
- **First 3 experiments**:
  1. Compare inference speed of autoregressive decoding vs. Speculative Decoding with simple drafter and greedy verification
  2. Vary drafter size and measure impact on acceptance rate and overall speedup
  3. Compare different verification criteria (greedy vs. speculative sampling) and measure impact on acceptance rate and output quality

## Open Questions the Paper Calls Out

### Open Question 1
How to balance speculation accuracy and drafting efficiency in Speculative Decoding to maximize speedup? The paper identifies that scaling up the drafter enhances speculation accuracy but reduces drafting efficiency, potentially negating overall speedup. Comparative studies evaluating different drafting strategies under varying model scales and tasks are needed.

### Open Question 2
How to apply Speculative Decoding in batched inference scenarios without sacrificing speedup? Current implementations lack support for batched inference, which is crucial for real-time LLM services. The varying decoding steps and increased computational complexity in batch settings pose significant challenges.

### Open Question 3
How to integrate Speculative Decoding with other leading techniques (e.g., vLLM, Non-AutoRegressive Generation, Flash-Attention) to further boost LLM inference efficiency? While Speculative Decoding shows promise as a general decoding paradigm, its potential synergies with other acceleration techniques remain largely unexplored.

## Limitations

- Evaluation focused on single model (Vicuna-7B) and hardware configuration (NVIDIA 3090 GPU, FP16)
- Benchmark coverage may not fully represent real-world usage patterns for longer sequences or complex reasoning tasks
- Results show speedup diminishes with higher sampling temperatures, limiting effectiveness for diverse, creative outputs

## Confidence

- **High Confidence**: Fundamental observation that LLMs are memory-bound and that speculative decoding leverages this constraint
- **Medium Confidence**: Categorization of existing methods into drafting strategies and verification methods is systematic and comprehensive
- **Medium Confidence**: Experimental results showing EAGLE's superior performance and PLD's effectiveness in high-similarity tasks
- **Low Confidence**: Predictions about future challenges and directions for speculative decoding

## Next Checks

1. **Cross-Platform Evaluation**: Reproduce Spec-Bench experiments on different GPU architectures (A100, H100) and CPU-based inference to assess generalizability across hardware platforms

2. **Model Scale Analysis**: Evaluate effectiveness of speculative decoding across range of model sizes (1B to 70B parameters) to identify optimal model scale

3. **Long-Context Assessment**: Design and execute experiments specifically targeting long-context scenarios (>4K tokens) to validate claims about speculative decoding's potential in this domain and identify any performance degradation