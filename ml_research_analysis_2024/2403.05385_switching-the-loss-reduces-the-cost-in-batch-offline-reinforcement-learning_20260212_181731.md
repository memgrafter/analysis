---
ver: rpa2
title: Switching the Loss Reduces the Cost in Batch (Offline) Reinforcement Learning
arxiv_id: '2403.05385'
source_url: https://arxiv.org/abs/2403.05385
tags:
- learning
- batch
- policy
- loss
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to batch reinforcement learning
  by replacing the standard squared loss with log-loss in fitted Q-iteration (FQI-LOG).
  The key innovation is showing that this switch enables achieving a small-cost bound,
  meaning the sample complexity scales with the optimal achievable cost rather than
  being worst-case.
---

# Switching the Loss Reduces the Cost in Batch (Offline) Reinforcement Learning

## Quick Facts
- arXiv ID: 2403.05385
- Source URL: https://arxiv.org/abs/2403.05385
- Reference count: 40
- Primary result: FQI-LOG achieves O(√C·v*/n) bound where v* is optimal value, a significant improvement when v* is small

## Executive Summary
This paper introduces a novel approach to batch reinforcement learning by replacing the standard squared loss with log-loss in fitted Q-iteration (FQI-LOG). The key innovation is showing that this switch enables achieving a small-cost bound, meaning the sample complexity scales with the optimal achievable cost rather than being worst-case. The authors prove that FQI-LOG achieves a bound of O(√C·v*/n) where v* is the optimal value, a significant improvement when v* is small. This is the first computationally efficient batch RL algorithm with this property.

## Method Summary
The authors propose FQI-LOG, which iteratively minimizes log-loss between predicted values f(Si,Ai) and targets Ci + γf^(j-1)(S'i) using a regression oracle on a function class F⊆[0,1]^S×A. The algorithm leverages the fact that the Bellman optimality operator is a contraction under Hellinger distance, enabling tighter error propagation analysis compared to using L2 norms. The method requires that the function class is bounded in [0,1] and satisfies a completeness assumption (closed under the Bellman operator).

## Key Results
- FQI-LOG achieves O(√C·v*/n) bound where v* is optimal value, improving when v* is small
- Empirical results on mountain car and inverted pendulum show FQI-LOG learns better policies with fewer samples than FQI using squared loss
- Results on Atari games show log-loss can match or outperform squared loss variants
- This is the first computationally efficient batch RL algorithm with a small-cost bound

## Why This Works (Mechanism)

### Mechanism 1
Log-loss penalizes errors more heavily near the boundaries (0 and 1) than squared loss, making it more sensitive to near-optimal value predictions. The log-loss function grows rapidly as predictions deviate from observed targets near 0 or 1, whereas squared loss increases quadratically but uniformly. This causes log-loss to prioritize fitting values close to the boundaries, which correspond to successful trajectories in sparse-reward problems.

### Mechanism 2
FQI-LOG achieves better sample efficiency when the optimal cost is small because the error bound scales with √v*/n instead of 1/√n. The proof shows that the suboptimality gap decomposes into a term proportional to √v* times the triangular deviation, plus a constant term. When v* is small, the first term dominates and decreases faster with more samples.

### Mechanism 3
Hellinger distance contraction properties enable tighter error propagation analysis compared to using L2 norms. The Bellman optimality operator contracts Hellinger distances by factor √γ, and the change of measure argument introduces factor √C. This allows bounding the distance between estimated and optimal Q-values by the distance between consecutive iterates, which concentrates as 1/√n.

## Foundational Learning

- Concept: Hellinger distance and its relationship to log-loss
  - Why needed here: The proof relies on showing Bellman optimality is a contraction under Hellinger distance, and log-loss concentration bounds use Hellinger loss
  - Quick check question: Why does the inequality (p-q)²/(p+q) ≤ 2(√p-√q)² ≤ 2h²(p,q) matter for connecting triangular deviation to Hellinger distance?

- Concept: Bellman optimality operator and contraction mappings
  - Why needed here: The algorithm iteratively applies the Bellman operator, and contraction properties are essential for error propagation analysis
  - Quick check question: What is the significance of showing T is a γ-contraction with respect to Hellinger distance rather than L2 distance?

- Concept: Performance difference lemma in RL
  - Why needed here: Used to decompose the suboptimality gap into a sum over time steps, which is then bounded using the triangular deviation
  - Quick check question: How does the performance difference lemma help connect the value of a greedy policy to the optimal value function?

## Architecture Onboarding

- Component map: Data pipeline -> Function approximator (logit models with sigmoid) -> Optimizer (BFGS) -> Bellman operator -> Policy extraction
- Critical path:
  1. Initialize f0 randomly from function class
  2. For each iteration j:
     - Compute targets: Ci + γf_{j-1}^∧(S'_i)
     - Minimize log-loss: f_j = arg min_f Σ ℓlog(f(Si,Ai), target_i)
     - Update policy if needed
  3. Return π_{f_k}
- Design tradeoffs:
  - Sigmoid activation vs softmax: Sigmoid is simpler for binary outcomes but softmax generalizes to multi-class
  - BFGS vs SGD: BFGS converges faster for strongly convex losses but doesn't scale to large models
  - Target clipping: Necessary for numerical stability but introduces bias that must be corrected
- Failure signatures:
  - Divergence: If function class is too rich or learning rate too high
  - Slow convergence: If optimal cost is not small or data is insufficient
  - Poor policy: If function class cannot represent optimal Q-values (realizability fails)
- First 3 experiments:
  1. Mountain car with 1 successful trajectory: Verify FQI-LOG learns near-optimal policy while FQI-SQ fails
  2. Inverted pendulum with varying dataset sizes: Show FQI-LOG achieves lower failure rate with fewer samples
  3. Atari games (Asterix, Seaquest): Compare DQN-LOG vs DQN-SQ vs C51 performance curves

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the theoretical framework be extended to infinite function classes like bounded logit models?
  - Basis in paper: The paper mentions that Theorem 5.1 could be extended to infinite model classes with "some extra work and appropriate modifications" but doesn't provide these extensions
  - Why unresolved: The current proof relies on finite function class assumptions and union bounds
  - What evidence would resolve it: A rigorous proof showing small-cost bounds for FQI-LOG with bounded logit models or other infinite function classes

- **Open Question 2**: Does the log-loss advantage persist in online reinforcement learning with linear function approximation?
  - Basis in paper: The authors mention this as an intriguing extension, noting that Wagenmaker et al. (2022) get small-return bounds for online RL in linear MDPs but small-cost bounds haven't been established
  - Why unresolved: The analysis techniques for batch RL don't directly translate to the online setting
  - What evidence would resolve it: Theoretical analysis showing small-cost bounds for online RL algorithms using log-loss with linear function approximation

- **Open Question 3**: What is the optimal way to incorporate goal-oriented objectives in log-loss based algorithms?
  - Basis in paper: The authors note their mountain car experiments indicate log-loss might perform well in goal-oriented MDPs but "it remains to be seen how this could be incorporated in algorithms like ours"
  - Why unresolved: The current framework focuses on cumulative cost without special treatment of goal states
  - What evidence would resolve it: An algorithm that modifies FQI-LOG to explicitly reward goal achievement while penalizing failure

## Limitations
- Theoretical advantages are most pronounced when optimal cost v* is small, which may not hold in many practical scenarios
- Reliance on completeness assumption (function class closed under Bellman operator) is strong and may not be satisfied in practice
- Empirical validation is limited in scope and doesn't fully demonstrate claimed benefits across diverse environments

## Confidence
- **High Confidence**: The mathematical derivation showing Bellman optimality is a contraction under Hellinger distance, and the resulting error bounds for FQI-LOG
- **Medium Confidence**: The empirical results on mountain car and inverted pendulum, which show clear advantages of FQI-LOG in sparse-reward scenarios
- **Medium Confidence**: The claim that log-loss is more sensitive to near-optimal predictions than squared loss, based on theoretical analysis of loss functions

## Next Checks
1. Cross-domain validation: Test FQI-LOG on additional sparse-reward problems (e.g., LunarLander, HalfCheetah with sparse rewards) to verify theoretical advantage holds beyond presented examples
2. Optimization robustness: Compare different optimization approaches (SGD vs BFGS) for minimizing log-loss in FQI-LOG, particularly for deep neural network function approximators
3. Completeness relaxation: Investigate how FQI-LOG performs when completeness assumption is violated, by testing with function classes not closed under Bellman operator