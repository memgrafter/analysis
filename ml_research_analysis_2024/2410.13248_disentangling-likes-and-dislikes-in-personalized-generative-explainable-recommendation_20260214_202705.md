---
ver: rpa2
title: Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation
arxiv_id: '2410.13248'
source_url: https://arxiv.org/abs/2410.13248
tags:
- positive
- negative
- features
- user
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces new datasets and evaluation methods for explainable
  recommendation that explicitly account for user sentiment. The authors construct
  datasets by extracting users' positive and negative opinions from reviews using
  an LLM, and propose evaluation metrics that measure sentiment alignment and feature
  accuracy.
---

# Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation

## Quick Facts
- arXiv ID: 2410.13248
- Source URL: https://arxiv.org/abs/2410.13248
- Reference count: 40
- Key outcome: New datasets and evaluation methods for sentiment-aware explainable recommendation, showing existing metrics miss sentiment alignment

## Executive Summary
This paper addresses a critical gap in explainable recommendation by introducing datasets and evaluation metrics that explicitly model user sentiment. The authors extract positive and negative opinions from reviews using LLMs, then benchmark several generative explanation models to reveal that strong performance on existing metrics does not guarantee sentiment-aware explanations. Their findings demonstrate that incorporating predicted or ground-truth user ratings as input significantly improves sentiment alignment in generated explanations.

## Method Summary
The authors construct new datasets by extracting users' positive and negative opinions from reviews using an LLM. They propose evaluation metrics that measure both sentiment alignment (matching user sentiment in explanations) and feature accuracy (correctly identifying discussed aspects). The benchmarking process compares several generative explanation models on these new metrics, revealing that traditional evaluation approaches fail to capture sentiment-aware explanation quality. They demonstrate that incorporating user ratings as input to models substantially improves sentiment alignment performance.

## Key Results
- Existing evaluation metrics for explainable recommendation do not ensure sentiment-aware explanations
- Incorporating predicted or ground-truth user ratings as input significantly improves sentiment alignment in generated explanations
- The proposed sentiment-aware evaluation metrics reveal gaps in current generative explanation models

## Why This Works (Mechanism)
The approach works by explicitly separating positive and negative user opinions and evaluating explanations on their alignment with these sentiment signals. By extracting structured opinion data from reviews and creating metrics that measure both sentiment alignment and feature accuracy, the method provides a more nuanced evaluation of explanation quality. The incorporation of user ratings as input signals helps models better capture the user's underlying preferences and sentiment patterns, leading to more aligned explanations.

## Foundational Learning
- Sentiment-aware evaluation metrics - Needed to measure whether explanations match user sentiment; Quick check: metrics distinguish between positive and negative opinion alignment
- LLM-based opinion extraction - Required to automatically identify positive/negative opinions from reviews; Quick check: extraction accuracy validated against human annotations
- Feature accuracy measurement - Essential for ensuring explanations discuss relevant aspects; Quick check: precision/recall of mentioned features

## Architecture Onboarding
**Component map:** Review text -> LLM opinion extraction -> Sentiment-labeled dataset -> Generative model -> Sentiment-aware evaluation metrics
**Critical path:** Opinion extraction → Dataset creation → Model training → Sentiment-aware evaluation
**Design tradeoffs:** Automated extraction vs. manual annotation (speed vs. accuracy); model complexity vs. interpretability
**Failure signatures:** Explanations misaligned with user sentiment; features mentioned but irrelevant to user preferences
**3 first experiments:**
1. Compare sentiment alignment across models using ground-truth vs. predicted ratings
2. Evaluate feature accuracy of different explanation generation approaches
3. Benchmark traditional metrics vs. sentiment-aware metrics on same models

## Open Questions the Paper Calls Out
None

## Limitations
- Uncertainty about biases or noise introduced by LLM-based opinion extraction
- Questions about whether rating signal exploitation conflates with genuine sentiment alignment
- Need for validation of sentiment-aware metrics across different domains and review styles

## Confidence
- Existing metrics miss sentiment alignment: Medium
- Rating input improves sentiment alignment: High
- Generalizability across domains: Low

## Next Checks
1. Conduct human evaluation studies to verify accuracy of automatically extracted positive and negative opinions
2. Replicate main experiments using datasets from different domains (movies, products, restaurants)
3. Perform ablation studies to disentangle contributions of rating signals versus genuine sentiment modeling