---
ver: rpa2
title: 'Fair and Accurate Regression: Strong Formulations and Algorithms'
arxiv_id: '2412.17116'
source_url: https://arxiv.org/abs/2412.17116
tags:
- regression
- fairness
- fair
- convex
- relaxation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents mixed-integer optimization formulations for
  training fair regression models by incorporating demographic parity constraints.
  The key innovation is a strong convex relaxation of the fair regression problem,
  derived by extending the convex hull of a set that captures the interplay between
  loss functions and binary fairness indicators.
---

# Fair and Accurate Regression: Strong Formulations and Algorithms

## Quick Facts
- **arXiv ID**: 2412.17116
- **Source URL**: https://arxiv.org/abs/2412.17116
- **Reference count**: 17
- **One-line primary result**: Strong convex relaxations for fair regression outperform convex proxy approaches, especially in high-fairness regimes, with up to 30× runtime improvements.

## Executive Summary
This paper presents mixed-integer optimization formulations for training fair regression models by incorporating demographic parity constraints. The key innovation is a strong convex relaxation of the fair regression problem, derived by extending the convex hull of a set that captures the interplay between loss functions and binary fairness indicators. The method outperforms existing convex proxy approaches, especially in high-fairness regimes, and scales efficiently to large datasets via coordinate descent. Experiments on synthetic and real data show competitive or superior out-of-sample performance compared to state-of-the-art reduction-based and convex-approximation methods.

## Method Summary
The method trains fair regression models by solving a mixed-integer optimization problem that directly incorporates demographic parity constraints. The approach uses a strong convex relaxation that convexifies the discrete substructure introduced by fairness metrics and loss functions. A coordinate descent algorithm efficiently improves solutions by exploiting the piecewise constant nature of the fairness regularizer. The method supports least squares and logistic regression and extends to generalized linear models. Three main approaches are presented: strong convex relaxation, coordinate descent initialized with relaxation solution, and mixed-integer optimization for exact solutions.

## Key Results
- Strong convex relaxation provides better out-of-sample performance than convex proxy methods, especially in high-fairness regimes
- Coordinate descent algorithm improves fairness while maintaining accuracy, with runtime improvements up to 30× over exact mixed-integer optimization
- Method scales efficiently to large datasets and outperforms state-of-the-art reduction-based and convex-approximation methods on multiple real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The strong convex relaxation in Corollary 1 is exact for single-observation and single-factor fair regression because it convexifies the discrete substructure introduced by fairness metrics and loss functions.
- Mechanism: The relaxation extends the perspective reformulation technique to model indicators over decision variables, capturing the interplay between the loss function L(v) and binary variables z_j that represent interval membership. This is achieved by introducing auxiliary variables p_j that model the portion of each prediction that falls within intervals defined by breakpoints b_j.
- Core assumption: The set X can be represented by its convex hull using the extended formulation with perspective reformulation, and this hull is closed.
- Evidence anchors:
  - [abstract]: "The key innovation is a strong convex relaxation of the fair regression problem, derived by extending the convex hull of a set that captures the interplay between loss functions and binary fairness indicators."
  - [section]: "We show that X can be used to model a single-factor or a single-observation fair regression as a building block for modeling and solving the general fair regression problems."
- Break condition: The relaxation becomes inexact when the loss function L(v) is not convex or when multiple factors (n > 1) are involved and the single-factor assumption is violated.

### Mechanism 2
- Claim: The coordinate descent algorithm efficiently improves solutions by exploiting the structure of the fairness regularizer, which can be computed by solving a sequence of O(m × ℓ) subproblems.
- Mechanism: At each iteration, the algorithm fixes all but one coordinate and solves a univariate problem over a discrete set of candidates. The fairness regularizer R(w_k) is efficiently updated by maintaining a sorted list of candidates and using a recursive relationship to update the binary variables z.
- Core assumption: The fairness regularizer R(w_k) is piecewise constant over intervals defined by the sorted candidates, and its value can be computed recursively.
- Evidence anchors:
  - [section]: "Given a solution w_t and coordinate direction k, first compute b_ij, i ∈ [m], j ∈ [ℓ], and set w_t+1_k to the best solution among these points and the unconstrained solution that ignores fairness."
- Break condition: The algorithm may converge to a local optimum if the objective is non-convex, and the quality of the solution depends on the initial point.

### Mechanism 3
- Claim: The strong convex relaxation provides better out-of-sample performance than convex proxy methods because it considers fairness and accuracy simultaneously, rather than using approximations that underestimate the fairness constraint.
- Mechanism: The relaxation uses the exact fairness metric dDPℓ, while convex proxy methods like those in Zafar et al. (2017) and Wu et al. (2019) use linear or non-linear approximations. The strong relaxation avoids the bias introduced by these approximations, leading to models that are both fair and accurate.
- Core assumption: The exact fairness metric dDPℓ is a better measure of fairness than the convex approximations used in the literature.
- Evidence anchors:
  - [section]: "The relaxation (17) exploits the non-linear objective, allowing for a convexification that considers fairness and accuracy simultaneously."
- Break condition: The relaxation may not be as effective when the discretization ℓ is too coarse, leading to an inaccurate approximation of the exact fairness metric.

## Foundational Learning

- Concept: Mixed-integer optimization (MIO) and convex relaxations
  - Why needed here: The fair regression problem involves binary variables representing fairness indicators, making it a mixed-integer problem. Convex relaxations are used to obtain tractable approximations of the problem.
  - Quick check question: What is the difference between a mixed-integer program and its convex relaxation?

- Concept: Perspective reformulation
  - Why needed here: The perspective reformulation is used to model indicators over decision variables in the convex relaxation, capturing the interplay between the loss function and fairness indicators.
  - Quick check question: How does the perspective reformulation extend the convex hull of a set involving a convex function and binary variables?

- Concept: Coordinate descent and piecewise constant functions
  - Why needed here: The coordinate descent algorithm exploits the piecewise constant nature of the fairness regularizer to efficiently update solutions.
  - Quick check question: Why is the fairness regularizer piecewise constant over intervals defined by sorted candidates?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training (Relax/CD-relax/MICQO) -> Evaluation (accuracy/fairness metrics) -> Visualization (trade-off curves)

- Critical path:
  1. Load and preprocess data
  2. Define fairness thresholds and discretization
  3. Choose training method (Relax, CD-relax, or MICQO)
  4. Train model and obtain regression coefficients w
  5. Evaluate accuracy and fairness on training and testing sets
  6. Visualize results

- Design tradeoffs:
  - Relax vs. CD-relax: Relax is faster but may not achieve the highest fairness levels; CD-relax is slower but can produce more fair models
  - MICQO vs. Relax/CD-relax: MICQO provides exact solutions but is computationally expensive; Relax/CD-relax provide approximations but are faster
  - Discretization granularity: Finer discretization (larger ℓ) leads to better fairness but increases computational cost

- Failure signatures:
  - Poor fairness: Relax may not achieve dDP < 0.07; CD-relax may converge to a local optimum
  - Poor accuracy: Too much emphasis on fairness regularization may lead to high relative loss increase
  - Computational issues: MICQO may not scale to large datasets; Relax may be too slow for real-time applications

- First 3 experiments:
  1. Run Relax on a small synthetic dataset with m = 15, n = 10, and compare the results with MICQO to verify the strength of the relaxation.
  2. Run CD-relax on the same dataset, initializing with the solution from Relax, to demonstrate the improvement in fairness.
  3. Run Relax on a real dataset (e.g., Communities & Crime) and compare the results with FR-Reduction to evaluate the out-of-sample performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed coordinate descent method scale when applied to high-dimensional feature spaces with thousands of features?
- Basis in paper: [inferred] The paper mentions the method can handle thousands of data points but does not explicitly discuss high-dimensional feature spaces or scalability beyond moderate dimensions.
- Why unresolved: The experiments focus on moderate dimensions (n=10), and the paper does not provide theoretical or empirical analysis for high-dimensional settings.
- What evidence would resolve it: Experimental results demonstrating the method's performance on datasets with high-dimensional feature spaces (e.g., n > 1000) would provide clarity.

### Open Question 2
- Question: What are the theoretical guarantees for the convergence rate and optimality gap of the coordinate descent method in the presence of fairness constraints?
- Basis in paper: [inferred] While the paper describes the algorithm and its empirical performance, it does not provide theoretical convergence analysis or bounds on the optimality gap.
- Why unresolved: The paper focuses on algorithmic design and empirical validation but lacks a formal theoretical framework for analyzing convergence.
- What evidence would resolve it: A theoretical analysis proving convergence rates and optimality gap bounds for the coordinate descent method under fairness constraints would address this gap.

### Open Question 3
- Question: How does the choice of discretization granularity (number of thresholds) affect the trade-off between computational efficiency and approximation accuracy in the proposed methods?
- Basis in paper: [explicit] The paper mentions that the discretization of thresholds is used to approximate exact fairness but does not systematically analyze how different levels of granularity impact the results.
- Why unresolved: The paper provides empirical results for specific discretization choices but does not explore the sensitivity of the methods to varying granularity levels.
- What evidence would resolve it: A sensitivity analysis comparing the performance of the methods across different discretization granularities (e.g., ℓ = 10, 20, 50, 100) would provide insights into this trade-off.

### Open Question 4
- Question: How does the proposed strong convex relaxation perform in comparison to exact mixed-integer optimization methods for larger-scale problems where exact methods are computationally infeasible?
- Basis in paper: [explicit] The paper states that exact mixed-integer optimization methods do not scale well for large instances (m > 100) and that the strong convex relaxation is computationally efficient.
- Why unresolved: The paper does not provide a direct comparison between the strong convex relaxation and exact methods for larger-scale problems, as exact methods are impractical for such cases.
- What evidence would resolve it: Benchmarking the strong convex relaxation against approximate mixed-integer optimization methods or other scalable heuristics for larger-scale problems would clarify its performance.

## Limitations

- The strong convex relaxation's exactness is limited to single-observation and single-factor cases, with performance degrading for multiple factors
- Coordinate descent algorithm lacks rigorous convergence guarantees and may get stuck in local optima for non-convex objectives
- Empirical validation focuses primarily on convex proxy methods, with limited comparison to other exact mixed-integer formulations or reduction-based approaches

## Confidence

- **High confidence**: The theoretical derivation of the strong convex relaxation and its exactness for single-factor problems. The mechanism of extending perspective reformulation to model fairness indicators is well-supported by the mathematical framework.
- **Medium confidence**: The empirical superiority of the strong relaxation over convex proxy methods, particularly in high-fairness regimes. While supported by experimental results, the comparison could be strengthened with more diverse baseline methods.
- **Low confidence**: The claim that the coordinate descent algorithm consistently finds high-quality solutions. The method's dependence on initialization and lack of convergence guarantees make this claim uncertain.

## Next Checks

1. **Multi-factor scalability test**: Evaluate the strong relaxation's performance on fair regression problems with n > 1 factors. Compare the solution quality and runtime against the exact mixed-integer formulation to quantify the relaxation gap in multi-factor scenarios.

2. **Coordinate descent robustness analysis**: Implement multiple random initializations for the coordinate descent algorithm and measure the variance in final solutions. Test on non-convex loss functions to assess the algorithm's ability to escape local optima.

3. **Memory and parallelization study**: Profile the memory usage of the strong relaxation and coordinate descent methods on large datasets (m > 1000). Explore parallelization strategies for the fairness regularizer computation and evaluate their impact on scalability.