---
ver: rpa2
title: Training-Free Activation Sparsity in Large Language Models
arxiv_id: '2408.14690'
source_url: https://arxiv.org/abs/2408.14690
tags:
- sparsity
- arxiv
- https
- teal
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TEAL introduces a training-free approach to activation sparsity
  in large language models by applying magnitude-based pruning to hidden states throughout
  the entire model. The method leverages the observation that activations in LLaMA-architecture
  models follow zero-mean unimodal distributions, enabling effective sparsification
  through threshold-based pruning.
---

# Training-Free Activation Sparsity in Large Language Models

## Quick Facts
- arXiv ID: 2408.14690
- Source URL: https://arxiv.org/abs/2408.14690
- Reference count: 40
- 40-50% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families

## Executive Summary
TEAL introduces a training-free approach to activation sparsity for efficient inference in large language models by applying magnitude-based pruning to hidden states throughout the entire model. The method exploits the observation that activations in LLaMA-architecture models follow zero-mean unimodal distributions, enabling effective sparsification through threshold-based pruning. TEAL achieves 40-50% model-wide sparsity with minimal performance degradation and demonstrates wall-clock decoding speed-ups of up to 1.53× and 1.8× at 40% and 50% sparsity respectively through specialized sparse kernels.

## Method Summary
TEAL implements training-free activation sparsity by first collecting activations from Llama-3-8B to estimate distributional properties and determine thresholds for desired sparsity levels. The method applies magnitude-based pruning to all weight matrices in Transformer blocks using these thresholds, with a block-wise greedy optimization algorithm to distribute sparsity across layers. Specialized sparse GEMV kernels are developed for acceleration, and the approach is evaluated on WikiText and downstream tasks using EleutherAI LM Harness. The implementation uses GPT-Fast with CUDA graphs and torch.compile for wall-clock speed measurements.

## Key Results
- Achieves 40-50% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families (7B to 70B parameters)
- Demonstrates wall-clock decoding speed-ups of up to 1.53× and 1.8× at 40% and 50% sparsity respectively
- Shows compatibility with weight quantization for additional efficiency gains, with errors from activation sparsity and weight quantization compounding somewhat independently

## Why This Works (Mechanism)

### Mechanism 1
Magnitude-based pruning works because activations in Llama-architecture models follow zero-mean unimodal distributions, where low-magnitude activations contribute less to output computation and can be removed without significantly affecting model output. This assumption breaks down if activation distributions become multimodal or non-zero-mean.

### Mechanism 2
Input sparsity is more effective than output sparsity for SwiGLU-based models in the training-free setting because input sparsity preserves information content before gating transformations, while output sparsity applies sparsification after the gating mechanism which may have already attenuated important signals. This could change if SiLU gating provides reliable saliency information.

### Mechanism 3
The block-wise greedy optimization algorithm effectively distributes sparsity across model layers by iteratively adding sparsity to the layer that causes the least activation error at each step. This approach may fail if error propagation between layers is non-linear rather than approximately additive.

## Foundational Learning

- **Zero-mean unimodal distributions**: Why needed - the paper relies on activations following this pattern to justify magnitude-based pruning. Quick check - what property of the activation distribution makes magnitude-based pruning effective?
- **Input vs output sparsity tradeoffs**: Why needed - the paper makes design choices between input and output sparsity that affect performance. Quick check - why does the paper choose input sparsity over output sparsity for Wup?
- **Greedy optimization algorithms**: Why needed - the paper uses a greedy algorithm to distribute sparsity across model layers. Quick check - how does the greedy algorithm decide which layer to sparsify next?

## Architecture Onboarding

- **Component map**: Transformer blocks with attention (Wq, Wk, Wv, Wo) and MLP (Wgate, Wup, Wdown) components; magnitude-based sparsification function applied to activations; specialized sparse GEMV kernels for acceleration
- **Critical path**: 1) Activation collection and distribution analysis, 2) Block-wise greedy sparsity optimization, 3) Sparse kernel development and optimization, 4) End-to-end integration and benchmarking
- **Design tradeoffs**: Input sparsity vs output sparsity (memory format vs effectiveness), uniform vs layer-wise sparsity distribution (simplicity vs performance), training-free vs fine-tuned approaches (deployment ease vs performance)
- **Failure signatures**: Significant performance degradation at high sparsity levels, kernel optimization not providing expected speedups, distributional assumptions breaking (non-zero-mean or multimodal activations)
- **First 3 experiments**: 1) Verify activation distributions follow zero-mean unimodal patterns on target model, 2) Test input vs output sparsity on Wup layer to confirm performance difference, 3) Benchmark greedy vs uniform sparsity distribution on a small model segment

## Open Questions the Paper Calls Out

### Open Question 1
Why do Llama-3 models exhibit lower sparsifiability compared to Llama-2 and Mistral models at the same sparsity levels? The authors observe that Llama-3 variants show more degradation compared to older Llama-2 and Mistral variants, but don't investigate the architectural or training differences that cause this discrepancy in sparsifiability between model families.

### Open Question 2
How do activation distributions and sparsifiability evolve across different layers within a single model? The authors observe distinct patterns in layer-level sparsifiability but don't explain the underlying mechanisms driving these variations or their relationship to model functionality.

### Open Question 3
What is the optimal balance between activation sparsity and weight sparsity (e.g., quantization) for maximum efficiency? The authors demonstrate compatibility with weight quantization but don't explore optimal combinations or find Pareto-optimal configurations for different use cases.

### Open Question 4
How does activation sparsity affect model robustness and generalization beyond standard evaluation metrics? The authors evaluate on standard benchmarks but don't examine robustness properties like adversarial resistance, out-of-distribution performance, or fine-tuning stability.

## Limitations
- The approach relies heavily on the assumption that activation distributions consistently follow zero-mean unimodal patterns, which may not generalize across all model sizes
- The block-wise greedy optimization algorithm uses a simple heuristic that may not find globally optimal sparsity distributions
- The generalizability of the approach to non-Llama architectures and the long-term stability of performance at high sparsity levels remain uncertain

## Confidence
High confidence: Core observation about zero-mean unimodal distributions and basic magnitude-based pruning mechanism; wall-clock speed-up measurements
Medium confidence: Effectiveness of block-wise greedy optimization algorithm; performance claims at 40-50% sparsity levels
Low confidence: Generalizability to non-Llama architectures; long-term stability at high sparsity levels

## Next Checks
1. Test the zero-mean unimodal distribution assumption on all model sizes (7B, 13B, 34B, 70B) using multiple datasets to verify consistency across the entire model family
2. Apply TEAL to a non-Llama architecture (such as OPT or GPT-NeoX) to validate whether the magnitude-based pruning approach works when activation distributions differ from the assumed pattern
3. Implement a dynamic sparsity version of TEAL that adapts thresholds based on input characteristics, and compare its performance against the static approach to assess the cost of simplicity