---
ver: rpa2
title: 'Chain of Logic: Rule-Based Reasoning with Large Language Models'
arxiv_id: '2402.10400'
source_url: https://arxiv.org/abs/2402.10400
tags:
- reasoning
- chain
- rule
- jurisdiction
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Chain of Logic, a novel prompting method for
  improving rule-based reasoning in large language models (LLMs). The method addresses
  the challenge of reasoning about compositional rules, which require multiple reasoning
  steps and attending to logical relationships between rule elements.
---

# Chain of Logic: Rule-Based Reasoning with Large Language Models

## Quick Facts
- **arXiv ID**: 2402.10400
- **Source URL**: https://arxiv.org/abs/2402.10400
- **Reference count**: 9
- **Primary result**: Chain of Logic improves rule-based reasoning performance by 3.9% on average compared to best baseline methods

## Executive Summary
Chain of Logic is a novel prompting method that improves rule-based reasoning in large language models by decomposing compositional rules into elements, solving them independently, and recomposing the sub-answers to resolve the underlying logical expression. Inspired by the IRAC framework used by lawyers, the method addresses the challenge of reasoning about rules with multiple elements connected by logical operators (AND/OR). Experiments across eight rule-based reasoning tasks from the LegalBench benchmark demonstrate that Chain of Logic consistently outperforms other prompting methods, including chain of thought and self-ask, using both open-source and commercial LLMs.

## Method Summary
Chain of Logic is a prompting method that improves rule-based reasoning by breaking down compositional rules into sub-tasks (elements), solving each element as a separate logical statement, and then recombining these sub-answers to resolve the underlying logical expression. The method uses a structured six-step process: (1) format inputs with clear labels, (2) decompose rules into elements and assign variables, (3) generate logical expressions representing relationships, (4) iterate through elements rephrasing as questions and solving, (5) replace element variables with sub-answers, and (6) resolve the final logical expression. The method was evaluated on eight rule-based reasoning tasks from the LegalBench benchmark, comparing performance against baseline methods like zero-shot, standard prompting, chain of thought, and self-ask.

## Key Results
- Chain of Logic improves performance by 3.9% on average compared to the best baseline method
- The method particularly excels in tasks requiring arithmetic reasoning
- Removing structured input formatting causes a 25.9 percentage point performance decrease
- Chain of Logic consistently outperforms other prompting methods across different model types (GPT-3.5, GPT-4, Llama-2-70b, Mistral-7B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain of Logic improves performance by explicitly decomposing rules into elements and recomposing them after solving each element independently
- Mechanism: The method breaks down compositional rules into sub-tasks (elements), solves each element as a separate logical statement, and then recombines these sub-answers to resolve the underlying logical expression
- Core assumption: Large language models struggle with multi-step reasoning when elements are processed together due to complexity and potential interference between elements
- Evidence anchors:
  - [abstract] "Chain of Logic, which elicits rule-based reasoning through decomposition (solving elements as independent threads of logic), and recomposition (recombining these sub-answers to resolve the underlying logical expression)"
  - [section] "Our method builds on chain of thought and self-ask by not only considering problem decomposition, but also attending to relationships between subtasks that can dictate the final answer"
- Break condition: If the model cannot reliably identify and extract rule elements from natural language text, the decomposition step fails

### Mechanism 2
- Claim: The structured input format in Step 1 significantly improves model performance by providing clear task boundaries and expectations
- Mechanism: By explicitly labeling task inputs (rule, facts, issue), the model receives clear structural guidance that helps it understand what components it needs to process and how they relate to each other
- Core assumption: Unstructured inputs create ambiguity for the model about which parts of the text are the rule, which are facts, and which are the question to be answered
- Evidence anchors:
  - [section] "During inference, the model performs each step in this process automatically given only a one-shot example and the inference task inputs (formatted as described in step 1)"
  - [section] "Unsurprisingly, removing step 1, switching from structured to unstructured task inputs, leads to a 25.9 absolute percentage point performance decrease"
- Break condition: If the model already has strong comprehension of natural language structure and can reliably distinguish rule components without explicit formatting

### Mechanism 3
- Claim: The recomposition step (Steps 5-6) prevents errors that occur when models have correct element-level answers but fail to properly evaluate the overall logical expression
- Mechanism: After solving individual elements, the method explicitly replaces element variables in the logical expression with sub-answers and resolves the complete expression, ensuring the final answer reflects the correct logical relationships
- Core assumption: Models using simpler methods like chain of thought or self-ask can correctly answer individual elements but make errors in the final logical evaluation step
- Evidence anchors:
  - [section] "The output in the latter figure shows the model only asks intermediate questions for two out of three rule elements, omitting whether the defendant had sufficient contact with the forum state"
  - [section] "The model correctly determines the defendant is domiciled in the forum state, which is sufficient to conclude personal jurisdiction exists, but still reaches the wrong final answer"
- Break condition: If the logical expressions become too complex for the model to reliably evaluate even with explicit recomposition

## Foundational Learning

- **Concept**: Rule-based reasoning and compositional rules
  - Why needed here: The entire method is designed specifically for compositional rules that have multiple elements connected by logical operators (AND/OR), which require different reasoning strategies than simple rules
  - Quick check question: Can you identify the rule elements and logical operators in this statement: "A court has jurisdiction if the defendant is domiciled in the state AND has sufficient contacts with the state"?

- **Concept**: Chain of thought and self-ask prompting methods
  - Why needed here: Chain of Logic builds upon these existing methods, so understanding their mechanisms and limitations is essential for understanding why Chain of Logic improves upon them
  - Quick check question: How does self-ask differ from chain of thought in terms of how it handles intermediate reasoning steps?

- **Concept**: IRAC framework (Issue, Rule, Application, Conclusion)
  - Why needed here: Chain of Logic is explicitly inspired by the rule application step of IRAC, which lawyers use to break down rules into elements and address them separately
  - Quick check question: In the IRAC framework, what is the purpose of the "Application" step and how does it relate to Chain of Logic's decomposition-recomposition approach?

## Architecture Onboarding

- **Component map**: Input formatting -> Rule decomposition -> Logical expression generation -> Question-answering iterator -> Element recomposition -> Expression resolver -> One-shot example store and retriever

- **Critical path**:
  1. Receive task with rule, facts, and question
  2. Format inputs with clear labels (Step 1)
  3. Decompose rule into elements and assign variables (Step 2)
  4. Generate logical expression representing relationships (Step 3)
  5. Iterate through elements, rephrasing as questions and solving (Step 4)
  6. Replace element variables with sub-answers (Step 5)
  7. Resolve final logical expression (Step 6)
  8. Return final answer

- **Design tradeoffs**:
  - Prompt complexity vs. performance: Chain of Logic uses longer, more structured prompts but achieves better performance
  - Model independence vs. task specificity: The method works across different rules without fine-tuning but requires careful prompt engineering
  - Transparency vs. efficiency: The detailed reasoning path aids explainability but increases token usage

- **Failure signatures**:
  - If element decomposition fails, the model will produce incomplete or incorrect element identification
  - If logical expression generation fails, the model will produce incorrect or missing logical operators
  - If question-answering iterator fails, some elements will be omitted or incorrectly rephrased
  - If recomposition fails, the model will have correct element answers but incorrect final evaluation

- **First 3 experiments**:
  1. Compare structured vs. unstructured input formatting on a single rule task to quantify the impact of Step 1
  2. Test element decomposition accuracy by providing the same rule with different fact patterns and checking if the model identifies the same elements
  3. Evaluate logical expression generation by checking if the model correctly identifies AND/OR relationships between elements across different rules

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does Chain of Logic perform on real-world legal rules that are more complex than those in LegalBench?
- **Basis in paper**: [inferred] The paper acknowledges that LegalBench rules are simplified for objective answers, but real-world rules can be more complex
- **Why unresolved**: The experiments were conducted on simplified LegalBench tasks, not real-world legal rules
- **What evidence would resolve it**: Testing Chain of Logic on real-world legal rules and comparing its performance to other methods

### Open Question 2
- **Question**: How does Chain of Logic perform on tasks that require reasoning beyond rule application, such as identifying the applicable rule?
- **Basis in paper**: [inferred] The paper mentions that Chain of Logic currently only addresses rules where the solution depends on whether the antecedent has been triggered, but real-world rules can contain complex consequences requiring additional reasoning
- **Why unresolved**: The experiments focused on rule application, not rule identification or other forms of reasoning
- **What evidence would resolve it**: Testing Chain of Logic on tasks requiring rule identification and other forms of reasoning, and comparing its performance to other methods

### Open Question 3
- **Question**: How does Chain of Logic perform on tasks that require access to external knowledge, such as term definitions?
- **Basis in paper**: [inferred] The paper suggests that incorporating retrieval-augmented generation could be useful in the legal domain where terms have distinct meanings based on jurisdiction or contract
- **Why unresolved**: The experiments did not explore the use of external knowledge
- **What evidence would resolve it**: Testing Chain of Logic with retrieval-augmented generation on tasks requiring external knowledge, and comparing its performance to other methods

## Limitations

- The method relies heavily on successful rule element decomposition, which may fail with complex or ambiguously worded rules
- Experiments focus exclusively on legal reasoning tasks from LegalBench, limiting generalizability to other domains
- Performance gains, while statistically significant, are relatively modest (3.9% average improvement), suggesting the method may not be transformative for all rule-based reasoning tasks

## Confidence

- **High confidence**: The decomposition-recomposition mechanism works as described, supported by clear evidence that removing structured inputs causes a 25.9 percentage point performance drop
- **Medium confidence**: The performance improvements generalize across different model types (open-source and commercial), though the absolute gains vary by task and model
- **Medium confidence**: The method particularly excels at arithmetic reasoning within rules, though the paper provides limited analysis of why this is the case

## Next Checks

1. Test the method's robustness to poorly formatted or ambiguous rules where element decomposition is not straightforward, to identify the failure conditions for the decomposition step.

2. Apply Chain of Logic to non-legal rule-based reasoning tasks (e.g., mathematical word problems or logical puzzles) to assess domain generalizability beyond the LegalBench benchmark.

3. Conduct ablation studies removing individual steps (particularly the recomposition steps) to quantify the contribution of each component to the overall performance improvement.