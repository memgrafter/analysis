---
ver: rpa2
title: 'Activations Through Extensions: A Framework To Boost Performance Of Neural
  Networks'
arxiv_id: '2408.03599'
source_url: https://arxiv.org/abs/2408.03599
tags:
- activation
- functions
- neural
- networks
- relu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework based on function extensions to
  explain and unify improvements from novel activation functions in neural networks.
  The core idea is that parameterized activation functions act as extensions of standard
  ones, expanding the function space and allowing better data fit.
---

# Activations Through Extensions: A Framework To Boost Performance Of Neural Networks

## Quick Facts
- arXiv ID: 2408.03599
- Source URL: https://arxiv.org/abs/2408.03599
- Reference count: 6
- One-line primary result: Learnable activation functions as extensions of standard ones can improve neural network performance by expanding the function space.

## Executive Summary
This paper introduces a framework that explains and unifies improvements from novel activation functions in neural networks by viewing them as extensions of standard activations. The core insight is that parameterized activation functions expand the function space, enabling better data fitting. The authors propose learnable linear (LLA) and quadratic (QLA) activation functions as combinations of a library of standard activations, and demonstrate their effectiveness on synthetic test functions and real-world time series datasets.

## Method Summary
The method introduces learnable activation functions (LLA and QLA) as extensions of standard activations, where LLA is a linear combination and QLA is a quadratic combination of a library of standard activations. These are implemented in feedforward neural networks and trained end-to-end with standard optimization algorithms. The approach is evaluated on synthetic benchmark functions (Ackley, Shubert, etc.) and real-world time series datasets (ETTh1, ETTh2, ETTm1, ETTm2) using MSE and MAE as metrics. The network architecture consists of 2 hidden layers with 20 nodes each for synthetic functions and 2 hidden layers with 7 nodes each for time series.

## Key Results
- QLA and LLA outperform standard activations on synthetic test functions, with QLA generally superior.
- On time series forecasting, QLA matches or exceeds ReLU, especially in modeling periodic patterns.
- The approach adds modest computational cost but can improve model accuracy by learning custom activation shapes suited to the data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The extension framework unifies and explains the performance gains of existing activation functions by expanding the function space.
- Mechanism: By defining parameterized activation functions as extensions of standard ones, the framework allows the neural network to search in a richer function space, enabling better fitting to data.
- Core assumption: Larger function spaces inherently lead to better data fitting if optimized correctly.
- Evidence anchors:
  - [abstract] "the core idea is that parameterized activation functions act as extensions of standard ones, expanding the function space and allowing better data fit"
  - [section] "A probable way to accomplish this task is to expand the search space (i.e. search in H ′ ⊇ H as minH ′ Ep [L (y, f(x, θ))] ≤ minH Ep [L (y, f(x, θ))])"
  - [corpus] Weak - no direct corpus evidence for this specific unification claim.
- Break condition: If the optimization process cannot effectively navigate the expanded space, or if the added complexity leads to overfitting.

### Mechanism 2
- Claim: Learnable activations (LLA and QLA) adapt to data intricacies, improving model accuracy.
- Mechanism: By learning the parameters of the activation function during training, the network can shape the activation to better capture complex input-output relationships.
- Core assumption: The data contains patterns that can be better modeled with custom-shaped activation functions.
- Evidence anchors:
  - [section] "The learned activation functions appear to adapt to the intricacies of the surface of the test function, especially in the case of QLA"
  - [section] "we notice similar phenomena in higher dimensions as well, also evident from experiments on real-world datasets described below"
  - [corpus] Weak - no direct corpus evidence for LLA/QLA specifically.
- Break condition: If the learned activations overfit to noise, or if the additional parameters hinder convergence.

### Mechanism 3
- Claim: Extensions dominate restrictions on a given dataset, leading to better performance.
- Mechanism: Since extensions are more general forms of functions, they can always match or outperform their restrictions on the same data distribution.
- Core assumption: The relationship between data and output can be captured by some function in the extended space.
- Evidence anchors:
  - [section] "extensions dominate restrictions on a given dataset/data distribution"
  - [section] "the relation extension forms a partial order in the space of neural networks and extensions dominate restrictions on a given dataset/data distribution"
  - [corpus] Weak - no direct corpus evidence for this dominance claim.
- Break condition: If the extended space introduces numerical instability or if the increased complexity outweighs the benefits.

## Foundational Learning

- Concept: Function extensions and restrictions
  - Why needed here: The paper's framework is built on the mathematical concept of extending functions, which is central to understanding how learnable activations improve performance.
  - Quick check question: Given f(x) = max(0, x) (ReLU), is g(x, a) = max(0, x) + a an extension of f? Why or why not?

- Concept: Statistical learning theory and function space optimization
  - Why needed here: The paper leverages statistical learning theory to justify why expanding the hypothesis space through extensions can lead to better generalization.
  - Quick check question: If H' ⊇ H, what can we say about min_{θ∈Θ'} Ep[L(y, f(x, θ))] compared to min_{θ∈Θ} Ep[L(y, f(x, θ))]?

- Concept: Feedforward neural network architecture
  - Why needed here: Understanding how activations are applied in feedforward networks is crucial for implementing the learnable activation functions.
  - Quick check question: In a feedforward network with L layers, how is the output of layer l computed from the output of layer l-1?

## Architecture Onboarding

- Component map:
  Input layer -> Hidden layers (with LLA/QLA) -> Output layer

- Critical path:
  1. Initialize network weights and biases
  2. Initialize learnable activation parameters (e.g., λ_i for LLA)
  3. Forward pass: Compute activations using learned parameters
  4. Compute loss (e.g., MSE)
  5. Backward pass: Compute gradients for weights, biases, and activation parameters
  6. Update parameters using optimizer (e.g., Adam)

- Design tradeoffs:
  - LLA vs QLA: LLA is less computationally expensive but may have less expressive power than QLA
  - Library size: Larger libraries increase expressiveness but also computational cost and risk of overfitting
  - Parameter constraints: Adding constraints (e.g., λ_i >= 0) can help with stability but may limit expressiveness

- Failure signatures:
  - Poor convergence: May indicate issues with initialization or too large a library
  - Overfitting: May suggest the learned activations are too complex for the data
  - Numerical instability: Could be caused by unconstrained learnable parameters leading to extreme activation shapes

- First 3 experiments:
  1. Implement LLA on a simple synthetic dataset (e.g., Shubert function) and compare performance to standard activations
  2. Test the effect of different library sizes on LLA performance
  3. Implement QLA on the same synthetic dataset and compare to LLA

## Open Questions the Paper Calls Out
- How do learnable activation functions like LLA and QLA generalize across different network architectures (e.g., CNNs, transformers) and tasks (e.g., NLP, computer vision) compared to fixed activations?
- What are the theoretical convergence guarantees for training neural networks with LLA and QLA activations under standard optimization algorithms?
- How should the library of activation functions S be optimally selected for a given problem domain to balance expressiveness and optimization difficulty?

## Limitations
- Limited empirical validation beyond synthetic functions and time series data
- Modest computational overhead of learnable activations
- Lack of ablation studies on library size effects

## Confidence
- Claims about unifying framework: Low confidence
- Performance gains on synthetic functions: Medium confidence
- Performance gains on time series tasks: Low confidence

## Next Checks
1. **Library Size Sensitivity**: Conduct experiments varying the number and types of base activations in the library to determine the optimal size and composition for LLA/QLA performance.
2. **Inductive Bias Comparison**: Compare the learned activation functions' shapes to hand-designed activations like ReLU and GELU to assess whether the framework truly discovers novel beneficial shapes or merely interpolates between known functions.
3. **Generalization Across Domains**: Test the framework on diverse datasets (e.g., image classification, natural language processing) to evaluate whether the benefits extend beyond the synthetic and time series tasks presented.