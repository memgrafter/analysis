---
ver: rpa2
title: 'FredNormer: Frequency Domain Normalization for Non-stationary Time Series
  Forecasting'
arxiv_id: '2410.01860'
source_url: https://arxiv.org/abs/2410.01860
tags:
- time
- frequency
- forecasting
- normalization
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of distribution shift in non-stationary
  time series forecasting by proposing a frequency-domain normalization method called
  FredNormer. While existing methods normalize in the time domain, they uniformly
  scale all frequency components, which fails to identify stable frequencies crucial
  for robust forecasting.
---

# FredNormer: Frequency Domain Normalization for Non-stationary Time Series Forecasting

## Quick Facts
- arXiv ID: 2410.01860
- Source URL: https://arxiv.org/abs/2410.01860
- Authors: Xihao Piao; Zheng Chen; Yushun Dong; Yasuko Matsubara; Yasushi Sakurai
- Reference count: 40
- Primary result: Improves averaged MSE of backbone forecasting models by 33.3% and 55.3% on ETTm2 dataset

## Executive Summary
This paper addresses distribution shift in non-stationary time series forecasting by proposing FredNormer, a frequency-domain normalization method. Unlike existing time-domain normalization methods that uniformly scale all frequency components, FredNormer first computes frequency stability using a coefficient of variation-based metric, then applies learnable weighting in the frequency domain to emphasize stable components. The method is designed as a plug-and-play module that operates only on input data, requiring minimal parameters while achieving significant performance improvements across multiple benchmark datasets.

## Method Summary
FredNormer tackles non-stationary time series forecasting by normalizing inputs in the frequency domain rather than the time domain. The method computes a stability score for each frequency component across the training set using the ratio of mean to standard deviation, then applies learnable linear projections to weight these components. After transformation back to the time domain, the normalized input is fed to the forecasting model. This approach allows stable frequencies to be up-weighted while unstable ones are down-weighted, addressing the limitation of uniform scaling in traditional normalization methods.

## Key Results
- Achieves 18 top-1 and 6 top-2 results out of 28 settings compared to baseline normalization methods
- Improves averaged MSE by 33.3% and 55.3% on ETTm2 dataset
- Runs 60-70% faster than state-of-the-art normalization method in 16 out of 28 settings
- Tested across 7 datasets: Electricity, ETTh1, ETTh2, ETTm1, ETTm2, Traffic, Weather

## Why This Works (Mechanism)

### Mechanism 1
Time-domain normalization uniformly scales non-zero frequency components, failing to distinguish stable from unstable frequencies. Lemma 1 proves that z-score normalization in the time domain results in all non-zero frequency components being scaled by the same factor (1/σ(X)), which means the relative proportions of stable vs unstable frequencies remain unchanged.

### Mechanism 2
FredNormer uses frequency stability metrics to selectively weight frequency components based on their statistical significance across the training set. FredNormer computes a stability score S(k) = μ(A(k))/σ(A(k)) for each frequency component, then applies learnable linear projections to adjust these weights, effectively up-weighting stable components and down-weighting unstable ones.

### Mechanism 3
FredNormer operates only on input data, making it a plug-and-play module that doesn't compromise efficiency. By transforming input to frequency domain, applying stability-based weighting, and transforming back, FredNormer requires minimal parameters (only linear layers) and no changes to the forecasting model architecture.

## Foundational Learning

- Concept: Discrete Fourier Transform (DFT) and its linearity property
  - Why needed here: The entire FredNormer approach relies on transforming time series to frequency domain where stable components can be identified and weighted
  - Quick check question: If f(t) has Fourier transform F(ω) and g(t) has Fourier transform G(ω), what is the Fourier transform of af(t) + bg(t)?

- Concept: Coefficient of Variation (CV) as a stability metric
  - Why needed here: FredNormer uses the reciprocal of CV (mean/variance) to measure frequency stability across training samples
  - Quick check question: If a frequency component has mean amplitude 10 and standard deviation 2, what is its stability score S(k)?

- Concept: Z-score normalization and its effect on distributions
  - Why needed here: Understanding how standard normalization affects frequency components is crucial to recognizing why time-domain methods fail for non-stationary data
  - Quick check question: After z-score normalization, what happens to the mean and standard deviation of the transformed time series?

## Architecture Onboarding

- Component map: Data → DFT → Stability Measure → Weighting Layer → IDFT → Forecasting Model
- Critical path: Input data flows through DFT, stability measurement, learnable weighting, IDFT, then to the forecasting model
- Design tradeoffs:
  - Pros: Captures frequency-specific patterns, plug-and-play, minimal parameters, improves forecasting accuracy
  - Cons: Additional computational overhead from DFT operations, requires careful hyperparameter tuning for stability metric
- Failure signatures:
  - No improvement in MSE/MAE: Could indicate stability metric isn't capturing useful patterns
  - Increased training time significantly: DFT operations may be too slow for real-time applications
  - Model overfitting: Learnable linear projections may be too flexible
- First 3 experiments:
  1. Compare MSE improvement on ETTm2 dataset with and without FredNormer on PatchTST backbone
  2. Measure computational overhead by comparing training time per epoch with and without FredNormer
  3. Ablation study: Replace FredNormer's stability metric with simple low-pass filtering to validate the importance of the stability measure

## Open Questions the Paper Calls Out

### Open Question 1
How does FredNormer perform on datasets with different types of non-stationarity, such as seasonality or trend shifts? The paper mentions that FredNormer improves performance on non-stationary datasets like Traffic, but does not provide a detailed analysis of its performance across different types of non-stationarity.

### Open Question 2
Can FredNormer be extended to handle multivariate time series with complex interdependencies? The paper focuses on univariate and multivariate time series, but does not explicitly address how FredNormer handles complex interdependencies between variables in multivariate time series.

### Open Question 3
How does the choice of the frequency stability metric affect FredNormer's performance? The paper introduces a new frequency stability metric based on the coefficient of variation, but does not compare it to other potential metrics or analyze its impact on performance.

## Limitations
- No empirical verification of Lemma 1's uniform scaling phenomenon across different non-stationary datasets
- The stability metric's assumption that higher mean-to-variance ratios indicate useful frequencies may not hold for all non-stationary patterns
- Computational efficiency claims rely on specific hardware (NVIDIA RTX A6000 48GB) and may not generalize

## Confidence
- High confidence: The mathematical framework and theoretical guarantees (Lemma 1)
- Medium confidence: The experimental results showing performance improvements on benchmark datasets
- Low confidence: The generalization of the stability metric across all types of non-stationary patterns

## Next Checks
1. Verify Lemma 1 empirically by applying z-score normalization to synthetic non-stationary time series with known frequency compositions, then measuring the scaling factors of individual frequency components in the DFT output.

2. Conduct an ablation study replacing FredNormer's stability metric with alternative frequency selection methods (e.g., simple thresholding or low-pass filtering) to quantify the specific contribution of the mean/variance-based stability measure.

3. Test FredNormer's computational efficiency claims on different hardware configurations, particularly comparing GPU vs CPU implementations and measuring the overhead of DFT/IDFT operations across varying time series lengths and batch sizes.