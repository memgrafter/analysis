---
ver: rpa2
title: Derivative-Free Diffusion Manifold-Constrained Gradient for Unified XAI
arxiv_id: '2411.15265'
source_url: https://arxiv.org/abs/2411.15265
tags:
- diffusion
- counterfactual
- freemcg
- gradient
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses limitations of gradient-based XAI methods:
  requiring white-box access, vulnerability to adversarial attacks, and producing
  off-manifold attributions. The authors propose Derivative-Free Diffusion Manifold-Constrained
  Gradients (FreeMCG), which approximates gradients on the data manifold using diffusion
  models and ensemble Kalman filters without needing model weights.'
---

# Derivative-Free Diffusion Manifold-Constrained Gradient for Unified XAI

## Quick Facts
- arXiv ID: 2411.15265
- Source URL: https://arxiv.org/abs/2411.15265
- Reference count: 40
- Primary result: Derivative-free gradient approximation using diffusion models and ensemble Kalman filters for unified feature attribution and counterfactual explanation

## Executive Summary
This paper addresses fundamental limitations of gradient-based XAI methods - requiring white-box access, vulnerability to adversarial attacks, and producing off-manifold attributions. The authors propose FreeMCG, which approximates gradients on the data manifold using diffusion models and ensemble Kalman filters without needing model weights. FreeMCG serves as a unified framework for both feature attribution and counterfactual explanation. Experiments show state-of-the-art performance: for counterfactual generation on ImageNet, FreeMCG achieves 51.5% flip rate with higher human-perceived changes and better similarity; for feature attribution, ROAD evaluation shows steeper prediction decline closer to SHAP lower bound. The method produces on-manifold, interpretable explanations while preserving essential XAI properties.

## Method Summary
FreeMCG approximates manifold-constrained gradients using particles generated through diffusion sampling and ensemble Kalman filter theory. The method uses Tweedie's formula to project noisy samples onto the data manifold, then applies preconditioning with particle covariance to expand along tangent space directions while contracting normal directions. This produces gradient directions that remain on-manifold and interpretable. The same framework applies to both feature attribution (when target class equals predicted class) and counterfactual explanation (when target class differs), with different weighting schemes. Implementation involves generating particles at multiple timesteps, computing covariance-based gradient approximations, and using these for either visualization or iterative ascent.

## Key Results
- Counterfactual generation on ImageNet: 51.5% flip rate (vs 61.7% for LDCE), with higher human-perceived changes (3.74 vs 2.59) and better similarity (3.52 vs 2.87)
- Feature attribution: ROAD evaluation shows steeper prediction decline closer to SHAP lower bound
- Unified framework successfully handles both feature attribution and counterfactual explanation tasks
- Produces on-manifold attributions that align with human perception unlike traditional gradient methods

## Why This Works (Mechanism)

### Mechanism 1
FreeMCG approximates gradients on the data manifold using diffusion models and ensemble Kalman filters without needing model weights. The method uses Tweedie's formula to project noisy samples onto the data manifold, then applies ensemble Kalman filter theory to approximate the gradient direction using particle covariance. The preconditioning with covariance matrix Cxx expands along tangent space directions and contracts normal directions, keeping gradients on-manifold. Core assumption: The data manifold is locally linear near the input point, and particles generated through diffusion sampling lie on/near this manifold.

### Mechanism 2
The preconditioned gradient direction produces semantically meaningful changes while avoiding adversarial attacks. By constraining gradient computation to the tangent space of the data manifold, FreeMCG ensures that perturbations remain interpretable to humans rather than producing imperceptible adversarial changes that lie off-manifold. Core assumption: Human perceptual similarity aligns with the data manifold structure, so on-manifold changes are interpretable while off-manifold changes are not.

### Mechanism 3
FreeMCG provides a unified framework for both feature attribution and counterfactual explanation. The same derivative-free gradient approximation can be used for feature attribution (when target class equals predicted class) and counterfactual explanation (when target class differs), with different weighting schemes applied to the gradient direction. Core assumption: Both tasks fundamentally require understanding how model outputs change with input perturbations, just with different objectives for the perturbation direction.

## Foundational Learning

- **Concept**: Ensemble Kalman Filters (EnKF)
  - Why needed here: EnKF provides the theoretical foundation for using particle covariance as a proxy for gradient information in derivative-free optimization.
  - Quick check question: How does EnKF use ensemble statistics to approximate gradients without requiring derivative computation?

- **Concept**: Diffusion Models and Tweedie's Formula
  - Why needed here: Diffusion models provide the mechanism for generating on-manifold samples, and Tweedie's formula projects noisy samples back onto the manifold.
  - Quick check question: What mathematical relationship does Tweedie's formula establish between noisy observations and their posterior mean on the manifold?

- **Concept**: Manifold Geometry and Tangent Spaces
  - Why needed here: Understanding manifold geometry is crucial for why preconditioning with covariance keeps gradients on-manifold and interpretable.
  - Quick check question: How does a locally linear manifold approximation enable the covariance matrix to act as a projection onto the tangent space?

## Architecture Onboarding

- **Component map**: Diffusion Model -> Ensemble Kalman Filter Module -> Gradient Approximation Engine -> Task-Specific Wrapper -> Classifier Interface
- **Critical path**: 1) Generate K particles through diffusion sampling from input point, 2) Compute particle statistics (mean, covariance), 3) Apply FreeMCG formula to approximate gradient direction, 4) Use gradient for either feature attribution (visualization) or counterfactual generation (iterative ascent)
- **Design tradeoffs**: Particle count (K) - higher K gives better gradient approximation but increases computation time; Diffusion timesteps - multiple timesteps capture multi-scale features but add complexity; Task adaptation - direct gradient ascent vs. reverse diffusion for counterfactuals involves quality vs. speed tradeoff
- **Failure signatures**: Off-manifold results - indicates manifold linearity assumption violated or particle generation failed; Noisy attributions - suggests insufficient particles or poor timestep selection; Slow counterfactual generation - may indicate need for reverse diffusion vs. direct ascent
- **First 3 experiments**: 1) Verify gradient approximation quality on a simple 2D function where ground truth gradients are known, 2) Test feature attribution on a small image classification model with known interpretable features, 3) Generate counterfactuals on a simple dataset (e.g., MNIST) to validate semantic changes

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in a dedicated section. However, based on the limitations discussed and the novelty of the approach, several implicit open questions emerge regarding scalability to higher resolutions, theoretical error bounds, sensitivity to diffusion model architecture, extension to non-image modalities, and computational complexity for real-time deployment.

## Limitations
- Manifold linearity assumption may break down for complex real-world data distributions
- Performance depends heavily on quality of diffusion model sampling and manifold approximation
- Human perception experiments rely on subjective scoring that may not generalize across user populations
- Limited ablation studies on hyperparameters like particle count and timestep selection

## Confidence
- High confidence in the mathematical formulation and theoretical foundation of FreeMCG
- Medium confidence in the experimental results due to limited ablation studies on hyperparameters
- Medium confidence in the unified framework claim, as task-specific adaptations are minimal
- Low confidence in the scalability to larger, more complex models beyond ResNet-50

## Next Checks
1. Conduct systematic ablation studies on particle count (K) and timestep selection to quantify their impact on gradient approximation quality and computational efficiency.
2. Test FreeMCG on additional model architectures (Vision Transformers, language models) and datasets to evaluate generalizability beyond ImageNet and CXR.
3. Perform robustness analysis by introducing noise and adversarial perturbations to validate the method's resistance to off-manifold artifacts and model sensitivity.