---
ver: rpa2
title: Self-training Large Language Models through Knowledge Detection
arxiv_id: '2406.11275'
source_url: https://arxiv.org/abs/2406.11275
tags:
- arxiv
- knowledge
- dataset
- training
- filtering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a self-training framework for improving truthfulness
  in large language models by having them autonomously curate training data and perform
  selective training on samples identified as unknown. The method uses a reference-free
  consistency approach to detect knowledge gaps, filters responses based on contradiction
  scores, and applies direct preference optimization only on filtered samples.
---

# Self-training Large Language Models through Knowledge Detection

## Quick Facts
- **arXiv ID**: 2406.11275
- **Source URL**: https://arxiv.org/abs/2406.11275
- **Reference count**: 17
- **Primary result**: Selective self-training improves factual accuracy while preserving knowledge retention across multiple model sizes

## Executive Summary
This paper introduces a self-training framework that enables large language models to autonomously curate their training data by detecting knowledge gaps and performing selective training only on uncertain samples. The approach uses a reference-free consistency method to identify responses that contradict known knowledge, then applies direct preference optimization on filtered data. Experiments across 1.1B, 7B, and 13B parameter models demonstrate improved factual accuracy on held-out questions while mitigating catastrophic forgetting on out-of-distribution benchmarks.

## Method Summary
The framework implements autonomous knowledge curation where models detect their own knowledge gaps through a reference-free consistency approach that identifies contradictions in responses. When a response receives a high contradiction score, the system flags it as representing unknown knowledge. Direct preference optimization is then applied selectively only to these filtered samples rather than all training data. This selective training approach reduces reliance on labeled datasets while maintaining knowledge retention through careful filtering of uncertain responses.

## Key Results
- Win rates against baseline SFT models range from 35-55% depending on model size
- Performance improvements observed across 1.1B, 7B, and 13B parameter models
- Out-of-distribution benchmark performance preserved or improved compared to full SFT training
- Reduced reliance on labeled datasets while maintaining factual accuracy gains

## Why This Works (Mechanism)
The method works by leveraging the model's own uncertainty detection to create a curriculum of challenging samples. By identifying contradictions through reference-free consistency, the system focuses training on knowledge gaps rather than reinforcing already-known information. This selective approach prevents overfitting to confident responses while addressing weaknesses, creating a more efficient learning signal than traditional full-data training methods.

## Foundational Learning
- **Knowledge gap detection**: Understanding how models identify their own uncertainty is crucial for self-training. Quick check: Verify contradiction scores correlate with actual knowledge gaps through human evaluation.
- **Direct preference optimization**: DPO provides the mechanism for fine-tuning on curated data. Quick check: Compare performance with standard supervised fine-tuning to isolate DPO benefits.
- **Selective training**: The concept of training only on uncertain samples rather than all data. Quick check: Measure performance degradation when applying full SFT vs. selective training.
- **Catastrophic forgetting mitigation**: Understanding how selective training preserves knowledge. Quick check: Track performance on out-of-distribution benchmarks across training epochs.
- **Reference-free consistency**: Method for detecting contradictions without external references. Quick check: Evaluate false positive/negative rates in contradiction detection.

## Architecture Onboarding
**Component Map**: Input questions -> Consistency scoring -> Contradiction filtering -> DPO training -> Output model
**Critical Path**: The consistency scoring and contradiction filtering stages are critical, as errors here directly impact which samples receive training and thus the final model quality.
**Design Tradeoffs**: The system trades computational efficiency (training on fewer samples) for potential information loss (missing valuable data through over-filtering). The contradiction threshold represents a key hyperparameter balancing precision and recall.
**Failure Signatures**: High false positive rates in contradiction detection lead to missing valuable training data; high false negatives result in reinforcing incorrect knowledge. Poor threshold selection can cause either catastrophic forgetting or negligible improvement.
**First Experiments**:
1. Vary contradiction score thresholds to identify optimal filtering parameters across different knowledge domains
2. Compare performance on multi-turn dialogue tasks to evaluate generalization beyond single-turn scenarios
3. Test on knowledge-intensive benchmarks with known ground truth to independently verify factual accuracy improvements

## Open Questions the Paper Calls Out
None

## Limitations
- Limited analysis of false positive/negative rates for contradiction detection
- Relative win rate improvements (35-55%) may not translate to same magnitude in real-world applications
- No quantification of actual reduction in human annotation effort required

## Confidence
- **High confidence**: Claims about selective training preserving knowledge retention compared to full SFT training
- **Medium confidence**: Claims about factual accuracy improvements on held-out questions
- **Medium confidence**: Claims about reducing labeled data requirements

## Next Checks
1. Conduct ablation studies varying the contradiction score threshold to determine sensitivity of performance improvements to filtering parameters
2. Test the approach on multi-turn dialogue tasks to evaluate generalization beyond single-turn question-answering
3. Evaluate performance on knowledge-intensive benchmarks with known ground truth (e.g., MMLU, Natural Questions) to independently verify factual accuracy improvements