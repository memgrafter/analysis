---
ver: rpa2
title: 'SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast
  Asian Languages'
arxiv_id: '2406.10118'
source_url: https://arxiv.org/abs/2406.10118
tags:
- indonesia
- seacrowd
- philippines
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the severe underrepresentation of Southeast
  Asian (SEA) languages in AI datasets and models, which limits the performance and
  cultural relevance of AI technologies for the 671 million people in the region.
  The authors introduce SEACrowd, a comprehensive, collaborative initiative that consolidates
  and standardizes nearly 1,000 SEA language datasets across text, image, and audio
  modalities, covering 36 indigenous languages in 13 tasks.
---

# SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages

## Quick Facts
- arXiv ID: 2406.10118
- Source URL: https://arxiv.org/abs/2406.10118
- Reference count: 40
- Primary result: Consolidated nearly 1,000 SEA language datasets into standardized hub, evaluated AI models on 36 indigenous languages across 13 tasks, revealing severe naturalness and cultural relevance gaps

## Executive Summary
This paper addresses the critical underrepresentation of Southeast Asian languages in AI datasets and models, which limits AI technology performance for the region's 671 million people. The authors introduce SEACrowd, a comprehensive initiative that consolidates and standardizes nearly 1,000 SEA language datasets across text, image, and audio modalities, covering 36 indigenous languages in 13 tasks. Through SEACrowd Benchmarks, they evaluate AI model quality, finding that while SEA-specific models perform best on natural language understanding, most struggle with producing natural outputs, with only 57.71% of sentences from the best model being natural in SEA languages. The work highlights the need for focused investment in R&D, data collection, and open collaboration to address resource equity and improve AI development for SEA languages.

## Method Summary
The study introduces SEACrowd as a two-platform system: SEACrowd Catalogue (housing datasheets and metadata) and SEACrowd Data Hub (storing standardized dataloaders and schemas). The methodology involves consolidating nearly 1,000 SEA language datasets, creating standardized evaluation benchmarks across 13 tasks for 36 indigenous languages, and conducting zero-shot evaluations of various AI models including GPT-4, Whisper v3, and SEA-LION. A translationese classifier was developed using mDeBERTa-v3 to assess generation quality, and language prioritization was based on potential demand and utility metrics. The evaluation used weighted F1-score for NLP tasks, chrF++/ROUGE-L for NLG, CIDEr for image captioning, and WER/CER for ASR.

## Key Results
- SEACrowd consolidates nearly 1,000 SEA language datasets across three modalities covering 36 indigenous languages
- Current LLMs produce "translationese" rather than natural language outputs in SEA languages, with only 57.71% naturalness rate
- SEA-specific models like AYA-101 and mT0 perform best on natural language understanding tasks, but most models struggle with cultural relevance
- National languages with significant naturalness gaps and under-resourced local dialects identified as priority areas for development

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SEACrowd's consolidation of nearly 1,000 SEA language datasets into a single standardized hub directly addresses resource scarcity and fragmentation that limits AI model development for SEA languages.
- Mechanism: By creating a comprehensive catalogue with datasheets and standardized dataloaders, SEACrowd makes it easy for researchers to discover, access, and use SEA language datasets across text, image, and audio modalities, thereby lowering the barrier to entry for developing SEA language AI models.
- Core assumption: The primary bottleneck to SEA language AI development is lack of accessible, high-quality datasets rather than technical capability or interest among researchers.
- Evidence anchors:
  - [abstract] "To address these challenges, we introduce SEACrowd, a collaborative initiative that consolidates a comprehensive resource hub that fills the resource gap by providing standardized corpora in nearly 1,000 SEA languages across three modalities."
  - [section] "SEACrowd addresses these issues through two primary contributions: 1) consolidating datasheets to enhance data discoverability; and 2) standardizing dataloaders for easier use, especially in multiple dataset loading."

### Mechanism 2
- Claim: SEACrowd Benchmarks provide the first comprehensive evaluation of AI models on 36 SEA indigenous languages across 13 tasks, revealing critical gaps in model performance and naturalness.
- Mechanism: By creating a standardized benchmark suite with manually annotated and validated datasets, SEACrowd allows for direct comparison of model performance across a wide range of SEA languages and tasks, highlighting areas where current models fail to produce natural, culturally relevant outputs.
- Core assumption: Evaluating model performance on diverse SEA languages and tasks is necessary to identify specific weaknesses and prioritize future development efforts.
- Evidence anchors:
  - [abstract] "Through our SEACrowd benchmarks, we assess the quality of AI models on 36 indigenous languages across 13 tasks, offering valuable insights into the current AI landscape in SEA."
  - [section] "We assess the performance of AI models on 36 indigenous languages across 13 tasks included in SEACrowd, offering valuable insights into the current AI landscape in SEA."

### Mechanism 3
- Claim: SEACrowd's analysis of generation quality reveals that current LLMs produce "translationese" rather than natural language outputs in SEA languages, highlighting the need for models specifically trained on culturally relevant data.
- Mechanism: By developing a translationese classifier and evaluating the generation quality of various LLMs on SEA languages, SEACrowd demonstrates that models with extensive language coverage but less focus on SEA languages tend to produce less natural sentences than models with greater focus on SEA languages.
- Core assumption: Quality of LLM outputs in SEA languages is critical for utility and acceptability to SEA language speakers, and models trained on culturally relevant data will produce more natural outputs.
- Evidence anchors:
  - [abstract] "Our study reveals that the generative outputs of existing LLMs exhibit a closer resemblance to 'translationese' rather than natural data in nine SEA languages."
  - [section] "We evaluate the generation quality of LLMs in 9 SEA languages by generating answers to natural, general, and safety questions from Sea-Bench (Nguyen et al., 2023). As shown in Table 2a, LLMs with extensive language coverage but less focus on SEA languages, e.g., AYA-101 (Üstün et al., 2024), GPT-4 (OpenAI et al., 2024), mT0 (Muennighoff et al., 2023; Xue et al., 2021), and Llama3 (AI@Meta, 2024), tend to produce natural sentences less than 20% of the time."

## Foundational Learning

- Concept: Data standardization and schema design
  - Why needed here: SEACrowd involves standardizing diverse datasets from various sources into common format for easy access and use. Understanding data standardization principles and schema design is crucial for contributing to and extending SEACrowd.
  - Quick check question: What are the key considerations when designing a data schema for a multilingual dataset that includes text, image, and audio modalities?

- Concept: Machine learning model evaluation and benchmarking
  - Why needed here: SEACrowd Benchmarks involve evaluating performance of various AI models on SEA languages across different tasks. Understanding evaluation metrics, benchmarking methodologies, and statistical analysis is essential for interpreting results and drawing meaningful conclusions.
  - Quick check question: What are the advantages and disadvantages of using weighted F1-score versus accuracy for evaluating model performance on imbalanced datasets with multiple languages?

- Concept: Natural language generation quality assessment
  - Why needed here: SEACrowd's analysis of generation quality involves developing a translationese classifier and evaluating the naturalness of LLM outputs in SEA languages. Understanding techniques for assessing generation quality, such as human evaluation and automated metrics, is crucial for this analysis.
  - Quick check question: What are the key challenges in developing an automated classifier to distinguish between translationese and natural language outputs, and how can these challenges be addressed?

## Architecture Onboarding

- Component map: SEACrowd Catalogue (datasheets/metadata) -> SEACrowd Data Hub (standardized dataloaders/seacrowd library) -> Evaluation (benchmark experiments)
- Critical path: Submit datasheet for existing dataset -> Build standardized dataloader -> Review and approval -> Select datasets for evaluation -> Run experiments on chosen models -> Analyze results
- Design tradeoffs: Prioritizes data discoverability and ease of use over strict adherence to original dataset formats, allowing easier integration of diverse datasets but potentially introducing some loss of information or biases
- Failure signatures: Incomplete or inaccurate datasheets, buggy dataloader implementations, non-reproducible evaluation results
- First 3 experiments:
  1. Load a sample dataset from SEACrowd using the standardized dataloader and verify that data is correctly formatted and accessible
  2. Run a simple experiment using a pre-trained LLM on a SEA language task from SEACrowd Benchmarks and compare results to published baseline
  3. Use the translationese classifier to evaluate the naturalness of LLM outputs on a SEA language and analyze results in context of generation quality analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AI models be developed to produce more natural and culturally relevant outputs in SEA languages, reducing translationese issues?
- Basis in paper: [explicit] The paper highlights that existing LLMs generate natural SEA language sentences only 57.71% of the time, with languages like Tagalog, Burmese, and Malay experiencing severe translationese problems. The authors emphasize the need for further improvements in LLMs to address linguistic diversity and complexity.
- Why unresolved: While the paper identifies the problem and suggests prioritizing languages with naturalness gaps, it does not provide concrete methodologies or technical solutions for reducing translationese in AI-generated outputs.
- What evidence would resolve it: Empirical studies demonstrating specific techniques (e.g., fine-tuning strategies, data augmentation, or culturally-informed prompt engineering) that significantly improve the naturalness of AI-generated text in SEA languages.

### Open Question 2
- Question: What are the most effective strategies for prioritizing SEA language development to maximize potential utility and resource equity?
- Basis in paper: [explicit] The authors propose prioritizing SEA language development based on two metrics: potential utility (gap between current and ideal utility) and resource equity (gap between existing and ideal resource availability). They suggest focusing on national languages with naturalness gaps and under-resourced local dialects.
- Why unresolved: The paper provides a framework for prioritization but lacks detailed implementation strategies or empirical validation of how different prioritization approaches impact AI development outcomes in SEA.
- What evidence would resolve it: Comparative studies evaluating the impact of different prioritization strategies on AI model performance, resource allocation, and language preservation across SEA languages.

### Open Question 3
- Question: How can collaborative efforts between governments, industry, and local communities be structured to effectively address the resource gap in SEA languages?
- Basis in paper: [explicit] The authors emphasize the need for concentrated efforts by stakeholders, including investment in R&D, data collection, and open collaboration. They suggest strategies like funding open data collection, promoting fair compensation for data workers, and orchestrating collaborations between data owners, AI developers, and application developers.
- Why unresolved: While the paper outlines general strategies, it does not provide specific models or case studies of successful collaborative efforts that have effectively addressed resource gaps in SEA languages.
- What evidence would resolve it: Case studies or pilot programs demonstrating successful collaborative models for SEA language resource development, including metrics on resource creation, community engagement, and AI model performance improvements.

## Limitations

- The study relies heavily on zero-shot evaluations, which may underestimate the true potential of fine-tuned models for SEA languages
- The translationese classifier, while achieving 87% accuracy, may not fully capture nuanced differences between natural and translationese outputs across culturally diverse SEA languages
- The 36 indigenous languages covered represent only a subset of the region's linguistic diversity, potentially overlooking critical resource gaps in less represented languages

## Confidence

- **High Confidence**: The consolidation of SEA language datasets into SEACrowd and identification of resource scarcity issues are well-supported by comprehensive dataset catalog and clear documentation of existing gaps
- **Medium Confidence**: The benchmark evaluation results are reliable for indicating current model performance trends, but zero-shot evaluation methodology may not fully represent optimal performance scenarios
- **Medium Confidence**: The translationese analysis provides valuable insights into generation quality issues, though classifier's limitations and complexity of naturalness assessment in multilingual contexts introduce some uncertainty

## Next Checks

1. **Dataset Quality Validation**: Conduct systematic review of randomly sampled datasets in SEACrowd to verify data quality, representativeness, and absence of biases across the 36 covered languages
2. **Fine-tuning Experiment**: Replicate key benchmark results using fine-tuned models rather than zero-shot evaluation to establish whether identified performance gaps persist with domain adaptation
3. **Translationese Classifier Robustness**: Test the translationese classifier across additional SEA language pairs and cultural contexts to assess its generalizability and identify potential blind spots in naturalness detection