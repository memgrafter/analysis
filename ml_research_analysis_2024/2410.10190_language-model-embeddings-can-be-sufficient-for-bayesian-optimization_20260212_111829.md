---
ver: rpa2
title: Language Model Embeddings Can Be Sufficient for Bayesian Optimization
arxiv_id: '2410.10190'
source_url: https://arxiv.org/abs/2410.10190
tags:
- optimization
- bayesian
- language
- regression
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using language model embeddings for regression
  in Bayesian optimization. The authors address the limitation of traditional regression
  methods that require structured, tabular input features and are restricted to fixed
  search spaces.
---

# Language Model Embeddings Can Be Sufficient for Bayesian Optimization

## Quick Facts
- **arXiv ID:** 2410.10190
- **Source URL:** https://arxiv.org/abs/2410.10190
- **Reference count:** 30
- **Primary result:** Language model embeddings achieve optimization performance comparable to state-of-the-art Gaussian Process methods like Google Vizier across synthetic, combinatorial, and hyperparameter optimization tasks.

## Executive Summary
This paper proposes using language model embeddings for regression in Bayesian optimization, addressing the limitation of traditional methods that require structured tabular features and fixed search spaces. The approach converts string representations of inputs into language model embeddings, which are then used as features for a Transformer Neural Process regressor pretrained on large offline evaluation datasets. The method achieves optimization performance comparable to Google Vizier across various domains including synthetic functions, combinatorial optimization, and hyperparameter tuning.

## Method Summary
The method involves converting string representations of optimization candidates into fixed-length language model embeddings using a frozen T5-XL encoder. These embeddings serve as input features for a Transformer Neural Process regressor that was pretrained on diverse offline evaluation data. During optimization, the regressor predicts objective values and uncertainties for candidate inputs, which are then used with UCB acquisition to propose new trials. The approach handles various optimization domains by designing appropriate string representations for each task type.

## Key Results
- Embed-then-Regress matches or exceeds GP-Bandit performance on BBOB, combinatorial, and hyperparameter optimization tasks
- Language model embeddings generalize across diverse optimization domains without task-specific feature engineering
- Pretraining on large offline datasets enables effective transfer to unseen objective functions
- The method achieves competitive results while offering flexibility for broader applications

## Why This Works (Mechanism)

### Mechanism 1
Language model embeddings can generalize across diverse optimization domains without task-specific feature engineering. By representing any input as a string and using a language model to produce fixed-length embeddings, the method sidesteps the need for structured tabular features or domain-specific kernels. This string-to-embedding pipeline feeds into a neural process regressor that learns to predict objective values from the embeddings across many offline tasks.

### Mechanism 2
Pretraining on large offline evaluation datasets allows the regressor to learn effective priors for unseen objective functions. The Transformer Neural Process is trained on trajectories from many different tasks, learning to condition predictions on observed evaluations and output calibrated uncertainties. This meta-learning step enables good performance on new tasks without additional fine-tuning.

### Mechanism 3
The embed-then-regress approach achieves optimization performance comparable to specialized Gaussian Process methods like Google Vizier. The language model embeddings provide a flexible input representation, the neural process regressor learns to predict with uncertainty, and the UCB acquisition balances exploration and exploitation. Together, this pipeline matches or exceeds GP-Bandit's optimization efficiency on multiple benchmark suites.

## Foundational Learning

- **Concept:** Bayesian Optimization (BO)
  - Why needed here: BO is the overarching framework; the paper's contribution is a new regression method within BO.
  - Quick check question: What are the two key components of BO that this paper aims to improve?

- **Concept:** Transformer Neural Process
  - Why needed here: This is the specific in-context learning model used to perform regression over string embeddings.
  - Quick check question: How does a Neural Process differ from a standard Transformer in terms of handling sequential context?

- **Concept:** Language Model Embeddings
  - Why needed here: The embeddings are the bridge between arbitrary string inputs and fixed-dimensional numeric features for regression.
  - Quick check question: What property of language model embeddings makes them suitable for representing diverse optimization inputs?

## Architecture Onboarding

- **Component map:** Input string → T5-XL embedding → Transformer Neural Process → UCB acquisition → Firefly/RE optimizer → Next trial

- **Critical path:**
  1. Input → string representation → T5 embedding
  2. Embed → append to history → Transformer Neural Process forward pass
  3. Predict mean and std → form UCB acquisition
  4. Acquisition optimizer samples and ranks → propose next trial

- **Design tradeoffs:**
  - Larger embedder improves predictions but increases inference cost and memory
  - Parallel predictions speed up training but require custom attention patterns
  - Freezing embedder weights simplifies deployment but may miss task-specific fine-tuning
  - Using a zeroth-order optimizer avoids gradient-based embedding optimization but may need many samples

- **Failure signatures:**
  - Poor predictions → likely embedder size too small or training data insufficient
  - Calibration errors → neural process not learning uncertainty well
  - Slow optimization → acquisition optimizer not suited to embedding space or UCB coefficient wrong
  - Memory blow-up → context window too large or batch size too big

- **First 3 experiments:**
  1. Verify embedder produces fixed-length vectors for varied string inputs (unit test)
  2. Train neural process on synthetic BBOB trajectories, evaluate NLL/MAE on held-out functions
  3. Run embed-then-regress vs GP-Bandit on a small subset of BBOB functions, compare best-so-far curves

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of string representation affect the performance of LLM embeddings for regression tasks?
- **Basis in paper:** The paper discusses various string representations for different optimization tasks, such as JSON for traditional optimization and index-based strings for combinatorial problems.
- **Why unresolved:** The paper does not provide a detailed analysis of how different string representations impact the effectiveness of LLM embeddings in capturing the necessary features for regression.
- **What evidence would resolve it:** A systematic comparison of regression performance using different string representations across various optimization tasks, highlighting which formats yield the best results and why.

### Open Question 2
- **Question:** Can the embed-then-regress approach be effectively scaled to larger and more complex language models for improved performance?
- **Basis in paper:** The paper uses relatively smaller language model sizes (e.g., T5-XL) and suggests that larger models could potentially offer better features even for numeric data formats.
- **Why unresolved:** The paper intentionally uses smaller models to demonstrate the approach on low compute budgets, leaving the impact of scaling to larger models unexplored.
- **What evidence would resolve it:** Experiments comparing the performance of the embed-then-regress method using progressively larger language models, assessing improvements in regression accuracy and optimization efficiency.

### Open Question 3
- **Question:** What are the limitations of using LLM embeddings for regression in highly dynamic or stateful environments, such as those encountered in LLM reasoning tasks?
- **Basis in paper:** The paper mentions the potential for applying the method to LLM reasoning as reward models to assist tree search-based approaches for stateful environments.
- **Why unresolved:** The paper does not explore the application of LLM embeddings in dynamic or stateful settings, focusing instead on stateless optimization problems.
- **What evidence would resolve it:** Empirical studies demonstrating the effectiveness of LLM embeddings in regression tasks within dynamic environments, such as those involving sequential decision-making or adaptive processes.

## Limitations
- The core assumption that language model embeddings capture meaningful structure for numeric regression tasks remains empirically supported but theoretically unproven
- The pretraining dataset composition and its relationship to target tasks is underspecified, raising questions about transfer reliability
- The string representation scheme for different optimization domains is described but not rigorously validated for preserving input semantics

## Confidence
- **High confidence:** The method achieves comparable optimization performance to GP-Bandit on the tested benchmark suites (BBOB, combinatorial, hyperparameter optimization)
- **Medium confidence:** Language model embeddings generalize across diverse optimization domains without task-specific feature engineering
- **Medium confidence:** Pretraining on large offline evaluation datasets enables effective transfer to unseen objective functions

## Next Checks
1. **Ablation study on embedding quality:** Measure regression performance when using different embedding sizes (from 128 to 4096 dimensions) and different language models (T5-small through T5-XXL) on held-out BBOB functions to quantify the relationship between embedding capacity and optimization performance.

2. **Pretraining data sensitivity analysis:** Systematically vary the diversity and size of the pretraining dataset (e.g., using only synthetic functions vs. including hyperparameter optimization trajectories) and measure transfer performance on each optimization domain to identify which pretraining data types are most valuable for different target tasks.

3. **String representation validation:** For each optimization domain, create controlled experiments where semantically equivalent inputs are represented with different string formats and measure the impact on embedding similarity and downstream regression accuracy to verify that the string encoding preserves relevant input structure.