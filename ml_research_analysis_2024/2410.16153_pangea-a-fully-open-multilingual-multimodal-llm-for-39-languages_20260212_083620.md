---
ver: rpa2
title: 'Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages'
arxiv_id: '2410.16153'
source_url: https://arxiv.org/abs/2410.16153
tags:
- multilingual
- languages
- image
- multimodal
- pangea
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Pangea, a multilingual multimodal large language
  model (MLLM) trained on PangeaIns, a 6 million sample dataset spanning 39 typologically
  diverse languages. The dataset combines high-quality English instructions, machine-translated
  instructions, and culturally relevant multimodal tasks to ensure comprehensive cross-cultural
  coverage.
---

# Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages

## Quick Facts
- arXiv ID: 2410.16153
- Source URL: https://arxiv.org/abs/2410.16153
- Authors: Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, Graham Neubig
- Reference count: 40
- Primary result: Pangea outperforms existing open-source MLLMs with 0.4% gains on English tasks and 10.9% gains on multilingual tasks

## Executive Summary
This paper introduces Pangea, a multilingual multimodal large language model (MLLM) trained on PangeaIns, a 6 million sample dataset spanning 39 typologically diverse languages. The dataset combines high-quality English instructions, machine-translated instructions, and culturally relevant multimodal tasks to ensure comprehensive cross-cultural coverage. To evaluate model performance, the authors develop PangeaBench, a holistic evaluation suite comprising 14 datasets across 47 languages, covering tasks such as open-domain multimodal chat, image captioning, cultural understanding, multimodal reasoning, and text-only tasks. Results demonstrate that Pangea significantly outperforms existing open-source MLLMs in both English and multilingual settings. The authors fully open-source Pangea, PangeaIns, PangeaBench, and code to advance inclusive and robust multilingual MLLMs.

## Method Summary
Pangea uses a two-stage training approach: first pretraining a vision-language connector on LLaVA LCS-558K for 1 epoch, then finetuning the full model on PangeaIns (6M multilingual multimodal samples) for 1 epoch. The training uses 8 H100 GPUs with batch size 128 for pretraining and batch size 512 for finetuning, with learning rate 2e-5 and cosine decay. The PangeaIns dataset is created by combining high-quality English instructions, machine-translated instructions using Gemini 1.5 Pro, and culturally diverse multimodal tasks filtered using LLM scoring for cultural relevance.

## Key Results
- Pangea achieves 0.4% average performance gain on English tasks compared to existing open-source MLLMs
- Pangea achieves 10.9% average performance gain on multilingual tasks across 47 languages
- PangeaBench evaluation suite demonstrates comprehensive coverage across 14 datasets and 47 languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pangea outperforms existing open-source models in multilingual settings due to its balanced 40% English / 60% multilingual training data ratio.
- Mechanism: English data enables cross-lingual transfer while sufficient multilingual data maintains cultural and linguistic diversity.
- Core assumption: Multilingual performance benefits from cross-lingual transfer without English data dominance.
- Evidence anchors:
  - [abstract]: "Pangea significantly outperforms existing open-source MLLMs in both English and multilingual settings, with gains of 0.4% on English tasks and 10.9% on multilingual tasks on average."
  - [section 5]: "Our analysis in Figure 7 revealed the relationship between training sample proportion and downstream performance... for low-resource languages, even a small increase in proportion yielded disproportionately large performance gains."
- Break condition: If English proportion exceeds 40%, multilingual performance drops sharply (Figure 6).

### Mechanism 2
- Claim: Pangea's multicultural understanding stems from curated culturally diverse images and LLM-generated instructions.
- Mechanism: Filtering LAION-Multi for cultural specificity and generating detailed instructions enhances cultural nuance recognition.
- Core assumption: Cultural specificity in training data translates to improved cultural understanding in model outputs.
- Evidence anchors:
  - [abstract]: "PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage."
  - [section 2.2]: "To ensure that our dataset captures a wide array of cultural contexts, we began by sampling 10 million images from the LAION-Multi dataset... We employed the Llama-3.1-8B-Instruct model to evaluate... cultural relevance of the accompanying text descriptions."
- Break condition: If cultural filtering is relaxed, model performance on cultural understanding tasks declines.

### Mechanism 3
- Claim: PangeaBench's fine-grained evaluation provides more accurate assessment than coarse LLM-as-Judge methods.
- Mechanism: Human-crafted evaluation criteria and score rubrics enable precise scoring of multimodal chat responses.
- Core assumption: Fine-grained rubrics better capture nuanced model performance than coarse criteria.
- Evidence anchors:
  - [abstract]: "xChat, a human-crafted benchmark designed to evaluate open-ended, information-seeking multimodal conversations. xChat employs a fine-grained evaluation pipeline where human annotators annotate both reference answers and scoring rubrics for each query."
  - [section 3.1]: "Previous works suggest that employing such coarse-grained evaluation criteria may lead to automatic evaluation results that diverge from how humans would evaluate them."
- Break condition: If evaluation criteria become too generic, assessment accuracy degrades.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding how English data improves performance on low-resource languages
  - Quick check question: Why does including English data improve multilingual performance rather than harm it?

- Concept: Cultural grounding in multimodal models
  - Why needed here: Recognizing how cultural context in training data affects model outputs
  - Quick check question: How does filtering for cultural specificity differ from general multilingual data collection?

- Concept: Fine-grained evaluation methodology
  - Why needed here: Understanding why detailed rubrics produce more accurate assessments than coarse criteria
  - Quick check question: What makes evaluation "fine-grained" versus "coarse-grained" in the context of multimodal chat?

## Architecture Onboarding

- Component map: Qwen2-7B-Instruct backbone -> CLIP vision encoder -> LLaVA-Next architecture -> PangeaIns dataset (6M samples) -> PangeaBench evaluation (14 datasets, 47 languages)

- Critical path:
  1. Pretrain vision-language connector on LLaVA LCS-558K
  2. Finetune full model on PangeaIns (40% English, 60% multilingual)
  3. Evaluate using PangeaBench with fine-grained criteria

- Design tradeoffs:
  - Machine translation vs human annotation: speed and scalability vs quality
  - English data proportion: cross-lingual transfer vs cultural dilution
  - Evaluation granularity: assessment accuracy vs implementation complexity

- Failure signatures:
  - Poor multilingual OCR performance indicates insufficient language-specific training
  - Low cultural understanding scores suggest inadequate cultural filtering
  - Inconsistent evaluation results point to rubric issues

- First 3 experiments:
  1. Vary English data proportion (0%, 20%, 40%, 60%, 100%) and measure cross-lingual performance
  2. Compare machine-translated vs human-translated instruction quality on downstream tasks
  3. Test evaluation consistency by having multiple annotators score same responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of English to multilingual data for maximizing both English and multilingual performance in multilingual MLLMs?
- Basis in paper: [explicit] The paper explicitly explores this question through an ablation study, varying the percentage of English data while keeping the total number of training samples fixed at 500K.
- Why unresolved: The study shows that performance peaks at 38.7% English data (with 40% English in the dataset), but this is based on a specific sample size and dataset. The optimal ratio might vary depending on the total dataset size, language distribution, and task types.
- What evidence would resolve it: Further ablation studies with different total dataset sizes and language distributions, along with task-specific performance analysis, would help determine the optimal English-to-multilingual ratio.

### Open Question 2
- Question: How does the proportion of training samples in a specific language affect downstream performance for low-resource languages?
- Basis in paper: [explicit] The paper analyzes the relationship between training sample proportion and downstream performance, noting that for low-resource languages, even a small increase in proportion yields disproportionately large performance gains.
- Why unresolved: While the paper observes this trend, it doesn't provide a detailed analysis of the underlying mechanisms or a quantitative model to predict performance improvements for different low-resource languages.
- What evidence would resolve it: A more comprehensive study involving a wider range of low-resource languages, with controlled variations in training sample proportions, would help establish a predictive model for performance improvements.

### Open Question 3
- Question: What are the key factors limiting multilingual OCR performance for non-Latin scripts, and how can they be addressed?
- Basis in paper: [explicit] The paper mentions that OCR accuracy for non-Latin scripts (e.g., Chinese, Japanese, and Korean) remains lower than for Latin-based languages, despite efforts to improve multilingual OCR.
- Why unresolved: The paper doesn't delve into the specific technical challenges or propose concrete solutions for improving OCR performance in non-Latin scripts.
- What evidence would resolve it: A detailed analysis of the OCR model's performance on different script types, identifying specific error patterns and challenges, along with experiments testing various architectural or training data modifications, would help address this question.

## Limitations

- The paper relies on LLM-generated instructions and scoring for dataset creation without validating whether these judgments align with human cultural understanding
- The evaluation relies heavily on automated metrics without extensive human evaluation of cross-cultural understanding
- Claims about fine-grained evaluation superiority lack direct comparison studies against traditional LLM-as-Judge approaches

## Confidence

*High Confidence:* The architectural approach (LLaVA-Next with Qwen2-7B backbone) and training methodology are well-specified and follow established practices. The performance improvements over baseline models are clearly demonstrated through quantitative metrics.

*Medium Confidence:* The claims about cultural understanding improvements are moderately supported but limited by the lack of human evaluation of cultural competency. While the methodology for creating culturally diverse training data is described, the actual impact on cultural understanding remains somewhat theoretical.

*Low Confidence:* The claims about PangeaBench providing more accurate assessment than existing evaluation methods lack direct comparison studies. The paper asserts that fine-grained rubrics improve evaluation accuracy but doesn't empirically demonstrate this superiority over coarse-grained approaches.

## Next Checks

1. Conduct human evaluation study comparing responses from Pangea and baseline models on cultural understanding tasks, using native speakers from multiple target language communities to assess whether the model demonstrates genuine cross-cultural competence beyond surface-level pattern matching.

2. Perform ablation study varying the proportion of machine-translated versus human-translated instructions (not just English vs multilingual proportions) to quantify the quality tradeoff and determine if the speed advantage of machine translation justifies any performance degradation.

3. Compare PangeaBench evaluation results against traditional LLM-as-Judge approaches using the same datasets to empirically validate whether the fine-grained rubric methodology produces meaningfully different and more accurate assessments of model performance.