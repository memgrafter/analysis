---
ver: rpa2
title: 'API Pack: A Massive Multi-Programming Language Dataset for API Call Generation'
arxiv_id: '2402.09615'
source_url: https://arxiv.org/abs/2402.09615
tags:
- instruction
- shot
- data
- language
- call
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces API Pack, a multi-programming language dataset
  with over one million instruction-API call pairs for training large language models
  to generate API calls. The dataset spans 10 programming languages and includes real-world
  API examples from four major API hubs.
---

# API Pack: A Massive Multi-Programming Language Dataset for API Call Generation

## Quick Facts
- arXiv ID: 2402.09615
- Source URL: https://arxiv.org/abs/2402.09615
- Reference count: 40
- Key outcome: Fine-tuning CodeLlama-13B on 20,000 Python API Pack instances achieves 49.5% accuracy on unseen API calls, outperforming GPT-3.5 by 10% and GPT-4 by 5%

## Executive Summary
API Pack introduces a massive multi-programming language dataset with over one million instruction-API call pairs spanning 10 programming languages. The dataset aggregates real-world API examples from four major API hubs (RapidAPI, APIGurus, SwaggerHub, IBM API Hub) and demonstrates superior performance in API call generation compared to leading models like GPT-3.5 and GPT-4. The dataset and fine-tuned models are open-sourced to support further research in API call generation across programming languages.

## Method Summary
The methodology involves scraping OpenAPI Specification (OAS) files from four API hubs, processing them to extract API parameters and authentication requirements, generating natural language instructions for each API call, and validating the data quality through filtering. The CodeLlama-13B model is fine-tuned on 20,000 Python instances from the dataset using a 3-shot fine-tuning template with retrieval augmentation. The evaluation framework tests three difficulty levels (seen APIs, new endpoints, unseen APIs) using SequenceMatcher similarity ratio with a threshold of 0.9.

## Key Results
- CodeLlama-13B fine-tuned on 20k Python API Pack instances achieves 49.5% accuracy on Level 3 (unseen APIs), outperforming GPT-3.5 (39.5%) and GPT-4 (44.3%)
- Cross-language generalization works effectively: mixing large dataset in one language with smaller datasets from others achieves comparable performance across 10 programming languages
- Performance scales with dataset size: increasing fine-tuning data to one million instances enhances generalization to new APIs
- Multi-source aggregation provides diversity benefits: outperforms single-source datasets in both 0-shot and 3-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on multi-source API data improves generalization across unseen APIs
- Mechanism: Aggregating API calls from diverse sources exposes the model to varied API patterns, structures, and naming conventions, enhancing its ability to handle novel API scenarios
- Core assumption: API calls from different sources contain sufficient diversity in patterns and structures to improve generalization
- Evidence anchors: Abstract mentions API Pack's multi-source approach reduces performance compared to single-source in both 0-shot and 3-shot settings

### Mechanism 2
- Claim: Larger fine-tuning datasets improve model's ability to generate unseen API calls
- Mechanism: Increased exposure to diverse API patterns through larger datasets enables the model to better recognize and generate novel API call formats
- Core assumption: API call patterns follow learnable structures that scale with data volume
- Evidence anchors: Increasing fine-tuning data to one million instances enhances generalization to new APIs, with roughly linear rise in both endpoint match accuracy and API call match accuracy

### Mechanism 3
- Claim: Cross-language skill transfer enables API call generation across programming languages
- Mechanism: Fine-tuning on large dataset in one language combined with smaller datasets in other languages allows model to learn transferable API call patterns across languages
- Core assumption: API call structures follow similar patterns across programming languages despite syntax differences
- Evidence anchors: Fine-tuning on a large dataset in one language, combined with smaller datasets from others, improves API generation accuracy across multiple languages

## Foundational Learning

- Concept: HTTP request structure and API call patterns
  - Why needed here: Understanding how API calls map to HTTP requests is fundamental for generating correct API calls
  - Quick check question: What are the essential components of an HTTP request that must be present in an API call?

- Concept: Regular expressions for URL and parameter validation
  - Why needed here: Used to verify API call validity by checking URL format and parameter placeholders
  - Quick check question: How would you construct a regex to validate a URL with optional query parameters?

- Concept: Similarity metrics for code comparison
  - Why needed here: Used to evaluate generated API calls against ground truth by comparing structural and semantic correctness
  - Quick check question: What threshold similarity ratio would you use to determine if two API calls are functionally equivalent?

## Architecture Onboarding

- Component map: API Hub scrapers → OAS file processing → API DB creation → Instruction generation → Data validation → Model fine-tuning → Retrieval augmentation → Cross-language generalization → Scaling experiments → Evaluation framework
- Critical path: Data Pipeline → Model Pipeline → Evaluation Pipeline (end-to-end from data collection to performance evaluation)
- Design tradeoffs:
  - Multi-source vs single-source: Multi-source provides diversity but increases complexity; single-source is simpler but may limit generalization
  - Language coverage vs quality: Including more languages increases coverage but may reduce per-language data quality
  - Filtering rigor vs dataset size: Stricter filtering improves quality but reduces dataset size
- Failure signatures:
  - Low accuracy across all levels suggests fundamental model limitations or poor data quality
  - High accuracy on seen APIs but low on unseen suggests overfitting to training data
  - Performance degradation with larger datasets suggests redundant patterns or diminishing returns
- First 3 experiments:
  1. Fine-tune CodeLlama-13B on 20k Python instances and evaluate on Level 3 to verify basic functionality
  2. Test cross-language generalization by fine-tuning on cURL data with 1k instances from other languages
  3. Conduct scaling experiment by fine-tuning on progressively larger subsets (10k, 20k, 40k, 80k, 100k, 1M) and measuring generalization performance

## Open Questions the Paper Calls Out

- Question: How does API Pack's performance scale with dataset size beyond one million instances?
  - Basis in paper: The paper mentions that increasing fine-tuning data to one million instances enhances generalization to new APIs, but doesn't explore beyond this point.
  - Why unresolved: The paper only tests up to one million instances, leaving open the question of whether further scaling would continue to improve performance or if diminishing returns set in.
  - What evidence would resolve it: Testing API Pack fine-tuning with 2-10 million instances and comparing generalization performance to the 1 million instance results would reveal if there's a saturation point or continued improvement.

- Question: How would API Pack perform with execution-based evaluation metrics like pass rate or hallucination rate?
  - Basis in paper: The paper explicitly states that execution-based metrics are challenging due to privacy protection and PII removal from the API database.
  - Why unresolved: The current evaluation relies on structural similarity metrics, which may not fully capture functional correctness or hallucination issues that execution-based testing would reveal.
  - What evidence would resolve it: Implementing privacy-preserving execution testing using synthetic endpoints or sanitized environments would allow direct comparison between structural and execution-based evaluation results.

- Question: Would a fine-tuned re-ranker specifically adapted to API characteristics improve retrieval performance over the generic re-ranker?
  - Basis in paper: The paper notes that 3-shot (retre & rerank) sometimes performs worse than 3-shot (retre), and mentions that better example selection could improve performance but the current re-ranker fails to capture these gains.
  - Why unresolved: The paper uses a generic re-ranker without testing whether an API-specific re-ranker would better identify relevant examples for the 3-shot retrieved & re-ranked setting.
  - What evidence would resolve it: Fine-tuning a re-ranker on API Pack data to distinguish relevant API examples and comparing its performance against the generic re-ranker would determine if specialization improves retrieval quality.

## Limitations

- Cross-language generalization results rely on a specific experimental setup where cURL serves as the "large dataset" language, without broader validation across different language combinations
- Scaling experiments show performance improvements up to 1M instances but lack analysis of computational cost-benefit tradeoffs or potential diminishing returns
- Evaluation relies on structural similarity metrics rather than execution-based testing due to privacy constraints, which may not fully capture functional correctness

## Confidence

- **High confidence**: Performance improvements over GPT-3.5 and GPT-4 on Level 3 unseen APIs (49.5% vs 39.5% and 44.3%) - directly measured with clear methodology
- **Medium confidence**: Cross-language generalization mechanism - supported by experimental results but limited to specific language combinations without broader validation
- **Medium confidence**: Multi-source diversity benefit - shown through comparison but lacks direct ablation studies isolating the contribution of individual sources

## Next Checks

1. Conduct ablation studies testing cross-language transfer with different "large dataset" languages (e.g., Python vs Java as the primary fine-tuning language) to verify the generality of the cross-language mechanism
2. Measure computational cost per training instance at different dataset scales to determine if performance gains justify the resource requirements beyond 100k instances
3. Perform controlled experiments comparing single-source vs multi-source datasets from the same API domains to isolate the true contribution of source diversity versus raw data volume