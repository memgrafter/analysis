---
ver: rpa2
title: 'MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations
  with Multimodal Language Models'
arxiv_id: '2404.00511'
source_url: https://arxiv.org/abs/2404.00511
tags:
- emotion
- multimodal
- cause
- extraction
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the winning submission to Subtask 2 of SemEval
  2024 Task 3 on multimodal emotion cause analysis in conversations. The authors propose
  a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE)
  framework that integrates text, audio, and visual modalities using specialized emotion
  encoders.
---

# MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models

## Quick Facts
- arXiv ID: 2404.00511
- Source URL: https://arxiv.org/abs/2404.00511
- Authors: Zebang Cheng; Fuqiang Niu; Yuxiang Lin; Zhi-Qi Cheng; Bowen Zhang; Xiaojiang Peng
- Reference count: 14
- Primary result: Weighted F1 score of 0.3435, ranking third in SemEval 2024 Task 3 Subtask 2

## Executive Summary
This paper presents the winning submission to Subtask 2 of SemEval 2024 Task 3 on multimodal emotion cause analysis in conversations. The authors propose a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized emotion encoders. The approach leverages modality-specific features for enhanced emotion understanding and causality inference. Experimental evaluation demonstrates the advantages of the multimodal approach, with the submission achieving a weighted F1 score of 0.3435, ranking third in the competition.

## Method Summary
The MER-MCE framework operates in two stages: Multimodal Emotion Recognition (MER) and Multimodal Cause Extraction (MCE). The MER stage employs text, audio, and visual encoders to extract modality-specific features and predict emotions for each utterance. These emotion predictions are then used as input to the MCE stage, which employs a generative LLM approach with the MiniGPTv2 model to extract emotion-cause pairs. The framework utilizes the ECF dataset, which contains 1001 training, 112 development, and 261 test conversations with text, audio, and visual modalities.

## Key Results
- Achieved weighted F1 score of 0.3435 on the test set, ranking third in SemEval 2024 Task 3 Subtask 2
- Demonstrated the effectiveness of integrating text, audio, and visual modalities for emotion-cause pair extraction
- Identified challenges in handling class imbalance, particularly for "disgust" and "fear" categories with limited annotations

## Why This Works (Mechanism)
The MER-MCE framework works by leveraging the complementary strengths of multiple modalities to capture the complex relationships between emotions and their causes in conversations. The multimodal approach allows the model to consider not only the textual content but also the acoustic and visual cues that often provide critical context for understanding emotions. By first predicting emotions using the MER stage and then extracting causes using the MCE stage, the framework can focus on the most relevant parts of the conversation and improve the accuracy of cause identification.

## Foundational Learning
- Multimodal feature extraction: Combining text, audio, and visual features to capture the full context of emotions in conversations. Needed for understanding the complete emotional landscape of conversations.
- Emotion classification: Assigning one of six emotion categories (anger, disgust, fear, joy, sadness, surprise) to each utterance. Required for identifying the emotional state of each utterance.
- Cause extraction: Identifying the specific words or phrases that trigger or contribute to the expressed emotion. Essential for understanding the underlying reasons behind emotions.
- Generative language models: Using models like MiniGPTv2 to generate or extract information based on input context. Enables the extraction of emotion-cause pairs in a flexible and context-aware manner.
- Multimodal fusion: Integrating features from different modalities to create a comprehensive representation. Critical for capturing the full emotional context and improving the accuracy of emotion-cause pair extraction.

## Architecture Onboarding

**Component Map:**
MER (Text, Audio, Visual Encoders) -> Emotion Prediction -> MCE (MiniGPTv2) -> Cause Extraction

**Critical Path:**
The critical path involves the sequential processing of input features through the MER stage, where emotions are predicted, and then the MCE stage, where causes are extracted based on the predicted emotions.

**Design Tradeoffs:**
- Modality selection: The choice of text, audio, and visual modalities balances the need for comprehensive emotional understanding with the complexity of processing multiple input streams.
- Two-stage approach: Separating emotion recognition and cause extraction allows for focused processing but may introduce errors that propagate from the MER stage to the MCE stage.

**Failure Signatures:**
- Poor emotion recognition in the MER stage can lead to incorrect or irrelevant cause extraction in the MCE stage.
- Imbalanced training data for certain emotion categories (e.g., disgust and fear) can result in biased predictions and reduced performance for those categories.

**First Experiments:**
1. Evaluate the performance of the MER stage by measuring the accuracy of emotion predictions on the development set.
2. Assess the impact of different historical context windows on the MCE stage's ability to extract causes accurately.
3. Analyze the contribution of each modality (text, audio, visual) to the overall performance by conducting ablation studies.

## Open Questions the Paper Calls Out
1. What are the specific limitations of the multimodal approach in handling class imbalance, particularly for the "disgust" and "fear" categories?
2. How does the model handle long-range dependencies and future context in real-time settings for emotion cause extraction?
3. What is the impact of facial occlusion and emotional distractors on the model's performance, and how can these issues be mitigated?

## Limitations
- Moderate effectiveness with significant room for improvement in handling complex conversational contexts
- Limited dataset size (1001 training conversations) constraining generalization across diverse real-world scenarios
- Lack of detailed hyperparameter specifications and ablation studies, making it difficult to assess individual component contributions and reproduce results faithfully

## Confidence
- Medium confidence in the overall framework effectiveness: The approach shows merit in combining multimodal features and achieving competitive performance, but the absence of detailed implementation specifics and limited evaluation metrics reduce confidence in the robustness of the findings.
- Low confidence in reproducibility: The paper does not provide specific training configurations, hyperparameter settings, or detailed architectural specifications for the multimodal fusion mechanisms, creating significant barriers to faithful reproduction.
- Medium confidence in the methodology's novelty: The two-stage MER-MCE approach represents a reasonable strategy for emotion-cause pair extraction, but the lack of comparative analysis with baseline models and ablation studies limits confidence in the claimed advantages over existing approaches.

## Next Checks
1. Conduct a systematic ablation study to quantify the individual contributions of text, audio, and visual modalities to overall performance, and determine the optimal fusion strategy.
2. Perform extensive hyperparameter tuning and sensitivity analysis to identify the most influential parameters and establish more robust training configurations.
3. Evaluate the model's generalization capabilities using cross-domain validation with conversations from different sources or domains to assess real-world applicability.