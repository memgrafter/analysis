---
ver: rpa2
title: Using LLMs to Discover Legal Factors
arxiv_id: '2410.07504'
source_url: https://arxiv.org/abs/2410.07504
tags:
- factors
- factor
- legal
- cases
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method using large language models (LLMs)
  to discover legal factors from raw court opinions. The approach involves prompting
  LLMs to identify and define factors relevant to a legal domain, with minimal human
  involvement.
---

# Using LLMs to Discover Legal Factors

## Quick Facts
- arXiv ID: 2410.07504
- Source URL: https://arxiv.org/abs/2410.07504
- Reference count: 30
- Key outcome: Semi-automated LLM approach discovers legal factors with moderate predictive success, finding a novel "disguise or deception" factor in drug interdiction cases.

## Executive Summary
This paper introduces a semi-automated method using large language models to discover legal factors from raw court opinions without relying on pre-existing factor lists. The approach involves prompting LLMs to identify and define factors relevant to a legal domain, followed by minimal human refinement. Evaluated on a drug interdiction dataset, the method achieves moderate predictive accuracy, demonstrating that LLMs can replicate the cognitive task of human legal readers in identifying new factors from case patterns. While not yet matching expert-defined factors, the work shows promise for automating factor discovery in legal reasoning systems.

## Method Summary
The methodology takes raw court opinions as input and uses LLMs to identify relevant factors through a two-stage process: initial factor induction via prompting, followed by refinement to create canonical factor definitions. The approach is evaluated on drug interdiction cases by comparing LLM-discovered factors against expert-defined canonical factors using Matthews Correlation Coefficient (MCC) for predictive performance. The method aims to replicate how attorneys identify patterns in case facts, with minimal human involvement required for refinement.

## Key Results
- LLMs successfully discovered a new "disguise or deception" factor not present in existing canonical lists
- Semi-automated approach produced factor representations with moderate success in predicting case outcomes
- Refined factor representations achieved strong predictive performance when compared to canonical factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can perform factor discovery by mimicking human legal readers who identify patterns in case facts.
- Mechanism: The LLM is prompted to analyze raw court opinions and extract generalized descriptions of relevant patterns (factors) without relying on pre-existing factor lists, similar to how human readers identify instances of fact-based reasons in case texts.
- Core assumption: LLMs can replicate the cognitive task of human legal readers in identifying factors when given appropriate prompts.
- Evidence anchors:
  - [abstract] The paper states that the method "takes as input raw court opinions and produces a set of factors and associated definitions" and that "LLMs to perform a task similar to that which lawyer readers, as distinguished from judges, perform in identifying new factors in case texts."
  - [section] "We prompted an LLM to replicate the work of an attorney in reviewing numerous case opinions, identifying relevant facts and potential factors, and ultimately defining a canonical list of factors."
  - [corpus] Weak evidence - the corpus shows related work on legal reasoning with LLMs but no direct evidence of factor discovery mechanisms.
- Break condition: If the LLM lacks sufficient legal domain knowledge or if the prompting strategy fails to guide the model toward relevant factor identification.

### Mechanism 2
- Claim: Semi-automated refinement of LLM-generated factors by human annotators improves predictive accuracy.
- Mechanism: Human annotators refine the rough factors generated by LLMs by combining similar factors, distinguishing subtle differences, and ensuring comprehensive coverage while maintaining meaningful boundaries between factor definitions.
- Core assumption: Human oversight can correct LLM errors and improve factor quality through domain expertise and judgment.
- Evidence anchors:
  - [abstract] "We demonstrate that a semi-automated approach, incorporating minimal human involvement, produces factor representations that can predict case outcomes with moderate success"
  - [section] "Next, the rough factors are refined into a Refined Factor Representation (RFR). The goal is to define each factor specifically enough to capture its core concept, while keeping it broad enough to cover all its potential manifestations."
  - [corpus] Weak evidence - corpus shows related work on legal reasoning but limited evidence of human refinement improving factor discovery.
- Break condition: If human annotators introduce bias or if the refinement process becomes too labor-intensive, negating the efficiency benefits.

### Mechanism 3
- Claim: LLMs can discover new factors not present in existing canonical lists through analysis of case patterns.
- Mechanism: By analyzing raw case texts without access to pre-existing factor lists, LLMs can identify novel patterns that may represent new factors or sub-factors, as demonstrated by the discovery of a "disguise or deception" factor in drug interdiction cases.
- Core assumption: Novel factors exist in case law that are not captured in existing canonical lists, and LLMs can identify these through pattern recognition.
- Evidence anchors:
  - [abstract] "In fact, an LLM in our experiment did discover a new factor or sub-factor potentially augmenting an existing factor list."
  - [section] "The Llama RFR identified the following 'factor': 'Use of Disguise or Deception: A driver's use of disguise or deception, such as wrapping packages in Christmas paper to blend in with innocent motorists, can support reasonable suspicion.'"
  - [corpus] Weak evidence - corpus shows related work on legal reasoning but limited evidence of LLMs discovering novel factors.
- Break condition: If the discovered factors are not legally meaningful or if they represent noise rather than genuine legal patterns.

## Foundational Learning

- Concept: Legal factor identification and definition
  - Why needed here: Understanding what constitutes a legal factor and how it's defined is crucial for both prompting LLMs and evaluating their output
  - Quick check question: What distinguishes a legal factor from other elements of case analysis?

- Concept: Matthews Correlation Coefficient (MCC) for evaluation
  - Why needed here: MCC is used to evaluate model performance given the class imbalance in the dataset (71% positive vs 29% negative cases)
  - Quick check question: Why is MCC preferred over accuracy for this binary classification problem with imbalanced classes?

- Concept: Factor-based legal reasoning frameworks
  - Why needed here: Understanding how factors are used in computational models of legal reasoning helps contextualize the importance of factor discovery
  - Quick check question: How do factor-based representations enable computational modeling of legal case-based reasoning?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Court opinion segmentation into analysis and conclusion sections
  - Factor induction: LLM prompts for initial factor discovery from raw opinions
  - Factor refinement: Human or LLM-based refinement of initial factor sets
  - Annotation: Factor identification in case texts for evaluation
  - Prediction: Binary classification of case outcomes based on factor presence/absence
  - Evaluation: MCC-based assessment of predictive performance

- Critical path:
  1. Raw court opinions → preprocessing → analysis/conclusion identification
  2. Segmented opinions → LLM factor induction → initial factor set
  3. Initial factors → refinement → refined factor representation
  4. Refined factors → annotation guidelines → factor annotations
  5. Annotated cases → binary vector conversion → predictive modeling
  6. Predictions → MCC evaluation → performance assessment

- Design tradeoffs:
  - Open-source vs proprietary LLMs: Open-source models (e.g., llama-3.1-70b) offer transparency but may require more prompt tuning compared to proprietary models (e.g., gpt-4-turbo)
  - Batch size vs model performance: Smaller batches (10 cases) ensure better sensitivity but increase processing time
  - Human involvement vs automation: Human refinement improves quality but reduces efficiency

- Failure signatures:
  - Low recall in factor identification: LLM missing relevant factors in case texts
  - Poor MCC scores: Refined factors not capturing predictive legal patterns
  - Mismatched factors: Refined factors not aligning with established canonical factors
  - Non-legal factors: LLM identifying factors that are not legally meaningful

- First 3 experiments:
  1. Test LLM factor induction with different batch sizes (5, 10, 20 cases) to find optimal sensitivity
  2. Compare human vs LLM refinement of initial factors using MCC as the evaluation metric
  3. Evaluate different prompting strategies for factor refinement (e.g., temperature settings, context window usage)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the factor refinement process be improved to produce factor representations that are both comprehensive and specific enough to match expert-defined factors?
- Basis in paper: [explicit] The paper notes that the Llama RFR factors were sometimes too specific and other times too abstract compared to the CFR factors, leading to poor predictive performance.
- Why unresolved: The paper identifies the issue but does not provide a solution for balancing specificity and comprehensiveness in factor definitions.
- What evidence would resolve it: Testing different prompting strategies or human-in-the-loop approaches to refine factors, and evaluating their impact on predictive accuracy compared to expert-defined factors.

### Open Question 2
- Question: Can the methodology be adapted to incorporate fundamental legal knowledge to improve the relevance and accuracy of discovered factors?
- Basis in paper: [explicit] The paper highlights that all models, including humans, returned factors that were not legally relevant, emphasizing the need to incorporate legal knowledge into model prompting.
- Why unresolved: The paper does not explore how to integrate legal knowledge into the LLM prompting process to filter out irrelevant factors.
- What evidence would resolve it: Developing and testing prompts that include legal rules or constraints, and measuring the reduction in irrelevant factors and improvement in predictive performance.

### Open Question 3
- Question: How effective is the methodology in identifying new factors in legal domains outside of drug interdiction cases?
- Basis in paper: [explicit] The paper demonstrates the methodology's success in discovering a new factor (disguise or deception) in drug interdiction cases but does not test its generalizability.
- Why unresolved: The paper focuses on a single legal domain and does not explore the methodology's effectiveness in other domains.
- What evidence would resolve it: Applying the methodology to different legal domains (e.g., intellectual property, family law) and comparing the number and quality of new factors discovered to expert-defined factors.

## Limitations
- Limited empirical validation scope: The study only evaluates factor discovery on one legal domain (drug interdiction), raising questions about generalizability to other areas of law.
- Human involvement trade-off: While the paper claims "minimal human involvement," the refinement process still requires significant human effort, potentially limiting scalability.
- Evaluation methodology concerns: The use of MCC for evaluation, while appropriate for imbalanced datasets, may not fully capture the legal validity of discovered factors beyond predictive accuracy.

## Confidence
- High confidence: The core methodology of using LLMs for factor discovery from raw text is technically sound and well-demonstrated.
- Medium confidence: The semi-automated refinement process shows promise but requires more extensive validation across multiple legal domains.
- Low confidence: The claim that LLMs can discover genuinely novel legal factors that improve upon existing canonical lists needs further substantiation.

## Next Checks
1. **Cross-domain validation**: Apply the factor discovery method to a different legal domain (e.g., employment law or intellectual property) to test generalizability of the approach.
2. **Longitudinal study**: Track discovered factors over time to assess whether LLM-identified factors remain relevant as legal precedents evolve.
3. **Expert legal review**: Conduct a blind evaluation where legal experts assess the quality and validity of LLM-discovered factors without knowing their source.