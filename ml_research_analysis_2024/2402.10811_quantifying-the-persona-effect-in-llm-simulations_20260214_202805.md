---
ver: rpa2
title: Quantifying the Persona Effect in LLM Simulations
arxiv_id: '2402.10811'
source_url: https://arxiv.org/abs/2402.10811
tags:
- persona
- variables
- llms
- prompting
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how persona variables influence LLM simulations
  in subjective NLP tasks. Persona variables explain less than 10% of annotation variance
  in most datasets.
---

# Quantifying the Persona Effect in LLM Simulations

## Quick Facts
- arXiv ID: 2402.10811
- Source URL: https://arxiv.org/abs/2402.10811
- Reference count: 39
- One-line primary result: Persona variables explain <10% of annotation variance in most NLP datasets, with persona prompting showing modest improvements primarily for samples with high annotator disagreement

## Executive Summary
This study systematically examines how persona variables influence LLM simulations in subjective NLP tasks. Through analysis of 10 diverse datasets with unaggregated annotations and persona information, the authors find that persona variables explain less than 10% of annotation variance in most cases. While incorporating persona variables via prompting provides modest improvements in LLM predictions, these gains are primarily observed when persona variables have explanatory power and for samples with high annotator disagreement. The research reveals a linear relationship between persona variable utility and LLM prediction accuracy, with powerful models achieving up to 81% of achievable variance.

## Method Summary
The study employs mixed-effect linear regression to quantify how much variance in human annotations can be explained by persona variables across 10 subjective NLP datasets. For the LLM evaluation, the authors use zero-shot persona prompting with four models (GPT-4, GPT-3.5 Turbo, Llama-2, and Tulu-2 variants) in 70B, 13B, and 7B sizes. Predictions are compared with and without persona information using R², Cohen's Kappa, MAE, and F1 metrics. The analysis includes robustness checks by varying prompt order and wording, and investigates the relationship between persona variable explanatory power and LLM prediction improvement.

## Key Results
- Persona variables explain less than 10% of annotation variance in most NLP datasets (1.4%-10.6% marginal R²)
- Persona prompting provides statistically significant improvements in LLM predictions, particularly for samples with high annotator disagreement
- A linear relationship exists between persona variable explanatory power and LLM prediction accuracy, with powerful models achieving up to 81% of achievable variance
- When persona variables have limited explanatory power (as in most NLP datasets), persona prompting shows minimal effect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Persona variables explain less than 10% of annotation variance in most subjective NLP datasets.
- Mechanism: The limited explanatory power of persona variables on annotation variance constrains the potential gains from persona prompting in LLM simulations.
- Core assumption: The persona variables available in existing datasets are representative of the factors that influence human annotations.
- Evidence anchors:
  - [abstract]: "We find that persona variables can explain <10% variance in annotations in existing subjective NLP datasets."
  - [section 3]: "While persona variables (fixed effect) do significantly explain some variance in annotation outcomes, they account for just 1.4%-10.6% of the total variance (Marginal R2), even when controlling for text variation."
  - [corpus]: Weak. Corpus evidence suggests related work exists on persona prompting, but does not directly support the specific claim about variance explained.
- Break condition: If persona variables with higher explanatory power are identified or collected, the mechanism would break.

### Mechanism 2
- Claim: Incorporating persona variables via prompting provides modest but statistically significant improvements in LLM predictions.
- Mechanism: When persona variables do have explanatory power, persona prompting allows LLMs to adjust their predictions to better match human annotations.
- Core assumption: LLMs can effectively utilize the persona information provided in the prompts to adjust their predictions.
- Evidence anchors:
  - [abstract]: "Nonetheless, incorporating them via prompting in LLMs provides modest improvement."
  - [section 4]: "On average, among the 6 models considered, persona prompting shows varying levels of improvement on 3/4 datasets."
  - [corpus]: Weak. Corpus evidence suggests related work on persona prompting, but does not directly support the specific claim about improvements in LLM predictions.
- Break condition: If LLMs are unable to effectively utilize the persona information in the prompts, the mechanism would break.

### Mechanism 3
- Claim: There is a linear relationship between the explanatory power of persona variables and the improvement in LLM predictions using persona prompting.
- Mechanism: As the explanatory power of persona variables increases, the potential for improvement in LLM predictions through persona prompting also increases.
- Core assumption: The relationship between persona variable explanatory power and LLM prediction improvement is linear.
- Evidence anchors:
  - [abstract]: "Notably, we find a linear relationship in our setting: the stronger the correlation between persona variables and human annotations, the more accurate the LLM predictions are using persona prompting."
  - [section 6]: "Our results show a positive correlation between the target and predicted R2 values - the higher the target R2 value, the higher the predicted R2."
  - [corpus]: Weak. Corpus evidence suggests related work on persona prompting, but does not directly support the specific claim about the linear relationship.
- Break condition: If the relationship between persona variable explanatory power and LLM prediction improvement is found to be non-linear, the mechanism would break.

## Foundational Learning

- Concept: Linear regression and mixed-effect models
  - Why needed here: The paper uses these statistical methods to analyze the explanatory power of persona variables and the performance of LLM predictions.
  - Quick check question: What is the difference between a linear regression model and a mixed-effect linear regression model?

- Concept: Cohen's Kappa and mean absolute error
  - Why needed here: These are the metrics used to evaluate the performance of LLM predictions in comparison to human annotations.
  - Quick check question: How is Cohen's Kappa calculated and what does it measure?

- Concept: Persona variables and their role in subjective NLP tasks
  - Why needed here: The paper investigates how persona variables influence human annotations and LLM simulations in subjective NLP tasks.
  - Quick check question: What are some examples of persona variables that might be relevant in subjective NLP tasks?

## Architecture Onboarding

- Component map: Datasets with unaggregated annotations and persona information -> Mixed-effect linear regression analysis -> Zero-shot persona prompting with LLM models -> Evaluation using R², Cohen's Kappa, MAE, and F1

- Critical path:
  1. Collect and prepare subjective NLP datasets with unaggregated annotations and persona information
  2. Analyze the explanatory power of persona variables using mixed-effect linear regression
  3. Evaluate the performance of LLM predictions with and without persona prompting using fixed-effect linear regression
  4. Investigate the relationship between persona variable explanatory power and LLM prediction improvement

- Design tradeoffs:
  - Zero-shot prompting vs. fine-tuning: Zero-shot prompting allows for quick experimentation but may not achieve the best performance. Fine-tuning can improve performance but requires more resources and time.
  - Prompt format: The choice of prompt format can impact the effectiveness of persona prompting. Different formats may be more suitable for different tasks or LLM models.

- Failure signatures:
  - Low R2 values for both persona variables and LLM predictions: Indicates that persona variables have limited explanatory power and persona prompting does not improve LLM predictions.
  - Inconsistent improvements across different LLM models: Suggests that the effectiveness of persona prompting may depend on the specific LLM model used.

- First 3 experiments:
  1. Analyze the explanatory power of persona variables in a new subjective NLP dataset using mixed-effect linear regression.
  2. Evaluate the performance of an LLM model on the same dataset with and without persona prompting using fixed-effect linear regression.
  3. Investigate the relationship between persona variable explanatory power and LLM prediction improvement by varying the persona variables used in the prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can persona prompting reliably simulate diverse perspectives in subjective NLP tasks?
- Basis in paper: [explicit] The study finds that persona variables explain less than 10% of annotation variance in most NLP datasets, and persona prompting shows limited improvements, especially when persona variables have low explanatory power.
- Why unresolved: The study suggests that persona variables in existing datasets have limited explanatory power, casting doubt on the efficacy of persona prompting in the current NLP landscape. However, it remains unclear if more nuanced or diverse datasets could change this outcome.
- What evidence would resolve it: Testing persona prompting on datasets with higher explanatory power from persona variables or on datasets from diverse cultural contexts could provide clearer insights.

### Open Question 2
- Question: How do individual-level persona variables compare to group-level persona variables in improving LLM predictions?
- Basis in paper: [inferred] The study notes that persona variables typically accessible to researchers are group-level, while individual identity formation involves both individual and group-level characteristics. This mismatch may explain the limited effectiveness of persona prompting.
- Why unresolved: The study does not explore the impact of individual-level persona variables, which could potentially lead to better simulation of diverse perspectives.
- What evidence would resolve it: Conducting experiments with individual-level persona variables and comparing their impact on LLM predictions to group-level variables would clarify this issue.

### Open Question 3
- Question: What is the impact of cultural and linguistic diversity on the effectiveness of persona prompting?
- Basis in paper: [explicit] The study acknowledges that most available datasets with persona information are from the U.S., and it speculates that persona prompting might be less effective in non-English languages due to cultural and linguistic differences.
- Why unresolved: The study does not include datasets from non-English languages or diverse cultural contexts, limiting its generalizability.
- What evidence would resolve it: Evaluating persona prompting on multilingual datasets and datasets from diverse cultural backgrounds would provide insights into its effectiveness across different contexts.

## Limitations
- The limited explanatory power of available persona variables in existing datasets constrains the generalizability of findings to settings where persona factors have stronger predictive relationships with annotations
- The study relies on zero-shot prompting rather than fine-tuning, which may underestimate the potential of persona-informed approaches
- The linear relationship assumption between persona explanatory power and LLM improvement needs further validation across diverse tasks and model architectures

## Confidence
- **High confidence**: The finding that persona variables explain <10% of annotation variance in most datasets is well-supported by the mixed-effect regression analysis across multiple datasets
- **Medium confidence**: The modest improvements from persona prompting are statistically significant but show variability across models and datasets, suggesting context-dependent effectiveness
- **Medium confidence**: The linear relationship between persona variable utility and LLM prediction accuracy is observed in the experimental setting but requires validation across broader task domains

## Next Checks
1. **Dataset expansion validation**: Test the persona prompting approach on newly collected datasets where persona variables are expected to have higher explanatory power (e.g., explicitly political opinion tasks) to verify if improvements scale beyond the 10% variance threshold observed in current datasets

2. **Fine-tuning comparison**: Implement persona-aware fine-tuning on a subset of models and tasks to determine if the modest zero-shot improvements can be substantially enhanced through parameter optimization rather than prompting alone

3. **Non-linear relationship assessment**: Conduct experiments systematically varying the strength of persona variable influence (by weighting or filtering persona features) to empirically test whether the relationship between persona utility and LLM improvement remains linear or exhibits threshold effects