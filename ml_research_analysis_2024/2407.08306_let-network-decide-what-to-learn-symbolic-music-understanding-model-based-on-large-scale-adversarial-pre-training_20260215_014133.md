---
ver: rpa2
title: 'Let Network Decide What to Learn: Symbolic Music Understanding Model Based
  on Large-scale Adversarial Pre-training'
arxiv_id: '2407.08306'
source_url: https://arxiv.org/abs/2407.08306
tags:
- music
- tokens
- tasks
- pre-training
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses bias problems in symbolic music understanding
  (SMU) models caused by mask language model (MLM) pre-training, where certain tokens
  are harder to infer from context and lead to overfitting to training data distributions.
  The proposed Adversarial-MidiBERT introduces an adversarial pre-training approach
  where a masker network learns to avoid masking tokens that are difficult to recover
  from context, thus reducing bias.
---

# Let Network Decide What to Learn: Symbolic Music Understanding Model Based on Large-scale Adversarial Pre-training

## Quick Facts
- **arXiv ID**: 2407.08306
- **Source URL**: https://arxiv.org/abs/2407.08306
- **Reference count**: 0
- **Primary result**: 97.92% accuracy on composer classification, outperforming previous BERT-based approaches while converging faster

## Executive Summary
This paper addresses bias problems in symbolic music understanding models caused by mask language model pre-training, where certain tokens are harder to infer from context and lead to overfitting to training data distributions. The proposed Adversarial-MidiBERT introduces an adversarial pre-training approach where a masker network learns to avoid masking tokens that are difficult to recover from context, thus reducing bias. Additionally, a mask fine-tuning mechanism is introduced to bridge the gap between pre-training and fine-tuning. The method is evaluated on four SMU tasks: composer classification (97.92% accuracy), emotion recognition (79.46%), velocity prediction (45.58%), and melody extraction (92.68%), outperforming previous BERT-based approaches while converging faster.

## Method Summary
The method involves adversarial pre-training where a masker network learns to avoid masking context-free tokens identified by their high recovery loss, while a recoverer network attempts to recover masked tokens. After 15 epochs, the top 15% hardest-to-recover tokens are frozen to prevent overfitting. The model uses an Octuple tokenization scheme with eight attributes (time signature, tempo, bar position, relative position, instrument, pitch, duration, velocity) embedded separately and concatenated. A mask fine-tuning mechanism is applied during downstream task training, replacing 30% of input tokens with [MASK] tokens to maintain consistency with pre-training. The approach is evaluated on four downstream tasks using a BERT encoder with 12 layers and 768 hidden size.

## Key Results
- Composer Classification: 97.92% accuracy
- Emotion Recognition: 79.46% accuracy
- Velocity Prediction: 45.58% accuracy
- Melody Extraction: 92.68% accuracy
- Outperforms previous BERT-based approaches while converging faster

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial masking reduces overfitting by preventing the model from learning context-free token patterns that are recoverable only from training data distribution.
- Mechanism: The masker network learns to avoid masking tokens that are difficult to recover from context. Tokens with high recovery loss are considered context-free and are frozen during pre-training, forcing the model to learn contextual relationships rather than distributional patterns.
- Core assumption: Context-free tokens are identifiable by their high recovery loss when masked, and avoiding their masking will lead to better generalization.
- Evidence anchors:
  - [abstract]: "By avoiding the masking of tokens that are difficult to infer from context, our model is better equipped to capture contextual structures and relationships, rather than merely conforming to the training data distribution."
  - [section 2.2]: "After repeating this process for k epochs, we believe the tokens with the highest masking probabilities are the most challenging to recover. These tokens correspond to context-free tokens, as they can only be predicted based on the data distribution of the training set, leading to the lowest accuracy."
  - [corpus]: Weak - No direct citations found in the corpus papers discussing this specific adversarial masking approach for music understanding.

### Mechanism 2
- Claim: Mask fine-tuning bridges the gap between pre-training and fine-tuning by maintaining the presence of [MASK] tokens during both phases.
- Mechanism: During fine-tuning, random tokens are replaced with [MASK] tokens at a fixed probability (p%), which maintains consistency with the pre-training phase where [MASK] tokens are always present. This reduces the domain gap and improves convergence.
- Core assumption: The presence of [MASK] tokens during pre-training but their absence during fine-tuning creates a domain gap that negatively impacts fine-tuning performance.
- Evidence anchors:
  - [section 2.3]: "During fine-tuning, we can still utilize the data augmentation methods employed in pre-training if the downstream tasks are tonality-independent. However, a potential gap may arise since the [MASK] token is present in every epoch during pre-training but is absent in fine-tuning."
  - [abstract]: "Furthermore, we propose a mask fine-tuning method to narrow the data gap between pre-training and fine-tuning, which can help the model converge faster and perform better."
  - [corpus]: Weak - No direct citations found in the corpus papers discussing this specific mask fine-tuning approach.

### Mechanism 3
- Claim: Dynamic attribute weighting improves model performance by allocating more attention to attributes with lower recovery accuracy.
- Mechanism: The weights for each attribute's loss function are dynamically adjusted based on the average recovery accuracy of that attribute in the previous epoch. Attributes with lower accuracy receive higher weights, forcing the model to focus more on learning these attributes.
- Core assumption: Different attributes have varying convergence speeds and performance characteristics, and dynamically adjusting their weights can balance the learning process.
- Evidence anchors:
  - [section 2.2]: "We notice that different attributes have varying convergence speeds and performance, so we design a dynamic weight to balance the loss between them. At the beginning of training, w1 ∼ w8 are set equally to 0.125. Then, in the nth epoch, wj is set as: wj = 1/aj / Σ(1/ai), where ai is the average recovered accuracy of the ith attribute in the (n − 1)th epoch."
  - [abstract]: No direct mention of dynamic attribute weighting.
  - [corpus]: Weak - No direct citations found in the corpus papers discussing this specific dynamic weighting approach.

## Foundational Learning

- Concept: Adversarial learning
  - Why needed here: To create a masker network that learns to avoid masking context-free tokens, thereby reducing bias in the pre-trained model.
  - Quick check question: What is the fundamental principle behind adversarial learning, and how does it differ from standard supervised learning?

- Concept: Bidirectional Encoder Representations from Transformers (BERT)
  - Why needed here: BERT serves as the backbone architecture for the Adversarial-MidiBERT model, providing the transformer-based architecture for capturing relationships in symbolic music.
  - Quick check question: How does BERT's bidirectional attention mechanism differ from the autoregressive attention used in models like GPT?

- Concept: Mask Language Model (MLM) pre-training
  - Why needed here: MLM is the standard pre-training approach that Adversarial-MidiBERT modifies to reduce bias. Understanding MLM is crucial for understanding the improvements made by Adversarial-MidiBERT.
  - Quick check question: What is the primary objective of MLM pre-training, and what are its limitations when applied to symbolic music understanding?

## Architecture Onboarding

- Component map: Input MIDI file -> Octuple tokenization -> Eight attribute embeddings -> Concatenation -> BERT encoder -> Masker network -> Recoverer network -> Frozen tokens (top 15%) -> Downstream task classifiers

- Critical path: 1. Input MIDI file → Octuple tokenization 2. Token attributes → Separate embeddings → Concatenation 3. BERT encoder processes embedded sequence 4. Masker generates masking probabilities 5. Recoverer attempts to recover masked tokens 6. Loss functions update masker and recoverer weights 7. Frozen tokens prevent certain tokens from being masked 8. Fine-tuning with mask fine-tuning mechanism

- Design tradeoffs:
  - Memory vs. performance: Using 12-layer BERT with 768 hidden size requires significant GPU memory (27GB) but provides good performance
  - Pre-training vs. fine-tuning: Longer pre-training improves performance but increases training time
  - Masking ratio: The percentage of tokens to mask affects both the difficulty of the task and the risk of overfitting

- Failure signatures:
  - Poor convergence: Model fails to learn meaningful representations despite sufficient training
  - Overfitting: High performance on training data but poor generalization to test data
  - Bias persistence: Model still exhibits bias in downstream tasks despite adversarial pre-training
  - Masker ineffectiveness: Masker fails to identify context-free tokens, leading to continued masking of context-dependent tokens

- First 3 experiments:
  1. Baseline comparison: Run the same downstream tasks with standard MidiBERT to establish baseline performance metrics
  2. Ablation study - without adversarial pre-training: Train the model with standard MLM pre-training to quantify the improvement from the adversarial mechanism
  3. Ablation study - without mask fine-tuning: Train the model with standard fine-tuning to measure the impact of the mask fine-tuning mechanism on convergence speed and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the unfreezing mechanism (randomly unfreezing b% of frozen tokens) impact the overall effectiveness of bias mitigation in Adversarial-MidiBERT?
- Basis in paper: [explicit] The paper mentions that b% of frozen tokens are randomly unfrozen to prevent incorrect freezing, but doesn't analyze the impact of different unfreezing rates on model performance.
- Why unresolved: The paper only mentions the existence of the unfreezing mechanism without quantifying its importance or testing different unfreezing rates to determine optimal values.
- What evidence would resolve it: Comparative experiments testing different b% values (e.g., 0%, 5%, 10%, 20%) to measure their impact on bias reduction and downstream task performance would provide clarity on the mechanism's importance.

### Open Question 2
- Question: Can the adversarial pre-training approach be effectively applied to other MIR tasks beyond symbolic music understanding, such as audio-based music classification or generation?
- Basis in paper: [inferred] The paper focuses on symbolic music understanding but suggests future work exploring applications to music generation tasks, implying potential generalizability.
- Why unresolved: The paper only demonstrates the effectiveness of the approach on symbolic music data and doesn't test whether the same methodology would work for raw audio data or other MIR domains.
- What evidence would resolve it: Applying the same adversarial pre-training framework to audio-based MIR tasks and comparing performance with standard pre-training methods would validate its broader applicability.

### Open Question 3
- Question: What is the long-term impact of avoiding context-free token masking on the model's ability to learn general musical patterns versus style-specific patterns?
- Basis in paper: [explicit] The paper states the goal is to learn "music structure, relationships, and regularities like basic mode, riff regularities, and modulation regularities" rather than style-specific patterns.
- Why unresolved: While the paper argues this approach reduces bias, it doesn't empirically demonstrate whether this leads to better generalization across different musical styles or whether it might sacrifice important style-specific knowledge.
- What evidence would resolve it: Cross-dataset evaluations testing the model's ability to generalize to musical styles not present in the pre-training data would clarify whether avoiding context-free tokens improves or hinders musical understanding.

## Limitations

- Limited validation of the adversarial mechanism: The paper focuses on downstream task performance rather than directly validating the masker's accuracy in distinguishing context-free from context-dependent tokens.
- Lack of comparative ablation studies: No comparison between standard MLM pre-training, pre-training with mask fine-tuning, and pre-training with only the adversarial mechanism.
- Implementation complexity and resource requirements: Requires significant computational resources (27GB GPU memory) without discussing hyperparameter sensitivity.

## Confidence

- **High confidence**: Reported downstream task performance metrics are based on standard evaluation protocols and show clear improvements over baseline MidiBERT approaches.
- **Medium confidence**: Theoretical justification for why adversarial masking reduces bias is logically sound but empirical validation is limited.
- **Low confidence**: Claim that mask fine-tuning significantly improves convergence speed lacks direct empirical support with convergence curves.

## Next Checks

- **Check 1: Ablation study of adversarial pre-training**: Implement controlled experiment comparing standard MidiBERT with MLM pre-training, Adversarial-MidiBERT without mask fine-tuning, and full Adversarial-MidiBERT to isolate the contribution of the adversarial pre-training component.

- **Check 2: Masker accuracy validation**: Design experiments to directly evaluate the masker network's ability to correctly identify context-free tokens using synthetic test cases or controlled experiments with known context-freeness.

- **Check 3: Resource sensitivity analysis**: Conduct experiments varying key hyperparameters including percentage of frozen tokens (5% to 25%), freezing epoch (5 to 20 epochs), and mask fine-tuning ratio (10% to 50%) to determine method robustness and optimal configurations.