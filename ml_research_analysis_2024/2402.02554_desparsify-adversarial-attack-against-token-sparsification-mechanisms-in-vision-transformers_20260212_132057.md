---
ver: rpa2
title: 'DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in
  Vision Transformers'
arxiv_id: '2402.02554'
source_url: https://arxiv.org/abs/2402.02554
tags:
- attack
- clean
- tokens
- adversarial
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DeSparsify, the first adversarial attack targeting
  token sparsification mechanisms in vision transformers. The attack exploits the
  dynamic nature of token sparsification to degrade model performance by forcing the
  use of all available tokens.
---

# DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers

## Quick Facts
- arXiv ID: 2402.02554
- Source URL: https://arxiv.org/abs/2402.02554
- Authors: Oryan Yehezkel; Alon Zolfi; Amit Baras; Yuval Elovici; Asaf Shabtai
- Reference count: 40
- Primary result: First adversarial attack targeting token sparsification mechanisms in vision transformers

## Executive Summary
This paper introduces DeSparsify, the first adversarial attack specifically designed to exploit token sparsification mechanisms in vision transformers. The attack crafts adversarial examples that force models to use all available tokens, dramatically increasing computational resources (74% FLOPs, 37% memory, 72% energy) while maintaining classification accuracy. DeSparsify demonstrates effectiveness across multiple sparsification mechanisms (ATS, AdaViT, A-ViT) and shows transferability between them, posing significant security concerns for resource-constrained applications using token pruning.

## Method Summary
DeSparsify employs a PGD-based attack with a custom loss function that balances two objectives: forcing maximum token utilization through the sparsification mechanism while preserving original classification. The attack uses an ℓ∞ norm bounded perturbation (ϵ = 16/255) with cosine annealing step size and scaling term λ = 8×10⁻⁴. The method targets three token sparsification mechanisms (ATS, AdaViT, A-ViT) across vision transformer models (DeiT-s, DeiT-t, T2T-ViT) using ImageNet and CIFAR-10 datasets. The attack can be implemented as single-image, class-universal, or universal perturbations.

## Key Results
- Successfully increases floating-point operations by 74% and memory usage by 37% on ATS mechanisms
- Achieves up to 90% token utilization increase while maintaining original classification accuracy
- Demonstrates transferability between different token sparsification mechanisms (ATS → AdaViT, AdaViT → ATS)
- Universal perturbations effective across multiple images with minimal loss in attack performance

## Why This Works (Mechanism)
The attack exploits the dynamic nature of token sparsification by crafting adversarial examples that force the decision mechanism to retain all tokens rather than pruning unimportant ones. Token sparsification mechanisms use learned or heuristic criteria to determine which tokens can be discarded during inference. DeSparsify's custom loss function manipulates the input to trigger these decision mechanisms in a way that maximizes token retention while still maintaining the original classification through the cross-entropy preservation term.

## Foundational Learning
- **Vision Transformer Architecture**: Understanding the token-based processing pipeline and attention mechanisms is essential for identifying how token pruning decisions are made and where they can be manipulated
- **Token Sparsification Mechanisms**: Knowledge of how ATS, AdaViT, and A-ViT decide which tokens to prune is crucial for crafting effective adversarial examples that maximize token retention
- **PGD Adversarial Attack Framework**: Familiarity with projected gradient descent attacks provides the foundation for understanding how DeSparsify extends this framework with custom loss functions
- **Adversarial Transferability**: Understanding why attacks transfer between different models and mechanisms helps explain DeSparsify's cross-mechanism effectiveness
- **Resource Consumption Metrics**: Knowledge of FLOPs, memory usage, and energy consumption metrics is necessary to evaluate the attack's impact on system resources

## Architecture Onboarding

Component Map: Input Image -> Vision Transformer -> Token Sparsification Mechanism -> Token Retention Decision -> Classification Output

Critical Path: The attack targets the Token Retention Decision component by manipulating the input to maximize the threshold for token pruning, forcing the mechanism to retain all tokens while maintaining classification through the preservation loss term.

Design Tradeoffs: The primary tradeoff is between attack effectiveness (token utilization) and classification preservation, controlled by the scaling term λ. Higher λ increases resource consumption but risks degrading accuracy, while lower λ preserves accuracy but reduces attack impact.

Failure Signatures: If the attack fails to increase token utilization, the custom loss function may not be correctly targeting the sparsification mechanism's decision process. If classification accuracy degrades significantly, the scaling term λ may need adjustment or the cross-entropy loss may be improperly implemented.

First Experiments:
1. Implement the custom loss function for ATS mechanism and verify it increases token utilization on clean images
2. Test basic PGD attack with standard loss function to establish baseline resource consumption
3. Evaluate the effect of different λ scaling values on the balance between token utilization and classification accuracy

## Open Questions the Paper Calls Out
- How effective would DeSparsify be against more advanced token sparsification mechanisms like A-ViT that use fewer parameters?
- Can the attack's transferability be improved through better loss function design or training strategies?
- What are the most effective real-time countermeasures that preserve model accuracy while mitigating DeSparsify attacks?

## Limitations
- Evaluation limited to three specific token sparsification mechanisms (ATS, AdaViT, A-ViT)
- Focus on relatively small vision transformer models (DeiT-s, DeiT-t, T2T-ViT)
- Universal perturbation variants require extensive computation (10K-50K images)
- Does not explore adaptive defenses or comprehensive mitigation strategies

## Confidence

High Confidence: The attack successfully increases resource consumption (FLOPs, memory, energy) while maintaining classification accuracy on tested models and mechanisms.

Medium Confidence: Transferability claims between different sparsification mechanisms are supported by experimental evidence but underlying reasons are not fully explained.

Low Confidence: Claims about real-world deployment scenarios and specific hardware impact (battery life, inference latency) are speculative and not empirically validated.

## Next Checks
1. Test transferability across different sparsification hyper-parameters and evaluate whether the attack remains effective when token pruning threshold is dynamically adjusted during inference.

2. Implement and evaluate potential adaptive defenses, such as token reconstruction mechanisms or adversarial training specifically targeting token utilization.

3. Extend evaluation to larger vision transformer models (DeiT-base, Swin) and more diverse sparsification mechanisms to validate generalizability.