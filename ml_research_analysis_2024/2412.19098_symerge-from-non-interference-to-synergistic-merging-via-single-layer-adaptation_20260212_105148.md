---
ver: rpa2
title: 'SyMerge: From Non-Interference to Synergistic Merging via Single-Layer Adaptation'
arxiv_id: '2412.19098'
source_url: https://arxiv.org/abs/2412.19098
tags:
- merging
- tasks
- performance
- task
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SyMerge, a method that redefines model merging
  by shifting from merely avoiding task interference to actively achieving synergy
  between tasks. SyMerge leverages a pilot study showing that cross-task performance
  is strongly correlated with merge quality, and that adapting a single task-specific
  layer can substantially enhance compatibility.
---

# SyMerge: From Non-Interference to Synergistic Merging via Single-Layer Adaptation

## Quick Facts
- **arXiv ID**: 2412.19098
- **Source URL**: https://arxiv.org/abs/2412.19098
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art performance in multi-task model merging by optimizing a single task-specific layer and encoder merging coefficients using expert-guided self-labeling

## Executive Summary
SyMerge redefines model merging by shifting focus from merely avoiding task interference to actively achieving synergy between tasks. The method leverages a pilot study showing that cross-task performance strongly predicts merge quality, and that adapting a single task-specific layer can substantially enhance compatibility. SyMerge jointly optimizes one task-specific layer and the encoder's merging coefficients using a robust self-labeling strategy guided by expert model predictions, rather than unstable entropy minimization. This approach demonstrates state-of-the-art performance across vision, dense prediction, and NLP benchmarks while producing highly transferable adapted layers that improve other merging methods.

## Method Summary
SyMerge performs multi-task learning through model merging by jointly optimizing merging coefficients and a single task-specific layer using unlabeled test data. The method initializes a merged encoder via task arithmetic, then adapts both the encoder's merging coefficients and a single task-specific layer (e.g., classifier) through self-labeling with predictions from individually fine-tuned expert models. This joint optimization creates synergy between tasks while reducing interference, and the adapted layers are shown to be highly transferable to other merging methods. The approach is lightweight, effective, and scales robustly with increasing numbers of tasks.

## Key Results
- Achieves state-of-the-art performance across vision, dense prediction (segmentation, depth, normals), and NLP benchmarks
- Single-layer adaptation produces comparable performance regardless of which layer is chosen
- Adapted layers are highly transferable and improve performance of other merging methods
- Scales robustly from 8 to 20 tasks with consistent performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enhancing functional alignment between tasks improves model merging performance.
- Mechanism: By training a task-specific layer on features from a merged encoder, the classifier becomes more compatible with encoders from other tasks, leading to better cross-task performance which predicts merge quality.
- Core assumption: Cross-task performance is a reliable indicator of merge success; functional alignment can be improved by minimal adaptation of a single layer.
- Evidence anchors: [abstract]: "when a classifier trained on one task is paired with the encoder of another, the resulting cross-task performance strongly predicts merge quality"; [section]: "Our intuition comes from a pilot study showing that when a classifier trained on one task is paired with the encoder of another, the resulting cross-task performance strongly predicts merge quality"
- Break condition: If cross-task performance does not correlate with merge quality, or if minimal adaptation fails to improve alignment.

### Mechanism 2
- Claim: Joint optimization of merging coefficients and a single task-specific layer creates synergy and reduces interference.
- Mechanism: By adapting both the shared encoder's merging coefficients and a single task-specific layer during test-time, the method allows the two components to adapt to each other, leading to better task specialization and reduced interference.
- Core assumption: Task-specific information can be effectively captured by refining a single layer; joint optimization is more effective than optimizing either component alone.
- Evidence anchors: [abstract]: "jointly optimizes one task-specific layer and the encoder's merging coefficients using a robust self-labeling strategy"; [section]: "Unlike conventional entropy minimization, which we find unstable, SyMerge employs a robust self-labeling strategy guided by expert model predictions"
- Break condition: If joint optimization does not outperform optimizing either component alone, or if task-specific information cannot be captured by a single layer.

### Mechanism 3
- Claim: Expert-guided self-labeling is more stable and effective than entropy minimization for test-time adaptation.
- Mechanism: Instead of minimizing prediction entropy, the method uses predictions from individually fine-tuned models as supervisory signals, minimizing cross-entropy between the merged model's predictions and these expert predictions.
- Core assumption: Expert model predictions provide more reliable and stable training signals than entropy-based methods, especially for tasks beyond classification.
- Evidence anchors: [abstract]: "employs a robust self-labeling strategy guided by expert model predictions, avoiding the pitfalls of entropy-based adaptation"; [section]: "To ensure stability without labels, SyMerge employs a robust self-labeling strategy guided by expert model predictions, avoiding the pitfalls of entropy-based adaptation"
- Break condition: If expert model predictions are not available or of poor quality, or if self-labeling introduces bias or instability.

## Foundational Learning

- Concept: Cross-task linearity assumption
  - Why needed here: The theoretical justification (Proposition 1) relies on the assumption that a merged model's output is approximately the average of individual model outputs, which simplifies the analysis of how task synergy affects merge performance.
  - Quick check question: If we merge two models with outputs f_i(x) and f_j(x), what does cross-task linearity assume about the merged model's output f_merge(x)?

- Concept: Convex loss functions
  - Why needed here: The proof of Proposition 1 requires the loss function to be convex in its output to apply Jensen's inequality, which provides the upper bound on the merged model's loss.
  - Quick check question: Why is convexity of the loss function important for the theoretical justification of why synergy improves merge performance?

- Concept: Functional alignment
  - Why needed here: The core insight is that enhancing functional alignment between tasks (by training task-specific layers on merged features) improves both cross-task performance and merge quality.
  - Quick check question: How does training a task-specific layer on merged encoder features improve functional alignment, and why is this important for model merging?

## Architecture Onboarding

- Component map:
  - Input data -> Shared encoder (initially merged via task arithmetic, then adapted) -> Task-specific layers (one per task, jointly optimized) -> Output predictions
  - Expert models provide predictions for self-labeling supervision

- Critical path:
  1. Initialize merged encoder via task arithmetic
  2. Initialize task-specific layers and merging coefficients
  3. For each batch of unlabeled data:
     a. Forward pass through shared encoder and task-specific layers
     b. Compute self-labeling loss using expert predictions
     c. Backpropagate and update both task-specific layers and merging coefficients
  4. Repeat until convergence

- Design tradeoffs:
  - Single-layer adaptation vs. full model adaptation: Minimal adaptation reduces computational cost but may limit performance gains
  - Joint optimization vs. sequential optimization: Joint optimization allows components to adapt to each other but may be more complex to implement
  - Self-labeling vs. entropy minimization: Self-labeling is more stable but requires access to expert models

- Failure signatures:
  - Poor performance on individual tasks: Indicates insufficient adaptation or interference between tasks
  - Degraded performance on specific tasks: Suggests the merging coefficients or task-specific layers are not well-tuned for those tasks
  - Instability during training: May indicate issues with the self-labeling strategy or learning rate settings

- First 3 experiments:
  1. Verify that training a task-specific layer on merged encoder features improves cross-task performance
  2. Compare joint optimization of task-specific layers and merging coefficients against optimizing either component alone
  3. Evaluate the stability and effectiveness of self-labeling using expert predictions versus entropy minimization on a small set of tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the enhanced cross-task performance from SyMerge's adapted layer transfer to other model architectures beyond ViT (e.g., ConvNets or Swin Transformers)?
- Basis in paper: [inferred] The paper demonstrates high transferability of SyMerge's adapted layer to other merging methods but does not test across different backbone architectures.
- Why unresolved: Transferability was only evaluated within the same architecture family, leaving questions about generalization to different model types.
- What evidence would resolve it: Conducting SyMerge adaptation experiments using ConvNet-based or Swin Transformer-based models and measuring cross-task performance gains.

### Open Question 2
- Question: What is the optimal layer to adapt in SyMerge for different task types (e.g., classification vs. dense prediction)?
- Basis in paper: [explicit] The paper shows that adapting a single layer achieves comparable performance regardless of which layer is chosen, but does not identify if certain tasks benefit more from adapting specific layers.
- Why unresolved: While layer choice appears robust, task-specific analysis was not conducted to determine if certain tasks might have preferred adaptation layers.
- What evidence would resolve it: Systematic experiments varying which layer is adapted for different task categories (classification, segmentation, depth estimation) and comparing performance gains.

### Open Question 3
- Question: How does SyMerge's performance scale with extremely large numbers of tasks (e.g., 50+ tasks) compared to other methods?
- Basis in paper: [explicit] The paper demonstrates robust scaling from 8 to 20 tasks but does not test performance at much larger scales.
- Why unresolved: The paper establishes good scaling properties up to 20 tasks but leaves open whether performance advantages persist at extreme scales.
- What evidence would resolve it: Scaling experiments testing SyMerge with 50+ tasks and comparing against other merging methods at these scales.

### Open Question 4
- Question: Can the expert models used in SyMerge's self-labeling strategy be dynamically composed rather than using fixed individual models?
- Basis in paper: [explicit] The paper mentions this as a promising future direction, noting that merged models can yield slight performance advantages over individual models.
- Why unresolved: The current implementation uses static expert models, but the paper acknowledges potential benefits of dynamic composition.
- What evidence would resolve it: Experiments implementing dynamic expert model composition (e.g., weighted combinations of individual models) and measuring impact on SyMerge performance.

## Limitations
- The method depends critically on the quality and availability of expert model predictions for self-labeling
- The assumption that a single task-specific layer can capture sufficient task-specific information may not hold for all task types
- Performance on tasks beyond vision, dense prediction, and NLP benchmarks remains unknown

## Confidence
- **High Confidence**: The correlation between cross-task performance and merge quality is well-established by the pilot study; effectiveness of joint optimization over sequential optimization is demonstrated across multiple benchmarks
- **Medium Confidence**: Superiority of expert-guided self-labeling over entropy minimization is shown, but method's robustness to noisy or biased expert predictions is not fully explored; transferability of adapted layers to other merging methods is demonstrated but may depend on specific task combinations
- **Low Confidence**: Theoretical justification relies on strong assumptions (e.g., cross-task linearity, convex losses) that may not hold in practice; scalability to very large numbers of tasks or extremely diverse task types is untested

## Next Checks
1. **Robustness to Expert Prediction Quality**: Systematically evaluate SyMerge's performance when expert model predictions are degraded (e.g., by reducing fine-tuning epochs, introducing label noise, or using less capable base models). This will test the method's reliance on high-quality self-labels.

2. **Transferability Across Task Types**: Apply SyMerge to a new task type not evaluated in the paper (e.g., speech recognition, graph-based tasks) and assess whether the adapted layers remain transferable to other merging methods. This will test the generalizability of the approach.

3. **Scalability and Task Diversity**: Increase the number of tasks beyond 20 and include tasks with diverse modalities (e.g., combining vision, NLP, and speech) to evaluate the method's scalability and ability to handle heterogeneous task combinations. This will test the limits of the cross-task linearity assumption and the single-layer adaptation strategy.