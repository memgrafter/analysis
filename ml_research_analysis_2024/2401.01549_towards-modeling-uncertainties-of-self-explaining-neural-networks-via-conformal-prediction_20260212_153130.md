---
ver: rpa2
title: Towards Modeling Uncertainties of Self-explaining Neural Networks via Conformal
  Prediction
arxiv_id: '2401.01549'
source_url: https://arxiv.org/abs/2401.01549
tags:
- prediction
- concept
- xtest
- self-explaining
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of quantifying uncertainty in
  self-explaining neural networks, which simultaneously generate predictions and explanations
  but lack distribution-free uncertainty quantification. The authors propose a novel
  framework called unSENN that provides rigorous uncertainty guarantees for both the
  generated explanations and final predictions.
---

# Towards Modeling Uncertainties of Self-explaining Neural Networks via Conformal Prediction

## Quick Facts
- arXiv ID: 2401.01549
- Source URL: https://arxiv.org/abs/2401.01549
- Authors: Wei Qian; Chenxu Zhao; Yangyi Li; Fenglong Ma; Chao Zhang; Mengdi Huai
- Reference count: 40
- One-line primary result: Proposed unSENN framework provides rigorous uncertainty guarantees for self-explaining neural networks, achieving higher label coverage (98.8% vs 90.3% on MNIST with 10% calibration data) and tighter prediction sets compared to baselines.

## Executive Summary
This paper addresses the challenge of quantifying uncertainty in self-explaining neural networks (SENNs) that generate both predictions and explanations but lack distribution-free uncertainty quantification. The authors propose unSENN, a novel framework that provides rigorous uncertainty guarantees for both generated explanations and final predictions using conformal prediction. The method constructs non-conformity measures to capture informative high-level basis concepts, enabling distribution-free uncertainty quantification even without ground-truth explanations, and designs effective transfer functions to move from concept-level prediction sets to final prediction sets. The approach is theoretically grounded with coverage guarantees and demonstrates strong empirical performance on MNIST and CIFAR-100 Super-class datasets.

## Method Summary
The unSENN framework applies conformal prediction to self-explaining neural networks by first constructing non-conformity measures that capture how "strange" the generated concept explanations are for a given sample. These measures quantify the alignment between predicted concept scores and true concept values. The method then optimizes over possible concept vectors to find those satisfying the non-conformity constraint while maximizing confidence for specific labels, using adversarial attack-style optimization. Finally, transfer functions map concept-level prediction sets to final label prediction sets, enabling distribution-free uncertainty quantification with coverage guarantees based on exchangeability assumptions rather than strict i.i.d. requirements.

## Key Results
- Achieved 98.8% label coverage on MNIST with 10% calibration data versus 90.3% for vanilla conformal prediction
- Demonstrated tighter prediction sets while maintaining coverage guarantees
- Successfully handled cases without ground-truth explanations through optimization-based concept approximation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The non-conformity measures capture informative high-level concepts in the interpretation layer, enabling distribution-free uncertainty quantification
- Mechanism: The non-conformity score quantifies how "strange" the generated concept explanations are for a given sample by comparing the predicted concept scores to the true concept values. Smaller scores indicate better alignment and higher confidence.
- Core assumption: The true concepts are either known (for concept bottleneck models) or can be approximated through optimization (for prototype-based models)
- Evidence anchors:
  - [section]: "In order to quantify how 'strange' the generated concept explanations are for a given sample xi, we utilize the underlying well-trained self-explaining model f to construct the following non-conformity measure"
  - [abstract]: "Their approach constructs non-conformity measures to capture informative high-level basis concepts, enabling distribution-free uncertainty quantification even without ground-truth explanations"
  - [corpus]: Weak evidence - corpus papers discuss conformal prediction and uncertainty but don't specifically address self-explaining networks or non-conformity measures for basis concepts
- Break condition: If the concept prediction scores are not well-calibrated or the true concepts are too distant from what the model can learn, the non-conformity scores become meaningless

### Mechanism 2
- Claim: The transfer functions effectively move from concept-level prediction sets to final prediction sets
- Mechanism: The method optimizes over possible concept vectors v to find those that satisfy the non-conformity constraint while maximizing the confidence for a specific label, using adversarial attack-style optimization to find worst-case valid predictions
- Core assumption: The relationship between concept space and label space is monotonic enough that a valid concept set implies a valid label set
- Evidence anchors:
  - [section]: "To get better conformal predictions for the final predicted labels, we then design effective transfer functions, which involve the search of possible labels under certain constraints"
  - [abstract]: "They then design effective transfer functions to move from concept-level prediction sets to final prediction sets"
  - [corpus]: Weak evidence - corpus papers discuss conformal prediction but not specifically the transfer from concept to label space in self-explaining networks
- Break condition: If the prediction head g is highly non-linear or discontinuous, the transfer function may fail to capture all valid label predictions

### Mechanism 3
- Claim: Exchangeability of calibration samples and test sample ensures coverage guarantees
- Mechanism: The theoretical framework relies on exchangeability rather than i.i.d. assumptions, allowing coverage guarantees even when the data distribution is unknown or non-stationary
- Core assumption: The calibration samples and test sample are exchangeable, meaning their joint distribution is invariant under permutations
- Evidence anchors:
  - [section]: "Suppose that the calibration samples ( Dcal = {(xi, ci, yi)}N cal i=1 ) and the given test sample xtest are exchangeable"
  - [abstract]: "The method is theoretically grounded with coverage guarantees"
  - [corpus]: Weak evidence - corpus papers discuss conformal prediction and exchangeability but don't specifically address self-explaining networks
- Break condition: If the test sample comes from a fundamentally different distribution than the calibration samples, exchangeability fails and coverage guarantees break

## Foundational Learning

- Concept: Conformal prediction
  - Why needed here: Provides distribution-free uncertainty quantification without requiring strict parametric assumptions
  - Quick check question: What is the key difference between conformal prediction and traditional confidence intervals?

- Concept: Self-explaining neural networks
  - Why needed here: The architecture simultaneously generates predictions and explanations, requiring uncertainty quantification for both outputs
  - Quick check question: How do concept bottleneck models differ from prototype-based self-explaining networks?

- Concept: Exchangeability
  - Why needed here: The weaker assumption underlying the theoretical coverage guarantees, more general than i.i.d.
  - Quick check question: Why is exchangeability a weaker assumption than i.i.d. for conformal prediction?

## Architecture Onboarding

- Component map: Input features x -> Concept layer h(x) -> Prediction head g(v) -> Non-conformity measure s(x, y, f) -> Transfer function optimization
- Critical path:
  1. Train self-explaining model (concept predictor h and prediction head g)
  2. Compute non-conformity scores on calibration set
  3. Calculate quantile Q1-ε
  4. Construct concept prediction set for test sample
  5. Optimize transfer function to get final prediction set
- Design tradeoffs:
  - λ2 parameter: Balances top-K concepts vs. remaining concepts in non-conformity measure
  - Calibration set size: Larger sets give more stable quantiles but increase computation time
  - Number of concepts C: More concepts increase interpretability but may hurt prediction accuracy
- Failure signatures:
  - Empty prediction sets: Non-conformity scores too high relative to Q1-ε
  - Very large prediction sets: Non-conformity scores too low relative to Q1-ε
  - Coverage failure: Exchangeability assumption violated or model poorly calibrated
- First 3 experiments:
  1. Verify non-conformity score distribution on calibration data
  2. Test coverage guarantee on a simple dataset with known ground truth
  3. Compare prediction set size and coverage against baseline methods on MNIST

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of non-conformity measure affect the quality of uncertainty quantification for self-explaining neural networks?
- Basis in paper: [explicit] The paper discusses designing novel non-conformity measures to capture informative high-level basis concepts for self-explaining neural networks.
- Why unresolved: The paper does not explore the impact of different non-conformity measures on the performance of uncertainty quantification.
- What evidence would resolve it: Experimental results comparing different non-conformity measures on various datasets and self-explaining network architectures.

### Open Question 2
- Question: How does the performance of the proposed method scale with the size of the calibration set?
- Basis in paper: [explicit] The paper mentions that a larger calibration set can enhance the stability of the prediction set.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between calibration set size and method performance.
- What evidence would resolve it: Empirical results showing the effect of varying calibration set sizes on key performance metrics such as label coverage and average set size.

### Open Question 3
- Question: Can the proposed method be extended to other types of self-explaining neural networks beyond concept bottleneck and prototype-based models?
- Basis in paper: [inferred] The paper focuses on concept bottleneck and prototype-based self-explaining networks but does not discuss generalizability to other architectures.
- Why unresolved: The paper does not provide theoretical or empirical evidence of the method's applicability to other self-explaining network types.
- What evidence would resolve it: Successful application of the method to different self-explaining network architectures, such as attention-based or gradient-based models, with comparable performance.

## Limitations

- Exchangeability assumption may not hold in real-world settings with distribution shift, potentially breaking coverage guarantees
- Method requires ground truth concepts for concept bottleneck models or carefully learned prototypes for prototype-based models, which may be difficult to obtain in practice
- Empirical evaluation limited to two relatively clean datasets (MNIST and CIFAR-100 Super-class), raising questions about generalizability to more complex domains

## Confidence

- **High Confidence**: The theoretical framework for conformal prediction is well-established and correctly applied. The basic architecture of self-explaining neural networks is sound and well-documented.
- **Medium Confidence**: The specific non-conformity measures designed for self-explaining networks appear reasonable but lack extensive empirical validation across diverse datasets. The transfer function optimization methodology is theoretically justified but its practical effectiveness depends heavily on implementation details.
- **Low Confidence**: The coverage guarantees in real-world settings where exchangeability assumptions may be violated. The method's robustness to noisy or ambiguous concepts that may arise in complex applications.

## Next Checks

1. **Exchangeability Validation**: Systematically test the coverage guarantees under distribution shift by evaluating performance when calibration and test data come from slightly different distributions, measuring actual coverage vs. theoretical guarantees.

2. **Concept Quality Assessment**: Evaluate the quality of learned prototypes or predicted concepts by measuring their alignment with human-interpretable concepts, and assess how concept quality impacts prediction set calibration and coverage.

3. **Generalization Testing**: Apply the method to more complex datasets with ambiguous or overlapping concepts (e.g., ImageNet with hierarchical labels) to test whether the theoretical advantages hold when concepts are not clearly defined.