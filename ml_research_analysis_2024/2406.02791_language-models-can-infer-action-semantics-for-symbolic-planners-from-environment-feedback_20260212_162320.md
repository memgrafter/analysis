---
ver: rpa2
title: Language Models can Infer Action Semantics for Symbolic Planners from Environment
  Feedback
arxiv_id: '2406.02791'
source_url: https://arxiv.org/abs/2406.02791
tags:
- action
- domain
- semantics
- psalm
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PSALM enables AI agents to automatically infer symbolic action
  semantics in new domains without human annotation by combining large language models
  with symbolic planners. The method uses LLMs to sample candidate action trajectories,
  executes them in simulation to collect environment feedback, then infers action
  semantics (pre- and post-conditions) based on execution outcomes.
---

# Language Models can Infer Action Semantics for Symbolic Planners from Environment Feedback

## Quick Facts
- arXiv ID: 2406.02791
- Source URL: https://arxiv.org/abs/2406.02791
- Reference count: 36
- Language Models can infer symbolic action semantics for symbolic planners from environment feedback without human annotation

## Executive Summary
PSALM enables AI agents to automatically infer symbolic action semantics in new domains without human annotation by combining large language models with symbolic planners. The method uses LLMs to sample candidate action trajectories, executes them in simulation to collect environment feedback, then infers action semantics (pre- and post-conditions) based on execution outcomes. A probabilistic memory of action semantics is iteratively updated and sampled to guide a symbolic planner toward the goal. Experiments on 7 benchmark domains show PSALM achieves 100% plan success rate compared to <40% for direct LLM planning, and learns correct action semantics with significantly fewer environment resets and execution steps than baselines.

## Method Summary
PSALM combines LLM-based trajectory sampling with symbolic planning to learn action semantics through interaction. The system maintains a probabilistic memory of possible action semantics, updating beliefs based on execution outcomes. It uses both LLM-based and rule-based action semantics generators, sampling from the belief distribution to guide planner search. Trajectory prospection checks action preconditions before execution to avoid invalid steps. The method learns action semantics from one task per domain and achieves 100% plan success on all 20 tasks in each domain.

## Key Results
- PSALM achieves 100% plan success rate compared to <40% for direct LLM planning
- Learns correct action semantics with significantly fewer environment resets and execution steps than baselines
- Demonstrates complementary benefits of LLM and rule-based action semantics predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PSALM's probabilistic memory of action semantics allows the system to gradually converge on correct action definitions through iterative execution and feedback.
- Mechanism: The system maintains belief probabilities for each possible action precondition/postcondition statement. After each execution attempt, it updates these beliefs using an exponential forgetting rule that strengthens correct predictions and weakens incorrect ones.
- Core assumption: The environment provides reliable feedback (error messages) that accurately indicates when action preconditions are unsatisfied.
- Evidence anchors:
  - [abstract] "PSALM maintains a belief over possible action semantics that is iteratively updated until a goal state is reached"
  - [section 4.4] "We keep a memory of the hypothesized action semantics. For each action's action semantics, we store two lists of predicted statements for pre-conditions and postconditions. Each statement ϕ is associated with a belief p(ϕ ∈ Φa|a)"
  - [corpus] Weak evidence - PSALM-V builds on this work but doesn't discuss the probabilistic memory mechanism in detail
- Break condition: If environment feedback is unreliable or doesn't clearly indicate precondition failures, the belief updates will reinforce incorrect action semantics.

### Mechanism 2
- Claim: Combining LLM-based and rule-based action semantics generators provides complementary benefits that accelerate learning.
- Mechanism: The LLM generator leverages commonsense reasoning to propose likely action semantics, while the rule-based parser directly infers preconditions from error messages and postconditions from state transitions. Both predictions update the shared memory.
- Core assumption: LLM commonsense knowledge is reasonably aligned with the domain's actual dynamics, and error messages accurately identify unsatisfied preconditions.
- Evidence anchors:
  - [section 4.3] "We combine LLM-based and rule-based action semantics generation given environment feedback"
  - [section 5.3] "LLM and rule-based action semantics predictions have complementary benefits"
  - [corpus] PSALM-V extends this approach but focuses more on visual environments rather than the text-based PDDL setup
- Break condition: If LLM commonsense is systematically misaligned with domain mechanics, or if error messages don't specify which preconditions are missing, the combined approach will struggle.

### Mechanism 3
- Claim: Trajectory prospection prevents wasted execution steps by rejecting invalid trajectories before simulator invocation.
- Mechanism: Before executing a sampled trajectory, the system performs forward prediction to check if the first v actions satisfy the currently hypothesized preconditions. If not, it samples replacement actions until finding valid ones.
- Core assumption: The current belief about action semantics is sufficiently accurate to predict trajectory validity, and replacement sampling will eventually find a valid path.
- Evidence anchors:
  - [section 4.2] "We add trajectory prospection, enabling the system to do forward prediction in an open-loop fashion before actually invoking the simulator"
  - [section 5.3] "Prospection induces redundant actions sometimes, but is necessary overall"
  - [corpus] How Far Are LLMs from Symbolic Planners? discusses LLM limitations in state tracking but doesn't address prospection specifically
- Break condition: If the current action semantics belief is too inaccurate, prospection will reject too many trajectories or accept invalid ones, wasting computation.

## Foundational Learning

- Concept: Symbolic planning in PDDL requires complete action semantics (preconditions and postconditions) to guarantee plan success.
  - Why needed here: PSALM's entire approach depends on learning these action semantics through interaction rather than receiving them as input
  - Quick check: Can the planner generate a valid plan when given complete action semantics?

- Concept: Probabilistic memory allows gradual convergence on correct action definitions through iterative updates.
  - Why needed here: Without probabilistic memory, the system would need to restart learning from scratch after each failed execution.
  - Quick check: Does the belief distribution stabilize over time as more feedback is collected?

- Concept: Trajectory prospection reduces wasted execution steps by checking preconditions before simulator invocation.
  - Why needed here: Without prospection, the system would execute many invalid trajectories, wasting environment resets.
  - Quick check: Does prospection reduce the number of environment resets compared to direct execution?

## Architecture Onboarding

### Component Map
PSALM consists of LLM trajectory sampler -> Action semantics generator (LLM + rule-based) -> Probabilistic memory -> Symbolic planner -> Environment -> Feedback loop

### Critical Path
The critical path is: LLM trajectory sampling -> Prospection check -> Execution -> Feedback collection -> Action semantics update -> Planner search

### Design Tradeoffs
- LLM-based vs rule-based action semantics: LLM provides commonsense reasoning but may be inaccurate; rule-based is precise but limited to observable transitions
- Probabilistic memory update rate: Faster updates learn quicker but may oscillate; slower updates are stable but learn slowly
- Trajectory prospection depth: Deeper prospection prevents more errors but increases computation; shallower prospection is faster but less effective

### Failure Signatures
- Low plan success rate (<100%) indicates incorrect action semantics inference
- High number of environment resets suggests inefficient exploration or poor prospection
- Slow convergence indicates belief updates are too conservative or trajectory sampling is ineffective

### First Experiments
1. Test PSALM on a simple domain (BARMAN) with perfect environment feedback to verify basic functionality
2. Compare PSALM's learning curve with and without trajectory prospection to quantify its impact
3. Evaluate PSALM's robustness to noisy environment feedback by adding random errors to execution results

## Open Questions the Paper Calls Out
None

## Limitations
- Probabilistic memory mechanism has not been validated in domains with complex state representations or unreliable feedback
- LLM commonsense alignment with domain dynamics is assumed rather than demonstrated
- The system relies on error messages accurately identifying unsatisfied preconditions, which may not hold in all environments

## Confidence

- **High**: PSALM learns correct action semantics faster than baselines (directly measured through plan success rates and action accuracy)
- **Medium**: Trajectory prospection prevents wasted execution (paper notes it "induces redundant actions sometimes" but lacks detailed analysis)
- **Low**: LLM commonsense knowledge is well-aligned with domain dynamics (assumed rather than demonstrated)

## Next Checks

1. Test PSALM on domains where environment feedback is noisy or ambiguous - do belief updates still converge correctly?

2. Compare PSALM's performance when using only LLM-based versus only rule-based action semantics generators to quantify their complementary benefits

3. Measure how PSALM's memory accuracy evolves over time - does it stabilize at correct action semantics or continue fluctuating?