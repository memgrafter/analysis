---
ver: rpa2
title: Characterizing LLM Abstention Behavior in Science QA with Context Perturbations
arxiv_id: '2404.12452'
source_url: https://arxiv.org/abs/2404.12452
tags:
- context
- questions
- abstention
- performance
- unanswerable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models (LLMs) handle
  unanswerable questions in scientific contexts by systematically removing or perturbing
  context passages. The authors probe six LLMs (GPT-4, GPT-3.5, Vicuna, Llama2, Flan-T5,
  Mistral) on four QA datasets (SQuAD2, PubmedQA, BioASQ, QASPER) using controlled
  context perturbations: removing context, replacing with random context, and adding
  noisy context.'
---

# Characterizing LLM Abstention Behavior in Science QA with Context Perturbations

## Quick Facts
- **arXiv ID:** 2404.12452
- **Source URL:** https://arxiv.org/abs/2404.12452
- **Reference count:** 15
- **Primary result:** No model consistently abstains when context is insufficient in scientific QA tasks

## Executive Summary
This paper investigates how large language models handle unanswerable questions in scientific contexts by systematically removing or perturbing context passages. The authors probe six LLMs across four QA datasets using controlled context perturbations to understand abstention behavior. Results reveal significant variation across datasets, question types, and model architectures, with counter-intuitive findings that adding irrelevant context sometimes improves both abstention and task performance. The study highlights the need for better evaluation frameworks and dataset designs that account for model abstention behavior in scientific QA.

## Method Summary
The authors systematically remove or perturb context passages in four scientific QA datasets (SQuAD2, PubmedQA, BioASQ, QASPER) and evaluate six LLMs (GPT-4, GPT-3.5, Vicuna, Llama2, Flan-T5, Mistral) on their ability to abstain from answering when context is insufficient. Three perturbation types are used: removing context entirely, replacing with random context, and adding noisy context. The study measures both abstention rates and task performance across different question types, particularly focusing on boolean questions which pose unique challenges for abstention behavior.

## Key Results
- No model consistently abstains when context is insufficient across all datasets
- Adding irrelevant context sometimes improves both abstention and task performance
- Boolean questions show particularly poor abstention rates regardless of context quality
- Significant variation in abstention behavior across different model architectures

## Why This Works (Mechanism)
The study reveals that LLM abstention behavior in scientific QA is highly context-dependent and non-linear. Models don't simply fail when context is removed; instead, their behavior varies based on dataset characteristics, question types, and the nature of context perturbations. The counter-intuitive improvement with irrelevant context suggests models may be using spurious correlations or pattern matching rather than true comprehension. This mechanism highlights the complexity of designing evaluation frameworks that accurately measure when models should abstain versus attempt answers.

## Foundational Learning
1. **Context Perturbation Methods** - Why needed: To systematically evaluate model behavior under varying information availability
   Quick check: Can the perturbation strategy be extended to more subtle context variations?
2. **Abstention vs. Task Performance Trade-off** - Why needed: To understand if models can balance accuracy with appropriate non-answers
   Quick check: Does abstention rate correlate with overall task performance across models?
3. **Dataset-Specific Behavior** - Why needed: To identify which scientific domains pose unique challenges for abstention
   Quick check: Can findings be generalized to emerging scientific fields or multilingual contexts?

## Architecture Onboarding

**Component Map:** Context Processing -> Question Analysis -> Answer Generation -> Abstention Decision

**Critical Path:** The abstention decision occurs at the intersection of context availability assessment and question type analysis, making it a critical control point in the QA pipeline.

**Design Tradeoffs:** Models must balance between attempting answers (risking incorrect responses) and abstaining (potentially missing answerable questions), with no clear optimal strategy across all contexts.

**Failure Signatures:** Poor abstention on boolean questions regardless of context, counter-intuitive performance improvements with irrelevant context, and inconsistent behavior across similar question types.

**First Experiments:**
1. Test abstention behavior on open-ended, multi-hop reasoning questions
2. Evaluate progressive relevance of added context versus completely random context
3. Implement human evaluation to verify model understanding of unanswerable questions

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Controlled context perturbations may not reflect real-world context quality variations
- Focus on binary and extractive question types may limit generalizability
- Counter-intuitive findings with irrelevant context raise questions about true model comprehension
- English-language focus may not represent multilingual scientific contexts

## Confidence

- General abstention behavior patterns: **High**
- Dataset-specific variations: **Medium**
- Counter-intuitive effects of irrelevant context: **Low**
- Cross-model architectural comparisons: **Medium**

## Next Checks

1. Test the context perturbation effects on open-ended, multi-hop reasoning questions to assess generalization beyond extractive tasks
2. Evaluate whether adding progressively relevant (rather than random) context affects abstention behavior differently than completely irrelevant context
3. Implement human evaluation to verify whether improved abstention rates correspond to better handling of genuinely unanswerable questions in real-world scientific scenarios