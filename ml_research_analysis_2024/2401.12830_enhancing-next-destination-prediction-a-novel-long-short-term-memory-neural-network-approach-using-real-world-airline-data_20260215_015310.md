---
ver: rpa2
title: 'Enhancing Next Destination Prediction: A Novel Long Short-Term Memory Neural
  Network Approach Using Real-World Airline Data'
arxiv_id: '2401.12830'
source_url: https://arxiv.org/abs/2401.12830
tags:
- customer
- data
- features
- destination
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study developed a novel LSTM-based model with a sliding window
  approach for next destination prediction in the airline industry. The model was
  tested on a real-world dataset with 115 million flight records from 36 million customers,
  using 222,000 customers with more than 17 flights each.
---

# Enhancing Next Destination Prediction: A Novel Long Short-Term Memory Neural Network Approach Using Real-World Airline Data

## Quick Facts
- arXiv ID: 2401.12830
- Source URL: https://arxiv.org/abs/2401.12830
- Reference count: 6
- Top-1 F1 score of 0.78 and top-7 F1 scores of 0.97 achieved

## Executive Summary
This study presents a novel LSTM-based model with a sliding window approach for next destination prediction in the airline industry. Using a real-world dataset of 115 million flight records from 36 million customers, the model achieved strong performance metrics including top-1 F1 scores of 0.78 and top-7 F1 scores of 0.97. The research demonstrates that increasing customer size significantly improves performance across all top-N F1 scores, while window size had no significant impact. The model successfully captures sequential patterns in travel data to predict future destinations, with the best results achieved using the largest customer sample size.

## Method Summary
The researchers developed a two-layer LSTM model with a sliding window approach to predict next destinations. The model was trained on a filtered dataset of 222,000 customers with more than 17 flights each, using three customer sizes (5K, 15K, 25K) and three window sizes (5, 10, 15). The architecture includes embedding layers for city-based features, two LSTM layers (100 and 20 nodes), and a dense layer with softmax activation. Custom feature engineering was implemented, including numerical, categorical, date, and embedding features, along with temporal features like average day difference and domestic flight count.

## Key Results
- Top-1 F1 scores reached 0.78 and top-7 F1 scores reached 0.97 with largest customer size
- Increasing customer size significantly improved performance across all top-N F1 scores
- Window size had no significant impact on model performance
- The model successfully captured sequential patterns in travel data to predict future destinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LSTM captures sequential travel patterns that enable next destination prediction
- Mechanism: LSTM's ability to model long-term dependencies and retain temporal information allows it to recognize patterns in a customer's travel history that are predictive of future destinations
- Core assumption: Travel behavior exhibits temporal dependencies that can be learned from historical sequences
- Evidence anchors:
  - [abstract]: "enabling accurate predictions of individual travelers' future destinations" through "capturing the sequential patterns and dependencies in travel data"
  - [section]: "LSTM because of its capacity to capture patterns and temporal dependencies within destination sequences"
- Break condition: If travel patterns are too random or if temporal dependencies are too weak to be statistically significant

### Mechanism 2
- Claim: Sliding window approach enhances LSTM's ability to capture both short-term and long-term dependencies
- Mechanism: By systematically using sequential trip pairs within a dynamic window, the model can identify temporal patterns at different scales
- Core assumption: Travel patterns exhibit dependencies at multiple time scales
- Evidence anchors:
  - [section]: "utilizes a unique real-world airline dataset... A novel model architecture with a sliding window approach based on Long Short-Term Memory (LSTM)"
  - [section]: "enables the LSTM model to recognize recent travel behaviors within a predetermined window of past trips" and "accommodate a variety of historical trips, including more distant and recent ones"
- Break condition: If the window size parameter is not optimized, or if travel patterns don't exhibit meaningful dependencies at different time scales

### Mechanism 3
- Claim: Larger customer size improves model performance by providing more diverse training data
- Mechanism: More customer data allows the model to learn a wider variety of travel patterns and generalize better to unseen cases
- Core assumption: Travel behavior patterns are consistent enough across customers to be learned from larger datasets
- Evidence anchors:
  - [section]: "the most favorable outcomes in terms of top-1, top-3, and top-7 F1 scores are obtained when the customer data is at its largest scale"
  - [section]: "Results showed that increasing customer size significantly improved performance across all top-N F1 scores"
- Break condition: If customer data becomes too large to manage computationally, or if travel patterns are highly individual-specific with little generalizability

## Foundational Learning

- Concept: Sequential data modeling
  - Why needed here: The problem involves predicting the next destination based on a sequence of previous trips
  - Quick check question: How does sequential modeling differ from traditional classification in terms of input representation?

- Concept: Feature engineering for temporal patterns
  - Why needed here: Custom features like average day difference, domestic flight count, and return trip count help capture travel-specific patterns
  - Quick check question: What types of temporal features might be most predictive of future travel behavior?

- Concept: Sliding window technique
  - Why needed here: Creates overlapping sequences of past trips to train the model on different temporal contexts
  - Quick check question: How does the choice of window size affect the model's ability to capture different time-scale dependencies?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline -> Embedding layers for city-based features -> Concatenation layer -> Two LSTM layers (100 nodes, then 20 nodes) -> Dense layer with softmax activation -> Custom features layer

- Critical path: Data → Windowing → Preprocessing → Embedding → LSTM layers → Dense → Output

- Design tradeoffs:
  - Window size vs. computational efficiency (larger windows capture more context but increase dimensionality)
  - LSTM layer size vs. overfitting (more nodes can capture more complexity but risk overfitting)
  - Customer sample size vs. resource constraints (larger samples improve performance but require more computational power)

- Failure signatures:
  - Low F1 scores across all top-N values → model isn't learning meaningful patterns
  - Performance plateaus despite increasing customer size → approaching data limitations
  - High variance between different customer subsets → model isn't generalizing well

- First 3 experiments:
  1. Test different window sizes (5, 10, 15) with fixed customer size to identify optimal temporal context
  2. Compare model performance across different customer sizes (5K, 15K, 25K) to quantify data scaling effects
  3. Evaluate the impact of custom features by training with and without them to measure their contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance change when incorporating more diverse contextual features like weather conditions, traffic congestion, or time of day?
- Basis in paper: [explicit] The paper mentions that LSTMs may struggle with integrating diverse contextual features like time of day, weather conditions, and traffic congestion.
- Why unresolved: The current study focuses on a limited set of features and does not explore the impact of incorporating more complex contextual information.
- What evidence would resolve it: Experiments comparing model performance with and without additional contextual features, using datasets that include this information.

### Open Question 2
- Question: What is the optimal balance between model complexity and computational efficiency for large-scale airline datasets?
- Basis in paper: [explicit] The paper notes that LSTMs can be computationally intensive and may struggle with extremely large datasets.
- Why unresolved: The study uses a subset of the full dataset due to computational constraints, leaving questions about scalability unanswered.
- What evidence would resolve it: Performance comparisons of different model architectures (varying complexity) on progressively larger subsets of the full dataset, measuring both accuracy and computational time.

### Open Question 3
- Question: How does the model's performance vary across different customer segments with distinct travel patterns?
- Basis in paper: [inferred] The study uses a diverse customer base but doesn't analyze performance variations across different customer types or travel behaviors.
- Why unresolved: The paper focuses on aggregate performance metrics without examining how well the model generalizes to different customer segments.
- What evidence would resolve it: Performance analysis broken down by customer characteristics such as frequent vs. occasional travelers, business vs. leisure travelers, or domestic vs. international travelers.

## Limitations
- Dataset filtering to 222,000 customers with >17 flights may introduce selection bias toward frequent travelers
- Only tested three window sizes and three customer sizes, leaving uncertainty about optimal parameters
- Evaluated on single airline dataset without comparison to alternative approaches or validation on different transportation domains

## Confidence
- **High Confidence**: The mechanism by which LSTM captures sequential travel patterns and the positive correlation between customer size and model performance
- **Medium Confidence**: The effectiveness of the sliding window approach for capturing temporal dependencies, as window size showed no significant impact
- **Low Confidence**: The generalizability of results to other transportation domains or different airline datasets

## Next Checks
1. Test the model on a different airline dataset or transportation domain to assess generalizability beyond the original data source
2. Expand the parameter space by testing additional window sizes (e.g., 3, 7, 20) and customer sizes (e.g., 10K, 20K) to identify optimal configurations
3. Implement and compare against baseline models (e.g., traditional ML approaches, simpler RNN architectures) to quantify the LSTM model's relative advantage