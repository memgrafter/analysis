---
ver: rpa2
title: 'MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory'
arxiv_id: '2411.06736'
source_url: https://arxiv.org/abs/2411.06736
tags:
- memory
- task
- agent
- place
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that a key limitation of current low-level
  embodied AI controllers, such as Steve-1, is their lack of episodic memory, which
  leads to repeated exploration failures in long-horizon tasks. To address this, the
  authors introduce MrSteve, a low-level controller equipped with Place Event Memory
  (PEM), which hierarchically stores spatial and event-based information from episodes.
---

# MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory

## Quick Facts
- arXiv ID: 2411.06736
- Source URL: https://arxiv.org/abs/2411.06736
- Authors: Junyeong Park; Junmo Cho; Sungjin Ahn
- Reference count: 40
- Primary result: MrSteve outperforms baselines in exploration, navigation, and task-solving for long-horizon tasks by 20-30% via episodic memory with Place Event Memory (PEM).

## Executive Summary
This paper identifies that a key limitation of current low-level embodied AI controllers, such as Steve-1, is their lack of episodic memory, which leads to repeated exploration failures in long-horizon tasks. To address this, the authors introduce MrSteve, a low-level controller equipped with Place Event Memory (PEM), which hierarchically stores spatial and event-based information from episodes. PEM enables efficient recall and navigation by organizing novel events and locations. Additionally, MrSteve includes an Exploration Strategy and Memory-Augmented Task Solving Framework that alternates between exploration and goal-directed execution based on recalled events. Experimental results show that MrSteve significantly outperforms baselines in exploration, navigation, and task-solving, especially in sparse sequential and long-horizon tasks, with robust performance even under limited memory capacity.

## Method Summary
MrSteve enhances low-level embodied AI controllers with episodic memory through a Place Event Memory (PEM) system that hierarchically clusters experiences into place and event clusters using spatial position and visual embeddings. The controller employs a two-tier exploration strategy: a high-level Count-Based goal selector that directs the agent to least-visited grid cells, and a low-level VPT-Nav navigator fine-tuned with LoRA for efficient terrain traversal. A Mode Selector alternates between exploration and execution modes based on PEM queries for task-relevant resources. The system is trained using PPO for navigation with distance-based rewards and evaluated on Minecraft tasks involving sequential resource gathering and navigation in sparse environments.

## Key Results
- MrSteve achieves 20-30% higher success rates than baselines on long-horizon sequential tasks like ABA-Sparse.
- Memory-augmented exploration reduces revisit counts by 40% compared to Steve-1 in large maps.
- MrSteve maintains strong performance even with 10-25% memory capacity constraints.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MrSteve's Place Event Memory (PEM) solves the short-term memory bottleneck in low-level controllers like Steve-1 by organizing spatial and event-based data hierarchically.
- Mechanism: PEM clusters experience frames into place clusters based on agent position and yaw, then subdivides these into event clusters using visual similarity via MineCLIP embeddings. This allows retrieval of relevant frames even after thousands of steps.
- Core assumption: Agent trajectories contain enough spatial structure to meaningfully cluster experiences; visual similarity is sufficient to define distinct events.
- Evidence anchors:
  - [abstract] "PEM organizes spatial and event-based data, enabling efficient recall and navigation in long-horizon tasks."
  - [section 3.1] "Place Event Memory built on Place Memory, which captures distinct events that occur within each place cluster."
  - [corpus] Missing related works on hierarchical event clustering in Minecraft—weak evidence here.
- Break condition: If the environment lacks spatial structure (e.g., procedurally generated terrain with no repeating landmarks), place clustering becomes ineffective and memory retrieval fails.

### Mechanism 2
- Claim: MrSteve's exploration strategy with Count-Based goal selection and VPT-Nav reduces redundant revisits and improves coverage in large maps.
- Mechanism: High-level Count-Based policy selects the least-visited grid cell as the next goal; low-level VPT-Nav uses fine-tuned navigation to reach it. This combination balances exploration breadth and local precision.
- Core assumption: Map visitation count is a good proxy for unexplored regions; VPT-Nav generalizes across terrain types without catastrophic forgetting.
- Evidence anchors:
  - [section 4.1] "Count-Based goal selector that directs the agent to the least-visited locations as goals and the VPT-Nav that effectively reaches those goals resulted in strong exploratory behavior."
  - [section L.3] "VPT-Nav with KL coefficient 10^-4 showed high performances in all tasks" (robust across terrains).
  - [corpus] Missing empirical comparison of Count-Based vs frontier-based exploration in Minecraft—weak evidence here.
- Break condition: If the map has deceptive sparsity (e.g., large unreachable zones), the count-based selector may waste steps exploring unreachable areas.

### Mechanism 3
- Claim: MrSteve's memory-augmented task solving framework alternates between exploration and execution modes based on task-relevant resource recall, drastically reducing task completion time in sequential tasks.
- Mechanism: Mode selector queries PEM for task-relevant frames; if found, agent navigates directly and executes instruction; otherwise, explores. This avoids repeated random search.
- Core assumption: PEM retrieval latency is low enough that switching modes does not dominate total task time; Steve-1 instruction execution is sufficiently reliable once positioned.
- Evidence anchors:
  - [abstract] "MrSteve includes an Exploration Strategy and Memory-Augmented Task Solving Framework that alternates between exploration and goal-directed execution based on recalled events."
  - [section 4.2] "MrSteve solves task A' much faster. The full results on all 20 tasks are in Appendix H."
  - [corpus] No direct evidence on mode-switch latency or instruction execution reliability—weak evidence here.
- Break condition: If task-relevant resources are frequently in dynamically changing locations (e.g., moving entities), cached PEM frames become stale and navigation to them fails.

## Foundational Learning

- Concept: Hierarchical memory organization (place + event clustering)
  - Why needed here: Enables retrieval of relevant past experiences without quadratic scaling of full memory search.
  - Quick check question: If a place cluster contains 100 frames, how many event clusters might DP-Means create, and what determines their number?

- Concept: Goal-conditioned navigation with LoRA adaptation
  - Why needed here: Provides efficient, terrain-aware low-level navigation without full fine-tuning, preserving VPT's human-like priors.
  - Quick check question: What is the role of the KL loss term in PPO fine-tuning of VPT-Nav, and what happens if it is set to zero?

- Concept: Count-based exploration and visitation maps
  - Why needed here: Directs exploration toward unvisited regions without requiring semantic understanding of the map.
  - Quick check question: How does discretizing the visitation map into grid cells affect the granularity of exploration?

## Architecture Onboarding

- Component map:
  - Memory Module: Place Event Memory (PEM) with place clusters, event clusters, dummy deques, and DP-Means clustering.
  - Solver Module: Mode Selector, Count-Based high-level goal selector, VPT-Nav low-level navigator, Steve-1 instruction executor.
  - Interaction: Mode Selector queries PEM → selects mode → invokes exploration or navigation → execute Steve-1 if applicable.

- Critical path:
  1. Task arrives → Mode Selector queries PEM → if task-relevant frame found → set goal → navigate via VPT-Nav → execute Steve-1 → task complete.
  2. If no frame found → Count-Based selects next goal → VPT-Nav navigates → repeat until resource found → store in PEM → execute.

- Design tradeoffs:
  - Memory vs. query speed: Larger PEM improves recall but slows top-k selection; smaller PEM is faster but may lose relevant frames.
  - Granularity vs. coverage: Smaller grid cells in Count-Based exploration increase precision but may over-partition the map.
  - Fine-tuning scope vs. adaptation speed: Full VPT fine-tuning gives best navigation but risks forgetting priors; LoRA preserves them but may limit expressivity.

- Failure signatures:
  - Mode Selector always picks EXPLORE → PEM retrieval failing (bad alignment threshold or empty memory).
  - VPT-Nav repeatedly collides with terrain → LoRA adaptation insufficient for new terrain types.
  - Agent revisits same cells → Count-Based selector misconfigured (grid size too large or visitation map not updated).

- First 3 experiments:
  1. Run MrSteve on ABA-Sparse task with memory capacity set to 0 → observe Steve-1-like performance → confirms memory necessity.
  2. Replace Count-Based selector with random goal selection → measure increase in revisit count → confirms Count-Based benefit.
  3. Disable LoRA in VPT-Nav and use full fine-tuning → compare navigation success in Mountain/River terrain → tests LoRA sufficiency.

## Open Questions the Paper Calls Out

- **Memory Efficiency**: How can Place Event Memory (PEM) be further optimized to reduce redundancy while maintaining task-solving performance? The authors propose storing only center embeddings of event clusters, but only tested the simplest optimization method.

- **High-Level Planner Integration**: How can high-level planners in LLM-augmented agents access and utilize PEM to generate more accurate plans for complex, long-horizon tasks? Current memory systems don't allow high-level planners to access PEM in the low-level controller.

- **Noisy Positional Information**: How can MrSteve be adapted to handle environments with noisy positional information, such as robotics tasks? The paper acknowledges that MrSteve uses exact position data in Minecraft, which may limit its adaptability to robotics where positional information is often noisy.

## Limitations

- **Memory Capacity Trade-off**: The analysis lacks a clear inflection point where performance degrades significantly with reduced memory capacity, though strong performance is shown even at 10-25% capacity.

- **Novel Environment Generalization**: Experiments focus on procedurally generated maps within known biome distributions; hierarchical clustering may not generalize to environments with fundamentally different spatial structures.

- **Steve-1 Dependency**: MrSteve assumes Steve-1 can reliably execute instructions once positioned at task-relevant locations, but this dependency is not rigorously validated.

## Confidence

**High Confidence**: The hierarchical memory organization (place + event clustering) provides measurable benefits for episodic recall in structured environments. The Count-Based exploration strategy demonstrably reduces redundant visits compared to random exploration.

**Medium Confidence**: MrSteve's memory-augmented task solving framework shows strong performance improvements, but this relies on unstated assumptions about Steve-1's instruction execution reliability and the sufficiency of cached memory frames for navigation.

**Low Confidence**: The claim that MrSteve is "the first embodied agent to solve long-horizon sequential tasks with sparse resources" lacks rigorous comparison to all potential baselines, particularly those using semantic memory or other episodic approaches.

## Next Checks

1. **Memory Capacity Degradation Analysis**: Systematically reduce MrSteve's memory capacity from 100% to 10% in 10% increments while measuring performance on ABA-Sparse tasks. Identify the exact capacity threshold where performance begins to degrade and compare against theoretical capacity requirements based on place/event clustering density.

2. **Steve-1 Dependency Validation**: Replace Steve-1's instruction execution with a deterministic action sequence (e.g., always mine when near target block). Measure the performance difference between MrSteve with real Steve-1 and MrSteve with deterministic execution. This isolates the contribution of memory vs. instruction-following reliability.

3. **Dynamic Resource Environment Test**: Modify the evaluation to include tasks where resource locations change between episodes (e.g., regrow trees, respawn mobs). Measure MrSteve's performance degradation compared to a semantic memory baseline that updates resource locations dynamically.