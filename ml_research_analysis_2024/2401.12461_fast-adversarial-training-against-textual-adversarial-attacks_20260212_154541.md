---
ver: rpa2
title: Fast Adversarial Training against Textual Adversarial Attacks
arxiv_id: '2401.12461'
source_url: https://arxiv.org/abs/2401.12461
tags:
- adversarial
- training
- perturbation
- robustness
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Fast Adversarial Training (FAT) to improve
  the robustness of NLP models against adversarial attacks in a synonym-unaware setting.
  The key idea is to generate adversarial perturbations in the embedding space using
  single-step gradient ascent, which is more efficient than multi-step methods.
---

# Fast Adversarial Training against Textual Adversarial Attacks

## Quick Facts
- arXiv ID: 2401.12461
- Source URL: https://arxiv.org/abs/2401.12461
- Authors: Yichen Yang; Xin Liu; Kun He
- Reference count: 0
- Key outcome: FAT improves robust accuracy by over 30% compared to the best baseline under TextFooler attacks

## Executive Summary
This paper presents Fast Adversarial Training (FAT) to improve the robustness of NLP models against adversarial attacks in a synonym-unaware setting. The key idea is to generate adversarial perturbations in the embedding space using single-step gradient ascent, which is more efficient than multi-step methods. FAT also initializes perturbations along the direction of historical perturbations from previous epochs. Experiments on three benchmark datasets demonstrate that FAT significantly outperforms existing defense methods under various attacks with character-level and word-level modifications.

## Method Summary
FAT uses single-step gradient ascent in the embedding space to generate adversarial perturbations, based on the observation that single-step and multi-step methods produce similar results for NLP adversarial training. The perturbations are initialized along the direction of historical perturbations from previous epochs, leveraging the stability of perturbation directions across training. A variant method FAT-I updates perturbations less frequently to further accelerate training while maintaining robustness. The approach is evaluated on IMDB, AGNEWS, and QNLI datasets using BERT base uncased model.

## Key Results
- FAT improves robust accuracy by over 30% compared to the best baseline under TextFooler attacks
- FAT-I variant reduces training time to 3/4 while maintaining comparable robustness to FAT
- Direction similarity between single-step and multi-step perturbations exceeds 90% on both fine-tuned and PGD-AT models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-step gradient ascent produces perturbations nearly equivalent to multi-step methods for NLP adversarial training.
- Mechanism: In NLP embedding space, the gradient direction stabilizes quickly, so a single update captures most of the adversarial signal without iterative refinement.
- Core assumption: The embedding perturbations needed for effective adversarial training are dominated by the initial gradient direction rather than fine-grained curvature.
- Evidence anchors:
  - [abstract] "Based on the observation that the adversarial perturbations crafted by single-step and multi-step gradient ascent are similar, FAT uses single-step gradient ascent..."
  - [section 3.2.1] Direction similarity over 90% between single-step and multi-step perturbations on both fine-tuned and PGD-AT models.
  - [corpus] Weak evidence; no related papers explicitly validate single-step sufficiency for NLP AT.
- Break condition: If the embedding space has highly non-convex regions where gradient direction changes rapidly, multi-step methods would capture more effective perturbations.

### Mechanism 2
- Claim: Initializing perturbations along the direction of the previous epoch's perturbation accelerates convergence and improves robustness.
- Mechanism: Historical perturbation directions provide a better starting point in the embedding space, reducing the randomness in perturbation generation and focusing updates toward adversarial directions.
- Core assumption: The optimal perturbation direction for a given sample changes slowly across training epochs, so previous directions are informative for current updates.
- Evidence anchors:
  - [abstract] "Based on the observation that the perturbations generated on the identical training sample in successive epochs are similar..."
  - [section 3.2.2] 77%-97% direction similarity between perturbations across epochs on the same sample.
  - [corpus] Weak evidence; no related papers discuss perturbation initialization using historical information for NLP.
- Break condition: If the model's decision boundary shifts dramatically between epochs, historical directions could mislead perturbation generation.

### Mechanism 3
- Claim: Updating perturbations less frequently (interval updates) maintains robustness while significantly reducing training time.
- Mechanism: Since perturbations are similar across epochs, recalculating them every I epochs (instead of every epoch) trades minimal robustness for large computational savings.
- Core assumption: The benefit of fresh perturbations diminishes when updates are too frequent relative to the stability of the model's robustness landscape.
- Evidence anchors:
  - [section 3.3] FAT-I variant described, showing comparable performance to FAT with fewer updates.
  - [section 4.4] FAT-2 (I=2) matches FAT robustness while reducing training time to 3/4.
  - [corpus] Weak evidence; no related papers explore interval-based perturbation updates.
- Break condition: If the model's robustness landscape changes rapidly (e.g., early in training), infrequent updates could cause the perturbations to become stale and ineffective.

## Foundational Learning

- Concept: Projected Gradient Descent (PGD) and its role in adversarial training
  - Why needed here: Understanding why multi-step PGD is standard but inefficient, and how FAT replaces it with single-step updates.
  - Quick check question: What is the key computational bottleneck when using PGD for large-scale NLP models like BERT?

- Concept: Embedding space perturbations vs. input space perturbations
  - Why needed here: Distinguishing FAT's continuous embedding-space approach from discrete synonym substitution methods that require linguistic knowledge.
  - Quick check question: How does perturbing embeddings differ from modifying input text in terms of differentiability and computational cost?

- Concept: Momentum and historical information in optimization
  - Why needed here: Recognizing that FAT's perturbation initialization leverages historical directions similar to momentum-based optimizers.
  - Quick check question: In what way does initializing with previous perturbations resemble the momentum concept in gradient-based optimization?

## Architecture Onboarding

- Component map: Embedding extractor -> Perturbation generator -> Model trainer -> Interval controller
- Critical path:
  1. Forward pass on clean embeddings
  2. Compute loss and gradients w.r.t. embeddings
  3. Generate perturbation (single-step + historical init)
  4. Apply perturbation and compute adversarial loss
  5. Backpropagate and update model parameters
- Design tradeoffs:
  - Single-step vs. multi-step: Speed vs. potential loss of fine-grained adversarial signal
  - Historical initialization vs. random: Better starting points vs. risk of stale directions
  - Interval updates vs. per-epoch: Computational savings vs. risk of stale perturbations
- Failure signatures:
  - Clean accuracy drops significantly: Perturbations too large or initialization too aggressive
  - Robust accuracy plateaus early: Perturbations become stale (interval too large) or gradient signal weak
  - Training diverges: Learning rate too high or perturbation magnitude Ïµ too large
- First 3 experiments:
  1. Verify direction similarity: Compare single-step vs. multi-step perturbations on a small BERT model to confirm >90% similarity.
  2. Test historical initialization: Run FAT with and without historical initialization on a subset of data to measure robustness impact.
  3. Evaluate interval trade-off: Compare FAT, FAT-2, and FAT-3 on a small dataset to find the optimal interval for balancing speed and robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- The computational efficiency gains may not scale linearly to larger models or different architectures
- The proposed mechanisms lack extensive external validation from the broader literature
- The interval-based update strategy requires more exploration of optimal update intervals across different model sizes and datasets

## Confidence

- High confidence: The experimental setup and baseline comparisons are rigorous, with clear metrics and multiple attack types evaluated.
- Medium confidence: The proposed mechanisms (single-step updates, historical initialization) are theoretically sound but lack extensive external validation.
- Low confidence: The interval-based update strategy (FAT-I) shows promise but requires more exploration of optimal update intervals across different model sizes and datasets.

## Next Checks

1. **Cross-Architecture Validation:** Test FAT on RoBERTa and DistilBERT to verify if the single-step gradient ascent and historical initialization benefits generalize beyond BERT base.
2. **Perturbation Stability Analysis:** Measure how perturbation directions evolve across epochs for different model sizes to quantify the risk of stale directions in larger models.
3. **Adaptive Interval Tuning:** Implement an automatic interval selection mechanism that adjusts I based on perturbation stability metrics, rather than using fixed intervals like FAT-2 or FAT-3.