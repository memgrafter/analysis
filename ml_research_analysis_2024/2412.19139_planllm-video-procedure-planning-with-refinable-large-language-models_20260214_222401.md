---
ver: rpa2
title: 'PlanLLM: Video Procedure Planning with Refinable Large Language Models'
arxiv_id: '2412.19139'
source_url: https://arxiv.org/abs/2412.19139
tags:
- step
- planning
- action
- visual
- procedure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses video procedure planning, where the goal is
  to generate a sequence of action steps from start and goal state video frames. Existing
  methods are limited to closed-set one-hot vector predictions and fixed action step
  descriptions, which restricts generalization and can introduce noise.
---

# PlanLLM: Video Procedure Planning with Refinable Large Language Models

## Quick Facts
- arXiv ID: 2412.19139
- Source URL: https://arxiv.org/abs/2412.19139
- Authors: Dejie Yang; Zijing Zhao; Yang Liu
- Reference count: 12
- One-line primary result: PlanLLM achieves up to 3.68% improvement in success rate for video procedure planning compared to previous methods

## Executive Summary
PlanLLM addresses the video procedure planning task by generating action step sequences from start and goal state video frames. The method introduces a cross-modal joint learning framework that leverages a trainable Large Language Model (LLM) to produce free-form planning outputs, overcoming the limitations of closed-set one-hot vector predictions in existing methods. Through a Mutual Information Maximization module and LLM-Enhanced Planning, PlanLLM connects visual state information with world-level commonsense knowledge to improve both closed-set and open-vocabulary procedure planning tasks.

## Method Summary
PlanLLM is a cross-modal joint learning framework that uses a trainable LLM (Vicuna-7B) with LoRA fine-tuning to generate free-form procedure planning outputs. The method employs a two-stage progressive training scheme: first freezing the LLM to align visual embeddings to textual space using mutual information maximization loss, then fine-tuning the LLM jointly with other modules. The Mutual Information Maximization module uses a Q-Former architecture to connect visual state embeddings with textual step embeddings, while the LLM-Enhanced Planning module generates action step sequences directly from integrated embeddings.

## Key Results
- Improves success rate by up to 3.68% compared to previous methods
- Achieves superior performance on three benchmarks (COIN, CrossTask, NIV)
- Demonstrates effectiveness for both closed-set and open-vocabulary procedure planning tasks

## Why This Works (Mechanism)

### Mechanism 1
The Mutual Information Maximization (MIM) module connects world-level commonsense in step descriptions with sample-specific visual state information, enabling LLMs to use reasoning abilities for generating step sequences. The MIM module uses a Q-Former architecture that takes visual state embeddings and textual step embeddings as inputs to generate cross-modal joint step embeddings, creating a bridge between general commonsense knowledge and specific visual context.

### Mechanism 2
The LLM-Enhanced Planning module uses a trainable LLM to produce free-form procedure planning outputs, allowing the model to handle open vocabulary procedure planning and new planning tasks. By fine-tuning a generative LLM (Vicuna-7B) through LoRA, PlanLLM can directly generate free-form output tokens as action step sequences rather than being limited to closed-set one-hot vector predictions.

### Mechanism 3
The progressive alignment training scheme effectively trains the modules to produce free-form procedure planning outputs by first aligning visual and textual embeddings to the LLM's input space, then fine-tuning the LLM. The two-stage training approach first freezes the LLM to align visual embeddings to textual space using mutual information maximization loss, then fine-tunes the LLM jointly with other modules for procedure planning.

## Foundational Learning

- **Cross-modal representation learning**: PlanLLM needs to connect visual state information with textual commonsense knowledge to generate coherent action sequences. Quick check: How does the Q-Former architecture help bridge the gap between visual and textual modalities in PlanLLM?

- **Large Language Model fine-tuning and adaptation**: The method uses LoRA to fine-tune a generative LLM for specific task of video procedure planning while preserving general reasoning abilities. Quick check: What is the purpose of using LoRA for fine-tuning the LLM in PlanLLM instead of full fine-tuning?

- **Weakly supervised learning**: PlanLLM follows a weakly supervised setting that relies only on textual action step labels during training without requiring intermediate visual states. Quick check: How does the weakly supervised setting in PlanLLM differ from fully supervised approaches in video procedure planning?

## Architecture Onboarding

- **Component map**: Visual state inputs → Feature Extraction (S3D + CLIP) → Mutual Information Maximization (Q-Former) → LLM Enhanced Planning (Vicuna-7B + LoRA) → Free-form outputs or closed-set step IDs

- **Critical path**: Start and goal video frames → Visual feature extraction → Cross-modal embedding fusion → LLM generation → Action step sequence output

- **Design tradeoffs**: Using a trainable LLM instead of frozen LLM provides better generalization but increases computational complexity and training time. The progressive training scheme adds complexity but improves alignment between modalities.

- **Failure signatures**: Poor performance on open vocabulary tasks indicates LLM fine-tuning issues; degraded closed-set performance suggests knowledge fusion problems; training instability may indicate misalignment in the progressive training stages.

- **First 3 experiments**:
  1. Train the MIM module with frozen LLM to verify visual-textual alignment (first stage of progressive training)
  2. Test the LLM's ability to generate coherent action sequences from integrated embeddings (validate free-form output capability)
  3. Evaluate the full pipeline on a small subset of COIN dataset to check both closed-set and open-vocabulary performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important questions emerge from the work:

1. How does performance scale with increasing video sequence length beyond tested horizons (t=3, t=4)?
2. To what extent does the quality of LLM-generated enriched step descriptions affect PlanLLM's performance?
3. How does PlanLLM's performance compare when using different visual backbones or encoders?

## Limitations

- Limited evaluation of longer action sequences beyond t=3 and t=4 horizons
- Does not provide detailed analysis of how LLM-generated description quality affects downstream performance
- Relies on specific visual backbone (S3D) without exploring alternative architectures

## Confidence

- **High confidence**: The general approach of using a trainable LLM with LoRA fine-tuning for free-form procedure planning is well-established in literature
- **Medium confidence**: The two-stage progressive training scheme is theoretically sound but lacks detailed ablations
- **Low confidence**: Specific claims about cross-modal alignment quality and MIM module effectiveness are supported by limited quantitative evidence

## Next Checks

1. Evaluate the quality of cross-modal embeddings produced by the Q-Former using visual-textual similarity metrics to verify alignment between visual states and textual commonsense knowledge

2. Conduct ablation studies comparing the two-stage progressive training approach against alternative schemes to quantify the contribution of each training stage

3. Design a controlled experiment with unseen action steps in the test set to evaluate PlanLLM's actual generalization capability beyond reported improvements on standard benchmarks