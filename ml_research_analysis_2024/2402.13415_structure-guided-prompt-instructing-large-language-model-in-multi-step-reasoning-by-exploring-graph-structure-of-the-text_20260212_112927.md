---
ver: rpa2
title: 'Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning
  by Exploring Graph Structure of the Text'
arxiv_id: '2402.13415'
source_url: https://arxiv.org/abs/2402.13415
tags:
- graph
- reasoning
- prompt
- structure
- guided
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Structure Guided Prompt, a three-stage task-agnostic
  prompting framework designed to enhance the multi-step reasoning capabilities of
  large language models (LLMs) in a zero-shot setting. The framework explicitly converts
  unstructured text into a graph via LLMs and instructs them to navigate this graph
  using task-specific strategies to formulate responses.
---

# Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text

## Quick Facts
- arXiv ID: 2402.13415
- Source URL: https://arxiv.org/abs/2402.13415
- Authors: Kewei Cheng; Nesreen K. Ahmed; Theodore Willke; Yizhou Sun
- Reference count: 40
- Primary result: Up to 37% improvement on relation prediction (CLUTRR) and 146% on entity prediction (tracking shuffled objects) using zero-shot prompting

## Executive Summary
This paper introduces Structure Guided Prompt, a three-stage task-agnostic prompting framework designed to enhance multi-step reasoning capabilities of large language models (LLMs) in a zero-shot setting. The framework explicitly converts unstructured text into a graph via LLMs and instructs them to navigate this graph using task-specific strategies to formulate responses. Experiments demonstrate significant performance improvements across diverse natural language reasoning tasks, including relation prediction, entity prediction over dynamic knowledge graphs, complex entity prediction, graph sorting, graph query, and logical inference.

## Method Summary
The framework operates through three sequential stages: first, it converts unstructured text into a structured knowledge graph representation; second, it develops task-specific planning strategies based on the graph structure; and third, it executes the navigation plan to reach conclusions. This systematic approach mirrors human problem-solving by visualizing complex information as graphs before reasoning. The method is inherently task-agnostic but requires task-specific planning templates tailored to each unique reasoning category. The framework was evaluated across six diverse reasoning tasks using datasets including CLUTRR, BIG-bench-hard, HotpotQA, and Entailment Bank, demonstrating substantial accuracy improvements over zero-shot chain-of-thought prompting approaches.

## Key Results
- 37% improvement in relation prediction accuracy on CLUTRR dataset
- 146% improvement in entity prediction accuracy on tracking shuffled objects task
- Maintained or improved accuracy as task complexity increased across all evaluated tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structure Guided Prompt improves multi-step reasoning by converting unstructured text into a graph representation, enabling systematic navigation through the reasoning path.
- **Mechanism:** The framework first constructs a knowledge graph from the input text, then uses task-specific planning strategies to navigate this graph, and finally executes the plan to reach the answer. This mirrors human problem-solving approaches where complex information is visualized as graphs before reasoning.
- **Core assumption:** Natural language text inherently contains sufficient information for answering questions when properly structured, and LLMs can effectively navigate these structured representations when given appropriate guidance.
- **Evidence anchors:**
  - [abstract] "This framework explicitly converts unstructured text into a graph via LLMs and instructs them to navigate this graph using task-specific strategies to formulate responses."
  - [section] "The proposed framework is general, inherently task-agnostic, and capable of eliciting multi-step reasoning across broader natural language scenarios with a unified template."
- **Break condition:** The approach fails when the constructed graph lacks completeness due to the LLM's inability to capture all relevant information from the text, or when the task-specific planning strategies are not well-suited to the particular reasoning task.

### Mechanism 2
- **Claim:** The three-stage prompting structure (graph construction, task-specific planning, execution) provides a systematic framework that guides LLMs through complex reasoning tasks.
- **Mechanism:** The first stage converts unstructured text into structured knowledge graphs, the second stage develops task-specific navigation strategies based on the graph structure, and the third stage executes the navigation plan to reach conclusions. This systematic approach reduces cognitive load on the LLM by breaking complex reasoning into manageable steps.
- **Core assumption:** LLMs benefit from structured, step-by-step guidance that mirrors human problem-solving approaches, particularly for tasks requiring multiple inference steps.
- **Evidence anchors:**
  - [abstract] "This framework explicitly converts unstructured text into a graph via LLMs and instructs them to navigate this graph using task-specific strategies to formulate responses."
  - [section] "This approach mirrors how humans tackle graph-based problems. To facilitate recognition, each stage of the prompt is color-coded: olive for the first stage, teal for the second, and violet for the third."
- **Break condition:** The framework breaks down when the LLM fails to properly execute any of the three stages, particularly during the execution phase where it may make errors despite correct graph construction and planning.

### Mechanism 3
- **Claim:** Task-specific planning strategies enable the framework to adapt to different types of reasoning tasks by tailoring the graph navigation approach to the specific requirements of each task.
- **Mechanism:** Different reasoning tasks (relation prediction, entity prediction, graph sorting, graph query, logical inference) require different navigation strategies. The framework provides specific planning approaches for each task type, allowing the LLM to focus on the most relevant aspects of the graph for each particular problem.
- **Core assumption:** Different reasoning tasks have distinct structural requirements that can be addressed through specialized navigation strategies, and LLMs can effectively apply these strategies when properly prompted.
- **Evidence anchors:**
  - [section] "Our framework is inherently task-agnostic, designed to accommodate a wide range of tasks with versatility. To cater to this diversity, we establish task-specific planning in 2nd stage prompt, tailored to each unique task."
  - [section] "We have categorized reasoning tasks into various categories as shown in Fig. 1. Each category is aligned with a unique graph structure."
- **Break condition:** The approach fails when the task-specific planning strategies are not sufficiently tailored to the particular task, or when the LLM cannot effectively distinguish between different task types and apply the appropriate strategy.

## Foundational Learning

- **Concept:** Knowledge Graph (KG) representation and reasoning
  - **Why needed here:** The framework relies on converting unstructured text into graph structures where nodes represent entities and edges represent relationships. Understanding how KGs work is fundamental to grasping how the framework structures information for reasoning.
  - **Quick check question:** How would you represent the sentence "John is the father of Mary and lives in New York" as a knowledge graph?

- **Concept:** Chain-of-thought (CoT) and zero-shot reasoning
  - **Why needed here:** The paper compares its approach against zero-shot chain-of-thought prompting, so understanding how CoT works and its limitations is important for understanding the motivation behind this new framework.
  - **Quick check question:** What is the key difference between zero-shot chain-of-thought prompting and the approach proposed in this paper?

- **Concept:** Multi-hop reasoning and inference
  - **Why needed here:** The framework is designed to improve multi-step reasoning capabilities, so understanding what multi-hop reasoning entails and why it's challenging for LLMs is crucial for appreciating the framework's contributions.
  - **Quick check question:** Why is multi-hop reasoning more challenging than single-step reasoning for LLMs?

## Architecture Onboarding

- **Component map:** Input text processor → Knowledge graph constructor → Task classifier → Task-specific planner → Graph navigator → Answer generator
- **Critical path:**
  1. Receive input text and question
  2. Construct knowledge graph from text
  3. Identify task type and select appropriate planning strategy
  4. Execute graph navigation according to the plan
  5. Extract and format the answer
- **Design tradeoffs:**
  - Graph completeness vs. computational efficiency: More detailed graphs provide better reasoning support but increase processing time
  - Task-specific complexity vs. framework generality: More specialized strategies improve performance but reduce framework flexibility
  - Prompt complexity vs. LLM comprehension: More detailed prompts provide better guidance but may overwhelm the LLM
- **Failure signatures:**
  - Incomplete graph construction leading to missing reasoning paths
  - Incorrect task classification resulting in inappropriate planning strategies
  - Navigation errors during execution phase despite correct planning
  - Answer extraction failures when the reasoning path doesn't clearly lead to a conclusion
- **First 3 experiments:**
  1. Test basic graph construction on simple sentences to verify the knowledge graph constructor works correctly
  2. Validate task classification by running sample questions through the classifier and checking if the correct task type is identified
  3. Test end-to-end reasoning on a simple relation prediction task to verify the complete pipeline works before scaling to more complex tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can LLMs spontaneously represent natural language text as a knowledge graph for multi-step reasoning without explicit prompting?
- **Basis in paper:** [explicit] The paper explicitly questions whether LLMs can spontaneously represent text as a KG for multi-step reasoning, noting that while LLMs are trained for next-token prediction, approaches like "Zero-Shot-CoT" have shown promise in encouraging structured thinking.
- **Why unresolved:** The paper suggests LLMs may not naturally structure text as KGs but can be prompted to do so. However, it remains unclear whether LLMs have an inherent ability to spontaneously create KGs during reasoning tasks without specific prompting strategies.
- **What evidence would resolve it:** Experiments comparing LLM performance on multi-step reasoning tasks with and without explicit prompting to create KGs, measuring accuracy and efficiency differences.

### Open Question 2
- **Question:** Are there more effective methods for representing relationships among information in natural language that go beyond traditional knowledge graph structures?
- **Basis in paper:** [inferred] The paper discusses the limitations of KGs in handling the richness of natural language, particularly in conveying emotions and sentiments, and suggests the need for enhanced mechanisms for detecting textual entailment, contradiction, and inference.
- **Why unresolved:** While KGs are effective for structuring factual information, the paper highlights their limitations in capturing the full expressiveness of natural language, indicating a need for more advanced representation methods.
- **What evidence would resolve it:** Development and testing of novel representation methods that integrate advanced NLP techniques (e.g., named entity recognition, dependency parsing) to capture complex relationships and nuances in textual data.

### Open Question 3
- **Question:** How can the conclusion-drawing phase of LLMs be improved to reduce errors, even when the inference process is accurate?
- **Basis in paper:** [explicit] The paper identifies a pattern where LLMs correctly follow prompts to construct accurate KGs and navigate them correctly but make mistakes during the conclusion-drawing phase, suggesting the potential use of an additional LLM to verify consistency.
- **Why unresolved:** The paper notes the issue but does not provide a definitive solution, indicating a need for further research to address this specific challenge in LLM reasoning.
- **What evidence would resolve it:** Experiments testing different strategies, such as using additional verification steps or ensemble methods, to improve the accuracy of conclusions drawn by LLMs after correct inference processes.

## Limitations
- The framework requires manual development of task-specific planning strategies, limiting scalability across diverse reasoning tasks
- Heavy reliance on LLM's ability to accurately construct knowledge graphs from unstructured text, which may degrade with complex or ambiguous inputs
- Evaluation focuses on specific task categories, with unclear generalization to truly novel or cross-domain reasoning scenarios

## Confidence

**High confidence (Mechanistic plausibility)**: The three-stage prompting approach (graph construction → task-specific planning → execution) represents a logically sound method for decomposing complex reasoning tasks. The framework's core architecture follows established principles of breaking down complex problems into manageable components.

**Medium confidence (Empirical results)**: The reported performance improvements are substantial, but the evaluation relies primarily on accuracy metrics without extensive error analysis or ablation studies. The comparison against zero-shot chain-of-thought prompting is appropriate, but additional baselines would strengthen the claims.

**Low confidence (Generalization claims)**: The assertion that the framework is "inherently task-agnostic" and can "accommodate a wide range of tasks with versatility" lacks sufficient empirical support. The evaluation focuses on specific task categories, and the framework's performance on truly novel or cross-domain reasoning tasks remains unverified.

## Next Checks
1. **Graph construction validation**: Implement automated verification of knowledge graph quality by comparing constructed graphs against ground truth representations for a subset of tasks. Measure precision and recall of graph elements to quantify the reliability of this foundational component.
2. **Ablation study on task-specific components**: Systematically remove or simplify task-specific planning strategies to determine their individual contributions to performance improvements. This would clarify whether the framework's benefits come primarily from graph structure or from the tailored planning approaches.
3. **Cross-domain generalization test**: Apply the framework to reasoning tasks outside the six evaluated categories, particularly tasks involving different knowledge representations (e.g., temporal reasoning, spatial reasoning, or multimodal inputs) to assess the claimed task-agnostic capabilities.