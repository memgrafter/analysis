---
ver: rpa2
title: 'Graph Memory Learning: Imitating Lifelong Remembering and Forgetting of Brain
  Networks'
arxiv_id: '2407.19183'
source_url: https://arxiv.org/abs/2407.19183
tags:
- graph
- learning
- memory
- information
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Graph Memory Learning (GML), a novel framework
  enabling graph models to continuously adapt to dynamic data by selectively remembering
  new information and forgetting outdated knowledge. Drawing inspiration from brain
  network dynamics, the authors propose BGML (Brain-inspired Graph Memory Learning),
  which incorporates a multi-granular hierarchical progressive learning mechanism
  (MGHPL) based on feature graph grain learning to clarify changes in local graph
  structures.
---

# Graph Memory Learning: Imitating Lifelong Remembering and Forgetting of Brain Networks

## Quick Facts
- arXiv ID: 2407.19183
- Source URL: https://arxiv.org/abs/2407.19183
- Reference count: 40
- One-line primary result: BGML achieves up to 96.72% accuracy on Coauthor-CS across five task types

## Executive Summary
This paper introduces Graph Memory Learning (GML), a novel framework enabling graph models to continuously adapt to dynamic data by selectively remembering new information and forgetting outdated knowledge. Drawing inspiration from brain network dynamics, the authors propose BGML (Brain-inspired Graph Memory Learning), which incorporates a multi-granular hierarchical progressive learning mechanism (MGHPL) based on feature graph grain learning to clarify changes in local graph structures. To address unreliable new knowledge structures, an information self-assessment ownership mechanism (ISAO) is introduced. The framework includes two key modules: memory forgetting and memory remembering, inspired by synaptic remodeling and dynamic integration mechanisms in the brain.

## Method Summary
BGML builds upon GNN foundations to process graph data and learn representations. The framework partitions initial graphs into coarse and fine-grained shards using BLPA and BEKM algorithms, then progressively learns graph knowledge at multiple granularities through MGHPL. When new nodes enter, ISAO calculates distances to shard centroids and selects the nearest shard and most similar neighbors for integration. The memory forgetting module implements localized pinpoint clearing by selectively retraining sub-models associated with forgotten data. The framework is evaluated across nine datasets and five task types using Micro F1-Score as the primary metric.

## Key Results
- BGML consistently outperforms baseline methods across all tested datasets and task types
- Achieves up to 96.72% accuracy on Coauthor-CS dataset
- Demonstrates superior performance in handling dynamic graph evolution with incremental and forgetting requests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MGHPL alleviates conflict between remembering and forgetting by progressively learning graph knowledge at multiple granularities
- Mechanism: Graph is partitioned into coarse and fine-grained shards, initial sub-models trained on first-level grains, progress-aware module identifies sub-models needing deeper learning supported by second-level grains
- Core assumption: Graph knowledge can be effectively decoupled and learned at multiple granularities without losing important information
- Evidence anchors: [abstract], [section] on MGHPL
- Break condition: If BLPA/BEKM algorithms fail to create meaningful shards or progress-aware module cannot accurately identify sub-models needing support

### Mechanism 2
- Claim: ISAO establishes credible graph structures for new knowledge, allowing incremental information to choose most suitable graph grain and neighbors
- Mechanism: New nodes calculate distance to all centroids, select nearest first-level shard, use L2 Euclidean distance to identify top-Âµ most similar nodes as neighbors
- Core assumption: New nodes can be meaningfully integrated based on feature similarity to shard centroids and other nodes
- Evidence anchors: [abstract], [section] on ISAO
- Break condition: If distance metrics fail to capture meaningful similarity or number of selected neighbors is inappropriate

### Mechanism 3
- Claim: Memory forgetting module implements efficient and localized pinpoint clearing by selectively locking sub-models associated with forgotten data
- Mechanism: FR requests remove forgotten nodes from graph shards, forget-guide module identifies related sub-models, re-FGGL retrains only pertinent sub-models
- Core assumption: Relationship between forgotten data and sub-models can be accurately identified, and selective retraining is sufficient to clear associated memory
- Evidence anchors: [abstract], [section] on memory forgetting
- Break condition: If forget-guide module cannot accurately identify all related sub-models or selective retraining is insufficient

## Foundational Learning

- **Graph Neural Networks (GNNs)**
  - Why needed here: BGML builds upon GNN foundations to process graph data and learn representations
  - Quick check question: What is the core idea behind graph convolutional networks (GCNs), and how do they differ from traditional convolutional neural networks?

- **Lifelong Learning**
  - Why needed here: BGML is a form of lifelong learning for graphs, aiming to continuously adapt to new knowledge while mitigating forgetting of past experiences
  - Quick check question: What are the main challenges in lifelong learning, and how do different approaches (architectural, regularization, rehearsal) address them?

- **Machine Unlearning**
  - Why needed here: BGML incorporates machine unlearning to selectively forget outdated knowledge when requested
  - Quick check question: What is the difference between exact unlearning and approximate unlearning, and what are the trade-offs between them?

## Architecture Onboarding

- **Component map**: Graph -> MGP (BLPA/BEKM partitioning) -> MGHPL (progressive learning) -> ISAO (new node integration) -> Memory forgetting/remembering modules
- **Critical path**: Partition initial graph using MGP, progressively learn using MGHPL, handle forgetting requests with memory forgetting module, incorporate new information using memory remembering module and ISAO
- **Design tradeoffs**: Balances granularity of graph partitioning (affecting learning detail) against computational efficiency; balances selective forgetting against preserving useful knowledge
- **Failure signatures**: Degraded performance on graph memory learning tasks, inability to effectively forget specified data, catastrophic forgetting of past experiences
- **First 3 experiments**:
  1. Test MGP on a simple graph to verify it can create meaningful coarse and fine-grained shards
  2. Test MGHPL with a single sub-model to ensure it can learn from graph grains and make predictions
  3. Test ISAO with a new node and simple graph shard to verify it can correctly identify node's belonging and relevant neighbors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the BGML framework be effectively extended to handle edge-level forgetting and incremental updates in dynamic graphs?
- Basis in paper: The paper focuses on node-level GML tasks and explicitly states "The scenario where only edges change during graph evolution has not been discussed in this work."
- Why unresolved: Authors acknowledge this as a limitation but do not explore edge-level GML strategies
- What evidence would resolve it: Experiments demonstrating BGML's performance on edge-level forgetting and incremental learning tasks

### Open Question 2
- Question: Is it possible to design a mechanism that allows experiences of individual sub-models to be stored and leveraged for handling similar graph evolution patterns?
- Basis in paper: Authors mention potential future work regarding interareal communication subspaces in the brain and establishing connections between BGML's sub-models
- Why unresolved: Current framework does not incorporate inter-sub-model communication or memory sharing
- What evidence would resolve it: Comparative experiments showing whether inter-sub-model communication improves accuracy, efficiency, or ability to handle complex graph evolution patterns

### Open Question 3
- Question: Can MGHPL mechanism be modified to dynamically adjust granularity of graph partitioning based on complexity of graph structure or specific task requirements?
- Basis in paper: Paper states "The number of shards is correlated with the dataset size, and adjusting it appropriately can enhance the overall model performance"
- Why unresolved: Current implementation uses fixed numbers of shards for each level of granularity
- What evidence would resolve it: Experiments demonstrating improved performance or efficiency using adaptive granularity partitioning compared to fixed partitioning

## Limitations

- Effectiveness heavily depends on quality of BLPA and BEKM graph partitioning algorithms, which are only referenced but not fully specified
- Reliance on L2 Euclidean distance for neighbor selection may not capture complex graph relationships in high-dimensional feature spaces
- Performance across different graph types and sizes is not thoroughly explored, with experiments primarily focused on citation networks and small-scale graphs

## Confidence

- **High Confidence**: Experimental results demonstrating BGML's superiority over baselines on tested datasets and tasks
- **Medium Confidence**: Theoretical framework and mechanisms (MGHPL, ISAO) are well-motivated but rely on external algorithms with unspecified implementation details
- **Low Confidence**: Framework's generalizability to diverse graph types (e.g., social networks, biological networks) and scalability to large-scale graphs with millions of nodes

## Next Checks

1. **Implement and validate BLPA/BEKM**: Implement the BLPA and BEKM partitioning algorithms and verify their effectiveness in creating meaningful graph shards for Cora, CiteSeer, and PubMed datasets. Measure shard quality using modularity and conductance metrics.

2. **Ablation study on ISAO**: Conduct an ablation study to quantify the impact of the ISAO mechanism on BGML's performance. Compare BGML with and without ISAO on Coauthor-CS and Ogbn-arXiv datasets, measuring Micro F1-Score and node classification accuracy.

3. **Stress test memory forgetting**: Design a stress test to evaluate BGML's memory forgetting module under extreme conditions. Gradually increase the number of forgetting requests (FR) on Reddit and Flickr datasets, measuring impact on overall performance and class forgetting rate.