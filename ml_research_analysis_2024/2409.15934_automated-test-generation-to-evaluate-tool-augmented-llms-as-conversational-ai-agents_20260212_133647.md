---
ver: rpa2
title: Automated test generation to evaluate tool-augmented LLMs as conversational
  AI agents
arxiv_id: '2409.15934'
source_url: https://arxiv.org/abs/2409.15934
tags:
- order
- customer
- message
- procedure
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an automated pipeline for generating evaluation
  datasets to assess tool-augmented LLMs as conversational AI agents. The method uses
  intermediate graph structures to ground generated conversations in user-defined
  procedures while ensuring coverage of diverse interaction paths.
---

# Automated test generation to evaluate tool-augmented LLMs as conversational AI agents

## Quick Facts
- arXiv ID: 2409.15934
- Source URL: https://arxiv.org/abs/2409.15934
- Authors: Samuel Arcadinho; David Aparicio; Mariana Almeida
- Reference count: 15
- Primary result: Automated pipeline generates high-quality test sets without human curation, with results closely matching curated data

## Executive Summary
This work presents an automated pipeline for generating evaluation datasets to assess tool-augmented LLMs as conversational AI agents. The method uses intermediate graph structures to ground generated conversations in user-defined procedures while ensuring coverage of diverse interaction paths. A manually curated dataset (ALMITA) was created with 1,420 tests spanning 192 conversations. Evaluation of multiple LLMs showed high performance on single-message accuracy and function calling (85-99%), but low overall conversation accuracy (4-15%), indicating limitations in handling complete conversational workflows. The automated pipeline can generate high-quality test sets without human curation, with results closely matching those from curated data.

## Method Summary
The automated test generation pipeline uses LLMs to create intents, procedures, APIs, flowgraphs, conversation graphs, conversations, and tests in a multi-stage process. The pipeline relies on intermediate graph structures (flowgraphs and conversation graphs) to ground conversations in procedures and reduce hallucinations. It includes noise generation to create realistic conversations and path sampling to ensure diverse test coverage. The pipeline can be run without human filtering, producing auto-ALMITA, which retains more data points and greater diversity compared to the manually curated ALMITA dataset.

## Key Results
- LLMs demonstrated high performance on single-message accuracy and function calling (85-99%)
- Overall conversation accuracy remained low (4-15%) across all evaluated models
- Automated pipeline produced test sets without human curation that closely matched curated data results
- High API accuracy but low reply accuracy suggests datasets focused solely on API calls don't comprehensively evaluate AI agent capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intermediate graph structures improve test generation quality and coverage by grounding LLM outputs to user-defined procedures.
- Mechanism: The pipeline converts procedures into flowgraphs and conversation graphs, which serve as structured representations that limit hallucination and enforce adherence to the procedure. This structured approach ensures that generated conversations follow the intended logic and cover all relevant paths.
- Core assumption: LLMs generate more accurate and complete content when provided with structured intermediate representations rather than directly generating from procedures.
- Evidence anchors:
  - [abstract]: "Our framework uses LLMs to generate diverse tests grounded on user-defined procedures. For that, we use intermediate graphs to limit the LLM test generator's tendency to hallucinate content that is not grounded on input procedures, and enforces high coverage of the possible conversations."
  - [section]: "Our assumption is that using these intermediate structured representations makes the task of creating the conversations grounded on the procedures more accurate; see Section 4.2 for supporting evidence."
  - [corpus]: Weak evidence - the corpus contains related work on using intermediate structures to reduce hallucinations, but none directly address the specific claim about improving test generation quality and coverage in the context of tool-augmented LLM evaluation.
- Break condition: If the intermediate graphs become too complex or if the LLM fails to accurately convert procedures into graphs, the quality and coverage improvements may not materialize.

### Mechanism 2
- Claim: Automated test generation pipeline can produce high-quality test sets without human curation, closely matching curated data results.
- Mechanism: The pipeline uses LLMs to generate intents, procedures, APIs, flowgraphs, conversation graphs, conversations, and tests in an automated fashion. It also includes noise generation and path sampling to create diverse and realistic conversations. The pipeline can be run without human filtering, producing auto-ALMITA.
- Core assumption: LLMs are capable of generating diverse, realistic, and procedure-adherent content across all stages of the pipeline without human intervention.
- Evidence anchors:
  - [abstract]: "The automated pipeline can generate high-quality test sets without human curation, with results closely matching those from curated data."
  - [section]: "Auto-ALMITA retains more data points and greater diversity (see Table 1), albeit with some reduction in quality. Being fully automatically generated, auto-ALMITA can also be easily extended without additional curation efforts."
  - [corpus]: Weak evidence - the corpus contains related work on automated dataset generation, but none directly address the specific claim about producing high-quality test sets without human curation in the context of tool-augmented LLM evaluation.
- Break condition: If LLMs generate low-quality content at any stage of the pipeline or fail to adhere to procedures, the automated test generation may not produce high-quality results.

### Mechanism 3
- Claim: Evaluating tool-augmented LLMs as conversational AI agents requires assessing multiple dimensions beyond single-message accuracy and function calling.
- Mechanism: The evaluation framework considers dimensions such as reply recall, correct reply, API recall, correct API, correct API parameters, test correctness, and conversation correctness. This multifaceted approach captures the complexities of real-world conversational AI agent performance.
- Core assumption: Tool-augmented LLMs face challenges in maintaining correct conversations throughout a user interaction, which cannot be fully captured by evaluating only single-message accuracy and function calling.
- Evidence anchors:
  - [abstract]: "Evaluation of multiple LLMs showed high performance on single-message accuracy and function calling (85-99%), but low overall conversation accuracy (4-15%), indicating limitations in handling complete conversational workflows."
  - [section]: "We observe that all LLMs demonstrate high accuracy when responding with an API, achieving over 85% correctness in both the correct API and correct API parameters dimensions. With the exception of Llama3.1-8b-I, which performs considerably worse, the other models correctly determine when an API should be called, with an API recall exceeding 90%. However, performance in other dimensions is notably lower, suggesting that datasets focused solely on API calls do not comprehensively evaluate an AI agent's capabilities."
  - [corpus]: Weak evidence - the corpus contains related work on evaluating conversational AI systems, but none directly address the specific claim about the need for multifaceted evaluation in the context of tool-augmented LLM evaluation.
- Break condition: If the evaluation framework fails to capture critical aspects of tool-augmented LLM performance or if the chosen metrics do not align with real-world requirements, the assessment may not accurately reflect the limitations of these models.

## Foundational Learning

- Concept: Understanding the role and capabilities of tool-augmented LLMs in customer support.
  - Why needed here: The paper focuses on evaluating tool-augmented LLMs as conversational AI agents in customer support. A solid understanding of these models' capabilities and limitations is crucial for interpreting the results and implications of the study.
  - Quick check question: What are the key challenges in deploying tool-augmented LLMs as autonomous customer support agents, and how does the paper's evaluation framework address these challenges?

- Concept: Familiarity with graph data structures and their applications in representing conversational flows.
  - Why needed here: The pipeline relies on converting procedures into flowgraphs and conversation graphs to improve test generation quality and coverage. Understanding graph data structures and their use in representing conversational flows is essential for grasping the pipeline's inner workings.
  - Quick check question: How do flowgraphs and conversation graphs differ in their structure and purpose within the pipeline, and what benefits do they provide over directly generating conversations from procedures?

- Concept: Knowledge of automated dataset generation techniques and their potential applications.
  - Why needed here: The paper presents an automated test generation pipeline that can produce high-quality test sets without human curation. Understanding automated dataset generation techniques and their potential applications is crucial for appreciating the significance and implications of this work.
  - Quick check question: What are the key components of the automated test generation pipeline, and how do they work together to produce diverse and realistic conversations that adhere to user-defined procedures?

## Architecture Onboarding

- Component map:
  Intent Generator → Procedure Generator → API Extractor → Flowgraph Generator → Conversation Graph Generator → Path Sampler → Conversation Generator → Test Extractor

- Critical path: Intent Generator → Procedure Generator → API Extractor → Flowgraph Generator → Conversation Graph Generator → Path Sampler → Conversation Generator → Test Extractor

- Design tradeoffs:
  - Using intermediate graph structures (flowgraphs and conversation graphs) improves quality and coverage but adds complexity to the pipeline.
  - Automated generation allows for diverse and scalable test sets but may introduce noise or inconsistencies that require manual filtering.
  - Evaluating multiple dimensions provides a comprehensive assessment but increases the complexity of the evaluation framework.

- Failure signatures:
  - Low conversation accuracy indicates that the pipeline may not be generating realistic or diverse enough conversations.
  - Inconsistent test results between ALMITA and auto-ALMITA suggest that automated generation may introduce noise or errors that affect evaluation quality.
  - High API accuracy but low reply accuracy indicates that the pipeline may be overemphasizing API calls at the expense of natural conversation flows.

- First 3 experiments:
  1. Run the pipeline with a small set of manually defined intents and procedures to verify that the generated tests accurately reflect the intended conversational flows.
  2. Compare the results of ALMITA and auto-ALMITA to assess the impact of manual filtering on test quality and evaluation outcomes.
  3. Evaluate the pipeline's performance on a diverse set of intents and procedures to ensure that it can handle a wide range of conversational scenarios and generate realistic tests.

## Open Questions the Paper Calls Out
None

## Limitations
- The reliance on intermediate graph structures adds complexity to the pipeline and may not generalize well to all conversational scenarios
- The evaluation framework may not fully capture the nuances of real-world tool-augmented LLM performance in dynamic customer support environments
- High API accuracy but low reply accuracy suggests the pipeline may overemphasize API calls at the expense of natural conversation flows

## Confidence

- **High Confidence**: The automated pipeline's ability to generate high-quality test sets without human curation, as evidenced by the close match between ALMITA and auto-ALMITA results.
- **Medium Confidence**: The effectiveness of intermediate graph structures in improving test generation quality and coverage, supported by the paper's findings but requiring further validation in diverse contexts.
- **Low Confidence**: The evaluation framework's ability to comprehensively assess tool-augmented LLM performance, given the complexity of real-world conversational AI agent interactions.

## Next Checks
1. **Generalization Across Domains**: Test the automated pipeline's performance on a wider range of conversational domains beyond customer support to assess its generalizability and robustness.

2. **Human-in-the-Loop Evaluation**: Conduct user studies to compare the quality and realism of conversations generated by the automated pipeline against those created by human experts, focusing on user satisfaction and task completion rates.

3. **Dynamic Scenario Testing**: Develop and evaluate the pipeline's performance on dynamic, multi-turn conversations with unexpected user behavior and changing requirements, simulating real-world customer support scenarios.