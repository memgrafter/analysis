---
ver: rpa2
title: Distribution-Consistency-Guided Multi-modal Hashing
arxiv_id: '2412.11216'
source_url: https://arxiv.org/abs/2412.11216
tags:
- noisy
- labels
- hashing
- label
- hash
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a distribution-consistency-guided multi-modal
  hashing method to address the issue of noisy labels in supervised multi-modal hashing.
  The key idea is to discover and exploit the consistency between the 1-0 distribution
  of labels and the high-low distribution of similarity scores relative to category
  centers.
---

# Distribution-Consistency-Guided Multi-modal Hashing

## Quick Facts
- arXiv ID: 2412.11216
- Source URL: https://arxiv.org/abs/2412.11216
- Reference count: 20
- Achieves 4.9%-9.3% average improvements over state-of-the-art on MIR Flickr, NUS-WIDE, and MS COCO at 40% noisy label ratio

## Executive Summary
This paper proposes a distribution-consistency-guided multi-modal hashing method that addresses noisy labels in supervised multi-modal retrieval. The key insight is that the presence/absence of category labels in training data follows a consistent pattern with the high/low similarity scores between hash codes and category centers. By exploiting this consistency, the method filters out noisy labels and reconstructs them, correcting high-confidence noisy labels while treating low-confidence ones as unlabeled for unsupervised learning. Experiments show significant performance improvements across three benchmark datasets at various noise ratios.

## Method Summary
The method combines multi-modal feature extraction and fusion with a novel label filtering and correction mechanism based on distribution consistency patterns. It uses a multi-loss approach that balances supervised learning on clean labels, semi-supervised learning on corrected labels, and unsupervised contrastive learning on low-confidence labels. The model initializes category centers randomly and computes similarity scores between hash codes and these centers to identify inconsistent label-similarity patterns. High-confidence noisy labels are corrected using clean label instances with similar similarity distributions, while low-confidence labels are treated as unlabeled and learned through contrastive loss.

## Key Results
- Achieves 4.9%, 6.7%, and 9.3% average improvements over state-of-the-art on MIR Flickr, NUS-WIDE, and MS COCO at 40% noisy label ratio
- Demonstrates robustness across different noisy label ratios (10%-90%) with slower performance decline compared to other models
- Shows consistent superiority in Mean Average Precision, Precision@N curves, and Precision-Recall curves at 64-bit hash length

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The consistency between 1-0 label distribution and high-low similarity score distribution enables effective noisy label filtering
- Mechanism: When an instance belongs to a category, its hash code shows higher similarity to that category's center than to other centers. This creates a consistent pattern where label presence (1) aligns with high similarity scores, and label absence (0) aligns with low similarity scores. The method exploits this pattern to filter out noisy labels that break this consistency.
- Core assumption: Category centers are well-separated and hash codes maintain semantic proximity to their true category centers
- Evidence anchors: Statistical analysis showing in clean labels, similarity scores to belonging categories are significantly higher; abstract states the 1-0 distribution consistency pattern

### Mechanism 2
- Claim: The correction strategy effectively distinguishes high-confidence from low-confidence noisy labels
- Mechanism: The method uses clean labels as a knowledge base to find the two instances with most similar similarity distributions to a noisy label instance. If their labels are consistent, the instance is considered high-confidence and corrected; otherwise, it's treated as unlabeled for unsupervised learning.
- Core assumption: Instances with similar similarity distributions likely share semantic meaning and labels
- Evidence anchors: Method description of using clean label set to identify similar instances and correct high-confidence labels

### Mechanism 3
- Claim: The multi-loss approach effectively balances supervised, semi-supervised, and unsupervised learning
- Mechanism: The method combines pointwise loss for clean labels, pairwise loss for corrected labels, and unsupervised contrastive loss for unlabeled instances, along with center separation and quantization losses. This multi-pronged approach ensures robust learning even with significant label noise.
- Core assumption: Different types of label quality (clean, corrected, unlabeled) benefit from different learning objectives
- Evidence anchors: Objective function definition combining multiple loss components

## Foundational Learning

- Concept: Hash code similarity preservation
  - Why needed here: The entire method relies on hash codes maintaining semantic similarity relationships where similar instances have similar hash codes
  - Quick check question: If two instances belong to the same category, should their hash codes be more similar to each other than to instances from different categories?

- Concept: Multi-modal fusion
  - Why needed here: The method combines image and text features through feature extraction and fusion before hashing
  - Quick check question: Does the method concatenate features or use another fusion strategy like weighted sum?

- Concept: Unsupervised contrastive learning
  - Why needed here: The method uses contrastive learning on unlabeled instances to extract semantic information without relying on potentially noisy labels
- Quick check question: In contrastive learning, should augmented views of the same instance be mapped to similar hash codes?

## Architecture Onboarding

- Component map: Multi-modal hashing network → Label filter → Label reconstructor → Multi-loss training
- Critical path: Feature extraction → Fusion → Hash code generation → Similarity computation → Label filtering → Label correction → Multi-loss training
- Design tradeoffs: The method trades computational complexity for robustness by using multiple filtering and correction steps rather than simple loss-based approaches
- Failure signatures: Poor performance on clean datasets (overfitting to noise filtering), sensitivity to hyperparameter settings, failure to separate category centers
- First 3 experiments:
  1. Run with all noisy labels treated as clean (no filtering) to establish baseline performance degradation
  2. Test label filtering only (no correction) to evaluate filtering effectiveness in isolation
  3. Compare different unsupervised learning strategies for low-confidence labels (contrastive vs. reconstruction)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal filtering threshold function ϵ(τ) for different datasets and noise patterns?
- Basis in paper: The paper mentions that "ϵ(τ) denotes the filtering threshold to ensure τ n instances are identified as noisy label instances" but does not specify how to determine this threshold.
- Why unresolved: The paper only states that the threshold should ensure the correct number of noisy instances are identified, but doesn't provide a method for calculating the optimal threshold.
- What evidence would resolve it: Experimental results showing performance across different threshold calculation methods, or a theoretical derivation of the optimal threshold function.

### Open Question 2
- Question: How does the performance of DCGMH vary with different numbers of category centers?
- Basis in paper: The paper mentions randomly initializing several category centers but doesn't explore the impact of varying this number.
- Why unresolved: The paper uses randomly initialized centers but doesn't analyze how the number of centers affects performance or discuss an optimal number.
- What evidence would resolve it: Comparative experiments showing performance across different numbers of category centers, or analysis of the trade-off between center number and model complexity.

### Open Question 3
- Question: How robust is the distribution consistency pattern to different types of noise beyond random label noise?
- Basis in paper: The paper states "in real-world scenarios, labels may be incorrectly annotated due to manual labeling" but only evaluates against random noise patterns.
- Why unresolved: The experiments focus on random label noise, but real-world label noise could have systematic patterns (e.g., certain categories being more prone to confusion).
- What evidence would resolve it: Experiments evaluating performance against structured noise patterns, or theoretical analysis of the distribution consistency pattern's sensitivity to different noise types.

### Open Question 4
- Question: What is the impact of warm-up training duration on final performance?
- Basis in paper: The paper mentions warm-up training but doesn't explore the relationship between warm-up duration and final performance.
- Why unresolved: The paper sets warm-up epochs empirically (5, 5, and 30 for different datasets) without investigating the optimal duration.
- What evidence would resolve it: Experiments varying warm-up duration and measuring its impact on final performance, or theoretical analysis of the warm-up's role in the learning process.

## Limitations
- Heavy reliance on clean label set quality for correction mechanism effectiveness
- Assumes well-separated category centers which may not hold in datasets with overlapping classes
- Increased computational overhead due to multiple filtering and correction steps

## Confidence
- Distribution-consistency filtering effectiveness: High (supported by statistical analysis and strong empirical results)
- Label correction mechanism: Medium (dependent on quality of clean label set)
- Overall robustness across noise ratios: Medium (based on provided experiments but requiring independent validation)

## Next Checks
1. **Ablation Study**: Test performance with only filtering (no correction) and with correction disabled to isolate the contribution of each component.
2. **Center Separation Analysis**: Visualize and quantify category center distances across datasets to verify the assumption of well-separated centers.
3. **Knowledge Base Dependency**: Vary the size of the clean label set and measure correction accuracy to quantify dependency on clean examples.