---
ver: rpa2
title: 'Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine
  Unlearning through Parameter Re-initialization'
arxiv_id: '2401.08998'
source_url: https://arxiv.org/abs/2401.08998
tags:
- unlearning
- forget
- machine
- data
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Attack-and-Reset for Unlearning (ARU), a
  novel method for machine unlearning that selectively forgets specific learned information
  from deep neural networks. ARU leverages adversarial noise to generate a parameter
  mask, identifying and reinitializing filters biased toward the forget set.
---

# Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization

## Quick Facts
- arXiv ID: 2401.08998
- Source URL: https://arxiv.org/abs/2401.08998
- Authors: Yoonhwa Jung; Ikhyun Cho; Shun-Hsiang Hsu; Julia Hockenmaier
- Reference count: 10
- Primary result: ARU achieves state-of-the-art performance on MUFAC and MUCAC benchmarks, improving NoMUS score by an average of 2.85 points

## Executive Summary
This paper introduces Attack-and-Reset for Unlearning (ARU), a novel method for machine unlearning that selectively forgets specific learned information from deep neural networks. ARU leverages adversarial noise to generate a parameter mask, identifying and reinitializing filters biased toward the forget set. By disentangling features using adversarial noise, ARU effectively resets over-parameterized elements, enabling efficient forgetting while preserving model utility. Experiments on two facial unlearning benchmark datasets, MUFAC and MUCAC, show that ARU achieves state-of-the-art performance, outperforming prior methods in both forgetting capability and utility preservation.

## Method Summary
ARU is a task-agnostic machine unlearning method for facial recognition that removes specific personal identities from trained models. The method generates adversarial noise on forget-set samples using PGD, computes gradient discrepancy between raw and adversarial inputs to identify convolutional filters most influenced by forget data, reinitializes the top 50% of these filters, and fine-tunes on retain data. The approach uses ResNet18 on 128x128 facial images from MUFAC and MUCAC datasets, with performance measured by NoMUS score combining utility (test accuracy) and forgetting (MIA classifier accuracy).

## Key Results
- ARU improves NoMUS score by an average of 2.85 points on facial unlearning benchmarks
- Outperforms prior state-of-the-art methods on both MUFAC and MUCAC datasets
- Achieves effective forgetting while preserving model utility through selective parameter re-initialization

## Why This Works (Mechanism)

### Mechanism 1
Adversarial noise disentangles feature contributions by contrasting gradients from raw images and perturbed inputs. The adversarial noise is generated using Projected Gradient Descent (PGD) to maximize the loss for forget-set samples. By comparing the gradients of the original and adversarial inputs through the network, ARU identifies convolutional filters whose parameter gradients differ significantly—indicating strong influence from forget-set features. If adversarial noise fails to create distinguishable gradient patterns, parameter selection becomes ineffective.

### Mechanism 2
Parameter re-initialization selectively removes over-parameterized elements biased toward forget-set features, improving forgetting efficiency. After generating a parameter mask by thresholding gradient discrepancy (using median filter selection), ARU re-initializes the identified convolutional filters. This reduces model capacity dedicated to forget-set features while preserving retain-set performance. If the retained filters are also critical for general features, model utility may degrade.

### Mechanism 3
Fine-tuning on retain data after re-initialization restores prediction accuracy for retained classes while preserving unlearning gains. The parameter mask selectively removes forget-set bias; fine-tuning with retain data compensates for any loss of general feature extraction capability, ensuring task performance remains high. If fine-tuning is insufficient or overfits to retain data, forgetting effectiveness may diminish.

## Foundational Learning

- Concept: Adversarial training and adversarial examples
  - Why needed here: ARU relies on adversarial noise generation to identify parameter biases; understanding PGD-based attacks is essential.
  - Quick check question: What is the difference between white-box and black-box adversarial attacks, and which is used here?

- Concept: Parameter masking and pruning strategies
  - Why needed here: ARU's core innovation is selective filter-level masking based on gradient discrepancy; familiarity with channel pruning and sparsity is key.
  - Quick check question: How does filter-level masking differ from weight-level pruning in terms of computational efficiency and impact on model performance?

- Concept: Membership inference attacks (MIA)
  - Why needed here: MIA is used to evaluate forgetting quality by testing whether forget-set data can be distinguished from unseen data after unlearning.
  - Quick check question: What metric does MIA use to quantify unlearning effectiveness, and what does a score of 0.5 indicate?

## Architecture Onboarding

- Component map: Adversarial noise generator -> Gradient discrepancy calculator -> Parameter mask generator -> Re-initialization layer -> Fine-tuning module

- Critical path:
  1. Generate adversarial noise for forget-set samples
  2. Compute gradient discrepancy between raw and adversarial inputs
  3. Generate parameter mask based on discrepancy threshold
  4. Re-initialize masked filters
  5. Fine-tune on retain data

- Design tradeoffs:
  - Masking threshold (e.g., median) vs. forgetting effectiveness
  - Adversarial noise strength (epsilon) vs. gradient distinguishability
  - Fine-tuning duration vs. utility preservation

- Failure signatures:
  - No significant gradient discrepancy → ineffective masking
  - High utility loss after re-initialization → over-aggressive masking
  - Low forgetting score → inadequate adversarial noise or mask

- First 3 experiments:
  1. Verify adversarial noise generation produces meaningful perturbations on forget-set samples
  2. Test gradient discrepancy computation on a small subset to confirm filtering logic
  3. Run a single ARU iteration and evaluate immediate utility/forgetting impact before full fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of the proportion of parameters to be reset (currently set at 50%) affect the trade-off between forgetting capability and model utility across different datasets and architectures? The paper mentions that "the optimal proportion of parameters to be reset may vary" but consistently uses 50% as the median threshold. Systematic experiments varying the masking ratio across multiple datasets and architectures would identify optimal trade-offs.

### Open Question 2
Can ARU be effectively extended to other types of adversarial noise attacks beyond PGD-based adversarial noise? The paper specifically uses PGD-based adversarial noise generation, suggesting potential for other methods. Comparative studies using different adversarial attack methods (e.g., FGSM, CW) to generate noise and evaluating ARU's performance would resolve this.

### Open Question 3
What is the impact of ARU on model robustness against adversarial attacks beyond the forgetting task? The paper uses adversarial noise to disentangle features, implying potential effects on model robustness. Empirical studies measuring ARU's impact on model robustness against various adversarial attacks post-unlearning would address this question.

## Limitations

- The method relies on adversarial noise to identify forget-set biases, which may not generalize across diverse data distributions
- The 50% filter selection threshold appears arbitrary and may not be optimal across different architectures or tasks
- Evaluation metrics focus on facial recognition benchmarks, limiting generalizability to other domains

## Confidence

- High confidence: The core mechanism of using adversarial noise to generate parameter masks and the overall ARU framework
- Medium confidence: The effectiveness of 50% filter selection threshold and PGD-based adversarial noise generation parameters
- Medium confidence: The generalizability of results to non-facial recognition tasks and different neural network architectures

## Next Checks

1. **Gradient Discrepancy Sensitivity Test**: Vary the adversarial noise strength (epsilon) and filter selection threshold (e.g., top 30%, 40%, 60%, 70%) to determine optimal parameters and assess robustness to these choices

2. **Cross-Domain Evaluation**: Apply ARU to non-facial recognition datasets (e.g., CIFAR-10, ImageNet subsets) to evaluate generalization beyond the facial benchmarks

3. **Ablation of Fine-tuning Duration**: Systematically vary fine-tuning epochs (1, 5, 10, 20) to quantify the minimum fine-tuning required for utility preservation while maintaining forgetting effectiveness