---
ver: rpa2
title: 'ToMBench: Benchmarking Theory of Mind in Large Language Models'
arxiv_id: '2402.15052'
source_url: https://arxiv.org/abs/2402.15052
tags:
- llms
- task
- does
- abilities
- story
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToMBench, the first systematic benchmark
  for evaluating the Theory of Mind (ToM) capabilities of large language models (LLMs).
  ToMBench features a comprehensive evaluation framework covering 8 tasks and 31 abilities
  in social cognition, uses a multiple-choice question format for automated and unbiased
  assessment, and is built from scratch in both Chinese and English to avoid data
  contamination.
---

# ToMBench: Benchmarking Theory of Mind in Large Language Models

## Quick Facts
- arXiv ID: 2402.15052
- Source URL: https://arxiv.org/abs/2402.15052
- Authors: Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu, Yunghwei Lai, Zexuan Xiong, Minlie Huang
- Reference count: 32
- Primary result: ToMBench is the first systematic benchmark for evaluating Theory of Mind in LLMs, showing even GPT-4 lags humans by over 10 percentage points

## Executive Summary
This paper introduces ToMBench, the first systematic benchmark for evaluating Theory of Mind (ToM) capabilities in large language models. The benchmark features 2,860 multiple-choice questions across 8 tasks and 31 abilities in social cognition, built from scratch in both Chinese and English to avoid data contamination. Experiments with 10 popular LLMs show that even the most advanced models like GPT-4 lag behind human performance by over 10 percentage points, indicating LLMs have not achieved human-level ToM.

The evaluation framework uses a multiple-choice format to enable automated and unbiased assessment, while the comprehensive coverage of both tasks and abilities ensures thorough evaluation of social intelligence. Further analysis reveals LLMs struggle with comprehensive social scenario understanding and tend to rely on semantic associations rather than human-like cognitive processes.

## Method Summary
ToMBench evaluates LLM Theory of Mind capabilities using a bilingual dataset of 2,860 multiple-choice questions organized into 8 tasks and 31 abilities. The evaluation uses vanilla prompting (direct answer) and CoT prompting (step-by-step reasoning), with option shuffling and majority voting for non-GPT-4 models. Performance is measured by accuracy across both task-oriented and ability-oriented perspectives, with human baseline established from 20 native speakers.

## Key Results
- Even GPT-4 lags human performance by over 10 percentage points on ToMBench
- LLMs struggle with comprehensive social scenario understanding across multiple tasks
- Models tend to rely on semantic associations rather than human-like cognitive processes
- Performance differences between Chinese and English versions reveal language-specific biases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ToMBench avoids contamination by building all test samples from scratch rather than reusing existing psychology inventories.
- Mechanism: By creating a new bilingual dataset (2,860 samples) in Chinese first and then translating to English, the benchmark ensures that LLMs have not been exposed to these specific prompts during pre-training, fine-tuning, or RLHF.
- Core assumption: Existing ToM tests (e.g., Sally-Anne) are likely in LLM training data, so using them would inflate scores without reflecting true capability.
- Evidence anchors: [abstract] states "built from scratch bilingual inventory to strictly avoid data leakage" and [section] explains "we do not use any existing inventories from psychological literature due to the potential risk of data contamination"
- Break condition: If any sample is accidentally constructed from a common ToM scenario that was already in training data, the contamination risk re-emerges.

### Mechanism 2
- Claim: Multiple-choice question format enables automated, unbiased, and consistent evaluation without human scoring.
- Mechanism: Each sample consists of a story, a question, and four answer options (one correct, three plausible distractors), allowing LLMs to be scored purely by accuracy.
- Core assumption: MCQ format removes subjectivity and manual annotation costs, making large-scale evaluation feasible.
- Evidence anchors: [abstract] mentions "multiple-choice question format to support automated and unbiased evaluation" and [section] explains "We construct ToMBench in the form of multiple-choice questions to avoid the high costs of manual scoring"
- Break condition: If distractor options are too obviously wrong, LLMs might guess correctly without real ToM reasoning, reducing the validity of the evaluation.

### Mechanism 3
- Claim: Organizing tests by both tasks (8) and abilities (31) provides comprehensive coverage of social cognition.
- Mechanism: Tasks map to well-known psychological tests, while abilities follow the ATOMS framework, ensuring both general and granular evaluation.
- Core assumption: Evaluating only tasks would miss specific ability-level weaknesses; evaluating only abilities would lose connection to established psychological paradigms.
- Evidence anchors: [abstract] states "systematic evaluation framework encompassing 8 tasks and 31 abilities" and [section] explains "We first review broad psychological literature and identify 8 well-defined theory-of-mind tasks... Then, we further ground ToMBench in a well-structured psychological framework, ATOMS"
- Break condition: If the mapping between tasks and abilities is incomplete or inaccurate, certain cognitive capabilities might be under- or over-represented.

## Foundational Learning

- Concept: Theory of Mind (ToM)
  - Why needed here: ToMBench evaluates whether LLMs can attribute mental states (beliefs, desires, emotions) to others, which is the core definition of ToM.
  - Quick check question: Can an LLM correctly infer that a character in a story holds a false belief about where an object is located, even though the LLM knows the true location?

- Concept: Social scenario understanding
  - Why needed here: ToM evaluation requires understanding complex social interactions, not just language patterns; the benchmark uses real-world scenarios to test this.
  - Quick check question: Given a story where one character hints indirectly to another, can an LLM infer the intended meaning without explicit instruction?

- Concept: Bilingual evaluation and cultural context
  - Why needed here: The benchmark is built in Chinese and translated to English to avoid contamination and ensure cultural neutrality; performance differences between languages reveal model biases.
  - Quick check question: Does an LLM perform significantly differently on the same ToM question presented in Chinese vs. English, indicating language-specific reasoning?

## Architecture Onboarding

- Component map: Data layer (2,860 bilingual MCQ samples organized into 8 tasks and 31 abilities) -> Evaluation layer (Prompt templates for vanilla and CoT prompting) -> Analysis layer (Accuracy metrics, task/ability breakdowns, coherent test mode) -> Baseline layer (Human performance from 20 native speakers)
- Critical path: Sample → Prompt → LLM response → Accuracy score → Aggregate by task/ability
- Design tradeoffs:
  - MCQ vs. open-ended: MCQ enables automation but may limit nuance in responses
  - Bilingual vs. monolingual: Bilingual avoids contamination but adds translation complexity
  - Task vs. ability focus: Dual focus increases comprehensiveness but requires more samples
- Failure signatures:
  - Low accuracy across all tasks → LLM lacks basic ToM
  - High accuracy on some tasks but low on others → LLM relies on task-specific patterns
  - Large drop in coherent test → LLM guesses rather than fully understands scenarios
- First 3 experiments:
  1. Run ToMBench on a new LLM and compare accuracy per task to GPT-4 baseline
  2. Evaluate the same LLM on coherent test mode to check for full scenario understanding
  3. Compare performance on Chinese vs. English versions to detect language bias

## Open Questions the Paper Calls Out

- **Question**: What is the impact of including multimodal ToM tasks (e.g., visual perspective-taking) on LLM performance compared to purely textual evaluations?
  - Basis in paper: [inferred] The paper discusses limitations around excluding visual ToM tasks and multimodal LLMs like GPT-4V.
  - Why unresolved: The current benchmark only covers textual tasks and abilities, leaving a gap in evaluating how well LLMs understand ToM in visual contexts.
  - What evidence would resolve it: A follow-up benchmark incorporating image/video-based ToM tasks and testing with multimodal LLMs.

- **Question**: How does the performance of LLMs on ToM tasks change when using prompting methods specifically designed for ToM reasoning (e.g., "perspective taking" or "foresee and reflect") compared to vanilla or CoT prompting?
  - Basis in paper: [explicit] The paper mentions these emerging prompting methods but does not test them, noting that CoT reasoning did not improve ToM performance.
  - Why unresolved: The current evaluation only uses vanilla and CoT prompting, missing potential gains from ToM-specific prompting strategies.
  - What evidence would resolve it: Experiments comparing ToM performance across different prompting methods including those designed for ToM.

- **Question**: To what extent does increasing the sample size for each ToM ability in the benchmark affect the reliability and accuracy of performance measurements for LLMs?
  - Basis in paper: [explicit] The paper acknowledges that some abilities have as few as 20 samples, which may lead to inadequate testing.
  - Why unresolved: The current benchmark's limited sample size per ability may not provide robust performance estimates for specific ToM capabilities.
  - What evidence would resolve it: A larger-scale version of the benchmark with increased samples per ability and comparison of performance consistency.

## Limitations
- Contamination avoidance relies on authors' claims without external validation
- MCQ format may not fully capture nuanced reasoning patterns of true ToM capability
- Some abilities have as few as 20 samples, potentially leading to inadequate testing

## Confidence

- **High confidence**: The dual-task-and-ability evaluation framework is well-grounded in psychological literature and provides comprehensive coverage of social cognition. The methodology for using multiple-choice questions to enable automated evaluation is standard in NLP and aligns with established practices.

- **Medium confidence**: The contamination avoidance claim is plausible given the bilingual construction approach, but lacks external validation. The performance gap between LLMs and humans (over 10 percentage points) is robust across experiments, but the interpretation that this definitively proves LLMs lack human-level ToM requires careful consideration of alternative explanations.

- **Low confidence**: The exact impact of CoT prompting on ToM performance is unclear, as the paper does not fully specify the prompt templates. The claim that LLMs rely on semantic associations rather than human-like cognitive processes is based on performance patterns but not directly measured cognitive mechanisms.

## Next Checks

1. **External Contamination Audit**: Have independent researchers attempt to find any overlap between ToMBench samples and common ToM scenarios in existing literature or training data to validate the contamination avoidance claim.

2. **Open-Ended Validation**: Run a subset of ToMBench questions using open-ended response format (not MCQ) to check if the performance gap between LLMs and humans persists, validating that MCQ format doesn't artificially limit LLM performance.

3. **Cross-Cultural Validation**: Test the same LLM on ToMBench in Chinese and English versions with native speakers of each language to quantify and explain any performance differences, validating the cultural neutrality claim.