---
ver: rpa2
title: 'Exploring Large Language Models for Feature Selection: A Data-centric Perspective'
arxiv_id: '2408.12025'
source_url: https://arxiv.org/abs/2408.12025
tags:
- feature
- selection
- llms
- arxiv
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores Large Language Models (LLMs) for feature selection
  from a data-centric perspective. The authors categorize existing LLM-based feature
  selection methods into two groups: data-driven (using sample values) and text-based
  (using descriptive context).'
---

# Exploring Large Language Models for Feature Selection: A Data-centric Perspective

## Quick Facts
- arXiv ID: 2408.12025
- Source URL: https://arxiv.org/abs/2408.12025
- Authors: Dawei Li; Zhen Tan; Huan Liu
- Reference count: 0
- One-line primary result: Text-based feature selection using LLMs is more effective and robust than data-driven approaches, particularly in low-resource settings, and shows stronger scaling laws with model size.

## Executive Summary
This paper explores the use of Large Language Models (LLMs) for feature selection from a data-centric perspective, categorizing existing LLM-based methods into data-driven (using sample values) and text-based (using descriptive context) approaches. Through extensive experiments across classification and regression tasks using various LLMs including GPT-4, ChatGPT, and LLaMA-2, the authors find that text-based feature selection is more effective and robust than data-driven approaches, particularly in low-resource settings. The study demonstrates that text-based methods leverage LLMs' prior semantic knowledge to make feature importance judgments without requiring statistical inference from sample values, leading to stronger scaling laws with model size and comparable performance to traditional feature selection methods.

## Method Summary
The authors investigate two LLM-based feature selection approaches: data-driven methods that use sample values as input to LLMs for statistical inference, and text-based methods that leverage LLMs' semantic knowledge through descriptive context about features and tasks. They evaluate these approaches across 10 benchmark datasets (6 classification, 4 regression) with varying sample sizes and feature counts using multiple LLMs (GPT-4, ChatGPT, LLaMA-2 7B, LLaMA-2 13B). The methods are compared against traditional feature selection techniques (MI, RFE, MRMR, random selection) using L2-penalized logistic/linear regression models. Additionally, they introduce a Retrieval-Augmented Feature Selection (RAFS) method for handling domain-specific features in biomedical applications, demonstrating its effectiveness for cancer survival prediction.

## Key Results
- Text-based feature selection outperforms data-driven methods in low-resource settings by leveraging LLMs' prior semantic knowledge
- Text-based methods show stronger scaling laws with model size compared to data-driven approaches
- Data-driven feature selection struggles with increasing sample sizes due to LLM limitations in processing long sequences
- The proposed RAFS method demonstrates effectiveness in real-world medical applications with domain-specific features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-based feature selection is more effective than data-driven methods in low-resource settings because LLMs leverage prior semantic knowledge rather than relying on limited statistical inference from small samples.
- Mechanism: When LLMs are provided with descriptive context about features and tasks, they use their pre-trained knowledge to infer feature importance without requiring statistical inference from sample values. This semantic association is more robust when data is scarce.
- Core assumption: LLMs have sufficient prior knowledge about the semantic relationships between features and target variables that can be leveraged through descriptive context.
- Evidence anchors:
  - [abstract] "text-based feature selection which utilizes prior knowledge of LLMs to do semantical associations using descriptive context"
  - [section 3.2] "Another line of work [8; 26] tries to employ the extensive semantics knowledge in LLMs [33] to perform feature selection"
  - [corpus] Weak - the corpus focuses on data-centric AI surveys but doesn't directly address semantic knowledge in LLMs
- Break condition: If the descriptive context is insufficient or the LLM lacks relevant prior knowledge about the domain, the text-based method would fail to perform meaningful semantic associations.

### Mechanism 2
- Claim: Text-based feature selection exhibits stronger scaling laws with model size compared to data-driven approaches because larger models have more comprehensive semantic knowledge and better natural language understanding.
- Mechanism: As LLM size increases, their ability to understand and reason about semantic relationships between features and targets improves, leading to better feature selection performance. This scaling is more pronounced in text-based methods than in data-driven ones.
- Core assumption: The semantic knowledge and reasoning capabilities of LLMs scale predictably with model size, and this knowledge is applicable to feature selection tasks.
- Evidence anchors:
  - [abstract] "text-based feature selection using LLMs is more effective and stable across various low-resource settings. Additionally, it shows a more pronounced scaling law with respect to the size of LLMs"
  - [section 4.2 Finding 4] "T ext-based feature selection exhibits a stronger scaling law with model size compared to data-driven feature selection with LLMs"
  - [corpus] Weak - corpus mentions scaling in LLMs but not specifically for feature selection tasks
- Break condition: If the scaling of semantic knowledge with model size plateaus or if the additional parameters don't translate to better semantic understanding, the scaling advantage would diminish.

### Mechanism 3
- Claim: Data-driven feature selection using LLMs struggles with increasing sample sizes due to limitations in processing long sequences, while text-based methods are unaffected by sample size.
- Mechanism: Data-driven methods require LLMs to process and reason about multiple data points, which becomes increasingly difficult as the number of samples grows due to context window limitations. Text-based methods only need to process descriptive context, which remains constant regardless of sample size.
- Core assumption: LLMs have fundamental limitations in processing long sequences that affect their ability to perform statistical inference on large numbers of data points.
- Evidence anchors:
  - [section 4.2 Finding 3] "Data-driven feature selection using LLMs struggles when number of samples increases... This drop is consistently observed across all four LLMs"
  - [section 4.2] "We attribute this issue to LLMs struggling with processing long sequences, a challenge highlighted in many previous studies"
  - [corpus] Weak - corpus mentions long sequence challenges but not specifically in the context of feature selection
- Break condition: If LLMs overcome their long-sequence processing limitations through architectural improvements or if the data-driven approach uses more efficient sampling strategies, this advantage of text-based methods would be reduced.

## Foundational Learning

- Concept: Few-shot learning
  - Why needed here: The paper explores LLMs' ability to perform feature selection with limited samples (16-shot, 32-shot, 64-shot, 128-shot), which is a form of few-shot learning.
  - Quick check question: What is the minimum number of examples needed for an LLM to demonstrate meaningful feature selection performance?

- Concept: Feature selection techniques
  - Why needed here: Understanding traditional feature selection methods (filter, wrapper, embedded) provides context for comparing LLM-based approaches and understanding their advantages/disadvantages.
  - Quick check question: How do mutual information-based feature selection and recursive feature elimination differ in their approach to identifying important features?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: The RAFS method uses RAG principles to enhance LLMs' understanding of domain-specific features by retrieving additional context from external sources.
  - Quick check question: How does retrieving domain-specific context improve an LLM's ability to perform feature selection in specialized fields like biomedical applications?

## Architecture Onboarding

- Component map: LLM backbone (GPT-4, ChatGPT, LLaMA-2 variants) -> Prompting strategy (data-driven vs text-based) -> Dataset processing module -> Feature selection evaluation pipeline -> Retrieval augmentation module for RAFS
- Critical path: Dataset -> Prompt construction -> LLM inference -> Feature importance scoring -> Subset selection -> Model training -> Performance evaluation
- Design tradeoffs: Text-based methods sacrifice the potential for data-driven statistical inference but gain robustness and sample-efficiency; data-driven methods could theoretically capture dataset-specific patterns but are limited by LLM context window and sample size.
- Failure signatures: Data-driven methods show performance degradation with increasing sample sizes; text-based methods show performance drops when domain-specific knowledge is insufficient; both methods fail when prompts are poorly constructed.
- First 3 experiments:
  1. Compare text-based vs data-driven feature selection on a simple dataset with known feature importance to establish baseline effectiveness
  2. Test scaling behavior by running both methods across different LLM sizes on the same dataset to verify the scaling law claims
  3. Implement and test the RAFS method on a biomedical dataset with domain-specific features to validate the retrieval augmentation approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of data-driven feature selection with LLMs change when using different sampling strategies or data preprocessing techniques?
- Basis in paper: [inferred] The paper mentions that data-driven feature selection struggles with long sequences and performance drops with increased sample size, suggesting that sampling strategy and preprocessing might impact results.
- Why unresolved: The paper only explores varying the number of samples (16-shot, 32-shot, 64-shot, 128-shot) but doesn't investigate different sampling strategies or preprocessing techniques that could potentially mitigate the long-sequence issue.
- What evidence would resolve it: Experiments comparing different sampling strategies (e.g., stratified sampling, systematic sampling) and preprocessing techniques (e.g., dimensionality reduction, feature engineering) alongside the existing methods would provide insights into their impact on data-driven feature selection performance.

### Open Question 2
- Question: Can the Retrieval-Augmented Feature Selection (RAFS) method be extended to other domains beyond biomedical applications, and how would its performance compare to traditional feature selection methods in those domains?
- Basis in paper: [explicit] The paper introduces RAFS for handling domain-specific feature names in biomedical applications and demonstrates its effectiveness, but doesn't explore its applicability to other domains.
- Why unresolved: While RAFS shows promise in biomedical applications, its effectiveness in other domains with different types of domain-specific features remains unexplored. Additionally, a comparison with traditional feature selection methods in these new domains is needed.
- What evidence would resolve it: Implementing RAFS in various non-biomedical domains (e.g., finance, social media, natural language processing) and comparing its performance against traditional feature selection methods would reveal its broader applicability and effectiveness.

### Open Question 3
- Question: How does the performance of text-based feature selection with LLMs scale when applied to datasets with a very large number of features (e.g., >10,000 features), and what are the computational implications?
- Basis in paper: [inferred] The paper mentions the challenge of large feature sets in biomedical applications and introduces RAFS to handle them, but doesn't explore the limits of text-based feature selection with extremely large feature sets.
- Why unresolved: While the paper demonstrates the effectiveness of text-based feature selection with LLMs, it doesn't address how well this approach scales to datasets with a massive number of features, which is a common challenge in many real-world applications.
- What evidence would resolve it: Experiments applying text-based feature selection with LLMs to datasets with varying numbers of features (e.g., 1,000; 10,000; 100,000) and measuring both performance and computational resources (e.g., time, memory) would provide insights into its scalability and practical limitations.

## Limitations

- The study's experimental scope may not fully capture edge cases where either method fails, particularly in domains with insufficient descriptive context or where LLMs lack relevant prior knowledge
- The reliance on LLMs' semantic knowledge introduces domain-specific limitations that may significantly impact performance when feature descriptions are inadequate or domain knowledge is lacking
- The scaling law observations for text-based methods need validation across a broader range of model sizes and architectures beyond the tested configurations

## Confidence

- High Confidence: Text-based feature selection outperforms data-driven methods in low-resource settings and shows stronger scaling laws with model size
- Medium Confidence: Text-based methods are more robust to increasing sample sizes compared to data-driven approaches
- Low Confidence: The general applicability of text-based feature selection across all domains

## Next Checks

1. **Domain Transferability Test**: Evaluate both methods across domains with varying levels of semantic complexity (e.g., image features with textual descriptions, time-series features, categorical features) to assess the generalizability of text-based feature selection.

2. **Context Window Analysis**: Systematically test the impact of different context window sizes on data-driven method performance, particularly focusing on identifying the exact point where performance degradation begins with increasing sample sizes.

3. **Knowledge Base Dependency Test**: Compare text-based feature selection performance using LLMs with different pretraining datasets (e.g., general vs. domain-specific pretraining) to quantify the importance of relevant prior knowledge for semantic associations.