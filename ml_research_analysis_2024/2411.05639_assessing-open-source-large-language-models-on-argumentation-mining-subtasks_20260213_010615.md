---
ver: rpa2
title: Assessing Open-Source Large Language Models on Argumentation Mining Subtasks
arxiv_id: '2411.05639'
source_url: https://arxiv.org/abs/2411.05639
tags:
- prompting
- mining
- chat
- instruct
- argumentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates four open-source large language models (LLMs)
  on argumentation mining subtasks: argumentative discourse unit classification (ADUC)
  and argumentative relation classification (ARC). Experiments are conducted on three
  corpora: persuasive essays (PE), argumentative microtexts (AMT1 and AMT2), using
  both zero-shot and few-shot learning scenarios.'
---

# Assessing Open-Source Large Language Models on Argumentation Mining Subtasks

## Quick Facts
- arXiv ID: 2411.05639
- Source URL: https://arxiv.org/abs/2411.05639
- Reference count: 37
- Primary result: Open-source LLMs perform well on argumentation mining subtasks, with context-aware prompting improving ARC and Llama3 showing strong cross-task performance.

## Executive Summary
This paper evaluates four open-source large language models (Mistral 7B, Mixtral 8x7B, Llama2 7B, Llama3 8B) on two core argumentation mining subtasks: argumentative discourse unit classification (ADUC) and argumentative relation classification (ARC). Experiments are conducted on three corpora (persuasive essays, argumentative microtexts AMT1/AMT2) using both zero-shot and few-shot learning scenarios. The study finds that context-aware prompting generally improves ARC performance, Llama3 demonstrates strong adaptability across tasks and datasets, and few-shot demonstrations primarily stabilize rather than enhance model performance.

## Method Summary
The study tests four open-source LLMs on ADUC (classifying discourse units as claims or premises) and ARC (classifying relations as support or attack) tasks. Two prompting strategies are compared: vanilla prompting (classifying units independently) and context-aware prompting (providing full essay context). Experiments are conducted across three datasets (AMT1, AMT2, PE) using both zero-shot and few-shot learning with 0-8 demonstrations. Micro F1 scores are used for evaluation due to class imbalance.

## Key Results
- Context-aware prompting significantly improves ARC performance across most models and datasets
- Llama3 shows consistently strong performance across both tasks and all datasets
- Few-shot demonstrations stabilize rather than enhance performance, particularly for ARC tasks
- Models generally perform worse on AMT2 than AMT1, possibly due to lower annotation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-aware prompting improves ARC performance more than ADUC because the former benefits from relational reasoning in full text context
- Mechanism: Embedding ADU pairs within full essay context allows models to infer semantic relationships using discourse cues and logical flow that vanilla prompts miss
- Core assumption: The full essay context contains sufficient relational cues to guide ADU pair classification
- Evidence anchors: Context-aware prompting generally improves performance, especially for ARC; context aids in better understanding relationships between sentences

### Mechanism 2
- Claim: Few-shot demonstrations stabilize rather than enhance performance by reducing variance in model outputs
- Mechanism: Demonstrations provide a reference schema that reduces random variation in predictions without improving accuracy
- Core assumption: Demonstrations are representative but not superior to the model's internal reasoning
- Evidence anchors: Few-shot learning reveals demonstrations stabilize rather than enhance performance; context-aware prompting reduces performance disparity between datasets

### Mechanism 3
- Claim: Llama3's strong cross-task and cross-dataset performance indicates higher adaptability to prompting methods and dataset variations
- Mechanism: Llama3 likely has better instruction-tuning and generalization, allowing flexible interpretation of prompts across varied argumentative corpora
- Core assumption: Llama3's architecture includes broader instruction-following capability than other tested models
- Evidence anchors: Llama3 shows strong performance across both tasks and all datasets; consistent performance across AMT1, AMT2, and PE suggests robustness

## Foundational Learning

- Concept: Argumentative discourse unit classification (ADUC)
  - Why needed here: Core subtask distinguishing claims from premises; prerequisite for downstream relation extraction
  - Quick check question: In a sentence "The sky is blue" followed by "Because it reflects sunlight", which is the claim and which is the premise?

- Concept: Argumentative relation classification (ARC)
  - Why needed here: Determines support vs attack links between ADUs; essential for building argumentation graphs
  - Quick check question: If sentence A states "Exercise is beneficial" and sentence B states "It strengthens muscles", what is the relation from B to A?

- Concept: Zero-shot vs few-shot learning
  - Why needed here: Determines whether demonstrations improve performance or just stabilize it; affects prompt design
  - Quick check question: If a model achieves 80% F1 with zero-shot and 81% with few-shot, what does this suggest about the demonstrations' impact?

## Architecture Onboarding

- Component map: Raw essay/microtext → ADU segmentation → Pairs for ARC → Prompt generation → LLM inference → Label extraction → F1 computation
- Critical path: Segmentation → Prompt generation → LLM inference → Label extraction → F1 computation
- Design tradeoffs:
  - Context length vs inference cost (full essays increase token usage)
  - Zero-shot simplicity vs few-shot stability (demonstrations increase prompt size)
  - Model size vs performance (larger models may handle context better but cost more)
- Failure signatures:
  - Context truncation causing poor ARC performance
  - Overfitting to demonstration style degrading generalization
  - Dataset quality differences causing inconsistent F1
- First 3 experiments:
  1. Run ADUC and ARC on AMT1 with vanilla prompting (baseline)
  2. Repeat with context-aware prompting to confirm improvement in ARC
  3. Add 1-2 demonstrations and compare F1 variance to assess stabilization effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance differences between AMT1 and AMT2 datasets relate to the quality of annotations in each corpus?
- Basis in paper: The paper states that LLMs generally perform worse on AMT2 than AMT1, which may be because of the lower quality of AMT2
- Why unresolved: The paper mentions this as a possible explanation but does not provide concrete evidence or analysis to support this claim
- What evidence would resolve it: A detailed comparison of annotation quality between AMT1 and AMT2 datasets, including inter-annotator agreement scores and annotation guidelines

### Open Question 2
- Question: How do different prompting strategies affect the performance of LLMs on argumentation mining tasks across various languages and domains?
- Basis in paper: The paper focuses on English argumentative corpora and compares vanilla and context-aware prompting methods, but does not explore other languages or domains
- Why unresolved: The study is limited to English corpora and does not investigate how prompting strategies might perform in other languages or domains
- What evidence would resolve it: Experiments on multilingual corpora and diverse domains using various prompting strategies

### Open Question 3
- Question: What is the impact of increasing the number of demonstrations on the performance of LLMs in few-shot learning scenarios for argumentation mining tasks?
- Basis in paper: The paper discusses the role of demonstrations as stabilizers rather than enhancers for both ADUC and ARC tasks
- Why unresolved: While the paper notes that demonstrations stabilize performance, it does not explore the optimal number of demonstrations or their impact on model performance
- What evidence would resolve it: A systematic study varying the number of demonstrations in few-shot learning scenarios, along with performance analysis

## Limitations
- The study relies on three corpora of varying quality, with AMT2 showing notably lower performance, raising questions about corpus reliability and generalizability
- Exact implementation details of context-aware vs. vanilla prompting are not fully specified, making precise replication challenging
- The stabilization effect observed in few-shot learning is interpreted as reduced variance rather than accuracy gains, but underlying statistical analysis is not detailed

## Confidence
- High confidence: Context-aware prompting generally improves ARC performance, and Llama3 demonstrates strong cross-task and cross-dataset adaptability
- Medium confidence: Few-shot demonstrations stabilize rather than enhance performance; the mechanism behind this effect needs further validation
- Low confidence: The exact causes of AMT2's lower quality and its impact on model performance are not fully explained

## Next Checks
1. Replicate the prompting methods with detailed prompt templates and measure performance variance across AMT1 and AMT2 to confirm stabilization vs. enhancement effects
2. Conduct ablation studies removing context from ARC prompts to quantify the contribution of relational reasoning in full-text context
3. Test Llama3 and other models on a fourth, independently curated corpus to assess robustness beyond the three datasets used here