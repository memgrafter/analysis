---
ver: rpa2
title: Performance Law of Large Language Models
arxiv_id: '2408.09895'
source_url: https://arxiv.org/abs/2408.09895
tags:
- performance
- data
- training
- uni00000013
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a simple empirical equation called "Performance
  Law" to predict the MMLU score of large language models (LLMs) based on a few key
  hyperparameters. The Performance Law takes into account the number of layers, hidden
  size, feed-forward network size, training data size, and an instability discount
  factor.
---

# Performance Law of Large Language Models

## Quick Facts
- arXiv ID: 2408.09895
- Source URL: https://arxiv.org/abs/2408.09895
- Authors: Chuhan Wu; Ruiming Tang
- Reference count: 31
- The paper proposes a simple empirical equation called "Performance Law" to predict the MMLU score of large language models (LLMs) based on a few key hyperparameters.

## Executive Summary
This paper introduces a simple empirical equation called "Performance Law" to predict the MMLU score of large language models using only a few key hyperparameters: number of layers, hidden size, feed-forward network size, training data size, and an instability discount factor. By fitting a few regression parameters on a limited set of open-source models, the Performance Law achieves surprisingly accurate predictions across diverse LLM sizes, architectures, and training settings. The method can guide LLM architecture design, resource allocation, and help identify potential issues like data contamination or training instability.

## Method Summary
The Performance Law uses a log-linear regression function to predict MMLU scores based on key hyperparameters: number of layers (N), hidden size (h), FFN size (d), training data size (T), and model size (S). An instability discount factor (u) accounts for the negative impact of deep and slim models on performance. For MoE models, an expansion factor (g) adjusts predictions based on activated parameters. The formula is trained on 10 popular open-source models from 2024 and achieves high accuracy across diverse LLM sizes and architectures.

## Key Results
- Achieves accurate MMLU score predictions across LLM sizes from 0.5B to 1000+B parameters
- Generalizes well to models from different years (2020-2024) and organizations
- Can be extended to predict performance on other benchmarks like MMLU-Pro and BIG-Bench
- Provides guidance for LLM architecture design and resource allocation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Performance Law can accurately predict MMLU scores using only a few hyperparameters and training data size.
- Mechanism: The formula uses a log-linear regression function with key variables like the number of layers, hidden size, FFN size, training data size, and an instability discount factor to estimate model performance.
- Core assumption: The relationship between these hyperparameters and MMLU scores is stable across different LLM architectures and training settings.
- Evidence anchors:
  - [abstract]: "Based on only a few key hyperparameters of the LLM architecture and the size of training data, we obtain a quite accurate MMLU prediction of various LLMs with diverse sizes and architectures developed by different organizations in different years."
  - [section]: "By learning a few regression parameters of the equation on only 10 popular open source models released in 2024, we obtain a surprisingly accurate performance prediction of LLMs of different sizes (from 0.5B to 1000+B) and in different years (from 2020 to 2024) released by different organizations around the world."
  - [corpus]: Found 25 related papers, average neighbor FMR=0.451, average citations=0.0. Top related titles: "Temporal Scaling Law for Large Language Models", "Scaling Law for Language Models Training Considering Batch Size", "The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models." Weak corpus evidence for direct performance prediction claims.
- Break condition: If the relationship between hyperparameters and MMLU scores becomes unstable due to changes in model architectures or training methodologies not captured by the current formula.

### Mechanism 2
- Claim: The model instability discount factor accounts for the negative impact of deep and slim models on performance.
- Mechanism: The instability discount is calculated using the ratio of high-activation parameters to the total number of parameters, penalizing models that are too deep or have insufficient width.
- Core assumption: Deeper and slimmer models are more prone to training instability, which negatively impacts their final performance.
- Evidence anchors:
  - [section]: "Inspired by [16], we devise a model unstable discount u to characterize the instability of large model training: u = e−[(10/d + 20/h)(γN)]2 where γ reflects the precision loss of the training infrastructure (larger is worse) and we set γ = 1 by default."
  - [section]: "However, different from the ideal prediction of the scaling law, in practice layer depth is not everything. The instability of deep model training hinders developers from increasing the model depth."
  - [corpus]: Found 25 related papers, average neighbor FMR=0.451, average citations=0.0. Weak corpus evidence for model instability claims.
- Break condition: If the relationship between model depth, width, and training instability changes due to advancements in training techniques or hardware capabilities.

### Mechanism 3
- Claim: The Performance Law can be extended to predict performance on other benchmarks and model types.
- Mechanism: The formula can be adapted to estimate performance on benchmarks like MMLU-Pro and BIG-Bench, and can be modified to predict performance for MoE models by incorporating the expansion factor and adjusted instability discount.
- Core assumption: The underlying principles of the Performance Law are generalizable to other benchmarks and model architectures.
- Evidence anchors:
  - [section]: "Note that the performance law can also be generalized to other benchmarks that comprehensively measure model capabilities. For example, for strong models (MMLU > 70) we can obtain an approximate estimation of the MMLU-Pro [20] performance by using a simple linear mapping y = 2.33x - 133 obtained by linear regression."
  - [section]: "We also see a very high correlation between BIG-Bench [15] and MMLU scores, thereby developers can map their results easily."
  - [corpus]: Found 25 related papers, average neighbor FMR=0.451, average citations=0.0. Weak corpus evidence for generalization claims.
- Break condition: If the correlation between the Performance Law's predicted scores and other benchmarks or model types becomes unreliable due to significant differences in task requirements or model architectures.

## Foundational Learning

- Concept: Scaling Laws
  - Why needed here: The Performance Law is based on the principles of scaling laws, which describe how model performance scales with model size and training data.
  - Quick check question: What is the primary focus of scaling laws in the context of large language models?

- Concept: Log-Linear Regression
  - Why needed here: The Performance Law uses a log-linear regression function to predict MMLU scores based on the input hyperparameters.
  - Quick check question: What is the purpose of using a log-linear regression function in the Performance Law?

- Concept: Transformer Architecture
  - Why needed here: The Performance Law is designed to predict the performance of Transformer-based LLMs, so understanding the key components of the Transformer architecture is essential.
  - Quick check question: What are the main components of a Transformer-based language model that are considered in the Performance Law?

## Architecture Onboarding

- Component map: Number of layers (N) -> Hidden size (h) -> FFN size (d) -> Training data size (T) -> Instability discount (u) -> Log-linear regression function -> MMLU score prediction

- Critical path: To use the Performance Law effectively, follow these steps:
  1. Identify the relevant hyperparameters of the target LLM (N, h, d, T, S)
  2. Calculate the instability discount factor (u) based on the model's depth and width
  3. If predicting for an MoE model, calculate the expansion factor (g) based on the number of activated parameters
  4. Apply the log-linear regression function to obtain the predicted MMLU score

- Design tradeoffs: The Performance Law offers a balance between simplicity and accuracy:
  - Simplicity: Uses only a few key hyperparameters and a simple regression function
  - Accuracy: Achieves surprisingly accurate predictions across diverse LLM sizes, architectures, and training settings

- Failure signatures: Potential issues that may arise when using the Performance Law:
  - Inaccurate predictions if the target LLM has a significantly different architecture or training methodology not captured by the current formula
  - Overestimation of performance if the training data quality or distribution is suboptimal
  - Underestimation of performance if the target LLM uses an innovative architecture or training strategy not accounted for in the formula

- First 3 experiments:
  1. Test the Performance Law on a small set of well-known LLMs with publicly available MMLU scores to verify its accuracy and identify any potential issues.
  2. Experiment with different instability discount factors to find the optimal values for various model architectures and training settings.
  3. Apply the Performance Law to predict the performance of LLMs on other benchmarks (e.g., MMLU-Pro, BIG-Bench) and validate the results against actual scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Performance Law be extended to predict performance on benchmarks other than MMLU, such as BIG-Bench or MMLU-Pro?
- Basis in paper: [explicit] The paper mentions that the Performance Law can be generalized to other benchmarks that comprehensively measure model capabilities, and provides a linear mapping for MMLU-Pro.
- Why unresolved: The paper only provides a simple linear mapping for MMLU-Pro and mentions a high correlation with BIG-Bench scores. However, it does not provide detailed methods or formulas for other benchmarks.
- What evidence would resolve it: Detailed formulas and validation results for predicting performance on various benchmarks beyond MMLU and MMLU-Pro, using the Performance Law.

### Open Question 2
- Question: What are the specific factors contributing to the overestimation or underestimation of model performance by the Performance Law, and how can these be accounted for in the model?
- Basis in paper: [explicit] The paper lists possible reasons for overestimation and underestimation, including data quality, vocabulary fit, hardware issues, and training strategies.
- Why unresolved: While the paper identifies potential factors, it does not provide a comprehensive analysis of how these factors quantitatively affect the predictions or how to adjust the model to account for them.
- What evidence would resolve it: A detailed study quantifying the impact of each factor on the Performance Law's predictions and methods to incorporate these factors into the model.

### Open Question 3
- Question: How does the Performance Law handle the prediction of performance for models with non-standard architectures or those that deviate significantly from the Transformer-based models it was designed for?
- Basis in paper: [inferred] The paper focuses on Transformer-based models and mentions that the Performance Law might have similar performance for different attention architectures, but does not explicitly address non-standard architectures.
- Why unresolved: The paper does not provide information on how the Performance Law would perform for models with significantly different architectures, such as those using recurrent neural networks or other non-Transformer-based models.
- What evidence would resolve it: Experimental results and analysis of the Performance Law's accuracy for models with non-Transformer-based architectures, including a discussion of any necessary modifications to the model.

## Limitations
- Accuracy depends on the stability of relationships between hyperparameters and MMLU scores across different architectures
- Instability discount factor may not capture all training-related performance variations
- Potential data contamination effects not fully accounted for in the current formula

## Confidence
- High: Basic predictive framework and log-linear regression approach
- Medium: Instability discount factor's universal applicability across different training infrastructures
- Low: Generalizability to non-Transformer-based architectures

## Next Checks
1. Test the Performance Law's predictions against LLMs trained on different data distributions and qualities to assess sensitivity to data factors
2. Validate the instability discount factor across different hardware configurations and training strategies to ensure its robustness
3. Extend the framework to predict performance on specialized benchmarks (e.g., code generation, mathematical reasoning) to test its domain generalization capabilities