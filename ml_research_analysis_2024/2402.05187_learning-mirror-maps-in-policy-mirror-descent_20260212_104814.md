---
ver: rpa2
title: Learning mirror maps in policy mirror descent
arxiv_id: '2402.05187'
source_url: https://arxiv.org/abs/2402.05187
tags:
- mirror
- policy
- entropy
- learning
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores Policy Mirror Descent (PMD), a reinforcement
  learning framework that generalizes various policy optimization algorithms. The
  authors investigate whether the choice of mirror map significantly impacts PMD's
  performance, as existing theoretical studies suggest only mild dependence.
---

# Learning mirror maps in policy mirror descent

## Quick Facts
- arXiv ID: 2402.05187
- Source URL: https://arxiv.org/abs/2402.05187
- Reference count: 40
- Key outcome: Learned mirror maps significantly outperform negative entropy in Policy Mirror Descent across multiple RL environments

## Executive Summary
This paper challenges the prevailing assumption in reinforcement learning that mirror map choice has minimal impact on Policy Mirror Descent (PMD) performance. Through extensive empirical studies, the authors demonstrate that conventional negative entropy mirror maps often yield suboptimal results, and that learned mirror maps can substantially improve both convergence speed and final policy value. Using evolutionary strategies, they discover more effective mirror maps that generalize across different tasks and environments, revealing a significant gap between existing theoretical guarantees and actual performance in practice.

## Method Summary
The authors employ evolutionary strategies to optimize mirror map parameters in Policy Mirror Descent and AMPO frameworks. For tabular environments (Grid-World), they use a neural network parameterization for ω-potentials and optimize with OpenAI-ES. For non-tabular environments (MinAtar, Basic Control Suite, MuJoCo), they use piecewise linear parameterizations optimized with Sep-CMA-ES. The fitness function is the final policy value achieved after training. The learned mirror maps are then tested across various environments to assess generalization capabilities, with comparisons made against negative entropy and ℓ2-norm baselines.

## Key Results
- Learned mirror maps consistently outperform negative entropy and ℓ2-norm in Grid-World environments
- In MinAtar and Basic Control Suite, learned mirror maps achieve higher performance than traditional alternatives
- Learned mirror maps demonstrate strong generalization across different tasks and environments
- Mirror maps that induce smaller policy update distances lead to smoother value improvements with fewer performance degradation instances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The choice of mirror map significantly impacts PMD performance, contrary to existing theoretical bounds.
- Mechanism: Different mirror maps induce different Bregman divergences, altering the geometry of the policy update space. This changes both convergence speed and the achievable error floor, even when theoretical bounds suggest mild dependence.
- Core assumption: Theoretical upper bounds on convergence rates and error floors do not tightly reflect actual performance in practice.
- Evidence anchors:
  - [abstract] "Through empirical experiments, they demonstrate that the conventional negative entropy mirror map often yields suboptimal outcomes."
  - [section] "Our empirical studies reveal that the choice of mirror map significantly influences both the speed of convergence and the minimum achievable error floor in PMD."
  - [corpus] Weak - related papers focus on convergence guarantees but not empirical comparison of mirror map choices.

### Mechanism 2
- Claim: Small policy update distances lead to smoother value improvements and fewer performance degradation instances.
- Mechanism: By constraining policy updates via the Bregman divergence, certain mirror maps (e.g., learned maps) keep ∥πt+1 - πt∥1 small, which according to Theorem 2.2 reduces the risk of performance drops.
- Core assumption: The quasi-monotonicity bound in Equation (3) reflects actual training dynamics.
- Evidence anchors:
  - [section] "Our experiments indicate that having small policy updates leads to smoother value improvements over time with less instances of performance degradation, as suggested by the monotonic policy improvement property given by Xiao (2022)."
  - [section] "This observation confirms the behaviour described by (3), whereby a small distance between policy updates prevents large performance degradation in policy updates."
  - [corpus] Weak - related papers analyze convergence rates but do not empirically validate the link between update distance and stability.

### Mechanism 3
- Claim: Learned mirror maps generalize well across tasks and environments.
- Mechanism: ES searches over a parameterized class of ω-potentials to maximize final policy value; the learned maps retain favorable properties (e.g., zeroing worst actions) across domains.
- Core assumption: The parameterized class of mirror maps is rich enough to capture improvements that transfer across MDPs.
- Evidence anchors:
  - [abstract] "Additionally, we demonstrate that the learned mirror maps generalize effectively to different tasks by testing each map across various other environments."
  - [section] "Our last result consists in testing each learned mirror map across the other environments we consider... These results show that our methodology can be useful in practice, as it benefits from good generalization across tasks."
  - [corpus] Weak - related works do not report cross-task generalization of learned mirror maps.

## Foundational Learning

- Concept: Mirror maps and Bregman divergences
  - Why needed here: Mirror maps define the geometry of policy updates in PMD; Bregman divergences measure the regularization penalty.
  - Quick check question: What is the Bregman divergence induced by the negative entropy mirror map?

- Concept: ω-potentials and their parameterization
  - Why needed here: ω-potentials provide a tractable parameterization for searching over mirror maps.
  - Quick check question: How does the choice of ϕ in an ω-potential affect the resulting mirror map?

- Concept: Evolution strategies for optimization
  - Why needed here: ES is used to search the space of mirror maps without requiring gradients.
  - Quick check question: What is the role of the fitness function in ES when optimizing mirror maps?

## Architecture Onboarding

- Component map:
  - Environment simulators (Grid-World, MinAtar, BCS, MuJoCo) -> Policy network (one-layer NN for tabular, deep NN for non-tabular) -> Scoring function network (deep NN for AMPO) -> Mirror map parameterization (neural net for PMD, piecewise linear for AMPO) -> ES optimizer (OpenAI-ES for PMD, Sep-CMA-ES for AMPO) -> Training loop with GAE for Q-function estimation

- Critical path:
  1. Initialize policy and mirror map parameters.
  2. Run PMD/AMPO updates using the current mirror map.
  3. Evaluate final policy value as fitness.
  4. Update mirror map parameters via ES.
  5. Repeat until convergence.

- Design tradeoffs:
  - Neural net mirror map: more expressive but higher ES dimension
  - Piecewise linear mirror map: fewer parameters, faster ES but less expressive
  - ES population size vs. convergence speed

- Failure signatures:
  - ES stuck in local optima → try larger population or different initialization
  - Training instability → check learning rates and Q-function estimation
  - Poor generalization → verify diversity of training tasks

- First 3 experiments:
  1. Verify that PMD with negative entropy matches baseline NPG performance
  2. Run ES to learn a mirror map on a simple Grid-World and confirm improvement over negative entropy
  3. Test learned mirror map on a held-out Grid-World task to check generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What theoretical guarantees, if any, can be derived for PMD performance that accurately reflect the impact of mirror map choice on convergence rate and error floor?
- Basis in paper: [explicit] The authors note that existing theoretical analyses show only mild dependence on mirror map choice, yet their empirical results reveal significant performance differences. They explicitly state this gap between theoretical guarantees and actual performance needs exploration.
- Why unresolved: Current PMD theoretical frameworks rely on upper bounds that fail to capture the nuanced effects of different mirror maps on convergence behavior and error floors.
- What evidence would resolve it: Theoretical analysis showing how specific mirror map properties (e.g., curvature, Lipschitz constants) directly influence convergence rates and error floors in a way that matches empirical observations.

### Open Question 2
- Question: How do mirror maps that prevent large policy updates throughout training lead to better long-term performance, and can this relationship be quantified?
- Basis in paper: [explicit] The authors observe that learned mirror maps induce smaller policy update distances and fewer performance dips, particularly in initial iterations, and connect this to Theorem 2.2's quasi-monotonicity property.
- Why unresolved: While the authors demonstrate this empirically, the underlying mechanism linking policy update stability to final performance remains theoretical rather than quantitative.
- What evidence would resolve it: Mathematical framework quantifying the relationship between policy update magnitude, stability metrics, and long-term performance, validated across diverse environments.

### Open Question 3
- Question: Under what environmental conditions do mirror maps that assign zero probability to worst actions become detrimental versus beneficial?
- Basis in paper: [explicit] The authors note that learned mirror maps set probabilities of worst actions to zero, which could be problematic in exploration-heavy or credit-assignment-challenging environments, but observe it works well in their tested settings.
- Why unresolved: The authors hypothesize about potential issues but don't systematically test these mirror maps in environments specifically designed to challenge this behavior.
- What evidence would resolve it: Comparative experiments across environments with varying exploration requirements, noise levels, and credit assignment difficulties, showing when zero-weighting improves versus harms performance.

## Limitations

- The generalizability of learned mirror maps across diverse environments remains partially demonstrated, as experiments focus on discrete action spaces and limited continuous control tasks
- The parameterization choices (neural network for PMD, piecewise linear for AMPO) may constrain the space of discoverable improvements
- The evolutionary strategies optimization could get stuck in local optima depending on hyperparameter settings

## Confidence

- High confidence: The empirical demonstration that learned mirror maps outperform negative entropy in Grid-World and show generalization across tasks
- Medium confidence: The claim that small policy update distances lead to smoother value improvements, as this depends on specific environment dynamics and the validity of Theorem 2.2 in practice
- Medium confidence: The assertion that theoretical bounds don't reflect actual performance, given that only empirical evidence is provided without rigorous bounds on the gap

## Next Checks

1. Test learned mirror maps on environments with continuous action spaces beyond the Basic Control Suite to verify generalization claims
2. Conduct ablation studies varying the parameterization of mirror maps (e.g., deeper networks, different activation functions) to assess sensitivity to design choices
3. Compare learned mirror maps against theoretically motivated alternatives (e.g., Tsallis entropy) to better understand what properties make a mirror map effective