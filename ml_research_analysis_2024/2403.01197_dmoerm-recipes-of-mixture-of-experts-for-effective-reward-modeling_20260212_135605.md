---
ver: rpa2
title: 'DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling'
arxiv_id: '2403.01197'
source_url: https://arxiv.org/abs/2403.01197
tags:
- training
- reward
- data
- capability
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two key challenges in training reward models
  (RMs) for large language models (LLMs): 1) multi-task interference when training
  on diverse data categories, and 2) noise in human annotations due to low consistency
  rates (60-75%). To tackle these issues, the authors introduce a Double-Layer Mixture-of-Experts
  (DMoERM) architecture.'
---

# DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling

## Quick Facts
- arXiv ID: 2403.01197
- Source URL: https://arxiv.org/abs/2403.01197
- Authors: Shanghaoran Quan
- Reference count: 39
- Key outcome: DMoERM achieves 70.7% consistency with human preferences, outperforming state-of-the-art ensemble methods by 6-8 percentage points

## Executive Summary
This paper introduces DMoERM, a Double-Layer Mixture-of-Experts architecture for training effective reward models for large language models. The approach addresses two key challenges: multi-task interference from diverse training data and noise in human annotations. The outer sparse MoE layer routes inputs to task-specific models, while the inner dense MoE decomposes tasks into capability dimensions with LoRA experts. Experiments demonstrate superior performance compared to ensemble methods, achieving 70.7% consistency with human preferences.

## Method Summary
DMoERM employs a double-layer MoE architecture to address multi-task interference and annotation noise in reward modeling. The outer layer is a sparse MoE that routes inputs to task-specific inner models based on pre-trained task classification. The inner layer is a dense MoE that decomposes each task into capability dimensions, with LoRA experts fine-tuned on each dimension and an MLP for aggregation. This design enables specialized modeling for different tasks while mitigating the impact of noisy human preferences through capability decomposition.

## Key Results
- DMoERM achieves 70.7% consistency with human preferences, outperforming ensemble methods by 6-8 percentage points
- Superior optimization performance in both Best-of-N sampling and reinforcement learning experiments
- Successfully mitigates overoptimization while maintaining high preference alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The outer sparse MoE layer avoids multi-task interference by routing inputs to task-specific inner models.
- Mechanism: Inputs are classified into task categories and routed to corresponding expert models using a pre-trained router.
- Core assumption: Different tasks have distinct characteristics that benefit from specialized modeling.
- Evidence anchors: [abstract] "The outer layer MoE is a sparse model. After classifying an input into task categories, we route it to the corresponding inner layer task-specific model." [section 4.1] "We divide the input into five categories according to tasks... and train an MoE for each category."
- Break condition: If task categories are not well-defined or the router fails to accurately classify inputs, multi-task interference may persist.

### Mechanism 2
- Claim: The inner dense MoE layer decomposes tasks into capability dimensions to reduce the impact of noisy human annotations.
- Mechanism: Each task is decomposed into multiple capability points, and LoRA experts are fine-tuned on each capability point. An MLP aggregates the outputs.
- Core assumption: Human annotation noise can be reduced by focusing on specific capability dimensions rather than overall preferences.
- Evidence anchors: [abstract] "The inner layer MoE is a dense model. We decompose the specific task into multiple capability dimensions and individually fine-tune a LoRA expert on each one." [section 3.2] "We find that the consistency on capability points is significantly higher than the consistency of directly evaluating the overall preference."
- Break condition: If capability points are not well-defined or the aggregation by MLP is not effective, the benefits of decomposition may not be realized.

### Mechanism 3
- Claim: Using LoRA for fine-tuning capability experts is cost-effective and efficient.
- Mechanism: LoRA is used to fine-tune experts on each capability point, reducing computational costs compared to full fine-tuning.
- Core assumption: LoRA can effectively adapt the base model to specific capability points with minimal additional parameters.
- Evidence anchors: [abstract] "Considering that capability points are equivalent to a decomposition of tasks in a low dimensional space, using low-rank adaptation (LoRA) fine-tuning will be very suitable." [section 4.2.1] "In this work, we obtain RMti by performing LoRA fine-tuning on RMtbase."
- Break condition: If LoRA is not sufficient to capture the nuances of each capability point, full fine-tuning may be necessary, increasing costs.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: MoE allows the model to have task-specific and capability-specific experts, improving performance and interpretability.
  - Quick check question: How does MoE differ from a standard dense model, and why is it beneficial in this context?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA is used to efficiently fine-tune experts on each capability point without the computational cost of full fine-tuning.
  - Quick check question: What is LoRA, and how does it enable efficient adaptation of large models?

- Concept: Human annotation consistency and noise
  - Why needed here: Understanding the challenges of human annotation is crucial for appreciating why the inner MoE layer is beneficial.
  - Quick check question: Why is human annotation consistency typically only 60-75%, and how does this affect reward modeling?

## Architecture Onboarding

- Component map: Input -> Router (frozen gating network) -> Task-specific outer MoE -> Capability decomposition -> LoRA experts -> MLP aggregation -> Final reward output

- Critical path: 1. Input classification by router 2. Routing to task-specific expert 3. Decomposition into capability points 4. Expert scoring via LoRA 5. Aggregation by MLP 6. Final reward output

- Design tradeoffs:
  - Sparse vs. dense MoE: Sparse outer layer reduces computation but requires accurate routing
  - LoRA vs. full fine-tuning: LoRA is efficient but may be less expressive
  - Number of capability points: More points increase granularity but also complexity

- Failure signatures:
  - Poor routing accuracy leading to multi-task interference
  - Inadequate decomposition of tasks into capability points
  - Overfitting to capability points at the expense of overall performance
  - Aggregation by MLP not capturing interactions between capabilities

- First 3 experiments:
  1. Test the accuracy of the task router on a held-out set of inputs
  2. Evaluate the performance of individual LoRA experts on their respective capability points
  3. Assess the impact of different numbers of capability points on overall model performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas warrant further investigation:

### Open Question 1
- Question: How does DMoERM's performance scale with the number of capability points per task? Is there an optimal number that maximizes performance without overfitting?
- Basis in paper: Inferred from the empirical study section which shows different accuracy results for different capability points in Phase 2, but doesn't explore the impact of varying the number of capability points.
- Why unresolved: The paper only uses a fixed number of capability points for each task (e.g., 6 for roleplay) without exploring whether more or fewer capability points would yield better results.
- What evidence would resolve it: Experiments systematically varying the number of capability points per task and measuring the resulting performance, consistency rates, and training efficiency.

### Open Question 2
- Question: What is the impact of different routing strategies in the outer MoE layer? Would alternative methods like entropy-based routing or learned routing improve performance?
- Basis in paper: Inferred from the methodology section which uses a frozen top-1 gating network for routing, but doesn't explore alternative routing strategies.
- Why unresolved: The paper assumes a pre-trained router is sufficient but doesn't investigate whether more sophisticated routing mechanisms could improve task classification and overall performance.
- What evidence would resolve it: Comparative experiments using different routing strategies (top-k, entropy-based, learned routing) and measuring their impact on task classification accuracy and downstream reward modeling performance.

### Open Question 3
- Question: How sensitive is DMoERM to the quality of capability point labels obtained from the LLM API? Would higher-quality human annotations significantly improve performance?
- Basis in paper: Inferred from the methodology section which uses LLM API calls to obtain capability point labels, acknowledging this reduces costs but may introduce noise.
- Why unresolved: The paper doesn't investigate the relationship between label quality and model performance, or whether investing in higher-quality human annotations would yield better results.
- What evidence would resolve it: Controlled experiments comparing models trained with API-derived labels versus human-annotated labels, measuring the impact on consistency rates and optimization performance.

## Limitations
- Computational overhead: Training one inner MoE takes about 80 GPU hours, approximately 8 times longer than traditional RMs
- Limited generalizability: Performance on languages other than Chinese-English and domains beyond the five specified task categories is uncertain
- Dependency on API quality: The approach relies on LLM API calls for capability point labels, which may introduce noise and limit reproducibility

## Confidence
- **High Confidence**: The general architecture of using MoE for multi-task learning and the basic principle of capability decomposition for handling annotation noise are well-established concepts. The reported performance improvements over baseline ensemble methods are quantifiable.
- **Medium Confidence**: The specific implementation details of the double-layer MoE and the effectiveness of LoRA for capability-specific fine-tuning are plausible but not fully validated through extensive ablation studies or comparisons with alternative approaches.
- **Low Confidence**: The generalizability of the proposed approach to languages other than Chinese-English bilingual datasets and to domains beyond the five specified task categories is uncertain.

## Next Checks
1. **Router Validation**: Evaluate the task router's accuracy on a held-out set of inputs from diverse sources to assess its robustness and generalizability.
2. **Capability Point Robustness**: Conduct ablation studies to determine the impact of different numbers and definitions of capability points on overall model performance and consistency with human preferences.
3. **Alternative Aggregation Methods**: Compare the performance of the MLP aggregator with alternative methods, such as weighted averaging or attention mechanisms, to assess whether the MLP is the optimal choice for combining capability-specific expert outputs.