---
ver: rpa2
title: Efficient Training of Large Vision Models via Advanced Automated Progressive
  Learning
arxiv_id: '2410.00350'
source_url: https://arxiv.org/abs/2410.00350
tags:
- training
- learning
- progressive
- growth
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an advanced automated progressive learning (AutoProg)
  framework to improve the efficiency of training large vision models (LVMs) such
  as Vision Transformers and diffusion models. The key idea is to gradually increase
  model capacity during training, which reduces computational cost while maintaining
  performance.
---

# Efficient Training of Large Vision Models via Advanced Automated Progressive Learning

## Quick Facts
- arXiv ID: 2410.00350
- Source URL: https://arxiv.org/abs/2410.00350
- Reference count: 40
- Key outcome: AutoProg accelerates ViT pre-training by up to 1.85× on ImageNet and diffusion model fine-tuning by up to 2.86×, with comparable or higher accuracy.

## Executive Summary
This paper introduces an advanced automated progressive learning (AutoProg) framework designed to improve the efficiency of training large vision models (LVMs) such as Vision Transformers and diffusion models. The core innovation is to gradually increase model capacity during training, reducing computational cost while maintaining or even improving performance. The framework leverages momentum growth (MoGrow) for model expansion and a one-shot growth schedule search via an elastic supernet. Experimental results demonstrate significant speedups—up to 1.85× for ViT pre-training and 2.86× for diffusion model fine-tuning—while preserving or exceeding accuracy on standard benchmarks.

## Method Summary
AutoProg enables efficient training of large vision models by progressively expanding model capacity during training, rather than using a fixed architecture from the start. The framework introduces two key innovations: momentum growth (MoGrow), which smoothly increases model size using momentum-based parameter initialization, and a one-shot growth schedule search via an elastic supernet, which identifies optimal expansion schedules without exhaustive tuning. This approach reduces computational overhead while maintaining or improving model performance across diverse architectures and tasks.

## Key Results
- AutoProg accelerates ViT pre-training by up to 1.85× on ImageNet with comparable or higher accuracy.
- Diffusion model fine-tuning is sped up by up to 2.86× using AutoProg.
- The method is validated across diverse architectures (Vision Transformers, CNNs) and tasks, demonstrating robust scalability.

## Why This Works (Mechanism)
Progressive learning reduces the computational burden by starting with smaller models and gradually expanding capacity as training progresses. This approach allows early-stage training to focus on simpler patterns, while later stages refine more complex features as the model grows. Momentum growth (MoGrow) ensures smooth parameter initialization during expansion, preventing abrupt changes that could destabilize training. The one-shot growth schedule search via an elastic supernet efficiently identifies optimal expansion timings without exhaustive grid search, making the process both effective and scalable.

## Foundational Learning
- **Progressive learning**: Gradually increasing model capacity during training to reduce early-stage computational cost. *Why needed*: Large models are expensive to train from scratch; progressive learning reduces early-stage overhead. *Quick check*: Compare training time and accuracy with and without progressive expansion.
- **Momentum growth (MoGrow)**: Smoothly scales model parameters during expansion using momentum-based initialization. *Why needed*: Prevents abrupt changes that could destabilize training during model growth. *Quick check*: Monitor training stability and convergence when expanding model capacity.
- **Elastic supernet**: A unified architecture that supports multiple model sizes for efficient growth schedule search. *Why needed*: Enables one-shot schedule search without exhaustive tuning. *Quick check*: Verify that the supernet can represent all candidate model sizes.

## Architecture Onboarding

### Component Map
Data → Supernet → MoGrow → Progressive Training Loop → Final Model

### Critical Path
The critical path is: Data → Supernet (growth schedule search) → MoGrow (smooth expansion) → Progressive Training (capacity increase) → Final Model (deployment).

### Design Tradeoffs
- **Supernet complexity vs. search efficiency**: A larger supernet enables more growth options but increases search time.
- **MoGrow smoothness vs. training speed**: Smoother expansions improve stability but may slightly slow growth.
- **Progressive vs. static training**: Progressive training saves compute but requires careful schedule tuning.

### Failure Signatures
- **Instability during expansion**: MoGrow parameters may not initialize smoothly, causing training collapse.
- **Suboptimal schedules**: Poor growth timing can lead to underfitting or wasted compute.
- **Supernet collapse**: If the supernet cannot represent all target architectures, growth schedules may be invalid.

### First Experiments
1. Validate MoGrow on a small ViT with controlled capacity expansion.
2. Test supernet search on a toy dataset to confirm schedule validity.
3. Compare progressive vs. static training on a lightweight CNN to measure compute savings.

## Open Questions the Paper Calls Out
None.

## Limitations
- Reliance on controlled experimental settings (ImageNet, CIFAR-100) may limit generalization to complex real-world datasets or imbalanced distributions.
- Performance gains are architecture-dependent, with MoGrow showing stronger benefits for ViTs than CNNs, suggesting limited universality.
- The one-shot growth schedule search may miss optimal configurations compared to exhaustive search, especially for novel architectures.

## Confidence
- **High confidence**: Computational efficiency improvements (1.85× for ViT pre-training, 2.86× for diffusion model fine-tuning) and empirical validation across diverse tasks and architectures.
- **Medium confidence**: Generalization claims to other LVMs and real-world datasets, as experiments are limited to standard benchmarks.
- **Medium confidence**: Scalability assertions for extremely large models, as results are bounded by tested parameter ranges.

## Next Checks
1. Test AutoProg on highly imbalanced or noisy real-world datasets to assess robustness beyond controlled benchmarks.
2. Conduct ablation studies isolating MoGrow's contribution versus the progressive learning framework to clarify architectural dependencies.
3. Evaluate full training pipelines for diffusion models (not just fine-tuning) to confirm scalability claims.