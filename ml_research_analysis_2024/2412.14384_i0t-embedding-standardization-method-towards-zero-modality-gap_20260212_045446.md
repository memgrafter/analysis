---
ver: rpa2
title: 'I0T: Embedding Standardization Method Towards Zero Modality Gap'
arxiv_id: '2412.14384'
source_url: https://arxiv.org/abs/2412.14384
tags:
- modality
- image
- text
- clip
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method (I0T) to reduce the modality gap in
  vision-language models like CLIP. The authors identify that each encoder independently
  learns modality-specific characteristics, causing image and text embeddings to occupy
  different regions in latent space.
---

# I0T: Embedding Standardization Method Towards Zero Modality Gap

## Quick Facts
- arXiv ID: 2412.14384
- Source URL: https://arxiv.org/abs/2412.14384
- Reference count: 37
- Key outcome: I0Tpost achieves state-of-the-art image-text retrieval (73.3% I2T, 76.3% T2I on Flickr30k) while reducing modality gap to near zero (centroid distance 0.0102, linear separability 0.5374)

## Executive Summary
This paper addresses the modality gap problem in vision-language models like CLIP, where image and text embeddings occupy different regions in latent space due to modality-specific characteristics learned by each encoder independently. The authors propose I0T, a two-stage framework that first enhances semantic representations via fine-tuning and then reduces the modality gap through embedding standardization. The post-hoc method (I0Tpost) standardizes normalized embeddings by subtracting modality-specific mean vectors and re-normalizing, achieving near-zero modality gap and state-of-the-art retrieval performance. The trainable method (I0Tasync) uses separate batch normalization layers to learn modality-specific characteristics, reducing the gap to a moderate level while maintaining competitive performance.

## Method Summary
I0T consists of a two-stage framework: first, CLIP is fine-tuned using Long-CLIP-only with cyclic losses to enhance semantic representations; second, the modality gap is reduced through either post-hoc standardization (I0Tpost) or trainable batch normalization layers (I0Tasync). I0Tpost works by subtracting modality-specific mean vectors from normalized embeddings and re-normalizing, while I0Tasync adds independent batch normalization layers to each encoder that learn modality-specific statistics during training. The methods are evaluated on image-text retrieval, classification, and image captioning tasks across multiple datasets.

## Key Results
- I0Tpost achieves state-of-the-art retrieval performance: 73.3% I2T and 76.3% T2I on Flickr30k
- I0Tpost reduces modality gap to near zero (centroid distance 0.0102, linear separability 0.5374)
- I0Tasync achieves competitive retrieval performance (72.5% I2T, 73.8% T2I) with moderate modality gap reduction (0.4795, 0.9960)
- I0T-S, the post-hoc method applied to image captioning evaluation, outperforms CLIPScore and PAC-S in correlation with human judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's image and text embeddings occupy different regions in latent space because each encoder independently learns modality-specific characteristics.
- Mechanism: Each encoder develops distinct activation patterns (e.g., negative peaks at dimension 93 for images, positive peaks at dimensions 134 and 313 for texts) that contribute to the modality gap.
- Core assumption: The modality gap arises from encoder-specific activation patterns rather than intrinsic differences between image and text data.
- Evidence anchors: [abstract] "we discover that this phenomenon is linked to the modality-specific characteristic that each image/text encoder independently possesses"; [section 3.2] "we find the actual attributing factor of the modality gap; CLIP inadvertently learns the inherent characteristic of each modality"

### Mechanism 2
- Claim: Standardizing normalized embeddings by subtracting modality-specific mean vectors and re-normalizing reduces the modality gap to near zero.
- Mechanism: I0Tpost removes encoder-specific activation patterns by centering embeddings around zero and normalizing, eliminating the statistical divergence between modalities.
- Core assumption: The modality-specific characteristics manifest as consistent mean activation patterns across dimensions that can be removed without losing semantic information.
- Evidence anchors: [section 3.3] "We now demonstrate how these peak activations in the normalized image and text embeddings prevent the cosine similarity from reaching high values"; [section 4.2] "I0Tpost offers a more explainable image captioning evaluation metric than CLIPScore by assigning a similar range of scores for across different modalities"

### Mechanism 3
- Claim: Adding independent batch normalization layers for each encoder allows the model to learn modality-specific characteristics and reduce the modality gap during training.
- Mechanism: I0Tasync uses separate BN layers to learn running means and variances for each modality, then applies these learned statistics to align embeddings without affecting semantic encoding.
- Core assumption: Batch normalization can learn modality-specific statistics that, when applied, align embeddings while preserving semantic information.
- Evidence anchors: [section 4.2] "The key point of our I0Tasync method is to add an independent batch normalization (BN) layers, BNimg and BNtxt for each encoder"; [section 5.2] "I0Tasync reduces this gap to a moderate level with CD and LS scores of 0.4795 and 0.9960"

## Foundational Learning

- Concept: Contrastive Language-Image Pretraining (CLIP)
  - Why needed here: CLIP forms the foundation that I0T modifies to reduce modality gap
  - Quick check question: What is the primary objective of CLIP's contrastive learning approach?

- Concept: Modality gap and its quantification
  - Why needed here: Understanding how to measure and characterize the gap is essential for evaluating I0T's effectiveness
  - Quick check question: How do centroid distance and linear separability metrics differ in measuring modality gap?

- Concept: Batch normalization mechanics
  - Why needed here: I0Tasync relies on BN layers to learn and apply modality-specific statistics
  - Quick check question: What is the difference between batch normalization during training vs. inference?

## Architecture Onboarding

- Component map: Image encoder → BN layer → embedding → similarity calculation; Text encoder → BN layer → embedding → similarity calculation
- Critical path: Image/text input → encoder → optional BN layer (I0Tasync) or standardization (I0Tpost) → embedding → similarity computation
- Design tradeoffs: I0Tpost is training-free but requires test data distribution; I0Tasync learns modality alignment but adds parameters and training complexity
- Failure signatures: If modality gap reduction hurts downstream performance, check if semantic information is being lost; if post-hoc method fails, check if test data distribution differs significantly from training
- First 3 experiments:
  1. Implement I0Tpost on frozen CLIP and measure modality gap reduction on Flickr30k
  2. Train I0Tasync with varying batch sizes to find optimal configuration
  3. Compare retrieval performance of I0T methods against baseline CLIP on multiple datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between modality gap reduction and downstream task performance improvement?
- Basis in paper: [explicit] The paper states "We emphasize that there is no direct causal relationship between the modality gap and downstream performances" and discusses ongoing debates about this relationship
- Why unresolved: The paper shows I0Tpost with lowest modality gap achieves highest retrieval performance, but this doesn't always translate to highest classification/correlation task performances, suggesting complex interactions
- What evidence would resolve it: Systematic experiments varying both modality gap and semantic representation quality independently, measuring downstream performance across multiple tasks, would clarify the relationship

### Open Question 2
- Question: How does I0Tasync's performance scale with different batch sizes and what is the optimal batch size configuration?
- Basis in paper: [explicit] The paper shows "variations in batch sizes have a minimal impact on training outcomes" but also provides batch size experiments
- Why unresolved: While the paper shows batch size variations don't significantly affect performance, it doesn't explore whether there's an optimal batch size for specific downstream tasks or different model architectures
- What evidence would resolve it: Extensive experiments testing different batch sizes (beyond 32, 64, 128, 256) across multiple tasks and model architectures would identify optimal configurations

### Open Question 3
- Question: How well would I0T's methodology generalize to other modalities beyond image-text (e.g., audio-text, video-text)?
- Basis in paper: [inferred] The paper mentions this as a limitation, stating "we leave it as a future study to explore different learning methods and additional modalities"
- Why unresolved: The paper only validates I0T on image-text pairs, but the methodology of addressing modality-specific characteristics through standardization could potentially apply to other multimodal scenarios
- What evidence would resolve it: Applying I0T to other modality pairs (audio-text, video-text) and measuring modality gap reduction and task performance would demonstrate generalizability

## Limitations

- The post-hoc method (I0Tpost) requires access to test data distributions for standardization, which may not be available in deployment scenarios
- Limited ablation studies on whether semantic content of embeddings is preserved after standardization
- Correlation improvements for captioning evaluation need validation on more diverse captioning datasets beyond Flickr8k

## Confidence

- **High Confidence**: The identification of modality-specific activation patterns in CLIP embeddings is well-supported by the evidence
- **Medium Confidence**: The claim that I0Tpost achieves state-of-the-art retrieval performance while reducing modality gap to near zero is supported by the presented results
- **Low Confidence**: The assertion that I0T-S serves as a superior automatic evaluation metric for image captioning requires more extensive validation

## Next Checks

1. **Semantic Preservation Test**: Evaluate whether I0Tpost's standardization preserves semantic information by testing if zero-shot classification accuracy on ImageNet remains comparable to baseline CLIP after modality gap reduction

2. **Generalization Study**: Apply I0T methods to other vision-language models (e.g., ALBEF, CLIP variants with different architectures) to verify if the approach generalizes beyond the specific CLIP-B/32 configuration tested

3. **Robustness Analysis**: Test I0Tpost's performance when the test data distribution differs from training data (e.g., domain shift scenarios) to quantify how sensitive the post-hoc standardization is to distribution mismatches