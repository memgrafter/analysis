---
ver: rpa2
title: Controlled Training Data Generation with Diffusion Models
arxiv_id: '2403.15309'
source_url: https://arxiv.org/abs/2403.15309
tags:
- prompts
- adversarial
- data
- image
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of generating training data
  that is both useful for improving a given supervised model and relevant to a specific
  target distribution. To achieve this, the authors propose a closed-loop system with
  two feedback mechanisms: model feedback that generates adversarial prompts to expose
  the model''s vulnerabilities, and target distribution feedback that guides the generation
  process towards a desirable distribution using CLIP guidance.'
---

# Controlled Training Data Generation with Diffusion Models

## Quick Facts
- **arXiv ID**: 2403.15309
- **Source URL**: https://arxiv.org/abs/2403.15309
- **Reference count**: 40
- **Primary result**: Proposed method outperforms open-loop approaches in sample efficiency and model performance across multiple tasks and distribution shifts

## Executive Summary
This paper addresses the challenge of generating training data that is both useful for improving a given supervised model and relevant to a specific target distribution. The authors propose a closed-loop system that uses two feedback mechanisms: model feedback to generate adversarial prompts that expose model vulnerabilities, and target distribution feedback using CLIP guidance to steer generations toward desirable distributions. By combining both mechanisms, the method produces "Guided Adversarial Prompts" that generate training data which both fools the model and fits the target distribution.

## Method Summary
The method involves optimizing text prompts via gradient descent to maximize model loss and match target distribution using CLIP guidance. For classification tasks, inpainting conditioning is used, while ControlNet depth conditioning is employed for depth estimation. The optimized prompts are then used to generate aligned image-label pairs, and the supervised model is fine-tuned on this synthetic data. The approach combines adversarial prompt optimization with CLIP-based target distribution guidance in a joint optimization framework.

## Key Results
- Demonstrates superior performance over open-loop methods in sample efficiency and model accuracy
- Effective across multiple tasks including image classification and depth estimation
- Successfully handles various distribution shifts such as spurious correlations and unseen domains
- Shows robustness to different types of distribution shifts (fog, blur, domain changes)

## Why This Works (Mechanism)

### Mechanism 1
Model feedback via adversarial prompts finds text embeddings that generate images maximizing model loss, exposing model vulnerabilities. This uses gradient-based optimization in the continuous embedding space of text prompts to maximize the supervised loss of the target model. Core assumption: The text-to-image diffusion model's embedding space is differentiable and allows gradients to propagate through the generation process.

### Mechanism 2
Target distribution feedback via CLIP guidance steers generations toward a specific target distribution. This uses optimization of prompt embeddings to minimize distance in CLIP embedding space between generated images and target distribution representations. Core assumption: CLIP embeddings capture meaningful semantic similarity between images and text descriptions that aligns with human perception of target distribution shifts.

### Mechanism 3
Combining model and target distribution feedback produces training data that both exposes model vulnerabilities and matches target distribution characteristics. This uses joint optimization that simultaneously maximizes model loss and minimizes CLIP distance to target distribution. Core assumption: The two feedback mechanisms are complementary and their combined optimization finds prompts that satisfy both objectives effectively.

## Foundational Learning

- **Text-to-image diffusion models and conditioning mechanisms**: Why needed - The entire method relies on being able to control text-to-image generation through prompt optimization. Quick check - Can you explain how ControlNet extends the conditioning of Stable Diffusion to accept depth maps?

- **Adversarial optimization and its application to prompt generation**: Why needed - The model feedback mechanism uses adversarial optimization to find prompts that maximize model loss. Quick check - How does the adversarial optimization differ from standard gradient-based attacks on images?

- **CLIP embeddings and their use for semantic similarity**: Why needed - The target distribution feedback mechanism uses CLIP embeddings to measure similarity between generated images and target distributions. Quick check - What is the difference between using cosine similarity vs L2 distance in CLIP embedding space?

## Architecture Onboarding

- **Component map**: Text-to-image diffusion model (Stable Diffusion) -> ControlNet/inpainting mechanism -> CLIP model -> Adam optimizer -> Pre-trained supervised model

- **Critical path**: 1. Initialize random prompt embeddings 2. Generate images using diffusion model with task conditioning 3. Compute model loss and CLIP guidance loss 4. Backpropagate through generation to update prompt embeddings 5. Repeat until convergence 6. Generate final training data using optimized prompts

- **Design tradeoffs**: Number of denoising steps during optimization vs quality vs computational cost; number of placeholder tokens in prompts vs expressiveness vs optimization stability; choice of loss function for adversarial optimization (cross-entropy vs entropy); use of SDEdit vs direct generation for alignment preservation

- **Failure signatures**: Prompt optimization diverges or produces unrealistic generations; generated images fail to match task conditioning; model feedback produces prompts generating images from wrong classes; target distribution guidance produces images that don't resemble target distribution

- **First 3 experiments**: 1. Test single iteration adversarial optimization on simple depth estimation task to verify gradients flow correctly 2. Validate CLIP guidance by generating images for known distribution shift (e.g., fog) and visually inspecting results 3. Combine both feedback mechanisms on small dataset to verify joint optimization converges and produces useful training data

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method scale to larger and more diverse target distributions beyond the controlled shifts evaluated in the experiments? The authors mention that target distribution guidance can work with textual descriptions or unlabeled images, but don't provide extensive evaluation on highly diverse or complex target distributions.

### Open Question 2
What is the impact of using different text-to-image diffusion models (e.g., newer or larger models) on the effectiveness of the adversarial and guided adversarial prompts? The authors use Stable Diffusion v1.5 but don't explore how choice of diffusion model affects quality or diversity of generated training data.

### Open Question 3
How does the method perform when the target distribution is significantly different from the training data distribution of the generative model? The authors assume the diffusion model can generalize to novel target distributions, but don't explicitly test scenarios where the target is highly dissimilar to the model's training distribution.

## Limitations
- Heavy reliance on differentiability of diffusion model's embedding space and effectiveness of CLIP embeddings
- Significant computational resources required for gradient-based optimization through diffusion generation
- Quality depends critically on quality and diversity of initial supervised model
- May require substantial hyperparameter tuning for different tasks and datasets

## Confidence

**High Confidence Claims**:
- General framework of using adversarial prompts combined with target distribution guidance is sound and novel
- Method demonstrates improvements over open-loop approaches in sample efficiency and model performance
- Optimization objective formulation (combining adversarial loss and CLIP guidance) is mathematically coherent

**Medium Confidence Claims**:
- Specific effectiveness of CLIP guidance for steering toward target distributions
- Generalizability of approach across different types of distribution shifts
- Optimal hyperparameter settings for different tasks and datasets

**Low Confidence Claims**:
- Claim that this approach is superior to all other data augmentation methods for distribution shifts
- Robustness of method to poor initial supervised models
- Scalability of approach to very large datasets or complex distribution shifts

## Next Checks

**Validation Check 1**: Run single iteration of adversarial optimization on simple depth estimation task with small network to verify gradients flow correctly through generation process. Monitor loss and ensure it increases as expected.

**Validation Check 2**: Generate images for known distribution shift (e.g., fog, rain) using CLIP guidance alone and visually inspect results. Compute CLIP similarity scores between generated and reference target distribution images.

**Validation Check 3**: Combine both feedback mechanisms on small dataset (e.g., MNIST with digit rotation shift) and monitor joint optimization process. Plot both adversarial loss and CLIP guidance loss over iterations to ensure they converge together rather than competing.