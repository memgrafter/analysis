---
ver: rpa2
title: Evaluating the fairness of task-adaptive pretraining on unlabeled test data
  before few-shot text classification
arxiv_id: '2410.00179'
source_url: https://arxiv.org/abs/2410.00179
tags:
- text
- test
- unlabeled
- pretraining
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether pretraining language models on
  unlabeled test set text introduces bias in few-shot text classification benchmarks.
  Through controlled experiments on 25 classification tasks and three language models
  (BERT, GPT-2, and Mistral 7B), the study finds no evidence of evaluation bias from
  pretraining on test set data versus independently drawn text.
---

# Evaluating the fairness of task-adaptive pretraining on unlabeled test data before few-shot text classification

## Quick Facts
- arXiv ID: 2410.00179
- Source URL: https://arxiv.org/abs/2410.00179
- Authors: Kush Dubey
- Reference count: 39
- Primary result: Pretraining on unlabeled test data shows no significant bias in few-shot text classification performance

## Executive Summary
This paper investigates whether pretraining language models on unlabeled test set text introduces bias in few-shot text classification benchmarks. Through controlled experiments on 25 classification tasks and three language models (BERT, GPT-2, and Mistral 7B), the study finds no evidence of evaluation bias from pretraining on test set data versus independently drawn text. While pretraining consistently improves performance, the boost from using test set text is statistically indistinguishable from zero. The analysis also demonstrates the importance of repeated subsampling in few-shot learning studies and recommends that benchmarks include multiple training folds to account for training variance.

## Method Summary
The study conducts controlled few-shot and zero-shot experiments across 25 classification tasks using BERT, GPT-2, and Mistral 7B models. For each task, data is subsampled into train, test, and extra (unlabeled) sets, with pretraining performed on either the extra set or the test set. The process is repeated multiple times to account for variance, and hierarchical Bayesian modeling is used to analyze accuracy differences. The key comparison is between models pretrained on test set text versus those pretrained on independently drawn text, measuring any resulting evaluation bias.

## Key Results
- Pretraining consistently improves performance across all models and tasks
- No statistically significant evaluation bias detected when pretraining on test set text
- Repeated subsampling reveals important variance in few-shot learning results
- Hierarchical Bayesian modeling provides robust estimates of evaluation bias
- Study demonstrates importance of multiple training folds in benchmark design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining on test set text does not introduce evaluation bias because the model learns general task patterns rather than memorizing specific test examples.
- Mechanism: When a model is pretrained on unlabeled test set text, it adapts to the task domain but does not overfit to individual test instances. The pretraining objective (masked language modeling for BERT or causal language modeling for GPT-2) focuses on general language understanding rather than memorizing specific sequences.
- Core assumption: The pretraining process is sufficiently general and does not memorize specific sequences or patterns that would lead to overfitting on the test set.
- Evidence anchors:
  - [abstract] "Controlled few-shot and zero-shot experiments on 25 classification tasks and 3 language models—BERT, GPT-2, and Mistral 7B—do not find evidence of overoptimism."
  - [section 5] "Table 1 contains means of these differences for each configuration of the experiment. It roughly suggests that while pretraining is consistently beneficial, pretraining on unlabeled test set text does not bias test set performance one way or the other."
  - [corpus] Weak evidence - corpus contains related work on pretraining but does not directly address test set contamination bias.
- Break condition: If the pretraining process involved memorizing specific sequences or if the test set text was highly repetitive and specific to certain classes, then overfitting could occur.

### Mechanism 2
- Claim: Repeated subsampling reveals the true variance in few-shot learning, making evaluation bias detection more reliable.
- Mechanism: By subsampling multiple train/test splits from the same dataset, the experiment accounts for variance introduced by different training data selections. This allows for more accurate estimation of the evaluation bias by averaging across multiple scenarios.
- Core assumption: The variance introduced by different train/test splits is significant enough to impact the evaluation bias measurement, and averaging across multiple splits provides a more robust estimate.
- Evidence anchors:
  - [abstract] "Furthermore, we demonstrate the importance of repeated subsampling when studying few-shot text classification, and recommend that few-shot learning benchmarks include multiple training folds."
  - [section 4.4] "A potentially important source of variation in this experiment is the particular subsamples, i.e., the particular realizations of extra, train, and test for a given classification task. To expose this variation, the experiment procedure is repeated tens of times for each task."
  - [corpus] Weak evidence - corpus contains related work on pretraining but does not directly address the importance of repeated subsampling.
- Break condition: If the dataset is too small or if the task is too simple, the variance introduced by different train/test splits may be negligible, making repeated subsampling less important.

### Mechanism 3
- Claim: Hierarchical Bayesian modeling accurately estimates the evaluation bias by accounting for the hierarchical structure of the data.
- Mechanism: The hierarchical model accounts for the nested structure of the data (language models within tasks within subsamples) and estimates the overall evaluation bias while allowing for task-specific effects. This provides a more accurate estimate than simple averaging.
- Core assumption: The hierarchical model structure accurately captures the dependencies in the data, and the priors are reasonable for the problem domain.
- Evidence anchors:
  - [section 6.1] "Specifically, for each LM type (indexed by i = 1, 2 for BERT and GPT-2), each classification task (indexed by j = 1, 2, ..., 25), each of their subsamples (indexed by k = 1, 2, ..., 20 for n = 500, for example), and a control and treatment (indexed by l = 0, 1), the number of correct predictions is modeled."
  - [section 6.1] "The model is fit using Markov Chain Monte Carlo, using the interface provided by the bambi package."
  - [corpus] Weak evidence - corpus contains related work on pretraining but does not directly address hierarchical Bayesian modeling for evaluation bias estimation.
- Break condition: If the hierarchical model assumptions are violated (e.g., if there are strong interactions between factors that are not captured by the model), then the estimates may be biased.

## Foundational Learning

- Concept: Understanding the difference between supervised and unsupervised learning
  - Why needed here: The paper distinguishes between pretraining on labeled test data (which would be biased) and pretraining on unlabeled test data (which may or may not be biased).
  - Quick check question: Why is pretraining on labeled test data considered unfair while pretraining on unlabeled test data may be fair?

- Concept: Familiarity with few-shot learning benchmarks and their evaluation methodologies
  - Why needed here: The paper discusses specific few-shot learning benchmarks like RAFT and their approach to using unlabeled test data.
  - Quick check question: What is the key difference between how RAFT and other benchmarks handle unlabeled test data?

- Concept: Understanding of statistical hypothesis testing and confidence intervals
  - Why needed here: The paper uses permutation testing and hierarchical Bayesian modeling to assess the significance of evaluation bias.
  - Quick check question: Why might simple averaging of evaluation bias across tasks be misleading, and how does hierarchical modeling address this issue?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Evaluation -> Analysis -> Reproducibility
- Critical path:
  1. Load dataset and subsample train/test/extra sets
  2. Pretrain model on extra or test data
  3. Finetune model on train data
  4. Evaluate model on test data
  5. Repeat steps 1-4 for multiple subsamples
  6. Analyze results using hierarchical Bayesian modeling
- Design tradeoffs:
  - Using more subsamples increases computational cost but provides more robust estimates
  - Using simpler models for analysis may miss important effects but is computationally cheaper
  - Including more tasks increases generalizability but may introduce more variance
- Failure signatures:
  - High variance in evaluation bias across subsamples may indicate that the task is too difficult or that the model is not generalizing well
  - Consistent positive or negative evaluation bias across all tasks may indicate a systematic issue with the pretraining process
  - Low variance in accuracy differences may indicate that the subsampling process is not introducing enough variation
- First 3 experiments:
  1. Run the experiment with BERT on a single task with n=500 and 20 subsamples to verify that the basic pipeline works
  2. Run the experiment with GPT-2 on the same task to verify that the results are consistent across models
  3. Run the experiment with Mistral 7B in the zero-shot setting to verify that the zero-shot results are consistent with the few-shot results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pretraining on unlabeled test set text consistently bias performance in few-shot learning tasks across different model architectures and task types?
- Basis in paper: [explicit] The paper states that pretraining on unlabeled test set text does not result in a consistent or significant evaluation bias across 25 classification tasks and two types of language models (BERT and GPT-2).
- Why unresolved: While the study found no consistent bias, the analysis only covers a specific set of tasks and models. The possibility remains that certain types of tasks or newer model architectures might exhibit different behaviors.
- What evidence would resolve it: Conducting similar experiments across a wider variety of tasks (e.g., multi-label classification, sequence labeling) and newer model architectures (e.g., transformer variants like XLNet, RoBERTa) would provide more comprehensive insights into the generalizability of these findings.

### Open Question 2
- Question: How does the choice of pretraining objective (e.g., masked language modeling vs. causal language modeling) influence the potential for evaluation bias when using unlabeled test set data?
- Basis in paper: [inferred] The paper mentions different pretraining objectives for BERT (masked language modeling) and GPT-2 (causal language modeling) but does not analyze their impact on evaluation bias.
- Why unresolved: The study uses different objectives for BERT and GPT-2 but does not isolate the effect of the pretraining objective on evaluation bias, leaving this aspect unexplored.
- What evidence would resolve it: Conducting controlled experiments where the same model architecture is trained with different pretraining objectives on the same tasks would clarify the influence of the pretraining objective on evaluation bias.

### Open Question 3
- Question: Is there an interaction effect between the size of the training set and the potential for evaluation bias when pretraining on unlabeled test set text?
- Basis in paper: [inferred] The paper varies the size of the training set (m = 50 or m = 100) but does not specifically analyze interaction effects with evaluation bias.
- Why unresolved: While the paper tests different training set sizes, it does not explicitly investigate whether smaller or larger training sets influence the magnitude or direction of evaluation bias differently.
- What evidence would resolve it: Analyzing evaluation bias across a broader range of training set sizes and conducting interaction analysis between training set size and evaluation bias would provide insights into any potential interaction effects.

## Limitations

- Computational expense limited the number of subsamples to 20 for n=500 and 5 for n=16
- Hierarchical Bayesian model relies on assumptions about data structure that may not capture all interactions
- Study only tested three specific language models, limiting generalizability to other architectures

## Confidence

**High Confidence:** The finding that pretraining consistently improves performance is well-supported across all three language models and multiple tasks. The lack of statistically significant differences between pretraining on test set text versus independently drawn text is also strongly supported by the permutation testing and hierarchical modeling results.

**Medium Confidence:** The recommendation that few-shot learning benchmarks include multiple training folds is well-reasoned but may be less generalizable to domains where data collection is extremely expensive or where tasks have very different characteristics that make multiple folds impractical.

**Low Confidence:** The assertion that the lack of evaluation bias extends to all possible pretraining scenarios is not fully supported, as the study only tested three specific language models and did not explore the full space of possible pretraining strategies and model architectures.

## Next Checks

1. **Extended Subsampling:** Replicate the experiment with a higher number of subsamples (e.g., 50-100) for the n=500 condition to verify that the current findings are robust and to assess whether the variance estimates are stable.

2. **Cross-Domain Validation:** Apply the same experimental framework to text classification tasks from different domains (e.g., biomedical, legal, or financial text) to test whether the findings generalize beyond the current dataset selection.

3. **Alternative Model Architectures:** Test the experimental protocol with additional language model architectures, including encoder-decoder models and models with different pretraining objectives, to determine whether the lack of evaluation bias is consistent across a broader range of pretraining approaches.