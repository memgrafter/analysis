---
ver: rpa2
title: 'MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions
  in Frozen Image-to-Video Diffusion Model'
arxiv_id: '2405.20222'
source_url: https://arxiv.org/abs/2405.20222
tags:
- motion
- video
- image
- animation
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MOFA-Video is a method for controllable image animation that generates
  videos from single images using various controllable signals such as human landmarks,
  manual trajectories, and provided videos. Unlike previous methods that work on specific
  motion domains or have weak control abilities, MOFA-Video uses domain-aware motion
  field adapters (MOFA-Adapters) to control generated motions in a frozen image-to-video
  diffusion model.
---

# MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model

## Quick Facts
- **arXiv ID**: 2405.20222
- **Source URL**: https://arxiv.org/abs/2405.20222
- **Reference count**: 40
- **Key outcome**: Controllable image animation using domain-aware motion field adapters in a frozen image-to-video diffusion model

## Executive Summary
MOFA-Video introduces a novel framework for controllable image animation that generates videos from single images using various controllable signals such as human landmarks, manual trajectories, and provided videos. Unlike previous methods that work on specific motion domains or have weak control abilities, MOFA-Video uses domain-aware motion field adapters (MOFA-Adapters) to control generated motions in a frozen image-to-video diffusion model. The adapters generate dense motion flows from sparse control conditions and warp multi-scale image features for stable video generation. MOFA-Video is trained on different motion domains individually and can combine adapters for more complex animations.

## Method Summary
MOFA-Video works by first generating dense motion flows from sparse control conditions using a sparse-to-dense motion generation network. The multi-scale features of the given image are then warped as guided features for stable video diffusion generation. The method trains domain-aware MOFA-Adapters separately for different motion domains (trajectories, facial landmarks) and combines them for more complex animations. The framework uses a periodic sampling strategy to generate longer videos than the base SVD's frame limit by reusing intermediate latents and averaging overlapping segments.

## Key Results
- Superior performance compared to state-of-the-art techniques in trajectory-based and portrait image animation
- Effective combination of multiple motion adapters for complex animations across different domains
- Successful extension of video length beyond SVD's default limit using periodic sampling strategy

## Why This Works (Mechanism)

### Mechanism 1
Sparse motion hints (e.g., trajectories or keypoints) are converted to dense motion flows, which then warp multi-scale features to guide video diffusion generation. The dense motion field provides spatial correspondence to align warped features with desired motion in output video. Break condition: If S2D network fails to produce accurate dense motion fields, warped features will misalign, causing artifacts or wrong object motion.

### Mechanism 2
Training MOFA-Adapters separately per motion domain and then combining them allows fine-grained control over different object parts without retraining. Motion domains are assumed to be largely independent in their spatial influence, so masking and merging warped features yields coherent combined control. Break condition: If motion domains overlap spatially in ways not captured by simple masks, merged features could cause interference and motion conflicts.

### Mechanism 3
Periodic sampling strategy enables generation of longer videos than base SVD's frame limit by reusing intermediate latents and averaging overlapping segments. Averaging latents for overlapping frames provides temporal consistency to hide stitching artifacts. Break condition: If averaging across groups fails to align features temporally, visible flickering or ghosting will appear at segment boundaries.

## Foundational Learning

- **Dense optical flow estimation**: The sparse-to-dense network relies on optical flow as a dense motion representation; understanding its generation and properties is essential for debugging S2D outputs. Quick check: What is the difference between forward and backward optical flow, and which does the S2D network use in this work?

- **Multi-scale feature warping in convolutional networks**: MOFA-Adapter warps features at multiple scales before injecting them into SVD; knowing how warping interacts with receptive fields and stride is critical for tuning. Quick check: If you warp a feature map at stride 2, how does that affect the spatial resolution of the warped output compared to the input?

- **Latent diffusion model conditioning**: The warped features are added to the SVD decoder as conditional signals; understanding how conditioning alters the denoising process is necessary for diagnosing generation quality. Quick check: In a latent diffusion model, at what stage of the UNet is external conditioning (like warped features) typically applied?

## Architecture Onboarding

- **Component map**: Sparse motion input → S2D network → Dense optical flow → Reference image encoder → Multi-scale features → Warping operation → Warped features → Fusion encoder → SVD decoder → Video output

- **Critical path**: Sparse motion → S2D → Warping → Feature fusion → SVD decoder → Output video

- **Design tradeoffs**: S2D network depth vs. inference speed; feature resolution at each fusion level vs. control granularity; number of overlapping frames in periodic sampling vs. temporal smoothness

- **Failure signatures**: Motion not following input hints (S2D output wrong or warped features misaligned); visual artifacts (warping introduces holes or incorrect interpolation); flickering in longer videos (periodic sampling averaging insufficient)

- **First 3 experiments**: 1) Generate short video with simple trajectory input and visualize S2D network output; 2) Disable warping step and observe if motion guidance disappears; 3) Test periodic sampling with minimal overlap to check for flickering, then increase overlap until artifacts disappear

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the MOFA-Adapter framework be extended to handle more complex motion domains beyond current ones? The paper mentions possibility of training multiple adapters for different domains but only demonstrates limited set. Evidence needed: Training and evaluating on wider range of motion domains like animal movements or complex object interactions.

- **Open Question 2**: How does performance scale with length of generated videos? The paper mentions periodic sampling strategy but lacks comprehensive quantitative evaluation. Evidence needed: Thorough quantitative evaluation on videos of varying lengths including visual quality, temporal consistency, and computational efficiency metrics.

- **Open Question 3**: Can MOFA-Adapter framework be integrated with other video generation models beyond SVD? The paper demonstrates effectiveness with SVD but doesn't discuss compatibility with other models. Evidence needed: Integration with other popular video generation models like Gen-2 or Pika Labs and evaluation of controllability, visual quality, and computational efficiency.

## Limitations
- Limited evidence for independence assumption of motion domains when combining multiple MOFA-Adapters
- Sparse-to-dense motion generation network architecture referenced but not fully detailed
- Periodic sampling strategy lacks direct ablation studies against alternative longer-video generation approaches

## Confidence
- **High confidence**: Basic feasibility of converting sparse motion hints to dense flows and warping features for video generation
- **Medium confidence**: Domain independence assumption for combining multiple adapters
- **Low confidence**: Robustness of periodic sampling for very long videos without temporal artifacts

## Next Checks
1. Generate videos using combined trajectory and landmark adapters on frames where motion domains clearly overlap and analyze for motion conflicts or artifacts
2. Implement alternative longer-video strategy (conditioning on previous frames rather than periodic averaging) and compare temporal consistency metrics
3. Perform ablation study removing sparse-to-dense network entirely and directly feeding sparse hints to diffusion model to quantify importance of dense flow generation