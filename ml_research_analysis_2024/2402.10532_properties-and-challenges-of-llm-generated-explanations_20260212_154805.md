---
ver: rpa2
title: Properties and Challenges of LLM-Generated Explanations
arxiv_id: '2402.10532'
source_url: https://arxiv.org/abs/2402.10532
tags:
- explanations
- explanation
- properties
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates properties of explanations generated by
  large language models (LLMs) for a broad range of instructions. It analyzes the
  Alpaca dataset, where GPT-4 provided both answers and explanations for diverse prompts
  spanning math problems, coding tasks, facts, and language-related questions.
---

# Properties and Challenges of LLM-Generated Explanations

## Quick Facts
- arXiv ID: 2402.10532
- Source URL: https://arxiv.org/abs/2402.10532
- Reference count: 29
- Primary result: Generated explanations exhibit selectivity and illustrative elements frequently, while subjective or misleading explanations are rare

## Executive Summary
This paper investigates properties of explanations generated by large language models (LLMs) for diverse instructions. Analyzing the Alpaca dataset where GPT-4 provided both answers and explanations, the study finds that 64.3% of outputs include explanations, closely matching human behavior. The generated explanations frequently exhibit selectivity and illustrative elements while rarely being subjective or misleading. These properties reflect patterns adopted from pre-training data. The authors discuss implications for different user goals including safety, trustworthiness, troubleshooting, and knowledge discovery.

## Method Summary
The study analyzes the Alpaca dataset, selecting 200 instructions across six categories (math, code, facts, language, list, instructions). Three human raters evaluate each instruction-output pair for six explanation properties using Label Studio. The analysis measures frequency of properties including selectivity (selecting relevant causes), illustrative elements (using analogies), subjectivity (personal opinions), and misleading explanations. Inter-rater agreement is calculated using Matthew's correlation coefficient.

## Key Results
- Generated explanations exhibit selectivity in 61 examples and illustrative elements in 58 examples frequently
- Subjective or misleading explanations are rare across all domains
- Explanation properties vary by domain, with math showing more complete reasoning while complex topics show more selectivity
- Explanations are present in 64.3% of outputs, closely matching human behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generated explanations reflect human explanation patterns adopted from pre-training data
- Mechanism: LLMs trained on large human-authored text corpora learn to generate explanations that mimic human communication styles, including selectivity, illustrative elements, and incompleteness
- Core assumption: The pre-training corpus contains sufficient examples of human explanations for the model to learn these properties
- Evidence anchors:
  - [abstract] "we hypothesise that LLMs adopt common properties of human explanations"
  - [section 3] "we expect their generated explanations to be similar to human explanations (McCoy et al., 2023)"
  - [corpus] Found 25 related papers with average FMR=0.423, showing moderate research focus on LLM explanations

### Mechanism 2
- Claim: Explanation properties vary by domain based on task formality
- Mechanism: Formal domains like math and coding can support complete reasoning paths, while complex topics require selectivity and illustrative elements
- Core assumption: The nature of the explanation task determines which properties are feasible and useful
- Evidence anchors:
  - [section 5.1] "math and code questions... are often possible to provide a complete reasoning path"
  - [section 6.1] "math problems have the least of the defined criteria, apparently having the least social and the most formal explanations"
  - [corpus] No direct corpus evidence for domain-specific patterns

### Mechanism 3
- Claim: Properties serve different user goals with varying trade-offs
- Mechanism: Incompleteness and selectivity can be positive for accessibility, while faithfulness is crucial for safety-critical applications
- Core assumption: User goals determine whether explanation properties are beneficial or harmful
- Evidence anchors:
  - [section 6.3.1] "safety requires critical reflection of incomplete explanations"
  - [section 6.3.4] "selectivity may be misleading in some cases, but simplification more often makes new information more accessible to learners"
  - [corpus] No direct corpus evidence for user goal analysis

## Foundational Learning

- Concept: Matthew's correlation coefficient
  - Why needed here: To measure agreement between raters on whether explanations are present
  - Quick check question: If two raters agree on 100 out of 200 samples with 50 expected by chance, what is the MCC?

- Concept: Human explanation properties
  - Why needed here: To systematically evaluate LLM-generated explanations against established human patterns
  - Quick check question: Which human explanation property refers to selecting only the most relevant causes?

- Concept: Instruction fine-tuning
  - Why needed here: To understand how post-training modifications affect explanation properties
  - Quick check question: What is the primary purpose of instruction fine-tuning in LLM development?

## Architecture Onboarding

- Component map: Instruction generation -> LLM response generation -> Human annotation -> Property classification
- Critical path: Instruction generation -> LLM response generation -> Human annotation -> Property classification
- Design tradeoffs: Balance between explanation completeness and user comprehension; trade-off between faithfulness and understandability
- Failure signatures: Low inter-rater agreement on commonsense concepts; absence of explanations for incorrect labels; domain-specific property distribution
- First 3 experiments:
  1. Test explanation generation on non-LLM-generated dataset to check for misleading explanations
  2. Vary instruction types to measure property distribution across domains
  3. Compare different LLM architectures to identify property variations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the tendency for LLM-generated explanations to include selectivity and illustrative elements vary based on the complexity or formality of the instruction domain?
- Basis in paper: [explicit] The paper discusses how selectivity is more prevalent in factual questions and list generation tasks, while illustrative elements are more common in math and code questions.
- Why unresolved: The study analyzed a limited set of categories. It's unclear whether these patterns hold across a broader range of domains or if there are other factors influencing the prevalence of these properties.
- What evidence would resolve it: A study analyzing a more diverse set of domains and instruction types, with a larger sample size, could provide more definitive evidence on how explanation properties vary across different domains.

### Open Question 2
- Question: To what extent does the instruction fine-tuning process influence the presence of subjectivity in LLM-generated explanations?
- Basis in paper: [explicit] The paper mentions that subjectivity is less common in explanations, likely due to its mitigation during the alignment phase of GPT-4 training.
- Why unresolved: The paper doesn't provide a detailed analysis of how different fine-tuning approaches or datasets impact the presence of subjectivity. It's unclear whether this property is consistently suppressed across different models and fine-tuning setups.
- What evidence would resolve it: A comparative study analyzing the explanations generated by different LLMs with varying fine-tuning approaches and datasets could shed light on the impact of fine-tuning on subjectivity.

### Open Question 3
- Question: How does the presence of commonsense concepts in LLM-generated explanations affect their usefulness for different user groups and goals?
- Basis in paper: [explicit] The paper discusses how commonsense concepts are a fundamental part of human explanations but were excluded from the study due to low inter-rater agreement.
- Why unresolved: The paper doesn't delve into the implications of commonsense concepts for different user groups and goals. It's unclear whether their presence enhances or hinders understanding and usefulness in various contexts.
- What evidence would resolve it: A study investigating the impact of commonsense concepts on user comprehension and task performance across different user groups and goals could provide insights into their practical implications.

## Limitations
- Analysis relies heavily on a single dataset (Alpaca) with limited domain diversity
- No systematic evaluation of whether explanation properties improve actual user comprehension or task performance
- Limited empirical validation of domain-specific patterns and user goal analyses

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| LLM explanations mirror human patterns (64.3% explanation rate matching human behavior) | Medium |
| Domain-specific property variations | Low |
| User goal implications | Medium |

## Next Checks
1. Test explanation properties on non-LLM-generated datasets to isolate whether patterns reflect training data or instruction-following behavior
2. Conduct controlled user studies measuring comprehension differences between selective vs. complete explanations across domains
3. Compare explanation properties across different LLM architectures (GPT-4, Claude, LLaMA) to identify model-specific variations