---
ver: rpa2
title: 'SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval'
arxiv_id: '2408.16312'
source_url: https://arxiv.org/abs/2408.16312
tags:
- test
- ndcg
- syndl
- collection
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SynDL, a large-scale synthetic test collection
  for passage retrieval developed by extending the TREC Deep Learning Track test collections
  with large language model (LLM) judgments. The authors leveraged GPT-4 to automatically
  annotate over 1.9K queries and 637K query-passage pairs with relevance labels, creating
  a test collection with deep and wide relevance assessments.
---

# SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval

## Quick Facts
- arXiv ID: 2408.16312
- Source URL: https://arxiv.org/abs/2408.16312
- Authors: Hossein A. Rahmani, Xi Wang, Emine Yilmaz, Nick Craswell, Bhaskar Mitra, Paul Thomas
- Reference count: 40
- Primary result: LLM-generated relevance judgments achieve Kendall's tau > 0.85 correlation with human judgments for system ranking

## Executive Summary
This paper introduces SynDL, a large-scale synthetic test collection for passage retrieval created by extending TREC Deep Learning Track collections with GPT-4-generated relevance judgments. The collection contains 1,988 queries and 637,063 query-passage pairs, enabling comprehensive evaluation of retrieval systems at scale. The authors demonstrate that LLM judgments maintain high correlation with human assessments while avoiding bias toward GPT-based systems, offering a cost-effective alternative to expensive human annotations for large-scale research.

## Method Summary
The authors aggregated queries from five TREC Deep Learning Track years (DL-19 to DL-23) and generated a depth-10 passage pool using submitted runs. GPT-4 was then used to automatically annotate all 637,063 query-passage pairs with relevance labels on a 0-3 scale (irrelevant to perfectly relevant). The resulting synthetic collection was validated by comparing system rankings using NDCG metrics against human judgments, achieving Kendall's tau correlations exceeding 0.85. The approach enables researchers to evaluate passage retrieval systems on a much larger scale than traditional human-annotated collections.

## Key Results
- SynDL achieves Kendall's tau > 0.85 correlation with human judgments across multiple TREC DL years
- System ranking order is preserved, with agreement showing τ = 0.8571 and 0.8286 for NDCG@10 and @100 respectively
- No favoritism toward GPT-based systems detected in bias analysis
- Collection includes 1,988 queries and 637,063 query-passage pairs, enabling large-scale research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM judgments maintain high correlation with human judgments for system ranking
- Mechanism: GPT-4's strong natural language comprehension allows it to produce relevance assessments that align closely with human evaluators
- Core assumption: GPT-4 can understand and evaluate passage relevance with accuracy comparable to human assessors
- Evidence anchors: Abstract states high correlation (Kendall's tau > 0.85) with human judgments; experimental results show τ = 0.8571 and 0.8286 for NDCG@10 and @100

### Mechanism 2
- Claim: Large-scale test collections improve research capabilities compared to small-scale collections
- Mechanism: Extending TREC DL test collections with LLM judgments creates a dataset with 1,988 queries and 637,063 query-passage pairs
- Core assumption: Larger datasets with more diverse queries and deeper relevance annotations provide better evaluation and training opportunities
- Evidence anchors: Abstract mentions enabling researchers to test systems at large scale; experimental results show system ordering agreement

### Mechanism 3
- Claim: LLM-generated test collections avoid bias toward systems using the same language model
- Mechanism: Evaluation shows that GPT-4 judgments do not favor GPT-based systems over other approaches
- Core assumption: LLM judgments are objective and not influenced by the specific architecture or training data of evaluated systems
- Evidence anchors: Abstract states no favoritism toward GPT-based systems; experimental results confirm GPT-based systems do not get higher ranks

## Foundational Learning

- Concept: Cranfield paradigm for information retrieval evaluation
  - Why needed here: The paper builds upon the Cranfield paradigm to extend existing test collections with LLM judgments
  - Quick check question: What are the three essential components of a test collection according to the Cranfield paradigm?

- Concept: Relevance judgment scales and their interpretation
  - Why needed here: The paper uses a 0-3 scale for relevance judgments (irrelevant, related, highly relevant, perfectly relevant)
  - Quick check question: What distinguishes a "highly relevant" passage from a "perfectly relevant" passage in the judgment scale used?

- Concept: Evaluation metrics for ranking systems (NDCG, MAP)
  - Why needed here: The paper evaluates system performance using NDCG@5, NDCG@10, NDCG@100, and MAP metrics
  - Quick check question: How does NDCG@10 differ from NDCG@100 in measuring ranking quality?

## Architecture Onboarding

- Component map: Query assembly -> Assessment pool generation -> LLM judgment -> Correlation analysis -> Bias evaluation
- Critical path: Query assembly → Assessment pool generation → LLM judgment → Correlation analysis → Bias evaluation
- Design tradeoffs: Large-scale generation vs. annotation quality, synthetic queries vs. human queries, GPT-4 vs. alternative LLMs
- Failure signatures: Low correlation with human judgments, systematic bias toward certain system types, insufficient coverage of query space
- First 3 experiments:
  1. Generate LLM judgments for a small subset of query-passage pairs and compare correlation with human judgments
  2. Evaluate system rankings using both human and LLM judgments on a sample of TREC DL submissions
  3. Analyze bias by categorizing systems based on architecture and examining ranking differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM-generated judgments compare to human judgments in terms of cost and scalability?
- Basis in paper: The paper mentions that LLM judgments are "greatly reduced cost" compared to human judgments and can enable "large-scale research" in passage retrieval.
- Why unresolved: The paper doesn't provide specific cost comparisons or scalability metrics for LLM judgments versus human judgments.
- What evidence would resolve it: Detailed cost analysis and scalability studies comparing LLM-generated judgments to human judgments, including time, resources, and accuracy metrics.

### Open Question 2
- Question: What is the impact of synthetic queries on the performance of passage retrieval systems?
- Basis in paper: The paper includes synthetic queries in the SynDL test collection and mentions the potential for "extensive research studies to evaluate passage retrieval systems on different types of queries."
- Why unresolved: The paper doesn't provide specific results or analysis on how synthetic queries affect system performance compared to human-provided queries.
- What evidence would resolve it: Comparative studies analyzing system performance on synthetic versus human-provided queries, including metrics like NDCG, mAP, and bias analysis.

### Open Question 3
- Question: How generalizable are passage retrieval models trained on LLM-generated judgments to real-world scenarios?
- Basis in paper: The paper discusses the potential for transfer learning and developing generalizable passage retrieval techniques on diverse queries.
- Why unresolved: The paper doesn't provide empirical evidence on the generalizability of models trained on LLM-generated judgments to real-world scenarios.
- What evidence would resolve it: Studies evaluating the performance of models trained on LLM-generated judgments in real-world retrieval tasks, comparing their effectiveness to models trained on human judgments.

## Limitations
- Evaluation relies on correlation with existing TREC human judgments, but actual accuracy compared to ideal human assessments remains unknown
- The synthetic collection may not fully capture edge cases or domain-specific relevance nuances that human assessors would identify
- GPT-4's judgment quality may degrade for highly technical or specialized domains not well-represented in its training data

## Confidence
- High confidence: Correlation results showing Kendall's tau > 0.85 with human judgments are well-supported by experimental data
- Medium confidence: Claims about bias avoidance are supported but require more diverse system evaluations to be conclusive
- Medium confidence: Scale benefits are demonstrated but the relationship between collection size and evaluation quality needs further exploration

## Next Checks
1. Conduct systematic error analysis comparing LLM judgments to human assessments on a stratified sample of query-passage pairs
2. Evaluate collection performance on domain-specific retrieval tasks (e.g., biomedical, legal) to assess generalization
3. Perform ablation studies removing different query sources or LLM judgment components to isolate their contributions