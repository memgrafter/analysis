---
ver: rpa2
title: 'Self-Retrieval: End-to-End Information Retrieval with One Large Language Model'
arxiv_id: '2403.00801'
source_url: https://arxiv.org/abs/2403.00801
tags:
- retrieval
- self-retrieval
- language
- information
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Retrieval, an end-to-end LLM-driven
  information retrieval architecture that unifies indexing, retrieval, and reranking
  in a single large language model. Unlike traditional systems that treat these components
  separately, Self-Retrieval internalizes the entire corpus through self-supervised
  learning, generates relevant passages via constrained decoding, and performs self-assessment
  for reranking.
---

# Self-Retrieval: End-to-End Information Retrieval with One Large Language Model

## Quick Facts
- arXiv ID: 2403.00801
- Source URL: https://arxiv.org/abs/2403.00801
- Authors: Qiaoyu Tang, Jiawei Chen, Zhuoqun Li, Bowen Yu, Yaojie Lu, Cheng Fu, Haiyang Yu, Hongyu Lin, Fei Huang, Ben He, Xianpei Han, Le Sun, Yongbin Li
- Reference count: 40
- Primary result: Achieves 70.00 MRR@5 on Natural Questions and 68.74 on TriviaQA, outperforming traditional retrieval-augmented generation approaches

## Executive Summary
Self-Retrieval introduces a unified information retrieval architecture that consolidates indexing, retrieval, and reranking into a single large language model. Unlike traditional systems that rely on separate components, Self-Retrieval internalizes the entire corpus through self-supervised learning, generates relevant passages via constrained decoding, and performs self-assessment for reranking. The approach eliminates the need for external indices while maintaining competitive performance on standard retrieval benchmarks, achieving significant improvements over existing retrieval-augmented generation systems.

## Method Summary
Self-Retrieval operates as a single LLM that learns to internalize the retrieval corpus through self-supervised training. The model is trained to map individual sentences to their source passages using auto-regressive generation, creating a unified index within its parameters. During retrieval, a trie-based constrained decoding algorithm ensures generated passages exactly match corpus content, preventing hallucination. The model then performs self-assessment by generating binary responses indicating whether it can answer the query from each passage, using rejection probabilities as relevance scores for reranking. This end-to-end approach eliminates traditional retrieval components while maintaining retrieval quality through the model's internal representations.

## Key Results
- Achieves 70.00 MRR@5 on Natural Questions and 68.74 on TriviaQA
- Significantly outperforms traditional retrieval-augmented generation (RAG) baselines
- Demonstrates competitive performance as corpus size increases
- Shows strong scaling properties with larger models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Internalizing the corpus into LLM parameters via self-supervised learning creates a unified index that enables direct generation of relevant passages.
- Mechanism: The model learns to map sentences to their source passages through auto-regressive training, encoding corpus structure into its weights.
- Core assumption: The model can store sufficient corpus information in its parameters to reconstruct passages without external indices.
- Evidence anchors:
  - [abstract] "Self-Retrieval internalizes the retrieval corpus through self-supervised learning"
  - [section] "the corpus is internalized into the LLM's parameters through self-supervised learning, enabling the model to process passages internally without relying on external indices"
  - [corpus] Weak - no direct corpus evidence of internal storage capacity
- Break condition: If corpus size exceeds model capacity or if passages contain information not present in training data.

### Mechanism 2
- Claim: Constrained decoding using a trie structure ensures generated passages exactly match corpus content.
- Mechanism: At each generation step, the vocabulary is restricted to valid continuations from the trie, preventing hallucination and ensuring exact matches.
- Core assumption: The trie can efficiently constrain generation while maintaining reasonable performance.
- Evidence anchors:
  - [abstract] "we employ the constrained decoding algorithm based on the trie of the corpus"
  - [section] "We construct a prefix tree from corpus D, where each path from the root to a leaf node represents a unique passage in the corpus"
  - [corpus] No corpus evidence of trie efficiency or coverage
- Break condition: If corpus passages have very long common prefixes, making trie navigation inefficient.

### Mechanism 3
- Claim: Self-assessment mechanism provides effective reranking by evaluating passage relevance within the same inference turn.
- Mechanism: The model generates binary responses ("can answer" vs "cannot answer") for each passage, using the probability of rejection as a relevance score.
- Core assumption: The model can accurately self-assess its own generation quality and relevance.
- Evidence anchors:
  - [abstract] "Self-Retrieval performs relevance assessment for reranking"
  - [section] "Self-Retrieval assesses the passage relevance by generating responses such as 'can answer the query' for relevant passages and 'cannot answer the query' for irrelevant ones"
  - [corpus] No corpus evidence of self-assessment accuracy
- Break condition: If the model's self-assessment capability degrades with scale or if it becomes overly confident in incorrect assessments.

## Foundational Learning

- Concept: Self-supervised learning for corpus internalization
  - Why needed here: Enables the model to learn the mapping between corpus content and its internal representations without labeled data
  - Quick check question: How does the model learn to generate passages from individual sentences during training?

- Concept: Constrained decoding algorithms
  - Why needed here: Ensures generated content matches existing corpus passages exactly, preventing hallucination
  - Quick check question: What data structure is used to constrain the generation vocabulary at each step?

- Concept: Self-assessment and confidence scoring
  - Why needed here: Provides a mechanism for the model to evaluate its own outputs and perform reranking
  - Quick check question: How does the model convert its generation probability into a relevance score for reranking?

## Architecture Onboarding

- Component map: Single LLM with three integrated functions - indexing (corpus internalization), retrieval (constrained generation), reranking (self-assessment). No external components.
- Critical path: Query → Constrained generation → Self-assessment → Reranking → Output
- Design tradeoffs: Unified architecture simplifies implementation but requires large model capacity; constrained decoding ensures accuracy but may slow generation; self-assessment eliminates need for external rerankers but depends on model's self-evaluation capability.
- Failure signatures: Performance degradation on unseen passages (indexing failure), generation of non-existent content (constrained decoding failure), poor reranking scores (self-assessment failure).
- First 3 experiments:
  1. Verify corpus internalization by checking if the model can reconstruct training passages from individual sentences
  2. Test constrained decoding by attempting to generate passages outside the corpus and confirming they're rejected
  3. Evaluate self-assessment accuracy by comparing model's relevance judgments against human annotations on generated passages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper limit of corpus size that Self-Retrieval can effectively handle before performance degradation becomes prohibitive?
- Basis in paper: [inferred] The paper mentions scalability analysis showing degradation rates diminish as corpus size increases, but doesn't establish a definitive ceiling for performance.
- Why unresolved: The experiments only tested up to 3M passages (200K documents). The paper doesn't explore what happens at larger scales like 10M+ passages.
- What evidence would resolve it: Systematic experiments scaling corpus size from 3M to 10M+ passages, measuring performance degradation rates and computational resource requirements.

### Open Question 2
- Question: How does Self-Retrieval perform on non-English corpora, particularly languages with different tokenization schemes or character sets?
- Basis in paper: [explicit] The paper only evaluates on English Wikipedia data and mentions scalability to larger corpora as future work, but doesn't address cross-lingual performance.
- Why unresolved: The method relies on constrained decoding and trie structures that may behave differently with non-Latin scripts or languages with different morphological properties.
- What evidence would resolve it: Experiments on multilingual corpora (e.g., cross-lingual retrieval tasks, non-Latin script languages) comparing performance against language-specific retrieval methods.

### Open Question 3
- Question: Can Self-Retrieval effectively incorporate incremental learning to update its index with new documents without full retraining?
- Basis in paper: [explicit] The paper lists "enabling incremental learning and dynamic corpus expansion" as a limitation and future research direction.
- Why unresolved: The current architecture requires full self-supervised training on the entire corpus, making it impractical for dynamic knowledge bases that require frequent updates.
- What evidence would resolve it: A proposed algorithm for efficient parameter updates that selectively incorporate new documents while preserving existing knowledge, validated through experiments showing performance retention after multiple incremental updates.

## Limitations

- Self-assessment mechanism lacks empirical validation of accuracy - no evidence provided that the model can reliably distinguish relevant from irrelevant passages
- Trie-based constrained decoding efficiency is not empirically tested, particularly for corpora with long common prefixes that could degrade performance
- Scaling analysis is incomplete - only tested on relatively small corpora (40K passages) with unproven performance at larger scales

## Confidence

**High Confidence**: Experimental results showing Self-Retrieval outperforming baseline RAG systems (70.00 MRR@5 on NQ vs 51.72 for vanilla RAG) are well-supported by the data presented. Architecture description is clear and methodology is standard.

**Medium Confidence**: Claims about competitive performance as corpus size increases are based on limited evidence - only tested with 100-word chunks without systematic scaling experiments.

**Low Confidence**: Claims about self-assessment mechanism effectiveness and trie-based constrained decoding efficiency lack direct empirical support - no ablation studies provided to quantify component contributions.

## Next Checks

1. Conduct human evaluation study where annotators rate the model's self-assessment judgments on generated passages, measuring precision and recall of relevance determinations.

2. Measure actual runtime performance of constrained decoding on corpora with varying prefix lengths, comparing against unconstrained generation to quantify computational overhead.

3. Systematically test performance degradation as corpus size increases (100K, 500K, 1M passages) to identify breaking points where Self-Retrieval performance drops below traditional index-based methods.