---
ver: rpa2
title: 'CASPFormer: Trajectory Prediction from BEV Images with Deformable Attention'
arxiv_id: '2409.17790'
source_url: https://arxiv.org/abs/2409.17790
tags:
- deformable
- prediction
- queries
- trajectories
- mode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multi-modal trajectory prediction
  for autonomous driving using only Bird's Eye View (BEV) images, eliminating the
  need for expensive HD maps. The proposed method, CASPFormer, leverages a transformer-based
  architecture with deformable attention to efficiently decode vectorized trajectories
  directly from multi-scale scene encodings.
---

# CASPFormer: Trajectory Prediction from BEV Images with Deformable Attention

## Quick Facts
- **arXiv ID:** 2409.17790
- **Source URL:** https://arxiv.org/abs/2409.17790
- **Reference count:** 39
- **Primary result:** State-of-the-art multi-modal trajectory prediction using only BEV images, eliminating HD map dependency

## Executive Summary
CASPFormer introduces a transformer-based architecture for multi-modal trajectory prediction in autonomous driving that operates exclusively on Bird's Eye View (BEV) images. The method eliminates the need for expensive HD maps by directly decoding vectorized trajectories from multi-scale scene encodings using deformable attention. Two key innovations address critical challenges: temporal queries capture temporal correlations while mode queries distinguish between different trajectory modes to prevent mode collapse. The system achieves state-of-the-art performance on nuScenes across multiple metrics while maintaining computational efficiency through deformable attention mechanisms.

## Method Summary
CASPFormer employs a transformer decoder architecture that processes multi-scale BEV image encodings to predict multi-modal trajectories. The core innovation lies in using two sets of learnable embeddings: temporal queries that capture sequential dependencies across time steps, and mode queries that differentiate between distinct trajectory hypotheses. Deformable attention is used to efficiently aggregate information from relevant spatial locations while reducing computational complexity. The network decodes trajectories directly from the encoded scene representations, outputting multiple plausible future paths for each observed scenario. This approach addresses the key challenges of mode collapse and computational inefficiency that plague traditional transformer-based trajectory predictors.

## Key Results
- Achieves state-of-the-art minADE5 of 1.15 and MR5 of 0.48 on nuScenes dataset
- Records minFDE1 of 6.70 and OffRoadRate of 0.01, demonstrating both accuracy and safety
- Ablation studies confirm mode queries are critical for preventing mode collapse and improving generalization
- Deformable attention reduces computational cost with minimal performance degradation

## Why This Works (Mechanism)
The method succeeds by addressing the fundamental challenges in multi-modal trajectory prediction through architectural innovations. The dual embedding system (temporal and mode queries) enables the network to capture both temporal dynamics and mode diversity simultaneously. Temporal queries learn to attend to relevant past states, establishing coherent motion patterns, while mode queries explicitly separate different plausible futures, preventing the network from collapsing to a single mode. Deformable attention further enhances efficiency by focusing computation on relevant spatial regions rather than attending to the entire image space, making the approach practical for real-time deployment while maintaining prediction quality.

## Foundational Learning

**Transformer Architecture** - Why needed: Enables modeling of complex temporal dependencies and long-range interactions in trajectory data
Quick check: Verify attention mechanisms can capture multi-step temporal patterns effectively

**Deformable Attention** - Why needed: Reduces computational complexity from quadratic to linear in input size while maintaining accuracy
Quick check: Compare FLOPs and runtime with standard attention mechanisms

**Mode Diversity** - Why needed: Critical for safe autonomous driving where multiple plausible futures must be considered
Quick check: Evaluate diversity metrics across predicted trajectory sets

## Architecture Onboarding

**Component Map:** BEV Images → Multi-scale Encoder → Deformable Attention Decoder → Mode Queries + Temporal Queries → Multi-modal Trajectory Outputs

**Critical Path:** Multi-scale BEV encoding → Deformable attention aggregation → Mode/temporal query interaction → Trajectory decoding

**Design Tradeoffs:** Uses only BEV images (simpler sensors) vs. incorporating lidar/radar (richer data); deformable attention (efficiency) vs. standard attention (potentially higher accuracy)

**Failure Signatures:** Mode collapse indicated by low diversity metrics; poor temporal consistency shown by high minADE values; computational bottlenecks at deformable attention layers

**First Experiments:**
1. Compare mode diversity metrics with and without mode queries
2. Benchmark runtime efficiency of deformable vs standard attention
3. Test generalization by evaluating on cross-dataset scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Performance comparison limited to BEV-only methods, lacking evaluation against multi-modal sensor fusion approaches
- Cross-dataset generalization not evaluated, leaving uncertainty about real-world robustness
- Detailed runtime analysis across different hardware configurations is absent

## Confidence
- **High confidence** in architectural design and implementation quality, supported by detailed ablation studies
- **Medium confidence** in claimed state-of-the-art performance, given strong results but limited comparison scope
- **Low confidence** in generalization capabilities across different datasets and deployment scenarios

## Next Checks
1. Evaluate CASPFormer on additional autonomous driving datasets (Argoverse, Lyft Level 5) to assess cross-dataset generalization
2. Conduct runtime and memory efficiency analysis across different hardware platforms to validate real-world deployment feasibility
3. Compare performance against methods incorporating multi-modal sensor fusion to quantify the trade-off between sensor simplicity and prediction accuracy