---
ver: rpa2
title: 'Decompose the model: Mechanistic interpretability in image models with Generalized
  Integrated Gradients (GIG)'
arxiv_id: '2409.01610'
source_url: https://arxiv.org/abs/2409.01610
tags:
- concept
- layer
- vector
- vectors
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to mechanistic interpretability
  in image models, addressing the gap in understanding model behavior beyond class-specific
  interpretations. The authors propose using Pointwise Feature Vectors (PFVs) and
  Effective Receptive Fields (ERFs) to decompose model embeddings into interpretable
  Concept Vectors.
---

# Decompose the model: Mechanistic interpretability in image models with Generalized Integrated Gradients (GIG)

## Quick Facts
- arXiv ID: 2409.01610
- Source URL: https://arxiv.org/abs/2409.01610
- Reference count: 7
- This paper introduces Generalized Integrated Gradients (GIG) for comprehensive mechanistic interpretability in image models

## Executive Summary
This paper presents a novel approach to mechanistic interpretability in image models by decomposing model embeddings into interpretable Concept Vectors using Pointwise Feature Vectors (PFVs) and Effective Receptive Fields (ERFs). The authors introduce Generalized Integrated Gradients (GIG) to calculate relevance between concept vectors across layers, enabling comprehensive dataset-wide analysis of model behavior. The method is validated on ResNet50, demonstrating superior performance in both concept extraction and inter-layer attribution tasks compared to existing approaches.

## Method Summary
The method extracts Pointwise Feature Vectors (PFVs) from image models and uses Effective Receptive Fields (ERFs) to label their semantic meaning. PFVs are clustered using bisecting k-means to generate concept vectors, which are then used to decompose PFVs via Lasso regression. Generalized Integrated Gradients (GIG) extends Integrated Gradients to measure how concept vectors in one layer influence concept vectors in subsequent layers. The approach is validated through concept insertion/deletion experiments and quantitative metrics like AUC difference on ImageNet validation set.

## Key Results
- The proposed method outperforms existing approaches in concept extraction and inter-layer attribution tasks
- GIG effectively measures inter-layer concept influence through gradient integration along PFV reconstruction paths
- Bisecting k-means clustering successfully handles the sparse and variable-density structure of PFV space
- The approach provides a holistic view of model behavior beyond class-specific interpretations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concept vectors are valid linear bases in the PFV space because ERF provides accurate semantic labels for clustering.
- Mechanism: PFVs are clustered based on ERF-mapped semantics, centroids become concept vectors, and PFVs are decomposed as sparse linear combinations of these centroids.
- Core assumption: ERF provides a reliable semantic mapping that correlates with the true latent concepts in the network.
- Evidence anchors: [abstract] "We utilize Pointwise Feature Vectors (PFVs) and Effective Receptive Fields (ERFs) to decompose model embeddings into interpretable Concept Vectors." [section 3.1] "We further utilize the instance-level Effective Receptive Fields (ERFs) of the PFVs to label their semantic meaning, enabling a direct investigation of the PFV vector space."
- Break condition: If ERF fails to capture true PFV semantics, concept vectors become arbitrary bases, invalidating subsequent attribution analysis.

### Mechanism 2
- Claim: GIG accurately measures inter-layer concept influence by integrating gradients along PFV reconstruction paths.
- Mechanism: Vary alpha from 0 to 1 reconstructing layer-a PFVs from concept vectors, track how this changes layer-b PFV projections, integrate gradients to get attribution.
- Core assumption: The PFV reconstruction from concept vectors is differentiable and captures the true flow of information between layers.
- Evidence anchors: [section 3.3] "We leverage Integrated Gradients (IG) ... to quantify the contribution of a specific concept vector in a layer to both the final class output and the concept vectors of subsequent layers." [section 3.3] "Let a and b denote the preceding and target layer, and X^l (l ∈ {a, b}) be the embeddings of the corresponding layer."
- Break condition: If PFV reconstruction is non-differentiable or discontinuous, gradient integration fails, breaking attribution accuracy.

### Mechanism 3
- Claim: Bisecting k-means clustering effectively handles the sparse and variable-density structure of PFV space.
- Mechanism: PFV space contains dense clusters (background) and sparse clusters (rare features), bisecting k-means splits until k clusters, handles uneven density by recursive partitioning.
- Core assumption: PFV space exhibits the described density structure and bisecting k-means is robust to it.
- Evidence anchors: [section 3.2.1] "The PFV vector space exhibits a diverse density, with high density around specific concepts and sparsity elsewhere. Hence, bisecting clustering, suitable for such data structures, is employed to extract concept vectors." [section 3.2.1] "For instance, background features often cluster densely, while critical features, such as 'the beak of a bird', may occupy a broader, less dense area."
- Break condition: If PFV density structure differs from assumption, bisecting k-means may fail to find meaningful clusters, invalidating concept extraction.

## Foundational Learning

- Concept: Effective Receptive Field (ERF)
  - Why needed here: ERF provides semantic labels for PFVs, enabling clustering into interpretable concept vectors.
  - Quick check question: If ERF labels a PFV region as "bird beak," what does that imply about the PFV's semantic content?

- Concept: Lasso regression for sparse decomposition
  - Why needed here: Lasso enforces sparsity in PFV reconstruction coefficients, ensuring each PFV is represented by few concept vectors.
  - Quick check question: Why is sparsity in PFV decomposition coefficients important for interpretability?

- Concept: Integrated Gradients attribution
  - Why needed here: GIG extends IG to measure how concept vectors in one layer influence concept vectors in subsequent layers.
  - Quick check question: How does GIG differ from standard Integrated Gradients in measuring attribution?

## Architecture Onboarding

- Component map: PFV-ERF dataset → Bisecting k-means clustering → Concept vectors → Lasso PFV decomposition → GIG inter-layer attribution → Concept attribution scores
- Critical path: Concept extraction (clustering + decomposition) → Inter-layer attribution (GIG) → Concept attribution (class relevance)
- Design tradeoffs: Clustering method choice (bisecting k-means vs SAE vs dictionary learning) vs computational cost and cluster quality
- Failure signatures: Poor clustering quality → ambiguous concept vectors → invalid GIG attribution → meaningless concept attribution scores
- First 3 experiments:
  1. Verify ERF labels match human-annotated semantics on sample PFVs
  2. Test PFV reconstruction fidelity using different clustering methods
  3. Validate GIG attribution by deleting top-attributed concepts and measuring output change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well would the PFV-ERF concept extraction and attribution method generalize to other image architectures beyond ResNet50, such as Vision Transformers or CNNs with different structures?
- Basis in paper: [explicit] The authors state their approach is "universally applicable across various modalities, including transformer architectures" but only demonstrate it on ResNet50.
- Why unresolved: The paper only validates the method on ResNet50, leaving the question of performance on other architectures unanswered. Different architectures may have different embedding spaces that could affect the method's effectiveness.
- What evidence would resolve it: Testing and comparing the method's performance on multiple other image architectures like Vision Transformers, MobileNets, EfficientNets, etc., would show how well it generalizes.

### Open Question 2
- Question: What is the optimal concept size (number of concept vectors) for each layer to balance interpretability and accuracy?
- Basis in paper: [explicit] The authors set the concept size as 8 times the number of channels in each layer, but note this is based on following Bricken et al. (2023) rather than optimizing for their method.
- Why unresolved: The chosen concept size is somewhat arbitrary. A smaller size may miss important concepts while a larger size may lead to redundant or less interpretable concepts. The optimal size likely varies by layer.
- What evidence would resolve it: Systematic experiments varying the concept size per layer and measuring the impact on both interpretability (e.g. concept clarity) and accuracy (e.g. C-Insertion/C-Deletion AUC) would identify the optimal sizes.

### Open Question 3
- Question: How do the discovered concept vectors relate to human-understandable semantic concepts? Can we directly map concept vectors to known visual concepts?
- Basis in paper: [inferred] The paper focuses on extracting concept vectors and attributing them, but doesn't deeply analyze what semantic concepts the vectors correspond to beyond showing some example visualizations.
- Why unresolved: While the method finds concept vectors, it's unclear how closely these align with human-defined semantic concepts. Some concept vectors may represent abstract or composite features rather than clear semantic concepts.
- What evidence would resolve it: A detailed semantic analysis mapping the discovered concept vectors to a known set of human-defined visual concepts (e.g. from a visual dictionary) would reveal the relationship between the method's concepts and human concepts.

## Limitations
- The assumption that ERF provides semantically meaningful labels for PFVs lacks rigorous empirical validation
- The differentiability of PFV reconstruction paths for GIG attribution is assumed rather than proven
- Limited testing across different model architectures beyond ResNet50

## Confidence
- **High Confidence**: The GIG framework as a theoretical extension of Integrated Gradients is well-founded mathematically
- **Medium Confidence**: The effectiveness of bisecting k-means for handling PFV density structures shows promising results but lacks extensive ablation studies
- **Low Confidence**: The assumption that ERF provides semantically meaningful labels for PFVs requires more rigorous validation

## Next Checks
1. Manually annotate a sample of PFVs with human-interpretable concepts and compare against ERF-mapped semantics to quantify labeling accuracy
2. Implement alternative clustering approaches (SAE, dictionary learning) and compare concept vector quality and downstream attribution performance against bisecting k-means
3. Test PFV reconstruction differentiability by examining gradient behavior across reconstruction paths and measuring attribution stability under small perturbations