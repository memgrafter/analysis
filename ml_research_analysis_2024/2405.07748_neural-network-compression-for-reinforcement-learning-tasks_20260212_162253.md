---
ver: rpa2
title: Neural Network Compression for Reinforcement Learning Tasks
arxiv_id: '2405.07748'
source_url: https://arxiv.org/abs/2405.07748
tags:
- neural
- network
- pruning
- quantization
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that neural network compression via pruning
  and quantization can significantly reduce the size of actor networks in reinforcement
  learning (RL) tasks by up to 400x with minimal performance loss. The authors systematically
  apply magnitude-based pruning and 8-bit symmetric quantization to Soft Actor-Critic
  (SAC) and Deep Q-Network (DQN) algorithms across MuJoCo and Atari environments.
---

# Neural Network Compression for Reinforcement Learning Tasks

## Quick Facts
- arXiv ID: 2405.07748
- Source URL: https://arxiv.org/abs/2405.07748
- Reference count: 40
- This work demonstrates that neural network compression via pruning and quantization can significantly reduce the size of actor networks in reinforcement learning (RL) tasks by up to 400x with minimal performance loss.

## Executive Summary
This paper presents a systematic approach to compressing neural networks used in reinforcement learning, specifically focusing on actor networks in Soft Actor-Critic (SAC) and Deep Q-Network (DQN) algorithms. The authors apply magnitude-based unstructured pruning and 8-bit symmetric quantization to achieve dramatic reductions in network size (90-99% sparsity) while maintaining or even improving performance across MuJoCo and Atari environments. The method enables energy-efficient, low-latency inference for RL applications in embedded systems and robotics.

## Method Summary
The compression pipeline involves two main techniques: gradual magnitude-based unstructured pruning during training (applied from 20% to 80% of training steps) and symmetric 8-bit quantization-aware training. The pruning strategy starts with low sparsity and gradually increases to target sparsity levels of 90-99%, while quantization converts weights to 8-bit integers to reduce memory footprint and accelerate inference. The combined approach is evaluated on SAC and DQN algorithms across MuJoCo and Atari benchmark environments.

## Key Results
The compression techniques achieved 90-99% sparsity with up to 400x reduction in network size while maintaining or slightly improving performance compared to uncompressed baselines. In MuJoCo environments, SAC with compressed actors showed comparable or better returns than full-precision models. For Atari games, DQN with compressed networks maintained competitive scores across multiple game titles. The 8-bit quantization further reduced memory requirements without significant accuracy degradation, enabling real-time inference on resource-constrained hardware.

## Why This Works (Mechanism)
The effectiveness stems from the observation that neural networks often contain redundant parameters that contribute minimally to decision-making. Magnitude-based pruning removes weights with smallest absolute values, which are least impactful to network output. The gradual pruning schedule allows the network to adapt by redistributing importance to remaining weights. Quantization exploits the fact that many neural network weights can be represented with lower precision without losing critical information, especially when combined with quantization-aware training that adjusts weights to minimize quantization error.

## Foundational Learning
This work builds upon established neural network compression techniques from supervised learning domains, adapting them to the specific challenges of RL. It extends magnitude-based pruning methods and integrates quantization-aware training into RL pipelines. The approach leverages insights from network redundancy in deep learning while addressing RL-specific considerations like non-stationary data distributions and exploration-exploitation tradeoffs. The authors also reference prior work on model compression for edge deployment and energy-efficient AI systems.

## Architecture Onboarding
The compression methods are applied specifically to actor networks in actor-critic frameworks like SAC and value networks in DQN. The pruning and quantization processes are integrated into the standard RL training loop without requiring architectural modifications. The techniques are compatible with existing RL implementations and can be applied incrementally during training. The compressed models maintain the same interface as original networks, allowing seamless integration with existing RL frameworks and deployment pipelines.

## Open Questions the Paper Calls Out
The authors identify several areas for future investigation: exploring structured pruning methods that could enable hardware acceleration, investigating compression effects on policy robustness and generalization, extending techniques to other RL algorithms beyond SAC and DQN, and studying the interaction between compression and exploration strategies. They also suggest examining the impact of compression on learning dynamics and sample efficiency, as well as developing adaptive compression strategies that adjust based on training progress and task requirements.

## Limitations
The study focuses primarily on actor networks in SAC and DQN, leaving compression of critic networks and other RL components unexplored. The evaluation is limited to specific benchmark environments (MuJoCo and Atari), which may not fully represent real-world deployment scenarios. The gradual pruning schedule requires careful tuning of sparsity schedules and may increase training time. Additionally, while 8-bit quantization is effective, further compression to lower bit-widths might be necessary for extreme resource constraints, potentially requiring more sophisticated techniques to maintain performance.

## Confidence
High confidence in the reported results based on systematic evaluation across multiple environments and algorithms. The compression techniques show consistent performance improvements or maintenance across different tasks, and the methodology follows established practices in neural network compression. However, some uncertainty exists regarding the generalizability to more complex, real-world RL applications and the long-term stability of compressed models in dynamic environments. The specific pruning schedules and quantization parameters may require task-specific tuning for optimal results.

## Next Checks
Verify the reproducibility of compression results across different hardware platforms and RL frameworks. Investigate the impact of compression on training stability and convergence speed. Explore structured pruning alternatives that could enable better hardware acceleration. Conduct ablation studies to isolate the contributions of pruning versus quantization. Test the compressed models in more diverse and challenging environments beyond standard benchmarks. Evaluate the robustness of compressed policies to domain shifts and adversarial perturbations.