---
ver: rpa2
title: 'SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems'
arxiv_id: '2405.19653'
source_url: https://arxiv.org/abs/2405.19653
tags:
- language
- system
- building
- attributes
- syscaps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SysCaps, a framework for creating language
  interfaces for simulation surrogates of complex energy systems. The core idea is
  to use natural language descriptions, or "system captions," to represent system
  configurations, making these models more accessible to both experts and non-experts.
---

# SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems

## Quick Facts
- arXiv ID: 2405.19653
- Source URL: https://arxiv.org/abs/2405.19653
- Reference count: 40
- Primary result: Language interfaces (SysCaps) for simulation surrogates achieve comparable accuracy to traditional methods while enabling new generalization abilities

## Executive Summary
This paper introduces SysCaps, a framework that uses natural language descriptions to represent complex energy system configurations for surrogate modeling. The authors develop a multimodal text and time-series regression model with a training pipeline that leverages LLMs to generate high-quality system captions from simulation metadata. Experiments on building energy and wind farm simulators demonstrate that SysCaps-augmented surrogates achieve better accuracy on held-out systems than traditional methods while enabling generalization to semantically related descriptions. The approach makes surrogate models more accessible to both experts and non-experts with minimal accuracy loss.

## Method Summary
SysCaps creates language interfaces for surrogate models by encoding system configurations as natural language descriptions (SysCaps) using LLMs like llama-2-7b-chat. A multimodal architecture combines text embeddings from DistilBERT/BERT with time-series inputs processed by bidirectional LSTM or S4 encoders, producing predictions through a shared MLP. The framework includes prompt augmentation to improve training in small-data regimes and uses recursive feature elimination for attribute selection. The approach is validated on EnergyPlus building simulations (Buildings-900K dataset) and FLORIS wind farm wake modeling data, comparing accuracy against traditional one-hot encoding baselines.

## Key Results
- SysCaps-augmented surrogates achieve comparable NRMSE to one-hot encoding (within 1% for buildings, 4% for wind farms)
- Language models successfully generalize to semantically related descriptions (e.g., synonyms), with NRMSE increases under 12% versus 54% for traditional methods
- Prompt augmentation provides regularization benefits in small-data regimes, achieving lowest NRMSE in some cases
- Attribute retention rates of 94% for natural language SysCaps and 91% for key-value templates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using language descriptions (SysCaps) as inputs to surrogate models enables both accessibility and accuracy for complex energy systems.
- Mechanism: Natural language SysCaps provide semantic embeddings that capture relationships between system attributes more flexibly than one-hot encodings. This allows the model to generalize to semantically related descriptions (e.g., synonyms) and handle missing attributes gracefully.
- Core assumption: Language models can extract meaningful semantic relationships from system descriptions that correlate with simulation outputs.
- Evidence anchors:
  - [abstract] "Our experiments on two real-world simulators of buildings and wind farms show that our SysCaps-augmented surrogates have better accuracy on held-out systems than traditional methods while enjoying new generalization abilities, such as handling semantically related descriptions of the same test system."
  - [section 6.2] "For 11/13 building type synonyms the increase in NRMSE is less than 12%, while the average increase in NRMSE for the control is 54%."
- Break condition: If the LLM generates captions with low attribute retention (as measured by the classifier), the semantic relationships will be corrupted and accuracy will degrade.

### Mechanism 2
- Claim: The multimodal architecture (text encoder + bidirectional sequence encoder + top model) effectively fuses system knowledge with time-series inputs.
- Mechanism: Text embeddings are broadcast across timesteps and concatenated with time-series inputs, creating a unified representation that captures both static system configuration and dynamic deployment scenarios. This allows the model to learn timestep-specific correlations between attributes and outputs.
- Core assumption: System attributes have consistent relationships with outputs across all timesteps, justifying the broadcast approach.
- Evidence anchors:
  - [section 5] "To efficiently embed long timeseries with thousands of timesteps, we explore both bidirectional LSTMs and bidirectional SSMs for gseq_ψ."
  - [section 6.1] "Surprisingly, the SSM with text templates achieves comparable test accuracy on held-out systems to the SSM with one-hot inputs."
- Break condition: If system attributes have timestep-specific effects, the broadcast approach may lose critical temporal information.

### Mechanism 3
- Claim: Prompt augmentation with multiple caption styles regularizes training in small-data regimes.
- Mechanism: Generating multiple captions per system configuration through different prompt styles increases the diversity of training examples, helping the model learn more robust representations and reducing overfitting when few systems are available.
- Core assumption: Different caption styles contain complementary information about the same system configuration.
- Evidence anchors:
  - [section 6.5] "This suggests SysCaps can have a regularizing effect in small data settings. Notably, the prompt augmentation helps the natural language SysCaps model to achieve the lowest NRMSE."
  - [corpus] Weak evidence - related work on prompt augmentation is limited in the corpus.
- Break condition: If the LLM consistently produces similar captions regardless of prompt style, augmentation provides no benefit.

## Foundational Learning

- Concept: Multimodal learning with text and time-series data
  - Why needed here: The problem requires integrating static system configuration (encoded as text) with dynamic deployment scenarios (time-series) to predict outputs
  - Quick check question: How does the architecture ensure that text embeddings are appropriately aligned with time-series inputs at each timestep?

- Concept: Language model fine-tuning for regression tasks
  - Why needed here: Standard language models are trained for classification and generation, not continuous regression, requiring adaptation of the architecture and training procedure
  - Quick check question: What modifications are made to the standard BERT/DistilBERT architecture to enable regression on continuous outputs?

- Concept: Feature selection and attribute importance
  - Why needed here: Complex energy systems have many attributes, but only a subset strongly influences outputs, making feature selection critical for model performance
  - Quick check question: How does recursive feature elimination help identify the most important attributes for building energy prediction?

## Architecture Onboarding

- Component map: Text encoder (DistilBERT/BERT) → Text embedding (768-dim) → Broadcast → Concatenate with time-series inputs → Bidirectional sequence encoder (LSTM or S4) → Shared MLP top model → Predictions for each timestep

- Critical path: Text embedding → Broadcast → Concatenate → Sequence encoding → MLP predictions

- Design tradeoffs:
  - Text encoder choice: BERT provides better accuracy but is slower than DistilBERT
  - Sequence encoder choice: LSTM is simpler but S4 handles longer sequences more efficiently
  - Caption style: Key-value templates are more reliable but natural language is more accessible

- Failure signatures:
  - Low accuracy on held-out systems: Check attribute retention in generated SysCaps
  - Poor generalization to synonyms: Caption quality may be insufficient for semantic understanding
  - Overfitting with few systems: Try prompt augmentation or regularization

- First 3 experiments:
  1. Compare ResNet with one-hot encoding vs ResNet with SysCaps to establish baseline accuracy
  2. Test bidirectional LSTM vs S4 as sequence encoders for handling long time-series
  3. Evaluate zero-shot generalization to different caption lengths to assess robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do SysCaps perform on simulators with a large number of attributes (e.g., over 100)?
- Basis in paper: [inferred] The authors note that creating SysCaps for simulators with many attributes may require a more powerful LLM than llama-2-7b-chat to generate high-quality captions.
- Why unresolved: The experiments were conducted on datasets with a limited number of attributes (13 for buildings, 5 for wind farms). The paper does not explore the performance of SysCaps on simulators with a large number of attributes.
- What evidence would resolve it: Experiments evaluating SysCaps performance on simulators with a large number of attributes, comparing different LLM sizes and types for caption generation.

### Open Question 2
- Question: Can language interfaces be extended to also describe the timeseries simulator inputs, not just system attributes?
- Basis in paper: [inferred] The authors suggest this as a future extension, proposing that language could be used to describe summary statistics of the timeseries inputs (e.g., "average exogenous temperature increased by five degrees").
- Why unresolved: The current framework focuses on using language to describe system attributes (SysCaps), while timeseries inputs are handled separately by the bidirectional sequence encoder.
- What evidence would resolve it: Development and evaluation of a framework that integrates language descriptions of both system attributes and timeseries inputs, assessing the impact on surrogate model accuracy and interpretability.

### Open Question 3
- Question: How can surrogate foundation models be developed that generalize across different simulators, not just across system configurations within a single simulator?
- Basis in paper: [explicit] The authors pose this as an important question for future work, highlighting the potential for SysCaps to contribute to this goal.
- Why unresolved: The experiments focus on training surrogate models for individual simulators (building energy and wind farm wake). The paper does not explore the development of surrogate models that can generalize across different types of simulators.
- What evidence would resolve it: Experiments developing and evaluating surrogate foundation models trained on data from multiple simulators, assessing their ability to generalize to new simulators and system configurations.

## Limitations

- Generalizability across energy system domains remains uncertain, with success demonstrated only for buildings and wind farms
- Performance sensitivity to caption quality and attribute retention rates may limit robustness in real-world applications
- Accessibility benefits to non-experts are largely theoretical with limited empirical validation

## Confidence

- High Confidence: The core finding that language interfaces can achieve comparable accuracy to traditional one-hot encodings (NRMSE within 1% for buildings, 4% for wind farms) is well-supported by experiments.
- Medium Confidence: The claim that prompt augmentation provides regularization benefits in small-data regimes is supported by a single experiment.
- Low Confidence: The assertion that SysCaps provides meaningful accessibility benefits to non-experts is largely theoretical, with limited empirical validation.

## Next Checks

1. **Domain Transferability Test**: Apply SysCaps to a third energy system domain (e.g., power grids or HVAC systems) and evaluate whether the 94% attribute retention rate and accuracy parity hold. This would validate the claim of broad applicability.

2. **Robustness to Caption Quality**: Systematically degrade the quality of generated SysCaps (e.g., by removing attributes or introducing errors) and measure the impact on surrogate accuracy. This would quantify the sensitivity of the approach to caption quality and test the robustness claim.

3. **Expert Evaluation of Accessibility**: Conduct a user study with both domain experts and non-experts to assess whether SysCaps actually improves the usability of surrogate models compared to traditional interfaces. This would provide empirical support for the accessibility claims.