---
ver: rpa2
title: Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented
  Generation
arxiv_id: '2407.19619'
source_url: https://arxiv.org/abs/2407.19619
tags:
- code
- translation
- dataset
- fortran
- instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automated code translation
  between programming languages, specifically focusing on translating Fortran to C++.
  It introduces a novel approach that combines Few-Shot Learning with Retrieval-Augmented
  Generation (RAG) to enhance translation accuracy.
---

# Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2407.19619
- Source URL: https://arxiv.org/abs/2407.19619
- Reference count: 22
- The paper introduces a RAG-based few-shot learning approach that significantly improves Fortran to C++ code translation accuracy.

## Executive Summary
This paper addresses the challenge of automated code translation between programming languages by introducing a novel approach that combines Few-Shot Learning with Retrieval-Augmented Generation (RAG). The method dynamically retrieves relevant translation examples from a repository and uses them as context to guide the translation process. Experiments with various open and commercial large language models demonstrate that this approach significantly improves translation quality compared to traditional zero-shot methods, especially for complex Fortran to C++ translations. The best-performing models, such as Granite-34B Code Instruct and Llama3-70B Instruct, showed substantial improvements in CodeBLEU scores, achieving up to 0.6 in the one-shot setting.

## Method Summary
The method combines Few-Shot Learning with Retrieval-Augmented Generation (RAG) to enhance code translation accuracy. The RAG pipeline embeds the input Fortran code, retrieves semantically similar translation pairs from a repository using vector similarity metrics, and injects these examples into the few-shot prompt before passing it to the LLM. The approach dynamically selects the best embedding and similarity metric per dataset, allowing flexible adaptation to diverse translation tasks without retraining. The study evaluates translation quality using CodeBLEU scores across varying shot configurations (0, 1, 2, 3 shots) and different embedding models.

## Key Results
- RAG-enhanced few-shot learning achieved CodeBLEU scores up to 0.6 in one-shot settings, significantly outperforming zero-shot baselines.
- Granite-34B Code Instruct showed the most substantial improvement, increasing from 0.237 (zero-shot) to 0.6 (one-shot) for HPC Fortran2CPP dataset.
- Embedding model choice critically impacts performance, with CodeBERT underperforming due to token limit constraints (512 tokens).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation improves code translation by providing contextually relevant examples to the model during inference.
- Mechanism: The RAG pipeline embeds the input Fortran code, retrieves semantically similar translation pairs from a repository using vector similarity (cosine or l2), and injects these examples into the few-shot prompt before passing it to the LLM.
- Core assumption: Embeddings from embedding models like Nomic-Embed, Starencoder, or CodeBERT preserve semantic similarity between code snippets.
- Evidence anchors:
  - [abstract] "Our method, based on Retrieval-Augmented Generation (RAG), significantly improves translation quality by providing contextual examples that the model can learn from in real-time."
  - [section] "These embeddings capture the semantic essence of the code snippets, enabling efficient retrieval of the most contextually relevant examples from ChromaDB."
- Break condition: If the embedding model fails to preserve semantic similarity (e.g., CodeBERT with token limit 512 underperforms), retrieved examples will not match the input's intent, reducing translation quality.

### Mechanism 2
- Claim: Few-shot learning with dynamic example retrieval enhances translation performance compared to static zero-shot inference.
- Mechanism: By conditioning the LLM on retrieved translation pairs from the repository, the model can leverage patterns and structures from similar past translations to improve accuracy.
- Core assumption: The model can generalize from few examples to unseen code patterns if those examples are contextually relevant.
- Evidence anchors:
  - [abstract] "We explored varying numbers of shots i.e. examples provided during inference, specifically 1, 2, and 3 shots and different embedding models for RAG..."
  - [section] "Granite-34B Code Instruct achieved a zero-shot CodeBLEU of 0.237 and improved to 0.6 in the one-shot setting for HPC Fortran2CPP dataset with l2 norm..."
- Break condition: If the retrieved examples are too dissimilar or too few, the few-shot benefit plateaus; e.g., Starcoder shows no improvement in one-shot settings due to token limit constraints.

### Mechanism 3
- Claim: RAG with appropriate embedding and retrieval metrics allows flexible adaptation to diverse translation tasks without retraining.
- Mechanism: The pipeline dynamically selects the best embedding and similarity metric (cosine vs l2) per dataset, allowing it to adapt to different code styles and translation pair distributions.
- Core assumption: Different embedding models and similarity metrics suit different dataset characteristics.
- Evidence anchors:
  - [section] "We evaluated the retrieval performances with l2 and cosine similarity metrics."
  - [section] "CodeBERT consistently underperformed compared to the other two models. This discrepancy can be attributed to the maximum token limit of each embedding model; CodeBERT has a token limit of 512..."
- Break condition: If the dataset has no prior translation pairs (e.g., Stack-V2), RAG cannot retrieve relevant examples, and performance drops to zero-shot levels.

## Foundational Learning

- Concept: CodeBLEU metric and its components (N-gram Match, Syntax Match, Dataflow Match)
  - Why needed here: CodeBLEU is the primary evaluation metric for assessing translation quality; understanding its components helps interpret results.
  - Quick check question: What does a high Dataflow Match Score indicate about a translated code snippet?
- Concept: Embedding models and vector similarity (cosine, l2 distance)
  - Why needed here: Embeddings enable semantic retrieval of translation examples; similarity metrics determine which examples are retrieved.
  - Quick check question: Why might CodeBERT underperform compared to Nomic-Embed or Starencoder in this task?
- Concept: Few-shot learning and in-context learning
  - Why needed here: The method relies on conditioning the model with retrieved examples rather than retraining; understanding this paradigm is key.
  - Quick check question: How does one-shot learning differ from zero-shot learning in this pipeline?

## Architecture Onboarding

- Component map: Embedding Model -> Vector Store -> Retriever -> Prompt Generator -> LLM -> Evaluator
- Critical path:
  1. Embed input Fortran code
  2. Retrieve top-k similar translation pairs
  3. Generate few-shot prompt with examples
  4. Send prompt to LLM
  5. Evaluate output with CodeBLEU
- Design tradeoffs:
  - Token limit vs embedding quality (CodeBERT vs others)
  - Number of shots vs latency (1-shot faster but less context)
  - Similarity metric vs dataset structure (cosine vs l2)
- Failure signatures:
  - Low CodeBLEU with high retrieval similarity → embedding or retrieval mismatch
  - No improvement from zero-shot to one-shot → poor example relevance or token limit issues
  - High variance in CodeBLEU → unstable retrieval or inconsistent model behavior
- First 3 experiments:
  1. Run zero-shot translation on HPC Fortran2CPP dataset with Granite-34B; record baseline CodeBLEU.
  2. Run one-shot translation using Nomic-Embed + cosine similarity; compare CodeBLEU improvement.
  3. Swap embedding to CodeBERT; observe performance drop due to token limit; confirm failure mode.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RAG-enhanced few-shot learning compare to fine-tuning large language models for code translation tasks?
- Basis in paper: [explicit] The paper mentions that fine-tuning requires substantial computational resources and time, while RAG offers a dynamic alternative without extensive retraining.
- Why unresolved: The paper does not provide direct comparative experiments between RAG-enhanced few-shot learning and fine-tuning approaches.
- What evidence would resolve it: A controlled experiment comparing CodeBLEU scores of RAG-enhanced few-shot learning versus fine-tuned models on the same datasets would provide clarity.

### Open Question 2
- Question: What is the optimal number of shots for RAG-based code translation, and how does this vary with different types of code translation tasks?
- Basis in paper: [explicit] The paper explores varying numbers of shots (1, 2, and 3) but does not determine an optimal number.
- Why unresolved: The paper shows improvements with more shots but does not analyze diminishing returns or task-specific optimizations.
- What evidence would resolve it: Systematic experiments varying the number of shots across different code translation tasks and analyzing the trade-off between performance gains and computational costs.

### Open Question 3
- Question: How does the choice of embedding model impact the effectiveness of RAG in code translation, and are there domain-specific embeddings that could further improve performance?
- Basis in paper: [explicit] The paper compares Nomic-Embed, Starencoder, and CodeBERT but does not explore domain-specific embeddings.
- Why unresolved: While the paper shows differences in performance between embedding models, it does not investigate embeddings tailored to specific programming domains or languages.
- What evidence would resolve it: Experiments comparing standard embeddings with domain-specific embeddings trained on large codebases in the target languages, measuring the impact on translation quality.

## Limitations
- The evaluation focuses exclusively on Fortran to C++ translation, limiting generalizability to other programming language pairs.
- CodeBERT's 512-token limit constrains its effectiveness for longer code snippets, potentially excluding meaningful retrieval examples.
- The study doesn't address computational overhead or latency impacts introduced by the RAG retrieval process in real-time applications.

## Confidence
- **High Confidence**: The core mechanism of RAG-enhanced few-shot learning improving translation quality (supported by CodeBLEU improvements from 0.237 to 0.6 in one-shot settings).
- **Medium Confidence**: The claim that model architecture and training data specificity matter (evidenced by Granite-34B outperforming other models, but with limited model comparisons).
- **Low Confidence**: The assertion that this approach generalizes to arbitrary programming language pairs (only tested on Fortran→C++).

## Next Checks
1. Test the RAG pipeline on non-numeric code translation tasks (e.g., Python→JavaScript) to validate cross-domain generalization.
2. Conduct ablation studies varying embedding model token limits to quantify the impact on retrieval quality for longer code snippets.
3. Measure end-to-end latency of the RAG pipeline across different shot settings to assess practical deployment viability.