---
ver: rpa2
title: Geometric-Averaged Preference Optimization for Soft Preference Labels
arxiv_id: '2409.06691'
source_url: https://arxiv.org/abs/2409.06691
tags:
- preference
- step
- arxiv
- labels
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces soft preference labels and geometric averaging
  for DPO, showing improved alignment performance compared to binary labels and conservative
  methods. The approach adjusts the gradient scale based on soft preferences, leading
  to better preference modeling and reduced objective mismatch.
---

# Geometric-Averaged Preference Optimization for Soft Preference Labels

## Quick Facts
- arXiv ID: 2409.06691
- Source URL: https://arxiv.org/abs/2409.06691
- Reference count: 40
- Key outcome: GDPO improves alignment performance with soft preference labels through geometric averaging, showing consistent gains especially for modestly-confident labels.

## Executive Summary
This paper introduces soft preference labels and geometric averaging for Direct Preference Optimization (DPO), addressing limitations of binary labels and conservative methods. The approach adjusts the gradient scale based on soft preferences, leading to better preference modeling and reduced objective mismatch. Experiments on multiple datasets demonstrate consistent improvements, particularly in cases with modestly-confident labels.

## Method Summary
The method extends DPO by incorporating soft preference labels (p ∈ [0.5, 1.0]) instead of binary preferences. Weighted geometric averaging is applied to the LLM output likelihoods, modifying the loss function to include a scaling factor proportional to (2p - 1)(1 - ρ'θ). This causes gradients from equally preferred responses to approach zero, effectively filtering out unnecessary updates. The approach is evaluated against baseline methods (DPO, cDPO, IPO, cIPO, ROPO) on preference datasets using AI feedback for soft label simulation.

## Key Results
- GDPO consistently outperforms DPO across all datasets, with particularly strong gains on modestly-confident preference pairs
- The approach mitigates performance drops seen in DPO when dealing with uncertain preferences
- Weighted geometric averaging resolves objective mismatch between text generation and preference modeling better than conservative interpolation methods

## Why This Works (Mechanism)

### Mechanism 1
Geometric averaging adjusts gradient scale based on soft preference labels, reducing over-optimization on equally preferred responses. Weighted geometric averaging modifies the loss function such that the scaling factor in the gradient becomes proportional to (2p - 1)(1 - ρ'θ), causing gradients from equally preferred responses (p close to 0.5) to approach zero.

### Mechanism 2
Soft preference labels improve alignment by capturing fine-grained preference relationships that binary labels miss. By encoding probability distributions over preferences rather than binary decisions, the model learns nuanced distinctions between response pairs, leading to better alignment with human preferences.

### Mechanism 3
Weighted geometric averaging resolves objective mismatch between text generation and preference modeling. Unlike conservative DPO methods that interpolate objectives to fit preference distributions (which can overfit to training data with modes in low-reward regions), geometric averaging maintains reward gap increase modestly while incorporating soft preference information.

## Foundational Learning

- **Concept**: Bradley-Terry model for pairwise preferences
  - Why needed here: The paper assumes human preferences follow a Bradley-Terry model where the probability of preferring one response over another is proportional to the exponential of their reward values
  - Quick check question: What is the mathematical form of the Bradley-Terry model probability p*(y1 ≻ y2|x) in terms of reward functions?

- **Concept**: Direct Preference Optimization (DPO) objective derivation
  - Why needed here: Understanding how DPO works and its limitations with binary labels is crucial to appreciate why the geometric averaging modification is beneficial
  - Quick check question: How does the DPO objective relate the policy log ratio to the reward difference under the Bradley-Terry model?

- **Concept**: Soft label estimation from AI feedback
  - Why needed here: The paper simulates soft preference labels using AI feedback from LLMs, so understanding this process is important for implementation
  - Quick check question: How are soft preference labels ˆpAI(y1 ≻ y2|x) computed from LLM scores under the Bradley-Terry assumption?

## Architecture Onboarding

- **Component map**: Base LLM (PaLM 2-XS/Gemma) → Supervised Fine-Tuning (SFT) → Preference Optimization (DPO variants) → Evaluation with AI feedback
- **Critical path**: 
  1. Prepare preference dataset with paired responses
  2. Simulate soft preference labels using AI feedback
  3. Implement weighted geometric averaging in loss function
  4. Train preference optimization model
  5. Evaluate using binary and percentage winning rates
- **Design tradeoffs**: 
  - Larger β values act as implicit preference filtering but may slow learning on high-confidence pairs
  - Geometric averaging adds computational overhead but improves alignment quality
  - AI feedback vs. human labels: scalability vs. reliability
- **Failure signatures**: 
  - Poor performance improvement: likely issues with soft label estimation or geometric averaging implementation
  - Degraded text quality: may indicate over-aggressive gradient scaling or objective mismatch
  - Inconsistent results across datasets: suggests sensitivity to preference label distribution
- **First 3 experiments**:
  1. Implement GDPO on a small synthetic preference dataset to verify gradient scaling behavior
  2. Compare GDPO vs DPO on Plasma Plan dataset with varying β values to find optimal scaling
  3. Test preference label classification accuracy on out-of-distribution pairs to validate robustness

## Open Questions the Paper Calls Out

### Open Question 1
How do soft preference labels behave in real-world human preference datasets, and how does this affect the performance of GDPO? The paper mentions that most RLHF datasets only consist of highly-confident pairs, and it is an important future direction to rethink the effect of preference data distribution on the performances. Empirical results comparing GDPO's performance on real-world human preference datasets with different soft preference label distributions would clarify the impact of label distribution on GDPO's effectiveness.

### Open Question 2
How does GDPO handle objective mismatch between text generation and preference modeling compared to cDPO? The paper identifies that cDPO suffers from objective mismatch between text generation and preference modeling, while GDPO can balance both and empirically works better. A theoretical analysis comparing the objectives of GDPO and cDPO, and explaining why GDPO's geometric averaging resolves the objective mismatch while cDPO's linear interpolation does not, would provide a deeper understanding of this issue.

### Open Question 3
How does GDPO perform under different types and levels of label noise in preference data? The paper briefly mentions that soft labels might help mitigate the effect of erroneous flipped labels and provides a table showing GDPO's performance under different label noise levels. Extensive experiments evaluating GDPO's robustness to various types and levels of label noise in preference data, and comparing its performance to other methods, would provide a comprehensive understanding of its noise tolerance.

## Limitations
- Empirical validation relies heavily on AI feedback rather than human preference judgments, limiting generalizability
- The geometric averaging approach introduces an additional hyperparameter (β) that requires careful tuning
- The claim that weighted geometric averaging resolves objective mismatch needs more rigorous theoretical justification

## Confidence

**High Confidence**: The mathematical formulation of weighted geometric averaging and its effect on gradient scaling is well-specified and theoretically sound. The experimental methodology for comparing different preference optimization approaches is robust and reproducible.

**Medium Confidence**: The claim that soft preference labels capture more nuanced human preferences than binary labels is supported by experimental results, but the reliance on AI-generated labels rather than human judgments limits generalizability. The mechanism explaining how geometric averaging prevents overfitting to low-reward modes in training data is plausible but could benefit from additional theoretical analysis.

**Low Confidence**: The paper's assertion that the approach works particularly well for "modestly-confident" labels needs more systematic exploration across different confidence levels. The comparison with conservative methods like cDPO is somewhat limited in scope, focusing on specific datasets rather than a comprehensive benchmark.

## Next Checks

1. **Human Preference Validation**: Conduct human preference studies comparing model outputs from GDPO, DPO, and cDPO to verify that AI feedback aligns with actual human preferences, particularly for modestly-confident cases.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary the β parameter in GDPO across multiple orders of magnitude to identify optimal ranges and test the claim that larger β values help maintain gradient scale without degrading performance.

3. **Cross-Dataset Generalization**: Evaluate GDPO on additional preference datasets with different characteristics (e.g., more diverse domains, varying label confidence distributions) to assess the robustness of the approach beyond the current experimental scope.