---
ver: rpa2
title: In Defence of Post-hoc Explainability
arxiv_id: '2412.17883'
source_url: https://arxiv.org/abs/2412.17883
tags:
- scientific
- understanding
- methods
- post-hoc
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that post-hoc explainability methods in machine
  learning can serve as legitimate tools for scientific knowledge production, despite
  criticisms about their reliability and epistemic status. The author develops a philosophical
  framework based on "mediated understanding" and "bounded factivity," which suggests
  that scientific insights can emerge through structured interpretation of model behavior
  without requiring complete mechanistic transparency.
---

# In Defence of Post-hoc Explainability

## Quick Facts
- arXiv ID: 2412.17883
- Source URL: https://arxiv.org/abs/2412.17883
- Authors: Nick Oh
- Reference count: 13
- Primary result: Post-hoc explainability methods can generate legitimate scientific understanding through structured interpretation despite approximative nature

## Executive Summary
This paper presents a philosophical defense of post-hoc explainability methods in scientific machine learning, arguing that they can serve as legitimate tools for knowledge production despite criticisms about reliability. The author develops a framework based on "mediated understanding" and "bounded factivity," showing how scientific insights can emerge through structured interpretation of model behavior without requiring complete mechanistic transparency. Through analysis of a biomedical ML application in type 2 diabetes research, the paper demonstrates that post-hoc methods, when properly integrated into scientific practice and validated through empirical testing, can generate novel hypotheses and advance scientific understanding.

## Method Summary
The method combines philosophical analysis with case study examination to develop a framework for understanding how post-hoc explainability methods can contribute to scientific knowledge production. The approach integrates mediated understanding (structured interaction between model behavior, interpretability methods, domain knowledge, and empirical validation) with bounded factivity (acknowledging approximations while maintaining empirical accountability). The framework is illustrated through analysis of a type 2 diabetes research case study where post-hoc methods helped generate novel hypotheses about disease pathogenesis.

## Key Results
- Post-hoc methods can generate legitimate scientific understanding through structured interpretation despite approximative nature
- Scientific insights can emerge without complete mechanistic transparency through "mediated understanding"
- Bounded factivity provides a practical resolution to the factivity dilemma in complex ML models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Post-hoc explainability methods can generate legitimate scientific understanding through structured interpretation despite approximative nature.
- **Mechanism:** The framework combines mediated understanding (structured interaction between model behavior, interpretability methods, domain knowledge, and empirical validation) with bounded factivity (acknowledging approximations while maintaining empirical accountability) to create a scientific knowledge generation pipeline.
- **Core assumption:** Scientific understanding can emerge through strategic simplifications and approximations rather than requiring complete mechanistic transparency.
- **Evidence anchors:**
  - [abstract] "scientific insights can emerge through structured interpretation of model behaviour without requiring complete mechanistic transparency"
  - [section 4.1.1] "scientific understanding emerges not through direct model interpretation, but through a complex process of mediated interaction"
  - [corpus] Weak evidence - only 1 relevant paper with citations
- **Break condition:** If empirical validation consistently fails to support model-derived hypotheses, or if bounded factivity boundaries are exceeded beyond acceptable scientific tolerances.

### Mechanism 2
- **Claim:** Post-hoc methods serve as epistemic interfaces that actively participate in knowledge falsification and expansion.
- **Mechanism:** Post-hoc interpretability methods mediate between ML systems and human scientific understanding, enabling bidirectional knowledge creation where model patterns generate hypotheses and domain knowledge guides interpretation refinement.
- **Core assumption:** The approximative nature of post-hoc explanations does not preclude their epistemic value when properly bounded and validated.
- **Evidence anchors:**
  - [abstract] "scientific insights can emerge through structured interpretation of model behaviour without requiring complete mechanistic transparency"
  - [section 4.1.3] "post-hoc methods serve not to eliminate uncertainty, but to manage it through rigorous, testable connections between model behaviour and phenomenal understanding"
  - [corpus] Weak evidence - limited corpus support for epistemic interface claims
- **Break condition:** If mediation process breaks down (e.g., domain knowledge cannot guide interpretation, or empirical validation cannot test hypotheses), or if approximation errors exceed bounds that preserve scientific validity.

### Mechanism 3
- **Claim:** Holistic representationality provides a practical resolution to the factivity dilemma in complex ML models.
- **Mechanism:** Instead of requiring component-level interpretability (elementwise representationality), holistic representationality analyzes model behavior as a whole through property descriptors, allowing meaningful scientific insights without complete mechanistic transparency.
- **Core assumption:** Scientific understanding can be achieved through system-level behaviors and patterns rather than individual component interpretations.
- **Evidence anchors:**
  - [section 3.3] "Rather than viewing this as a limitation, the authors proposed HR. Instead of trying to interpret individual components, they suggest analysing the model's behaviour as a whole"
  - [section 4.1.1] "Scientific understanding through machine learning emerges not through direct model interpretation, but through a complex process of mediated interaction"
  - [corpus] Weak evidence - only theoretical support, limited empirical validation
- **Break condition:** If holistic approaches fail to provide actionable scientific insights, or if system-level analysis cannot be validated against empirical evidence.

## Foundational Learning

- **Concept:** Mediated understanding in scientific ML
  - Why needed here: This concept explains how scientific knowledge emerges through structured interaction between model behavior, interpretability methods, domain knowledge, and empirical validation, rather than through direct model interpretation.
  - Quick check question: How does mediated understanding differ from traditional notions of model interpretability, and why is this distinction important for scientific applications?

- **Concept:** Bounded factivity and strategic simplification
  - Why needed here: This addresses the fundamental tension between accuracy and comprehensibility in complex models, showing how approximations can provide legitimate scientific understanding when properly bounded and validated.
  - Quick check question: What is the key difference between bounded factivity and traditional factivity requirements, and how does this distinction enable scientific progress with complex ML models?

- **Concept:** Theory-ladenness of scientific data and ML practice
  - Why needed here: This concept demonstrates that ML methods are not theory-free but involve theoretical assumptions in data collection, preparation, and interpretation, similar to traditional scientific methods.
  - Quick check question: How does the theory-laden nature of scientific data challenge the "distinctness claim" about ML methods, and what implications does this have for evaluating post-hoc explainability methods?

## Architecture Onboarding

- **Component map:** Philosophical framework (mediated understanding, bounded factivity, holistic representationality) -> Scientific practice components (empirical validation, domain knowledge integration, hypothesis generation) -> Interpretability methods (local/global, model-agnostic/model-specific approaches) -> Validation mechanisms (statistical testing, biomarker development, generalization controls)

- **Critical path:**
  1. Define scientific question and formalize as statistical query
  2. Select appropriate post-hoc interpretability methods based on domain knowledge
  3. Generate interpretable visualizations and patterns from model behavior
  4. Develop testable hypotheses from model-derived insights
  5. Validate hypotheses through empirical testing and statistical analysis
  6. Refine interpretations based on validation results

- **Design tradeoffs:**
  - Local vs. global interpretability: Granular insights vs. comprehensive understanding
  - Model-agnostic vs. model-specific methods: Flexibility vs. performance optimization
  - Approximation fidelity vs. computational efficiency
  - Theoretical rigor vs. practical applicability in scientific domains

- **Failure signatures:**
  - Persistent lack of empirical support for model-derived hypotheses
  - Inability to bound approximations within acceptable scientific tolerances
  - Breakdown of mediation between model behavior and domain knowledge
  - Failure to generate novel, testable scientific insights

- **First 3 experiments:**
  1. Apply framework to a simple supervised learning problem in a well-understood scientific domain to validate core mechanisms
  2. Compare performance of model-agnostic vs. model-specific post-hoc methods on a biomedical dataset
  3. Test bounded factivity approach by deliberately introducing approximations and measuring impact on scientific insight quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we empirically measure and operationalize the concept of "bounded factivity" in post-hoc interpretability methods?
- Basis in paper: [inferred] from discussion of bounded factivity as a theoretical concept lacking measurable criteria
- Why unresolved: The paper acknowledges bounded factivity as a theoretical construct but doesn't provide concrete metrics for determining what constitutes sufficient "bounds" for scientific practice
- What evidence would resolve it: Development and validation of specific quantitative metrics that operationalize bounded factivity, along with case studies demonstrating their application across different scientific domains

### Open Question 2
- Question: What specific mechanisms allow post-hoc interpretability methods to generate genuinely novel scientific hypotheses rather than merely confirming pre-existing theoretical expectations?
- Basis in paper: [explicit] from the claim that post-hoc methods can "generate novel hypotheses and advance phenomenal understanding"
- Why unresolved: While the paper presents a philosophical framework suggesting this is possible, it doesn't clearly delineate the conditions under which post-hoc methods produce genuinely novel insights versus confirmatory findings
- What evidence would resolve it: Comparative studies of post-hoc methods applied to well-understood versus unknown phenomena, measuring the novelty and impact of generated hypotheses against domain expert expectations

### Open Question 3
- Question: How do we systematically validate post-hoc interpretability methods in scientific domains where ground truth explanations are fundamentally unknowable?
- Basis in paper: [explicit] from the acknowledgment that "we rarely have the ground truth for benchmark evaluation" in scientific contexts
- Why unresolved: The paper proposes empirical validation but doesn't specify how to validate explanations when the underlying mechanisms are not fully understood
- What evidence would resolve it: Development of alternative validation frameworks that rely on consistency with established scientific knowledge, predictive power of derived hypotheses, and reproducibility across independent studies

## Limitations

- Limited empirical validation across diverse scientific domains beyond the single type 2 diabetes case study
- Unclear operational criteria for determining when approximations compromise scientific validity
- Need for more systematic examination of how domain-specific assumptions affect ML interpretation validity

## Confidence

**High confidence:** The core argument that post-hoc methods can serve as epistemic tools when properly bounded and validated. The logical structure connecting mediated understanding to scientific knowledge production is internally consistent.

**Medium confidence:** The practical implementation of holistic representationality and its ability to generate actionable scientific insights across different ML architectures. While theoretically justified, empirical demonstration is limited to a single case study.

**Low confidence:** The precise boundaries of "bounded factivity" and how to systematically determine when approximations compromise scientific validity. The framework provides conceptual guidance but lacks operational metrics for assessing approximation limits.

## Next Checks

1. **Cross-domain validation:** Apply the framework to a non-biomedical ML application (e.g., materials science or climate modeling) to test generalizability of mediated understanding across scientific domains.

2. **Approximation boundary testing:** Systematically vary the degree of approximation in post-hoc explanations and measure the impact on hypothesis quality and empirical validation success rates.

3. **Theory-ladenness audit:** Conduct a systematic analysis of how different theoretical assumptions in scientific domains affect the interpretation and validation of post-hoc explanations, quantifying the impact on scientific insight quality.