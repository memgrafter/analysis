---
ver: rpa2
title: Multi-Feature Fusion and Compressed Bi-LSTM for Memory-Efficient Heartbeat
  Classification on Wearable Devices
arxiv_id: '2405.15312'
source_url: https://arxiv.org/abs/2405.15312
tags:
- accuracy
- classification
- signal
- classes
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a resource-efficient approach for ECG-based
  heartbeat classification using multi-feature fusion and bidirectional LSTM (Bi-LSTM).
  The method employs discrete wavelet transform and dual moving average windows for
  preprocessing, and fuses time intervals and under-the-curve areas as input features.
---

# Multi-Feature Fusion and Compressed Bi-LSTM for Memory-Efficient Heartbeat Classification on Wearable Devices

## Quick Facts
- arXiv ID: 2405.15312
- Source URL: https://arxiv.org/abs/2405.15312
- Reference count: 34
- The proposed Bi-LSTM approach achieves 33.8% accuracy with 28% fewer parameters than conventional LSTM for RBBB classification

## Executive Summary
This paper presents a resource-efficient approach for ECG-based heartbeat classification using multi-feature fusion and bidirectional LSTM networks. The method employs discrete wavelet transform and dual moving average windows for preprocessing, and fuses time intervals and under-the-curve areas as input features. The approach achieves significant improvements in classification accuracy for challenging RBBB and LBBB classes while maintaining computational efficiency suitable for wearable devices.

## Method Summary
The proposed method uses discrete wavelet transform (DWT) with Daubechies 4 wavelet up to level 9 for noise reduction, followed by dual moving average windows for peak detection of R, P, T, Q, and S waves. Six time intervals and four under-the-curve area features are extracted and fused using early fusion approach. Four Bi-LSTM model variants (84k to 1.25M parameters) are trained using mini-batch gradient descent with sparse categorical cross-entropy loss on MIT-BIH Arrhythmia Database.

## Key Results
- Bi-LSTM achieves 33.8% accuracy for RBBB class vs 21.8% for conventional LSTM with 28% fewer parameters
- Large model reaches 84.3% accuracy for RBBB and 87.0% for LBBB classes
- Four model variants developed with parameters ranging from 84k to 1.25M parameters
- Multi-feature fusion improves RBBB classification from 31.4% to 84.3% and LBBB from 69.6% to 87.0%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-feature fusion of time intervals and under-the-curve areas improves classification accuracy for difficult RBBB and LBBB classes.
- Mechanism: Under-the-curve area features inherently average signal amplitudes over intervals, making them more robust to noise and artifacts. This averaging property compensates for noisy P and T peak detection while preserving discriminative information for bundle branch blocks.
- Core assumption: The summation of absolute signal values over time intervals contains sufficient discriminative information for classification and is less sensitive to peak detection errors.
- Evidence anchors:
  - [abstract] "Simulations demonstrated that incorporating under-the-curve area features improved the classification accuracy for the challenging RBBB and LBBB classes from 31.4% to 84.3% for RBBB, and from 69.6% to 87.0% for LBBB."
  - [section 2.3] "Multi-feature fusion is achieved by utilizing time intervals and the proposed under-the-curve areas, which are inherently robust against noise, as input features."
  - [corpus] Weak evidence - no direct mentions of under-the-curve areas in related papers.
- Break condition: If noise levels exceed the averaging capacity of under-the-curve features, or if the specific morphological patterns of RBBB/LBBB are not adequately captured by area calculations.

### Mechanism 2
- Claim: Bidirectional LSTM (Bi-LSTM) networks achieve higher accuracy with fewer parameters than conventional LSTM for RBBB classification.
- Mechanism: Bi-LSTM processes sequences in both forward and backward directions, capturing temporal dependencies from future heartbeats that help disambiguate RBBB patterns which may appear similar to normal beats when viewed only from past context.
- Core assumption: Heartbeats exhibit bidirectional temporal correlations where future context provides meaningful information for classification.
- Evidence anchors:
  - [abstract] "Using a Bi-LSTM network, rather than a conventional LSTM network, resulted in higher accuracy (33.8% vs 21.8%) with a 28% reduction in required network parameters for the RBBB class."
  - [section 2.5.3] "Intuitively, in a sequence of human heartbeats, each signal is correlated with its preceding and succeeding heartbeats."
  - [corpus] Weak evidence - no direct mentions of Bi-LSTM comparisons in related papers.
- Break condition: If the temporal dependencies in ECG sequences are predominantly unidirectional, or if the bidirectional processing introduces unnecessary complexity for this specific task.

### Mechanism 3
- Claim: Early fusion of multiple feature types before LSTM layers provides better classification performance than late fusion or separate processing.
- Mechanism: Combining time intervals and area features at the input layer allows the LSTM to learn joint representations that capture both temporal relationships and amplitude characteristics simultaneously, rather than learning them sequentially.
- Core assumption: The LSTM can effectively process the combined feature space when features are concatenated at the input rather than processed separately.
- Evidence anchors:
  - [abstract] "Multi-feature fusion is achieved by utilizing time intervals and the proposed under-the-curve areas"
  - [section 2.3] "These two sets, comprising six time intervals and four under-the-curve areas, are integrated using an early fusion approach"
  - [corpus] Weak evidence - no direct mentions of fusion strategies in related papers.
- Break condition: If the feature dimensions become too disparate after fusion, overwhelming the LSTM's ability to learn meaningful joint representations.

## Foundational Learning

- Concept: ECG signal preprocessing using discrete wavelet transform (DWT)
  - Why needed here: Raw ECG signals contain noise and artifacts that obscure key morphological features necessary for accurate classification, particularly for subtle differences between RBBB, LBBB, and normal beats.
  - Quick check question: What frequency components are removed by discarding detailed coefficients of levels 1, 2, and 3 in the DWT approach?

- Concept: Feature engineering for time-series medical signals
  - Why needed here: Raw ECG waveforms contain too much information and noise for direct neural network processing; engineered features like time intervals and area calculations extract clinically relevant information while reducing dimensionality.
  - Quick check question: Why might under-the-curve area features be more robust to noise than peak-to-peak time intervals?

- Concept: Bidirectional sequence modeling
  - Why needed here: ECG classification benefits from context in both temporal directions, as heart rhythm patterns and morphological features may depend on both preceding and following heartbeats for accurate classification.
  - Quick check question: What is the computational trade-off between using Bi-LSTM versus two separate LSTM layers (one forward, one backward)?

## Architecture Onboarding

- Component map:
  - Signal preprocessing: DWT noise reduction → Dual moving average peak detection
  - Feature extraction: 6 time intervals + 4 under-the-curve areas
  - Classification backbone: Bi-LSTM layers (32-64-128 units) → Dense layers → Dropout → Output layer
  - Model variants: Tiny (84k), Small (150k), Medium (478k), Large (1.25M) parameters

- Critical path:
  - Signal preprocessing → Feature extraction → Bi-LSTM classification → Model evaluation
  - The most critical components are accurate feature extraction (affects all downstream performance) and Bi-LSTM layer configuration (determines accuracy vs. efficiency tradeoff)

- Design tradeoffs:
  - Accuracy vs. parameter count: Larger models provide better accuracy but are less suitable for resource-constrained wearable devices
  - Feature complexity vs. noise robustness: Under-the-curve areas are more robust but may lose some fine-grained temporal information
  - Model size vs. inference latency: Larger models require more memory and processing time, limiting real-time deployment

- Failure signatures:
  - High training loss but low test loss: Overfitting due to insufficient regularization or too complex model for dataset size
  - Poor RBBB/LBBB accuracy: Inadequate feature representation or insufficient model capacity for these challenging classes
  - Degradation with noise: Preprocessing parameters not tuned for specific noise characteristics in target deployment environment

- First 3 experiments:
  1. Baseline test: Implement single LSTM with only time interval features to establish performance floor
  2. Feature ablation: Test Bi-LSTM with time intervals only vs. time intervals + under-the-curve areas to quantify fusion benefit
  3. Model scaling: Compare tiny vs. large Bi-LSTM models to establish accuracy vs. efficiency curve for deployment decisions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed approach perform on other arrhythmia databases beyond MIT-BIH, such as PTB Diagnostic ECG Database or AHA Database?
- Basis in paper: [explicit] The authors state they used MIT-BIH Arrhythmia Database but acknowledge that including more classes in future work would necessitate more complex neural networks, suggesting potential for testing on other databases.
- Why unresolved: The paper only evaluates the approach on the MIT-BIH dataset and does not explore its generalizability to other arrhythmia databases.
- What evidence would resolve it: Testing the proposed method on multiple arrhythmia databases and comparing performance metrics (accuracy, F1 scores) across databases.

### Open Question 2
- Question: What is the optimal number of wavelet decomposition levels for noise reduction in different types of ECG signals or recording conditions?
- Basis in paper: [explicit] The authors mention that "decomposition up to level 6 is necessary to capture this frequency band effectively" for MIT-BIH data, but this is presented as specific to their experimental setup.
- Why unresolved: The optimal wavelet decomposition levels are presented as fixed parameters rather than explored systematically across different signal conditions.
- What evidence would resolve it: A systematic study varying wavelet decomposition levels across different ECG recordings with varying noise characteristics and signal quality.

### Open Question 3
- Question: How does the proposed multi-feature fusion approach compare to other feature fusion methods (e.g., late fusion, weighted fusion) in terms of classification accuracy and computational efficiency?
- Basis in paper: [explicit] The authors use an early fusion approach for multi-feature fusion but do not compare it with alternative fusion strategies.
- Why unresolved: Only early fusion is implemented and evaluated, without exploring or benchmarking other fusion methodologies.
- What evidence would resolve it: Implementing and comparing multiple feature fusion strategies (early, late, weighted) using the same dataset and evaluating both classification performance and computational overhead.

### Open Question 4
- Question: What is the impact of different activation functions in the LSTM and Bi-LSTM layers on classification performance and model size?
- Basis in paper: [inferred] The authors use tanh activation in LSTM/Bi-LSTM layers and ReLU/softmax in dense layers, but do not explore alternative activation functions or their impact on model performance.
- Why unresolved: The activation functions are presented as fixed choices without exploring the sensitivity of the model to different activation function selections.
- What evidence would resolve it: Systematic experimentation with different activation functions (ReLU, sigmoid, tanh, etc.) in LSTM/Bi-LSTM layers and measuring their impact on accuracy, F1 scores, and model size across all classes.

## Limitations
- The paper lacks specific architectural details including dropout rates and exact layer configurations for each model variant
- The 28% parameter reduction claim for Bi-LSTM versus conventional LSTM lacks baseline comparison details
- No exploration of alternative feature fusion strategies or their comparative performance
- Limited evaluation to only the MIT-BIH database without testing generalizability to other arrhythmia datasets

## Confidence
- **High**: The core concept of using multi-feature fusion (time intervals + area features) for noise-robust ECG classification
- **Medium**: The specific accuracy improvements for RBBB/LBBB classes (84.3% and 87.0% respectively)
- **Low**: The parameter reduction claim (28%) and specific architectural configurations across the four model variants

## Next Checks
1. Implement baseline single LSTM with only time interval features to establish performance floor and verify the claimed improvement from Bi-LSTM
2. Conduct ablation study comparing Bi-LSTM with time intervals only versus time intervals + under-the-curve areas to quantify the fusion benefit
3. Systematically vary dropout rates and layer configurations across the four model sizes to determine optimal architecture for the trade-off between accuracy and efficiency