---
ver: rpa2
title: 'AmazonQAC: A Large-Scale, Naturalistic Query Autocomplete Dataset'
arxiv_id: '2411.04129'
source_url: https://arxiv.org/abs/2411.04129
tags:
- search
- prefix
- term
- context
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AmazonQAC, a large-scale dataset of 395M
  query autocomplete (QAC) examples from Amazon Search logs, including actual user-typed
  prefixes and contextual information like session IDs and timestamps. The dataset
  enables realistic evaluation of QAC systems, which predict search queries based
  on partial inputs.
---

# AmazonQAC: A Large-Scale, Naturalistic Query Autocomplete Dataset

## Quick Facts
- arXiv ID: 2411.04129
- Source URL: https://arxiv.org/abs/2411.04129
- Reference count: 11
- Key outcome: Introduces AmazonQAC, a 395M example dataset for query autocomplete, showing finetuned LLMs with context achieve 37% Success@10, highlighting QAC as a complex recommendation problem.

## Executive Summary
This paper introduces AmazonQAC, a large-scale dataset of 395 million query autocomplete (QAC) examples from Amazon Search logs. The dataset includes actual user-typed prefixes and contextual information like session IDs and timestamps, enabling realistic evaluation of QAC systems. The authors assess Prefix Trees, semantic retrieval, and Large Language Models (LLMs), with and without finetuning. Finetuned LLMs with contextual information perform best, achieving 37% Success@10, but this remains only half of the theoretical upper bound of 69.8%, highlighting the complexity of the QAC task. The findings suggest QAC is a challenging recommendation problem influenced by user context, not just prefix matching. The dataset is open-sourced to encourage further research.

## Method Summary
The AmazonQAC dataset was constructed from anonymized Amazon Search logs, filtering out non-English queries and those containing PII. The dataset includes user-typed prefixes, final search terms, session IDs, timestamps, and popularity metrics. The authors evaluate three methods: Prefix Trees (exact prefix matching with popularity ranking), semantic retrieval (ColBERTv2 for semantic similarity), and finetuned LLMs (Mistral-7B with LoRA on 200M examples). Models are evaluated using Success@10 and MRR@10 metrics. The LLM approach incorporates context by including past searches in the prompt and finetuning data.

## Key Results
- Finetuned LLMs with contextual information achieve the best performance (37% Success@10).
- Prefix trees alone achieve 25.3% Success@10, limited by inability to handle unseen or non-matching prefixes.
- Semantic retrieval improves prefix tree performance by 3.6% in Success@10.
- The theoretical upper bound for Success@10 is 69.8%, indicating significant room for improvement.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prefix trees achieve moderate performance by matching exact prefix-to-search-term pairs, leveraging popularity as a ranking signal.
- Mechanism: A trie structure is built from prefix/search term pairs, where each node represents a character. Leaf nodes store popularity counts. At inference, the prefix is traversed in the trie, and the top 10 most popular completions are returned.
- Core assumption: Users mostly type prefixes that are literal substrings of their intended search term, and popularity correlates with relevance.
- Evidence anchors:
  - [abstract] "We find that the QAC problem is not just a simple case of prefix-search term memorization... but rather that it is a complex recommendation problem"
  - [section 4.1] "Since the prefix tree is a memorization of training prefix/search term pairs, the theoretical success upperbound of the prefix tree on this test set is 58.9%"
  - [corpus] Weak: No direct mention of prefix tree performance in corpus papers; only general QAC themes.
- Break condition: Performance degrades when user prefixes do not match the final search term (13% of test cases) or when new, unseen prefix/search term pairs are encountered.

### Mechanism 2
- Claim: Semantic retrieval improves coverage by finding related search terms even when they don't share the exact prefix.
- Mechanism: ColBERTv2 indexes search terms with token-level embeddings and retrieves completions based on semantic similarity to the prefix. Partial words are handled by appending all possible prefixes of each search term word to the index.
- Core assumption: Semantic similarity between prefix and search term is a reliable signal for relevance, even without exact prefix matching.
- Evidence anchors:
  - [section 4.2] "We find that this semantic retrieval-augmented prefix tree outperforms the basic prefix tree matching by +3.6% in Success@10"
  - [abstract] "The prefix-tree approach cannot readily incorporate context like past searches, and it cannot cover cases where the submitted search does not exactly match the typed prefix"
  - [corpus] Weak: Semantic retrieval mentioned in related papers but not directly tied to prefix matching.
- Break condition: Semantic retrieval fails when the prefix is highly ambiguous or when semantic similarity does not align with user intent.

### Mechanism 3
- Claim: Finetuned LLMs outperform other methods by learning complex patterns between prefixes, context, and search terms.
- Mechanism: Mistral-7B is finetuned on (context, prefix, search term) triplets with a prompt that instructs the model to generate ranked suggestions. Context is included in both training and inference prompts.
- Core assumption: The LLM can learn to use contextual signals (past searches) to disambiguate ambiguous prefixes and generate relevant suggestions.
- Evidence anchors:
  - [abstract] "We find that finetuned LLMs perform best, particularly when incorporating contextual information"
  - [section 4.4] "We find that this setup is the best in both MRR@10 and Success@10, far surpassing the next best in success@10 by +8.1% and MRR by +0.06"
  - [corpus] Weak: No direct mention of LLM finetuning in corpus papers; only general QAC and LLM themes.
- Break condition: Performance degrades on short, ambiguous prefixes without context or when the model cannot access popularity information.

## Foundational Learning

- Concept: Trie data structure and prefix matching
  - Why needed here: Prefix trees are a baseline method for QAC, and understanding their limitations is key to appreciating the need for more advanced approaches.
  - Quick check question: How does a trie structure enable efficient prefix matching, and what are its limitations in the context of QAC?

- Concept: Semantic embeddings and retrieval
  - Why needed here: Semantic retrieval is used to improve coverage beyond exact prefix matching, so understanding how embeddings capture semantic similarity is crucial.
  - Quick check question: How do semantic embeddings enable retrieval of related search terms even when they don't share the exact prefix?

- Concept: Prompt engineering and finetuning for LLMs
  - Why needed here: The finetuned LLM is the best-performing method, and understanding how to construct effective prompts and finetune models is essential for replicating and improving results.
  - Quick check question: How does including context in the prompt and finetuning data influence the LLM's ability to generate relevant suggestions?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training/construction -> Inference -> Evaluation
- Critical path: Data preprocessing → Model training/construction → Inference → Evaluation
- Design tradeoffs:
  - Prefix trees: Fast and interpretable but limited to exact prefix matches and cannot use context
  - Semantic retrieval: Improves coverage but adds complexity and computational overhead
  - LLMs: Most powerful but require significant computational resources and careful prompt engineering
- Failure signatures:
  - Prefix trees: Low Success@10 when prefixes don't match final search terms or when new pairs are encountered
  - Semantic retrieval: Low recall when semantic similarity doesn't align with user intent
  - LLMs: Poor performance on short, ambiguous prefixes without context or when popularity information is not accessible
- First 3 experiments:
  1. Evaluate prefix tree performance on a small subset of the data to understand its limitations
  2. Implement and evaluate semantic retrieval to see if it improves coverage beyond prefix matching
  3. Finetune an LLM on a small sample of the data to assess the impact of context and prompt engineering on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of user context, such as past searches, improve the accuracy of query autocomplete (QAC) systems beyond the observed 37% Success@10 in finetuned LLMs?
- Basis in paper: [explicit] The paper notes that finetuned LLMs with contextual information perform best, achieving 37% Success@10, but this is only half of the theoretical upper bound of 69.8%.
- Why unresolved: The current finetuned LLMs still fall short of the theoretical upper bound, suggesting that there are additional factors or methods that could further enhance the use of context.
- What evidence would resolve it: Empirical studies comparing the performance of QAC systems with varying levels of contextual information and identifying which types of context (e.g., session history, time-based patterns) provide the most significant improvements.

### Open Question 2
- Question: What are the limitations of using prefix trees for QAC, and how can these limitations be addressed to improve their performance beyond the 25.3% Success@10 observed?
- Basis in paper: [explicit] The paper indicates that prefix trees perform well on seen and popular prefix-search term pairs but fail on unseen and rarer pairs, achieving only 25.3% Success@10.
- Why unresolved: The prefix tree approach is limited by its inability to handle semantic variations and unseen queries, which are common in real-world scenarios.
- What evidence would resolve it: Development and evaluation of hybrid models that combine prefix trees with semantic retrieval or machine learning techniques to handle a broader range of queries.

### Open Question 3
- Question: How can large language models (LLMs) be further optimized to incorporate popularity information and improve their performance on shorter, more ambiguous prefixes?
- Basis in paper: [explicit] The paper suggests that LLMs struggle with shorter prefixes and lack direct knowledge of popularity, impacting their Success@10 to 21.2% without context.
- Why unresolved: LLMs do not inherently have access to popularity metrics, which are crucial for generating relevant suggestions for ambiguous prefixes.
- What evidence would resolve it: Research into methods for integrating popularity data into LLM training or inference processes, such as retrieval-augmented generation or popularity-weighted decoding strategies.

## Limitations
- Dataset is specific to Amazon Search and may not reflect user behavior in other domains.
- The study focuses on Success@10 and MRR@10 metrics, which may not fully capture user experience or business impact.
- The LLM finetuning was performed on a subset of the data (200M examples) and may not represent the full diversity of the dataset.

## Confidence
- Core claims about the complexity of the QAC task and the benefits of context-aware LLMs are supported by strong empirical evidence (High confidence).
- The analysis of failure modes for prefix trees and semantic retrieval is well-grounded in the data (Medium confidence).
- The study does not extensively explore the robustness of these methods to domain shifts or adversarial prefixes, which could affect their real-world applicability (Low confidence).

## Next Checks
1. **Domain Transferability**: Evaluate the performance of the best methods (finetuned LLMs) on QAC datasets from other domains (e.g., web search, e-commerce) to assess generalizability.
2. **Robustness to Adversarial Prefixes**: Test the methods on intentionally misspelled or ambiguous prefixes to quantify their resilience to noisy or incomplete user input.
3. **Impact of Tokenization**: Compare the performance of the methods using different tokenization strategies (e.g., byte-pair encoding, wordpiece) to understand the impact on prefix matching and semantic retrieval.