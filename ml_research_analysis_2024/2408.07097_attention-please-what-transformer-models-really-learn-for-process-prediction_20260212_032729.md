---
ver: rpa2
title: 'Attention Please: What Transformer Models Really Learn for Process Prediction'
arxiv_id: '2408.07097'
source_url: https://arxiv.org/abs/2408.07097
tags:
- attention
- prediction
- process
- scores
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether attention scores from transformer\
  \ models can reliably explain next-activity predictions in business process monitoring.\
  \ The authors propose two global explainers\u2014Backward Explainer and Attention\
  \ Exploration Explainer\u2014that construct directed graphs to visualize the control\
  \ flow learned by the transformer model."
---

# Attention Please: What Transformer Models Really Learn for Process Prediction
## Quick Facts
- arXiv ID: 2408.07097
- Source URL: https://arxiv.org/abs/2408.07097
- Reference count: 23
- Primary result: Attention-based explainers effectively visualize control flow in transformer models for next-activity prediction in business process monitoring

## Executive Summary
This paper investigates whether attention scores from transformer models can reliably explain next-activity predictions in business process monitoring. The authors propose two global explainers—Backward Explainer and Attention Exploration Explainer—that construct directed graphs to visualize the control flow learned by the transformer model. A pre-study confirms that attention scores are meaningful for explanations. The explainers are evaluated on eight real-world event logs using metrics like correctness, completeness, continuity, contrastivity, and compactness. Results show that attention-based explanations are effective, with the Attention Exploration Explainer outperforming the simpler Backward Explainer. The work provides insights into model interpretability and offers a benchmark for future research on explainable predictive process monitoring.

## Method Summary
The authors propose two global explainers to visualize transformer model explanations for next-activity predictions in business process monitoring. The Backward Explainer constructs explanations by tracing back through attention scores, while the Attention Exploration Explainer provides a more sophisticated approach to graph construction. Both methods generate directed graphs representing the control flow learned by the transformer. The pre-study validates that attention scores are meaningful for explanations, and the evaluation uses eight real-world event logs with standard interpretability metrics to assess effectiveness.

## Key Results
- Attention-based explanations are effective for next-activity prediction in business process monitoring
- Attention Exploration Explainer outperforms Backward Explainer on standard interpretability metrics
- The proposed explainers successfully visualize control flow through directed graphs
- Evaluation across eight real-world event logs demonstrates practical applicability

## Why This Works (Mechanism)
The mechanism works because attention scores in transformer models capture the relationships between different activities in a business process. When a transformer predicts the next activity, the attention mechanism has already learned which previous activities are most relevant for that prediction. By analyzing these attention scores across multiple prediction instances, the explainers can identify consistent patterns in how the model makes decisions. The directed graph visualization makes these learned relationships interpretable to human analysts, showing which activities tend to lead to which subsequent activities according to the model's learned representation.

## Foundational Learning
1. **Transformer Architecture** - Why needed: Understanding how transformers process sequential data and generate predictions. Quick check: Can explain self-attention, multi-head attention, and position encoding.
2. **Business Process Mining** - Why needed: Domain context for event logs and process monitoring. Quick check: Can define event logs, process discovery, and next-activity prediction.
3. **Model Explainability** - Why needed: Understanding different approaches to interpreting ML models. Quick check: Can distinguish local vs global explanations and different explainer types.
4. **Graph Theory** - Why needed: For constructing and interpreting directed graphs from attention scores. Quick check: Can explain nodes, edges, and graph visualization concepts.
5. **Interpretability Metrics** - Why needed: For evaluating explanation quality. Quick check: Can define correctness, completeness, continuity, contrastivity, and compactness.

## Architecture Onboarding
**Component Map:** Event Logs -> Transformer Model -> Attention Scores -> Backward Explainer/Attention Exploration Explainer -> Directed Graph Visualization

**Critical Path:** Event Log Processing -> Transformer Training -> Attention Score Extraction -> Graph Construction -> Explanation Visualization

**Design Tradeoffs:** Simple backward tracing (faster, less comprehensive) vs. exploration-based approach (slower, more complete); global explanations (broad patterns) vs. local explanations (specific predictions)

**Failure Signatures:** Low attention scores across all heads indicate poor learning; disconnected graph components suggest incomplete model understanding; overly complex graphs may indicate overfitting

**First Experiments:** 1) Run pre-study on single dataset to verify attention score meaningfulness; 2) Generate explanations for a simple event log to validate graph construction; 3) Compare both explainers on a small dataset to assess performance differences

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implicit questions remain regarding the generalizability of attention-based explanations across different transformer architectures, the potential for attention score saturation in deeper models, and the comparison with alternative interpretability methods from other domains.

## Limitations
- Pre-study validation limited to single dataset, raising concerns about generalizability
- Evaluation metrics are primarily quantitative without qualitative expert assessment
- Limited comparison scope, only benchmarking two explainer variants against each other
- Does not address attention score saturation or head redundancy issues in deeper transformer architectures

## Confidence
High: Attention-based explanations effectively visualize control flow for next-activity prediction
Medium: Attention Exploration Explainer superior to Backward Explainer, but limited comparison scope
Low: Generalizability across different process domains and transformer architectures

## Next Checks
1. Conduct pre-study across multiple diverse event logs to confirm attention scores' general meaningfulness for explanations
2. Perform qualitative evaluation with domain experts to assess practical utility and interpretability of generated explanations
3. Compare proposed explainers against established interpretability methods for transformer models in other domains to establish relative performance and identify improvements