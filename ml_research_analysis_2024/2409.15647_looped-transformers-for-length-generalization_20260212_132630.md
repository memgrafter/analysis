---
ver: rpa2
title: Looped Transformers for Length Generalization
arxiv_id: '2409.15647'
source_url: https://arxiv.org/abs/2409.15647
tags:
- length
- input
- steps
- number
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of length generalization in
  Transformers, where models trained on inputs of a fixed length struggle to generalize
  to inputs of unseen lengths. The authors propose Looped Transformers with an adaptive
  number of steps, trained using a novel step-dependent supervision method.
---

# Looped Transformers for Length Generalization

## Quick Facts
- arXiv ID: 2409.15647
- Source URL: https://arxiv.org/abs/2409.15647
- Reference count: 40
- Key outcome: Looped Transformers with adaptive depth and step-dependent supervision significantly improve length generalization on algorithmic tasks compared to vanilla next-token prediction.

## Executive Summary
This paper addresses the challenge of length generalization in Transformers, where models trained on inputs of a fixed length struggle to generalize to inputs of unseen lengths. The authors propose Looped Transformers with an adaptive number of steps, trained using a novel step-dependent supervision method. This approach allows the model to learn intermediate steps that can be reused for inputs of arbitrary lengths. The method is evaluated on various tasks including Parity, Copy, Addition, Multiplication, Binary Sum, and Unique Set. Results show that Looped Transformers with adaptive depth significantly outperform vanilla next-token prediction and other baseline methods in terms of length generalization accuracy.

## Method Summary
Looped Transformers use a decoder-only architecture with a single transformer block that is iteratively applied multiple times. During training, the model receives end-to-end supervision only after a variable number of looped steps (Ti), forcing the decoder block to learn transformations that work for both final and intermediate states. Input injection preserves information flow across iterations by adding original input embeddings to each iteration's input. The model can stop adaptively either by oracle knowledge of required steps or by a maximum confidence stopping criterion based on output cross-entropy. This approach enables the model to learn length-generalizable solutions without requiring intermediate supervision data.

## Key Results
- Looped Transformers achieve near-perfect accuracy on Parity tasks with inputs up to 40 digits when trained on inputs up to 20 digits
- The proposed method significantly outperforms vanilla next-token prediction and other baseline methods across all tested tasks
- Step-dependent supervision enables learning of intermediate representations that generalize across different problem lengths and step counts
- Input injection is crucial for maintaining information flow across iterations and improving performance

## Why This Works (Mechanism)

### Mechanism 1: Step-dependent supervision enables learning of intermediate representations
- During training, the model receives end-to-end supervision only after variable looped steps (Ti), forcing the decoder block to learn transformations that work for both final and intermediate states
- The same decoder block receives supervision from multiple step counts and input lengths, encouraging the development of length-generalizable intermediate representations
- Core assumption: The same decoder block can learn to handle multiple intermediate states when supervised only at the final step, without requiring explicit intermediate supervision
- Evidence anchors: The paper observes that looped Transformers learn highly length-generalizable solutions for various tasks when trained with step-dependent supervision

### Mechanism 2: Adaptive depth allocates computational resources proportional to problem complexity
- By using a looped transformer architecture where the same decoder block is applied iteratively, the model can adjust the number of iterations based on input length
- This creates a natural relationship between problem complexity and computational budget - longer inputs require more iterations
- Core assumption: The number of steps required to solve a problem scales predictably with input length for the tasks studied
- Evidence anchors: The paper demonstrates that looped Transformers with adaptive depth significantly improve length generalization across multiple tasks

### Mechanism 3: Input injection preserves information flow across iterations
- At each iteration, the original input embeddings are added to the output embeddings from the previous decoder block
- This ensures that the model maintains a strong connection to the original input throughout the iterative process, preventing information loss
- Core assumption: Without input injection, the model would lose critical information about the original input as it progresses through iterations
- Evidence anchors: The paper shows that with input injection, the model can maintain a strong connection to the original input, preventing information loss with improved performance

## Foundational Learning

- **RASP-L (Restricted Access Sequence Processing Language)**: Understanding RASP-L is crucial because the paper defines n-RASP-L problems based on the constraint that each iteration can be expressed as a fixed-depth decoder-only Transformer. Quick check question: What are the key restrictions of RASP-L programs that make them both learnable by transformers and useful for defining n-RASP-L problems?

- **Positional embeddings and their impact on length generalization**: The paper deliberately uses NoPE (no positional embeddings) to isolate the effect of the looped architecture and step-dependent supervision. Quick check question: How do standard positional embeddings like sinusoidal or learned embeddings potentially interfere with the length-generalizable properties the authors are trying to achieve?

- **Chain-of-Thought (CoT) reasoning and its limitations**: The paper contrasts its approach with CoT methods that require explicit intermediate supervision. Quick check question: What is the key difference between the supervision strategy used in this paper and traditional Chain-of-Thought approaches, and why is this distinction important for practical deployment?

## Architecture Onboarding

- **Component map**: Input embeddings → Looped decoder block (T times with input injection) → Decoding → Output
- **Critical path**: Input → Embedding → Looped decoder block (T times with input injection) → Decoding → Output
- **Design tradeoffs**: Using NoPE simplifies the architecture but may limit performance on tasks where positional information is crucial; step-dependent supervision requires knowing ground-truth number of steps during training; adaptive depth adds inference-time complexity
- **Failure signatures**: Poor performance on test lengths indicates insufficient learning of length-generalizable intermediate steps; oscillation in output when using confidence-based stopping suggests the model hasn't learned to converge; degradation in performance as length increases beyond training maximum suggests overfitting to specific lengths
- **First 3 experiments**: 
  1. Parity task with oracle stopping: Train a looped transformer on parity problems with lengths 1-20, using oracle knowledge of required steps. Test on lengths 21-40 to verify length generalization.
  2. Ablation study without input injection: Repeat the parity experiment without input injection to measure its impact on performance.
  3. Confidence-based stopping validation: Implement the maximum confidence stopping criterion on the trained parity model and compare performance against oracle stopping.

## Open Questions the Paper Calls Out

- **Question**: How does the performance of Looped Transformers scale with increasingly longer test lengths beyond the maximum training length?
- **Question**: Can the step-dependent supervision approach be adapted to work without requiring the ground-truth number of steps T(n) during training?
- **Question**: How do different positional encoding schemes affect the performance of Looped Transformers on length generalization tasks?
- **Question**: Can the Looped Transformer architecture be extended to handle tasks requiring multiple sequential loops or nested loop structures?

## Limitations
- Step-dependent supervision requires oracle knowledge of the ground-truth number of steps during training, which may not be available for all real-world tasks
- The NoPE positional embedding choice, while helping isolate the looped architecture's benefits, may limit applicability to tasks where positional information is crucial
- Evaluation focuses primarily on synthetic algorithmic tasks rather than natural language processing tasks where Transformers are typically deployed
- The n-RASP-L definition does not support tasks requiring multiple loops followed by each other

## Confidence

**High Confidence**: The core empirical finding that looped Transformers with step-dependent supervision outperform vanilla next-token prediction on length generalization tasks.

**Medium Confidence**: The claim that input injection is essential for maintaining information flow across iterations, and that the method works without requiring intermediate supervision.

## Next Checks

1. **Cross-task robustness test**: Evaluate the looped transformer approach on a broader range of tasks including natural language processing benchmarks to assess generalizability beyond synthetic algorithmic problems.

2. **Alternative information preservation ablation**: Compare input injection against alternative mechanisms like residual connections or skip connections to isolate what specifically enables information preservation across iterations.

3. **Step-dependency sensitivity analysis**: Systematically vary the oracle step information during training (e.g., using ±1 step offsets) to understand how sensitive the method is to exact step knowledge and whether approximate step information suffices.