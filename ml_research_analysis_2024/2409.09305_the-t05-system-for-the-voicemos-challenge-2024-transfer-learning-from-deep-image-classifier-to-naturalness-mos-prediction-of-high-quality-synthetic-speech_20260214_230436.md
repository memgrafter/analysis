---
ver: rpa2
title: 'The T05 System for The VoiceMOS Challenge 2024: Transfer Learning from Deep
  Image Classifier to Naturalness MOS Prediction of High-Quality Synthetic Speech'
arxiv_id: '2409.09305'
source_url: https://arxiv.org/abs/2409.09305
tags:
- speech
- system
- prediction
- feature
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the T05 system (UTMOSv2) for the VoiceMOS Challenge
  2024 Track 1, which focuses on predicting naturalness mean opinion scores (MOS)
  for high-quality synthetic speech. The system addresses the challenge of range-equalizing
  bias in MOS tests, where listeners tend to use the entire rating scale regardless
  of the absolute quality of samples.
---

# The T05 System for The VoiceMOS Challenge 2024: Transfer Learning from Deep Image Classifier to Naturalness MOS Prediction of High-Quality Synthetic Speech

## Quick Facts
- arXiv ID: 2409.09305
- Source URL: https://arxiv.org/abs/2409.09305
- Reference count: 0
- Achieved first place in 7 out of 16 metrics in VMC 2024 Track 1 evaluation

## Executive Summary
This paper presents UTMOSv2, the T05 system for the VoiceMOS Challenge 2024 Track 1, which predicts naturalness mean opinion scores (MOS) for high-quality synthetic speech. The system addresses range-equalizing bias in MOS tests through a novel approach that combines spectrogram features from a pretrained image classifier (EfficientNetV2) with self-supervised learning (SSL) speech features from wav2vec2.0. Using a two-stage fine-tuning strategy with data-domain encoding, the system achieved first place in 7 out of 16 evaluation metrics, demonstrating superior performance particularly in correlation-based metrics.

## Method Summary
The T05 system employs a two-stage fine-tuning strategy that extracts features from both mel-spectrograms (using EfficientNetV2) and raw waveforms (using wav2vec2.0). The system incorporates data-domain encoding to condition predictions on dataset ID, addressing systematic biases across different MOS tests. Training follows a multi-stage approach: first training feature extractors separately, then fine-tuning with feature fusion, and finally optimizing all parameters together. The model is trained on multiple MOS datasets (BVCC, Blizzard Challenge, SOMOS, sarulab-data) using five-fold cross-validation with mixup augmentation.

## Key Results
- Achieved first place in 7 out of 16 evaluation metrics in VMC 2024 Track 1
- Demonstrated superior performance in correlation-based metrics (LCC, SRCC, KTAU)
- Ablation studies showed feature fusion improves correlation-based metrics
- Multi-stage learning and diverse training datasets were essential for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining spectrogram features from pretrained image classifiers with SSL speech features improves correlation-based MOS prediction.
- Mechanism: Spectrogram-based CNNs capture fine-grained acoustic patterns that SSL features miss, while SSL features provide robust linguistic and semantic information. Their fusion balances absolute MOS prediction (spectrograms) with ranking quality (SSL).
- Core assumption: The two feature types encode complementary information about speech quality.
- Evidence anchors:
  - [abstract] "Our system incorporates a pretrained image feature extractor to capture the difference of synthetic speech observed in speech spectrograms."
  - [section 4.4.2] "These results indicate that our systems are more effective for zoomed-in MOS prediction compared to the existing baseline systems, particularly in correlation-based metrics."
  - [corpus] Weak evidence - no direct citations comparing spectrogram+SSL fusion vs single features.

### Mechanism 2
- Claim: Multi-stage learning prevents catastrophic forgetting when fine-tuning separate feature extractors together.
- Mechanism: Stage 1 trains each extractor independently on MOS prediction. Stage 2 freezes these weights and trains only the fusion layer. Stage 3 fine-tunes all parameters with small learning rates. This staged approach preserves specialized feature extraction while enabling joint optimization.
- Core assumption: Individual feature extractors can be pre-optimized before joint fine-tuning.
- Evidence anchors:
  - [section 3.4] "We introduce multi-stage learning... we first train the two extractors separately. Then, we fine-tune the two predictors for better MOS prediction using the fusion of two extracted features."
  - [section 4.5.2] "These results suggest that the proposed multi-stage learning is essential for boosting the ability of the SSL features to capture differences between multiple synthetic speech samples."
  - [corpus] Weak evidence - no direct citations about catastrophic forgetting in MOS prediction.

### Mechanism 3
- Claim: Data-domain encoding addresses dataset-specific biases including range-equalizing bias.
- Mechanism: Each dataset gets a unique embedding that conditions predictions on training source. This allows the model to adjust for different listener pools, recording conditions, and rating scales across datasets.
- Core assumption: Dataset ID captures most systematic differences between MOS tests.
- Evidence anchors:
  - [abstract] "we build our MOS prediction system using multiple MOS datasets for the model training with the data-domain encoding (i.e., conditioning the system on the dataset ID)."
  - [section 3.1.3] "This aims to address the biases in different MOS tests, possibly including the range-equalizing bias [6]."
  - [section 4.6.2] "These results suggest that the negative effects caused by the range-equalizing bias are dominant in the prediction of the absolute MOS."
  - [corpus] Weak evidence - no direct citations about data-domain encoding for MOS prediction.

## Foundational Learning

- Concept: Self-supervised learning in speech (wav2vec 2.0)
  - Why needed here: Provides rich speech representations without requiring labeled data for pretraining
  - Quick check question: What's the key difference between wav2vec 2.0 and traditional MFCC features?

- Concept: Transfer learning from computer vision to audio
  - Why needed here: CNNs pretrained on ImageNet can extract meaningful patterns from spectrograms
  - Quick check question: Why use spectrograms as "images" rather than raw waveforms?

- Concept: Multi-task learning and feature fusion
  - Why needed here: Combines complementary information sources for improved prediction
  - Quick check question: What's the difference between early fusion and late fusion of features?

## Architecture Onboarding

- Component map: Raw waveform → Mel-spectrogram transforms (multiple window sizes) → EfficientNetV2 → spectrogram features; Raw waveform → wav2vec2.0 → SSL features; Dataset ID → embedding; Concatenate all features → FC layers → MOS prediction

- Critical path: Input speech → feature extraction (spectrogram + SSL) → data-domain encoding → feature fusion → MOS prediction

- Design tradeoffs: Multiple window sizes improve frequency/time resolution tradeoff but increase computation; separate vs joint training of extractors affects catastrophic forgetting risk

- Failure signatures: High MSE but low correlation suggests absolute prediction issues; low MSE but high MSE suggests ranking issues; poor performance on new datasets suggests domain shift

- First 3 experiments:
  1. Train spectrogram-only model on BVCC data, evaluate on validation set
  2. Train SSL-only model on same data, compare correlation metrics to spectrogram model
  3. Train fusion model with multi-stage learning, compare to individual models on both MSE and correlation metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the proposed multi-stage learning strategy compare when applied to different SSL models beyond wav2vec2.0?
- Basis in paper: [explicit] The paper mentions using wav2vec2.0 as the backbone SSL model but does not explore other SSL models for comparison.
- Why unresolved: The study focuses on wav2vec2.0 and does not investigate the impact of using other SSL models, leaving open whether the multi-stage learning strategy is universally effective across different SSL architectures.
- What evidence would resolve it: Comparative experiments using different SSL models (e.g., HuBERT, WavLM) with the same multi-stage learning strategy and evaluation metrics.

### Open Question 2
- Question: What is the impact of the window size settings in the spectrogram feature extraction on the MOS prediction performance?
- Basis in paper: [explicit] The paper mentions using multiple window sizes to mitigate the trade-off between frequency and time resolution but does not explore the specific impact of different window size combinations on performance.
- Why unresolved: The study uses a fixed set of window sizes without exploring the effect of varying these settings, leaving uncertainty about the optimal configuration for different speech synthesis tasks.
- What evidence would resolve it: Experiments systematically varying the window size settings and measuring their impact on MOS prediction accuracy across different evaluation metrics.

### Open Question 3
- Question: How does the proposed system perform on MOS prediction for speech synthesis domains outside of the high-quality synthetic speech focus of the VMC 2024 Track 1?
- Basis in paper: [inferred] The system is designed for high-quality synthetic speech and uses data-domain encoding, but the paper does not evaluate its performance on other speech synthesis domains.
- Why unresolved: The study is limited to the VMC 2024 Track 1 dataset and does not explore the system's generalization to other speech synthesis domains, such as low-quality speech or different languages.
- What evidence would resolve it: Evaluation of the system on MOS datasets from different speech synthesis domains and languages, comparing performance across these varied contexts.

## Limitations
- No direct comparisons between spectrogram-only, SSL-only, and fusion approaches in ablation studies
- Limited analysis of model generalization to truly unseen datasets
- No discussion of computational costs and efficiency trade-offs
- Effectiveness of data-domain encoding in addressing range-equalizing bias not theoretically explained

## Confidence

- **High confidence**: The overall approach of combining spectrogram and SSL features with data-domain encoding improves MOS prediction performance, as evidenced by first-place rankings in multiple metrics.
- **Medium confidence**: The specific claim that multi-stage learning prevents catastrophic forgetting and that data-domain encoding addresses range-equalizing bias, as these mechanisms are supported by ablation studies but lack direct experimental validation.
- **Low confidence**: The exact contribution of each individual component (spectrogram features, SSL features, data-domain encoding, multi-stage learning) to the final performance, due to the absence of component-level ablation comparisons.

## Next Checks

1. **Component-level ablation**: Conduct experiments comparing performance of models using only spectrogram features, only SSL features, only data-domain encoding, and only multi-stage learning to isolate individual contributions.

2. **Range-equalizing bias isolation**: Design controlled experiments using artificially biased MOS data to directly measure the effectiveness of data-domain encoding in mitigating this specific bias.

3. **Generalization test**: Evaluate the trained model on completely new MOS datasets not seen during training to assess true generalization capabilities and potential domain shift issues.