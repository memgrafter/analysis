---
ver: rpa2
title: In-Context Learning Can Re-learn Forbidden Tasks
arxiv_id: '2402.05723'
source_url: https://arxiv.org/abs/2402.05723
tags:
- in-context
- attack
- examples
- learning
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether in-context learning (ICL) can\
  \ undo safety fine-tuning designed to make models refuse certain harmful tasks.\
  \ The authors fine-tune LLaMA-2-70B to refuse two tasks\u2014sentiment classification\
  \ and summarizing made-up news articles\u2014and show that ICL can recover the original\
  \ task performance."
---

# In-Context Learning Can Re-learn Forbidden Tasks

## Quick Facts
- **arXiv ID**: 2402.05723
- **Source URL**: https://arxiv.org/abs/2402.05723
- **Reference count**: 40
- **Key outcome**: In-context learning (ICL) can recover task performance that was deliberately removed during safety fine-tuning, with attack success rates varying from 0% to over 50% depending on model and fine-tuning strength.

## Executive Summary
This paper investigates whether in-context learning can undo safety fine-tuning designed to make models refuse certain harmful tasks. The authors fine-tune LLaMA-2-70B to refuse two tasks—sentiment classification and summarizing made-up news articles—and show that ICL can recover the original task performance. They extend this to attacking safety alignment using the AdvBench dataset on VICUNA-7B, STARLING-7B, and LLaMA-2-7B. Results show ICL attacks succeed on VICUNA-7B and STARLING-7B, increasing attack success rates from ~3% to over 50% in some cases, but fail to generate harmful outputs on LLaMA-2-7B (attack success rate remains at 0%). However, even on LLaMA-2-7B, the log probability of generating an affirmative (harmful) response increases with ICL, suggesting the attack makes harmful responses more likely even if not fully successful. An "In-Chat ICL" attack using chat template tokens further improves attack success on VICUNA-7B and STARLING-7B. The findings indicate that safety fine-tuning can be undermined by ICL, highlighting a potential vulnerability in LLM safety training, though stronger fine-tuning (as in LLaMA-2-7B) can provide some resilience.

## Method Summary
The researchers fine-tune base LLMs (LLaMA-2-70B, VICUNA-7B, STARLING-7B, LLaMA-2-7B) to refuse specific tasks using either supervised fine-tuning (SFT) or direct preference optimization (DPO). They then apply ICL attacks using task-relevant demonstrations as context to measure whether the model will comply with the forbidden task instead of refusing. For the initial experiments, they use SST-2 for sentiment classification and CNN/Daily Mail for link hallucination tasks. For the safety alignment attack, they use the AdvBench dataset containing harmful prompts. They measure attack success through keyword-based detection of refusals or harmful content, and analyze log probability changes in affirmative responses. An additional "In-Chat ICL" attack uses chat template tokens ({User}, {Assistant}) to potentially bypass safety fine-tuning.

## Key Results
- ICL successfully recovers sentiment classification performance on LLaMA-2-70B fine-tuned to refuse, with attack success rates reaching 71.7% with 16 examples
- ICL attacks on AdvBench succeed on VICUNA-7B and STARLING-7B (attack success increases from ~3% to over 50%) but fail on LLaMA-2-7B (attack success remains at 0%)
- Even when harmful outputs aren't generated, ICL increases the log probability of affirmative responses on all tested models, suggesting attacks make harmful responses more likely even when not fully successful
- In-Chat ICL using chat template tokens further improves attack success on VICUNA-7B and STARLING-7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning can recover task performance that was deliberately removed during fine-tuning.
- Mechanism: The model's parametric knowledge for the forbidden task is still present but masked by fine-tuning; ICL provides cues that reactivate this knowledge without modifying weights.
- Core assumption: The model retains latent ability to perform the forbidden task despite safety fine-tuning.
- Evidence anchors:
  - [abstract] "we investigate whether in-context learning (ICL) can be used to re-learn forbidden tasks despite the explicit fine-tuning of the model to refuse them."
  - [section] "we fine-tune the model on the SST-2 dataset with direct preference optimisation (DPO)... We train for 100 steps with batch size 16 and we reach convergence of the log probability of the rejected and chosen responses."
  - [corpus] Weak - corpus mentions general ICL attacks but no direct evidence for forbidden task re-learning.
- Break condition: If fine-tuning sufficiently suppresses task-relevant parameters or if ICL examples are too dissimilar from task structure.

### Mechanism 2
- Claim: Chat template tokens ({User}, {Assistant}) can be exploited as a prompt injection vector to bypass safety fine-tuning.
- Mechanism: In-Chat ICL mimics legitimate conversation patterns, causing the model to treat harmful demonstrations as benign context, reducing refusal likelihood.
- Core assumption: The model's safety alignment is context-sensitive and can be deceived by format manipulation.
- Evidence anchors:
  - [abstract] "Finally, we propose an ICL attack that uses the chat template tokens like a prompt injection attack to achieve a better attack success rate on VICUNA -7B and STARLING -7B ."
  - [section] "The second, which we call In-Chat ICL attack, consists of giving the demonstrations as a previous chat between the language model and the user, i.e. {User}:x0; {Assistant}:y0; ... {User}:xi; {Assistant}:yi; {User}:x."
  - [corpus] Moderate - corpus includes papers on prompt injection but none specifically on chat template exploitation.
- Break condition: If model implements template-aware safety checks or if templates are randomized/obfuscated.

### Mechanism 3
- Claim: Even when harmful output is not generated, ICL increases the log probability of affirmative responses, making future attacks more likely.
- Mechanism: ICL shifts the model's internal probability distribution toward harmful responses, lowering the activation threshold for future harmful generations.
- Core assumption: Log probability shifts reflect meaningful changes in model behavior beyond immediate outputs.
- Evidence anchors:
  - [abstract] "even on LLaMA-2-7B, the log probability of generating an affirmative (harmful) response increases with ICL, suggesting the attack makes harmful responses more likely even if not fully successful."
  - [section] "We further look at the log probability of generating tokens which correspond to an affirmative response... STARLING -7B does not tend to give an affirmative response, but instead immediately answers the query making it hard to know which token probabilities to look at."
  - [corpus] Weak - corpus does not contain studies on log probability shifts from ICL attacks.
- Break condition: If probability shifts are transient or if safety fine-tuning includes probability regularization.

## Foundational Learning

- Concept: Fine-tuning vs In-context Learning
  - Why needed here: Understanding how safety fine-tuning differs from ICL is crucial for analyzing why ICL can bypass fine-tuning.
  - Quick check question: What is the key difference between fine-tuning (which modifies model weights) and in-context learning (which does not modify weights)?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the fine-tuning method used to train models to refuse tasks; understanding it helps explain the mechanism of refusal.
  - Quick check question: How does DPO differ from standard supervised fine-tuning in terms of training objective?

- Concept: Log probability analysis
  - Why needed here: Log probability changes indicate shifts in model behavior even when outputs don't change; this is key to understanding attack effectiveness.
  - Quick check question: What does an increase in log probability of affirmative tokens suggest about the model's internal state?

## Architecture Onboarding

- Component map:
  - Base LLM (e.g., LLaMA-2-70B) -> Fine-tuning module (DPO/SFT) -> ICL injection mechanism (standard or In-Chat) -> Evaluation pipeline (keyword-based refusal detection, log probability analysis)

- Critical path:
  1. Fine-tune base model to refuse specific tasks
  2. Apply ICL with task-relevant demonstrations
  3. Measure output (refusal vs. compliance) and log probabilities
  4. Analyze semantic similarity between queries and demonstrations

- Design tradeoffs:
  - Fine-tuning strength vs. helpfulness: Stronger refusal training may reduce model utility
  - ICL example length vs. attack success: Longer examples may help up to a point, then hurt
  - Keyword-based evaluation vs. semantic evaluation: Keywords are fast but may miss nuanced refusals

- Failure signatures:
  - Attack fails but log probabilities still increase (partial bypass)
  - Semantic similarity doesn't correlate with attack success (demonstration selection matters)
  - In-Chat ICL only works on some models (template exploitation is model-dependent)

- First 3 experiments:
  1. Replicate sentiment classification fine-tuning and ICL attack on LLaMA-2-70B
  2. Test link hallucination task with both SFT and DPO fine-tuning methods
  3. Apply standard ICL and In-Chat ICL attacks on VICUNA-7B using AdvBench dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of in-context learning attacks on safety alignment improve with increasingly sophisticated and numerous in-context examples, and if so, at what point does the attack reach diminishing returns?
- Basis in paper: [inferred] The paper suggests that more in-context examples generally improve attack success rates, but also notes a decrease in effectiveness beyond a certain point (around 350-500 tokens for sentiment classification and link hallucination tasks). This implies there may be a ceiling to the attack's effectiveness.
- Why unresolved: The paper only tests up to a certain number of in-context examples and token lengths. It's unclear if further increasing the number and sophistication of examples would lead to continued improvement or if a plateau is reached.
- What evidence would resolve it: Conducting experiments with a wider range of in-context example numbers and token lengths, including extremely long and sophisticated examples, would help determine if there's a definitive ceiling to the attack's effectiveness.

### Open Question 2
- Question: Are there specific types of in-context examples that are more effective at breaking safety alignment than others, and can these examples be identified through semantic analysis or other means?
- Basis in paper: [inferred] The paper performs a semantic analysis of VICUNA-7B's responses and finds that queries with higher semantic similarity to in-context examples are not necessarily more likely to elicit affirmative responses. This suggests that the effectiveness of in-context examples may depend on factors beyond semantic similarity.
- Why unresolved: The paper's semantic analysis is limited to a single model (VICUNA-7B) and a small set of in-context examples. It's unclear if the findings generalize to other models or if there are other factors that influence the effectiveness of in-context examples.
- What evidence would resolve it: Conducting similar semantic analyses on a wider range of models and in-context examples, as well as exploring other potential factors influencing effectiveness (e.g., prompt structure, specific keywords, etc.), would help identify the characteristics of effective in-context examples.

### Open Question 3
- Question: Can models be trained to be more resistant to in-context learning attacks on safety alignment, and if so, what training methods are most effective?
- Basis in paper: [explicit] The paper finds that LLAMA 2-7B, which has undergone more extensive safety fine-tuning, is resistant to in-context learning attacks. This suggests that training methods can play a role in mitigating the risk of such attacks.
- Why unresolved: The paper only tests a limited number of training methods (SFT and DPO) and doesn't explore other potential approaches (e.g., adversarial training, curriculum learning, etc.). It's unclear which methods are most effective at improving resistance to in-context learning attacks.
- What evidence would resolve it: Experimenting with a wider range of training methods and comparing their effectiveness at improving resistance to in-context learning attacks would help identify the most promising approaches.

## Limitations

- Attack success rates vary dramatically across models, suggesting findings may not generalize uniformly across different LLM architectures and fine-tuning approaches
- Keyword-based evaluation for detecting refusals and harmful content may miss nuanced responses or produce false positives/negatives
- The study focuses on specific tasks and datasets, leaving open questions about whether findings extend to other harmful behaviors or more sophisticated safety fine-tuning methods

## Confidence

- **High Confidence**: The core finding that ICL can recover task performance on fine-tuned models is well-supported by experimental results, particularly for sentiment classification and link hallucination tasks. The methodology for measuring attack success rates and log probability changes is clearly specified and reproducible.
- **Medium Confidence**: The claim that In-Chat ICL provides better attack success than standard ICL is supported but requires careful interpretation, as the improvement is model-dependent. The mechanism explanation (template tokens as prompt injection) is plausible but not definitively proven.
- **Low Confidence**: The practical implications of increased log probabilities without corresponding output changes are not well-established. The paper doesn't provide evidence that these probability shifts meaningfully increase the likelihood of successful attacks on subsequent attempts.

## Next Checks

1. **Cross-model generalization test**: Apply the same ICL attack methodology to a broader set of open-weight models with varying fine-tuning strengths (including different DPO hyperparameters and training dataset sizes) to determine whether the LLaMA-2-7B resistance is unique or indicative of a broader pattern related to fine-tuning intensity.

2. **Longitudinal attack persistence study**: Conduct repeated ICL attacks over multiple sessions to measure whether increased log probabilities translate to actual increases in harmful output generation over time, and whether models develop any resistance to repeated attacks through exposure.

3. **Semantic evaluation validation**: Replace keyword-based refusal detection with human annotation or advanced semantic similarity metrics to verify that the reported attack success rates accurately capture the intended behaviors, particularly for nuanced refusals or affirmative responses that don't contain obvious keywords.