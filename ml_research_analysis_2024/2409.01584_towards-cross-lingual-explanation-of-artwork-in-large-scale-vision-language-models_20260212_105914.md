---
ver: rpa2
title: Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language
  Models
arxiv_id: '2409.01584'
source_url: https://arxiv.org/abs/2409.01584
tags:
- section
- llav
- subsection
- a-next
- title
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates cross-lingual explanation generation capabilities
  of Large-scale Vision Language Models (LVLMs) for artworks across ten languages.
  The research addresses limitations in existing multilingual benchmarks that rely
  on machine translation, potentially introducing cultural biases.
---

# Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models

## Quick Facts
- arXiv ID: 2409.01584
- Source URL: https://arxiv.org/abs/2409.01584
- Authors: Shintaro Ozaki; Kazuki Hayashi; Yusuke Sakai; Hidetaka Kamigaito; Katsuhiko Hayashi; Taro Watanabe
- Reference count: 40
- One-line primary result: LVLMs perform optimally when both instructions and outputs are in English, with significant performance drops when using other languages, suggesting need for multilingual Vision Encoder training.

## Executive Summary
This study investigates cross-lingual explanation generation capabilities of Large-scale Vision Language Models (LVLMs) for artworks across ten languages. The research addresses limitations in existing multilingual benchmarks that rely on machine translation, potentially introducing cultural biases. A novel dataset was created from Wikipedia without machine translation, focusing on artwork articles with consistent titles and images across languages. The study evaluated three settings (Alignment-10, Alignment-5, Full) and found that LVLMs perform optimally when both instructions and outputs are in English, with significant performance drops when using other languages. Results showed that LVLMs struggle to transfer knowledge learned from English data to other languages, even when instructed in non-English languages. LoRA Tuning in English further degraded performance, suggesting the need for Vision Encoder training on multilingual data rather than English-only datasets.

## Method Summary
The study collected Wikipedia articles about artworks in ten languages, filtering for pages with consistent titles and images across languages without machine translation. Three experimental settings were evaluated: Alignment-10 (all 10 languages), Alignment-5 (5 languages), and Full (full dataset). Four instruction-output patterns were tested: {En, Lang}-{En, Lang}. Three LVLM models (mPLUG-Owl2, LLaV A-NeXT, Qwen-VL) were evaluated using Entity Coverage, Entity F1, and Entity Cooccurrence metrics. Two models underwent LoRA Tuning with English training data to assess fine-tuning effects.

## Key Results
- LVLMs perform optimally when both instructions and outputs are in English across all experimental settings
- Significant performance degradation occurs when instructions or outputs are in non-English languages
- LoRA Tuning in English further degrades cross-lingual performance, suggesting reinforcement of English-specific knowledge patterns
- Performance drops are consistent across different LVLM architectures, indicating a fundamental limitation in cross-lingual knowledge transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LVLMs perform optimally when both instructions and outputs are in English because their Vision Encoder and LLM components are pre-trained predominantly on English data.
- Mechanism: The Vision Encoder and LLM are trained separately on English datasets, and during LVLM integration, the knowledge alignment is optimized for English. When inputs are in other languages, the LLM struggles to properly process the instruction, leading to degraded performance even if the output language is English.
- Core assumption: Vision Encoder and LLM components retain language-specific knowledge learned during pre-training and this knowledge does not fully transfer across languages.
- Evidence anchors:
  - [abstract]: "pre-training of Vision Encoder and the integrated training of LLMs with Vision Encoder are mainly conducted using English training data"
  - [section]: "the integrated training of LVLMs and the pre-training of Vision Encoder are mainly trained in English data, limiting their ability to achieve optimal performance when handling other languages"
  - [corpus]: Weak evidence - related papers focus on multilingual hallucination but don't directly support this specific mechanism
- Break condition: If Vision Encoder is retrained on multilingual data or if LLM instruction tuning is done in multiple languages, this performance gap should diminish.

### Mechanism 2
- Claim: LVLMs struggle to transfer knowledge learned in English to other languages even when instructed in non-English languages.
- Mechanism: The LVLM architecture creates a knowledge representation that is optimized for English. When given non-English instructions, the LLM component fails to properly map the instruction to the Vision Encoder's English-optimized visual representations, resulting in poor cross-lingual knowledge transfer.
- Core assumption: The knowledge representation created during LVLM integration is not language-agnostic and remains tied to the language of the pre-training data.
- Evidence anchors:
  - [abstract]: "it was observed that LVLMs struggle to effectively manage the knowledge learned from English data"
  - [section]: "LVLMs struggle to effectively utilize the knowledge learned in English when applied to other languages"
  - [corpus]: Weak evidence - related papers discuss multilingual hallucination but not this specific transfer mechanism
- Break condition: If the LVLM architecture incorporates language-agnostic knowledge representations or if multilingual pre-training is implemented.

### Mechanism 3
- Claim: LoRA Tuning in English further degrades performance in other languages because it reinforces English-specific knowledge patterns.
- Mechanism: LoRA fine-tuning adapts the model parameters specifically to English training data patterns. This adaptation process makes the model even more specialized for English, reducing its ability to generalize to other languages that weren't represented in the fine-tuning data.
- Core assumption: LoRA fine-tuning creates parameter adjustments that are specific to the language and domain of the training data.
- Evidence anchors:
  - [section]: "LoRA Tuning in English further degraded performance, suggesting the need for Vision Encoder training on multilingual data rather than English-only datasets"
  - [corpus]: Weak evidence - no direct support in related papers, but this is a logical extension of the multilingual performance gap
- Break condition: If LoRA fine-tuning is performed on multilingual data or if the fine-tuning process incorporates cross-lingual regularization techniques.

## Foundational Learning

- Concept: Knowledge representation alignment between Vision Encoder and LLM
  - Why needed here: The study shows that LVLM performance depends on proper alignment between visual and linguistic knowledge, which is language-specific
  - Quick check question: Why does the same LVLM perform differently when instructions are in English vs. other languages?

- Concept: Cross-lingual knowledge transfer limitations
  - Why needed here: The paper demonstrates that LVLMs cannot effectively transfer English-learned knowledge to other languages
  - Quick check question: What architectural changes would be needed for LVLMs to perform equally well across all languages?

- Concept: Fine-tuning adaptation specificity
  - Why needed here: LoRA tuning in English degraded performance in other languages, showing fine-tuning creates language-specific adaptations
  - Quick check question: How does the choice of fine-tuning data language affect cross-lingual performance?

## Architecture Onboarding

- Component map: Vision Encoder (image processing) → Cross-modal integration layer → LLM (language processing) → Output generation
- Critical path: Image input → Vision Encoder features → Cross-modal alignment → LLM instruction processing → Response generation
- Design tradeoffs: English-only pre-training vs. multilingual pre-training (performance vs. resource requirements)
- Failure signatures: Performance drops when instructions are in non-English languages; LoRA tuning in English degrades multilingual performance
- First 3 experiments:
  1. Test LVLM performance with English instructions but non-English outputs
  2. Compare LoRA tuning results when using multilingual vs. English-only data
  3. Evaluate knowledge transfer by measuring performance on tasks that require cross-lingual reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pre-training Vision Encoders on multilingual data beyond English significantly improve cross-lingual artwork explanation generation performance?
- Basis in paper: [explicit] The paper concludes that Vision Encoders need to be trained on multilingual data and hypothesizes that current LVLMs perform best only when instructed and output in English due to English-centric pre-training.
- Why unresolved: The paper tested this hypothesis indirectly by showing performance degradation when using non-English instructions/outputs, but did not conduct experiments where Vision Encoders were pre-trained on multilingual data to directly verify the impact.
- What evidence would resolve it: Controlled experiments comparing LVLMs with Vision Encoders pre-trained on multilingual versus English-only data, measuring performance across all ten languages in the Alignment-10 task.

### Open Question 2
- Question: What specific aspects of cultural nuance are lost when creating multilingual datasets through machine translation, and how does this affect artwork explanation quality?
- Basis in paper: [explicit] The authors state that machine translation fails to account for cultural nuances, citing examples like "Mona Lisa" being called "La Gioconda" in Spanish versus direct translations in other languages, and that cultural differences make artwork explanations vary across countries.
- Why unresolved: While the authors created a dataset without machine translation to address this issue, they did not systematically analyze what specific cultural nuances were preserved or lost through their Wikipedia-based approach versus machine translation.
- What evidence would resolve it: Comparative analysis of dataset entries created with and without machine translation, identifying specific cultural references, idiomatic expressions, and context that differ, along with human evaluations of explanation quality in each version.

### Open Question 3
- Question: Why does LoRA Tuning in English training data lead to performance degradation in cross-lingual tasks, and what mechanisms cause this "forgetting" effect?
- Basis in paper: [explicit] The authors observed that LoRA Tuning with English-only data further worsened performance compared to base models, suggesting it led to "forgetting of original performance" and decreased effectiveness in cross-lingual tasks.
- Why unresolved: The paper identifies the phenomenon but does not investigate the underlying mechanisms - whether this is catastrophic forgetting, interference between language representations, or degradation of multilingual capabilities during fine-tuning.
- What evidence would resolve it: Detailed analysis of parameter changes during LoRA Tuning, examining how English fine-tuning affects cross-lingual representations, and experiments testing different fine-tuning strategies (e.g., multilingual fine-tuning, parameter-efficient methods) to preserve cross-lingual capabilities.

## Limitations

- Limited to artwork domain from Wikipedia, may not generalize to other visual content or domains
- Dataset construction may introduce selection bias toward culturally significant artworks across multiple languages
- Limited architectural analysis of why cross-lingual knowledge transfer fails, relying primarily on empirical observations
- No empirical validation of proposed solution (multilingual Vision Encoder training)

## Confidence

- Confidence in core finding that LVLMs perform best with English instructions/outputs: **High**
- Confidence in LoRA tuning degradation observation: **High**
- Confidence in mechanism explaining cross-lingual transfer failure: **Medium**
- Confidence in proposed multilingual Vision Encoder solution: **Low-Medium**

## Next Checks

1. **Ablation Study on Language-Specific Components**: Conduct controlled experiments to isolate whether performance degradation is due to the Vision Encoder, LLM, or cross-modal integration layer by testing each component's language-specific behavior.

2. **Multilingual Pre-training Validation**: Implement a small-scale multilingual pre-training experiment for the Vision Encoder to empirically test whether multilingual training improves cross-lingual performance as hypothesized.

3. **Domain Generalization Test**: Evaluate the same LVLM models on cross-lingual explanation tasks outside the artwork domain (e.g., everyday objects, scenes) to determine if the observed limitations are specific to artworks or generalize to broader visual content.