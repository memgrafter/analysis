---
ver: rpa2
title: Sentiment Analysis of Lithuanian Online Reviews Using Large Language Models
arxiv_id: '2407.19914'
source_url: https://arxiv.org/abs/2407.19914
tags:
- sentiment
- lithuanian
- language
- text
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents a novel approach to sentiment analysis of\
  \ Lithuanian online reviews using fine-tuned multilingual large language models\
  \ (LLMs). The authors address the challenge of sentiment classification for Lithuanian,\
  \ a less-studied and less-resourced language, by applying transformer models\u2014\
  specifically DistilBERT and ByT5\u2014to a dataset of five-star-based reviews from\
  \ multiple domains."
---

# Sentiment Analysis of Lithuanian Online Reviews Using Large Language Models

## Quick Facts
- arXiv ID: 2407.19914
- Source URL: https://arxiv.org/abs/2407.19914
- Reference count: 0
- Primary result: Fine-tuned multilingual LLMs achieve 80.74% accuracy for one-star and 89.61% for five-star Lithuanian reviews, outperforming GPT-4

## Executive Summary
This study presents a novel approach to sentiment analysis of Lithuanian online reviews using fine-tuned multilingual large language models (LLMs). The authors address the challenge of sentiment classification for Lithuanian, a less-studied and less-resourced language, by applying transformer models—specifically DistilBERT and ByT5—to a dataset of five-star-based reviews from multiple domains. The dataset was collected, cleaned, and prepared to simulate real-world conditions, including handling spam, language inconsistencies, and varying review lengths. The fine-tuned models achieved high accuracy, particularly for unambiguous sentiments, significantly outperforming the commercial GPT-4 model.

## Method Summary
The authors collected 123,604 Lithuanian five-star-based online reviews from pigu.lt, atsiliepimai.lt, and google.com/maps. After cleaning and anonymizing the data, they fine-tuned two multilingual transformer models: DistilBERT (distilbert-base-multilingual-cased) and ByT5 (ByT5-Lithuanian-gec-100h). The models were trained using supervised learning with the star ratings as sentiment labels, and evaluated using accuracy and F1-score metrics on test splits. The study focused on both five-category (emotional/rational positive/negative, neutral) and simplified three-category sentiment classification.

## Key Results
- Fine-tuned models achieved 80.74% accuracy for one-star and 89.61% for five-star reviews
- Both models outperformed GPT-4 on Lithuanian sentiment classification task
- Simplified three-category classification (negative/neutral/positive) improved overall accuracy
- Models showed improved performance on datasets with imbalanced class distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning multilingual BERT and T5 models significantly outperforms a general-purpose LLM (GPT-4) on Lithuanian sentiment analysis.
- Mechanism: Multilingual models pre-trained on Wikipedia in 104 languages, including Lithuanian, capture foundational linguistic patterns. Fine-tuning these models on a curated Lithuanian review dataset adapts the general representations to the specific domain and language nuances, improving classification accuracy.
- Core assumption: The multilingual pre-training includes sufficient Lithuanian data for the model to learn relevant language features, and fine-tuning can effectively adapt these features for the specific sentiment classification task.
- Evidence anchors:
  - [abstract] "The fine-tuned models achieved high accuracy, particularly for unambiguous sentiments: 80.74% for one-star and 89.61% for five-star reviews, significantly outperforming the commercial GPT-4 model."
  - [section 4.2] "We analyze a DistilBERT-type model... The model we chose for finetuning was 'distilbert-base-multilingual-cased'... trained on a Wikipedia dataset of 104 languages, including Lithuanian."
  - [corpus] Weak evidence: No direct citations comparing multilingual vs. monolingual models for Lithuanian sentiment analysis found.

### Mechanism 2
- Claim: Handling language inconsistencies and removing spam improves model performance.
- Mechanism: Cleaning the dataset by removing non-Lithuanian reviews, empty comments, and spam ensures the model trains on relevant, high-quality data, reducing noise and improving its ability to learn meaningful sentiment patterns.
- Core assumption: The cleaning steps effectively remove irrelevant or misleading data without discarding valuable information, and the remaining data accurately represents the target sentiment distribution.
- Evidence anchors:
  - [section 4.1] "To improve the quality of the data, the following main steps have been taken... Language selection... We translated straightforward reviews... Finally, we discarded over 10,000 reviews in this step."
  - [section 5] "Both models demonstrated improved performance on datasets with more imbalanced distribution of classes."
  - [corpus] Weak evidence: No direct citations on the impact of data cleaning on Lithuanian sentiment analysis performance found.

### Mechanism 3
- Claim: Using a five-star rating system as sentiment labels provides a clear and intuitive classification task for the model.
- Mechanism: The five-star rating system directly maps to sentiment intensity (1-star = negative, 5-star = positive), providing the model with explicit, user-generated sentiment labels for supervised learning. This simplifies the task compared to inferring sentiment from unstructured text alone.
- Core assumption: Users consistently and accurately express their sentiment through the star rating, and the textual review content aligns with the assigned star rating.
- Evidence anchors:
  - [section 4.1] "The scraped user responses include a variety of subjects... We aimed to predict the marked ratings based on the review text alone for supervised sentiment analysis. Using the review rating as a sentiment label is common in sentiment analysis."
  - [section 5] "When the sentiment categories are reduced from the five to three (negative, neutral, positive), the overall accuracy improves..."
  - [corpus] Weak evidence: No direct citations on the effectiveness of star ratings as sentiment labels for Lithuanian reviews found.

## Foundational Learning

- Concept: Tokenization
  - Why needed here: LLMs cannot process raw text; tokenization breaks text into manageable units (tokens) that the model can understand and process.
  - Quick check question: What are the two main tokenization methods mentioned for BERT and T5 models, and how do they differ?
- Concept: Word Embeddings
  - Why needed here: Word embeddings provide dense vector representations of words, capturing semantic relationships and allowing the model to understand word meanings and context.
  - Quick check question: Which embedding methods are used in BERT and T5 models, and how do they relate to their respective tokenization algorithms?
- Concept: Fine-tuning
  - Why needed here: Fine-tuning adapts pre-trained models to a specific downstream task (sentiment analysis) by training them on a labeled dataset relevant to that task.
  - Quick check question: Why is fine-tuning preferred over training a model from scratch for this task, especially for a less-resourced language like Lithuanian?

## Architecture Onboarding

- Component map: Data collection and cleaning pipeline → Dataset preparation (train/eval/test splits) → Tokenization → Model loading (DistilBERT/ByT5) → Fine-tuning → Evaluation (accuracy, F1-score)
- Critical path: Data cleaning → Tokenization → Fine-tuning → Evaluation
- Design tradeoffs: Using multilingual models vs. monolingual models (resource constraints vs. potential performance gains), fine-tuning the entire model vs. freezing some layers (adaptability vs. overfitting risk), five-star labels vs. inferred sentiment (clear labels vs. potential label noise)
- Failure signatures: Overfitting (validation loss increases, training accuracy much higher than test accuracy), underfitting (low accuracy on both training and test sets), class imbalance issues (poor performance on minority classes)
- First 3 experiments:
  1. Fine-tune DistilBERT on the entire dataset and evaluate accuracy and F1-score on the test set.
  2. Fine-tune ByT5 on the entire dataset and evaluate accuracy and F1-score on the test set.
  3. Fine-tune DistilBERT on a balanced subset of the dataset and evaluate accuracy and F1-score on the test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would a monolingual Lithuanian LLM significantly outperform the multilingual models tested in this study?
- Basis in paper: [explicit] The authors state "Currently, there are no known monolingual Lithuanian language LLM models openly available" and suggest that "creating and pre-training a monolingual model is a computationally expensive and time-consuming task" but believe it "could help the LLMs to generalize specific language in more detail and extract greater language context features."
- Why unresolved: No monolingual Lithuanian LLM exists to compare against the multilingual models tested.
- What evidence would resolve it: Development and testing of a monolingual Lithuanian LLM on the same sentiment analysis task to compare performance metrics.

### Open Question 2
- Question: How would different dataset sizes and compositions affect model performance for Lithuanian sentiment analysis?
- Basis in paper: [inferred] The authors note that both models showed "improved performance on datasets with more imbalanced distribution of classes" and observed "quick overfitting to the training data." They also mention that "careful dataset selection" is needed.
- Why unresolved: The study used specific dataset sizes and compositions, but did not systematically explore how variations in these factors affect performance.
- What evidence would resolve it: Controlled experiments varying dataset size, class balance, and composition to measure impact on model accuracy and generalization.

### Open Question 3
- Question: What strategies could effectively mitigate overfitting in fine-tuned Lithuanian sentiment analysis models?
- Basis in paper: [explicit] The authors observed "a tendency for quick overfitting to the training data was observed in both cases" for both DistilBERT and ByT5 models, and noted this as a challenge requiring further attention.
- Why unresolved: While overfitting was observed, the paper did not implement or test specific regularization or data augmentation strategies to address this issue.
- What evidence would resolve it: Implementation and testing of various regularization techniques, data augmentation methods, or architectural modifications to reduce overfitting and improve model generalization.

## Limitations

- Exact dataset composition and preprocessing steps are not fully specified, limiting reproducibility
- Hyperparameter configurations for model training are not explicitly stated, which could significantly impact performance
- Study lacks detailed error analysis, leaving reasons for misclassifications unclear
- Comparison with GPT-4 is based on a single commercial model without exploring other fine-tuned alternatives

## Confidence

- **High Confidence**: Fine-tuned multilingual models (DistilBERT and ByT5) outperform GPT-4 on Lithuanian sentiment analysis
- **Medium Confidence**: Data cleaning steps (language filtering, spam removal) effectively improve model performance
- **Low Confidence**: Five-star rating system provides consistent and accurate sentiment labels without noise

## Next Checks

1. Perform an ablation study to quantify the impact of each data cleaning step (language filtering, spam removal, length constraints) on model performance by comparing results with and without each step.

2. Conduct a sensitivity analysis on key hyperparameters (learning rate, batch size, dropout) to identify optimal configurations and assess their impact on model performance and overfitting.

3. Fine-tune a monolingual BERT model specifically trained on Lithuanian (if available) and compare its performance with the multilingual models to determine if the multilingual approach provides a significant advantage for this task.