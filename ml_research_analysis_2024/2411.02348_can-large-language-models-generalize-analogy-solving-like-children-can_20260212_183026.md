---
ver: rpa2
title: Can Large Language Models generalize analogy solving like children can?
arxiv_id: '2411.02348'
source_url: https://arxiv.org/abs/2411.02348
tags:
- llms
- children
- alphabet
- symbol
- greek
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study compared human (children and adults) and large language
  model (LLM) performance on letter-string analogy problems across three alphabet
  domains: Latin (familiar), Greek (near transfer), and a symbol list (far transfer).
  While both children and adults maintained high performance across all domains, all
  LLMs showed significant performance degradation when moving from the Latin to Greek
  and especially to the symbol domain.'
---

# Can Large Language Models generalize analogy solving like children can?

## Quick Facts
- arXiv ID: 2411.02348
- Source URL: https://arxiv.org/abs/2411.02348
- Authors: Claire E. Stevenson; Alexandra Pafford; Han L. J. van der Maas; Melanie Mitchell
- Reference count: 19
- Primary result: LLMs show significant performance degradation when transferring analogical reasoning to novel alphabets, unlike humans

## Executive Summary
This study compares human (children and adults) and large language model (LLM) performance on letter-string analogy problems across three alphabet domains: Latin (familiar), Greek (near transfer), and a symbol list (far transfer). While both children and adults maintain high performance across all domains, all LLMs show significant performance degradation when moving from the Latin to Greek and especially to the symbol domain. The findings suggest that current LLMs struggle with the robust, flexible analogical transfer that is characteristic of human intelligence, and this inability persists even with model scaling.

## Method Summary
The study tested 42 children (7-9 years), 62 adults, and 55 runs of each of four LLMs (Claude-3.5, Gemma-2 27B, GPT-4o, Llama-3.1 405B) on letter-string analogy problems across three alphabet conditions. Participants solved problems like "If abc changes to abd, what should pqr change to?" using Latin, Greek, or a symbol list alphabet. Human participants used a browser-based interface with practice items and feedback, while LLMs were tested with specific prompt templates and temperature=0. Performance was analyzed using mixed ANOVAs to compare accuracy across groups and alphabets, with additional error analysis using Levenshtein string distance.

## Key Results
- Children and adults maintained high performance across all three alphabet domains (Latin, Greek, symbol list)
- All LLMs showed significant performance degradation when moving from Latin to Greek and especially to symbol domain
- Error patterns differed: children's errors were more distant from correct responses compared to LLMs, which often applied literal or single-rule transformations
- Model scaling improved performance but did not eliminate the transfer gap

## Why This Works (Mechanism)

### Mechanism 1
LLMs fail to transfer analogical reasoning to novel alphabets because they lack an abstract representation of "alphabet as ordered sequence." They rely on surface-level character embeddings and next-token prediction, which work for familiar alphabets but break down when the sequence structure is unfamiliar.

### Mechanism 2
Error patterns differ because children use associative strategies while LLMs apply literal or single-rule transformations. Children's errors are more distant from correct answers, suggesting they may revert to non-analogical strategies when problem load exceeds capacity, while LLMs' errors are closer to correct answers, often applying one correct rule partially.

### Mechanism 3
Model size improves generalization because larger models better capture abstract relational structures required for analogical transfer. Scaling laws suggest larger models develop richer internal representations that can handle more complex rule combinations and abstract mappings.

## Foundational Learning

- **Concept: Analogical reasoning and transfer**
  - Why needed here: The study investigates whether LLMs can transfer analogical reasoning across different alphabet domains
  - Quick check question: What is the difference between near transfer (Greek alphabet) and far transfer (symbol list) in this study?

- **Concept: Letter-string analogy solving**
  - Why needed here: The task involves transforming letter strings according to rules, which is the core cognitive ability being tested
  - Quick check question: In the example "If abc changes to abd, what should pqr change to?", what is the correct answer and why?

- **Concept: Error analysis and Levenshtein distance**
  - Why needed here: Understanding how error patterns differ between humans and LLMs requires knowledge of string distance metrics
  - Quick check question: What does a Levenshtein distance of 6 between two responses indicate about their similarity?

## Architecture Onboarding

- **Component map**: Data collection pipeline (human vs. LLM) -> Prompt engineering templates -> Performance analysis -> Error analysis -> Rule check task generator -> Next-previous letter task generator
- **Critical path**: Prompt template selection → Data collection → Performance analysis → Error analysis → Interpretation of results
- **Design tradeoffs**: Prompt engineering complexity vs. performance, model size vs. computational cost, error categorization granularity vs. automated coding feasibility
- **Failure signatures**: Performance degradation in novel alphabets, error patterns dominated by literal or single-rule transformations, inability to handle predecessor/second successor rules
- **First 3 experiments**:
  1. Test whether explicit ordinal encoding of novel alphabets improves LLM performance
  2. Compare error patterns when varying task complexity and cognitive load
  3. Evaluate whether fine-tuning on diverse alphabet domains enables better transfer

## Open Questions the Paper Calls Out

### Open Question 1
What specific cognitive mechanisms enable children to flexibly represent novel alphabets as ordered sequences during analogical reasoning? While the paper identifies that LLMs struggle with this flexibility, it does not investigate the underlying cognitive mechanisms that allow children to successfully represent novel alphabets as ordered sequences.

### Open Question 2
Does model scaling continue to improve LLM performance on far transfer analogical reasoning tasks, or is there a plateau beyond which scaling laws break down? The study tested only four LLM sizes, and while scaling effects were observed, the relationship between model size and far transfer performance remains unclear.

### Open Question 3
Can LLMs be trained or prompted to develop more flexible representations of ordered sequences that would enable better analogical transfer to novel domains? While the paper identifies this limitation, it does not explore whether architectural modifications or specialized prompting strategies could help LLMs develop more flexible sequence representations.

## Limitations

- Findings may be limited to letter-string analogies and may not generalize to other forms of analogical reasoning
- Only four LLM models were tested, all of which are decoder-only or encoder-decoder architectures
- The symbol list domain used a limited set of 8 characters, which may not adequately represent truly novel alphabets

## Confidence

- **High confidence**: LLMs show significant performance degradation when transferring analogical reasoning to novel alphabets
- **Medium confidence**: This degradation reflects an inability to create on-the-fly representations of novel alphabets
- **Low confidence**: Children use associative strategies while LLMs apply literal or single-rule transformations

## Next Checks

1. Test explicit ordinal encoding by providing positional information for novel alphabet characters to determine if the issue is representational rather than fundamental
2. Expand alphabet complexity by testing LLM performance on analogies using larger symbol sets (20+ characters) to better assess far transfer capabilities
3. Cross-domain transfer validation by testing whether LLMs that perform well on letter-string analogies also show similar transfer limitations on other analogical reasoning tasks (visual analogies, semantic analogies)