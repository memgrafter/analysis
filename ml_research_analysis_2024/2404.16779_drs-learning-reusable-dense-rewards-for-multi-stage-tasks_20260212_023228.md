---
ver: rpa2
title: 'DrS: Learning Reusable Dense Rewards for Multi-Stage Tasks'
arxiv_id: '2404.16779'
source_url: https://arxiv.org/abs/2404.16779
tags:
- reward
- rewards
- stage
- learning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DrS (Dense reward learning from Stages), a
  method to learn reusable dense rewards for multi-stage tasks from sparse rewards
  and demonstrations. DrS trains discriminators to distinguish success from failure
  trajectories using sparse rewards, optionally leveraging demonstrations.
---

# DrS: Learning Reusable Dense Rewards for Multi-Stage Tasks

## Quick Facts
- arXiv ID: 2404.16779
- Source URL: https://arxiv.org/abs/2404.16779
- Reference count: 40
- Primary result: DrS learns dense rewards from sparse rewards/demonstrations, improving RL sample efficiency and matching human-engineered rewards on some tasks.

## Executive Summary
DrS (Dense reward learning from Stages) addresses the challenge of reward engineering for multi-stage tasks by automatically learning dense rewards from sparse rewards and demonstrations. The method trains discriminators to distinguish success from failure trajectories, then extends this to multi-stage tasks by combining stage-specific discriminators into a structured reward. Evaluated across 1000+ task variants from three ManiSkill task families, DrS significantly improves reinforcement learning sample efficiency and performance compared to sparse rewards, while reducing the human effort required for reward design.

## Method Summary
DrS learns dense rewards through a two-stage process: first training discriminators on success vs failure trajectories using sparse rewards (optionally augmented with demonstrations), then extending to multi-stage tasks by training separate discriminators for each stage. These stage-specific discriminators are combined into a structured reward signal that guides RL agents through complex task hierarchies. The method leverages the structure of multi-stage tasks by decomposing them into sequential stages, each with its own success criteria, and learns to provide dense feedback at each stage transition.

## Key Results
- DrS significantly improves RL sample efficiency compared to sparse rewards across 1000+ task variants
- Learned dense rewards match performance of human-engineered rewards on some tasks
- Method reduces reward engineering effort by automatically generating dense rewards from weak supervision
- Demonstrated effectiveness across three ManiSkill task families with diverse multi-stage requirements

## Why This Works (Mechanism)
DrS works by transforming weak supervision (sparse rewards/demonstrations) into rich, dense reward signals through discriminator learning. By training discriminators to distinguish between successful and failed trajectories, the method captures nuanced progress indicators that sparse rewards miss. The multi-stage extension leverages task structure by decomposing complex behaviors into sequential stages, each with its own discriminator, enabling the agent to receive meaningful feedback at every stage transition rather than only at task completion.

## Foundational Learning
- **Sparse Reward RL**: Why needed - baseline challenging setting; Quick check - verify agent struggles with only terminal rewards
- **Imitation Learning from Demonstrations**: Why needed - provides additional supervision when sparse rewards insufficient; Quick check - confirm demonstrations improve discriminator quality
- **Multi-Stage Task Decomposition**: Why needed - enables structured reward learning for complex behaviors; Quick check - ensure task can be broken into sequential stages
- **Discriminator-based Reward Learning**: Why needed - transforms weak signals into dense feedback; Quick check - validate discriminator accurately distinguishes success/failure
- **Reward Shaping Theory**: Why needed - ensures learned rewards preserve optimal policy; Quick check - verify dense rewards don't change optimal behavior

## Architecture Onboarding

Component map: State/Action -> Discriminator -> Dense Reward -> RL Agent

Critical path: The discriminators are trained first using sparse rewards and demonstrations, then frozen while the RL agent learns using the generated dense rewards. The multi-stage extension adds a stage identification module that routes observations to the appropriate stage discriminator.

Design tradeoffs: DrS trades increased computation during discriminator training for improved sample efficiency during RL. The method assumes access to demonstrations as optional supervision, which may not always be available. The stage decomposition requires prior knowledge of task structure, limiting applicability to tasks where such structure is known.

Failure signatures: Poor discriminator training leads to uninformative dense rewards that don't improve over sparse rewards. Incorrect stage identification causes the agent to receive rewards from wrong stage discriminators. Overfitting to training trajectories results in dense rewards that don't generalize to agent behaviors.

First experiments:
1. Train a single-stage discriminator on a simple task and verify it generates meaningful dense rewards
2. Test multi-stage extension on a task with clearly defined stages and confirm stage identification works
3. Compare RL sample efficiency with DrS rewards versus sparse rewards on a representative task

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization to entirely different domains or real-world robotics scenarios remains uncertain
- Performance on tasks with highly complex or long-horizon dependencies may be limited
- Computational overhead of training multiple discriminators could be prohibitive for very complex tasks
- Reliance on sparse rewards and demonstrations as supervision sources constrains applicability

## Confidence

**High Confidence**: The core contribution of learning dense rewards from sparse rewards and demonstrations is well-validated through extensive experiments on ManiSkill tasks.

**Medium Confidence**: The method's ability to generalize across 1000+ task variants and its potential to reduce reward engineering effort are supported, but real-world applicability requires further validation.

**Low Confidence**: The long-term scalability of DrS to extremely complex or unstructured environments is not thoroughly addressed.

## Next Checks

1. **Cross-Domain Evaluation**: Test DrS on non-ManiSkill domains, such as autonomous driving or manipulation tasks with different sensory inputs, to assess generalizability.

2. **Real-World Deployment**: Validate DrS on physical robotic systems to evaluate its robustness to sensor noise, actuation delays, and environmental variability.

3. **Scalability Analysis**: Investigate the computational and memory requirements of DrS for tasks with very long horizons or high-dimensional state spaces to determine practical limits.