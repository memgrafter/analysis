---
ver: rpa2
title: Offline Imitation Learning from Multiple Baselines with Applications to Compiler
  Optimization
arxiv_id: '2403.19462'
source_url: https://arxiv.org/abs/2403.19462
tags:
- policy
- which
- learning
- policies
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies offline imitation learning with multiple baseline
  policies, each suboptimal in isolation but strong in complementary state space regions.
  The authors propose BC-MA X, a simple behavior cloning algorithm that selects the
  highest-reward trajectory for each context and mimics its actions.
---

# Offline Imitation Learning from Multiple Baselines with Applications to Compiler Optimization

## Quick Facts
- arXiv ID: 2403.19462
- Source URL: https://arxiv.org/abs/2403.19462
- Authors: Teodor V. Marinov; Alekh Agarwal; Mircea Trofin
- Reference count: 19
- One-line primary result: BC-MA X combines multiple baseline policies via trajectory selection and behavior cloning, achieving minimax optimality and outperforming initial RL policies in compiler optimization tasks.

## Executive Summary
This work studies offline imitation learning where multiple baseline policies, each suboptimal in isolation, are combined to achieve superior performance. The authors propose BC-MA X, a simple algorithm that selects the highest-reward trajectory for each context and mimics its actions through behavior cloning. They provide theoretical guarantees showing the algorithm is minimax optimal with sample complexity bounds, and demonstrate its effectiveness in compiler optimization for inlining decisions across two real-world datasets, achieving significant binary size savings.

## Method Summary
BC-MA X is a behavior cloning algorithm that addresses offline imitation learning with multiple baseline policies. For each initial state (context), it identifies which baseline policy yields the highest cumulative reward trajectory, then uses behavior cloning to mimic only that trajectory. The algorithm is theoretically justified with minimax optimality guarantees and is applied iteratively in practice, where each new policy is added to the set of baselines for subsequent rounds. The method is demonstrated on compiler optimization tasks, specifically inlining decisions, where it significantly outperforms initial reinforcement learning policies.

## Key Results
- BC-MA X achieves minimax optimality with regret bounds matching theoretical lower bounds up to polylogarithmic factors
- In Chrome on Android experiments, the learned policies achieved 8-10 MB savings in binary size compared to initial RL policies
- Iterative application of BC-MA X progressively improves performance, with each new iteration adding the previous policy to the baseline set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The policy improvement stems from selecting the highest-reward trajectory for each context across all baseline policies.
- Mechanism: For each initial state (context), the algorithm identifies which baseline policy yields the highest cumulative reward trajectory and then uses behavior cloning to mimic only that trajectory. This ensures the learned policy inherits the best-performing actions for each context.
- Core assumption: The best trajectory for a given context is a good representative for all similar states in that context, and behavior cloning can effectively replicate the action sequence from the trajectory.
- Evidence anchors:
  - [abstract]: "BC-MA X, a simple behavior cloning algorithm that selects the highest-reward trajectory for each context and mimics its actions."
  - [section]: "For each contextxj in our dataset, we first choose the trajectory with the highest cumulative reward across all the baselines. Then we use a standard behavior cloning loss to mimic the choice of actions in this trajectory."
- Break condition: If the highest-reward trajectory for a context is highly stochastic or if there are multiple nearly optimal trajectories with different actions, the cloning step may overfit to a single suboptimal choice or fail to generalize.

### Mechanism 2
- Claim: The algorithm achieves minimax optimality because the regret bound matches the lower bound up to logarithmic factors.
- Mechanism: The theoretical analysis shows that BC-MA X's sample complexity bound on regret matches a proven lower bound, meaning no other algorithm can do significantly better in this offline sparse-reward setting without additional assumptions.
- Core assumption: The policy class Π can approximate the best per-context baseline within error ϵ (Assumption 3.1), and the horizon H and number of samples n are the primary factors limiting performance.
- Evidence anchors:
  - [abstract]: "They provide theoretical guarantees showing the algorithm is minimax optimal with sample complexity bounds."
  - [section]: "We complement our analysis with a lower bound showing that the result is unimprovable beyond polylogarithmic factors in our setting."
- Break condition: If the realizability assumption (Assumption 3.1) fails—meaning no policy in Π can closely match the best baseline per context—the regret bound no longer holds, and the algorithm cannot guarantee minimax optimality.

### Mechanism 3
- Claim: Iterative application of BC-MA X improves performance because each new iteration uses the previous learned policy as an additional baseline, expanding the coverage of good trajectories.
- Mechanism: After each BC-MA X iteration, the new policy π_t is added to the set of baselines for the next iteration. The exploration strategy then samples from both the previous policy and the new one, increasing the diversity of high-reward trajectories available for cloning in subsequent rounds.
- Core assumption: The new policy π_t can generate trajectories that are at least as good as or better than previous policies in some contexts, and the exploration step can find these improvements.
- Evidence anchors:
  - [abstract]: "Through iterative application, the learned policies outperform baselines, with the Chrome on Android experiment showing 8-10 MB savings in binary size."
  - [section]: "We demonstrate the versatility of BC-MA X by iteratively applying BC-MA X on the initial expert, along with all prior policies trained using previous BC-MA X iterations as the next set of baselines."
- Break condition: If the new policy π_t does not generate novel or improved trajectories, or if the exploration step fails to find better actions, the iterative improvement stalls and performance plateaus.

## Foundational Learning

- Concept: Behavior cloning (BC) - learning a policy by mimicking expert actions in observed states.
  - Why needed here: BC-MA X is fundamentally a behavior cloning algorithm; understanding how BC works is essential to grasp why selecting the highest-reward trajectory and cloning it is effective.
  - Quick check question: If you have a dataset of state-action pairs from an expert, how would you train a policy to replicate the expert's behavior?

- Concept: Contextual Markov Decision Process (MDP) - an MDP where the initial state (context) determines the transition and reward functions for that episode.
  - Why needed here: The problem setting assumes each module (context) has its own dynamics, and the goal is to learn a policy that performs well across all contexts by combining the strengths of multiple baseline policies.
  - Quick check question: In a contextual MDP, if you have two different initial states, do they share the same transition kernel and reward function?

- Concept: Regret minimization - measuring how much reward is lost by using a learned policy compared to the best possible policy.
  - Why needed here: The theoretical guarantees are framed in terms of regret to the best per-context baseline, which is the metric used to evaluate whether BC-MA X is effective.
  - Quick check question: If a learned policy achieves zero regret, what does that say about its performance relative to the best baseline?

## Architecture Onboarding

- Component map: Data collection -> BC-MA X algorithm -> Weighted behavior cloning loss -> Iterative policy updates -> Evaluation (binary size savings)
- Critical path:
  1. Collect initial corpus and baseline trajectories
  2. Run BC-MA X to learn first policy π_1
  3. Use π_1 (and exploration) to collect new trajectories
  4. Add π_1 to baselines, run BC-MA X again to get π_2
  5. Repeat until convergence or iteration limit
  6. Evaluate final policy on test set
- Design tradeoffs:
  - Deterministic vs. stochastic policies: The learned policy is stochastic (outputs a distribution), but the baseline policies used for data collection are deterministic (argmax). This allows exploration during data collection but may introduce variance in training.
  - Exploration frequency: More exploration steps increase diversity but also computational cost and may introduce low-reward trajectories into the dataset.
  - Weighting scheme: Using size-based weights focuses learning on important modules but may neglect smaller modules that could still contribute to overall performance.
- Failure signatures:
  - If the learned policy performs worse than the initial baseline, likely causes are: insufficient exploration, poor weighting scheme, or the policy class Π cannot represent the best per-context baseline (Assumption 3.1 fails).
  - If training is unstable or the policy collapses to a single action, likely causes are: overfitting to a single high-reward trajectory, insufficient regularization, or poor exploration strategy.
- First 3 experiments:
  1. Train a single iteration of BC-MA X on the initial baseline (e.g., ES or PPO) and measure binary size savings on the training corpus.
  2. Run two iterations of BC-MA X with and without exploration, compare performance to isolate the impact of exploration.
  3. Evaluate the final policy on a held-out test set to confirm generalization beyond the training corpus.

## Open Questions the Paper Calls Out

- Question: How can the exploration strategy in BC-MAX be improved to balance exploration and exploitation more effectively?
  - Basis in paper: [explicit] The paper mentions that the exploration strategy uses the non-deterministic policy ˆπt−1 and explores at the call-site where ˆπ is least confident about the action. However, it leaves such approaches as future work.
  - Why unresolved: The current exploration strategy might not be optimal and could potentially lead to suboptimal exploration in certain scenarios.
  - What evidence would resolve it: Empirical results comparing the performance of BC-MAX with different exploration strategies on various datasets would help determine the most effective approach.

- Question: Can BC-MAX be extended to handle sparse reward settings where the reward is not only sparse at the trajectory level but also noisy or partially observable?
  - Basis in paper: [inferred] The paper assumes deterministic rewards and does not address the case of noisy or partially observable rewards. This limitation could hinder the applicability of BC-MAX in real-world scenarios where such conditions are common.
  - Why unresolved: The current theoretical analysis and empirical results do not cover the case of noisy or partially observable rewards.
  - What evidence would resolve it: Theoretical analysis and empirical results demonstrating the performance of BC-MAX in settings with noisy or partially observable rewards would be necessary to address this question.

- Question: How does the performance of BC-MAX scale with the number of baseline policies (K) and the number of trajectories per policy (n)?
  - Basis in paper: [explicit] The paper provides theoretical guarantees on the regret of BC-MAX, but does not explore the empirical scaling behavior with respect to K and n.
  - Why unresolved: The theoretical analysis provides bounds on the regret, but does not give insights into how the performance of BC-MAX changes with the number of baseline policies and trajectories.
  - What evidence would resolve it: Empirical results showing the performance of BC-MAX on datasets with varying numbers of baseline policies and trajectories would help understand its scaling behavior.

## Limitations
- The policy class Π and its capacity to approximate the best baseline per context are not specified, which is critical for the theoretical guarantees to hold.
- The exploration strategy details are underspecified, making it unclear how much diversity is injected into the trajectory set between iterations.
- No ablation on the weighting scheme (size-based vs. uniform) is provided, so the impact of this design choice on real-world performance remains unclear.

## Confidence
- High Confidence: The core mechanism of selecting the highest-reward trajectory per context and using behavior cloning is clearly described and theoretically justified.
- Medium Confidence: The iterative improvement claim is supported by experimental results but lacks theoretical backing and detailed ablation studies.
- Low Confidence: The lower bound result and its tightness relative to the upper bound are not fully verified due to lack of independent theoretical analysis.

## Next Checks
1. Reproduce the single-iteration BC-MA X experiment on a small corpus to verify the basic mechanism works as described.
2. Run an ablation study comparing size-based weighting vs. uniform weighting on binary size outcomes to quantify the impact of the weighting scheme.
3. Evaluate the learned policy on a held-out test set with modules not seen during training to confirm generalization beyond the training corpus.