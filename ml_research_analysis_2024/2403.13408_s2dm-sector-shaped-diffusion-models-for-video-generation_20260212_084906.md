---
ver: rpa2
title: 'S2DM: Sector-Shaped Diffusion Models for Video Generation'
arxiv_id: '2403.13408'
source_url: https://arxiv.org/abs/2403.13408
tags:
- video
- generation
- diffusion
- temporal
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Sector-Shaped Diffusion Model (S2DM) for
  video generation. The key idea is to model the generation of a group of intrinsically
  related video frames sharing the same semantic and stochastic features, but varying
  in temporal features, as a sector-shaped inverse diffusion area formed by ray-shaped
  reverse diffusion processes starting from the same noise point.
---

# S2DM: Sector-Shaped Diffusion Models for Video Generation

## Quick Facts
- arXiv ID: 2403.13408
- Source URL: https://arxiv.org/abs/2403.13408
- Reference count: 40
- Primary result: S2DM achieves state-of-the-art FVD/KVD scores on MHAD and MUG datasets without explicit temporal-feature modeling modules.

## Executive Summary
This paper introduces Sector-Shaped Diffusion Models (S2DM) for video generation, addressing the challenge of maintaining consistency and continuity across frames. The key innovation is modeling the generation of a group of intrinsically related video frames as a sector-shaped inverse diffusion area formed by ray-shaped reverse diffusion processes starting from the same noise point. This allows multiple frames to share semantic and stochastic features while varying in temporal features. The authors apply S2DM to video generation tasks using optical flow as temporal conditions, achieving superior performance on MHAD and MUG datasets. For text-to-video generation, they propose a two-stage strategy that decouples content and motion generation, enabling competitive performance without additional training.

## Method Summary
S2DM modifies standard diffusion by enforcing a shared noise assumption during training, where all frames in a video sample start from the same noise point in latent space. The model is built on Stable Diffusion 1.5, fine-tuned on video datasets with optical flow as temporal conditions injected via cross-attention. For text-to-video tasks without explicit temporal guidance, S2DM uses a two-stage approach: first generating optical flow from text using a pretrained flow generator, then running S2DM with the synthesized flow. The model is trained using Adam optimizer with learning rate 2e-5 on 128x128 resolution videos divided into 16 frames.

## Key Results
- S2DM achieves best or near-best FVD and KVD scores on MHAD and MUG datasets compared to existing methods
- Outperforms VDM and LVDM in video generation without explicit temporal-feature modeling modules
- Two-stage text-to-video generation achieves comparable performance to existing works without additional training
- Demonstrates high-quality, consistent videos with strong continuity across frames

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sector-shaped reverse diffusion allows multiple frames to share a common noise seed while varying only in temporal features
- Mechanism: In standard diffusion, each frame starts from independent noise; in S2DM, all frames start from the same noise and follow ray-shaped reverse paths in the latent space. This shared starting point forces semantic and stochastic consistency across frames, while conditional guidance on optical flow injects temporal variation.
- Core assumption: Frames from the same video, when perturbed with the same Gaussian noise at every diffusion step, will converge to the same final noise point in latent space.
- Evidence anchors:
  - [abstract] "model the generation of a group of intrinsically related video frames sharing the same semantic and stochastic features, but varying in temporal features, as a sector-shaped inverse diffusion area formed by ray-shaped reverse diffusion processes starting from the same noise point."
  - [section 3.2] Derivation of shared-noise forward process and convergence argument.
- Break condition: If optical flow guidance is noisy or inconsistent, shared-noise assumption alone cannot guarantee frame consistency; temporal conditions must be high-quality.

### Mechanism 2
- Claim: Optical flow as temporal condition decouples motion modeling from content modeling
- Mechanism: Flow fields encode per-pixel motion between the first frame and each target frame; feeding them as cross-attention conditions to the U-Net steers denoising toward the correct spatial arrangement without needing explicit frame-to-frame denoising.
- Core assumption: Optical flow between frames captures sufficient motion information to guide consistent frame synthesis without recurrent or temporal-attention modules.
- Evidence anchors:
  - [section 3.3] "We explore the use of optical flow as temporal conditions. Experimental results show that S2DM outperforms many existing methods in the task of video generation without any temporal-feature modelling modules."
  - [section 4.2] Quantitative FVD/KVD improvements over VDM and LVDM.
- Break condition: For highly complex or occluded motion, optical flow estimates degrade, causing drift in generated content.

### Mechanism 3
- Claim: Two-stage T2V generation decouples content and motion generation, enabling zero-shot temporal condition synthesis
- Mechanism: Stage 1 uses a 3D U-Net to synthesize optical flow from text and a reference frame; Stage 2 runs S2DM with the synthesized flow and text. This avoids the need for paired text-flow data.
- Core assumption: A pretrained flow generator (LFDM) can generalize to unseen text prompts and produce plausible motion cues.
- Evidence anchors:
  - [section 3.4] Description of the two-stage pipeline.
  - [section 4.3] "without additional training, our model integrated with another temporal conditions generative model can still achieve comparable performance with existing works."
- Break condition: If flow generator hallucinates implausible motion, Stage 2 will produce temporally incoherent videos.

## Foundational Learning

- Concept: Diffusion probabilistic models and noise scheduling
  - Why needed here: S2DM modifies the standard diffusion forward process by enforcing shared noise; understanding variance schedules and reparameterization is essential to implement this correctly.
  - Quick check question: What is the role of the α_t schedule in ensuring convergence to pure noise, and how does sharing ν_t at every step change the geometry of the forward process?

- Concept: Cross-attention conditioning in diffusion models
  - Why needed here: Semantic and temporal conditions are injected via CLIP embeddings and optical flow through cross-attention; correct tensor shapes and conditioning scale are critical for stable training.
  - Quick check question: How does the concatenation of CLIP embedding and flow along the channel dimension interact with the attention layer's query/key/value dimensions?

- Concept: Optical flow estimation and its representation
  - Why needed here: Flow is used as a temporal condition; understanding horizontal/vertical displacement encoding and occlusion handling determines the quality of guidance.
  - Quick check question: Why does S2DM concatenate flow with occlusion maps, and what happens if occlusion is ignored?

## Architecture Onboarding

- Component map: CLIP text encoder -> Optical flow extractor (RAFT) -> U-Net noise predictor -> Stable Diffusion 1.5 decoder
- Critical path:
  1. Sample shared noise ν_t for a video batch.
  2. Compute noised latents z_t = sqrt(α̂_t)·z_0 + sqrt(1-α̂_t)·ν_t.
  3. Concatenate CLIP(flow) with text embedding.
  4. Feed into U-Net with timestep embedding.
  5. Predict noise; apply classifier-free guidance.
  6. Optimize L2 noise prediction loss.
- Design tradeoffs:
  - Shared noise vs. independent noise: Consistency gains vs. diversity loss.
  - Optical flow vs. other temporal cues: Precise motion control vs. simpler conditioning.
  - Classifier-free guidance scale: Better alignment vs. over-saturation.
- Failure signatures:
  - High FVD but low KVD: Consistent semantics but poor temporal coherence (flow guidance weak).
  - Low FVD but high KVD: Good temporal but inconsistent semantics (shared noise insufficient).
  - Mode collapse: Over-aggressive guidance scale or too small noise schedule variance.
- First 3 experiments:
  1. Ablation: Train with shared noise vs. independent noise; measure FVD/KVD.
  2. Guidance scaling: Sweep classifier-free guidance scale; plot trade-off curves.
  3. Flow quality: Swap optical flow with random noise or zero flow; observe impact on consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can S2DM effectively separate and decouple semantic conditions and temporal conditions during the diffusion process to improve video generation quality?
- Basis in paper: [inferred] The paper mentions that the current method of incorporating semantic and temporal conditions is relatively straightforward and lacks the ability to effectively separate and decouple these two conditions.
- Why unresolved: The paper does not provide a detailed solution or experimental results on how to improve the separation and decoupling of these conditions.
- What evidence would resolve it: Experimental results demonstrating improved video generation quality when semantic and temporal conditions are incorporated separately at different stages of the diffusion process.

### Open Question 2
- Question: How would using different types of conditions (e.g., posture information as temporal conditions and reference images as semantic conditions) affect the generality and performance of S2DM?
- Basis in paper: [explicit] The paper suggests exploring different conditions as semantic and temporal conditions in future work, such as utilizing posture information as temporal conditions and reference images as semantic conditions.
- Why unresolved: The paper does not provide experimental results on using different types of conditions in S2DM.
- What evidence would resolve it: Experimental results comparing the performance of S2DM using different combinations of semantic and temporal conditions.

### Open Question 3
- Question: How can S2DM be extended to generate more complex and longer videos by composing multiple S2DMs guided by different conditions?
- Basis in paper: [inferred] The paper mentions that following the idea of the two-stage generation strategy, multiple S2DMs could be composed to generate more complex and longer videos.
- Why unresolved: The paper does not provide a detailed plan or experimental results on composing multiple S2DMs.
- What evidence would resolve it: Experimental results demonstrating the ability of composed S2DMs to generate more complex and longer videos compared to a single S2DM.

## Limitations

- The shared noise assumption relies on strong geometric convergence properties that are theoretically intuitive but not empirically validated through latent trajectory visualization
- Optical flow estimation errors or occlusions directly propagate into frame synthesis, with no reported robustness tests under noisy or incomplete flow inputs
- The two-stage text-to-video approach depends on a separate flow generator's generalization ability, with uncertain performance on diverse text prompts without fine-tuning

## Confidence

- High confidence: The experimental results on MHAD and MUG datasets (FVD/KVD scores) are directly measurable and reproducible. The qualitative improvements in consistency are observable from the provided samples.
- Medium confidence: The mechanism of shared noise improving consistency is plausible but not rigorously validated beyond quantitative metrics. The claim that S2DM outperforms existing methods without temporal-feature modeling modules is supported by ablation comparisons, but the ablation study lacks some important baselines (e.g., recurrent models).
- Low confidence: The generalization of the two-stage T2V approach to arbitrary text prompts is asserted but not thoroughly tested; the paper does not report results on standard T2V benchmarks like MSR-VTT or text-to-video zero-shot transfer.

## Next Checks

1. **Latent space trajectory visualization**: Sample a video batch, record the latents at each denoising step, and plot their paths in 2D (e.g., via PCA) to verify that frames indeed follow ray-shaped reverse diffusion from a common noise seed.
2. **Flow robustness test**: Replace ground-truth or RAFT-estimated flow with random noise or zero flow during inference; measure the degradation in FVD/KVD and inspect generated videos for temporal coherence breakdown.
3. **Cross-dataset generalization**: Train S2DM on MHAD, freeze the model, and generate videos on an unseen action dataset (e.g., Weizmann) using the same optical flow conditioning; compare FVD/KVD to a model trained directly on Weizmann.