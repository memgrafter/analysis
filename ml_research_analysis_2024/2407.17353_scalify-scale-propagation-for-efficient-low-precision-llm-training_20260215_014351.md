---
ver: rpa2
title: 'Scalify: scale propagation for efficient low-precision LLM training'
arxiv_id: '2407.17353'
source_url: https://arxiv.org/abs/2407.17353
tags:
- scale
- training
- scalify
- scaling
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Scalify, a method for efficient low-precision
  training of large language models. Scalify generalizes and formalizes existing tensor
  scaling methods by introducing end-to-end scale propagation in computational graphs.
---

# Scalify: scale propagation for efficient low-precision LLM training

## Quick Facts
- arXiv ID: 2407.17353
- Source URL: https://arxiv.org/abs/2407.17353
- Reference count: 11
- Key outcome: Method achieves float32-level accuracy with float8 matrix multiplications and float16 master weights

## Executive Summary
Scalify introduces a novel approach to low-precision training of large language models by generalizing tensor scaling methods. The method represents tensors as pairs of data and scale components, enabling end-to-end scale propagation through computational graphs. This approach supports efficient float8 matrix multiplication, gradient representation, and float16 optimizer state storage while maintaining training accuracy comparable to higher precision baselines.

## Method Summary
Scalify implements end-to-end scale propagation in computational graphs by representing tensors as ScaledArray objects containing data and scale components. The SCALIFY transform traces the computational graph and propagates scales using static rules based on input scales and shapes, with custom rules for activation and normalization layers. The method supports out-of-the-box float8 matrix multiplication and enables float16 optimizer state storage through scaled representations. Dynamic rescaling operations are introduced in the backward pass, particularly for normalization layers, to maintain training stability when gradient distributions shift away from unit-scaling.

## Key Results
- Matches float32 baseline accuracy on GPT2 models using float8 matrix multiplications
- Enables float16 optimizer state storage without accuracy loss through scaled representations
- Achieves training stability with minimal dynamic rescaling operations
- Simplifies low-precision training by decoupling matrix multiplication from scaling operations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling matrix multiplication from scaling allows more efficient low-precision training.
- Mechanism: By representing tensors as pairs of data and scale components, Scalify propagates scaling information end-to-end through the computational graph. This enables matrix multiplications to operate directly on scaled data without requiring intermediate scaling operations within the kernel.
- Core assumption: The unit-scaling property E[X²d] ≈ 1 can be maintained throughout the computational graph using static rules based on input scales and shapes.
- Evidence anchors:
  - [abstract] states that Scalify "generalizes and formalizes existing tensor scaling methods" and "supports out-of-the-box float8 matrix multiplication"
  - [section] describes how "the SCALIFY graph tracer propagates ScaledArray inputs in the computational graph"
- Break condition: If the unit-scaling assumption fails (e.g., with non-Gaussian or highly skewed distributions), the static rules may produce inaccurate scaling estimates, leading to numerical instability.

### Mechanism 2
- Claim: Custom scale propagation rules for activation and normalization layers improve accuracy and efficiency.
- Mechanism: For activation functions, Scalify propagates the same scale through the operation, using the decomposition f(X) = X · g(X) where g is a bounded gating function. For normalization layers, it normalizes the data component and assigns scale 1 to the output.
- Core assumption: Activation functions can be represented as X · g(X) where g is bounded and g(±∞) = 0, and normalization layers aim to produce outputs with mean 0 and variance 1.
- Evidence anchors:
  - [section] provides the mathematical formulation for activation layers: "Common activation functions in deep learning (i.e. relu, gelu, swish, ...) can be represented as: f(X) = X · g(X)"
  - [section] describes the normalization approximation: "We approximate scale propagation in the former as following: X - E[X]p / sqrt(Var[X] + ε) ≃ Xd - E[Xd]p / sqrt(Var[Xd] + ε)"
- Break condition: If the assumed mathematical properties of activation functions or normalization layers don't hold (e.g., non-standard activation functions), the custom rules may produce incorrect scaling.

### Mechanism 3
- Claim: Dynamic rescaling in backward pass gradients maintains training stability.
- Mechanism: Scalify introduces dynamic rescaling operations in the backward pass, particularly for normalization layers, to correct for the shifting distribution of gradients away from the ideal unit-scaling property.
- Core assumption: The distribution of gradients tends to shift away from unit-scaling during the backward pass, requiring correction to maintain training stability.
- Evidence anchors:
  - [section] states: "it is natural to insert some dynamic rescaling operations on gradients in the backward pass, and we decided to follow a similar strategy to forward pass normalization by adding one in each residual path of the Transformer layer"
  - [section] notes: "With the additional LayerNorm gradient dynamic rescaling, experiment SCALIFY FP16 #2 shows similarly to the work of Peng et al. (2023) that per-tensor scaling FP16 can be used for master weight representation without loss in accuracy"
- Break condition: If dynamic rescaling is insufficient to correct for gradient distribution shifts, training may become unstable or fail to converge.

## Foundational Learning

- Concept: ScaledArray representation
  - Why needed here: Provides the fundamental data structure for propagating scale information through the computational graph
  - Quick check question: What are the two components of a ScaledArray and how do they relate to the original tensor?

- Concept: Unit-scaling property
  - Why needed here: Ensures numerical stability and accuracy by maintaining E[X²d] ≈ 1 throughout the computational graph
  - Quick check question: Why is E[X²d] ≈ 1 considered the "sweet spot" for maximizing SNR in FP8 and FP16 representations?

- Concept: Custom scale propagation rules
  - Why needed here: Allows efficient and accurate handling of non-linear operations like activation functions and normalization layers
  - Quick check question: How does the custom scale propagation rule for activation functions differ from automatic scale propagation?

## Architecture Onboarding

- Component map:
  - ScaledArray data structure (data + scale components)
  - SCALIFY transform (graph tracer and primitive implementations)
  - Custom scale propagation rules (activation and normalization layers)
  - Dynamic rescaling operations (backward pass corrections)
  - JAX primitives integration (basic operations and casting)

- Critical path:
  1. Model initialization with ScaledArray state
  2. Forward pass with scale propagation through operations
  3. Backward pass with gradient scaling and dynamic rescaling
  4. Optimizer update with scaled gradients
  5. Optional dynamic rescaling of model state

- Design tradeoffs:
  - Static vs. dynamic scaling estimation (computational efficiency vs. accuracy)
  - Unit-scaling vs. alternative scaling strategies (simplicity vs. potential precision loss)
  - Custom vs. automatic scale propagation (accuracy vs. implementation complexity)

- Failure signatures:
  - Numerical instability (overflow/underflow in computations)
  - Training divergence or failure to converge
  - Accuracy degradation compared to higher precision baselines
  - Increased computational overhead from excessive dynamic rescaling

- First 3 experiments:
  1. Replace loss scaling with Scalify for pure FP16 training and compare accuracy and stability
  2. Implement FP8 matrix multiplications with Scalify and evaluate training accuracy with minimal dynamic rescaling
  3. Store master weights and optimizer state in scaled FP16 using Scalify and measure memory savings and training stability

## Open Questions the Paper Calls Out
- The authors note that dynamic rescaling of model state was not necessary in their experiments to match baseline accuracy, but state this needs further investigation for larger scale models.

## Limitations
- Evaluation limited to single GPT2 model (168M parameters) on WikiText-103 dataset
- Actual performance characteristics of FP8 operations not thoroughly characterized
- Unit-scaling assumption's robustness across diverse model architectures not validated

## Confidence
- **High confidence**: Core mechanism of representing tensors as data+scale pairs and propagating this information through the computational graph
- **Medium confidence**: Custom scale propagation rules for activation functions and normalization layers
- **Medium confidence**: Claim that Scalify can match float32 accuracy with minimal dynamic rescaling based on single model-dataset combination

## Next Checks
1. Evaluate Scalify's effectiveness on larger language models (e.g., 1B+ parameters) and different model architectures to assess scalability and generalizability
2. Conduct ablation studies to quantify the impact of different scale propagation rules on training stability and final accuracy across various precision formats
3. Benchmark Scalify against other low-precision training methods on the same tasks to provide comprehensive comparison of accuracy and computational efficiency