---
ver: rpa2
title: Linguistic Changes in Spontaneous Speech for Detecting Parkinsons Disease Using
  Large Language Models
arxiv_id: '2404.05160'
source_url: https://arxiv.org/abs/2404.05160
tags:
- disease
- language
- speech
- large
- parkinson
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study addresses the challenge of detecting Parkinson\u2019\
  s disease (PD), the second most prevalent neurodegenerative disorder, by leveraging\
  \ large language models (LLMs) to analyze spontaneous speech. Traditional PD detection\
  \ relies on motor symptoms, which appear late in the disease course, limiting early\
  \ diagnosis."
---

# Linguistic Changes in Spontaneous Speech for Detecting Parkinsons Disease Using Large Language Models

## Quick Facts
- arXiv ID: 2404.05160
- Source URL: https://arxiv.org/abs/2404.05160
- Reference count: 0
- Up to 73% accuracy achieved using text-embedding-3 models on PC-GITA dataset

## Executive Summary
This study explores the use of large language models (LLMs) to detect Parkinson's disease from spontaneous speech, addressing the challenge that traditional motor symptom-based diagnosis occurs late in disease progression. The proposed approach automatically transcribes speech using Whisper, generates high-dimensional linguistic embeddings with state-of-the-art LLMs (BERT, XLNet, GPT-2, and OpenAI's text-embedding models), and classifies using support vector machines. Evaluated on the PC-GITA dataset with 50 PD patients and 50 healthy controls, the method achieves up to 73% accuracy, with text-embedding-3 models outperforming other LLMs. The results demonstrate the potential of LLMs in PD detection, though limitations include small dataset size and variability in disease duration.

## Method Summary
The method involves automatically transcribing spontaneous Spanish speech from the PC-GITA dataset using Whisper ASR, then generating text embeddings with various LLMs including BERT, XLNet, GPT-2, and OpenAI's text-embedding models. These high-dimensional embeddings (768-3072 dimensions) serve as features for a support vector machine classifier. The model is evaluated using 10-fold stratified cross-validation with grid search over SVM hyperparameters (kernel, C, and gamma). The approach leverages LLMs' ability to capture linguistic patterns that may correlate with prodromal PD indicators, with the SVM providing robust classification despite the small sample size and high-dimensional feature space.

## Key Results
- Achieved up to 73% accuracy in distinguishing PD patients from healthy controls
- Text-embedding-3 models outperformed BERT, XLNet, and GPT-2
- SVM with appropriate kernel and regularization effectively classified high-dimensional embeddings
- Results demonstrate potential for LLM-based PD detection from spontaneous speech

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can encode linguistic features that capture prodromal PD indicators.
- Mechanism: The intrinsic architecture of LLMs (e.g., BERT, XLNet, GPT-2, text-embedding-3) learns hierarchical linguistic representations during pretraining, enabling them to extract subtle language patterns correlated with early-stage PD.
- Core assumption: Language impairments in prodromal PD are sufficiently distinct to be captured by general-purpose embeddings.
- Evidence anchors:
  - [abstract] "large language models is advancing rapidly, presenting the opportunity to explore the use of these new models for detecting Parkinsons disease"
  - [section] "The architecture, parameter structure, and training of the large language models can be leveraged to extract and encode into text embeddings a unique feature space representing the morphology, syntax, semantics, and pragmatics"
  - [corpus] Weak: No direct corpus citation linking LLM embeddings to PD markers; only high-level FMR overlap with related work.
- Break condition: If prodromal linguistic markers are too subtle or overlap with healthy aging, LLM embeddings may not yield separable feature spaces.

### Mechanism 2
- Claim: Support vector machines with regularization can effectively classify high-dimensional linguistic embeddings despite small sample sizes.
- Mechanism: SVMs construct optimal hyperplanes in the high-dimensional embedding space, and regularization prevents overfitting when the number of features (768-3072) far exceeds the number of samples (100).
- Core assumption: The linguistic embedding space is linearly separable (or can be made so by kernel transformation) for PD vs. HC classification.
- Evidence anchors:
  - [section] "A support vector machine (SVM) is utilized for its robustness when handling such high-dimensional data and for its effectiveness in classification tasks through the creation of optimal hyperplanes in a transformed feature space"
  - [section] "Specifically, SVM models are resistant to overfitting when regularized (Xu et al., 2008)"
  - [corpus] Weak: No corpus citation on SVMs handling embeddings in medical contexts.
- Break condition: If the embedding space is inherently non-linear or the sample size is too small to support reliable hyperplane estimation, SVM performance will degrade.

### Mechanism 3
- Claim: Automatic speech recognition (ASR) transcriptions preserve sufficient linguistic content for PD detection.
- Mechanism: Whisper ASR converts spontaneous speech to text without introducing systematic biases that would mask linguistic PD markers; transcription errors are random and thus do not affect embedding feature distributions significantly.
- Core assumption: ASR errors are independent of PD status and do not systematically distort linguistic features relevant to PD.
- Evidence anchors:
  - [section] "The speech is transcribed automatically with an automated speech recognition model"
  - [section] "Automated transcription may lower accuracy in performance tasks, specifically in the classification of neurodegenerative diseases (Soroski et al., 2022)"
  - [corpus] Weak: No corpus evidence on ASR error patterns in PD vs. HC groups.
- Break condition: If ASR systematically underperforms on PD speech (e.g., due to dysarthria), critical linguistic markers may be lost.

## Foundational Learning

- Concept: Embedding dimensionality and its impact on classification.
  - Why needed here: Understanding why 768 vs. 3072 dimensions are used and how dimensionality reduction affects performance.
  - Quick check question: If a model's embedding is 768-dimensional, how many features will the SVM classifier use?

- Concept: Cross-validation and hyperparameter tuning in small datasets.
  - Why needed here: Ensuring robust model evaluation despite only 100 samples via stratified 10-fold CV.
  - Quick check question: In 10-fold CV with 100 samples, how many samples are in each training and validation fold?

- Concept: Kernel selection for SVMs (poly, rbf, sig).
  - Why needed here: Choosing the kernel that best maps high-dimensional embeddings to separable space.
  - Quick check question: What kernel was used for text-embedding-3-small and why might that choice matter?

## Architecture Onboarding

- Component map: Audio input -> Whisper ASR (Spanish) -> Text transcription -> LLM embedding (BERT, XLNet, GPT-2, text-embedding-3 variants) -> High-dimensional feature vector -> SVM classifier (kernel + C + gamma) -> PD / HC prediction
- Critical path: ASR -> LLM embedding -> SVM classification
- Design tradeoffs:
  - ASR accuracy vs. scalability: manual transcription would be more accurate but not scalable
  - Embedding dimensionality vs. cost: higher dimensions improve representation but increase compute and risk of overfitting
  - Kernel complexity vs. interpretability: RBF offers non-linear separation but is less interpretable than linear
- Failure signatures:
  - Low accuracy despite high embedding quality -> likely due to insufficient separation in feature space
  - High variance across CV folds -> likely overfitting or unstable embeddings
  - ASR errors correlated with PD status -> systematic bias in transcriptions
- First 3 experiments:
  1. Compare SVM performance on manually vs. automatically transcribed data
  2. Test dimensionality reduction (768 vs. 3072) on text-embedding-3-large
  3. Swap SVM for logistic regression to check if linear separability is sufficient

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How early can linguistic markers of Parkinson's disease be detected in spontaneous speech?
- Basis in paper: [inferred] The paper mentions that language impairment can present in the prodromal phase and precede motor symptoms, but the dataset includes patients diagnosed 0.4 to 43 years prior, with a mean of 11.2 years. This variability in disease duration makes it unclear when linguistic changes first appear.
- Why unresolved: The study uses a cross-sectional dataset rather than longitudinal data, and the disease duration varies widely among participants.
- What evidence would resolve it: Longitudinal studies tracking linguistic changes in individuals from pre-diagnosis through various disease stages would clarify the earliest detectable point of linguistic impairment.

### Open Question 2
- Question: Do different neurodegenerative diseases have unique linguistic signatures that can be distinguished from one another?
- Basis in paper: [inferred] The paper notes that previous research has shown high accuracy in using speech signals to distinguish various neurodegenerative diseases, but most studies use binary classification with healthy controls. The authors question whether each disease has a unique signature in its speech patterns.
- Why unresolved: Most existing research focuses on distinguishing one disease from healthy controls rather than differentiating between multiple neurodegenerative conditions.
- What evidence would resolve it: Studies using multi-class classification models trained on spontaneous speech from multiple neurodegenerative diseases (e.g., Parkinson's, Alzheimer's, Huntington's) would determine if unique linguistic signatures exist.

### Open Question 3
- Question: How does the performance of large language models for Parkinson's detection vary across different languages and dialects?
- Basis in paper: [explicit] The authors recommend extending the Spanish-based model to other languages and mention that the manifestation of language impairments across various languages is still unclear.
- Why unresolved: The current study only uses a Spanish dataset, and linguistic features may manifest differently across languages due to structural differences.
- What evidence would resolve it: Evaluating the same LLM-based approach on datasets from multiple languages and dialects would reveal performance variations and potentially identify language-specific linguistic markers.

## Limitations
- Small sample size (n=100) raises concerns about model generalizability and overfitting
- Limited exploration of alternative classification approaches beyond SVMs
- Lack of validation for the independence assumption between ASR errors and PD status
- Cross-sectional dataset prevents evaluation of early detection capability

## Confidence

**High**: LLM embeddings capture linguistic features relevant to PD (supported by prior work on language models in neurodegenerative disease detection)

**Medium**: SVM classification is effective in this high-dimensional, small-sample setting (based on established SVM robustness, but not empirically validated here)

**Low**: ASR transcriptions preserve sufficient linguistic markers for PD detection (weak evidence, potential for systematic bias not ruled out)

## Next Checks
1. Test the impact of manual vs. automatic transcription on classification accuracy to quantify ASR-induced bias
2. Perform ablation studies varying embedding dimensionality (e.g., 768 vs. 3072) to assess optimal feature space size
3. Compare SVM performance against simpler linear classifiers (e.g., logistic regression) to verify the necessity of non-linear kernels