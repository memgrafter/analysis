---
ver: rpa2
title: Scaling Laws for the Value of Individual Data Points in Machine Learning
arxiv_id: '2405.20456'
source_url: https://arxiv.org/abs/2405.20456
tags:
- scaling
- data
- points
- dataset
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces individualized scaling laws that describe
  how the contribution of each data point to model performance changes as the dataset
  size increases. The authors propose a log-linear parametric form for this scaling
  behavior and validate it empirically across multiple model types (logistic regression,
  MLPs, SVMs) and datasets.
---

# Scaling Laws for the Value of Individual Data Points in Machine Learning

## Quick Facts
- arXiv ID: 2405.20456
- Source URL: https://arxiv.org/abs/2405.20456
- Reference count: 40
- Key outcome: Individualized scaling laws show data point contributions follow predictable log-linear decay as dataset size increases, with R² > 0.9 fit across multiple model types

## Executive Summary
This paper introduces individualized scaling laws that describe how the contribution of each data point to model performance changes as the dataset size increases. The authors propose a log-linear parametric form for this scaling behavior and validate it empirically across multiple model types (logistic regression, MLPs, SVMs) and datasets. They develop efficient estimators to learn these scaling parameters from limited noisy observations per data point, including a maximum likelihood estimator and an amortized neural estimator. The results show that the scaling law achieves R² > 0.9 in explaining variance in marginal contributions across most data points and model types. The individualized scaling laws enable applications in data valuation, achieving correlation >0.93 with ground truth values using as few as 10 samples per data point, and in data subset selection, where selecting points based on their predicted contributions at the current dataset size leads to improved model accuracy.

## Method Summary
The paper introduces individualized scaling laws that model how each data point's marginal contribution to model performance decays as dataset size increases, following the form ψk(z) ≈ c(z)/k^α(z). The authors estimate these scaling parameters using two approaches: a maximum likelihood estimator that fits the parametric form to observed marginal contributions across multiple dataset sizes, and an amortized neural estimator that predicts scaling parameters for all points simultaneously using a shared representation. For each data point, they estimate marginal contributions at multiple dataset sizes through leave-one-out retraining, then fit the scaling law parameters. The method is validated on datasets like MiniBooNE, CIFAR-10, and IMDB using logistic regression, MLPs, and SVMs, showing strong predictive accuracy (R² > 0.9) and enabling applications in data valuation and subset selection.

## Key Results
- Individualized scaling laws achieve R² > 0.9 in explaining variance of marginal contributions across most data points and model types
- Data valuation applications achieve correlation >0.93 with ground truth values using as few as 10 samples per data point
- Data subset selection based on predicted contributions at current dataset size leads to improved model accuracy
- Different data points have varying scaling exponents α(z), indicating that relative value depends on dataset size
- Amortized neural estimator reduces samples needed per point from hundreds to tens while maintaining reasonable accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The marginal contribution of individual data points to model performance decreases predictably as dataset size increases, following a log-linear scaling law.
- Mechanism: As more data is added to the training set, each individual point contributes less to the model's improvement because the model can learn the underlying patterns from the larger pool of examples. The contribution decays as c(z)/k^α(z) where k is dataset size and α(z) varies per point.
- Core assumption: The model learns efficiently from data, and the learning curves follow predictable power-law relationships.
- Evidence anchors:
  - [abstract] "we find that a data point's contribution to model's performance shrinks predictably with the size of the dataset in a log-linear manner"
  - [section 2.1] "we observe empirically that it holds across diverse model classes"
  - [corpus] Weak - no direct evidence in corpus papers about individualized scaling

### Mechanism 2
- Claim: Different data points have varying scaling exponents α(z), meaning some points are more valuable in small datasets while others are more useful in large datasets.
- Mechanism: Points near the decision boundary tend to have lower α values and remain influential across dataset sizes, while distant points have higher α values and their influence decays faster as more data becomes available.
- Core assumption: The relative position of data points to the decision boundary determines their long-term value as dataset size grows.
- Evidence anchors:
  - [section 2.1] "certain points are more valuable in small datasets while others are relatively more useful as a part of large datasets"
  - [section 4.1] "There is a strong correlation between α(z) and the distance to the decision boundary"
  - [corpus] Weak - corpus papers focus on aggregate scaling, not individualized behavior

### Mechanism 3
- Claim: Efficient estimation of individualized scaling parameters is possible using amortized neural networks that share information across data points.
- Mechanism: Instead of fitting separate scaling curves for each data point, a neural network predicts scaling parameters for all points simultaneously, reducing the number of samples needed per point from hundreds to tens.
- Core assumption: Scaling behavior exhibits enough regularity across similar data points that shared representations can capture the patterns.
- Evidence anchors:
  - [section 3.2] "we share information across examples z to use fewer samples per data point"
  - [section 4.2] "The amortized estimator is relatively accurate given the small number of samples"
  - [corpus] Weak - corpus doesn't address amortized approaches for scaling laws

## Foundational Learning

- Concept: Marginal contribution of data points
  - Why needed here: The entire framework is built around quantifying how much each data point improves model performance when added to a training set.
  - Quick check question: If a data point's removal decreases model accuracy by 0.5%, what is its marginal contribution?

- Concept: Power-law scaling relationships
  - Why needed here: The scaling laws follow the form L ∝ k^(-λ), which is fundamental to understanding how performance changes with dataset size.
  - Quick check question: If doubling the dataset size reduces error by 30%, what is the scaling exponent λ?

- Concept: Maximum likelihood estimation
  - Why needed here: The parametric estimators fit the scaling law parameters by maximizing the likelihood of observed marginal contributions.
  - Quick check question: What distribution is assumed for the noise in marginal contributions when fitting the likelihood-based estimator?

## Architecture Onboarding

- Component map:
  - Data sampling module -> Model training pipeline -> Marginal contribution calculator -> Scaling law estimator -> Application modules

- Critical path:
  1. Sample marginal contributions for each data point across multiple dataset sizes
  2. Fit scaling parameters using likelihood-based or amortized approach
  3. Validate predictions against held-out observations
  4. Apply scaling laws to downstream tasks (valuation, selection)

- Design tradeoffs:
  - Sample efficiency vs accuracy: Amortized estimator uses fewer samples but may be less precise
  - Model complexity vs interpretability: Neural amortized estimator is black-box compared to parametric approach
  - Dataset size vs computational cost: Larger datasets provide better estimates but increase training time quadratically

- Failure signatures:
  - Poor R² scores (<0.8) indicate the scaling law doesn't fit the data well
  - High variance in α(z) estimates suggests unstable behavior
  - Degradation in prediction accuracy when extrapolating beyond fitting range

- First 3 experiments:
  1. Validate log-linear scaling on a simple logistic regression with synthetic data where ground truth contributions are known
  2. Compare parametric vs amortized estimators on a small dataset to verify sample efficiency gains
  3. Test scaling law predictions on held-out dataset sizes to measure extrapolation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do individualized scaling laws change when training large-scale deep learning models on massive datasets?
- Basis in paper: The authors explicitly note this as an important future direction, stating their experiments "focused on relatively simple models and small-scale datasets, so an important direction for future work is studying the same phenomenon for large-scale deep learning models."
- Why unresolved: The current study only examined logistic regression, MLPs, and SVMs on datasets like CIFAR-10, IMDB, and MiniBooNE. The scaling behavior for transformers, large language models, or other modern architectures on billion-parameter datasets remains unexplored.
- What evidence would resolve it: Empirical studies measuring how individual data points' marginal contributions scale with dataset size for models like GPT, BERT, or vision transformers on datasets with millions to billions of examples.

### Open Question 2
- Question: What factors beyond distance to decision boundary influence a data point's scaling exponent α(z)?
- Basis in paper: The authors found a strong correlation between α(z) and distance to decision boundary, but note "α(z) is not completely determined by this factor" and there is "significant heterogeneity very near the decision boundary, so there are other factors at play."
- Why unresolved: The study only examined one potential factor (distance to decision boundary) and found it explains part but not all of the variation in scaling exponents.
- What evidence would resolve it: Systematic analysis of other data properties (e.g., feature variance, class balance, noise level, outlier status) and their relationship to α(z) values across multiple datasets and model types.

### Open Question 3
- Question: Can individualized scaling laws be used to predict contributions of data points to specific, non-random datasets?
- Basis in paper: The authors state their method "is not specifically designed for dataset selection" and note "selecting many highly ranked points may lead to negative interaction effects that our scaling law does not take into account." They speculate "our study of scaling behavior could also be useful for predicting contributions to a specific dataset."
- Why unresolved: The current scaling law estimates expected contributions when added to random datasets, but real dataset selection requires understanding interactions between specific data points.
- What evidence would resolve it: Development and validation of methods that predict how data points will interact with specific existing datasets, potentially by extending the scaling law framework to model joint contributions.

## Limitations

- The individualized scaling law framework assumes log-linear behavior holds across all dataset sizes and model types, though this may break down at extreme scales or with non-standard architectures.
- The amortization approach trades some accuracy for sample efficiency, potentially limiting precision in high-stakes applications.
- The method requires multiple retraining runs per data point, which becomes computationally prohibitive for very large datasets or complex models.

## Confidence

- High confidence: The log-linear scaling law form itself, showing strong empirical fit (R² > 0.9) across multiple datasets and model classes
- Medium confidence: The amortized neural estimator's ability to generalize scaling parameters to unseen data points
- Low confidence: The theoretical guarantees for data subset selection, as empirical results show improvements but lack rigorous bounds

## Next Checks

1. Test scaling law predictions beyond the fitting range (k_min to k_max) to measure extrapolation accuracy and identify potential break points in the log-linear assumption.
2. Evaluate the amortized estimator's performance on entirely new datasets not seen during training to assess generalization across data distributions.
3. Measure computational overhead of the scaling law estimation process on larger datasets (10K+ points) to determine practical scalability limits.