---
ver: rpa2
title: 'ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction
  on an Academic Budget'
arxiv_id: '2408.00103'
source_url: https://arxiv.org/abs/2408.00103
tags:
- entity
- relik
- entities
- relation
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReLiK introduces a Retriever-Reader architecture that achieves
  state-of-the-art performance for Entity Linking and Relation Extraction while maintaining
  high inference speed. The system uses a shared Retriever to identify candidate entities/relations
  and a Reader that encodes the text with all candidates in a single forward pass
  to establish links.
---

# ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget

## Quick Facts
- arXiv ID: 2408.00103
- Source URL: https://arxiv.org/abs/2408.00103
- Authors: Riccardo Orlando; Pere-Lluis Huguet Cabot; Edoardo Barba; Roberto Navigli
- Reference count: 27
- Key outcome: Achieves state-of-the-art performance on Entity Linking (63.4% F1 on AIDA) and Relation Extraction while providing up to 40x faster inference than competitors

## Executive Summary
ReLiK introduces a Retriever-Reader architecture that achieves state-of-the-art performance for Entity Linking and Relation Extraction while maintaining high inference speed. The system uses a shared Retriever to identify candidate entities/relations and a Reader that encodes the text with all candidates in a single forward pass to establish links. This approach achieves 63.4% F1 on AIDA (vs 60.5% previously), outperforms existing systems on NYT (75.0% vs 76.8%) and REBEL (85.1% vs 82.7%), and provides up to 40x faster inference compared to competitors. The unified framework also enables efficient closed Information Extraction with a single shared Reader.

## Method Summary
ReLiK employs a Retriever-Reader architecture where a bi-encoder Retriever (E5-based) first identifies top-k candidate entities/relations, followed by a DeBERTa-v3 Reader that processes the text with all candidates in a single forward pass. The Retriever is trained with noise contrastive estimation loss, while the Reader handles mention detection and linking/relation extraction sequentially. The same architecture supports both Entity Linking and Relation Extraction tasks, with the unified framework enabling efficient closed Information Extraction by sharing components between tasks.

## Key Results
- Achieves 63.4% F1 on AIDA benchmark, surpassing previous state-of-the-art of 60.5%
- Outperforms existing systems on NYT (75.0% vs 76.8%) and REBEL (85.1% vs 82.7%) datasets
- Provides up to 40x faster inference compared to competing systems
- Enables efficient closed Information Extraction with a single shared Reader

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared Retriever enables fast retrieval of relevant entities/relations before expensive encoding.
- Mechanism: The Retriever module uses a bi-encoder architecture to encode both the input text and candidate entities/relations, then ranks them using similarity scores. This pre-filtering step reduces the number of candidates that need to be processed by the more computationally expensive Reader.
- Core assumption: The top-k retrieved candidates contain most of the relevant entities/relations for the given text.
- Evidence anchors:
  - [abstract] "The system uses a shared Retriever to identify candidate entities/relations"
  - [section] "the Retriever module undertakes the identification of candidate entities or relations that could potentially appear within the text"
  - [corpus] Weak - corpus mentions related retrieval tasks but doesn't directly confirm this specific mechanism
- Break condition: If the Retriever fails to retrieve relevant candidates, the Reader will have no chance to link/extract them correctly.

### Mechanism 2
- Claim: Single forward pass encoding of all candidates enables faster inference than per-candidate processing.
- Mechanism: The Reader encodes the input text concatenated with all top-k retrieved candidates in one forward pass, rather than requiring separate passes for each candidate. This leverages the full contextualization capabilities of the language model.
- Core assumption: The language model can effectively contextualize all candidates simultaneously without losing important details.
- Evidence anchors:
  - [abstract] "incorporates the candidate entities or relations alongside the text, making it possible to link entities or extract relations in a single forward pass"
  - [section] "we encode the input text with all its retrieved passages... linking all the entities or extracting all the relational triplets in a single forward pass"
  - [corpus] Weak - corpus mentions efficiency gains but doesn't provide specific evidence for this mechanism
- Break condition: If the sequence becomes too long, the model may lose context or exceed memory constraints.

### Mechanism 3
- Claim: Retriever-Reader separation enables efficient closed Information Extraction.
- Mechanism: The same architecture can perform both Entity Linking and Relation Extraction by sharing the Retriever and using a unified Reader that handles both tasks sequentially, with RE conditioned on EL predictions.
- Core assumption: The tasks are sufficiently related that shared components can handle both without significant interference.
- Evidence anchors:
  - [abstract] "Finally, we show how our architecture can be used seamlessly for Information Extraction (cIE), i.e. EL + RE"
  - [section] "since both tasks share the same mention detection approach, ReLiK allows for closed IE with a single Reader"
  - [corpus] Moderate - corpus shows related tasks but doesn't directly confirm this specific mechanism
- Break condition: If the tasks interfere with each other during training, performance on one or both may degrade.

## Foundational Learning

- Concept: Dense Passage Retrieval (DPR) architecture
  - Why needed here: Understanding the Retriever's bi-encoder architecture is crucial for implementing and tuning the retrieval component
  - Quick check question: What are the two encoders in DPR and what do they encode separately?

- Concept: Transformer encoder mechanics
  - Why needed here: The Reader uses a Transformer encoder to process concatenated input, so understanding self-attention and positional encoding is essential
  - Quick check question: How does the attention mechanism handle the concatenated input of text and candidates?

- Concept: Multi-label noise contrastive estimation (NCE)
  - Why needed here: The Retriever is trained using NCE loss, which requires understanding how to construct positive and negative samples
  - Quick check question: How are hard negatives mined in the training process described?

## Architecture Onboarding

- Component map:
  - Retriever: E5-based bi-encoder for fast candidate retrieval
  - Reader: DeBERTa-v3-based encoder for mention detection and linking/relation extraction
  - Unified pipeline: Retriever output feeds into Reader input

- Critical path:
  1. Input text → Retriever encoding
  2. Retrieve top-k candidates using similarity scoring
  3. Concatenate input with candidates
  4. Single Reader forward pass
  5. Mention detection → Entity linking/Relation extraction

- Design tradeoffs:
  - Speed vs. accuracy: Fewer candidates = faster but potentially less accurate
  - Model size vs. performance: Larger DeBERTa versions improve performance but increase inference time
  - Pretraining vs. fine-tuning: BLINK pretraining helps but adds training complexity

- Failure signatures:
  - Low recall in retrieval suggests Retriever issues
  - Poor linking despite correct mentions suggests Reader issues
  - Degraded performance with more candidates suggests sequence length issues

- First 3 experiments:
  1. Test Retriever recall@K with different K values to find optimal trade-off
  2. Compare single-pass Reader vs. per-candidate processing for speed validation
  3. Evaluate impact of candidate ordering on Reader performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ReLiK compare to other state-of-the-art systems when evaluated on emerging entities or in domains with significant temporal shifts?
- Basis in paper: [explicit] The authors mention that the performance of recent systems is reaching a plateau on many benchmarks and identify emerging entities as a promising area for further exploration. They also note that the AIDA dataset used for training contains biases, such as conflicting labels regarding Taiwan and China.
- Why unresolved: The paper does not provide experimental results on emerging entities or domains with significant temporal shifts. The evaluation is limited to held-out benchmarks like AIDA, NYT, and REBEL, which may not fully capture the model's performance on evolving data.
- What evidence would resolve it: Experiments comparing ReLiK's performance on datasets containing emerging entities or temporal shifts against other state-of-the-art systems would provide concrete evidence of its robustness and adaptability.

### Open Question 2
- Question: What is the impact of using different textual representations for entities/relations (e.g., Wikipedia titles vs. opening text) on the performance of the Retriever and Reader components?
- Basis in paper: [explicit] The authors note that in the Reader component, they use Wikipedia titles instead of opening text due to computational constraints, and mention that this choice has proven to be informative and discriminative in most situations. They also discuss the potential for automatic generation of entity and relation verbalizations as a future research direction.
- Why unresolved: The paper does not provide a systematic comparison of different textual representations for entities/relations on the performance of the Retriever and Reader components. The choice of representation is presented as a practical consideration rather than an experimental finding.
- What evidence would resolve it: Experiments comparing the performance of ReLiK using different textual representations for entities/relations (e.g., titles, opening text, or automatically generated verbalizations) would provide insights into the impact of this choice on the model's effectiveness.

### Open Question 3
- Question: How does the performance of ReLiK vary with different values of K (number of retrieved passages) in the Reader component, and what is the optimal trade-off between performance and computational efficiency?
- Basis in paper: [explicit] The authors discuss the impact of reducing the number of retrieved passages (K) on the performance and inference time of the Reader component. They report that reducing K to 8 does not significantly impact performance, and in some cases, even improves it while halving the inference time.
- Why unresolved: The paper does not provide a comprehensive analysis of the relationship between K and performance across different datasets or tasks. The experiments focus on specific values of K and do not explore the full range of possibilities or the optimal trade-off between performance and efficiency.
- What evidence would resolve it: Experiments varying K across a wider range of values and evaluating the performance and inference time on different datasets and tasks would provide a more complete understanding of the optimal trade-off between performance and computational efficiency.

## Limitations
- The system's performance is bounded by the Retriever's recall, and the paper lacks thorough analysis of failure cases where relevant entities fall outside the top-k retrieved candidates
- The single forward pass approach may face context window limitations with very long documents or large candidate sets, though this is not explicitly discussed
- The unified architecture for closed Information Extraction assumes tasks can be effectively combined without interference, but limited analysis of potential task interference is provided

## Confidence
- High Confidence: The speed improvements (40x faster inference) and basic Retriever-Reader architecture design are well-supported by described methodology and results
- Medium Confidence: Claims about state-of-the-art performance on NYT are less certain due to narrow margin and potential evaluation differences
- Low Confidence: Scalability analysis for very large knowledge bases and robustness to domain shifts are not thoroughly explored

## Next Checks
1. Measure Retriever recall@K across different K values to determine optimal trade-off between speed and coverage
2. Systematically evaluate Reader performance as input sequence length increases by varying the number of retrieved candidates
3. Conduct ablation study comparing unified closed Information Extraction approach against separate EL and RE pipelines to quantify task interference