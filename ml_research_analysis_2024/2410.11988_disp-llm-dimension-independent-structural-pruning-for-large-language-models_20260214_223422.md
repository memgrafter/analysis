---
ver: rpa2
title: 'DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models'
arxiv_id: '2410.11988'
source_url: https://arxiv.org/abs/2410.11988
tags:
- pruning
- structural
- bismuth
- slicegpt
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of efficiently deploying large
  language models (LLMs) on resource-limited devices by proposing a dimension-independent
  structural pruning method called DISP-LLM. The core innovation lies in breaking
  the structural dependence between layers, allowing different blocks to utilize different
  subsets of feature maps along the embedding dimension.
---

# DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models

## Quick Facts
- **arXiv ID**: 2410.11988
- **Source URL**: https://arxiv.org/abs/2410.11988
- **Reference count**: 40
- **Primary result**: Achieves accuracy comparable to semi-structural pruning methods without recovery fine-tuning

## Executive Summary
DISP-LLM introduces a dimension-independent structural pruning method that breaks the structural dependence between layers in large language models. Unlike previous methods that require uniform pruning across all layers, DISP-LLM allows different blocks to utilize different subsets of feature maps along the embedding dimension. The method employs a hypernetwork and gradient-based optimization to learn the width of each layer, providing fine-grained control over pruning without introducing additional parameters. Experiments demonstrate that DISP-LLM outperforms state-of-the-art structural pruning methods while maintaining performance comparable to semi-structural approaches like SparseGPT and Wanda, all without the need for recovery fine-tuning.

## Method Summary
DISP-LLM addresses the limitations of traditional structural pruning by introducing dimension independence between layers. The method uses a hypernetwork to generate selection matrices that determine which feature maps to keep in each layer, allowing different layers to have different widths. This is achieved through gradient-based optimization where the hypernetwork learns to predict the optimal width configuration. The approach eliminates the need for recovery fine-tuning by preserving sufficient model capacity during pruning. DISP-LLM is evaluated on various LLM architectures including OPT, LLaMA, LLaMA-2, Phi-1.5, and Phi-2, demonstrating superior performance compared to existing structural pruning methods while maintaining accuracy levels similar to semi-structural approaches.

## Key Results
- DISP-LLM achieves accuracy comparable to semi-structural pruning methods (SparseGPT, Wanda) without requiring recovery fine-tuning
- Outperforms state-of-the-art structural pruning methods on multiple LLM architectures including OPT, LLaMA-2, and Phi models
- Pruned models exhibit improved throughput while maintaining strong performance across language modeling and zero-shot tasks

## Why This Works (Mechanism)
DISP-LLM's effectiveness stems from breaking the structural dependence between layers, allowing each layer to independently determine its optimal width. This dimension independence enables more flexible and efficient pruning compared to uniform structural methods. The hypernetwork learns to generate selection matrices that identify the most important feature maps for each layer, preserving critical information while removing redundancy. By avoiding recovery fine-tuning, the method reduces computational overhead and enables faster deployment of pruned models. The gradient-based optimization ensures that the pruning process is guided by actual model performance rather than heuristic rules.

## Foundational Learning

**Hypernetworks**: Neural networks that generate parameters for other networks
- *Why needed*: Generate dynamic selection matrices for each layer based on learned width configurations
- *Quick check*: Verify hypernetwork can produce valid selection matrices for different layer configurations

**Gradient-based optimization**: Using gradients to optimize discrete decisions
- *Why needed*: Enable learning of which feature maps to keep while maintaining differentiability
- *Quick check*: Confirm gradients flow through selection process during training

**Structural vs. semi-structural pruning**: Different approaches to reducing model size
- *Why needed*: Understand why DISP-LLM achieves semi-structural performance with purely structural constraints
- *Quick check*: Compare parameter counts and accuracy between pruning methods

## Architecture Onboarding

**Component map**: Input -> LLM blocks -> Hypernetwork -> Selection matrices -> Pruned model

**Critical path**: Hypernetwork output → Selection matrix application → Forward pass through pruned layers

**Design tradeoffs**: Dimension independence provides flexibility but requires hypernetwork training overhead

**Failure signatures**: 
- Poor hypernetwork convergence leading to suboptimal pruning
- Over-aggressive pruning causing accuracy degradation
- Hypernetwork collapse resulting in uniform width selection

**First experiments**:
1. Test hypernetwork ability to generate valid selection matrices across different layer types
2. Validate gradient flow through selection process using simple models
3. Compare pruning ratios and accuracy on small-scale model before full deployment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of dimension-independent structural pruning scale with model size, particularly for very large language models beyond those tested in the paper?
- Basis in paper: [inferred] The paper demonstrates effectiveness on models up to LLaMA-2 13B, but does not explore models significantly larger than this, such as those with hundreds of billions of parameters
- Why unresolved: The computational resources required to train hypernetworks and evaluate very large models make this difficult to test, but understanding scalability is crucial for practical deployment
- What evidence would resolve it: Systematic testing of DISP-LLM on progressively larger models, including frontier-scale LLMs, comparing performance, computational overhead, and throughput gains across different model sizes

### Open Question 2
- Question: What is the theoretical relationship between the number of hypernetwork parameters and the optimal pruning ratio for different model architectures?
- Basis in paper: [explicit] The paper uses a hypernetwork to generate selection matrices but does not provide theoretical analysis of how hypernetwork complexity relates to pruning efficiency or optimal parameter budgets
- Why unresolved: While the paper demonstrates empirical success, understanding the theoretical underpinnings would allow for more principled design choices and potentially more efficient implementations
- What evidence would resolve it: Mathematical analysis deriving the relationship between hypernetwork capacity, model architecture, and optimal pruning configurations, potentially leading to more efficient hypernetwork designs

### Open Question 3
- Question: How does the performance of dimension-independent structural pruning compare to unstructured pruning methods when both are constrained to the same number of parameters?
- Basis in paper: [inferred] The paper focuses on structural pruning advantages over other structural methods but does not directly compare against unstructured pruning under equivalent parameter budgets, despite mentioning unstructured pruning in related work
- Why unresolved: The paper demonstrates superiority over structural and semi-structural methods but leaves open the question of whether the structural constraints provide additional benefits compared to the flexibility of unstructured pruning when both achieve similar compression ratios
- What evidence would resolve it: Head-to-head comparison of DISP-LLM against state-of-the-art unstructured pruning methods, ensuring both methods are constrained to identical parameter budgets and evaluating performance across the same tasks and datasets

## Limitations
- Generalizability of results to other model architectures and downstream tasks remains uncertain
- Lack of detailed performance metrics (inference time, memory usage, energy consumption) to quantify practical benefits
- No discussion of impact on model interpretability or ability to identify important features

## Confidence
- **High**: DISP-LLM's ability to break structural dependence between layers and allow different blocks to utilize different subsets of feature maps along the embedding dimension
- **Medium**: Claims about DISP-LLM's performance relative to state-of-the-art structural pruning methods and the lack of discussion on model interpretability
- **Low**: Claims about improved throughput for pruned models without providing specific metrics or measurements

## Next Checks
1. Conduct experiments with a wider range of LLM architectures and downstream tasks to assess the generalizability of DISP-LLM's performance improvements
2. Provide detailed performance metrics, such as inference time, memory usage, and energy consumption, to quantify the practical benefits of DISP-LLM in terms of throughput and efficiency
3. Investigate the impact of DISP-LLM on model interpretability by analyzing the pruned models' ability to identify important features and their alignment with human-understandable concepts