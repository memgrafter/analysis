---
ver: rpa2
title: Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?
arxiv_id: '2411.05775'
source_url: https://arxiv.org/abs/2411.05775
tags:
- llms
- annotation
- political
- annotations
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using open-source large language models
  (LLMs) as annotators for political factuality detection. The authors developed a
  framework where multiple LLMs label news articles as factually correct or incorrect,
  followed by human review and LLM-based evaluation.
---

# Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?
## Quick Facts
- arXiv ID: 2411.05775
- Source URL: https://arxiv.org/abs/2411.05775
- Reference count: 19
- Five-shot prompting achieves 89.3% accuracy for political factuality detection

## Executive Summary
This paper investigates using open-source large language models as annotators for political factuality detection. The authors developed a framework where multiple LLMs label news articles as factually correct or incorrect, followed by human review and LLM-based evaluation. Using a dataset of 6,100 news articles covering North American politics from May 2023-2024, they compared several open-source models including Llama-3-8B, Llama-3.1-8B, Mistral-7B, Gemma-2-9B, and Phi-3-medium. Results show that five-shot prompting significantly outperforms zero-shot, with Llama-3-8B-Instruct achieving 89.3% accuracy against human-verified gold labels.

## Method Summary
The framework combines prompt-based annotation using multiple open-source LLMs with human review and LLM-based evaluation. Articles are first annotated by LLMs using both zero-shot and five-shot prompting approaches, then validated by human experts, and finally evaluated by stronger LLM judges. The study tested five different open-source models and found that five-shot prompting significantly improved performance. Human reviewers from diverse backgrounds created gold labels, while LLM judges (GPT-4o-mini and Llama-3.1-70B) assessed annotation quality through agreement calculations.

## Key Results
- Five-shot prompting significantly outperforms zero-shot, with Llama-3-8B-Instruct achieving 89.3% accuracy
- LLM-based evaluations using GPT-4o-mini and Llama-3.1-70B as judges show agreement rates of 76-79% with LLM annotations
- The framework provides a scalable, cost-effective alternative to manual fact-checking while maintaining high accuracy

## Why This Works (Mechanism)
### Mechanism 1
- Claim: LLM annotators can effectively label political news articles as factually correct or incorrect
- Mechanism: Open-source LLMs leverage their training on diverse political discourse to identify factual claims and verify them against known information
- Core assumption: LLMs have sufficient knowledge of political events and can distinguish factual content from misinformation
- Evidence anchors:
  - [abstract] "Using open-source LLMs, we create a politically diverse dataset, labelled for bias through LLM-generated annotations"
  - [section] "Empirical analysis indicates that annotations generated by LLMs closely match human annotations, as evidenced by high reference-based scores (Table 1)"
  - [corpus] Weak evidence - corpus only shows related papers on fact-checking and LLMs but no direct evidence about open-source model performance
- Break condition: LLMs fail when encountering recent events not in their training data or when dealing with complex political narratives requiring nuanced understanding

### Mechanism 2
- Claim: Five-shot prompting significantly improves annotation accuracy over zero-shot prompting
- Mechanism: Providing examples in the prompt helps LLMs understand the task format and expected reasoning patterns for political factuality assessment
- Core assumption: LLMs can generalize from a small number of demonstrations to perform the annotation task
- Evidence anchors:
  - [section] "Results show that five-shot prompting significantly outperforms zero-shot, with Llama-3-8B-Instruct achieving 89.3% accuracy against human-verified gold labels"
  - [abstract] "Using open-source LLMs... labeled for bias through LLM-generated annotations"
  - [corpus] Weak evidence - corpus contains papers about annotation but no specific evidence about shot-based prompting effectiveness
- Break condition: When demonstrations are not representative of the task diversity or when the task requires knowledge beyond the examples provided

### Mechanism 3
- Claim: LLM judges can evaluate the quality of LLM-generated annotations with reasonable agreement rates
- Mechanism: Stronger LLMs assess whether annotator outputs align with given labels and text, providing an automated evaluation mechanism
- Core assumption: LLMs can objectively assess the alignment between annotations and source material
- Evidence anchors:
  - [section] "LLM-based evaluations for assessing label quality demonstrate strong performance of LLM annotators (Table 2)"
  - [abstract] "These annotations are validated by human experts and further evaluated by LLM-based judges to assess the accuracy and reliability of the annotations"
  - [corpus] Weak evidence - corpus shows related work on LLM evaluation but no direct evidence about judge performance on political factuality
- Break condition: When judges exhibit bias toward their own responses or when the evaluation criteria are ambiguous

## Foundational Learning
- Concept: Zero-shot vs few-shot prompting
  - Why needed here: Understanding the difference is crucial for implementing the annotation framework and explaining why five-shot outperforms zero-shot
  - Quick check question: What is the key difference between zero-shot and five-shot prompting, and why would five-shot improve performance?

- Concept: LLM-as-a-judge methodology
  - Why needed here: The evaluation framework relies on using stronger LLMs to assess annotation quality, requiring understanding of this approach
  - Quick check question: How does using an LLM as a judge differ from traditional evaluation metrics, and what are the potential biases to watch for?

- Concept: Political misinformation detection
  - Why needed here: The framework targets political factuality, so understanding the characteristics of political misinformation is essential
  - Quick check question: What distinguishes political misinformation from other types of misinformation, and why might this make it particularly challenging for LLMs?

## Architecture Onboarding
- Component map: Data collection pipeline (scraping + filtering) -> LLM annotation pipeline (multiple models + majority voting) -> Human review process (validation + consensus) -> LLM evaluation pipeline (judge models + agreement calculation) -> Performance analysis module (metrics calculation)
- Critical path: Data collection → LLM annotation → Human review → LLM evaluation → Performance analysis
- Design tradeoffs:
  - Multiple open-source models vs single paid model: trade cost for potential variability
  - Human review vs fully automated: trade speed for quality assurance
  - Two judge models vs one: trade evaluation robustness for computational cost
- Failure signatures:
  - Low agreement between annotators: indicates model confusion or ambiguous input
  - Judge disagreement with high human agreement: indicates judge bias
  - Performance degradation over time: indicates model knowledge cutoff issues
- First 3 experiments:
  1. Compare zero-shot vs five-shot performance on a small validation set
  2. Test different majority voting schemes (simple majority vs weighted)
  3. Evaluate single vs multiple judge models on agreement rates

## Open Questions the Paper Calls Out
None

## Limitations
- Framework performance may degrade for political events beyond the May 2023-2024 training cutoff date
- Study assumes human reviewers were free from bias despite demographic diversity efforts
- Evaluation framework may exhibit circularity as LLM judges could share training biases with annotator models

## Confidence
- High confidence: Five-shot prompting superiority is well-supported with clear statistical backing
- Medium confidence: 89.3% accuracy figure is robust against human-verified gold labels
- Medium confidence: LLM judge agreement rates (76-79%) suggest reasonable evaluation quality

## Next Checks
1. Test model performance on temporally distant political events to assess knowledge cutoff effects
2. Conduct inter-annotator agreement analysis among human reviewers to quantify gold label reliability
3. Implement cross-validation using held-out human-verified samples to independently verify LLM judges' agreement rates