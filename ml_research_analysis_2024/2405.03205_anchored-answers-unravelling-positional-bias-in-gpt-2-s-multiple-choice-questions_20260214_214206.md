---
ver: rpa2
title: 'Anchored Answers: Unravelling Positional Bias in GPT-2''s Multiple-Choice
  Questions'
arxiv_id: '2405.03205'
source_url: https://arxiv.org/abs/2405.03205
tags:
- bias
- anchored
- token
- gpt2
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the anchored bias phenomenon in GPT-2 models,
  where they consistently favor the first choice 'A' in multiple-choice questions
  regardless of the correct answer's actual position. Using mechanistic interpretability,
  the authors identify specific Multi-Layer Perceptron (MLP) layers and attention
  heads responsible for this bias through the "logit lens" method.
---

# Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions

## Quick Facts
- arXiv ID: 2405.03205
- Source URL: https://arxiv.org/abs/2405.03205
- Authors: Ruizhe Li; Yanjun Gao
- Reference count: 4
- Primary result: Identifies and mitigates anchored bias in GPT-2 models where they favor the first choice 'A' in MCQs regardless of correct answer position

## Executive Summary
This paper investigates the anchored bias phenomenon in GPT-2 models, where they consistently favor the first choice 'A' in multiple-choice questions regardless of the correct answer's actual position. Using mechanistic interpretability, the authors identify specific Multi-Layer Perceptron (MLP) layers and attention heads responsible for this bias through the "logit lens" method. They locate value vectors in MLP that store this positional bias and propose two strategies to mitigate it: directly updating these value vectors by modifying their stored knowledge, and recalibrating attention patterns by swapping weights between the anchored position and correct answer position. These interventions successfully mitigate the anchored bias and improve overall MCQ prediction accuracy across the GPT-2 family, achieving over 70% average accuracy across various datasets.

## Method Summary
The researchers employ mechanistic interpretability to analyze GPT-2's anchored bias in MCQs. They use the "logit lens" method to trace value vectors in MLP layers and attention patterns that contribute to the bias. The approach involves formatting MCQ prompts with a specific structure, then applying logit lens analysis to identify which MLP layers and attention heads exhibit anchored bias. Two mitigation strategies are proposed: (1) directly updating identified value vectors in MLP using a simple update rule, and (2) recalibrating attention patterns by swapping weights between anchored position and correct answer position. These interventions are tested across GPT-2 models of different sizes (Small-124M, Medium-355M, Large-774M, XL-1.5B) using five datasets with varying choice counts (2-5 choices).

## Key Results
- Successfully identifies specific MLP layers (9, 20, 34, 37/38/44) and attention heads responsible for anchored bias in GPT-2 models
- Direct value vector updates in MLP effectively mitigate anchored bias with minimal performance degradation on other tasks
- Attention pattern recalibration through weight swapping between anchored and correct positions significantly reduces bias
- Achieves over 70% average accuracy across various MCQ datasets after mitigation
- Demonstrates that anchored bias is stored as key-value memories in MLP modules, specifically associated with the token 'A' in first position

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MLP value vectors store the anchored bias as key-value memories, where specific rows in the output weight matrix are associated with the token 'A'.
- **Mechanism**: During pretraining, the model associates the position of the first answer choice with the token 'A' in contexts like "Answer Choices: A: ... B: ... C: ...". These associations are stored in value vectors of the MLP module. When encountering MCQ prompts, these stored associations cause the model to favor the first choice regardless of the correct answer's position.
- **Core assumption**: The MLP module acts as a key-value memory where columns of the input weight matrix produce coefficients that weight the corresponding value vectors in the output weight matrix.
- **Evidence anchors**:
  - [abstract] "We find that certain value vectors in the MLP, which inherently harbour this bias, and specific attention heads pay more weight on the 'A' position over the correct answer choice positions in the input prompt."
  - [section] "Based on the consensus about MLP module, we aim to solve these research questions: 1) Is MLP responsible for the anchored bias in GPT2 family? 2) Which layer and dimensionality of MLP is anchored bias relevant to? 3) Is this bias stored as knowledge in a specific value vector of Wℓ out?"
  - [corpus] Weak evidence; related work on MLP as key-value memories but not specifically on anchored bias.

### Mechanism 2
- **Claim**: Updating the identified value vectors in the MLP directly mitigates the anchored bias by replacing the stored bias with correct choice associations.
- **Mechanism**: The researchers directly modify the value vectors associated with the anchored bias using a simple update rule: vℓ,n = vℓ,n − λ1WU[A] + λ2WU[B/C/D/E]. This replaces the association with 'A' with associations with the correct choices, effectively removing the bias.
- **Core assumption**: The value vectors can be directly modified without causing significant harm to the model's performance on other tasks.
- **Evidence anchors**:
  - [abstract] "Inspired from (Geva et al., 2021, 2022) where MLPs can be treated as key-value memories, we use a straightforward yet potent method (Dai et al., 2022) to update these critical value vectors in the MLP, effectively mitigate the anchored bias."
  - [section] "Following (Dai et al., 2022), we directly modify and update the identified value vector as: vℓ,n = vℓ,n − λ1WU[A] + λ2WU[B/C/D/E]"
  - [corpus] Weak evidence; the cited works use value vector updating but not specifically for anchored bias.

### Mechanism 3
- **Claim**: Attention heads contribute to the anchored bias by assigning higher weights to the position of the 'A' choice during inference.
- **Mechanism**: Certain attention heads in the later layers of the model pay more attention to the 'A' choice position in the input prompt. By recalibrating these attention patterns (e.g., swapping attention weights between the 'A' position and the correct answer position), the model can reduce its preference for the first choice.
- **Core assumption**: The attention patterns learned during pretraining reinforce the association between the first position and the 'A' token.
- **Evidence anchors**:
  - [abstract] "We find that certain value vectors in the MLP, which inherently harbour this bias, and specific attention heads pay more weight on the 'A' position over the correct answer choice positions in the input prompt."
  - [section] "Locating attention heads of GPT2 family for anchored bias. Following a similar method as locating anchored bias in MLP, we also aim to solve these research questions: 1) Is the attention head also responsible for the anchored bias in GPT2 family? 2) Which layer and head of attention pattern is anchored bias relevant to?"
  - [corpus] Weak evidence; related work on attention heads and positional bias but not specifically on anchored bias in MCQs.

## Foundational Learning

- **Concept**: Transformer architecture and attention mechanisms
  - **Why needed here**: Understanding how the model processes input prompts and generates predictions is crucial for identifying the source of the anchored bias.
  - **Quick check question**: Can you explain how the attention mechanism in a transformer works and how it allows the model to focus on different parts of the input?

- **Concept**: Logit lens method
  - **Why needed here**: The logit lens is used to analyze the contributions of different layers and components to the final prediction, allowing the researchers to pinpoint the source of the anchored bias.
  - **Quick check question**: How does the logit lens method work, and what information can it provide about the model's internal processing?

- **Concept**: Key-value memory models
  - **Why needed here**: The researchers treat the MLP module as a key-value memory, which is essential for understanding how the anchored bias is stored and can be modified.
  - **Quick check question**: What is a key-value memory model, and how does it relate to the MLP module in a transformer?

## Architecture Onboarding

- **Component map**: Input embedding layer -> Transformer blocks (attention + MLP layers) -> Output unembedding layer -> Logit lens analysis tool

- **Critical path**: 
  1. Input prompt encoding
  2. Multi-head self-attention processing
  3. MLP layer processing
  4. Residual connections and layer normalization
  5. Output generation and logit computation

- **Design tradeoffs**:
  - Direct value vector updating vs. fine-tuning the entire model
  - Attention pattern recalibration vs. MLP modification
  - Zero-shot vs. few-shot learning approaches

- **Failure signatures**:
  - Persistent anchored bias even after interventions
  - Degradation in performance on non-MCQ tasks
  - Unintended side effects from value vector updates

- **First 3 experiments**:
  1. Apply the logit lens to analyze the contributions of different layers and attention heads to the anchored bias.
  2. Identify the specific value vectors in the MLP that are associated with the anchored bias and update them using the proposed method.
  3. Recalibrate the attention patterns of the identified attention heads and evaluate the impact on the anchored bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does few-shot learning compare to direct value vector updating in mitigating anchored bias across different prompt lengths and complexities?
- Basis in paper: [explicit] The paper mentions that few-shot learning could mitigate anchored bias to some extent depending on the length and complexity of few-shot prompts, but it remains unclear how effective it is compared to direct value vector updating.
- Why unresolved: The paper only provides a preliminary experiment on few-shot learning and does not conduct a comprehensive comparison with direct value vector updating.
- What evidence would resolve it: A systematic study comparing the effectiveness of few-shot learning and direct value vector updating across various prompt lengths and complexities, with detailed analysis of their impact on anchored bias mitigation.

### Open Question 2
- Question: What is the long-term impact of direct value vector updating on the overall performance of GPT-2 models for tasks other than MCQs?
- Basis in paper: [explicit] The paper mentions a preliminary experiment showing that direct value vector updating did not harm GPT-2's performance on the original IOI task, but it does not explore the long-term impact on other tasks.
- Why unresolved: The paper only provides a brief experiment on one specific task (IOI) and does not investigate the broader implications of direct value vector updating on GPT-2's general capabilities.
- What evidence would resolve it: A comprehensive study evaluating the performance of GPT-2 models with updated value vectors on a wide range of tasks, including both MCQs and other common NLP benchmarks, to assess any potential degradation in performance.

### Open Question 3
- Question: Can the identified value vectors in MLP modules be generalized across different transformer-based language models, or are they specific to the GPT-2 family?
- Basis in paper: [explicit] The paper identifies specific value vectors in MLP modules responsible for anchored bias in GPT-2 models, but it does not explore whether these findings can be applied to other transformer-based models.
- Why unresolved: The paper focuses exclusively on the GPT-2 family and does not investigate the generalizability of its findings to other transformer architectures.
- What evidence would resolve it: A study applying the same mechanistic analysis techniques to other transformer-based models (e.g., BERT, RoBERTa, GPT-3) to identify similar value vectors and assess their role in anchored bias, followed by validation of the mitigation strategies across different model families.

## Limitations

- The study focuses exclusively on GPT-2 family models, limiting generalizability to other architectures
- The prompt format used may not represent all MCQ-style tasks, and anchored bias could manifest differently with alternative formatting
- While interventions show success, long-term stability and potential side effects on other downstream tasks remain underexplored
- The mechanistic interpretability approach relies heavily on the "logit lens" method, which may not capture all aspects of model's internal representations

## Confidence

**High Confidence**: The identification of MLP layers and attention heads responsible for anchored bias through logit lens analysis is well-supported by methodology and experimental results. The claim that specific value vectors store the positional bias has strong empirical backing through direct modification experiments.

**Medium Confidence**: The effectiveness of mitigation strategies (value vector updates and attention pattern recalibration) is demonstrated across multiple datasets and model sizes, but exact hyperparameters and their sensitivity to different task domains require further validation.

**Low Confidence**: The generalization of these findings to other language models beyond the GPT-2 family and to different prompt formats remains uncertain without additional experiments.

## Next Checks

1. **Cross-architecture validation**: Apply the same mechanistic analysis and mitigation strategies to other transformer-based models (e.g., GPT-3, LLaMA) to test generalizability of the anchored bias findings and interventions.

2. **Prompt format robustness**: Test whether the anchored bias persists and can be mitigated under different MCQ prompt formats (e.g., "Select the correct answer: A) ..., B) ..., C) ...") to assess the robustness of the identified mechanisms.

3. **Long-term stability evaluation**: Conduct extended evaluations of the modified models on diverse downstream tasks over longer time periods to identify any emergent side effects or degradation in performance that may not be apparent in immediate post-intervention testing.