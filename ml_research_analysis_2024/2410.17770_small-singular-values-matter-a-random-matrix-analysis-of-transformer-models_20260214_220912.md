---
ver: rpa2
title: 'Small Singular Values Matter: A Random Matrix Analysis of Transformer Models'
arxiv_id: '2410.17770'
source_url: https://arxiv.org/abs/2410.17770
tags:
- singular
- values
- matrices
- matrix
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies Random Matrix Theory (RMT) to analyze singular-value
  spectra of weight matrices in pretrained transformer models like BERT and Llama-8B.
  By comparing empirical spectra to RMT predictions, the authors identify deviations
  that indicate learned features.
---

# Small Singular Values Matter: A Random Matrix Analysis of Transformer Models

## Quick Facts
- arXiv ID: 2410.17770
- Source URL: https://arxiv.org/abs/2410.17770
- Authors: Max Staats; Matthias Thamm; Bernd Rosenow
- Reference count: 36
- Primary result: Small singular values in transformer weight matrices contain crucial learned information, contrary to conventional assumptions

## Executive Summary
This paper applies Random Matrix Theory (RMT) to analyze singular-value spectra of weight matrices in pretrained transformer models like BERT and Llama-8B. The authors discover that small singular values and their corresponding vectors can be as informative as large ones, with strong overlap between singular vectors and eigenvectors of activation covariance matrices. Surprisingly, fine-tuning makes small singular values even more crucial for model performance. These findings challenge the conventional wisdom that only large singular values contain meaningful information and provide guidance for SVD-based pruning and compression of large language models.

## Method Summary
The authors apply Random Matrix Theory to analyze singular-value spectra of weight matrices in pretrained transformer models. They compare empirical spectra against RMT predictions to identify deviations that indicate learned features. Through systematic pruning experiments, they remove singular values based on their deviation from RMT predictions and measure the impact on validation accuracy. The analysis spans multiple transformer models including BERT and Llama-8B, examining various weight matrices across different layers. They also propose a linear random-matrix model to explain why small singular values become important after fine-tuning.

## Key Results
- Small singular values show strong deviations from RMT predictions and overlap significantly with activation covariance eigenvectors
- Removing singular values that deviate from RMT predictions causes larger accuracy drops than removing random values
- Fine-tuning enhances the importance of small singular values, making them crucial for model performance
- Query and key matrices consistently show strong RMT deviations across different models, while attention.output and value matrices remain closer to RMT predictions

## Why This Works (Mechanism)
The mechanism appears to involve learned features being encoded in the deviations from RMT predictions, with both large and small singular values contributing meaningful information. The linear random-matrix model proposed suggests that fine-tuning specifically enhances the information content in small singular values, though the exact mechanism requires further investigation.

## Foundational Learning
- Random Matrix Theory (RMT): Statistical framework for analyzing large random matrices; needed to establish baseline expectations for untrained weight matrices
- Singular Value Decomposition (SVD): Matrix factorization technique; needed to decompose weight matrices and analyze their spectral properties
- Covariance matrices: Statistical measures of variable relationships; needed to connect weight matrix structure to activation patterns
- RMT deviations: Differences between empirical and theoretical spectra; needed to identify learned features versus random initialization
- Fine-tuning dynamics: Weight updates during adaptation; needed to understand how small singular values become important
- Activation alignment: Relationship between weight matrices and activation patterns; needed to explain information storage

## Architecture Onboarding
**Component Map**: Input -> Weight Matrices (Q,K,V,FFN) -> Attention/MLP Layers -> Output
**Critical Path**: Weight initialization → Pre-training → RMT analysis → Fine-tuning → Performance evaluation
**Design Tradeoffs**: Model compression vs. preserving alignment information; computational cost of SVD analysis vs. pruning benefits
**Failure Signatures**: Large accuracy drops when removing RMT-deviating singular values; loss of alignment information
**First Experiments**: 1) Compare RMT deviations across different transformer architectures, 2) Track singular value evolution during pre-training, 3) Test RMT-based pruning on smaller models first

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do small singular values become crucial for model performance after fine-tuning?
- Basis in paper: The paper shows that fine-tuning makes small singular values crucial for performance, with validation accuracy dropping significantly when these values are removed after fine-tuning.
- Why unresolved: The paper proposes a linear random-matrix model but doesn't fully explain the mechanism by which fine-tuning enhances the importance of small singular values.
- What evidence would resolve it: Detailed analysis of weight updates during fine-tuning showing how small singular values are modified, or experimental validation showing that preserving small singular values during fine-tuning maintains alignment better than removing them.

### Open Question 2
- Question: Are there architectural patterns that determine which weight matrices exhibit strong deviations from RMT?
- Basis in paper: The paper identifies consistent patterns where query and key matrices show strong deviations while attention.output and value matrices remain close to RMT predictions across different models.
- Why unresolved: While patterns are observed, the paper doesn't explain the underlying architectural reasons for these differences.
- What evidence would resolve it: Systematic study of different transformer architectures varying specific components to isolate which architectural features drive RMT deviations.

### Open Question 3
- Question: Can RMT-based pruning methods be developed that preserve model alignment while reducing model size?
- Basis in paper: The paper shows that removing small singular values after fine-tuning degrades performance, suggesting alignment information is stored there, but doesn't propose specific pruning methods.
- Why unresolved: The paper identifies the problem but doesn't provide solutions for balancing model compression with alignment preservation.
- What evidence would resolve it: Development and validation of pruning algorithms that selectively preserve small singular values corresponding to alignment-critical directions while removing others.

### Open Question 4
- Question: How does the positional encoding affect the activation covariance matrix spectra and RMT deviations?
- Basis in paper: The paper notes that activation covariance matrices have large outliers likely due to positional encoding, but doesn't analyze this effect systematically.
- Why unresolved: The paper observes the phenomenon but doesn't investigate the relationship between positional encoding and RMT behavior.
- What evidence would resolve it: Comparative analysis of RMT deviations in models with and without positional encoding, or systematic variation of positional encoding schemes to study their impact on weight matrix spectra.

## Limitations
- The exact nature of the random matrix model that best represents untrained weight matrices in transformers is not fully established
- The linear random-matrix model's explanatory power and limitations need more thorough validation
- Binary removal of singular values oversimplifies the true importance landscape compared to continuous pruning magnitudes
- The overlap between singular vectors and activation covariance eigenvectors could be influenced by architectural similarities rather than genuine information content

## Confidence
- **High confidence**: The empirical observation that removing RMT-deviating singular values causes larger accuracy drops than random removal is well-supported and robust across multiple models.
- **Medium confidence**: The claim that small singular values are "as informative as large ones" is supported but may be context-dependent and requires more nuanced interpretation.
- **Medium confidence**: The linear random-matrix model provides a plausible explanation but needs more rigorous validation and comparison to alternatives.

## Next Checks
1. **Alternative Pruning Methods**: Validate findings using continuous magnitude-based pruning rather than binary removal to better understand the relative importance landscape across the full spectrum of singular values.

2. **Cross-Architectural Validation**: Test whether the observed importance of small singular values generalizes beyond transformers to other architectures like MLPs, CNNs, or attention-free models to determine if this is a fundamental property or architecture-specific.

3. **Pre-training Dynamics Analysis**: Track the evolution of singular value distributions and their RMT deviations during pre-training to determine when and how small singular values become informative, providing insight into the learning dynamics.