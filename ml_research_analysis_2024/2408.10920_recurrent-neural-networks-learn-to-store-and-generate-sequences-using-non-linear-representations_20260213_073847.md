---
ver: rpa2
title: Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear
  Representations
arxiv_id: '2408.10920'
source_url: https://arxiv.org/abs/2408.10920
tags:
- representations
- input
- sequence
- onion
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RNNs can learn non-linear representations by encoding token positions
  as different orders of magnitude rather than distinct directions. This "onion" representation
  allows multiple tokens to be stored in the same subspace at different scales, with
  autoregressive decoding peeling off each layer.
---

# Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations

## Quick Facts
- arXiv ID: 2408.10920
- Source URL: https://arxiv.org/abs/2408.10920
- Reference count: 24
- Key outcome: RNNs can learn non-linear representations by encoding token positions as different orders of magnitude rather than distinct directions.

## Executive Summary
This paper challenges the Linear Representation Hypothesis (LRH) by demonstrating that gated recurrent neural networks (GRUs) can learn non-linear "onion" representations when trained on a simple sequence repetition task. Small GRUs encode tokens at different magnitudes in the same subspace, while larger GRUs use position-specific linear subspaces. The mechanism relies on autoregressive decoding to sequentially unmask tokens by removing dominant scales. This counterexample shows that mechanistic interpretability methods limited to linear representations may miss important model behaviors.

## Method Summary
The authors trained GRUs of varying sizes (48 to 1024 hidden units) on a repeat task where models must memorize and reproduce random token sequences. They used interchange interventions to probe the structure of learned representations, applying both linear subspace interventions (for testing LRH) and onion interventions (for testing magnitude-based representations). The interventions involved learning transformations to replace token representations at specific positions. Models were trained with autoregressive decoding using AdamW optimizer, and interventions were evaluated on held-out data.

## Key Results
- Small GRUs (N ≤ 64) successfully use onion representations where tokens are encoded as different magnitudes in the same subspace
- Large GRUs (N ≥ 512) learn position-specific linear subspaces consistent with LRH
- Medium GRUs (N = 128, 256) appear to use bigram representations storing consecutive token pairs
- Interchange intervention accuracy distinguishes between these representation types

## Why This Works (Mechanism)

### Mechanism 1: Onion Representations
Small GRUs use "onion" representations where tokens are encoded as different magnitudes in the same subspace rather than as directions in separate subspaces. The model gradually closes all gates synchronously during the input phase, creating exponentially decaying scaling factors for each position. Multiple tokens can be stored in the same subspace at different scales, with autoregressive decoding peeling off each layer by feeding back the decoded token and subtracting its scaled representation.

### Mechanism 2: Linear Representations
Larger GRUs learn linear representations consistent with the Linear Representation Hypothesis, storing each position in a distinct subspace. The model uses sharp gating to freeze individual channels at different times, creating position-specific subspaces. Each token position has its own linear subspace that can be independently accessed and manipulated.

### Mechanism 3: Bigram Representations
Medium-sized GRUs learn bigram representations, storing tuples of consecutive tokens in linear subspaces. Instead of representing individual tokens, the model stores (token_t, token_{t+1}) pairs in subspaces. This benefits autoregressive input by allowing comparison of the current input to stored tuples and generation from the second element of the tuple.

## Foundational Learning

- **Linear Representation Hypothesis (LRH)**: States that neural networks encode concepts as directions in linear subspaces. Understanding LRH is crucial because the paper presents a counterexample to its strong form, showing that not all neural network representations are linear.

- **Interchange interventions**: A method for testing whether representations are linear or non-linear by fixing representations to counterfactual values. The paper uses interchange interventions to test whether representations are linear or non-linear by fixing representations to counterfactual values.

- **Autoregressive decoding**: A decoding strategy where the model receives its own previous output as input. Autoregressive decoding is essential for onion representations because it allows sequential unmasking of tokens by feeding back decoded tokens and removing their scaled contributions.

## Architecture Onboarding

- **Component map**: Input embedding layer (E) -> GRU cell with update gate (z), reset gate (r), and candidate state (u) -> Hidden state vector h that stores the sequence representation -> Output layer with softmax for token prediction -> Intervention mechanism for testing representations

- **Critical path**: 1) Input sequence processed through GRU to generate final hidden state hL 2) hL contains the compressed representation of the input sequence 3) Autoregressive decoding uses hL to generate output tokens sequentially 4) Interventions test the structure of hL by replacing token representations

- **Design tradeoffs**: Model size vs. representation type: Larger models can use linear representations, smaller models must use onion representations. Autoregressive vs. non-autoregressive: Onion representations require autoregressive decoding, linear representations can work with both. Capacity vs. efficiency: Onion representations are more compact but require specific decoding mechanisms.

- **Failure signatures**: Small models failing with non-autoregressive decoding (cannot decode onion representations). Linear intervention probes failing on small models (no separate subspaces). Onion interventions working on large models (compatible with both representation types). Probes failing to decode non-autoregressive GRU representations (no autoregressive unmasking).

- **First 3 experiments**: 1) Train GRUs of different sizes on the repeat task and verify they all achieve high accuracy. 2) Apply linear subspace interventions to test if small models have separate position subspaces. 3) Apply onion interventions to test if small models use magnitude-based representations.

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the capacity limit of onion representations in terms of sequence length, and how does this depend on the numerical precision of the model? The paper mentions that the number of elements that can be stored in onion representations depends on the numerical precision of the data type used for the activations, but this capacity is not empirically characterized.

- **Open Question 2**: Do onion representations emerge in other neural architectures beyond gated RNNs, such as Transformers or state-space models? The authors hypothesize that similar representations might emerge in Transformers and state space models but do not verify this empirically.

- **Open Question 3**: Can onion representations support arbitrary access patterns, or are they limited to sequential access only? The authors note that onion representations are well-suited for memorizing sequences in order or reverse order but cannot provide general storage with arbitrary access patterns.

- **Open Question 4**: What is the relationship between model size and the emergence of onion versus linear representations, and what determines this transition? The paper observes that small models (N ≤ 64) use onion representations while larger models (N ≥ 512) use linear position-specific subspaces, but the underlying mechanism for this transition is not explained.

## Limitations

- The findings are based on a specific task (sequence repetition) and may not generalize to more complex tasks or different training regimes.
- The exact thresholds for when models switch between representation types are not precisely defined, and the transition mechanism is unclear.
- The paper doesn't adequately address whether onion representations provide any advantage over linear ones in terms of efficiency or generalization.

## Confidence

**High Confidence**: The existence of onion representations in small GRUs is well-supported by the intervention experiments. The mechanistic description of how these representations work through scaling factors and autoregressive unmasking is internally consistent and experimentally validated.

**Medium Confidence**: The claim that larger GRUs consistently use linear representations is supported but could be more rigorously tested. The paper shows that large models have linear subspaces, but doesn't conclusively prove that they never use onion mechanisms.

**Low Confidence**: The paper doesn't adequately address whether onion representations provide any advantage over linear ones, or under what conditions each type would be preferred. The theoretical implications for mechanistic interpretability methods are suggested but not empirically tested across diverse model architectures or tasks.

## Next Checks

1. **Representation Switching Threshold**: Systematically vary model sizes around the suspected threshold (between 64 and 128 hidden units) to precisely identify when models switch from onion to linear representations, and test whether this transition is gradual or abrupt.

2. **Non-Autoregressive Decoding Test**: Train identical models without autoregressive decoding to determine whether onion representations can still function or if they are strictly dependent on the autoregressive training regime.

3. **Cross-Task Generalization**: Apply the onion representation intervention method to GRUs trained on different sequence tasks (e.g., language modeling, algorithmic tasks) to determine whether onion representations are a general phenomenon across RNN architectures or specific to the repeat task.