---
ver: rpa2
title: 'Large Language Models and Cognitive Science: A Comprehensive Review of Similarities,
  Differences, and Challenges'
arxiv_id: '2409.02387'
source_url: https://arxiv.org/abs/2409.02387
tags:
- llms
- cognitive
- language
- human
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review examines the intersection of Large Language Models
  (LLMs) and cognitive science, analyzing similarities and differences between LLMs
  and human cognitive processes. Researchers have found that LLMs exhibit human-like
  behaviors in language processing and sensory judgments, with GPT models showing
  correlations with human data across multiple modalities.
---

# Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges

## Quick Facts
- arXiv ID: 2409.02387
- Source URL: https://arxiv.org/abs/2409.02387
- Reference count: 40
- Primary result: Systematic review of LLM-human cognitive comparisons across 40+ studies, identifying strengths in pattern matching but limitations in causal reasoning

## Executive Summary
This comprehensive review examines the intersection of Large Language Models (LLMs) and cognitive science, analyzing how LLMs replicate, diverge from, and inform our understanding of human cognitive processes. The review synthesizes findings from over 40 studies that compare LLMs to human performance across language processing, reasoning, sensory judgments, and decision-making tasks. Researchers found that while LLMs exhibit remarkable human-like behaviors in familiar contexts and can capture individual differences when fine-tuned on psychological datasets, they generally underperform humans in reasoning tasks, particularly when faced with novel or out-of-distribution prompts. The review also highlights how LLMs can serve as cognitive models in specific domains while revealing important limitations in their ability to replicate deeper cognitive functions like functional linguistic competence and causal reasoning.

## Method Summary
The authors conducted a systematic review of existing research comparing LLMs to human cognitive processes, synthesizing findings from over 40 studies across cognitive psychology, neuroscience, and AI literature. The review employed multiple assessment methods including cognitive psychology experiments adapted for LLMs, neuroimaging comparisons using fMRI and MEG data, and traditional psychological tests. The analysis focused on identifying similarities and differences between LLM and human performance across various cognitive domains, evaluating methods for assessing LLM cognitive abilities, and examining applications of LLMs as cognitive models. The review also investigated cognitive biases in LLMs and explored approaches for integrating LLMs with cognitive architectures.

## Key Results
- LLMs show strong correlations with human data in language processing and sensory judgments but underperform in reasoning tasks, especially with novel prompts
- Fine-tuned LLMs can capture individual differences in decision-making tasks and outperform traditional cognitive models
- LLMs exhibit multiple cognitive biases similar to human reasoning biases, requiring mitigation strategies
- Integration approaches between LLMs and cognitive architectures remain challenging, with no consensus on optimal methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit human-like cognitive behaviors in specific domains but fail in reasoning under novel conditions due to distributional constraints.
- Mechanism: LLMs learn statistical patterns from training data, enabling them to replicate human-like responses in familiar contexts (e.g., language prediction, sensory judgments) but cannot generalize to out-of-distribution prompts because they lack the underlying causal models that humans use.
- Core assumption: Human cognitive processes rely on both pattern recognition and causal reasoning, while LLMs rely primarily on statistical pattern matching.
- Evidence anchors:
  - [abstract] Humans generally outperform LLMs in reasoning tasks, especially with out-of-distribution prompts, demonstrating greater robustness and flexibility.
  - [section] LLMs struggle to emulate human-like reasoning when faced with novel and constrained problems, indicating limitations in their ability to generalize beyond their training data.
  - [corpus] Weak - no direct corpus evidence addressing causal reasoning limitations.
- Break condition: If an LLM encounters a prompt that requires reasoning about novel scenarios or abstract concepts not well-represented in its training data, its performance degrades significantly compared to humans.

### Mechanism 2
- Claim: LLMs serve as cognitive models when fine-tuned on psychological experiment data, capturing individual differences and outperforming traditional models in decision-making tasks.
- Mechanism: Fine-tuning LLMs on specific psychological datasets allows them to learn the statistical regularities of human behavior in controlled experimental settings, effectively transforming them into specialized cognitive models.
- Core assumption: Psychological experiments produce consistent, measurable patterns of human behavior that can be learned by statistical models like LLMs.
- Evidence anchors:
  - [abstract] LLMs exhibit human-like behaviors in language processing and sensory judgments, with GPT models showing correlations with human data across multiple modalities.
  - [section] Fine-tuned LLMs can capture individual differences in decision-making tasks, providing insights into cognitive processes.
  - [corpus] Moderate - evidence from studies showing improved performance after fine-tuning on psychological datasets.
- Break condition: If the fine-tuning dataset is too small or not representative of the target population, the LLM may fail to capture genuine individual differences or may overfit to specific patterns.

## Foundational Learning

### Statistical Learning Theory
- Why needed: Explains how LLMs learn patterns from data and why they excel at pattern matching but struggle with novel reasoning
- Quick check: Can you identify which cognitive tasks rely on learned patterns versus causal reasoning?

### Cognitive Architecture Principles
- Why needed: Provides framework for understanding how human cognition differs from statistical pattern matching in LLMs
- Quick check: What are the key architectural differences between human cognition and LLM processing?

### Neuroimaging Analysis Methods
- Why needed: Enables comparison of LLM representations with brain activity patterns
- Quick check: How do fMRI/MEG comparisons reveal similarities and differences between LLM and human processing?

### Cognitive Bias Formation
- Why needed: Explains why LLMs develop human-like biases and how they differ from human bias formation
- Quick check: Can you identify which biases arise from training data versus architectural constraints?

## Architecture Onboarding

### Component Map
LLM Base Model -> Fine-tuning Layer -> Cognitive Task Adapter -> Evaluation Interface

### Critical Path
Data Collection -> Model Training -> Task Adaptation -> Performance Evaluation -> Bias Analysis

### Design Tradeoffs
- Statistical accuracy vs. causal understanding
- Pattern matching efficiency vs. reasoning flexibility
- Training data breadth vs. task-specific performance
- Computational cost vs. cognitive modeling fidelity

### Failure Signatures
- Performance degradation on out-of-distribution prompts
- Inconsistent results across different prompting strategies
- Overfitting to specific task patterns
- Failure to capture temporal dynamics of cognitive processes

### Three First Experiments
1. Test multiple LLM models on standard cognitive psychology tasks (priming effects, syllogism judgments) and compare correlation with human baselines
2. Systematically vary prompting strategies and temperature settings while tracking performance changes across different cognitive domains
3. Compare LLM processing patterns with time-resolved neuroimaging data to assess temporal characteristics of cognitive processing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs develop deeper, context-dependent understanding and reasoning similar to human functional linguistic competence?
- Basis in paper: [explicit] The paper states that while LLMs exhibit near human-level formal linguistic competence, they show patchy performance in functional linguistic competence, suggesting a gap in deeper, context-dependent understanding and reasoning.
- Why unresolved: Current LLMs struggle with functional linguistic competence despite strong formal capabilities, and the fundamental differences in architecture and learning processes between LLMs and the human brain remain a challenge.
- What evidence would resolve it: Demonstrations of LLMs consistently performing at human levels on diverse functional linguistic tasks across various contexts, showing robust generalization beyond their training data.

### Open Question 2
- Question: How can we effectively mitigate cognitive biases in LLMs to align them more closely with human reasoning processes?
- Basis in paper: [explicit] The paper identifies multiple cognitive biases in LLMs similar to those found in human reasoning and emphasizes the need for increased awareness and mitigation strategies.
- Why unresolved: While LLMs exhibit human-like cognitive biases, current methods for identifying and addressing these biases are still developing, and the complex interplay between AI architecture and bias formation is not fully understood.
- What evidence would resolve it: Development of comprehensive frameworks for bias identification and mitigation in LLMs, along with empirical demonstrations of reduced bias-related errors in diverse reasoning tasks.

### Open Question 3
- Question: What is the optimal integration method for combining LLMs with cognitive architectures to create more robust and adaptable AI systems?
- Basis in paper: [explicit] The paper discusses various approaches to integrate LLMs with cognitive architectures, including modular, agency, and neurosymbolic methods, but notes that challenges remain in ensuring accuracy, managing computational costs, and addressing limitations of both systems.
- Why unresolved: Different integration approaches show promise in specific domains, but there is no consensus on the most effective method for creating robust, efficient, and adaptable AI systems that leverage the strengths of both LLMs and cognitive architectures.
- What evidence would resolve it: Comparative studies demonstrating superior performance of one integration method across diverse cognitive tasks and real-world applications, along with analyses of computational efficiency and scalability.

## Limitations
- Analysis constrained by availability and quality of existing studies, with most research focusing on specific cognitive domains rather than comprehensive evaluations
- Many comparisons rely on correlational analyses between LLM outputs and human behavioral or neuroimaging data, which may not establish causal mechanisms
- Current assessment methods primarily capture surface-level patterns rather than underlying cognitive mechanisms

## Confidence
- **High confidence**: LLMs demonstrate strong formal linguistic competence and show human-like patterns in language processing tasks and sensory judgments where distributional statistics are sufficient
- **Medium confidence**: LLMs capture individual differences in decision-making when fine-tuned on psychological datasets, though generalizability across domains remains uncertain
- **Low confidence**: Claims about LLMs as cognitive models require more rigorous validation, particularly regarding their ability to capture the underlying mechanisms of human cognition rather than just surface-level patterns

## Next Checks
1. **Causal mechanism testing**: Design experiments that systematically vary input distributions to test whether LLM-human correlations persist when trained on different datasets, distinguishing pattern matching from genuine cognitive modeling
2. **Cross-modal generalization**: Evaluate the same LLM models across multiple cognitive domains (language, reasoning, sensory judgments) using standardized protocols to assess domain-specific versus general cognitive capabilities
3. **Temporal dynamics validation**: Compare LLM processing patterns with time-resolved neuroimaging data (EEG/MEG) to determine whether temporal characteristics of human cognition are captured beyond static behavioral outputs