---
ver: rpa2
title: Unsupervised learning of Data-driven Facial Expression Coding System (DFECS)
  using keypoint tracking
arxiv_id: '2406.05434'
source_url: https://arxiv.org/abs/2406.05434
tags:
- facial
- face
- dfecs
- keypoints
- component
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DFECS, an unsupervised learning approach
  for automated facial expression coding using keypoint tracking. The method addresses
  the labor-intensive nature of manual facial coding systems like FACS by leveraging
  computer vision-based facial keypoint tracking and advanced dimensionality reduction
  techniques.
---

# Unsupervised learning of Data-driven Facial Expression Coding System (DFECS) using keypoint tracking

## Quick Facts
- arXiv ID: 2406.05434
- Source URL: https://arxiv.org/abs/2406.05434
- Authors: Shivansh Chandra Tripathi; Rahul Garg
- Reference count: 40
- Primary result: DFECS AUs estimated from DISFA dataset account for up to 91.29% variance in test datasets (CK+ and BP4D-Spontaneous)

## Executive Summary
This paper introduces DFECS, an unsupervised learning approach for automated facial expression coding using keypoint tracking. The method addresses the labor-intensive nature of manual facial coding systems like FACS by leveraging computer vision-based facial keypoint tracking and advanced dimensionality reduction techniques. DFECS employs a Full Face Model (FFM) with two-level decomposition using Dictionary Learning (DL) and Non-negative Matrix Factorization (NMF) to estimate Action Units (AUs). This approach introduces sparsity and positivity constraints to enhance interpretability. Results show that DFECS AUs, estimated from the DISFA dataset, account for up to 91.29% variance in test datasets (CK+ and BP4D-Spontaneous), surpassing keypoint-based FACS AUs. Additionally, 87.5% of DFECS AUs are interpretable, aligning with facial muscle movement directions.

## Method Summary
DFECS uses facial keypoint motion vectors (KPM) relative to a neutral frame, with preprocessing steps including frontalization and registration to remove geometric confounds. The Full Face Model employs a two-level decomposition: first, Dictionary Learning decomposes each face part into sparse, non-negative components; second, Non-negative Matrix Factorization combines these into correlated face-part components. The encoding matrices are constrained to be positive and sparse, mirroring biological muscle activation patterns. The model is trained unsupervised on DISFA dataset and evaluated on CK+ and BP4D-Spontaneous test sets using variance explained and interpretability metrics.

## Key Results
- DFECS AUs account for up to 91.29% variance in test datasets (CK+ and BP4D-Spontaneous)
- 87.5% of DFECS AUs are interpretable, aligning with facial muscle movement directions
- DFECS outperforms keypoint-based FACS AUs in variance explained metrics
- Model achieves 95% train variance explained on DISFA training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-level decomposition produces more interpretable AUs than single-stage PCA
- Mechanism: First-level DL creates sparse, non-negative part-face components; second-level NMF combines these into correlated face-part components, enforcing both part-wise and inter-part sparsity and positivity
- Core assumption: Facial muscle movements are sparse and mostly unidirectional from neutral
- Evidence: "These techniques enhance the interpretability of AUs by introducing constraints such as sparsity and positivity to the encoding matrix."

### Mechanism 2
- Claim: Keypoint motion vectors capture muscle movement direction and magnitude while removing geometric confounders
- Mechanism: KPM vectors encode displacement of facial landmarks relative to neutral pose, with preprocessing removing head pose and scale differences
- Core assumption: Neutral-to-expression displacement vectors reflect underlying muscle activation
- Evidence: "To prevent any potential bias or inaccuracies in our model due to geometric variabilities across different subjects—such as head movement, the difference in face sizes, or the relative position of face parts—we preprocess facial keypoints."

### Mechanism 3
- Claim: Non-negative encoding matrices align with biologically plausible muscle activation patterns
- Mechanism: Positive-only weights ensure each AU represents activation in a single direction from neutral, with sparse encoding ensuring only relevant AUs fire
- Core assumption: Muscle activations are additive and do not involve inhibitory signals in the same unit
- Evidence: "The DL applies constraints that ensure positive coefficients in Vf for the unidirectional movement of basis vectors from a neutral face."

## Foundational Learning

- Concept: Sparse coding and non-negative matrix factorization
  - Why needed here: Enables decomposition into interpretable, biologically plausible components that match muscle activation patterns
  - Quick check question: Why does enforcing non-negativity improve interpretability of facial action units?

- Concept: Dimensionality reduction and basis vector representation
  - Why needed here: Reduces high-dimensional keypoint motion data into a compact set of expressive components
  - Quick check question: How does the choice of basis affect both reconstruction accuracy and interpretability?

- Concept: Facial keypoint tracking and geometric normalization
  - Why needed here: Ensures that KPM vectors reflect expression changes, not pose or scale differences
  - Quick check question: What preprocessing steps are critical to remove geometric confounds before computing KPM vectors?

## Architecture Onboarding

- Component map: raw video → facial keypoint tracking → geometric corrections (frontalization, affine, similarity) → KPM vectors → PFM (DL per face part) → HFM (NMF over concatenated PFM encodings) → final AU matrix
- Critical path: KPM generation → FFM training → encoding matrix computation → variance/interpretability evaluation
- Design tradeoffs:
  - More keypoints → better expression capture but higher dimensionality and computational cost
  - Fewer AU components → sparser, more interpretable but lower reconstruction fidelity
  - Steeper L1 regularization → higher sparsity but possible underfitting
- Failure signatures:
  - Low interpretability despite high variance → constraints too weak or KPM noise
  - Low variance on test data → overfitting to training set or too aggressive sparsity
  - High variance but nonsensical AU shapes → preprocessing errors or tracking inaccuracies
- First 3 experiments:
  1. Train FFM on DISFA (train) subset, evaluate variance explained and interpretability on CK+ test set
  2. Compare DFECS AUs to PCA AUs by varying number of components and measuring both metrics
  3. Ablate preprocessing steps (e.g., remove similarity registration) and observe impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would incorporating more facial keypoints impact accuracy and interpretability of DFECS AUs in capturing subtle expressions?
- Basis: Paper acknowledges current 68-point limitation and suggests more keypoints could improve system capabilities
- Why unresolved: No empirical evidence provided, merely a suggestion for future improvement
- What evidence would resolve it: Experiments with larger number of keypoints compared to current 68-point model

### Open Question 2
- Question: What are potential benefits and challenges of creating KPM-based FACS AUs using responses from multiple subjects?
- Basis: Paper suggests current comparison method may not be optimal and proposes exploring KPM-based FACS AUs from multiple subjects
- Why unresolved: Not explored or any insights provided
- What evidence would resolve it: Experiments creating KPM-based FACS AUs from multiple subjects and comparing with DFECS AUs

### Open Question 3
- Question: How can interpretability of DFECS AUs be further improved by developing more robust metrics?
- Basis: Paper acknowledges current interpretability analysis may be limited due to complexity of facial expressions
- Why unresolved: No specific metrics or methods proposed, left as future work suggestion
- What evidence would resolve it: Developing and testing new interpretability metrics that account for expression complexity

## Limitations

- The unsupervised nature relies heavily on the assumption that keypoint motion vectors capture muscle activation patterns without explicit labeling
- Interpretability assessment depends on volunteer labeling, which may introduce subjective bias without inter-rater reliability metrics
- Performance claims based on variance explained metrics don't directly measure AU detection accuracy against ground truth

## Confidence

- **High Confidence**: Technical framework of using keypoint motion vectors and two-level decomposition (DL + NMF) is well-specified and reproducible
- **Medium Confidence**: Claims about interpretability (87.5% of AUs aligning with muscle movements) are supported by methodology but depend on subjective human evaluation
- **Low Confidence**: Claims comparing to FACS AUs are difficult to validate since comparison method for keypoint-based FACS AUs is not fully specified

## Next Checks

1. Replicate preprocessing pipeline (frontalization, affine, similarity registration) on held-out DISFA subset and verify KPM vectors capture expected expression patterns
2. Conduct formal inter-rater reliability study for interpretability assessment with multiple volunteers labeling same DFECS AUs
3. Implement keypoint-based FACS AU extraction method and compare against DFECS AUs on same test sets to verify 91.29% variance claim