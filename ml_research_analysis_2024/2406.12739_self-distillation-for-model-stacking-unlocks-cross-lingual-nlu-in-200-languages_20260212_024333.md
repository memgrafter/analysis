---
ver: rpa2
title: Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages
arxiv_id: '2406.12739'
source_url: https://arxiv.org/abs/2406.12739
tags:
- performance
- latn
- language
- languages
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of extending the natural language
  understanding (NLU) capabilities of large language models (LLMs) from English to
  underrepresented languages. While LLMs excel in English NLU due to pretraining on
  web-scale corpora, their performance degrades significantly in cross-lingual transfer
  to low-resource languages.
---

# Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages

## Quick Facts
- arXiv ID: 2406.12739
- Source URL: https://arxiv.org/abs/2406.12739
- Reference count: 34
- Primary result: Self-distillation integrating MT encoders into LLMs improves zero-shot cross-lingual transfer (ZS-XLT) by 3-11% over translate-test across 127 languages and three NLU tasks.

## Executive Summary
This work addresses the challenge of extending the natural language understanding (NLU) capabilities of large language models (LLMs) from English to underrepresented languages. While LLMs excel in English NLU due to pretraining on web-scale corpora, their performance degrades significantly in cross-lingual transfer to low-resource languages. Machine translation (MT) models, on the other hand, provide excellent multilingual representations but lack the knowledge necessary for comprehensive NLU. To get the best of both worlds, the authors propose a method to integrate MT encoders directly into LLM backbones via sample-efficient self-distillation. This integration creates a unified multilingual MT-LLM that preserves the inherent multilingual representational alignment from the MT encoder while tapping into the rich knowledge embedded in English-centric LLMs. The resulting MT-LLMs substantially and consistently outperform translate-test based on the same MT model, showing that they truly unlock multilingual language understanding for LLMs.

## Method Summary
The authors propose a self-distillation approach that integrates a machine translation (MT) encoder into the backbone of a large language model (LLM) to create a unified multilingual model capable of cross-lingual natural language understanding (NLU). The method leverages the multilingual representational alignment inherent in the MT encoder and the knowledge embedded in English-centric LLMs. Through sample-efficient self-distillation, the integrated MT-LLM model achieves substantial improvements in zero-shot cross-lingual transfer (ZS-XLT) performance over traditional translate-test methods across 127 predominantly low-resource languages and three NLU tasks.

## Key Results
- MT-LLMs improve zero-shot cross-lingual transfer (ZS-XLT) performance by 3-11% over translate-test across three NLU tasks.
- Improvements are observed across 127 predominantly low-resource languages.
- The approach consistently outperforms translate-test based on the same MT model, demonstrating unlocked multilingual NLU for LLMs.

## Why This Works (Mechanism)
The proposed method works by leveraging the complementary strengths of MT encoders and LLMs. MT encoders provide robust multilingual representations that capture semantic meaning across languages, while LLMs contribute extensive world knowledge and reasoning capabilities developed through English-centric pretraining. The self-distillation process allows the LLM backbone to learn these multilingual representations without requiring parallel corpora or extensive retraining. By integrating the MT encoder directly into the LLM architecture, the model maintains representational alignment across languages while inheriting the knowledge base of the LLM. This architectural integration enables the model to perform NLU tasks in multiple languages by routing inputs through the MT encoder for multilingual encoding, then applying the LLM's knowledge and reasoning capabilities to produce final outputs.

## Foundational Learning
- Cross-lingual transfer: The ability of a model trained in one language to generalize to another. **Why needed**: Low-resource languages lack sufficient training data for effective NLU. **Quick check**: Evaluate model performance on zero-shot tasks in target languages.
- Multilingual representations: Encodings that capture meaning across multiple languages. **Why needed**: LLMs trained primarily on English data struggle with non-English inputs. **Quick check**: Measure representational similarity across languages using similarity metrics.
- Self-distillation: A training technique where a model learns from its own outputs. **Why needed**: Efficiently transfers knowledge from MT models to LLMs without requiring extensive parallel corpora. **Quick check**: Compare performance against supervised fine-tuning baselines.
- MT encoder integration: Incorporating machine translation components directly into LLM architectures. **Why needed**: Combines multilingual capabilities of MT with NLU knowledge of LLMs. **Quick check**: Verify that multilingual representations are preserved post-integration.

## Architecture Onboarding
- **Component map**: MT Encoder -> Self-Distillation Module -> LLM Backbone -> NLU Output
- **Critical path**: MT encoder produces multilingual representations that are distilled into the LLM backbone through self-distillation, enabling cross-lingual NLU capabilities.
- **Design tradeoffs**: Balances between preserving multilingual representational alignment from MT encoder and maintaining NLU knowledge from LLM; prioritizes sample efficiency over exhaustive training.
- **Failure signatures**: Performance degradation when MT encoder and LLM are mismatched; limited gains if distillation iterations are insufficient.
- **First experiments**: 1) Test MT-LLM performance on held-out languages not in the 127-language set. 2) Evaluate on additional NLU and generative tasks beyond XQuAD, PAWS-X, and XNLI. 3) Conduct ablation studies to isolate contributions of MT encoder versus LLM backbone.

## Open Questions the Paper Calls Out
- How well do MT-LLMs generalize to languages beyond the 127 evaluated?
- What is the minimal number of distillation iterations required for peak performance?
- How do different MT-LLM architecture combinations affect cross-lingual transfer quality?

## Limitations
- Evaluation restricted to 127 languages, leaving uncertain whether gains extend to remaining 70+ languages in coverage set.
- Performance improvements reported only for three NLU tasks, making generalization to other tasks unclear.
- Experiments rely on a single base LLM and MT model pair, limiting claims about robustness across different architectures.
- Sample efficiency claims lack direct comparisons of computational cost or training time against baselines.

## Confidence
- High: Core claim that self-distillation with MT encoders improves zero-shot cross-lingual NLU performance is supported by consistent gains across three benchmarks and large language sets.
- Medium: Broader claim that this approach "unlocks" multilingual NLU for LLMs in general is limited by narrow scope of models and tasks evaluated.
- Low: Assertion that the method is sample-efficient and practical for low-resource languages lacks direct comparisons of sample efficiency or computational cost.

## Next Checks
1. Replicate the self-distillation pipeline with alternative LLM backbones (e.g., Llama, Mistral) and MT models (e.g., mBART, m2m-100) to assess robustness.
2. Evaluate performance on a broader set of NLU and generative tasks, including those with minimal training data per language.
3. Conduct ablation studies to measure the relative contributions of the MT encoder and LLM backbone, and to determine the minimal number of distillation iterations required for peak performance.