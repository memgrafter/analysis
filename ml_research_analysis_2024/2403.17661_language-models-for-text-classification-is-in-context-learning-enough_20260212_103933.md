---
ver: rpa2
title: 'Language Models for Text Classification: Is In-Context Learning Enough?'
arxiv_id: '2403.17661'
source_url: https://arxiv.org/abs/2403.17661
tags:
- text
- language
- classification
- llama
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the effectiveness of large language models
  using in-context learning with traditional fine-tuning approaches for text classification.
  The study evaluates five models (LLaMA, Flan-T5, T5, RoBERTa, and FastText) across
  16 datasets covering binary, multiclass, and multilabel classification tasks.
---

# Language Models for Text Classification: Is In-Context Learning Enough?

## Quick Facts
- arXiv ID: 2403.17661
- Source URL: https://arxiv.org/abs/2403.17661
- Reference count: 22
- Primary result: Fine-tuned smaller models outperform few-shot larger models for complex text classification tasks

## Executive Summary
This paper systematically compares the effectiveness of large language models using in-context learning (ICL) with traditional fine-tuning approaches for text classification. The study evaluates five models (LLaMA, Flan-T5, T5, RoBERTa, and FastText) across 16 datasets covering binary, multiclass, and multilabel classification tasks. Key findings show that while instruction-tuned models like Flan-T5 perform well in zero- and one-shot settings, especially for binary and multiclass problems, fine-tuned smaller models like RoBERTa still outperform them in more complex multilabel tasks. The results suggest that fine-tuning smaller and more efficient language models can be more effective than few-shot approaches of larger language models for text classification, particularly when sufficient training data is available.

## Method Summary
The study compares five models using two approaches: in-context learning (zero- and one-shot) for LLaMA, Flan-T5, and GPT 3.5, and fine-tuning for RoBERTa, T5, and FastText. Models were evaluated on 16 text classification datasets across 7 domains, covering binary, multiclass, and multilabel tasks. Three prompt types (generic, task, domain) were tested for ICL approaches. Evaluation used standard micro and macro averaged F1 scores. Fine-tuning used specific hyperparameters: RoBERTa (4 epochs, 2e-5 learning rate), T5 (2 epochs, 5e-5 learning rate), and FastText (25 epochs).

## Key Results
- Instruction-tuned models (Flan-T5) outperform larger autoregressive models (LLaMA) in zero- and one-shot settings for binary and multiclass problems
- Fine-tuned smaller models (RoBERTa) significantly outperform few-shot approaches on complex multilabel classification tasks
- Model performance shows relative insensitivity to prompt variations across zero- and one-shot settings
- Larger autoregressive models perform poorly in few-shot settings compared to smaller instruction-tuned models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller instruction-tuned models (Flan-T5) outperform larger autoregressive models (LLaMA) in zero- and one-shot text classification tasks.
- Mechanism: Instruction tuning provides models with better generalization capabilities by exposing them to diverse task instructions during training, allowing them to understand and follow task descriptions more effectively than models trained solely on autoregressive objectives.
- Core assumption: The instruction-tuning dataset contains representative task descriptions that capture the patterns needed for text classification.
- Evidence anchors:
  - [abstract] "instruction-tuned models like Flan-T5 perform well in zero- and one-shot settings"
  - [section 4.1] "smaller but instruction-tuned models can be more beneficial in zero- and few- shot classification in comparison to larger text generation models"
  - [corpus] "Average neighbor FMR=0.587" - moderate similarity to related work suggests this finding is in line with other research on instruction-tuned models

### Mechanism 2
- Claim: Fine-tuned smaller language models (RoBERTa) outperform few-shot approaches of larger language models for complex classification tasks.
- Mechanism: Fine-tuning adapts the model's weights to the specific task distribution, allowing it to learn task-specific patterns that cannot be captured through few-shot demonstrations alone, particularly for multilabel and multiclass problems.
- Core assumption: Sufficient training data is available for fine-tuning to be effective.
- Evidence anchors:
  - [abstract] "fine-tuned smaller models like RoBERTa still outperform them in more complex multilabel tasks"
  - [section 4.2] "fine-tuned masked language models are more suitable for complex classification tasks such as multiclass and multilabeling problems when the number of labels is higher"
  - [corpus] "A Comparative Study of Task Adaptation Techniques of Large Language Models" - suggests this is an active area of research

### Mechanism 3
- Claim: Model performance is relatively insensitive to prompt variations for zero- and one-shot settings.
- Mechanism: The text generation models have learned robust representations that allow them to understand the task intent even with simple or varied prompt formulations.
- Core assumption: The models' pre-training and instruction-tuning have provided sufficient semantic understanding to interpret different prompt phrasings.
- Evidence anchors:
  - [section 3.4] "prompt choice does not lead to significant changes in the models behaviour where the deviation for the three prompts across all models is relatively small"
  - [section 4.1] "deviation for the three prompts across all models is relatively small"
  - [corpus] "The Impact of Role Design in In-Context Learning" - indicates prompt engineering is an active research area

## Foundational Learning

- Concept: Zero-shot and few-shot learning paradigms
  - Why needed here: The paper compares models in both zero-shot and one-shot settings, which are fundamentally different from traditional supervised learning approaches.
  - Quick check question: What is the key difference between zero-shot and few-shot learning in the context of in-context learning?

- Concept: Masked Language Models vs Autoregressive Models
  - Why needed here: The paper compares different model architectures (RoBERTa vs LLaMA) and their suitability for text classification tasks.
  - Quick check question: How do the training objectives of masked language models differ from autoregressive models, and why might this affect classification performance?

- Concept: Prompt engineering and instruction tuning
  - Why needed here: The paper uses different prompt formulations and compares instruction-tuned models to standard models, making understanding of these concepts essential.
  - Quick check question: What is instruction tuning and how does it differ from standard pre-training?

## Architecture Onboarding

- Component map: Datasets (16) -> Models (5) -> Training approach (Fine-tuning vs ICL) -> Prompt selection (3 types) -> Evaluation (F1 scores) -> Analysis
- Critical path: Dataset → Model selection → Training approach (fine-tuning vs ICL) → Prompt selection → Evaluation → Analysis
- Design tradeoffs:
  - Model size vs performance: Larger models don't always outperform smaller ones when instruction-tuned
  - Data efficiency vs accuracy: Few-shot approaches work well for simpler tasks but struggle with complex classification
  - Prompt engineering vs robustness: Simple prompts work reasonably well, but may not be optimal for all tasks
- Failure signatures:
  - Poor performance on multilabel tasks with ICL approaches
  - High variance in results across different prompts
  - Decreased performance as number of classification labels increases
  - Data contamination issues when test datasets overlap with training data
- First 3 experiments:
  1. Replicate binary classification results with Flan-T5 vs RoBERTa on Twitter datasets to verify prompt insensitivity
  2. Test one-shot learning performance on multiclass datasets to confirm advantage over zero-shot
  3. Evaluate multilabel classification with different prompt formulations to identify optimal approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of few-shot learning with language models scale with the number of training examples per label?
- Basis in paper: [inferred] The paper briefly mentions that they used a single training example per label for one-shot prompting and notes that "the choice of one shot training instances can influence the performance of models in few-shot learning" but doesn't explore this systematically.
- Why unresolved: The authors explicitly state this as a direction for future work, noting they "performed experiments for Flan-T5 and LLaMA in zero- and one- shot ICL settings" but did not investigate the impact of varying the number of examples.
- What evidence would resolve it: Conducting experiments with different numbers of training examples per label (e.g., 1, 2, 4, 8, 16) and comparing performance curves across models would quantify how many examples are needed for few-shot learning to approach fine-tuning performance.

### Open Question 2
- Question: What is the impact of prompt engineering techniques on the performance of few-shot text classification with language models?
- Basis in paper: [explicit] The authors acknowledge that "the paper presents a study for zero- and one-shot prompting" and note that "considering the sensitivity of in-context learning approaches to the given instructions, it would be beneficial to perform further analysis on a larger more diverse set of prompts."
- Why unresolved: While the paper tests three prompt types (generic, task, domain), it explicitly states this was not the focus and that "majority of existing research focusing on optimisation techniques for prompt creation" was not their approach.
- What evidence would resolve it: Testing a wider variety of prompt formats, including different template structures, exemplar selection strategies, and prompt optimization methods, would reveal the true impact of prompt engineering on few-shot performance.

### Open Question 3
- Question: How do language models perform on few-shot text classification tasks in languages other than English?
- Basis in paper: [explicit] The authors explicitly state this as a limitation: "Experiments for other languages (especially low-resource) could show a different tendency."
- Why unresolved: The paper only evaluates English-language datasets, and the authors acknowledge this as a limitation without exploring multilingual performance.
- What evidence would resolve it: Conducting similar experiments on multilingual datasets, particularly for low-resource languages, would reveal whether the observed trends (e.g., Flan-T5 outperforming larger models) hold across different languages and resource levels.

## Limitations

- Limited dataset diversity may not capture real-world task complexity
- Only three prompt formulations tested, potentially missing optimal configurations
- Resource considerations (computational costs, inference latency) not comprehensively addressed
- Results may not generalize to non-English languages or specialized domains

## Confidence

- High Confidence: Fine-tuned RoBERTa outperforms few-shot approaches on multilabel classification tasks
- Medium Confidence: Instruction-tuned models perform well in zero- and one-shot settings for binary and multiclass problems
- Low Confidence: Prompt choice does not significantly affect model performance

## Next Checks

1. **Cross-Domain Robustness Test**: Evaluate the same models and approaches on a dataset from a domain not represented in the original study (e.g., legal documents or medical text) to assess generalizability of the findings.

2. **Prompt Sensitivity Analysis**: Systematically vary prompt formulations across a wider range of styles and complexity levels to quantify the actual sensitivity of each model to prompt engineering, particularly for specialized domains.

3. **Computational Efficiency Benchmark**: Measure and compare inference latency, memory usage, and total cost of ownership for each approach (fine-tuned vs few-shot) to provide a more complete picture of practical deployment considerations.