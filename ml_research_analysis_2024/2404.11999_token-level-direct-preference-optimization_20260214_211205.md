---
ver: rpa2
title: Token-level Direct Preference Optimization
arxiv_id: '2404.11999'
source_url: https://arxiv.org/abs/2404.11999
tags:
- uni00000013
- divergence
- function
- uni00000018
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Token-level Direct Preference Optimization
  (TDPO), a novel method for fine-tuning Large Language Models (LLMs) by optimizing
  policy at the token level to better align with human preferences. Unlike existing
  approaches like DPO that operate at the sentence level, TDPO incorporates forward
  KL divergence constraints for each token, improving alignment and diversity.
---

# Token-level Direct Preference Optimization

## Quick Facts
- arXiv ID: 2404.11999
- Source URL: https://arxiv.org/abs/2404.11999
- Reference count: 40
- Key outcome: TDPO optimizes LLMs at token level using forward KL divergence constraints, outperforming DPO on alignment-diversity trade-offs across sentiment and dialogue tasks.

## Executive Summary
Token-level Direct Preference Optimization (TDPO) introduces a novel fine-tuning approach for Large Language Models that operates at the token level rather than the sentence level. Unlike existing methods like DPO, TDPO incorporates forward KL divergence constraints for each token, improving alignment with human preferences while maintaining generation diversity. The method uses the Bradley-Terry model for token-based reward estimation and avoids explicit reward modeling, keeping the approach simple. Experiments demonstrate superior performance compared to DPO on IMDb sentiment control, single-turn dialogue, and response quality tasks, while also improving diversity metrics.

## Method Summary
TDPO fine-tunes LLMs by optimizing at the token level using preference data, incorporating forward KL divergence constraints to maintain diversity while aligning with human preferences. The method employs the Bradley-Terry model for token-based reward estimation without requiring explicit reward modeling. TDPO uses the log-likelihood ratio of preferred versus dispreferred tokens as the optimization objective, combined with a forward KL divergence term that encourages exploration and diversity. The approach is trained using preference pairs from datasets like IMDb and Anthropic HH, with the token-level rewards derived from the relative likelihood of each token in preferred versus dispreferred responses.

## Key Results
- TDPO outperforms DPO on controlled sentiment generation tasks, achieving better alignment with human preferences while maintaining higher diversity scores
- On single-turn dialogue tasks from MT-bench, TDPO produces significantly higher quality responses compared to both DPO and PPO-based RLHF methods
- The method demonstrates improved balance between alignment and diversity metrics, showing that token-level optimization can achieve better trade-offs than sentence-level approaches

## Why This Works (Mechanism)
TDPO works by optimizing at the token level rather than the sentence level, which allows for finer-grained control over the generation process. By incorporating forward KL divergence constraints at each token position, the method maintains a balance between exploiting known good tokens and exploring new ones, preventing premature convergence to conservative outputs. The Bradley-Terry model provides a principled way to estimate token-level rewards from pairwise preferences without requiring explicit reward modeling, making the approach more scalable and simpler to implement.

## Foundational Learning

**Forward KL Divergence**: Measures the difference between two probability distributions, encouraging the model to cover all modes of the true distribution rather than just the mode. Needed to prevent mode collapse and maintain diversity in generated text. Quick check: Compare entropy of generated samples with and without KL constraint.

**Bradley-Terry Model**: A pairwise comparison model that estimates the probability of one item being preferred over another. Used here to derive token-level rewards from preference data without explicit reward modeling. Quick check: Verify that token preferences correlate with human judgment on held-out data.

**Token-level vs Sentence-level Optimization**: Operating at the token level allows for more granular control and better capture of local preferences, while sentence-level optimization may miss important token-level patterns. Quick check: Compare token-level and sentence-level gradient updates on simple preference tasks.

## Architecture Onboarding

**Component Map**: Preference Data -> Bradley-Terry Scoring -> Token-level Rewards -> KL-constrained DPO Objective -> Model Updates

**Critical Path**: The core training loop involves computing token-level rewards using Bradley-Terry scores, calculating the DPO loss with forward KL constraints, and performing gradient updates on the LLM parameters.

**Design Tradeoffs**: Token-level optimization provides finer control but increases computational complexity compared to sentence-level approaches. The forward KL constraint helps maintain diversity but requires careful tuning of the weighting hyperparameter.

**Failure Signatures**: Potential issues include reward hacking where the model learns to exploit the Bradley-Terry scoring mechanism, instability in token-level reward estimation for rare tokens, and difficulty in scaling to very long sequences due to cumulative KL divergence effects.

**First Experiments**: 1) Verify KL constraint effectiveness on synthetic preference data with known distributions, 2) Compare token-level vs sentence-level optimization on simple sentiment classification, 3) Test Bradley-Terry reward estimation on a small-scale preference dataset with manual verification.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implicit questions remain regarding the method's scalability to multi-turn conversations, performance on non-dialogue tasks, and behavior with different model scales and dataset sizes.

## Limitations
- Evaluation is limited to controlled sentiment generation and single-turn dialogue tasks, leaving multi-turn conversation performance unclear
- The forward KL divergence hyperparameter sensitivity and its interaction with different model scales is not thoroughly explored
- The Bradley-Terry model may face instability issues when applied to rare or out-of-distribution tokens

## Confidence

**Major Claims Confidence:**
- TDPO's superiority over sentence-level methods (Medium): Supported by experimental results on limited datasets, but lacks broader domain coverage
- Forward KL divergence as a key enabler of diversity (Medium): Conceptually sound, but hyperparameter sensitivity is underexplored
- Token-level reward modeling simplicity (High): The approach is well-defined and technically straightforward

## Next Checks

1. Evaluate TDPO on multi-turn dialogue benchmarks (e.g., MultiWOZ) and task-oriented datasets to assess scalability and robustness
2. Conduct ablation studies on the forward KL divergence weight to quantify its impact on alignment-diversity trade-offs across different model sizes
3. Test TDPO's performance on non-dialogue tasks such as code generation or mathematical reasoning to verify generalizability