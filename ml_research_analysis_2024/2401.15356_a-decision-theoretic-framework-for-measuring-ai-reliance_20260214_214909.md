---
ver: rpa2
title: A Decision Theoretic Framework for Measuring AI Reliance
arxiv_id: '2401.15356'
source_url: https://arxiv.org/abs/2401.15356
tags:
- rational
- human
- reliance
- decision
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a formal statistical framework for measuring
  AI reliance in human-AI decision-making. The authors argue that existing definitions
  of appropriate reliance lack formal grounding and can lead to contradictions.
---

# A Decision Theoretic Framework for Measuring AI Reliance

## Quick Facts
- arXiv ID: 2401.15356
- Source URL: https://arxiv.org/abs/2401.15356
- Reference count: 40
- Primary result: Introduces formal statistical framework for measuring AI reliance in human-AI decision-making

## Executive Summary
This paper presents a formal statistical framework for measuring AI reliance that addresses fundamental limitations in existing approaches. The authors demonstrate that current definitions of appropriate reliance are inconsistent and can lead to contradictions. Their proposed framework separates reliance from challenges in signal differentiation and belief formation, providing a more principled way to evaluate human-AI team performance. The framework introduces benchmarks for complementary performance (rational decision-maker with access to both human and AI predictions) and baselines (rational decision-maker with access to only human or AI), allowing researchers to isolate losses due to mis-reliance from losses due to poor signal quality.

## Method Summary
The framework formalizes decision tasks by defining states, actions, signals, and scoring rules. It calculates expected payoffs using empirical distributions from experimental data, comparing behavioral performance against benchmarks. The rational baseline identifies the best response to the empirical distribution of states, while the rational benchmark simulates the best response to each signal. The authors apply this framework to three AI-advised decision-making studies, calculating rational baseline, rational benchmark, behavioral performance, and mis-reliant rational benchmark. The approach allows researchers to quantify how much of the performance gap between humans and AI is due to over- or under-reliance versus fundamental limitations in the available signals.

## Key Results
- Existing definitions of appropriate AI reliance are inconsistent and can lead to contradictions
- Framework separates reliance from signal differentiation challenges, providing more nuanced performance insights
- Applied framework to three studies, demonstrating how it isolates loss due to mis-reliance from loss due to poor signal quality

## Why This Works (Mechanism)
The framework works by establishing a principled statistical foundation for measuring reliance that avoids the contradictions inherent in previous approaches. By using decision theory to define rational baselines and benchmarks, it provides objective reference points for evaluating human behavior. The separation of reliance from signal quality allows researchers to identify whether performance issues stem from decision-making biases or fundamental limitations in the available information. The framework's ability to calculate expected payoffs under different scenarios enables quantitative comparison of human performance against optimal decision-making.

## Foundational Learning

Expected Utility Theory - why needed: Provides the mathematical foundation for rational decision-making under uncertainty; quick check: verify that all payoff calculations follow expected utility maximization principles

Scoring Rules - why needed: Define how rewards are allocated based on prediction accuracy; quick check: confirm that scoring rules are properly specified and consistently applied across experiments

Empirical Distribution - why needed: Captures the actual state probabilities observed in experimental data; quick check: validate that empirical distributions are calculated correctly from raw data

Rational Baseline vs Benchmark - why needed: Distinguish between optimal decisions given available information versus optimal decisions given specific signals; quick check: ensure baseline and benchmark calculations are methodologically distinct

Signal Space - why needed: Defines the set of possible information states humans and AI can provide; quick check: verify that signal discretization preserves meaningful distinctions

Cross-validation - why needed: Prevents overfitting when calculating rational benchmarks on empirical data; quick check: confirm that out-of-sample performance matches in-sample estimates

## Architecture Onboarding

Component Map:
Experimental Design -> Formalization -> Rational Baseline Calculation -> Rational Benchmark Calculation -> Performance Comparison

Critical Path:
1. Formalize experimental design as decision problem
2. Calculate rational baseline from empirical state distribution
3. Calculate rational benchmark by simulating best response to each signal
4. Compare behavioral performance against benchmarks

Design Tradeoffs:
- Signal discretization granularity vs computational tractability
- Model complexity vs interpretability of results
- Theoretical assumptions vs practical applicability

Failure Signatures:
- Large gaps between behavioral performance and rational benchmark suggest reliance issues
- Small gaps between rational baseline and rational benchmark indicate signal quality limitations
- Inconsistent results across experiments may indicate framework assumptions violations

First Experiments:
1. Calculate rational baseline for a simple binary decision task
2. Compute rational benchmark using cross-validation to prevent overfitting
3. Compare behavioral performance metrics across different experimental conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Framework assumes rational human decision-making that may not reflect actual cognitive processes
- Empirical validation limited to three existing studies with varying experimental designs
- Performance calculations may be sensitive to scoring rule specifications and data preprocessing choices

## Confidence

Theoretical Framework: Medium - mathematical formalization appears sound but depends on strong rationality assumptions
Empirical Validation: Low-Medium - limited scope of validation studies and potential overfitting concerns
Generalizability: Low-Medium - framework's performance across diverse AI contexts remains unclear

## Next Checks

1. Test framework across diverse AI decision-making contexts (medical diagnosis, financial forecasting) to assess generalizability
2. Compare framework predictions against controlled experiments where human decision-making is measured under varying AI assistance levels
3. Evaluate framework sensitivity to different scoring rules and information disclosure levels to understand robustness to experimental design choices