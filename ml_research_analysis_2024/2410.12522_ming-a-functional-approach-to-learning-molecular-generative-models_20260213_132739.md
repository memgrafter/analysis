---
ver: rpa2
title: 'MING: A Functional Approach to Learning Molecular Generative Models'
arxiv_id: '2410.12522'
source_url: https://arxiv.org/abs/2410.12522
tags:
- ming
- function
- molecular
- diffusion
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MING introduces a novel generative model for molecules by representing
  them as functions in function space, rather than using traditional graph or sequence
  representations. It leverages implicit neural representations (INRs) and a diffusion-based
  approach, specifically a denoising probabilistic model that jointly denoises in
  both input and output spaces.
---

# MING: A Functional Approach to Learning Molecular Generative Models

## Quick Facts
- arXiv ID: 2410.12522
- Source URL: https://arxiv.org/abs/2410.12522
- Reference count: 22
- Key outcome: MING achieves superior molecular generation performance by representing molecules as functions in function space, outperforming state-of-the-art methods on QM9, ZINC250k, and MOSES datasets

## Executive Summary
MING introduces a novel generative model for molecules that represents them as functions in function space rather than traditional graph or sequence representations. The method leverages implicit neural representations (INRs) and a diffusion-based approach, specifically a denoising probabilistic model that jointly denoises in both input and output spaces. By avoiding graph-permutation symmetry constraints and using a streamlined architecture, MING generates chemically valid molecules with high uniqueness and novelty while being significantly faster and using fewer parameters than existing methods.

## Method Summary
MING uses a TwinINR architecture with two identical conditional INR networks (latent model ψ and denoising network θ) that share parameters through modulation vectors. The model represents molecules as continuous functions using graph spectral embeddings for coordinate systems, with eigenvectors of the graph Laplacian creating node coordinates. Training employs an expectation-maximization denoising process that iteratively optimizes a noisy latent input and updates both networks. The diffusion process operates only on the output signal space while keeping the topology input invariant, sidestepping the need for equivariant architectures.

## Key Results
- Achieves superior performance on benchmark datasets (QM9, ZINC250k, MOSES) with state-of-the-art validity and uniqueness scores
- Outperforms data-space methods in distribution-based metrics (NSPDK, FCD) while being 9x faster and using 40x fewer parameters
- Demonstrates scalability advantages on larger datasets where graph-permutation constraints become more problematic

## Why This Works (Mechanism)

### Mechanism 1
The joint denoising in input and output spaces via EM procedure enables learning distributions of molecular functions without graph-permutation symmetry constraints. MING uses an expectation-maximization denoising process that iteratively optimizes a noisy latent input `zt` (E-step) and then updates both the denoising network `θ` and latent model `ψ` (M-step). By diffusing only in the output signal space while keeping the topology input invariant, the model sidesteps the need for equivariant architectures.

### Mechanism 2
TwinINR architecture with latent modulation enables efficient parameter sharing and avoids complex equivariant designs. TwinINR uses two identical conditional INR networks (`ψ` for latent model and `θ` for denoising) with modulation vectors that condition on the latent input `z`. This allows the same architecture to represent different molecular functions by adjusting the latent input, rather than requiring a unique set of parameters per function.

### Mechanism 3
Graph spectral embeddings as coordinate systems enable functional representations on irregular domains like molecular graphs. MING uses eigenvectors of the graph Laplacian to create node coordinates, and pairwise products for edge coordinates. This transforms the irregular graph domain into a coordinate system suitable for INRs, enabling the model to represent molecular functions in a continuous space.

## Foundational Learning

- **Diffusion probabilistic models**: Understanding how noise schedules and reverse processes work in function space rather than data space is crucial for MING's approach. Quick check: What is the relationship between the forward noise kernel and the reverse denoising process in standard diffusion models?

- **Implicit Neural Representations (INRs)**: MING represents molecules as continuous functions using INRs, requiring understanding of how neural networks can approximate arbitrary functions given coordinate inputs. Quick check: How do conditional INRs differ from standard INRs in terms of parameter efficiency?

- **Expectation-Maximization optimization**: The EM denoising process is central to MING's training algorithm, requiring understanding of how E-step and M-step work in this functional setting. Quick check: In MING's context, what exactly is being optimized in the E-step versus the M-step?

## Architecture Onboarding

- **Component map**: Coordinate generation (eigenvectors) → Function evaluation (s = (ϕ, z, y)) → EM optimization (E-step: optimize zt, M-step: update θ and ψ) → Sampling (reverse diffusion)
- **Critical path**: Graph topology → Eigenvector-based coordinates → Latent input generation → TwinINR function representation → EM denoising optimization → Molecule generation
- **Design tradeoffs**: Using function space avoids graph-permutation symmetry but requires coordinate system design; using INRs enables continuous representation but needs careful latent input dimension selection
- **Failure signatures**: Poor validity scores indicate issues with coordinate representation or latent input optimization; high denoising loss suggests problems with the diffusion schedule or network capacity
- **First 3 experiments**:
  1. Test coordinate generation with different eigenvector dimensions on a small molecule dataset to verify topological representation quality
  2. Validate the EM optimization by checking if the latent input zt converges and if the denoising loss decreases over training
  3. Compare sampling quality with different numbers of diffusion steps to find the optimal tradeoff between speed and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MING perform when generating 3D molecular conformations compared to 2D molecular graphs?
- Basis in paper: The paper mentions extending the framework to address broader molecule-related problems in the function space, such as molecule conformation and entire 3D physical structure generation.
- Why unresolved: The current MING model focuses on generating 2D molecular graphs and does not explicitly address 3D conformation generation. The paper only mentions this as a future direction without providing experimental results.

### Open Question 2
- Question: What is the impact of different activation functions on MING's performance for representing molecular signals on their topological space?
- Basis in paper: The paper presents an ablation study on non-linear activation functions, specifically comparing SINE and ReLU activations, showing that ReLU is incapable of modeling molecular signals effectively.
- Why unresolved: While the paper shows the impact of SINE vs ReLU, it does not explore other potential activation functions or provide a comprehensive analysis of why SINE works better for molecular representations.

### Open Question 3
- Question: How does MING's performance scale with increasingly large molecular datasets beyond MOSES?
- Basis in paper: The paper benchmarks MING on QM9, ZINC250k, and MOSES datasets, showing improved performance on larger datasets, but does not test scalability beyond MOSES.
- Why unresolved: While MING shows promise on current large datasets, its behavior on even larger molecular datasets with more complex structures and diverse chemical properties remains unknown.

## Limitations
- Graph spectral embedding implementation details are not fully specified, which could significantly impact reproducibility and performance
- The relationship between latent dimension size and model performance is not thoroughly explored, with potential overfitting risks
- Limited analysis of EM optimization convergence properties and empirical validation of the KL divergence convergence claim

## Confidence
- **High Confidence**: Claims about improved validity, uniqueness, and novelty metrics compared to baselines (supported by quantitative results in Tables 1-3)
- **Medium Confidence**: Claims about computational efficiency and parameter reduction (supported by comparisons but limited ablation studies on architecture choices)
- **Low Confidence**: Claims about the general applicability of functional representations to other domains beyond molecular generation (largely speculative and not empirically validated)

## Next Checks
1. **Coordinate system sensitivity analysis**: Systematically vary the eigenvector dimensions and normalization methods used for graph spectral embeddings, then measure the impact on molecule validity and uniqueness.
2. **Latent dimension ablation study**: Train MING models with varying latent input dimensions (e.g., 128, 256, 512, 1024) and measure performance on all metrics.
3. **Convergence analysis of EM optimization**: Track the KL divergence between posteriors, latent input reconstruction error, and denoising loss throughout training across multiple runs.