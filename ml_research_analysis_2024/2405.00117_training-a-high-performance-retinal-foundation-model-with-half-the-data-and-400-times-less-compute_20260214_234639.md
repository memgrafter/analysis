---
ver: rpa2
title: Training a high-performance retinal foundation model with half-the-data and
  400 times less compute
arxiv_id: '2405.00117'
source_url: https://arxiv.org/abs/2405.00117
tags:
- retfound-green
- images
- retinal
- deretfound
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces RETFound-Green, a retinal image foundation
  model trained using a novel Token Reconstruction objective that achieves comparable
  performance to previous models while using 50% less data, 400x less compute, and
  offering substantial efficiency gains in downstream applications. RETFound-Green
  matches or outperforms RETFound-MEH and DERETFound on 68 of 119 task comparisons
  across three datasets, requires only 75,000 training images versus 150,000-900,000
  for previous models, and can be trained for under $100 versus $10,000-$14,000.
---

# Training a high-performance retinal foundation model with half-the-data and 400 times less compute

## Quick Facts
- arXiv ID: 2405.00117
- Source URL: https://arxiv.org/abs/2405.00117
- Authors: Justin Engelmann; Miguel O. Bernabeu
- Reference count: 40
- Primary result: RETFound-Green achieves comparable performance to previous models using 50% less data, 400x less compute, and offers substantial efficiency gains

## Executive Summary
This paper introduces RETFound-Green, a retinal image foundation model that achieves state-of-the-art performance while dramatically reducing computational requirements and data needs. The model uses a novel Token Reconstruction objective that focuses on abstract feature reconstruction rather than pixel-level reconstruction, enabling effective pre-training with only 75,000 images compared to 150,000-900,000 for previous models. RETFound-Green matches or outperforms RETFound-MEH and DERETFound on 68 of 119 task comparisons across three datasets while being 14x smaller, 2.7x faster at computing embeddings, and requiring 2.6x less storage.

## Method Summary
RETFound-Green uses a novel Token Reconstruction self-supervised learning objective that reconstructs output tokens from a frozen reference model rather than pixel values. The model starts with pre-trained DinoV2 weights and adapts them to retinal imaging using a smaller vision transformer architecture ("small" version with four extra register tokens). Training uses 75,000 publicly available retinal color fundus images at 392x392 resolution, with a training procedure that includes AdamW optimizer, cosine learning rate scheduling, and mixed precision (bfloat16) for efficiency. The model is evaluated using linear probes on downstream classification tasks, demonstrating comparable performance to much larger models while requiring only 400x less compute and costing under $100 to train.

## Key Results
- RETFound-Green matches or outperforms RETFound-MEH and DERETFound on 68 of 119 task comparisons across three datasets
- Requires only 75,000 training images versus 150,000-900,000 for previous models
- Trained for under $100 versus $10,000-$14,000 for baseline models
- 14x smaller model size, computes embeddings 2.7x faster, requires 2.6x less storage for embeddings

## Why This Works (Mechanism)

### Mechanism 1
Token Reconstruction objective enables effective pre-training with far less data by focusing on abstract feature reconstruction rather than pixel-level reconstruction. Instead of reconstructing exact pixel values like MAE, the model learns to reconstruct output tokens from a frozen reference model given noisy inputs. This forces the model to understand abstract retinal image structure and dependencies while ignoring pixel-level noise. Core assumption: Abstract features are more important than exact pixel values for retinal image analysis, and pre-trained vision transformer weights already capture general image features that can be adapted.

### Mechanism 2
Using pre-trained weights from DinoV2 and only adapting them to retinal images drastically reduces computational requirements compared to training from scratch. By starting with weights already trained on natural images, the model only needs to learn domain-specific features of retinal images rather than general visual features from scratch. This allows using a smaller model architecture while maintaining performance. Core assumption: General visual features learned on natural images transfer effectively to retinal imaging domain, and domain adaptation is more efficient than learning from scratch.

### Mechanism 3
The smaller model size and higher resolution input (392x392 vs 224x224) provide better efficiency-performance tradeoff. Using a smaller vision transformer architecture allows processing higher resolution images which captures more detail, while being more computationally efficient than larger models at lower resolution. The Token Reconstruction objective works well with this architecture. Core assumption: Higher resolution input provides more useful information for retinal analysis than larger model capacity, and smaller models can process higher resolution more efficiently.

## Foundational Learning

- **Self-supervised learning and Masked Autoencoder (MAE) approach**: Foundation models need to learn useful representations from unlabeled data, and MAE has shown effectiveness in computer vision. Why needed here: Understanding how self-supervised learning differs from traditional supervised learning and why it's particularly useful for medical imaging with limited labeled data.

- **Vision transformer architecture and its differences from convolutional neural networks**: Understanding why vision transformers were chosen over traditional CNNs and how they process images differently. Why needed here: Grasping the key architectural differences between vision transformers and CNNs and how these differences impact their performance on retinal images.

- **Foundation models and their role in democratizing AI**: Understanding the broader context of why efficient foundation models matter for medical AI accessibility. Why needed here: Understanding how foundation models differ from traditional task-specific models and what advantages they provide for medical imaging applications.

## Architecture Onboarding

- **Component map**: Input preprocessing pipeline (resizing, normalization, noise addition) -> Vision transformer backbone (small variant with register tokens) -> Token Reconstruction objective module (frozen reference model, noise injection, loss computation) -> Training loop (AdamW optimizer, cosine learning rate scheduler, mixed precision) -> Downstream linear probe adapter (logistic regression on embeddings)

- **Critical path**: 1. Load pre-trained DinoV2 weights 2. Apply Token Reconstruction training objective for 120 epochs 3. Save optimized model weights (remove decoder and optimizer states) 4. For downstream tasks: generate embeddings and fit linear probe

- **Design tradeoffs**: Smaller model vs larger model: RETFound-Green uses smaller model but higher resolution input, trading model capacity for computational efficiency. Pixel-level vs token-level reconstruction: Token Reconstruction focuses on abstract features rather than exact pixel values, trading reconstruction fidelity for computational efficiency. Mixed precision training vs full precision: Uses bfloat16 for faster training with minimal accuracy loss.

- **Failure signatures**: Poor downstream performance despite good pre-training loss: Indicates Token Reconstruction objective may not capture relevant features. Training instability or NaN losses: Check learning rate, gradient clipping, and noise parameter ranges. Memory issues: Reduce batch size or switch to gradient accumulation if using high resolution inputs.

- **First 3 experiments**: 1. Verify basic training works: Train for 1 epoch with reduced dataset and check loss decreases 2. Test Token Reconstruction vs direct fine-tuning: Compare downstream performance of both approaches 3. Validate efficiency claims: Measure training time and inference speed compared to baseline models

## Open Questions the Paper Calls Out

- **Can the Token Reconstruction objective be scaled up to achieve even higher performance than RETFound-Green?**: The authors anticipate that their Token Reconstruction objective could be scaled up for even higher performance but did not attempt to scale up the approach due to limited resources. Training a larger version of RETFound-Green with more data and compute would resolve this question.

- **How would RETFound-Green perform on datasets from other geographic regions and populations?**: While results are strong, evaluations focused on Brazilian and Indian datasets, leaving generalizability to other populations unknown. Evaluating RETFound-Green on diverse datasets from different countries, ethnic groups, and healthcare systems would assess its performance across varied populations.

- **Can the Token Reconstruction objective be effectively applied to other medical imaging modalities beyond retinal images?**: While the approach is not specific to retinal images and likely could be applied to many different imaging domains, the authors did not investigate this. Applying the Token Reconstruction objective to train foundation models for other medical imaging types (e.g., CT, MRI, X-ray) would determine its effectiveness on other modalities.

## Limitations

- Evaluation focuses primarily on binary classification tasks with AU-ROC as the main metric, potentially missing nuanced performance differences in multi-class scenarios or tasks requiring localization.
- The 75,000 image dataset may not capture the full diversity of global retinal pathology patterns, particularly for rare diseases or populations underrepresented in the training data.
- Claims about environmental sustainability benefits are largely inferred rather than directly measured, depending on data center energy sources and training infrastructure details not provided.

## Confidence

- **High Confidence**: Claims about computational efficiency gains (400x less compute, $100 vs $10,000-$14,000 training cost, 14x smaller model) are well-supported by detailed specifications and verifiable through direct comparison of model parameters and training configurations.
- **Medium Confidence**: Claims about performance parity with larger models (68/119 task comparisons) are reasonably supported but limited by the evaluation framework. The comparison would be stronger with statistical significance testing across multiple runs and broader task coverage including regression tasks.
- **Low Confidence**: Claims about environmental sustainability benefits are largely inferred rather than directly measured. While reduced compute implies lower carbon footprint, actual environmental impact depends on data center energy sources and training infrastructure details not provided.

## Next Checks

1. **Ablation study on pre-training strategy**: Train RETFound-Green with different initialization strategies (random initialization, ImageNet pre-training, DinoV2 pre-training) to quantify the actual contribution of pre-trained weights to the efficiency gains.

2. **Multi-task and regression task evaluation**: Extend evaluation beyond binary classification to include multi-class disease classification, severity grading tasks, and quantitative biomarker prediction to assess model versatility and robustness.

3. **Cross-population generalization study**: Test model performance on retinal datasets from different geographic regions and demographic groups not represented in the training data to evaluate potential biases and real-world deployment readiness.