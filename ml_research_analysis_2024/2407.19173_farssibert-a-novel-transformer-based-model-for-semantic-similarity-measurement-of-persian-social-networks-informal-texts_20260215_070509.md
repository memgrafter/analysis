---
ver: rpa2
title: 'FarSSiBERT: A Novel Transformer-based Model for Semantic Similarity Measurement
  of Persian Social Networks Informal Texts'
arxiv_id: '2407.19173'
source_url: https://arxiv.org/abs/2407.19173
tags:
- persian
- similarity
- language
- texts
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of measuring semantic similarity
  between informal Persian short texts from social networks. The authors propose FarSSiBERT,
  a transformer-based model pre-trained on 104 million Persian tweets, and construct
  a novel dataset named FarSSiM for evaluating semantic similarity in informal texts.
---

# FarSSiBERT: A Novel Transformer-based Model for Semantic Similarity Measurement of Persian Social Networks Informal Texts

## Quick Facts
- arXiv ID: 2407.19173
- Source URL: https://arxiv.org/abs/2407.19173
- Reference count: 40
- Outperforms ParsBERT, laBSE, and multilingual BERT on Persian semantic similarity tasks

## Executive Summary
This paper introduces FarSSiBERT, a transformer-based model specifically designed to measure semantic similarity between informal Persian texts from social networks. The model addresses a critical gap in existing Persian language models, which are primarily trained on formal text and struggle with the colloquial, abbreviated, and context-dependent nature of social media communication. FarSSiBERT achieves state-of-the-art performance on both formal and informal Persian semantic similarity datasets through pre-training on 104 million Persian tweets and employing a specialized tokenizer trained on informal language patterns.

## Method Summary
FarSSiBERT employs a BERT-base architecture (12 layers, 12 attention heads, 768 hidden size) pre-trained using Masked Language Modeling (MLM) on approximately 104 million Persian tweets from Twitter spanning 2019-2023. The model uses a custom tokenizer with 100K vocabulary items specifically trained on informal Persian text patterns, enabling better handling of colloquial expressions, abbreviations, and slang. For downstream semantic similarity tasks, the model extracts sentence embeddings by averaging the last four layers and computes cosine similarity between pairs. The pre-training process involves 5 epochs with batch size 32, sequence length 512, and learning rate 1e-12 using the Adam optimizer.

## Key Results
- Achieved Pearson correlation of 0.770 and Spearman correlation of 0.643 on FarSSiM informal dataset
- Outperformed ParsBERT (0.616 Pearson, 0.583 Spearman) and other baselines on informal Persian text
- Demonstrated superior performance on FarSick formal dataset with Pearson correlation of 0.770
- Successfully handled colloquial Persian expressions and informal abbreviations missed by standard tokenizers

## Why This Works (Mechanism)

### Mechanism 1
Pre-training on informal social media text significantly improves performance on colloquial Persian similarity tasks. The model learns linguistic patterns, abbreviations, and informal expressions specific to Persian social media that are absent from formal text corpora. Core assumption: Informal text contains distinct vocabulary, grammar, and semantic structures requiring specialized training data. Evidence: Pre-training on 104 million Persian tweets from social networks. Break condition: If formal text contains sufficient coverage of informal patterns.

### Mechanism 2
Specialized tokenizer effectively handles informal Persian tokens missed by standard tokenizers. Custom tokenizer trained on social media data learns to recognize informal words, abbreviations, and slang as single tokens rather than splitting them. Core assumption: Standard tokenizers fail on informal Persian text because they were trained on formal corpora. Evidence: Custom tokenizer with 100K vocabulary specifically designed for informal Persian short texts. Break condition: If informal tokens are rare or standard tokenizers can be fine-tuned adequately.

### Mechanism 3
Bidirectional transformer architecture enables better understanding of context-dependent meanings in informal text. BERT processes text bidirectionally, capturing context-dependent meanings of words that change based on surrounding text. Core assumption: Informal text relies heavily on context for meaning, more so than formal text. Evidence: BERT's left-to-right and right-to-left text processing enables understanding of semantic similarities between words in context. Break condition: If context-independent features suffice for similarity measurement.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)**
  - Why needed here: MLM allows the model to learn word representations based on context without requiring labeled data
  - Quick check question: Why was Next Sentence Prediction (NSP) not used in this model's pre-training?

- **Concept: Subword tokenization (WordPiece algorithm)**
  - Why needed here: Handles out-of-vocabulary words and informal expressions common in social media text
  - Quick check question: How does the WordPiece algorithm balance between character-level and word-level tokenization?

- **Concept: Semantic similarity measurement metrics**
  - Why needed here: Pearson and Spearman coefficients provide standardized evaluation of similarity model performance
  - Quick check question: What is the key difference between Pearson and Spearman correlation in the context of similarity measurement?

## Architecture Onboarding

- **Component map**: Raw text → Pre-processing → Tokenization → BERT encoding → Embedding extraction → Similarity calculation
- **Critical path**: Pre-processed Persian social media text flows through custom tokenizer → BERT-base encoder → Last 4 layer averaging → Cosine similarity computation
- **Design tradeoffs**: Informal vs formal pre-training data (informal improves colloquial understanding but may reduce formal task performance); Tokenizer size (100K balances vocabulary coverage and computational cost); Layer averaging (balances shallow and deep representations)
- **Failure signatures**: Poor performance on formal text tasks; Inability to generalize to new informal expressions; Overfitting to specific social media patterns
- **First 3 experiments**:
  1. Compare FarSSiBERT performance on FarSick (formal) vs FarSSiM (informal) datasets
  2. Ablation study: Replace custom tokenizer with ParsBERT tokenizer and measure performance impact
  3. Test model on cross-lingual informal text to evaluate domain generalization

## Open Questions the Paper Calls Out

### Open Question 1
How does FarSSiBERT perform on longer informal Persian texts beyond short tweets, such as forum discussions or longer social media posts? The paper evaluates FarSSiBERT on short text pairs (tweets) but does not test performance on longer informal texts that maintain colloquial characteristics.

### Open Question 2
What is the relative contribution of the specialized tokenizer versus the informal text training data to FarSSiBERT's improved performance? The authors note that FarSSiBERT includes "a novel specialized informal language tokenizer" and was pre-trained on informal social media data, but do not isolate which factor contributes more to performance gains.

### Open Question 3
How well does FarSSiBERT generalize to other informal Persian text domains beyond Twitter, such as messaging apps, comments, or regional dialects? The model is trained exclusively on Twitter data from 2019-2023, but Persian colloquial language varies significantly across different platforms and regions.

### Open Question 4
Does the inclusion of NSP (Next Sentence Prediction) training improve FarSSiBERT's performance on semantic similarity tasks for informal texts? The authors deliberately excluded NSP from training, stating "Informal data do not follow a specific structure for sentences."

## Limitations

- Data representativeness uncertainty due to reliance on Twitter data from 2019-2023, potentially missing broader Persian informal communication patterns
- Limited annotation quality information with unclear inter-annotator agreement and guidelines for the FarSSiM dataset
- Significant performance degradation on formal text tasks indicates potential over-specialization to informal patterns

## Confidence

**Claim Cluster 1: FarSSiBERT outperforms baseline models on semantic similarity tasks** (High confidence): Experimental results show consistent improvement over ParsBERT, laBSE, and multilingual BERT across both datasets with statistically significant correlation scores.

**Claim Cluster 2: Pre-training on informal social media text is essential for measuring colloquial Persian similarity** (Medium confidence): Results support this claim, but lack ablation studies comparing against formal text pre-training to isolate the specific contribution.

**Claim Cluster 3: The custom tokenizer significantly improves informal text handling** (Low confidence): Paper provides tokenizer examples but lacks quantitative analysis of tokenizer impact through ablation studies.

## Next Checks

**Validation Check 1: Cross-domain performance evaluation**: Test FarSSiBERT on diverse Persian text domains including news articles, academic papers, and literature to quantify the trade-off between informal and formal text performance.

**Validation Check 2: Tokenizer contribution isolation**: Create controlled experiment replacing FarSSiBERT's custom tokenizer with ParsBERT tokenizer while keeping all other components identical, then measure performance differences on FarSSiM dataset.

**Validation Check 3: Temporal robustness assessment**: Evaluate FarSSiBERT on Persian tweets from different time periods (pre-2019 and post-2023) to assess how well the model handles evolving informal language patterns and changing communication styles over time.