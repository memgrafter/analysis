---
ver: rpa2
title: Adversarial Multi-dueling Bandits
arxiv_id: '2406.12475'
source_url: https://arxiv.org/abs/2406.12475
tags:
- bandits
- lemma
- adversarial
- multi-dueling
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces adversarial multi-dueling bandits, a setting
  where the learner selects multiple arms per round and receives only the identity
  of the most preferred arm based on an arbitrary preference matrix chosen obliviously.
  The paper proposes MiDEX, a novel algorithm that learns from preference feedback
  under a pairwise-subset choice model.
---

# Adversarial Multi-dueling Bandits

## Quick Facts
- arXiv ID: 2406.12475
- Source URL: https://arxiv.org/abs/2406.12475
- Authors: Pratik Gajane
- Reference count: 40
- One-line primary result: Introduces MiDEX algorithm for adversarial multi-dueling bandits with regret bound O((K log K)^(1/3) T^(2/3)) and matching lower bound Ω(K^(1/3) T^(2/3))

## Executive Summary
This paper introduces adversarial multi-dueling bandits, where a learner selects multiple arms per round and receives only the identity of the most preferred arm based on an arbitrary preference matrix. The proposed MiDEX algorithm learns from preference feedback under a pairwise-subset choice model, extending exponential weights to handle dueling feedback. The key theoretical results include an upper bound on expected cumulative regret of O((K log K)^(1/3) T^(2/3)) compared to a Borda winner, along with a matching lower bound of Ω(K^(1/3) T^(2/3)), demonstrating near-optimal performance.

## Method Summary
MiDEX extends the EXP3 algorithm to handle multi-dueling bandit feedback by maintaining non-zero selection probabilities for all arms through an exploration parameter γ. The algorithm samples two arms, constructs a multiset with replicated arms, receives winner feedback, and estimates pairwise preferences using a transformation function g(m, ot, xt). These unbiased estimates are then used to update arm weights multiplicatively. The careful balance between exploration and exploitation through tuned parameters γ and learning rate η enables the algorithm to achieve the near-optimal regret scaling.

## Key Results
- MiDEX achieves expected cumulative regret upper bounded by O((K log K)^(1/3) T^(2/3))
- A matching lower bound of Ω(K^(1/3) T^(2/3)) is proven, demonstrating near-optimality
- The algorithm maintains non-zero selection probability for all arms, ensuring continuous exploration
- Unbiased score estimation is achieved through the pairwise-subset choice model transformation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MiDEX's ability to maintain non-zero selection probability for all arms via parameter γ ensures continuous exploration and avoids premature convergence.
- Mechanism: The algorithm explicitly sets qt(i) ≥ γ/K for all arms i at every round, guaranteeing that each arm retains a minimum selection probability. This is enforced by the multiplicative weight update combined with a uniform exploration term γ/K.
- Core assumption: The exploration parameter γ can be tuned as a function of the learning rate η and problem size K to balance exploration and exploitation optimally.
- Evidence anchors:
  - [section] "A parameter γ∈ (0, 1] is incorporated to ensure that for all t∈ [T ] and i∈ [K], qt(i)≥ γ/K which translates to the selection probability of any arm always being above zero."
  - [section] "Let γ = √3ηK/2. Then, for any t ∈ [T ], i∈ [K] and η > 0, it holds that ηˆst(i)∈ [0, 1]."

### Mechanism 2
- Claim: The pairwise-subset choice model allows the algorithm to extract meaningful preference information from winner feedback even when m > 2 arms are selected.
- Mechanism: By replicating arms in the multiset At and using the transformation function g(m, ot, xt), MiDEX converts winner feedback into an unbiased estimate of the pairwise preference Pt(xt, yt) between two sampled arms.
- Core assumption: The underlying preference matrices Pt satisfy Pt(i, j) = 1 − Pt(j, i) and Pt(i, i) = 1/2, ensuring valid probability distributions over pairwise comparisons.
- Evidence anchors:
  - [section] "The probability of any index i∈ [m] being selected as the winner is defined as W (i|A, P ) := m∑j=1,j̸=i 2P (A(i),A(j)) m(m− 1) ."
  - [section] "Lemma 1 proves how the transformed feedback can be interpreted as the probability of xt winning the duel against yt."

### Mechanism 3
- Claim: MiDEX achieves near-optimal regret scaling of O((K log K)^(1/3) T^(2/3)) by carefully balancing the exploration-exploitation tradeoff through the learning rate η and γ.
- Mechanism: The algorithm uses a carefully chosen learning rate η = (2 log K / (T √(K m')))^(2/3) and exploration parameter γ = √3ηK/2 to optimize the upper bound on expected regret, derived from the standard EXP3 analysis adapted to the dueling setting.
- Core assumption: The choice of m' = (√3/2 + √(2/3) (3m+1)^2 / (4(m+1)^2)) is optimal for balancing the variance of score estimates and the convergence rate of the exponential weights.
- Evidence anchors:
  - [abstract] "Our analysis demonstrates that the expected cumulative regret of MiDEX is upper bounded by O((K log K)1/3 T 2/3)."
  - [section] "Theorem 1. Let γ = √3ηK/2 and η = (2 log K / T √(K m'))^(2/3) where m' = (√3/2 + √(2/3) (3m+1)^2 / (4(m+1)^2)). For any T , K ≥ 2 and m≥ 2, the expected regret of MiDEX satisfies E[RT ]≤ 3.78 (m')^(2/3) (K log K)^(1/3) T^(2/3)."

## Foundational Learning

- Concept: Pairwise-subset choice model in dueling bandits
  - Why needed here: MiDEX relies on this model to interpret winner feedback from multi-dueling rounds as pairwise preferences, which is essential for unbiased score estimation.
  - Quick check question: If m=2, does the pairwise-subset choice model reduce to the standard dueling bandit feedback?

- Concept: Exponential weights (EXP3) algorithm and its regret guarantees
  - Why needed here: MiDEX extends the EXP3 framework to handle dueling feedback by estimating pairwise preferences and updating arm weights accordingly.
  - Quick check question: What is the role of the learning rate η in the EXP3 algorithm, and how does it affect the regret bound?

- Concept: Borda winner and regret minimization in adversarial settings
  - Why needed here: The algorithm's performance is measured against the Borda winner, a robust notion of optimality in adversarial preference settings.
  - Quick check question: How does the Borda winner differ from the Condorcet winner, and why is it preferred in adversarial contexts?

## Architecture Onboarding

- Component map:
  - Arm sampling -> Multiset construction -> Winner feedback processing -> Score estimation -> Weight update

- Critical path:
  - Sampling → Multiset construction → Winner feedback → Score estimation → Weight update.
  - The score estimation step is critical, as it directly affects the unbiasedness of the update.

- Design tradeoffs:
  - Exploration vs. exploitation: The parameter γ controls the minimum probability of selecting any arm, trading off between exploring all arms and focusing on promising ones.
  - Variance vs. bias: The transformation function g(m, ot, xt) trades off variance in score estimates for unbiasedness.
  - Computational complexity: Replicating arms and processing winner feedback increases computational overhead compared to standard EXP3.

- Failure signatures:
  - If γ is set too low, the algorithm may fail to explore all arms, leading to suboptimal regret.
  - If the pairwise-subset model is violated, score estimates become biased, and regret bounds no longer hold.
  - If the learning rate η is not properly tuned, the algorithm may converge too slowly or too quickly.

- First 3 experiments:
  1. Verify unbiasedness of score estimates: Run MiDEX on a synthetic problem with known preference matrices and check if E[ˆst(i)] ≈ st(i) for all arms i.
  2. Test exploration guarantee: Confirm that qt(i) ≥ γ/K for all arms i and rounds t.
  3. Evaluate regret scaling: Measure the empirical regret RT on a range of problem sizes (K, T, m) and check if it matches the theoretical O((K log K)^(1/3) T^(2/3)) scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the regret bounds change if a different choice model (e.g., Random Utility Models) were used instead of the pairwise-subset choice model?
- Basis in paper: [inferred] The paper assumes a pairwise-subset choice model but mentions that other models like Random Utility Models exist in the literature.
- Why unresolved: The paper does not explore or analyze the performance of the algorithm under alternative choice models, leaving the impact on regret bounds unknown.
- What evidence would resolve it: Implementing and analyzing the algorithm under different choice models and comparing the resulting regret bounds would provide evidence.

### Open Question 2
- Question: Can the algorithm be extended to handle time-varying preferences that are not necessarily adversarial (e.g., stochastic with non-stationary drift)?
- Basis in paper: [inferred] The paper focuses on adversarial preferences but mentions that real-world applications may have non-stationary preferences, suggesting potential for extension.
- Why unresolved: The paper does not explore or analyze the algorithm's performance under non-adversarial non-stationary preferences, leaving the extension's feasibility unknown.
- What evidence would resolve it: Extending the algorithm to handle non-adversarial non-stationary preferences and analyzing its performance under such conditions would provide evidence.

### Open Question 3
- Question: How does the algorithm's performance compare to other multi-dueling bandit algorithms in terms of regret bounds and computational efficiency?
- Basis in paper: [inferred] The paper introduces a novel algorithm and provides regret bounds, but does not compare it to existing multi-dueling bandit algorithms.
- Why unresolved: The paper does not include a comparative analysis with other algorithms, leaving the relative performance and efficiency unknown.
- What evidence would resolve it: Conducting a comparative analysis of the algorithm's regret bounds and computational efficiency against other multi-dueling bandit algorithms would provide evidence.

## Limitations
- The pairwise-subset choice model assumption may not hold in all practical scenarios where feedback is generated.
- Regret bounds rely on optimal tuning of exploration parameter γ and learning rate η, which may be challenging without prior knowledge.
- Computational overhead increases with subset size m due to arm replication and winner feedback processing.

## Confidence

- Mechanism 1 (Exploration guarantee): High confidence - the exploration mechanism is well-defined and theoretically justified with explicit bounds on γ.
- Mechanism 2 (Unbiased score estimation): High confidence - the transformation function g(m, ot, xt) and its unbiasedness are rigorously proven in the supplementary material.
- Mechanism 3 (Regret bound): Medium confidence - while the theoretical bound is derived and a matching lower bound is proven, practical experiments are needed to verify the O((K log K)^(1/3) T^(2/3)) scaling in real-world scenarios.

## Next Checks

1. Empirical verification of unbiasedness: Run MiDEX on synthetic problems with known preference matrices and measure the bias in score estimates to confirm that E[ˆst(i)] ≈ st(i) holds empirically.
2. Exploration guarantee validation: Implement MiDEX and monitor the minimum selection probability qt(i) across all arms and rounds to ensure it never falls below γ/K.
3. Regret scaling evaluation: Conduct experiments across a range of problem sizes (K, T, m) and measure the empirical regret RT to verify if it matches the theoretical O((K log K)^(1/3) T^(2/3)) scaling.