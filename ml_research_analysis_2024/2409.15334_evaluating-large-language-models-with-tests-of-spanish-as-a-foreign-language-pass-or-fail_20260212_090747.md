---
ver: rpa2
title: 'Evaluating Large Language Models with Tests of Spanish as a Foreign Language:
  Pass or Fail?'
arxiv_id: '2409.15334'
source_url: https://arxiv.org/abs/2409.15334
tags:
- spanish
- llms
- language
- test
- teleia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates Large Language Models (LLMs) on their understanding
  of Spanish using TELEIA, a benchmark designed with questions similar to those in
  Spanish exams for foreign students (PCE, SIELE, and Cervantes AVE). The study assesses
  eight LLMs on tasks covering reading comprehension, word formation, semantics, and
  grammar.
---

# Evaluating Large Language Models with Tests of Spanish as a Foreign Language: Pass or Fail?

## Quick Facts
- arXiv ID: 2409.15334
- Source URL: https://arxiv.org/abs/2409.15334
- Reference count: 5
- Key outcome: LLMs perform well on reading comprehension but struggle significantly with Spanish grammar, morphology, and semantics, showing errors similar to English-speaking learners.

## Executive Summary
This study evaluates eight Large Language Models on their understanding of Spanish using TELEIA, a benchmark designed with questions similar to those in Spanish exams for foreign students (PCE, SIELE, and Cervantes AVE). The research assesses LLMs across tasks covering reading comprehension, word formation, semantics, and grammar. Results reveal that while LLMs demonstrate strong performance in reading comprehension, their abilities in grammatical competence, particularly morphology and semantics, are notably weaker. No model achieves native-like proficiency, and error patterns often mirror those made by English-speaking learners of Spanish. The study highlights the need for improved LLMs in handling language-specific nuances and grammar.

## Method Summary
The study evaluates eight LLMs (GPT-3.5, GPT-4, Llama-3-8B, Yi-6B, Gemma-7b, Mistral-7B, Occiglot-7b, Llama-2-7b) on TELEIA, a 100-question benchmark covering Spanish language exams (PCE, SIELE, Cervantes AVE). Automated scripts run questions through each model via API or local inference, logging answers for manual analysis. Accuracy rates are calculated for each test part and compared to native speaker performance.

## Key Results
- LLMs perform well on reading comprehension tasks but struggle with grammatical competence, particularly morphology and semantics.
- No model achieves native-like proficiency in Spanish language understanding.
- Errors made by LLMs often mirror those of English-speaking learners of Spanish, suggesting negative language transfer from English-dominated training data.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs show strong performance on reading comprehension tasks because these tasks rely on pattern recognition and information extraction, which are transferable skills from English training data.
- **Mechanism**: Reading comprehension questions often involve locating explicit information in the text, matching it to answer choices, and applying basic inference—skills that LLMs can acquire from large multilingual corpora without needing deep grammatical knowledge.
- **Core assumption**: The knowledge required for reading comprehension is largely language-agnostic and can be learned from exposure to diverse text, even if the training data is predominantly English.
- **Evidence anchors**:
  - [abstract] "Results show that while LLMs perform well in reading comprehension..."
  - [section] "The knowledge acquired might be transferable to other languages, unlike specifics about Spanish grammar."
  - [corpus] Weak evidence; no direct citations on cross-linguistic transferability in the corpus.
- **Break condition**: If the reading comprehension task requires deep cultural context or idiomatic understanding not present in the training data, performance would degrade significantly.

### Mechanism 2
- **Claim**: LLMs make grammatical errors in Spanish that mirror those of English-speaking learners due to negative language transfer from English.
- **Mechanism**: LLMs trained predominantly on English data develop patterns and associations that do not align with Spanish grammar rules. When generating or selecting Spanish text, these ingrained patterns lead to errors like incorrect verb conjugations, preposition usage, and word formation processes.
- **Core assumption**: The training data distribution heavily favors English, leading to English grammatical patterns being internalized and applied to Spanish tasks.
- **Evidence anchors**:
  - [abstract] "...errors often mirror those made by English-speaking learners of Spanish."
  - [section] "This is consistent with observations from recent works on the multilingual capabilities of LLMs (Terryn 2024; Wendler2024) and the fact that English dominates the training datasets of most LLMs accounting in most cases for more than 90% of the text used for training."
  - [corpus] Moderate evidence; related papers discuss English bias in multilingual models.
- **Break condition**: If training data included a more balanced representation of Spanish text, the negative transfer effect would diminish, and grammatical errors would align more closely with native Spanish patterns.

### Mechanism 3
- **Claim**: LLMs struggle with Spanish morphology and neologisms because these require language-specific knowledge not captured in the training data.
- **Mechanism**: Morphological processes (e.g., suffixation, prefixation) and the creation of new words (neologisms) are highly language-specific and often not explicitly represented in the training data. LLMs lack the ability to generalize these rules beyond the examples seen during training.
- **Core assumption**: The training data does not contain sufficient examples of Spanish morphological rules or neologisms for the model to learn these patterns effectively.
- **Evidence anchors**:
  - [abstract] "their performance drops significantly in grammatical competence, particularly in morphology and semantics."
  - [section] "However, all models fail —and also failed in the original test—a question about the neologisms 'cafedemia' (café + pandemia ~ coffee + pandemic) and 'cafexicación' (café + intoxicación ~ coffee + intoxication), clearly indicating that models are currently incapable of processing this type of data that humans can naturally identify."
  - [corpus] Weak evidence; no direct citations on morphological processing in the corpus.
- **Break condition**: If the training data included a comprehensive set of Spanish morphological examples and neologisms, the model's ability to process these would improve significantly.

## Foundational Learning

- **Concept**: Negative language transfer
  - **Why needed here**: Understanding how the dominance of English in training data leads to Spanish grammatical errors is crucial for interpreting the results and identifying areas for improvement.
  - **Quick check question**: If an LLM is trained primarily on English data, what kind of errors would you expect it to make when processing Spanish grammar?
- **Concept**: Morphology and word formation
  - **Why needed here**: Recognizing that LLMs struggle with Spanish morphology and neologisms highlights the need for language-specific training data and evaluation.
  - **Quick check question**: Why are neologisms particularly challenging for LLMs, and how does this relate to their training data?
- **Concept**: Reading comprehension vs. grammar
  - **Why needed here**: Distinguishing between the skills required for reading comprehension and grammar helps explain why LLMs perform better on one than the other.
  - **Quick check question**: What makes reading comprehension more transferable across languages than grammar?

## Architecture Onboarding

- **Component map**: Spanish text questions (TELEIA benchmark) -> LLM (various models) -> Predicted answer choice -> Accuracy rate evaluation
- **Critical path**: 
  1. Load TELEIA questions
  2. Send questions to LLM via API or local inference
  3. Receive and parse LLM responses
  4. Compare responses to correct answers
  5. Calculate accuracy rates
- **Design tradeoffs**:
  - Using multiple LLM models provides a broad assessment but increases computational cost.
  - Focusing on Spanish-specific tasks may limit generalizability to other languages.
  - Manual analysis of errors provides deep insights but is time-consuming.
- **Failure signatures**:
  - Consistently low accuracy on grammar tasks indicates insufficient language-specific training.
  - Errors mirroring English learner mistakes suggest negative language transfer.
  - Inability to process neologisms points to a lack of exposure to language-specific morphological rules.
- **First 3 experiments**:
  1. Evaluate LLMs on a subset of TELEIA focusing solely on reading comprehension to confirm transferability of skills.
  2. Test LLMs on Spanish morphology tasks with explicit training on Spanish-specific data to assess improvement.
  3. Compare LLM performance on Spanish neologisms before and after fine-tuning on a corpus of Spanish word formation examples.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do linguistic biases in training data (e.g., dominance of English) influence the types of errors LLMs make in Spanish language tasks?
  - **Basis in paper**: [explicit] The paper notes that errors made by LLMs are similar to those of English-speaking learners of Spanish and attributes this to the dominance of English in LLM training datasets.
  - **Why unresolved**: While the paper identifies the correlation between training data bias and error patterns, it does not quantify the extent of this influence or test LLMs trained on more balanced multilingual datasets.
  - **What evidence would resolve it**: Comparative analysis of LLMs trained on different proportions of Spanish vs. English data, showing error rates and types across language-specific tasks.

- **Open Question 2**: What are the limitations of current evaluation benchmarks like TELEIA in capturing the full range of linguistic competencies needed for native-like Spanish proficiency?
  - **Basis in paper**: [explicit] The paper acknowledges that no model achieves native-like proficiency and highlights gaps in handling morphology, semantics, and grammar, but does not explore the adequacy of the benchmark itself.
  - **Why unresolved**: The study focuses on model performance but does not critically assess whether the benchmark questions fully represent the complexities of native-level Spanish usage.
  - **What evidence would resolve it**: Analysis of benchmark coverage against native speaker linguistic competence, including underrepresented areas like pragmatics, idiomatic expressions, or regional dialects.

- **Open Question 3**: How do different fine-tuning strategies (e.g., instruction tuning vs. task-specific tuning) impact the performance of LLMs on language-specific tasks like Spanish grammar and semantics?
  - **Basis in paper**: [inferred] The paper evaluates instruction-tuned models but does not compare their performance with models fine-tuned on language-specific datasets or tasks.
  - **Why unresolved**: The study uses instruction-tuned models but does not explore whether task-specific fine-tuning could improve performance in areas like morphology and grammar.
  - **What evidence would resolve it**: Experimental comparison of instruction-tuned models versus models fine-tuned on Spanish-specific grammatical and semantic tasks, measuring improvements in error rates and accuracy.

## Limitations

- The study does not provide detailed information about prompt templates and evaluation parameters, which could influence results.
- The small sample size of 100 questions may not capture the full complexity of Spanish language understanding.
- The paper does not explore whether task-specific fine-tuning could improve performance in areas like morphology and grammar.

## Confidence

- **High confidence**: LLMs perform significantly better on reading comprehension than grammar tasks; the performance gap between tasks is real and substantial.
- **Medium confidence**: Error patterns mirror those of English-speaking learners of Spanish; while the observation is reasonable given the English-dominated training data, direct evidence of negative transfer could be stronger.
- **Medium confidence**: LLMs cannot process Spanish neologisms effectively; this conclusion is based on single-question evidence and could benefit from more comprehensive testing of morphological processing.

## Next Checks

1. Replicate the study using standardized prompt templates and documented evaluation parameters across all models to verify the reported accuracy differences between reading comprehension and grammar tasks.
2. Conduct a controlled experiment comparing LLM performance on Spanish tasks before and after fine-tuning on balanced Spanish-English corpora to isolate the effect of English language dominance.
3. Expand testing to include a dedicated morphology and word formation benchmark with multiple neologism examples to better assess LLMs' capabilities in this specific area.