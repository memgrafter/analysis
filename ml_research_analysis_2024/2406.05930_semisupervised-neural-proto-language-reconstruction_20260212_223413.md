---
ver: rpa2
title: Semisupervised Neural Proto-Language Reconstruction
arxiv_id: '2406.05930'
source_url: https://arxiv.org/abs/2406.05930
tags:
- uni032f
- m-bst
- ours
- reconstruction
- uni02b0
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces semisupervised comparative reconstruction,
  where models are trained on a small labeled set of cognate sets with protoforms
  and a larger unlabeled set without protoforms. To address this task, the authors
  propose DPD-BiReconstructor, which incorporates the linguist's comparative method
  by learning to reconstruct reflexes from its own reconstructions.
---

# Semisupervised Neural Proto-Language Reconstruction

## Quick Facts
- **arXiv ID**: 2406.05930
- **Source URL**: https://arxiv.org/abs/2406.05930
- **Reference count**: 40
- **Primary result**: DPD-BiReconstructor improves proto-language reconstruction in semisupervised settings by learning bidirectional sound changes, achieving performance close to fully supervised models even with minimal labeled data.

## Executive Summary
This paper introduces semisupervised comparative reconstruction for neural proto-language reconstruction, where models are trained on a small labeled set of cognate sets with protoforms and a larger unlabeled set without protoforms. The authors propose DPD-BiReconstructor, which incorporates the linguist's comparative method by learning to reconstruct reflexes from its own reconstructions. This approach outperforms strong semisupervised baselines, especially when labeled data is scarce. Experiments show that DPD-based strategies generalize well across different labeling settings and achieve performance close to fully supervised models. Analysis reveals that DPD improves phoneme representations and error handling, demonstrating its effectiveness in semisupervised proto-language reconstruction.

## Method Summary
The paper addresses semisupervised comparative reconstruction by proposing DPD-BiReconstructor, which uses a reconstruction sub-network (D2P) and a reflex-prediction sub-network (P2D) with shared phoneme embeddings. The P2D sub-network is trained to reconstruct reflexes from the D2P's intermediate representation of protoforms, creating a bridge between labeled and unlabeled data. Errors in P2D are backpropagated to D2P, guiding it to produce protoforms that can deterministically yield correct reflexes. The model is evaluated on WikiHan (8 Sinitic languages) and Rom-phon (5 Romance languages) datasets with varying labeling percentages (5%, 10%, 20%, 30%, 100%).

## Key Results
- DPD-BiReconstructor outperforms strong semisupervised baselines, especially when labeled data is scarce (e.g., doubling accuracy at 5% labeling)
- At 30% labeling, some strategies achieve accuracy close to fully supervised models
- DPD-based strategies generalize well across different labeling settings and datasets
- The approach captures linguistically meaningful phoneme representations that improve reconstruction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPD improves protoform reconstruction by forcing the model to learn bidirectional sound changes, enabling use of unlabeled cognate sets.
- Mechanism: DPD architecture contains two sub-networks—D2P (daughter-to-proto) and P2D (proto-to-daughter). The P2D sub-network is trained to reconstruct reflexes from the D2P's intermediate representation of protoforms, creating a bridge between labeled and unlabeled data. Errors in P2D are backpropagated to D2P, guiding it to produce protoforms that can deterministically yield correct reflexes.
- Core assumption: Sound changes are regular and reversible enough for a model to learn both directions simultaneously.
- Evidence anchors:
  - [abstract]: "DPD-BiReconstructor...incorporating an essential insight from linguists’ comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words."
  - [section 2.1]: "Our model comprises a reconstruction sub-network (D2P for daughter-to-protoform) and a reflex-prediction sub-network (P2D for protoform-to-daughter) with shared phoneme embeddings."

### Mechanism 2
- Claim: DPD captures linguistically meaningful phoneme representations that improve reconstruction accuracy.
- Mechanism: The shared phoneme embeddings and bidirectional training encourage DPD to learn hierarchical phoneme clusters that align with linguistic features (e.g., vowels vs. consonants, tone groupings). These embeddings are refined through the constraint of predicting correct reflexes from reconstructions, which implicitly requires distinguishing phonemes with minimal feature differences.
- Core assumption: Better phoneme representations will lead to better protoform reconstruction because reconstruction depends on accurate phonetic distinctions.
- Evidence anchors:
  - [section 4.3]: "DPD-based strategies are better at capturing linguistically meaningful representations of phonemes... DPDBased strategies need good phonetic representations to perform well on multiple phonology-intensive tasks."
  - [section 4.1]: "Reflexes are more accurately predicted from correct protoform reconstructions... CRINGE loss appears to lead to slightly lower reflex accuracy given incorrect reconstruction at earlier epochs."

### Mechanism 3
- Claim: DPD generalizes effectively to low labeling settings and even benefits supervised training.
- Mechanism: The reflexive prediction sub-network can learn from unlabeled cognate sets by inferring plausible protoforms and checking if they generate correct reflexes. This weak supervision substitutes for labeled data. Additionally, when trained on fully labeled data, the reflex prediction task regularizes the model, reducing overfitting and improving accuracy.
- Core assumption: Unlabeled cognate sets contain sufficient information to infer plausible protoforms when constrained by reflex prediction.
- Evidence anchors:
  - [section 3]: "At a 5% labeling setting, for example, GRU-DPD-ΠM-BST almost doubles the accuracy of GRU-SUPV on WikiHan... At a 30% labeling setting, some strategies attain accuracy close to existing fully supervised reconstruction models."
  - [section 4.5]: "ΠM, DPD, and DPD-ΠM can often outperform SUPV... On average, Trans-DPD-ΠM performs the best on WikiHan for all metrics."

## Foundational Learning

- Concept: Bidirectional sequence transduction (D2P and P2D)
  - Why needed here: Reconstruction requires learning both directions of sound change; DPD explicitly models this.
  - Quick check question: What is the purpose of having both D2P and P2D sub-networks in the DPD architecture?

- Concept: Bridge network for latent representation propagation
  - Why needed here: Token predictions are not differentiable, so the bridge network connects D2P decoder outputs to P2D encoder inputs in a continuous latent space.
  - Quick check question: Why can't we directly feed token predictions from D2P into P2D?

- Concept: CRINGE loss for negative example regularization
  - Why needed here: Prevents P2D from producing correct reflexes given incorrect protoforms, which would mislead D2P training.
  - Quick check question: How does CRINGE loss discourage the model from learning incorrect mappings?

## Architecture Onboarding

- Component map: D2P encoder-decoder → Bridge network (dense layer) → P2D encoder-decoder; shared phoneme embeddings; optional Π-model augmentation
- Critical path: D2P generates protoform → Bridge encodes latent representation → P2D reconstructs reflexes → Loss backpropagated to D2P
- Design tradeoffs: Bidirectional training improves unlabeled data usage but adds complexity and compute; shared embeddings encourage phoneme-level consistency but may limit language-specific flexibility
- Failure signatures: Poor reflex prediction accuracy → degraded protoform reconstruction; high variance across dataset seeds → instability in unlabeled data usage; degraded performance when unlabeled data is removed → reliance on weak supervision
- First 3 experiments:
  1. Train DPD on a small labeled set with no unlabeled data; verify it matches baseline supervised performance
  2. Add unlabeled cognate sets; check if reflex prediction accuracy improves and if protoform accuracy increases
  3. Remove unlabeled data again; confirm that performance drops, demonstrating dependence on weak supervision

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms allow DPD to leverage unlabeled cognate sets to improve reconstruction performance, beyond simply learning better phonetic representations?
- Basis in paper: [explicit] The authors state that DPD learns to recover reflexes from its own reconstructions, utilizing unlabeled cognate sets, and that the reflex prediction sub-network acts as weak supervision. However, they acknowledge uncertainty about what specific sound changes DPD learns from unlabeled data that enable better reconstruction.
- Why unresolved: The paper demonstrates that DPD improves performance and captures linguistically meaningful phoneme representations, but lacks explicit evidence that the model's reasoning matches that of a linguist beyond the step of deriving reflexes from reconstructions.
- What evidence would resolve it: Analysis showing that DPD identifies specific sound correspondence patterns or rules that align with established historical linguistic knowledge, or comparison of DPD's internal representations with those of linguists' analyses.

### Open Question 2
- Question: How does the performance of DPD-based strategies compare to other semisupervised learning approaches when applied to supervised reconstruction on the full training set?
- Basis in paper: [explicit] The authors test ΠM, DPD, and DPD-ΠM on 100% labeled data and find that, with the right architecture, they can outperform SUPV. However, they acknowledge that the role of data augmentation and the DPD architecture in supervised settings requires further investigation.
- Why unresolved: While the paper shows that DPD-based strategies can be useful for supervised reconstruction, the specific advantages of DPD over other semisupervised techniques (like ΠM alone) in this setting are not fully explored.
- What evidence would resolve it: Direct comparison of DPD-based strategies against other semisupervised methods (like ΠM alone) on fully labeled datasets, isolating the contribution of the DPD architecture from data augmentation.

### Open Question 3
- Question: Can DPD-based strategies be effectively applied to other low-resource scenarios beyond proto-language reconstruction, such as other historical linguistics tasks or even non-linguistic sequence-to-sequence problems?
- Basis in paper: [inferred] The authors frame DPD as a general approach to incorporating the comparative method into neural architectures, motivated by the need for practical reconstruction systems with limited labeled data. The success of DPD in leveraging unlabeled cognate sets suggests potential applicability to other tasks where unlabeled data is abundant but labeled data is scarce.
- Why unresolved: The paper focuses specifically on proto-language reconstruction and does not explore the broader applicability of the DPD approach to other domains.
- What evidence would resolve it: Experiments applying DPD-based strategies to other low-resource sequence-to-sequence tasks, such as other historical linguistics problems (e.g., reflex prediction in other language families) or even non-linguistic tasks (e.g., low-resource machine translation), to assess its generalizability.

## Limitations

- The evaluation relies on transductive settings where the model predicts labels for the same unlabeled training examples used during training, which may overstate real-world performance where test examples are truly unseen.
- The effectiveness of DPD depends critically on the assumption that sound changes are regular enough for bidirectional learning, but the paper provides limited analysis of when this assumption breaks down.
- The bridge network architecture lacks detailed specification of its dimensions and initialization, which could significantly impact reproducibility.

## Confidence

**High Confidence**: DPD improves protoform reconstruction accuracy compared to strong semisupervised baselines, particularly in low-labeling settings. This is supported by consistent quantitative improvements across multiple datasets and metrics.

**Medium Confidence**: DPD captures linguistically meaningful phoneme representations that contribute to reconstruction accuracy. While the paper provides some analysis of phoneme clustering and error patterns, the connection between representation quality and reconstruction performance is not fully established through ablation studies.

**Medium Confidence**: DPD generalizes effectively to low labeling settings and benefits supervised training through regularization. The paper shows performance improvements but does not isolate the regularization effect from other factors like increased model capacity.

## Next Checks

1. **Inductive Evaluation**: Evaluate the model on truly unseen unlabeled cognate sets (held-out from training) to verify that transductive improvements generalize to inductive settings, which better reflects real-world deployment scenarios.

2. **Bridge Network Sensitivity**: Conduct ablation studies varying the bridge network architecture (different layer sizes, activation functions) to determine how sensitive DPD performance is to this component and identify optimal configurations.

3. **Regularization Analysis**: Train supervised models with and without the reflex prediction task (but without unlabeled data) to isolate whether performance gains in fully supervised settings are due to regularization effects or increased model capacity.