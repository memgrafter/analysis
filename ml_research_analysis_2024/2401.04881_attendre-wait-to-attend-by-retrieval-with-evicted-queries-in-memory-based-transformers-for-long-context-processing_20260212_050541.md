---
ver: rpa2
title: 'Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based
  Transformers for Long Context Processing'
arxiv_id: '2401.04881'
source_url: https://arxiv.org/abs/2401.04881
tags:
- memory
- attention
- context
- arxiv
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently processing long
  input sequences with transformers by introducing eviction policies and a wait-to-attend
  mechanism. The core method idea is to use eviction policies like LRA and LFA to
  reduce the memory size required to store keys and values from past chunks, and introduce
  the ATTENDRE layer that retrieves key-value memory with evicted queries in the query
  memory.
---

# Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing

## Quick Facts
- arXiv ID: 2401.04881
- Source URL: https://arxiv.org/abs/2401.04881
- Authors: Zi Yang; Nan Hua
- Reference count: 17
- One-line primary result: Performance reaches level of original model processing entire long sequence with average EM score up to 69.38 on TriviaQA

## Executive Summary
This paper addresses the challenge of efficiently processing long input sequences with transformers by introducing eviction policies and a wait-to-attend mechanism. The authors propose LRA (least recently attended) and LFA (least frequently attended) policies to reduce memory size while retaining contextually important information based on attention scores. The ATTENDRE layer enables bidirectional attention by retrieving key-value memory with evicted queries, allowing queries to attend to "future" key-values beyond the current step. On the TriviaQA reading comprehension task, the proposed methods achieve performance comparable to processing the entire long sequence with an average EM score of 69.38, outperforming baseline methods.

## Method Summary
The method involves chunking input sequences into smaller segments and processing them through transformer layers with an ATTENDRE layer that incorporates eviction policies. Queries are stored in Q memory while key-value pairs are stored in KV memory. The LRA and LFA eviction policies use attention scores as proxies for importance to evict the least attended KV pairs, maintaining a small memory footprint. The ATTENDRE layer retrieves KV pairs using evicted queries, enabling bidirectional attention by allowing queries to access KV entries from future chunks. The approach uses memory-based transformers with LM-Infinite method to extend context window for RoPE-based models, and is evaluated on the TriviaQA reading comprehension task.

## Key Results
- Performance reaches level of original model processing entire long sequence with average EM score of 69.38
- A memory size of 128 with proposed policies performs on par with baseline methods using memory size of 2,048
- Outperforms baseline methods on TriviaQA reading comprehension task

## Why This Works (Mechanism)

### Mechanism 1
LRA and LFA eviction policies use attention scores to prioritize key-value entries for retention. The policies track attention scores for each KV pair across queries and time, evicting the least attended pairs to maintain a small memory footprint while retaining contextually important information. Core assumption: Attention scores serve as a proxy for the importance of KV pairs to future queries.

### Mechanism 2
ATTENDRE layer enables bidirectional attention by retrieving "future" KV entries with evicted queries. By storing both queries and KV pairs in separate memories, the ATTENDRE layer allows evicted queries to retrieve KV pairs that were inserted in subsequent chunks, enabling context-aware attention beyond the current step. Core assumption: Queries evicted from the Q memory can effectively retrieve relevant KV pairs from future chunks.

### Mechanism 3
Using a smaller memory with effective eviction policies can match the performance of larger memory systems. By retaining only the most important KV pairs based on attention scores, the model can achieve similar or better performance with a significantly smaller memory footprint compared to FIFO-based approaches. Core assumption: A smaller set of highly relevant KV pairs is more effective than a larger set of less relevant pairs.

## Foundational Learning

- Concept: Transformer attention mechanism and self-attention scores
  - Why needed here: Understanding how attention scores are computed and used is crucial for grasping how LRA and LFA policies work
  - Quick check question: How are attention scores calculated in a multi-head self-attention layer, and what do they represent?

- Concept: Memory management and eviction policies (FIFO, LRU, LFU)
  - Why needed here: Familiarity with different memory management strategies is necessary to understand the proposed LRA and LFA policies
  - Quick check question: What are the key differences between FIFO, LRU, and LFU eviction policies, and how do they prioritize which entries to evict?

- Concept: Bidirectional vs. unidirectional attention in transformers
  - Why needed here: Understanding the limitations of unidirectional attention and the benefits of bidirectional attention is crucial for appreciating the ATTENDRE layer's contribution
  - Quick check question: What are the main differences between bidirectional and unidirectional attention, and why is bidirectional attention important for certain tasks like reading comprehension?

## Architecture Onboarding

- Component map: Input sequence split into chunks -> Transformer layers with ATTENDRE layer -> Q memory (stores queries) -> KV memory (stores key-value pairs) -> Eviction policies (LRA, LFA) -> Output sequence (shifted due to Q memory)
- Critical path: 1) Chunk input sequence 2) Process each chunk through transformer layers 3) Store queries in Q memory and KV pairs in KV memory 4) Apply eviction policies to manage memory size 5) Use evicted queries to retrieve KV pairs from future chunks 6) Generate output sequence
- Design tradeoffs:
  - Memory size vs. performance: Smaller memory requires more aggressive eviction but reduces computational cost
  - Q memory size vs. retrieval effectiveness: Larger Q memory allows more "future" KV pairs to be retrieved but increases computational cost
  - Choice of eviction policy: LRA vs. LFA vs. other policies affects which KV pairs are retained
- Failure signatures:
  - Poor performance on tasks requiring long-range dependencies
  - Inability to retrieve relevant KV pairs with evicted queries
  - Inconsistent performance across different sequence lengths or chunk sizes
- First 3 experiments:
  1. Compare performance of LRA and LFA policies against FIFO baseline on a reading comprehension task
  2. Evaluate the impact of Q memory size on model performance and computational cost
  3. Test the effectiveness of ATTENDRE layer on bidirectional attention tasks vs. unidirectional attention tasks

## Open Questions the Paper Calls Out

### Open Question 1
How can the Q memory design be improved to enable gradient backpropagation while maintaining its ability to delay queries for future key-value retrieval? The paper acknowledges that the Q memory stops the backward propagation of gradients, and suggests a potential solution using rematerialization techniques. This requires further research to determine its feasibility and effectiveness.

### Open Question 2
What are the optimal initialization strategies for the attention scores in the LRA and LFA policies to maximize performance across different model architectures and tasks? The paper discusses the importance of setting appropriate initial scores for new positions in the memory, but notes that the optimal value depends on various factors like position bias, layer normalization, and layer depth.

### Open Question 3
How can the encoder output memory in the encoder-decoder architecture be effectively compressed without sacrificing performance on long-context tasks? The paper suggests that the encoder output memory is large and could benefit from compression, but does not provide a specific solution.

## Limitations
- Limited evidence of generalization beyond the TriviaQA reading comprehension task
- Specific benefits and limitations of bidirectional attention for different task types not thoroughly explored
- Evaluation focuses primarily on exact match scores without extensive ablation studies on individual components

## Confidence

**High confidence**: The basic mechanism of using eviction policies (LRA/LFA) based on attention scores is well-established in memory management literature and the implementation appears sound

**Medium confidence**: The ATTENDRE layer's ability to enable effective bidirectional attention through evicted query retrieval is demonstrated on one task but needs broader validation

**Medium confidence**: The claim that small memory with intelligent eviction can match larger memory performance is supported by results but could benefit from more extensive comparative analysis

## Next Checks

1. **Cross-task validation**: Test the ATTENDRE architecture on additional long-context tasks beyond reading comprehension (e.g., document summarization, multi-document QA) to assess generalization.

2. **Component ablation study**: Conduct controlled experiments isolating the contributions of the eviction policies, ATTENDRE layer, and Q memory size to better understand their individual impacts on performance.

3. **Attention score validation**: Empirically measure the correlation between attention scores and actual contextual importance by analyzing which evicted KV pairs were truly important for downstream performance across multiple examples.