---
ver: rpa2
title: Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with Large
  Language Models
arxiv_id: '2409.19382'
source_url: https://arxiv.org/abs/2409.19382
tags:
- question
- reasoning
- country
- answer
- truth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MZQA, a Monte-Carlo tree search-based framework
  for zero-shot multi-hop question answering that avoids error propagation in sequential
  reasoning by exploring multiple reasoning paths. MZQA uses instruction-based zero-shot
  prompting without hand-crafted examples and incorporates a behavioral cloning variant
  (MZQA-BC) trained on self-generated MCTS trajectories to achieve over 10x speedup
  with minimal performance loss.
---

# Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with Large Language Models

## Quick Facts
- arXiv ID: 2409.19382
- Source URL: https://arxiv.org/abs/2409.19382
- Reference count: 40
- One-line primary result: Achieves over 10x speedup with minimal performance loss using behavioral cloning on MCTS trajectories

## Executive Summary
This paper introduces MZQA, a Monte-Carlo tree search-based framework for zero-shot multi-hop question answering that avoids error propagation in sequential reasoning by exploring multiple reasoning paths. MZQA uses instruction-based zero-shot prompting without hand-crafted examples and incorporates a behavioral cloning variant (MZQA-BC) trained on self-generated MCTS trajectories to achieve over 10x speedup with minimal performance loss. The method outperforms existing approaches on HotpotQA, 2WikiMultiHopQA, and MuSiQue benchmarks, achieving higher F1 and exact match scores while maintaining robustness to few-shot prompt variations.

## Method Summary
MZQA implements a Monte-Carlo Tree Search (MCTS) framework for zero-shot multi-hop question answering using large language models. The approach employs instruction-based zero-shot prompting to generate sub-questions, retrieve relevant documents, and evaluate reasoning trajectories without relying on hand-crafted examples. The system uses a BM25-based retriever with Elasticsearch to fetch documents based on sub-questions, while an LLM generates action candidates and evaluates trajectory usefulness using UCT-based selection. A behavioral cloning variant (MZQA-BC) is trained on self-generated MCTS trajectories to accelerate inference, achieving over 10x speedup with minimal performance degradation. The framework is evaluated on HotpotQA, 2WikiMultiHopQA, and MuSiQue benchmarks using F1 and exact match scores.

## Key Results
- MZQA-BC achieves over 10x speedup compared to full MCTS while maintaining competitive performance
- Zero-shot instruction prompting shows robustness with at most 2.9 F1 points variation across different few-shot benchmarks
- Outperforms existing approaches on HotpotQA, 2WikiMultiHopQA, and MuSiQue with higher F1 and exact match scores

## Why This Works (Mechanism)

### Mechanism 1
MCTS with zero-shot prompting reduces error propagation by exploring multiple reasoning paths before committing to an answer. The UCT formula balances exploration of new reasoning paths with exploitation of promising ones, allowing the system to evaluate multiple potential reasoning sequences rather than following a single autoregressive chain that could accumulate errors. The core assumption is that the LLM can generate useful sub-questions and evaluate trajectory usefulness without requiring few-shot examples. Evidence shows MZQA uses instruction-based zero-shot prompting without hand-crafted examples, and MCTS is employed for principled search that exploits high-value options while exploring alternatives. If the LLM cannot generate diverse, useful sub-questions or if reward evaluation becomes unreliable, the MCTS exploration becomes ineffective.

### Mechanism 2
Behavioral cloning on self-generated MCTS trajectories accelerates reasoning while maintaining performance. The system generates expert trajectories using MCTS, then fine-tunes the LLM to imitate these trajectories directly, allowing the model to make decisions without running the full MCTS search each time and reducing computation by over 10x. The core assumption is that MCTS-generated trajectories contain generalizable patterns that can be learned by the BC model for efficient inference. Evidence includes claims of over 10x speedup with minimal performance loss and that BC enables the model to mimic optimal paths identified by MCTS without extensive reasoning computations. If MCTS trajectories are too specific to individual questions or if the BC model overfits to training trajectories, generalization performance degrades.

### Mechanism 3
Zero-shot instruction prompting provides robust performance across different question types without requiring few-shot examples. The system uses instruction-based prompts that guide the LLM to decompose questions, retrieve relevant information, and evaluate reasoning usefulness without relying on in-context examples, eliminating sensitivity to example composition and domain expertise requirements. The core assumption is that the instruction-tuned LLM has sufficient reasoning capability to handle diverse multi-hop questions when properly prompted. Evidence shows MZQA relies solely on instructions without hand-crafted few-shot examples that typically require domain expertise, and the maximum F1 score difference across benchmarks was at most 2.9 points. If the instruction-tuned LLM lacks sufficient reasoning capability for complex multi-hop questions, the zero-shot approach fails to match few-shot performance.

## Foundational Learning

- **Monte-Carlo Tree Search and UCT**: MCTS provides the exploration-exploitation balance needed to find optimal reasoning paths in a large action space of natural language sub-questions. Quick check: How does the UCT formula balance exploration of unvisited nodes versus exploitation of high-value nodes?

- **Behavioral cloning and offline imitation learning**: BC allows the system to learn from MCTS-generated trajectories to make fast decisions without running the full search, addressing computational inefficiency. Quick check: What is the key difference between online planning (MCTS) and offline imitation learning (BC) in terms of computational cost and generalization?

- **Zero-shot instruction prompting**: Instruction prompting enables the system to operate without hand-crafted examples, making it more versatile and reducing dependency on domain expertise. Quick check: How does instruction-based zero-shot prompting differ from few-shot prompting in terms of sensitivity to example quality and domain requirements?

## Architecture Onboarding

- **Component map**: Question → Action Candidate Generation → Document Retrieval → Answer Generation → Reward Evaluation → MCTS Update → Final Answer Generation
- **Critical path**: Question flows through action candidate generation (LLM), document retrieval (BM25), answer generation (LLM), reward evaluation (LLM), MCTS update, and final answer generation (LLM)
- **Design tradeoffs**: MCTS iterations vs. inference speed (more iterations improve accuracy but increase latency); action candidates vs. search space (more candidates improve coverage but increase computational cost); retriever quality vs. answer accuracy (better retrieval improves reasoning but may require more sophisticated systems)
- **Failure signatures**: Low diversity in generated sub-questions indicates action candidate generator limitations; consistently low rewards suggests evaluator cannot distinguish useful from useless trajectories; high variance across runs points to instability in LLM generation or reward evaluation; performance degradation on few-shot prompts reveals over-reliance on instruction tuning
- **First 3 experiments**: 1) Compare MCTS performance with varying iterations (2, 10, 20) on held-out validation set to find accuracy-speed sweet spot; 2) Test zero-shot prompting against few-shot variants (1-shot, 5-shot, 15-shot) on small validation set to confirm robustness claims; 3) Train BC model on MCTS trajectories and evaluate speedup and performance retention compared to full MCTS on validation set

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The reliance on instruction-tuned LLMs without few-shot examples may limit performance on highly complex reasoning tasks requiring domain-specific knowledge
- The 10x speedup through behavioral cloning assumes MCTS-generated trajectories are sufficiently generalizable, but the paper doesn't extensively validate whether BC models trained on limited samples (100 trajectories) can handle full diversity of questions
- The evaluation focuses primarily on F1 and EM scores without detailed analysis of reasoning quality or error types, making it difficult to assess whether the approach truly reduces error propagation or simply produces more confident but potentially incorrect answers

## Confidence

- **High Confidence**: The MCTS framework implementation and its ability to explore multiple reasoning paths are well-supported by the methodology description and align with established MCTS principles. The reported performance improvements over baselines on standard benchmarks are credible given the rigorous experimental setup.

- **Medium Confidence**: The zero-shot prompting claims are supported by robustness tests across different few-shot variations, showing only 2.9-point F1 variation in the worst case. However, the paper doesn't provide extensive ablation studies on prompt engineering or compare against state-of-the-art few-shot approaches that might narrow this gap.

- **Low Confidence**: The behavioral cloning component's generalization capability is asserted but not thoroughly validated. With only 100 training samples and 500 evaluation samples per benchmark, the scalability and robustness of MZQA-BC across diverse question types remain uncertain.

## Next Checks

1. **Ablation Study on Prompt Engineering**: Systematically vary the instruction prompt templates across different complexity levels (simple decomposition, context-aware reasoning, multi-hop chaining) and measure performance degradation to identify prompt sensitivity thresholds.

2. **Generalization Stress Test**: Evaluate MZQA-BC on a held-out subset of questions that require reasoning patterns not present in the 100 training trajectories, measuring performance drop to assess true generalization capability.

3. **Error Analysis and Reasoning Quality**: Conduct a detailed error analysis categorizing failures by reasoning type (missing entity, incorrect relation, wrong hop order) and assess whether MCTS actually reduces error propagation compared to autoregressive baselines through qualitative examination of reasoning paths.