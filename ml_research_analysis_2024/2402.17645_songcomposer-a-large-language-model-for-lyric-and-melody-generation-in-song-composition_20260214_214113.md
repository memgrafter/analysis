---
ver: rpa2
title: 'SongComposer: A Large Language Model for Lyric and Melody Generation in Song
  Composition'
arxiv_id: '2402.17645'
source_url: https://arxiv.org/abs/2402.17645
tags:
- song
- lyrics
- melody
- generation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SongComposer is a large language model for song composition that
  generates lyrics and melodies in symbolic format. It addresses the challenge of
  unified song composition by introducing a flexible tuple format for word-level alignment
  of lyrics and melodies, extending the tokenizer vocabulary for song notes with musical
  knowledge, and using a multi-stage training pipeline.
---

# SongComposer: A Large Language Model for Lyric and Melody Generation in Song Composition

## Quick Facts
- arXiv ID: 2402.17645
- Source URL: https://arxiv.org/abs/2402.17645
- Reference count: 17
- Primary result: A large language model that generates lyrics and melodies in symbolic format, outperforming GPT-4 on multiple song composition tasks

## Executive Summary
SongComposer is a large language model designed to generate both lyrics and melodies for song composition. The model addresses the challenge of unified song composition by introducing a flexible tuple format for word-level alignment of lyrics and melodies, extending the tokenizer vocabulary for song notes with musical knowledge, and using a multi-stage training pipeline. Trained on a large-scale dataset of 280K pure lyrics, 20K pure melodies, and 15K lyric-melody pairs in Chinese and English, SongComposer demonstrates superior performance compared to advanced LLMs like GPT-4 on tasks including lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation.

## Method Summary
SongComposer employs a two-stage training approach: first pretraining on a large corpus of pure lyrics and melodies with next token prediction, then fine-tuning using carefully crafted QA pairs to develop instruction-following capability for song generation tasks. The model uses a specialized tuple-based data format that explicitly aligns lyrics with melody attributes at the word level, and extends the tokenizer vocabulary with 512 discrete duration tokens and 120 pitch class tokens to enable direct musical understanding.

## Key Results
- Achieves 47.74% pitch distribution similarity on lyric-to-melody generation compared to 35.90% for GPT-4
- Demonstrates superior performance on melody-to-lyric generation, song continuation, and text-to-song creation tasks
- Successfully generates coherent lyrics-melody pairs in both Chinese and English

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The tuple data format explicitly aligns lyrics with melody attributes at the word level, enabling precise synchronization.
- Mechanism: By structuring each tuple to contain a lyric word alongside its corresponding pitch, duration, and rest duration, the model receives clear, aligned input that maintains the relationship between text and musical notes.
- Core assumption: The LLM can effectively learn and leverage this explicit alignment to generate coherent lyrics-melody pairs.
- Evidence anchors:
  - [abstract] "a flexible tuple format for word-level alignment of lyrics and melodies"
  - [section 4.1] "we propose a unified input organization utilizing a tuple-based format, where each tuple represents a discrete musical unit"

### Mechanism 2
- Claim: Extending the tokenizer vocabulary with musical tokens allows the model to understand and generate musical concepts directly.
- Mechanism: By adding 512 discrete duration tokens and 120 pitch class tokens to the vocabulary, the model can process and produce musical information without relying on abstract or inefficient encodings.
- Core assumption: The added tokens are sufficient to represent the musical concepts needed for song composition and that the model can learn the relationships between these tokens.
- Evidence anchors:
  - [abstract] "an extended tokenizer vocabulary for song notes, with scalar initialization based on musical knowledge"
  - [section 4.1] "we specialize our representation for discretized time units and note values within the model's vocabulary by introducing auxiliary tokens"

### Mechanism 3
- Claim: The multi-stage training pipeline improves the model's ability to generate coherent songs by first learning basic musical understanding and then focusing on lyric-melody alignment.
- Mechanism: The pretraining stage builds foundational knowledge of lyrics and melodies separately, while the supervised fine-tuning stage teaches the model to align them, leading to better overall song composition.
- Core assumption: The staged approach allows the model to build upon a strong foundation before tackling the more complex task of alignment, and that the dataset is sufficiently large and diverse.
- Evidence anchors:
  - [abstract] "a multi-stage pipeline that captures musical structure, starting with motif-level melody patterns and progressing to phrase-level structure for improved coherence"
  - [section 4.2] "we first pretrain the model on a substantial corpus of pure lyrics and pure melodies... Then, we use our paired data to guide the model in generating aligned lyric-melody pairs"

## Foundational Learning

- Concept: Symbolic song representation
  - Why needed here: It provides a mature and efficient way to represent music as text, making it suitable for LLM input and output.
  - Quick check question: How does symbolic song representation differ from quantized audio signals in terms of token efficiency and semantic representation?

- Concept: Tuple-based data formatting
  - Why needed here: It ensures precise alignment between lyrics and melodies by structuring the input data into discrete units containing both textual and musical information.
  - Quick check question: What are the advantages of using a tuple-based format over sequential arrangement for lyric-melody alignment?

- Concept: Vocabulary extension for musical tokens
  - Why needed here: It allows the model to understand and generate musical concepts directly, without relying on abstract or inefficient encodings.
  - Quick check question: How does adding musical tokens to the vocabulary improve the model's ability to process and generate musical content compared to using existing tokens?

## Architecture Onboarding

- Component map: LLM base model (InternLM-7B) extended with specialized vocabulary for musical tokens → trained on multi-stage pipeline (pretraining on pure lyrics/melodies → supervised fine-tuning on aligned lyric-melody pairs)
- Critical path: Model processes input tuple sequence → leverages learned musical structure and lyric-melody alignment → generates output sequence maintaining tuple format
- Design tradeoffs: Symbolic representation over quantized audio prioritizes token efficiency and semantic representation but may limit capturing subtle audio nuances; tuple format ensures alignment but may restrict flexibility in generating non-aligned content
- Failure signatures: Misaligned lyrics and melodies indicate issues with tuple format or alignment mapping; lack of musical coherence suggests problems with pretraining data or multi-stage training approach
- First 3 experiments:
  1. Evaluate model's ability to generate aligned lyric-melody pairs using held-out test set and measure alignment accuracy
  2. Compare model's performance on pure lyric and melody generation tasks to assess pretraining stage effectiveness
  3. Analyze impact of different tuple formats or vocabulary sizes on model performance to identify potential optimizations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the tuple-based format for lyric-melody alignment compare to other alignment methods in terms of accuracy and efficiency?
- Basis in paper: [explicit] The paper mentions that the tuple-based format ensures precise alignment between lyrics and melodies, but does not provide a direct comparison with other alignment methods.
- Why unresolved: The paper does not provide a detailed comparison of the tuple-based format with other alignment methods, such as dynamic time warping or forced alignment, in terms of accuracy and efficiency.
- What evidence would resolve it: A comparative study of the tuple-based format against other alignment methods using metrics such as alignment error rate, computational time, and memory usage would provide a clear answer.

### Open Question 2
- Question: What is the impact of the size of the SongCompose-PT dataset on the performance of SongComposer?
- Basis in paper: [inferred] The paper mentions that the dataset contains 280K pure lyrics, 20K pure melodies, and 15K lyric-melody pairs, but does not explore the impact of dataset size on model performance.
- Why unresolved: The paper does not provide an analysis of how the size of the SongCompose-PT dataset affects the performance of SongComposer, leaving open the question of whether larger datasets would lead to better results.
- What evidence would resolve it: Experiments training SongComposer on datasets of varying sizes and comparing their performance on the same tasks would provide insights into the relationship between dataset size and model performance.

### Open Question 3
- Question: How does SongComposer handle the generation of lyrics and melodies in different musical genres and styles?
- Basis in paper: [explicit] The paper mentions that the dataset includes songs in Chinese and English, but does not explore the model's ability to generate lyrics and melodies in different genres and styles.
- Why unresolved: The paper does not provide an analysis of how SongComposer handles the generation of lyrics and melodies in different musical genres and styles, such as pop, rock, or classical music.
- What evidence would resolve it: Experiments testing SongComposer's ability to generate lyrics and melodies in various genres and styles, and comparing the results with human-composed songs in those genres, would provide insights into the model's versatility and creativity.

## Limitations
- Generalization across languages and musical styles not thoroughly evaluated
- Limited assessment of nuanced musical qualities like emotional expression and melodic catchiness
- Insufficient transparency regarding dataset diversity and potential biases

## Confidence
- High confidence in the core mechanism of tuple-based alignment
- High confidence in the vocabulary extension approach
- Medium confidence in the multi-stage training pipeline
- Medium confidence in the evaluation results

## Next Checks
1. Conduct an ablation study on tuple format by testing the model's performance using alternative alignment approaches to quantify the specific contribution of the tuple format to alignment quality
2. Evaluate SongComposer on song composition tasks in languages and musical traditions not represented in the training data to assess true generalization capabilities
3. Conduct a more comprehensive human evaluation focused on musical qualities (catchiness, emotional expression, melodic coherence) with expert musicians to complement technical metrics and assess real-world applicability