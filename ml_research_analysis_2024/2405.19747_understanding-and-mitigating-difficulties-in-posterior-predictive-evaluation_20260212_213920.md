---
ver: rpa2
title: Understanding and mitigating difficulties in posterior predictive evaluation
arxiv_id: '2405.19747'
source_url: https://arxiv.org/abs/2405.19747
tags:
- data
- where
- inference
- estimator
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the signal-to-noise ratio (SNR) of Monte Carlo
  estimators for predictive posterior densities (PPD) in Bayesian inference. The authors
  show that SNR decays exponentially with (1) mismatch between training and test data,
  (2) dimensionality of latent space, and (3) relative size of test data.
---

# Understanding and mitigating difficulties in posterior predictive evaluation

## Quick Facts
- arXiv ID: 2405.19747
- Source URL: https://arxiv.org/abs/2405.19747
- Authors: Abhinav Agrawal; Justin Domke
- Reference count: 40
- This paper analyzes SNR decay in Monte Carlo estimators for predictive posterior densities and proposes learned importance sampling to address this issue

## Executive Summary
This paper addresses a critical challenge in Bayesian inference: evaluating posterior predictive densities (PPD) when Monte Carlo estimators suffer from exponentially decaying signal-to-noise ratios (SNR). The authors identify three key factors that degrade SNR - mismatch between training and test data, latent space dimensionality, and relative test data size. To solve this, they propose a learned importance sampling approach that optimizes a proposal distribution at test time using the importance-weighted evidence lower bound (IW-ELBO), significantly improving PPD evaluation accuracy compared to naive Monte Carlo sampling.

## Method Summary
The authors analyze SNR decay in Monte Carlo estimators for PPD evaluation and propose learned importance sampling as a solution. The method involves learning a proposal distribution optimized via IW-ELBO at test time, which is then used to improve importance sampling estimates. This approach addresses the exponential SNR decay that occurs when there's mismatch between training and test data, high latent space dimensionality, or large test data relative to training data. The proposal distribution is optimized using DReG (Doubly Reparameterized Gradient) estimator in JAX, and experiments demonstrate significant improvements over naive Monte Carlo sampling across various model families.

## Key Results
- SNR decays exponentially with mismatch between training and test data, latent space dimensionality, and relative test data size
- Learned importance sampling with IW-ELBO optimization achieves up to 5x better performance in comparing inference methods
- The proposed method significantly improves PPD estimation accuracy compared to naive Monte Carlo sampling across exponential family models, linear regression, logistic regression, and hierarchical MovieLens models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SNR decays exponentially with mismatch between training and test data
- Mechanism: The posterior predictive density estimation relies on samples from the approximate posterior. When test data differs from training data, the posterior shifts, making the naive Monte Carlo estimator less accurate due to high variance in the importance weights
- Core assumption: The approximate posterior qD(z) is reasonably close to the true posterior p(z|D), and datasets are sufficiently large for CLT to apply
- Evidence anchors:
  - [abstract]: "SNR decays exponentially as there is increase in (a) the mismatch between training and test data"
  - [section 2]: Theorem 1 shows SNR depends on KL divergence between posteriors with and without test data
  - [corpus]: No direct corpus evidence found for this specific exponential decay relationship
- Break condition: If the approximate posterior is very poor (large approximation error), the mechanism may not hold as the fundamental assumption breaks down

### Mechanism 2
- Claim: SNR decays exponentially with latent space dimensionality
- Mechanism: As latent space dimensionality increases, the posterior becomes more concentrated in higher dimensions, making it harder for naive Monte Carlo samples to capture the relevant regions for test data evaluation
- Core assumption: Datasets are large enough that posteriors concentrate around their MLEs, and the Hessian of the log-likelihood is similar across training and test data combinations
- Evidence anchors:
  - [abstract]: "SNR decays exponentially as there is increase in (b) the dimensionality of the latent space"
  - [section 2]: Proposition 2 shows δ ≈ d/2 log(1 + |D*|/|D|)√(1 + 2|D*|/|D|), linear in d
  - [corpus]: No direct corpus evidence found for this specific exponential decay relationship
- Break condition: If the dimensionality increase is accompanied by proportionally larger datasets, the effect may be mitigated

### Mechanism 3
- Claim: SNR decays exponentially with relative test data size
- Mechanism: Larger test datasets create more concentrated posteriors when combined with training data, increasing the mismatch between p(z|D) and p(z|D+D*) in terms of concentration
- Core assumption: The test data is drawn from a similar distribution to training data, just with larger sample size
- Evidence anchors:
  - [abstract]: "SNR decays exponentially as there is increase in (c) the size of the test data relative to the training data"
  - [section 2]: Proposition 2 shows δ increases logarithmically with |D*|/|D| ratio
  - [corpus]: No direct corpus evidence found for this specific exponential decay relationship
- Break condition: If the test data size increase is accompanied by proportionally larger training data, the effect may be mitigated

## Foundational Learning

- Concept: Signal-to-Noise Ratio (SNR) in Monte Carlo estimation
  - Why needed here: Understanding why naive Monte Carlo estimators fail requires grasping how signal strength relates to noise in importance sampling
  - Quick check question: If E[R] = 10 and V[R] = 100, what is SNR(R)?

- Concept: Importance Weighted Evidence Lower Bound (IW-ELBO)
  - Why needed here: The paper proposes optimizing IW-ELBO as a surrogate for maximizing SNR in importance sampling
  - Quick check question: What is the relationship between IW-ELBO and the variance of an importance sampling estimator?

- Concept: Variational Inference and Approximate Posteriors
  - Why needed here: The paper analyzes SNR for both exact and approximate inference cases
  - Quick check question: In variational inference, what is the goal of the approximate posterior q(z) compared to the true posterior p(z|D)?

## Architecture Onboarding

- Component map: Data preprocessing -> Model definition -> Inference method selection -> Posterior approximation (qD) -> PPD evaluation (naive MC or learned IS)
- Critical path:
  1. Generate or load training and test datasets
  2. Perform inference to obtain qD(z)
  3. Evaluate naive MC estimator SNR to determine if learned IS is needed
  4. If needed, optimize IW-ELBO to learn proposal rw(z)
  5. Use learned IS estimator for final PPD evaluation
- Design tradeoffs:
  - Computational cost: Learned IS requires additional optimization at test time
  - Accuracy vs. speed: More optimization iterations improve proposal quality but increase computation
  - Proposal family choice: Gaussian vs. normalizing flows affects expressiveness and computational requirements
- Failure signatures:
  - Extremely low SNR in naive MC indicates need for learned IS
  - Poor performance of learned IS despite optimization suggests issues with proposal family choice or optimization hyperparameters
  - Discrepancy between training and test data distributions may require data preprocessing
- First 3 experiments:
  1. Implement naive MC estimator for a simple conjugate model and verify SNR decay with increasing mismatch
  2. Add learned IS with Gaussian proposal and compare performance against naive MC
  3. Experiment with different proposal families (e.g., normalizing flows) for a more complex model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can importance sampling with a learned proposal distribution be applied to models with discrete latent variables?
- Basis in paper: [inferred] The paper focuses on continuous latent spaces and does not address discrete cases.
- Why unresolved: The paper does not discuss or experiment with discrete latent spaces.
- What evidence would resolve it: Experiments applying learned importance sampling to models with discrete latent variables, such as latent Dirichlet allocation or discrete state-space models.

### Open Question 2
- Question: How does the computational cost of learning a proposal distribution at test time compare to the accuracy gains in real-world applications?
- Basis in paper: [explicit] The paper mentions that learned IS involves learning a proposal distribution at test time, which can be computationally expensive, and suggests future work could explore the trade-offs between accuracy and computational cost.
- Why unresolved: The paper does not provide a detailed analysis of the computational cost versus accuracy trade-off.
- What evidence would resolve it: Empirical studies comparing the computational time and accuracy of learned IS versus naive MC across different model sizes and datasets.

### Open Question 3
- Question: Can the theoretical bounds on signal-to-noise ratio (SNR) be extended to non-conjugate models?
- Basis in paper: [inferred] The paper provides theoretical analysis for conjugate models but does not extend these results to non-conjugate models.
- Why unresolved: The paper focuses on conjugate models and does not discuss non-conjugate cases.
- What evidence would resolve it: Theoretical extensions of the SNR analysis to non-conjugate models, possibly using approximation techniques like variational inference.

## Limitations

- The theoretical analysis relies on asymptotic assumptions that may not hold for small datasets or high-dimensional problems
- The exponential decay relationships, while mathematically derived, may not capture all practical scenarios where posterior predictive evaluation fails
- The proposed solution adds computational overhead at test time, which may not be acceptable for all applications

## Confidence

- SNR decay mechanisms (High): The mathematical derivation and theoretical framework for SNR decay are well-established and rigorously proven
- Learned importance sampling effectiveness (Medium): Experimental results show improvements, but the theoretical guarantees for IW-ELBO optimization are limited
- Applicability to non-exponential family models (Low): The paper focuses primarily on exponential family models and their extensions, with limited analysis of other model classes

## Next Checks

1. **Cross-validation of theoretical predictions**: Generate synthetic datasets with controlled mismatch, dimensionality, and test data size to empirically verify the exponential decay relationships predicted by the theory.

2. **Comparison with alternative methods**: Benchmark the learned importance sampling approach against other posterior predictive evaluation methods (e.g., leave-one-out cross-validation, Pareto-smoothed importance sampling) on diverse model families.

3. **Scalability analysis**: Evaluate the computational cost and accuracy of the proposed method on high-dimensional models (e.g., deep neural networks) to assess practical limitations and potential modifications needed for broader applicability.