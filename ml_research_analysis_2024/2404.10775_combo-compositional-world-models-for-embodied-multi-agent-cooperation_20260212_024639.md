---
ver: rpa2
title: 'COMBO: Compositional World Models for Embodied Multi-Agent Cooperation'
arxiv_id: '2404.10775'
source_url: https://arxiv.org/abs/2404.10775
tags:
- world
- agents
- state
- actions
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of embodied multi-agent cooperation
  where decentralized agents must work together using only partial egocentric visual
  observations. The key contribution is a compositional world model that learns to
  simulate world dynamics conditioned on joint actions of an arbitrary number of agents
  by factorizing these actions and compositionally generating future frames.
---

# COMBO: Compositional World Models for Embodied Multi-Agent Cooperation

## Quick Facts
- arXiv ID: 2404.10775
- Source URL: https://arxiv.org/abs/2404.10775
- Reference count: 23
- Key outcome: Achieves near-perfect success rates (up to 100%) on 2-4 agent embodied cooperation tasks with significantly fewer steps than baselines

## Executive Summary
This paper introduces COMBO, a framework for embodied multi-agent cooperation where decentralized agents must work together using only partial egocentric visual observations. The key innovation is a compositional world model that learns to simulate world dynamics conditioned on joint actions of arbitrary numbers of agents by factorizing these actions and compositionally generating future frames. This is combined with Vision Language Models (VLMs) that propose actions, infer other agents' intents, and evaluate outcomes, all integrated through a tree search procedure. On three challenging benchmarks with 2-4 agents, COMBO achieves near-perfect success rates (up to 100%) with significantly fewer steps than baseline methods, demonstrating its effectiveness in enabling efficient multi-agent cooperation.

## Method Summary
COMBO uses a compositional world model that factorizes joint actions into per-agent components and generates future frames through video diffusion. The model is trained in two stages: first learning single-agent dynamics, then fine-tuning for joint actions with composition. Agent-dependent loss scaling focuses training on relevant regions for each agent's actions. Three VLMs handle action proposal, intent tracking, and outcome evaluation. These components are integrated through a tree search procedure that maintains B best-scored plan beams and explores P action proposals at each step for D rollout steps. The framework is evaluated on TDW-Game and TDW-Cook environments with 2-4 agents.

## Key Results
- Achieves near-perfect success rates (up to 100%) on TDW-Game and TDW-Cook tasks with 2-4 agents
- Requires significantly fewer steps than baseline methods for successful task completion
- Generalizes from 4-agent training to 2- and 3-agent versions with high success rates
- Successfully handles scenarios with 4 agents without any pre-training on 2- or 3-agent cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factorizing joint actions into per-agent components enables compositional video generation that accurately models multi-agent world dynamics.
- Mechanism: The compositional world model learns separate score functions for each agent's action conditioned on the current world state, then composes these scores to generate future frames.
- Core assumption: Joint actions of multiple agents can be decomposed into independent components that can be composed to accurately predict future world states.
- Evidence anchors: [abstract] "factorizing the naturally composable joint actions of multiple agents and compositionally generating the video conditioned on the world state" [section] "Observing that the joint action a can be naturally factorized into n components a1, · · · , an corresponding to the action of each agent"
- Break condition: If agent actions are not independent or have complex interdependencies that cannot be captured through simple composition.

### Mechanism 2
- Claim: Agent-dependent loss scaling improves video synthesis accuracy by focusing training on relevant regions for each agent's actions.
- Mechanism: A loss coefficient matrix weights pixel errors based on each agent's reachable region in the image, forcing the model to prioritize accuracy in areas affected by specific agents' actions.
- Core assumption: Different agents affect different spatial regions of the world state, and these regions can be identified and weighted appropriately during training.
- Evidence anchors: [section] "We simply set the loss coefficient matrix based on each agent's reachable region in the image to supervise the model to focus more on the related pixels" [abstract] "even this straightforward loss coefficient approach brings a significant improvement to the modeling accuracy"
- Break condition: If agents' reachable regions overlap significantly or if actions affect the entire state space equally.

### Mechanism 3
- Claim: Tree search with limited planning beams effectively balances exploration and computational efficiency in multi-agent planning.
- Mechanism: The algorithm maintains B best-scored plan beams and explores P action proposals at each step for D rollout steps, allowing the agent to search for high-quality plans without exhaustive exploration.
- Core assumption: The best plans can be found by exploring a limited number of high-scoring trajectories rather than exhaustive search of the entire space.
- Evidence anchors: [section] "we implement a limited tree search procedure instead by always keeping the B best-scored plan beams and exploring P action proposals at each plan step for a maximum of D rollout steps" [abstract] "By leveraging this compositional world model, in combination with Vision Language Models... we can use a tree search procedure to integrate these modules and facilitate online cooperative planning"
- Break condition: If the optimal plan requires exploring trajectories that are initially scored poorly or if the planning horizon is too long for the limited beams.

## Foundational Learning

- Concept: Partial observability in multi-agent systems
  - Why needed here: Agents only have egocentric views and must estimate the overall world state from incomplete observations
  - Quick check question: How does the framework handle situations where multiple agents' observations provide conflicting information about the same object?

- Concept: Compositional generation with diffusion models
  - Why needed here: Enables generation of future states conditioned on multiple agents' actions by composing individual action-conditioned models
  - Quick check question: What happens if two agents' actions contradict each other (e.g., both trying to place different objects in the same location)?

- Concept: Tree search algorithms for planning
  - Why needed here: Balances exploration of possible action sequences with computational constraints in the multi-agent setting
  - Quick check question: How do the parameters P, B, and D affect the quality of the final plan versus computation time?

## Architecture Onboarding

- Component map: World State Estimation -> Intent Tracker -> Action Proposer -> Compositional World Model -> Outcome Evaluator -> Tree Search -> Action Execution

- Critical path: World State Estimation → Intent Tracker → Action Proposer → Compositional World Model → Outcome Evaluator → Tree Search → Action Execution

- Design tradeoffs:
  - Single vs. multiple VLMs: Using separate VLMs for each sub-module allows specialized training but increases inference cost
  - Action factorization: Simplifies multi-agent modeling but may miss complex interactions between agents
  - Planning depth vs. breadth: Deeper plans may find better solutions but increase computation time exponentially

- Failure signatures:
  - Poor world state estimation → Inaccurate planning from incomplete observations
  - Incorrect action composition → Inconsistent or impossible future states
  - Weak intent tracking → Poor coordination with other agents
  - Suboptimal outcome evaluation → Exploration of unpromising plans

- First 3 experiments:
  1. Test world state estimation quality by comparing reconstructed states against ground truth in controlled scenarios
  2. Evaluate compositional world model accuracy by generating future states from single vs. multiple agent actions
  3. Validate intent tracking by measuring prediction accuracy against known cooperator policies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of COMBO change if the compositional world model were trained on data that includes realistic physics, such as object mass and collision dynamics?
- Basis in paper: [inferred] The paper mentions that incorporating hidden physical parameters like object mass is an essential direction for making the task closer to real-world scenarios. It also suggests extending the framework to include physics-aware video generation approaches.
- Why unresolved: The current compositional world model does not explicitly model physical interactions, leading to errors in 25% of cases when predicting multi-agent actions. Adding physics-aware generation could potentially reduce these errors.
- What evidence would resolve it: Training a new compositional world model with physics-aware video generation (e.g., Liu et al., 2024; Zhang et al., 2024) and evaluating its performance on the same benchmarks (TDW-Game, TDW-Cook) compared to the current model would show whether physics modeling improves accuracy.

### Open Question 2
- Question: What is the impact of using different VLM architectures (e.g., larger models or models with different pretraining data) on the planning performance of COMBO?
- Basis in paper: [explicit] The paper uses LLaVA-v1.5-7B for all VLM-based submodules and mentions that VLMs offer better generalizability and performance compared to simpler supervised models due to their pretraining on large-scale internet data.
- Why unresolved: While LLaVA-v1.5-7B performs well, it's unclear whether larger or differently pretrained VLMs would lead to better action proposals, intent tracking, or outcome evaluation, especially in complex multi-agent scenarios.
- What evidence would resolve it: Replacing LLaVA-v1.5-7B with other VLM architectures (e.g., GPT-4V, Gemini, or LLaVA-NeXT) in all submodules and comparing success rates and steps across the same tasks would quantify the impact of VLM choice.

### Open Question 3
- Question: How does the performance of COMBO scale with the number of agents beyond 4, and what are the computational bottlenecks?
- Basis in paper: [explicit] The paper shows that COMBO trained on 4-agent TDW-Game generalizes to 2- and 3-agent versions with high success rates, but does not test scenarios with more than 4 agents. It also mentions that the current tree search is slow due to multiple inferences using large models.
- Why unresolved: The scalability of COMBO to larger teams is unknown, and the computational cost of planning with more agents could become prohibitive with the current tree search approach.
- What evidence would resolve it: Extending the experiments to test COMBO on 5+ agent versions of TDW-Game (or similar tasks) and measuring success rates, steps, and inference time per step would reveal both performance trends and computational limits.

## Limitations

- Scalability to larger agent groups remains untested, with potential exponential growth in computational cost
- Limited evaluation to controlled ThreeDWorld environments with specific object types and task structures
- Heavy dependence on VLM quality for action proposal, intent tracking, and outcome evaluation

## Confidence

- **High confidence** in the compositional world model's ability to generate future states from factorized joint actions
- **Medium confidence** in the effectiveness of Agent-Dependent Loss Scaling
- **Medium confidence** in the tree search procedure's ability to find cooperative plans

## Next Checks

1. Test scalability beyond 4 agents: Evaluate COMBO on a new benchmark with 5-8 agents to measure how performance scales with agent count and identify potential bottlenecks.

2. Stress test action factorization: Design scenarios where agents' actions have strong interdependencies (e.g., passing objects between agents) and measure how well the compositional world model handles these cases compared to modeling joint actions directly.

3. Analyze VLM sub-module contributions: Perform an ablation study where each VLM sub-module (action proposer, intent tracker, outcome evaluator) is replaced with simpler alternatives to quantify their individual contributions to overall success.