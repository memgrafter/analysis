---
ver: rpa2
title: Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation
  from Resource-Rich Languages
arxiv_id: '2402.12204'
source_url: https://arxiv.org/abs/2402.12204
tags:
- language
- sdrrl
- languages
- multilingual
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SDRRL, a self-distillation method that enhances
  multilingual capabilities of LLMs by leveraging their stronger performance in resource-rich
  languages like English. Instead of relying solely on translated data, SDRRL constructs
  a transfer set using LLM-generated responses in the resource-rich language, translates
  them into target languages, and applies sentence-level knowledge distillation.
---

# Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages

## Quick Facts
- arXiv ID: 2402.12204
- Source URL: https://arxiv.org/abs/2402.12204
- Reference count: 18
- Primary result: SDRRL improves multilingual capabilities of LLMs by +1.5 average accuracy on BELEBELE, +6.0 BLEU on FLORES, and +0.55 ROUGE-L on XL-SUM

## Executive Summary
This paper proposes SDRRL, a self-distillation method that enhances multilingual capabilities of LLMs by leveraging their stronger performance in resource-rich languages like English. Instead of relying solely on translated data, SDRRL constructs a transfer set using LLM-generated responses in the resource-rich language, translates them into target languages, and applies sentence-level knowledge distillation. The method also incorporates external parallel corpora to improve translation quality. Experiments on LLaMA-2 and SeaLLM across 14 target languages show that SDRRL significantly improves multilingual comprehension and generation tasks while preserving performance in the source language.

## Method Summary
SDRRL enhances multilingual capabilities through self-distillation from resource-rich languages. The method generates responses in English using the LLM, translates these responses to target languages using machine translation, and applies knowledge distillation to train the model. Code-switching is used to increase language diversity by randomly replacing tokens with bilingual dictionary equivalents. An external parallel corpus is incorporated to regularize the training process and mitigate noise from machine translation. The method is evaluated on LLaMA-2 and SeaLLM across 14 target languages using benchmarks including BELEBELE, FLORES, XL-SUM, and MKQA.

## Key Results
- On BELEBELE, SDRRL improves target language performance by +1.5 average accuracy
- On FLORES, SDRRL achieves up to +6.0 BLEU score improvement in both translation directions
- On XL-SUM, SDRRL gains +0.55 ROUGE-L F1 score improvement despite longer text generation challenges

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-distillation from high-resource language responses creates semantically equivalent training pairs across languages.
- **Mechanism:** The model generates responses in English (high-resource), then these are translated to target languages, creating aligned question-answer pairs in different languages that share the same meaning.
- **Core assumption:** The model's English responses contain useful semantic information that can be transferred to other languages through translation.
- **Evidence anchors:**
  - [abstract]: "constructs a transfer set using LLM-generated responses in the resource-rich language, translates them into target languages, and applies sentence-level knowledge distillation"
  - [section]: "SDRRL comprises two parts: (1) Self-Distillation: Instead of the ground-truth answer, responses from LLMs in resource-rich languages are collected to construct a transfer set."
  - [corpus]: Found 25 related papers; weak evidence - no direct citation anchor found for this specific mechanism
- **Break condition:** If machine translation quality drops below threshold τ = 0.8, or if the model's English responses are poor quality, the distillation signal becomes noisy.

### Mechanism 2
- **Claim:** Code-switching introduces token-level linguistic diversity without harming generative quality.
- **Mechanism:** Tokens in questions are randomly replaced with their bilingual dictionary equivalents with probability p = 0.15, creating mixed-language input while keeping responses intact.
- **Core assumption:** Partial token replacement maintains semantic meaning while exposing the model to cross-lingual patterns.
- **Evidence anchors:**
  - [abstract]: "We use WMT22-cometkiwi-da (Rei et al., 2022b) as a reference-free metric to assess the translation quality where the translation quality with scores below a threshold τ = 0.8 is rejected."
  - [section]: "Additionally, token-level alignment is introduced using a code-switching tool, applied only to the question components"
  - [corpus]: Weak evidence - no direct citation found for code-switching mechanism in this context
- **Break condition:** If p > 0.15 or dictionary coverage is poor, responses may become incoherent or off-target.

### Mechanism 3
- **Claim:** External parallel corpus regularization mitigates noise from machine translation.
- **Mechanism:** A small amount of clean parallel data (1,000 entries per language) is added to the training set to provide ground truth supervision and stabilize the distillation process.
- **Core assumption:** Clean parallel data can counteract the negative effects of noisy machine-translated responses.
- **Evidence anchors:**
  - [abstract]: "we employ parallel translation-based instruction data to further improve multilingual generation capabilities"
  - [section]: "we leverage a tiny external parallel corpus P = {(si, ti)L i=1} between the resource-rich language Src and the target language Tgt"
  - [corpus]: Found 25 related papers; weak evidence - no direct citation anchor found for this specific regularization mechanism
- **Break condition:** If parallel corpus is too small or too noisy, it cannot effectively regularize the distillation process.

## Foundational Learning

- **Concept:** Knowledge Distillation
  - Why needed here: The method transfers knowledge from the model's strong performance in English to its weaker performance in other languages
  - Quick check question: What is the difference between teacher-student knowledge distillation and self-distillation?

- **Concept:** Cross-Lingual Alignment
  - Why needed here: The method aims to align representation spaces across languages so that semantically equivalent content has similar embeddings
  - Quick check question: How does cross-lingual alignment differ from simple translation?

- **Concept:** Supervised Fine-Tuning (SFT)
  - Why needed here: The baseline and method both involve fine-tuning on instruction-following data, with the method extending SFT with additional techniques
  - Quick check question: What is the role of cross-entropy loss in supervised fine-tuning?

## Architecture Onboarding

- **Component map:** Base LLM (LLaMA-2 or SeaLLM) -> Machine translation (NLLB-200-3.3B) -> Quality filter (WMT22-cometkiwi-da, threshold 0.8) -> Code-switching dictionary -> External parallel corpus (1,000 entries per language) -> Training pipeline with early stopping

- **Critical path:** LLM generation → Translation → Quality filtering → Code-switching → Parallel corpus integration → Training

- **Design tradeoffs:**
  - Translation quality vs. coverage (low-resource languages may have poor translations)
  - Parallel corpus size vs. noise (small corpus may not fully regularize)
  - Code-switching probability vs. coherence (higher p increases diversity but may harm quality)
  - English vs. French as source language (English gives better gains due to stronger model performance)

- **Failure signatures:**
  - Off-target responses (model generates in wrong language)
  - Grammatical errors in target language
  - Performance degradation in English
  - Translation quality scores below 0.8 threshold

- **First 3 experiments:**
  1. Test with English as source language on a single target language (e.g., Japanese) to verify basic mechanism
  2. Test with code-switching disabled to measure its contribution
  3. Test with parallel corpus removed to measure regularization effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SDRRL perform on extremely low-resource languages not included in the current evaluation?
- Basis in paper: [inferred] The paper mentions that SDRRL may not be as effective for extremely low-resource languages without architectural modifications or additional data.
- Why unresolved: The current experiments only cover 14 target languages and do not explicitly test SDRRL on extremely low-resource languages.
- What evidence would resolve it: Experiments on a diverse set of extremely low-resource languages to measure the effectiveness of SDRRL.

### Open Question 2
- Question: What is the impact of using different machine translation systems on SDRRL's performance?
- Basis in paper: [explicit] The paper states that SDRRL relies on external machine translation systems and suggests that using LLMs for self-translation could be a promising direction.
- Why unresolved: The current experiments only use one machine translation system (NLLB-200-3.3B) and do not explore the impact of using different systems.
- What evidence would resolve it: Comparative experiments using different machine translation systems to evaluate their impact on SDRRL's performance.

### Open Question 3
- Question: How does SDRRL handle code-switching in languages with complex grammatical structures?
- Basis in paper: [explicit] The paper mentions that SDRRL uses code-switching to increase language diversity, but does not provide details on how it handles complex grammatical structures.
- Why unresolved: The paper does not provide specific examples or analysis of how SDRRL handles code-switching in languages with complex grammatical structures.
- What evidence would resolve it: Detailed analysis of SDRRL's performance on languages with complex grammatical structures, including examples of code-switching and its impact on the quality of generated responses.

## Limitations
- The method's effectiveness heavily depends on machine translation quality, with a threshold of 0.8 for accepting translations, which may not be achievable for truly low-resource languages.
- The assumption that English responses contain transferable semantic information is not empirically validated, and the magnitude of improvements varies significantly across different task types and language pairs.
- The claim about improved cross-lingual alignment of representation spaces is supported by visualizations but lacks quantitative metrics to measure alignment quality or downstream benefits.

## Confidence
- **High Confidence:** The basic self-distillation mechanism (collecting English responses and translating to target languages) is technically sound and the experimental improvements on BELEBELE (+1.5 average accuracy) and FLORES (up to +6.0 BLEU) are well-documented.
- **Medium Confidence:** The effectiveness of code-switching (p=0.15) and parallel corpus regularization is supported by ablation studies, but the optimal values for these hyperparameters are not thoroughly explored.
- **Low Confidence:** The claim about improved cross-lingual alignment of representation spaces is supported by t-SNE visualizations, but lacks quantitative metrics to measure alignment quality or downstream benefits.

## Next Checks
1. **Translation Quality Sensitivity Analysis:** Systematically vary the translation quality threshold τ from 0.6 to 0.95 and measure the impact on multilingual performance to determine if the method can tolerate lower-quality translations for truly low-resource languages.

2. **Semantic Equivalence Validation:** Conduct human evaluation studies comparing English responses to their translated counterparts to verify that the self-distillation pairs maintain semantic equivalence, particularly for languages with significant cultural or linguistic differences from English.

3. **Cross-Lingual Transfer Generalization:** Test the method's performance when using different source languages (e.g., French instead of English) and measure whether the improvements transfer proportionally or if English's dominance creates a bias in the distillation process.