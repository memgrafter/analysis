---
ver: rpa2
title: 'TLDR: Token-Level Detective Reward Model for Large Vision Language Models'
arxiv_id: '2410.04734'
source_url: https://arxiv.org/abs/2410.04734
tags:
- caption
- tldr
- language
- original
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TLDR, a token-level detective reward model
  designed to provide fine-grained, interpretable feedback for large vision-language
  models. Unlike traditional binary reward models that evaluate entire outputs, TLDR
  assigns rewards at each token, improving interpretability and grounding in images.
---

# TLDR: Token-Level Detective Reward Model for Large Vision Language Models

## Quick Facts
- arXiv ID: 2410.04734
- Source URL: https://arxiv.org/abs/2410.04734
- Authors: Deqing Fu; Tong Xiao; Rui Wang; Wang Zhu; Pengchuan Zhang; Guan Pang; Robin Jia; Lawrence Chen
- Reference count: 40
- Primary result: Token-level detective reward model improves hallucination detection and self-correction in VLMs

## Executive Summary
This paper introduces TLDR, a token-level detective reward model designed to provide fine-grained, interpretable feedback for large vision-language models. Unlike traditional binary reward models that evaluate entire outputs, TLDR assigns rewards at each token, improving interpretability and grounding in images. The authors generate synthetic hard negatives using a perturbation-based method and train TLDR models to detect hallucinations at the token level. TLDR can guide models to self-correct their outputs and speed up human annotation by 3x when fixing synthetic image captions. Experimental results show that TLDR improves the base model's performance significantly and achieves strong hallucination detection accuracy, with a 41.3 mAP(neg) on synthetic data and correlation between hallucination rates and model performance on benchmarks like MMMU (Pearson correlation 0.902).

## Method Summary
TLDR is trained on synthetic hard negatives generated by perturbing gold labels from datasets like Visual Genome and DOCCI. The model uses PaliGemma-3B-Mix-448 or Llama-3.2-11B-Vision backbone with LoRA fine-tuning on multimodal projection and decoder layers. Training optimizes cross-entropy loss on token-level labels, where each token is classified as hallucinated or grounded. The model architecture includes an image encoder, linear projection module, transformer decoder, and reward model head that maps hidden states to binary predictions via sigmoid activation. A key finding is that TLDR simultaneously improves the backbone VLM through likelihood optimization during training.

## Key Results
- Achieves 41.3 mAP(neg) on synthetic hallucination detection data
- Shows strong correlation (Pearson 0.902) between hallucination rates and model performance on MMMU benchmark
- Improves base model performance significantly through automatic likelihood optimization
- Speeds up human annotation by 3x when fixing synthetic image captions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level detective reward modeling enables fine-grained hallucination detection and correction.
- Mechanism: TLDR assigns binary rewards at each token rather than providing a single scalar score for the entire response. This granularity allows the model to identify exactly which tokens are problematic and self-correct them.
- Core assumption: Token-level supervision provides more actionable feedback than sequence-level supervision for hallucination correction.
- Evidence anchors:
  - [abstract]: "Unlike traditional binary reward models that evaluate entire outputs, TLDR assigns rewards at each token, improving interpretability and grounding in images."
  - [section]: "By evaluating and assigning rewards at each token, rather than across entire sequences, TLDR enables greater transparency."
  - [corpus]: Weak - corpus contains related work on token-level methods but lacks direct comparison to TLDR's approach.
- Break condition: If token-level granularity doesn't improve over sequence-level supervision for hallucination correction, or if token-level annotations are too noisy to be useful.

### Mechanism 2
- Claim: TLDR improves model grounding in images by reducing text-only bias.
- Mechanism: By providing token-level rewards based on both image and text inputs, TLDR prevents the reward model from developing implicit biases toward text-only information. The model learns to be more visually grounded.
- Core assumption: Naive binary reward models develop text-only biases that make them less effective in multimodal contexts.
- Evidence anchors:
  - [abstract]: "In the realm of multimodal language models, where models are required to process both images and texts, a naive reward model may learn implicit biases toward texts and become less grounded in images."
  - [section]: "We show that TLDR automatically trains a token-level likelihood optimization, and can improve the base model's performance significantly."
  - [corpus]: Weak - corpus mentions grounded concreteness but not specifically TLDR's grounding improvements.
- Break condition: If TLDR doesn't improve visual grounding compared to baseline models, or if the visual grounding improvement is negligible.

### Mechanism 3
- Claim: TLDR serves as a likelihood optimization method that automatically improves the backbone VLM.
- Mechanism: Training TLDR with binary cross-entropy objectives simultaneously updates the backbone VLM weights. The model is promoted to generate good tokens and suppressed from generating bad tokens, improving the base model's performance.
- Core assumption: Binary cross-entropy on token probabilities is equivalent to likelihood optimization for the backbone model.
- Evidence anchors:
  - [section]: "We find that, a free by-product of training the TLDR model is that the backbone model's weights are simultaneously updated together with the reward model head."
  - [section]: "Such automatic improvement is in fact that TLDR simultaneously trains the backbone VLM with likelihood optimization."
  - [corpus]: Weak - corpus contains related work on reward modeling but not specifically TLDR's likelihood optimization effects.
- Break condition: If simultaneous training doesn't improve the backbone model, or if the improvement is limited to specific tasks only.

## Foundational Learning

- Concept: Multimodal large language models (MLLMs) and their architecture
  - Why needed here: Understanding how VLMs process both images and text is crucial for grasping why TLDR's token-level approach is novel and effective
  - Quick check question: How do typical VLMs represent visual features and integrate them with textual representations?

- Concept: Reinforcement Learning from Human Feedback (RLHF) and reward modeling
  - Why needed here: TLDR is positioned as a reward model that can be used for RLHF training, so understanding RLHF basics is essential
  - Quick check question: What is the difference between sequence-level reward models and token-level reward models in the context of RLHF?

- Concept: Hallucination detection in VLMs
  - Why needed here: TLDR's primary application is hallucination detection and correction, so understanding what constitutes a hallucination is fundamental
  - Quick check question: What are the common types of hallucinations in vision-language models, and how can they be categorized?

## Architecture Onboarding

- Component map: Image and text -> Backbone VLM -> Linear projection (fproj) -> Transformer decoder (fdec) -> Reward model head (h) -> Token-level classifier (Î³) -> Binary predictions

- Critical path:
  1. Input image and text go through the backbone VLM
  2. Hidden states are extracted for each token in the response
  3. Reward model head applies linear transformation to hidden states
  4. Sigmoid activation produces token-level probabilities
  5. Thresholding (typically 0.5) produces binary predictions
  6. Cross-entropy loss computed for each token against ground truth labels

- Design tradeoffs:
  - Token-level vs sequence-level supervision: Token-level provides more granularity but requires more computational resources
  - Fixed threshold vs adaptive threshold: 0.5 is standard but may not be optimal for all tasks
  - Backbone model choice: PaliGemma vs Llama-3.2-Vision affects performance and computational requirements

- Failure signatures:
  - High false positive rate: Reward model flags too many tokens as hallucinations when they're actually correct
  - High false negative rate: Reward model misses actual hallucinations
  - Poor visual grounding: Model performs well on text-only tasks but poorly on image-grounded tasks
  - Overfitting to synthetic data: Model performs well on synthetic data but poorly on real data

- First 3 experiments:
  1. Train TLDR on synthetic data and evaluate token-level accuracy on test split to verify basic functionality
  2. Compare TLDR's hallucination detection against human annotations on WinoGround dataset to measure real-world performance
  3. Test self-correction capability by prompting VLM with TLDR guidance and measuring improvement rate compared to no guidance condition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TLDR's performance scale with larger VLMs, and what architectural changes might be needed for optimal integration with massive models?
- Basis in paper: [inferred] The paper primarily evaluates TLDR on relatively small VLMs (PaliGemma-3B and Llama-3.2-11B-Vision), but mentions GPT-4o and Llama-3.2-90B-Vision as benchmarks.
- Why unresolved: The paper doesn't explore TLDR's performance on larger, more capable VLMs or investigate necessary architectural modifications for seamless integration.
- What evidence would resolve it: Extensive experiments comparing TLDR performance across a wide range of VLM scales, with analysis of computational costs and architectural adaptations needed for larger models.

### Open Question 2
- Question: Can TLDR be extended to handle other types of hallucinations beyond object presence/absence, such as attribute hallucinations or complex relational errors?
- Basis in paper: [explicit] The paper focuses on detecting object hallucinations and mentions eight taxonomies, but primarily evaluates on object identification and spatial relationships.
- Why unresolved: The current evaluation metrics and synthetic data generation primarily target object-level hallucinations, leaving other types of hallucinations less explored.
- What evidence would resolve it: Comprehensive evaluation of TLDR on diverse hallucination types, including attribute and relational hallucinations, with corresponding improvements in detection accuracy.

### Open Question 3
- Question: How does TLDR's token-level reward compare to alternative fine-grained reward approaches, such as span-level or phrase-level rewards, in terms of effectiveness and computational efficiency?
- Basis in paper: [explicit] The paper positions TLDR as a token-level approach and briefly mentions other fine-grained methods in related work, but doesn't compare against them.
- Why unresolved: Without direct comparisons, it's unclear whether token-level rewards are the most effective or efficient approach for hallucination detection and correction.
- What evidence would resolve it: Systematic comparison of TLDR against alternative fine-grained reward models, evaluating both performance metrics and computational costs across various VLM tasks.

## Limitations
- Heavy reliance on synthetic data for training and initial evaluation, with limited testing on real-world human-annotated data
- Focus primarily on caption-based tasks, leaving uncertainty about effectiveness on more complex reasoning or open-ended generation tasks
- Perturbation-based data generation may not capture all types of hallucinations that occur in practice

## Confidence
**High Confidence Claims:**
- TLDR's architecture and training methodology are sound and reproducible
- The token-level approach provides more granular feedback than sequence-level models
- TLDR demonstrates strong performance on synthetic hallucination detection tasks
- The model shows correlation between hallucination rates and benchmark performance

**Medium Confidence Claims:**
- TLDR improves visual grounding compared to text-only reward models
- The automatic backbone improvement through likelihood optimization is significant
- TLDR's self-correction capabilities are practically useful for real-world applications

**Low Confidence Claims:**
- TLDR generalizes well to all types of multimodal tasks beyond image captioning
- The 3x speedup in human annotation is consistent across different annotation tasks
- TLDR's performance on real-world data matches its synthetic data performance

## Next Checks
1. **Real-World Generalization Test**: Evaluate TLDR on a diverse set of human-annotated multimodal datasets (beyond WinoGround) including visual question answering, image retrieval, and multimodal reasoning tasks to assess real-world performance and identify potential failure modes.

2. **Ablation Study on Visual Components**: Conduct systematic ablation experiments removing or modifying the visual components (image encoder, multimodal projection) to quantify the contribution of visual grounding to TLDR's performance and identify the minimum visual information required.

3. **Long-Term Stability Analysis**: Test TLDR's performance over extended use periods on dynamic datasets where the underlying distribution may shift, evaluating whether the model maintains its hallucination detection accuracy and whether it develops new biases over time.