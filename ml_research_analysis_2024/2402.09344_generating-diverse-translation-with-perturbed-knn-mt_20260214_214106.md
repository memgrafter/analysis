---
ver: rpa2
title: Generating Diverse Translation with Perturbed kNN-MT
arxiv_id: '2402.09344'
source_url: https://arxiv.org/abs/2402.09344
tags:
- knn-mt
- bleu
- translation
- randomize
- also
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of low diversity in machine translation
  candidate outputs, which stems from the overcorrection issue where models underestimate
  predictions differing from training data. To address this, the authors propose perturbed
  k-nearest neighbor machine translation (kNN-MT) methods that expand the kNN-MT search
  space through stochastic perturbations (noised-kNN, randomized-kNN) and deterministic
  filtering (uniquify-kNN).
---

# Generating Diverse Translation with Perturbed kNN-MT

## Quick Facts
- arXiv ID: 2402.09344
- Source URL: https://arxiv.org/abs/2402.09344
- Authors: Yuto Nishida; Makoto Morishita; Hidetaka Kamigaito; Taro Watanabe
- Reference count: 28
- Key outcome: This paper tackles the problem of low diversity in machine translation candidate outputs, which stems from the overcorrection issue where models underestimate predictions differing from training data. To address this, the authors propose perturbed k-nearest neighbor machine translation (kNN-MT) methods that expand the kNN-MT search space through stochastic perturbations (noised-kNN, randomized-kNN) and deterministic filtering (uniquify-kNN). Experiments on domain adaptation and general-domain settings across multiple language pairs show that these methods drastically improve candidate diversity (up to ~60% BLEU-based discrepancy) while maintaining translation quality (BLEU@20 and fluency). The degree of diversity can be controlled by tuning perturbation magnitude. Quantitative analysis confirms that the proposed methods alleviate overcorrection and improve diversity without lowering fluency or oracle translation quality.

## Executive Summary
This paper addresses the challenge of generating diverse translation candidates in neural machine translation by tackling the overcorrection problem, where models underestimate predictions that differ from training data. The authors propose perturbed k-nearest neighbor machine translation (kNN-MT) methods that expand the search space through stochastic perturbations (noised-kNN, randomized-kNN) and deterministic filtering (uniquify-kNN). Experiments across multiple language pairs and domain adaptation settings demonstrate that these methods significantly improve candidate diversity while maintaining translation quality. The proposed approach allows control over the degree of diversity through perturbation magnitude tuning.

## Method Summary
The proposed method builds upon k-nearest neighbor machine translation (kNN-MT) by introducing three perturbation strategies to expand the search space and improve diversity. Noised-kNN adds Gaussian noise to the kNN query vectors, randomized-kNN randomly samples from expanded neighbor sets, and uniquify-kNN removes duplicate tokens from retrieved neighbors. These perturbations are combined with diversified decoding methods (DBS, Nucleus) to generate diverse and likely translation candidates. The approach creates a datastore from training data using decoder hidden states as keys and target tokens as values, then during inference applies perturbations to the kNN search process before interpolating kNN probabilities with the base model's probabilities. The methods aim to alleviate the overcorrection problem that limits diversity by allowing the model to access alternative but valid translations.

## Key Results
- The proposed perturbed kNN-MT methods improve diversity by up to ~60% BLEU-based discrepancy while maintaining translation quality (BLEU@20 and fluency)
- Diversity can be controlled by tuning perturbation magnitude, with the methods showing consistent improvements across multiple language pairs and domain adaptation settings
- The methods alleviate overcorrection and improve diversity without lowering fluency or oracle translation quality, as confirmed by quantitative analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic perturbations in the kNN search space alleviate the overcorrection problem, which increases translation diversity.
- Mechanism: Adding noise to the kNN query vectors or randomizing the selection of neighbors expands the search space beyond the most similar tokens. This allows the model to access alternative but valid translations that are less similar to the training data, counteracting the overcorrection effect where the model underestimates predictions that differ from training data.
- Core assumption: Overcorrection directly reduces diversity by assigning low probabilities to valid but dissimilar translations.
- Evidence anchors:
  - [abstract] "The proposed methods drastically improve candidate diversity and control the degree of diversity by tuning the perturbation's magnitude."
  - [section] "We hypothesize that this problem decreases the diversity of candidates due to the low probabilities for alternative tokens assigned by the underlying model."
  - [corpus] Weak: no direct corpus evidence provided for overcorrection mechanism.
- Break condition: If the perturbations are too large, the retrieved neighbors may no longer be relevant, causing quality degradation.

### Mechanism 2
- Claim: Uniquify-kNN reduces dominance of duplicated tokens, leading to more diverse translation candidates.
- Mechanism: The datastore accumulates all target-side tokens from training data, which can lead to duplicated tokens in the k-nearest neighbors. Uniquify-kNN removes duplicates by only considering the closest instance of each unique token, preventing spuriously high probabilities for repeated tokens and allowing more diverse tokens to be considered.
- Core assumption: Duplicated tokens in kNN search create a peaky probability distribution that limits diversity.
- Evidence anchors:
  - [abstract] "uniquify-kNN (Figure 1 (3)), removes duplicates from the retrieved kNN tokens so that no token can be dominant and thus more diverse candidates remain."
  - [section] "Biased probabilities can negatively impact diversity. Since a larger datastore implies more potential for overlapped tokens, it would further degrade the diversity."
  - [corpus] Weak: no direct corpus evidence provided for uniquify mechanism.
- Break condition: If the datastore is too small or contains very few duplicates, uniquify-kNN may have minimal effect.

### Mechanism 3
- Claim: Combining kNN-MT with diversified decoding methods (DBS, Nucleus) further expands the search space and improves diversity.
- Mechanism: kNN-MT alleviates overcorrection by retrieving alternative tokens from training data. When combined with diversified decoding methods that explicitly maintain diverse candidates in the search space, the model can explore more varied translation options while maintaining quality.
- Core assumption: Diversified decoding methods can effectively maintain diversity when combined with kNN-MT's expanded search space.
- Evidence anchors:
  - [abstract] "Our methods expand the search space of kNN-MT and help incorporate diverse words into candidates by addressing the overcorrection problem."
  - [section] "Furthermore, the search space is extensively explored using a diversified decoding method to generate diverse and likely translation candidates."
  - [corpus] Weak: no direct corpus evidence provided for combined effect.
- Break condition: If diversified decoding parameters are poorly tuned, the combination may not improve diversity or could harm quality.

## Foundational Learning

- Concept: k-nearest neighbor machine translation (kNN-MT)
  - Why needed here: Understanding kNN-MT is essential because the proposed methods build upon and modify its search space.
  - Quick check question: What are the two main steps in kNN-MT and how do they work together during inference?

- Concept: Overcorrection problem in neural machine translation
  - Why needed here: The paper's proposed methods specifically target this problem as the root cause of limited diversity.
  - Quick check question: How does cross-entropy loss training contribute to the overcorrection problem?

- Concept: Diversified decoding methods (DBS, Nucleus sampling)
  - Why needed here: The proposed methods combine these with kNN-MT to further expand the search space.
  - Quick check question: How do DBS and Nucleus sampling differ in their approach to maintaining diversity during decoding?

## Architecture Onboarding

- Component map:
  - Base NMT model (Transformer) -> kNN datastore creation -> kNN search with perturbations -> probability interpolation -> diversified decoding -> candidate generation
  - Key components: datastore (key-value pairs of hidden states and tokens), kNN search algorithm, perturbation modules (noised, randomized, uniquify), diversified decoding algorithm

- Critical path:
  1. Create datastore from training data (decoder hidden states as keys, target tokens as values)
  2. During inference, for each decoding step:
     - Compute decoder hidden state as query
     - Apply perturbation (noise addition, randomization, or uniquification)
     - Retrieve k-nearest neighbors from datastore
     - Compute kNN probability and interpolate with NMT probability
     - Apply diversified decoding to maintain diverse candidates
  3. Output final diverse candidate set

- Design tradeoffs:
  - Perturbation magnitude vs. translation quality: larger perturbations increase diversity but may retrieve less relevant neighbors
  - kNN search efficiency vs. datastore size: larger datastores provide more diverse neighbors but increase search time and memory
  - Number of candidates (beam size) vs. diversity: larger beams allow more diversity but increase computation

- Failure signatures:
  - Quality degradation: hallucinations or incorrect translations appearing in diverse candidates
  - Diversity plateau: methods not improving diversity beyond baseline despite parameter tuning
  - Performance overhead: inference speed significantly slower than baseline

- First 3 experiments:
  1. Implement noised-kNN with static noise on a small dataset (e.g., IT domain) and compare DP and BLEU@20 to baseline kNN-MT
  2. Add uniquify-kNN to the noised-kNN implementation and measure the additional diversity gain
  3. Combine the perturbed kNN-MT with DBS decoding and tune the diversity strength parameter to find the optimal balance between diversity and quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between the magnitude of perturbation and the quality-diversity trade-off in kNN-MT?
- Basis in paper: [explicit] The paper states that the degree of diversity can be controlled by tuning the perturbation's magnitude, and Figure 2 shows the relationship between DP and BLEU@20 in the De-En IT domain.
- Why unresolved: The paper does not provide a quantitative analysis of the relationship between perturbation magnitude and the trade-off, only qualitative observations.
- What evidence would resolve it: A systematic study varying perturbation magnitude and measuring DP, BLEU, and DEQ across multiple domains and language pairs.

### Open Question 2
- Question: How does the uniquify-kNN method perform compared to other perturbation methods in the general domain setting?
- Basis in paper: [explicit] The paper mentions that uniquify-kNN substantially improved DP in the general domain setting, but the effect of stochastic perturbations was limited, especially on Nucleus-based methods.
- Why unresolved: The paper does not provide a detailed comparison of uniquify-kNN with other perturbation methods in the general domain setting.
- What evidence would resolve it: A comparative analysis of uniquify-kNN and other perturbation methods in the general domain setting, measuring DP, BLEU, and DEQ.

### Open Question 3
- Question: What are the computational costs of the proposed methods, and how do they scale with the size of the datastore?
- Basis in paper: [inferred] The paper mentions that kNN-MT suffers from high inference latency for kNN searches and requires much memory to load the datastore.
- Why unresolved: The paper does not provide a detailed analysis of the computational costs of the proposed methods.
- What evidence would resolve it: A thorough analysis of the computational costs of the proposed methods, including memory usage and inference time, across different datastore sizes.

### Open Question 4
- Question: How do the proposed methods perform in end-to-end applications, such as human post-editing or reranking?
- Basis in paper: [explicit] The paper mentions that the benefit in end-applications remains unclear and suggests that the benefit can be shown by measuring the performance after using a reranking method such as quality-aware decoding.
- Why unresolved: The paper does not provide any experiments on end-to-end applications.
- What evidence would resolve it: Experiments evaluating the proposed methods in end-to-end applications, such as human post-editing or reranking, and measuring the impact on final translation quality.

## Limitations

- The core hypothesis that overcorrection directly causes low diversity is primarily theoretical with limited direct empirical validation
- The perturbation methods introduce additional hyperparameters that require careful tuning and may not generalize well across different domains
- The uniquify-kNN method's effectiveness depends on the presence of duplicate tokens in the datastore, which varies by training corpus
- The paper lacks extensive analysis of computational overhead and inference speed trade-offs

## Confidence

- High Confidence: The claim that perturbed kNN-MT methods can improve translation diversity is well-supported by experimental results across multiple datasets and language pairs
- Medium Confidence: The claim that these methods maintain translation quality while improving diversity is supported but requires more nuanced interpretation
- Low Confidence: The claim that overcorrection is the primary mechanism limiting diversity is based on theoretical reasoning and indirect evidence

## Next Checks

1. **Mechanism Validation Experiment**: Design an experiment to directly test whether the diversity improvements are specifically due to addressing overcorrection rather than general search space expansion. This could involve comparing the perturbed kNN-MT methods against a control condition where perturbations are applied but without the overcorrection mechanism (e.g., using a model trained with different loss functions).

2. **Cross-Domain Robustness Analysis**: Evaluate the proposed methods on a broader range of domain adaptation scenarios, particularly focusing on domains with varying degrees of training data overlap. This would help determine whether the methods are robust to different levels of overcorrection and whether the perturbation hyperparameters need to be adapted for different domain pairs.

3. **Quality-Diversity Trade-off Characterization**: Conduct a more granular analysis of the quality-diversity trade-off by examining individual candidate pairs rather than aggregate metrics. This could involve human evaluation of diverse candidates to determine whether quality degradation occurs at the individual candidate level, even if aggregate metrics remain stable.