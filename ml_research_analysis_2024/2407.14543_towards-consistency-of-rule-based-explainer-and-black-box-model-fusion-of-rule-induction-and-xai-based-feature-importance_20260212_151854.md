---
ver: rpa2
title: Towards consistency of rule-based explainer and black box model -- fusion of
  rule induction and XAI-based feature importance
arxiv_id: '2407.14543'
source_url: https://arxiv.org/abs/2407.14543
tags:
- rule
- rule-based
- black
- data
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of verifying whether rule-based
  surrogate models accurately mimic the decision-making process of black-box models
  they approximate. It proposes a novel method that integrates rule induction with
  feature importance rankings from XAI methods (e.g., SHAP, Permutation Feature Importance)
  to generate rule-based explanations that are more consistent with the underlying
  black-box model.
---

# Towards consistency of rule-based explainer and black box model -- fusion of rule induction and XAI-based feature importance

## Quick Facts
- **arXiv ID**: 2407.14543
- **Source URL**: https://arxiv.org/abs/2407.14543
- **Reference count**: 40
- **Primary result**: Importance-based feature ordering improves consistency of rule-based explanations with black-box models (inclusion 0.68-0.78, correlation 0.68-0.78)

## Executive Summary
This paper addresses the challenge of ensuring that rule-based surrogate models accurately represent the decision-making process of black-box models. The authors propose a novel method that combines rule induction with feature importance rankings from XAI techniques to generate rule-based explanations that better align with the underlying black-box model's reasoning. By prioritizing features based on their importance scores, the method produces more consistent and interpretable explanations compared to traditional rule induction approaches.

## Method Summary
The proposed method integrates rule induction with feature importance rankings from XAI methods to generate rule-based explanations. The approach uses a sequential covering algorithm where at each step, it selects the most important features based on XAI-generated importance scores. These features are then used to generate rules that cover instances from the training set. The process iterates until all instances are covered or a stopping criterion is met. The method was evaluated using three types of black-box models (XGB, SVM, RF) across 30 tabular datasets, measuring consistency through inclusion and correlation metrics.

## Key Results
- Using importance-based feature ordering significantly improves consistency metrics (inclusion: 0.68-0.78, correlation: 0.68-0.78) compared to methods without feature importance
- The proposed approach outperformed the Anchors method in maintaining consistency between rule-based explanations and black-box models
- Case study demonstrated that the method produces more interpretable and accurate explanations by focusing on the most relevant features

## Why This Works (Mechanism)
The method works by leveraging feature importance rankings from XAI methods to guide the rule induction process. By prioritizing features that the black-box model considers most important, the generated rules are more likely to capture the same decision logic as the original model. This creates a feedback loop where the explanation mechanism aligns more closely with the model's internal reasoning process.

## Foundational Learning
- **XAI methods (SHAP, Permutation Feature Importance)**: Why needed - to quantify feature importance for guiding rule generation; Quick check - ensure importance scores are stable across different model runs
- **Rule induction algorithms**: Why needed - to create interpretable explanations from feature importance rankings; Quick check - verify rules cover sufficient portion of training data
- **Consistency metrics (inclusion, correlation)**: Why needed - to measure alignment between rule-based and black-box explanations; Quick check - compare metrics across different feature ordering strategies

## Architecture Onboarding
**Component map**: Black-box model -> XAI feature importance -> Rule induction -> Rule-based explanations

**Critical path**: The XAI feature importance step is critical as it determines which features are prioritized in rule generation. Without accurate importance rankings, the method cannot effectively align rules with black-box reasoning.

**Design tradeoffs**: The method trades some rule interpretability for improved consistency with black-box models. More complex feature interactions may be simplified to maintain rule clarity.

**Failure signatures**: Poor feature importance rankings from XAI methods will cascade into misaligned rules. The method may struggle with datasets where feature importance is highly unstable or where black-box models use complex feature interactions.

**First experiments**: 
1. Compare consistency metrics when using random vs. XAI-based feature ordering
2. Evaluate rule coverage and accuracy across different black-box model types
3. Test sensitivity to XAI method choice (SHAP vs. Permutation vs. others)

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to tabular datasets, limiting generalizability to other data types
- Comparison with Anchors method doesn't address trade-offs with explanation fidelity or runtime efficiency
- Only three black-box models (XGB, SVM, RF) and three XAI methods were tested
- Case study provides qualitative evidence but lacks systematic human evaluation of interpretability improvements

## Confidence
- **Improved consistency metrics**: High confidence (directly measured)
- **Improved interpretability**: Low confidence (based primarily on single case study)
- **Generalizability**: Medium confidence (results on 30 datasets but limited model types)

## Next Checks
1. Evaluate the method on non-tabular datasets (images, text) to assess generalizability beyond the tested domain
2. Conduct ablation studies to determine the individual contributions of different XAI methods to the consistency improvements
3. Perform runtime efficiency analysis comparing the proposed method with baseline approaches across varying dataset sizes and complexities