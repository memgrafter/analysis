---
ver: rpa2
title: Continuous-time Riemannian SGD and SVRG Flows on Wasserstein Probabilistic
  Space
arxiv_id: '2401.13530'
source_url: https://arxiv.org/abs/2401.13530
tags:
- riemannian
- svrg
- space
- gradient
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops continuous optimization methods for minimizing
  Kullback-Leibler divergence in Wasserstein space, extending Riemannian stochastic
  gradient descent (SGD) and stochastic variance reduction gradient (SVRG) flows.
  The authors construct stochastic differential equations to approximate discrete
  optimization dynamics, then derive the corresponding probability measure flows using
  the Fokker-Planck equation.
---

# Continuous-time Riemannian SGD and SVRG Flows on Wasserstein Probabilistic Space

## Quick Facts
- arXiv ID: 2401.13530
- Source URL: https://arxiv.org/abs/2401.13530
- Authors: Mingyang Yi; Bohan Wang
- Reference count: 40
- Primary result: Continuous optimization methods for minimizing KL divergence in Wasserstein space with theoretical convergence rates

## Executive Summary
This paper develops continuous-time optimization methods for minimizing Kullback-Leibler divergence in Wasserstein space by extending Riemannian stochastic gradient descent (SGD) and stochastic variance reduction gradient (SVRG) flows. The authors construct stochastic differential equations to approximate discrete optimization dynamics and derive the corresponding probability measure flows using the Fokker-Planck equation. They establish convergence rates for both non-convex and log-Sobolev (Riemannian PL-inequality) settings, demonstrating O(1/√T) and O(N^(2/3)/T) rates for SGD and SVRG flows respectively in the non-convex case, with improved exponential rates under the log-Sobolev condition.

## Method Summary
The paper introduces continuous-time Riemannian SGD and SVRG flows by constructing stochastic differential equations that approximate discrete optimization dynamics in Wasserstein space. The authors derive probability measure flows using the Fokker-Planck equation and establish convergence rates for both non-convex and log-Sobolev settings. The methods connect theoretical continuous-time analysis with practical sampling processes and demonstrate computational complexity improvements over existing approaches through rigorous mathematical frameworks.

## Key Results
- Establishes O(1/√T) and O(N^(2/3)/T) convergence rates for Riemannian SGD and SVRG flows in non-convex settings
- Proves improved exponential convergence rates under log-Sobolev (Riemannian PL-inequality) conditions
- Demonstrates theoretical computational complexity improvements over existing optimization methods in Wasserstein space

## Why This Works (Mechanism)
The continuous-time framework allows for more precise analysis of optimization dynamics by leveraging the smoothness of stochastic differential equations and the Fokker-Planck equation. This approach captures the underlying probability measure evolution more accurately than discrete-time methods, enabling tighter convergence bounds. The connection to Wasserstein geometry provides natural geometric structures for analyzing the optimization landscape, while the stochastic differential equation formulation naturally incorporates noise effects in the optimization process.

## Foundational Learning

**Wasserstein Space**: A space of probability measures equipped with optimal transport distance. *Why needed*: Provides the geometric framework for analyzing probability measure flows. *Quick check*: Verify understanding of Wasserstein-2 distance properties and optimal transport.

**Fokker-Planck Equation**: A partial differential equation describing the time evolution of probability density functions. *Why needed*: Connects stochastic differential equations to probability measure flows. *Quick check*: Confirm ability to derive the Fokker-Planck equation from a given SDE.

**Log-Sobolev Inequality**: A functional inequality relating entropy and energy functionals. *Why needed*: Enables exponential convergence proofs in continuous-time optimization. *Quick check*: Verify understanding of how log-Sobolev conditions imply exponential convergence.

**Riemannian PL-inequality**: A generalization of the Polyak-Łojasiewicz condition to Riemannian manifolds. *Why needed*: Provides conditions for fast convergence in non-convex settings. *Quick check*: Confirm ability to verify Riemannian PL-conditions in specific manifolds.

**Stochastic Differential Equations**: Differential equations with random components. *Why needed*: Model the continuous-time optimization dynamics. *Quick check*: Verify understanding of Ito calculus and SDE solution properties.

## Architecture Onboarding

**Component Map**: SDE Formulation -> Fokker-Planck Derivation -> Convergence Analysis -> Rate Bounds
**Critical Path**: Construct SDE → Apply Fokker-Planck → Prove convergence rates → Verify conditions
**Design Tradeoffs**: Continuous-time analysis provides tighter bounds but requires strong regularity conditions; discrete implementations may lose theoretical guarantees
**Failure Signatures**: Convergence rates degrade when log-Sobolev conditions fail; numerical instability in solving SDEs for high-dimensional problems
**First Experiments**: 
1. Implement numerical simulation comparing continuous-time Riemannian SGD/SVRG with discrete counterparts
2. Test convergence on simple non-convex functions in Wasserstein space
3. Validate exponential convergence under controlled log-Sobolev conditions

## Open Questions the Paper Calls Out

None

## Limitations

- Strong theoretical assumptions (log-Sobolev, Riemannian PL-inequalities) may not hold in practical applications
- Continuous-time approximations may not accurately capture discrete optimization dynamics in all scenarios
- Computational complexity improvements are theoretical and may not translate to practical implementations due to numerical challenges in high-dimensional Wasserstein spaces

## Confidence

- **High** confidence in theoretical convergence rates under log-Sobolev conditions
- **Medium** confidence in non-convex convergence rates due to dependence on specific geometric conditions
- **Low** confidence in practical computational complexity improvements until numerical validation

## Next Checks

1. Implement numerical simulations comparing continuous-time Riemannian SGD/SVRG flows with discrete counterparts to verify theoretical complexity improvements
2. Test the methods on practical machine learning problems where log-Sobolev conditions may not be strictly satisfied
3. Analyze the discretization error when converting continuous-time flows to practical algorithms, measuring the impact on convergence rates