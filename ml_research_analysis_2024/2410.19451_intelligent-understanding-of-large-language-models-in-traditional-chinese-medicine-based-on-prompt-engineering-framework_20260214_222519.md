---
ver: rpa2
title: Intelligent Understanding of Large Language Models in Traditional Chinese Medicine
  Based on Prompt Engineering Framework
arxiv_id: '2410.19451'
source_url: https://arxiv.org/abs/2410.19451
tags:
- language
- large
- tasks
- medicine
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TCM-Prompt, a framework integrating prompt
  engineering with large language models (LLMs) to improve their performance on Traditional
  Chinese Medicine (TCM) tasks. The framework combines pre-trained language models,
  templates, tokenization, and verbalization methods, enabling modular construction
  of domain-specific models.
---

# Intelligent Understanding of Large Language Models in Traditional Chinese Medicine Based on Prompt Engineering Framework

## Quick Facts
- arXiv ID: 2410.19451
- Source URL: https://arxiv.org/abs/2410.19451
- Reference count: 3
- TCM-Prompt framework achieves 1.85% to 19.99% accuracy improvements over baselines

## Executive Summary
This paper introduces TCM-Prompt, a novel framework that leverages prompt engineering to enhance the performance of large language models on Traditional Chinese Medicine tasks. The framework integrates pre-trained language models with domain-specific templates, tokenization strategies, and verbalization methods to create modular, task-specific models. Experiments demonstrate significant improvements across multiple TCM applications including disease classification, syndrome identification, and herbal medicine recommendation, with the best results achieved through combined use of prefix tuning, soft prompting, and template-based approaches.

## Method Summary
The TCM-Prompt framework operates through a modular architecture that combines domain-specific knowledge with prompt engineering techniques. It integrates pre-trained language models with customized templates designed for TCM terminology and concepts, employs specialized tokenization methods to handle TCM-specific vocabulary, and uses verbalization techniques to convert model outputs into clinically meaningful responses. The framework allows for flexible construction of domain-specific models by combining these components, with particular emphasis on prefix tuning and soft prompting strategies to optimize performance on TCM tasks while maintaining efficiency.

## Key Results
- TCM-Prompt achieves accuracy improvements of 1.85% to 19.99% across four benchmark tasks
- Best performance obtained through combination of prefix tuning, soft prompting, and template-based prompting
- Framework demonstrates effectiveness across diverse TCM applications including disease classification, syndrome identification, and herbal medicine recommendation

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to bridge the gap between general-purpose LLMs and specialized TCM domain knowledge. By incorporating domain-specific templates and vocabulary, the framework enables LLMs to better understand and process TCM concepts that may be unfamiliar or ambiguously represented in standard training data. The modular design allows for task-specific optimization while maintaining the underlying model's capabilities, and the combination of different prompting strategies provides flexibility to adapt to various TCM sub-domains and task requirements.

## Foundational Learning

1. **Prompt Engineering Fundamentals**
   - *Why needed*: Essential for understanding how to structure inputs to guide LLM behavior
   - *Quick check*: Can design basic prompts for simple classification tasks

2. **Traditional Chinese Medicine Terminology**
   - *Why needed*: Domain-specific vocabulary crucial for effective template design
   - *Quick check*: Familiarity with basic TCM concepts like syndromes and herbal medicine classifications

3. **Prefix Tuning and Soft Prompting**
   - *Why needed*: Core techniques used in TCM-Prompt for model optimization
   - *Quick check*: Understanding of parameter-efficient fine-tuning methods

4. **Tokenization Strategies**
   - *Why needed*: Critical for handling domain-specific vocabulary in LLMs
   - *Quick check*: Can explain differences between standard and domain-specific tokenization

5. **Modular Framework Design**
   - *Why needed*: Understanding the architecture enables customization and extension
   - *Quick check*: Can map components to their respective functions in the pipeline

6. **Template-Based Prompting**
   - *Why needed*: Primary method for incorporating domain knowledge into prompts
   - *Quick check*: Can create effective templates for specific TCM tasks

## Architecture Onboarding

**Component Map:**
Pre-trained LLM -> Prompt Templates -> Tokenization Module -> Prefix Tuning Layer -> Soft Prompting Layer -> Verbalization Output

**Critical Path:**
The critical path flows from input through template application, tokenization, and both tuning layers before reaching the final verbalization output. Performance bottlenecks typically occur at the tokenization and prompt template stages, particularly when handling complex TCM terminology.

**Design Tradeoffs:**
The framework balances between model complexity and task specificity. While more complex prompt combinations yield better results, they also increase computational overhead. The modular design allows for selective component activation based on task requirements and available resources.

**Failure Signatures:**
Common failures include template misalignment with domain concepts, tokenization errors for specialized TCM terms, and overfitting when prefix tuning is too aggressive. Performance degradation often manifests as reduced accuracy on specific TCM sub-domains rather than uniform decline across all tasks.

**First 3 Experiments:**
1. Baseline comparison using standard LLM without any TCM-specific modifications
2. Template-only implementation to isolate the impact of domain-specific prompt structure
3. Full TCM-Prompt implementation with all components active to establish maximum performance potential

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Performance improvements primarily demonstrated against basic baseline models rather than competing prompt engineering approaches
- Limited evaluation metrics (accuracy only) without comprehensive assessment using precision, recall, or F1-score
- Lack of discussion regarding generalization across different TCM subdomains and prompt template retraining requirements

## Confidence

**High Confidence:**
- Modular framework architecture and basic functionality
- Effectiveness of combined prompting strategies (prefix tuning + soft prompting + templates)

**Medium Confidence:**
- Performance improvements on benchmark tasks
- Framework's adaptability across different TCM applications

**Low Confidence:**
- Claims about superiority over alternative approaches
- Practical deployment feasibility without additional validation

## Next Checks
1. Compare TCM-Prompt against other prompt engineering frameworks and established TCM-specific LLMs on the same benchmark tasks to establish relative performance
2. Conduct ablation studies to determine which components (prefix tuning, soft prompting, templates) contribute most to performance gains
3. Evaluate the framework's performance across a broader range of TCM tasks and assess whether prompts require task-specific retraining for optimal results