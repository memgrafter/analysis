---
ver: rpa2
title: 'HJ-Ky-0.1: an Evaluation Dataset for Kyrgyz Word Embeddings'
arxiv_id: '2411.10724'
source_url: https://arxiv.org/abs/2411.10724
tags:
- word
- dataset
- kyrgyz
- language
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HJ-Ky-0.1, the first evaluation dataset for
  Kyrgyz word embeddings, created by translating the Russian HJ dataset into Kyrgyz.
  The dataset contains 361 word pairs with similarity scores.
---

# HJ-Ky-0.1: an Evaluation Dataset for Kyrgyz Word Embeddings

## Quick Facts
- arXiv ID: 2411.10724
- Source URL: https://arxiv.org/abs/2411.10724
- Reference count: 38
- Key outcome: HJ-Ky-0.1, the first evaluation dataset for Kyrgyz word embeddings, shows FastText models trained on Kyrgyz text consistently outperform Russian embeddings and word2vec.

## Executive Summary
This paper introduces HJ-Ky-0.1, the first evaluation dataset for Kyrgyz word embeddings, created by translating the Russian HJ dataset into Kyrgyz. The dataset contains 361 word pairs with similarity scores. The authors trained and evaluated several word embedding models (word2vec and fastText) on Kyrgyz text data and compared them with pre-trained Russian models. Quality was assessed using Spearman's and Pearson's correlation coefficients. FastText models trained on Kyrgyz text consistently outperformed Russian embeddings and word2vec, validating the dataset's suitability. The results suggest that HJ-Ky-0.1 is a reliable resource for evaluating and improving Kyrgyz word embedding methods.

## Method Summary
The HJ-Ky-0.1 dataset was created by manually translating word pairs from the Russian HJ dataset, maintaining expert-assessed similarity scores. The authors trained word2vec Skip-Gram Negative Sampling and fastText Skip-Gram models with 100/300 dimensions, window size 5, 5/10 negative samples, and 10 epochs on Kyrgyz text data. Models were evaluated using Spearman's rank correlation and Pearson's correlation coefficient between human similarity scores and cosine similarity of embeddings. Pre-trained Russian fastText embeddings and models trained on CommonCrawl and Leipzig Corpus were compared.

## Key Results
- FastText models trained on Kyrgyz text consistently outperformed Russian embeddings and word2vec
- The Leipzig Corpus-trained fastText model outperformed the CommonCrawl-trained model
- The HJ-Ky-0.1 dataset proved suitable for evaluating Kyrgyz word embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translation from a high-resource language dataset preserves semantic similarity judgments for low-resource evaluation.
- Mechanism: The HJ-Ky-0.1 dataset was created by manually translating word pairs from the Russian HJ dataset, maintaining expert-assessed similarity scores. This "silver standard" approach leverages existing high-quality judgments to bootstrap evaluation for Kyrgyz.
- Core assumption: Semantic similarity judgments are largely transferable across languages when translation preserves word sense and context.
- Evidence anchors:
  - [abstract] "The dataset contains 361 word pairs with similarity scores... translated from Russian HJ dataset"
  - [section 4] "words in any language can have multiple meanings (senses), and in Kyrgyz, each sense may correspond to a distinct word... preference was given to the translation closest in meaning to the second word in the pair"
  - [corpus] Weak evidence - no direct neighbor citations, but related papers exist for Kyrgyz NER and BERT models
- Break condition: Translation introduces semantic drift, or target language has significantly different word sense distributions that don't align with source language judgments.

### Mechanism 2
- Claim: FastText embeddings outperform word2vec on morphologically rich languages like Kyrgyz.
- Mechanism: FastText represents words as sums of character n-gram vectors, making it robust to morphological variation and out-of-vocabulary words. This is particularly important for agglutinative languages.
- Core assumption: Morphological richness requires subword modeling for effective embeddings.
- Evidence anchors:
  - [abstract] "FastText models trained on Kyrgyz text consistently outperformed Russian embeddings and word2vec"
  - [section 5] "fastText, each word is represented as a combination of character n-grams, summing these to compute the final vector representation. This approach is less sensitive to vocabulary and word normalization"
  - [corpus] No direct citations, but neighboring papers discuss morphological segmentation strategies
- Break condition: If language morphology is simpler or if character n-gram hashing collisions dominate representation quality.

### Mechanism 3
- Claim: Corpus quality (Leipzig vs CommonCrawl) significantly impacts embedding performance.
- Mechanism: Clean, curated corpora produce better embeddings than automatically collected web data, as evidenced by fastText models trained on Leipzig corpus outperforming those trained on CommonCrawl.
- Core assumption: Noise and domain mismatch in web-crawled data degrades embedding quality.
- Evidence anchors:
  - [section 5] "fastText model trained on the Leipzig Corpus (fastText, Skip-Gram, Leipzig, 100) outperformed the CommonCrawl-Ky model, even without hyperparameter tuning"
  - [section 5] "This suggests that the Leipzig Corpus is cleaner and more suitable than the automatically collected CommonCrawl dataset, which may include mislabeled or noisy text"
  - [corpus] Weak evidence - no direct citations about corpus quality, but neighboring papers discuss data challenges
- Break condition: If corpus size differences dominate quality effects, or if downstream task requires web-style text patterns.

## Foundational Learning

- Concept: Distributional semantics and the distributional hypothesis
  - Why needed here: The entire word embedding approach relies on the principle that words appearing in similar contexts share semantic properties
  - Quick check question: If "king" and "queen" appear in similar contexts (e.g., "royal family," "crown"), what should their embedding vectors look like relative to each other?

- Concept: Evaluation metrics for word similarity tasks
  - Why needed here: The paper uses Spearman's rank correlation and Pearson's correlation coefficient to validate the dataset by comparing human similarity scores with cosine similarities from embeddings
  - Quick check question: If human annotators rate word pairs (1,2,3) with scores (0.9, 0.8, 0.1) and an embedding model produces cosine similarities (0.95, 0.75, 0.05), what would Spearman's correlation measure?

- Concept: Morphological analysis and stemming in agglutinative languages
  - Why needed here: The paper discusses the impact of "rough" stemming on embedding quality, showing that aggressive morphological reduction can degrade performance
  - Quick check question: In Kyrgyz, the word "китептерди" (books, accusative) could be stemmed to what root form, and what information might be lost in this process?

## Architecture Onboarding

- Component map: Dataset creation → Translation pipeline → Embedding training (word2vec/fastText) → Quality evaluation (correlation metrics) → Model comparison
- Critical path: Translation quality → Embedding training quality → Evaluation metric reliability
- Design tradeoffs: Manual translation provides semantic accuracy but limits dataset size; automatic translation would scale but risk semantic drift; stemming reduces vocabulary size but loses morphological information
- Failure signatures: Poor translation choices show up as low correlation between human and embedding similarities; stemming issues manifest as missing word representations or degraded correlations; corpus quality problems appear as embeddings that don't capture semantic relationships
- First 3 experiments:
  1. Train word2vec and fastText on a small, clean Kyrgyz corpus and evaluate on HJ-Ky-0.1 to establish baseline performance differences
  2. Apply different preprocessing (tokenization vs stemming) to the same corpus and measure impact on embedding quality scores
  3. Compare embeddings trained on different corpora (Leipzig vs CommonCrawl) to quantify corpus quality effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal stemming approach for morphologically rich languages like Kyrgyz in word embedding training?
- Basis in paper: [explicit] The authors note that their "rough" stemming approach caused significant information loss and excluded certain words from the dataset
- Why unresolved: The paper only tested one basic stemming approach and observed quality degradation, but didn't explore alternative stemming techniques or their impact on embedding quality
- What evidence would resolve it: Systematic comparison of different stemming/lemmatization approaches (e.g., more sophisticated morphological analyzers) on embedding quality metrics using the HJ-Ky-0.1 dataset

### Open Question 2
- Question: How do manually annotated word similarity scores compare to the translated HJ-Ky-0.1 dataset scores?
- Basis in paper: [explicit] The authors state they plan to improve the dataset by "manually re-annotating the word pairs with expert ratings provided by native speakers"
- Why unresolved: The current dataset was created through translation rather than direct human annotation, which may have altered similarity perceptions
- What evidence would resolve it: Direct comparison between the translated scores and scores from native speakers annotating the Kyrgyz word pairs independently

### Open Question 3
- Question: Does training on cleaner corpora (like Leipzig) consistently outperform training on larger but noisier web data (like CommonCrawl) across different embedding architectures?
- Basis in paper: [explicit] The authors observed that fastText trained on Leipzig Corpus outperformed CommonCrawl-trained models even without hyperparameter tuning
- Why unresolved: The comparison was limited to one model type and corpus pair; it's unclear if this pattern holds for other embedding methods or corpus combinations
- What evidence would resolve it: Systematic evaluation of multiple embedding architectures (word2vec, GloVe, fastText) trained on various corpora with different cleanliness levels and sizes using HJ-Ky-0.1 as evaluation benchmark

## Limitations
- Dataset size is relatively small (361 word pairs), limiting statistical power for detecting subtle performance differences
- Manual translation process introduces potential subjectivity in word sense selection and semantic judgment transfer
- Limited comparison scope - only word2vec and fastText models evaluated, no comparison with transformer-based approaches
- Single-source corpus quality assessment without systematic comparison across multiple corpus types

## Confidence
- High confidence: FastText consistently outperforms word2vec on Kyrgyz embeddings
- Medium confidence: HJ-Ky-0.1 is suitable for evaluating Kyrgyz word embeddings
- Low confidence: Leipzig Corpus is definitively cleaner than CommonCrawl

## Next Checks
1. Expand HJ-Ky-0.1 to 1000+ word pairs using semi-automated translation with human validation to improve statistical power
2. Evaluate modern transformer-based embeddings (BERT, RoBERTa) alongside traditional word2vec/fastText to establish comprehensive baseline performance
3. Conduct ablation studies with different preprocessing pipelines (tokenization, stemming, lemmatization) to quantify their impact on embedding quality across multiple evaluation metrics