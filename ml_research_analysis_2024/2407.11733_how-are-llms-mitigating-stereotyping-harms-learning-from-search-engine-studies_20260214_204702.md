---
ver: rpa2
title: How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies
arxiv_id: '2407.11733'
source_url: https://arxiv.org/abs/2407.11733
tags:
- prompt
- arxiv
- language
- refusal
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study audits stereotyping harms in seven large language models\
  \ using autocomplete-style prompts across 170+ social groups. The authors apply\
  \ four metrics\u2014refusal rates, toxicity, sentiment, and regard\u2014with and\
  \ without safety system prompts."
---

# How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies
## Quick Facts
- arXiv ID: 2407.11733
- Source URL: https://arxiv.org/abs/2407.11733
- Reference count: 40
- Key outcome: Audits seven LLMs across 170+ social groups using autocomplete-style prompts, finding that safety system prompts improve but don't eliminate stereotyping harms

## Executive Summary
This study evaluates stereotyping harms in large language models by auditing seven models using autocomplete-style prompts across more than 170 social groups. The researchers apply four metrics—refusal rates, toxicity, sentiment, and regard—both with and without safety system prompts to assess how different models handle sensitive topics. The findings reveal that while safety interventions can improve outcomes, they do not fully mitigate stereotyping harms, with intersectional identities experiencing the most severe stereotyping.

The study identifies significant variation in model performance, with Llama-2 showing the highest refusal rates, Falcon producing the most toxicity, and Starling achieving the most positive sentiment and regard. Notably, the research demonstrates that removing chat templates dramatically increases toxic outputs, suggesting that interface design plays a crucial role in mitigating harmful content. The authors call for improved bias evaluation in LLM leaderboards and more nuanced moderation policies.

## Method Summary
The researchers conducted a systematic audit of seven large language models using autocomplete-style prompts across 170+ social groups. They evaluated model outputs using four metrics: refusal rates (whether models declined to answer), toxicity (using Perspective API), sentiment (positive vs negative), and regard (how positively the content portrays the social group). The study tested each model under two conditions: with and without a safety system prompt, and also examined the impact of removing chat templates. This approach allowed them to isolate the effects of different safety interventions and identify which social groups and model architectures were most susceptible to stereotyping harms.

## Key Results
- Llama-2 achieves the highest refusal rates when encountering sensitive social group prompts
- Falcon generates the most toxic outputs across tested social groups
- Starling model achieves the highest positive sentiment and regard scores
- Safety system prompts improve outcomes but fail to fully eliminate stereotyping harms
- Removing chat templates significantly increases toxic outputs across all models

## Why This Works (Mechanism)
This audit methodology works by systematically exposing LLMs to sensitive social group prompts in a controlled manner, allowing researchers to measure how different models handle potentially harmful content. The approach leverages the autocomplete-style prompts to simulate real-world search scenarios where users might encounter biased information. By applying multiple metrics simultaneously, the study captures different dimensions of stereotyping harm—from explicit refusals to subtle negative regard. The comparison between models with and without safety interventions provides clear evidence of what safety measures actually achieve versus their limitations.

## Foundational Learning
- Social group representation in LLMs: Why needed - Understanding how different social groups are portrayed is fundamental to assessing bias; Quick check - Audit coverage should span diverse demographic categories and intersectionality
- Safety intervention effectiveness: Why needed - Determines whether current moderation approaches actually work; Quick check - Compare model outputs with and without safety prompts across multiple metrics
- Toxicity detection metrics: Why needed - Provides standardized way to measure harmful content; Quick check - Use established APIs like Perspective API for consistency
- Intersectionality in bias studies: Why needed - Single-axis analysis misses compounded discrimination; Quick check - Include combinations of social identities in prompt design
- Interface design impact: Why needed - Shows how presentation affects content generation; Quick check - Test with and without chat templates to isolate effect
- Multilingual generalization: Why needed - English results may not apply globally; Quick check - Replicate study in multiple languages

## Architecture Onboarding
The audit system follows this critical path: Prompt Generation -> Model Input -> Output Generation -> Metric Application -> Safety Intervention Comparison. The key design tradeoff involves balancing prompt sensitivity (to elicit stereotypes) against ethical concerns about generating harmful content. Critical components include the autocomplete prompt template, the four evaluation metrics, and the safety system prompt framework.

Failure signatures include: refusal cascades (excessive declines), toxicity spikes (especially for certain social groups), sentiment polarization (extreme positive/negative responses), and intersectional amplification (disproportionate harm to combined identities). The first three experiments should test: 1) Baseline model outputs without any interventions, 2) Safety prompt effectiveness across different social groups, and 3) Chat template impact on output toxicity.

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Restricted scope of 170+ social groups may miss emerging or less common identities
- Autocomplete-style prompts may systematically bias how stereotypes are elicited
- Four metrics may not capture nuanced forms of stereotyping harms
- English-language focus limits generalizability to other linguistic contexts
- Different model architectures and training approaches introduce confounding variables

## Confidence
- Model-specific performance rankings: Medium confidence (controlled conditions but confounding variables)
- Safety prompts improve but don't eliminate harms: High confidence (tested across multiple models)
- Intersectional identities experience more harm: Medium confidence (limited intersectional combinations tested)

## Next Checks
1. Expand social group coverage to include additional intersectional combinations and emerging identity categories, then replicate the audit to assess whether relative model performance rankings remain stable
2. Conduct multilingual replications using the same prompt templates in at least three non-English languages to evaluate whether observed patterns generalize across linguistic contexts
3. Implement complementary qualitative analysis where human annotators code model outputs for subtle stereotyping forms (microaggressions, coded language) not captured by automated metrics