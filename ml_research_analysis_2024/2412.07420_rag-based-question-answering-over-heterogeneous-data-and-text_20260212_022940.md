---
ver: rpa2
title: RAG-based Question Answering over Heterogeneous Data and Text
arxiv_id: '2412.07420'
source_url: https://arxiv.org/abs/2412.07420
tags:
- evidence
- question
- answer
- quasar
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Quasar is a RAG-based QA system that jointly taps into text, tables,
  and knowledge graphs. It uniquely combines question understanding to extract structured
  intent, iterative re-ranking to distill evidence, and answer generation via a fine-tuned
  LLM.
---

# RAG-based Question Answering over Heterogeneous Data and Text

## Quick Facts
- arXiv ID: 2412.07420
- Source URL: https://arxiv.org/abs/2412.07420
- Reference count: 40
- Primary result: Quasar achieves 75.4% P@1 on temporal QA and 56.4% on CompMix using fine-tuned Llama-3.1-8B-Instruct

## Executive Summary
Quasar is a retrieval-augmented generation (RAG) system designed for question answering over heterogeneous data sources including text, tables, and knowledge graphs. The system uniquely combines structured intent decomposition, iterative re-ranking and filtering of evidence, and answer generation via a fine-tuned language model. Experiments on three benchmarks (CompMix, Crag, TimeQuestions) demonstrate competitive or state-of-the-art performance, with Quasar outperforming larger GPT models while using fewer parameters and lower computational cost.

## Method Summary
Quasar implements a RAG pipeline with four stages: Question Understanding (QU) extracts structured intent from questions using BART, Evidence Retrieval (ER) searches heterogeneous sources via Clocq for KGs and BM25 for text/tables, Re-Ranking and Filtering (RF) applies GNN or cross-encoder models to iteratively reduce top-1000 evidence to top-30, and Answer Generation (AG) uses a fine-tuned Llama-3.1-8B-Instruct model to generate answers grounded in the filtered evidence. The system is trained on benchmarks including Wikidata KG, Wikipedia text and tables, and evaluated on CompMix, Crag, and TimeQuestions datasets.

## Key Results
- Achieves 75.4% P@1 on TimeQuestions benchmark for temporal QA
- Scores 56.4% P@1 on CompMix benchmark for complex questions
- Outperforms larger GPT models while using significantly fewer parameters and lower computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative re-ranking and filtering with graph neural networks and cross-encoders substantially improves answer precision by pruning noisy evidence while retaining answer-relevant signals.
- Mechanism: The system first retrieves a large pool of evidence (top-1000) from KG, text, and tables. It then constructs a bipartite graph linking evidence to entities, applies message-passing GNNs to jointly score evidence and candidate answers, and iteratively shrinks the pool (1000→100→30) while preserving high answer presence.
- Core assumption: Noisy evidence can be filtered without losing the correct answer if the re-ranking models are trained on weakly supervised QA pairs.
- Evidence anchors:
  - [abstract] "...and for re-ranking and filtering the retrieved evidence before feeding the most informative pieces into the answer generation."
  - [section] "For both of these reasons, we have devised light-weight techniques for iteratively reducing the top-1000 pieces to a small subset, say top-30 or top-10, that can be fed into an LLM at much lower cost..."
  - [corpus] Weak: no explicit GNN re-ranking mention in neighbors; only general RAG filtering discussed.
- Break condition: If the GNN/CE models are poorly trained, pruning may discard the correct evidence, causing precision to drop sharply.

### Mechanism 2
- Claim: Structured intent decomposition guides more precise retrieval by explicitly capturing answer type, entities, relations, and temporal/spatial cues.
- Mechanism: A small BART model converts each question into a faceted structured intent (SI) containing Ans-Type, Entities, Relation, Time, and Location. This SI is concatenated to form retrieval queries for KG, text, and tables, improving recall of relevant evidence.
- Core assumption: Decomposition into semantic facets captures information needs better than raw questions, especially for multi-hop or temporal queries.
- Evidence anchors:
  - [abstract] "...components for question understanding, to derive crisper input for evidence retrieval..."
  - [section] "To prepare the retrieval from different kinds of sources... it is useful to analyze and decompose the user question... in a structured intent (SI) representation..."
  - [corpus] Weak: neighbors mention ConvQA and financial QA but not structured intent decomposition.
- Break condition: If the SI generation model mislabels facets (e.g., wrong entities or time scope), retrieval will miss correct evidence, lowering answer presence.

### Mechanism 3
- Claim: Fine-tuning the LLM with top-k evidence pieces makes it learn to faithfully extract answers grounded in provided evidence rather than hallucinating.
- Mechanism: The answer generation stage uses a fine-tuned Llama-3.1-8B-Instruct model trained on top-30 evidence snippets per question, teaching it to rely on evidence for answer extraction.
- Core assumption: LLMs can be steered to avoid hallucinations if trained on evidence-conditioned answer pairs.
- Evidence anchors:
  - [abstract] "...with the latter powered by a moderate-sized language model."
  - [section] "Fine-Tuning the LLM: We considered adding an instruction... However, this turned out to be ineffective. The reason why the model works well without such instructions is our task-specific fine-tuning."
  - [corpus] Weak: no neighbor mentions evidence-conditioned LLM fine-tuning.
- Break condition: If fine-tuning data is noisy or evidence is insufficient, the LLM may still hallucinate or refuse to answer.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG) basics
  - Why needed here: Quasar builds on RAG architecture; understanding retriever-reader pipelines is essential to grasp the system.
  - Quick check question: What are the two main stages of a RAG system and what does each do?

- Concept: Graph neural networks (GNN) for re-ranking
  - Why needed here: GNNs score evidence and entity nodes jointly in the iterative re-ranking stage.
  - Quick check question: How does message passing in a bipartite evidence-entity graph improve evidence selection?

- Concept: Cross-encoders for pairwise scoring
  - Why needed here: Alternative to GNNs; cross-encoders score question-evidence pairs directly in the RF stage.
  - Quick check question: What is the difference between a cross-encoder and a bi-encoder in retrieval tasks?

## Architecture Onboarding

- Component map: QU (BART SI extractor) → ER (Clocq KG, BM25 text/table) → RF (GNN/CE iterative re-rank) → AG (fine-tuned Llama-3.1)
- Critical path: QU → ER → RF (1000→100→30) → AG
- Design tradeoffs: Small LMs in QU/ER/RF keep costs low vs. big LLM-only; iterative RF balances recall/precision vs. latency
- Failure signatures: Low answer presence → ER recall problem; low precision → RF noise retention; hallucinations → AG fine-tuning issue
- First 3 experiments:
  1. Compare end-to-end P@1 with and without SI-guided ER on CompMix dev set.
  2. Test RF ablation: BM25-only vs. GNN vs. CE re-ranking.
  3. Vary top-k evidence sizes (5, 10, 30, 100) in AG stage and measure precision/recall tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Quasar compare to other state-of-the-art methods when using only text-based sources for question answering?
- Basis in paper: [explicit] The paper states that all types of sources contribute to Quasar's performance, with the combination of Text+KG+Tables performing best.
- Why unresolved: The paper does not provide a direct comparison of Quasar's performance using only text-based sources against other state-of-the-art methods that also use text-based sources.
- What evidence would resolve it: An experiment where Quasar is run using only text-based sources and compared to other state-of-the-art methods using the same text-based sources.

### Open Question 2
- Question: What is the impact of the quality and completeness of the Structured Intent (SI) on the overall performance of Quasar?
- Basis in paper: [inferred] The paper mentions that the quality and robustness of the SIs were assessed, and that more complete and high-quality SIs can be generated using in-context learning (ICL) with GPT-4.
- Why unresolved: The paper does not provide a systematic study on how the quality and completeness of the SIs affect the performance of Quasar.
- What evidence would resolve it: An experiment where the performance of Quasar is measured using SIs of varying quality and completeness, generated using different methods (e.g., BART, ICL with GPT-4).

### Open Question 3
- Question: How does the iterative re-ranking and filtering (RF) stage in Quasar affect the performance compared to other RF strategies, such as using only the initial BM25 ranking?
- Basis in paper: [explicit] The paper presents an ablation study on different RF strategies, including using only the initial BM25 ranking without explicit re-ranking.
- Why unresolved: While the paper shows that the iterative re-ranking and filtering stage improves performance, it does not provide a comprehensive comparison of different RF strategies in terms of their impact on performance.
- What evidence would resolve it: An experiment where the performance of Quasar is compared to other RF strategies, such as using only the initial BM25 ranking or other re-ranking methods, on the same benchmarks.

## Limitations
- The exact training procedures and hyperparameter choices for iterative re-ranking GNN and cross-encoder models are not fully specified, which could significantly impact performance if not properly tuned.
- Limited ablation studies make it difficult to determine the relative contributions of each component, especially the structured intent decomposition stage.
- Reported gains over larger GPT models may not generalize to domains beyond the tested benchmarks.

## Confidence
- High confidence in the core RAG architecture and overall performance claims (P@1 scores, efficiency gains)
- Medium confidence in the effectiveness of iterative re-ranking mechanisms (limited ablation details provided)
- Medium confidence in the structured intent decomposition approach (no standalone evaluation of QU stage)
- Low confidence in exact training procedures for fine-tuning components (specific hyperparameters not provided)

## Next Checks
1. Replicate end-to-end performance on CompMix dev set with and without SI-guided retrieval to isolate the contribution of structured intent decomposition.
2. Conduct ablation studies varying top-k evidence sizes (5, 10, 30, 100) in AG stage to quantify precision-recall tradeoff and determine optimal evidence pool size.
3. Test RF stage independently using both GNN and cross-encoder approaches on held-out validation data to compare their effectiveness in iterative evidence pruning.