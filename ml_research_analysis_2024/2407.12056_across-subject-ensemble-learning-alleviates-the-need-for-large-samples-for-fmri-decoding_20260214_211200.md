---
ver: rpa2
title: Across-subject ensemble-learning alleviates the need for large samples for
  fMRI decoding
arxiv_id: '2407.12056'
source_url: https://arxiv.org/abs/2407.12056
tags:
- decoding
- subjects
- ensemble
- datasets
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an ensemble learning method to improve fMRI-based
  cognitive state decoding, addressing the challenge of limited per-subject data.
  The method pre-trains classifiers on multiple subjects, then combines their predictions
  to decode a new subject's brain activity.
---

# Across-subject ensemble-learning alleviates the need for large samples for fMRI decoding

## Quick Facts
- arXiv ID: 2407.12056
- Source URL: https://arxiv.org/abs/2407.12056
- Reference count: 32
- This paper introduces an ensemble learning method to improve fMRI-based cognitive state decoding, addressing the challenge of limited per-subject data.

## Executive Summary
This paper introduces an ensemble learning method to improve fMRI-based cognitive state decoding, addressing the challenge of limited per-subject data. The method pre-trains classifiers on multiple subjects, then combines their predictions to decode a new subject's brain activity. Results show that this ensemble approach outperforms conventional decoding by up to 20% in accuracy, particularly when using voxel-level features and Multi-layer Perceptron as the final classifier. The approach is most beneficial with limited training data per subject, reducing the required number of samples by 5-10 per class. Accuracy gains increase with the number of subjects in the ensemble, though the method struggles with very small subject cohorts.

## Method Summary
The method pre-trains separate classifiers on N-1 subjects using linear SVC with L2 regularization, then trains a final classifier on their predictions for the Nth subject. The final classifier can be an MLP, linear SVC, or Random Forest. The approach is tested on five fMRI datasets with varying cognitive tasks, voxel resolution downsampled to 3mm, and GLM-derived trial-by-trial effect size maps as features. Evaluation compares ensemble vs. conventional decoding accuracy across 20 cross-validation splits and varying training set sizes.

## Key Results
- Ensemble approach outperforms conventional decoding by up to 20% in accuracy
- Voxel-level features are more beneficial than reduced dimensionality features
- Multi-layer Perceptron is a robust default choice as the final ensemble classifier

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble learning with stacked classifiers outperforms conventional single-subject decoding when per-subject data is limited.
- Mechanism: By pre-training individual classifiers on multiple subjects and then combining their predictions via a meta-learner, the ensemble method reduces variance while maintaining low bias, especially when individual models are high-variance/low-bias estimators.
- Core assumption: Individual classifiers trained on different subjects learn complementary features, and their errors are not perfectly correlated.
- Evidence anchors:
  - [abstract] "Results show that this ensemble approach outperforms conventional decoding by up to 20% in accuracy, particularly when using voxel-level features and Multi-layer Perceptron as the final classifier."
  - [section] "For optimal λ, both terms are balanced, hence the squared prediction error is ∝ var(y)d(λ)/Nsamples. If the ensembling model g is ordinary least squares, it has no estimation bias, but a model bias ∝ var(y)/N due to the projection of the feature space on the (fi(XN ))i∈[N−1] span, and a variance ∝ var(y)N/Nsamples."
  - [corpus] Weak/no direct evidence in corpus neighbors; this is a novel contribution.
- Break condition: When the number of subjects N becomes comparable to or larger than the number of samples per subject, the ensemble model's variance term dominates and regularization is required; also fails when very few subjects are available (e.g., BOLD5000 with N=3).

### Mechanism 2
- Claim: Using voxel-level features rather than reduced dimensionality features improves ensemble performance.
- Mechanism: High-dimensional voxel space preserves more fine-grained information that is partially shared across subjects, and the ensemble approach can effectively combine these signals even with limited per-subject samples.
- Core assumption: Relevant discriminative information is distributed across voxels and not fully captured by dimensionality reduction methods like DiFuMo.
- Evidence anchors:
  - [abstract] "The ensemble approach is particularly advantageous when the classifier is trained in voxel space."
  - [section] "Using full-voxel feature space for pre-training is more beneficial than a reduced feature space, in all datasets, irrespective of the classifier used."
  - [corpus] No direct support in corpus neighbors.
- Break condition: If voxel-level noise dominates signal, or if dimensionality reduction better captures task-relevant variance than raw voxels.

### Mechanism 3
- Claim: Multi-layer Perceptron (MLP) as the final ensemble classifier is a robust default choice across datasets.
- Mechanism: MLP can model non-linear relationships between the predictions of pre-trained classifiers and the true labels, adapting to dataset-specific patterns better than linear models in many cases.
- Core assumption: The relationship between ensemble predictions and true labels is not purely linear.
- Evidence anchors:
  - [abstract] "Furthermore, a Multi-layer Perceptron turns out to be a good default choice as an ensemble method."
  - [section] "In general, we see that MLP is the best-performing classifier — at least it performs similarly to the best-performing classifier in all datasets and decoding settings."
  - [corpus] No direct support in corpus neighbors.
- Break condition: When the dataset is very large with many samples, simpler linear models or Random Forests may outperform MLP due to lower variance.

## Foundational Learning

- Concept: Bias-variance decomposition in Ridge regression and ensemble models.
  - Why needed here: To understand under which data regimes ensemble learning provides benefits over conventional single-subject decoding.
  - Quick check question: If N (number of subjects) is much smaller than the number of samples per subject, which term dominates the ensemble model's error?

- Concept: Functional alignment techniques for cross-subject correspondence.
  - Why needed here: To improve across-subject correspondence and potentially enhance ensemble performance further.
  - Quick check question: How does functional alignment impact the assumption that classifiers trained on different subjects learn complementary features?

- Concept: Feature importance extraction from ensemble models.
  - Why needed here: To interpret spatial patterns of brain activity that are most informative for decoding and gain insights into cognitive processes.
  - Quick check question: What method is used in the paper to visualize voxel-wise feature importance scores?

## Architecture Onboarding

- Component map: Pre-training classifiers on N-1 subjects (linear SVC with L2 penalty) → Meta-learner training on predictions from pre-trained classifiers → Evaluation on held-out test set. Three meta-learner options: MLP, LinearSVC, Random Forest.
- Critical path: 1) Pre-train N-1 subject classifiers → 2) Collect predictions for Nth subject → 3) Train meta-learner → 4) Evaluate accuracy.
- Design tradeoffs: Voxel space vs reduced dimensionality (voxel space better but higher computational cost); MLP vs other meta-learners (MLP more flexible but potentially higher variance); L1 vs L2 penalty in pre-training (L2 better for small N, L1 better for large N).
- Failure signatures: Poor performance when N < 3; degraded accuracy with very large per-subject samples; negative gains when voxel noise dominates signal.
- First 3 experiments:
  1. Replicate main result on Neuromod dataset with voxel features and MLP meta-learner; verify 20% gain over conventional approach.
  2. Test ensemble performance on BOLD5000 dataset (N=3) to confirm failure case.
  3. Compare voxel vs DiFuMo features on Forrest dataset to verify voxel space advantage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ensemble approach scale with datasets having different cognitive task complexities?
- Basis in paper: [explicit] The authors note that gains continue to increase even with more than 10 subjects in the ensemble for RSVP-IBC, which has the largest number of samples (360) and stimulus classes (6).
- Why unresolved: The paper only tested five datasets with varying complexities, but did not systematically explore how task complexity affects ensemble performance.
- What evidence would resolve it: Testing the ensemble approach across a wider range of datasets with varying cognitive task complexities and comparing performance metrics.

### Open Question 2
- Question: What are the long-term benefits of using ensemble learning in fMRI-based brain-computer interfaces (BCIs)?
- Basis in paper: [inferred] The authors suggest that the ensemble approach could be beneficial in real-time BCI applications where initial data scarcity is a major issue.
- Why unresolved: The paper does not provide empirical evidence or case studies on the application of ensemble learning in BCIs over extended periods.
- What evidence would resolve it: Longitudinal studies or real-world implementations of ensemble learning in BCIs to assess performance and benefits over time.

### Open Question 3
- Question: How does the choice of pre-training strategies, such as using deep learning models, impact the performance of the ensemble approach with larger sample sizes?
- Basis in paper: [explicit] The authors mention that they did not explore other pre-training strategies and suggest that pre-training deep learning models instead could be beneficial when larger sample sizes are available.
- Why unresolved: The paper only tested linear SVC with l1 and l2 penalization during pre-training and did not explore deep learning models or other pre-training strategies.
- What evidence would resolve it: Comparative studies using different pre-training strategies, including deep learning models, to evaluate their impact on ensemble performance with larger sample sizes.

## Limitations

- The method struggles with very small subject cohorts (e.g., BOLD5000 with only 3 subjects).
- Computational overhead of voxel-level feature space versus dimensionality reduction methods remains unquantified.
- The assumption that voxel-level information is more informative than reduced representations needs further validation across diverse cognitive tasks.

## Confidence

- High confidence in the core finding that ensemble learning reduces the need for large per-subject samples, supported by consistent accuracy gains (up to 20%) across five independent fMRI datasets.
- Medium confidence in the superiority of voxel-level features and MLP meta-learners, as these results are less consistent across all experimental conditions and datasets.
- Low confidence in the scalability claims to very large subject cohorts (>20) without additional regularization strategies.

## Next Checks

1. Test ensemble performance on a dataset with 30+ subjects to validate scalability claims and identify regularization thresholds.
2. Compare computational efficiency and memory requirements between voxel-level and dimensionality-reduced ensemble approaches.
3. Apply the ensemble method to a clinical dataset with known brain disorders to assess real-world applicability beyond healthy controls.