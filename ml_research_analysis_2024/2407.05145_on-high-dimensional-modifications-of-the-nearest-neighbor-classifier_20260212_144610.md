---
ver: rpa2
title: On high-dimensional modifications of the nearest neighbor classifier
arxiv_id: '2407.05145'
source_url: https://arxiv.org/abs/2407.05145
tags:
- classifier
- classifiers
- misclassification
- example
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance degradation of the nearest
  neighbor classifier in high-dimensional, low-sample-size settings, especially when
  scale differences between classes dominate location differences. The authors analyze
  the limitations of existing scale-adjusted methods (CH and MCH classifiers) and
  propose new classifiers based on distance-based features.
---

# On high-dimensional modifications of the nearest neighbor classifier

## Quick Facts
- arXiv ID: 2407.05145
- Source URL: https://arxiv.org/abs/2407.05145
- Reference count: 40
- One-line primary result: Proposes distance-based feature extraction methods (MDist, MDist1, rMDist variants) that outperform traditional nearest neighbor classifiers in high-dimensional, low-sample-size settings

## Executive Summary
This paper addresses the performance degradation of nearest neighbor classifiers in high-dimensional, low-sample-size (HDLSS) settings, particularly when scale differences between classes dominate location differences. The authors propose new classifiers based on distance-based features that transform the original high-dimensional data into lower-dimensional spaces. These methods, including MDist, MDist1, and rMDist variants, leverage minimum distances from each class and distances from multiple neighbors to achieve improved classification performance. Theoretical analysis shows these classifiers can achieve perfect classification under certain conditions, and empirical evaluations demonstrate superior performance compared to existing approaches on both simulated and benchmark datasets.

## Method Summary
The proposed method transforms high-dimensional data into a lower-dimensional feature space based on distances. For binary classification, the MDist classifier computes minimum distances from a test point to each class and uses these as features for a 1-NN classifier. The MDist1 variant uses ℓ1 (Manhattan) distances instead of ℓ2 (Euclidean) distances. The rMDist family extends this by considering distances from multiple nearest neighbors (r neighbors) from each class. These transformations help avoid the concentration of pairwise distances that plagues standard nearest neighbor methods in HDLSS settings. Cross-validation is used to select the number of neighbors r for the rMDist variants.

## Key Results
- Proposed MDist and MDist1 classifiers achieve superior performance compared to standard nearest neighbor, CH, and MCH classifiers in high-dimensional scenarios
- rMDist variants, particularly rMDist1 and rMDistC, show the best overall performance across different scenarios and outperform popular classifiers like random forest and SVM
- Theoretical analysis demonstrates that under assumptions (A1)-(A3), the proposed classifiers achieve perfect classification as dimension grows
- Empirical evaluations on both simulated and benchmark datasets confirm the effectiveness of the proposed methods in HDLSS settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MDist classifier uses minimum distances from each class as features and applies nearest neighbor classification in this transformed space, achieving perfect classification in high dimensions when scale and location differences dominate.
- Mechanism: In high dimensions, the minimum distance from a test point to each class converges to distinct values when classes differ in location or scale. By transforming the data into a space of these minimum distances, the classifier exploits this separation while avoiding the concentration of pairwise distances that plagues standard nearest neighbor methods.
- Core assumption: The competing classes satisfy assumptions (A1)-(A3) ensuring convergence of pairwise distances and distinct limiting values for minimum distances from different classes.
- Evidence anchors:
  - [abstract] The key contributions include: (1) introducing the MDist and MDist1 classifiers, which use minimum distances from each class as features and apply nearest neighbor classification in this transformed space
  - [section 3.1] Note that in a binary classification problem with training samples{X11, X12, . . . ,X1n1 } and {X21, X22, . . . ,X2n2 } from the two competing classes, for classification of a test case Z, the 1-NN classifier computes its minimum distances d1(Z) = min1≤i≤n1 ∥Z − X1i∥ and d2(Z) = min1≤i≤n2 ∥Z − X2i∥ from Class-1 and Class-2, respectively
- Break condition: If the classes are mixtures of sub-classes that violate assumption (A2), the minimum distances may not converge to distinct values, causing the classifier to fail.

### Mechanism 2
- Claim: The rMDist classifier extends MDist by using distances from multiple nearest neighbors (r neighbors) as features, improving performance when single neighbor distances are insufficient.
- Mechanism: By including distances from multiple neighbors, the feature space captures more information about class separability. This is particularly useful when the single minimum distance doesn't provide sufficient separation between classes, as might occur with mixture distributions or when classes have complex structures.
- Core assumption: The first r nearest neighbors from each class provide meaningful features that capture class differences.
- Evidence anchors:
  - [section 3.2] Instead of considering only the distance of the first neighbor from each class, here we consider the distances of the first r (r ≥ 1) neighbors from each class and use them as features
  - [section 3.2] In Examples 1-3, when the underlying distributions are unimodal, the TRAD classifier, which considers the average of distances from all observations, performed better than MDist and MDist1 classifiers, which consider the distance of one nearest neighbor only
- Break condition: If r is chosen too large, the feature space may become too high-dimensional relative to the sample size, potentially leading to overfitting.

### Mechanism 3
- Claim: The MDist1 classifier uses ℓ1 distances (Manhattan distance) instead of ℓ2 distances (Euclidean distance), enabling discrimination between distributions differing outside their first two moments.
- Mechanism: ℓ1 distances capture differences in the marginal distributions of each feature, not just differences in mean and variance. This allows the classifier to distinguish between distributions that differ in higher moments or have different tail behaviors.
- Core assumption: The competing classes differ in their one-dimensional marginals, ensuring non-zero energy distances.
- Evidence anchors:
  - [section 3.1] Example 7: We consider two normal distributions having the same mean vector 0d but different dispersion matrices Λ1=diag(λ11, . . . , λ1d) and Λ2=diag(λ21, . . . , λ2d)
  - [section 3.1] Figure 8(c) shows the scatter plot of these features for all test set observations and the class boundary estimated by the 1-NN classifier on this feature space (we call it the MDist1 classifier)
- Break condition: If the classes have identical marginal distributions, ℓ1 distances will not provide discriminatory information.

## Foundational Learning

- Concept: High-dimensional concentration of distances
  - Why needed here: Understanding why standard nearest neighbor classifiers fail in high dimensions is crucial for appreciating the need for modifications
  - Quick check question: What happens to the ratio of nearest to farthest neighbor distances as dimension increases?

- Concept: Distance-based feature extraction
  - Why needed here: The proposed methods transform the original high-dimensional data into lower-dimensional feature spaces based on distances
  - Quick check question: How does using minimum distances from each class differ from using all pairwise distances?

- Concept: Asymptotic convergence in HDLSS settings
  - Why needed here: The theoretical analysis relies on understanding how distances and other quantities behave as dimension grows
  - Quick check question: Under what conditions do pairwise distances converge in high dimensions?

## Architecture Onboarding

- Component map: Data preprocessing -> Feature extraction (MDist/MDist1/rMDist) -> Classification (1-NN in feature space) -> Model selection (cross-validation for r)
- Critical path: 1. Compute training set minimum distances from each class, 2. Transform test points into feature space using these distances, 3. Apply nearest neighbor classification in feature space
- Design tradeoffs:
  - Using ℓ1 vs ℓ2 distances: ℓ1 is more robust to outliers but may be less sensitive to certain types of differences
  - Number of neighbors (r): More neighbors capture more information but increase dimensionality and computational cost
  - Cross-validation complexity: rMDist variants require tuning r, increasing computational overhead
- Failure signatures:
  - Poor performance on mixture distributions: When classes are mixtures, minimum distances may not converge to distinct values
  - Overfitting with large r: Too many neighbors can lead to overfitting in low-sample-size settings
  - Sensitivity to outliers: ℓ2-based methods can be affected by outliers in the data
- First 3 experiments:
  1. Implement MDist classifier on a simple two-class normal distribution problem and compare to standard 1-NN
  2. Test MDist1 on Example 7 from the paper (two normals with same mean but different dispersion matrices)
  3. Compare rMDist with r=1,2,3 on a benchmark dataset to understand the impact of r on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed MDist classifier outperform the k-NN classifier (for k>1) in high-dimensional settings when the scale differences between classes are significant?
- Basis in paper: The paper introduces the MDist classifier and demonstrates its superiority over 1-NN, CH, and MCH classifiers in high-dimensional scenarios. However, it does not explicitly compare MDist to k-NN with k>1.
- Why unresolved: The paper focuses on comparing MDist to other classifiers like 1-NN, CH, MCH, and popular classifiers like SVM and random forest. A direct comparison with k-NN (k>1) is missing.
- What evidence would resolve it: Empirical results comparing the performance of MDist and k-NN (with various values of k) on high-dimensional datasets, particularly those with significant scale differences between classes.

### Open Question 2
- Question: How does the choice of distance metric (e.g., Euclidean, Manhattan, Minkowski) affect the performance of the proposed MDist and MDist1 classifiers in high-dimensional settings?
- Basis in paper: The paper explores the use of both ℓ2 (Euclidean) and ℓ1 (Manhattan) distances in the proposed classifiers and observes differences in performance. However, it does not systematically investigate the impact of different distance metrics.
- Why unresolved: The paper mentions the use of ℓ1 and ℓ2 distances but does not provide a comprehensive analysis of how different distance metrics influence the classifiers' performance.
- What evidence would resolve it: Empirical results comparing the performance of MDist and MDist1 classifiers using various distance metrics (e.g., Euclidean, Manhattan, Minkowski) on high-dimensional datasets.

### Open Question 3
- Question: Can the proposed feature extraction methods (MDist, MDist1, rMDist, rMDist1, rMDistC) be extended to handle multi-class classification problems with more than two classes?
- Basis in paper: The paper primarily focuses on binary classification problems and provides some insights into multi-class extensions of the CH and MCH classifiers. However, it does not explicitly address the application of the proposed feature extraction methods to multi-class scenarios.
- Why unresolved: The paper's analysis is mainly centered around binary classification, and the extension to multi-class problems is not thoroughly explored.
- What evidence would resolve it: Empirical results demonstrating the performance of the proposed feature extraction methods in multi-class classification problems with more than two classes, along with a discussion of any challenges or modifications required for such extensions.

## Limitations
- Performance depends on assumptions (A1)-(A3) about class distributions, which may not hold for mixture distributions
- Computational complexity increases significantly with ℓ1 distances due to pairwise distance calculations
- Theoretical guarantees rely on asymptotic behavior as dimension grows, which may not translate perfectly to finite-sample settings

## Confidence
- **High Confidence**: Empirical evaluation showing improved performance over traditional NN, CH, and MCH classifiers
- **Medium Confidence**: Theoretical claims about perfect classification under assumptions (A1)-(A3)
- **Medium Confidence**: Claims about ℓ1 distance superiority for distributions differing outside first two moments

## Next Checks
1. Test proposed classifiers on mixture distributions where assumption (A2) is violated to assess robustness beyond unimodal cases
2. Evaluate computational scaling with increasing dimension and sample size, particularly for ℓ1 distance calculations
3. Implement cross-validation for r selection in rMDist variants and compare performance across different r values to identify optimal hyperparameter ranges