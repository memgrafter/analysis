---
ver: rpa2
title: 'MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers'
arxiv_id: '2406.10163'
source_url: https://arxiv.org/abs/2406.10163
tags:
- mesh
- meshes
- generation
- arxiv
- shape
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of converting automatically
  generated 3D assets into high-quality meshes suitable for 3D industry applications.
  Current mesh extraction methods produce dense, inefficient meshes with poor topology,
  limiting their usability.
---

# MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers

## Quick Facts
- arXiv ID: 2406.10163
- Source URL: https://arxiv.org/abs/2406.10163
- Reference count: 27
- Primary result: Generates Artist-Created Meshes (AMs) with hundreds of times fewer faces than previous methods while maintaining comparable precision

## Executive Summary
This paper addresses the critical challenge of converting automatically generated 3D assets into high-quality, industry-ready meshes. Current mesh extraction methods produce dense, inefficient meshes with poor topology that limit their usability in professional applications. The authors introduce MeshAnything, a novel approach that treats mesh extraction as a generation problem rather than an extraction problem, producing meshes specifically designed to align with specified shapes while optimizing for efficiency.

MeshAnything employs a VQ-VAE to learn a mesh vocabulary and a shape-conditioned decoder-only transformer for autoregressive mesh generation. The method demonstrates dramatic improvements in storage, rendering, and simulation efficiencies through significantly reduced face counts, while achieving precision comparable to previous methods. Importantly, MeshAnything can be integrated with various 3D asset production methods, potentially enhancing their application across the 3D industry.

## Method Summary
MeshAnything addresses the challenge of producing high-quality meshes from 3D assets by reframing mesh extraction as a generation problem. The method employs a two-stage approach: first, a VQ-VAE learns a discrete vocabulary of mesh components from training data, creating a compact representation space. Second, a shape-conditioned decoder-only transformer performs autoregressive generation of mesh vertices and faces, conditioned on the target shape's features. This architecture enables the generation of Artist-Created Meshes (AMs) that are specifically optimized for the target shape, resulting in meshes with dramatically fewer faces while maintaining high precision. The approach can be integrated with various 3D asset production methods, making it broadly applicable across different generation pipelines.

## Key Results
- Generates Artist-Created Meshes with hundreds of times fewer faces compared to previous methods
- Achieves precision comparable to existing mesh extraction approaches
- Significantly improves storage, rendering, and simulation efficiencies through reduced mesh complexity

## Why This Works (Mechanism)
The method works by treating mesh generation as a conditional generation problem rather than a reconstruction problem. By learning a vocabulary of mesh components through VQ-VAE and using a shape-conditioned transformer for autoregressive generation, MeshAnything can create meshes that are specifically tailored to the target shape. This approach allows for intentional design choices in topology and structure that prioritize both quality and efficiency, rather than simply extracting whatever geometry exists in the original representation.

## Foundational Learning

### VQ-VAE (Vector Quantized Variational Autoencoder)
- Why needed: To learn a discrete vocabulary of mesh components that can be efficiently generated and combined
- Quick check: Verify that the learned vocabulary captures essential mesh patterns and enables reconstruction of training examples

### Autoregressive Transformers
- Why needed: To generate meshes sequentially, allowing the model to condition each generation step on previously generated elements
- Quick check: Ensure that the autoregressive nature produces coherent mesh structures without self-intersections or topological errors

### Shape-Conditioned Generation
- Why needed: To ensure the generated mesh accurately represents the target shape rather than producing generic mesh patterns
- Quick check: Validate that conditioning features effectively guide generation toward the correct shape representation

## Architecture Onboarding

### Component Map
VQ-VAE (mesh vocabulary learning) -> Shape encoder (feature extraction) -> Decoder-only Transformer (autoregressive mesh generation) -> Mesh output

### Critical Path
The critical path involves extracting shape features, conditioning the transformer on these features, and generating the mesh sequentially through autoregressive steps. The quality of the generated mesh depends heavily on the effectiveness of the shape encoding and the transformer's ability to generate coherent mesh components conditioned on these features.

### Design Tradeoffs
- Vocabulary size vs. generation quality: Larger vocabularies may capture more detail but increase complexity
- Generation speed vs. mesh complexity: More detailed meshes require more generation steps
- Conditioning strength vs. creative freedom: Stronger conditioning ensures shape accuracy but may limit topological optimization

### Failure Signatures
- Self-intersecting meshes indicate issues with generation coherence
- Poor shape alignment suggests inadequate conditioning or feature extraction
- Excessive face counts indicate failure to optimize topology
- Missing geometric details suggest vocabulary limitations

### 3 First Experiments
1. Test mesh reconstruction on training examples to verify VQ-VAE vocabulary quality
2. Generate meshes for simple geometric shapes (spheres, cubes) to validate basic generation capability
3. Evaluate mesh quality metrics (face count, precision) on a held-out validation set

## Open Questions the Paper Calls Out
None

## Limitations
- Potential overfitting to training distribution, with performance degradation on out-of-distribution shapes
- Unverified claims about integration capability with various 3D asset production methods
- Uncertain performance on extremely complex geometries or topologically challenging shapes

## Confidence

- High confidence: The core technical approach (VQ-VAE + autoregressive transformer) is sound and well-implemented
- Medium confidence: The efficiency improvements (face count reduction) are robustly demonstrated
- Medium confidence: The method's integration capability with other 3D asset production pipelines
- Low confidence: Performance on highly complex or topologically challenging shapes

## Next Checks

1. Test the model on out-of-distribution shapes and evaluate performance degradation
2. Implement integration with at least two different 3D asset generation pipelines (e.g., NeRF-based and SDF-based methods) to validate the claimed compatibility
3. Conduct a user study with professional 3D artists to assess the practical usability and quality of the generated Artist-Created Meshes in real production workflows