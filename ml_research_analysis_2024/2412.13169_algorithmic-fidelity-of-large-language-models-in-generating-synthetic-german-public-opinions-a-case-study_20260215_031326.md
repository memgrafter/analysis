---
ver: rpa2
title: 'Algorithmic Fidelity of Large Language Models in Generating Synthetic German
  Public Opinions: A Case Study'
arxiv_id: '2412.13169'
source_url: https://arxiv.org/abs/2412.13169
tags:
- survey
- policy
- party
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the ability of large language models to generate
  synthetic German public opinions that align with real survey data. The study uses
  open-ended survey responses from the German Longitudinal Election Study (GLES) and
  prompts three LLMs (Llama2, Mixtral, Gemma) with demographic features to simulate
  German subpopulations.
---

# Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study

## Quick Facts
- arXiv ID: 2412.13169
- Source URL: https://arxiv.org/abs/2412.13169
- Reference count: 40
- Primary result: Llama2 outperforms other LLMs at representing German subpopulations when generating synthetic public opinions, with better alignment for left-leaning parties and improved performance when including more demographic variables in prompts

## Executive Summary
This study evaluates large language models' ability to generate synthetic German public opinions that align with real survey data from the German Longitudinal Election Study (GLES). The researchers prompt three LLMs (Llama2, Mixtral, Gemma) with demographic features to simulate German subpopulations and compare the outputs to actual survey responses. Results show that Llama2 demonstrates superior representativeness, particularly when opinion diversity within groups is lower. The study reveals systematic biases in how LLMs represent different political parties, with better performance for left-leaning parties compared to right-leaning ones. The findings highlight the critical role of prompt engineering and the importance of demographic conditioning in generating representative synthetic opinions.

## Method Summary
The researchers use GLES survey data containing demographic variables and open-ended responses about important political problems in Germany. They prompt three LLMs (Llama2, Mixtral, Gemma) with demographic persona information and generate synthetic responses. These outputs are classified using a fine-tuned German BERT model into 16 coarse categories. The study evaluates representativeness through JS Divergence, entropy, conditional entropy, information gain, and Cramér's V statistics, comparing the distribution of synthetic responses to actual survey data. Prompt ablation experiments test the impact of including different combinations of demographic variables.

## Key Results
- Llama2 outperforms Mixtral and Gemma in representing German subpopulations, particularly when opinion diversity within groups is lower
- Including more demographic variables in prompts improves LLM performance, with party affiliation being the most influential factor
- Llama2 shows better alignment with left-leaning parties (The Greens, The Left) compared to right-leaning parties, with the poorest performance for AfD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Including demographic variables in LLM prompts improves representativeness of synthetic opinions
- Mechanism: Demographic variables act as conditioning information that constrains the LLM's output distribution to match known subgroup characteristics, reducing the entropy of responses within each group and increasing alignment with survey data
- Core assumption: The LLM's training data contains sufficient information about how different demographic groups express opinions on political issues
- Evidence anchors:
  - [abstract]: "the inclusion or exclusion of specific variables in the prompts can significantly impact the models' predictions"
  - [section]: "Including all variables reduces the JS distance by 0.15 compared to the base prompt. Adding a single variable improves predictions."
  - [corpus]: Found related work on prompt engineering for survey prediction (FMR scores 0.53-0.58), supporting the importance of demographic conditioning

### Mechanism 2
- Claim: LLMs exhibit systematic biases in representing different political parties, performing better for left-leaning parties
- Mechanism: Training data and fine-tuning processes introduce ideological preferences that manifest as higher conditional entropy for right-leaning parties, making them harder to model accurately
- Core assumption: The model's training corpus contains disproportionate representation of left-leaning perspectives, or reinforcement learning from human feedback introduces this bias
- Evidence anchors:
  - [abstract]: "the model performs better for supporters of left-leaning parties like The Greens and The Left compared to other parties, and matches the least with the right-party AfD"
  - [section]: "Llama2 better models groups with left-leaning parties" and information gain analysis showing AfD is modeled worse
  - [corpus]: Related work on political sample simulations (FMR 0.55) found similar representation biases

### Mechanism 3
- Claim: Higher in-group opinion diversity reduces LLM representativeness of that group
- Mechanism: When a demographic group shows high variability in opinions (high conditional entropy), the LLM struggles to capture this diversity and instead produces more stereotypical responses that better match the overall population distribution
- Core assumption: LLMs are better at modeling low-variance distributions within demographic groups
- Evidence anchors:
  - [abstract]: "Llama performs better than other LLMs at representing subpopulations, particularly when there is lower opinion diversity within those groups"
  - [section]: "Pearson's correlation coefficient between survey entropy and the JS distance and got r = −0.35, indicating that the model's representativeness of the population decreases as the diversity in answers increases"
  - [corpus]: Related work on synthetic survey data generation found similar challenges with subgroup variation (FMR 0.51)

## Foundational Learning

- Concept: Jensen-Shannon Divergence as a symmetric measure of distributional distance
  - Why needed here: Used to quantify how closely LLM-generated opinion distributions match real survey data
  - Quick check question: Why is JS divergence preferred over KL divergence for comparing LLM outputs to survey data?

- Concept: Conditional entropy and information gain in subgroup analysis
  - Why needed here: These metrics reveal how much knowing demographic information reduces uncertainty about opinions, helping identify which groups are harder to model
  - Quick check question: What does a high information gain value indicate about a demographic group's opinion diversity?

- Concept: Prompt engineering and in-context learning for demographic conditioning
  - Why needed here: The study relies on carefully crafted prompts that include demographic variables to generate representative synthetic opinions
  - Quick check question: How does including vs. excluding demographic variables affect the LLM's output distribution?

## Architecture Onboarding

- Component map: Survey data preprocessing -> Prompt generation with demographic variables -> LLM text generation -> Classification of outputs -> Statistical comparison with original survey
- Critical path: The prompt generation and LLM response phases are most critical, as errors here propagate through the entire analysis pipeline
- Design tradeoffs: Using coarse classification categories improves annotation consistency but may lose nuanced opinion distinctions; German language prompts limit model options but ensure cultural relevance
- Failure signatures: High JS distances across all experiments suggest prompt engineering issues; systematic bias toward certain parties indicates training data problems; low variability in subgroup responses suggests over-regularization
- First 3 experiments:
  1. Generate responses with only party affiliation vs. all demographics to measure impact on representativeness
  2. Compare JS distances across different demographic subgroups to identify hardest-to-model populations
  3. Test different prompt formulations (variable order, phrasing) to optimize conditioning effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Llama2 compare to other open-weight LLMs in generating synthetic German public opinions across different political party leanings?
- Basis in paper: [explicit] The paper evaluates three LLMs (Llama2, Mixtral, Gemma) and finds that Llama2 performs better than others at representing subpopulations, particularly for left-leaning parties
- Why unresolved: The study only uses three models, and the comparison is limited to specific demographic variables. There may be other models or prompting strategies that could yield different results
- What evidence would resolve it: Comparative experiments with a broader range of LLMs and prompting techniques, including different model sizes and architectures, would provide more comprehensive insights into their relative performance

### Open Question 2
- Question: To what extent does the inclusion of demographic variables in prompts influence the diversity of opinions generated by LLMs, and how can this be optimized?
- Basis in paper: [explicit] The paper shows that including more variables in prompts improves performance, with party affiliation being the most influential factor
- Why unresolved: While the study identifies the impact of variable inclusion, it does not explore the optimal number or combination of variables, nor does it address how to balance diversity and representativeness
- What evidence would resolve it: Systematic ablation studies varying the number and types of demographic variables, along with analysis of the resulting opinion diversity, would clarify the optimal prompt design

### Open Question 3
- Question: How can LLMs be further developed to reduce political bias and improve the representation of diverse opinions within subpopulations?
- Basis in paper: [inferred] The paper highlights that LLMs, including Llama2, tend to generate stereotypical representations and favor left-leaning parties, indicating a need for better alignment with diverse opinions
- Why unresolved: The study identifies the issue of bias but does not propose or test specific methods to mitigate it, such as advanced alignment techniques or bias-correction mechanisms
- What evidence would resolve it: Experiments testing different alignment strategies, such as reinforcement learning from human feedback (RLHF) or debiasing techniques, would provide insights into reducing political bias in LLM-generated opinions

## Limitations

- The study's focus on German political discourse may limit generalizability to other cultural or political contexts
- The use of coarse classification categories (16 classes) may obscure important nuances in opinion expression
- The systematic bias toward left-leaning parties could reflect both model training biases and genuine differences in opinion structure within these groups

## Confidence

**High Confidence**: The methodological framework for evaluating LLM representativeness using JS Divergence and entropy metrics is sound. The correlation between demographic variable inclusion and improved representativeness (r = -0.35 for entropy-diversity relationship) is statistically robust.

**Medium Confidence**: The claim about systematic political bias in LLMs is supported by the data but requires further validation. The observed performance differences between left and right-leaning parties could be influenced by both model training biases and genuine differences in opinion structure within these groups.

**Low Confidence**: The generalizability of prompt engineering techniques across different LLMs and political contexts remains uncertain, as the study only tested three specific models on German political opinions.

## Next Checks

1. **Bias Verification Test**: Conduct blind annotation of LLM outputs to verify that performance differences between left and right-leaning parties are not due to rater bias in the German BERT classifier training data

2. **Prompt Robustness Analysis**: Test alternative prompt formulations (different variable ordering, phrasing, and completeness levels) to determine if the observed demographic conditioning effects are robust to prompt variations

3. **Cross-Cultural Validation**: Replicate the study using survey data from a different political system (e.g., US or UK) to assess whether the observed patterns of LLM performance and bias are consistent across different political contexts