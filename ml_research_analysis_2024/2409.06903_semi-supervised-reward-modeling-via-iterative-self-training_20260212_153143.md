---
ver: rpa2
title: Semi-Supervised Reward Modeling via Iterative Self-Training
arxiv_id: '2409.06903'
source_url: https://arxiv.org/abs/2409.06903
tags:
- data
- ssrm
- reward
- dataset
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Semi-Supervised Reward Modeling (SSRM), an
  approach to improve reward models using unlabeled data. The method involves iteratively
  pseudo-labeling unlabeled examples, selecting high-confidence examples through a
  confidence threshold, and supervised fine-tuning on the refined dataset.
---

# Semi-Supervised Reward Modeling via Iterative Self-Training

## Quick Facts
- arXiv ID: 2409.06903
- Source URL: https://arxiv.org/abs/2409.06903
- Authors: Yifei He; Haoxiang Wang; Ziyan Jiang; Alexandros Papangelis; Han Zhao
- Reference count: 18
- Primary result: SSRM improves reward models using unlabeled data through iterative self-training, achieving performance comparable to fully supervised models while using only a fraction of labeled data.

## Executive Summary
This paper introduces Semi-Supervised Reward Modeling (SSRM), a method that significantly improves reward models by leveraging unlabeled data through iterative self-training. The approach involves pseudo-labeling unlabeled examples, selecting high-confidence examples via thresholding, and fine-tuning on the augmented dataset. Across extensive experiments with models ranging from 0.4B to 8B parameters, SSRM demonstrates substantial performance improvements without additional labeling costs, achieving results comparable to models trained entirely on labeled data of equivalent volumes.

## Method Summary
SSRM operates through an iterative process that begins with initial supervised reward modeling (SRM) on a small labeled dataset. The method then enters a loop where the current model pseudo-labels unlabeled examples, applies confidence thresholding (typically 0.8) to select high-quality pseudo-labels, and performs supervised fine-tuning on the augmented dataset. This process repeats for a preset number of iterations (typically 3), progressively improving the model's calibration and performance while reducing dependency on large volumes of human-annotated data.

## Key Results
- SSRM achieves performance comparable to fully supervised models using only one-fourth of the labeled data
- Performance improvements are consistent across model sizes from 0.4B to 8B parameters
- Confidence thresholding effectively prevents error propagation while enabling meaningful data augmentation
- The method demonstrates significant calibration improvements, with confidence distributions better aligned with true probabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-training iteratively improves reward model calibration by selectively incorporating high-confidence pseudo-labels.
- Mechanism: At each iteration, the model pseudo-labels unlabeled examples, retains only those with confidence ≥ threshold, and fine-tunes on the augmented dataset. This process progressively shifts the confidence distribution rightward and improves alignment between predicted and true probabilities.
- Core assumption: The initial supervised model has enough accuracy to generate mostly correct pseudo-labels at the confidence threshold.
- Evidence anchors:
  - [abstract] "Across extensive experiments on various model configurations, we demonstrate that SSRM significantly improves reward models without incurring additional labeling costs."
  - [section 3.4] "The model after three iterations of SSRM (Figure 2b) shows a curve that adheres more closely to the diagonal across the entire probability spectrum."
- Break condition: If initial model accuracy is too low, pseudo-labels will be noisy, leading to degradation after fine-tuning.

### Mechanism 2
- Claim: Confidence thresholding prevents performance collapse by filtering out low-confidence pseudo-labels.
- Mechanism: By setting a high threshold (0.8), only examples where the model is certain are added to the training set, ensuring that the majority of pseudo-labels are correct and the model does not reinforce its own errors.
- Core assumption: The confidence score correlates with prediction accuracy, so high-confidence pseudo-labels are correct.
- Evidence anchors:
  - [section 3.2] "it is crucial not to directly use the entirety of pseudo-labeled data for self-training, as doing so will result in a final model with identical performance as the initial model."
  - [section 3.4] "The notable improvement in calibration at higher confidence scores underscores the effectiveness of confidence thresholding."
- Break condition: If the confidence score is poorly calibrated (overconfident), the threshold will admit too many incorrect labels.

### Mechanism 3
- Claim: Semi-supervised learning with SSRM can achieve performance comparable to fully supervised training using only a fraction of labeled data.
- Mechanism: SSRM leverages large amounts of unlabeled data to compensate for limited labeled data, iteratively improving the model until it approaches the performance of a model trained on the full labeled dataset.
- Core assumption: Unlabeled data is sufficiently representative of the task distribution and the model can generalize from pseudo-labels to improve performance.
- Evidence anchors:
  - [abstract] "SSRM can achieve performance comparable to models trained entirely on labeled data of equivalent volumes."
  - [section 3.2] "The model with SSRM achieves performance metrics closely approaching those of the full SRM model, despite using only one-fourth of the labeled data."
- Break condition: If unlabeled data distribution is too different from labeled data, the model may not generalize effectively.

## Foundational Learning

- Concept: Supervised Reward Modeling (SRM)
  - Why needed here: SRM is the initial step to train a baseline reward model on labeled data, which serves as the starting point for SSRM iterations.
  - Quick check question: What is the loss function used in SRM, and how does it differ from standard supervised fine-tuning?

- Concept: Confidence Scoring and Thresholding
  - Why needed here: Confidence scores are used to filter pseudo-labels, ensuring only high-quality examples are added to the training set to prevent error propagation.
  - Quick check question: How is the confidence score computed, and why is a threshold of 0.8 chosen?

- Concept: Iterative Self-Training
  - Why needed here: Self-training allows the model to iteratively improve by using its own predictions to generate training data, leveraging unlabeled data effectively.
  - Quick check question: What are the three key steps in each iteration of SSRM, and how do they contribute to model improvement?

## Architecture Onboarding

- Component map:
  Pretrained LLM (π_pre_θ) -> Initial SRM on D_l -> Iterative SSRM loop -> Final reward model

- Critical path:
  1. Initial SRM on D_l to obtain π_0_θ
  2. For each iteration t=1 to T-1:
     a. Pseudo-label D_u using π_t_θ
     b. Apply confidence thresholding to select high-confidence examples
     c. Augment D_l with selected pseudo-labels to form D_t
     d. Perform SRM on D_t to obtain π_t+1_θ

- Design tradeoffs:
  - Confidence threshold: Higher threshold reduces noise but may limit data, lower threshold increases data but risks error propagation.
  - Number of iterations: More iterations can improve performance but may lead to overfitting or diminishing returns.
  - Model size: Larger models may benefit more from SSRM due to higher capacity to leverage unlabeled data.

- Failure signatures:
  - Performance plateaus or degrades after iterations: May indicate overfitting or poor quality pseudo-labels.
  - Confidence scores are poorly calibrated: May lead to incorrect filtering of pseudo-labels.
  - Model performs well on pseudo-labels but poorly on held-out data: May indicate overfitting to pseudo-labels.

- First 3 experiments:
  1. Run SSRM with T=1 iteration on a small subset of data to verify the basic pipeline works.
  2. Vary the confidence threshold (e.g., 0.7, 0.8, 0.9) to find the optimal value for the dataset.
  3. Compare SSRM performance with different numbers of iterations (e.g., T=1, 2, 3) to find the point of diminishing returns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal confidence threshold for SSRM across different model sizes and domains?
- Basis in paper: [explicit] The paper mentions using a confidence threshold of 0.8 but notes that "more implementation details can be found in Appendix A" without providing specific analysis on threshold optimization.
- Why unresolved: The paper uses a fixed confidence threshold without exploring how different thresholds might affect performance across various model sizes and domains.
- What evidence would resolve it: A systematic analysis showing how varying the confidence threshold impacts SSRM performance across different model sizes, domains, and iteration counts.

### Open Question 2
- Question: How does SSRM performance degrade when the initial supervised model has very low accuracy?
- Basis in paper: [inferred] The paper notes that "the model after only the initial stage of supervised training might not acquire enough knowledge to accurately assign pseudo-labels to the unlabeled dataset" but doesn't quantify this degradation.
- Why unresolved: While the paper mentions this limitation, it doesn't provide empirical evidence showing how poor initial model performance affects final SSRM results.
- What evidence would resolve it: Experiments demonstrating SSRM performance when starting from models with varying levels of initial accuracy, particularly in cases where the initial model performs near random guessing.

### Open Question 3
- Question: Can SSRM be effectively combined with other semi-supervised learning techniques like consistency regularization?
- Basis in paper: [inferred] The paper focuses on confidence thresholding but doesn't explore potential synergies with other SSL methods like mean teacher or consistency regularization.
- Why unresolved: The paper presents SSRM as a standalone method without investigating how it might benefit from or complement other established SSL techniques.
- What evidence would resolve it: Experiments comparing SSRM performance with and without additional SSL techniques, and analysis of potential complementary effects between methods.

### Open Question 4
- Question: How does SSRM scale with extremely large unlabeled datasets compared to the labeled dataset?
- Basis in paper: [explicit] The paper notes that "the volume of unlabeled data vastly exceeds that of labeled data (n ≫ m)" but doesn't explore scenarios with extreme data imbalances.
- Why unresolved: While the paper acknowledges the typical scale difference between labeled and unlabeled data, it doesn't examine how SSRM performs when this difference becomes extreme.
- What evidence would resolve it: Experiments varying the ratio of unlabeled to labeled data by orders of magnitude, showing how SSRM performance changes as this ratio increases.

## Limitations

- Dataset Representativeness: The effectiveness of SSRM depends on the quality and distribution of the unlabeled dataset, which is not fully specified in the paper and may not generalize to all reward modeling scenarios.
- Confidence Threshold Sensitivity: The fixed confidence threshold of 0.8 may not be optimal across different tasks or datasets, and the sensitivity of SSRM performance to threshold selection is not thoroughly explored.
- Iterative Convergence: The paper doesn't establish clear stopping criteria for SSRM iterations or investigate whether additional iterations beyond three would provide further benefits or potentially cause degradation.

## Confidence

**High Confidence**: The core mechanism of SSRM - using iterative self-training with confidence thresholding to improve reward models using unlabeled data - is well-supported by experimental results across multiple model sizes and the observed calibration improvements.

**Medium Confidence**: The claim that SSRM can achieve performance comparable to fully supervised training using only a fraction of labeled data is supported by experiments, but the exact ratio of labeled to unlabeled data effectiveness may vary significantly based on dataset characteristics.

**Medium Confidence**: The assertion that confidence thresholding prevents performance collapse is theoretically sound and supported by calibration plots, but the robustness of this mechanism across different confidence score distributions and model architectures needs further validation.

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the confidence threshold (0.6, 0.7, 0.8, 0.9) across multiple datasets and model sizes to quantify the sensitivity of SSRM performance to this critical hyperparameter and identify optimal values for different scenarios.

2. **Convergence Behavior Study**: Conduct experiments with 1, 2, 3, 4, and 5 iterations of SSRM to establish clear stopping criteria and identify the point of diminishing returns. Monitor both performance metrics and calibration curves to detect potential overfitting or degradation patterns.

3. **Dataset Distribution Robustness Test**: Evaluate SSRM performance when the unlabeled dataset distribution significantly differs from the labeled dataset. This would test the assumption that unlabeled data must be representative of the task distribution for effective generalization.