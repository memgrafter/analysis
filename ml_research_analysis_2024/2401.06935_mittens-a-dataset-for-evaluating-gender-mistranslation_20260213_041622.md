---
ver: rpa2
title: 'MiTTenS: A Dataset for Evaluating Gender Mistranslation'
arxiv_id: '2401.06935'
source_url: https://arxiv.org/abs/2401.06935
tags:
- gender
- evaluation
- translation
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MiTTenS, a dataset for evaluating gender
  mistranslation across 26 languages, addressing the need for more precise measurement
  of potential harms in translation systems. The dataset includes handcrafted passages
  targeting known failure patterns, synthetically generated passages, and natural
  passages from multiple domains.
---

# MiTTenS: A Dataset for Evaluating Gender Mistranslation

## Quick Facts
- **arXiv ID**: 2401.06935
- **Source URL**: https://arxiv.org/abs/2401.06935
- **Reference count**: 9
- **Primary result**: MiTTenS dataset reveals gender mistranslation across 26 languages in both NMT and foundation models, with systems performing worse on passages requiring translation to "she" compared to "he"

## Executive Summary
MiTTenS is a comprehensive dataset for evaluating gender mistranslation across 26 languages, addressing the critical need for precise measurement of potential harms in translation systems. The dataset combines handcrafted passages targeting known failure patterns, synthetically generated passages for scalable evaluation, and natural passages from multiple domains. Through evaluation of both neural machine translation systems and foundation models, the authors demonstrate that all systems exhibit gender mistranslation and potential harm, even in high-resource languages. The disaggregated analysis reveals that systems perform worse on passages requiring translation to "she" compared to "he", and that there is no clear pattern to which languages are most challenging across systems.

## Method Summary
The MiTTenS dataset is constructed with three types of evaluation sets: handcrafted passages targeting known failure patterns, longer synthetically generated passages, and natural passages from multiple domains. The dataset covers 26 languages and includes passages requiring translation in both directions (into and out of English). Automated evaluation is used for 2en translation while human evaluation protocol is applied for 2xx translation. The evaluation includes multiple systems including NLLB, GPT-4, GPT 3.5, Gemini Pro, PaLM 2, and Mistral. Performance is analyzed disaggregated by language, gender pronoun, and evaluation set to identify specific areas for improvement.

## Key Results
- All evaluated systems exhibit gender mistranslation across all 26 languages
- Systems perform significantly worse on passages requiring translation to "she" compared to "he"
- No clear pattern emerges for which languages are most challenging across different systems
- Synthetic data generation successfully avoids contamination while enabling automated evaluation
- Disaggregated analysis reveals specific failure modes that overall metrics miss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Handcrafted evaluation sets targeting known failure patterns improve measurement precision for gender mistranslation.
- Mechanism: By constructing passages that explicitly trigger gender agreement and coreference errors, the dataset can isolate specific linguistic phenomena that are prone to mistranslation.
- Core assumption: Translators exhibit predictable failure modes on structured linguistic constructions that can be manually identified.
- Evidence anchors:
  - [abstract]: "The dataset is constructed with handcrafted passages that target known failure patterns"
  - [section 2.1]: "The Gender Sets evaluation set was built from error analysis in publicly available translation systems"
  - [corpus]: Weak evidence - corpus shows related work on gender bias evaluation but doesn't directly confirm handcrafted approach effectiveness
- Break condition: If systems learn to recognize and avoid these specific patterns during training, the handcrafted sets lose their discriminative power.

### Mechanism 2
- Claim: Synthetic data generation avoids contamination while enabling scalable evaluation.
- Mechanism: By creating synthetic passages with single-gendered entities, the dataset can be automatically scored for gender pronoun accuracy without manual annotation.
- Core assumption: Synthetic data can represent realistic translation scenarios while being provably uncontaminated by training data.
- Evidence anchors:
  - [section 2.2]: "Using synthetic data avoids potential data contamination from sources like Translated Wikipedia Biographies"
  - [abstract]: "The dataset is constructed with... longer synthetically generated passages"
  - [corpus]: Weak evidence - related papers discuss synthetic data but don't validate this specific contamination avoidance claim
- Break condition: If synthetic passages fail to capture the complexity of real-world translation scenarios, the evaluation becomes less representative.

### Mechanism 3
- Claim: Disaggregated evaluation by language, gender pronoun, and evaluation set reveals system-specific weaknesses.
- Mechanism: By analyzing performance across multiple dimensions, the dataset can identify precise areas for improvement rather than just overall accuracy.
- Core assumption: Translation systems exhibit different failure patterns across languages and linguistic constructions.
- Evidence anchors:
  - [section 3]: "We highlight specific areas of improvement for each system with disaggregated analysis by language and evaluation set"
  - [abstract]: "The dataset can be used to pinpoint areas for targeted improvement"
  - [corpus]: Weak evidence - corpus shows interest in disaggregated evaluation but doesn't directly confirm this mechanism
- Break condition: If systems optimize for overall accuracy rather than individual dimensions, disaggregated metrics may not drive improvement.

## Foundational Learning

- **Concept: Gender encoding across languages**
  - Why needed here: Understanding how gender is encoded (pronouns, agreement, lexical choices) is crucial for interpreting evaluation results
  - Quick check question: Why do Finnish and Bengali require different evaluation approaches than Spanish?

- **Concept: Translation evaluation metrics**
  - Why needed here: Knowing the limitations of automated metrics (BLEU, BLEURT) helps understand why this dataset focuses on specific error patterns
  - Quick check question: What aspect of gender mistranslation might automated metrics miss?

- **Concept: Data contamination in ML evaluation**
  - Why needed here: Understanding contamination helps appreciate why synthetic data generation was chosen
  - Quick check question: How might training data contamination affect the validity of evaluation results?

## Architecture Onboarding

- **Component map**: Dataset construction → Synthetic generation → Manual crafting → Evaluation pipeline → System comparison
- **Critical path**: Dataset construction → Translation evaluation → Disaggregated analysis → Targeted improvement
- **Design tradeoffs**: Precision vs. coverage (targeted handcrafted vs. broad natural passages), automation vs. accuracy (synthetic vs. human evaluation)
- **Failure signatures**: Systems performing worse on "she" vs. "he" passages, poor performance on late-binding constructions, degradation in low-resource languages
- **First 3 experiments**:
  1. Evaluate a system on the coref:coreference subset and compare "she" vs. "he" accuracy
  2. Test the same system on late_binding subset to identify late-binding failures
  3. Run the system on encoded_in_nouns subset for languages without pronoun gender encoding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automated evaluation methods be improved to capture gender mistranslation in direct translation between languages beyond English?
- Basis in paper: [inferred] The paper mentions that the dataset does not include examples for direct translation between languages beyond English and that evaluation of gender agreement out of English remains challenging and time-intensive.
- Why unresolved: Direct translation between languages beyond English is a common use case for translation systems, but the current dataset and evaluation methods are limited in their ability to capture gender mistranslation in these scenarios.
- What evidence would resolve it: Development and validation of automated evaluation methods specifically designed to capture gender mistranslation in direct translation between languages beyond English, with demonstrated effectiveness across a range of language pairs.

### Open Question 2
- Question: What are the underlying factors that contribute to the observed differences in performance across languages and evaluation sets, and how can these be addressed?
- Basis in paper: [explicit] The paper highlights that there is no clear pattern to which languages are most challenging across systems and that different areas of weakness are observed even within the same model families.
- Why unresolved: Understanding the factors that contribute to differences in performance across languages and evaluation sets is crucial for developing targeted improvements to translation systems.
- What evidence would resolve it: Analysis of the linguistic and cultural factors that contribute to differences in performance across languages and evaluation sets, along with proposed solutions for addressing these factors.

### Open Question 3
- Question: How can translation systems be improved to better handle non-binary gender expressions across languages and cultures?
- Basis in paper: [explicit] The paper acknowledges that the dataset does not cover non-binary gender expressions and that such work requires participatory perspectives and expert knowledge on both gender and individual languages.
- Why unresolved: Non-binary gender expressions are an important aspect of gender identity, and translation systems need to be able to accurately represent these expressions across languages and cultures.
- What evidence would resolve it: Development and validation of translation systems that can accurately handle non-binary gender expressions across languages and cultures, with demonstrated effectiveness in real-world applications.

## Limitations
- The dataset excludes several high-resource languages with documented gender translation issues (e.g., German, Dutch)
- Synthetic passages may not fully capture the complexity of real-world translation scenarios
- Automated evaluation relies on exact string matching, potentially missing nuanced gender translation errors

## Confidence

**High Confidence**: The core claim that gender mistranslation exists across all evaluated systems is well-supported by the experimental results. The disaggregated analysis methodology and the observation that systems perform worse on "she" passages are robust findings.

**Medium Confidence**: The claim that MiTTenS can pinpoint targeted areas for improvement is supported by the disaggregated analysis, but the practical impact of these insights on system development is not demonstrated.

**Low Confidence**: The assertion that no clear pattern exists for which languages are most challenging across systems is based on comparison across different model families, but the analysis doesn't control for factors like training data size or language family similarities.

## Next Checks

1. **Cross-dataset Validation**: Evaluate the same systems on other established gender bias datasets (e.g., WinoMT, Basta) to determine whether MiTTenS captures unique failure modes or overlaps significantly with existing evaluation approaches.

2. **Human Evaluation Correlation**: Conduct human evaluation studies on a subset of MiTTenS passages to validate the automated evaluation metrics, particularly for synthetic passages where human judgment might differ from exact string matching.

3. **System Improvement Cycle**: Use MiTTenS to identify specific weaknesses in a baseline system, implement targeted improvements addressing those weaknesses, and re-evaluate to demonstrate whether the dataset effectively guides system enhancement.