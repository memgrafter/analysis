---
ver: rpa2
title: Sentence-Level or Token-Level? A Comprehensive Study on Knowledge Distillation
arxiv_id: '2404.14827'
source_url: https://arxiv.org/abs/2404.14827
tags:
- distillation
- token-level
- sentence-level
- knowledge
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a comprehensive study on knowledge distillation
  for neural machine translation, analyzing the differences between sentence-level
  and token-level distillation methods. The authors hypothesize that token-level distillation
  is more effective in simpler scenarios, while sentence-level distillation excels
  in complex ones.
---

# Sentence-Level or Token-Level? A Comprehensive Study on Knowledge Distillation

## Quick Facts
- arXiv ID: 2404.14827
- Source URL: https://arxiv.org/abs/2404.14827
- Authors: Jingxuan Wei; Linzhuang Sun; Yichong Leng; Xu Tan; Bihui Yu; Ruifeng Guo
- Reference count: 24
- Key outcome: Proposes a hybrid knowledge distillation method that dynamically combines sentence-level and token-level approaches using a gating mechanism, achieving BLEU score of 39.30 on IWSLT14 German-to-English dataset

## Executive Summary
This paper conducts a comprehensive study on knowledge distillation methods for neural machine translation, systematically comparing sentence-level and token-level approaches. The authors hypothesize that token-level distillation is more effective in simpler scenarios, while sentence-level distillation excels in complex ones. To validate this, they conduct extensive experiments varying student model size, text complexity, and decoding difficulty. Based on their findings, they propose a hybrid method that dynamically combines both distillation approaches using a gating mechanism, which outperforms individual distillation methods and achieves state-of-the-art results on IWSLT14 German-to-English translation.

## Method Summary
The method involves training a student NMT model using knowledge distillation from a fixed BiBERT teacher model. Two distillation approaches are compared: sentence-level (using pseudo target sequences) and token-level (using teacher token distributions). The proposed hybrid method introduces a gating mechanism that dynamically weights these two losses per input sequence. The gate value is learned during training and adjusts the balance between sentence-level and token-level distillation based on input sequence difficulty. Experiments are conducted on multiple datasets (IWSLT13 en→fr, IWSLT14 de→en, WMT14 en→de, IWSLT17 ar→en) with varying student model sizes, text complexity (via noise injection), and decoding strategies (teacher forcing vs beam search).

## Key Results
- Hybrid method achieves BLEU score of 39.30 on IWSLT14 German-to-English dataset
- Token-level distillation performs better in simpler scenarios (larger student models, less complex text, teacher forcing)
- Sentence-level distillation excels in complex scenarios (smaller student models, more complex text, beam search)
- Dynamic gating mechanism effectively balances the two distillation methods, with gate value shifting from favoring sentence-level early in training to token-level later

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level distillation performs better in simpler scenarios because it transfers detailed distribution information that the student can more easily learn when the context is less complex.
- Mechanism: When the student model has sufficient capacity, the text is simple, and decoding is well-guided (e.g., teacher forcing), the fine-grained probability distributions from the teacher are easier to match and lead to better performance.
- Core assumption: The complexity of the distillation objective is directly related to the complexity of the scenario; token-level (distribution matching) is more complex than sentence-level (sequence matching).
- Evidence anchors:
  - [abstract] "we argue that token-level distillation, with its more complex objective (i.e., distribution), is better suited for 'simple' scenarios, while sentence-level distillation excels in 'complex' scenarios."
  - [section 3.3] "In the more complex beam search scenario, sentence-level distillation tends to outperform token-level distillation, as indicated by the positive values in △BS."
- Break condition: If the student model is too small or the text is too complex, the fine-grained token-level supervision becomes overwhelming and harms learning.

### Mechanism 2
- Claim: Sentence-level distillation performs better in complex scenarios because it provides a simpler, more globally coherent target that reduces training difficulty.
- Mechanism: When the student model is small, the text is complex, or decoding is ambiguous (e.g., beam search), matching the full output sequence is easier than matching the detailed token distribution, and this simplification aids learning.
- Core assumption: Simpler training targets reduce cognitive load on the student model, especially under constrained capacity or noisy inputs.
- Evidence anchors:
  - [abstract] "sentence-level distillation excels in 'complex' scenarios."
  - [section 3.3] "As the text complexity increases, both token-level and sentence-level distillation show a decrease in performance. However, sentence-level distillation demonstrates greater resilience."
- Break condition: If the scenario is too simple, the coarser sentence-level target may underutilize the student model's capacity and miss fine-grained improvements.

### Mechanism 3
- Claim: The hybrid gating mechanism works by dynamically balancing sentence-level and token-level distillation according to the input sequence's difficulty, allowing the model to adapt during training.
- Mechanism: The gate value g(x) modulates the loss between the two distillation methods per sequence; initially favoring sentence-level (easier), then shifting toward token-level as training progresses and the model becomes more capable.
- Core assumption: The difficulty of learning from each distillation method varies across training epochs and sequences; a learnable gate can track this and adjust accordingly.
- Evidence anchors:
  - [section 4.6] "We find that at the beginning of the learning process of G, its value is around 0.72... With further training (around 50 epochs), G gradually rises to 0.85... Eventually, the value of G approaches 1."
  - [section 4.1] "This formulation allows L(x) to represent the combined loss for a given input sequence x, effectively integrating the token-level and sentence-level distillation losses."
- Break condition: If the gate becomes stuck at an extreme value or fails to adapt to sequence difficulty, the hybrid method loses its advantage over individual distillation approaches.

## Foundational Learning

- Concept: Knowledge distillation in NMT
  - Why needed here: The paper's main contribution hinges on understanding how and when to apply sentence-level vs. token-level distillation, so foundational knowledge of both methods is essential.
  - Quick check question: What is the key difference between sentence-level and token-level distillation in terms of training target?

- Concept: Model capacity and its impact on learning
  - Why needed here: The paper systematically varies student model size to show how complexity affects distillation effectiveness; understanding capacity constraints is critical.
  - Quick check question: Why might a smaller student model benefit more from sentence-level distillation than token-level?

- Concept: Decoding strategies (teacher forcing vs. beam search)
  - Why needed here: The paper contrasts these methods to show how decoding difficulty influences distillation choice; familiarity with both is necessary.
  - Quick check question: How does teacher forcing simplify the decoding process compared to beam search?

## Architecture Onboarding

- Component map: Teacher BiBERT model -> Student NMT model (receives both sentence-level and token-level supervision) -> Gating mechanism (dynamically weights losses) -> Combined loss (backpropagated to update student)
- Critical path: (1) Teacher generates pseudo outputs and token distributions; (2) Student processes input and produces predictions; (3) Losses from both distillation methods are computed; (4) Gate g(x) weights each loss; (5) Combined loss is backpropagated to update the student
- Design tradeoffs: Sentence-level distillation is simpler but may miss fine-grained knowledge; token-level distillation is richer but harder to learn from; the gate adds complexity and training overhead but aims to combine strengths adaptively
- Failure signatures: If the gate fails to adapt, the hybrid may degrade to one method; if either distillation method is poorly implemented, overall BLEU drops; if the student is too small, both methods may fail, especially token-level
- First 3 experiments:
  1. Reproduce the model size ablation: train student models of varying sizes (3M, 9M, 38M) and compare sentence-level vs. token-level BLEU scores
  2. Test text complexity impact: corrupt the dataset with noise at moderate and high levels, then measure relative performance of both distillation methods
  3. Evaluate decoding difficulty: train with both teacher forcing and beam search decoding, and compare how BLEU scores shift between distillation strategies

## Open Questions the Paper Calls Out

- Question: How does the effectiveness of the hybrid method scale when applied to languages with significantly different syntactic structures, such as Chinese or Japanese?
- Basis in paper: [inferred] The paper focuses on European languages (German, French, English, Arabic) and does not explore the method's applicability to languages with different syntactic structures.
- Why unresolved: The paper's experimental scope is limited to languages with relatively similar syntactic structures, leaving the method's performance on languages with more complex or divergent syntax unexplored.
- What evidence would resolve it: Conducting experiments on a diverse set of languages with varying syntactic structures, including those with logographic writing systems like Chinese or Japanese, would provide insights into the method's generalizability.

- Question: Can the dynamic gating mechanism be extended to incorporate additional distillation strategies beyond sentence-level and token-level, such as multi-teacher distillation or data augmentation techniques?
- Basis in paper: [inferred] The paper focuses on combining sentence-level and token-level distillation but does not explore the potential integration of other distillation strategies.
- Why unresolved: The paper's hybrid method is specifically designed for sentence-level and token-level distillation, and the impact of incorporating additional strategies is not investigated.
- What evidence would resolve it: Developing an extended gating mechanism that can dynamically balance multiple distillation strategies and evaluating its performance on various NMT tasks would shed light on the potential benefits of such an approach.

- Question: How does the hybrid method perform in low-resource scenarios, where the amount of training data is limited compared to the datasets used in the paper?
- Basis in paper: [inferred] The paper's experiments are conducted on datasets with substantial amounts of parallel data, and the method's effectiveness in low-resource settings is not explored.
- Why unresolved: The paper does not investigate the impact of data scarcity on the hybrid method's performance, leaving its suitability for low-resource NMT applications unclear.
- What evidence would resolve it: Evaluating the hybrid method on low-resource language pairs with limited parallel data and comparing its performance to existing low-resource NMT techniques would provide insights into its potential in such scenarios.

## Limitations

- Generalizability to larger models: All experiments use relatively small student models (3M to 38M parameters); findings may not extend to larger transformer-based models
- Dataset bias: All experiments use IWSLT datasets focused on spoken language translation; results may not transfer to larger, more diverse datasets like WMT news translation tasks
- Teacher model dependency: Study relies on fixed BiBERT teacher; effectiveness may vary substantially with different teacher architectures or capacity mismatches

## Confidence

- **High confidence**: The core hypothesis that token-level distillation performs better in simpler scenarios while sentence-level excels in complex ones is well-supported by systematic ablation studies across multiple dimensions
- **Medium confidence**: The proposed hybrid gating mechanism's superiority over individual methods is demonstrated, but evidence is primarily from IWSLT datasets and may be sensitive to initialization
- **Medium confidence**: The claim that the hybrid method achieves state-of-the-art performance with BLEU 39.30 on IWSLT14 de-en is credible but lacks comparison to most recent KD methods

## Next Checks

1. **Scale validation**: Reproduce the main experiments using larger student models (100M+ parameters) and a strong teacher model (e.g., BERT-large or T5) to verify if the token-level vs sentence-level effectiveness pattern holds at scale

2. **Dataset generalization**: Test the hybrid method on WMT14 en-de dataset with both small and large student models to assess performance in a larger-scale, news-domain setting with different linguistic complexity

3. **Teacher architecture ablation**: Replace the BiBERT teacher with a standard BERT-base teacher and a smaller DistilBERT teacher to measure how sensitive the hybrid method's performance is to teacher model choice and capacity