---
ver: rpa2
title: Since the Scientific Literature Is Multilingual, Our Models Should Be Too
arxiv_id: '2403.18251'
source_url: https://arxiv.org/abs/2403.18251
tags:
- language
- languages
- latin
- which
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The scientific literature is largely multilingual, with English
  comprising 85% of abstracts and 50 other languages making up the remainder. English-only
  models fail to create meaningful representations for non-English papers, with models
  like SciBERT producing unknown tokens for 92.95% of Chinese characters and 81.66%
  of Korean characters.
---

# Since the Scientific Literature Is Multilingual, Our Models Should Be Too

## Quick Facts
- **arXiv ID**: 2403.18251
- **Source URL**: https://arxiv.org/abs/2403.18251
- **Reference count**: 26
- **Primary result**: Scientific literature is largely multilingual, with English-only models failing to represent non-English papers meaningfully, producing unknown tokens for 92.95% of Chinese characters and 81.66% of Korean characters in SciBERT.

## Executive Summary
The scientific literature is predominantly multilingual, with English comprising only 85% of abstracts while 50 other languages make up the remainder. Current English-centric language models like SciBERT and Specter perform poorly on non-English scientific content, producing unknown tokens and nonsensical outputs for papers written in non-Latin scripts. This creates significant gaps in scientific search, retrieval, and summarization systems that rely on these models. The authors demonstrate that pseudo-perplexity scores for non-English languages using SciBERT range from 6.26 to 148.25, compared to 4.05 for multilingual models like XLM-R, highlighting the critical need for truly multilingual scientific language models.

## Method Summary
The authors analyzed 12.7 million scientific paper abstracts from the Semantic Scholar Open Research Corpus, detecting 51 languages and identifying the prevalence of multilingual content in scientific publishing. They evaluated the performance of English-only models (SciBERT) and multilingual models (XLM-R) on non-English content, measuring token-level failures and pseudo-perplexity scores. The study examined document similarity metrics using text-based models like Specter and documented real-world failures in Semantic Scholar's "tl;dr" generation system for non-Latin script papers. The analysis revealed systematic failures of monolingual models to handle the multilingual nature of scientific literature.

## Key Results
- SciBERT produces unknown tokens for 92.95% of Chinese characters and 81.66% of Korean characters, rendering non-Latin script papers essentially unrepresentable
- Pseudo-perplexity scores for non-English languages using SciBERT range from 6.26 to 148.25, compared to 4.05 for XLM-R
- Cosine similarity between document vectors and reference centroids is lower for non-English papers when using text-based models like Specter
- Semantic Scholar's "tl;dr" system generates nonsensical or random outputs for non-Latin script papers due to model limitations

## Why This Works (Mechanism)
The multilingual nature of scientific literature creates fundamental challenges for English-only language models. When these models encounter non-English text, they either produce unknown tokens or generate poor-quality representations that fail to capture semantic meaning. This occurs because monolingual models lack the vocabulary and contextual understanding needed to process diverse linguistic structures, scripts, and scientific terminology across languages. The failure is particularly severe for non-Latin scripts where tokenization and subword modeling break down completely, leading to cascading errors in downstream applications like search, retrieval, and summarization.

## Foundational Learning

**Multilingual Document Representation**: Understanding how documents in different languages can be meaningfully compared and clustered
- Why needed: Essential for cross-lingual scientific search and discovery
- Quick check: Cosine similarity should be comparable across languages for equivalent documents

**Tokenization and Subword Modeling**: How language models split text into processing units
- Why needed: Critical for understanding why non-Latin scripts fail in English models
- Quick check: Unknown token rates indicate tokenization breakdown

**Pseudo-Perplexity**: A measure of how well a model predicts held-out text
- Why needed: Quantifies model performance on non-English scientific content
- Quick check: Lower scores indicate better language modeling capability

**Cross-Lingual Transfer**: How knowledge from one language can benefit model performance in another
- Why needed: Basis for building effective multilingual scientific models
- Quick check: Similar performance metrics across languages indicate successful transfer

## Architecture Onboarding

**Component Map**: Language Detection -> Tokenization -> Model Processing -> Output Generation -> Evaluation Metrics
**Critical Path**: Input Text → Language Detection → Appropriate Model Selection → Tokenization → Model Forward Pass → Output Generation → Evaluation
**Design Tradeoffs**: Monolingual models offer depth in specific languages but fail on others; multilingual models provide breadth but may sacrifice some language-specific nuance
**Failure Signatures**: High unknown token rates, poor pseudo-perplexity scores, low cosine similarity for non-English documents, nonsensical generated summaries
**3 First Experiments**:
1. Test language detection accuracy across 51 languages using the Semantic Scholar corpus
2. Measure unknown token rates for SciBERT and XLM-R across different language families
3. Compare pseudo-perplexity scores for English-only vs multilingual models on non-English scientific text

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Sample representativeness uncertainty for newer publications and underrepresented scientific fields
- Abstract-level analysis may not capture full-text multilingual patterns
- Token-level error rates have High confidence for Chinese/Korean but Medium for other languages due to smaller samples
- Pseudo-perplexity comparisons may not generalize across all scientific domains

## Confidence
- **High**: Specific token-level error rates for Chinese and Korean datasets tested with SciBERT
- **Medium**: Token-level error rates for other languages due to smaller sample sizes
- **High**: Pseudo-perplexity metric comparison for specific test sets used
- **Medium**: Generalization of findings across all scientific domains
- **Low**: Representativeness of findings for all time periods and scientific fields

## Next Checks
1. Validate findings across different time periods to assess temporal trends in multilingual scientific publishing
2. Test model performance on full-text scientific papers rather than just abstracts to identify scale differences in multilingual handling
3. Conduct human evaluation studies to verify that nonsensical outputs for non-Latin scripts are perceived as such across different linguistic backgrounds