---
ver: rpa2
title: 'SOAP: Improving and Stabilizing Shampoo using Adam'
arxiv_id: '2409.11321'
source_url: https://arxiv.org/abs/2409.11321
tags:
- shampoo
- soap
- learning
- adamw
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SOAP (ShampoO with Adam in the Preconditioner's
  eigenbasis), a novel optimizer that improves upon Shampoo by running AdamW in the
  eigenbasis provided by Shampoo's preconditioner. The authors establish a formal
  connection between Shampoo and Adafactor, showing that Shampoo is equivalent to
  running Adafactor in its own eigenspace.
---

# SOAP: Improving and Stabilizing Shampoo using Adam

## Quick Facts
- arXiv ID: 2409.11321
- Source URL: https://arxiv.org/abs/2409.11321
- Reference count: 40
- One-line primary result: SOAP achieves 40% fewer iterations and 35% faster wall clock time than AdamW while being more robust to hyperparameters

## Executive Summary
This paper introduces SOAP (ShampoO with Adam in the Preconditioner's eigenbasis), a novel optimizer that improves upon Shampoo by running AdamW in the eigenbasis provided by Shampoo's preconditioner. The authors establish a formal connection between Shampoo and Adafactor, showing that Shampoo is equivalent to running Adafactor in its own eigenspace. This insight leads to the design of SOAP, which offers several advantages: it introduces only one additional hyperparameter (preconditioning frequency) compared to AdamW, demonstrates greater robustness to large preconditioning frequencies, and outperforms both Shampoo and AdamW in language model pre-training tasks.

## Method Summary
SOAP rotates the gradient into Shampoo's eigenbasis, applies AdamW updates, then rotates back. This allows SOAP to continually update the second moment estimate (like Adam) in the slowly changing coordinate basis, mitigating the performance degradation seen in Shampoo when preconditioning frequency is reduced. The method is implemented by computing the eigenvectors of Shampoo's L and R matrices, rotating the gradient into this basis, applying AdamW updates, and rotating back before updating the weights.

## Key Results
- Reduces iterations by over 40% and wall clock time by over 35% compared to AdamW
- Achieves approximately 20% improvements in both metrics compared to Shampoo
- Demonstrates greater robustness to large preconditioning frequencies than Shampoo
- Introduces only one additional hyperparameter (preconditioning frequency) compared to AdamW

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SOAP achieves faster convergence than Shampoo and AdamW by running AdamW in the eigenbasis provided by Shampoo's preconditioner
- Mechanism: SOAP rotates the gradient into Shampoo's eigenbasis, applies AdamW updates, then rotates back, continually updating the second moment estimate in the slowly changing coordinate basis
- Core assumption: Shampoo's preconditioner provides a coordinate basis that is close to optimal for running a diagonal preconditioner like AdamW
- Evidence anchors: [abstract] "SOAP mitigates this degradation by continually updating the running average of the second moment"; [section 4.1] "SOAP mitigates this degradation by continually updating the running average of the second moment"
- Break condition: If the preconditioning frequency is too low, or if the eigenbasis provided by Shampoo is far from optimal

### Mechanism 2
- Claim: SOAP reduces the number of hyperparameters compared to Shampoo, resulting in only one additional hyperparameter compared to AdamW
- Mechanism: By using Shampoo's eigenbasis as the coordinate system, SOAP inherits the adaptive learning rates of AdamW without needing separate hyperparameters for the Shampoo preconditioner
- Core assumption: The eigenbasis provided by Shampoo is sufficient to capture the essential geometry of the loss landscape
- Evidence anchors: [abstract] "since SOAP is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter"; [section 4.1] "since SOAP is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter"
- Break condition: If the eigenbasis provided by Shampoo is not sufficient to capture the essential geometry of the loss landscape

### Mechanism 3
- Claim: SOAP demonstrates greater robustness to large preconditioning frequency compared to Shampoo
- Mechanism: By continually updating the second moment estimate in the eigenbasis, SOAP is less sensitive to the frequency of preconditioner updates than Shampoo
- Core assumption: Continually updating the second moment estimate in the eigenbasis is more important for performance than the frequency of preconditioner updates
- Evidence anchors: [abstract] "SOAP demonstrates greater robustness to large preconditioning frequency compared to Shampoo"; [section 6.2] "At higher frequencies, the performance of both SOAP and Shampoo degrades but SOAP's performance degrades significantly slower than Shampoo's"
- Break condition: If the preconditioning frequency is too low, or if the eigenbasis provided by Shampoo is far from optimal

## Foundational Learning

- Concept: Kronecker factorization
  - Why needed here: Shampoo and SOAP use Kronecker factorization to approximate the second moment matrix, allowing more efficient capture of loss landscape geometry
  - Quick check question: What is the computational complexity of Kronecker factorization compared to full matrix factorization?

- Concept: Eigenbasis
  - Why needed here: SOAP rotates the gradient into the eigenbasis provided by Shampoo's preconditioner to apply AdamW updates in a coordinate system that better captures loss landscape geometry
  - Quick check question: How does the eigenbasis of a matrix relate to its eigenvectors and eigenvalues?

- Concept: Preconditioning frequency
  - Why needed here: SOAP's performance is sensitive to how often the eigenbasis is recomputed
  - Quick check question: How does the preconditioning frequency affect the trade-off between computational cost and convergence speed?

## Architecture Onboarding

- Component map: Gradient -> Rotation into eigenbasis -> AdamW updates -> Rotation back -> Weight update
- Critical path:
  1. Compute the gradient of the loss with respect to the weights
  2. Rotate the gradient into the eigenbasis provided by Shampoo's preconditioner
  3. Apply AdamW updates in the rotated space
  4. Rotate the updated gradient back to the original space
  5. Update the weights
- Design tradeoffs:
  - Preconditioning frequency vs computational cost: Higher frequency leads to better performance but higher computational cost
  - Eigenbasis vs diagonal approximation: Eigenbasis captures more geometry but is more expensive to compute
  - AdamW vs Adafactor: AdamW has more hyperparameters but is more robust to hyperparameters than Adafactor
- Failure signatures:
  - Poor convergence: May indicate preconditioning frequency is too low or eigenbasis is not optimal
  - High computational cost: May indicate preconditioning frequency is too high or eigenbasis is too expensive to compute
  - Unstable training: May indicate hyperparameters are not well-tuned or architecture is not well-suited for SOAP
- First 3 experiments:
  1. Run SOAP with different preconditioning frequencies and compare convergence speed and computational cost
  2. Run SOAP with different eigenbasis approximations (e.g., full vs. truncated) and compare convergence speed and computational cost
  3. Run SOAP with different diagonal preconditioners (e.g., AdamW vs. Adafactor) and compare convergence speed and robustness to hyperparameters

## Open Questions the Paper Calls Out

- Question: What is the theoretical justification for the optimal preconditioning frequency that balances computational overhead and convergence rate?
  - Basis in paper: [inferred] The paper observes that SOAP's performance degrades slower than Shampoo with increasing preconditioning frequency, but does not provide a theoretical explanation
  - Why unresolved: The relationship between preconditioning frequency, computational overhead, and convergence is complex and likely depends on multiple factors
  - What evidence would resolve it: Theoretical analysis deriving optimal preconditioning frequency as a function of model characteristics, validated by extensive empirical studies

- Question: How does SOAP's performance compare to other second-order optimization methods like KFAC or E-KFAC in terms of both efficiency and final model quality?
  - Basis in paper: [explicit] The paper mentions related works on second-order optimization but does not include direct comparisons
  - Why unresolved: The paper focuses on comparing SOAP with Shampoo and AdamW, leaving open questions about relative performance compared to other second-order methods
  - What evidence would resolve it: Comprehensive empirical studies comparing SOAP, KFAC, E-KFAC, and other second-order methods across various model architectures and tasks

- Question: Can the space and time efficiency improvements proposed for SOAP be further enhanced through novel algorithmic approaches or hardware-specific optimizations?
  - Basis in paper: [explicit] The paper discusses potential improvements to SOAP's space and time complexity but does not explore novel algorithmic approaches
  - Why unresolved: The paper identifies existing methods for improving efficiency but does not investigate new techniques that could further reduce computational requirements
  - What evidence would resolve it: Development and empirical evaluation of new algorithmic approaches or hardware-specific optimizations that significantly reduce SOAP's space and time complexity

## Limitations
- Lacks direct experimental comparisons of SOAP vs Shampoo or AdamW convergence behavior
- Does not provide detailed comparisons of SOAP's hyperparameter count vs Shampoo's
- Missing direct experimental comparisons of SOAP's robustness to preconditioning frequency vs Shampoo's

## Confidence
- Low: The core assumptions underlying the mechanisms lack direct empirical support in the corpus
- Low: While theoretical connections are established, practical implications for convergence and robustness are not fully validated

## Next Checks
1. Run SOAP with different preconditioning frequencies and directly compare convergence speed and computational cost to Shampoo and AdamW
2. Implement SOAP from the provided theoretical framework and validate that it correctly rotates gradients into Shampoo's eigenbasis, applies AdamW updates, and rotates back
3. Conduct ablation studies varying the eigenbasis approximation quality and measure the impact on convergence speed to test the assumption about the sufficiency of Shampoo's eigenbasis