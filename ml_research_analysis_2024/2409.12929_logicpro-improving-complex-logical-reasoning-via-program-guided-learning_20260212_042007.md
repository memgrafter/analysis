---
ver: rpa2
title: 'LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning'
arxiv_id: '2409.12929'
source_url: https://arxiv.org/abs/2409.12929
tags:
- data
- reasoning
- step
- code
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LogicPro is a novel data synthesis method that leverages LeetCode-style
  algorithm problems and their program solutions to generate complex logical reasoning
  data in text format. The approach transforms algorithmic problems and test cases
  into reasoning tasks, executes Python solutions to obtain intermediate variable
  values, and uses these as guidance to synthesize high-quality reasoning processes.
---

# LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning

## Quick Facts
- arXiv ID: 2409.12929
- Source URL: https://arxiv.org/abs/2409.12929
- Reference count: 29
- One-line primary result: LogicPro improves model performance across multiple reasoning benchmarks with 2.3%-4.7% gains on BBH and 3.9%-8% average improvements

## Executive Summary
LogicPro introduces a novel data synthesis method that transforms LeetCode-style algorithm problems and their program solutions into complex logical reasoning tasks in text format. The approach executes Python solutions to capture intermediate variable values, then uses these as guidance to synthesize high-quality reasoning processes. This program-guided learning framework produces a 540K dataset from 2,360 algorithm problems that significantly improves model performance on multiple reasoning benchmarks.

The method addresses the challenge of creating sufficiently difficult logical reasoning data by leveraging the inherent complexity of algorithmic problems. Through a three-step synthesis process involving problem transformation, code execution, and reasoning synthesis, LogicPro generates data that is both challenging for current models and effective for improving their reasoning capabilities across diverse tasks including BBH27, LogicBench, DROP, AR-LSAT, and GSM8K.

## Method Summary
LogicPro synthesizes complex logical reasoning data through a three-step pipeline: (1) constructing reasoning problems from LeetCode algorithm problems and their test cases, (2) executing code solutions to extract intermediate variable values and final answers, and (3) generating human-readable reasoning processes guided by the intermediate variables. The method uses 2,360 LeetCode problems with GPT-4-generated test cases (150 per problem) and transforms them into narrative reasoning tasks while preserving their logical complexity. The synthesized dataset contains 540K examples with standard answers and reasoning paths, demonstrating significant improvements across multiple reasoning benchmarks.

## Key Results
- LogicPro improves model performance by 2.3%-4.7% on BBH benchmarks and 3.9%-8% on average metrics across different model scales
- The dataset is sufficiently difficult, with baseline models achieving only 45.6% average accuracy on LogicPro problems
- Outperforms existing reasoning datasets including RuleTakers, LogicNLI, ProofWriter, and LogicalFix on out-of-distribution benchmarks
- Improves performance across diverse reasoning tasks: mathematical reasoning (GSM8K), commonsense reasoning (BBH), and logical reasoning (LogicBench, AR-LSAT)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Program-guided learning improves reasoning quality by exposing intermediate variable states during execution
- Mechanism: The synthesis process executes standard Python solutions with test cases, capturing key intermediate variable values, which are then used to guide the generation of human-readable reasoning steps
- Core assumption: Intermediate variables in algorithmic code reflect the reasoning steps a human would use to solve the problem
- Evidence anchors: [abstract] "With the guidance of code intermediate variables, we synthesize the text reasoning process for each reasoning problems" and [section 2.3] "we modify the code to print important intermediate variable values"

### Mechanism 2
- Claim: Transforming algorithmic problems into text-based reasoning tasks preserves the underlying logical structure while making them accessible to language models
- Mechanism: LeetCode-style algorithm problems and test cases are rewritten into narrative reasoning problems, maintaining the computational challenge but in a format LLMs can process without code execution
- Core assumption: The logical complexity of an algorithm problem is preserved when converted to a text-only reasoning format
- Evidence anchors: [abstract] "First, we synthesize complex reasoning problems through source algorithm problems and test cases" and [section 2.2] "the model combine LeetCode Problem 70 'Climbing Stairs' and the test case input n=17 into a specific text reasoning problem"

### Mechanism 3
- Claim: Data synthesis from algorithm problems provides sufficiently difficult training data that challenges LLMs beyond their baseline performance
- Mechanism: By using algorithm problems as seeds, the synthesized data requires multi-step logical reasoning that LLMs struggle with, creating a performance gap that training on this data can close
- Core assumption: Algorithm problems represent reasoning challenges that are more difficult for LLMs than typical synthetic reasoning data
- Evidence anchors: [abstract] "Experimental results demonstrate that LogicPro significantly improves model performance across multiple reasoning benchmarks" and [section 4.4] "their performance drops significantly on the LogicPro dataset, with an average accuracy of only 45.6%"

## Foundational Learning

- Concept: Test case construction and validation
  - Why needed here: The quality and diversity of test cases directly determine the variety and difficulty of synthesized reasoning problems
  - Quick check question: How would you ensure that a generated test case for a sorting algorithm covers edge cases like empty input, duplicates, and reverse-sorted data?

- Concept: Code execution and intermediate variable extraction
  - Why needed here: The synthesis pipeline requires reliable execution of Python solutions to obtain both final answers and intermediate states
  - Quick check question: What Python features would you use to capture intermediate variable values during execution without modifying the original algorithm logic?

- Concept: Text-to-code consistency checking
  - Why needed here: The rewritten reasoning problems must align with their corresponding code implementations to ensure valid training data
  - Quick check question: How would you design a consistency check between a natural language problem description and its Python implementation?

## Architecture Onboarding

- Component map: Data Collection -> Problem Transformation -> Code Execution -> Reasoning Synthesis -> Quality Control
- Critical path: Problem Transformation -> Code Execution -> Reasoning Synthesis -> Quality Control
  The most time-sensitive components are code execution and reasoning synthesis, as they depend on each other's outputs.
- Design tradeoffs:
  - Test case generation: More test cases improve data diversity but increase computational cost
  - Intermediate variable selection: More variables provide better guidance but may introduce noise
  - Problem rewriting: More detailed narratives improve engagement but may introduce ambiguity
- Failure signatures:
  - High execution failure rate → Problem with code quality or test case validity
  - Low consistency scores → Problem with transformation quality or alignment
  - Poor downstream performance → Problem with intermediate variable relevance or reasoning quality
- First 3 experiments:
  1. Run the pipeline on a small set (100 problems) and measure execution success rate and intermediate variable quality
  2. Generate reasoning paths for 10 problems and manually evaluate reasoning quality and correctness
  3. Train a small model on 1K synthesized examples and test on a held-out algorithm problem to verify knowledge transfer

## Open Questions the Paper Calls Out
The paper acknowledges that there are numerous high-quality algorithm problems in the real world beyond LeetCode, such as Luogu, ACM competitions, and various Online Judge (OJ) platforms, suggesting this as future work for expanding LogicPro's scope.

## Limitations
- The dataset generation process relies heavily on GPT-4 for test case generation, which may introduce variability in data quality
- The method's effectiveness may be limited to reasoning tasks similar to algorithmic problems, potentially reducing generalization to more diverse real-world reasoning challenges
- The approach requires executing Python code solutions, which may fail frequently for complex problems or edge cases, limiting the usable dataset size

## Confidence
- **High confidence**: The mechanism of using intermediate variable states to guide reasoning synthesis is well-supported by experimental results showing consistent improvements across multiple benchmarks
- **Medium confidence**: The claim that LogicPro produces "sufficiently difficult" data is supported by the observed performance drop (45.6% accuracy), but may not fully capture real-world difficulty
- **Low confidence**: The long-term scalability and maintenance of this approach remain uncertain as algorithm problems evolve and new reasoning challenges emerge

## Next Checks
1. **Execution reliability audit**: Run the code execution pipeline on a stratified sample of 500 problems to measure execution success rates and identify common failure patterns
2. **Cross-dataset generalization test**: Evaluate models trained on LogicPro across a broader range of reasoning benchmarks including commonsense and spatial reasoning tasks
3. **Human evaluation of reasoning quality**: Conduct blind human evaluation where annotators rate the logical coherence and correctness of synthesized reasoning processes from LogicPro compared to baseline datasets