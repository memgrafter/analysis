---
ver: rpa2
title: 'Contextualized Evaluations: Judging Language Model Responses to Underspecified
  Queries'
arxiv_id: '2411.07237'
source_url: https://arxiv.org/abs/2411.07237
tags:
- context
- query
- response
- evaluation
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the problem of evaluating language model responses\
  \ to underspecified queries, where user context (intent, preferences, background)\
  \ is not explicitly provided. The authors propose contextualized evaluations\u2014\
  a protocol that synthetically generates relevant follow-up question-answer pairs\
  \ (representing user context) and incorporates them during evaluation."
---

# Contextualized Evaluations: Judging Language Model Responses to Underspecified Queries

## Quick Facts
- **arXiv ID**: 2411.07237
- **Source URL**: https://arxiv.org/abs/2411.07237
- **Reference count**: 25
- **Primary result**: Context-aware evaluation increases agreement between evaluators by 3-10% absolute and can change model win rates and rankings.

## Executive Summary
This paper addresses the challenge of evaluating language model responses to underspecified user queries where critical context (intent, preferences, background) is missing. The authors propose a novel contextualized evaluation protocol that synthetically generates follow-up question-answer pairs to represent this missing context. By incorporating these contexts during evaluation, they demonstrate that both human and model-based evaluators achieve significantly higher agreement rates, shift away from superficial criteria like style toward content relevance, and reveal implicit biases in default model responses. The study also shows that different models vary in their ability to adapt to contextual constraints, particularly around length, detail, and demographic factors.

## Method Summary
The method involves generating synthetic follow-up question-answer pairs using GPT-4o to represent missing user context for underspecified queries. These contexts are validated by human annotators for realism, completeness, and diversity. Responses are then generated using three model pairs (GPT-4o vs Gemini-1.5-Flash, Claude-3.5-Sonnet vs Llama-3.1-405B, Gemma-2-27B vs Jamba-1.5-Large) in both context-agnostic and context-aware settings. Pairwise preference judgments are collected using both human evaluators and model-based evaluators (autoraters), with agreement rates and win rates computed across different evaluation settings to assess the impact of context.

## Key Results
- Providing context to evaluators increases agreement by 3-10% absolute, with larger improvements for model-based evaluators.
- Context-aware evaluation changes model win rates and can flip benchmark rankings compared to context-agnostic evaluation.
- Context reduces evaluators' reliance on surface-level criteria like style, shifting focus to content relevance and completeness.
- Default model responses show implicit biases toward WEIRD (Western, Educated, Industrialized, Rich, Democratic) contexts, and models vary in their ability to adapt to different contextual attributes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing context during evaluation increases agreement between evaluators (both human and model-based).
- Mechanism: Context grounds evaluators around a shared understanding of the user's intent, reducing arbitrary judgments.
- Core assumption: The added context (follow-up QAs) is realistic, complete, and diverse, representing plausible user needs.
- Evidence anchors:
  - [abstract] "providing context to evaluators (both human and model-based) increases agreement by 3-10% absolute"
  - [section] "we find that context-aware evaluation can significantly increase the agreement between evaluators"
- Break condition: If generated contexts are unrealistic, incomplete, or irrelevant, they may confuse evaluators rather than help them align.

### Mechanism 2
- Claim: Context changes the criteria used by evaluators, reducing reliance on surface-level properties like style.
- Mechanism: When context is present, evaluators focus more on whether the response meets the specific constraints outlined in the context, rather than judging based on superficial aspects.
- Core assumption: Context provides more objective criteria for evaluation than the query alone.
- Evidence anchors:
  - [abstract] "nudge evaluators to make fewer judgments based on surface-level criteria, like style"
  - [section] "context-aware evaluation can decrease the frequency with which evaluators make judgments using surface-level properties like style"
- Break condition: If evaluators ignore the context and continue to focus on surface-level properties, the mechanism fails.

### Mechanism 3
- Claim: Context reveals implicit biases in default model responses and allows assessment of model sensitivity to different user contexts.
- Mechanism: By providing specific contexts during evaluation, we can see how well models adapt their responses to different user needs and backgrounds.
- Core assumption: Models have implicit biases in their default responses that become apparent when specific contexts are provided.
- Evidence anchors:
  - [abstract] "our procedure suggests a potential bias towards WEIRD (Western, Educated, Industrialized, Rich and Democratic) contexts in models' 'default' responses"
  - [section] "models are not equally sensitive to following different contexts, even when they are provided in prompts"
- Break condition: If models perform equally well across all contexts, the mechanism fails to reveal any biases or sensitivity differences.

## Foundational Learning

- Concept: Underspecification in user queries
  - Why needed here: The entire study is based on the premise that many user queries are underspecified, lacking essential context about the user's intent, background, or preferences.
  - Quick check question: Can you identify whether a query is underspecified by looking for missing information about user intent, background, or preferences?

- Concept: Follow-up question-answer pairs as context representation
  - Why needed here: The study uses synthetically generated follow-up QAs to represent the missing context in underspecified queries.
  - Quick check question: Can you explain why follow-up QAs are a good way to represent context for underspecified queries?

- Concept: Pairwise preference judgments
  - Why needed here: The study uses pairwise judgments to compare model responses, both with and without context.
  - Quick check question: Can you describe how pairwise preference judgments work in the context of evaluating model responses?

## Architecture Onboarding

- Component map: Query datasets → Context generation (GPT-4o) → Response generation (model pairs) → Evaluation settings (context-agnostic/implicit/adaptive) → Human evaluators + model-based evaluators → Judgment aggregation
- Critical path: Underspecified query → Context generation → Response generation → Evaluation (with/without context) → Judgment aggregation
- Design tradeoffs: The tradeoff is between the quality and realism of generated contexts (which requires human validation) and the scalability of using language models for context generation.
- Failure signatures: If agreement rates don't improve with context, if context-aware evaluation doesn't change model win rates, or if models don't show sensitivity to different contexts, the approach may not be working as intended.
- First 3 experiments:
  1. Generate contexts for a sample of underspecified queries and validate them with human annotators to ensure they meet the criteria of being realistic, complete, and diverse.
  2. Compare agreement rates between context-agnostic and context-aware evaluation settings for a small set of queries and model pairs.
  3. Analyze the types of justifications given by evaluators in context-agnostic vs. context-aware settings to see if context reduces reliance on surface-level criteria.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of follow-up questions affect the agreement between evaluators and the quality of model responses?
- Basis in paper: [inferred] The paper mentions that they only considered up to 10 follow-up questions for each query but did not analyze the effect of the number of follow-up questions on the results.
- Why unresolved: The study focused on a fixed number of follow-up questions, leaving the impact of varying the number unexplored.
- What evidence would resolve it: Experiments varying the number of follow-up questions and measuring evaluator agreement and response quality across different settings.

### Open Question 2
- Question: How do human evaluators compare to model evaluators in terms of reliability and consistency when provided with context?
- Basis in paper: [explicit] The paper discusses differences between human and model evaluators but suggests that stronger correlations between human and model evaluators are needed to strengthen conclusions about biases.
- Why unresolved: The paper relies primarily on model evaluators for certain analyses, and the correlation between human and model evaluations is only partially explored.
- What evidence would resolve it: Direct comparison studies with larger samples of human evaluators and model evaluators across diverse contexts.

### Open Question 3
- Question: What are the implications of implicit biases in default model responses for different cultural and demographic groups?
- Basis in paper: [explicit] The paper identifies potential WEIRD biases in default model responses but notes that stronger correlations between human and model evaluators are needed to confirm these findings.
- Why unresolved: The study identifies biases but does not fully explore their implications or test mitigation strategies.
- What evidence would resolve it: Studies testing the impact of these biases on real-world user experiences and evaluating interventions to reduce them.

## Limitations

- The quality and realism of synthetically generated follow-up QAs may not fully capture diverse user needs or may introduce their own biases.
- The WEIRD bias finding is based on limited demographic categories and may not represent the full spectrum of cultural contexts.
- The study uses GPT-4o for both context generation and as one of the evaluated models, which could introduce circularity or bias in the results.

## Confidence

- **High Confidence**: The finding that context increases evaluator agreement (3-10% absolute) is well-supported by the methodology and results.
- **Medium Confidence**: The claim about reduced surface-level judgments and revealed implicit biases is supported by the results but relies more heavily on qualitative analysis.
- **Medium Confidence**: The finding that model win rates and rankings can change with context is well-demonstrated, but practical significance depends on context representativeness.

## Next Checks

1. **Context Realism Validation**: Conduct a larger-scale human evaluation specifically focused on whether the synthetically generated follow-up QAs feel realistic and representative of actual user contexts, particularly across diverse demographic and cultural backgrounds.

2. **Bias Characterization Study**: Extend the WEIRD bias analysis to include more demographic categories and cultural dimensions, and test whether models can be fine-tuned to reduce these biases when provided with appropriate context.

3. **Cross-Domain Transferability**: Apply the contextualized evaluation protocol to different domains (e.g., medical, technical, creative) to assess whether the benefits of context-aware evaluation generalize beyond the general-purpose queries studied here.