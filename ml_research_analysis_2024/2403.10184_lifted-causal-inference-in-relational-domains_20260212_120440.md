---
ver: rpa2
title: Lifted Causal Inference in Relational Domains
arxiv_id: '2403.10184'
source_url: https://arxiv.org/abs/2403.10184
tags:
- causal
- inference
- lifted
- rain
- pcfg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces parametric causal factor graphs (PCFGs) as
  an extension of parametric factor graphs to enable efficient causal inference in
  relational domains. The key contribution is the lifted causal inference (LCI) algorithm,
  which computes causal effects on a lifted level rather than requiring full grounding
  of the model.
---

# Lifted Causal Inference in Relational Domains

## Quick Facts
- arXiv ID: 2403.10184
- Source URL: https://arxiv.org/abs/2403.10184
- Authors: Malte Luttermann; Mattis Hartwig; Tanya Braun; Ralf MÃ¶ller; Marcel Gehrke
- Reference count: 15
- Primary result: LCI algorithm computes causal effects on lifted level using representatives for indistinguishable objects

## Executive Summary
This paper introduces parametric causal factor graphs (PCFGs) as an extension of parametric factor graphs to enable efficient causal inference in relational domains. The key contribution is the lifted causal inference (LCI) algorithm, which computes causal effects on a lifted level rather than requiring full grounding of the model. The method exploits symmetries in relational models by using representatives for indistinguishable objects, allowing for tractable inference with respect to domain sizes.

Empirical evaluation demonstrates that LCI achieves significant speed-ups compared to propositional inference methods, with run times remaining stable even as domain sizes grow exponentially. The approach enables scalable causal effect computation in models representing objects and their causal relationships, addressing a critical gap between probabilistic and causal inference in relational domains.

## Method Summary
The method introduces parametric causal factor graphs (PCFGs) as an extension of parametric factor graphs to support causal reasoning in relational domains. Lifted causal inference (LCI) is the core algorithm that exploits symmetries in relational models by representing groups of indistinguishable objects through representatives. The approach modifies standard factor graphs to handle interventions through parfactor splitting and parent factor modifications. The algorithm computes causal effects by performing lifted variable elimination on the modified PCFG structure, avoiding the exponential blow-up that occurs when grounding relational models to propositional representations.

## Key Results
- LCI achieves significant speed-ups compared to propositional inference methods (Bayesian networks, directed factor graphs)
- Run times remain stable even as domain sizes grow exponentially for domain-liftable models
- The approach enables scalable causal effect computation in models representing objects and their causal relationships
- LCI successfully handles interventions on groups of random variables through parfactor splitting

## Why This Works (Mechanism)
The mechanism works by exploiting symmetries in relational models through the use of representatives for indistinguishable objects. Instead of grounding the entire relational model, LCI operates on a lifted level where similar substructures are represented once. When interventions occur, the algorithm splits parfactors appropriately and modifies parent factors to block backdoor paths, enabling correct causal effect computation while maintaining the lifted representation. This allows the algorithm to avoid the exponential complexity that would arise from propositional grounding.

## Foundational Learning
- Parametric causal factor graphs (PCFGs): Relational causal models that extend parametric factor graphs with causal semantics - needed to represent objects and their causal relationships compactly
- Lifted variable elimination (LVE): Inference algorithm that operates on lifted representations rather than groundings - needed to efficiently compute queries in relational models
- Parfactor splitting: Technique for handling interventions by dividing parfactors that contain intervened variables - needed to correctly compute interventional distributions
- Quick check: Verify that LVE correctly handles the modified PCFG structure produced by intervention splitting

## Architecture Onboarding
Component map: Relational Domain -> PCFG Construction -> LCI Algorithm -> LVE Subroutines -> Causal Effects

Critical path: The algorithm first constructs the PCFG from the relational domain, then applies LCI which involves parfactor splitting and parent factor modification, followed by LVE to compute the final causal effects.

Design tradeoffs: The main tradeoff is between model expressiveness and computational tractability. LCI achieves tractability by exploiting symmetries but requires that the model be domain-liftable, limiting the class of models that can be efficiently processed.

Failure signatures: Incorrect parfactor splitting when handling interventions on groups of random variables, failure to properly block backdoor paths when computing interventional distributions, and incorrect handling of representative objects during LVE.

First experiments: 1) Test LCI on simple relational models where ground truth causal effects are known, 2) Compare LCI performance against propositional inference on incrementally larger domains, 3) Verify that LCI correctly handles different types of interventions (hard vs soft interventions).

## Open Questions the Paper Calls Out
1. Can the LCI algorithm be extended to handle cyclic dependencies in relational causal models? The paper mentions that prior work has extended relational causal models to cover cyclic dependency structures (Ahsan et al., 2022, 2023), but notes that these works do not allow for lifted causal inference. The current LCI algorithm is defined for acyclic PCFGs and would need significant modifications to handle cycles while maintaining lifted inference efficiency.

2. What is the theoretical complexity of LCI compared to propositional causal inference algorithms? While the paper shows empirical speedups, it lacks a formal complexity analysis comparing LCI to its propositional counterparts. A formal proof showing the complexity class of LCI problems and comparison with propositional causal inference complexity would be valuable.

3. Can PCFGs be learned directly from relational data while maintaining causal semantics? The paper identifies this as a "basic problem for future research" in the conclusion. The paper focuses on inference given a specified PCFG, but doesn't address the challenging problem of learning PCFG structure and parameters from data.

## Limitations
- The approach requires models to be domain-liftable, limiting applicability to certain relational structures
- Formal complexity analysis comparing LCI to propositional methods is lacking
- Empirical evaluation focuses on a single domain (employee training), limiting generalizability
- The paper doesn't address learning PCFGs from data, only inference given a specified model

## Confidence
- High: The core LCI algorithm mechanics and its ability to exploit symmetries through representatives are well-specified and reproducible
- Medium: Speed-up claims relative to propositional methods are supported but depend on specific PCFG structures
- Low: Generalization of results to arbitrary relational domains without further validation

## Next Checks
1. Implement LVE algorithm and verify it correctly handles the modified PCFG structure produced by LCI
2. Test LCI on additional relational domains (e.g., social network influence, disease transmission) to assess generalizability
3. Compare LCI performance against both propositional inference and specialized lifted inference methods on benchmark relational datasets