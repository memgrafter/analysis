---
ver: rpa2
title: Learning interpretable positional encodings in transformers depends on initialization
arxiv_id: '2406.08272'
source_url: https://arxiv.org/abs/2406.08272
tags:
- learning
- brain
- network
- training
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how initialization affects the ability of transformers
  to learn interpretable positional encodings (PEs) that improve generalization. The
  authors propose that PEs initialized from small-norm distributions enable models
  to learn interpretable representations that mirror ground truth positions, while
  large-norm initializations lead to memorization and poor generalization.
---

# Learning interpretable positional encodings in transformers depends on initialization

## Quick Facts
- arXiv ID: 2406.08272
- Source URL: https://arxiv.org/abs/2406.08272
- Reference count: 40
- Primary result: Small-norm initialization of learnable positional encodings enables transformers to learn interpretable representations that mirror ground truth positions and improve generalization.

## Executive Summary
This paper demonstrates that the initialization scale of learnable positional encodings critically determines whether transformers learn interpretable representations or fall into memorization regimes. Through experiments on a 2D relational reasoning task, a nonlinear stochastic network simulation, and real-world fMRI data, the authors show that PEs initialized from small-norm distributions (e.g., N(0,0.1I)) can recover ground truth positional arrangements, learn interpretable attention maps, and achieve superior generalization performance compared to standard PE schemes. The results challenge the common assumption that fixed sinusoidal PEs are always optimal and suggest that initialization scale should be treated as a crucial hyperparameter for PE design.

## Method Summary
The authors evaluate transformers with learnable positional encodings initialized from normal distributions with varying standard deviations (σ). They test three experimental settings: a 2D Latin Square Task requiring relational reasoning, a nonlinear stochastic network simulation modeling brain dynamics, and a real-world fMRI dataset predicting brain activity patterns. Models are trained using Adam optimizer (0.0001 learning rate) for 4000 epochs (LST) or 50k steps (fMRI), with performance measured by accuracy/MSE and interpretability of learned PEs assessed through alignment with ground truth positions and network modularity analyses.

## Key Results
- LST task: Small-norm PEs (σ=0.2) achieved 95.6% accuracy, nearly matching ground truth 2D PE performance of 97.7%
- Attention alignment: Degree of agreement between learned and ground truth attention maps predicted generalization (ρ=0.96, p<0.0001)
- fMRI results: Small-norm initialized PEs learned modular organization consistent with known brain networks, outperforming standard PE schemes

## Why This Works (Mechanism)

### Mechanism 1
Small-norm initialization of learnable positional encodings enables the transformer to enter a "rich feature learning" regime rather than a "lazy learning" regime. In the NTK/lazy learning regime, weights converge to functions close to their initialization and memorization dominates; in the rich regime, the network learns structured, interpretable representations aligned with ground truth positions. Core assumption: The initialization norm determines which training regime the model enters, and small-norm PEs preserve the ability to learn structured positional relationships.

### Mechanism 2
Learned PEs initialized from small-norm distributions recover ground truth positional arrangements (e.g., 2D grid, modular network structure) that are critical for downstream task performance. Small-norm initialization keeps the PE parameters in a regime where they can adapt to capture the true geometry of the data rather than collapsing into random high-dimensional noise. Core assumption: The ground truth positional structure exists and is learnable from the data distribution.

### Mechanism 3
The alignment of learned attention maps with ground truth attention maps predicts generalization performance. When PEs encode correct relative positions, the attention mechanism can form accurate relational patterns (e.g., same row/column in LST, same network cluster in fMRI), leading to better generalization. Core assumption: Attention maps derived from correct PEs are structurally similar to those from ground truth PEs.

## Foundational Learning

- Neural Tangent Kernel (NTK) / lazy learning regime vs. rich feature learning regime: Determines whether the model memorizes or learns structured representations; initialization norm controls regime entry. Quick check: If a model initialized with large-norm PEs converges but generalizes poorly, which regime is it in?

- Positional encoding design for non-1D data: Many real-world datasets require capturing spatial or relational structure beyond simple 1D order. Quick check: Why does flattening a 2D grid into 1D without preserving row/column information make LST harder?

- Orthogonal Procrustes transform: Learned PE dimensions are arbitrary; Procrustes aligns them to measure true similarity. Quick check: What does an L2 norm between aligned PEs measure?

## Architecture Onboarding

- Component map: Transformer encoder (4 layers, 160/64 dim embedding) → PE layer (fixed or learnable) → self-attention (single or multi-head) → MLP → output
- Critical path: PE initialization → embedding addition → attention computation → forward pass → loss → gradient → PE update
- Design tradeoffs:
  - Small σ: better interpretability & generalization, but may require careful optimizer choice (SGD vs Adam)
  - Large σ: faster training but memorization & poor generalization
  - Learnable vs fixed PE: learnable can adapt to task geometry but adds parameters; fixed is stable but may be suboptimal
- Failure signatures:
  - Poor generalization with large σ → memorization regime
  - Vanishing gradients with very small σ → training stall
  - Inconsistent attention maps → PE not capturing structure
- First 3 experiments:
  1. Train LST with σ=0.1, 0.2, 0.5 and compare accuracy & attention map alignment
  2. Run NMAR network simulation; compute modularity of learned PEs vs ground truth
  3. Predict masked fMRI activity; evaluate MSE and network clustering of PEs

## Open Questions the Paper Calls Out

### Open Question 1
How do specific architectural choices (number of layers, attention heads) interact with PE initialization to influence representation learning? Basis in paper: The authors mention evaluating models with 2 and 4 attention heads, noting that adding heads tended to reduce generalization performance, but don't provide a comprehensive analysis of architectural interactions.

### Open Question 2
What is the precise relationship between initialization scale (σ) and optimizer choice for optimal PE learning? Basis in paper: The authors note that Adam's adaptive learning rates interact with small σ values, sometimes producing poor generalization, and that SGD can outperform Adam for very small σ values.

### Open Question 3
Can learned PEs generalize to tasks with dynamic or unknown numbers of elements? Basis in paper: The authors acknowledge that their LST experiments use a fixed 4x4 grid and question how well the approach would work with variable-length or dynamically changing sequences.

## Limitations
- Critical role of initialization norm for regime selection is primarily demonstrated in the LST task; generalization to other architectures (e.g., vision transformers) remains untested
- The proposed mechanism linking small-norm initialization to NTK/lazy learning regime is inferred but not directly measured through kernel analysis
- fMRI results depend on specific preprocessing choices that are not fully detailed, making exact replication challenging

## Confidence

- High confidence: Small-norm initialization improves LST generalization and produces PEs that align with ground truth positions
- Medium confidence: The learned PEs for fMRI data capture known modular brain networks, though preprocessing details are incomplete
- Low confidence: The mechanism connecting initialization norm to NTK/lazy learning regime is speculative without direct kernel analysis

## Next Checks

1. Test whether the initialization effect holds for different transformer architectures (e.g., ViT, GNNs) on structured datasets
2. Conduct NTK/GP kernel analysis to empirically verify the transition between lazy and rich feature learning regimes as initialization norm varies
3. Replicate the fMRI results with fully specified preprocessing steps and alternative brain atlases to confirm the modular organization finding