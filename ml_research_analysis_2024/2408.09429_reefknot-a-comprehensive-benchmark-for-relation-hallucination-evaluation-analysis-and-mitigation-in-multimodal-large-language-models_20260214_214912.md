---
ver: rpa2
title: 'Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation,
  Analysis and Mitigation in Multimodal Large Language Models'
arxiv_id: '2408.09429'
source_url: https://arxiv.org/abs/2408.09429
tags:
- hallucination
- relation
- zhang
- arxiv
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of relation hallucination in multimodal
  large language models (MLLMs), which occurs when models generate incorrect or misleading
  information about the relationships between objects in images. Existing benchmarks
  and mitigation strategies are limited, focusing primarily on object-level or attribute-level
  hallucinations while neglecting the more complex relation hallucinations.
---

# Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2408.09429
- Source URL: https://arxiv.org/abs/2408.09429
- Reference count: 34
- Primary result: Introduces Reefknot benchmark with over 20,000 samples to evaluate relation hallucinations in MLLMs, achieving 9.75% average reduction in hallucination rates using confidence-based detection-and-calibration method

## Executive Summary
This paper addresses the critical problem of relation hallucination in multimodal large language models (MLLMs), where models generate incorrect or misleading information about relationships between objects in images. Existing benchmarks and mitigation strategies primarily focus on object-level or attribute-level hallucinations while neglecting the more complex relation hallucinations. The authors introduce Reefknot, a comprehensive benchmark specifically targeting relation hallucinations, constructed from the Visual Genome scene graph dataset. They propose a systematic definition of relation hallucinations, categorizing them into perceptive (locational/state-based) and cognitive (action-based) types, and introduce a confidence-based detection-then-calibration method that analyzes entropy and probability distributions across model layers to reduce hallucination rates by 9.75% on average.

## Method Summary
The paper introduces Reefknot, a benchmark constructed from the Visual Genome scene graph dataset with over 20,000 real-world samples, specifically targeting relation hallucinations in MLLMs. The benchmark includes three evaluation tasks: Yes/No questions, multiple choice questions, and visual question answering. The authors propose a systematic definition of relation hallucinations, categorizing them into perceptive (locational/state-based) and cognitive (action-based) types. To mitigate relation hallucinations, they develop a confidence-based detection-then-calibration method that analyzes entropy and probability distributions across model layers. The approach identifies potential hallucinations when entropy exceeds a threshold and calibrates final layer outputs using intermediate layer representations to correct these hallucinations.

## Key Results
- Reefknot benchmark successfully identifies that relation hallucination is more severe than object hallucination in current MLLMs
- Perceptive hallucinations (locational/state-based) are consistently 10% more common than cognitive hallucinations (action-based) across all evaluated models
- The Detect-then-Calibrate mitigation method achieves an average 9.75% reduction in hallucination rates across three relation hallucination benchmarks
- Entropy-based detection shows that when relation hallucinations occur, response probability drops significantly, hovering just above 50% compared to the usual nearly 90%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Entropy-based detection of relation hallucinations works because hallucinations manifest as low-confidence predictions with entropy > 0.6.
- **Mechanism**: When relation hallucinations occur, the model's probability distribution over potential answers becomes more uniform, increasing entropy. This entropy spike is detectable using a threshold-based classifier.
- **Core assumption**: Hallucinations correlate with uncertainty in the model's probability distribution, and this uncertainty is captured by entropy.
- **Evidence anchors**:
  - [abstract]: "when relation hallucinations occur, the response probability drops significantly, hovering just above 50% in extreme cases compared to the usual nearly 90%."
  - [section]: "Figure 7 also illustrates that when E(X) > 0.6, relation hallucinations occur to a significant degree, indicating the effectiveness of our method to detect relation hallucination via entropy."
  - [corpus]: Weak - no direct corpus evidence provided for entropy threshold selection.

### Mechanism 2
- **Claim**: Layer-specific calibration can mitigate hallucinations by adjusting logits from intermediate layers where hallucinations first emerge.
- **Mechanism**: The model analyzes probability distributions across layers and identifies where hallucinations begin (typically deep layers). It then calibrates final layer outputs using intermediate layer representations to correct these hallucinations.
- **Core assumption**: Hallucinations originate in deep layers due to knowledge contamination, and intermediate layers contain less corrupted representations that can be used for correction.
- **Evidence anchors**:
  - [abstract]: "we applied a calibration strategy to mitigate hallucination at intermediate confidence levels."
  - [section]: "As depicted in the changes of colors, when it reaches the final Transformer block (layer 38-40th), the model's choices suddenly become uncertain and entropy rises, accompanied by the emergence of hallucinations."
  - [corpus]: Weak - no corpus evidence provided for the specific layer ranges or calibration approach.

### Mechanism 3
- **Claim**: Categorizing relations into perceptive and cognitive types enables more targeted evaluation and mitigation strategies.
- **Mechanism**: By distinguishing between locational/state-based relations (perceptive) and action-based relations (cognitive), the benchmark can better assess different reasoning capabilities and identify where models struggle most.
- **Core assumption**: Different relation types require different reasoning mechanisms, and models will have varying performance on each type.
- **Evidence anchors**:
  - [abstract]: "We provide a systematic definition of relation hallucinations, integrating perceptive and cognitive perspectives."
  - [section]: "Across all models and settings, the incidence of perceptive hallucinations is consistently 10% higher than that of cognitive hallucinations."
  - [corpus]: Moderate - corpus shows perceptual hallucinations are more common, supporting the categorization approach.

## Foundational Learning

- **Concept**: Entropy as a measure of uncertainty in probability distributions
  - Why needed here: Entropy quantifies the model's confidence in its predictions, which is crucial for detecting hallucinations
  - Quick check question: If a model assigns probabilities of 0.9 and 0.1 to two options, what is the entropy? What if it assigns 0.5 and 0.5?

- **Concept**: Transformer layer architecture and forward propagation
  - Why needed here: Understanding how information flows through layers is essential for analyzing where hallucinations emerge and how to calibrate them
  - Quick check question: In a transformer with N layers, which layer's output is passed to the next token prediction head?

- **Concept**: Scene graph representation and semantic triplet extraction
  - Why needed here: The benchmark construction relies on extracting subject-relation-object triples from visual data
  - Quick check question: What is the difference between a scene graph and a simple object detection output?

## Architecture Onboarding

- **Component map**: Vision encoder (ViT or similar) → Projection layer → LLM decoder (transformer blocks) → Classification head
- **Critical path**: Image → Vision encoder → Projection → Transformer layers → Classification head → Entropy calculation → Calibration (if needed) → Final output
- **Design tradeoffs**:
  - Fixed vs. adaptive entropy threshold: Fixed is simpler but may not generalize; adaptive requires calibration data
  - Layer selection for calibration: Earlier layers preserve more raw information but may lack refinement; later layers are more refined but may be more corrupted
  - Detection-only vs. detection-and-correction: Detection-only is less intrusive but doesn't fix the issue
- **Failure signatures**:
  - High false positive rate: Entropy threshold too low, detecting non-hallucinations
  - High false negative rate: Entropy threshold too high, missing actual hallucinations
  - Calibration degrades performance: Incorrect layer selection or calibration strength
- **First 3 experiments**:
  1. **Entropy threshold validation**: Run the model on Reefknot data, plot entropy distributions for hallucination vs. non-hallucination cases, and select an optimal threshold
  2. **Layer-wise hallucination analysis**: For each layer, calculate hallucination rates to identify where hallucinations first appear
  3. **Calibration ablation study**: Test different calibration strengths (α parameter) and layer offsets (λ parameter) to find optimal settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying cause of MLLMs' tendency to favor positive responses (Yes) over negative responses (No) in Yes/No relation hallucination tasks?
- Basis in paper: [explicit] The paper observes that in Yes/No questions, models tend to favor positive responses, with instances where a No label is incorrectly classified as Yes being twice as common as instances where a Yes label is incorrectly classified as No.
- Why unresolved: While the paper identifies this tendency, it does not provide a detailed analysis of why this bias exists. Understanding the root cause could help in developing more balanced models and evaluation methods.
- What evidence would resolve it: Detailed analysis of training data distribution, model architecture, and potential biases in pre-training and fine-tuning processes that could contribute to this positive response bias.

### Open Question 2
- Question: How does the performance of MLLMs on relation hallucination tasks compare to their performance on object and attribute hallucination tasks?
- Basis in paper: [explicit] The paper mentions that relation hallucination can be more severe than object hallucination in current MLLMs, as shown in Figure 2.
- Why unresolved: While the paper provides a comparison between relation and object hallucination, it does not offer a comprehensive analysis comparing all three types of hallucinations (object, attribute, and relation) across various MLLM models.
- What evidence would resolve it: Systematic evaluation of multiple MLLM models on standardized benchmarks for all three types of hallucinations, followed by a comparative analysis of performance across these tasks.

### Open Question 3
- Question: What is the impact of relation hallucination mitigation techniques on the overall performance and trustworthiness of MLLMs in real-world applications?
- Basis in paper: [inferred] The paper introduces a Detect-then-Calibrate method that reduces hallucination rates by an average of 9.75% across three relation hallucination datasets, but does not explore its broader implications.
- Why unresolved: While the effectiveness of the mitigation technique is demonstrated, its practical impact on real-world MLLM applications and user trust is not addressed.
- What evidence would resolve it: Case studies and user studies evaluating MLLMs with and without the mitigation technique in real-world scenarios, measuring both performance improvements and user perceptions of trustworthiness.

## Limitations

- **Dataset construction quality**: The paper doesn't specify the exact criteria for relation triplet selection or inter-annotator agreement rates, and the claim of "over 20,000 samples" lacks detailed breakdown by relation type or difficulty level
- **Generalizability of mitigation method**: The Detect-then-Calibrate approach is evaluated primarily on LLaVA-13B, without demonstration that it transfers to other model architectures or scales to larger models effectively
- **Entropy threshold selection**: The specific entropy threshold of 0.6 is used without clear explanation of the methodology for selecting this value, raising concerns about generalizability across different model families or tasks

## Confidence

- **High confidence**: The categorization of relation hallucinations into perceptive and cognitive types is well-supported by empirical evidence showing consistent performance differences across models
- **Medium confidence**: The effectiveness of the entropy-based detection method is demonstrated, but the specific threshold and its robustness across datasets need further validation
- **Medium confidence**: The calibration method shows measurable improvement, but the long-term stability and generalization across different model architectures remain unproven

## Next Checks

1. **Cross-model validation**: Test the Detect-then-Calibrate method on at least three different MLLM architectures (e.g., LLaVA, MiniGPT-4, and another architecture) to assess whether the approach generalizes beyond the specific model used in the paper

2. **Threshold robustness analysis**: Conduct experiments varying the entropy threshold (e.g., 0.5, 0.6, 0.7) and analyze how detection rates and calibration effectiveness change, providing a sensitivity analysis for the threshold selection

3. **Long-term stability test**: Evaluate the calibrated models over extended usage periods to ensure that the mitigation method doesn't introduce degradation in non-hallucination cases or create new failure modes in downstream applications