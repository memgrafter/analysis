---
ver: rpa2
title: 'Agent-as-a-Judge: Evaluate Agents with Agents'
arxiv_id: '2410.10934'
source_url: https://arxiv.org/abs/2410.10934
tags:
- arxiv
- agent-as-a-judge
- code
- agents
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Agent-as-a-Judge, a framework that uses agentic
  systems to evaluate other agentic systems, addressing the limitations of traditional
  evaluation methods that either focus only on final outcomes or require excessive
  manual labor. The authors present DevAI, a new benchmark of 55 realistic AI development
  tasks with rich manual annotations, including 365 hierarchical user requirements.
---

# Agent-as-a-Judge: Evaluate Agents with Agents

## Quick Facts
- arXiv ID: 2410.10934
- Source URL: https://arxiv.org/abs/2410.10934
- Reference count: 40
- Agents can evaluate other agents, achieving 92% alignment with human evaluations while saving 97.72% time and 97.64% cost

## Executive Summary
This paper introduces Agent-as-a-Judge, a framework that uses agentic systems to evaluate other agentic systems, addressing the limitations of traditional evaluation methods that either focus only on final outcomes or require excessive manual labor. The authors present DevAI, a new benchmark of 55 realistic AI development tasks with rich manual annotations, including 365 hierarchical user requirements. They benchmark three popular agentic systems (MetaGPT, GPT-Pilot, and OpenHands) using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge, achieving up to 92% alignment with human evaluations while saving 97.72% of time and 97.64% of cost compared to human evaluation. The framework provides rich intermediate feedback throughout the task-solving process, marking a concrete step forward for modern agentic systems by enabling dynamic and scalable self-improvement.

## Method Summary
The paper introduces Agent-as-a-Judge, a framework that uses agentic systems to evaluate other agentic systems. The authors create DevAI, a new benchmark of 55 realistic AI development tasks with rich manual annotations including 365 hierarchical user requirements. They benchmark three popular agentic systems (MetaGPT, GPT-Pilot, and OpenHands) using Agent-as-a-Judge and compare its performance against human evaluation and LLM-as-a-Judge approaches. The framework provides rich intermediate feedback throughout the task-solving process, measuring both final outcomes and intermediate steps.

## Key Results
- Agent-as-a-Judge achieves up to 92% alignment with human evaluations
- Saves 97.72% of time and 97.64% of cost compared to human evaluation
- Dramatically outperforms LLM-as-a-Judge in evaluation quality

## Why This Works (Mechanism)
Agent-as-a-Judge works by leveraging the same capabilities that enable agents to solve complex tasks to evaluate those solutions. The framework uses hierarchical requirements and intermediate feedback to provide comprehensive assessment throughout the task-solving process, rather than just evaluating final outcomes. This approach addresses the limitations of traditional evaluation methods that either focus solely on outcomes or require excessive manual labor.

## Foundational Learning
- **Hierarchical Requirements**: Why needed: To capture the complexity of real-world tasks; Quick check: Can the framework handle multi-level task decomposition?
- **Intermediate Feedback**: Why needed: To provide comprehensive evaluation beyond just final outcomes; Quick check: Does the framework assess progress at each task stage?
- **Agent Evaluation Capabilities**: Why needed: To leverage agents' understanding of task complexity for assessment; Quick check: Can agents effectively evaluate solutions they could potentially generate?

## Architecture Onboarding
**Component Map**: User Task -> Agent Solver -> Agent-as-a-Judge Evaluator -> Feedback System -> Improvement Loop
**Critical Path**: Task decomposition → Solution generation → Multi-stage evaluation → Feedback integration
**Design Tradeoffs**: Evaluates intermediate steps vs. focusing only on final outcomes; uses agent evaluators vs. human evaluators
**Failure Signatures**: Poor evaluation alignment with human judgment; failure to assess intermediate progress; inability to handle task complexity
**First Experiments**: 1) Test framework on simple vs. complex tasks; 2) Compare evaluation speed vs. human evaluation; 3) Measure alignment with human evaluators across different task types

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is primarily focused on AI development tasks, potentially limiting generalizability to other domains
- Performance metrics may vary with different agent architectures or more complex task types
- Claims about enabling "dynamic and scalable self-improvement" lack longitudinal studies showing actual improvement cycles

## Confidence
- **High Confidence**: Framework methodology is technically sound and addresses genuine limitations in current evaluation practices
- **Medium Confidence**: Comparative performance metrics are based on specific benchmark and agent systems, may not generalize
- **Low Confidence**: Claims about enabling self-improvement are speculative without longitudinal validation studies

## Next Checks
1. Test Agent-as-a-Judge on non-development tasks (healthcare, education, or creative domains) to assess generalizability
2. Implement a multi-round evaluation cycle where agents are iteratively improved based on Agent-as-a-Judge feedback, measuring actual performance gains over time
3. Conduct a formal study of human evaluator consistency on the DevAI benchmark to establish baseline for what constitutes "alignment"