---
ver: rpa2
title: 'WDMoE: Wireless Distributed Mixture of Experts for Large Language Models'
arxiv_id: '2411.06681'
source_url: https://arxiv.org/abs/2411.06681
tags:
- expert
- latency
- wdmoe
- selection
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a wireless distributed Mixture-of-Experts (WDMoE)
  architecture for large language models (LLMs), enabling collaborative deployment
  across edge servers at the base station and mobile devices in wireless networks.
  The MoE layer is decomposed by placing the gating network and preceding neural network
  layer at the base station, while distributing expert networks among mobile devices.
---

# WDMoE: Wireless Distributed Mixture of Experts for Large Language Models

## Quick Facts
- arXiv ID: 2411.06681
- Source URL: https://arxiv.org/abs/2411.06681
- Authors: Nan Xue; Yaping Sun; Zhiyong Chen; Meixia Tao; Xiaodong Xu; Liang Qian; Shuguang Cui; Wenjun Zhang; Ping Zhang
- Reference count: 40
- Key outcome: Proposes WDMoE architecture reducing latency by 45.75% on PIQA dataset without model capability deterioration

## Executive Summary
This paper introduces a wireless distributed Mixture of Experts (WDMoE) architecture for large language models (LLMs) that leverages edge servers and mobile devices in wireless networks. The MoE layer is decomposed by placing the gating network and preceding neural network layer at the base station, while distributing expert networks among mobile devices. This enables parallel inference across devices, utilizing their limited computing and caching resources. A performance metric accounting for both model capability and latency is developed, and expert selection and bandwidth allocation are jointly optimized to minimize latency while maintaining accuracy. Hardware testbed validation using NVIDIA Jetson kits demonstrates significant latency reduction without compromising LLM performance.

## Method Summary
The WDMoE architecture decomposes the MoE layer in LLMs by placing the gating network and preceding neural network layer at the base station, while distributing expert networks among mobile devices. This enables parallel processing of tokens across devices, leveraging their computing resources. To minimize latency while maintaining accuracy, the paper introduces a weight-to-latency ratio (WLR) metric that balances model accuracy with processing efficiency. Expert selection and bandwidth allocation are jointly optimized based on this metric. The approach is validated through both theoretical simulations and practical hardware experiments using NVIDIA Jetson kits as mobile devices connected to a base station via WiFi.

## Key Results
- WDMoE reduces latency by 45.75% on average compared to vanilla expert selection on the PIQA dataset
- No model capability deterioration observed despite significant latency reduction
- Hardware testbed validation confirms effectiveness of the distributed architecture
- Performance metric accounts for both model capability and latency in optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing the MoE layer and distributing expert networks across mobile devices enables parallel inference, leveraging device computing resources.
- Mechanism: The MoE layer is split so the gating network and preceding neural network layer reside at the base station, while expert networks are deployed on individual mobile devices. This allows each device to process tokens assigned to its experts in parallel, reducing overall inference latency.
- Core assumption: Expert networks within the same MoE layer operate independently and can be processed in parallel without affecting each other's results.
- Evidence anchors:
  - [abstract]: "We decompose the MoE layer in LLMs by placing the gating network and the preceding neural network layer at BS, while distributing the expert networks among the devices."
  - [section II-B]: "Experts operate in parallel and do not impact each other, making this approach suitable for deployment in distributed mobile edge networks."
- Break condition: If expert networks are not truly independent (e.g., shared state or sequential dependencies), parallel execution could produce incorrect results or require synchronization overhead that negates benefits.

### Mechanism 2
- Claim: Introducing attention waiting latency modeling accounts for the asynchronous return of processed tokens from different devices, providing a more accurate latency estimate.
- Mechanism: Tokens are processed by different devices and return to the base station at varying times due to differences in channel conditions and device capabilities. The base station must wait for all tokens to return before proceeding with the attention mechanism in the next block, creating a bottleneck.
- Core assumption: The latency experienced by users is determined by the slowest token return, not just the average processing time.
- Evidence anchors:
  - [section III-B]: "Once each expert completes processing a token, the result is transmitted back to the BS. The BS aggregates the results, after which the tokens are forwarded to the next block."
  - [section III-B]: "The total delay of the sequence is determined by the device with the slowest processing and transmission time, necessitating load balancing."
- Break condition: If all devices have similar processing capabilities and channel conditions, the variation in token return times becomes negligible, and simple average latency modeling might suffice.

### Mechanism 3
- Claim: The weight-to-latency ratio (WLR) metric enables dynamic expert selection that balances model accuracy with latency reduction by prioritizing devices that offer better efficiency.
- Mechanism: For each token and device, WLR combines the weight assigned by the gating network (indicating importance for model accuracy) with the processing latency. Devices with higher WLR values are prioritized for processing tokens, effectively reducing load on slower devices while maintaining accuracy.
- Core assumption: The gating network's weight assignment correlates with the importance of each expert for the final model output, so prioritizing higher-weight experts maintains accuracy even when reducing total expert participation.
- Evidence anchors:
  - [section III-C]: "We introduce a weight-to-latency ratio from the perspective of the processing device to quantify its processing efficiency."
  - [section III-C]: "The WLR for device k in the i-th block is defined as the sum of weights for tokens assigned to device k divided by the processing time for those tokens."
- Break condition: If the gating network's weight assignment doesn't correlate with actual importance (e.g., due to distribution shift or poor training), WLR-based selection could degrade model accuracy despite latency improvements.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture and its decomposition
  - Why needed here: Understanding how MoE layers work and can be split across devices is fundamental to grasping the WDMoE architecture
  - Quick check question: In a standard MoE layer, what components are separated in the WDMoE architecture, and where are they placed?

- Concept: Wireless communication latency modeling
  - Why needed here: The performance of WDMoE depends heavily on accurately modeling communication delays between base station and devices
  - Quick check question: What factors contribute to the total latency for a token processed by a device in the WDMoE architecture?

- Concept: Attention mechanism in transformers and its sequential nature
  - Why needed here: The attention waiting latency concept relies on understanding how the attention mechanism processes all tokens as a sequence
  - Quick check question: Why does the base station need to wait for all tokens to return before proceeding with the attention mechanism in the next block?

## Architecture Onboarding

- Component map: Base Station -> Gating Network, Preceding Neural Network Layer, Attention Mechanism, Token Aggregation -> Mobile Devices -> Expert Networks -> Communication Channel -> Base Station
- Critical path:
  1. User prompt embedding at BS or locally
  2. Token routing through gating network at BS
  3. Token distribution to devices based on expert selection
  4. Token processing on devices
  5. Results transmission back to BS
  6. Token aggregation and attention computation at BS
  7. Output generation

- Design tradeoffs:
  - Device placement: Closer devices have lower latency but fewer available devices
  - Expert distribution: Even distribution vs. load balancing based on device capabilities
  - Bandwidth allocation: Uniform allocation vs. dynamic allocation based on channel conditions
  - Model accuracy vs. latency: More experts improve accuracy but increase latency

- Failure signatures:
  - High variance in device processing times causing attention waiting latency to dominate
  - Poor channel conditions causing communication delays to exceed processing delays
  - Ineffective expert selection leading to accuracy degradation
  - Bandwidth starvation causing communication queues to build up

- First 3 experiments:
  1. Implement a single MoE layer with 2-3 devices, measure baseline latency with uniform bandwidth allocation and simple round-robin expert selection
  2. Implement WLR-based expert selection algorithm, measure latency and accuracy improvements
  3. Implement dynamic bandwidth allocation based on channel conditions, measure latency improvements compared to uniform allocation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of WDMoE scale with the number of mobile devices and experts per MoE layer in practical wireless network deployments?
- Basis in paper: [explicit] The paper discusses the potential of leveraging more mobile devices and expert networks but does not provide detailed scalability analysis or performance bounds for varying numbers of devices and experts.
- Why unresolved: Scalability analysis requires extensive simulations or experiments across different network sizes and configurations, which were not fully explored in the paper.
- What evidence would resolve it: Experimental results showing latency, accuracy, and resource utilization metrics for WDMoE with varying numbers of devices and experts under different network conditions.

### Open Question 2
- Question: What are the optimal strategies for dynamically adjusting the number of active experts per token in WDMoE to balance latency and model performance in real-time?
- Basis in paper: [inferred] The paper introduces a weight-to-latency ratio (WLR) and an expert selection policy, but does not fully explore dynamic adjustment strategies based on real-time network conditions and token complexity.
- Why unresolved: Real-time adaptation requires continuous monitoring of network conditions and token characteristics, which involves complex decision-making algorithms not fully developed in the paper.
- What evidence would resolve it: Implementation of adaptive algorithms that dynamically adjust expert selection based on real-time metrics, with evaluation of their impact on latency and accuracy.

### Open Question 3
- Question: How does the attention waiting latency in WDMoE impact the overall user experience, and what are the perceptual thresholds for acceptable latency in different application scenarios?
- Basis in paper: [explicit] The paper models attention waiting latency but does not provide user experience studies or perceptual thresholds for acceptable latency.
- Why unresolved: User experience is subjective and varies across applications; empirical studies with human subjects are needed to determine acceptable latency thresholds.
- What evidence would resolve it: User studies measuring perceived quality of service and satisfaction across different latency levels and application scenarios using WDMoE.

### Open Question 4
- Question: What are the security and privacy implications of deploying WDMoE in wireless networks, particularly concerning the transmission of model parameters and intermediate results between devices?
- Basis in paper: [inferred] The paper does not address security and privacy concerns related to data transmission and model parameter sharing in distributed deployments.
- Why unresolved: Security and privacy analysis requires comprehensive threat modeling and testing, which were not covered in the paper.
- What evidence would resolve it: Security assessments and privacy impact analyses demonstrating the vulnerabilities and mitigation strategies for WDMoE in wireless networks.

## Limitations
- Experimental validation relies on a small testbed with only three mobile devices, limiting scalability assessment
- Evaluation focuses primarily on the PIQA dataset with limited validation across diverse real-world datasets
- Model architecture details remain underspecified, making it difficult to assess performance with different LLM sizes or MoE configurations

## Confidence
- High Confidence: The core architectural contribution of decomposing MoE layers and distributing across edge servers and mobile devices is technically sound
- Medium Confidence: The latency modeling accurately identifies attention waiting latency as a bottleneck, but comprehensive validation across device heterogeneity scenarios is limited
- Medium Confidence: The WLR-based optimization provides an intuitive framework, but sensitivity to gating network variations and device capability distributions needs further exploration

## Next Checks
1. **Scalability Testing**: Evaluate WDMoE performance with 10+ mobile devices under varying channel conditions and device capabilities to assess how the architecture scales beyond the current three-device testbed.

2. **Dataset Diversity Validation**: Test the approach across multiple diverse datasets (including conversational AI, code generation, and domain-specific tasks) to verify that latency improvements generalize beyond the PIQA benchmark.

3. **Robustness to Gating Network Variations**: Systematically vary the gating network's weight assignment patterns and measure the impact on both accuracy and latency to understand the sensitivity of WLR-based optimization to potential distribution shifts or suboptimal gating decisions.