---
ver: rpa2
title: 'Multimodal Structured Generation: CVPR''s 2nd MMFM Challenge Technical Report'
arxiv_id: '2406.11403'
source_url: https://arxiv.org/abs/2406.11403
tags:
- structured
- generation
- multimodal
- document
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving document understanding
  performance in multimodal foundation models without expensive fine-tuning. The proposed
  approach, Multimodal Structured Generation, applies hard constraints to output logits
  to force models to generate strictly structured outputs and reason before answering.
---

# Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical Report

## Quick Facts
- arXiv ID: 2406.11403
- Source URL: https://arxiv.org/abs/2406.11403
- Reference count: 31
- Placed 2nd in Phase 2 and 3rd place overall of CVPR's 2nd MMFM Challenge using only frozen models augmented with structured generation

## Executive Summary
This paper introduces Multimodal Structured Generation, a technique that applies hard constraints to output logits to force multimodal foundation models (MMFMs) to produce strictly structured outputs and reason before answering. The approach was evaluated on CVPR's 2nd MMFM Challenge, achieving 2nd place in Phase 2 and 3rd place overall without any model fine-tuning. The method demonstrates that lightweight engineering approaches can significantly boost document understanding performance while avoiding the computational costs of fine-tuning.

## Method Summary
The approach applies hard constraints directly to output logits, zeroing out logits corresponding to invalid tokens to enforce a strictly structured JSON output format. This forces models to generate parseable outputs that can be immediately used by downstream APIs without post-processing. The structured generation also enforces a reasoning step by requiring the model to output reasoning tokens before answer tokens through sequential dependencies in the JSON schema.

## Key Results
- Achieved 2nd place in Phase 2 and 3rd place overall of CVPR's 2nd MMFM Challenge
- Outperformed several teams using fine-tuned multimodal models
- Demonstrated effectiveness across multiple document understanding tasks including IconQA, FUNSD, WildReceipt, TextbookQA, TabFact, DocVQA, InfographicVQA, WebSRC, and WTQ

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hard constraints on output logits ensure parseable structured outputs.
- Mechanism: By zeroing out logits corresponding to invalid tokens, the model is forced to generate only tokens that fit the predefined JSON schema. This eliminates the need for post-processing and guarantees outputs are immediately usable by downstream APIs.
- Core assumption: The constrained token space still contains sufficient valid completions to express the intended answer.
- Evidence anchors:
  - [abstract]: "forces (frozen) MMFMs to produce outputs in a strictly structured format by applying hard constraints directly to the output logits"
  - [section 2.1]: "Our approach enforces hard constraints that zero out the logits corresponding to invalid tokens [3]"
  - [corpus]: Weak - no direct evidence about logit constraints in corpus papers.

### Mechanism 2
- Claim: Structured generation forces reasoning before answering.
- Mechanism: The JSON format requires the model to first output a "reasoning" field before the answer, creating a sequential dependency that encourages the model to think through the problem before generating the final response.
- Core assumption: The autoregressive generation process respects the ordering constraints and that the reasoning step genuinely improves answer quality.
- Evidence anchors:
  - [abstract]: "allows us to force the model to reason before producing an answer"
  - [section 2.2]: "Our Structured Generation method forces the multimodal models to 'reason' before producing an answer"
  - [corpus]: Missing - no corpus evidence about reasoning-first prompting strategies.

### Mechanism 3
- Claim: Structured generation reduces the need for expensive fine-tuning.
- Mechanism: By constraining the output space and forcing reasoning, the approach improves performance without modifying model weights, avoiding the computational and engineering costs of fine-tuning.
- Core assumption: The frozen model has sufficient capacity and knowledge to solve the task when properly guided, and that structured outputs capture the essential performance gains.
- Evidence anchors:
  - [abstract]: "significantly boosts performance without the need for expensive fine-tuning"
  - [section 1]: "adapting these models for more specialized tasks via finetuning is both costly and labor-intensive"
  - [section 4]: "our finetuning-free implementation not only reduces computational and engineering overhead"

## Foundational Learning

- Concept: Logit manipulation in autoregressive generation
  - Why needed here: Understanding how zeroing logits affects token selection is critical to implementing structured generation correctly
  - Quick check question: If you zero out logits for tokens 5, 12, and 17 in a vocabulary of 50k, what happens to the probability distribution over tokens?

- Concept: JSON schema design and validation
  - Why needed here: The structured output must conform to a specific schema for downstream API compatibility
  - Quick check question: Given a JSON schema requiring "reasoning" as a string and "answer" as an integer, what happens if the model generates "answer": "42"?

- Concept: Multimodal model architecture basics
  - Why needed here: Understanding how vision and text encoders interact in MMFMs helps diagnose when visual information is truly necessary
  - Quick check question: In a typical MMFM, if the visual encoder outputs 1024 tokens but the language model has a context window of 2048, what fraction of the context is visual information?

## Architecture Onboarding

- Component map: MMFM (frozen) → Text Generation Interface (TGI) → Logit Constraint Layer → Structured Output
- Critical path:
  1. Image preprocessing and OCR extraction
  2. MMFM inference with structured generation
  3. Logit constraint application
  4. JSON output formatting
  5. Evaluation against ground truth
- Design tradeoffs:
  - Strict schema vs. model flexibility: Tighter constraints improve parseability but may limit expressiveness
  - Reasoning field length: Longer reasoning may improve answer quality but increases token costs
  - Single vs. multi-entity extraction: Supporting multiple entities increases complexity but improves utility
- Failure signatures:
  - Empty or malformed JSON outputs indicate overly restrictive constraints
  - Consistently poor answers despite valid structure suggest the model lacks necessary knowledge
  - Slow inference times may indicate inefficient logit constraint computation
- First 3 experiments:
  1. Baseline: Run MMFM without constraints, measure parse failure rate
  2. Minimal constraints: Apply constraints only to JSON structure tokens, measure impact on answer quality
  3. Full structured generation: Apply complete constraints including reasoning field, compare against fine-tuned baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does structured generation improve performance equally across all document understanding tasks, or are there specific task types where it provides greater benefits?
- Basis in paper: Explicit - The paper notes that the approach "placed 2nd in the hidden test set of Phase 2 and 3rd place overall" but does not provide detailed performance breakdowns across different task types or datasets
- Why unresolved: The paper provides overall results but lacks detailed ablation studies showing which specific document understanding tasks benefit most from structured generation
- What evidence would resolve it: Detailed performance metrics broken down by task type (e.g., key information extraction vs. chart interpretation vs. infographic understanding) showing relative improvements

### Open Question 2
- Question: What is the optimal number of visual tokens needed for effective document understanding, and how does this vary by document type?
- Basis in paper: Explicit - The authors state "we may not be using enough image tokens for effective document understanding" and cite prior work showing that "increasing the number of image tokens improves performance on document understanding benchmarks"
- Why unresolved: The paper uses LLaVA-NeXT with standard visual tokenization but does not conduct experiments varying the number of visual tokens for different document types
- What evidence would resolve it: Systematic experiments varying visual token count across different document types (text-heavy vs. chart-heavy vs. mixed) to identify optimal tokenization strategies

### Open Question 3
- Question: How does structured generation compare to fine-tuning in terms of generalization to completely unseen document types and layouts?
- Basis in paper: Explicit - The authors note their method achieved competitive results on "never-before-seen evaluation datasets" but acknowledge that "our framework can be extended to finetuned models"
- Why unresolved: While the paper demonstrates good performance on held-out datasets, it does not directly compare structured generation versus fine-tuning on truly novel document types
- What evidence would resolve it: Controlled experiments testing both approaches on entirely new document types and layouts not seen during any training phase, measuring zero-shot transfer capability

## Limitations

- Schema Expressiveness: The approach's success depends critically on the JSON schema design, which may not scale to tasks requiring complex, nested, or variable-length structured outputs
- Knowledge Bottleneck: Structured generation cannot compensate for fundamental knowledge gaps in frozen models, providing no advantage when domain-specific training would be necessary
- Generalization Across Tasks: The performance gains may be specific to the structured nature of the challenge datasets rather than demonstrating broad applicability across multimodal domains

## Confidence

**High Confidence**: The claim that structured generation can improve parseable output quality is well-supported by the technical mechanism and experimental results.

**Medium Confidence**: The assertion that structured generation forces genuine reasoning before answering has mixed support, as there's no evidence that models generate substantive reasoning versus superficial text.

**Low Confidence**: The claim that this approach broadly avoids the need for expensive fine-tuning is overstated, as it only works for tasks where frozen models already possess sufficient knowledge.

## Next Checks

1. **Schema Stress Test**: Systematically vary JSON schema complexity across different document understanding tasks and measure the point at which structured generation fails to produce valid outputs.

2. **Knowledge Gap Analysis**: Identify specific questions from the challenge datasets that frozen models answer incorrectly, then evaluate whether structured generation improves these answers versus cases where fine-tuning would be necessary.

3. **Cross-Domain Transfer**: Apply the structured generation methodology to non-document domains (video QA, scientific paper analysis, real-time document streams) and measure performance degradation relative to document understanding tasks.