---
ver: rpa2
title: Real-Time Recurrent Learning using Trace Units in Reinforcement Learning
arxiv_id: '2409.01449'
source_url: https://arxiv.org/abs/2409.01449
tags:
- learning
- rtus
- recurrent
- linear
- rtrl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Recurrent Trace Units (RTUs), a lightweight
  recurrent neural network architecture designed for online reinforcement learning
  in partially observable environments. The key innovation is a complex-valued diagonal
  recurrent layer that enables efficient Real-Time Recurrent Learning (RTRL) while
  maintaining representation capacity.
---

# Real-Time Recurrent Learning using Trace Units in Reinforcement Learning

## Quick Facts
- arXiv ID: 2409.01449
- Source URL: https://arxiv.org/abs/2409.01449
- Reference count: 40
- Primary result: RTUs outperform standard GRU architectures and linear recurrent units across multiple partially observable prediction and control tasks while using significantly less computation

## Executive Summary
This paper introduces Recurrent Trace Units (RTUs), a lightweight recurrent neural network architecture designed for online reinforcement learning in partially observable environments. The key innovation is a complex-valued diagonal recurrent layer that enables efficient Real-Time Recurrent Learning (RTRL) while maintaining representation capacity. RTUs use a cosine representation for complex numbers and incorporate nonlinearity in the recurrence, making them more stable and effective than previous diagonal architectures. The authors demonstrate that RTUs outperform standard GRU architectures and linear recurrent units across multiple partially observable prediction and control tasks, while using significantly less computation.

## Method Summary
The paper proposes RTUs as a new recurrent architecture that combines diagonal recurrent weights with complex-valued representations to enable efficient RTRL. RTUs represent complex numbers using cosine parameterization (r(cos(θ) + i sin(θ))) and incorporate optional nonlinearity in the recurrence. The architecture is trained using RTRL, which maintains gradient traces for online updates without needing to store past data. The method is evaluated across trace conditioning prediction tasks and Mujoco/POMDP control tasks, comparing against GRU and LRU baselines under computational constraints.

## Key Results
- RTUs outperform standard GRU architectures and linear recurrent units across multiple partially observable prediction and control tasks
- RTUs achieve superior performance even when computational resources are constrained and scale better with increasing parameter counts
- The complex-valued diagonal recurrence provides an effective middle ground between computational efficiency and representational power for online RL settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complex-valued diagonal recurrence enables efficient Real-Time Recurrent Learning while preserving representational capacity.
- Mechanism: By decomposing a dense weight matrix using eigenvalue decomposition, the recurrence can be represented with a complex-valued diagonal matrix. This maintains equivalence to the original dense RNN while reducing computational complexity from quartic to linear in hidden dimension.
- Core assumption: The nonlinearity commutes with eigenvector matrices (or that a real-valued rotation representation can be used instead).
- Evidence anchors:
  - [abstract]: "A promising direction is to use linear recurrent architectures (LRUs), where dense recurrent weights are replaced with a complex-valued diagonal, making RTRL efficient."
  - [section]: "There exists an equivalence between using a dense linear recurrent layer and a diagonal recurrent layer with complex values, indicating no loss of representational capacity."
  - [corpus]: Weak - no direct evidence about complex-valued diagonals in corpus papers, but related work on RTRL exists.
- Break condition: If the eigenvalue decomposition doesn't exist or if the activation function doesn't commute with eigenvector matrices, the equivalence fails.

### Mechanism 2
- Claim: Cosine representation of complex numbers enables real-valued hidden states while maintaining rotational dynamics.
- Mechanism: Complex numbers are represented using r(cos(θ) + i sin(θ)), which translates to two real-valued vectors that behave like complex rotations. This avoids the need for complex arithmetic and allows direct use of hidden states for prediction.
- Core assumption: The rotational dynamics of complex multiplication can be preserved using real-valued components.
- Evidence anchors:
  - [abstract]: "RTUs use a cosine representation for complex numbers"
  - [section]: "Each element of ht = Λht−1 + Wx xt ∈ R2n has two components hc1 t , hc2 t , updated recursively: hc1 t = r cos(θ) ⊙ hc1 t−1 − r sin(θ) ⊙ hc2 t−1 + Wc1 x xt"
  - [corpus]: Missing - no direct evidence about cosine representation in corpus papers.
- Break condition: If the rotation dynamics are not preserved accurately, the representational capacity may be compromised.

### Mechanism 3
- Claim: Incorporating nonlinearity in the recurrence improves performance over strictly linear recurrence.
- Mechanism: Adding activation functions (like ReLU) directly to the recurrent state updates allows RTUs to capture nonlinear dynamics while maintaining computational efficiency.
- Core assumption: The benefits of nonlinearity outweigh the potential loss of equivalence to dense RNNs.
- Evidence anchors:
  - [abstract]: "RTUs incorporate nonlinearity in the recurrence, making them no longer linear"
  - [section]: "We also evaluated a different variation of RTUs where the non-linearity is added to the recurrence directly... Nonlinear RTUs lose the equivalence to dense RNNs, though in our experiments, we find they perform as well or better than Linear RTUs."
  - [corpus]: Weak - related work mentions nonlinearities but not specifically for RTRL.
- Break condition: If the nonlinearity causes instability or prevents effective learning, performance may degrade.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The paper addresses reinforcement learning in environments where the agent doesn't have full state information and must maintain its own state representation.
  - Quick check question: What is the key challenge in POMDPs that recurrent networks help address?

- Concept: Real-Time Recurrent Learning (RTRL)
  - Why needed here: RTRL enables online learning without needing to store past data, making it suitable for continual interaction with environments.
  - Quick check question: How does RTRL differ from Truncated Backpropagation Through Time in terms of computational requirements?

- Concept: Eigenvalue decomposition and diagonalization
  - Why needed here: The core innovation relies on showing that dense recurrent weights can be represented as diagonal matrices through eigenvalue decomposition.
  - Quick check question: What mathematical condition must be satisfied for a matrix to be diagonalizable?

## Architecture Onboarding

- Component map: Input layer (d-dimensional) -> Recurrent Trace Units (2n-dimensional real-valued hidden state) -> Output layer (linear transformation to predictions)
- Critical path:
  1. Forward pass: Compute recurrent state using RTU equations
  2. Output prediction from hidden state
  3. Compute loss and gradients
  4. Update RTRL gradient traces
  5. Update parameters using computed gradients
- Design tradeoffs:
  - Real-valued vs complex-valued hidden states: Real-valued avoids complex arithmetic complexity but requires careful parameterization
  - Linear vs nonlinear recurrence: Linear maintains equivalence to dense RNNs but nonlinear may improve performance
  - RTRL vs T-BPTT: RTRL enables true online learning but is more complex to implement
- Failure signatures:
  - Gradient explosion/vanishing: Check parameter constraints (r ∈ (0,1]) and learning rate
  - Poor performance: Verify RTRL implementation and gradient trace updates
  - Memory issues: Monitor storage of gradient traces (O(n + nd) complexity)
- First 3 experiments:
  1. Implement RTU forward pass and verify it produces expected outputs for simple sequences
  2. Implement RTRL gradient computation and verify gradients match finite differences
  3. Train on simple trace conditioning task and verify learning occurs

## Open Questions the Paper Calls Out

- Question: How would RTUs perform with more than two layers, and what computational challenges would arise?
  - Basis in paper: [explicit] The authors discuss limitations of extending RTUs to multilayer settings in the conclusion, noting that gradient traces would need to be saved across all preceding layers, making the approach computationally expensive.
  - Why unresolved: The paper only evaluates single-layer RTUs and mentions the theoretical extension to multilayer settings but does not provide empirical results or solutions for the computational complexity challenges.
  - What evidence would resolve it: Empirical comparison of single-layer vs. multi-layer RTUs on standard RL benchmarks, along with analysis of computational scaling and potential approximations to make multilayer RTUs tractable.

- Question: What is the impact of gradient staleness when using RTRL with PPO, and why does it sometimes improve performance?
  - Basis in paper: [explicit] The authors find that using stale gradients with RTUs and PPO sometimes outperforms fresh gradients, and they hypothesize this may help maintain the trust region, but acknowledge this needs more investigation.
  - Why unresolved: The authors only provide preliminary evidence and speculation about why stale gradients might be beneficial, without a rigorous theoretical explanation or extensive empirical validation.
  - What evidence would resolve it: Controlled experiments varying gradient staleness across different RL tasks and environments, combined with analysis of policy divergence metrics and trust region behavior to understand the mechanisms behind this phenomenon.

- Question: Can RTUs be effectively combined with parallel scan methods to enable scalable training of nonlinear RNNs?
  - Basis in paper: [inferred] The authors mention that recent work has shown parallel scans can be used for nonlinear RNNs, and suggest this as a future direction for RTUs in the conclusion, but do not provide any implementation or results.
  - Why unresolved: The paper only mentions parallel scans as a potential future direction without exploring how they could be adapted for RTUs or what performance benefits they might provide.
  - What evidence would resolve it: Implementation of parallel scan methods specifically designed for RTUs, with comparison to standard sequential RTRL training on large-scale RL benchmarks to demonstrate computational speedups and any trade-offs in performance.

## Limitations
- The paper assumes diagonalizability of weight matrices, which may not hold for all initializations or training trajectories
- The cosine parameterization, while elegant, may introduce numerical stability challenges in extreme cases
- The comparison to GRU is somewhat limited, as GRUs use different computational primitives (gates, nonlinearities) that aren't directly comparable to diagonal recurrence

## Confidence
- Core claims about RTU mathematical foundations: Medium-High
- Empirical performance improvements: Medium-High
- Computational complexity claims: Medium-High

## Next Checks
1. **Robustness testing**: Evaluate RTUs across a wider range of POMDP tasks, including different observation masking patterns and longer time horizons
2. **Ablation studies**: Systematically test the impact of each RTU component (complex representation, nonlinearity in recurrence, cosine parameterization) on performance
3. **Scalability analysis**: Verify that the claimed computational advantages hold as hidden dimension scales, particularly measuring actual FLOPs vs theoretical complexity