---
ver: rpa2
title: Dense Connector for MLLMs
arxiv_id: '2405.13800'
source_url: https://arxiv.org/abs/2405.13800
tags:
- visual
- dense
- connector
- arxiv
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Dense Connector is a simple, plug-and-play vision-language connector\
  \ that leverages multi-layer visual features to enhance MLLMs. It introduces three\
  \ instantiations\u2014Sparse Token Integration, Sparse Channel Integration, and\
  \ Dense Channel Integration\u2014to fuse features from different ViT layers, improving\
  \ visual understanding without extra parameters."
---

# Dense Connector for MLLMs

## Quick Facts
- arXiv ID: 2405.13800
- Source URL: https://arxiv.org/abs/2405.13800
- Authors: Huanjin Yao; Wenhao Wu; Taojiannan Yang; YuXin Song; Mengxi Zhang; Haocheng Feng; Yifan Sun; Zhiheng Li; Wanli Ouyang; Jingdong Wang
- Reference count: 40
- Key outcome: Dense Connector achieves state-of-the-art performance on GQA (63.8%), MMBench (66.8%), and zero-shot video QA with 75% fewer visual tokens

## Executive Summary
Dense Connector is a simple, plug-and-play vision-language connector that leverages multi-layer visual features to enhance Multimodal Large Language Models (MLLMs). It introduces three instantiations—Sparse Token Integration, Sparse Channel Integration, and Dense Channel Integration—to fuse features from different ViT layers, improving visual understanding without extra parameters. Experimental results show consistent gains across 11 image benchmarks and 8 video benchmarks, including state-of-the-art performance on GQA (63.8%), MMBench (66.8%), and zero-shot video QA. The Efficient Dense Connector variant matches LLaVA-v1.5 performance with 75% fewer visual tokens.

## Method Summary
The Dense Connector addresses the underexplored visual aspect of MLLMs by fusing features from multiple layers of frozen ViT encoders. Unlike traditional methods that only use final-layer features, Dense Connector integrates embeddings from different ViT layers through three instantiations: Sparse Token Integration (STI) averages features at the token level, Sparse Channel Integration (SCI) concatenates features at the channel level, and Dense Channel Integration (DCI) aggregates features through grouped convolutions. The approach is parameter-efficient, reusing frozen visual features while enhancing visual representation. An Efficient variant further reduces visual tokens by 75% while maintaining performance comparable to LLaVA-v1.5.

## Key Results
- Achieves state-of-the-art performance on GQA (63.8%) and MMBench (66.8%)
- Matches LLaVA-v1.5 performance with 75% fewer visual tokens in Efficient Dense Connector
- Shows consistent gains across 11 image benchmarks and 8 video benchmarks
- Validated across different vision encoders, LLM sizes (2B→70B), and MLLM architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-layer visual features provide complementary semantic information that enhances the final-layer features typically used in MLLMs.
- Mechanism: The Dense Connector fuses visual embeddings from multiple layers of the frozen ViT encoder, capturing both shallow-level details and high-level semantics.
- Core assumption: Different layers of the ViT encoder contain complementary visual information that can improve downstream understanding.
- Evidence anchors:
  - [abstract]: "An intriguing trend in current MLLM research is that the focus of model learning and performance improvement seems to primarily center around the language aspect... with less exploration into the visual signals fed into the connector."
  - [section]: "In Fig. 1 (a), we illustrate attention maps from different layers of a 24-layer CLIP [ 8] pre-trained ViT-L [ 21], showing that different layers of the same visual encoder emphasize different regions of interest."
  - [corpus]: "Weak evidence - no directly comparable multi-layer fusion studies found in neighbor corpus."

### Mechanism 2
- Claim: The Dense Connector achieves significant performance gains without introducing extra parameters by reusing existing frozen visual features.
- Mechanism: By leveraging offline features from different layers, the Dense Connector implicitly enhances visual information without additional computational overhead during training.
- Core assumption: The "free lunch" of utilizing pre-extracted features from different layers is effective for enhancing visual representation.
- Evidence anchors:
  - [abstract]: "Building on this, we also propose the Efficient Dense Connector, which achieves performance comparable to LLaVA-v1.5 with only 25% of the visual tokens."
  - [section]: "Furthermore, this way also complements techniques that directly increase visual signals, e.g., increasing image resolution [18, 24–28] or introducing additional visual encoders [29, 30, 18]."
  - [corpus]: "No direct evidence in neighbor corpus for parameter-free performance gains through feature reuse."

### Mechanism 3
- Claim: The Dense Connector is compatible across different MLLM architectures and scales effectively with larger models.
- Mechanism: The plug-and-play design allows integration with various vision encoders, LLM sizes (2.7B→70B), and MLLM architectures without requiring architectural modifications.
- Core assumption: The connector architecture is generic enough to work across diverse MLLM configurations.
- Evidence anchors:
  - [abstract]: "Experimental results across various vision encoders, image resolutions, training dataset scales, varying sizes of LLMs (2B→70B), and diverse MLLMs architectures (e.g., LLaVA-v1.5 [16], LLaVA-NeXT [25], Mini-Gemini [18]) validate the versatility and scalability of our approach."
  - [section]: "Tab. 2, we first replace the CLIP-ViT-L [8] with the more advanced visual encoder SigLIP-ViT-SO [31]."
  - [corpus]: "No neighbor papers directly validate cross-architecture compatibility, but the method description suggests broad applicability."

## Foundational Learning

- Concept: Multimodal Large Language Models (MLLMs) architecture
  - Why needed here: Understanding how MLLMs integrate visual and language modalities is crucial for implementing the Dense Connector
  - Quick check question: What are the three main components of existing MLLM architectures according to the paper?

- Concept: Vision Transformer (ViT) layer features
  - Why needed here: The Dense Connector leverages features from different ViT layers, requiring understanding of how these layers encode visual information
  - Quick check question: How do different layers of a ViT encoder typically encode visual information?

- Concept: Feature fusion techniques
  - Why needed here: The Dense Connector uses different integration strategies (token, channel, dense channel) that require understanding of feature manipulation
  - Quick check question: What is the difference between token-level and channel-level feature integration?

## Architecture Onboarding

- Component map: Vision encoder (frozen) → Multi-layer features → Dense Connector → LLM
  - Tokenizer → Text embeddings → LLM (concatenated with visual embeddings)

- Critical path: Visual encoder → Dense Connector → LLM processing
  - The Dense Connector is the bottleneck component that transforms multi-layer visual features into LLM-compatible embeddings

- Design tradeoffs: Parameter-free vs. learnable fusion methods
  - Non-parameterized approaches (average pooling, concatenation) vs. parameterized approaches (1D/2D convolutions, additional linear layers)
  - Computational efficiency vs. potential performance gains from learned fusion

- Failure signatures: Performance degradation on visual understanding tasks
  - If the Dense Connector introduces noise or fails to properly align visual features with language space
  - Incompatible integration with specific MLLM architectures

- First 3 experiments:
  1. Implement STI with CLIP-ViT-L using 8th, 16th, and 24th layers with average pooling (α=8)
  2. Test SCI with the same layer selection and compare performance against STI baseline
  3. Evaluate DCI with grouped layer aggregation and compare across all three instantiations on GQA benchmark

## Open Questions the Paper Calls Out

None

## Limitations

- Limited ablation of layer selection: The paper tests specific layer combinations (8th, 16th, 24th) without systematic exploration of optimal layer selection strategies across different tasks or visual encoders.
- Evaluation on specialized visual tasks: Most benchmark gains are on general VQA tasks, lacking validation on specialized domains like medical imaging or remote sensing where multi-layer feature fusion might behave differently.
- Generalization to non-CLIP architectures: All experiments use CLIP-based visual encoders, with untested effectiveness on non-CLIP architectures despite one experiment with SigLIP.

## Confidence

- High confidence: Parameter efficiency claims (75% reduction in visual tokens) are well-supported by controlled experiments comparing against LLaVA-v1.5 baseline.
- Medium confidence: Performance improvements on standard benchmarks are demonstrated but may be influenced by dataset-specific characteristics.
- Low confidence: The assertion that Dense Connector is "plug-and-play" across diverse MLLM architectures is based on limited architectural variations without testing truly different architectural paradigms.

## Next Checks

1. **Layer selection ablation study**: Systematically test different layer combinations (including non-uniform sampling, overlapping layers, and progressive layer selection) across multiple tasks to identify optimal strategies and validate whether 8th/16th/24th is truly optimal or just convenient.

2. **Cross-encoder generalization**: Implement Dense Connector with non-CLIP visual encoders (DINOv2, OpenCLIP variants, SigLIP with different pre-training) to verify that the performance gains transfer beyond CLIP-specific feature representations and pre-training objectives.

3. **Domain-specific benchmarking**: Evaluate Dense Connector on specialized visual domains (medical imaging, satellite imagery, industrial inspection) where visual understanding requirements differ substantially from general VQA tasks, testing whether multi-layer fusion provides consistent benefits across diverse visual modalities.