---
ver: rpa2
title: 'NLAS-multi: A Multilingual Corpus of Automatically Generated Natural Language
  Argumentation Schemes'
arxiv_id: '2402.14458'
source_url: https://arxiv.org/abs/2402.14458
tags:
- argument
- language
- argumentation
- schemes
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NLAS- MULTI, the largest multilingual corpus
  of automatically generated natural language argumentation schemes, addressing the
  limitations of existing argument mining and generation datasets. The authors propose
  an effective prompt-based methodology using large language models (GPT-3.5-Turbo
  and GPT-4) to generate structured arguments following Walton's argumentation scheme
  model across 20 different schemes, 50 topics, and two stances (in favor/against)
  in both English and Spanish.
---

# NLAS-multi: A Multilingual Corpus of Automatically Generated Natural Language Argumentation Schemes

## Quick Facts
- arXiv ID: 2402.14458
- Source URL: https://arxiv.org/abs/2402.14458
- Reference count: 20
- Introduces NLAS-MULTI, the largest multilingual corpus of automatically generated natural language argumentation schemes

## Executive Summary
This paper presents NLAS-MULTI, a novel multilingual corpus containing 3,810 automatically generated natural language argumentation schemes across 20 different schemes, 50 topics, and two stances in English and Spanish. The authors employ a prompt-based methodology using GPT-3.5-Turbo and GPT-4 to generate arguments following Walton's argumentation scheme model, achieving 95.25% validity through a two-iteration human validation process. The corpus demonstrates the feasibility of automatic generation of structured arguments in multiple languages and provides strong baselines for argument scheme classification with RoBERTa models achieving 99.5% F1-macro score.

## Method Summary
The authors propose a prompt-based generation approach using large language models to create natural language arguments following Walton's argumentation scheme model. The process involves defining 20 argumentation schemes and 50 topics in both English and Spanish, generating arguments with GPT-3.5-Turbo, and validating outputs through expert human annotation. Invalid arguments are regenerated using GPT-4 in a second iteration. The final corpus is used to train RoBERTa models for automatic classification of argumentation schemes, achieving high performance across 20 classes.

## Key Results
- NLAS-MULTI contains 3,810 valid argumentation schemes with 253,516 words and 7,964 inferences
- 95.25% overall validity rate after human validation and GPT-4 regeneration
- RoBERTa-large achieves 99.5% F1-macro score for argumentation scheme classification
- 74.8% initial accuracy with GPT-3.5-Turbo, improved to 78.8% with GPT-4 for regenerated arguments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based generation using structured argumentation schemes yields high-precision natural language arguments.
- Mechanism: By providing LLMs with formal argumentation scheme templates and constraining output format via JSON prompts, the model can reliably generate arguments that preserve logical structure while producing fluent natural language.
- Core assumption: LLMs can correctly instantiate abstract scheme variables into contextually appropriate natural language when the scheme is explicitly annotated with brackets.
- Evidence anchors:
  - 99.5% F1-macro score with RoBERTa-large on classifying these generated arguments
  - NLAS-MULTI contains 3,810 arguments with consistent argumentative structures
  - Authors redefined argument scheme specifications to make variables more explicit

### Mechanism 2
- Claim: A two-iteration human validation process significantly improves argument generation accuracy.
- Mechanism: Initial generation with GPT-3.5-Turbo followed by expert annotation to filter invalid arguments, then regeneration with GPT-4 for rejected cases, allows iterative refinement and leverages stronger reasoning capabilities of GPT-4 for difficult cases.
- Core assumption: Expert annotators can reliably distinguish valid arguments based on adherence to the argumentation scheme, topic, and stance.
- Evidence anchors:
  - GPT-3.5-Turbo achieved 74.8% initial accuracy, improved to 95.25% overall with GPT-4 regeneration
  - Final corpus contains 3,810 valid arguments out of 4,000 generated
  - English IAA of 0.65 and Spanish IAA of 0.22 indicate varying annotation consistency

### Mechanism 3
- Claim: Multilingual argument generation is feasible with minimal language-specific adaptation.
- Mechanism: Using the same prompt templates and argumentation schemes across English and Spanish allows the LLM to generate arguments in both languages while maintaining structural consistency.
- Core assumption: The LLM's multilingual capabilities are sufficient to understand and generate arguments in both languages without requiring separate fine-tuning.
- Evidence anchors:
  - 1,794 valid Spanish arguments achieved 89.7% initial accuracy
  - Similar performance metrics between English and Spanish subsets
  - XLM-RoBERTa-large used for bilingual experiments

## Foundational Learning

- Concept: Argumentation schemes and their formal structure
  - Why needed here: Understanding Walton's argumentation schemes is essential for designing prompt templates and evaluating generated arguments
  - Quick check question: Can you explain the difference between the Major Premise, Minor Premise, and Conclusion in Walton's Argument from Position to Know scheme?

- Concept: Prompt engineering and template-based generation
  - Why needed here: The effectiveness of the generation process depends on designing prompts that clearly specify argumentation scheme structure and output format
  - Quick check question: What are the three main parts of the prompt structure used in this paper, and why is each part important?

- Concept: Inter-annotator agreement and its implications
  - Why needed here: Understanding IAA scores helps interpret the reliability of human validation and consistency of the annotation process
  - Quick check question: What does a Cohen's Kappa score of 0.65 versus 0.22 indicate about agreement between annotators in English and Spanish?

## Architecture Onboarding

- Component map: LLM API integration (GPT-3.5-Turbo, GPT-4) -> Prompt template management system -> Human annotation interface -> Argument validation and filtering pipeline -> Corpus compilation and storage -> Classification model training pipeline

- Critical path: 1. Prompt template creation and testing 2. Initial argument generation with GPT-3.5-Turbo 3. Human validation and filtering 4. Regenerate rejected arguments with GPT-4 5. Final validation and corpus compilation 6. Classification model training and evaluation

- Design tradeoffs:
  - Using pre-trained LLMs without fine-tuning vs. potential benefits of domain-specific fine-tuning
  - Cost and time of human validation vs. automated validation approaches
  - Generating arguments in multiple languages simultaneously vs. language-specific optimization

- Failure signatures:
  - High rate of invalid arguments from LLM indicates prompt template issues
  - Low inter-annotator agreement suggests unclear validation criteria
  - Classification model poor performance indicates corpus quality issues or insufficient training data

- First 3 experiments:
  1. Test prompt templates with a small set of argumentation schemes and manually verify generated arguments
  2. Run full generation pipeline with one topic and one stance to measure initial accuracy and identify common failure modes
  3. Compare GPT-3.5-Turbo and GPT-4 performance on regenerating arguments that GPT-3.5-Turbo failed to generate correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of NLAS generation vary across different argumentation schemes when using the proposed prompt-based methodology with LLMs?
- Basis in paper: The paper discusses error distribution across different argumentation schemes in both English and Spanish, noting that some schemes were more challenging than others.
- Why unresolved: The paper does not provide detailed analysis of why certain argumentation schemes performed better than others, nor does it explore underlying reasons for these differences in performance.
- What evidence would resolve it: Comprehensive analysis comparing linguistic features of each argumentation scheme with their respective generation success rates, including breakdown of common errors and their linguistic causes.

### Open Question 2
- Question: Can the proposed methodology for automatic generation of natural language arguments be effectively applied to other languages beyond English and Spanish?
- Basis in paper: The paper demonstrates successful generation of NLAS in English and Spanish, suggesting potential applicability to other languages.
- Why unresolved: The paper does not explore effectiveness of methodology in languages other than English and Spanish, leaving open question of its broader applicability.
- What evidence would resolve it: Experimental results showing performance of methodology in generating NLAS for diverse set of additional languages, including those with different linguistic structures and complexities.

### Open Question 3
- Question: How does the quality and persuasiveness of automatically generated arguments compare to human-generated arguments in terms of logical soundness and rhetorical effectiveness?
- Basis in paper: The paper mentions that aspects such as soundness, persuasiveness, or strength of an argument were not considered in the annotation process.
- Why unresolved: The paper focuses on structural validity of generated arguments but does not assess their logical soundness or persuasiveness compared to human-generated arguments.
- What evidence would resolve it: Comparative studies evaluating logical soundness and rhetorical effectiveness of automatically generated arguments against human-generated ones, using standardized evaluation criteria.

## Limitations
- Reliance on expert human validation introduces subjectivity, particularly evident in low Spanish IAA score of 0.22
- High classification accuracy (99.5% F1-macro) may reflect overfitting to specific generation patterns rather than genuine understanding of argumentation schemes
- No evaluation of argument persuasiveness, soundness, or real-world applicability beyond structural validity

## Confidence

**High confidence**: The effectiveness of prompt-based generation for structured arguments is well-supported by systematic two-iteration approach and substantial corpus size (3,810 valid arguments). Classification results with >99% F1-macro demonstrate clear technical success in defined task.

**Medium confidence**: The claim of multilingual feasibility is supported by corpus statistics but limited by low inter-annotator agreement in Spanish and lack of cross-linguistic comparison studies. Methodology appears sound but may not generalize to other language pairs or domains.

**Low confidence**: The assertion that generated arguments preserve genuine argumentative quality beyond structural validity is unsupported. No evaluation of argument persuasiveness, soundness, or real-world effectiveness is provided.

## Next Checks

1. **Annotation consistency validation**: Re-run human validation with detailed annotation guidelines and measure inter-annotator agreement across multiple rounds to establish reliability thresholds for corpus inclusion.

2. **Cross-linguistic robustness test**: Generate arguments for same topics in both languages using identical prompts, then conduct blind evaluation to identify systematic quality differences between English and Spanish outputs.

3. **Classification generalization assessment**: Test RoBERTa classification model on held-out validation set from different argumentation domains (e.g., legal vs. social media arguments) to evaluate whether high performance reflects true scheme understanding or prompt-specific memorization.