---
ver: rpa2
title: Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft
  Value-Based Decoding
arxiv_id: '2408.08252'
source_url: https://arxiv.org/abs/2408.08252
tags:
- diffusion
- arxiv
- svdd
- reward
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SVDD, a method for optimizing downstream
  reward functions in pre-trained diffusion models without fine-tuning or differentiable
  proxy models. The core idea is to integrate soft value functions into the inference
  process, allowing the model to look ahead and select intermediate noisy states that
  lead to high rewards.
---

# Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding

## Quick Facts
- arXiv ID: 2408.08252
- Source URL: https://arxiv.org/abs/2408.08252
- Reference count: 40
- Primary result: Introduces SVDD, a derivative-free method for optimizing downstream rewards in pre-trained diffusion models through soft value-based decoding

## Executive Summary
This paper introduces SVDD (Soft Value-Based Decoding in Diffusion models), a method for optimizing downstream reward functions in pre-trained diffusion models without fine-tuning or differentiable proxy models. The core innovation is integrating soft value functions into the inference process, allowing the model to look ahead and select intermediate noisy states that lead to high rewards. SVDD operates by generating multiple noisy states from the pre-trained model and selecting the state with the highest value function at each time step, eliminating the need for computationally expensive fine-tuning.

The method is validated across image generation, molecule generation, and DNA/RNA sequence generation, demonstrating consistent improvement over baselines like Best-of-N and DPS. The authors show that SVDD can achieve higher reward scores while maintaining the naturalness of generated samples, with up to 12.41% improvement in docking scores for molecule generation compared to pre-trained models.

## Method Summary
SVDD optimizes downstream rewards in pre-trained diffusion models through a two-step process: generating multiple noisy states from the pre-trained model and selecting the state with the highest value according to a value function. The method comes in two variants - SVDD-MC, which learns value functions through Monte Carlo regression, and SVDD-PM, which approximates value functions using the pre-trained model's predicted x0. At each denoising step, SVDD generates M samples from the pre-trained model's policy and selects the one with the highest value function score, approximating sampling from the value-weighted optimal policy without requiring fine-tuning or differentiable models.

## Key Results
- SVDD consistently outperforms baselines (Best-of-N, DPS) across image, molecule, and DNA/RNA generation tasks
- Achieved up to 12.41% improvement in docking scores for molecule generation compared to pre-trained models
- SVDD-PM variant eliminates the need for additional value function learning by leveraging forward process characteristics
- Method maintains sample naturalness while optimizing for downstream rewards, with improved validity and diversity metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SVDD approximates the soft optimal policy by combining multiple noisy states from the pre-trained model and selecting based on value function scores.
- Mechanism: At each denoising step, SVDD generates M noisy states from the pre-trained diffusion model and selects the one with the highest value function score. This approximates sampling from the value-weighted optimal policy without requiring fine-tuning or differentiable models.
- Core assumption: The pre-trained diffusion model's policy can serve as a good proposal distribution for importance sampling, and the value function can be accurately estimated.
- Evidence anchors:
  - [abstract] "SVDD involves two steps: (1) generating multiple noisy states from pre-trained models, and (2) selecting the state with the highest value according to the value function."
  - [section 4.2] "We can approximately sample from p⋆,α t−1(·|xt) by obtaining multiple (M) samples from pre-trained diffusion models and selecting the sample based on an index, which is determined by sampling from the categorical distribution with mean {w⟨m⟩ t−1 /Pj w⟨j⟩ t−1}⟨M ⟩ m=1."
- Break condition: If the pre-trained model's policy is a poor proposal distribution, the importance sampling weights will be highly variable, leading to suboptimal selection.

### Mechanism 2
- Claim: SVDD-PM eliminates the need for additional value function learning by leveraging the forward process in diffusion models.
- Mechanism: Instead of learning value functions through regression, SVDD-PM uses the pre-trained model's predicted x0 (ˆx0(xt)) and directly applies the reward function to it as the value function estimate: r(ˆx0(xt)).
- Core assumption: The pre-trained model's prediction of x0 from xt is a reasonable approximation of the expected x0 given xt, and the reward function applied to this prediction correlates with the true expected reward.
- Evidence anchors:
  - [section 4.3] "we perform the following approximation: vt(x) := α log Ex0∼ppre(x0|xt)[exp(r(x0)/α)|xt] ≈ α log(exp(r(ˆx0(xt))/α) = r(ˆx0(xt))."
  - [abstract] "Notably, the SVDD-PM approach does not require any additional learning as long as we have access to the reward feedback by utilizing the characteristics of diffusion models."
- Break condition: If the pre-trained model's prediction ˆx0(xt) is poor or the reward function is highly non-linear, this approximation will fail.

### Mechanism 3
- Claim: SVDD avoids the computational complexity of computing gradients in classifier guidance by using importance sampling instead.
- Mechanism: Instead of computing gradients through differentiable models to guide the sampling process, SVDD generates multiple samples and selects based on value function scores, eliminating the need for backward passes.
- Core assumption: The computational cost of generating M samples is acceptable compared to the cost of computing gradients, and the selection process based on value functions is effective.
- Evidence anchors:
  - [section 5.1] "No need for constructing differentiable models. Unlike classifier guidance, SVDD does not require differentiable proxy models, as there is no need for derivative computations."
  - [section 4.2] "Line 3 can be computed in parallel at the cost of additional memory (scaled by M)."
- Break condition: If M becomes very large (e.g., thousands), the memory and computational cost may exceed that of gradient-based methods.

## Foundational Learning

- Concept: Importance sampling and resampling in sequential Monte Carlo methods
  - Why needed here: SVDD uses importance sampling to approximate sampling from the value-weighted optimal policy at each denoising step.
  - Quick check question: What is the relationship between the effective sample size and the variance of importance weights in sequential Monte Carlo?

- Concept: Soft value functions and entropy-regularized reinforcement learning
  - Why needed here: SVDD relies on soft value functions that incorporate temperature parameter α to balance reward optimization and naturalness preservation.
  - Quick check question: How does the temperature parameter α affect the trade-off between exploitation (high rewards) and exploration (naturalness) in SVDD?

- Concept: Forward process in diffusion models and its relationship to the denoising process
  - Why needed here: SVDD-PM leverages the forward process characteristics to approximate value functions without additional learning.
  - Quick check question: Why does the forward process in diffusion models enable the approximation used in SVDD-PM?

## Architecture Onboarding

- Component map: Pre-trained diffusion model -> Importance sampling engine -> Value function estimator/Reward function -> Selected state
- Critical path:
  1. Load pre-trained diffusion model
  2. Estimate value functions (SVDD-MC) OR prepare reward function (SVDD-PM)
  3. For each time step t from T to 1:
     - Generate M noisy states from ppre t−1(·|xt)
     - Compute value function scores for each state
     - Select state with highest score
  4. Output final sample x0

- Design tradeoffs:
  - M vs. computational cost: Larger M improves approximation quality but increases memory and computation linearly
  - α vs. reward optimization: Higher α emphasizes reward optimization; lower α preserves naturalness
  - Value function learning vs. direct reward application: SVDD-MC requires training value functions; SVDD-PM is training-free but relies on model quality

- Failure signatures:
  - Low diversity in generated samples: May indicate α is too low or M is too small
  - Poor reward optimization: May indicate value function learning failed or α is too low
  - High computational cost: May indicate M is too large for available resources
  - Invalid samples (e.g., invalid molecules): May indicate pre-trained model quality issues

- First 3 experiments:
  1. Ablation study on M: Run SVDD with M ∈ {1, 5, 10, 20, 50} on a simple task (e.g., molecule QED optimization) and measure reward improvement vs. computational cost.
  2. SVDD-PM vs. SVDD-MC comparison: Run both variants on the same task and compare performance, computational cost, and sensitivity to reward function characteristics.
  3. Cross-domain validation: Apply SVDD to at least two different domains (e.g., images and molecules) to verify the method's generality and identify domain-specific considerations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SVDD's performance scale with increasing batch sizes in SMC-based methods compared to SVDD's M parameter?
- Basis in paper: [explicit] Section 6 discusses SMC-based methods and their performance dependence on batch sizes, while Section 7 mentions SVDD's performance plateaus with increasing M.
- Why unresolved: The paper doesn't provide direct comparative experiments showing how SMC performance scales with batch size versus SVDD's M scaling.
- What evidence would resolve it: Controlled experiments varying batch size for SMC methods and M for SVDD on identical tasks, measuring both reward optimization and diversity metrics.

### Open Question 2
- Question: What is the theoretical guarantee for SVDD-PM's approximation of soft value functions using the posterior mean?
- Basis in paper: [explicit] Section 4.3 introduces SVDD-PM and its approximation v_t(x) ≈ r(ˆx_0(xt)), but doesn't provide theoretical analysis of approximation error.
- Why unresolved: The paper states this is an approximation but doesn't analyze its bias or variance properties, or under what conditions it's accurate.
- What evidence would resolve it: Theoretical bounds on approximation error for SVDD-PM, or empirical analysis showing correlation between true and approximated value functions across different tasks.

### Open Question 3
- Question: How sensitive is SVDD's performance to the choice of proposal distribution in Algorithm 5 compared to the default pre-trained model?
- Basis in paper: [explicit] Section D mentions the possibility of using different proposal distributions but doesn't empirically compare their performance.
- Why unresolved: The paper only uses the pre-trained model as proposal distribution in experiments, leaving the benefit of more sophisticated proposals unexplored.
- What evidence would resolve it: Experiments comparing SVDD with various proposal distributions (e.g., classifier guidance, DPS) against the default, measuring both reward optimization and computational efficiency.

## Limitations

- Dependence on pre-trained model quality: SVDD's effectiveness relies heavily on the pre-trained diffusion model generating diverse enough samples for meaningful selection.
- Computational cost scaling: Performance improves with larger M but at the cost of linear increases in memory and computation.
- Lack of theoretical analysis: The paper doesn't provide theoretical bounds on approximation error or sensitivity analysis for key hyperparameters like α and M.

## Confidence

**High Confidence**: The core algorithmic framework of SVDD and the computational advantage over classifier guidance are well-established with clear experimental validation.

**Medium Confidence**: The theoretical justification for SVDD-PM's approximation and the claim that SVDD maintains sample naturalness while optimizing rewards are reasonable but lack rigorous error analysis.

**Low Confidence**: The paper doesn't adequately address sensitivity to M selection across domains or provide clear guidelines for choosing M based on computational constraints versus performance requirements.

## Next Checks

1. **Ablation study on M**: Systematically vary M from 1 to 50+ and measure the trade-off between computational cost (GPU memory, inference time) and reward improvement. Plot reward vs. M to identify the point of diminishing returns and establish practical guidelines for M selection.

2. **α sensitivity analysis**: Conduct a grid search over α values (e.g., 0.1, 0.5, 1.0, 2.0, 5.0) on a representative task and measure both reward optimization and sample diversity/naturalness metrics. Generate curves showing the trade-off curve between reward and diversity as α varies.

3. **Cross-model validation**: Apply SVDD to multiple pre-trained diffusion models within the same domain (e.g., multiple Stable Diffusion checkpoints) to test the method's robustness to model quality variations. Measure whether SVDD's performance advantage holds consistently across models of different qualities or if it's sensitive to the pre-trained model's inherent capabilities.