---
ver: rpa2
title: 'MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs'
arxiv_id: '2409.02257'
source_url: https://arxiv.org/abs/2409.02257
tags:
- mmlu-pro
- correct
- reasoning
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMLU-Pro+ addresses the problem of benchmark saturation in large
  language model (LLM) evaluation by introducing a more challenging test that requires
  higher-order reasoning. The core method extends MMLU-Pro by adding questions with
  multiple correct answers and introducing novel metrics to assess shortcut learning
  and anchoring bias.
---

# MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs

## Quick Facts
- arXiv ID: 2409.02257
- Source URL: https://arxiv.org/abs/2409.02257
- Reference count: 27
- 12,032 questions across 14 domains show 10-15% performance drop from MMLU-Pro

## Executive Summary
MMLU-Pro+ extends the MMLU-Pro benchmark to address saturation issues in LLM evaluation by introducing more challenging questions that require higher-order reasoning. The benchmark adds questions with multiple correct answers and novel metrics to assess shortcut learning and anchoring bias. Six state-of-the-art LLMs were evaluated, showing significant performance drops and revealing varying degrees of anchoring bias across models. The benchmark successfully differentiates model capabilities and provides a more rigorous test of reasoning skills.

## Method Summary
MMLU-Pro+ extends MMLU-Pro by adding questions with multiple correct answers and introducing novel metrics to assess shortcut learning and anchoring bias. The benchmark includes 12,032 questions across 14 domains, with 3,718 questions modified to include true positive pairs and additional distractor options. GPT-4o was used to generate true positive pairs, with human auditing ensuring quality. The evaluation framework includes accuracy metrics plus shortcut selection ratio and correct pair identification ratio to reveal anchoring bias and reasoning capabilities.

## Key Results
- All evaluated models showed 10-15% performance decrease compared to MMLU-Pro
- O1-preview achieved highest accuracy at 68.3% on MMLU-Pro+
- Novel metrics revealed varying degrees of anchoring bias across models, with Sonnet-3.5 showing strongest performance in distinguishing correct answer pairs from misleading options

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark successfully increases difficulty by introducing multiple correct answers
- Mechanism: By adding "Both X and Y are correct" options where Y is either a true positive (genuinely correct) or false positive (incorrect but plausible), the benchmark forces models to evaluate multiple statements independently rather than selecting the single best answer
- Core assumption: Models trained on traditional single-answer MCQs will struggle with multi-answer evaluation because they rely on selecting the most probable single answer
- Evidence anchors:
  - [abstract] "By incorporating questions with multiple correct answers across diverse domains, MMLU-Pro+ tests LLMs' ability to engage in complex reasoning and resist simplistic problem-solving strategies"
  - [section] "This approach increases the complexity of the benchmark, forcing models to engage in higher-order reasoning, recognizing and evaluating nuanced or multi-faceted concepts rather than relying on memorized patterns or simplistic heuristics"
- Break condition: If models develop a heuristic of always selecting "both" options or if they can trivially identify all correct answers through pattern matching

### Mechanism 2
- Claim: The novel metrics reveal anchoring bias and shortcut learning behaviors
- Mechanism: Shortcut Selection Ratio measures how often models stick with their original (possibly wrong) answers when new correct options are introduced, while Correct Pair Identification Ratio measures ability to distinguish correct multi-answer combinations from misleading ones
- Core assumption: Models that perform well on traditional benchmarks may still exhibit strong anchoring bias and fail to adapt reasoning when presented with new valid information
- Evidence anchors:
  - [section] "We introduce novel metrics like shortcut selection ratio and correct pair identification ratio, offering deeper insights into model behavior and anchoring bias"
  - [section] "The graph reveals that all models exhibit a tendency to stick with their initial selections, both for previously wrong and partially correct options, suggesting a degree of anchoring bias"
- Break condition: If models show no variation in SSR or CPI across different models, indicating the metrics don't differentiate reasoning capabilities

### Mechanism 3
- Claim: The benchmark maintains independence from its creation process
- Mechanism: Despite GPT-4o being used to generate true positive pairs, other models outperform GPT-4o on the benchmark, demonstrating the augmentation genuinely increases difficulty rather than introducing model-specific advantages
- Core assumption: If the dataset creation process introduced biases favoring GPT-4o, that model would show superior performance on the benchmark
- Evidence anchors:
  - [section] "Despite GPT-4o's involvement in generating True Positive pairs, it does not exhibit an advantage in the question-answering task"
  - [section] "This discrepancy in performance across models suggests that MMLU-Pro successfully challenges the LLMs, requiring genuine reasoning capabilities rather than pattern matching or exploitation of dataset artifacts"
- Break condition: If all models show similar performance patterns that correlate with their involvement in dataset creation

## Foundational Learning

- Concept: Higher-order reasoning (analysis, synthesis, evaluation)
  - Why needed here: The benchmark specifically tests models' ability to engage in complex cognitive processes beyond simple pattern matching or recall
  - Quick check question: Can you explain the difference between remembering a fact and evaluating multiple valid solutions to a problem?

- Concept: Anchoring bias in decision-making
  - Why needed here: The shortcut selection ratio metric directly measures models' tendency to stick with initial selections even when presented with new valid information
  - Quick check question: If a model initially selects a wrong answer but a new correct option is added, what behavior would indicate strong anchoring bias?

- Concept: Multi-answer question evaluation
  - Why needed here: The benchmark introduces questions where multiple answers can be correct, requiring different evaluation strategies than traditional single-answer MCQs
  - Quick check question: How would you approach a question where you need to determine if "both A and B are correct" rather than selecting the single best answer?

## Architecture Onboarding

- Component map: Dataset modification pipeline -> Question augmentation (LLM-generated true positives) -> Distractor generation (random selection) -> Quality assurance (human auditing) -> Evaluation framework (accuracy + novel metrics) -> Result analysis
- Critical path: Data modification -> Quality checks -> Model evaluation -> Metric calculation -> Analysis
- Design tradeoffs: Using LLM for augmentation risks introducing model-specific advantages vs. manual creation being too labor-intensive; solved by validating independence through cross-model performance
- Failure signatures: Models showing no performance variation across question types, metrics showing no differentiation between models, human audits revealing systematic errors in augmentation
- First 3 experiments:
  1. Run a subset of models on original MMLU-Pro vs MMLU-Pro+ to verify consistent performance drop
  2. Calculate SSR and CPI for a diverse set of models to confirm metric differentiation
  3. Perform human audit on randomly selected augmented questions to validate quality and identify systematic issues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MMLU-Pro+ performance correlate with real-world reasoning tasks across different domains?
- Basis in paper: [explicit] The paper introduces MMLU-Pro+ as testing higher-order reasoning skills and shows models struggle with nuanced reasoning in complex scenarios
- Why unresolved: The paper demonstrates improved discrimination between models but doesn't validate whether MMLU-Pro+ performance translates to real-world reasoning tasks
- What evidence would resolve it: Empirical studies showing correlation between MMLU-Pro+ scores and performance on real-world reasoning tasks in domains like medical diagnosis, legal analysis, or scientific research

### Open Question 2
- Question: Can training techniques be developed to specifically target the higher-order reasoning skills evaluated by MMLU-Pro+?
- Basis in paper: [inferred] The paper identifies anchoring bias and shortcut learning as limitations, suggesting room for improvement in model training
- Why unresolved: The paper demonstrates these limitations but doesn't explore potential training methods to address them
- What evidence would resolve it: Successful implementation and evaluation of training techniques that improve model performance on MMLU-Pro+ metrics like correct pair identification ratio

### Open Question 3
- Question: How robust is MMLU-Pro+ to potential biases introduced during the LLM-assisted dataset construction process?
- Basis in paper: [explicit] The paper discusses using GPT-4o for dataset construction but shows it doesn't confer advantages, yet acknowledges this as an area for future work
- Why unresolved: While the paper validates the benchmark's independence from its creation process, the potential for subtle biases remains unexplored
- What evidence would resolve it: Systematic analysis of model performance across different dataset augmentation methods and validation that results are consistent regardless of the LLM used in construction

## Limitations

- Limited generalizability across different model families and sizes remains uncertain
- Quality control challenges in human auditing process may not have caught all errors
- Novel metric interpretation complexity - relationship to actual reasoning capabilities not fully established

## Confidence

High: Cross-model performance validation demonstrates benchmark independence from creation process
Medium: Human auditing ensures dataset quality but error rates not quantified
Low: Correlation between MMLU-Pro+ performance and real-world reasoning tasks remains unproven

## Next Checks

1. Validate performance drop consistency across different model families beyond the six evaluated
2. Quantify human audit error rates and systematic bias patterns in augmented dataset
3. Test benchmark performance correlation with real-world reasoning tasks in specific domains like medical or legal reasoning