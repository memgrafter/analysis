---
ver: rpa2
title: Extractive Structures Learned in Pretraining Enable Generalization on Finetuned
  Facts
arxiv_id: '2412.04614'
source_url: https://arxiv.org/abs/2412.04614
tags:
- epoch
- facts
- rank
- extractive
- first-hop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces extractive structures as a framework for\
  \ understanding how pretrained language models generalize to implications of facts\
  \ they are finetuned on. The framework posits that three groups of components\u2014\
  informative components that store facts as weight changes, upstream components that\
  \ extract facts from the input, and downstream components that process the extracted\
  \ facts\u2014work together during inference."
---

# Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts

## Quick Facts
- arXiv ID: 2412.04614
- Source URL: https://arxiv.org/abs/2412.04614
- Reference count: 40
- This paper introduces extractive structures as a framework for understanding how pretrained language models generalize to implications of facts they are finetuned on.

## Executive Summary
This paper introduces extractive structures as a framework for understanding how pretrained language models generalize to implications of facts they are finetuned on. The framework posits that three groups of components—informative components that store facts as weight changes, upstream components that extract facts from the input, and downstream components that process the extracted facts—work together during inference. The authors empirically identify these structures in the OLMo-7b model and show that facts can be stored across multiple layers but in different layers enable different forms of generalization. They also propose a mechanism for how extractive structures are learned during pretraining, predicting a data ordering effect where facts must precede implications during training and a weight grafting effect where weight changes can be transferred to predict counterfactual implications. These predictions are validated across multiple models including OLMo-7b, Llama 3-8b, Gemma 2-9b, and Qwen 2-7b.

## Method Summary
The authors finetune transformer language models on synthetic datasets containing fictitious facts and their implications, using cross-entropy loss for 8 epochs with Adam optimizer. They identify extractive structures by computing linearized causal metrics that approximate the effect of weight changes on model outputs. The framework measures informative components (which store facts), upstream components (which extract facts from inputs), and downstream components (which process extracted facts). Layer-freezing experiments validate the importance of different component groups for different types of generalization. Weight grafting experiments transfer weight changes between models to test whether extractive structures can be transferred to enable counterfactual generalization.

## Key Results
- Extractive structures consisting of informative, upstream, and downstream components can be identified in OLMo-7b and other models
- Facts stored in different layers enable different forms of generalization: early layers support first-hop reasoning while late layers support second-hop reasoning
- Data ordering effects show that facts must precede their implications during pretraining for extractive structures to form
- Weight grafting experiments demonstrate that extractive structures can be transferred to enable generalization to counterfactual implications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Extractive structures enable generalization by encoding facts in specific components that can be recalled during inference.
- **Mechanism:** During finetuning, facts are stored as weight changes in informative components. Upstream components parse inputs to query these weights, and downstream components process the retrieved information into correct responses.
- **Core assumption:** The communication problem between encoding and decoding can be solved by component specialization and coordination.
- **Evidence anchors:**
  - [abstract] "The structures consist of informative components that store training facts as weight changes, and upstream and downstream extractive components that query and process the stored information to produce the correct implication."
  - [section] "Informative components store the salient information about newly learned facts as weight updates. While all components undergo weight updates during finetuning, only the weight updates in the informative components are important for correctly predicting implications."
  - [corpus] Weak - corpus neighbors discuss knowledge learning/unlearning but not this specific mechanism of component-based information storage and retrieval.
- **Break condition:** If weight changes are distributed uniformly across components rather than localized to specific informative components.

### Mechanism 2
- **Claim:** Pretraining creates extractive structures when models encounter implications of already-known facts.
- **Mechanism:** When pretraining encounters implications of previously learned facts, the model learns to extract those facts from weights and produce implications, creating training signals for extractive structures.
- **Core assumption:** The model's internal state during pretraining determines whether it learns extractive structures versus simple memorization.
- **Evidence anchors:**
  - [abstract] "We hypothesize that extractive structures are learned during pretraining when encountering implications of previously known facts."
  - [section] "Suppose at a point during pretraining the model already knows a fact F (i.e. the earlier gradient step on F has already encoded it in the weights), and now encounters its implication Impl F."
  - [corpus] Missing - corpus doesn't discuss pretraining dynamics or when extractive structures are learned.
- **Break condition:** If facts always precede their implications during pretraining, extractive structures cannot form.

### Mechanism 3
- **Claim:** Weight grafting can transfer extractive structures to enable generalization to counterfactual implications.
- **Mechanism:** The weight difference between models trained on facts and implications contains the extractive structures. Grafting this difference onto a model trained on counterfactual facts enables generalization to counterfactual implications.
- **Core assumption:** Weight changes encoding extractive structures are transferable across different but related training tasks.
- **Evidence anchors:**
  - [abstract] "a weight grafting effect where extractive structures can be transferred to predict counterfactual implications."
  - [section] "We expect the change in weights between the middle and final checkpoints of continued pretraining to carry the extractive structures."
  - [corpus] Missing - corpus doesn't discuss weight grafting or counterfactual implication transfer.
- **Break condition:** If weight changes are too specific to particular facts and cannot generalize to related but distinct scenarios.

## Foundational Learning

- **Concept:** Linearized causal metrics for component identification
  - Why needed here: The framework requires identifying which components serve as informative, upstream, or downstream structures, which cannot be done through simple gradient analysis.
  - Quick check question: Can you explain how the linearized informative score ∂R/∂WC(W′C - WC) approximates the causal effect of weight changes on reward?

- **Concept:** Two-hop reasoning composition
  - Why needed here: The empirical analysis relies on understanding how novel facts combine with known facts to form implications requiring sequential recall.
  - Quick check question: Given facts A→B and B→C, can you describe the computational path the model takes to answer a question about A→C?

- **Concept:** Layer-wise knowledge storage and generalization
  - Why needed here: The results show that facts stored in different layers enable different forms of generalization, which is critical for understanding where to perform knowledge editing.
  - Quick check question: Why would freezing early layers harm first-hop OCR but not second-hop OCR, while freezing late layers has the opposite effect?

## Architecture Onboarding

- **Component map:** Transformer architecture with attention heads and MLPs at each layer/token position. Informative components store facts as weight changes, upstream components parse inputs for querying, downstream components process retrieved information.

- **Critical path:** Forward pass → component outputs → reward calculation → backward pass for metric computation. For identification, need to cache activations and gradients efficiently.

- **Design tradeoffs:** Using linearized metrics trades off exact causal measurement for computational efficiency; choosing between precise but expensive causal interventions versus approximate but scalable linear approximations.

- **Failure signatures:** OCR failure when facts and implications are presented in wrong order during training; poor generalization when weight changes are not localized to specific components; learning rate sensitivity affecting whether OCR occurs.

- **First 3 experiments:**
  1. Implement linearized extractive scores and verify they identify known components in a simple synthetic task
  2. Test data ordering effect by training on impl-first vs fact-first datasets and measuring OCR performance
  3. Perform weight grafting experiment to transfer structures from one task to another and measure counterfactual generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the OCR mechanism change when dealing with longer prompts and more complex factual associations?
- Basis in paper: [inferred] The paper primarily studies two-hop reasoning with relatively simple prompts. It mentions that extractive structures could be useful more broadly for understanding out-of-context generalization but doesn't explore how they scale.
- Why unresolved: The paper focuses on synthetic datasets with controlled complexity. Real-world applications would likely involve longer prompts and more complex fact compositions.
- What evidence would resolve it: Experiments testing OCR on prompts with multiple entities, nested factual associations, or varying prompt lengths, measuring whether the same extractive structure framework applies.

### Open Question 2
- Question: What is the nature of the alternative mechanism hinted at in the 'Impl-first' data ordering experiments?
- Basis in paper: [explicit] The paper notes that the 'Impl-first' model exhibits a small amount of OCR generalization despite not learning extractive structures, suggesting an additional unknown mechanism.
- Why unresolved: The authors explicitly state this is an unknown mechanism that contributes to OCR, and they don't investigate it further.
- What evidence would resolve it: Detailed analysis of weight changes and activation patterns in the 'Impl-first' model to identify what differs from the extractive structure mechanism.

### Open Question 3
- Question: How do the findings about layer-specific generalization inform the design of knowledge editing techniques for practical applications?
- Basis in paper: [explicit] The paper shows that facts stored in early layers enable first-hop generalization while late layers enable second-hop generalization, suggesting localization matters for knowledge editing.
- Why unresolved: While the paper identifies this phenomenon, it doesn't provide practical guidance on how to apply these findings to real-world knowledge editing systems.
- What evidence would resolve it: Development and testing of knowledge editing techniques that specifically target different layers based on the desired type of generalization, measuring their effectiveness compared to current methods.

## Limitations

- Linearized causal metrics provide only approximate measures of component influence rather than exact causal effects
- Synthetic evaluation datasets may not fully capture complexity of real-world factual knowledge and implications
- Weight grafting transfer mechanism shows limited empirical validation and may not generalize well to complex factual relationships

## Confidence

- **High Confidence:** The empirical identification of informative, upstream, and downstream components in OLMo-7b is well-supported by systematic layer-freezing experiments and consistent patterns across multiple models.
- **Medium Confidence:** The pretraining mechanism hypothesis is plausible but relies on assumptions about internal model states during training that are difficult to verify directly.
- **Low Confidence:** The weight grafting transfer mechanism, while theoretically sound, shows limited empirical validation and may not generalize well to more complex factual relationships.

## Next Checks

1. Test the linearized metrics against ground-truth causal interventions in a simplified synthetic task where component effects can be measured exactly, quantifying approximation error.
2. Validate the pretraining mechanism by training models from scratch with controlled fact-implication orderings and measuring when extractive structures emerge.
3. Extend weight grafting experiments to non-synthetic factual relationships and test transfer between different domains (e.g., geography to science facts) to assess generalizability limits.