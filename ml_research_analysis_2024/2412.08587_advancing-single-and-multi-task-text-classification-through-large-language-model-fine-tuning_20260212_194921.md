---
ver: rpa2
title: Advancing Single and Multi-task Text Classification through Large Language
  Model Fine-tuning
arxiv_id: '2412.08587'
source_url: https://arxiv.org/abs/2412.08587
tags:
- classi
- cation
- arxiv
- llms
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically compares encoder-only models (e.g., RoBERTa)
  and large language models (LLMs, e.g., Llama3) for single- and multi-task text classification.
  It evaluates openly-available and closed-source models of varying sizes, both with
  and without fine-tuning, on the 20 Newsgroups and MASSIVE datasets.
---

# Advancing Single and Multi-task Text Classification through Large Language Model Fine-tuning

## Quick Facts
- **arXiv ID**: 2412.08587
- **Source URL**: https://arxiv.org/abs/2412.08587
- **Reference count**: 40
- **Primary result**: Fully fine-tuned Llama3-70B models with five-shot prompting achieve the best performance on single- and multi-task text classification

## Executive Summary
This study systematically compares encoder-only models (e.g., RoBERTa) and large language models (LLMs, e.g., Llama3) for single- and multi-task text classification. The research evaluates both openly-available and closed-source models of varying sizes, both with and without fine-tuning, on the 20 Newsgroups and MASSIVE datasets. Results demonstrate that fully fine-tuned Llama3-70B models with five-shot prompting outperform other configurations across all tasks, while consolidated multi-task fine-tuned LLMs match dual-model setups with potential efficiency benefits.

## Method Summary
The study evaluates encoder-only models (RoBERTa variants) and decoder LLMs (Llama3, GPT-3.5/4, Claude-3) on two datasets using zero-shot, few-shot, and fully fine-tuned approaches. Models are assessed on both single-task and multi-task classification scenarios, with multi-task consolidation tested by combining separate fine-tuned models. Performance is measured using accuracy and F1 scores across standard text classification benchmarks.

## Key Results
- Fully fine-tuned Llama3-70B with five-shot prompting achieves the highest performance across all tasks
- Consolidated multi-task fine-tuned LLMs match the performance of dual-model setups
- Encoder-only models (RoBERTa-large) are outperformed by fine-tuned LLMs on all evaluated tasks

## Why This Works (Mechanism)
The superior performance of fine-tuned LLMs stems from their ability to leverage in-context learning through few-shot prompting while maintaining task-specific knowledge through fine-tuning. The decoder architecture of Llama3 enables better generation and reasoning capabilities compared to encoder-only models, particularly when fine-tuned on task-specific data. Multi-task consolidation works by efficiently sharing parameters across related classification tasks while preserving task-specific decision boundaries.

## Foundational Learning

**Encoder-only models (RoBERTa)**: Why needed - Provide strong baseline for text classification; Quick check - Compare with decoder LLMs on same tasks.

**Decoder LLMs (Llama3)**: Why needed - Enable in-context learning and generation capabilities; Quick check - Evaluate few-shot vs zero-shot performance.

**Fine-tuning approaches**: Why needed - Adapt pre-trained models to specific classification tasks; Quick check - Compare fully fine-tuned vs parameter-efficient methods.

**Multi-task learning**: Why needed - Enable single model to handle multiple classification tasks efficiently; Quick check - Measure performance degradation in consolidated models.

**Prompt engineering**: Why needed - Guide model behavior without additional training; Quick check - Test different shot configurations (1, 3, 5, 10).

## Architecture Onboarding

**Component map**: Data preprocessing -> Model selection (encoder-only/decoder) -> Fine-tuning strategy -> Evaluation metrics

**Critical path**: Data preprocessing → Model fine-tuning → Prompt engineering → Performance evaluation → Multi-task consolidation

**Design tradeoffs**: Large models offer better performance but require more resources; fine-tuning improves task-specific performance but adds training cost; multi-task consolidation reduces latency but may impact accuracy.

**Failure signatures**: Performance degradation in consolidated models when tasks are dissimilar; overfitting during fine-tuning on small datasets; prompt sensitivity affecting zero/few-shot performance.

**First experiments**:
1. Compare zero-shot performance of RoBERTa vs Llama3 on 20 Newsgroups
2. Test few-shot learning with 1, 3, 5, and 10 examples for each model
3. Evaluate fully fine-tuned models on single vs multi-task setups

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on relatively small datasets (20 Newsgroups and MASSIVE) may not generalize to more complex classification tasks
- Multi-task consolidation approach lacks validation in mixed-task inference scenarios
- Comparison between open-source and closed-source models introduces potential confounds from different training regimes

## Confidence
- **High confidence**: Encoder-only models (RoBERTa) are outperformed by fine-tuned LLMs on the evaluated datasets
- **Medium confidence**: Llama3-70B with five-shot prompting represents the optimal configuration for the studied tasks
- **Medium confidence**: Consolidated multi-task fine-tuned LLMs can match dual-model performance with potential efficiency gains

## Next Checks
1. Test the multi-task consolidation approach on larger, more diverse datasets to verify scalability and robustness
2. Evaluate model performance on domain-specific text classification tasks (e.g., medical or legal text) to assess generalizability
3. Conduct ablation studies to quantify the impact of prompt engineering and parameter-efficient fine-tuning methods on model performance