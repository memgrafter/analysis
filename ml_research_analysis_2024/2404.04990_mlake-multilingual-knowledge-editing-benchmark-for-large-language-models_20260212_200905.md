---
ver: rpa2
title: 'MLaKE: Multilingual Knowledge Editing Benchmark for Large Language Models'
arxiv_id: '2404.04990'
source_url: https://arxiv.org/abs/2404.04990
tags:
- knowledge
- editing
- language
- languages
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MLaKE, a benchmark for multilingual knowledge
  editing in large language models. MLaKE contains 5,360 single-hop and 4,072 multi-hop
  questions across five languages, created by collecting fact chains from Wikipedia
  and generating questions with LLMs.
---

# MLaKE: Multilingual Knowledge Editing Benchmark for Large Language Models

## Quick Facts
- **arXiv ID:** 2404.04990
- **Source URL:** https://arxiv.org/abs/2404.04990
- **Reference count:** 27
- **Primary result:** Existing knowledge editing methods struggle to generalize across languages, showing significantly higher accuracy on English than other languages

## Executive Summary
MLaKE is a benchmark designed to evaluate multilingual knowledge editing in large language models. The benchmark contains 5,360 single-hop and 4,072 multi-hop questions across five languages, created by collecting fact chains from Wikipedia and generating questions with LLMs. The evaluation reveals that current knowledge editing methods fail to generalize across languages, with English performing substantially better than other languages. Cross-lingual transfer is particularly limited, working better within language families than across them. The analysis suggests that multilingual models encode knowledge for different languages using distinct parameters, and differences in multilingual encoding efficiency further impair generalization.

## Method Summary
The MLaKE benchmark was constructed by first collecting fact chains from Wikipedia articles across five languages. These fact chains were then used to generate questions through large language models. The resulting benchmark contains two types of questions: single-hop (requiring one fact) and multi-hop (requiring multiple facts). The evaluation framework tests knowledge editing methods by measuring their ability to update factual knowledge in multilingual models and assessing whether these updates generalize across languages.

## Key Results
- Knowledge editing methods show significantly higher accuracy on English compared to other languages
- Cross-lingual transfer is limited, with better performance within the same language family than across different families
- Multilingual models encode knowledge for different languages using distinct parameters, affecting generalization

## Why This Works (Mechanism)
The benchmark reveals that knowledge editing in multilingual models faces fundamental challenges due to how these models encode language-specific information. When knowledge is edited in one language, the changes do not effectively transfer to other languages because the model appears to use separate parameter sets for different languages. This architecture leads to poor cross-lingual generalization even when the underlying facts are identical across languages. The efficiency of multilingual encoding also varies across languages, further contributing to the observed performance gaps.

## Foundational Learning
- **Knowledge editing in LLMs**: Why needed - To update factual knowledge without full model retraining; Quick check - Can existing methods reliably update facts across all tested languages?
- **Cross-lingual transfer**: Why needed - To understand if knowledge updates in one language benefit other languages; Quick check - Do models show better transfer within language families?
- **Multilingual model architecture**: Why needed - To understand parameter sharing and language-specific encoding; Quick check - Are parameters truly distinct across languages or is there some sharing?

## Architecture Onboarding
**Component map:** Fact chain collection -> Question generation (LLMs) -> Benchmark creation -> Knowledge editing methods -> Evaluation across 5 languages
**Critical path:** Fact chain collection → Question generation → Knowledge editing application → Cross-lingual evaluation
**Design tradeoffs:** Single-hop vs multi-hop questions (complexity vs coverage), language selection (representativeness vs availability), question generation method (LLM vs manual)
**Failure signatures:** Poor English performance indicates issues with knowledge editing methods themselves; poor non-English performance indicates cross-lingual generalization failure
**Three first experiments:** 1) Test knowledge editing on single-hop English questions only, 2) Test cross-lingual transfer from English to one other language, 3) Compare performance across different language families

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions beyond the general challenge of improving cross-lingual generalization in knowledge editing for multilingual models.

## Limitations
- Benchmark limited to only five languages, restricting generalizability to other language families
- Insufficient mechanistic explanation for why cross-lingual transfer works better within language families
- Limited analysis of model internals to understand parameter sharing patterns

## Confidence
**High confidence:** Empirical finding that knowledge editing methods perform significantly better on English than other languages, and that cross-lingual transfer is limited
**Medium confidence:** Claim that multilingual models use distinct parameters for different languages, though more detailed analysis would strengthen this
**Low confidence:** Explanation for why cross-lingual transfer works better within language families lacks sufficient mechanistic detail

## Next Checks
1. Test MLaKE on additional languages from underrepresented language families to validate whether observed patterns hold beyond current five languages
2. Conduct parameter-level analysis to identify which specific components of multilingual models are responsible for encoding language-specific knowledge and whether these parameters are shared across language families
3. Evaluate whether knowledge editing techniques that explicitly target language-agnostic parameters show improved cross-lingual transfer performance on MLaKE