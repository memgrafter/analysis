---
ver: rpa2
title: 'HelpSteer2-Preference: Complementing Ratings with Preferences'
arxiv_id: '2410.01257'
source_url: https://arxiv.org/abs/2410.01257
tags:
- response
- preference
- reward
- better
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HelpSteer2-Preference, a new dataset of preference
  annotations collected alongside existing ratings in the HelpSteer2 dataset. The
  authors conduct the first head-to-head comparison of Bradley-Terry and Regression
  reward modeling when adequately matched for data, finding that both approaches perform
  similarly in isolation.
---

# HelpSteer2-Preference: Complementing Ratings with Preferences

## Quick Facts
- arXiv ID: 2410.01257
- Source URL: https://arxiv.org/abs/2410.01257
- Authors: Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, Yi Dong
- Reference count: 40
- Primary result: Novel dataset combining ratings and preferences enables superior reward models (94.1 on RewardBench)

## Executive Summary
This paper introduces HelpSteer2-Preference, a new dataset of preference annotations collected alongside existing ratings in the HelpSteer2 dataset. The authors conduct the first head-to-head comparison of Bradley-Terry and Regression reward modeling when adequately matched for data, finding that both approaches perform similarly in isolation. Based on these insights, they propose a novel combined approach using Scaled Bradley-Terry initialized with Helpfulness-Only SteerLM Regression followed by ExPO, achieving 94.1 on RewardBench (top of 140+ models). This reward model enables REINFORCE training to produce an Instruct model scoring 85.0 on Arena Hard (also top-ranked). The study demonstrates that combining data collection formats and training paradigms leads to superior reward models for alignment.

## Method Summary
The authors create HelpSteer2-Preference by collecting pairwise preference annotations on HelpSteer2 tasks, including preference direction, magnitude (1-3), and justifications. They train multiple reward models: SteerLM Regression predicting helpfulness scores, Bradley-Terry models using different loss variants (Regular, Margin, Scaled), and a Pairwise Justifier. The key innovation combines Scaled Bradley-Terry initialized with regression weights, then applies ExPO extrapolation. The reward model is used in REINFORCE training with KL regularization to align Llama-3.1-70B-Instruct, achieving state-of-the-art performance on multiple benchmarks.

## Key Results
- HelpSteer2-Preference dataset with 6,766 training pairs and 352 validation pairs
- Scaled Bradley-Terry initialized with regression achieves 93.7 on RewardBench
- ExPO extrapolation improves to 94.1 (top of 140+ models)
- RLHF with this reward model achieves 85.0 on Arena Hard (top-ranked)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling preference strength in Bradley-Terry loss improves reward modeling by weighting stronger preferences more heavily
- Mechanism: The Scaled BT loss multiplies the log-sigmoid of reward differences by the preference magnitude (1, 2, or 3), effectively repeating high-magnitude preference pairs in training
- Core assumption: Preference magnitude reflects true quality differences that should be weighted more heavily in training
- Evidence anchors:
  - [abstract] "we use the margin term outside of the log-sigmoid function rather than inside... This can be viewed as a repeated sampling of response pairs for which the preference magnitude is higher"
  - [section 3.2] "Scaled BT can most effectively use the preference magnitude information to guide model training"
  - [corpus] Weak - no direct corpus evidence, but aligned with general machine learning principle that harder examples should be weighted more
- Break condition: If preference magnitude is unreliable or noisy, scaling could amplify errors rather than improve signal quality

### Mechanism 2
- Claim: Initializing Bradley-Terry models with regression-trained weights provides better starting point than random initialization
- Mechanism: Helpfulness-only regression models learn to predict scalar values 0-4, providing a reasonable initial mapping from responses to rewards
- Core assumption: The scalar prediction task of regression provides useful initialization for the scalar output of BT models
- Evidence anchors:
  - [section 3.2] "we also train BT models initialized on the Helpfulness-Only SteerLM Regression Model... can potentially initialize the model better than the base model"
  - [table 1] "Bradley-Terry Regular (init. with Helpfulness-only Regression Model) 92.7" vs "Bradley-Terry Regular (from scratch) 91.5"
  - [corpus] Weak - no direct corpus evidence, but standard practice in deep learning to use pre-trained weights when available
- Break condition: If regression and BT objectives diverge significantly, initialization could hurt rather than help convergence

### Mechanism 3
- Claim: Combining SteerLM Regression and Bradley-Terry approaches through initialization and extrapolation yields synergistic improvements
- Mechanism: Regression model provides initial weights, BT fine-tunes with pairwise comparisons, ExPO extrapolates delta weights for further improvement
- Core assumption: Regression and BT capture complementary aspects of preference modeling that can be combined effectively
- Evidence anchors:
  - [abstract] "we propose a novel approach to combine Bradley-Terry and Regression reward modeling"
  - [section 3.2] "when initialized with the Helpfulness-only Regression model, a Scaled Bradley-Terry can reach overall RewardBench of 93.7"
  - [section 4] "Neither the Regular BT model nor the Margin BT model improved upon the Helpfulness-only Regression Model that they were initialized with"
- Break condition: If the two approaches are fundamentally incompatible or if ExPO extrapolation is unstable

## Foundational Learning

- Concept: Bradley-Terry preference modeling
  - Why needed here: Understanding how pairwise comparisons are converted to reward signals is crucial for implementing and debugging the reward model
  - Quick check question: What loss function does Bradley-Terry use to train reward models, and how does it differ from regression approaches?

- Concept: Regression-based reward modeling
  - Why needed here: Needed to understand the SteerLM approach and how it differs from pairwise methods
  - Quick check question: How does SteerLM Regression predict multiple attributes, and what loss function does it use?

- Concept: RewardBench evaluation framework
  - Why needed here: Understanding the evaluation metrics and categories is essential for interpreting model performance
  - Quick check question: What are the four main categories in RewardBench, and how is the overall score calculated?

## Architecture Onboarding

- Component map:
  - Data collection: HelpSteer2-Preference dataset (prompts + response pairs + Likert ratings + preference rankings + justifications)
  - Reward models: SteerLM Regression, Bradley-Terry variants (Regular, Margin, Scaled), Pairwise Justifier
  - Training pipeline: Base model initialization → Reward model training → Model selection → RLHF alignment
  - Evaluation: RewardBench leaderboard, MT Bench, AlpacaEval 2.0 LC, Arena Hard

- Critical path: Data collection → Reward model training → RLHF alignment → Model evaluation
- Design tradeoffs:
  - Preference vs regression formats: Different data collection methods, but comparable performance when properly matched
  - Pairwise justifier vs independent scoring: Justifiers are more interpretable but less accurate
  - Initialization strategy: Regression initialization helps BT but not vice versa
- Failure signatures:
  - Poor reward model performance: Check data quality, model initialization, and whether loss function matches annotation format
  - RLHF instability: Verify reward model quality, check KL penalty settings, ensure proper baseline estimation
  - Evaluation inconsistencies: Confirm prompt overlap between training and test sets, check for position bias in data
- First 3 experiments:
  1. Train SteerLM Regression on HelpSteer2 helpfulness only, evaluate on RewardBench
  2. Train Scaled BT on HelpSteer2-Preference, initialize with regression model, evaluate
  3. Apply ExPO extrapolation to combine regression and BT models, evaluate improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of reward models trained on preference annotations compare to those trained on ratings across different model sizes and architectures?
- Basis in paper: [explicit] The paper compares Bradley-Terry and Regression reward models when adequately matched for data, finding that both approaches perform similarly in isolation.
- Why unresolved: The study only compares Llama-3.1-70B-Instruct as the base model. Different model sizes and architectures might exhibit different behaviors when trained on preference annotations versus ratings.
- What evidence would resolve it: Conducting head-to-head comparisons of Bradley-Terry and Regression models across various model sizes (e.g., Llama-3.1-8B, 70B, 405B) and architectures (e.g., Gemma, Mistral) would reveal whether the observed performance parity holds universally or is specific to the tested configuration.

### Open Question 2
- Question: To what extent does the quality of preference annotations affect the performance of reward models compared to ratings?
- Basis in paper: [inferred] The paper emphasizes the importance of high-quality data collection, noting that without meeting a high bar on data quality, it may not be possible to discern the advantages of a particular data annotation methodology over another.
- Why unresolved: While the paper collects high-quality preference annotations and demonstrates their effectiveness, it does not explore how varying annotation quality impacts reward model performance relative to ratings.
- What evidence would resolve it: Systematically degrading the quality of preference annotations (e.g., by reducing inter-rater agreement thresholds) and comparing the resulting reward model performance to models trained on ratings of equivalent quality would clarify the relative sensitivity to annotation quality.

### Open Question 3
- Question: Can the synergy between preference annotations and ratings be leveraged to improve reward models for specialized domains beyond general conversation?
- Basis in paper: [explicit] The authors propose a novel approach combining Bradley-Terry and Regression reward models, achieving superior performance on RewardBench.
- Why unresolved: The study primarily focuses on general-domain conversation tasks. The effectiveness of combining preference annotations and ratings for specialized domains (e.g., medical, legal, technical) remains unexplored.
- What evidence would resolve it: Applying the combined Bradley-Terry and Regression approach to specialized datasets and evaluating the resulting reward models on domain-specific benchmarks would demonstrate the generalizability of the synergy observed in the general domain.

### Open Question 4
- Question: How do different preference collection methodologies (e.g., forced choice vs. allowing ties) impact the effectiveness of Bradley-Terry reward models?
- Basis in paper: [explicit] The authors choose to force annotators to make a preference choice between two responses, except for the "Neither response is valid" option, to reduce sitting-on-the-fence behavior.
- Why unresolved: While the forced-choice methodology is implemented, the paper does not explore alternative preference collection methodologies or their impact on reward model performance.
- What evidence would resolve it: Collecting preference annotations using different methodologies (e.g., allowing ties, using continuous scales) and training Bradley-Terry models on each dataset would reveal whether the forced-choice approach is optimal or if alternative methodologies yield better results.

### Open Question 5
- Question: What is the impact of including preference justifications on the interpretability and performance of reward models?
- Basis in paper: [explicit] The authors collect preference justifications alongside preference annotations and explore training reward models using these justifications.
- Why unresolved: While the paper experiments with training models on preference justifications, it does not fully investigate the trade-offs between interpretability and performance or the optimal way to utilize justifications.
- What evidence would resolve it: Conducting a comprehensive study on the impact of preference justifications, including varying the amount of justification information provided, the format of justifications, and the model architectures used to process them, would clarify their role in enhancing reward model interpretability and performance.

## Limitations
- The preference magnitude annotations rely on human judgment, which may be inconsistent across annotators
- The ExPO extrapolation factor (1.52) was selected through grid search on validation data, raising concerns about overfitting
- The RLHF alignment pipeline uses specific hyperparameters that may not generalize to other base models

## Confidence
- High confidence: The head-to-head comparison of Bradley-Terry vs Regression approaches with adequately matched data (RewardBench scores 91.5-92.7 for BT vs 92.7 for regression)
- Medium confidence: The ExPO extrapolation improvement from 93.7 to 94.1 (limited by single validation dataset)
- Medium confidence: The RLHF alignment results (Arena Hard 85.0) given the specific REINFORCE implementation details

## Next Checks
1. Conduct ablation study removing ExPO extrapolation to verify the 1.4 point RewardBench improvement is not due to overfitting
2. Test reward model generalization on out-of-distribution preference data from different domains or annotation styles
3. Implement alternative RL algorithms (PPO, DPO) to verify the reward model quality rather than algorithm-specific effects