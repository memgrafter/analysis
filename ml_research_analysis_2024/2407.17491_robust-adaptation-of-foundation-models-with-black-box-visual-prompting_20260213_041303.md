---
ver: rpa2
title: Robust Adaptation of Foundation Models with Black-Box Visual Prompting
arxiv_id: '2407.17491'
source_url: https://arxiv.org/abs/2407.17491
tags:
- visual
- prompt
- learning
- blackvip
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BlackVIP, a black-box visual prompting method
  for robust transfer learning of large pre-trained models without parameter access.
  The key idea is to use a tiny Coordinator network to generate input-dependent visual
  prompts and train it via zeroth-order optimization (SPSA-GC) that only requires
  model output predictions.
---

# Robust Adaptation of Foundation Models with Black-Box Visual Prompting

## Quick Facts
- **arXiv ID**: 2407.17491
- **Source URL**: https://arxiv.org/abs/2407.17491
- **Reference count**: 40
- **Primary result**: BlackVIP achieves up to 13% better accuracy than zero-shot baselines while using only 9K parameters and 2-3x fewer API queries

## Executive Summary
This paper introduces BlackVIP, a black-box visual prompting method that enables parameter-efficient transfer learning of large pre-trained models without requiring access to model parameters. The approach uses a tiny Coordinator network to generate input-dependent visual prompts that are optimized through zeroth-order optimization (SPSA-GC) based solely on model output predictions. The method demonstrates significant improvements over zero-shot learning and other black-box baselines across 19 diverse datasets, achieving better accuracy with fewer parameters and reduced API queries. A variant called BlackVIP-SE further reduces runtime and parameters to 1K by replacing the feature extractor with PCA.

## Method Summary
BlackVIP addresses the challenge of adapting black-box vision APIs to downstream tasks without parameter access. The method employs a Coordinator network consisting of a frozen feature extractor and a trainable decoder that generates input-dependent visual prompts. These prompts are optimized using SPSA-GC, a zeroth-order optimization algorithm that estimates gradients from black-box model outputs. The approach supports both full Coordinator training (BlackVIP) and a lightweight variant (BlackVIP-SE) using statistical feature extraction. The method is evaluated across 19 datasets including synthetic benchmarks and real-world WILDS datasets, demonstrating consistent improvements in accuracy, memory efficiency, and query efficiency.

## Key Results
- BlackVIP achieves up to 13% better accuracy than zero-shot baselines on certain tasks
- Reduces learnable parameters from 69K (prior work) to 9K while maintaining or improving performance
- Requires 2-3x fewer API queries compared to existing methods
- BlackVIP-SE variant further reduces parameters to 1K with comparable performance on datasets with low intrinsic dimensionality

## Why This Works (Mechanism)
BlackVIP's effectiveness stems from its input-dependent prompt design that adapts to specific data characteristics while maintaining computational efficiency. The Coordinator network generates prompts conditioned on input features, allowing the model to handle distribution shifts and domain-specific variations without modifying the underlying black-box model. The SPSA-GC optimization algorithm enables gradient estimation without parameter access, making the approach compatible with any black-box vision API. The theoretical connection to randomized smoothing provides robustness guarantees, while the statistical feature extraction in BlackVIP-SE offers a lightweight alternative for simpler datasets.

## Foundational Learning

**Zeroth-order optimization (SPSA-GC)**: A gradient-free optimization method that estimates gradients from function evaluations without requiring explicit parameter access. Needed because black-box models don't expose gradients, making traditional backpropagation impossible. Quick check: Verify that SPSA-GC can accurately estimate gradients for simple test functions before applying to complex vision tasks.

**Visual prompting**: The technique of adding learnable perturbations to input images to adapt pre-trained models to new tasks. Required because it enables parameter-efficient transfer learning without fine-tuning. Quick check: Confirm that small input perturbations can meaningfully affect model outputs for the target task.

**Distribution shift robustness**: The ability of models to maintain performance when test data differs from training data. Critical for real-world applications where data distributions vary. Quick check: Evaluate performance degradation when applying models trained on one dataset to related but different datasets.

## Architecture Onboarding

**Component map**: Input images -> Coordinator encoder (frozen) -> Feature extraction -> Coordinator decoder (trainable) -> Visual prompt generation -> CLIP model (black-box) -> Classification output

**Critical path**: The key optimization loop where the Coordinator decoder generates prompts based on extracted features, which are then applied to input images and evaluated by the black-box model. Gradients are estimated via SPSA-GC and used to update the decoder parameters.

**Design tradeoffs**: Full Coordinator (BlackVIP) vs. statistical extraction (BlackVIP-SE) - balances expressiveness with efficiency. SPSA-GC vs. other zeroth-order methods - trades off convergence speed with implementation simplicity. Input-dependent vs. fixed prompts - balances adaptation capability with computational cost.

**Failure signatures**: Poor convergence in SPSA-GC optimization, memory overflow during training, suboptimal performance on datasets with high intrinsic dimensionality, and degraded accuracy under severe distribution shifts.

**Three first experiments**:
1. Train BlackVIP on a simple dataset (Biased MNIST) to verify basic functionality and convergence
2. Compare BlackVIP-SE performance across datasets with varying intrinsic dimensionality
3. Evaluate robustness to input perturbations using the theoretical robustness radius

## Open Questions the Paper Calls Out

**Distribution shift generalization**: How does BlackVIP's input-dependent prompt design affect generalization to unseen distribution shifts beyond the tested WILDS datasets? The paper only evaluates three specific datasets with distinct shift characteristics, and the relationship between dataset intrinsic dimensionality and BlackVIP-SE's effectiveness is demonstrated but not thoroughly validated across broader scenarios.

**Theoretical robustness relationship**: What is the theoretical relationship between the learned visual prompt's variance and the certified robustness radius R in practice? While the theoretical relationship is established, the paper does not empirically validate how the learned prompt variance correlates with actual robustness performance across different datasets and tasks.

**Statistical feature extraction methods**: How does the choice of statistical feature extraction method (e.g., PCA vs. kernel PCA) affect BlackVIP-SE's performance and efficiency? The paper only reports results using vanilla PCA and does not explore the impact of different statistical methods on feature extraction quality or downstream performance.

## Limitations

- Performance gains may vary significantly depending on the specific dataset and task characteristics
- The exact architecture details of the Coordinator decoder are not fully specified, potentially impacting reproducibility
- Limited evaluation of the theoretical robustness guarantees in practical scenarios

## Confidence

- **High confidence**: Overall approach and methodology of BlackVIP
- **Medium confidence**: Specific implementation details and hyperparameter choices
- **Low confidence**: Generalizability of results to all potential datasets and tasks

## Next Checks

1. Implement the Coordinator architecture with provided specifications and train BlackVIP on a subset of benchmark datasets to verify reproducibility of reported performance gains.

2. Conduct ablation studies to evaluate the impact of different Coordinator decoder architectures and hyperparameter settings on final performance, identifying key success factors.

3. Test BlackVIP on additional datasets and tasks beyond those presented in the paper to assess generalizability and robustness across diverse domains, revealing potential limitations and improvement areas.