---
ver: rpa2
title: 'Fi$^2$VTS: Time Series Forecasting Via Capturing Intra- and Inter-Variable
  Variations in the Frequency Domain'
arxiv_id: '2407.21275'
source_url: https://arxiv.org/abs/2407.21275
tags:
- uni00000013
- frequency
- time
- series
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FreqTSF, a time series forecasting framework
  that improves long-term prediction by capturing intra- and inter-variable variations
  in the frequency domain. The method employs a Frequency Transform Module using Short-Time
  Fourier Transform (STFT) with multiple window sizes to extract real and imaginary
  components from time series data.
---

# Fi$^2$VTS: Time Series Forecasting Via Capturing Intra- and Inter-Variable Variations in the Frequency Domain

## Quick Facts
- arXiv ID: 2407.21275
- Source URL: https://arxiv.org/abs/2407.21275
- Reference count: 36
- Primary result: FreqTSF achieves 15% MSE reduction and 11% MAE reduction compared to state-of-the-art baselines

## Executive Summary
This paper introduces FreqTSF, a time series forecasting framework that captures intra- and inter-variable variations in the frequency domain. The method employs STFT with multiple window sizes to extract real and imaginary components, then uses a novel Frequency Cross Attention mechanism based on Kramers-Kronig relations to establish connections between these components. The model achieves significant performance improvements over state-of-the-art baselines while theoretically reducing computational complexity from O(L²) to O(L) per block.

## Method Summary
FreqTSF processes time series data through a residual architecture of multiple FreqBlocks. Each block performs STFT with predefined windows to obtain real and imaginary frequency components, applies Frequency Cross Attention to capture intra-variable variations using Kramers-Kronig relations, and integrates inter-variable correlations through inception blocks. The model uses L2 loss with Adam optimizer (learning rate 1e-3, batch size 32) and achieves theoretical complexity reduction while maintaining strong empirical performance.

## Key Results
- 15% relative MSE reduction compared to state-of-the-art baselines
- 11% relative MAE reduction across four benchmark datasets
- Theoretical complexity reduction from O(L²) to O(L) per block
- Residual architecture prevents degradation issues in deep models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frequency Cross Attention leverages Kramers-Kronig relations to enforce causal structure between real and imaginary parts
- Mechanism: Cross-attention between real and imaginary components simulates bidirectional mathematical connections required by KKRs
- Core assumption: Time series data can be approximated as causal systems where future outputs don't depend on past inputs
- Break condition: Non-causal dependencies or future values depending on past inputs in ways that violate KKRs

### Mechanism 2
- Claim: STFT with multiple window sizes captures both intra- and inter-variable variations
- Mechanism: Multi-scale frequency decomposition allows capturing periodic patterns and correlations across variables
- Core assumption: Real-world time series exhibit multi-periodicity effectively decomposed using STFT
- Break condition: Time series lacking clear periodic patterns or misaligned window sizes

### Mechanism 3
- Claim: Residual architecture with multiple FreqBlocks prevents degradation
- Mechanism: Stacked FreqBlocks with residual connections enable deep modeling without vanishing gradients
- Core assumption: Deep networks benefit from residual connections and frequency domain representations can be hierarchically refined
- Break condition: Residual connections failing to propagate gradients effectively

## Foundational Learning

- Concept: Fourier Transform and its properties
  - Why needed: Understanding frequency domain transformation is fundamental to grasping the Frequency Transform Module
  - Quick check: What does the Fourier Transform decompose a time series signal into, and why is this useful for capturing periodic patterns?

- Concept: Kramers-Kronig relations and causal systems
  - Why needed: Frequency Cross Attention is built on KKRs relating real and imaginary parts of causal system responses
  - Quick check: Why is it important that the model approximates data as a causal system in time series forecasting?

- Concept: Short-Time Fourier Transform (STFT) and windowing
  - Why needed: STFT is the specific transform used to obtain time-frequency representations
  - Quick check: How does using multiple window sizes in STFT help capture multi-periodicity in time series data?

## Architecture Onboarding

- Component map: Input → Embedding → [FreqBlock × N] → Output
- Critical path: 1. Normalize input time series, 2. Embed to higher dimension, 3. Apply N FreqBlocks (STFT → Cross-attention → Inception), 4. Denormalize output
- Design tradeoffs: Multiple window sizes increase power but add cost, Top-M selection reduces noise but may discard information, Cross-attention adds parameters but enforces causal structure
- Failure signatures: Poor performance on non-periodic/irregular time series, failure to capture sharp transitions, degradation in very deep configurations, overfitting with large M values
- First 3 experiments: 1. Baseline test vs vanilla Transformer on ETTh1, 2. Ablation test removing Frequency Cross Attention, 3. Hyperparameter sensitivity varying M values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Frequency Cross Attention compare to alternative attention mechanisms for frequency-domain representations?
- Basis: Paper introduces it as novel mechanism but doesn't benchmark against other attention approaches
- Resolution: Direct comparison experiments between Frequency Cross Attention and alternatives on benchmark datasets

### Open Question 2
- Question: What is the optimal strategy for selecting top-M frequency components?
- Basis: Paper uses Top-M selection but only examines effect of M's magnitude, not selection strategy
- Resolution: Comparative experiments testing different frequency component selection strategies

### Open Question 3
- Question: How does FreqTSF perform on time series with different characteristics compared to specialized models?
- Basis: Paper shows good performance on benchmarks but acknowledges limitations with datasets containing mutations
- Resolution: Controlled experiments on synthetically generated time series with varying characteristics

## Limitations
- Theoretical foundation linking Kramers-Kronig relations to attention mechanism requires more rigorous mathematical justification
- Performance may not generalize to real-world industrial time series with irregular patterns and non-stationarities
- Claims of O(L) complexity reduction need independent verification across different sequence lengths

## Confidence
- High Confidence: Residual architecture design and overall framework implementation
- Medium Confidence: Frequency domain approach and multi-window STFT benefits
- Low Confidence: Theoretical foundation of Kramers-Kronig relations application

## Next Checks
1. Test FreqTSF on synthetic time series with known non-causal dependencies to verify KKRs-based attention doesn't incorrectly impose causal structure
2. Independently measure actual computational complexity of full pipeline across different sequence lengths to verify claimed O(L) reduction
3. Evaluate FreqTSF on real-world datasets with known anomalies and abrupt changes to assess handling of non-periodic patterns