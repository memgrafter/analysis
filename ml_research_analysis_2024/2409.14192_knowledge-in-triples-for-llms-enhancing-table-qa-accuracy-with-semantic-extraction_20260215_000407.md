---
ver: rpa2
title: 'Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic Extraction'
arxiv_id: '2409.14192'
source_url: https://arxiv.org/abs/2409.14192
tags:
- answers
- data
- table
- tables
- triples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of interpreting complex semi-structured
  tables in natural language processing (NLP), particularly in the FeTaQA dataset,
  where traditional methods like SQL and SPARQL struggle with irregular table structures.
  The proposed solution extracts triples directly from tabular data and integrates
  them into a retrieval-augmented generation (RAG) model, which is then fine-tuned
  with a GPT-3.5-turbo-0125 model.
---

# Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic Extraction

## Quick Facts
- arXiv ID: 2409.14192
- Source URL: https://arxiv.org/abs/2409.14192
- Reference count: 33
- Primary result: Proposes triple extraction + RAG + fine-tuned GPT-3.5-turbo-0125, achieving Sacre-BLEU 31.3 and ROUGE-1/2/L: 0.67/0.44/0.55 on FeTaQA.

## Executive Summary
This paper addresses the challenge of interpreting complex semi-structured tables in NLP by extracting triples directly from tabular data and integrating them into a retrieval-augmented generation (RAG) model. The approach uses RDFLib to convert table rows and headers into Subject-Predicate-Object triples, which are then used by a fine-tuned GPT-3.5-turbo-0125 model to generate accurate long-form answers. The method significantly improves performance on the FeTaQA dataset, achieving strong Sacre-BLEU and ROUGE scores while reducing hallucinations compared to traditional SQL/SPARQL methods.

## Method Summary
The approach extracts RDF triples from tables using RDFLib, converts them into a text-friendly format, and integrates them into a RAG pipeline. A GPT-3.5-turbo-0125 model is fine-tuned on a subset of the FeTaQA training data (800 samples over 3 epochs) to adapt to table-specific patterns. The RAG retriever accesses the most relevant triple-based chunks during generation, grounding responses in factual table data. The final output is evaluated using Sacre-BLEU and ROUGE metrics.

## Key Results
- Achieves Sacre-BLEU score of 31.3 on FeTaQA validation set.
- ROUGE-1, ROUGE-2, and ROUGE-L scores of 0.67, 0.44, and 0.55 respectively.
- Outperforms existing baselines like T5 and Codex models on FeTaQA.
- Demonstrates effectiveness in reducing hallucinations through factual grounding.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Triples extraction transforms structured tabular relationships into a text-friendly format that aligns with LLM training.
- Mechanism: RDFLib constructs Subject-Predicate-Object triples from table rows and headers, enabling LLMs to interpret structured data as text sequences they were trained on.
- Core assumption: LLMs struggle with structured tabular formats but excel at text-based relational reasoning.
- Evidence anchors:
  - [section]: "RDFLib converts data into RDF triples, which consist of a subject, predicate, and object... This format helps LLMs model to understand the table better in the way that it has been trained."
  - [abstract]: "Traditional approaches, such as SQL and SPARQL, often fail to fully capture the semantics of such data, especially in the presence of irregular table structures like web tables."
- Break Condition: If triples fail to preserve semantic relationships or introduce ambiguity in entity identification.

### Mechanism 2
- Claim: Retrieval-augmented generation (RAG) grounds LLM responses in factual table data, reducing hallucinations.
- Mechanism: RAG retrieves relevant triple-based chunks (1024-token segments) based on query context, feeding them to GPT-3.5-turbo-0125 during generation.
- Core assumption: LLM hallucinations decrease when responses are anchored to retrieved factual evidence.
- Evidence anchors:
  - [abstract]: "Integrating structured knowledge in triples format... from RAG alongside the advanced capabilities of the fine-tuned LLM. This combination significantly improves the accuracy and relevance of generated outputs."
  - [section]: "The retriever is responsible for accessing triples’ most relevant extracted information, which the language model then utilizes during the response generation process."
- Break Condition: If retrieval fails to fetch relevant chunks or if model overfits to retrieved context.

### Mechanism 3
- Claim: Fine-tuning GPT-3.5-turbo-0125 on table-derived text samples adapts the model to FeTaQA’s structural and semantic patterns.
- Mechanism: Model fine-tuned on 800 samples over 3 epochs learns table-specific language patterns and question-answering strategies.
- Core assumption: Domain-specific fine-tuning improves performance on structurally complex tasks compared to zero-shot baselines.
- Evidence anchors:
  - [section]: "The fine-tuning process was conducted on a partial set of the training dataset consisting of 800 out of 7326 samples in 3 epochs. This process was designed to adapt the model to the specific characteristics of the tables format and FeTaQA dataset."
  - [section]: "The comparison between GPT-3.5-turbo-0125 fine-tuned with and without triples and RAG integration highlights the importance of these components."
- Break Condition: If fine-tuning data is insufficient or does not capture table-specific reasoning patterns.

## Foundational Learning

- Concept: RDF triples (Subject-Predicate-Object)
  - Why needed here: Provides a structured yet text-friendly representation of table relationships for LLM consumption.
  - Quick check question: What are the three components of an RDF triple and how do they map to table cells?
- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Grounds LLM outputs in factual evidence from tables, reducing hallucinations and improving accuracy.
  - Quick check question: How does RAG differ from standard LLM prompting in terms of information access?
- Concept: Evaluation metrics (SacreBLEU, ROUGE-1/2/L)
  - Why needed here: Quantifies improvements in n-gram overlap and sequence coherence between generated and reference answers.
  - Quick check question: What is the key difference between ROUGE-1 and ROUGE-L in evaluating generated text?

## Architecture Onboarding

- Component map: Input tables -> RDFLib triple extraction -> RAG retriever (1024-token chunks) -> Fine-tuned GPT-3.5-turbo-0125 -> Evaluated via S-BLEU and ROUGE
- Critical path: Triple extraction -> RAG retrieval -> LLM generation -> evaluation
- Design tradeoffs:
  - Token chunking vs. retrieval completeness (1024 tokens may truncate context)
  - Fine-tuning data size vs. model adaptation quality (800 samples may limit generalization)
  - Triple granularity vs. semantic clarity (overly fine triples may introduce noise)
- Failure signatures:
  - Low S-BLEU/ROUGE scores indicate poor semantic alignment
  - Retrieval failures result in hallucinated or off-topic answers
  - Token truncation in chunks may lose critical table relationships
- First 3 experiments:
  1. Test triple extraction fidelity: Input simple table -> verify RDFLib output triples match expected Subject-Predicate-Object structure.
  2. Evaluate RAG retrieval quality: Query with known question -> check top-10 retrieved chunks contain relevant table data.
  3. Baseline LLM performance: Run fine-tuned GPT-3.5-turbo-0125 without RAG on FeTaQA -> compare S-BLEU/ROUGE to full pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of triples and RAG compare to other knowledge graph-based approaches for table QA?
- Basis in paper: [explicit] The paper discusses the limitations of existing knowledge extraction methods and proposes a direct approach using triples and RAG.
- Why unresolved: The paper does not provide a direct comparison with other knowledge graph-based methods, such as those using DBpedia or Wikidata.
- What evidence would resolve it: A comparative study between the triples-RAG approach and other knowledge graph-based methods on the same datasets.

### Open Question 2
- Question: How does the fine-tuning process affect the model's ability to handle different table structures and domains?
- Basis in paper: [inferred] The paper mentions that the fine-tuning process was designed to adapt the model to the specific characteristics of the FeTaQA dataset, but does not discuss its generalizability to other datasets.
- Why unresolved: The paper does not provide evidence on how the model performs on datasets with different table structures or domains.
- What evidence would resolve it: Experiments on multiple datasets with varying table structures and domains to evaluate the model's performance and generalizability.

### Open Question 3
- Question: How can user-centric evaluation metrics, such as user satisfaction, be incorporated into the assessment of table-based QA systems?
- Basis in paper: [explicit] The paper acknowledges the limitations of traditional evaluation metrics like S-BLEU and ROUGE and suggests incorporating user-centric metrics in future work.
- Why unresolved: The paper does not provide a concrete framework or methodology for incorporating user-centric metrics into the evaluation process.
- What evidence would resolve it: Development and implementation of a user-centric evaluation framework that includes metrics such as user satisfaction, correctness, fluency, adequacy, and faithfulness.

## Limitations
- Data Dependency: Performance relies heavily on RDFLib's ability to accurately extract triples from irregular web tables.
- Evaluation Scope: Lacks absolute performance benchmarks and detailed ablation studies to isolate component contributions.
- Implementation Gaps: Missing fine-tuning configuration and RAG retriever parameter details hinder faithful reproduction.

## Confidence
- High Confidence: Using RDF triples to convert tabular data into a text-friendly format for LLM consumption is well-established.
- Medium Confidence: Reported improvements in Sacre-BLEU and ROUGE metrics are promising but lack absolute benchmarks and detailed ablation studies.
- Low Confidence: No evidence provided for hallucination reduction, a critical claim for table QA systems.

## Next Checks
1. Test triple extraction fidelity: Input a simple FeTaQA table into RDFLib and verify that the extracted triples accurately represent the Subject-Predicate-Object relationships in the table.
2. Evaluate RAG retrieval quality: Use a known FeTaQA question to query the RAG retriever and inspect the top-10 retrieved chunks for relevance to the table data.
3. Hallucination reduction: Design a test set of FeTaQA questions with known answers and evaluate the model's outputs for factual accuracy compared to a baseline LLM without RAG or fine-tuning.