---
ver: rpa2
title: 'YouTube Video Analytics for Patient Engagement: Evidence from Colonoscopy
  Preparation Videos'
arxiv_id: '2410.02830'
source_url: https://arxiv.org/abs/2410.02830
tags:
- videolevel
- metadata
- page
- numberof
- youtube
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates a data analysis pipeline for retrieving
  and analyzing medical information from YouTube videos on colonoscopy preparation.
  The approach combines the YouTube Data API for metadata collection, Google Video
  Intelligence API for content analysis, and a BiLSTM model for medical term extraction.
---

# YouTube Video Analytics for Patient Engagement: Evidence from Colonoscopy Preparation Videos

## Quick Facts
- arXiv ID: 2410.02830
- Source URL: https://arxiv.org/abs/2410.02830
- Reference count: 4
- This study demonstrates a data analysis pipeline for retrieving and analyzing medical information from YouTube videos on colonoscopy preparation, achieving 0.83 F-measure for medical information classification and 0.95 accuracy for recommendation classification.

## Executive Summary
This study presents a comprehensive framework for analyzing YouTube videos about colonoscopy preparation, combining multiple APIs and machine learning models to classify videos based on medical information content, understandability, and recommendation status. The approach integrates YouTube Data API for metadata collection, Google Cloud Video Intelligence API for content analysis, and a BiLSTM model for medical term extraction. The resulting classification models achieve strong performance in identifying medically informative videos (F-measure 0.83) and predicting recommendation status (accuracy 0.95), while demonstrating challenges in assess understandability (F-measure 0.50). The methodology provides healthcare stakeholders with a scalable approach for evaluating and generating educational video content.

## Method Summary
The study collected 312 YouTube videos about colonoscopy using 20 search keywords, extracting metadata through YouTube Data API and video content features using Google Cloud Video Intelligence API. Videos were annotated by three graduate research associates for medical information level, understandability (using PEMAT framework), and overall recommendation status. A BiLSTM model extracted medical terms from video descriptions and transcripts, while three logistic regression classifiers were built to categorize videos based on medical information level (achieving F-measure 0.83), understandability (F-measure 0.50), and recommendation status (accuracy 0.95). The feature set combined metadata features (title, description, duration) with video-level features including medical term count, transcription confidence, and video organization metrics.

## Key Results
- Medical information classifier achieves 0.83 F-measure in identifying videos with valid medical content
- Recommendation classifier achieves 0.95 overall accuracy in predicting video recommendation status
- Understandability classifier shows lower performance with 0.50 F-measure, indicating challenges in assessing video accessibility
- Medical term count, transcription quality, and video organization metrics are significant predictors of recommendation status

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Medical term extraction accuracy directly predicts video recommendation status
- Mechanism: The BiLSTM model identifies medical terms from video descriptions and speech-to-text transcripts, producing a count of unique medical terms per video used as a predictor in the recommendation classifier
- Core assumption: Higher counts of relevant medical terms correlate with video quality and usefulness for patient education
- Evidence anchors: [abstract] "The medical information classifier achieves 0.83 F-measure"; [section] "The number of unique medical terms is a significant predictor for recommended videos"

### Mechanism 2
- Claim: Video understandability features (OCR confidence, readability, sentence structure) predict recommendation status
- Mechanism: Google Cloud Video Intelligence API provides features like OCR confidence, readability scores, and sentence counts that indicate video organization and accessibility, used as inputs to the recommendation classifier
- Core assumption: Videos with higher OCR confidence and better readability are more likely to be recommended for patient education
- Evidence anchors: [abstract] "The understandability classifier achieves 0.50 F-measure"; [section] "OCR confidence, number of sentences, and readability are significant predictors in the recommendation model"

### Mechanism 3
- Claim: Combining metadata features (title, description, tags) with video-level features improves classification accuracy
- Mechanism: The study uses both metadata from YouTube API and video-level features from Google Video Intelligence API to build a comprehensive feature set for classification
- Core assumption: Metadata and video content provide complementary information that improves prediction accuracy when combined
- Evidence anchors: [abstract] "The recommendation classifier achieves 0.95 overall accuracy"; [section] "Feature set includes metadata features plus video-level features like medical term count, transcription confidence, and video organization metrics"

## Foundational Learning

- Concept: Bidirectional LSTM for named entity recognition
  - Why needed here: To extract medical terms from unstructured video descriptions and transcripts
  - Quick check question: How does a bidirectional LSTM differ from a standard LSTM in processing sequential data?

- Concept: Logistic regression classification
  - Why needed here: To classify videos based on medical information level, understandability, and recommendation status using extracted features
  - Quick check question: What assumptions does logistic regression make about the relationship between features and the log-odds of the outcome?

- Concept: Google Cloud Video Intelligence API features
  - Why needed here: To extract structured features from video content including OCR, transcription, and object detection
  - Quick check question: What types of features does the LABEL_DETECTION capability provide for video content analysis?

## Architecture Onboarding

- Component map: YouTube Data API -> Google Cloud Video Intelligence API -> BiLSTM model -> Feature engineering -> Logistic regression classifiers
- Critical path: YouTube API → Video Intelligence API → BiLSTM extraction → Feature engineering → Logistic regression classification
- Design tradeoffs: Using Google APIs provides comprehensive features but creates dependency on external services and potential cost constraints. BiLSTM offers good performance but requires training data and computational resources.
- Failure signatures: Low F-measure scores indicate poor model performance; high false positive rates suggest issues with feature relevance; API failures indicate external service problems.
- First 3 experiments:
  1. Test BiLSTM medical term extraction on a small sample of manually annotated videos to verify performance
  2. Validate that Google API features (OCR, transcription) are being extracted correctly from test videos
  3. Run logistic regression with only metadata features to establish baseline performance before adding video-level features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the causal relationship between the recommendation status of colonoscopy videos and actual patient engagement metrics like watch time, likes, and shares?
- Basis in paper: [explicit] The authors state their intention to conduct causal analysis with engagement measures to evaluate whether recommended videos actually engage patients more effectively.
- Why unresolved: The study focused on developing classification models but did not conduct the planned causal analysis with engagement metrics.
- What evidence would resolve it: Analyzing engagement metrics for recommended vs. non-recommended videos to determine if recommendation status correlates with increased patient engagement.

### Open Question 2
- Question: How do patient-centered think-aloud protocols and group studies influence the evaluation of selected colonoscopy videos?
- Basis in paper: [explicit] The authors mention their intention to use think-aloud protocols and group study protocols to evaluate how patients and educators perceive the selected videos.
- Why unresolved: These qualitative evaluation methods were planned but not yet implemented in the study.
- What evidence would resolve it: Conducting think-aloud sessions and group studies with patients and healthcare educators to gather qualitative feedback on video effectiveness and understandability.

### Open Question 3
- Question: What are the optimal boundary conditions and preprocessing techniques for medical term extraction from YouTube video content?
- Basis in paper: [inferred] The authors noted challenges with boundary conditions in their medical term extraction approach and suggested future work on developing heuristic algorithms for preprocessing.
- Why unresolved: The study encountered issues with boundary conditions in medical term extraction but did not fully resolve them.
- What evidence would resolve it: Testing different boundary detection algorithms and preprocessing techniques to improve medical term extraction accuracy from video transcripts and descriptions.

## Limitations
- Understandability classifier shows notably low performance (F-measure 0.50), suggesting challenges in reliably assessing video accessibility for patients
- Annotation process relies on three graduate research associates, which could introduce bias despite inter-rater agreement measures
- Focus on YouTube as a single platform limits generalizability to other video-sharing platforms or medical education contexts

## Confidence
- **High Confidence**: The medical information classifier performance (F-measure 0.83) and recommendation classifier accuracy (0.95) are well-supported by clear methodology and results
- **Medium Confidence**: The feature engineering approach and logistic regression methodology are sound, but the low understandability classifier performance raises questions about feature selection and model adequacy
- **Low Confidence**: The generalizability of findings beyond YouTube and the specific context of colonoscopy preparation, given the platform-specific nature of the analysis and limited cross-platform validation

## Next Checks
1. Conduct inter-rater reliability analysis on understandability annotations to identify sources of low classifier performance and refine scoring criteria
2. Test the recommendation classifier on an independent dataset of colonoscopy preparation videos to assess model robustness and generalizability
3. Compare classification performance when using only metadata features versus combined metadata and video-level features to quantify the added value of Google Video Intelligence API features