---
ver: rpa2
title: What do physics-informed DeepONets learn? Understanding and improving training
  for scientific computing applications
arxiv_id: '2411.18459'
source_url: https://arxiv.org/abs/2411.18459
tags:
- physics-informed
- functions
- basis
- deeponet
- deeponets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates what physics-informed DeepONets learn by
  analyzing extracted basis functions and their potential for model reduction. The
  authors demonstrate that physics-informed DeepONets can train as accurately as data-driven
  models for smooth problems like advection-diffusion and viscous Burgers with higher
  viscosity, but require fewer basis functions (25% fewer for advection-diffusion)
  when used in spectral methods.
---

# What do physics-informed DeepONets learn? Understanding and improving training for scientific computing applications

## Quick Facts
- arXiv ID: 2411.18459
- Source URL: https://arxiv.org/abs/2411.18459
- Reference count: 40
- Physics-informed DeepONets learn effective basis functions for model reduction, requiring fewer modes than data-driven approaches for smooth problems

## Executive Summary
This paper investigates what physics-informed DeepONets learn by analyzing extracted basis functions and their potential for model reduction. The authors demonstrate that physics-informed DeepONets can train as accurately as data-driven models for smooth problems like advection-diffusion and viscous Burgers with higher viscosity, but require fewer basis functions (25% fewer for advection-diffusion) when used in spectral methods. The decay of singular values and expansion coefficients provides clear metrics for assessing DeepONet performance. The paper introduces a transfer learning approach that significantly improves training for difficult problems where physics-informed DeepONets typically struggle, such as viscous Burgers with low viscosity (ν=0.0001) and Korteweg-de Vries equations.

## Method Summary
The authors train DeepONets on various PDEs using both data-driven and physics-informed approaches, then extract basis functions from the trunk network via SVD to evaluate what is learned. They compare singular value decay and expansion coefficient behavior between training methods, and implement spectral evolution using extracted basis functions. For difficult problems, they apply transfer learning by initializing physics-informed DeepONets with parameters from related, easier problems. The study uses NTK/CK adaptive weighting schemes and evaluates performance across advection-diffusion, viscous Burgers (multiple viscosity values), and Korteweg-de Vries equations.

## Key Results
- Physics-informed DeepONets require 25% fewer basis functions than data-driven models for advection-diffusion while achieving similar accuracy
- Transfer learning from high viscosity (ν=0.001) to low viscosity (ν=0.0001) viscous Burgers reduces error by up to 50%
- Singular value decay and expansion coefficient behavior provide clear metrics for assessing DeepONet performance and basis function effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Physics-informed training leads to more effective basis functions for model reduction.
- Mechanism: By incorporating physical laws directly into the training loss through automatic differentiation, the DeepONet learns basis functions that inherently satisfy the PDE structure. These basis functions capture the essential solution space more efficiently, requiring fewer modes to achieve the same accuracy.
- Core assumption: The physical constraints embedded during training guide the learning toward basis functions that better represent the solution manifold of the PDE.
- Evidence anchors:
  - [abstract] "The results show that physics-informed training leads to more effective basis functions for model reduction"
  - [section] "Using the custom basis functions from the physics-informed DeepONet results in lower error"
  - [corpus] Weak - no direct corpus support found
- Break condition: If the PDE is highly nonlinear or the physical constraints are poorly formulated, the physics-informed training may not provide advantages over data-driven approaches.

### Mechanism 2
- Claim: Transfer learning significantly improves training for difficult PDE problems by leveraging knowledge from related, easier problems.
- Mechanism: Parameters learned from solving a simpler version of a PDE (e.g., higher viscosity) provide a good initialization for solving the more difficult version (e.g., lower viscosity). This "warm start" allows the model to converge faster and achieve lower error.
- Core assumption: Related PDEs share similar solution structures, and knowledge gained from one can be effectively transferred to another.
- Evidence anchors:
  - [abstract] "transfer learning approach that significantly improves training for physics-informed DeepONets between parameters of the same PDE as well as across different, but related, PDEs"
  - [section] "Initializing the physics-informed DeepONet for solving viscous Burgers with ν = 0.0001 with the trained parameters from ν = 0.001, this error decreases to almost 7%"
  - [corpus] Weak - no direct corpus support found
- Break condition: If the source and target problems are not sufficiently related, transfer learning may provide little benefit or even degrade performance.

### Mechanism 3
- Claim: The decay of singular values and expansion coefficients provides clear metrics for assessing DeepONet performance and the effectiveness of learned basis functions.
- Mechanism: Rapid decay in singular values indicates that the learned basis functions efficiently capture the solution space with few modes. Similarly, rapid decay in expansion coefficients shows that the target functions can be well-approximated by the learned basis set.
- Core assumption: The spectrum of the trunk network's function space reflects how well the DeepONet has learned to represent the solution operator.
- Evidence anchors:
  - [abstract] "Results provide clarity about measuring the performance of a physics-informed DeepONet through the decays of singular values and expansion coefficients"
  - [section] "The singular values of the basis functions extracted from the physics-informed model trained with NTK adaptive weighting show significantly faster decay"
  - [corpus] Weak - no direct corpus support found
- Break condition: If the problem has highly oscillatory or fractal-like solutions, the singular value decay may not be as informative about model quality.

## Foundational Learning

- Concept: DeepONet architecture (branch and trunk networks)
  - Why needed here: Understanding how DeepONets approximate operators is crucial for interpreting what is learned and how transfer learning works across parameters or related PDEs.
  - Quick check question: How does the dot product of branch and trunk networks approximate a continuous nonlinear operator?

- Concept: Physics-informed neural networks and automatic differentiation
  - Why needed here: The physics-informed approach uses automatic differentiation to enforce PDE constraints, which is central to understanding why the learned basis functions are more effective.
  - Quick check question: How does automatic differentiation enable the incorporation of PDE residuals into the training loss?

- Concept: Singular value decomposition and basis function extraction
  - Why needed here: The SVD-based method for extracting basis functions from the trunk network is key to the model reduction approach and understanding what the DeepONet learns.
  - Quick check question: How does the SVD of the trunk network's function space provide an orthonormal basis for the solution operator?

## Architecture Onboarding

- Component map: Branch network (encodes input function space) → Dot product layer → Trunk network (encodes output function space). For physics-informed training, additional terms for PDE residuals, initial conditions, and boundary conditions are added to the loss function.
- Critical path: Input function → Branch network encoding → Trunk network evaluation at output locations → Dot product aggregation → Output function approximation. For physics-informed training, the critical path includes computing derivatives via automatic differentiation.
- Design tradeoffs: Physics-informed training trades data requirements for computational cost of automatic differentiation and potentially slower convergence. Transfer learning trades training time for improved performance on difficult problems.
- Failure signatures: Slow decay of singular values or expansion coefficients indicates poor learning. High error on test problems suggests inadequate training or inappropriate transfer initialization.
- First 3 experiments:
  1. Train a data-driven DeepONet on advection-diffusion and examine singular value decay and expansion coefficients.
  2. Train a physics-informed DeepONet on the same problem and compare the basis functions and error metrics.
  3. Apply transfer learning from high viscosity to low viscosity viscous Burgers and measure error reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How universal are the basis functions extracted from physics-informed DeepONets across different types of PDEs, and what determines their universality?
- Basis in paper: [explicit] The authors demonstrate that basis functions from data-driven and physics-informed DeepONets show good agreement across models for advection-diffusion, suggesting potential universality in what is learned during training.
- Why unresolved: While the paper shows some universality in basis functions for specific problems like advection-diffusion and viscous Burgers, it does not systematically investigate the conditions under which basis functions might be universal across different PDE types or problem parameters.
- What evidence would resolve it: Systematic comparisons of basis functions extracted from DeepONets trained on diverse PDE types, parameter regimes, and initial conditions, along with analysis of their functional similarities and differences.

### Open Question 2
- Question: What are the fundamental limitations of physics-informed DeepONets for training on problems with low viscosity or other challenging parameter regimes, and can these limitations be systematically addressed?
- Basis in paper: [explicit] The paper shows that physics-informed DeepONets struggle to train for viscous Burgers with low viscosity (ν=0.0001) but can be improved through transfer learning from easier problems.
- Why unresolved: The paper demonstrates that transfer learning helps for this specific case, but does not investigate the fundamental reasons why physics-informed DeepONets fail for low viscosity problems or whether other systematic approaches could address these limitations.
- What evidence would resolve it: Theoretical analysis of the loss landscape and optimization dynamics for physics-informed DeepONets in challenging regimes, combined with empirical studies of alternative training strategies.

### Open Question 3
- Question: How does the choice between neural tangent kernel (NTK) and conjugate kernel (CK) weighting schemes affect the quality of learned basis functions and their effectiveness in spectral methods?
- Basis in paper: [explicit] The paper mentions that CK weights demonstrated better performance than NTK weights for Korteweg-de Vries, while NTK was better for viscous Burgers, but does not systematically compare their effects on basis function quality.
- Why unresolved: While the paper briefly compares NTK and CK performance, it does not investigate how these different weighting schemes affect the extracted basis functions and their subsequent effectiveness in spectral methods.
- What evidence would resolve it: Systematic comparison of basis functions and spectral method performance using NTK vs CK weighting schemes across multiple PDE types and parameter regimes.

## Limitations
- Physics-informed DeepONets struggle with highly nonlinear or multi-scale problems
- Transfer learning effectiveness depends heavily on similarity between source and target problems
- SVD-based basis extraction requires sufficient quadrature points and polynomial expansion order

## Confidence
- High confidence: Claims about singular value decay as a performance metric, effectiveness of transfer learning for moderate parameter changes (ν=0.001→0.0001)
- Medium confidence: Claims about physics-informed training requiring 25% fewer basis functions, effectiveness across different PDE families
- Low confidence: Generalizability to highly nonlinear or multi-scale problems not explicitly tested

## Next Checks
1. Test transfer learning effectiveness across a broader parameter range (e.g., ν=0.1→0.0001) to establish failure boundaries
2. Evaluate performance on highly nonlinear PDEs like Navier-Stokes to assess limitations
3. Compare with alternative operator learning methods (Fourier Neural Operator, etc.) on the same problems to benchmark relative performance