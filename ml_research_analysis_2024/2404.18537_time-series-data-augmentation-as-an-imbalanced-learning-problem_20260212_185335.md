---
ver: rpa2
title: Time Series Data Augmentation as an Imbalanced Learning Problem
arxiv_id: '2404.18537'
source_url: https://arxiv.org/abs/2404.18537
tags:
- series
- time
- tser
- data
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach called TSER for generating
  synthetic time series samples to improve forecasting accuracy. The key idea is to
  frame forecasting with collections of time series as an imbalanced learning problem,
  where the observations for a target time series are under-represented.
---

# Time Series Data Augmentation as an Imbalanced Learning Problem

## Quick Facts
- arXiv ID: 2404.18537
- Source URL: https://arxiv.org/abs/2404.18537
- Authors: Vitor Cerqueira; Nuno Moniz; Ricardo InÃ¡cio; Carlos Soares
- Reference count: 20
- Key outcome: TSER outperforms standard global and local methods, achieving a better trade-off between the two approaches.

## Executive Summary
This paper proposes TSER (Time Series Data Augmentation as an Imbalanced Learning Problem), a novel approach to improve forecasting accuracy for time series by framing the problem as imbalanced learning. The key insight is that observations from a target time series are under-represented in the training data. TSER uses oversampling techniques like SMOTE to create synthetic samples for the target series, augmenting the training data. Experiments on 7 datasets with 5502 time series show TSER outperforms standard global and local methods, achieving a better trade-off between the two. The method is most effective when using a 2:1 sampling ratio between the target series and other series.

## Method Summary
TSER transforms time series forecasting into an imbalanced learning problem by oversampling the target series using techniques like SMOTE. The method involves mean normalization and time delay embedding to create supervised learning data, then applies oversampling to generate synthetic samples for the target series while preserving the input-output dependency. A global forecasting model (using lightgbm) is trained on the augmented dataset. The approach aims to balance local characteristics of the target series with global patterns from the entire collection.

## Key Results
- TSER outperforms both global models (trained on all series) and local models (trained on target series only)
- Best performance achieved with approximately 2:1 sampling ratio between target series and other series
- Experiments conducted on 7 datasets containing 5502 total time series
- Demonstrated improved forecasting accuracy across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TSER improves forecasting accuracy by balancing the representation of a target time series in the training data using oversampling.
- Mechanism: The method transforms the forecasting problem into an imbalanced learning task, where the target series is the minority class. Oversampling techniques like SMOTE create synthetic samples for the target series, increasing its representation in the training set.
- Core assumption: The observations of a particular time series represent a small fraction of the total dataset, leading to underrepresentation and reduced model performance.
- Evidence anchors:
  - [abstract] "Our approach stems from the insight that the observations concerning a particular time series of interest represent only a small fraction of all observations."
  - [section 2.2] "In this context, we frame the problem of training a forecasting model with several time series as an imbalanced learning task."

### Mechanism 2
- Claim: Creating synthetic samples preserves the input-output dependency of the time series.
- Mechanism: The resampling algorithms interpolate between instances from the target series within their neighborhood, generating new (x', y') samples that maintain the structure of the original data.
- Core assumption: The interpolation between neighboring instances of the target series preserves the underlying temporal patterns and relationships.
- Evidence anchors:
  - [section 3.2] "Since we preserve the input x and output y dependency, these samples have the same structure as the original data."

### Mechanism 3
- Claim: TSER achieves a better global-local trade-off by augmenting the dataset for a specific time series while retaining information from other series.
- Mechanism: The augmented dataset combines synthetic samples of the target series with the original data from other series, allowing the model to learn both global patterns and local nuances.
- Core assumption: Information from other time series in the collection is valuable for improving the forecasting accuracy of the target series.
- Evidence anchors:
  - [abstract] "We found that the proposed solution outperforms both a global and a local model, thus providing a better trade-off between these two approaches."
  - [section 3.2] "This forecasting model has some global traits because the input data contains information from multiple time series."

## Foundational Learning

- Concept: Time series forecasting using autoregressive modeling
  - Why needed here: TSER is applied to univariate time series forecasting, where each observation is modeled as a function of its past lags.
  - Quick check question: How does time delay embedding transform a time series into a supervised learning problem?

- Concept: Imbalanced domain learning and resampling strategies
  - Why needed here: TSER frames the forecasting problem as an imbalanced learning task and uses oversampling techniques like SMOTE to create synthetic samples.
  - Quick check question: What is the main idea behind SMOTE and how does it generate synthetic samples?

- Concept: Global vs. local forecasting models
  - Why needed here: TSER aims to improve the trade-off between global models (trained on multiple time series) and local models (trained on a single time series).
  - Quick check question: What are the advantages and disadvantages of global and local forecasting models?

## Architecture Onboarding

- Component map:
  Data preparation -> Time delay embedding -> Oversampling (SMOTE) -> Model training -> Evaluation

- Critical path:
  1. Prepare the data by applying mean normalization and time delay embedding.
  2. Create an auxiliary binary variable to indicate samples from the target series.
  3. Apply the oversampling algorithm to generate synthetic samples for the target series.
  4. Combine the synthetic samples with the original dataset.
  5. Train a global forecasting model on the augmented dataset.
  6. Evaluate the model's performance on the target series and other series in the collection.

- Design tradeoffs:
  - Balancing the sampling ratio between the target series and other series
  - Choosing the appropriate oversampling algorithm (e.g., SMOTE, ADASYN)
  - Determining the size of the neighborhood for interpolation in SMOTE
  - Deciding whether to include or discard data from other series after oversampling

- Failure signatures:
  - Poor performance on the target series despite oversampling
  - Overfitting to the synthetic samples, leading to reduced generalization
  - Increased computational cost due to training separate models for each series

- First 3 experiments:
  1. Apply TSER with SMOTE to a collection of time series and compare the performance with a global and local model on the target series.
  2. Vary the sampling ratio and analyze its impact on the model's performance and the trade-off between global and local characteristics.
  3. Experiment with different oversampling algorithms (e.g., SMOTE, ADASYN, BSMOTE) and assess their effectiveness in improving forecasting accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TSER's performance vary when applied to multivariate time series instead of univariate time series?
- Basis in paper: [inferred] The paper mentions that TSER is applicable to collections of univariate time series with exogenous variables or collections of multivariate time series, but does not provide experimental results for these cases.
- Why unresolved: The experiments were only conducted on univariate time series datasets, leaving the performance on multivariate time series unexplored.
- What evidence would resolve it: Conducting experiments on multivariate time series datasets and comparing the performance of TSER with other methods would provide insights into its effectiveness for multivariate time series.

### Open Question 2
- Question: What is the optimal sampling ratio for TSER when dealing with time series of varying lengths?
- Basis in paper: [explicit] The paper mentions that the best sampling ratio is around 2:1 based on experiments, but this was estimated on a particular collection with time series of similar lengths.
- Why unresolved: The optimal sampling ratio may vary depending on the length of the time series in the collection, which was not explored in the experiments.
- What evidence would resolve it: Conducting experiments on collections with time series of varying lengths and analyzing the performance of TSER with different sampling ratios would help determine the optimal ratio for such cases.

### Open Question 3
- Question: How does the performance of TSER change when using different resampling algorithms, such as SMOTE variants or other oversampling techniques?
- Basis in paper: [explicit] The paper mentions that TSER was tested with SMOTE, ADASYN, BSMOTE, and Near-Miss, but does not provide a comprehensive comparison of all possible resampling algorithms.
- Why unresolved: The paper only explores a limited set of resampling algorithms, leaving the potential benefits of other techniques unexplored.
- What evidence would resolve it: Conducting experiments with a wider range of resampling algorithms and comparing their performance with TSER would provide insights into the most effective algorithms for time series data augmentation.

## Limitations
- TSER requires separate models for each target series, limiting scalability
- Effectiveness depends on the relationship between target series and other series in collection
- Optimal sampling ratio may vary with time series characteristics and lengths
- Limited exploration of different resampling algorithms beyond SMOTE variants

## Confidence
- **High confidence**: The core insight that time series forecasting can be framed as an imbalanced learning problem is well-supported by the evidence provided.
- **Medium confidence**: The claim that TSER achieves a better global-local trade-off is supported by experimental results, but the analysis could be more comprehensive.
- **Medium confidence**: The assertion that TSER is most effective with a 2:1 sampling ratio is based on experiments, but the optimal ratio may vary depending on the dataset characteristics.

## Next Checks
1. Experiment with different sampling ratios to determine the optimal ratio between target series and other series for various dataset characteristics.
2. Compare different oversampling algorithms (e.g., SMOTE, ADASYN, BSMOTE) to identify the most effective approach for time series data augmentation.
3. Analyze how the relationship between the target series and other series in the collection affects TSER's performance, and explore strategies to handle cases where series are largely independent.