---
ver: rpa2
title: "Gl\xF3rIA -- A Generative and Open Large Language Model for Portuguese"
arxiv_id: '2402.12969'
source_url: https://arxiv.org/abs/2402.12969
tags:
- language
- portuguese
- tasks
- text
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Gl\xF3rIA, a new large language model (LLM)\
  \ for European Portuguese. The authors assembled a large and diverse corpus of 35\
  \ billion tokens from various sources, including web crawls, news articles, encyclopedic\
  \ knowledge, and dialogue data."
---

# GlórIA -- A Generative and Open Large Language Model for Portuguese

## Quick Facts
- arXiv ID: 2402.12969
- Source URL: https://arxiv.org/abs/2402.12969
- Reference count: 13
- Key outcome: GlórIA achieves strong performance on Portuguese language modeling and generation tasks, outperforming existing open models on the CALAME-PT benchmark.

## Executive Summary
This paper introduces GlórIA, a new large language model specifically designed for European Portuguese (PT-PT). The authors assembled a comprehensive corpus of 35 billion tokens from diverse PT-PT sources including web crawls, news archives, encyclopedic knowledge, and dialogue data. They pre-trained two decoder-only transformer models (1.3B and 2.7B parameters) using GPT-3-like architecture and introduced CALAME-PT, a new benchmark for evaluating zero-shot language modeling in Portuguese. The results demonstrate that GlórIA significantly outperforms existing open Portuguese models in language modeling tasks and shows strong potential for various downstream applications.

## Method Summary
The authors collected and filtered a large corpus of 35.5 billion PT-PT tokens from multiple sources (web crawls, news archives, encyclopedic knowledge, and dialogue data), cleaned and processed the data, and pre-trained decoder-only transformer models (1.3B and 2.7B parameters) using GPT-3-like architecture with causal language modeling objective and cross-entropy loss. Training used BF-16 precision, cosine annealing scheduler, and a data sampling strategy to balance diverse sources. The models were evaluated on a newly introduced CALAME-PT benchmark for zero-shot language modeling and on discriminative tasks using GLUE-PTPT and ASSIN2 benchmarks.

## Key Results
- GlórIA significantly outperforms existing open Portuguese decoder models on the CALAME-PT zero-shot language modeling benchmark
- The 1.3B parameter model achieves 42.7% exact match accuracy on CALAME-PT, compared to 22.4% for the previous best model
- GlórIA demonstrates competitive performance on Portuguese discriminative tasks compared to encoder-based models when fine-tuned
- The model generates coherent and contextually correct PT-PT text across various domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A large and diverse corpus of 35B+ tokens from multiple PT-PT sources improves language modeling performance.
- Mechanism: Training on rich, domain-diverse data allows the model to capture the nuances of European Portuguese across different contexts, leading to better generalization and generation quality.
- Core assumption: The diversity and scale of the corpus directly contribute to improved modeling of PT-PT linguistic intricacies.
- Evidence anchors:
  - [abstract] "To pre-train GlórIA, we assembled a comprehensive PT-PT text corpus comprising 35 billion tokens from various sources."
  - [section 3.1] "To gather high-quality, large-scale, PT language resources, we resorted to multiple PT-PT text sources, summarized in Table 1."
  - [corpus] The corpus includes web crawls, news articles, encyclopedic knowledge, and dialogue data from various PT-PT sources.
- Break condition: If the corpus lacks diversity or contains low-quality data, the model's performance may not improve as expected.

### Mechanism 2
- Claim: Using a decoder-only transformer architecture similar to GPT-3 enables effective language generation for PT-PT.
- Mechanism: The decoder-only architecture, with its causal language modeling objective, is well-suited for generating coherent and contextually correct text in PT-PT.
- Core assumption: The decoder-only architecture is effective for language generation tasks in PT-PT, similar to its success in English.
- Evidence anchors:
  - [abstract] "GlórIA is a robust European Portuguese decoder LLM."
  - [section 4] "GlórIA is a decoder-based LLM with an architecture similar to GPT-3's (Brown et al., 2020), competing with it in linguistic, physical, and scientific reasoning tasks."
  - [corpus] The model uses a Causal LM pre-training objective with cross-entropy loss.
- Break condition: If the decoder-only architecture is not well-suited for PT-PT language generation, the model's performance may suffer.

### Mechanism 3
- Claim: Introducing a zero-shot language modeling benchmark (CALAME-PT) enables effective evaluation of PT-PT generative models.
- Mechanism: CALAME-PT, a benchmark for guessing the final word given the context, allows for the assessment of a model's language modeling capabilities in a zero-shot setting.
- Core assumption: A benchmark specifically designed for PT-PT language modeling can effectively evaluate the performance of generative models in this language.
- Evidence anchors:
  - [abstract] "Additionally, to evaluate our models' language modeling capabilities, we introduce CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot language-modeling benchmark."
  - [section 5] "We introduce the first zero-shot Portuguese language modeling benchmark, CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese)."
  - [corpus] CALAME-PT comprises 2076 texts and respective last words, covering a wide variety of domains and contexts.
- Break condition: If CALAME-PT does not effectively capture the nuances of PT-PT language modeling, the benchmark may not provide an accurate evaluation of model performance.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: Understanding the transformer architecture is crucial for grasping how GlórIA processes and generates text.
  - Quick check question: What are the key components of a transformer architecture, and how do they contribute to language modeling?

- Concept: Causal language modeling
  - Why needed here: Causal language modeling is the objective function used to train GlórIA, and understanding it is essential for comprehending the model's learning process.
  - Quick check question: How does causal language modeling differ from other language modeling objectives, and why is it suitable for text generation?

- Concept: Zero-shot learning
  - Why needed here: CALAME-PT is a zero-shot benchmark, and understanding zero-shot learning is crucial for evaluating the model's performance.
  - Quick check question: What is zero-shot learning, and how does it differ from few-shot or supervised learning in the context of language modeling?

## Architecture Onboarding

- Component map: Data preprocessing and filtering -> Model architecture configuration -> Pre-training with large-scale PT-PT corpus -> Evaluation using CALAME-PT benchmark

- Critical path: Data preprocessing and filtering → Model architecture configuration → Pre-training with large-scale PT-PT corpus → Evaluation using CALAME-PT benchmark

- Design tradeoffs: Larger model size vs. training efficiency; longer training duration vs. model performance; diverse corpus vs. data quality and consistency

- Failure signatures: Poor performance on CALAME-PT benchmark; inability to generate coherent and contextually correct PT-PT text; overfitting to specific domains or topics in the corpus

- First 3 experiments:
  1. Train GlórIA on a smaller subset of the corpus and evaluate performance on CALAME-PT.
  2. Compare the performance of GlórIA with and without data sampling strategy.
  3. Evaluate the impact of different decoding strategies (greedy vs. beam search + top-k) on CALAME-PT performance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain implicit based on the work presented.

## Limitations
- The specific filtering criteria for identifying PT-PT content in mixed-language corpora are not detailed, making it difficult to assess the true PT-PT nature of the training data
- The CALAME-PT benchmark, while novel, has limited validation - its sensitivity to linguistic nuances and correlation with real-world generation quality remain unclear
- The evaluation on discriminative tasks uses fine-tuning rather than zero-shot or few-shot approaches, making direct comparison with zero-shot GPT-style models less meaningful

## Confidence
- **High confidence**: Claims about corpus assembly methodology and training procedures
- **Medium confidence**: Claims about CALAME-PT benchmark effectiveness and zero-shot language modeling improvements
- **Medium confidence**: Claims about discriminative task performance relative to encoder-based models

## Next Checks
1. Conduct ablation studies on CALAME-PT by varying context lengths and domain distributions to establish benchmark sensitivity
2. Test whether similar improvements can be achieved with encoder-decoder architectures on the same corpus
3. Evaluate model performance on held-out domain-specific PT-PT text to assess generalization beyond the training distribution