---
ver: rpa2
title: Reinforcement Learning for Causal Discovery without Acyclicity Constraints
arxiv_id: '2408.13448'
source_url: https://arxiv.org/abs/2408.13448
tags:
- data
- causal
- learning
- graphs
- alias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ALIAS introduces a reinforcement learning-based method for causal
  discovery that bypasses acyclicity constraints by parameterizing DAGs via a novel
  continuous mapping (Vec2DAG). This mapping directly converts unconstrained vectors
  into valid DAGs, enabling efficient, one-step DAG generation without iterative acyclicity
  enforcement.
---

# Reinforcement Learning for Causal Discovery without Acyclicity Constraints

## Quick Facts
- arXiv ID: 2408.13448
- Source URL: https://arxiv.org/abs/2408.13448
- Reference count: 40
- Primary result: ALIAS achieves near-perfect SHD scores on challenging causal discovery tasks without enforcing acyclicity constraints

## Executive Summary
ALIAS introduces a novel reinforcement learning approach for causal discovery that eliminates the need for explicit acyclicity enforcement. By parameterizing DAGs through a continuous mapping called Vec2DAG, ALIAS directly converts unconstrained vectors into valid DAGs in a single step. The method employs policy gradient techniques to explore the full DAG space effectively, achieving state-of-the-art performance across synthetic and real datasets while scaling quadratically with the number of nodes.

## Method Summary
ALIAS combines a novel continuous DAG parameterization (Vec2DAG) with reinforcement learning to perform causal discovery without explicit acyclicity constraints. The Vec2DAG operator transforms continuous vectors into valid DAGs through a gradient flow mechanism that implicitly enforces acyclicity. A Gaussian policy is trained using policy gradient methods (PPO/A2C) to maximize a scoring function (typically BIC or LS) over the DAG space. The approach avoids the combinatorial complexity of traditional ordering-based methods while maintaining theoretical guarantees for DAG validity.

## Key Results
- Near-perfect structural Hamming distances on dense 30-node graphs (SHD=0.2)
- Exceptional performance on 200-node graphs (SHD=2.0)
- Robustness to nonlinearities, noise misspecification, and data standardization
- Significant improvements over gradient-based and prior RL methods

## Why This Works (Mechanism)

### Mechanism 1
Vec2DAG bypasses acyclicity constraints by directly mapping continuous vectors to valid DAGs without iterative enforcement. The operator combines node potentials and edge potentials using gradient flow to enforce acyclicity implicitly, allowing single-step DAG generation with quadratic complexity. The core assumption is that node potential ordering and edge potential signs together guarantee acyclicity without explicit cycle detection.

### Mechanism 2
Reinforcement learning enables effective exploration of the full DAG space without restricting to orderings. The policy gradient approach with stochastic Gaussian policies explores DAG space directly rather than through sequential ordering generation, enabling parallel exploration. The method assumes policy gradient techniques can navigate the high-dimensional continuous parameter space effectively to find high-scoring DAGs.

### Mechanism 3
The proximity property of Vec2DAG ensures efficient exploration near high-reward DAGs. Lemma 3 shows that representations of different DAGs are close in the continuous space, allowing policy gradients to find nearby high-reward DAGs efficiently. This assumes the continuous parameterization creates a smooth landscape where small parameter changes lead to small DAG changes.

## Foundational Learning

- Concept: Reinforcement learning policy gradient methods
  - Why needed here: Provides the exploration-exploitation framework to navigate the continuous DAG parameter space effectively
  - Quick check question: How does the policy gradient theorem enable direct optimization of the expected reward without needing a value function?

- Concept: Directed acyclic graph structure and acyclicity constraints
  - Why needed here: Understanding why traditional methods struggle with acyclicity enforcement and how Vec2DAG bypasses this challenge
  - Quick check question: Why is maintaining acyclicity combinatorially difficult in traditional DAG learning approaches?

- Concept: Bayesian Information Criterion and other DAG scoring functions
  - Why needed here: The reward function depends on scoring how well each DAG explains the observed data
  - Quick check question: How does BIC balance model fit against complexity to prevent overfitting?

## Architecture Onboarding

- Component map: Gaussian policy -> Vec2DAG operator -> DAG scoring function -> RL algorithm (PPO/A2C)

- Critical path:
  1. Initialize Gaussian policy with random parameters
  2. Sample continuous vectors from policy distribution
  3. Apply Vec2DAG to generate DAGs
  4. Score DAGs using chosen scoring function
  5. Update policy using policy gradient methods
  6. Repeat until convergence or maximum steps

- Design tradeoffs:
  - Vec2DAG provides quadratic complexity but requires careful parameter initialization
  - Policy gradient enables exploration but may need more samples than gradient-based methods
  - Direct DAG generation avoids ordering constraints but loses the structure that orderings provide

- Failure signatures:
  - Policy parameters diverging or becoming unstable
  - Generated DAGs consistently poor quality despite training
  - Exploration failing to find diverse DAGs

- First 3 experiments:
  1. Implement Vec2DAG mapping and verify it produces valid DAGs for random inputs
  2. Test policy gradient updates on a simple scoring function with known optimal solution
  3. Compare DAG generation speed against ordering-based methods on small graphs

## Open Questions the Paper Calls Out

### Open Question 1
How can the sample efficiency of ALIAS be improved for large-scale causal discovery tasks? The paper suggests that more sample-efficient RL approaches, such as Optimistic PPO or reward redesign, could enhance exploration in ALIAS.

### Open Question 2
What are the theoretical convergence guarantees for ALIAS in the limit of infinite data? While the paper proves that ALIAS will recover the true DAG under identifiable causal models, it does not provide specific theoretical guarantees for the RL training process itself.

### Open Question 3
How does ALIAS perform on causal discovery tasks with hidden confounders or feedback loops? The paper only considers a limited number of hidden confounders and does not explore performance on more complex scenarios with many hidden confounders or feedback loops.

## Limitations

- The sample efficiency of the RL-based approach may be limited for very large-scale causal discovery tasks
- Theoretical convergence guarantees for the RL training process itself are not established
- Performance on scenarios with many hidden confounders or feedback loops remains unexplored

## Confidence

- Vec2DAG provides quadratic complexity: High confidence
- Policy gradient methods enable superior exploration: Medium confidence
- Near-perfect SHD scores on challenging scenarios: High confidence
- Robustness to nonlinearities and noise: Medium confidence

## Next Checks

1. **Scalability Verification**: Implement ALIAS on larger graphs (500+ nodes) to verify that the quadratic complexity claim holds in practice and to assess memory and runtime requirements at scale.

2. **Acyclicity Guarantee Testing**: Systematically test the Vec2DAG output on thousands of random inputs to empirically verify that the acyclicity guarantee holds with high probability and to characterize any edge cases.

3. **Comparative Analysis with Ordering-Based RL**: Conduct a direct comparison between ALIAS and a properly tuned ordering-based RL method (like RL-BIC) on identical problems to quantify the claimed exploration advantages and determine whether the gains justify the additional complexity.