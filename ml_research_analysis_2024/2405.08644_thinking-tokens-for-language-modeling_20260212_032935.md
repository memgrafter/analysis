---
ver: rpa2
title: Thinking Tokens for Language Modeling
arxiv_id: '2405.08644'
source_url: https://arxiv.org/abs/2405.08644
tags:
- lstm
- tokens
- thinking
- language
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "thinking tokens" as a way to enhance language
  models' reasoning capabilities on complex tasks like mathematical calculations.
  The authors propose adding special tokens after each word in sentences requiring
  complex reasoning, allowing the model more computational steps to process the problem
  before producing an answer.
---

# Thinking Tokens for Language Modeling

## Quick Facts
- arXiv ID: 2405.08644
- Source URL: https://arxiv.org/abs/2405.08644
- Authors: David Herel; Tomas Mikolov
- Reference count: 30
- Primary result: Perplexity improves from 18 to 13.1 on reasoning-intensive sentences using thinking tokens

## Executive Summary
This paper introduces "thinking tokens" as a mechanism to enhance language models' reasoning capabilities on complex tasks like mathematical calculations. The approach involves adding special tokens after each word in sentences requiring complex reasoning, effectively giving the model additional computational steps to process problems before producing answers. Experiments on various datasets show improved perplexity scores on reasoning-intensive sentences, though the methodology lacks crucial implementation details needed for proper validation.

## Method Summary
The authors propose a simple yet innovative approach where thinking tokens are inserted after each word in sentences requiring complex reasoning. These tokens act as computational placeholders, allowing the model additional processing time to work through mathematical calculations and logical deductions. The method is evaluated on multiple datasets including Penn TreeBank, WikiText-2, economics textbooks, and math problems, with the key hypothesis being that more computational steps lead to better reasoning performance.

## Key Results
- Perplexity drops from 18 to 13.1 on the sentence "What is the remainder when 8922293 is divided by 263? 18" using thinking tokens
- Improved perplexity scores observed on reasoning-intensive sentences across multiple datasets
- Adding more thinking tokens further reduces perplexity but risks context forgetting

## Why This Works (Mechanism)
The thinking token approach works by providing language models with additional computational steps during inference. By inserting explicit tokens that signal "think more about this," the model is encouraged to spend more processing time on complex reasoning tasks rather than rushing to an answer. This is analogous to how humans benefit from taking time to think through difficult problems. The mechanism leverages the transformer architecture's ability to attend to previous tokens, allowing the model to iteratively refine its understanding before committing to a final answer.

## Foundational Learning
- **Language modeling fundamentals**: Understanding how next-token prediction works and how perplexity measures model performance is crucial for grasping why additional computation might help.
  - *Why needed*: To understand what perplexity improvements actually mean and how token-level predictions work
  - *Quick check*: Can you explain why a perplexity of 13.1 is better than 18?

- **Transformer attention mechanisms**: The self-attention mechanism allows models to reference earlier tokens, which is essential for understanding how thinking tokens can help with context retention.
  - *Why needed*: To understand how models can "think" by attending to previous tokens and reasoning steps
  - *Quick check*: Can you describe how self-attention works in a transformer layer?

- **Chain-of-thought reasoning**: The concept of breaking down complex problems into intermediate reasoning steps is foundational to understanding why thinking tokens might help.
  - *Why needed*: To connect thinking tokens with established reasoning enhancement techniques
  - *Quick check*: Can you explain how chain-of-thought prompting differs from thinking tokens?

## Architecture Onboarding
- **Component map**: Input sentence -> Thinking token insertion -> Transformer processing -> Output generation
- **Critical path**: Tokenization → Thinking token insertion → Multi-head attention → Feed-forward layers → Output logits
- **Design tradeoffs**: More thinking tokens improve reasoning but increase computational cost and risk context forgetting; fewer tokens are faster but may not provide enough reasoning steps
- **Failure signatures**: Excessive thinking tokens cause context degradation; insufficient tokens lead to rushed, incorrect answers on complex problems
- **First experiments**: 1) Measure perplexity improvement on math problems with varying numbers of thinking tokens; 2) Compare attention patterns with and without thinking tokens; 3) Test context retention across different thinking token counts

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Missing crucial methodological details including model architecture specifications and training procedures
- Single sentence example (perplexity improvement from 18 to 13.1) appears cherry-picked without broader statistical validation
- Claims about context forgetting lack quantitative evidence or measurement methodology
- No comparison against established reasoning enhancement baselines like chain-of-thought prompting

## Confidence
- Low confidence in quantitative claims: No model specifications, training details, or statistical significance testing provided
- Medium confidence in conceptual validity: The core idea of additional computational steps through explicit tokens is plausible
- Low confidence in practical applicability: Missing ablation studies and computational overhead analysis

## Next Checks
1. Replicate perplexity experiments on Penn TreeBank and WikiText-2 using standardized transformer architectures with identical hyperparameters
2. Conduct controlled experiments measuring context retention across varying thinking token counts using attention visualization
3. Perform ablation studies comparing thinking tokens against chain-of-thought prompting on identical mathematical reasoning datasets