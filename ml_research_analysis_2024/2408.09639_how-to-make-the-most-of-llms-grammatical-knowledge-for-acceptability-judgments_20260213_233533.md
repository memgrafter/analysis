---
ver: rpa2
title: How to Make the Most of LLMs' Grammatical Knowledge for Acceptability Judgments
arxiv_id: '2408.09639'
source_url: https://arxiv.org/abs/2408.09639
tags:
- in-template
- methods
- sentence
- accuracy
- prob
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper compares nine methods for obtaining acceptability judgments\
  \ from large language models (LLMs) using minimal pair benchmarks. It finds that\
  \ two methods\u2014in-template log probability (in-template LP) and Yes/No probability\
  \ computing (Yes/No prob comp)\u2014outperform conventional sentence probability\
  \ readout approaches."
---

# How to Make the Most of LLMs' Grammatical Knowledge for Acceptability Judgments

## Quick Facts
- arXiv ID: 2408.09639
- Source URL: https://arxiv.org/abs/2408.09639
- Reference count: 23
- Primary result: Two methods - in-template log probability and Yes/No probability computing - significantly outperform conventional sentence probability readout approaches for LLM acceptability judgments

## Executive Summary
This paper systematically compares nine methods for obtaining acceptability judgments from large language models (LLMs) using minimal pair benchmarks. The authors find that conventional sentence probability readout approaches are inadequate due to biases and limitations in how LLMs process grammatical information. Two methods - in-template log probability (in-template LP) and Yes/No probability computing (Yes/No prob comp) - consistently outperform conventional approaches across multiple LLMs and benchmarks. These methods not only achieve higher accuracy but also access different aspects of grammatical knowledge, making them complementary. The paper recommends using either method individually or ensembled as more effective alternatives to conventional approaches.

## Method Summary
The study compares nine methods for obtaining acceptability judgments from LLMs using minimal pair benchmarks. Eight state-of-the-art LLMs (Llama-3-70B, Mixtral-8x7B-v0.1, Qwen2-57B-A14B, Yi-1.5-34B and their instruct/chat variants) were evaluated on two minimal pair benchmarks: BLiMP (English) and CLiMP (Chinese). The nine methods include three sentence probability readout methods (LP, MeanLP, PenLP), three in-template probability readout methods (in-template LP, in-template MeanLP, in-template PenLP), one in-template comparative LP, A/B prompting, and Yes/No probability computing. Experiments were conducted in a zero-shot setting using 4-bit quantized models.

## Key Results
- In-template LP consistently outperforms conventional methods, achieving the highest accuracies on the Chinese benchmark
- Yes/No prob comp achieves the highest accuracies on the English benchmark in all but one setting
- Ensembling in-template LP and Yes/No prob comp improves accuracy by 1.6 percentage points over human performance on the English benchmark
- The two methods excel in different linguistic phenomena, accessing different aspects of LLMs' grammatical knowledge

## Why This Works (Mechanism)

### Mechanism 1: In-template LP Effectiveness
In-template LP improves acceptability judgments by embedding the target sentence in a task-specific context that guides the model to focus on grammaticality rather than raw probability. The template frames the sentence as a test of grammatical correctness, providing implicit instruction that activates grammatical knowledge rather than statistical fluency.

### Mechanism 2: Yes/No Probability Computing Robustness
Yes/No prob comp is robust against token-length bias because it normalizes probability across mutually exclusive response options rather than comparing raw sentence probabilities. By computing P("Yes") / (P("Yes") + P("No")) for each sentence, the method eliminates the confounding effect of sentence length on probability estimates.

### Mechanism 3: Complementary Strengths Through Ensembling
Ensembling in-template LP and Yes/No prob comp leverages their complementary strengths, with each method excelling at different linguistic phenomena. In-template LP excels at phenomena like Ellipsis and Quantifiers while Yes/No prob comp excels at Subject-verb agreement and Binding, accessing orthogonal dimensions of grammatical knowledge.

## Foundational Learning

- **Minimal pair benchmarks**: Controlled comparisons where the only difference between sentence pairs is a specific grammatical feature, allowing precise measurement of grammatical knowledge
  - Why needed: Provides controlled environment to isolate grammatical knowledge from other factors
  - Quick check: What makes minimal pairs superior to single-sentence acceptability judgments for measuring grammatical knowledge?

- **Token-length bias in language models**: Longer sentences typically receive lower probability scores from language models due to multiplicative probability calculations
  - Why needed: Understanding this bias is crucial for interpreting why raw probability comparisons fail
  - Quick check: Why do longer sentences typically receive lower probability scores from language models?

- **Instruction-tuning and task framing**: LLMs respond to contextual framing cues that activate grammatical knowledge rather than statistical fluency
  - Why needed: The effectiveness of in-template LP relies on LLMs responding to task-specific prompts
  - Quick check: How does instruction-tuning enable LLMs to perform tasks without task-specific fine-tuning?

## Architecture Onboarding

- **Component map**: Input processing (Template insertion) → Model inference (4-bit quantized LLM forward pass) → Probability extraction (Log probability readout) → Score computation (LP/MeanLP/PenLP/Yes/No normalization) → Judgment logic (Comparison/ensembling)
- **Critical path**: Template → LLM inference → Probability extraction → Score computation → Judgment decision
- **Design tradeoffs**: Template complexity vs. model performance; single method simplicity vs. ensemble accuracy; computational cost of multiple inferences vs. accuracy gains
- **Failure signatures**: Low accuracy on word-shuffling paradigms; systematic preference for one choice in A/B prompting; high correlation between in-template LP and Yes/No prob comp errors
- **First 3 experiments**: 
  1. Compare LP vs. in-template LP on a small subset of BLiMP to verify template effectiveness
  2. Test Yes/No prob comp on sentences with varying lengths to confirm bias robustness
  3. Ensemble the two methods on a mixed-phenomenon subset to observe complementary behavior

## Open Questions the Paper Calls Out

- **Open Question 1**: Why do word-shuffling paradigms remain challenging for LLMs despite our best-performing methods?
  - Basis: The paper demonstrates that word-shuffling paradigms consistently yield much lower accuracy across all methods and models compared to non-word-shuffling paradigms
  - Why unresolved: The paper hypothesizes that LLMs are insensitive to word order but this remains a hypothesis rather than confirmed explanation
  - What evidence would resolve it: Systematic experiments testing whether pre-trained models specifically trained to preserve word order perform better on word-shuffling paradigms

- **Open Question 2**: What causes in-template LP and Yes/No prob comp to excel in different linguistic phenomena?
  - Basis: The paper shows that the two methods have complementary strengths across different linguistic phenomena
  - Why unresolved: The paper explicitly states the underlying cause is unexplained despite hypothesizing about token-length differences
  - What evidence would resolve it: Analysis of internal representations or attention patterns of the two methods across different linguistic phenomena

- **Open Question 3**: Would incorporating few-shot examples into in-template LP and Yes/No prob comp further improve accuracy?
  - Basis: The paper mentions that experiments were conducted in a zero-shot setting, suggesting potential for improvement
  - Why unresolved: The paper explicitly states this was beyond their scope but would be worth investigating
  - What evidence would resolve it: Controlled experiments comparing zero-shot versus few-shot performance for both methods across multiple linguistic phenomena

## Limitations
- Findings rely heavily on controlled minimal pair benchmarks that may not generalize to naturalistic language use
- Computational cost of ensembling methods may not be practical for all applications despite modest accuracy gains
- Effectiveness of template-based methods could vary significantly across different LLM architectures

## Confidence
- **High Confidence**: The comparative performance of in-template LP and Yes/No prob comp across multiple benchmarks and models
- **Medium Confidence**: The mechanism explanations for why these methods work better, particularly claims about order insensitivity and bias robustness
- **Low Confidence**: The generalization of findings to non-minimal pair contexts and practical significance of modest accuracy improvements from ensembling

## Next Checks
1. **Cross-architecture validation**: Test the two recommended methods on a diverse set of LLM architectures beyond the 4-bit quantized models used in the study
2. **Out-of-distribution testing**: Evaluate the methods on acceptability judgments involving multiple interacting grammatical phenomena simultaneously
3. **Efficiency analysis**: Conduct a detailed cost-benefit analysis of the ensembling approach measuring both accuracy improvements and computational overhead