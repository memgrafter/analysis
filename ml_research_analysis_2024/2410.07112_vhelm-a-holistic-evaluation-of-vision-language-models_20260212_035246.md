---
ver: rpa2
title: 'VHELM: A Holistic Evaluation of Vision Language Models'
arxiv_id: '2410.07112'
source_url: https://arxiv.org/abs/2410.07112
tags:
- image
- images
- visual
- language
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VHELM, a holistic evaluation framework for
  vision-language models (VLMs) that assesses 9 critical aspects: visual perception,
  knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and
  safety. The authors standardized evaluation procedures and automated metrics across
  21 existing datasets to enable fair comparisons among 22 prominent VLMs.'
---

# VHELM: A Holistic Evaluation of Vision Language Models

## Quick Facts
- arXiv ID: 2410.07112
- Source URL: https://arxiv.org/abs/2410.07112
- Reference count: 40
- Key outcome: No model excels across all aspects - GPT-4o (0513) performs best overall but has weaknesses in bias and safety

## Executive Summary
This paper introduces VHELM, a holistic evaluation framework for vision-language models (VLMs) that assesses 9 critical aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. The authors standardized evaluation procedures and automated metrics across 21 existing datasets to enable fair comparisons among 22 prominent VLMs. Their key findings include significant performance gaps between closed-API and open-weight models, efficiency-focused models performing notably worse on bias benchmarks, and most models struggling with multilingual tasks and toxicity detection.

## Method Summary
VHELM aggregates 21 existing VLM benchmark datasets and maps them to 9 critical evaluation aspects. The framework standardizes evaluation by using zero-shot prompting as the sole adaptation strategy across all models, ensuring fair comparison. Automated metrics including Prometheus-Vision for similarity scoring and exact match for multiple-choice questions replace expensive human annotation. The evaluation covers 22 prominent VLMs including both closed-API models and open-weight models, providing comprehensive multi-dimensional assessment of model capabilities.

## Key Results
- No model excels across all aspects - GPT-4o (0513) performs best overall but has weaknesses in bias and safety
- Closed-API models significantly outperform open-weight ones, likely due to instruction-following limitations
- Efficiency-focused models perform notably worse on bias benchmarks (e.g., Claude 3 Haiku 8% vs Claude 3 Opus 58.7%)
- Most models struggle with multilingual tasks (up to 33.7% performance drop) and toxicity detection (best model 62.2% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Standardizing evaluation procedures across diverse datasets enables fair model comparison
- **Mechanism**: The paper implements consistent adaptation strategies (zero-shot prompting) and automated metrics across all 21 datasets, eliminating protocol variability that previously prevented direct model comparisons
- **Core assumption**: Zero-shot prompting represents the most common real-world usage pattern and is sufficient to reveal meaningful performance differences
- **Evidence anchors**:
  - [abstract] "we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models"
  - [section 3] "An adaptation is a specific procedure for invoking a model. Adaptation strategies include zero-shot prompting, k-shots prompting, and chain-of-thought prompting. In this study, we use only zero-shot prompting as it is the most common strategy used by the layperson."
  - [corpus] Weak evidence - corpus contains related work on VLM evaluation but no direct confirmation of standardization effectiveness
- **Break condition**: If zero-shot prompting proves insufficient for capturing model capabilities, or if different models require fundamentally different prompting approaches to perform optimally

### Mechanism 2
- **Claim**: Aggregating multiple datasets covering diverse aspects provides a comprehensive model assessment
- **Mechanism**: By mapping 21 existing datasets to 9 critical aspects (visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety), the framework captures multidimensional model capabilities that single-aspect benchmarks miss
- **Core assumption**: Existing datasets can be meaningfully combined and mapped to coherent aspects without losing diagnostic value
- **Evidence anchors**:
  - [abstract] "VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety"
  - [section 3.1] Detailed mapping of each dataset to specific aspects with examples
  - [corpus] Weak evidence - corpus contains related work but no validation that this particular aggregation approach captures comprehensive model capabilities
- **Break condition**: If the mapping between datasets and aspects becomes too loose or if dataset quality varies significantly across aspects

### Mechanism 3
- **Claim**: Automated metrics enable efficient, consistent evaluation while maintaining sufficient diagnostic power
- **Mechanism**: The framework uses Prometheus-Vision for similarity scoring and exact match for multiple-choice questions, avoiding expensive human annotation while maintaining evaluation quality
- **Core assumption**: Automated metrics correlate sufficiently with human judgment for the evaluation purposes
- **Evidence anchors**:
  - [abstract] "we eschew metrics that require manual annotation by humans and adopt automated metrics for VHELM so that evaluation runs are low-cost, fast, and consistent"
  - [section 3.2] "Prometheus-Vision is a VLM that judges the similarity between the prediction and the ground truth on a scale of 1 (bad) to 5 (good) and has been shown to emulate human evaluators"
  - [corpus] Weak evidence - corpus contains related work but no independent validation of Prometheus-Vision's correlation with human judgment
- **Break condition**: If automated metrics fail to capture important qualitative differences or if they introduce systematic biases

## Foundational Learning

- **Concept: Multi-aspect model evaluation**
  - Why needed here: Single-dimension benchmarks miss critical capabilities like bias, fairness, and safety that are essential for real-world deployment
  - Quick check question: Can a model that excels at visual perception but fails at toxicity detection be considered truly capable?

- **Concept: Dataset mapping and standardization**
  - Why needed here: Different datasets use different formats, metrics, and protocols, making direct comparison impossible without systematic standardization
  - Quick check question: How do you ensure that a multiple-choice question format preserves the diagnostic value of the original open-ended question?

- **Concept: Automated evaluation metrics**
  - Why needed here: Human evaluation is too slow and expensive for comprehensive benchmarking across many models and datasets
  - Quick check question: What correlation threshold should automated metrics meet to be considered acceptable substitutes for human judgment?

## Architecture Onboarding

- **Component map**: Dataset ingestion pipeline → prompt standardization module → model execution layer → automated evaluation module → result aggregation and visualization
- **Critical path**: Dataset ingestion → prompt standardization → model execution → metric calculation → result aggregation
- **Design tradeoffs**: Standardized zero-shot prompting sacrifices potential performance gains from model-specific optimization to ensure fair comparison across models
- **Failure signatures**: Inconsistent results across similar datasets, high variance in automated metric scores, or poor correlation with known human judgments
- **First 3 experiments**:
  1. Run a single model across all datasets with standardized prompts to verify consistency and identify any systematic failures
  2. Compare automated metric scores with a small set of human-annotated samples to validate metric quality
  3. Test the prompt standardization process on a diverse set of model types to ensure compatibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training differences cause open-weight VLMs to fail at following instructions, despite achieving comparable performance on other aspects?
- Basis in paper: Inferred from finding that "Open-weight models struggle to follow instructions" and "they may ignore the command to output only a single option or number as the answer and instead produce long sentences"
- Why unresolved: The paper identifies this as a key limitation but doesn't investigate the underlying technical causes. It's unclear whether this stems from pretraining data distribution, fine-tuning methodology, architecture choices, or evaluation protocols.
- What evidence would resolve it: Comparative analysis of instruction-following capabilities across VLMs with controlled architectural differences, or ablation studies varying instruction-tuning procedures on open-weight models.

### Open Question 2
- Question: How do efficiency-focused models (like Claude 3 Haiku and Gemini 1.5 Flash) achieve their speed advantages while maintaining performance on most aspects, and can this efficiency be transferred to larger models?
- Basis in paper: Explicit finding that "efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects"
- Why unresolved: The paper notes the performance gap but doesn't investigate the architectural or algorithmic optimizations that enable efficiency-focused models to perform well on most aspects while lagging specifically on bias detection.
- What evidence would resolve it: Technical analysis of the model architectures, training procedures, and optimization techniques used in efficiency-focused models, and experiments testing whether these optimizations can be applied to larger models without degrading performance.

### Open Question 3
- Question: What specific cultural and linguistic factors contribute to the performance drop in multilingual tasks, and can these be addressed through targeted training or architectural modifications?
- Basis in paper: Finding that "Most models don't perform as well when prompted in another language other than English" with up to 33.7% performance drop, and observation that performance varies by language (Spanish > Chinese > Hindi > Swahili)
- Why unresolved: The paper quantifies the multilingual performance gap but doesn't investigate whether it stems from training data distribution, language model architecture limitations, or cultural knowledge gaps specific to different languages.
- What evidence would resolve it: Detailed analysis of training data composition across languages, experiments testing the impact of language-specific fine-tuning, or architectural modifications designed to better handle multilingual inputs.

## Limitations

- The study's findings rely heavily on zero-shot prompting as the evaluation strategy, which may systematically disadvantage open-weight models
- Prometheus-Vision similarity metric has not been independently verified across all dataset types used in VHELM
- The performance gap between API-based and open-weight models could reflect evaluation methodology limitations rather than inherent model quality differences

## Confidence

- **High confidence**: Overall finding that no model excels across all aspects (confirmed by consistent results across multiple independent datasets)
- **Medium confidence**: Performance differences between closed-API and open-weight models (methodological limitations may contribute to this gap)
- **Medium confidence**: Efficiency model performance drops on bias benchmarks (though results are consistent, the mechanism explanation remains speculative)

## Next Checks

1. **Cross-evaluation with human judgment**: Validate Prometheus-Vision scores against human annotators on a stratified sample of predictions across all 9 aspects to quantify correlation strength and identify systematic biases
2. **Prompt optimization comparison**: Run a controlled experiment comparing zero-shot prompting against minimal few-shot prompting on a subset of open-weight models to quantify the true performance gap independent of prompting methodology
3. **Multilingual task verification**: Manually audit a random sample of translated prompts and model responses across different language pairs to verify semantic alignment and rule out translation-induced performance degradation