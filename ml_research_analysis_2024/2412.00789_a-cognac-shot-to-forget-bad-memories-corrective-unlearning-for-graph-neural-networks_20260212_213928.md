---
ver: rpa2
title: 'A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural
  Networks'
arxiv_id: '2412.00789'
source_url: https://arxiv.org/abs/2412.00789
tags:
- unlearning
- graph
- nodes
- cognac
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles corrective unlearning for Graph Neural Networks
  (GNNs) in the presence of adversarial manipulations. Existing GNN unlearning methods
  fail even when the full set of manipulated entities is known.
---

# A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural Networks

## Quick Facts
- arXiv ID: 2412.00789
- Source URL: https://arxiv.org/abs/2412.00789
- Reference count: 40
- Primary result: Cognac achieves strong GNN unlearning performance even when only 5% of manipulated nodes are identified

## Executive Summary
This paper addresses a critical gap in GNN unlearning by proposing Cognac, a method that can correct the adverse effects of adversarial manipulations even when the full set of manipulated entities is unknown. Existing GNN unlearning methods fail in the presence of adversarial attacks, but Cognac consistently outperforms them by alternating between contrastive learning (CoGN) and gradient-based correction (AC DC). The method achieves strong performance by pushing affected neighbors away from manipulated nodes while pulling them toward unaffected neighbors, combined with gradient ascent on the deletion set and descent on remaining data.

## Method Summary
Cognac is a corrective unlearning method for GNNs that alternates between two components: CoGN and AC DC. CoGN uses contrastive learning to push affected neighbors away from manipulated nodes while keeping them close to unaffected neighbors, mitigating the propagation of adversarial influence through message passing. AC DC applies gradient ascent on the deletion set and gradient descent on the remaining data to undo incorrect label learning. The method first identifies affected nodes using a feature inversion heuristic that exploits the n-hop neighborhood locality of GNNs, then alternates between the two components for fixed epochs to achieve unlearning.

## Key Results
- Cognac consistently outperforms existing GNN unlearning methods and even retraining from scratch
- Achieves strong unlearning performance with as little as 5% of the manipulated set identified
- Scales well to large datasets like OGB-Arxiv while recovering most performance of an oracle model
- Demonstrates effectiveness across multiple datasets and attack types

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contrastive learning (CoGN) removes the influence of manipulated nodes on their neighbors by pushing affected neighbors away from deletion set nodes while keeping them close to unaffected neighbors.
- **Mechanism**: CoGN uses a log-based contrastive loss with sigmoid terms to optimize a GNN such that representations of affected neighbors (Vaff) are pushed away from manipulated nodes (Vf) while staying close to their unaffected neighbors.
- **Core assumption**: The homophily assumption holds in the graph; that is, nodes connected to manipulated nodes are more likely to be affected and should be corrected through contrastive repulsion.
- **Evidence anchors**:
  - [abstract]: "CoGN, which uses contrastive learning to push affected neighbors away from manipulated nodes while staying close to unaffected neighbors"
  - [section]: "Equation 1 offers a direct way to enforce separation between positive and negative pairs, but it has notable shortcomings... To overcome these limitations, we use a log-based loss function that applies non-linear sigmoid terms"
  - [corpus]: No direct corpus support for this contrastive mechanism; inferred from the paper's description.

### Mechanism 2
- **Claim**: AC DC removes incorrect label learning by applying gradient ascent on the deletion set and gradient descent on the retain set.
- **Mechanism**: AC DC explicitly undoes the effect of task loss on manipulated nodes by maximizing loss on the deletion set (gradient ascent) and minimizing loss on the rest (gradient descent), using separate optimizers with different learning rates to balance the two steps.
- **Core assumption**: The manipulated set is small relative to the total training data, so ascent on the representative subset (Sf) can counteract the wrong label learning without overwhelming the descent step.
- **Evidence anchors**:
  - [abstract]: "AC DC, which applies gradient ascent on the deletion set and descent on the remaining data"
  - [section]: "This requires a careful balance between ascent and descent, which we can achieve by using two different optimizers and starting learning rates for these steps"
  - [corpus]: No direct corpus support; inferred from the paper's description of AC DC.

### Mechanism 3
- **Claim**: Affected Node Identification reduces the search space for unlearning by leveraging the n-hop neighborhood locality of GNNs.
- **Mechanism**: Cognac identifies affected nodes by inverting features of known manipulated nodes, running forward passes, and selecting top-k nodes with largest logit changes. This heuristic exploits the fact that manipulations only propagate within n-hop neighborhoods.
- **Core assumption**: The n-hop neighborhood contains most nodes significantly affected by the manipulation, and feature inversion is a reliable proxy for identifying them.
- **Evidence anchors**:
  - [abstract]: "It recovers most of the performance of a strong oracle with fully corrected training data, even beating retraining from scratch"
  - [section]: "Lemma 3.2... For a node s ∈ V, let N^n(s) denote the n-hop neighborhood of s... In an n-layer GNN, the representation zs of node s can affect the representations zv of nodes v only if v ∈ N^n(s)"
  - [corpus]: No direct corpus support; inferred from the paper's Lemma 3.2 and affected node identification description.

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs) and message passing
  - **Why needed here**: Cognac operates on GNNs and relies on understanding how node representations are updated via aggregation from neighbors; this is fundamental to both the attack and unlearning mechanisms.
  - **Quick check question**: In a 2-layer GNN, which nodes can influence the representation of a given node v?
    - Answer: Nodes within 2 hops of v (i.e., v itself, its 1-hop neighbors, and their neighbors).

- **Concept**: Homophily assumption in graphs
  - **Why needed here**: Cognac's contrastive component assumes nodes with similar features/labels are connected; this guides which neighbors to pull toward during unlearning.
  - **Quick check question**: What happens to contrastive unlearning performance if the graph is heterophilic?
    - Answer: It may misalign unaffected neighbors and degrade utility because the pull toward "similar" neighbors may be wrong.

- **Concept**: Influence functions and data attribution
  - **Why needed here**: Understanding how individual data points influence model parameters helps reason about why certain unlearning methods work or fail.
  - **Quick check question**: Why might a method that only unlinks edges during inference (like UtU) fail to fully unlearn?
    - Answer: Because it doesn't counteract the influence already baked into node representations and model weights.

## Architecture Onboarding

- **Component map**: Trained GNN M -> Affected node identification -> CoGN (contrastive loss) <-> AC DC (gradient ascent/descent) -> Unlearned GNN M*

- **Critical path**:
  1. Identify affected nodes using feature inversion heuristic
  2. Sample positive/negative pairs for contrastive learning
  3. Alternate CoGN (update embeddings) and AC DC (update weights) for fixed epochs
  4. Return updated model

- **Design tradeoffs**:
  - Contrastive loss vs. simple margin loss: log-based with sigmoid terms provide stronger penalization for small margins and smoother gradients.
  - Separate optimizers for ascent/descent: Allows tuning balance but adds hyperparameter complexity.
  - k% top affected nodes vs. full n-hop neighborhood: Faster but may miss some affected nodes.

- **Failure signatures**:
  - High Accrem but low Accaff: Unlearning is not targeting the right nodes or the contrastive component is too weak.
  - Low Accrem and low Accaff: Over-aggressive unlearning erasing useful knowledge.
  - No improvement over original: Components not properly tuned or graph lacks homophily.

- **First 3 experiments**:
  1. Run CoGN alone on a small synthetic graph with known manipulation to verify it pushes affected neighbors away from deletion set.
  2. Run AC DC alone on the same graph to verify it can undo wrong label learning without contrastive guidance.
  3. Combine CoGN and AC DC on a slightly larger graph, measure Accaff and Accrem, and compare to baseline unlearning methods.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does Cognac perform on heterophilic datasets where the homophily assumption is violated?
- **Basis in paper**: [inferred] The authors acknowledge that their method relies on the homophily assumption and is evaluated primarily on homophilic datasets like previous works.
- **Why unresolved**: The paper does not provide empirical results on heterophilic datasets, leaving the method's effectiveness in such settings unexplored.
- **What evidence would resolve it**: Testing Cognac on a variety of heterophilic datasets and comparing its performance to that on homophilic datasets would clarify its robustness and limitations.

### Open Question 2
- **Question**: Can more sophisticated influence functions improve the identification of affected nodes beyond the current heuristic?
- **Basis in paper**: [explicit] The authors mention that more sophisticated influence functions, such as those presented in (Chen et al., 2023), could be used to potentially improve performance in affected node identification.
- **Why unresolved**: The current heuristic is described as cheap and simple, but its effectiveness is not compared against more complex methods.
- **What evidence would resolve it**: Comparing the performance of Cognac using the current heuristic against using advanced influence functions on the same datasets would quantify the potential gains.

### Open Question 3
- **Question**: What is the theoretical limit of unlearning performance when only a fraction of the manipulation set is known?
- **Basis in paper**: [explicit] The authors state that Cognac achieves strong unlearning with as little as 5% of the manipulation set known, but they do not provide a theoretical upper bound on what can be achieved with partial information.
- **Why unresolved**: The paper focuses on empirical results and does not explore the theoretical limits of corrective unlearning with incomplete information.
- **What evidence would resolve it**: Developing theoretical bounds on unlearning performance as a function of the fraction of known manipulated data would provide insight into the method's potential and limitations.

## Limitations

- **Graph homophily dependency**: Cognac's contrastive mechanism assumes the graph exhibits homophily, which may not hold in heterophilic graphs or certain real-world networks.
- **Affected node identification reliability**: The feature inversion heuristic may miss nodes affected by sophisticated attacks or fail in low-dimensional feature spaces where logit changes are noisy.
- **Hyperparameter sensitivity**: The balance between CoGN and AC DC components requires careful tuning of separate learning rates, and the paper lacks extensive sensitivity analysis.

## Confidence

- **High confidence**: The core mechanism of alternating contrastive learning (CoGN) and gradient-based correction (AC DC) is theoretically sound and addresses a real gap in GNN unlearning.
- **Medium confidence**: The affected node identification method using feature inversion is practical but may not be universally reliable across all graph types and attack strategies.
- **Medium confidence**: The claim that Cognac "consistently outperforms" existing methods and "even beats retraining from scratch" is supported by reported results but lacks detailed statistical analysis.

## Next Checks

1. **Heterophilic graph validation**: Test Cognac on a heterophilic graph dataset (e.g., Texas, Cornell, or Wisconsin) to verify the contrastive component doesn't degrade performance when the homophily assumption is violated.

2. **Ablation study on affected node identification**: Compare Cognac's performance with and without the feature inversion heuristic to quantify the contribution of accurate affected node identification to overall unlearning success.

3. **Scalability stress test**: Evaluate Cognac on progressively larger graphs (beyond OGB-Arxiv) and measure both accuracy retention and computational overhead to verify the claimed scalability holds in practice.