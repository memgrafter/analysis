---
ver: rpa2
title: Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced
  Optimal Transport
arxiv_id: '2402.05443'
source_url: https://arxiv.org/abs/2402.05443
tags:
- generative
- transport
- training
- gradient
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a scalable Wasserstein gradient flow (WGF)
  generative model that addresses the quadratic training complexity limitation of
  previous JKO-based methods. The key idea is to reformulate the JKO step as an unbalanced
  optimal transport (UOT) problem and introduce a reparameterization trick that reduces
  the training complexity from O(K^2) to O(K), where K is the number of JKO steps.
---

# Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport

## Quick Facts
- arXiv ID: 2402.05443
- Source URL: https://arxiv.org/abs/2402.05443
- Reference count: 40
- One-line result: Achieves FID scores of 2.62 on CIFAR-10 and 5.46 on CelebA-HQ-256, significantly outperforming previous WGF-based generative models

## Executive Summary
This paper introduces a scalable Wasserstein gradient flow (WGF) generative model that addresses the quadratic training complexity limitation of previous JKO-based methods. The key innovation is reformulating the JKO step as an unbalanced optimal transport (UOT) problem and introducing a reparameterization trick that reduces training complexity from O(K²) to O(K). The proposed Semi-dual JKO (S-JKO) model achieves competitive performance on image datasets while demonstrating improved scalability compared to existing WGF-based approaches.

## Method Summary
The paper presents a generative modeling approach based on Wasserstein gradient flows (WGF) using the Jordan-Kinderlehrer-Otto (JKO) scheme. The key innovation is reformulating the JKO step as an unbalanced optimal transport (UOT) problem and introducing a reparameterization trick that reduces training complexity from O(K²) to O(K). The model uses a semi-dual formulation of the JKO step, enabling adversarial learning with a discriminator network. The transport network maps from source to intermediate distributions using a reparameterized formulation, while the discriminator distinguishes between transported and target distributions. The approach is evaluated on synthetic datasets and image datasets (CIFAR-10 and CelebA-HQ) with competitive FID scores.

## Key Results
- S-JKO achieves FID scores of 2.62 on CIFAR-10 and 5.46 on CelebA-HQ-256
- Outperforms existing WGF-based generative models significantly
- Demonstrates improved scalability with O(K) training complexity
- Successfully generates high-quality images from complex multi-modal distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The JKO step is equivalent to a Source-fixed Unbalanced Optimal Transport (UOT) problem.
- Mechanism: By expressing the JKO step's objective function in terms of the Wasserstein distance, it can be reformulated as minimizing transport cost plus a divergence term, matching the UOT formulation.
- Core assumption: The functional F(ρ) in the JKO step can be expressed as a f-divergence with respect to the target distribution ν.
- Evidence anchors:
  - [abstract] "Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport."
  - [section] "By configuring φ1 = ι and φ2 = f in Eq 22, it boils down to Eq 27. Thus, by applying Lemma A.1, the semi-dual of Eq 27 is written as follows..."
- Break condition: If the functional F(ρ) cannot be expressed as a f-divergence, or if the equivalence between JKO and UOT does not hold for other functionals.

### Mechanism 2
- Claim: The reparameterization trick reduces training complexity from O(K²) to O(K).
- Mechanism: Instead of parametrizing each transport map Tk separately, a single transport map Tθ is used that directly maps from the source distribution to each intermediate distribution, eliminating the need for sequential inference through k networks.
- Core assumption: The transport map Tk can be decomposed as a composition of simpler transport maps ∆T0, ..., ∆Tk.
- Evidence anchors:
  - [abstract] "Our approach reduces the training complexity to O(K)."
  - [section] "For each phase k, we possess a direct transport map Tk−1 that connects µ to µk. In other words, for each phase, sampling from µk does not require simulating the entire trajectory."
- Break condition: If the decomposition of Tk is not valid or if the reparameterization introduces significant approximation errors.

### Mechanism 3
- Claim: The semi-dual form of the JKO step enables an adversarial learning objective.
- Mechanism: By expressing the JKO step in its semi-dual form, it can be optimized using a discriminator network vϕ that distinguishes between the transported source distribution and the target distribution.
- Core assumption: The semi-dual form of the UOT problem is equivalent to the semi-dual form of the JKO step when the source-fixed variant is used.
- Evidence anchors:
  - [abstract] "Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport."
  - [section] "The semi-dual form of JKO step is obtained from its UOT interpretation... By parametrizing ∆Tk as follows... Then, ∆Tk satisfies the following..."
- Break condition: If the semi-dual formulation does not accurately capture the JKO dynamics or if the adversarial training is unstable.

## Foundational Learning

- Concept: Unbalanced Optimal Transport (UOT)
  - Why needed here: The JKO step is shown to be equivalent to a Source-fixed UOT problem, allowing the use of UOT techniques.
  - Quick check question: What is the main difference between classical Optimal Transport and Unbalanced Optimal Transport?

- Concept: Wasserstein Gradient Flow (WGF)
  - Why needed here: WGF provides the theoretical foundation for the JKO scheme and describes the dynamics of probability densities in the Wasserstein space.
  - Quick check question: How does the JKO scheme approximate the continuous WGF?

- Concept: Semi-dual formulation
  - Why needed here: The semi-dual form of the JKO step enables the adversarial learning objective and reduces computational complexity.
  - Quick check question: What is the relationship between the primal, dual, and semi-dual forms of optimization problems?

## Architecture Onboarding

- Component map:
  - Transport network Tθ: Maps from source distribution to intermediate distributions
  - Discriminator network vϕ: Distinguishes between transported source and target distributions
  - Loss functions: Lv for discriminator, LT for transport network

- Critical path:
  1. Initialize Told = Id
  2. For each JKO step k:
     a. Sample batches from source and target distributions
     b. Update discriminator vϕ using loss Lv
     c. Update transport network Tθ using loss LT
     d. Update Told ← Tθ

- Design tradeoffs:
  - Number of JKO steps K vs. computational cost
  - Step size h vs. convergence speed and stability
  - Choice of f-divergence (KL vs. JSD) vs. performance

- Failure signatures:
  - Mode collapse: Generated distribution concentrated around a subset of modes
  - Training instability: Oscillating or diverging losses
  - Poor scalability: High computational cost or memory usage

- First 3 experiments:
  1. Verify equivalence between JKO step and Source-fixed UOT on a simple 2D dataset
  2. Test the reparameterization trick on a small image dataset (e.g., MNIST) to confirm O(K) complexity
  3. Evaluate the impact of different f-divergences (KL vs. JSD) on a benchmark dataset (e.g., CIFAR-10)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of S-JKO scale with the dimensionality of the data distribution ν, particularly in extremely high-dimensional spaces?
- Basis in paper: [inferred] The paper demonstrates S-JKO's effectiveness on CIFAR-10 (32x32) and CelebA-HQ (256x256), but does not explore higher dimensional datasets. The paper mentions the O(K) training complexity but doesn't explicitly analyze how this scales with data dimensionality.
- Why unresolved: The paper focuses on image datasets with moderate dimensions (32x32 and 256x256) and does not provide experiments or analysis for significantly higher dimensional data distributions.
- What evidence would resolve it: Experiments evaluating S-JKO on higher dimensional datasets (e.g., 512x512 images, video data, or other high-dimensional data) with quantitative performance metrics and training time comparisons.

### Open Question 2
- Question: What is the theoretical convergence rate of S-JKO to the true Wasserstein gradient flow solution as the number of JKO steps K approaches infinity?
- Basis in paper: [explicit] The paper mentions that S-JKO approximates the Wasserstein gradient flow through the JKO scheme but does not provide theoretical analysis of the convergence rate or error bounds.
- Why unresolved: The paper focuses on empirical performance and scalability rather than providing theoretical guarantees about convergence to the true Wasserstein gradient flow.
- What evidence would resolve it: A mathematical proof establishing the convergence rate of S-JKO to the true Wasserstein gradient flow solution, including error bounds as a function of K and step size h.

### Open Question 3
- Question: How does the choice of f-divergence (KLD vs JSD) impact the quality and diversity of generated samples in S-JKO?
- Basis in paper: [explicit] The paper compares KLD and JSD functionals and mentions that S-JKO-JSD is more robust to larger h, but doesn't provide a comprehensive analysis of their impact on sample quality and diversity.
- Why unresolved: While the paper presents some ablation studies on different f-divergences, it doesn't thoroughly investigate how these choices affect the quality and diversity of generated samples beyond FID scores.
- What evidence would resolve it: A comprehensive study comparing KLD and JSD using multiple metrics including FID, Inception Score, precision-recall curves, and human perceptual studies to evaluate both quality and diversity of generated samples.

### Open Question 4
- Question: What is the impact of the reparametrization trick on the stability and convergence of S-JKO compared to other JKO-based models?
- Basis in paper: [explicit] The paper introduces the reparametrization trick as a key innovation that reduces training complexity and improves scalability, but doesn't provide a detailed analysis of its impact on training stability.
- Why unresolved: While the paper mentions that the reparametrization trick contributes to stable training, it doesn't provide quantitative evidence or comparison with other JKO-based models to demonstrate its specific benefits for stability and convergence.
- What evidence would resolve it: A controlled experiment comparing S-JKO with and without the reparametrization trick, measuring training stability metrics (e.g., gradient norms, loss curves), and analyzing convergence behavior across different datasets and hyperparameters.

## Limitations

- Theoretical guarantees for the semi-dual approach in high-dimensional spaces remain incomplete
- The reparameterization trick's effectiveness depends critically on decomposition assumptions with limited theoretical justification
- Empirical performance comparisons could be more comprehensive against broader range of generative models

## Confidence

- **High confidence**: The computational complexity reduction from O(K²) to O(K) via reparameterization is mathematically sound and clearly demonstrated.
- **Medium confidence**: The empirical performance improvements (FID scores) are well-supported by experiments, though the comparison with other methods could be more comprehensive.
- **Low confidence**: The theoretical equivalence between JKO steps and UOT problems in the semi-dual form requires more rigorous proof, particularly regarding convergence properties in high-dimensional settings.

## Next Checks

1. **Theoretical validation**: Prove or disprove the convergence guarantees for the semi-dual JKO formulation in high-dimensional spaces, focusing on the stability of the adversarial learning objective.

2. **Ablation study**: Systematically evaluate the impact of different f-divergence choices (KL vs. JSD) and step sizes on convergence speed and final FID scores across multiple datasets.

3. **Scalability test**: Train the model on higher-resolution datasets (e.g., 512×512 images) to verify whether the O(K) complexity claim holds as dimensionality increases.