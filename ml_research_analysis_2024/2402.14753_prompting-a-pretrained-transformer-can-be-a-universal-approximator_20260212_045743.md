---
ver: rpa2
title: Prompting a Pretrained Transformer Can Be a Universal Approximator
arxiv_id: '2402.14753'
source_url: https://arxiv.org/abs/2402.14753
tags:
- kvmf
- attention
- functions
- lemma
- universal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether prompting and prefix-tuning can
  be universal approximators for sequence-to-sequence functions. The authors show
  that prefix-tuning a single attention head is sufficient to approximate any continuous
  function on the hypersphere Sm, and that any sequence-to-sequence function can be
  approximated by prefixing a transformer with depth linear in the sequence length.
---

# Prompting a Pretrained Transformer Can Be a Universal Approximator

## Quick Facts
- arXiv ID: 2402.14753
- Source URL: https://arxiv.org/abs/2402.14753
- Authors: Aleksandar Petrov; Philip H. S. Torr; Adel Bibi
- Reference count: 40
- Primary result: Prefix-tuning a single attention head is sufficient to approximate any continuous function on a hypersphere

## Executive Summary
This paper establishes theoretical foundations for understanding the universal approximation capabilities of prompting and prefix-tuning in transformers. The authors prove that a single attention head, when prefixed with a learned sequence of control points, can approximate any continuous function on a hypersphere. They extend this result to show that any sequence-to-sequence function can be approximated by a transformer whose depth scales linearly with sequence length. The work provides Jackson-type bounds quantifying the relationship between prefix length and approximation precision, demonstrating that attention heads are uniquely suited for universal approximation.

## Method Summary
The paper employs theoretical analysis combining spherical harmonics, Jackson-type approximation theory, and the Kolmogorov-Arnold representation theorem. For hyperspherical approximation, they show that attention heads can interpolate between control points placed on the hypersphere to approximate continuous functions. For sequence-to-sequence approximation, they leverage the Kolmogorov-Arnold theorem to decompose multivariate functions into compositions of univariate functions, which can be implemented using alternating attention heads and MLPs in a transformer architecture.

## Key Results
- Prefix-tuning a single attention head is sufficient to approximate any continuous function on the hypersphere Sm
- Any sequence-to-sequence function can be approximated by a transformer with depth linear in sequence length
- Jackson-type bounds provide explicit relationships between prefix length and approximation precision
- Attention heads are uniquely suited for universal approximation compared to other mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A single attention head can approximate any continuous function on a hypersphere when prefixed with a suitable sequence of control points.
- **Mechanism:** The attention mechanism computes a weighted sum of control points, where the weights are determined by exponential dot products between the input and the control points. By placing enough control points on the hypersphere and tuning their values, this sum can interpolate to approximate any continuous function.
- **Core assumption:** The dot product on the hypersphere acts as a valid similarity measure, so that nearby points have larger dot products and thus larger influence on the output.
- **Evidence anchors:**
  - [abstract] "attention heads are uniquely suited for universal approximation with prefix-tuning a single attention head being sufficient to approximate any continuous function"
  - [section 3] "the dot product is a measure of closeness over the hypersphere"
- **Break condition:** If the control points are not uniformly distributed, or if the target function is too discontinuous for the chosen precision, the approximation error grows beyond tolerance.

### Mechanism 2
- **Claim:** By increasing the prefix length, the error of the approximation can be driven arbitrarily low for any smooth target function.
- **Mechanism:** The Jackson-type bound provides an explicit relationship between prefix length and approximation precision. More control points allow finer partitioning of the hypersphere, reducing interpolation error.
- **Core assumption:** The smoothness of the target function (captured by the modulus of continuity) bounds how fast it can vary, so that a finite number of control points suffices.
- **Evidence anchors:**
  - [abstract] "Jackson-type bounds on the length of the prefix needed to approximate a function to a desired precision"
  - [section 3] "the smoother the target f is, i.e., the smaller L, CH, the shorter the prefix length N"
- **Break condition:** For extremely non-smooth functions, the required prefix length grows exponentially, making the method impractical.

### Mechanism 3
- **Claim:** Any sequence-to-sequence function can be approximated by a transformer with depth linear in the sequence length.
- **Mechanism:** The Kolmogorov-Arnold representation theorem allows decomposition of multivariate functions into univariate functions composed in a sum-product form. The transformer alternates between attention heads (for element-wise maps) and MLPs (for scalar operations) to implement this decomposition.
- **Core assumption:** The pretrained model's attention patterns and MLPs are expressive enough to realize the univariate functions and compositions required by the Kolmogorov-Arnold theorem.
- **Evidence anchors:**
  - [abstract] "any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length"
  - [section 4] "using only 2 attention layers, we have compressed the whole sequence in a single scalar R"
- **Break condition:** If the required univariate functions or their compositions are not representable with the given model depth or if new attention patterns are needed, the approximation fails.

## Foundational Learning

- **Concept:** Hyperspherical analysis and spherical harmonics
  - Why needed here: The approximation theory for functions on the hypersphere relies on properties of spherical harmonics and spherical convolutions, which are essential for the Jackson-type bounds.
  - Quick check question: What is the relationship between spherical harmonics and the approximation of continuous functions on the hypersphere?

- **Concept:** Kolmogorov-Arnold representation theorem
  - Why needed here: This theorem allows decomposition of any multivariate continuous function into a sum of compositions of univariate functions, which is leveraged to construct the sequence-to-sequence approximation.
  - Quick check question: How does the Kolmogorov-Arnold theorem extend to sequence-to-sequence settings?

- **Concept:** Jackson-type approximation rates
  - Why needed here: Jackson bounds give explicit estimates of how many parameters (prefix length) are needed to achieve a desired precision, which is central to the theoretical claims.
  - Quick check question: What factors determine the prefix length needed for a given approximation error?

## Architecture Onboarding

- **Component map:** Input sequence -> Prefix + Input -> Attention head (weighted sum of control points) -> MLPs (element-wise transformations) -> Output sequence

- **Critical path:**
  1. Generate prefix sequence (control points)
  2. Feed prefix + input to transformer
  3. Attention head computes weighted sum of control points
  4. MLPs apply element-wise transformations
  5. Output sequence is the approximation

- **Design tradeoffs:**
  - Prefix length vs. approximation precision: Longer prefixes allow better approximation but increase computation and memory.
  - Choice of control point placement: Uniform distribution on hypersphere is optimal for minimizing interpolation error.
  - Model depth: Linear in sequence length is sufficient for sequence-to-sequence approximation, but deeper models may be needed for complex attention patterns.

- **Failure signatures:**
  - High approximation error: Likely due to insufficient prefix length or poor control point placement.
  - Training instability: May occur if control points are initialized poorly or if the target function is too non-smooth.
  - Overfitting: Can happen if prefix length is too large relative to the complexity of the target function.

- **First 3 experiments:**
  1. Approximate a simple scalar function on a 2D hypersphere (e.g., cosine) and measure error vs. prefix length.
  2. Test sequence-to-sequence approximation with a fixed-length sequence and simple mapping.
  3. Vary the smoothness of the target function and observe the effect on required prefix length.

## Open Questions the Paper Calls Out
None

## Limitations
- The required prefix length may grow exponentially for non-smooth functions, making the method impractical for many real-world applications
- The theory assumes access to a pretrained transformer with sufficient representational capacity, which may not hold for common pretrained models
- Achieving uniformly distributed control points on high-dimensional hyperspheres may be challenging in practice

## Confidence

**High Confidence**: The theoretical framework for single-head approximation on hyperspheres (Mechanism 1) is well-established and rigorously proven. The mathematical arguments connecting attention weights to interpolation on control points are sound.

**Medium Confidence**: The Jackson-type bounds relating prefix length to approximation precision (Mechanism 2) are theoretically valid, but their practical implications for real-world functions remain uncertain. The bounds provide worst-case guarantees that may be overly pessimistic for typical applications.

**Low Confidence**: The extension from hyperspherical approximation to general sequence-to-sequence functions (Mechanism 3) relies on strong assumptions about the pretrained model's ability to implement Kolmogorov-Arnold decompositions. Without empirical validation, the practical feasibility of this approach remains speculative.

## Next Checks
1. **Empirical Prefix Efficiency Test**: Implement the theoretical framework on a concrete sequence-to-sequence task (such as copying or sorting) and measure the actual prefix length required versus the theoretical bounds. Compare prefix-tuning performance against full fine-tuning to quantify the efficiency gap.

2. **Control Point Distribution Sensitivity**: Systematically vary the initialization and optimization of control points on hyperspheres of different dimensions. Measure how non-uniform distributions affect approximation quality and whether alternative initialization strategies (beyond uniform sampling) improve practical performance.

3. **Pretrained Model Expressivity Benchmark**: Test whether standard pretrained transformers (BERT, GPT variants) can actually implement the required univariate functions and compositions for Kolmogorov-Arnold decomposition. Identify which types of sequence-to-sequence mappings are achievable with current models versus which require architectural modifications.