---
ver: rpa2
title: Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained
  Answer Decomposition
arxiv_id: '2409.17073'
source_url: https://arxiv.org/abs/2409.17073
tags:
- answer
- information
- decomposition
- question
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of attributing answer text to
  its source document in long document question answering systems. The authors propose
  a novel approach using template-based in-context learning for coarse-grained answer
  decomposition, which breaks down answers into contextually relevant information
  units.
---

# Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition

## Quick Facts
- arXiv ID: 2409.17073
- Source URL: https://arxiv.org/abs/2409.17073
- Reference count: 26
- Authors: Pritika Ramu, Koustava Goswami, Apoorv Saxena, Balaji Vasan Srinivasan
- Key outcome: Template-based coarse-grained answer decomposition achieves 3% precision gain over baseline models on Citation Verifiability dataset and improves LLM-based attribution performance

## Executive Summary
This paper addresses the challenge of attributing answer text to its source document in long document question answering systems. The authors propose a novel approach using template-based in-context learning for coarse-grained answer decomposition, which breaks down answers into contextually relevant information units. They evaluate this method with both retrieval-based systems (BM25, GTR, MonoT5) and large language models (GPT-4, GPT-3.5, LLaMa 2 70B, LLaMa 2 13B) as attributors.

The proposed method demonstrates significant improvements in attribution performance, achieving state-of-the-art results on both Citation Verifiability and QASPER datasets. By decomposing answers into coarse-grained, question-contextualized units, the approach enables more accurate attribution of answer components to their source documents, addressing a critical gap in post-hoc attribution for long document comprehension.

## Method Summary
The proposed method employs template-based in-context learning to decompose answers into coarse-grained, contextually relevant information units. The approach involves three main stages: (1) answer decomposition using predefined templates to identify key information units, (2) question-contextualized processing of these units to enhance relevance, and (3) attribution of each unit to its source document using both retrieval-based systems and large language models as attributors. The method leverages the strengths of both template-based decomposition and LLM capabilities to achieve improved attribution precision.

## Key Results
- 3% average precision gain over baseline models on Citation Verifiability dataset
- State-of-the-art attribution performance on both Citation Verifiability and QASPER datasets
- Improved LLM-based attribution performance across GPT-4, GPT-3.5, LLaMa 2 70B, and LLaMa 2 13B models

## Why This Works (Mechanism)
The template-based coarse-grained answer decomposition works by breaking down complex answers into smaller, contextually relevant units that can be more easily attributed to their source documents. By using predefined templates, the method ensures consistency in decomposition across different answer types while maintaining the contextual relevance of each unit. The question-contextualized processing further enhances the relevance of these units, making it easier for both retrieval-based systems and LLMs to accurately attribute them to their sources.

## Foundational Learning
- **Coarse-grained decomposition**: Breaking answers into larger, meaningful units rather than fine-grained tokens, enabling better contextual understanding and attribution accuracy.
- **Template-based in-context learning**: Using predefined templates to guide the decomposition process, ensuring consistency and structure in how answers are broken down.
- **Question-contextualized processing**: Incorporating the original question into the processing of decomposed units to maintain relevance and improve attribution accuracy.
- **Attribution verification**: The process of linking answer components back to their source documents, crucial for establishing credibility and traceability in long document QA.
- **LLM-based attribution**: Leveraging large language models as attribution tools, combining their understanding of context with structured decomposition outputs.

## Architecture Onboarding

**Component Map:**
Answer Decomposition Templates -> Coarse-grained Unit Extraction -> Question-Contextualization -> Attribution (Retrieval-based/LLM)

**Critical Path:**
The critical path follows: Template-based decomposition → Unit extraction → Context incorporation → Attribution assignment. Each stage must complete successfully for accurate attribution.

**Design Tradeoffs:**
The method trades computational complexity for attribution accuracy, as template-based decomposition adds an extra processing step. The choice between retrieval-based and LLM attribution involves balancing precision (LLMs typically higher) against computational cost and inference time.

**Failure Signatures:**
- Incorrect template application leading to irrelevant decomposition units
- Loss of contextual information during unit extraction
- Attribution failures when source documents contain similar content across multiple sections

**First 3 Experiments:**
1. Compare attribution precision with and without coarse-grained decomposition on a subset of the Citation Verifiability dataset
2. Evaluate template effectiveness by measuring attribution accuracy across different answer types
3. Benchmark retrieval-based vs LLM attribution methods using identical decomposed units

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Template-based approach may not scale well to highly complex or domain-specific questions
- Primary evaluation focuses on English language documents, limiting generalizability
- Computational overhead of the template-based decomposition step is not thoroughly analyzed

## Confidence

**High Confidence:**
- 3% precision improvement over baseline models on Citation Verifiability dataset is well-supported by experimental results
- Methodology for answer decomposition and attribution is clearly defined and reproducible

**Medium Confidence:**
- State-of-the-art claims are supported but could benefit from more comprehensive comparison with recent methods
- LLM-based attribution improvements are promising but may vary with different model versions

**Low Confidence:**
- Generalizability of template-based approach across diverse domains remains to be thoroughly validated
- Scalability for extremely long documents or high-throughput applications is not fully explored

## Next Checks
1. Conduct ablation studies to isolate the contribution of each component (template-based decomposition, contextualization, and attribution) to overall performance
2. Evaluate method performance across multiple languages and domains to assess generalizability beyond English-centric datasets
3. Perform comprehensive computational efficiency analysis including runtime comparisons and resource usage metrics to establish practical viability for real-world applications