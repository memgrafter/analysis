---
ver: rpa2
title: Assessing and Verifying Task Utility in LLM-Powered Applications
arxiv_id: '2405.02178'
source_url: https://arxiv.org/abs/2405.02178
tags:
- criteria
- task
- arxiv
- problem
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces AgentEval, a framework for automatically
  assessing the utility of LLM-powered applications through multi-agent collaboration.
  The method involves three agents: CriticAgent generates evaluation criteria from
  task descriptions and example solutions, QuantifierAgent scores solutions against
  these criteria, and VerifierAgent validates and refines the criteria for robustness.'
---

# Assessing and Verifying Task Utility in LLM-Powered Applications

## Quick Facts
- arXiv ID: 2405.02178
- Source URL: https://arxiv.org/abs/2405.02178
- Reference count: 40
- Framework for automatic multi-dimensional utility assessment of LLM applications

## Executive Summary
AgentEval introduces a novel framework for automatically assessing LLM-powered application utility through multi-agent collaboration. The framework employs three specialized LLM agents: CriticAgent generates evaluation criteria from task descriptions, QuantifierAgent scores solutions against these criteria, and VerifierAgent validates and refines criteria for robustness. Experiments demonstrate the framework can effectively distinguish between successful and failed cases across multiple dimensions like accuracy, clarity, and efficiency, going beyond simple binary success metrics.

## Method Summary
AgentEval is a three-agent framework for multi-dimensional task utility assessment. CriticAgent generates task-specific evaluation criteria from task descriptions and example solutions, QuantifierAgent measures solutions against these criteria using accepted value ranges, and VerifierAgent validates criteria robustness through stability testing and discriminative power analysis. The framework uses GPT-4 via Azure OpenAI with temperature=0 for deterministic outputs, implementing agent communication through the AutoGen framework. It employs coefficient of variation analysis for stability assessment and adversarial testing with disturbed solutions to verify discriminative power.

## Key Results
- AgentEval effectively distinguishes between successful and failed cases across multiple dimensions (accuracy, clarity, efficiency)
- Framework demonstrates stability with coefficient of variation values below 0.5 indicating reliable quantification
- Solution-based criteria generation produces more diverse criteria than task-based approaches alone
- Maintains robustness against adversarial examples through iterative verification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AgentEval can reliably quantify task utility across multiple dimensions beyond simple success/failure metrics.
- Mechanism: The framework employs a three-agent system where CriticAgent generates task-specific criteria, QuantifierAgent measures solutions against these criteria using accepted value ranges, and VerifierAgent validates criteria robustness and discriminative power through repeated quantification and adversarial testing.
- Core assumption: LLM agents can generate and validate evaluation criteria that are meaningful and stable for assessing task utility.
- Evidence anchors:
  - [abstract] "AgentEval can effectively distinguish between successful and failed cases across multiple dimensions (e.g., accuracy, clarity, efficiency)"
  - [section 3] "CriticAgent would return a set of criteria C = {c1, . . . , cn}, where each criterion ci is accompanied by a set of accepted values"
  - [corpus] Weak - no direct citations found, but the approach is novel
- Break condition: If LLM agents fail to generate coherent criteria or if quantified results show high variance (coefficient of variation > 0.5) across repeated runs.

### Mechanism 2
- Claim: The framework maintains stability and robustness through iterative verification and noise testing.
- Mechanism: VerifierAgent performs two key validation steps - checking criteria stability through repeated quantification (coefficient of variation analysis) and testing discriminative power by comparing original solutions against disturbed versions with portions of sentences removed.
- Core assumption: Solutions with portions removed should score worse on criteria like completeness and clarity, allowing the framework to validate its assessment quality.
- Evidence anchors:
  - [section 6.2.2] "Assuming sample S is more likely to outperform its disturbed version S', our assessment should confirm this assumption by assigning better quantified performance S in comparison to S'"
  - [section 6.2.1] "Using this consolidated list, we measure the dispersion of quantified results using the coefficient of variation"
  - [corpus] Weak - approach appears novel with no direct corpus support
- Break condition: If disturbed solutions score equal to or better than original solutions on criteria measuring solution quality, indicating the framework cannot distinguish between good and degraded outputs.

### Mechanism 3
- Claim: Solution-based criteria generation produces more diverse and task-relevant evaluation criteria than task-based approaches alone.
- Mechanism: When CriticAgent receives both task descriptions and solution examples (successful/failed cases), it generates additional criteria specific to the solution approach, such as "Code Efficiency" when coding is used to solve math problems.
- Core assumption: Different solution approaches to the same task require different evaluation criteria to properly assess utility.
- Evidence anchors:
  - [section 6.1] "a solution to a mathematical problem, might satisfy criteria such as 'Accuracy' and 'Clarity', independent of the solution. However, when additional tools such as coding are used to solve the problems, additional criteria like 'Code Efficiency' may be introduced"
  - [section 6.1] "Fig. 4 illustrates the impact of different Ï„ values... This analysis shows that the solution-based approach has potential to produce more diverse criteria than the task-based approach"
  - [corpus] Weak - framework appears to be novel approach
- Break condition: If solution-based criteria generation does not produce meaningfully different criteria than task-based generation for the same task.

## Foundational Learning

- Concept: Multi-dimensional evaluation metrics
  - Why needed here: AgentEval moves beyond binary success/failure to assess utility across multiple dimensions like clarity, efficiency, and completeness
  - Quick check question: What are the advantages of multi-dimensional evaluation over simple success metrics for LLM-powered applications?

- Concept: Coefficient of variation as stability metric
  - Why needed here: Used to measure the dispersion of QuantifierAgent's outputs across repeated runs to ensure criteria are stable and reliable
  - Quick check question: How would you interpret a coefficient of variation of 0.3 versus 0.7 for a given evaluation criterion?

- Concept: Adversarial testing for evaluation frameworks
  - Why needed here: VerifierAgent uses noise injection (removing portions of solutions) to test whether the framework can distinguish between original and degraded solutions
  - Quick check question: What does it mean if a disturbed solution scores as high or higher than the original on completeness criteria?

## Architecture Onboarding

- Component map:
  - CriticAgent -> QuantifierAgent -> VerifierAgent
  - Task description and solutions -> Generated criteria -> Quantified scores -> Validated criteria

- Critical path:
  1. Task description and solution examples fed to CriticAgent
  2. Generated criteria passed to QuantifierAgent for initial quantification
  3. VerifierAgent tests criteria stability across multiple runs
  4. VerifierAgent tests discriminative power with disturbed samples
  5. Final verified criteria set used for evaluation

- Design tradeoffs:
  - Flexibility vs. specificity: General framework can assess any LLM-powered application but may miss task-specific nuances
  - Computational cost vs. robustness: Multiple runs for stability testing increase computational requirements but ensure reliable results
  - LLM dependency vs. consistency: Using GPT-4 provides strong reasoning capabilities but may introduce variability across model versions

- Failure signatures:
  - High coefficient of variation (>0.5) across repeated runs indicates unstable criteria
  - Disturbed solutions scoring equal to or better than originals on quality criteria indicates poor discriminative power
  - Convergence to small number of criteria after multiple runs suggests diminishing returns in diversity

- First 3 experiments:
  1. Run AgentEval on a simple math problem with known ground truth solutions to verify it can distinguish between correct and incorrect answers
  2. Test stability by running QuantifierAgent 10 times on the same solution and measuring coefficient of variation for each criterion
  3. Perform adversarial testing by removing 25% of sentences from a solution and verifying it scores worse on completeness and clarity criteria

## Open Questions the Paper Calls Out
- The paper does not explicitly call out specific open questions, but identifies areas for future work including incorporating a broader array of LLM models, exploring tasks where success is not clearly defined, and potentially incorporating human domain experts in the loop.

## Limitations
- Framework's reliance on GPT-4 may not generalize to other LLM architectures or open-source models
- Fixed threshold values for stability and discriminative power validation lack clear justification
- Does not validate LLM-generated criteria against human-annotated ground truth evaluations
- Assumes success is clearly defined, limiting applicability to tasks with ambiguous success criteria

## Confidence
- **High confidence**: The framework can reliably distinguish between successful and failed cases across multiple dimensions
- **Medium confidence**: The iterative verification process ensures criteria stability
- **Low confidence**: Solution-based criteria generation produces meaningfully different criteria than task-based approaches alone

## Next Checks
1. Test AgentEval's performance using open-source LLM alternatives (e.g., Llama-2, Claude) to assess model dependency and generalizability across different architectures
2. Conduct human evaluation studies to validate whether LLM-generated criteria align with expert judgment and whether the multi-dimensional assessment captures aspects humans consider important
3. Implement cross-validation by applying AgentEval-generated criteria to a completely different domain (e.g., code generation or creative writing) to test framework transferability beyond the tested math and household task domains