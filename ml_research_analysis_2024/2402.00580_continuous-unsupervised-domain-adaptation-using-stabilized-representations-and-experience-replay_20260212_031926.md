---
ver: rpa2
title: Continuous Unsupervised Domain Adaptation Using Stabilized Representations
  and Experience Replay
arxiv_id: '2402.00580'
source_url: https://arxiv.org/abs/2402.00580
tags:
- domain
- learning
- tasks
- performance
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel algorithm for continual unsupervised
  domain adaptation (UDA) in continual learning (CL) scenarios. The method addresses
  the challenge of maintaining model generalization under domain shift when new domains
  arrive continually with only unlabeled data.
---

# Continuous Unsupervised Domain Adaptation Using Stabilized Representations and Experience Replay

## Quick Facts
- arXiv ID: 2402.00580
- Source URL: https://arxiv.org/abs/2402.00580
- Reference count: 40
- Introduces a novel algorithm for continual unsupervised domain adaptation that outperforms existing methods on multiple benchmark datasets.

## Executive Summary
This paper addresses the challenge of continual unsupervised domain adaptation (UDA) in continual learning scenarios, where models must adapt to new domains without forgetting previously learned ones. The proposed LDAuCID algorithm stabilizes the learned internal distribution using a Gaussian Mixture Model (GMM) and aligns new domain distributions to this model using Sliced Wasserstein Distance (SWD). Experience replay is employed to mitigate catastrophic forgetting. The method demonstrates superior performance across digit recognition tasks, ImageCLEF-DA, Office-Home, and Office-Caltech datasets, with theoretical analysis providing an upper bound for expected error.

## Method Summary
The LDAuCID algorithm first pre-trains on a labeled source domain, then estimates a GMM from the source embeddings. For each new unlabeled target domain, it generates pseudo-labeled samples from the GMM, aligns the target domain's distribution to the GMM using SWD, and updates the model while replaying representative samples from memory to prevent forgetting. The GMM is updated after each domain adaptation. The method combines three key components: GMM-based internal distribution modeling, SWD for distribution alignment, and experience replay for forgetting mitigation.

## Key Results
- LDAuCID outperforms existing UDA methods across multiple benchmark datasets including digit recognition, ImageCLEF-DA, Office-Home, and Office-Caltech
- The algorithm effectively mitigates forgetting effects while improving generalizability in continual UDA tasks
- Theoretical analysis provides an upper bound for the expected error on all learned tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stabilizing the internally learned distribution preserves model generalization across domain shifts.
- Mechanism: The encoder maps inputs into a multimodal distribution in the embedding space, where each mode represents a class. By modeling this distribution using a GMM and aligning new domain distributions to it, the model retains class separability and thus generalization.
- Core assumption: The internal distribution learned from the source domain is transferable to new domains if maintained.
- Evidence anchors:
  - [abstract] "Our solution is based on stabilizing the learned internal distribution to enhances the model generalization on new domains."
  - [section 3] "Our approach is centered around consolidating the intermediate distribution acquired through discriminative embedding, aiming to preserve the generalizability of the model."
- Break condition: If the new domain's distribution deviates too far from the source (e.g., SVHN → MNIST), the alignment may degrade performance.

### Mechanism 2
- Claim: Experience replay mitigates catastrophic forgetting by preserving representative samples from past domains.
- Mechanism: After learning each domain, the model stores the most representative samples (closest to cluster means) in a memory buffer. These samples are replayed during training on new domains to prevent forgetting.
- Core assumption: A small set of representative samples can effectively approximate the past domain distributions.
- Evidence anchors:
  - [abstract] "Additionally, we leverage experience replay to overcome the problem of catastrophic forgetting..."
  - [section 3] "When a task is learned, we select a small subset of the training data points as representative samples, which are then stored in a memory buffer."
- Break condition: If the buffer size is too small or the domains are too dissimilar, forgetting may still occur.

### Mechanism 3
- Claim: Sliced Wasserstein distance provides a differentiable, efficient metric for aligning distributions in high-dimensional spaces.
- Mechanism: SWD approximates the Wasserstein distance by projecting high-dimensional distributions onto 1D subspaces and integrating over these projections. This allows gradient-based optimization for distribution alignment.
- Core assumption: SWD is a good proxy for Wasserstein distance and can be computed efficiently from samples.
- Evidence anchors:
  - [section 3] "we use SWD as this metric" and "SWD is suitable choice for UDA."
  - [appendix A] Detailed explanation of SWD formulation and its advantages.
- Break condition: If the number of projection directions is insufficient, the approximation may be poor.

## Foundational Learning

- Concept: Gaussian Mixture Models (GMMs) for modeling multimodal distributions.
  - Why needed here: The internal distribution in the embedding space is multimodal (one mode per class), and GMMs provide a parametric way to model and estimate it.
  - Quick check question: How do you estimate the parameters of a GMM when you have labeled data for each mode?

- Concept: Sliced Wasserstein Distance (SWD) for distribution alignment.
  - Why needed here: SWD provides a differentiable metric to measure and minimize the distance between the internal distribution and new domain distributions in the embedding space.
  - Quick check question: Why is SWD preferred over other distribution metrics like MMD in this context?

- Concept: Experience replay in continual learning.
  - Why needed here: Storing and replaying representative samples from past domains helps the model retain knowledge and mitigate catastrophic forgetting when learning new domains.
  - Quick check question: How does the choice of representative samples (e.g., closest to cluster means) affect the effectiveness of experience replay?

## Architecture Onboarding

- Component map: Encoder (ϕv) -> Classifier (hw) -> GMM estimator -> Memory buffer -> SWD calculator

- Critical path:
  1. Train on source domain with labeled data.
  2. Estimate GMM from source embeddings.
  3. For each new domain:
     a. Generate pseudo-labeled data from GMM.
     b. Align new domain embeddings to GMM using SWD.
     c. Replay stored samples to mitigate forgetting.
     d. Update GMM with new domain information.

- Design tradeoffs:
  - Buffer size vs. forgetting: Larger buffers reduce forgetting but increase memory usage.
  - SWD approximation vs. accuracy: More projection directions improve alignment but increase computation.
  - GMM complexity vs. expressiveness: More components improve fit but risk overfitting.

- Failure signatures:
  - Poor performance on new domains: Likely due to misalignment or insufficient pseudo-labeled data.
  - Catastrophic forgetting: Buffer too small or samples not representative enough.
  - Unstable training: Hyperparameters (λ, τ) not tuned properly.

- First 3 experiments:
  1. Test GMM estimation on source domain embeddings (visualize with UMAP).
  2. Validate SWD computation on synthetic distributions with known alignment.
  3. Evaluate experience replay with a small buffer on a simple continual learning task (e.g., permuted MNIST).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform when the internal distribution modes cannot be approximated well with a single Gaussian?
- Basis in paper: [explicit] The paper mentions that more sophisticated techniques for estimating the internal distribution could potentially improve the performance of continual learning, particularly when the internal distribution modes cannot be approximated well with a single Gaussian.
- Why unresolved: The current approach uses a straightforward method for estimating the internal distribution using a Gaussian Mixture Model (GMM). However, the paper acknowledges that more complex internal distributions might require more advanced estimation techniques.
- What evidence would resolve it: Comparative experiments using different internal distribution estimation methods (e.g., more complex mixture models, non-parametric methods) on datasets with complex internal distributions would provide insights into the impact of this limitation.

### Open Question 2
- Question: What is the optimal task order for the best performance in continual learning?
- Basis in paper: [explicit] The paper identifies exploring the optimal task order for the best performance in continual learning as a potential area of future research.
- Why unresolved: While the paper demonstrates that the order of tasks can impact performance, it does not investigate strategies for determining the optimal order.
- What evidence would resolve it: Experimental studies comparing different task ordering strategies (e.g., based on domain similarity, task complexity, or a combination of factors) and their impact on performance metrics such as accuracy, forgetting, and transfer would provide insights into optimal task ordering.

### Open Question 3
- Question: How does the proposed method perform in the incremental learning setting, where new classes can be introduced beyond the initial training phase?
- Basis in paper: [explicit] The paper mentions extending the algorithm to the incremental learning setting, where new classes can be introduced beyond the initial training phase, as a potential research direction.
- Why unresolved: The current method is designed for domain adaptation within a fixed set of classes. Extending it to handle new classes would require modifications to the internal distribution estimation and adaptation process.
- What evidence would resolve it: Experiments comparing the proposed method with existing incremental learning approaches on benchmark datasets with class-incremental scenarios would provide insights into its effectiveness in this setting.

## Limitations
- The GMM-based distribution modeling assumes class-conditional distributions remain stable across domains, which may not hold for highly dissimilar domains
- The experience replay mechanism's effectiveness depends on buffer size (500 samples) which may be inadequate for complex domains
- The SWD approximation quality depends on projection direction count, potentially affecting alignment accuracy

## Confidence
- High confidence: The core algorithmic framework (GMM modeling + SWD alignment + experience replay) is well-defined and the experimental results on benchmark datasets are convincing
- Medium confidence: The effectiveness of the representative sample selection method and the sufficiency of the memory buffer size
- Low confidence: The theoretical error bound's practical relevance given real-world distribution shifts

## Next Checks
1. **Distribution Stability Analysis**: Conduct ablation studies on domain pairs with varying degrees of similarity (e.g., similar: MNIST→MNIST-M, dissimilar: SVHN→MNIST) to quantify how distribution stability affects performance and identify the breaking point where GMM alignment fails.

2. **Memory Buffer Sensitivity**: Systematically vary the memory buffer size (e.g., 100, 500, 1000 samples) across different domain complexities and measure the trade-off between forgetting mitigation and memory overhead, particularly for complex domains like Office-Home.

3. **SWD Approximation Quality**: Evaluate the impact of projection direction count on alignment quality by comparing performance with different numbers of projections (e.g., 50, 100, 200) and validate whether the approximation error correlates with downstream task performance.