---
ver: rpa2
title: 'Gla-AI4BioMed at RRG24: Visual Instruction-tuned Adaptation for Radiology
  Report Generation'
arxiv_id: '2412.04954'
source_url: https://arxiv.org/abs/2412.04954
tags:
- radiology
- report
- findings
- generation
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents a radiology-focused visual language model for
  generating chest X-ray reports. The model integrates a vision encoder with a Vicuna-7B
  LLM using a two-stage fine-tuning approach: first aligning chest X-ray features
  with the LLM, then fine-tuning for report generation using LoRA.'
---

# Gla-AI4BioMed at RRG24: Visual Instruction-tuned Adaptation for Radiology Report Generation

## Quick Facts
- arXiv ID: 2412.04954
- Source URL: https://arxiv.org/abs/2412.04954
- Reference count: 13
- 4th place on RRG24 leaderboard with F1-RadGraph scores of 24.13 (Findings) and 22.79 (Impressions) on public test set

## Executive Summary
This work presents a radiology-focused visual language model for generating chest X-ray reports by integrating a vision encoder with a Vicuna-7B LLM through a two-stage fine-tuning approach. The model processes multiple chest X-ray images by stitching them together and generates separate reports for Findings and Impressions sections. Evaluated on the RRG24 dataset, the system achieves competitive performance placing 4th on the leaderboard with F1-RadGraph scores of 24.13 (Findings) and 22.79 (Impressions) on the public test set.

## Method Summary
The approach uses a two-stage fine-tuning process: first aligning chest X-ray features with the LLM Vicuna-7B using a CLIP image encoder and MLP adapter, then fine-tuning for report generation using LoRA. Multiple chest X-ray images are stitched together horizontally to form a single input, and separate models are trained for Findings and Impressions sections due to their different characteristics. The model uses LoRA-based fine-tuning to efficiently adapt the large language model without full retraining.

## Key Results
- Achieved 4th place on RRG24 leaderboard
- F1-RadGraph scores of 24.13 (Findings) and 22.79 (Impressions) on public test set
- F1-RadGraph scores of 24.13 (Findings) and 22.10 (Impressions) on hidden test set
- Uses efficient LoRA-based fine-tuning for LLM adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage fine-tuning process significantly improves alignment between chest X-ray features and radiology report text.
- Mechanism: Initial alignment of chest X-ray features with the LLM in stage 1 creates a shared semantic space for medical imagery and language, followed by LoRA-based fine-tuning for radiology report generation in stage 2, allowing efficient parameter updates without retraining the entire model.
- Core assumption: Chest X-ray features can be effectively mapped to the LLM's existing semantic representations, and LoRA can capture the specialized knowledge needed for radiology report generation.
- Evidence anchors:
  - [abstract]: "The training process involves a two-stage approach: (i) initial alignment of chest X-ray features with the LLM (ii) followed by fine-tuning for radiology report generation"
  - [section 3.2]: "In the first epoch training phase on the provided dataset... we keep the visual encoder and LLM weights unchanged, focusing solely on updating the MLP adapter. This approach aligns the features from chest X-ray images with their textual embeddings in the LLM."
- Break condition: If the MLP adapter cannot effectively bridge the semantic gap between chest X-ray features and LLM embeddings, or if the LoRA parameters fail to capture the specialized medical knowledge needed for accurate report generation.

### Mechanism 2
- Claim: Stitching multiple chest X-ray images together enables the model to process varying numbers of images with limited computational resources.
- Mechanism: By horizontally merging multiple images into a single input, the model can leverage a single image encoder to process information from multiple X-ray images simultaneously, reducing computational complexity while maintaining the ability to capture multi-image context.
- Core assumption: The merged image representation preserves sufficient information for the model to understand the relationships between different X-ray views, and the model can effectively process this combined input.
- Evidence anchors:
  - [section 4.1]: "we select up to the first four images for the image input. We merge multiple images horizontally to form a single-image input, which is proven to be robust in our experiments."
  - [section 5]: "we adopt a method of stitching multiple images together, allowing a single image encoder to process multiple image inputs simultaneously."
- Break condition: If the image stitching process results in information loss or if the model cannot effectively interpret the relationships between different anatomical regions in the merged image representation.

### Mechanism 3
- Claim: Fine-tuning separate models for Findings and Impressions sections improves performance by tailoring the model architecture to the distinct characteristics of each section.
- Mechanism: The Findings section focuses on objective descriptions of symptoms, while the Impressions section provides diagnostic conclusions. By training separate models for each section, the model can optimize its language generation strategy to match the specific requirements and style of each section.
- Core assumption: The distinct purposes and characteristics of the Findings and Impressions sections warrant separate model architectures, and this specialization improves overall performance.
- Evidence anchors:
  - [section 3.3]: "We have developed separate models for each section because their focuses are different. The 'Findings' section provides a factual description based on the images, while the 'Impressions' section offers the radiologist's conclusions and recommendations."
- Break condition: If the performance gains from separate models do not outweigh the increased computational and maintenance costs, or if the distinction between sections is not as significant as assumed.

## Foundational Learning

- Concept: Vision-language model alignment
  - Why needed here: To enable the model to understand the relationship between chest X-ray images and their corresponding textual descriptions in radiology reports.
  - Quick check question: How does the MLP adapter help bridge the gap between image features and language model embeddings?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: To efficiently fine-tune the large language model for radiology report generation without retraining the entire model, reducing computational costs while maintaining performance.
  - Quick check question: What are the advantages of using LoRA for fine-tuning compared to full fine-tuning of the LLM?

- Concept: Multi-image processing in medical imaging
  - Why needed here: To replicate the clinical workflow where radiologists analyze multiple X-ray images to form a comprehensive understanding of a patient's condition.
  - Quick check question: Why is it important for the model to process multiple X-ray images rather than just a single image?

## Architecture Onboarding

- Component map: CLIP image encoder (frozen) -> MLP adapter (trained in stage 1) -> Vicuna-7B LLM (fine-tuned in stage 2 with LoRA) -> Image stitching module (preprocessing)

- Critical path:
  1. Input: Multiple chest X-ray images
  2. Preprocessing: Stitch images horizontally
  3. Encoding: CLIP encoder extracts features
  4. Adaptation: MLP adapter aligns features with LLM
  5. Generation: Vicuna LLM generates report text

- Design tradeoffs:
  - Single vs. separate models for Findings and Impressions sections
  - Image stitching vs. separate image encoding
  - LoRA vs. full fine-tuning for LLM adaptation

- Failure signatures:
  - Poor performance on multi-image inputs
  - Inconsistent findings between Findings and Impressions sections
  - Hallucinations or irrelevant content in generated reports

- First 3 experiments:
  1. Test single-image vs. multi-image input performance to validate the stitching approach
  2. Compare LoRA-based fine-tuning vs. full fine-tuning for efficiency and performance
  3. Evaluate the impact of separate vs. unified models for Findings and Impressions sections

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed model vary when processing single vs. multiple chest X-ray images, and what is the optimal number of images for achieving the best balance between accuracy and computational efficiency?
- Basis in paper: [inferred] The paper discusses stitching multiple images together for joint encoding and mentions that "our analysis indicates that the performance may be compromised in multi-image inference scenarios where it does not account for superfluous images."
- Why unresolved: The paper does not provide a detailed comparison of model performance across different numbers of input images or analyze the trade-off between accuracy gains and computational costs.
- What evidence would resolve it: Systematic experiments comparing model performance (using metrics like F1-RadGraph, BLEU4, ROUGEL) when processing 1, 2, 3, 4, and 5+ images, along with computational cost analysis (inference time, memory usage) for each scenario.

### Open Question 2
- Question: What specific architectural modifications or training strategies could improve the model's ability to distinguish between relevant and superfluous images in multi-image inference scenarios?
- Basis in paper: [explicit] The discussion section states "Enhancing the ability of model to differentiate between relevant and superfluous images could significantly improve diagnostic accuracy."
- Why unresolved: The paper identifies this as a limitation but does not propose or test specific solutions for improving image relevance discrimination.
- What evidence would resolve it: Implementation and evaluation of methods such as attention mechanisms that weight image importance, contrastive learning approaches to learn image relevance, or post-processing techniques to filter irrelevant images before report generation.

### Open Question 3
- Question: How does the two-stage training approach (chest X-ray feature alignment followed by fine-tuning for report generation) compare to alternative training strategies in terms of final model performance and training efficiency?
- Basis in paper: [explicit] The methodology section describes a specific two-stage training process but does not compare it to other potential approaches.
- Why unresolved: The paper presents the two-stage approach as the chosen method but does not provide ablation studies or comparisons with alternative training strategies.
- What evidence would resolve it: Comparative experiments testing different training strategies such as end-to-end training, alternative pre-training tasks, or different fine-tuning schedules, with performance metrics and training time comparisons.

## Limitations

- The image stitching approach may lose spatial relationships between anatomical regions across different X-ray views.
- Separate models for Findings and Impressions sections double computational requirements and may introduce inconsistencies.
- The evaluation focuses primarily on F1-RadGraph scores without extensive clinical validation of generated report quality.

## Confidence

- **High Confidence**: The fundamental approach of using a two-stage fine-tuning process with LoRA for efficient adaptation of large language models to radiology report generation is technically sound and well-established in the literature.
- **Medium Confidence**: The specific implementation details, including the MLP adapter architecture and LoRA configuration, are likely to work as described but may require fine-tuning for optimal performance.
- **Medium Confidence**: The reported leaderboard rankings are accurate based on the official RRG24 evaluation, but the absolute performance scores suggest room for improvement compared to top-performing systems.

## Next Checks

1. **Clinical Accuracy Validation**: Have the generated radiology reports reviewed by medical professionals to assess whether the F1-RadGraph scores correlate with clinically meaningful report quality and accuracy.

2. **Ablation Study**: Conduct controlled experiments comparing: (a) single vs. multi-image processing without stitching, (b) LoRA vs. full fine-tuning, and (c) unified vs. separate models for Findings and Impressions to quantify the contribution of each design choice.

3. **Error Analysis**: Systematically analyze failure cases to identify common patterns of hallucination, anatomical region confusion, or clinically significant errors that could impact diagnostic utility.