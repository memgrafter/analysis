---
ver: rpa2
title: How Multilingual Are Large Language Models Fine-Tuned for Translation?
arxiv_id: '2405.20512'
source_url: https://arxiv.org/abs/2405.20512
tags:
- translation
- language
- languages
- fine-tuning
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how well large language models (LLMs) fine-tuned
  for translation perform on tasks beyond their training data, focusing on 132 translation
  directions between 12 languages. The authors analyze the TOWER family of models
  and compare them to zero-shot LLAMA2 and the NLLB system.
---

# How Multilingual Are Large Language Models Fine-Tuned for Translation?

## Quick Facts
- arXiv ID: 2405.20512
- Source URL: https://arxiv.org/abs/2405.20512
- Authors: Aquia Richburg; Marine Carpuat
- Reference count: 24
- Primary result: Fine-tuning improves translation quality for zero-shot languages on average, but impact is uneven and depends on language pairs involved.

## Executive Summary
This paper evaluates how well large language models (LLMs) fine-tuned for translation perform on tasks beyond their training data, focusing on 132 translation directions between 12 languages. The authors analyze the TOWER family of models and compare them to zero-shot LLAMA2 and the NLLB system. They find that fine-tuning improves translation quality for zero-shot languages on average, but the impact is uneven and depends on the language pairs involved. The best-performing model (TOWER INSTRUCT -13B) is competitive with NLLB on average but shows higher variance in quality, particularly for unsupervised language pairs. The study highlights the potential of fine-tuning for multilingual MT while emphasizing the need for further research to address challenges in low-resource and unsupervised translation tasks.

## Method Summary
The authors evaluate the TOWER family of LLMs (TOWER BASE and TOWER INSTRUCT, with 7B and 13B parameters) on 132 translation tasks between 12 languages using the FLORES-200 devtest set. They compare these models to zero-shot LLAMA2 and NLLB, using COMET-22 as the main metric (with BLEU as secondary). Models are run with beam search (beam-width=5, max tokens=128). Results are analyzed by supervision type (fully supervised, partially supervised, unsupervised) and language pair.

## Key Results
- Fine-tuning improves translation quality for zero-shot languages on average, but the impact is uneven and depends on the language pairs involved.
- The best-performing model (TOWER INSTRUCT -13B) is competitive with NLLB on average but shows higher variance in quality, particularly for unsupervised language pairs.
- Off-target translations remain a problem even after fine-tuning when translating into unsupervised languages.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning improves translation quality for zero-shot languages on average.
- Mechanism: Fine-tuning injects controlled parallel data into an LLM that already has broad multilingual pretraining, enabling transfer learning to languages not directly seen in fine-tuning data.
- Core assumption: Multilingual pretraining provides generalizable linguistic representations that can be adapted via fine-tuning.
- Evidence anchors:
  - [abstract]: "we find that translation fine-tuning improves translation quality even for zero-shot languages on average"
  - [section]: "continued pre-training in the TOWER BASE models improves COMET scores compared to the zero-shot LLAMA2 results"
- Break condition: If the pretraining corpus is too narrow in language coverage, transfer will fail for unseen languages.

### Mechanism 2
- Claim: Larger models with translation fine-tuning yield higher translation quality than smaller models.
- Mechanism: Larger models have higher capacity to represent complex multilingual mappings; fine-tuning specializes them to translation tasks.
- Core assumption: Model capacity correlates with ability to handle linguistic diversity and complex cross-lingual patterns.
- Evidence anchors:
  - [abstract]: "the larger TOWER INSTRUCT -13B yields higher mean COMET scores than NLLB"
  - [section]: "TOWER INSTRUCT improves translation quality further, but only the larger TOWER INSTRUCT -13B yields higher mean COMET scores than NLLB"
- Break condition: If fine-tuning data is insufficient or noisy, the added capacity may overfit or propagate errors.

### Mechanism 3
- Claim: Fine-tuning reduces off-target translation outputs for supervised languages but not for unsupervised languages.
- Mechanism: Fine-tuning teaches the model to follow explicit instructions for translation, reducing language confusion for seen languages, but lack of exposure to unsupervised languages limits this benefit.
- Core assumption: Language identification and task-following are learned behaviors that improve with fine-tuning exposure.
- Evidence anchors:
  - [abstract]: "off-target outputs remain a problem even after fine-tuning when translating into unsupervised languages"
  - [section]: "off-target outputs remain a problem even after fine-tuning when translating into unsupervised languages"
- Break condition: If the instruction format is ambiguous or inconsistent, the model may still default to incorrect target languages.

## Foundational Learning

- Concept: Transfer learning in multilingual settings
  - Why needed here: The paper hinges on whether LLMs can transfer translation ability to unseen languages after fine-tuning.
  - Quick check question: If an LLM is pretrained on 100 languages but fine-tuned only on 6, what linguistic properties might still transfer to the other 94?

- Concept: Subword tokenization and its impact on translation quality
  - Why needed here: The paper shows tokenization differences across languages correlate with translation performance.
  - Quick check question: If Korean text is tokenized into 3x more subwords than English, how might that affect model fluency or accuracy?

- Concept: Evaluation metrics for translation quality (COMET, BLEU, ChrF)
  - Why needed here: The paper uses COMET as main metric but also reports BLEU and ChrF; understanding their strengths/weaknesses is critical.
  - Quick check question: If COMET correlates better with human judgment than BLEU, why might the authors still report BLEU?

## Architecture Onboarding

- Component map:
  - Base LLM (LLAMA2) -> Pretraining -> Fine-tuning (TOWER BASE) -> Instruction fine-tuning (TOWER INSTRUCT)
  - Evaluation pipeline: FLORES-200 test set -> COMET/BLEU/ChrF scoring
  - Comparison models: NLLB (dedicated MT) and zero-shot LLAMA2 baseline

- Critical path:
  1. Load fine-tuned TOWER model
  2. Set beam search parameters (beam-width=5, max tokens=128)
  3. Format prompt per fine-tuning instructions
  4. Run inference on FLORES-200 devtest
  5. Compute COMET score
  6. Aggregate by supervision type and language pair

- Design tradeoffs:
  - Model size vs. inference speed (13B vs. 7B)
  - Fine-tuning data diversity vs. quantity
  - Tokenization granularity vs. language coverage
  - Automatic metrics vs. human evaluation

- Failure signatures:
  - Off-target translations (wrong target language)
  - Very low COMET scores on specific language pairs (e.g., Icelandic, Korean)
  - High variance in scores across language pairs
  - Over-tokenization for certain scripts (Korean, Icelandic)

- First 3 experiments:
  1. Run zero-shot LLAMA2 on all 132 translation directions and compute COMET scores.
  2. Fine-tune TOWER BASE on parallel data and compare COMET scores to baseline.
  3. Fine-tune TOWER INSTRUCT and evaluate impact on both supervised and unsupervised language pairs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do larger LLAMA2 models (13B) consistently underperform smaller ones (7B) in zero-shot translation tasks?
- Basis in paper: [explicit] The paper observes that zero-shot LLAMA2-13B models yield lower mean COMET scores than zero-shot LLAMA2-7B models.
- Why unresolved: The paper notes this observation but does not provide a definitive explanation. It suggests the increased capacity might not benefit all language representations equally, but this is speculative.
- What evidence would resolve it: Controlled experiments varying model size while holding other factors constant, or analysis of internal model representations to identify why larger models struggle with certain languages.

### Open Question 2
- Question: What specific factors cause Icelandic to be consistently poorly translated by all models, regardless of supervision?
- Basis in paper: [explicit] The paper notes that Icelandic is consistently the worst-performing language across all models and supervision types.
- Why unresolved: While the paper mentions Icelandic's fusional grammar and unique orthography as potential factors, it does not conduct a detailed analysis to isolate the specific causes of its poor performance.
- What evidence would resolve it: Comparative analysis of Icelandic's linguistic features against other languages, focusing on morphology, syntax, and orthographic differences, and their impact on model performance.

### Open Question 3
- Question: How does the performance of instruction-tuned models (TOWER INSTRUCT) on zero-shot languages compare to traditional multilingual models (NLLB) when controlling for training data size and language diversity?
- Basis in paper: [inferred] The paper compares TOWER INSTRUCT to NLLB but does not control for training data size or language diversity, which are known factors affecting model performance.
- Why unresolved: The paper does not provide a controlled comparison that isolates the impact of instruction tuning from other factors like training data size and language coverage.
- What evidence would resolve it: Experiments training NLLB on the same amount and diversity of parallel data used for TOWER INSTRUCT, or vice versa, to compare performance while controlling for these factors.

## Limitations
- Reliance on automatic evaluation metrics without human validation may not fully capture translation quality nuances.
- Evaluation is limited to 12 languages, raising questions about scalability to truly low-resource or typologically distant languages.
- Computational efficiency and inference costs are not addressed, which are critical for practical deployment.

## Confidence
- High confidence: Fine-tuning improves translation quality for zero-shot languages on average, supported by consistent COMET score improvements across multiple model sizes and supervision types.
- Medium confidence: Larger models yield higher translation quality, as this depends on the specific language pairs and may not hold for all unsupervised directions.
- Medium confidence: Off-target translations remain problematic for unsupervised languages, though the exact prevalence and impact require further investigation.

## Next Checks
1. Conduct human evaluation studies to validate the automatic metrics' findings, particularly for language pairs showing high variance or low scores.
2. Extend the evaluation to include additional low-resource and typologically diverse languages beyond the 12 studied to assess scalability.
3. Analyze the computational efficiency and inference costs of the fine-tuned models compared to dedicated systems like NLLB to provide a more complete practical assessment.