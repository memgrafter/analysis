---
ver: rpa2
title: 'PSPU: Enhanced Positive and Unlabeled Learning by Leveraging Pseudo Supervision'
arxiv_id: '2407.06698'
source_url: https://arxiv.org/abs/2407.06698
tags:
- learning
- data
- positive
- training
- imbalanced
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Positive and Unlabeled (PU)
  learning, particularly the problem of overfitted risk estimation when labeled positive
  data distributions differ from those in unlabeled data. The proposed solution, PSPU
  (Pseudo-Supervised PU learning), introduces a two-stage framework that first trains
  a basic PU model to generate pseudo supervision from unlabeled data, then applies
  this supervision in a non-PU training phase using semi-supervised learning objectives.
---

# PSPU: Enhanced Positive and Unlabeled Learning by Leveraging Pseudo Supervision

## Quick Facts
- arXiv ID: 2407.06698
- Source URL: https://arxiv.org/abs/2407.06698
- Reference count: 26
- Key outcome: Up to 17% F1 score improvement on imbalanced CIFAR-10 and 11% on MVTecAD industrial anomaly detection

## Executive Summary
This paper addresses the challenge of overfitted risk estimation in Positive and Unlabeled (PU) learning when labeled positive data distributions differ from those in unlabeled data. The proposed PSPU framework introduces a two-stage approach that first trains a basic PU model to generate pseudo supervision from unlabeled data, then applies this supervision in a non-PU training phase using semi-supervised learning objectives. The method incorporates feature-based consistency loss to handle noisy samples and employs progressive knowledge transfer between models to avoid over-correction. Experiments demonstrate significant improvements over state-of-the-art PU learning methods across multiple benchmark datasets.

## Method Summary
PSPU is a two-stage framework that addresses PU learning's overfitted risk estimation problem. First, a basic PU net (gpu) is trained on labeled positive data (Dp) and unlabeled data (Du) using nnPU loss to learn initial positive sample identification. The gpu then generates pseudo supervision by selecting confident positive and negative samples from Du, which are Mixup-ed to create pseudo-labeled data (Dps). Second, a pseudo-supervised net (gps) is trained on the combined dataset {Dp, Dps, Du(u)} using semi-supervised learning objectives like MixMatch, incorporating a feature consistency loss to handle noisy pseudo labels. Progressive knowledge transfer gradually merges weights from gps back to gpu to prevent over-correction while maintaining learned positive sample identification.

## Key Results
- Up to 17% F1 score improvement on imbalanced CIFAR-10 compared to state-of-the-art PU methods
- 11% improvement on MVTecAD industrial anomaly detection tasks
- Consistent performance gains across MNIST, CIFAR-10, CIFAR-100, and MVTecAD datasets
- Effectiveness demonstrated in both balanced and imbalanced settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage framework mitigates overfitting in risk estimation by decoupling positive sample learning from pseudo-supervised correction.
- Mechanism: First, a basic PU net (gpu) learns from positive and unlabeled data using nnPU loss, generating pseudo supervision from confident unlabeled samples. Then, a separate network (gps) is trained using this pseudo supervision with standard SSL objectives (e.g., MixMatch), avoiding the biased risk estimation problem inherent in PU learning.
- Core assumption: The distribution of labeled positive data differs from that of unlabeled positive data, causing PU risk estimators to overfit to the labeled set.
- Evidence anchors:
  - [abstract] "This paper addresses the challenge of Positive and Unlabeled (PU) learning, particularly the problem of overfitted risk estimation when labeled positive data distributions differ from those in unlabeled data."
  - [section] "One main problem of Eq (2) is that the estimation of R−(g) and Rp(g) individually based on Du and Dp strongly relies on the assumption that the distribution Dp should be identical to that of the unlabeled positive data covered by Du."
- Break condition: If the pseudo supervision generation fails to identify confident samples correctly, the gps model will learn incorrect patterns, potentially worsening performance.

### Mechanism 2
- Claim: Progressive knowledge transfer between gpu and gps prevents over-correction while maintaining learned positive sample identification.
- Mechanism: After each epoch, weights are transferred from gps back to gpu using a weighted average: θ(e+1)gpu = λθ(e)gpu + (1-λ)θ(e)gps, where λ controls the transfer rate. This allows gpu to benefit from the pseudo-supervised learning while avoiding drastic changes that could lead to over-correction.
- Core assumption: Direct weight transfer from gps to gpu would cause over-correction of the overfitted risk estimation.
- Evidence anchors:
  - [section] "However, we empirically find that such operation may potentially lead to over-correction of the overfitted risk estimation. Therefore, we adopt a progressive knowledge transfer strategy."
  - [section] "In this way, knowledge learned from gps, which is benefited from pseudo-supervised training, can be effectively merged with the original capacity contained in gpu gradually through the iterative training."
- Break condition: If λ is set too high, over-correction may still occur; if too low, the benefits of pseudo-supervised learning may not transfer effectively.

### Mechanism 3
- Claim: Feature-based consistency loss reduces the impact of noisy pseudo labels during training.
- Mechanism: For each input sample, two augmented views are created, their features are extracted from the backbone network, and the KL divergence between these feature distributions is minimized. This encourages consistent feature representations across augmentations, making the model more robust to label noise.
- Core assumption: Noisy pseudo labels create inconsistent feature representations for the same underlying sample across different augmentations.
- Evidence anchors:
  - [section] "To further enhance the robustness against noise, we introduce an additional feature consistency loss Lcon. To be specific, for two augmented view xaug1, xaug2 of the same data x, we extract their features δxaug1, δxaug2 after all blocks from the backbone network, and minimize their KL divergence as Lcon(f) = DKL(δxaug1||δxaug2)."
- Break condition: If the consistency loss is too strong relative to other objectives, it may suppress meaningful variations in the feature space needed for discrimination.

## Foundational Learning

- Concept: Positive and Unlabeled (PU) learning and its risk estimation problem
  - Why needed here: Understanding the fundamental challenge of PU learning (overfitted risk estimation when labeled positive data distributions differ from unlabeled positive data) is crucial for grasping why PSPU's approach is necessary.
  - Quick check question: Why can't we simply use standard binary classification objectives in PU learning when we have labeled positive data?

- Concept: Semi-supervised learning objectives and Mixup
  - Why needed here: PSPU leverages pseudo-supervised training with SSL objectives like MixMatch, which require understanding of how these methods use unlabeled data for training.
  - Quick check question: How does Mixup help with noisy pseudo labels in the context of PSPU?

- Concept: Knowledge distillation and progressive transfer learning
  - Why needed here: The progressive knowledge transfer mechanism between gpu and gps is inspired by knowledge distillation techniques, requiring understanding of how to transfer knowledge between models effectively.
  - Quick check question: What is the difference between PSPU's progressive knowledge transfer and standard knowledge distillation?

## Architecture Onboarding

- Component map:
  Basic PU Net (gpu) -> Pseudo Supervision Generator -> Mixup Processor -> Pseudo-supervised Net (gps) -> Weight Transfer Module

- Critical path:
  1. Train gpu on Dp and Du using nnPU loss
  2. Generate pseudo supervision from gpu predictions on Du
  3. Create D' by combining Dp, Dps (Mixup-ed pseudo supervision), and Du(u)
  4. Train gps on D' using SSL objective + consistency loss
  5. Transfer weights from gps to gpu using progressive strategy
  6. Repeat until convergence

- Design tradeoffs:
  - Choice of SSL objective for pseudo-supervised training (MixMatch, ReMixMatch, FlexMatch) vs. simplicity and computational cost
  - Selection number ns for pseudo supervision vs. risk of including noisy samples
  - Transfer coefficient λ vs. speed of convergence and risk of over-correction
  - Feature consistency loss weight vs. model robustness to noise

- Failure signatures:
  - Rapid degradation of gpu performance during training (over-correction)
  - gps performance plateaus despite good pseudo supervision (SSL objective mismatch)
  - Both models converge to poor solutions (pseudo supervision generation failing)
  - Inconsistent results across runs (randomness in pseudo supervision selection)

- First 3 experiments:
  1. Verify the overfitted risk estimation problem by comparing nnPU performance on CIFAR-10 with limited labeled positive data vs. oracle performance on unlabeled positive data
  2. Test different selection numbers ns for pseudo supervision generation and measure impact on gps performance
  3. Compare progressive knowledge transfer (PKT) vs. direct weight inheritance from gps to gpu on extremely imbalanced CIFAR-10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of pseudo-labeled negative to positive samples (ns) in the pseudo supervision generation step, and how does this vary across different data distributions and imbalance levels?
- Basis in paper: [explicit] The paper states "We investigate the size of sampled data of a pre-defined sample ratio r, and ns = rπpnu" and compares results with r = 10%, 50%, and 100%, but notes "the performance even when we half the sampled data size (r = 50%), the results keeps similar to the counterpart with r = 100%."
- Why unresolved: The study only tested three specific ratios (10%, 50%, 100%) and found similar performance, suggesting there may be a range of acceptable values or an optimal point that wasn't precisely identified. The paper doesn't explore intermediate values or provide theoretical guidance on the optimal ratio.
- What evidence would resolve it: A systematic grid search over a wider range of sampling ratios (e.g., 5% to 95% in 5% increments) with statistical analysis of performance differences would identify the optimal ratio and whether performance plateaus within a certain range.

### Open Question 2
- Question: How does the proposed PSPU framework perform when the prior probability πp is unknown and must be estimated from the data?
- Basis in paper: [inferred] The paper states "In PU learning, the prior probability of the positive samples πp = P(y = +1) is assumed to be available" as a fundamental assumption, but this is rarely true in practical applications where πp must be estimated.
- Why unresolved: The paper explicitly assumes πp is known, which is a significant limitation since most real-world applications don't have this information available. The impact of πp estimation errors on PSPU's performance is not addressed.
- What evidence would resolve it: Experiments comparing PSPU's performance when using true πp versus estimated πp (using various estimation methods) across different datasets would quantify the sensitivity to this assumption and identify how estimation errors affect performance.

### Open Question 3
- Question: What is the theoretical relationship between the overfitted risk estimation problem and the effectiveness of the progressive knowledge transfer mechanism?
- Basis in paper: [explicit] The paper states "To further prevent the overfitted risk estimation from the basic PU net, we design a weights transfer operation between the basic PU net and pseudo-supervised net" and provides ablation results showing PKT's importance, but doesn't provide theoretical justification for why this mechanism works.
- Why unresolved: While the paper demonstrates empirically that progressive knowledge transfer improves performance, it doesn't explain the theoretical mechanism by which this prevents overfitted risk estimation or provide conditions under which this transfer is most effective.
- What evidence would resolve it: Theoretical analysis connecting the divergence between labeled and unlabeled positive data distributions to the optimal transfer coefficient λ, or empirical studies varying λ across different degrees of distribution mismatch, would establish the theoretical foundation for the PKT mechanism.

## Limitations

- Effectiveness depends on the assumption that labeled positive data distributions differ from unlabeled positive data, which may not hold in all scenarios
- Performance relies heavily on the quality of pseudo supervision generation, which could fail if the basic PU net makes incorrect predictions on unlabeled data
- Progressive knowledge transfer requires careful hyperparameter tuning (particularly λ), and improper settings could lead to over-correction or insufficient knowledge transfer

## Confidence

**High Confidence**: The core mechanism of addressing overfitted risk estimation through two-stage training and the empirical improvements shown on multiple datasets (CIFAR-10, CIFAR-100, MVTecAD).

**Medium Confidence**: The effectiveness of the progressive knowledge transfer strategy in preventing over-correction, as this is primarily supported by empirical observations rather than theoretical guarantees.

**Medium Confidence**: The feature consistency loss contribution, as while the mechanism is sound, its relative impact compared to other components needs further ablation studies.

## Next Checks

1. **Robustness to distribution mismatch**: Test PSPU on datasets where labeled positive data distributions closely match unlabeled positive data to verify the method doesn't degrade performance in the absence of the problem it's designed to solve.

2. **Ablation of progressive knowledge transfer**: Compare PSPU against a variant where direct weight inheritance is used instead of progressive transfer to quantify the exact benefit of the proposed strategy.

3. **Sensitivity analysis**: Systematically vary the transfer coefficient λ across a wide range and measure its impact on performance to establish guidelines for hyperparameter selection.