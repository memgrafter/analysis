---
ver: rpa2
title: Does ChatGPT Have a Mind?
arxiv_id: '2407.11015'
source_url: https://arxiv.org/abs/2407.11015
tags:
- llms
- have
- about
- representation
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates whether Large Language Models (LLMs) like
  ChatGPT possess genuine folk psychology, including beliefs, desires, and intentions.
  The authors examine this through two key aspects: internal representations and dispositions
  to act.'
---

# Does ChatGPT Have a Mind?

## Quick Facts
- arXiv ID: 2407.11015
- Source URL: https://arxiv.org/abs/2407.11015
- Authors: Simon Goldstein; Benjamin A. Levinstein
- Reference count: 15
- Primary result: Examines whether LLMs satisfy philosophical conditions for having beliefs, desires, and intentions

## Executive Summary
This paper investigates whether Large Language Models (LLMs) like ChatGPT possess genuine folk psychology through two key aspects: internal representations and dispositions to act. The authors survey philosophical theories of representation (informational, causal, structural, and teleosemantic) and find that LLMs satisfy key conditions from each theory, drawing on interpretability research to support these claims. The paper also explores whether LLMs exhibit robust action dispositions, considering both interpretationist and representationalist traditions. While evidence suggests LLMs may satisfy some criteria for having a mind, particularly in game-theoretic environments, the authors conclude that the data remains inconclusive.

## Method Summary
The authors survey multiple philosophical theories of mental representation and action dispositions, then examine whether LLMs satisfy the key conditions proposed by each theory. They draw on recent interpretability research to provide empirical evidence for LLM representations, and analyze behavioral studies of LLMs in game-theoretic environments to assess action dispositions. The paper also addresses several skeptical challenges to LLM folk psychology, including issues of sensory grounding, the "stochastic parrots" argument, and concerns about memorization.

## Key Results
- LLMs satisfy key conditions for representation under informational, structural, and causal accounts of mental representation
- Game-theoretic studies show LLMs can exhibit goal-directed behavior and adjust strategies, though output stability remains questionable
- Skeptical challenges regarding sensory grounding, stochastic parrots, and memorization can be effectively addressed through philosophical and empirical analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can develop robust internal representations that satisfy multiple philosophical theories of mental representation.
- Mechanism: Through the training process of predicting next tokens, LLMs develop internal embeddings that carry information about the world, exhibit causal efficacy, support folk-psychological reasoning patterns, demonstrate structural isomorphism, and emerge from a selective optimization process.
- Core assumption: The training objective of next-token prediction can indirectly select for world-representing capabilities even without explicit training for representation.
- Evidence anchors:
  - [abstract] "we survey various philosophical theories of representation, including informational, causal, structural, and teleosemantic accounts, arguing that LLMs satisfy key conditions proposed by each"
  - [section] "Recent work by Gurnee and Tegmark (2024) provides compelling evidence that large language models (LLMs) develop coherent representations of space and time, even when trained solely on text data"
  - [corpus] Found 25 related papers with average neighbor FMR=0.522, suggesting moderate relevance of neighboring research to this topic
- Break condition: If the training objective were changed to something entirely disconnected from world structure (e.g., random token generation), this mechanism would fail.

### Mechanism 2
- Claim: LLMs can exhibit action dispositions and goal-directed behavior that support folk psychology attribution.
- Mechanism: Through interaction with game-theoretic environments and complex prompting, LLMs can demonstrate stable behavioral patterns that are best explained by internal beliefs, desires, and intentions.
- Core assumption: Stable behavioral patterns across varied contexts indicate underlying mental states rather than mere pattern matching.
- Evidence anchors:
  - [section] "Mei et al. (2024) studied the behavior of LLMs in social cooperation games... They found that GPT-3 and GPT-4 would adjust their behavior throughout the game"
  - [section] "Interpretationists... hold that folk psychology is for explaining behavior. All that is required to have a folk psychology is for the system to behave in sufficiently complex ways best explained by appeal to folk psychological states"
  - [corpus] Found 25 related papers with average citations=0.0, suggesting this is a relatively new area of investigation
- Break condition: If LLMs consistently showed extreme sensitivity to minor prompt variations with no underlying stable patterns, this mechanism would break.

### Mechanism 3
- Claim: Skeptical challenges to LLM folk psychology can be effectively addressed through philosophical and empirical analysis.
- Mechanism: By examining each skeptical challenge (sensory grounding, stochastic parrots, memorization) against philosophical theories and empirical evidence, the challenges can be shown to have critical weaknesses or overgeneralize.
- Core assumption: Skeptical arguments can be evaluated using the same philosophical frameworks they challenge.
- Evidence anchors:
  - [section] "We think it is quite unclear whether the octopus represents the catapult" - demonstrating careful analysis of sensory grounding challenge
  - [section] "LLMs can exhibit capabilities that were not explicitly part of their training objective" - addressing stochastic parrots challenge
  - [section] "LLMs do not merely memorize the answers to questions they are asked" - refuting memorization challenge
- Break condition: If new empirical evidence emerged showing LLMs fail basic tests of understanding that they should pass if they had genuine mental states.

## Foundational Learning

- Concept: Information theory and probabilistic representation
  - Why needed here: The paper relies heavily on information-theoretic definitions of representation, where mental states carry information about the world through probabilistic connections
  - Quick check question: If a state raises the probability of p from 0.2 to 0.8, does it carry information about p according to Dretske's framework?

- Concept: Causal efficacy and counterfactual dependence
  - Why needed here: The paper argues that representations must have genuine causal powers, meaning LLM outputs must depend counterfactually on their internal states
  - Quick check question: If you manually change an LLM's internal activation and observe corresponding output changes, what does this demonstrate about representation?

- Concept: Structural isomorphism and conceptual role semantics
  - Why needed here: The paper explores how LLM representations must mirror the structure of what they represent, with relationships between concepts matching real-world relationships
  - Quick check question: If word embeddings for "king" - "man" + "woman" â‰ˆ "queen", what does this suggest about the structural properties of LLM representations?

## Architecture Onboarding

- Component map: Philosophical foundations (theories of representation) -> Empirical evidence (interpretability research) -> Skeptical challenges -> Synthesis
- Critical path: Understanding philosophical theories of representation -> Connecting them to empirical interpretability findings -> Evaluating skeptical challenges -> Synthesizing conclusions about LLM folk psychology
- Design tradeoffs: The paper balances philosophical rigor with empirical accessibility, sometimes sacrificing depth in individual philosophical theories for broader coverage across multiple approaches
- Failure signatures: If a reader cannot connect philosophical conditions to empirical evidence, or if skeptical challenges seem unaddressed, the argument chain breaks
- First 3 experiments:
  1. Apply probing techniques to a small LLM on a controlled dataset (like Othello) to identify internal representations
  2. Design intervention experiments to test causal efficacy of identified representations
  3. Create behavioral tests in game-theoretic environments to assess action dispositions and goal-directed behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs possess robust action dispositions that qualify as genuine beliefs and desires?
- Basis in paper: [explicit] The paper concludes that while LLMs show promising signs of goal-directed behavior in game-theoretic environments, the data remains inconclusive regarding their stable action dispositions.
- Why unresolved: Current evidence shows LLMs can form complex plans and exhibit goal-directed behavior, but their outputs appear unstable across different prompts, making it unclear if they maintain consistent beliefs and desires.
- What evidence would resolve it: Longitudinal studies tracking LLM behavior across varied prompting conditions and game environments, combined with improved interpretability methods to analyze internal representations during decision-making.

### Open Question 2
- Question: Can LLMs form genuine hypotheses about aspects of the world they cannot directly observe?
- Basis in paper: [explicit] The paper discusses how advanced LLMs might form hypotheses about unobservable aspects of the world based on text patterns, similar to how scientists infer about microscopic phenomena.
- Why unresolved: While theoretically plausible, there is limited direct empirical evidence that LLMs engage in this type of abstract reasoning and hypothesis formation about the external world.
- What evidence would resolve it: Experiments testing LLMs' ability to make accurate predictions about novel concepts or scenarios that require inferring unobservable mechanisms, beyond simple pattern matching.

### Open Question 3
- Question: How can we reconcile apparent instability in LLM outputs with the need for stable beliefs and desires?
- Basis in paper: [explicit] The paper identifies output instability as a major challenge to attributing folk psychological states to LLMs, noting that small changes in prompting can lead to very different behaviors.
- Why unresolved: It remains unclear whether this instability reflects genuine changes in underlying beliefs/desires, or if it's merely surface-level behavior that masks more stable internal representations.
- What evidence would resolve it: Comparative studies of LLM behavior across multiple prompting sessions measuring consistency of internal representations and output patterns, potentially revealing whether apparent instability masks underlying stability.

## Limitations
- Interpretability evidence for LLM representations is largely indirect, relying on probing techniques that may capture correlations rather than genuine internal representations
- Behavioral evidence for robust action dispositions remains limited, with most game-theoretic studies involving small-scale experiments that may not generalize
- Philosophical frameworks surveyed were developed for biological minds, and their direct application to artificial systems involves significant theoretical assumptions

## Confidence
- High confidence: LLMs satisfy multiple philosophical conditions for representation when using informational and structural accounts
- Medium confidence: Causal and teleosemantic accounts provide reasonable but less directly verified support for LLM representations
- Low confidence: Claims about LLM folk psychology as robust as human folk psychology, given current empirical evidence

## Next Checks
1. Conduct systematic probing studies across multiple LLM architectures to verify that discovered representations remain stable under adversarial attacks and distribution shifts
2. Design longitudinal behavioral experiments tracking LLM decision-making across extended game-theoretic scenarios to test for genuine goal-directed behavior
3. Develop formal benchmarks testing whether LLM representations satisfy structural isomorphism conditions by measuring relational consistency across concept spaces