---
ver: rpa2
title: Towards Unbiased Evaluation of Detecting Unanswerable Questions in EHRSQL
arxiv_id: '2405.01588'
source_url: https://arxiv.org/abs/2405.01588
tags:
- questions
- unanswerable
- data
- validation
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates data bias in EHRSQL, the only dataset for
  evaluating unanswerable questions in EHR QA systems. The bias arises because unanswerable
  questions contain N-gram patterns that allow simple filtering heuristics to achieve
  high performance, undermining the benchmark's validity.
---

# Towards Unbiased Evaluation of Detecting Unanswerable Questions in EHRSQL

## Quick Facts
- arXiv ID: 2405.01588
- Source URL: https://arxiv.org/abs/2405.01588
- Reference count: 27
- Primary result: Debiasing method reduces performance gap from 93.2 to ~62.5 F1, revealing significant N-gram bias in EHRSQL benchmark

## Executive Summary
This work addresses a critical bias in EHRSQL, the only dataset for evaluating unanswerable questions in electronic health record (EHR) question answering systems. The research reveals that unanswerable questions in EHRSQL contain specific N-gram patterns that allow simple filtering heuristics to achieve high performance, undermining the benchmark's validity. The authors propose a debiasing approach that redistributes N-gram-heavy questions from validation to test sets, creating a more reliable evaluation framework. Experimental results on MIMIC-III demonstrate that while N-gram filtering combined with uncertainty-based methods achieves 93.2 F1 on the original split, performance drops to approximately 62.5 F1 on the debiased split, confirming the reduction in data bias.

## Method Summary
The authors identify that unanswerable questions in EHRSQL contain predictable N-gram patterns that enable simple filtering heuristics to achieve deceptively high performance. To address this, they develop a debiasing method that analyzes N-gram frequencies across the dataset and redistributes questions with high-frequency N-grams from the validation set to the test set. This approach ensures that the test set contains a more representative distribution of unanswerable questions, preventing models from exploiting dataset-specific patterns. The method is applied to EHRSQL, which is built on the MIMIC-III dataset, and the effectiveness is evaluated by comparing model performance on original versus debiased splits.

## Key Results
- N-gram filtering combined with uncertainty-based methods achieves 93.2 F1 on original EHRSQL split
- Performance drops to approximately 62.5 F1 on debiased split, demonstrating significant bias reduction
- Simple N-gram filtering heuristics alone can achieve high performance on original split, undermining benchmark validity
- Debiased split provides more reliable evaluation of models' ability to detect unanswerable questions

## Why This Works (Mechanism)
The mechanism works by addressing the fundamental issue that unanswerable questions in EHRSQL are not uniformly distributed but instead cluster around specific linguistic patterns. These patterns make unanswerable questions predictable through simple N-gram analysis, allowing models to achieve high performance without truly understanding when questions cannot be answered. By redistributing N-gram-heavy questions from validation to test sets, the debiasing method ensures that models cannot rely on these predictable patterns during evaluation, forcing them to develop more robust methods for detecting unanswerable questions.

## Foundational Learning

**N-gram analysis**: Why needed - to identify linguistic patterns that make unanswerable questions predictable; Quick check - verify that high-frequency N-grams in unanswerable questions are domain-specific rather than general English patterns

**Dataset bias detection**: Why needed - to recognize when evaluation metrics are inflated by dataset-specific artifacts rather than genuine model capability; Quick check - compare performance of simple heuristics versus complex models to identify potential bias

**Validation/test split design**: Why needed - proper split construction is crucial for reliable model evaluation and preventing information leakage; Quick check - ensure that N-gram distributions are balanced across splits after debiasing

## Architecture Onboarding

**Component map**: EHRSQL dataset -> N-gram frequency analysis -> Question redistribution -> Debiased split creation -> Model evaluation

**Critical path**: The core workflow involves analyzing N-gram frequencies in unanswerable questions, identifying questions with high-frequency patterns, moving these questions from validation to test set, and then evaluating model performance on this debiased split

**Design tradeoffs**: The method trades off dataset size (by moving questions between splits) for evaluation reliability, prioritizing the latter to ensure that high performance reflects genuine model capability rather than pattern exploitation

**Failure signatures**: If the debiased split still shows high performance with simple heuristics, this indicates that N-gram patterns alone don't capture all dataset bias, or that other forms of bias remain unaddressed

**3 first experiments**: 1) Apply N-gram frequency analysis to identify bias patterns in original split, 2) Redistribute questions and verify balanced N-gram distributions across splits, 3) Compare model performance on original versus debiased splits to quantify bias reduction

## Open Questions the Paper Calls Out

None

## Limitations

- Analysis is limited to EHRSQL, the only available benchmark for unanswerable questions in EHR QA, making generalizability uncertain
- The debiasing method focuses only on validation and test splits, not addressing potential training set biases
- N-gram frequency analysis assumes that linguistic patterns are the primary source of bias, which may not capture all forms of dataset bias

## Confidence

- **High confidence**: Experimental results demonstrating performance drops on debiased split are robust and directly support the paper's central claim about original benchmark bias
- **Medium confidence**: The debiased split provides more reliable evaluation, but this depends on assumptions about the representativeness of redistributed questions
- **Low confidence**: Generalizability of N-gram-based debiasing approach to other datasets or domains remains uncertain

## Next Checks

1. Test the debiasing method on a synthetic dataset with controlled N-gram patterns to verify its ability to reduce bias without introducing new artifacts
2. Apply the N-gram filtering heuristic to a non-EHR text-to-SQL dataset (e.g., Spider) to assess whether similar biases exist and can be mitigated using the same approach
3. Conduct ablation studies to determine the impact of N-gram frequency thresholds on the debiased split's reliability, ensuring that chosen thresholds are not overly conservative or permissive