---
ver: rpa2
title: 'From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical
  Debugging'
arxiv_id: '2410.01215'
source_url: https://arxiv.org/abs/2410.01215
tags:
- code
- mgdebugger
- debugging
- focus
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-Granularity Debugger (MGDebugger),
  a hierarchical code debugger for LLM-generated code. MGDebugger decomposes problematic
  code into a hierarchical tree structure of subfunctions, isolating, identifying,
  and resolving bugs at various levels of granularity.
---

# From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging

## Quick Facts
- arXiv ID: 2410.01215
- Source URL: https://arxiv.org/abs/2410.01215
- Reference count: 40
- Primary result: 18.9% accuracy improvement over seed generations in HumanEval

## Executive Summary
This paper introduces Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger for LLM-generated code that decomposes problematic code into a hierarchical tree structure of subfunctions. By isolating, identifying, and resolving bugs at various levels of granularity, MGDebugger employs an LLM-simulated Python executor to trace code execution and track important variable states for accurate error pinpointing. The method demonstrates superior performance compared to existing debugging systems, achieving significant improvements in code repair success rates across different bug categories and difficulty levels.

## Method Summary
MGDebugger operates through a three-step hierarchical debugging process: First, it decomposes buggy code into a tree structure of subfunctions using LLM prompts. Second, it generates test cases for each subfunction using public test cases and LLM assistance, then executes them to identify failures. Third, it employs LLM-simulated execution with variable state tracking to debug failed subfunctions iteratively from bottom to top of the hierarchy, propagating fixes to dependent functions. The method uses CodeQwen1.5, DeepSeek-Coder-V2-Lite, and Codestral models for the debugging process.

## Key Results
- Achieved 18.9% accuracy improvement over seed generations in HumanEval benchmark
- Demonstrated 97.6% repair success rate in HumanEvalFix dataset
- Outperformed existing debugging systems across multiple bug categories and difficulty levels

## Why This Works (Mechanism)
MGDebugger leverages hierarchical decomposition to break down complex debugging tasks into manageable subproblems, reducing the cognitive load on LLMs and enabling more precise error localization. The LLM-simulated execution mechanism allows for lightweight, context-aware debugging without requiring external execution environments, while the bottom-up iterative approach ensures that fixes at lower levels propagate correctly through the function hierarchy. This combination addresses the limitations of flat debugging approaches that struggle with complex, interdependent code structures.

## Foundational Learning
**Hierarchical code decomposition**: Breaking complex functions into minimal reusable subfunctions with clear inputs/outputs - needed to enable granular debugging; quick check: verify subfunction boundaries align with logical code units.

**LLM-simulated execution**: Using language models to trace variable states and execution flow without external interpreters - needed for lightweight debugging; quick check: confirm simulated execution matches actual runtime behavior on simple test cases.

**Bottom-up debugging propagation**: Starting from leaf subfunctions and propagating fixes upward through dependencies - needed to maintain consistency across the hierarchy; quick check: validate that fixes at lower levels don't break higher-level functionality.

**Test case generation for subfunctions**: Creating comprehensive test cases for individual subfunctions using existing test cases as templates - needed to enable isolated debugging; quick check: ensure generated tests cover edge cases and boundary conditions.

## Architecture Onboarding

**Component Map**: Code Input -> Hierarchical Decomposition -> Test Case Generation -> LLM-Simulated Execution -> Subfunction Debugging -> Fix Propagation -> Code Output

**Critical Path**: The LLM-simulated execution with variable state tracking is the critical path, as it enables accurate error localization and guides the debugging process. Without effective simulation, the hierarchical approach cannot identify the root causes of failures.

**Design Tradeoffs**: MGDebugger trades off execution speed (simulated execution is slower than direct runtime debugging) for accuracy and safety (no external dependencies, works with any LLM). The hierarchical approach increases upfront complexity but reduces the search space for bug fixes.

**Failure Signatures**: Common failure modes include incorrect hierarchical decomposition (producing non-reusable subfunctions), inadequate test case generation (missing edge cases), and LLM-simulated execution errors (inaccurate variable state tracking). These manifest as persistent failures even after multiple debugging iterations.

**First Experiments**:
1. Apply MGDebugger to simple functions with known bugs to verify the hierarchical decomposition and debugging workflow.
2. Test the LLM-simulated execution mechanism on functions with complex variable states to validate accuracy.
3. Evaluate repair success rates on functions with varying bug types (syntax vs. logical errors) to assess robustness.

## Open Questions the Paper Calls Out

**Open Question 1**: Does MGDebugger's hierarchical decomposition approach maintain computational efficiency when applied to very large codebases with deeply nested function calls? The paper demonstrates effectiveness on HumanEval and MBPP benchmarks but doesn't evaluate performance on larger, more complex codebases.

**Open Question 2**: How does MGDebugger handle situations where multiple functions in the hierarchy need simultaneous debugging, and changes in one subfunction affect the correctness of others? The bottom-up approach suggests sequential debugging, but real code often has circular or complex dependencies between functions.

**Open Question 3**: Can MGDebugger's LLM-simulated execution strategy scale effectively to handle functions with complex data structures, large datasets, or performance-critical code? The simulated execution approach relies on the LLM's reasoning capabilities, which may struggle with large-scale data processing or performance optimization tasks.

## Limitations
- Lack of complete implementation details for critical components, particularly prompt templates for hierarchical decomposition and debugging
- Limited evaluation on complex, real-world codebases beyond curated benchmark datasets
- Uncertainty about scalability to functions with complex dependencies and circular references

## Confidence
**High Confidence**: The reported HumanEvalFix 97.6% repair success rate and the general effectiveness of hierarchical decomposition over flat debugging approaches.

**Medium Confidence**: The claimed 18.9% accuracy improvement over seed generations in HumanEval, though statistical significance reporting could be more detailed.

**Low Confidence**: The scalability claims and performance on real-world, complex codebases, as evaluation focuses on relatively simple benchmark functions.

## Next Checks
1. **Reproduce on alternative datasets**: Test MGDebugger on MBPP and other code generation benchmarks to verify consistent performance across different domains and difficulty levels.

2. **Analyze prompt template sensitivity**: Systematically vary the prompt templates for hierarchical decomposition and debugging to quantify their impact on repair success rates.

3. **Evaluate scalability limits**: Apply MGDebugger to progressively larger and more complex code functions (beyond the 1-2 function scope) to identify practical limitations and performance degradation points.