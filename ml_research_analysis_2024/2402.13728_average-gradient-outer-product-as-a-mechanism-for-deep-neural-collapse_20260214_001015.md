---
ver: rpa2
title: Average gradient outer product as a mechanism for deep neural collapse
arxiv_id: '2402.13728'
source_url: https://arxiv.org/abs/2402.13728
tags:
- deep
- neural
- data
- layer
- agop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the average gradient outer product (AGOP)
  as a data-dependent mechanism for deep neural collapse (DNC) in neural networks.
  The AGOP is defined as the uncentered covariance matrix of input-output gradients
  averaged over training data.
---

# Average gradient outer product as a mechanism for deep neural collapse

## Quick Facts
- arXiv ID: 2402.13728
- Source URL: https://arxiv.org/abs/2402.13728
- Authors: Daniel Beaglehole; Peter Súkeník; Marco Mondelli; Mikhail Belkin
- Reference count: 40
- Primary result: Introduces AGOP as data-dependent mechanism for DNC, demonstrated empirically and theoretically

## Executive Summary
This paper establishes the Average Gradient Outer Product (AGOP) as a data-dependent mechanism for Deep Neural Collapse (DNC). The authors introduce Deep Recursive Feature Machine (Deep RFM), which iteratively applies AGOP projection followed by random feature mapping. They demonstrate that Deep RFM exhibits DNC across multiple datasets and theoretically prove DNC occurs in high-dimensional settings through kernel learning as an implicit bias. The work connects DNC formation in standard DNNs to the right singular structure of weight matrices, which is highly correlated with AGOP through the Neural Feature Ansatz.

## Method Summary
The method introduces Deep RFM, which computes AGOP as the uncentered covariance of input-output gradients, then iteratively projects data onto this matrix and applies random feature mapping. The AGOP computation uses the uncentered covariance matrix of gradients averaged over training data. Deep RFM then minimizes a kernel ridge regression objective using this learned kernel. The approach is validated on CIFAR-10, MNIST, and SVHN using MLP, VGG, and ResNet architectures with various activation functions.

## Key Results
- AGOP computed at each layer of Deep RFM induces DNC across standard settings
- Theoretical proof of DNC in Deep RFM in asymptotic settings via kernel learning
- AGOP matrix is highly correlated with the Neural Feature Ansatz (W^T W), explaining DNC in standard DNNs
- Empirical validation shows DNC in Deep RFM across CIFAR-10, MNIST, and SVHN

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AGOP acts as data-dependent feature extractor inducing DNC through iterative projection
- Mechanism: AGOP captures uncentered covariance of input-output gradients. Iterative projection onto AGOP preserves between-class structure while collapsing within-class variability
- Core assumption: AGOP contains meaningful feature information about class structure
- Evidence anchors:
  - "AGOP is defined with respect to a learned predictor and is equal to the uncentered covariance matrix of its input-output gradients"
  - "We demonstrate empirically that DNC occurs in Deep RFM across standard settings as a consequence of the projection with the AGOP matrix"
- Break condition: If AGOP fails to capture meaningful feature structure or projection becomes unstable

### Mechanism 2
- Claim: Right singular structure of weight matrices correlates with AGOP, making AGOP projection natural for DNC in standard networks
- Mechanism: Through Neural Feature Ansatz, W^T W ≈ AGOP. Forward propagation effectively projects data onto AGOP-aligned subspaces
- Core assumption: Neural Feature Ansatz holds with high correlation in practice
- Evidence anchors:
  - "This singular structure is highly correlated with that of the AGOP"
  - "Within-class variability for DNNs trained using SGD with small initialization is primarily reduced by the application of the right singular vectors"
- Break condition: If Neural Feature Ansatz correlation breaks down (e.g., with large initialization)

### Mechanism 3
- Claim: DNC emerges as implicit bias of kernel learning when optimizing over kernel and coefficients
- Mechanism: Minimizing norm of predictor over kernel choice and coefficients biases solution toward neural collapse geometry (Y Y^T)
- Core assumption: Kernel learning problem with AGOP-based kernels has neural collapse as unique optimal solution
- Evidence anchors:
  - "DNC emerges in parameterized kernel ridge regression... DNC arises as a natural consequence of minimizing the norm of the predictor jointly over the choice of kernel function and the regression coefficients"
  - "We theoretically explain DNC in Deep RFM in an asymptotic setting and as a result of kernel learning"
- Break condition: If optimization landscape changes or other solution modes become favorable

## Foundational Learning

- Concept: Neural Collapse (NC) and Deep Neural Collapse (DNC)
  - Why needed here: Paper builds on and extends these phenomena; understanding NC1 and NC2 properties is essential
  - Quick check question: What are the four properties of neural collapse, and which two does this paper focus on?

- Concept: Kernel Methods and Reproducing Kernel Hilbert Spaces (RKHS)
  - Why needed here: Theoretical analysis relies heavily on kernel learning theory and RKHS optimization
  - Quick check question: How does kernel ridge regression relate to the optimization problem being solved in Deep RFM?

- Concept: Matrix Analysis (Singular Value Decomposition, Eigenvalue Decomposition)
  - Why needed here: Analysis of weight matrices and their singular structures is central to understanding AGOP connection
  - Quick check question: What is the relationship between W^T W and the singular value decomposition of W?

## Architecture Onboarding

- Component map: Data → AGOP computation → Iterative projection → Random feature mapping → Kernel learning → DNC emergence
- Critical path: The system processes data through AGOP-based feature extraction, random feature mapping, and kernel learning to induce DNC
- Design tradeoffs: Balances data-dependent feature learning (through AGOP) vs. data-agnostic approaches, and computational cost of AGOP computation vs. accuracy of DNC induction
- Failure signatures: (1) Poor AGOP-NNM correlation indicates Neural Feature Ansatz failure, (2) Slow or absent DNC formation suggests AGOP isn't capturing relevant structure, (3) Numerical instability in matrix operations
- First 3 experiments:
  1. Verify AGOP computation on simple synthetic data with known class structure
  2. Test Deep RFM on a small dataset (e.g., MNIST subset) and measure DNC metrics
  3. Compare NFM-NNM correlation in trained networks vs. random initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the AGOP mechanism extend to non-classification tasks like regression or unsupervised learning?
- Basis in paper: [explicit] The paper focuses exclusively on classification tasks and DNC, which is specific to classification
- Why unresolved: The paper establishes AGOP as a mechanism for DNC in classification but doesn't explore its role in other learning paradigms where DNC isn't defined
- What evidence would resolve it: Experiments showing AGOP's effect on feature collapse or representation structure in regression tasks, or theoretical analysis of AGOP in unsupervised settings

### Open Question 2
- Question: What is the precise mathematical relationship between the Neural Feature Ansatz correlation threshold and the strength of within-class collapse?
- Basis in paper: [inferred] The paper notes NFA correlations of 0.74±0.17 and 0.74±0.13 for ResNet and MLP respectively, but doesn't establish a quantitative relationship between correlation strength and collapse magnitude
- Why unresolved: The paper demonstrates correlation between NFA and DNC but doesn't provide a formal characterization of how correlation strength translates to collapse strength
- What evidence would resolve it: A mathematical derivation or extensive empirical study showing the functional relationship between NFA correlation values and DNC metrics across multiple architectures

### Open Question 3
- Question: How does initialization scale affect the AGOP mechanism's ability to induce DNC in deep networks?
- Basis in paper: [explicit] The paper mentions "small initialization" as a condition for NFA to hold, but doesn't systematically study how initialization scale affects AGOP's DNC-inducing capability
- Why unresolved: While the paper uses small initialization in experiments and notes its importance for NFA, it doesn't explore the critical scale at which AGOP loses its DNC-inducing properties
- What evidence would resolve it: Experiments varying initialization scale systematically while measuring both NFA correlation and DNC metrics, or theoretical analysis of how initialization scale affects AGOP-weight matrix alignment

## Limitations
- Theoretical proof relies on asymptotic settings that may not fully capture finite-sample behavior
- Connection between AGOP-based kernel learning and standard SGD-trained networks remains correlational rather than causal
- Neural Feature Ansatz correlation, while empirically observed, lacks rigorous theoretical justification

## Confidence
- **High confidence**: Empirical demonstration of DNC in Deep RFM across CIFAR-10, MNIST, and SVHN; theoretical proof of DNC in asymptotic settings
- **Medium confidence**: The AGOP-NNM correlation and its role in standard DNN collapse; kernel learning as implicit bias mechanism
- **Low confidence**: Universal applicability across all network architectures and activation functions

## Next Checks
1. Test Deep RFM on larger, more diverse datasets (e.g., ImageNet) to verify scalability of the AGOP mechanism
2. Conduct ablation studies varying initialization scales to quantify the impact on Neural Feature Ansatz correlation
3. Implement and compare alternative feature extraction methods (e.g., random projections) to isolate the specific contribution of AGOP to DNC formation