---
ver: rpa2
title: Counterfactual Image Editing
arxiv_id: '2403.09683'
source_url: https://arxiv.org/abs/2403.09683
tags:
- counterfactual
- image
- causal
- generative
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses counterfactual image editing by modeling
  causal relationships among latent generative factors. The authors formalize the
  problem using augmented structural causal models (ASCMS), proving two fundamental
  impossibility results: counterfactual editing is impossible from i.i.d.'
---

# Counterfactual Image Editing

## Quick Facts
- arXiv ID: 2403.09683
- Source URL: https://arxiv.org/abs/2403.09683
- Authors: Yushu Pan; Elias Bareinboim
- Reference count: 40
- Primary result: Demonstrates that counterfactual image editing is fundamentally impossible from observational data alone, even with known causal relationships, and introduces counterfactual-consistent estimators that approximate non-identifiable distributions while preserving user-specified invariances.

## Executive Summary
This paper formalizes counterfactual image editing using augmented structural causal models (ASCMS) to model causal relationships among latent generative factors. The authors prove two fundamental impossibility results: counterfactual editing cannot be achieved from i.i.d. samples alone, and even with causal relationships, no guarantees exist for output quality. To address this, they introduce counterfactual-consistent (Ctf-consistent) estimators that approximate non-identifiable counterfactual distributions while preserving user-specified features across factual and counterfactual worlds. An efficient algorithm (ANCMs) leverages neural causal models to generate samples from these estimators, demonstrating causally consistent edits on synthetic and real datasets.

## Method Summary
The framework addresses counterfactual image editing by first formalizing the task using augmented structural causal models that model causal relationships between latent generative factors and images. It demonstrates that counterfactual distributions are non-identifiable from observational data alone, leading to the development of counterfactual-consistent estimators that preserve user-specified features. The ANCM algorithm implements this approach by training G-constrained neural causal models to fit the observational distribution, then generating counterfactual images through intervention on specified factors while maintaining causal invariances. The method uses a VAE-ANCM architecture with a decoder as G-constrained NCM and encoder as inference network, trained to maximize both observational likelihood and conditional factor likelihood.

## Key Results
- Proves two impossibility results: counterfactual editing is impossible from i.i.d. samples alone, and no guarantees exist for output quality even with known causal relationships
- Introduces counterfactual-consistent estimators that approximate non-identifiable distributions while preserving user-specified invariances
- Demonstrates ANCM algorithm generates causally consistent edits on synthetic (Colored MNIST with Bars) and real (CelebA-HQ) datasets
- Shows generated counterfactual images maintain relevant causal relationships while reflecting true causal effects among features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework leverages augmented structural causal models (ASCMS) to formalize counterfactual image editing, enabling modeling of causal relationships among latent generative factors and images.
- Mechanism: ASCMS extend standard SCMs by including the image generation step where observed generative factors and unobserved factors combine to produce the image, capturing causal relationships between latent factors and the image.
- Core assumption: The image generation mechanism f_I is invertible with respect to observed generative factors V.
- Evidence anchors: [abstract] formalizes the task using ASCMS; [section] defines ASCM with invertible f_I mapping from V ∪ UI to I.

### Mechanism 2
- Claim: The framework demonstrates that image counterfactual distributions are non-identifiable from observational data alone, even with causal diagrams.
- Mechanism: Multiple generative models can produce identical observational distributions but yield different counterfactual images, establishing that counterfactual distributions cannot be uniquely determined from observational data.
- Core assumption: Non-identifiability holds even when causal relationships between generative factors and images are given.
- Evidence anchors: [abstract] shows counterfactual editing is impossible from i.i.d. samples alone; [section] proves no image counterfactual distribution is identifiable given causal diagram and observational distribution.

### Mechanism 3
- Claim: The framework introduces ANCM algorithm that leverages neural causal models to generate samples from counterfactual-consistent estimators.
- Mechanism: ANCMs train G-constrained neural causal models to fit observational distribution and sample images with counterfactual counterparts, guaranteeing counterfactually consistent estimation through graphical constraints.
- Core assumption: Graphical constraints encoded in causal diagram are sufficient to ensure counterfactually consistent estimation.
- Evidence anchors: [abstract] develops efficient algorithm leveraging neural causal models; [section] proves P cM(I, I′_x′) is Ctf-consistent estimator when cM ∈ ΩI(G) and P cM(V, I) = P (V, I).

## Foundational Learning

- Concept: Augmented Structural Causal Models (ASCMS)
  - Why needed here: Provide formal framework for modeling causal relationships between latent generative factors and images, essential for formalizing counterfactual image editing tasks.
  - Quick check question: How do ASCMS extend standard SCMs to include the image generation step?

- Concept: Counterfactual Consistency
  - Why needed here: Ensures generated counterfactual images preserve relevant causal invariances and relationships across factual and counterfactual worlds, crucial for realistic and causally grounded edits.
  - Quick check question: What are key properties of counterfactual-consistent estimators, and how do they differ from traditional estimators?

- Concept: Neural Causal Models (NCMs)
  - Why needed here: Provide neural network architecture for implementing counterfactual-consistent estimators, enabling efficient training and sampling of high-quality counterfactual images.
  - Quick check question: How do NCMs leverage causal constraints encoded in causal diagram to ensure counterfactually consistent estimation?

## Architecture Onboarding

- Component map: Augmented Structural Causal Models (ASCMS) -> Counterfactual-Consistent (Ctf-Consistent) Estimators -> Neural Causal Models (NCMs) -> Causal Diagram

- Critical path: 1) Formalize counterfactual image editing using ASCMS, 2) Demonstrate non-identifiability of counterfactual distributions, 3) Develop Ctf-consistent estimators, 4) Implement ANCM algorithm using NCMs, 5) Evaluate effectiveness on datasets

- Design tradeoffs: Generality vs. Identifiability (trades arbitrary causal relationship modeling for identifiability, requiring Ctf-consistent estimators), Computational Complexity vs. Quality (NCMs and diffusion models enable high quality but may increase computational cost)

- Failure signatures: Poor reconstruction of observational distribution (generated counterfactual images may not be causally consistent), Violation of causal invariances (generated images may be unrealistic or incorrect)

- First 3 experiments: 1) Implement and train ANCMs on Colored MNIST with Bars dataset, evaluating counterfactual consistency, 2) Compare ANCMs against CVAE, CGN, DEAR on Celeba-HQ dataset, focusing on preservation of causal relationships, 3) Analyze impact of different care sets W on counterfactual consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can counterfactual image editing be performed when only unlabeled observational data is available?
- Basis in paper: [inferred] The paper assumes labeled generative factors are available for training. Section 4 discusses relaxing identifiability problem but does not address unlabeled data.
- Why unresolved: The theoretical results and algorithms rely on labeled data (V) being available. No method is proposed for handling unlabeled observational data.
- What evidence would resolve it: A method that can perform counterfactual image editing using only unlabeled observational data (images without associated labels) and still provide Ctf-consistent estimators.

### Open Question 2
- Question: How can efficiency and scalability of inference methods be improved for high-dimensional image counterfactual editing?
- Basis in paper: [explicit] Section 6 mentions enhancing efficiency and scalability of inferences as important area for future research.
- Why unresolved: Current ANCM algorithm uses two-stage VAE-DDPM approach which may be computationally expensive. No computational complexity comparison is provided.
- What evidence would resolve it: Benchmarks comparing computational efficiency of different inference methods, or more efficient algorithm handling high-dimensional images with reduced computational cost.

### Open Question 3
- Question: How sensitive are Ctf-consistent estimators to errors in assumed causal diagram?
- Basis in paper: [explicit] Section D discusses what happens if assumed causal diagram is incomplete or wrong, but only mentions "hallucination" effects.
- Why unresolved: No quantitative analysis of how errors in causal diagram affect quality of counterfactual edits is provided. Paper only shows method can still provide Ctf-consistent estimators.
- What evidence would resolve it: Experiments showing how different types of errors in causal diagram (missing edges, wrong edge directions, etc.) affect quality of counterfactual edits, or method to quantify uncertainty in causal diagram.

## Limitations
- Non-identifiability of counterfactual distributions remains fundamental barrier - multiple generative models can produce identical observational data while yielding different counterfactual outcomes
- Ctf-consistent estimators provide approximations rather than exact solutions, potentially sacrificing some counterfactual accuracy for practical feasibility
- Implementation requires specific neural architectures and hyperparameters that are not fully specified, creating barriers to exact reproduction

## Confidence
- **High confidence**: Impossibility results are mathematically rigorous and well-supported
- **Medium confidence**: ANCM algorithm implementation and experimental results are plausible but depend on unspecified architectural details
- **Medium confidence**: Theoretical framework for counterfactual-consistent estimators is sound, but practical approximation quality may vary across datasets

## Next Checks
1. Implement Colored MNIST with Bars synthetic dataset generator and verify feature counterfactual queries match theoretical optimal bounds
2. Train simplified version of ANCM with specified causal diagram on MNIST to test if counterfactual consistency can be achieved without diffusion refinement
3. Compare counterfactual editing results when using different care sets W to validate framework correctly preserves user-specified invariances while modifying causal effects