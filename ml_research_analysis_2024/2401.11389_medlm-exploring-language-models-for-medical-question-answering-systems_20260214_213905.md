---
ver: rpa2
title: 'MedLM: Exploring Language Models for Medical Question Answering Systems'
arxiv_id: '2401.11389'
source_url: https://arxiv.org/abs/2401.11389
tags:
- medical
- language
- question
- performance
- work
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates Large Language Models for medical question
  answering, addressing the challenge of reliably automating healthcare Q&A. The authors
  compare base LLMs (GPT-2/3.5, Bloom, T5), fine-tuned distilled versions, and in-context
  learning with static and dynamic prompting.
---

# MedLM: Exploring Language Models for Medical Question Answering Systems

## Quick Facts
- arXiv ID: 2401.11389
- Source URL: https://arxiv.org/abs/2401.11389
- Authors: Niraj Yagnik; Jay Jhaveri; Vivek Sharma; Gabriel Pila
- Reference count: 6
- Primary result: GPT-3.5 with static prompts performs best on BLEU/ROUGE, but fine-tuned models achieve comparable scores with better data; dynamic prompting reduces hallucination

## Executive Summary
This study investigates Large Language Models for medical question answering, addressing the challenge of reliably automating healthcare Q&A. The authors compare base LLMs (GPT-2/3.5, Bloom, T5), fine-tuned distilled versions, and in-context learning with static and dynamic prompting. Key findings show that GPT-3.5 with static prompts performs best on BLEU and ROUGE metrics, but fine-tuned Bloom and T5 achieve comparable scores when augmented with additional data. Dynamic prompting, especially type-specific retrieval, improves output quality and reduces hallucination. Qualitative surveys reveal user and doctor preference for GPT-generated answers over ground truth, while fine-tuned models exhibit higher hallucination rates. Overall, decoder-only models excel, and prompting strategies and data quality critically impact performance.

## Method Summary
The authors evaluate LLMs for medical Q&A using two datasets (MedQuAD and iCliniq) containing filtered question-answer pairs. They employ three main approaches: fine-tuning distilled models (T5, Bloom, GPT-2) on concatenated Q&A pairs with truncation; testing base LLMs (GPT-2, GPT-3.5, Bloom, T5) on test sets; and in-context learning with static prompts (random QA pairs) and dynamic prompts (cosine similarity with question-type filtering using BERT classifier and InstructOR Embedder). Performance is measured using BLEU and ROUGE metrics, supplemented by human surveys on answer comprehensibility, factual accuracy, and hallucination propensity.

## Key Results
- GPT-3.5 with static prompts achieves the highest BLEU/ROUGE scores among all tested approaches
- Fine-tuned Bloom and T5 models perform comparably to GPT-3.5 when trained on higher quality, augmented datasets
- Dynamic prompting with question-type specific retrieval significantly reduces hallucination and improves output quality
- Human surveys show user and doctor preference for GPT-generated answers despite lower metric scores
- Decoder-only models (GPT-2/3.5) consistently outperform encoder-decoder models (T5) on medical Q&A tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoder-only models (GPT-2/3.5) outperform encoder-decoder models (T5) on medical Q&A tasks.
- Mechanism: Decoder-only models leverage autoregressive generation, enabling better fluency and coherence in open-ended answer generation.
- Core assumption: The task requires generating free-form answers without strict reliance on input context, favoring generative capacity over encoding precision.
- Evidence anchors:
  - [section] "Overall, our research sheds light on the strengths and weaknesses of various models in the context of Medical QnA. The combination of quantitative and qualitative evaluations provides a comprehensive understanding of their performance and highlights avenues for future improvements."
  - [section] "Our analysis indicates that GPT 3.5 with static prompts yields the best results. Notably, the fine-tuned models do not surpass these scores but achieve comparable performance."

### Mechanism 2
- Claim: Dynamic prompting improves model performance by selecting contextually relevant examples.
- Mechanism: Embeddings from training questions are used to compute cosine similarity with test questions, enabling retrieval of semantically similar examples for in-context learning.
- Core assumption: The training set contains sufficient question-answer pairs covering diverse medical topics to provide useful prompts.
- Evidence anchors:
  - [section] "We then utilized these two questions, along with their corresponding answers, as prompts for performing in-context learning with large language models (LLMs)."
  - [section] "As we will discuss in the Results section, this Question-Type Specific Dynamic Prompting approach yielded a significant performance boost compared to the Vanilla Dynamic Prompting approach."

### Mechanism 3
- Claim: Fine-tuning on domain-specific datasets improves model performance on medical Q&A.
- Mechanism: Fine-tuning adapts pre-trained models to the medical domain, enhancing their ability to generate accurate and relevant answers.
- Core assumption: The domain-specific dataset contains high-quality, diverse question-answer pairs representative of real medical inquiries.
- Evidence anchors:
  - [section] "By following this process, we aimed to optimize the generative modelsâ€™ performance by fine-tuning them on the concatenated question-answer pairs, standardizing input length, and utilizing appropriate tokenization techniques for each model architecture."
  - [section] "The Bloom model performs almost as well as GPT 3.5 did in the previous analysis Table 2. This finding suggests that incorporating more data and higher quality data improves the fine-tuning process."

## Foundational Learning

- Concept: Tokenization and truncation strategies
  - Why needed here: Standardizing input length for fine-tuning and inference is crucial for efficient model training and consistent performance.
  - Quick check question: Why was the input length truncated to 300 tokens for MedQuAD and 150 tokens for Icliniq datasets?

- Concept: Embeddings and cosine similarity for dynamic prompting
  - Why needed here: Embeddings capture semantic meaning of questions, enabling retrieval of relevant prompts for in-context learning.
  - Quick check question: How does the InstructOR Embedder differ from classic embedders in terms of input requirements?

- Concept: BLEU and ROUGE metrics for evaluation
  - Why needed here: These metrics assess the structural accuracy and semantic relevance of generated answers, providing quantitative performance measures.
  - Quick check question: Why did the authors find these metrics unreliable for generative QnA tasks, and what alternative did they propose?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Prompting -> Evaluation
- Critical path: 1. Preprocess datasets (tokenize, truncate, concatenate) 2. Fine-tune distilled models on preprocessed data 3. Test base LLMs on preprocessed data 4. Implement dynamic prompting strategies 5. Evaluate model performance using metrics and human surveys
- Design tradeoffs: Model size vs. computational efficiency; data quality vs. quantity; static vs. dynamic prompting
- Failure signatures: High hallucination rates; poor BLEU/ROUGE scores; inconsistent performance across question types
- First 3 experiments: 1. Fine-tune a distilled model (e.g., T5) on the MedQuAD dataset and evaluate performance using BLEU and ROUGE metrics. 2. Implement static prompting with a base LLM (e.g., GPT-3.5) and compare performance to the fine-tuned model. 3. Develop a dynamic prompting strategy using embeddings and cosine similarity, and assess its impact on model performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different question types affect the performance of various language models in medical QnA tasks?
- Basis in paper: [explicit] The authors mention that the Medquad training set included question types associated with each question, such as "Symptoms," "Treatments," and "Information," and they developed a Question-Type Specific Dynamic Prompting approach based on this observation.
- Why unresolved: The paper notes the imbalance in question types but leaves handling that for future work, indicating that a comprehensive analysis of how different question types impact model performance is still needed.
- What evidence would resolve it: A detailed analysis of model performance across all question types, including balanced datasets and evaluation metrics, would provide insights into the strengths and weaknesses of different models for various question types.

### Open Question 2
- Question: What are the most effective strategies for reducing hallucination in fine-tuned language models for medical QnA?
- Basis in paper: [explicit] The authors note that hallucination is prevalent in the answers generated, especially in the case of fine-tuned models of T5, BLOOM, and GPT-2, and suggest that proper data augmentation could resolve this issue.
- Why unresolved: The paper acknowledges the problem of hallucination but does not provide a definitive solution or evaluate the effectiveness of different data augmentation techniques in reducing hallucination.
- What evidence would resolve it: Experimental results comparing the hallucination rates of models trained with various data augmentation strategies would identify the most effective methods for reducing hallucination in fine-tuned models.

### Open Question 3
- Question: How do advanced evaluation metrics compare to traditional metrics like BLEU and ROUGE in assessing the quality of generated medical QnA responses?
- Basis in paper: [explicit] The authors highlight that traditional metrics like BLEU and ROUGE are unreliable for evaluating generative QnA tasks and emphasize the need for new metrics that better capture the quality of generated responses.
- Why unresolved: The paper does not explore or propose specific alternative evaluation metrics, leaving the question of how to effectively assess the quality of generated medical QnA responses unanswered.
- What evidence would resolve it: Comparative studies of model performance using both traditional and proposed advanced evaluation metrics would demonstrate the effectiveness of new metrics in capturing the nuances of generated medical QnA responses.

## Limitations
- Human surveys are limited in scale and may not represent broader user populations
- Fine-tuning process details (hyperparameters, parameter-efficient methods) are not fully specified
- Dynamic prompting depends on training set quality and coverage of medical domains
- BLEU/ROUGE metrics may not capture answer quality for generative tasks, as shown by human preference for GPT answers despite lower metric scores

## Confidence
- High: Decoder-only models (GPT-2/3.5) outperform encoder-decoder models on medical Q&A tasks; fine-tuning improves performance when augmented with high-quality data.
- Medium: Dynamic prompting improves performance by selecting contextually relevant examples; current evaluation metrics (BLEU/ROUGE) are unreliable for generative Q&A.
- Low: User and doctor preference for GPT-generated answers over ground truth; fine-tuned models exhibit higher hallucination rates.

## Next Checks
1. Conduct a larger-scale human evaluation with diverse medical professionals to validate the preference for GPT-generated answers and assess hallucination rates across different question types.
2. Compare the performance of fine-tuned models using parameter-efficient methods (e.g., LoRA) versus full fine-tuning to determine the optimal approach for domain adaptation.
3. Develop and test alternative evaluation metrics that better capture the quality of generative answers in medical Q&A, such as semantic similarity measures or domain-specific criteria.