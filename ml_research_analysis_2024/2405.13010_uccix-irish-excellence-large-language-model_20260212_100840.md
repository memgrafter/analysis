---
ver: rpa2
title: 'UCCIX: Irish-eXcellence Large Language Model'
arxiv_id: '2405.13010'
source_url: https://arxiv.org/abs/2405.13010
tags:
- irish
- language
- languages
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UCCIX, the first Irish-based large language
  model, addressing the challenge of developing LLMs for extremely low-resource languages
  like Irish. The authors propose a novel framework for continued pre-training of
  LLMs specifically adapted for such languages, requiring only a fraction of the textual
  data typically needed according to scaling laws.
---

# UCCIX: Irish-eXcellence Large Language Model

## Quick Facts
- **arXiv ID**: 2405.13010
- **Source URL**: https://arxiv.org/abs/2405.13010
- **Reference count**: 33
- **Primary result**: First Irish-based LLM outperforming much larger models by up to 12% on Irish language tasks

## Executive Summary
This paper introduces UCCIX, the first Irish-based large language model, addressing the challenge of developing LLMs for extremely low-resource languages like Irish. The authors propose a novel framework for continued pre-training of LLMs specifically adapted for such languages, requiring only a fraction of the textual data typically needed according to scaling laws. Starting from Llama 2-13B, the model is trained on a carefully curated Irish dataset and employs techniques like parallel data scheduling and tokenizer vocabulary expansion. UCCIX outperforms much larger models on Irish language tasks, achieving up to 12% performance improvement. The paper also contributes comprehensive Irish benchmarking datasets, including IrishQA and an Irish version of MT-bench, enabling rigorous evaluation and facilitating future research in Irish LLM systems.

## Method Summary
UCCIX employs a continued pre-training approach starting from Llama 2-13B, using parallel English-Irish data followed by monolingual Irish data to establish linguistic connections and refine proficiency. The model incorporates an expanded vocabulary with 10k additional Irish tokens (from 32k to 42k total) to improve generation efficiency and quality. After pre-training, supervised instruction fine-tuning aligns the model with human preferences. The training uses carefully curated Irish datasets including CulturaX-ga, Glot500-ga, Gaparacrawl, Gawiki2, and Corpora Irish, with comprehensive evaluation on newly developed benchmarks.

## Key Results
- UCCIX achieves up to 12% performance improvement over much larger models on Irish language tasks
- The model demonstrates strong performance on multiple Irish-specific benchmarks including IrishQA, MT-bench (Irish version), Cloze Test, SIB-200, and gaHealth
- Tokenizer vocabulary expansion to 42k tokens (including 10k Irish tokens) significantly improves generation efficiency and quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The parallel data scheduling in the continued pre-training phase improves Irish language understanding.
- Mechanism: The model is first trained on parallel English-Irish data to establish linguistic connections between the two languages, then on larger mono-Irish data to refine proficiency.
- Core assumption: Linguistic connections between English and Irish can be transferred during continued pre-training.
- Evidence anchors:
  - [section] "We employ a tailored data scheduler to feed parallel English-Irish data first, facilitating the establishment of linguistic connections between the two languages."
  - [abstract] "Our model, based on Llama 2-13B, outperforms much larger models on Irish language tasks with up to 12% performance improvement, showcasing the effectiveness and efficiency of our approach."
- Break condition: If the parallel data quality is poor or the linguistic differences between English and Irish are too significant, the transfer may not be effective.

### Mechanism 2
- Claim: The expanded vocabulary with native Irish tokens improves generation efficiency and quality.
- Mechanism: The original vocabulary of 32k tokens is expanded to include 10k Irish tokens, allowing for more efficient and meaningful representation of Irish text.
- Core assumption: Including native Irish tokens in the vocabulary improves the model's ability to generate coherent Irish text.
- Evidence anchors:
  - [section] "The expanded vocabulary provides a more meaningful representation of Irish text and enables bilingual capabilities in the final LLM."
  - [abstract] "UCCIX outperforms much larger models on Irish language tasks with up to 12% performance improvement, showcasing the effectiveness and efficiency of our approach."
- Break condition: If the expanded vocabulary introduces too many tokens, it may lead to increased memory usage and slower inference times.

### Mechanism 3
- Claim: The supervised instruction fine-tuning aligns the model with human preferences and improves instruction-following capability.
- Mechanism: After pre-training, the model is fine-tuned on supervised instruction data to enhance its ability to follow human instructions effectively.
- Core assumption: Fine-tuning on instruction data improves the model's ability to understand and execute human instructions.
- Evidence anchors:
  - [section] "To align the model more closely with human preferences, we apply Supervised Instruction Fine-tuning [18, 21]."
  - [abstract] "UCCIX outperforms much larger models on Irish language tasks with up to 12% performance improvement, showcasing the effectiveness and efficiency of our approach."
- Break condition: If the instruction data is not representative of real-world use cases, the fine-tuning may not generalize well.

## Foundational Learning

- Concept: Continued pre-training
  - Why needed here: The amount of mono-Irish data is insufficient for pre-training LLMs from scratch, so continued pre-training is used to adapt existing LLMs to the Irish language.
  - Quick check question: What is the main difference between pre-training and continued pre-training?

- Concept: Data scheduling
  - Why needed here: Parallel data is scheduled first to establish linguistic connections between English and Irish, followed by mono-Irish data to refine proficiency.
  - Quick check question: Why is it important to schedule parallel data before mono-Irish data in the continued pre-training phase?

- Concept: Tokenizer vocabulary expansion
  - Why needed here: The original vocabulary is expanded to include native Irish tokens, improving generation efficiency and quality.
  - Quick check question: How does expanding the tokenizer vocabulary with native Irish tokens benefit the model?

## Architecture Onboarding

- Component map: Base LLM (Llama 2-13B) → Expanded tokenizer vocabulary (42k tokens) → Continued pre-training with parallel and mono-Irish data → Supervised instruction fine-tuning → Evaluation

- Critical path: Data collection and preprocessing → Continued pre-training with parallel and mono-Irish data → Tokenizer vocabulary expansion → Supervised instruction fine-tuning → Evaluation and benchmarking

- Design tradeoffs: The expanded vocabulary improves Irish generation but may increase memory usage and inference time. The parallel data scheduling improves transfer but relies on high-quality parallel data.

- Failure signatures: Poor performance on Irish tasks may indicate issues with data quality, tokenizer expansion, or fine-tuning. Catastrophic forgetting on English tasks suggests a need for better balancing of language-specific knowledge.

- First 3 experiments:
  1. Evaluate the base Llama 2-13B model on Irish tasks to establish a baseline.
  2. Continue pre-training the base model on parallel English-Irish data and evaluate performance.
  3. Expand the tokenizer vocabulary with native Irish tokens and assess the impact on generation efficiency and quality.

## Open Questions the Paper Calls Out

The paper identifies several areas for future research and exploration:

1. **Cross-linguistic transfer capabilities**: The authors note that their framework can be adapted for other low-resource languages, but do not explore UCCIX's performance on languages other than Irish or the factors influencing cross-linguistic transfer.

2. **Optimal balance of parallel vs. monolingual data**: While the paper demonstrates the usefulness of parallel data, it does not investigate the optimal ratio of parallel to monolingual data for different low-resource languages or its impact on model characteristics.

3. **Catastrophic forgetting mitigation**: The authors observe instances of catastrophic forgetting when benchmarked on English-related datasets, acknowledging this as an area for improvement, but do not propose or evaluate solutions to maintain performance on high-resource languages.

## Limitations

- Evaluation framework relies on internally developed benchmarks (IrishQA, Irish MT-bench, SIB-200 subset) with no external validation on established multilingual datasets
- Parallel data quality and size remain unspecified, limiting assessment of the foundation for claimed linguistic transfer
- Tokenizer expansion process lacks detailed documentation regarding token selection criteria and integration methodology
- No comparison with external benchmarks or other low-resource language adaptation approaches

## Confidence

**High Confidence** - The foundational methodology for continued pre-training of LLMs for low-resource languages is well-established in the literature.

**Medium Confidence** - The claim of outperforming much larger models by up to 12% is supported by presented results but lacks broader validation.

**Low Confidence** - Specific mechanisms of parallel data scheduling effectiveness and tokenizer vocabulary expansion impact cannot be fully validated due to insufficient methodological detail.

## Next Checks

1. Evaluate UCCIX on established multilingual benchmarks like XTREME and FLORES to verify performance claims against standardized, externally validated datasets beyond internally created Irish-specific tests.

2. Conduct systematic ablation experiments removing each key component (parallel data scheduling, tokenizer expansion, supervised fine-tuning) to quantify the individual contribution of each mechanism to overall performance improvements.

3. Test the model on closely related Celtic languages (Scottish Gaelic, Welsh) to assess whether architectural innovations provide benefits beyond the specific Irish language context, establishing broader applicability of the approach.