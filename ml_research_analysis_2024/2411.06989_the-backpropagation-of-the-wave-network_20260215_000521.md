---
ver: rpa2
title: The Backpropagation of the Wave Network
arxiv_id: '2411.06989'
source_url: https://arxiv.org/abs/2411.06989
tags:
- token
- input
- gradient
- wave
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the Wave Network, a novel token representation
  method that uses complex vector token representations to capture both global and
  local semantics of input text through wave-inspired operations. The method represents
  each token using a complex vector comprising a magnitude component for global semantics
  and a phase component for relationships between tokens and global context.
---

# The Backpropagation of the Wave Network

## Quick Facts
- arXiv ID: 2411.06989
- Source URL: https://arxiv.org/abs/2411.06989
- Reference count: 25
- Primary result: Wave network achieves 90.36% accuracy on AG News using 0.30 GB VRAM vs BERT's 94.64% with 1.28 GB VRAM

## Executive Summary
This paper analyzes the Wave Network, a novel token representation method using complex vector representations to capture both global and local semantics of input text. The method employs wave-inspired operations (interference and modulation) to update token representations, achieving significant reductions in VRAM usage and training time compared to BERT while maintaining competitive accuracy. The study investigates convergence behavior, backpropagation characteristics, and embedding independence within the Token2Wave framework.

## Method Summary
The Wave Network represents each token using a complex vector with magnitude (capturing global semantics) and phase (encoding relationships between tokens and global context) components. Wave-inspired operations including interference (complex addition) and modulation (complex multiplication) replace pairwise attention computations, reducing computational complexity from O(n²·d) to O(n·d²). The model converts real-valued embeddings to complex vectors, applies wave operations, then converts back for classification. Experiments on AG News dataset demonstrate 90.36% accuracy using only 0.30 GB VRAM and 146.90 seconds training time, compared to BERT's 94.64% accuracy using 1.28 GB VRAM and 1034.99 seconds.

## Key Results
- Wave network achieves 90.36% accuracy on AG News vs BERT's 94.64%
- VRAM usage reduced from 1.28 GB (BERT) to 0.30 GB (Wave network)
- Training time reduced from 1034.99 seconds to 146.90 seconds
- Computational complexity reduced from O(n²·d) to O(n·d²)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wave network's complex vector representation enables efficient global-local semantic encoding with reduced VRAM and training time.
- Mechanism: By using complex vectors, each token encodes global semantics in magnitude and local semantics in phase. This allows direct propagation of global context through magnitude and local token relationships through phase, reducing the need for pairwise attention computations.
- Core assumption: The polar coordinate representation (magnitude, phase) effectively captures both global and local semantics better than standard real-valued embeddings.
- Evidence anchors:
  - [abstract] "Each token is represented with a magnitude component, capturing the global semantics of the entire input text, and a phase component, encoding the relationships between individual tokens and the global semantics."
  - [section 3] "Complex vectors naturally align with the physical properties of waves [9, 10], enabling the use of wave-inspired operations for efficient updates to complex vector token representations."

### Mechanism 2
- Claim: Wave-inspired operations (interference and modulation) enable efficient backpropagation and gradient updates.
- Mechanism: Complex addition (interference) and multiplication (modulation) replace pairwise attention computations, reducing computational complexity from O(n²·d) to O(n·d²). This allows faster gradient propagation and model updates.
- Core assumption: Wave-based operations can substitute for attention mechanisms without losing semantic relationships between tokens.
- Evidence anchors:
  - [section 3.1] "We use complex vectors addition to simulate wave interference [11] and obtain the combined complex vector token representation interference Zj for token wj as follows:"
  - [section 5.4] "In the Wave network, as shown in Figure 5, the gradient of the [CLS] token embedding is propagated to the overall token embeddings through magnitude and phase in polar coordinates."

### Mechanism 3
- Claim: Dimensional independence in [CLS] embeddings improves classification performance through orthogonal semantic features.
- Mechanism: The Wave network achieves better dimensional independence (lower eigenvalue ratios) compared to Transformers, reducing redundancy and improving feature separation for classification tasks.
- Core assumption: Lower dimensional correlation (higher independence) in [CLS] embeddings leads to better classification accuracy.
- Evidence anchors:
  - [section 5.6] "The dimensional independence of the [CLS] token embedding in the Transformer is stronger than that in the Wave network" - This is an error in the paper, the opposite is true according to their results.
  - [section 5.6] "In the Wave network, the KDE of the [CLS] token embedding eigenvalue ratio shows that most samples cluster around 0.28, suggesting low correlation and relatively independent information across dimensions."

## Foundational Learning

- Concept: Complex number representation in polar coordinates
  - Why needed here: The wave network uses complex vectors with magnitude and phase components to represent tokens, requiring understanding of how to convert between Cartesian and polar forms.
  - Quick check question: Given a complex number z = 3 + 4i, what are its magnitude and phase in polar coordinates?

- Concept: Wave interference and modulation principles
  - Why needed here: The network uses these physical wave phenomena to update token representations through complex addition and multiplication operations.
  - Quick check question: How does wave interference differ from wave modulation in terms of their mathematical operations on complex vectors?

- Concept: Eigenvalue analysis for dimensionality
  - Why needed here: The paper analyzes the dimensional independence of [CLS] embeddings using eigenvalue ratios from covariance matrices.
  - Quick check question: What does a high ratio of maximum to minimum eigenvalues indicate about the independence of dimensions in a matrix?

## Architecture Onboarding

- Component map: Input embeddings → Embedding to Complex Vectors → Wave Operations (Interference/Modulation) → Feed-forward + Normalization → Complex Vectors to Embedding → [CLS] classification
- Critical path: Embedding → Complex conversion → Wave operations → Back to real embedding → Classification
- Design tradeoffs: Wave network trades pairwise attention (O(n²·d)) for complex operations (O(n·d²)), reducing VRAM but potentially limiting long-range dependency capture
- Failure signatures: Degraded accuracy on tasks requiring complex token relationships, increased training instability due to complex number operations, failure to converge on long sequences
- First 3 experiments:
  1. Compare embedding dimensional independence (eigenvalue ratios) between Wave network and Transformer on a simple text classification task
  2. Measure VRAM usage and training time for both models on varying sequence lengths
  3. Test convergence behavior on AG News dataset, tracking [CLS] token gradient norms across epochs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Token2Wave's complex vector representation perform on tasks requiring fine-grained semantic distinctions compared to transformer-based models?
- Basis in paper: [explicit] The paper mentions Token2Wave captures both global and local semantics through magnitude and phase components, but primarily focuses on text classification tasks
- Why unresolved: The experiments only evaluated Token2Wave on text classification datasets (AG News, DBpedia14, IMDB). Performance on tasks requiring nuanced semantic understanding (like question answering, summarization, or semantic similarity) remains untested
- What evidence would resolve it: Comparative experiments on diverse NLP tasks including question answering benchmarks, summarization datasets, and semantic similarity evaluations would demonstrate Token2Wave's effectiveness beyond classification

### Open Question 2
- Question: What is the theoretical relationship between the dimensional independence of embeddings and classification accuracy in Token2Wave?
- Basis in paper: [explicit] The paper analyzes the independence level among dimensions of [CLS] embeddings and finds stronger dimensional independence in Transformer compared to Wave network, but doesn't establish a direct correlation with accuracy
- Why unresolved: While the paper observes differences in eigenvalue ratios and dimensional independence between models, it doesn't empirically demonstrate how this independence impacts classification performance or whether more independent dimensions lead to better generalization
- What evidence would resolve it: Controlled experiments varying embedding dimensionality while measuring both eigenvalue ratios and classification accuracy across multiple datasets would reveal the relationship between dimensional independence and model performance

### Open Question 3
- Question: How does Token2Wave's convergence behavior scale with increasing sequence lengths and batch sizes?
- Basis in paper: [explicit] The paper demonstrates fast convergence for short sequences (AG News dataset with average length ~37.85 tokens) but doesn't explore performance on longer texts or different batch sizes
- Why unresolved: The experiments used relatively short text sequences and fixed batch sizes. The paper mentions Token2Wave's O(n·d²) time complexity advantage over Transformer's O(n²·d), but doesn't empirically validate this advantage for long sequences or study how batch size affects convergence
- What evidence would resolve it: Experiments varying sequence lengths from short to long documents and testing different batch sizes while measuring convergence speed, memory usage, and accuracy would reveal Token2Wave's scalability characteristics

### Open Question 4
- Question: How do the wave interference and modulation operations in Token2Wave affect the model's ability to capture long-range dependencies compared to attention mechanisms?
- Basis in paper: [explicit] The paper describes wave interference and modulation as alternatives to attention for updating token representations, but doesn't directly compare their effectiveness for capturing distant token relationships
- Why unresolved: While the paper demonstrates Token2Wave's efficiency advantages, it doesn't systematically evaluate whether wave operations can match or exceed attention mechanisms in modeling dependencies between tokens separated by large distances in the input text
- What evidence would resolve it: Experiments measuring the model's ability to correctly process and utilize information from distant tokens (using tasks like pronoun resolution, coreference resolution, or dependency parsing) would reveal the comparative strengths of wave operations versus attention for long-range dependencies

## Limitations
- Potential error in dimensional independence analysis where text states Transformers have stronger independence than Wave Network, contradicting presented results
- Implementation details for complex vector operations and conversion between real-valued embeddings and complex vectors are underspecified
- Performance gap of ~4.28% accuracy compared to BERT raises questions about practical utility of efficiency gains
- Lack of comprehensive ablation studies to isolate which components drive efficiency improvements

## Confidence
- High Confidence: Computational complexity analysis showing O(n·d²) vs O(n²·d) reduction is mathematically sound and well-established. VRAM usage measurements (0.30 GB vs 1.28 GB) are directly comparable and verifiable.
- Medium Confidence: Accuracy claims on AG News dataset are supported by methodology, though performance gap with BERT requires investigation. Gradient analysis methodology appears sound but KDE plot interpretation requires domain expertise.
- Low Confidence: Dimensional independence claims suffer from textual inconsistency undermining confidence in analysis. Effectiveness of wave operations in capturing semantic relationships compared to attention lacks direct empirical validation.

## Next Checks
1. **Dimensional Independence Verification**: Replicate the eigenvalue ratio analysis for both Wave Network and Transformer [CLS] embeddings on the AG News dataset, ensuring calculation methodology is correct and resolving textual inconsistency regarding which model shows stronger independence.

2. **Component Ablation Study**: Implement ablations to isolate contribution of complex vector representation, wave operations, and reduced model size to efficiency gains. Test real-valued version of model architecture without complex operations to determine if efficiency primarily comes from architectural changes or complex number implementation.

3. **Semantic Relationship Capture Test**: Design controlled experiment comparing how well Wave Network versus Transformer capture token relationships on synthetic datasets with known semantic structures. Use probing tasks that specifically test whether wave interference and modulation operations can match attention's ability to model complex token dependencies, particularly for long-range relationships.