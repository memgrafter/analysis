---
ver: rpa2
title: 'LLMs for Targeted Sentiment in News Headlines: Exploring the Descriptive-Prescriptive
  Dilemma'
arxiv_id: '2403.00418'
source_url: https://arxiv.org/abs/2403.00418
tags:
- sentiment
- entity
- targeted
- negative
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are explored for targeted sentiment
  analysis (TSA) of news headlines, where sentiment is directed towards specific entities.
  This work investigates how prompt design influences LLM performance, drawing parallels
  to annotation paradigms in subjective tasks.
---

# LLMs for Targeted Sentiment in News Headlines: Exploring the Descriptive-Prescriptive Dilemma

## Quick Facts
- arXiv ID: 2403.00418
- Source URL: https://arxiv.org/abs/2403.00418
- Authors: Jana Juroš; Laura Majer; Jan Šnajder
- Reference count: 21
- Large language models (LLMs) outperform fine-tuned encoder models on descriptive datasets for targeted sentiment analysis, with optimal prompt prescriptiveness varying by model architecture.

## Executive Summary
This work investigates how prompt design influences LLM performance for targeted sentiment analysis (TSA) of news headlines, where sentiment is directed towards specific entities. Drawing parallels to annotation paradigms in subjective tasks, the study evaluates LLMs using prompts with varying levels of prescriptiveness, ranging from basic zero-shot to elaborate few-shot prompts aligned with annotation guidelines. The research also assesses LLM uncertainty quantification through calibration error and correlation with human inter-annotator agreement. Results demonstrate that LLMs generally outperform fine-tuned encoder models on descriptive datasets, with both calibration and F1-score improving with increased prompt prescriptiveness, though the optimal level varies by model.

## Method Summary
The study evaluates four LLMs (GPT-3.5, GPT-4, Mistral, Neural Chat) on targeted sentiment analysis using two datasets: STONE (Croatian news headlines with six annotator labels each) and SEN (English and Polish news headlines with aggregated gold labels). Six prompt prescriptiveness levels are implemented based on STONE annotation guidelines, ranging from plain zero-shot to elaborate few-shot prompts. Performance is measured using F1-score for predictive accuracy and expected calibration error (ECE) for uncertainty quantification. Three uncertainty quantification methods are assessed: self-consistency sampling, distribution sampling, and verbal confidence assessment. Results are compared against fine-tuned BERT models to establish baselines.

## Key Results
- LLMs outperform fine-tuned BERT models on descriptive datasets for targeted sentiment analysis
- Calibration and F1-score generally improve with increased prompt prescriptiveness, though optimal levels vary by model
- LLM uncertainty quantification methods show positive correlation with human inter-annotator agreement, but correlation strength varies across methods and models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform better on descriptive datasets than fine-tuned encoders due to broader linguistic and world knowledge.
- Mechanism: LLMs leverage their large pre-training corpus to generalize across languages and domains without needing task-specific fine-tuning, unlike BERT which is constrained by its fine-tuning dataset.
- Core assumption: LLMs' pre-training covers sufficient linguistic diversity to handle varied sentiment expressions in news headlines.
- Evidence anchors:
  - [abstract]: "LLMs offer a potentially universal solution for TSA due to their broad linguistic and world knowledge along with in-context learning abilities, without the need for annotated datasets or model fine-tuning."
  - [section 1]: "Fine-tuned encoder models such as BERT demonstrate strong TSA performance... However, using these models in different languages or domains requires new fine-tuning, and adapting them to low-resource languages necessitates pretrained models and labeled data. In contrast, large language models (LLMs) offer a reliable and versatile approach to TSA across various contexts by leveraging their broad linguistic and world knowledge, as well as in-context learning (Brown et al., 2020), without the need for annotated datasets or model fine-tuning."
- Break condition: If the task requires highly specialized domain knowledge not well-represented in the LLM's pre-training data.

### Mechanism 2
- Claim: Prompt prescriptiveness level affects LLM performance by controlling the interpretation freedom.
- Mechanism: More prescriptive prompts provide clearer task boundaries and reduce ambiguity, leading to more consistent outputs aligned with the desired interpretation. Less prescriptive prompts allow for more diverse interpretations but may lead to inconsistency.
- Core assumption: LLMs can accurately interpret and follow varying levels of task instructions.
- Evidence anchors:
  - [section 4.2]: "Building on this parallel, we evaluate the predictive accuracy of state-of-the-art LLMs using prompts with different levels of prescriptiveness, ranging from plain zero-shot to elaborate few-shot prompts matching annotation guidelines."
  - [section 4.2]: "Our findings indicate that, apart from few-shot prompting, predictive accuracy rises with prompt prescriptiveness level, though the optimal level varies by model."
- Break condition: If the prompt becomes too restrictive and prevents the LLM from using its world knowledge effectively.

### Mechanism 3
- Claim: LLM uncertainty quantification methods can model human label variation in subjective tasks.
- Mechanism: Methods like self-consistency sampling, distribution sampling, and verbal confidence assessment leverage the stochastic nature of LLMs or their ability to express confidence to generate multiple outputs that can approximate human disagreement patterns.
- Core assumption: LLM prediction variability correlates with human subjective disagreement in the task.
- Evidence anchors:
  - [abstract]: "Recognizing the subjective nature of TSA, we evaluate the ability of LLMs to quantify predictive uncertainty via calibration error and correlation with human inter-annotator agreement."
  - [section 4.3]: "Using STONE, we approach this question from two angles: (1) investigating if the uncertainty of LLM predictions aligns with inter-annotator disagreement and (2) examining the relationship between LLMs' predictive and calibration accuracies."
- Break condition: If the correlation between LLM uncertainty and human agreement is consistently weak or negative.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: LLMs perform TSA without fine-tuning by learning from examples or instructions provided in the prompt.
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of data requirements and model adaptation?

- Concept: Calibration in probabilistic predictions
  - Why needed here: Assessing how well LLM confidence scores reflect actual prediction accuracy is crucial for understanding their reliability in subjective tasks.
  - Quick check question: What is expected calibration error (ECE) and why is it important for evaluating LLM uncertainty quantification?

- Concept: Subjective task annotation paradigms
  - Why needed here: Understanding descriptive vs. prescriptive annotation approaches helps in designing effective prompts that elicit desired interpretations from LLMs.
  - Quick check question: How do descriptive and prescriptive annotation paradigms differ in their treatment of annotator subjectivity?

## Architecture Onboarding

- Component map: LLM (GPT-3.5, GPT-4, Mistral, Neural Chat) → Prompt generator → TSA classifier → Uncertainty quantifier → Evaluation metrics (F1, ECE, correlation)
- Critical path: Prompt generation → LLM inference → Sentiment classification → Uncertainty quantification → Performance evaluation
- Design tradeoffs: Prompt prescriptiveness vs. model performance vs. consistency; uncertainty quantification method choice vs. correlation with human agreement
- Failure signatures: Poor performance despite high prescriptiveness may indicate task complexity beyond prompt instructions; low uncertainty correlation may suggest misalignment between LLM and human subjectivity
- First 3 experiments:
  1. Compare zero-shot LLM performance vs. fine-tuned BERT on descriptive and prescriptive datasets
  2. Test varying levels of prompt prescriptiveness on a single dataset to find optimal configuration
  3. Evaluate different uncertainty quantification methods and their correlation with human inter-annotator agreement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal level of prompt prescriptiveness for achieving the best balance between predictive accuracy and calibration error across different LLM models?
- Basis in paper: [explicit] The paper explores how performance is affected by prompt prescriptiveness, ranging from plain zero-shot to elaborate few-shot prompts matching annotation guidelines. It finds that calibration and F1-score generally improve with increased prescriptiveness, yet the optimal level varies by model.
- Why unresolved: The paper does not provide a definitive answer on the optimal level of prescriptiveness for all models, indicating that it varies depending on the specific LLM used.
- What evidence would resolve it: Empirical results showing the performance of different LLMs at various levels of prompt prescriptiveness, including a clear indication of which level yields the best balance between predictive accuracy and calibration error for each model.

### Open Question 2
- Question: How does the sensitivity of LLMs to the selection and ordering of examples in few-shot prompting affect their performance in targeted sentiment analysis?
- Basis in paper: [explicit] The paper mentions that the performance drop seen at levels 5 and 6, the only ones with few-shot examples, may be due to the sensitivity regarding the selection and ordering of examples, a phenomenon observed in few-shot prompting.
- Why unresolved: The paper does not investigate the specific impact of example selection and ordering on LLM performance, leaving the extent of this sensitivity unclear.
- What evidence would resolve it: Systematic experiments varying the selection and ordering of examples in few-shot prompts to determine their impact on LLM performance in targeted sentiment analysis.

### Open Question 3
- Question: To what extent does the cultural and linguistic background of the news headlines influence the performance of LLMs in targeted sentiment analysis?
- Basis in paper: [inferred] The paper evaluates LLMs on datasets in Croatian, English, and Polish, and notes that the performance differences could be due to the models' ability to understand the language and cultural/political background, both essential for the task.
- Why unresolved: The paper does not provide a detailed analysis of how cultural and linguistic factors affect LLM performance, nor does it compare the performance across different cultural contexts.
- What evidence would resolve it: Comparative studies of LLM performance on datasets from diverse cultural and linguistic backgrounds, controlling for other variables to isolate the impact of cultural and linguistic factors.

## Limitations
- Model-specific performance variability: Optimal prompt prescriptiveness levels vary significantly across different LLM architectures, suggesting findings may not generalize uniformly.
- Dataset-specific constraints: Evaluation relies heavily on Croatian, English, and Polish news headlines, limiting generalizability to other domains or languages.
- Uncertainty quantification correlation strength: While correlations exist between LLM uncertainty and human agreement, the strength varies and practical meaningfulness is unclear.

## Confidence
- High confidence: LLMs outperform fine-tuned BERT models on descriptive datasets for targeted sentiment analysis
- Medium confidence: Predictive accuracy increases with prompt prescriptiveness level (excluding few-shot prompting)
- Medium confidence: LLM uncertainty quantification correlates with human inter-annotator agreement
- Low confidence: LLMs provide a "universal solution" for TSA across various contexts

## Next Checks
1. Evaluate the same LLM models and prompt strategies on non-news domains (e.g., product reviews, social media posts) to assess whether descriptive-prescriptive performance patterns hold across different text types.
2. Systematically remove individual components from the most prescriptive prompts to identify which specific instructions contribute most to performance gains.
3. Compare LLM uncertainty quantification against simpler baseline methods (e.g., entropy from multiple decoding runs, majority voting from different prompts) to determine if sophisticated approaches provide meaningful advantages.