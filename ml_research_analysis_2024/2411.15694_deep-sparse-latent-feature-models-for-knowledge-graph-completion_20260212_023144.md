---
ver: rpa2
title: Deep Sparse Latent Feature Models for Knowledge Graph Completion
arxiv_id: '2411.15694'
source_url: https://arxiv.org/abs/2411.15694
tags:
- latent
- graph
- knowledge
- fb15k-237
- wn18rr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DSLFM-KGC, a novel knowledge graph completion
  method that leverages sparse latent feature models with a deep variational autoencoder
  framework. The approach dynamically integrates global clustering information with
  local textual features to complete missing triples while providing enhanced interpretability
  of the underlying latent structures.
---

# Deep Sparse Latent Feature Models for Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2411.15694
- Source URL: https://arxiv.org/abs/2411.15694
- Reference count: 40
- Primary result: Achieves 76.3% MRR and 67.2% Hit@1 on Wikidata5M-Ind, outperforming previous state-of-the-art by 5.0% and 6.5% respectively

## Executive Summary
This paper introduces DSLFM-KGC, a novel knowledge graph completion method that leverages sparse latent feature models with a deep variational autoencoder framework. The approach dynamically integrates global clustering information with local textual features to complete missing triples while providing enhanced interpretability of the underlying latent structures. By employing an Indian Buffet Process prior to determine the number of latent communities and using contrastive learning for improved triple completion, the method demonstrates significant performance gains across four benchmark datasets.

## Method Summary
DSLFM-KGC combines sparse latent feature models with deep variational autoencoders to complete missing triples in knowledge graphs. The method uses an Indian Buffet Process prior to dynamically learn the number of latent communities, then applies Hadamard products to capture community membership strength for both queries and entities. A BERT-based encoder processes textual descriptions, while a decoder with contrastive learning optimizes triple completion. The model jointly updates encoder and decoder parameters using stochastic gradient variational Bayes, achieving scalable inference of community structures while maintaining end-to-end differentiability.

## Key Results
- Achieves 76.3% MRR and 67.2% Hit@1 on Wikidata5M-Ind, outperforming previous SOTA by 5.0% and 6.5% respectively
- Shows 3.5% improvement in Hit@1 on WN18RR compared to text-based methods
- Narrows the gap between text-based and embedding-based approaches by 2-3 percentage points on FB15k-237
- Particularly strong on datasets with distinct clustering patterns, demonstrating the effectiveness of community-based modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model leverages global clustering patterns through sparse latent features to improve KGC performance, especially on graphs with distinct community structures.
- Mechanism: The Indian Buffet Process prior dynamically learns the number of latent communities, and the sparse latent feature model (via Hadamard product) captures community membership strength for both queries and entities.
- Core assumption: Knowledge graphs exhibit community structure where intra-community connections are denser than inter-community ones, and this structure can be inferred probabilistically.
- Evidence anchors:
  - [abstract]: "Stochastic blockmodels (SBMs), especially the latent feature relational model (LFRM), offer robust probabilistic frameworks for identifying latent community structures and improving link prediction."
  - [section 3.1]: "The generative process of a KG unfolds as follows: For each query (h, r) ∈ Q and each answer t ∈ E, draw the membership indicator vectors: zhr ∼ MB(z|πhr), zt ∼ MB(z|πt)"
  - [corpus]: Weak evidence - corpus neighbors focus on path-based and subgraph reasoning rather than community structure
- Break condition: If the KG lacks meaningful community structure (e.g., FB15k-237 with modularity 0.074), the community-based approach loses effectiveness and performance gains diminish.

### Mechanism 2
- Claim: The deep variational autoencoder architecture enables scalable inference of the sparse latent features while maintaining end-to-end differentiability.
- Mechanism: The VAE encodes textual descriptions into posterior distributions over latent variables (community membership and feature strength), then decodes to generate triple confidence scores using contrastive learning.
- Core assumption: The latent community structure can be approximated by variational inference with reasonable computational complexity, and the VAE framework can handle the stochastic nature of the IBP prior.
- Evidence anchors:
  - [abstract]: "optimized via a deep variational autoencoder (V AE)"
  - [section 3.2]: "We adopt the stick-breaking construction of the IBP [50] to model zhr" and "Following recent progress in text-based approaches for the KGC task [62, 55], we employ the strategy that individually encodes the textual descriptions of queries and answers using two BERT encoders"
  - [section 3.4]: "We jointly update the encoder hϕ and the decoder gθ by minimizing the negative of the evidence lower bound (ELBO)"
- Break condition: If the VAE fails to converge or posterior collapse occurs, the learned latent features become uninformative and the model loses its advantage over simpler methods.

### Mechanism 3
- Claim: Contrastive learning with negative sampling improves triple completion by maximizing mutual information between query-answer pairs and their representations.
- Mechanism: The model uses supervised contrastive loss where positive entities are the correct answers and negative samples are other entities in the same batch, with cosine similarity scoring.
- Core assumption: The contrastive objective can effectively learn discriminative representations that distinguish correct triples from incorrect ones in the latent feature space.
- Evidence anchors:
  - [abstract]: "uses contrastive learning for improved triple completion"
  - [section 3.4]: "we express the triple completion term log pθ(A|H) as a contrastive loss for its renowned capacity to learn expressive representations"
  - [section 3.4]: "log pθ(Ahr,t|zhr, zt, whr, wt) = 1/|N +| X t∈N + log eS(ghr,gt) / (eS(ghr,gt) + P t′∈N − eS(ghr,gt))"
- Break condition: If negative sampling is ineffective (e.g., batch size too small or negative entities too dissimilar), the contrastive loss fails to provide meaningful gradients and performance plateaus.

## Foundational Learning

- Concept: Indian Buffet Process (IBP) for nonparametric Bayesian community modeling
  - Why needed here: The IBP allows the model to learn the number of communities dynamically without pre-specifying K, which is crucial for handling KGs of varying sizes and structures
  - Quick check question: How does the stick-breaking construction of IBP differ from traditional Dirichlet process priors in terms of computational efficiency and community representation?

- Concept: Variational Autoencoder (VAE) inference with reparameterization tricks
  - Why needed here: The VAE framework enables scalable, differentiable inference of the latent community structure while handling the stochastic nature of the IBP prior through Monte Carlo approximation
  - Quick check question: What is the key difference between reparameterizing Gaussian variables versus Beta/Concrete variables in the context of this model?

- Concept: Contrastive learning for representation discrimination
  - Why needed here: The contrastive objective helps the model learn discriminative representations in the latent feature space that can distinguish correct triples from incorrect ones, improving triple completion accuracy
  - Quick check question: How does the temperature parameter τ in the contrastive loss affect the model's sensitivity to similarity scores between query-answer pairs?

## Architecture Onboarding

- Component map: BERT encoders → VAE sampler → Latent features (Hadamard product) → MLP decoder → Contrastive scorer → Loss computation
- Critical path: Text input → Encoder → Latent variable sampling → Hadamard product → Decoder → Contrastive scoring → Loss computation → Parameter updates
- Design tradeoffs: IBP prior enables dynamic community discovery but adds computational overhead vs fixed K; contrastive learning improves discriminative power but requires careful negative sampling; VAE enables scalability but introduces posterior approximation error
- Failure signatures: Posterior collapse (latent variables ignore input), community sparsity mismatch (too many/few communities), contrastive loss saturation (negatives too easy/hard), slow convergence (learning rate or architecture mismatch)
- First 3 experiments:
  1. Ablation: Replace IBP prior with fixed K communities to measure impact of dynamic community discovery
  2. Ablation: Remove contrastive learning component to evaluate its contribution to triple completion accuracy
  3. Scalability test: Run on progressively larger subsets of Wikidata5M to identify computational bottlenecks and memory constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the DSLFM-KGC model's performance scale with increasing knowledge graph size, particularly for datasets exceeding 100 million triples?
- Basis in paper: [inferred] The paper demonstrates strong performance on Wikidata5M with ~20 million triples but does not test on larger datasets, and the authors mention future work on reducing computational overhead.
- Why unresolved: The paper only evaluates up to Wikidata5M and acknowledges computational challenges for larger KGs, but doesn't provide empirical evidence of scalability beyond this point.
- What evidence would resolve it: Experiments on KGs with 100M+ triples showing sustained performance gains and computational efficiency metrics.

### Open Question 2
- Question: What is the impact of varying the number of communities (K) on model performance and interpretability across different types of knowledge graphs?
- Basis in paper: [explicit] The paper uses K=128 consistently across datasets but notes that "the effective number of communities engaged can be learned" and performs some analysis on community activation patterns.
- Why unresolved: While the paper mentions community activation analysis, it doesn't systematically explore how different K values affect performance or provide guidelines for selecting optimal K values for different KG characteristics.
- What evidence would resolve it: A comprehensive study varying K across datasets with different structural properties (density, modularity, etc.) showing performance curves and interpretability trade-offs.

### Open Question 3
- Question: How does DSLFM-KGC compare to LLM-based approaches on sparsely connected KGs where external knowledge cannot be reliably provided?
- Basis in paper: [explicit] The paper discusses LLM limitations on sparse KGs and claims superiority in such scenarios, but doesn't provide direct comparative experiments.
- Why unresolved: The authors make theoretical arguments about LLM limitations on sparse KGs but don't empirically validate their claims through head-to-head comparisons on datasets specifically designed to test this hypothesis.
- What evidence would resolve it: Controlled experiments on datasets with varying sparsity levels comparing DSLFM-KGC against state-of-the-art LLM approaches while measuring both performance and computational efficiency.

## Limitations
- Performance gains on FB15k-237 are modest (2-3 percentage points), suggesting the method may be less effective on datasets with weak community structure (modularity 0.074)
- The model's reliance on textual descriptions means performance is bounded by the quality and completeness of available entity/relation descriptions
- Computational complexity scales with the number of latent communities, which could become prohibitive for very large KGs

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| VAE and contrastive learning mechanisms | High |
| IBP-based community discovery effectiveness | Medium |
| Overall performance claims | Medium |

## Next Checks
1. Perform ablation study on community structure: Train the model with fixed K communities vs dynamic IBP to quantify the contribution of nonparametric community discovery
2. Test on synthetic KGs with controlled community structure: Generate graphs with varying modularity to validate the claimed relationship between community strength and performance gains
3. Analyze posterior distributions: Examine the learned Beta distributions over community memberships to verify they capture meaningful clustering patterns rather than degenerate solutions