---
ver: rpa2
title: Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision
  Encoder
arxiv_id: '2411.05195'
source_url: https://arxiv.org/abs/2411.05195
tags:
- clip
- text
- encoder
- llav
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLIP models struggle with visual reasoning tasks requiring spatial
  relationships, compositionality, and fine-grained details. A key hypothesis is that
  CLIP's vision encoder fails to capture essential visual information for these tasks.
---

# Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder

## Quick Facts
- **arXiv ID**: 2411.05195
- **Source URL**: https://arxiv.org/abs/2411.05195
- **Authors**: Siting Li; Pang Wei Koh; Simon Shaolei Du
- **Reference count**: 27
- **Primary result**: Generative MLLMs using CLIP vision encoders outperform CLIP on visual reasoning tasks, demonstrating that architectural design, not encoder quality, limits CLIP's performance

## Executive Summary
This paper challenges the common assumption that CLIP's vision encoder fails to capture essential visual information for complex reasoning tasks. Through systematic experiments, the authors demonstrate that when the same CLIP vision encoder is used in generative multimodal large language models (MLLMs), it significantly outperforms CLIP on visual reasoning tasks requiring spatial relationships, compositionality, and fine-grained details. This surprising result indicates that the encoder preserves relevant information but CLIP fails to extract it effectively. The study identifies patch tokens, position embeddings, and prompt-based weighting as critical architectural components that enable better feature extraction, while training data and stronger text encoders provide minimal benefit. Even when converted to CLIP-like encoders, these MLLMs maintain superior performance, showing that autoregressive loss is not necessary for strong visual reasoning.

## Method Summary
The authors conducted controlled experiments comparing CLIP models with generative MLLMs that share the same vision encoder. They systematically isolated and tested different architectural components including patch tokens, position embeddings, and prompt-based weighting to determine their impact on visual reasoning performance. The study also converted MLLMs to CLIP-like encoders to test whether autoregressive loss is necessary for strong performance. Performance was evaluated across multiple visual reasoning benchmarks that test spatial relationships, compositionality, and fine-grained details. The controlled nature of the experiments allowed the authors to attribute performance differences to specific architectural choices rather than differences in training data or model scale.

## Key Results
- Generative MLLMs using the same CLIP vision encoder significantly outperform CLIP on visual reasoning tasks
- Patch tokens, position embeddings, and prompt-based weighting are critical for improved feature extraction
- Training data, stronger text encoders, and additional text tokens offer minimal performance benefits
- Converted MLLMs to CLIP-like encoders maintain superior performance, demonstrating autoregressive loss is not necessary
- The vision encoder preserves relevant information that CLIP fails to extract effectively

## Why This Works (Mechanism)
The mechanism underlying the performance gap between CLIP and generative MLLMs using the same vision encoder centers on how different architectures extract and utilize visual features. While the CLIP vision encoder contains the necessary information for visual reasoning tasks, CLIP's architecture and training objective fail to effectively extract and leverage this information. The generative MLLM architecture, through its use of patch tokens, position embeddings, and prompt-based weighting, creates a more effective pathway for extracting spatial relationships, compositional information, and fine-grained details from the same visual features. This suggests that the bottleneck in CLIP's performance is not the quality of its vision encoder but rather how it processes and interprets the visual information.

## Foundational Learning
- **Vision encoder architecture**: Understanding how vision encoders process and represent visual information is fundamental to this work. This knowledge is needed to appreciate why the same encoder can perform differently across architectures. Quick check: Can you explain the difference between how vision encoders and vision-language encoders process images?
- **Patch tokens**: These represent localized visual features extracted from image patches. They are needed because they provide the basic building blocks for visual reasoning. Quick check: How do patch tokens differ from global image representations?
- **Position embeddings**: These encode spatial relationships between image patches. They are needed to preserve spatial information that is crucial for tasks requiring understanding of object relationships and spatial reasoning. Quick check: Why are position embeddings critical for visual reasoning but not necessarily for image classification?
- **Prompt-based weighting**: This mechanism allows the model to dynamically weight different visual features based on task context. It is needed to enable flexible extraction of relevant visual information. Quick check: How does prompt-based weighting differ from fixed attention mechanisms?
- **Autoregressive vs. discriminative training**: The study compares models trained with different objectives. This knowledge is needed to understand why autoregressive loss is not necessary for strong visual reasoning. Quick check: What are the key differences between autoregressive and discriminative training objectives in VLMs?
- **Visual reasoning benchmarks**: Understanding the specific tasks that test spatial relationships, compositionality, and fine-grained details is crucial for interpreting the results. Quick check: Can you name three types of visual reasoning tasks that require spatial understanding?

## Architecture Onboarding

Component map: Image -> Vision Encoder -> Patch Tokens + Position Embeddings -> Prompt-based Weighting -> Language Model -> Output

Critical path: The critical path for visual reasoning performance flows from the vision encoder through patch tokens and position embeddings, where prompt-based weighting plays a crucial role in extracting relevant features for the language model to process.

Design tradeoffs: The study reveals that architectural design choices around feature extraction (patch tokens, position embeddings) and dynamic weighting (prompt-based weighting) are more important than model scale or training data diversity. The tradeoff is between simple, direct feature extraction versus complex, adaptive mechanisms for feature utilization.

Failure signatures: CLIP fails on visual reasoning tasks not because of poor vision encoders but because its architecture cannot effectively extract spatial relationships, compositional information, and fine-grained details from the visual features. The failure manifests as poor performance on tasks requiring understanding of object relationships and fine details.

First experiments to run:
1. Compare CLIP's performance on visual reasoning tasks before and after adding position embeddings to its architecture
2. Test whether CLIP can benefit from prompt-based weighting mechanisms borrowed from generative MLLMs
3. Evaluate whether CLIP's performance improves when using patch tokens extracted from the generative MLLM architecture

## Open Questions the Paper Calls Out
None

## Limitations
- The precise mechanisms by which patch tokens, position embeddings, and prompt-based weighting enable better feature extraction remain unclear
- The experiments focus on specific visual reasoning benchmarks and may not generalize to other task types or real-world scenarios
- The comparison of autoregressive versus discriminative training does not account for potential differences in optimization procedures or architectural constraints

## Confidence
- Core finding that same vision encoder performs differently across architectures: High
- Specific attributions to patch tokens, position embeddings, and prompt-based weighting: Medium
- Claim about autoregressive loss not being necessary: Medium

## Next Checks
1. Conduct ablation studies that systematically disable or modify each architectural component (patch tokens, position embeddings, prompt weighting) to isolate their individual contributions
2. Test the CLIP vision encoder in additional VLM architectures beyond generative MLLMs to verify that performance gains are architecture-general rather than specific to tested models
3. Evaluate the models on a broader range of visual tasks including object detection, segmentation, and real-world image understanding to assess generalization beyond visual reasoning benchmarks