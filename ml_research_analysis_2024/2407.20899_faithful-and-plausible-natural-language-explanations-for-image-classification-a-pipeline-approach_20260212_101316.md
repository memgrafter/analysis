---
ver: rpa2
title: 'Faithful and Plausible Natural Language Explanations for Image Classification:
  A Pipeline Approach'
arxiv_id: '2407.20899'
source_url: https://arxiv.org/abs/2407.20899
tags:
- image
- explanations
- explanation
- neurons
- neuron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a pipeline method for generating natural language
  explanations (NLEs) for image classification. The approach analyzes influential
  neurons in a CNN classifier to identify key visual patterns and their locations
  in the image, creating a structured meaning representation.
---

# Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach

## Quick Facts
- arXiv ID: 2407.20899
- Source URL: https://arxiv.org/abs/2407.20899
- Reference count: 29
- Key outcome: Generates natural language explanations for image classification by analyzing influential neurons in CNN classifiers, achieving significantly higher plausibility and faithfulness than baseline methods

## Executive Summary
This paper presents a novel pipeline approach for generating faithful and plausible natural language explanations (NLEs) for image classification. The method analyzes influential neurons in CNN classifiers to identify key visual patterns and their locations, creating structured meaning representations that are converted to text using a large language model. Unlike post-hoc concept-based methods, this approach provides detailed neuron-level analysis while maintaining both faithfulness to the classifier's decision process and readability for human users.

## Method Summary
The pipeline consists of three main stages: First, Layer-wise Relevance Propagation (LRP) identifies the most influential neurons in a CNN classifier for each prediction. Second, MILAN annotates these neurons with descriptive patterns and spatial positions to create structured meaning representations. Third, a large language model (GPT-4) converts these representations into natural language explanations. The method is post-hoc, meaning it can be applied to any standard CNN classifier without modifying its training or architecture, and includes neuron masking experiments to validate faithfulness.

## Key Results
- Human evaluations show generated explanations are significantly more plausible than baseline methods
- Neuron masking experiments demonstrate explanations are three times more faithful than baselines
- The method achieves over five times higher decrease in predicted class probability when masking neurons compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-hoc neuron-level analysis can generate faithful explanations without modifying the classifier.
- Mechanism: By applying Layer-wise Relevance Propagation (LRP) to identify influential neurons and then using MILAN to annotate them, the method grounds explanations in the actual internal decision process of the CNN.
- Core assumption: Influential neurons identified by LRP truly reflect the classifier's decision logic.
- Evidence anchors:
  - [abstract] "By analysing influential neurons and the corresponding activation maps, the method generates a faithful description of the classifier’s decision process"
  - [section 2.1] "To pick the most relevant neurons, we apply the well-established Layer-wise Relevance Propagation (LRP) method"
- Break condition: If LRP identifies neurons that are not actually driving the classification decision, the explanation becomes unfaithful.

### Mechanism 2
- Claim: Converting structured neuron annotations to natural language via LLM preserves faithfulness while improving plausibility.
- Mechanism: The LLM is prompted to faithfully represent the structured meaning representation (MR) while prioritizing readability, effectively translating technical neuron information into accessible explanations.
- Core assumption: The LLM can accurately represent the MR content without hallucinating additional information.
- Evidence anchors:
  - [abstract] "which is then converted into text by a language model"
  - [section 2.2] "As we do not have any gold-standard explanation texts, the task is performed by prompting a large language model (LLM)"
- Break condition: If the LLM hallucinates information not present in the MR, the explanation loses faithfulness.

### Mechanism 3
- Claim: Masking neurons indicated in explanations disrupts the classifier's decision more effectively than baselines.
- Mechanism: Since the method identifies truly influential neurons, masking them should significantly reduce the predicted class probability and increase class flip rate.
- Core assumption: The neurons identified as most influential are genuinely critical to the classification decision.
- Evidence anchors:
  - [abstract] "user interventions in the neural network structure (masking of neurons) are three times more effective than the baselines"
  - [section 3.3] "masking neurons suggested by our NLEs led to a five times higher decrease in the predicted class probability and over three times higher class flip rate than baselines"
- Break condition: If the identified neurons are not actually influential, masking them would not significantly affect predictions.

## Foundational Learning

- Concept: Layer-wise Relevance Propagation (LRP)
  - Why needed here: LRP is used to identify which neurons in the CNN are most influential in making a specific prediction, providing the foundation for faithful explanations.
  - Quick check question: How does LRP propagate relevance scores backward through a neural network to identify influential neurons?

- Concept: Convolutional Neural Networks (CNNs)
  - Why needed here: The method analyzes influential neurons in CNN classifiers, requiring understanding of how CNNs process images through convolutional layers.
  - Quick check question: What is the role of convolutional filters in a CNN, and how do they detect visual patterns?

- Concept: Natural Language Generation (NLG)
  - Why needed here: The method converts structured neuron information into natural language explanations using an LLM, requiring understanding of NLG principles.
  - Quick check question: What are the key challenges in converting structured data into fluent, natural-sounding text?

## Architecture Onboarding

- Component map: Input image → CNN classifier prediction → LRP neuron importance scoring → MILAN annotation → MR construction → LLM conversion → Natural language explanation

- Critical path: Image → CNN classifier → LRP → MILAN → MR → LLM → NLE

- Design tradeoffs:
  - Using post-hoc analysis preserves classifier performance but relies on the faithfulness of analysis methods
  - LLM conversion improves plausibility but introduces potential hallucination risks
  - Neuron-level analysis provides detailed explanations but may be harder to summarize than concept-level approaches

- Failure signatures:
  - Explanation doesn't match classifier's actual decision process (low faithfulness)
  - Explanation is too technical or incoherent (low plausibility)
  - MR contains irrelevant neurons or incorrect annotations
  - LLM generates hallucinations not grounded in MR

- First 3 experiments:
  1. Verify LRP identifies truly influential neurons by masking them and checking prediction changes
  2. Test LLM conversion reliability by comparing MR content with generated text for hallucinations and omissions
  3. Evaluate explanation plausibility with human annotators using the established 5-factor rubric

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of LLM affect the faithfulness and plausibility of generated explanations?
- Basis in paper: [explicit] The paper mentions using GPT-4 and comparing it with Llama 3 70B, finding differences in hallucination rates and omissions.
- Why unresolved: While the paper shows that GPT-4 performs better than Llama 3 70B, it doesn't explore a broader range of LLMs or investigate which specific LLM properties (size, architecture, training data) most impact explanation quality.
- What evidence would resolve it: Systematic comparison of multiple LLMs with varying architectures and sizes on both faithfulness and plausibility metrics.

### Open Question 2
- Question: How sensitive is the method to different neuron selection thresholds beyond the fixed k=10 used in experiments?
- Basis in paper: [inferred] The method selects k neurons with highest LRP scores, but the paper doesn't explore how varying k affects explanation quality.
- Why unresolved: The paper uses a fixed k=10 but doesn't investigate the trade-off between brevity and completeness, or how different values of k might better balance faithfulness and readability.
- What evidence would resolve it: Experiments testing multiple values of k and measuring their impact on both faithfulness (via intervention experiments) and plausibility (via human evaluation).

### Open Question 3
- Question: Can the method be extended to non-CNN architectures like transformers or vision-language models?
- Basis in paper: [explicit] The method is described as applicable to "any standard CNN-based classifier" but doesn't explore other architectures.
- Why unresolved: While the pipeline approach seems adaptable, the current implementation relies heavily on CNN-specific features like activation maps and LRP, which may not directly transfer to other architectures.
- What evidence would resolve it: Implementation and evaluation of the method on transformer-based vision models, testing whether the neuron-level analysis approach remains effective.

## Limitations
- The method relies on post-hoc analysis of a fixed CNN classifier, which may not capture all decision-relevant factors
- LLM conversion introduces potential hallucination risks that could compromise faithfulness
- Evaluation focuses primarily on ImageNet, limiting generalizability to other datasets or domains

## Confidence
- High confidence: The post-hoc pipeline architecture and basic mechanism of neuron identification through LRP are well-established and clearly explained
- Medium confidence: The effectiveness of neuron masking interventions is demonstrated, but could benefit from testing on more diverse image types
- Medium confidence: Human evaluation results are positive, though the evaluation criteria could be more explicitly defined

## Next Checks
1. Test the pipeline on additional image classification datasets beyond ImageNet to verify generalizability
2. Conduct ablation studies comparing different neuron selection methods (beyond LRP) to quantify the contribution of the analysis approach
3. Implement a more rigorous faithfulness evaluation using concept activation vectors (TCAV) to verify that identified neurons truly correspond to human-interpretable visual concepts