---
ver: rpa2
title: Inference over Unseen Entities, Relations and Literals on Knowledge Graphs
arxiv_id: '2410.06742'
source_url: https://arxiv.org/abs/2410.06742
tags:
- knowledge
- entities
- embedding
- relations
- link
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of extending knowledge graph embedding
  (KGE) models to handle unseen entities, relations, and literals at inference time,
  a limitation of current transductive KGE approaches. The authors propose an attentive
  byte-pair encoding layer (BYT E) that represents entities and relations as sequences
  of byte-pair encoded subword units.
---

# Inference over Unseen Entities, Relations and Literals on Knowledge Graphs

## Quick Facts
- arXiv ID: 2410.06742
- Source URL: https://arxiv.org/abs/2410.06742
- Reference count: 40
- Extends knowledge graph embedding models to handle unseen entities, relations, and literals at inference time

## Executive Summary
This paper addresses a fundamental limitation of transductive knowledge graph embedding models: their inability to handle entities, relations, or literals that were not seen during training. The authors propose BYT E, an attentive byte-pair encoding layer that represents entities and relations as sequences of subword units. This approach enables KGE models to construct embeddings on-the-fly from subword units rather than relying on fixed embeddings for each entity and relation. The method leads to massive feature reuse via weight tying and removes the constraint of embedding matrix sizes being bound to the number of unique entities and relations.

## Method Summary
BYT E extends knowledge graph embedding models by introducing a byte-pair encoding layer that tokenizes entity and relation strings into subword units. Each subword unit has its own embedding, and a linear mapping (with optional attention) reconstructs the full entity/relation embedding from the sequence. The method can be applied to any existing KGE model, including DistMult, ComplEx, QMult, and Keci. During training, the model predicts triple likelihoods by combining embeddings of subword units representing the triple components. The approach enables inference over unseen entities and relations by constructing their embeddings from their string representations, making it truly inductive.

## Key Results
- BYT E improves link prediction performance on datasets with semantically meaningful entity/relation strings (e.g., Countries-S1: MRR 0.566 vs 0.208 for base Keci)
- Performance gains diminish or reverse on datasets with non-semantic representations (e.g., FB15K-237, WN18RR)
- BYT E enables inference over unseen entities and relations through on-the-fly embedding construction
- L2 regularization and dropout can reduce over-confidence in predictions for unseen entities

## Why This Works (Mechanism)

### Mechanism 1: Subword Tokenization for On-the-Fly Embedding Construction
Byte-pair encoding converts entities and relations into subword sequences, enabling KGE models to construct embeddings dynamically rather than retrieving fixed vectors. BPE iteratively merges frequent byte pairs into new tokens, producing subword units that capture compositional structure. Each subword unit gets its own embedding, and a linear mapping reconstructs the full entity/relation embedding from the sequence. This works when the syntactic representation contains semantically meaningful subword structure. The mechanism fails when entities/relations are represented as opaque IDs or URIs without meaningful string decomposition.

### Mechanism 2: Weight Tying for Feature Reuse
Instead of learning unique embeddings per entity/relation, BYT E learns embeddings only for subword units that are reused across all entities/relations. This reduces embedding matrix size from O(|E| + |R|) to O(V), where V is the BPE vocabulary size. The core assumption is that subword units are shared across entities/relations and carry transferable semantic content. Weight tying gains diminish when BPE vocabulary grows large or subwords are too fine-grained, increasing overfitting risks.

### Mechanism 3: Linear Mapping with Attention for Flexible Reconstruction
After embedding lookup for each subword, the sequence is flattened and projected via a shared weight matrix (plus optional bias) to the target embedding dimension. Attention can weight subwords before projection. The core assumption is that a linear (or attention-augmented linear) transformation suffices to reconstruct meaningful entity/relation embeddings from subword embeddings. This reconstruction may fail if subword sequences are too long or lack internal structure.

## Foundational Learning

- **Byte-pair encoding and subword tokenization**: BPE enables decomposition of arbitrary entity/relation strings into reusable subword units, which is the foundation for on-the-fly embedding construction. *Quick check: What is the maximum number of subword units allowed per entity/relation in BYT E, and how are longer strings handled?*

- **Knowledge graph embedding scoring functions**: BYT E acts as a preprocessing layer that feeds embeddings into any KGE scoring function; understanding the scoring mechanism clarifies how BYT E integrates. *Quick check: In DistMult, how is a triple score computed from head, relation, and tail embeddings?*

- **Transductive vs. inductive learning settings**: BYT E aims to extend transductive KGE models to inductive settings by enabling inference over unseen entities/relations. *Quick check: What is the key limitation of transductive KGE models that BYT E addresses?*

## Architecture Onboarding

- **Component map**: Tokenizer (BPE) → Embedding lookup for subword units → Linear mapping (with optional attention) → KGE scoring function → Loss/backprop
- **Critical path**: Tokenizer → Embedding lookup → Linear mapping → Scoring function → Loss
- **Design tradeoffs**: BPE vocabulary size vs. memory reuse (larger vocabularies reduce sharing but increase capacity); embedding dimension vs. overfitting (higher dimensions risk overfitting on small datasets); attention vs. linear mapping (attention adds expressiveness but increases parameters and compute)
- **Failure signatures**: Degraded performance on datasets with opaque IDs/URIs (e.g., FB15K-237, WN18RR); overconfidence in predictions for unseen entities (mitigated by L2 regularization and dropout); instability when subword sequence length grows large (training may diverge)
- **First 3 experiments**: 1) Verify tokenization on a small synthetic KG with meaningful strings and inspect subword sequences; 2) Test embedding lookup and linear reconstruction on a fixed triple to confirm BYT E outputs match expected dimensions; 3) Train DistMult-BYT E on Countries-S1 and compare MRR/Hits@1/10 to base DistMult to confirm performance gain

## Open Questions the Paper Calls Out

**Open Question 1**: How does the effectiveness of BYT E vary across different knowledge graph domains and entity/relation naming conventions? The authors observed that BYT E improves performance on knowledge graphs with semantically meaningful syntactic representations but shows poor results on graphs with non-semantic representations. This is unresolved because the paper does not provide systematic analysis of which specific naming conventions or domain characteristics determine BYT E's effectiveness.

**Open Question 2**: What is the optimal strategy for handling large byte-pair encoded sequences in BYT E? The authors observed that as byte-pair encoded sequence length increases, performance decreases, potentially due to unstable training. This is unresolved because the paper does not explore solutions like sequence truncation, hierarchical encoding, or alternative normalization strategies.

**Open Question 3**: How does BYT E compare to specialized inductive KGE approaches like NodePiece in practical applications? The authors acknowledge they do not compare BYT E to inductive methods like NodePiece, as their focus was extending transductive models rather than competing with inductive approaches. This is unresolved because without direct comparison, it's unclear whether BYT E's advantage of not requiring additional information outweighs potential performance differences.

## Limitations
- Strong dependence on syntactic structure of entity and relation names limits applicability to real-world KGs with URIs or numeric IDs
- Performance degradation on standard benchmarks (FB15K-237, WN18RR) suggests limited generalization to typical real-world KGs
- Core assumption about semantic meaningfulness of BPE subwords lacks empirical validation and analysis

## Confidence
- **High confidence**: The core mechanism of BYT E is well-specified and the mathematical formulation is correct. The experimental results on the Countries and UMLS datasets are internally consistent with the claimed improvements.
- **Medium confidence**: The claim that BYT E enables true inductive inference over unseen entities/relations is supported by methodology but not extensively validated.
- **Low confidence**: The assertion that BYT E is broadly applicable to knowledge graphs with "meaningful" entity names is overstated. The experimental evidence only covers a narrow set of biomedical and geographic datasets.

## Next Checks
1. **Semantic Analysis of BPE Subwords**: Analyze the byte-pair encoded subwords produced for entities in Countries-S1, UMLS, and FB15K-237. Categorize whether subwords correspond to interpretable semantic units versus random character sequences to validate the core assumption about semantic meaningfulness.

2. **Cross-Dataset Generalization Test**: Train BYT E on Countries-S1 and evaluate on Countries-S2 and Countries-S3 without fine-tuning. This would test whether learned subword embeddings transfer across related datasets, demonstrating true inductive capability.

3. **Ablation Study on BPE Parameters**: Systematically vary the BPE vocabulary size (e.g., 100, 500, 1000, 5000) and maximum sequence length (m) on Countries-S1. Measure the impact on performance and memory usage to identify optimal parameter settings and understand trade-offs between feature reuse and representational capacity.