---
ver: rpa2
title: Attention Down-Sampling Transformer, Relative Ranking and Self-Consistency
  for Blind Image Quality Assessment
arxiv_id: '2409.07115'
source_url: https://arxiv.org/abs/2409.07115
tags:
- image
- quality
- assessment
- transformer
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ADTRS, a blind image quality assessment (NR-IQA)
  model that combines CNNs and Transformers to capture both local and non-local image
  features. The model incorporates relative ranking and self-consistency mechanisms
  to improve performance.
---

# Attention Down-Sampling Transformer, Relative Ranking and Self-Consistency for Blind Image Quality Assessment

## Quick Facts
- arXiv ID: 2409.07115
- Source URL: https://arxiv.org/abs/2409.07115
- Authors: Mohammed Alsaafin; Musab Alsheikh; Saeed Anwar; Muhammad Usman
- Reference count: 0
- ADTRS achieves state-of-the-art results on five popular IQA datasets with PLCC of 0.972 and SRCC of 0.970 on LIVE dataset

## Executive Summary
This paper introduces ADTRS, a blind image quality assessment model that combines CNNs and Transformers to capture both local and non-local image features. The model incorporates relative ranking and self-consistency mechanisms to improve performance, achieving state-of-the-art results on five popular IQA datasets. ADTRS effectively fuses local features from ResNet50 with global context from Transformer encoders, while using composite loss functions that include quality prediction, relative ranking, and self-consistency objectives.

## Method Summary
ADTRS uses a dual-architecture approach where ResNet50 extracts local features from input images, which are then processed by Transformer encoders to capture long-range dependencies. The model employs a composite loss function combining quality prediction loss, relative ranking loss (using triplet loss format to compare image qualities within batches), and self-consistency loss (enforcing prediction consistency between original and horizontally flipped images). The model is trained on five IQA datasets with synthetically or naturally distorted images, extracting random patches per image for training.

## Key Results
- Achieves PLCC of 0.972 and SRCC of 0.970 on LIVE dataset, surpassing previous methods
- Strong performance on other datasets: CSIQ, TID2013, LIVE-C, and KonIQ10K
- Outperforms existing algorithms especially on smaller datasets
- Success attributed to effective fusion of local and non-local features with relative ranking and self-consistency losses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual feature extraction approach combining CNNs and Transformers captures both local and non-local image quality features effectively.
- Mechanism: CNNs extract local features capturing fine-grained spatial details, while Transformers process these features sequentially to capture long-range dependencies and global context.
- Core assumption: Local features from CNNs contain sufficient information for Transformers to build meaningful global representations.
- Evidence anchors:
  - [abstract] "The utilization of Transformer encoders aims to mitigate locality bias and generate a non-local representation by sequentially processing CNN features, which inherently capture local visual structures."
  - [section] "For an input image I defined in the space R3×m×n with dimensions m and n symbolizing the width and height, respectively, the objective is to evaluate its perceptual QS. A CNN, represented by fϕ with learnable parameters ϕ, is utilized to extract features Fi from the ith block..."
  - [corpus] Weak evidence - no direct mention of similar dual-architecture approaches in related papers.

### Mechanism 2
- Claim: Relative ranking loss improves model performance by explicitly modeling the ordinal relationships between image qualities.
- Mechanism: By comparing the highest, second highest, lowest, and second lowest quality scores within batches, the model learns to rank images correctly rather than just predicting absolute values.
- Core assumption: Quality scores have meaningful ordinal relationships that can be exploited during training.
- Evidence anchors:
  - [abstract] "Establishing a stronger connection between subjective and objective assessments is achieved through sorting within batches of images based on relative distance information."
  - [section] "In image batch B, qamax, qa′max, qamin, and qa′min represent predicted qualities for the highest, second highest, lowest, and second lowest subjective quality scores, respectively. Utilizing triplet loss with d(x, y) = |x − y|..."
  - [corpus] No direct evidence in related papers about relative ranking approaches for IQA.

### Mechanism 3
- Claim: Self-consistency mechanism enhances model robustness by enforcing consistency between original and horizontally flipped images.
- Mechanism: The model learns that horizontally flipped versions of the same image should produce similar quality predictions, creating a self-supervised regularization signal.
- Core assumption: Human subjective quality ratings remain consistent for flipped images.
- Evidence anchors:
  - [abstract] "A self-consistency approach to self-supervision is presented, explicitly addressing the degradation of no-reference image quality assessment (NR-IQA) models under equivariant transformations."
  - [section] "Given that human subjective scores remain consistent for the horizontally flipped version of the input image, we anticipate ζϵ,conv(I) = ζϵ,conv(τ (I)) and ζψ,atten(I) = ζψ,atten(τ (I)), where τ signifies the horizontal flipping transformation."
  - [corpus] Weak evidence - no direct mention of self-consistency mechanisms in related papers.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: Transformers capture long-range dependencies that CNNs miss, essential for global quality assessment
  - Quick check question: How does multi-head attention in Transformers help capture different types of relationships between image regions?

- Concept: Loss function design for regression and ranking tasks
  - Why needed here: The composite loss function combines quality prediction, relative ranking, and self-consistency objectives
  - Quick check question: Why does the relative ranking loss use triplet loss format rather than direct regression?

- Concept: Image quality assessment metrics (SRCC and PLCC)
  - Why needed here: These metrics evaluate both correlation and monotonicity between predicted and subjective scores
  - Quick check question: What's the key difference between Spearman's Rank-Order Correlation Coefficient and Pearson's Linear Correlation Coefficient?

## Architecture Onboarding

- Component map:
  - Input: Raw image tensor (3×m×n)
  - CNN Backbone: ResNet50 feature extraction
  - Feature Processing: Normalization, pooling, dropout
  - Transformer Encoder: Multi-head self-attention processing
  - Fusion Layer: Fully connected layers combining CNN and Transformer features
  - Output: Quality scores and relative rankings
  - Loss Functions: Quality loss, relative ranking loss, self-consistency loss

- Critical path:
  1. Image → CNN feature extraction
  2. CNN features → Transformer encoding
  3. Transformer output → Fully connected fusion
  4. Fusion output → Quality predictions
  5. Predictions → Composite loss calculation

- Design tradeoffs:
  - ResNet50 vs smaller backbones: Larger models provide better accuracy but slower processing
  - Number of Transformer layers: More layers capture more complex relationships but increase computational cost
  - Batch size: Larger batches improve training stability but require more memory
  - Patch size: Larger patches capture more context but reduce the number of samples per image

- Failure signatures:
  - Poor SRCC but good PLCC: Model predicts correct magnitudes but wrong rankings
  - Good training loss but poor test performance: Overfitting, likely need more regularization
  - Self-consistency loss dominates: Model focusing too much on transformations rather than quality prediction
  - Relative ranking loss not converging: Margin values may be incorrectly set

- First 3 experiments:
  1. Test with ResNet50 backbone on LIVE dataset only, using only quality loss
  2. Add relative ranking loss to LIVE dataset experiment
  3. Add self-consistency loss to previous configuration and evaluate on all five datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ADTRS performance scale with increasing dataset size and diversity compared to other NR-IQA methods?
- Basis in paper: [inferred] The paper mentions that ADTRS performs well on smaller datasets and real distorted datasets like LIVE-C and KonIQ10K, but does not explicitly test scaling performance on much larger or more diverse datasets.
- Why unresolved: The paper only evaluates on five specific datasets and does not explore performance trends as dataset characteristics change significantly.
- What evidence would resolve it: Testing ADTRS on a wide range of dataset sizes and diversities, including datasets significantly larger than KonIQ10K and with different types of distortions, would clarify its scaling behavior.

### Open Question 2
- Question: How does the choice of backbone architecture (e.g., ResNet50 vs. smaller models) affect the trade-off between model performance and computational efficiency in ADTRS?
- Basis in paper: [explicit] The paper mentions experimenting with different backbone architectures like ResNet34 and ResNet18, but does not provide a detailed analysis of the performance-efficiency trade-off.
- Why unresolved: The paper only states that smaller backbones yielded worse results without quantifying the performance difference or discussing the computational benefits.
- What evidence would resolve it: A systematic comparison of different backbone architectures, including their performance on various datasets and their computational requirements, would provide insights into the optimal balance between accuracy and efficiency.

### Open Question 3
- Question: How does the self-consistency mechanism in ADTRS generalize to other types of equivariant transformations beyond horizontal flipping?
- Basis in paper: [explicit] The paper describes the self-consistency mechanism using horizontal flipping as the equivariant transformation, but does not explore its applicability to other transformations.
- Why unresolved: The paper only demonstrates the effectiveness of self-consistency with one specific transformation, leaving open the question of its generalizability to other transformations that might be more relevant for different image quality assessment tasks.
- What evidence would resolve it: Testing the self-consistency mechanism with various types of equivariant transformations, such as rotation or scaling, and evaluating its impact on model performance would determine its broader applicability.

## Limitations

- The specific transformer encoder architecture details (number of layers, attention heads) are not fully specified in the paper, which could affect reproducibility
- The exact margin values for relative ranking loss (margin1 and margin2) are not provided, potentially impacting the relative ranking performance
- While the paper reports strong performance on synthetic datasets, the relatively fewer results on the larger KonIQ10K dataset (only 4 metrics reported) raises questions about real-world generalization

## Confidence

- High confidence in the dual architecture approach combining CNNs and Transformers for local and non-local feature extraction
- Medium confidence in the relative ranking loss effectiveness due to limited comparison with alternative ranking methods
- Medium confidence in the self-consistency mechanism as its contribution could be partially attributed to the dropout layers in the model

## Next Checks

1. Verify the relative ranking loss implementation by testing different margin values on the LIVE dataset to identify optimal settings
2. Conduct ablation studies removing the self-consistency loss to quantify its specific contribution to overall performance
3. Test the model's robustness to different patch extraction strategies to validate the patch-based training approach effectiveness