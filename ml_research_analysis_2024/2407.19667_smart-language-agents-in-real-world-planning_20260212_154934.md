---
ver: rpa2
title: Smart Language Agents in Real-World Planning
arxiv_id: '2407.19667'
source_url: https://arxiv.org/abs/2407.19667
tags:
- prompt
- planning
- constraints
- llms
- human-in-the-loop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a semi-automated prompt generation framework
  that combines LLM-automated prompt generation with human-in-the-loop iteration to
  improve travel planning agent performance. The method uses GPT-4o to automatically
  generate initial prompts from evaluation scripts and reference data, then iteratively
  refines these prompts based on manual failure analysis of training data.
---

# Smart Language Agents in Real-World Planning

## Quick Facts
- arXiv ID: 2407.19667
- Source URL: https://arxiv.org/abs/2407.19667
- Reference count: 5
- One-line primary result: Semi-automated prompt generation with human-in-the-loop iteration improves travel planning agent performance by 139% over initial automated prompt

## Executive Summary
This paper introduces a semi-automated prompt generation framework that combines LLM-automated prompt creation with human-in-the-loop iteration to enhance travel planning agent performance. The framework uses GPT-4o to automatically generate initial prompts from evaluation scripts and reference data, then iteratively refines these prompts based on manual failure analysis of training data. The method addresses the challenge of creating effective prompts for constraint-based planning tasks while reducing the labor intensity of manual prompt engineering.

The approach demonstrates that while automated prompt generation alone underperforms baseline methods, incorporating even a single iteration of human feedback can significantly improve performance, achieving results approaching those of manually-crafted prompts. The framework shows particular promise for applications where constraint satisfaction is critical and where the cost of human intervention can be amortized across multiple planning tasks.

## Method Summary
The semi-automated prompt generation framework operates through a three-step process: first, GPT-4o automatically generates an initial prompt by summarizing evaluation scripts and reference data; second, the LLM planner (GPT-4 Turbo) generates travel plans using this automated prompt; third, human analysts examine failed plans to identify patterns and manually refine the prompt with targeted examples or rules. The framework emphasizes structured data formats, converting reference information from unstructured JSON to CSV files to improve LLM processing efficiency. The human-in-the-loop component focuses on failure analysis of the most challenging plans, using insights to augment the prompt with specific corrective examples or rules.

## Key Results
- Initial automated prompt generated by GPT-4o achieves only 2.78% final pass rate, significantly underperforming baseline (5.55%)
- One iteration of human-in-the-loop feedback increases performance by 139% to 6.67% final pass rate, exceeding baseline and approaching manual prompt performance (12.78%)
- Structured CSV reference format improves LLM ability to parse and utilize constraint information compared to unstructured JSON
- Framework demonstrates potential for improving LLM performance in constraint-based planning through minimal human intervention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-in-the-loop iteration improves prompt quality by targeting specific failure cases
- Mechanism: The framework identifies failed plans in training data through manual failure analysis, then augments the automated prompt with corrected examples or rules addressing those specific weaknesses. This targeted refinement leads to measurable performance improvements (139% increase in final pass rate with one iteration).
- Core assumption: Human analysis can effectively identify the root causes of LLM planning failures and translate those into prompt improvements that generalize across similar cases.
- Evidence anchors:
  - [abstract] "one iteration of human-in-the-loop feedback increases performance by 139% to 6.67% final pass rate"
  - [section 4.4] "To further enhance the prompt, we added a manually selected example from the TravelPlanner training dataset's most challenging plans"
- Break condition: If the cost of human analysis exceeds the performance gains, or if the manual analysis fails to identify generalizable patterns that improve performance across multiple test cases.

### Mechanism 2
- Claim: Structured reference information improves LLM planning performance compared to unstructured JSON
- Mechanism: Converting reference data from unstructured CSV-within-JSON format to separate CSV files allows the LLM to more easily parse and utilize the constraint information during planning, leading to better adherence to budget, timing, and attraction diversity constraints.
- Core assumption: LLMs can better process and reason about tabular data formats (CSV) than embedded text within JSON structures when planning under constraints.
- Evidence anchors:
  - [section 4.2] "restructuring the reference information as a series of CSV files would allow the travel-planning agent to perform better"
  - [section 4.4] "we created a script that converted each raw JSON reference information into a list of CSV files"
- Break condition: If the LLM's ability to parse structured formats doesn't translate to improved constraint adherence, or if the overhead of managing multiple files outweighs the benefits.

### Mechanism 3
- Claim: Automated prompt generation provides a foundation that can be efficiently refined through human feedback
- Mechanism: GPT-4o automatically extracts rules from evaluation scripts and reference data to create an initial prompt, reducing the labor intensity of prompt creation. This initial prompt serves as a baseline that human analysts can improve through targeted iterations based on failure analysis.
- Core assumption: The automated extraction of rules captures the essential constraints and requirements, even if imperfectly, providing a useful starting point for human refinement.
- Evidence anchors:
  - [abstract] "Our framework demonstrates potential for improving LLM performance in constraint-based planning tasks through minimal human intervention"
  - [section 3] "The 1st step is creating an initial prompt by LLMs through its automatic summarization of external resources"
- Break condition: If the automated extraction produces prompts that are fundamentally misaligned with evaluation criteria, making human refinement ineffective or requiring complete prompt reconstruction.

## Foundational Learning

- Concept: Constraint satisfaction problems (CSPs) and their theoretical foundations
  - Why needed here: The travel planning task is fundamentally a multi-constraint optimization problem where the LLM must satisfy hard constraints (budget, room rules) and commonsense constraints (diverse attractions, reasonable driving times) simultaneously.
  - Quick check question: What are the three types of constraints identified in the related work, and how do they differ in terms of flexibility and evaluation?

- Concept: Large language model prompting strategies and prompt engineering
  - Why needed here: The framework relies on both automated and manual prompt generation techniques, including the strategic use of examples, rules extraction, and iterative refinement based on failure analysis.
  - Quick check question: Why did the authors choose to avoid including examples in the initial automated prompt generation?

- Concept: Evaluation methodologies for planning agents
  - Why needed here: The framework uses a specific evaluation algorithm that distinguishes between commonsense and hard constraints, requiring understanding of how to measure plan feasibility, reliability, and adherence to requirements.
  - Quick check question: How does the evaluation algorithm differentiate between passing a commonsense constraint versus a hard constraint?

## Architecture Onboarding

- Component map: Evaluation scripts/reference data → GPT-4o prompt generation → GPT-4 Turbo planning → Evaluation → Failure analysis → Prompt refinement → Re-planning

- Critical path: Reference data → GPT-4o prompt generation → GPT-4 Turbo planning → Evaluation → Failure analysis → Prompt refinement → Re-planning

- Design tradeoffs:
  - Automation vs. manual effort: Initial prompt generation is automated but requires human analysis for refinement
  - Structured vs. unstructured data: CSV format improves LLM performance but adds data management complexity
  - Iteration depth: Single iteration shows significant improvement, but additional iterations may yield diminishing returns
  - Cost vs. performance: GPT-4 Turbo (expensive) for planning vs. GPT-4o (cheaper) for prompt generation

- Failure signatures:
  - Low commonsense pass rate indicates issues with logical planning or diversity requirements
  - Low hard constraint pass rate suggests problems with budget adherence or rule compliance
  - Poor initial automated prompt performance but good improvement after iteration indicates the framework works but needs better automation
  - No improvement after iteration suggests failure analysis isn't identifying correct patterns

- First 3 experiments:
  1. Run baseline GPT-4 Turbo with unstructured data to establish performance metrics
  2. Generate initial GPT-4o prompt and evaluate against same test set
  3. Perform manual failure analysis on 10-15 failed plans and augment prompt with specific examples or rules addressing identified weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How many iterations of human-in-the-loop feedback are needed to achieve optimal performance, and what is the diminishing returns curve?
- Basis in paper: [explicit] The paper states "we only ran this automation module loop once" and "we are hopeful the gaps will become smaller with more iterations" but does not empirically test multiple iterations.
- Why unresolved: The authors only tested one iteration and speculate about further improvements without providing data on multiple iterations.
- What evidence would resolve it: Empirical results showing performance improvements (or lack thereof) after 2, 3, 4+ iterations, with a clear graph or table demonstrating the diminishing returns curve.

### Open Question 2
- Question: Can the human-in-the-loop component be automated using another LLM agent, and how would this affect scalability and performance?
- Basis in paper: [explicit] The authors acknowledge that "human-in-the-loop is inherently not scalable due to the manual nature of the analysis, and another LLM agent should be designed for alleviating this drawback."
- Why unresolved: The paper proposes the need for automation but does not implement or test an automated feedback mechanism.
- What evidence would resolve it: Results comparing human-in-the-loop performance against an automated LLM feedback system, measuring both performance metrics and scalability (time/cost per iteration).

### Open Question 3
- Question: How does the semi-automated prompt framework generalize to other constraint-based planning domains beyond travel planning?
- Basis in paper: [inferred] The authors suggest broader applicability ("While we have shown that this framework is applicable to travel planning, we have not tested it with other use cases") but do not test other domains.
- Why unresolved: The framework is only validated on the travel planning benchmark, and the paper does not provide evidence of its effectiveness in other constraint-based scenarios.
- What evidence would resolve it: Successful application and performance results of the framework on at least two different constraint-based planning domains (e.g., course scheduling, resource allocation, or manufacturing planning).

## Limitations

- The human-in-the-loop component remains a scalability bottleneck, as manual failure analysis is labor-intensive and doesn't scale well for large datasets or real-time applications.
- Automated prompt generation shows poor initial performance (2.78% final pass rate), indicating the rule extraction and prompt synthesis capabilities may be insufficient for complex constraint satisfaction problems without substantial human intervention.
- The framework's effectiveness is limited to domains where failure patterns can be identified and codified through human analysis, potentially excluding problems with highly stochastic or unpredictable failure modes.

## Confidence

- High confidence in the core finding that human-in-the-loop iteration improves travel planning agent performance
- Medium confidence in the structured data format improvement claim
- Low confidence in the scalability of the automated prompt generation component

## Next Checks

1. **Automated Prompt Quality Analysis**: Conduct ablation studies to determine which specific components of the GPT-4o automated prompt generation are most responsible for the poor initial performance (2.78% final pass rate), and test whether fine-tuning the prompt generation model on TravelPlanner data improves baseline performance.

2. **Scaling Human-in-the-Loop**: Test the framework's performance with multiple iterations of human feedback (2-3 rounds) to determine if the 139% improvement per iteration follows a diminishing returns pattern, and measure the time/cost tradeoff for each iteration.

3. **Cross-Domain Generalization**: Apply the semi-automated prompt generation framework to a different planning domain (e.g., conference scheduling or meal planning) using the same TravelPlanner evaluation methodology to validate whether the human-in-the-loop improvement pattern generalizes beyond travel planning.