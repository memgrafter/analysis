---
ver: rpa2
title: Can LLMs Improve Multimodal Fact-Checking by Asking Relevant Questions?
arxiv_id: '2410.04616'
source_url: https://arxiv.org/abs/2410.04616
tags:
- image
- fcqs
- questions
- fact-checking
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LRQ-FACT, a framework that leverages large
  language models (LLMs) to generate targeted fact-checking questions (FCQs) for multimodal
  misinformation detection. LRQ-FACT generates both visual and textual FCQs to probe
  consistency between news articles and images, then uses a rule-based decision-maker
  to classify news into four categories: Real, Textual Veracity Distortion, Visual
  Veracity Distortion, and Cross-Modal Mismatch.'
---

# Can LLMs Improve Multimodal Fact-Checking by Asking Relevant Questions?

## Quick Facts
- **arXiv ID**: 2410.04616
- **Source URL**: https://arxiv.org/abs/2410.04616
- **Reference count**: 40
- **Primary result**: LRQ-FACT achieves up to 45.5% improvement in F1 score on MMFakeBench by generating targeted LLM-based fact-checking questions

## Executive Summary
This paper introduces LRQ-FACT, a framework that leverages large language models to generate targeted fact-checking questions (FCQs) for multimodal misinformation detection. The framework generates both visual and textual FCQs to probe consistency between news articles and images, then uses a rule-based decision-maker to classify news into four categories: Real, Textual Veracity Distortion, Visual Veracity Distortion, and Cross-Modal Mismatch. Human and LLM-based evaluations show that 73% of visual and 93.6% of textual FCQs are relevant. Extensive experiments on three datasets demonstrate that LRQ-FACT significantly outperforms baseline methods.

## Method Summary
LRQ-FACT is a multimodal fact-checking framework that generates both visual and textual fact-checking questions (FCQs) using LLMs. The method first creates image descriptions using VLMs, then generates targeted FCQs for both modalities. Visual FCQs are answered by VLMs while textual FCQs are answered using Retrieval-Augmented Generation (RAG) with Google Search. A rule-based decision-maker classifies news into four categories based on the consistency of answers. The framework is evaluated on three benchmark datasets (MMFakeBench, DGM4, Factify) using macro-F1 and macro-accuracy metrics, showing significant improvements over baseline VLMs.

## Key Results
- LRQ-FACT achieves up to 45.5% improvement in F1 score on MMFakeBench dataset
- 73% of visual FCQs and 93.6% of textual FCQs are deemed relevant by human evaluators
- The framework significantly outperforms baseline VLMs (InstructBLIP, VILA, BLIP-2, LLaVA-1.6, GPT-4V-1.7T, GPT-4o) across all three datasets
- Ablation studies confirm that incorporating LLM-generated FCQs consistently improves multimodal fact-checking performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated questions can effectively guide the fact-checking process by structuring the verification into targeted queries.
- Mechanism: By generating focused visual and textual questions based on the news article, the model breaks down the verification process into specific, manageable sub-tasks that probe both content and authenticity.
- Core assumption: Structured questioning improves the model's ability to detect inconsistencies and misinformation compared to free-form verification.
- Evidence anchors:
  - [abstract] "LRQ-FACT generates both visual and textual FCQs to probe consistency between news articles and images"
  - [section] "LRQ-FACT first generates two types of FCQs: (1) visual FCQs, which assess whether an image accurately represents critical details such as people, objects, or events mentioned in the text, and (2) textual FCQs, which question whether the textual claims or statements are supported by evidence."
  - [corpus] Weak evidence - no direct corpus mention of structured questioning effectiveness.
- Break condition: If the LLM fails to generate contextually relevant questions or the questions become too broad to be actionable.

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) significantly improves fact-checking accuracy by grounding responses in verifiable external evidence.
- Mechanism: By retrieving relevant documents from Google Search and using them as context for the LLM, the model can verify claims against up-to-date and authoritative sources rather than relying solely on its internal knowledge.
- Core assumption: External evidence retrieval provides more accurate verification than internal LLM knowledge alone.
- Evidence anchors:
  - [abstract] "The up-to-date online information is particularly valuable when fact-checking claims related to emerging or rapidly evolving events, where LLMs often lack sufficient ground truth knowledge."
  - [section] "To enhance factual reliability, we employ Retrieval-Augmented Generation (RAG), which grounds responses in external, verifiable sources."
  - [corpus] Weak evidence - no direct corpus mention of RAG effectiveness in fact-checking.
- Break condition: If the retrieved documents are irrelevant, outdated, or of low quality, leading to incorrect verification outcomes.

### Mechanism 3
- Claim: Cross-modal consistency checking improves detection of multimodal misinformation by comparing textual claims with visual evidence.
- Mechanism: By generating both visual and textual questions and comparing their answers, the system can detect inconsistencies between text and image content that might indicate manipulation or misrepresentation.
- Core assumption: Comparing answers from both modalities reveals inconsistencies that single-modality analysis would miss.
- Evidence anchors:
  - [abstract] "LRQ-FACT generates both visual and textual FCQs to probe consistency between news articles and images"
  - [section] "We define the task of multimodal fact-checking as a multiclass classification problem... categories: Real, Textual Veracity Distortion, Visual Veracity Distortion, and Cross-Modal Mismatch."

## Foundational Learning

### Multimodal Misinformation Detection
- **Why needed**: Modern misinformation often combines deceptive text with manipulated or mismatched images, requiring analysis of both modalities simultaneously
- **Quick check**: Can detect cases where text claims one thing but image shows something different, or where either modality contains false information

### Retrieval-Augmented Generation (RAG)
- **Why needed**: LLMs have knowledge cutoffs and may hallucinate facts; RAG grounds responses in verifiable external evidence
- **Quick check**: Retrieves relevant documents from search engines to verify claims against up-to-date, authoritative sources

### Visual Language Models (VLMs)
- **Why needed**: VLMs can understand and reason about visual content, answering questions about image content and detecting manipulations
- **Quick check**: Can generate image descriptions and answer visual fact-checking questions about people, objects, and events in images

## Architecture Onboarding

### Component Map
Image/Text Input -> VLM Description Generator -> LLM FCQ Generator -> VLM/RAG Answer Module -> Rule-Based Decision-Maker -> Classification Output

### Critical Path
Image/Text → FCQ Generation → Answer Generation → Decision Making

### Design Tradeoffs
- Uses multiple LLMs (GPT-4o, Paligemma, LLaMA 3.1) for different tasks vs. single model approach
- Combines RAG with VLM answers vs. relying solely on model knowledge
- Rule-based decision-maker vs. learned classifier

### Failure Signatures
- Poor FCQ generation leading to irrelevant questions
- RAG retrieving irrelevant or outdated documents
- Rule-based classifier making incorrect decisions due to threshold issues
- VLM failing to accurately describe or answer visual questions

### 3 First Experiments
1. Generate and evaluate FCQs on a small sample to assess relevance
2. Test RAG retrieval with sample queries to verify document relevance
3. Run decision-maker on pre-generated answers to validate classification logic

## Open Questions the Paper Calls Out

The paper acknowledges that FCQs were evaluated by PhD students but notes this does not fully represent expert-level validation. The authors suggest that professional fact-checkers across different domains (medical, legal, political) would provide more rigorous evaluation of FCQ quality and relevance.

## Limitations

- Evaluation relies on human annotations and LLM-based evaluations without detailed inter-annotator agreement statistics
- Rule-based decision-maker's thresholds and rules are described but not fully specified, creating potential reproducibility issues
- Performance improvements (45.5% F1 gain) appear to be on specific subsets and may not generalize across all real-world scenarios

## Confidence

- **High confidence**: Core methodology (LLM-generated questions + RAG + VLM integration) and claim that structured questioning improves fact-checking performance
- **Medium confidence**: Specific performance improvements (45.5% F1 gain) due to limited dataset details and potential cherry-picking of results
- **Low confidence**: Generalizability of results across different domains and robustness of rule-based decision-maker without full specification

## Next Checks

1. **Ablation Study Extension**: Conduct additional ablation experiments to isolate the individual contributions of visual FCQs, textual FCQs, and RAG components across all three datasets, not just MMFakeBench, to verify the claimed improvements hold consistently.

2. **Decision-Maker Rule Validation**: Implement and test the rule-based decision-maker with varying threshold parameters to assess its sensitivity and robustness, including stress-testing with adversarial examples that could break the logic.

3. **Human Evaluation Replication**: Replicate the human evaluation of FCQ relevance with multiple annotators and calculate inter-annotator agreement scores to validate the claimed 73% and 93.6% relevance rates, ensuring they're not inflated by confirmation bias or subjective interpretation.