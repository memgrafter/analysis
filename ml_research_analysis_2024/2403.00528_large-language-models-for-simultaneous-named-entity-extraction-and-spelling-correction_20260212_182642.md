---
ver: rpa2
title: Large Language Models for Simultaneous Named Entity Extraction and Spelling
  Correction
arxiv_id: '2403.00528'
source_url: https://arxiv.org/abs/2403.00528
tags:
- data
- text
- character
- which
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Large Language Models (LLMs)
  for simultaneous Named Entity (NE) extraction and spelling correction from Japanese
  shop receipt images processed through OCR. The authors fine-tune two BERT models
  and eight open-source LLMs to generate NEs from OCR text by formulating the task
  as question answering via text completion.
---

# Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction

## Quick Facts
- arXiv ID: 2403.00528
- Source URL: https://arxiv.org/abs/2403.00528
- Reference count: 13
- Primary result: LLMs slightly outperform BERT models (85.6 vs 84.6 F-measure) for simultaneous NE extraction and spelling correction from Japanese receipt OCR

## Executive Summary
This paper investigates using Large Language Models (LLMs) for simultaneous Named Entity (NE) extraction and spelling correction from Japanese shop receipt images processed through OCR. The authors fine-tune two BERT models and eight open-source LLMs to generate NEs from OCR text by formulating the task as question answering via text completion. They compare performance across six key NE categories (shop name, address, item, telephone, date, total) using precision, recall, and weighted F-measure. The best LLM, rinna/youri-7b, slightly outperforms the best BERT model, demonstrating that LLMs can correct some OCR errors, especially for categories like address and shop name.

## Method Summary
The study uses 968 Japanese shop receipt images with manual annotations for up to 344 NE categories, split into train (872), validation (18), and test (78) sets. Models are fine-tuned using LoRA (r=16, alpha=32, dropout=0.05) on instruction-style question-answer pairs where receipt text and NE category are embedded in prompts ending with "です。". Both BERT and decoder-only LLMs are trained to generate NE surface forms, with evaluation using precision, recall, and weighted F-measure across six NE categories. The approach leverages the generative nature of LLMs to potentially recover correct spellings even when input tokens are corrupted.

## Key Results
- Best LLM (rinna/youri-7b) achieved F-measure of 85.6 vs 84.6 for best BERT on test data
- LLMs demonstrated ability to correct OCR errors, particularly for address and shop name categories
- Performance varied significantly by NE category and error type
- LoRA parameter-efficient fine-tuning enabled efficient adaptation of large LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate NE surface forms that correct OCR errors when fine-tuned on QA pairs
- Mechanism: Generative decoder-only LLMs can sample from learned probability distributions conditioned on receipt text and NE category, potentially recovering correct spellings even when input tokens are corrupted
- Core assumption: Pre-trained LLM contains sufficient linguistic knowledge to reconstruct correct NE forms, and fine-tuning on QA pairs teaches it to prefer these over erroneous input forms
- Evidence anchors: Abstract states LLMs can recover correct surface forms; section 2.2 describes formulation as maximum likelihood answer selection; corpus neighbors focus on spelling correction but not NE extraction from OCR

### Mechanism 2
- Claim: Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large LLMs without full weight updates
- Mechanism: LoRA learns a low-rank decomposition of weight updates, freezing original parameters and only training small LoRA matrices, drastically reducing memory and compute requirements while maintaining performance
- Core assumption: Pre-trained LLM's weights contain a low-dimensional subspace relevant to target task which LoRA can approximate efficiently
- Evidence anchors: Section 2.1 describes LoRA as optimizing only parameters of low-rank representation; section 3.3.2 details specific LoRA hyper-parameters used

### Mechanism 3
- Claim: Formulating NE extraction as question answering via text completion aligns with LLM training objectives
- Mechanism: Embedding receipt text and NE category into declarative prompt ending with "です。" allows LLM to complete sentence with correct NE surface form, leveraging pre-trained language modeling capabilities
- Core assumption: Prompt structure uniquely identifies instruction-response pair across different tokenizers, and LLM can generalize from QA pairs to novel receipt inputs
- Evidence anchors: Section 2.3 provides exact prompt template and explains character sequence choices; section 3.3.2 describes generation parameters used during inference

## Foundational Learning

- Concept: Optical Character Recognition (OCR) and its error characteristics
  - Why needed here: Entire task assumes receipt images converted to text via OCR, introducing errors models must handle
  - Quick check question: What types of character confusions are most common in OCR of Japanese receipts (e.g., visually similar characters, font issues)?

- Concept: Named Entity (NE) recognition and categorization
  - Why needed here: Models must identify and classify specific NE types (shop name, address, item, telephone, date, total) from receipt text
  - Quick check question: How does the BIO tagging scheme work, and why was it chosen over B- and I- tag combinations in this study?

- Concept: Language model fine-tuning strategies (supervised vs. unsupervised)
  - Why needed here: Paper compares supervised fine-tuning of BERT and LLM models on QA pairs vs. using pre-trained weights directly
  - Quick check question: What are the key differences between classifier-based and generative approaches to NE extraction?

## Architecture Onboarding

- Component map: Receipt images -> OCR text -> Fine-tuned LLM/BERT models -> Extracted NEs (with potential spelling correction) -> Evaluation metrics

- Critical path: 1) OCR converts receipt image to text 2) Text and NE category embedded in prompt 3) Fine-tuned model generates answer 4) Answer compared to ground truth for evaluation

- Design tradeoffs:
  - BERT: Faster, explicit NE location identification, but no spelling correction
  - LLM: Potential spelling correction, but slower, no explicit location
  - LoRA vs. full fine-tuning: Memory/compute efficiency vs. potential performance

- Failure signatures:
  - Low precision: Model generating incorrect NEs or hallucinating
  - Low recall: Model failing to generate NEs that exist in ground truth
  - Tokenizer mismatch: Prompt not properly parsed across different models

- First 3 experiments:
  1. Run inference on test set with best LLM configuration, verify output format matches expectations
  2. Compare BERT vs. LLM performance on a single NE category to understand tradeoffs
  3. Test spelling correction capability by manually corrupting test inputs and checking outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of LLMs compare to BERT when trained on full set of 344 NE categories available in dataset?
- Basis in paper: [explicit] Paper mentions multilingual BERT LM performs even better when trained on all 344 NE categories, and suggests training on full set would be essential for production system
- Why unresolved: Study only used six key NE categories due to computational constraints, leaving comparative performance on full set unexplored
- What evidence would resolve it: Comprehensive experiment training both LLMs and BERT models on all 344 NE categories, comparing precision, recall, and F-measure across all categories

### Open Question 2
- Question: To what extent does spelling-correction ability of LLMs come from knowledge inherited from pre-training versus exposure during fine-tuning?
- Basis in paper: [explicit] Paper notes best fine-tuned LLM demonstrated some ability to correct OCR errors, but unclear whether this ability came from fine-tuning or from knowledge contained in pre-trained LLM
- Why unresolved: Experiments did not isolate source of spelling-correction ability by testing pre-trained LLM without fine-tuning or using larger confusion matrix
- What evidence would resolve it: Testing pre-trained LLM on same OCR error correction task without fine-tuning, and expanding confusion matrix to include more diverse character substitutions

### Open Question 3
- Question: Would full parameter fine-tuning of LLMs yield better performance than using LoRA for this NE extraction task?
- Basis in paper: [explicit] Paper used LoRA for parameter-efficient fine-tuning and mentions full parameter fine-tuning of BERT LMs showed good results, suggesting it might be beneficial for LLMs as well
- Why unresolved: Study did not explore full fine-tuning of LLMs due to computational constraints, leaving potential performance benefits untested
- What evidence would resolve it: Conducting full fine-tuning of LLMs on same task and comparing performance metrics against LoRA-based fine-tuned models

### Open Question 4
- Question: How does choice of prompt format affect performance of LLMs in this NE extraction task?
- Basis in paper: [explicit] Paper used uniform prompt format for all LLMs to avoid tokenization issues but notes this may not be optimal for each individual LLM and that instruction-tuned variants might perform better
- Why unresolved: Study did not optimize prompt format for each LLM or explore instruction-tuned variants, leaving impact of prompt design unexplored
- What evidence would resolve it: Experimenting with different prompt formats and instruction-tuned LLMs, then comparing performance metrics to determine optimal configuration

### Open Question 5
- Question: Would including all alternative correct NEs in training data improve performance of models?
- Basis in paper: [explicit] Paper mentions data typically has multiple annotated ground-truth NEs of same category, but only first NE was used as target during training
- Why unresolved: Study did not include all alternative correct NEs as additional training examples, leaving potential impact of this approach unexplored
- What evidence would resolve it: Training models with datasets that include all alternative correct NEs for each category and comparing performance metrics to models trained with only first NE

## Limitations
- Synthetic OCR error generation uses estimated confusion matrix based on limited real data, may not capture full distribution of OCR errors
- Evaluation focuses on only six NE categories, potentially missing broader generalizability to other receipt types or domains
- Mechanism by which LLMs correct OCR errors is not fully explored - unclear whether models are truly "understanding" correct form or simply memorizing patterns

## Confidence
- High Confidence: BERT models' performance metrics are reliable using standard evaluation protocols and well-established classification tasks; LoRA fine-tuning methodology is well-documented with clearly stated parameters
- Medium Confidence: LLM performance comparison is credible but less certain due to novelty of using decoder-only models for this task formulation; modest difference (85.6 vs 84.6 F-measure) may not justify increased computational cost
- Low Confidence: Claims about general applicability to other receipt types or languages are speculative; mechanism by which LLMs correct OCR errors is not well understood and may not generalize beyond specific Japanese receipt domain

## Next Checks
1. Systematically categorize OCR errors in test set by type (visual similarity, font issues, segmentation errors) and measure how each error type affects LLM vs BERT performance to validate whether LLMs truly "correct" errors or simply handle certain error patterns better

2. Test best fine-tuned models on receipts from different stores, regions, or countries to assess whether spelling correction capability generalizes beyond specific Japanese receipt domain used in training

3. Experiment with removing explicit "です。" stop token, varying prompt structure, or using different temperature settings to determine which aspects of prompt formulation are most critical for models' performance, particularly for spelling correction