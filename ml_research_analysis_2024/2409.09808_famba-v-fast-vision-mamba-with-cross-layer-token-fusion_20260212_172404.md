---
ver: rpa2
title: 'Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion'
arxiv_id: '2409.09808'
source_url: https://arxiv.org/abs/2409.09808
tags:
- token
- fusion
- famba-v
- vision
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Famba-V, a cross-layer token fusion technique\
  \ designed to enhance the training efficiency of Vision Mamba (Vim) models. The\
  \ core idea is to identify and fuse similar tokens across different Vim layers using\
  \ three cross-layer strategies\u2014interleaved, lower-layer, and upper-layer token\
  \ fusion\u2014instead of applying fusion uniformly across all layers."
---

# Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion

## Quick Facts
- arXiv ID: 2409.09808
- Source URL: https://arxiv.org/abs/2409.09808
- Authors: Hui Shen; Zhongwei Wan; Xin Wang; Mi Zhang
- Reference count: 36
- Key outcome: Famba-V achieves up to 4.5 minutes faster training and 2.7 GB less peak memory usage on CIFAR-100 compared to baseline Vision Mamba models

## Executive Summary
Famba-V introduces a cross-layer token fusion technique for Vision Mamba (Vim) models that strategically identifies and fuses similar tokens across different layers rather than applying fusion uniformly. This approach reduces training time and memory usage while maintaining accuracy through selective fusion in specific layer subsets. Experiments demonstrate significant efficiency gains with the upper-layer fusion strategy offering the best accuracy-efficiency balance on CIFAR-100 benchmarks.

## Method Summary
Famba-V enhances Vision Mamba training efficiency by implementing selective cross-layer token fusion using cosine similarity to identify similar tokens across layers, which are then fused via averaging. Unlike previous all-layer fusion approaches, Famba-V employs three strategies - interleaved, lower-layer, and upper-layer fusion - applying fusion to specific layer subsets rather than uniformly across all layers. The class token is positioned at the sequence head to prevent fusion, preserving global representation. The method reduces computational load while maintaining accuracy by preserving tokens containing unique information.

## Key Results
- 4.5 minutes faster training time compared to baseline Vision Mamba models
- 2.7 GB reduction in peak memory usage during training
- Upper-layer token fusion strategy achieves optimal accuracy-efficiency tradeoff

## Why This Works (Mechanism)

### Mechanism 1
Cross-layer token fusion improves training efficiency by reducing tokens processed in later layers while preserving critical information through selective fusion. The method identifies similar tokens across layers using cosine similarity, fuses them via averaging, and applies fusion selectively to specific layer subsets rather than uniformly across all layers. This reduces computational load while maintaining accuracy by preserving tokens that carry unique information. Core assumption: Tokens that are similar across layers contain redundant information that can be merged without significant loss of representational capacity, and the remaining tokens contain sufficient information for accurate classification. Evidence: The key idea of Famba-V is to identify and fuse similar tokens across different Vim layers based on a suit of cross-layer strategies instead of simply applying token fusion uniformly across all the layers. Break condition: If tokens that appear similar actually represent distinct semantic features, or if the similarity measure fails to capture meaningful redundancy, the fusion could eliminate critical information and degrade model performance.

### Mechanism 2
The cross-layer strategies (interleaved, lower-layer, upper-layer) optimize the accuracy-efficiency tradeoff by strategically placing token fusion at layers where it has maximal impact with minimal accuracy loss. Different strategies place token fusion at different layer positions - interleaved applies fusion to alternate layers, lower-layer applies to early layers, and upper-layer applies to later layers. This exploits the observation that early layers may benefit more from fusion due to redundancy in low-level features, while upper layers preserve high-level reasoning capabilities. Core assumption: The impact of token fusion on accuracy depends on which layers it's applied to, with some layers being more tolerant to fusion than others due to the nature of features processed at different depths. Evidence: Among the three cross-layer token fusion strategies incorporated in Famba-V, the lower-layer token fusion strategy achieves the lowest accuracy, but provides the most significant reductions in training time and peak memory usage. Break condition: If the assumption about which layers are most tolerant to fusion is incorrect for specific datasets or model architectures, the strategy selection could lead to suboptimal performance.

### Mechanism 3
Placing the class token at the head of the sequence prevents it from being fused with other tokens, preserving global representation capability. The design choice to position the class token as the first token in the sequence ensures it's excluded from the token fusion process, maintaining its role as a global representation of the entire patch sequence. Core assumption: The class token's position in the sequence determines whether it's subject to fusion operations, and keeping it unfused preserves the model's ability to make accurate predictions. Evidence: The class token is designed to be placed at the head of the sequence so as to prevent it from being fused with other tokens by our proposed token fusion scheme. Break condition: If the class token's position doesn't actually protect it from fusion operations, or if the model can function without a preserved class token, this design choice may be unnecessary.

## Foundational Learning

- Concept: Vision Mamba (Vim) architecture and its adaptation from Mamba for vision tasks
  - Why needed here: Understanding how Vim processes images through patch tokenization and state space modeling is essential to grasp why token fusion can improve efficiency without sacrificing accuracy
  - Quick check question: How does Vim transform a 2D image into a sequence of tokens, and what role does the class token play in this process?

- Concept: State Space Models (SSMs) and their advantages over attention mechanisms
  - Why needed here: Famba-V builds on Mamba, which uses SSMs instead of attention. Understanding the linear time complexity and hardware-aware design of SSMs explains why Mamba-based models are attractive for efficiency improvements
  - Quick check question: What are the computational complexity differences between attention mechanisms and SSMs, and how does this relate to training efficiency?

- Concept: Token similarity measurement and fusion operations
  - Why needed here: The core mechanism of Famba-V relies on measuring cosine similarity between tokens and fusing similar ones through averaging. Understanding these operations is crucial for implementing and debugging the method
  - Quick check question: How does cosine similarity work for comparing token embeddings, and what are the implications of fusing tokens through averaging versus other operations?

## Architecture Onboarding

- Component map: Input → Patch tokenization → Linear projection + class token → Cross-layer token fusion (selected layers) → Vision Mamba SSM processing → Output class token → Normalization → MLP head → Classification
- Critical path: The key modification is the addition of token fusion modules within the Vim layers, where cosine similarity identifies similar tokens across layers that are then fused via averaging
- Design tradeoffs: The main tradeoff is between accuracy and efficiency, controlled by the choice of cross-layer fusion strategy and the number of tokens fused per layer. More aggressive fusion improves efficiency but may reduce accuracy by eliminating informative tokens
- Failure signatures: Accuracy degradation beyond acceptable thresholds, training instability due to improper token fusion, or insufficient efficiency gains despite token reduction. These can be diagnosed by monitoring validation accuracy, training loss curves, and memory usage metrics
- First 3 experiments:
  1. Baseline comparison: Run vanilla Vim without token fusion on CIFAR-100 to establish baseline accuracy, training time, and memory usage
  2. All-layer token fusion: Implement uniform token fusion across all layers and compare against baseline to quantify accuracy drop and efficiency gains
  3. Cross-layer strategy comparison: Implement each of the three cross-layer strategies (interleaved, lower-layer, upper-layer) and compare their accuracy-efficiency tradeoffs against both baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal starting layer for token fusion to maximize efficiency without sacrificing accuracy?
- Basis in paper: The paper explores different starting layers for token fusion under the upper-layer strategy, finding that starting from the 6th layer achieves the highest efficiency
- Why unresolved: While the paper identifies the 6th layer as optimal, the exact relationship between starting layer and efficiency/accuracy across different model architectures and datasets is not fully explored
- What evidence would resolve it: Systematic experiments varying the starting layer across multiple model architectures and datasets to determine if the 6th layer consistently yields optimal results

### Open Question 2
- Question: How does the choice of cross-layer token fusion strategy affect the generalization of Vision Mamba models to other vision tasks?
- Basis in paper: The paper evaluates the performance of Famba-V on CIFAR-100 but does not explore its effectiveness on other vision tasks
- Why unresolved: The paper's focus on CIFAR-100 limits understanding of how different cross-layer strategies generalize to tasks like object detection or semantic segmentation
- What evidence would resolve it: Experiments applying Famba-V with different cross-layer strategies to a variety of vision tasks and datasets to assess generalization and task-specific performance

### Open Question 3
- Question: What is the impact of combining token fusion with other efficiency enhancement techniques on the overall performance of Vision Mamba models?
- Basis in paper: The paper mentions the potential for combining token fusion with other techniques but does not explore this integration
- Why unresolved: The interaction between token fusion and other efficiency techniques (e.g., pruning, quantization) and their combined effect on accuracy and efficiency is not investigated
- What evidence would resolve it: Experiments combining token fusion with other efficiency techniques and evaluating their impact on accuracy, training time, and memory usage across different scenarios

## Limitations

- Evaluation is limited to a single dataset (CIFAR-100) and two small-scale Vision Mamba variants (Vim-Ti and Vim-S), limiting generalizability
- Lacks ablation studies on how token fusion thresholds, similarity metrics, or fusion operation choices affect performance
- No analysis provided on how the method scales with increasing model depth or different patch sizes

## Confidence

- **High confidence**: Claims about training time and memory usage improvements are well-supported by direct experimental measurements
- **Medium confidence**: Claims about accuracy-efficiency tradeoffs across the three cross-layer strategies are supported by CIFAR-100 results but lack validation on larger models or diverse datasets
- **Low confidence**: Claims about the mechanism of cross-layer token fusion preserving critical information through selective similarity-based merging are theoretically reasonable but lack extensive ablation studies or theoretical analysis

## Next Checks

1. **Cross-dataset validation**: Test Famba-V on ImageNet-1K and CIFAR-10 to verify that accuracy-efficiency improvements generalize beyond CIFAR-100 and to assess performance on both small and large-scale datasets

2. **Architecture scaling study**: Apply Famba-V to larger Vision Mamba variants (Vim-B, Vim-L) and evaluate how accuracy, training time, and memory usage scale with model size to identify potential bottlenecks or diminishing returns

3. **Ablation of fusion parameters**: Systematically vary the token fusion threshold, similarity metric (cosine vs. Euclidean), and fusion operation (averaging vs. weighted averaging) to determine their individual impacts on accuracy and efficiency, and to identify optimal configurations for different use cases