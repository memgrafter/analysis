---
ver: rpa2
title: 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models'
arxiv_id: '2403.20262'
source_url: https://arxiv.org/abs/2403.20262
tags:
- score
- meeting
- gpt-4
- questions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ELITR-Bench is a new benchmark for long-context language models,
  focused on a practical meeting assistant scenario. It uses noisy, ASR-generated
  transcripts as long contexts, presenting unique challenges due to their oral nature.
---

# ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models
## Quick Facts
- arXiv ID: 2403.20262
- Source URL: https://arxiv.org/abs/2403.20262
- Reference count: 40
- New benchmark for long-context LLMs using noisy, ASR-generated meeting transcripts with 271 questions

## Executive Summary
ELITR-Bench is a new benchmark designed to evaluate long-context language models on meeting assistant tasks using noisy, ASR-generated transcripts. The benchmark introduces 271 manually crafted questions with ground-truth answers, along with noisy transcript versions targeting different Word Error Rate levels. Experiments with 12 long-context models reveal that GPT-4/4o achieves the best overall performance, while newer open models approach its performance on clean transcripts but struggle with noise robustness. The evaluation using GPT-4 as a judge shows strong correlation with human scores but appears limited to distinguishing three quality levels despite a 10-point scale.

## Method Summary
The ELITR-Bench benchmark is built on the ELITR corpus of meeting transcripts, augmented with 271 manually crafted questions and ground-truth answers. Noisy transcript versions are created by altering transcripts to target different Word Error Rate levels. The benchmark is evaluated using 12 long-context LLMs in both single-turn and multi-turn conversational modes. Responses are automatically scored using GPT-4 as a judge, with evaluation validated against human annotators. The methodology includes a detailed score rubric and analysis of performance across question types and answer positions.

## Key Results
- GPT-4/4o outperforms other models on ELITR-Bench across all metrics and conditions
- Open models like LLaMA-3.1-8B and Phi-3-small approach GPT-4/4o performance on clean transcripts but lag in noise robustness
- GPT-4-based evaluation correlates well with human judgment (0.82 with Gold Human, 0.78 with Silver Human) but appears limited to distinguishing three quality levels despite a 10-point scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long-context LLMs benefit from extensive context when answering questions about noisy, lengthy meeting transcripts.
- Mechanism: The model leverages its extended context window to retain and retrieve relevant information from long, unstructured transcripts, even when they contain speech recognition errors.
- Core assumption: The model can effectively attend to and process relevant portions of long contexts despite the presence of noise.
- Evidence anchors:
  - [abstract] "Our benchmark, ELITR-Bench, augments the existing ELITR corpus by adding 271 manually crafted questions with their ground-truth answers, as well as noisy versions of meeting transcripts altered to target different Word Error Rate levels."
  - [section] "Our experiments with 12 long-context LLMs on ELITR-Bench confirm the progress made across successive generations of both proprietary and open models, and point out their discrepancies in terms of robustness to transcript noise."
- Break condition: If the noise level in the transcripts exceeds the model's ability to disambiguate and retrieve correct information, performance will degrade significantly.

### Mechanism 2
- Claim: The multi-turn conversational mode can improve performance for questions that depend on context from previous questions.
- Mechanism: By maintaining conversational context, the model can resolve coreferences and ellipses in follow-up questions, leading to more accurate answers.
- Core assumption: The model retains and utilizes conversational context effectively across multiple turns.
- Evidence anchors:
  - [abstract] "We also designed a modified ELITR-Bench-Conv version where questions are to be asked in sequence, in a pre-defined order within a conversation."
  - [section] "In this setting, some of the questions contain pronominal references or ellipses, for which previous conversational context (i.e., previous questions and answers) must be used to answer properly."
- Break condition: If the model's attention mechanism fails to maintain conversational context over many turns, it will not resolve references correctly.

### Mechanism 3
- Claim: GPT-4-based evaluation correlates well with human judgment for assessing the quality of responses.
- Mechanism: The GPT-4 judge uses a detailed score rubric to evaluate responses based on their proximity to ground-truth answers, mimicking human assessment criteria.
- Core assumption: The LLM judge can accurately interpret and apply the score rubric to assess response quality.
- Evidence anchors:
  - [abstract] "We also provide a thorough analysis of our GPT-4-based evaluation, including insights from a crowdsourcing study. Our findings indicate that while GPT-4's scores align with human judges, its ability to distinguish beyond three score levels may be limited."
  - [section] "GPT-4 shows a strong correlation with the two human-based evaluators (0.82 with Gold Human and 0.78 with Silver Human), which is in agreement with the findings from previous studies on GPT-4 judges."
- Break condition: If the score rubric is too granular or the LLM judge cannot interpret subtle quality differences, the evaluation may not distinguish between fine-grained quality levels.

## Foundational Learning

- Concept: Word Error Rate (WER)
  - Why needed here: Understanding WER is crucial for interpreting the impact of noise in meeting transcripts on model performance.
  - Quick check question: If a transcript has a WER of 40%, what percentage of words are incorrect?

- Concept: Long-context modeling techniques
  - Why needed here: Knowing how different techniques (e.g., sparse transformers, linear transformers) handle long contexts helps in understanding model capabilities and limitations.
  - Quick check question: What is the primary challenge addressed by sparse transformers in long-context modeling?

- Concept: Evaluation metrics for LLM responses
  - Why needed here: Familiarity with evaluation metrics like BLEU, ROUGE, or rubric-based scoring is important for assessing model performance and comparing different approaches.
  - Quick check question: What is the difference between a rubric-based evaluation and a reference-free evaluation?

## Architecture Onboarding

- Component map: Data preparation -> Model inference -> Evaluation -> Analysis
- Critical path: Clean transcripts -> Question generation -> Response generation -> Evaluation -> Performance analysis
- Design tradeoffs:
  - Single-turn vs. multi-turn modes: Simplicity and independence vs. conversational context and coreference resolution.
  - Noise levels: Realistic assessment of model robustness vs. potential degradation of performance.
  - Evaluation method: Automated (GPT-4 judge) vs. human evaluation: Scalability vs. nuanced assessment.
- Failure signatures:
  - Low scores on noisy transcripts: Model struggles with speech recognition errors.
  - Performance drop in multi-turn mode: Model fails to maintain conversational context.
  - Discrepancy between GPT-4 judge and human scores: Judge unable to distinguish fine-grained quality differences.
- First 3 experiments:
  1. Evaluate all models on clean transcripts in single-turn mode to establish baseline performance.
  2. Test models on noisy transcripts at different WER levels to assess robustness.
  3. Compare single-turn vs. multi-turn performance for conversational questions to evaluate context retention.

## Open Questions the Paper Calls Out

- Open Question 1: How do different de-identification strategies (e.g., anonymization vs. pseudonymization) impact the performance of long-context LLMs on ELITR-Bench, particularly for Who questions?
- Open Question 2: Can retrieval-augmented generation (RAG) models improve the performance of answering questions on noisy meeting transcripts in ELITR-Bench compared to standard long-context LLMs?
- Open Question 3: How does the performance of long-context LLMs on ELITR-Bench vary across different meeting domains (e.g., NLP vs. non-NLP topics)?

## Limitations
- The evaluation methodology using GPT-4 as a judge may be limited in distinguishing fine-grained quality differences, with an apparent ceiling at three quality levels despite a 10-point scale.
- The noise injection procedure for creating different WER levels is not fully specified, which may affect reproducibility and generalizability of robustness findings.
- Performance comparisons between proprietary and open models may be influenced by factors such as access to optimized implementations and varying context window sizes.

## Confidence
- **High Confidence:** The overall finding that GPT-4/4o outperforms other models on the ELITR-Bench benchmark is well-supported by experimental results across multiple metrics and conditions.
- **Medium Confidence:** The claim that newer open models approach GPT-4/4o performance on clean transcripts but lag in noise robustness is based on the available evidence but may be influenced by implementation details and optimization strategies.
- **Low Confidence:** The specific mechanisms by which long-context LLMs leverage extended context for noisy transcript processing are not fully elucidated, and the evaluation methodology's ability to capture nuanced quality differences is uncertain.

## Next Checks
1. **Noise Injection Validation:** Implement and validate the noise injection procedure to create transcripts with controlled WER levels, ensuring consistency and realism in the simulated errors.
2. **Cross-Evaluation Study:** Conduct a cross-evaluation study where multiple LLM judges (including GPT-4, GPT-3.5, and open alternatives) assess the same responses to determine the reliability and consistency of automated evaluation.
3. **Fine-Grained Quality Analysis:** Design and execute a human evaluation study with a larger pool of annotators to assess whether the current score rubric can effectively distinguish between fine-grained quality levels (e.g., scores 1-3, 4-6, 7-10) and identify potential improvements to the rubric or evaluation methodology.