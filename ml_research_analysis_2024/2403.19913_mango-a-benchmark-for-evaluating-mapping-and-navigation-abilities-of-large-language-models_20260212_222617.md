---
ver: rpa2
title: 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large
  Language Models'
arxiv_id: '2403.19913'
source_url: https://arxiv.org/abs/2403.19913
tags:
- questions
- each
- easy
- hard
- walkthrough
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MANGO is a benchmark that evaluates the mapping and navigation
  abilities of large language models (LLMs) using 53 mazes from text-based adventure
  games. Each maze is paired with a walkthrough, and LLMs are tasked with answering
  hundreds of mapping and navigation questions after reading the walkthrough.
---

# MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models

## Quick Facts
- **arXiv ID**: 2403.19913
- **Source URL**: https://arxiv.org/abs/2403.19913
- **Reference count**: 40
- **Primary result**: MANGO benchmark shows GPT-4 achieves only 50% success rate on route-finding questions in text-based navigation tasks

## Executive Summary
MANGO is a benchmark designed to evaluate the mapping and navigation capabilities of large language models using 53 mazes from text-based adventure games. Each maze is paired with a walkthrough, and LLMs must answer hundreds of mapping and navigation questions after reading the walkthrough. The benchmark includes both destination-finding and route-finding questions, some of which involve paths not covered in the walkthrough. Despite the apparent simplicity of these tasks for humans, even GPT-4, the state-of-the-art LLM, struggles significantly, achieving only 50% success on route-finding questions and showing performance degradation on harder questions involving longer and unseen routes.

## Method Summary
The MANGO benchmark consists of 53 mazes from text-based adventure games, each paired with a walkthrough that traces a sequence of location-changing actions. For each maze, hundreds of destination-finding (DF) and route-finding (RF) questions are generated. LLMs read the walkthrough and answer questions using provided prompt templates that include lists of legal actions and reachable locations to mitigate hallucination. The models are required to provide structured output in Python list format to facilitate evaluation. Performance is measured using success rate (fraction of correctly answered questions) and reasoning accuracy metrics.

## Key Results
- GPT-4 achieves only 50% success rate on route-finding questions, despite being state-of-the-art
- LLM performance degrades significantly on harder questions involving longer and unseen routes
- Strong mapping and navigation abilities demonstrated to benefit LLMs in downstream text-based game playing tasks

## Why This Works (Mechanism)

### Mechanism 1
LLMs can leverage walkthrough context to answer navigation questions, but performance degrades significantly when routes involve unseen steps. The model reads a walkthrough and attempts to answer questions about paths that may include steps not covered in the walkthrough. Success depends on the model's ability to generalize spatial reasoning from the observed partial map.

### Mechanism 2
Providing explicit lists of legal actions and reachable locations in prompts improves LLM performance by reducing hallucination. The prompt templates include "The allowed actions are: ..." and "The list of places are: ..." to constrain the model's output space and guide it toward valid answers.

### Mechanism 3
Requiring structured output (Python lists of dictionaries) improves LLM performance and enables deeper evaluation. The prompt templates request the model to describe trajectories in structured format, which makes parsing easier and may improve the model's reasoning process through Chain-of-Thought prompting.

## Foundational Learning

- **Concept**: Spatial reasoning and mental mapping
  - **Why needed here**: The benchmark evaluates whether LLMs can construct and navigate mental maps of text-based environments, a core component of human intelligence
  - **Quick check**: Can you explain how humans create mental maps and navigate using them? How might this process differ for an LLM processing text-based environments?

- **Concept**: Chain-of-Thought prompting and structured output
  - **Why needed here**: The benchmark uses structured prompts and requires structured output to improve LLM performance and enable detailed evaluation
  - **Quick check**: What is Chain-of-Thought prompting, and how does it differ from standard prompting? How might requiring structured output influence a model's reasoning process?

- **Concept**: Simultaneous Localization and Mapping (SLAM)
  - **Why needed here**: The benchmark investigates LLMs' capabilities in a text-based version of SLAM, where they must build a map from textual descriptions and use it for navigation
  - **Quick check**: What is SLAM, and how is it traditionally implemented in robotics? How does the text-based SLAM in MANGO differ from traditional SLAM?

## Architecture Onboarding

- **Component map**: Data generation (walkthrough enhancement, location annotation, edge imputation) -> Question generation (traversing mazes to create DF and RF skeletons) -> Evaluation (processing model outputs, computing success rates and reasoning accuracy)
- **Critical path**: Read walkthrough -> Answer DF and RF questions -> Parse model output -> Compute evaluation metrics
- **Design tradeoffs**: Comprehensive question generation (3M DF questions, 200K RF questions) vs. practical evaluation (models evaluated on subsets due to context window limitations); exact match vs. edit-distance-based evaluation for realism
- **Failure signatures**: Hallucinating nonexistent locations or edges; failing to infer implicit connections between locations; struggling with longer and unseen routes; providing ill-structured answers that cannot be parsed
- **First 3 experiments**:
  1. Evaluate GPT-4 on a simple maze (e.g., 905) with only explicit edges to establish a baseline
  2. Evaluate GPT-4 on a maze with imputed edges (e.g., Zork-I) to test its ability to infer implicit connections
  3. Compare GPT-3.5 and GPT-4 on a maze with long unseen routes (e.g., OMNIQuest) to quantify the performance gap on hard questions

## Open Questions the Paper Calls Out

### Open Question 1
**Can large language models develop and utilize internal spatial representations of text-based environments?**
- **Basis in paper**: Explicit. The paper states: "It will be interesting to investigate how the internal representations of each LLM have—if any—captured the structures of the maps."
- **Why unresolved**: Current evaluation focuses on end-task performance rather than examining internal representations
- **What evidence would resolve it**: Experiments showing whether LLM activations correlate with spatial features of the mazes, similar to Li et al. (2023)'s work on board games

### Open Question 2
**How well can language models generalize mapping and navigation abilities to completely new, unseen environments?**
- **Basis in paper**: Explicit. The paper mentions: "can an LLM, trained on a limited set of mazes, generalize its knowledge and reasoning capability to unseen mazes?"
- **Why unresolved**: Current benchmark uses fixed set of 53 mazes
- **What evidence would resolve it**: Experiments testing LLMs on mazes structurally different from training data and measuring performance drop

### Open Question 3
**What specific architectural or training modifications could improve LLM mapping and navigation abilities?**
- **Basis in paper**: Inferred from methodology limitations. The paper shows GPT-4 achieves only 50% success on route-finding despite being state-of-the-art
- **Why unresolved**: The paper benchmarks current LLMs but doesn't experiment with modifications
- **What evidence would resolve it**: Comparative experiments testing architectural changes or training strategies against baseline performance on MANGO benchmark

## Limitations

- Benchmark relies on text-based adventure game walkthroughs, which may not fully represent real-world navigation complexity
- Performance degradation on unseen routes raises questions about whether this reflects genuine spatial reasoning limitations or artifacts of prompt design
- Limited testing of model families beyond GPT-4 and GPT-3.5 makes generalization across LLM landscape difficult

## Confidence

- **High confidence**: Benchmark design and evaluation methodology are sound with clear metrics and reproducible procedures
- **Medium confidence**: Attribution of performance differences to spatial reasoning capabilities versus other factors (prompt engineering, training data exposure) requires further investigation

## Next Checks

1. **Cross-Model Generalization Test**: Evaluate the benchmark across a broader range of LLM architectures to determine if performance patterns are consistent or model-specific
2. **Ablation Study on Prompt Components**: Systematically remove or modify the constraint lists and structured output requirements to quantify their individual contributions to performance improvements
3. **Human Baseline Validation**: Conduct a controlled human study where participants complete the same tasks using only the walkthrough text, measuring response times and accuracy to establish a more rigorous baseline for comparison with LLM performance