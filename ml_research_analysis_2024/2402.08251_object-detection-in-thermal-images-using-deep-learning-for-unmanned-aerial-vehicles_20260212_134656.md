---
ver: rpa2
title: Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial
  Vehicles
arxiv_id: '2402.08251'
source_url: https://arxiv.org/abs/2402.08251
tags:
- object
- thermal
- images
- detection
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel neural network for detecting small
  and tiny objects in thermal images collected by unmanned aerial vehicles (UAVs).
  The method enhances YOLOv5 with transformer encoders, attention mechanisms, and
  sliding windows to improve accuracy while maintaining a low computational cost suitable
  for embedded systems.
---

# Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles

## Quick Facts
- arXiv ID: 2402.08251
- Source URL: https://arxiv.org/abs/2402.08251
- Authors: Minh Dang Tu; Kieu Trang Le; Manh Duong Phung
- Reference count: 25
- Primary result: Proposed YOLOv5-transformer model achieves mAP50 of 94.57% and mAP95 of 60.23% on custom thermal UAV dataset

## Executive Summary
This paper addresses the challenge of detecting small and tiny objects in thermal images captured by unmanned aerial vehicles. The authors propose a novel neural network architecture that enhances YOLOv5 with transformer encoders, attention mechanisms, and sliding windows to improve accuracy while maintaining computational efficiency suitable for embedded systems. The method demonstrates superior performance compared to state-of-the-art approaches on both public (VEDAI) and custom thermal datasets, with successful deployment on Jetson AGX hardware achieving real-time performance.

## Method Summary
The proposed method modifies YOLOv5 by adding a transformer encoder block at the end of the backbone, implementing sliding window attention in the neck, and adding a fourth prediction head from low-level features. The model uses Bi-FPN for feature fusion and incorporates GhostConv layers to reduce parameters. Training employs Adam optimizer with a learning rate of 3.2 × 10^-5, 10 initial stabilization epochs, and 150 total training epochs with batch size 16. The architecture is designed specifically for thermal imagery where objects are small and thermal cues are sparse.

## Key Results
- Achieves mAP50 of 94.57% and mAP95 of 60.23% on custom thermal dataset
- Outperforms ResNet, Faster R-CNN, and ViT on both VEDAI and custom datasets
- Successfully deploys on Jetson AGX with over 90% stability in real-time operation
- Demonstrates robustness to high noise levels in thermal imagery

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer encoders in the backbone improve feature representation for small objects
- Mechanism: Added transformer encoder block performs multi-head self-attention on backbone output, aggregating global and local context before feeding into detection heads
- Core assumption: Attention over wider receptive field compensates for low-resolution and low-contrast nature of thermal imagery
- Evidence anchors: [abstract] "backbone is developed based on YOLOv5 combined with transformer encoder"; [section] "apply transformer block at end of backbone"
- Break condition: If transformer attention weights collapse or extra computation causes timing violations on embedded hardware

### Mechanism 2
- Claim: Sliding window + attention fusion in neck expands informative regions without dense computation
- Mechanism: Applies sliding window over Bi-FPN feature maps, extracting local patches and running self-attention on them
- Core assumption: Thermal objects occupy small, coherent spatial regions so local window-based attention suffices
- Evidence anchors: [section] "combine sliding windows with attention mechanism instead of usual Transformer method"; [abstract] "neck includes BI-FPN combined with sliding window and transformer"
- Break condition: If sliding windows miss objects spanning multiple patches or window size mismatches typical object scale

### Mechanism 3
- Claim: Additional prediction head from low-level features boosts small-object recall
- Mechanism: Adds 4th prediction head from lowest-level feature map, capturing fine spatial details that higher-level heads smooth out
- Core assumption: Low-level features retain spatial resolution necessary to localize small objects
- Evidence anchors: [section] "use 4 prediction heads instead of 3 for images captured from drones"; [abstract] "backbone developed based on YOLOv5 structure"
- Break condition: If extra head adds too much parameter count or low-level features are too noisy

## Foundational Learning

- Concept: Transformer attention mechanism and self-attention
  - Why needed: Understanding Q, K, V matrices and scaled dot-product attention is critical for tuning encoder block
  - Quick check: What is the role of the scaling factor 1/√dk in attention softmax computation?

- Concept: Feature pyramid networks (FPN) and BiFPN
  - Why needed: Understanding multi-scale feature merging and BiFPN vs vanilla FPN helps debug neck design
  - Quick check: In BiFPN, why are edges bidirectional and how does that differ from standard FPN?

- Concept: Sliding window convolution and local context aggregation
  - Why needed: Sliding window + attention step is central to neck design; understanding patch extraction is needed to tune window size
  - Quick check: How does sliding window reduce input shape from (h × w × c) to (n × mh × mw × c)?

## Architecture Onboarding

- Component map: Backbone (YOLOv5 + transformer encoder) -> Neck (BiFPN + sliding window + attention) -> 4 prediction heads -> sigmoid + Soft-NMS
- Critical path: Backbone → Neck (sliding window + attention) → 4 heads → sigmoid + Soft-NMS
- Design tradeoffs:
  - Transformer encoder improves accuracy but increases latency; mitigated by lightweight GhostConv
  - Sliding window localizes attention but risks missing large objects; balanced by multi-scale heads
  - Extra prediction head increases parameters but improves small-object recall; offset by parameter-efficient layers
- Failure signatures:
  - Accuracy drops on tiny objects: likely extra low-level head is missing or poorly initialized
  - Latency spikes on Jetson AGX: transformer encoder or sliding window attention may be too large
  - Training diverges: attention layers may be too deep; reduce heads or add layer normalization
- First 3 experiments:
  1. Remove transformer encoder block and measure mAP drop on VEDAI
  2. Replace sliding window attention with full-image transformer and compare inference time and accuracy
  3. Train with only 3 prediction heads (drop low-level one) to confirm its role in small-object detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed model perform in detecting small objects in thermal images when the temperature difference between objects and background is minimal?
- Basis in paper: [inferred] Paper discusses challenges detecting small objects in thermal images, particularly when objects have similar temperatures to background, but provides no specific results for minimal temperature differences
- Why unresolved: Paper lacks experimental results or analysis for scenarios with minimal temperature differences between objects and background
- What evidence would resolve it: Additional experiments showing model performance with minimal temperature differences, compared to other state-of-the-art methods under these conditions

### Open Question 2
- Question: How does the proposed model handle occlusions or partial occlusions of objects in thermal images?
- Basis in paper: [inferred] Paper mentions challenge of detecting close objects recognized as single object and discusses attention layers and Bi-FPN, but does not specifically address occlusions
- Why unresolved: Paper does not provide information on handling occlusions or partial occlusions of objects
- What evidence would resolve it: Experiments showing performance detecting partially occluded objects in thermal images, compared to other methods under similar conditions

### Open Question 3
- Question: What is the impact of different thermal camera resolutions and noise levels on the proposed model's performance?
- Basis in paper: [explicit] Paper mentions model performs well on high noise thermal images and discusses overcoming limitations related to low information, but does not provide specific results for different resolutions and noise levels
- Why unresolved: Paper lacks experimental results or analysis for different thermal camera resolutions and noise levels
- What evidence would resolve it: Experiments showing model performance with different thermal camera resolutions and noise levels, compared to other state-of-the-art methods

## Limitations

- Architecture details for sliding window and attention mechanism implementation are not fully specified
- Exact configurations for transformer encoder block integration into YOLOv5 backbone are vaguely described
- Custom dataset collection methodology and class distribution are not specified
- Performance comparisons limited to only three baseline models without other YOLO variants or recent thermal-specific detectors
- Jetson AGX deployment results show only stability percentages without latency measurements or power consumption data

## Confidence

- Architecture novelty and design: Medium - General approach described but implementation details insufficient
- Performance metrics on VEDAI: Medium - Public dataset but custom evaluation protocol not specified
- Performance on custom dataset: Low - Dataset details missing, results not reproducible
- Real-time deployment claims: Low - No quantitative timing or power data provided

## Next Checks

1. Implement ablation studies removing each architectural component (transformer encoder, sliding window attention, extra prediction head) to quantify their individual contributions to mAP
2. Collect timing measurements (inference time per image) on Jetson AGX platform to verify real-time performance claims and identify computational bottlenecks
3. Evaluate model on standard thermal object detection benchmarks (if available) or publish custom dataset with train/test splits to enable independent verification