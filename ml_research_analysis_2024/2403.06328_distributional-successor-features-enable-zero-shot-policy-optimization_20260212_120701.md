---
ver: rpa2
title: Distributional Successor Features Enable Zero-Shot Policy Optimization
arxiv_id: '2403.06328'
source_url: https://arxiv.org/abs/2403.06328
tags:
- policy
- learning
- dispos
- features
- successor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiSPOs are a novel framework for zero-shot policy optimization
  across reward functions in reinforcement learning. They learn a distribution of
  successor features from an offline dataset, capturing all possible long-term outcomes
  achievable by the behavior policy, along with a readout policy that generates actions
  to realize specific outcomes.
---

# Distributional Successor Features Enable Zero-Shot Policy Optimization

## Quick Facts
- arXiv ID: 2403.06328
- Source URL: https://arxiv.org/abs/2403.06328
- Reference count: 40
- DiSPOs are a novel framework for zero-shot policy optimization across reward functions in reinforcement learning.

## Executive Summary
DiSPOs are a novel framework for zero-shot policy optimization across reward functions in reinforcement learning. They learn a distribution of successor features from an offline dataset, capturing all possible long-term outcomes achievable by the behavior policy, along with a readout policy that generates actions to realize specific outcomes. By modeling cumulative outcomes directly, DiSPOs avoid compounding errors that plague autoregressive model-based RL methods. At test time, DiSPOs enable zero-shot transfer by performing simple linear regression to infer reward weights and selecting the optimal outcome under the learned distribution.

## Method Summary
DiSPOs learn a distribution over successor features that represent cumulative discounted feature sums achievable in the dataset. The framework consists of two components: an outcome model that learns the distribution of possible outcomes from each state, and a readout policy that maps desired outcomes to actions. Using diffusion models as a practical instantiation, DiSPOs are trained on offline datasets using denoising score matching. For new tasks, reward weights are inferred through linear regression on state-reward pairs, and planning is performed to find optimal outcomes under the learned distribution.

## Key Results
- DiSPOs demonstrate strong empirical performance across long-horizon robotics domains including Antmaze navigation, Franka Kitchen manipulation, and Hopper locomotion
- Outperforms both successor feature methods and model-based RL baselines on zero-shot transfer tasks
- Theoretical analysis shows DiSPOs converge to "best-in-data" policies with bounded suboptimality
- Can handle arbitrary reward functions beyond goal-reaching and enable trajectory stitching by combining suboptimal trajectories from the dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DiSPOs avoid compounding error by modeling long-term outcomes directly instead of autoregressively generating trajectories
- Mechanism: Instead of predicting one-step dynamics and rolling out autoregressively (which accumulates prediction errors), DiSPOs learn a distribution over successor features that represent cumulative discounted feature sums. This captures all possible long-term outcomes achievable in the dataset without sequential prediction.
- Core assumption: The behavior policy in the dataset covers a diverse enough set of trajectories that the distribution of outcomes is representative of what's achievable in the environment
- Evidence anchors:
  - [abstract]: "By directly modeling long-term outcomes in the dataset, DiSPOs avoid compounding error while enabling a simple scheme for zero-shot policy optimization across reward functions."
  - [section 1]: "In practice, however, autoregressive generation suffers from compounding error [31, 1, 26], which arises when small, one-step approximation errors accumulate over long horizons."
  - [corpus]: Weak evidence - no corpus papers directly address this specific mechanism, though related papers mention "compounding error" in model-based RL
- Break condition: If the dataset doesn't cover the state space well or if the behavior policy is too limited, the outcome distribution won't be representative of achievable outcomes

### Mechanism 2
- Claim: Zero-shot transfer works by inferring reward weights through linear regression and selecting optimal outcomes under the learned distribution
- Mechanism: Given a new reward function, linear regression on state-reward pairs yields reward weights w. These weights transform the outcome distribution p(ψ|s) into a value distribution p(w⊤ψ|s). The optimal action is obtained by finding the outcome with highest value that has sufficient data support.
- Core assumption: Rewards are linear in the chosen feature space, i.e., r(s) = w⊤ϕ(s) for some w
- Evidence anchors:
  - [section 4.2]: "Once the task reward is observed, the model must provide a way to quickly evaluate and improve the policy since different tasks require different optimal policies."
  - [section 4.2]: "The value of each outcome can be evaluated as w⊤ψ. That is, knowing w effectively transforms the distribution of outcomes p(ψ|s) into a distribution of task-specific values (sum of rewards)p(R|s)."
  - [corpus]: Weak evidence - no corpus papers directly address this specific mechanism
- Break condition: If rewards are not linear in the feature space, this approach won't work and more expressive feature learning would be needed

### Mechanism 3
- Claim: Trajectory stitching enables combining suboptimal trajectories to achieve optimal behavior
- Mechanism: The dynamic programming update in DiSPOs allows combining subtrajectories from the dataset. By modeling p(θ(ϕ(s) + γψs′|s)), DiSPOs can represent outcomes that are combinations of different trajectory segments, enabling "best-in-data" policies.
- Core assumption: The dataset contains sufficient coverage of different subtrajectories that can be combined
- Evidence anchors:
  - [section 4.1]: "An additional benefit of the dynamic programming procedure is trajectory stitching, where combinations of subtrajectories in the dataset will be represented in the outcome distribution."
  - [section 6.1]: "DiSPOs naturally enables this type of trajectory stitching via the distributional Bellman backup, recovering 'best-in-data' policies for downstream tasks."
  - [section 6.5]: "Since the goal of this experiment is to evaluate stitching, not transfer, we choose the features as the task rewardsϕ(s) = r(s)."
- Break condition: If the dataset only contains isolated trajectories without overlapping states or if subtrajectories cannot be meaningfully combined

## Foundational Learning

- Concept: Successor Features and their policy dependence
  - Why needed here: Understanding why standard successor features don't work for zero-shot transfer (they're tied to a specific policy) is crucial for appreciating DiSPOs' innovation
  - Quick check question: Why can't we just use standard successor features for zero-shot policy optimization across different rewards?

- Concept: Diffusion models and guided sampling
  - Why needed here: DiSPOs use diffusion models to represent outcome distributions and employ guided diffusion for efficient planning
  - Quick check question: How does adding the reward weights as a guidance term in diffusion sampling relate to planning?

- Concept: Linear reward regression
  - Why needed here: The ability to infer reward weights from state-reward pairs through linear regression is fundamental to DiSPOs' zero-shot capability
- Quick check question: What assumption about the reward function makes linear regression sufficient for weight inference?

## Architecture Onboarding

- Component map:
  Outcome model -> Readout policy -> Planning module -> Action selection

- Critical path:
  1. Pretrain DiSPOs on offline dataset (optimize Eq 1 and 2)
  2. For new task: estimate reward weights w via linear regression
  3. Use planning (guided diffusion) to find optimal outcome ψ∗
  4. Query readout policy for action a∗ = π(a|s, ψ∗)

- Design tradeoffs:
  - Feature choice: random Fourier features are simple but may not capture all reward structures
  - Dataset coverage: more diverse datasets enable better stitching but increase training complexity
  - Planning method: guided diffusion is faster but sensitive to guidance coefficient vs. random shooting's robustness

- Failure signatures:
  - Poor performance on new tasks: likely indicates insufficient dataset coverage or poor feature choice
  - Slow planning: guidance coefficient in diffusion sampling may need tuning
  - Suboptimal behavior: reward weights estimation may be noisy or features may not linearly express rewards

- First 3 experiments:
  1. Train DiSPOs on a simple gridworld with random Fourier features and test zero-shot transfer to different goal locations
  2. Compare performance with and without guided diffusion planning on Antmaze medium-diverse
  3. Test trajectory stitching by creating a dataset with separate subtrajectories and evaluating if DiSPOs can combine them for full task completion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of state feature function ϕ(s) impact the performance and generalization capabilities of DiSPOs?
- Basis in paper: [explicit] The paper mentions that "DiSPOs require a choice of features ϕ(s) that linearly express the rewards; this assumption may fail, necessitating more expressive feature learning methods."
- Why unresolved: The paper uses random Fourier features for experiments but acknowledges this might not be optimal. Different environments might require different feature representations, and there's no systematic study of how feature choice affects performance.
- What evidence would resolve it: Systematic experiments comparing different feature learning methods (random Fourier features, learned features, etc.) across multiple environments and reward functions.

### Open Question 2
- Question: Can DiSPOs maintain their performance when transferred to environments with stochastic dynamics?
- Basis in paper: [inferred] The paper states "As described previously, DiSPOs enable zero-shot transfer to arbitrary new rewards in an environment without accumulating compounding error or requiring expensive test-time policy optimization." However, all experiments are conducted on deterministic MDPs, and the authors note that "DiSPO framework and theoretical results are derived under deterministic MDPs."
- Why unresolved: The theoretical analysis and experiments are limited to deterministic MDPs, leaving open whether the approach would work in more realistic stochastic environments.
- What evidence would resolve it: Experiments demonstrating DiSPO performance on stochastic environments, particularly those with significant stochasticity beyond just action noise.

### Open Question 3
- Question: How does the performance of DiSPOs scale with the complexity and dimensionality of the state and action spaces?
- Basis in paper: [inferred] The paper demonstrates effectiveness on several robotics domains but doesn't systematically explore scaling properties. The discussion mentions "Limitations of our work open future research opportunities" including scalability concerns.
- Why unresolved: The experiments focus on specific robotic domains but don't provide a comprehensive analysis of how performance degrades (or doesn't) as problem complexity increases.
- What evidence would resolve it: Empirical studies varying state/action space dimensionality and complexity while measuring performance, planning time, and data efficiency.

### Open Question 4
- Question: Can the online adaptation variant of DiSPOs (Algorithm 3) achieve comparable performance to the offline version while requiring fewer samples?
- Basis in paper: [explicit] The paper mentions "a potential future direction could apply this paradigm to online adaptation, where the reward is inferred from online interactions" but only provides pseudocode without empirical validation.
- Why unresolved: The online adaptation algorithm is presented but not evaluated, leaving questions about its practical utility and sample efficiency.
- What evidence would resolve it: Comparative experiments between online and offline DiSPO variants measuring sample efficiency, adaptation speed, and final performance.

### Open Question 5
- Question: How does the guidance coefficient in the guided diffusion planner affect the trade-off between planning quality and computational efficiency?
- Basis in paper: [explicit] The paper states "We found planning with guided diffusion to be sensitive to the guidance coefficient" and suggests using random shooting to get baseline performance before tuning.
- Why unresolved: The paper acknowledges sensitivity to the guidance coefficient but doesn't provide systematic analysis of how different values affect the exploration-exploitation trade-off in planning.
- What evidence would resolve it: Ablation studies varying the guidance coefficient across multiple tasks, measuring both planning quality (return achieved) and computational efficiency (planning time).

## Limitations
- The framework relies on linear reward assumptions, which may not hold for all real-world tasks
- Performance heavily depends on dataset coverage and diversity, which isn't thoroughly analyzed
- The computational overhead of diffusion-based planning versus simpler alternatives is unclear

## Confidence
- High confidence in the core algorithmic framework and theoretical guarantees for linear rewards
- Medium confidence in empirical results across domains, though some baseline comparisons may be incomplete
- Low confidence in the practical scalability and performance for non-linear reward functions

## Next Checks
1. Test DiSPOs on tasks with non-linear reward structures to evaluate the framework's limits
2. Systematically analyze how dataset size and diversity impact zero-shot transfer performance
3. Compare the planning efficiency of guided diffusion versus simpler random shooting methods across different task complexities