---
ver: rpa2
title: 'Conifer: Improving Complex Constrained Instruction-Following Ability of Large
  Language Models'
arxiv_id: '2404.02823'
source_url: https://arxiv.org/abs/2404.02823
tags:
- conifer
- dataset
- instructions
- instruction
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving large language
  models' (LLMs) ability to follow complex, constrained instructions. The authors
  introduce Conifer, a novel instruction tuning dataset designed to enhance LLMs'
  capabilities in this area.
---

# Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models

## Quick Facts
- arXiv ID: 2404.02823
- Source URL: https://arxiv.org/abs/2404.02823
- Reference count: 23
- A 7B Conifer model outperforms state-of-the-art open-source 7B models and even exceeds the performance of models 10 times larger on certain metrics

## Executive Summary
This paper addresses the challenge of improving large language models' (LLMs) ability to follow complex, constrained instructions. The authors introduce Conifer, a novel instruction tuning dataset designed to enhance LLMs' capabilities in this area. Conifer is constructed using GPT-4 with a series of refinement processes to ensure high quality. A progressive learning scheme is also proposed, emphasizing an easy-to-hard progression and learning from process feedback. Experiments show that models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints.

## Method Summary
The method involves using GPT-4 to generate and refine instructions with complex constraints through a task decomposition approach. The process includes query reframing, constraint generation, recombination, and a two-stage filtering process. A progressive learning scheme is employed, organizing instruction-response pairs into multi-turn conversations with increasing difficulty levels. Models are fine-tuned using the Conifer dataset and further trained with direct preference optimization (DPO). The objective is to enhance LLMs' capabilities in following multi-level instructions with complex constraints, evaluated on benchmarks such as IFEval, FollowBench, InFoBench, AlpacaEval, and MT-Bench.

## Key Results
- The 7B Conifer model outperforms state-of-the-art open-source 7B models on instruction-following benchmarks
- Conifer-7B-DPO exceeds the performance of models 10 times larger on certain metrics
- Progressive learning scheme significantly improves performance on complex constraint-following tasks

## Why This Works (Mechanism)

### Mechanism 1: Progressive Difficulty Scaling with Multi-Turn Conversations
- Claim: Organizing instruction-response pairs into multi-turn conversations with increasing difficulty levels improves LLM performance on complex constraints.
- Mechanism: By breaking down complex instructions into simpler precursors, models learn to build understanding incrementally rather than attempting to parse all constraints simultaneously.
- Core assumption: LLMs can effectively transfer knowledge from simpler constraint-following tasks to more complex ones when presented in a structured progression.
- Evidence anchors:
  - [abstract] "We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback."
  - [section] "We aggregate multi-level instructions and answers generated from the same seed instruction into a single sample using the multi-turn conversational format."
- Break condition: If the model fails to establish connections between simpler and more complex constraint patterns, or if the progression gaps are too large to bridge effectively.

### Mechanism 2: Process Feedback Learning (Internal and External)
- Claim: Providing explicit reasoning about constraint adherence improves model's ability to follow complex instructions.
- Mechanism: Internal feedback requires the model to articulate how it follows constraints within its responses, creating self-monitoring. External feedback uses GPT-4 to identify constraint violations and provide corrective guidance.
- Core assumption: LLMs can internalize constraint-following patterns when explicitly prompted to reason about their adherence during generation.
- Evidence anchors:
  - [abstract] "The model is enabled to learn from both internal and external process feedback, utilizing GPT-4's insights on the explicit reasoning process required to follow complex constraints."
  - [section] "When prompted to respond to instructions that contain format and numerical constraints, GPT-4 is instructed to illustrate how it adheres to the specified constraints explicitly within its responses."
- Break condition: If the feedback process becomes too verbose or the model learns to generate plausible-sounding but incorrect reasoning rather than actual constraint adherence.

### Mechanism 3: Task Decomposition for Complex Constraint Generation
- Claim: Breaking down the generation of complex constrained instructions into smaller, manageable tasks improves dataset quality.
- Mechanism: Instead of directly prompting GPT-4 to generate instructions with multiple complex constraints, the approach decomposes this into query reframing, constraint generation, recombination, and filtering stages.
- Core assumption: GPT-4's performance on constraint generation improves when tasks are structured into sequential, focused operations rather than open-ended generation.
- Evidence anchors:
  - [abstract] "To address the challenge of generating instructions with multiple complex constraints, we break the hard task into smaller, more manageable tasks."
  - [section] "We have decomposed this challenging task into smaller, more manageable tasks for GPT-4, including query reframing, constraint generation, recombination, and a two-stage filtering process."
- Break condition: If the recombination stage fails to properly integrate constraints or if filtering stages become too restrictive, eliminating valuable instruction diversity.

## Foundational Learning

- Concept: Curriculum Learning
  - Why needed here: Complex instruction following requires models to build understanding progressively. Curriculum learning principles suggest that starting with simpler examples and gradually increasing complexity leads to better learning outcomes.
  - Quick check question: If you randomly shuffled instruction difficulty levels instead of ordering them easy-to-hard, what would happen to model performance according to the ablation studies?

- Concept: Process Supervision
  - Why needed here: Traditional end-task supervision may not provide sufficient signal for learning complex constraint adherence. Process supervision, which provides feedback at each reasoning step, can help models develop better internal reasoning about constraints.
  - Quick check question: What's the difference between internal feedback (model articulating its own constraint adherence) and external feedback (GPT-4 judging constraint adherence) in terms of what they teach the model?

- Concept: Task Decomposition and Recombination
  - Why needed here: Complex tasks often exceed the capabilities of current LLMs when presented holistically. Breaking them into manageable sub-tasks that can be handled sequentially, then recombining them, enables the creation of higher-quality training data.
  - Quick check question: Why might GPT-4 struggle to generate instructions with multiple complex constraints in a single step, but succeed when the task is broken into query reframing, constraint generation, and recombination stages?

## Architecture Onboarding

- Component map: Seed query collection -> Query reframing -> Constraint generation -> Recombination -> Two-stage filtering -> Multi-turn conversation formatter -> Internal feedback generator -> External feedback generator -> Training scheduler
- Critical path: The data generation pipeline must complete successfully before any training can begin. The progressive learning system depends on having the multi-turn formatted dataset. Evaluation depends on both training completion and benchmark availability.
- Design tradeoffs:
  - Dataset size vs. quality: Generating more complex instructions could improve model capability but may reduce overall dataset size due to filtering.
  - Internal vs. external feedback: Internal feedback is cheaper to generate but may be less reliable; external feedback is more expensive but provides better quality signals.
  - Difficulty progression granularity: Too fine-grained may be inefficient; too coarse may create learning gaps.
- Failure signatures:
  - If models don't improve on constraint-following benchmarks, check whether the progressive difficulty scaling is actually working (are easier instructions being learned first?).
  - If contamination is detected, verify that the cosine similarity and rephrasing detection mechanisms are functioning properly.
  - If training becomes unstable, check whether the feedback mechanisms are introducing conflicting signals.
- First 3 experiments:
  1. Test the multi-turn conversation formatting by generating a small dataset with known difficulty progression and verify that models learn the easy constraints before the hard ones.
  2. Compare internal vs. external feedback by training two models, one with only internal feedback and one with both, and measure performance differences on constraint-following tasks.
  3. Validate the task decomposition approach by attempting to generate complex constrained instructions both directly and through the decomposition pipeline, measuring quality and success rates for each approach.

## Open Questions the Paper Calls Out

- How does Conifer's performance scale with model size beyond 7B and 13B parameters?
- What is the long-term effectiveness of Conifer-trained models on real-world, complex instructions not seen during training?
- How does the quality of GPT-4-generated constraints in Conifer compare to human-curated constraints in terms of instruction-following performance?

## Limitations

- Heavy reliance on GPT-4 for both dataset generation and quality filtering creates potential circular dependency
- Limited ablation studies make it difficult to isolate the specific contribution of progressive learning vs. dataset quality
- Potential overfitting to patterns present in ShareGPT seed queries rather than developing truly generalizable constraint-handling capabilities

## Confidence

**High Confidence:** The claim that Conifer improves instruction-following performance on standard benchmarks (IFEval, FollowBench, InFoBench) is well-supported by the experimental results.

**Medium Confidence:** The assertion that progressive difficulty scaling is the primary driver of performance improvements has moderate support but would benefit from additional ablation studies.

**Low Confidence:** The claim that the task decomposition approach for constraint generation is necessary (rather than sufficient) for high-quality dataset creation lacks direct experimental validation.

## Next Checks

1. Conduct an ablation study on progressive learning by training a model with randomly ordered difficulty levels and comparing performance to the progressive learning model.

2. Perform human evaluation studies where annotators rate the quality of constraint-following on a subset of instructions, comparing models trained with Conifer against baseline models.

3. Test models on instructions containing constraint types not present in the Conifer training data to evaluate whether they have developed general constraint-handling capabilities or simply memorized specific patterns.