---
ver: rpa2
title: '$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens'
arxiv_id: '2402.13718'
source_url: https://arxiv.org/abs/2402.13718
tags:
- context
- tasks
- llms
- contexts
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces \u221EBENCH, the first benchmark designed\
  \ to evaluate large language models (LLMs) on processing contexts exceeding 100,000\
  \ tokens, addressing the lack of standardized long-context evaluation tools. The\
  \ benchmark includes 12 diverse tasks across domains like retrieval, code, math,\
  \ novels, and dialogue, with both synthetic and realistic contexts in English and\
  \ Chinese."
---

# $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens

## Quick Facts
- arXiv ID: 2402.13718
- Source URL: https://arxiv.org/abs/2402.13718
- Authors: Xinrong Zhang; Yingfa Chen; Shengding Hu; Zihang Xu; Junhao Chen; Moo Khai Hao; Xu Han; Zhen Leng Thai; Shuo Wang; Zhiyuan Liu; Maosong Sun
- Reference count: 13
- Primary result: Introduces first benchmark for evaluating LLMs on contexts exceeding 100K tokens

## Executive Summary
This paper introduces $\infty$BENCH, the first benchmark designed to evaluate large language models on processing contexts exceeding 100,000 tokens. The benchmark includes 12 diverse tasks across domains like retrieval, code, math, novels, and dialogue, with both synthetic and realistic contexts in English and Chinese. Experiments on state-of-the-art LLMs reveal significant performance degradation when handling long contexts, with average scores ranging from 19.96 to 45.63 across tasks.

## Method Summary
The evaluation involves using state-of-the-art LLMs like GPT-4, Claude 2, Kimi-Chat, and YaRN-Mistral. The models are prompted with specific templates tailored for each task, and inputs are truncated if they exceed the model's input length limit. Performance is measured by the model's ability to accurately complete tasks, using metrics such as accuracy, ROUGE-L-Sum for summarization, and exact match for other tasks.

## Key Results
- Current state-of-the-art LLMs show significant performance degradation on tasks with contexts exceeding 100K tokens
- Average scores range from 19.96 to 45.63 across 12 diverse tasks spanning retrieval, code, math, novels, and dialogue
- Retrieval tasks are relatively easier than complex reasoning tasks, but challenges persist in tasks requiring state tracking or sequential processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic tasks stress-test long-context abilities beyond what real-world data alone can reveal
- Mechanism: Auto-generated tasks like Math.Calc and Code.Run isolate specific capabilities by constructing controlled, scalable inputs
- Core assumption: Long-context degradation is detectable in synthetic, algorithmic contexts
- Evidence anchors: [abstract] Tasks designed to require understanding of long dependencies; [section] Synthetic tasks engineered to evaluate lengthy context processing
- Break condition: If synthetic tasks fail to correlate with real-world long-context performance

### Mechanism 2
- Claim: Context truncation at center preserves boundary information models can still attend to
- Mechanism: Removes middle of context and concatenates start and end, preserving likely key metadata
- Core assumption: Important information in long contexts is usually at start or end
- Evidence anchors: [section] Approach predicated on assumption that key information is typically located at either start or end
- Break condition: If critical information is mid-context or models evolve to distribute attention more uniformly

### Mechanism 3
- Claim: Context recalling prompting can boost performance by forcing intermediate memorization
- Mechanism: Prompt explicitly asks model to repeat or restate relevant code before analysis
- Core assumption: Models lose intermediate context over long inputs; re-stating improves accuracy
- Evidence anchors: [section] Information may be more effective to first prompt model to recall relevant information; [section] Accuracy on Code.Debug improved to 39.59% with this technique
- Break condition: If model already retains context well or extra recall step causes over-fitting

## Foundational Learning

- Concept: Attention mechanisms and computational complexity
  - Why needed here: Long-context evaluation depends on understanding how attention scales quadratically
  - Quick check question: What is the computational complexity of standard self-attention over sequence of length n?

- Concept: Positional encoding schemes for long sequences
  - Why needed here: Benchmark tests models with 100K+ tokens, understanding rotary embeddings is key to diagnosing context length failures
  - Quick check question: How does rotary positional encoding differ from absolute positional encoding in handling very long sequences?

- Concept: Long-context evaluation methodologies
  - Why needed here: Designing benchmark requires knowledge of how to construct synthetic and realistic tasks that probe different failure modes
  - Quick check question: Why might synthetic tasks be necessary in addition to real-world long-context data for benchmarking?

## Architecture Onboarding

- Component map: Synthetic task generator -> Context truncation pipeline -> Prompt template selector -> Model inference harness -> Evaluation scorer
- Critical path: Generate synthetic task -> Truncate if needed -> Apply prompt template -> Run model -> Parse and score output
- Design tradeoffs: Synthetic vs realistic tasks (scalability vs ecological validity), truncation vs full context (efficiency vs completeness), strict vs lenient scoring (consistency vs robustness)
- Failure signatures: Zero/random outputs in Retrieve.PassKey suggest truncation removed key; uniform low scores suggest model context window limit; task-specific failure suggests architectural weaknesses
- First 3 experiments:
  1. Run all models on truncated vs full synthetic tasks to confirm truncation preserves performance
  2. Vary answer position in Retrieve.KV to test for "lost in the middle" effects
  3. Apply context recalling prompt to Code.Debug and compare against baseline to measure prompting impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications or training strategies could enable LLMs to effectively process contexts exceeding 100,000 tokens without significant performance degradation?
- Basis in paper: [explicit] Paper identifies severe performance degradation and highlights need for advancements in long-context processing
- Why unresolved: Paper demonstrates problem but does not propose specific solutions or architectural modifications
- What evidence would resolve it: Experimental results comparing various architectural modifications or training strategies on long-context performance benchmarks

### Open Question 2
- Question: How does the "lost in the middle" phenomenon manifest in models processing contexts longer than 16,000 tokens, and what factors influence its occurrence?
- Basis in paper: [inferred] Paper investigates phenomenon but finds no consistent trend in models processing contexts exceeding 100,000 tokens
- Why unresolved: Analysis does not provide definitive explanation for absence of phenomenon in experiments
- What evidence would resolve it: Systematic experiments varying context length, answer position, and model architecture

### Open Question 3
- Question: What are the limitations of using exact match evaluation for long-context tasks, and how can evaluation metrics be improved?
- Basis in paper: [explicit] Paper acknowledges limitations of exact match evaluation, noting dependence on prompt templates and answer parsing methods
- Why unresolved: Paper does not explore alternative evaluation metrics or discuss how to address limitations
- What evidence would resolve it: Comparative studies of different evaluation metrics on long-context tasks

## Limitations
- Synthetic tasks may not fully capture complexity of real-world long-context scenarios
- Truncation strategy introduces potential biases by assuming key information is at context boundaries
- Evaluation focuses primarily on English and Chinese languages, potentially limiting generalizability

## Confidence
- **High Confidence**: Core observation that state-of-the-art LLMs significantly degrade on long-context tasks is well-supported by consistent results across multiple models and tasks
- **Medium Confidence**: Effectiveness of context recalling prompting and specific degradation patterns are well-documented but may be sensitive to prompt engineering details
- **Low Confidence**: Generalizability of synthetic task results to real-world scenarios and long-term validity of truncation approach remain uncertain

## Next Checks
1. Cross-language validation: Test benchmark on additional languages beyond English and Chinese to assess multilingual performance
2. Dynamic truncation analysis: Compare performance across different truncation strategies to quantify impact on evaluation outcomes
3. Real-world task correlation: Conduct study correlating synthetic task performance with real-world long-context task performance to validate benchmark's predictive power