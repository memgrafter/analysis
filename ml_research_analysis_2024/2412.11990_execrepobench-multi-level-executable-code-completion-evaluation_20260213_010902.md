---
ver: rpa2
title: 'ExecRepoBench: Multi-level Executable Code Completion Evaluation'
arxiv_id: '2412.11990'
source_url: https://arxiv.org/abs/2412.11990
tags:
- code
- completion
- arxiv
- qwen2
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ExecRepoBench, a repository-level code completion
  benchmark with 1.2K executable samples from active Python repositories, and Repo-Instruct,
  a corresponding instruction corpus. The authors propose a multi-level grammar-based
  completion methodology that masks code fragments at various logical units (statements,
  expressions, functions) using abstract syntax trees.
---

# ExecRepoBench: Multi-level Executable Code Completion Evaluation

## Quick Facts
- arXiv ID: 2412.11990
- Source URL: https://arxiv.org/abs/2412.11990
- Reference count: 10
- 7B-parameter Qwen2.5-Coder model achieves SOTA performance across programming languages

## Executive Summary
This paper introduces ExecRepoBench, a repository-level code completion benchmark with 1.2K executable samples from active Python repositories, and Repo-Instruct, a corresponding instruction corpus. The authors propose a multi-level grammar-based completion methodology that masks code fragments at various logical units (statements, expressions, functions) using abstract syntax trees. They fine-tune the 7B-parameter Qwen2.5-Coder model on Repo-Instruct to create Qwen2.5-Coder-Instruct-C, which is evaluated against benchmarks including MultiPL-E and ExecRepoBench. The model consistently outperforms prior baselines across all programming languages, achieving state-of-the-art performance in code completion tasks.

## Method Summary
The approach uses multi-level grammar-based completion with AST masking to create instruction samples from code repositories. The method parses code into abstract syntax trees and randomly masks nodes at different logical levels (expressions, statements, functions, classes). The model conditions on prefix/suffix code in the current file plus code snippets from N other files in the same repository. The 7B-parameter Qwen2.5-Coder model is fine-tuned using hybrid instruction tuning (completion + standard QA) on the Repo-Instruct corpus, trained with Megatron-LM on 64 NVIDIA H100 GPUs using specified hyperparameters.

## Key Results
- Qwen2.5-Coder-Instruct-C achieves state-of-the-art performance on MultiPL-E benchmark across all programming languages
- Model outperforms prior baselines on ExecRepoBench with significant margins in Pass@k scores
- Hybrid instruction tuning approach shows strong performance on both code completion and general instruction-following tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grammar-based AST masking captures multi-level code structure better than random masking
- Mechanism: Parses code into AST and randomly masks nodes at different logical levels (expressions, statements, functions, classes)
- Core assumption: Different logical units represent distinct semantic and syntactic patterns that LLMs must learn separately
- Evidence anchors: Multi-level masking shown in abstract and section 3.2; related work on repository-level completion

### Mechanism 2
- Claim: Repository-level context improves completion accuracy over single-file approaches
- Mechanism: Conditions on prefix/suffix code plus code snippets from N other files in same repository
- Core assumption: Real-world completion often requires understanding relationships between files
- Evidence anchors: REPO-INSTRUCT designed for complex dependencies; cross-file context validated by related work

### Mechanism 3
- Claim: Hybrid instruction tuning creates more capable models than completion-only fine-tuning
- Mechanism: Trains on both code completion data and standard instruction data under unified objective
- Core assumption: General instruction-following capabilities transfer to and enhance code completion
- Evidence anchors: Unified instruction model in section 3.2; performance on MultiPL-E shows general capabilities

## Foundational Learning

- Concept: Abstract Syntax Tree (AST) structure and traversal
  - Why needed here: Completion methodology relies on parsing code into ASTs and extracting logical units
  - Quick check question: Given a Python code snippet, can you identify which AST node types correspond to expressions, statements, functions, and classes?

- Concept: Context window management and truncation strategies
  - Why needed here: Repository-level completion requires managing context from multiple files within token limits
  - Quick check question: How would you prioritize which cross-file context to include when total tokens exceed maximum context length?

- Concept: Edit similarity vs execution-based evaluation metrics
  - Why needed here: Paper shows mismatch between n-gram based metrics (ES) and execution-based metrics (Pass@k)
  - Quick check question: Why might a model achieve high edit similarity but low pass@1 scores on executable completion tasks?

## Architecture Onboarding

- Component map: GitHub repository scraping -> AST parsing -> instruction generation -> model training -> evaluation
- Critical path: Repository collection -> AST parsing -> instruction generation -> model fine-tuning -> evaluation
- Design tradeoffs: Parameter efficiency vs performance (7B vs larger models); execution time vs comprehensiveness (<2 min vs full tests); context coverage vs token limits
- Failure signatures: Low pass@1 despite high ES (execution issues); training instability (conflicting objectives); context truncation problems (important dependencies excluded)
- First 3 experiments:
  1. Run AST parsing on sample code to verify multi-level node extraction works correctly
  2. Test context file sorting and truncation logic with synthetic repository data
  3. Validate instruction format conversion by running sample completions through evaluation pipeline

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain implicit in the methodology and evaluation approach.

## Limitations
- ExecRepoBench contains only Python samples despite claims of cross-language performance
- Execution-based evaluation assumes standardized test environments that may not capture all real-world scenarios
- Repository context mechanism's greedy truncation strategy may systematically exclude critical dependencies

## Confidence
- High confidence: Multi-level grammar-based completion methodology using AST masking is technically sound
- Medium confidence: Repository-level context mechanism and cross-file dependency handling show promise but need more validation
- Low confidence: Generalizability beyond Python repositories remains unproven

## Next Checks
1. Apply multi-level AST masking methodology to non-Python repositories and evaluate effectiveness across different language paradigms
2. Systematically vary number of context files and token allocation ratios to identify optimal settings for different repository sizes
3. Conduct ablation studies comparing hybrid completion+QA training versus specialized completion-only training