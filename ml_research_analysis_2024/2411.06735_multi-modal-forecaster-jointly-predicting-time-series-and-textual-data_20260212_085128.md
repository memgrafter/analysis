---
ver: rpa2
title: 'Multi-Modal Forecaster: Jointly Predicting Time Series and Textual Data'
arxiv_id: '2411.06735'
source_url: https://arxiv.org/abs/2411.06735
tags:
- time
- series
- text
- forecasting
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TimeText Corpus (TTC), a curated dataset
  combining time series and aligned text from climate and healthcare domains, and
  proposes Hybrid-MMF, a multimodal forecasting model that jointly predicts both modalities
  using shared embeddings. Despite leveraging a pre-trained LLM and time-series embeddings,
  the Hybrid-MMF model does not outperform baseline methods in experiments.
---

# Multi-Modal Forecaster: Jointly Predicting Time Series and Textual Data

## Quick Facts
- **arXiv ID**: 2411.06735
- **Source URL**: https://arxiv.org/abs/2411.06735
- **Reference count**: 25
- **Primary result**: Hybrid-MMF model fails to outperform baselines despite multimodal approach

## Executive Summary
This paper introduces TimeText Corpus (TTC), a novel dataset combining time series and aligned text from climate and healthcare domains, and proposes Hybrid-MMF, a multimodal forecasting model that jointly predicts both modalities using shared embeddings. The authors develop a comprehensive framework for multimodal forecasting, incorporating pre-trained LLM and time-series embeddings to handle both numerical and textual predictions. Despite the sophisticated architecture, experimental results show that Hybrid-MMF does not outperform baseline methods, highlighting the significant challenges in multimodal forecasting and suggesting that larger datasets or different architectural approaches may be needed.

## Method Summary
The authors propose a hybrid multimodal forecasting framework that jointly predicts time series and textual data. The approach uses pre-trained models for both modalities - a Large Language Model (LLM) for text and time-series embeddings for numerical data - which are then combined through a shared embedding space. The model is trained on TimeText Corpus (TTC), a curated dataset containing aligned time series and text from climate (weather patterns) and healthcare (EHR data) domains. The framework includes mechanisms for handling time-series tokens and integrating textual context into numerical predictions, with separate heads for forecasting both modalities.

## Key Results
- Hybrid-MMF model matches but does not exceed baseline performance despite incorporating both text and time series data
- Incorporating text embeddings improved numerical predictions in both NLinear and fine-tuned models, indicating textual context helps recognize patterns
- Dataset limitations (7,589 timesteps for medical, 3,621 for weather) may constrain model performance and multimodal potential

## Why This Works (Mechanism)
Unknown: The paper does not provide explicit mechanism explanations for why the hybrid approach was expected to work or why it failed to improve upon baselines. The authors note that incorporating text embeddings improved numerical predictions in some cases, suggesting textual context helps recognize patterns, but do not explain the underlying mechanisms that should enable multimodal forecasting to outperform unimodal approaches.

## Foundational Learning
Unknown: The paper does not discuss foundational learning principles or insights that informed the design of the Hybrid-MMF architecture. The authors focus on technical implementation details rather than explaining what foundational concepts from multimodal learning or forecasting informed their approach.

## Architecture Onboarding
- **Component map**: Time series data → Time-series embeddings → Shared embedding space; Text data → Pre-trained LLM → Shared embedding space; Shared space → Numerical forecast head + Text forecast head
- **Critical path**: Input time series and text → Separate embeddings → Fusion in shared space → Dual-head prediction
- **Design tradeoffs**: Separate pre-trained models preserve domain expertise but may create integration challenges; dual-head architecture enables multimodal output but increases complexity
- **Failure signatures**: Inability to improve on textual forecasting suggests text-time series alignment issues; matching but not exceeding baselines indicates potential architectural inefficiencies
- **First experiments**: 1) Ablation study removing text components to isolate contribution, 2) Test with alternative alignment strategies between modalities, 3) Scale up training with larger datasets to assess data dependency

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What specific characteristics of text embeddings (e.g., semantic richness, temporal alignment) contribute most to improving numerical forecasting accuracy?
- Basis in paper: [inferred] The paper notes that incorporating text embeddings improved numerical predictions in both NLinear and fine-tuned models, suggesting textual context helps recognize patterns in numerical data.
- Why unresolved: The experiments compared models with and without text embeddings but did not analyze which embedding characteristics drive the improvements or whether different embedding models yield different benefits.
- What evidence would resolve it: Ablation studies testing various embedding models (semantic vs. temporal focus), embedding dimensions, and alignment methods against forecasting performance would identify key characteristics.

### Open Question 2
- Question: What is the minimum dataset size and temporal coverage required for multimodal forecasting models to outperform unimodal baselines?
- Basis in paper: [explicit] The authors note their dataset limitations (7,589 timesteps for medical, 3,621 for weather) and state that "larger datasets" are needed to realize multimodal forecasting potential.
- Why unresolved: The experiments were conducted on relatively small, domain-specific datasets, and the paper doesn't explore how performance scales with dataset size or diversity.
- What evidence would resolve it: Systematic experiments varying dataset size, temporal range, and domain diversity while measuring multimodal vs. unimodal performance would establish minimum requirements.

### Open Question 3
- Question: Why does the hybrid model fail to improve upon the NLinear baseline despite incorporating both text and time series data?
- Basis in paper: [explicit] The authors state "our hybrid model matched but did not exceed these baselines" and speculate this is due to "inherent difficulty in improving on textual forecasting and the inability for models to adapt well to the time-series token."
- Why unresolved: The paper provides hypotheses but doesn't conduct detailed error analysis, study the impact of different architectural choices, or test alternative ways of integrating modalities.
- What evidence would resolve it: Detailed error analysis comparing model predictions, ablation studies removing text components, and experiments testing different integration architectures (e.g., attention mechanisms, cross-modal transformers) would identify failure points.

## Limitations
- Negative result: Hybrid-MMF fails to outperform baseline methods despite sophisticated multimodal architecture
- Dataset size constraints: Limited timesteps (7,589 medical, 3,621 weather) may prevent model from realizing multimodal potential
- Potential sampling biases: Wikipedia-based climate data may not capture all relevant meteorological concepts

## Confidence
- **Dataset quality**: Medium - curated methodology but potential sampling biases in domain coverage
- **Experimental design**: Medium - proper validation splits but limited scale and scope
- **Negative result interpretation**: Medium - result is clear but underlying causes remain uncertain

## Next Checks
1. Test the model on a significantly larger multimodal dataset to assess whether scale is the limiting factor
2. Conduct ablation studies to isolate which components of Hybrid-MMF contribute most to its underperformance
3. Evaluate alternative alignment strategies between text and time series embeddings to determine if the current approach is suboptimal for multimodal forecasting