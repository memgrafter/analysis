---
ver: rpa2
title: 'DP-MLM: Differentially Private Text Rewriting Using Masked Language Models'
arxiv_id: '2407.00637'
source_url: https://arxiv.org/abs/2407.00637
tags:
- privacy
- text
- dp-mlm
- token
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DP-MLM, a differentially private text rewriting
  method using masked language models (MLMs) instead of autoregressive models. The
  approach rewrites text one token at a time by leveraging BERT-based encoder-only
  models with a contextualization technique that concatenates the original sentence
  with the masked sentence.
---

# DP-MLM: Differentially Private Text Rewriting Using Masked Language Models

## Quick Facts
- arXiv ID: 2407.00637
- Source URL: https://arxiv.org/abs/2407.00637
- Reference count: 40
- Key outcome: DP-MLM achieves better utility preservation than state-of-the-art DP text rewriting methods at lower epsilon values using encoder-only masked language models with contextualization.

## Executive Summary
This paper introduces DP-MLM, a differentially private text rewriting method that leverages masked language models (MLMs) instead of autoregressive models. The approach rewrites text one token at a time by using BERT-based encoder-only models with a contextualization technique that concatenates the original sentence with the masked sentence. Experiments on GLUE benchmark tasks demonstrate that DP-MLM outperforms existing methods (DP-Paraphrase and DP-Prompt) in utility preservation across multiple epsilon values, particularly at lower privacy budgets. The method achieves comparable efficiency to DP-Paraphrase while being significantly faster than DP-Prompt, offering a practical balance between utility and privacy preservation.

## Method Summary
DP-MLM rewrites text through sequential token replacement using a masked language model (ROBERTA-BASE). For each token position, the token is replaced with a mask, the original and masked sentences are concatenated for context, and the MLM predicts replacement tokens. Temperature sampling on clipped logits achieves differential privacy through the exponential mechanism, with ε = 2Δu/T. By sequential composition, rewriting an n-token sentence requires nε-DP. The method optionally supports variable-length outputs through token addition and deletion probabilities.

## Key Results
- DP-MLM outperforms DP-Paraphrase and DP-Prompt on GLUE tasks across epsilon values {10, 25, 50, 100, 250}
- Encoder-only MLMs provide better utility preservation at lower epsilon levels compared to autoregressive methods
- Empirical privacy tests on Trustpilot and Yelp datasets show competitive privacy protection while maintaining superior utility scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DP-MLM achieves better utility preservation at lower ε levels compared to autoregressive methods.
- Mechanism: Uses encoder-only masked language models (MLMs) with a contextualization technique that concatenates the original sentence with the masked sentence, enabling context-aware token replacement.
- Core assumption: BERT-based encoder-only models provide better contextual information for token replacement than decoder-only models, leading to more semantically similar outputs while maintaining privacy.
- Evidence anchors:
  - [abstract]: "We find that utilizing encoder-only MLMs provides better utility preservation at lower ε levels, as compared to previous methods relying on larger models with a decoder."
  - [section 4.2]: "contextualization of the DP-MLM mechanism is achieved via the concatenation of the original input sentence, which is given along with the masked sentence as input to the MLM."
  - [corpus]: Weak - corpus shows only 0 citations for DP-MLM paper itself, so no external validation yet.
- Break condition: If the concatenated context fails to provide sufficient guidance, the model may produce outputs that deviate significantly from the original meaning, reducing utility.

### Mechanism 2
- Claim: Temperature sampling in MLMs is equivalent to the exponential mechanism for achieving differential privacy.
- Mechanism: Applies temperature sampling on clipped logits from the MLM, which maps to the exponential mechanism with ε = 2Δu/T.
- Core assumption: The logit values from the MLM are bounded, ensuring the sensitivity Δu is finite and enabling the privacy guarantee.
- Evidence anchors:
  - [section 4.3]: "the selection algorithm satisfies DP and this DP mechanism is termed as an Exponential Mechanism... we can derive that ε = 2Δu/T."
  - [section 5.1.1]: Describes clipping values based on empirical estimation of logit ranges to bound sensitivity.
  - [corpus]: No direct evidence in corpus; this is theoretical derivation from the paper.
- Break condition: If clipping is too aggressive, it may remove meaningful logit distinctions, reducing both utility and privacy guarantees.

### Mechanism 3
- Claim: Sequential token replacement with composition maintains the privacy guarantee.
- Mechanism: Each token is replaced independently using DP-MLM, and by sequential composition, the total privacy budget is nε-DP for a sentence of n tokens.
- Core assumption: The composition theorem for differential privacy applies when each token replacement is an independent mechanism.
- Evidence anchors:
  - [section 4.3]: "By sequential composition, the total privacy budget spent for rewriting the entire text would be nε-DP."
  - [section 4.2]: Describes the iterative process of replacing each token in the sentence.
  - [corpus]: No external validation in corpus; relies on standard DP composition theory.
- Break condition: If tokens are not replaced independently (e.g., if replacement of one token affects the context for the next), the composition guarantee may not hold.

## Foundational Learning

- Concept: Differential Privacy (DP) and Local Differential Privacy (LDP)
  - Why needed here: DP provides the formal privacy guarantee that the text rewriting mechanism must satisfy, ensuring that the rewritten text does not leak sensitive information.
  - Quick check question: What is the difference between central DP and local DP, and why is LDP stricter?

- Concept: Masked Language Models (MLMs) and their pre-training objectives
  - Why needed here: DP-MLM relies on MLMs to predict masked tokens in a context-aware manner, which is fundamental to the rewriting mechanism.
  - Quick check question: How does an MLM differ from an autoregressive model in terms of token prediction, and why is this difference important for DP-MLM?

- Concept: Temperature sampling and its equivalence to the exponential mechanism
  - Why needed here: Temperature sampling is used to sample replacement tokens in a way that satisfies DP, and understanding its connection to the exponential mechanism is crucial for the privacy analysis.
  - Quick check question: How does temperature sampling with logits relate to the exponential mechanism, and what role does the temperature parameter play in the privacy guarantee?

## Architecture Onboarding

- Component map:
  Input -> Tokenizer -> MLM (ROBERTA-BASE) -> Clipping module -> Temperature sampling -> Concatenation -> Output

- Critical path:
  1. Tokenize input sentence
  2. For each token position:
     - Replace token with mask
     - Concatenate original and masked sentences
     - Pass through MLM to get logits
     - Clip logits to bound sensitivity
     - Apply temperature sampling to select replacement token
  3. Return rewritten sentence

- Design tradeoffs:
  - Using ROBERTA-BASE (encoder-only) vs. larger decoder models: Tradeoff between utility preservation and model size/compute requirements
  - Clipping values: Tradeoff between ensuring finite sensitivity and preserving meaningful logit distinctions
  - Temperature parameter: Tradeoff between privacy (lower temperature) and utility (higher temperature)

- Failure signatures:
  - If rewritten text has very low BLEU/cosine similarity to original but high utility: Indicates excessive privacy preservation at the cost of utility
  - If rewritten text has high similarity but poor utility: Indicates insufficient privacy preservation
  - If privacy F1 scores are high but utility F1 is low: Indicates the model is successfully hiding information but at the cost of meaningful content

- First 3 experiments:
  1. Run DP-MLM on a small sample from GLUE SST2 dataset with ε=10, compare BLEU and utility scores to baseline
  2. Test the effect of different clipping values on the sensitivity and resulting privacy guarantees
  3. Compare DP-MLM rewritten outputs to original inputs using semantic similarity metrics (e.g., cosine similarity with sentence transformers)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of base MLM model (e.g., BERT, RoBERTa, or other variants) impact the utility-privacy trade-off in DP-MLM compared to the currently used RoBERTa-base?
- Basis in paper: [explicit] The paper states: "As our rewriting mechanism leveraging DP-MLM relies on token-level DP replacements... we propose an improved version of the rewriting mechanism which enables variable length outputs" and mentions that "we choose one model as the representative model for DP-MLM... namely RoBERTa-base" as a limitation.
- Why unresolved: The paper explicitly identifies this as a limitation, noting that "The implications of choosing and evaluating other BERT-based models were considered outside of the scope of this work."
- What evidence would resolve it: Comparative experiments testing DP-MLM with different base MLM models (BERT, ALBERT, ELECTRA, etc.) across the same GLUE tasks and privacy budgets to measure differences in utility scores, privacy protection, and efficiency.

### Open Question 2
- Question: What is the optimal balance between token addition and deletion probabilities in Algorithm 3 for maximizing the utility-privacy trade-off?
- Basis in paper: [explicit] The paper introduces Algorithm 3 for variable-length output and presents empirical results showing "higher utility results but lower empirical privacy" with token addition, but does not systematically explore the optimal balance.
- Why unresolved: The paper only tests two addition probabilities (0.1 and 0.25) with a fixed deletion probability of 0.05, without exploring the full parameter space or determining optimal combinations.
- What evidence would resolve it: Systematic grid search experiments varying both addition (A) and deletion (D) probabilities across multiple values (e.g., 0.0, 0.05, 0.1, 0.25, 0.5) to identify parameter combinations that maximize relative gain across different ε values and tasks.

### Open Question 3
- Question: How does the length variability introduced by Algorithm 3 affect the empirical privacy measurements, particularly in the adaptive setting?
- Basis in paper: [explicit] The paper notes that Algorithm 3 "offers a privacy guarantee of 2nε-DP" in the worst case and presents results showing "lower empirical privacy, especially in the adaptive setting" when using token addition.
- Why unresolved: The paper observes this phenomenon but does not provide a theoretical explanation for why length variability specifically impacts adaptive privacy measurements differently than static measurements.
- What evidence would resolve it: Theoretical analysis of how length variability affects the adversary's ability to train on privatized data in the adaptive setting, combined with controlled experiments isolating the effect of length changes on privacy metrics while holding other variables constant.

### Open Question 4
- Question: What is the relationship between the temperature parameter T (derived from ε and sensitivity) and the utility-privacy trade-off in DP-MLM, and how does this compare to temperature sampling in autoregressive models?
- Basis in paper: [explicit] The paper describes the temperature sampling mechanism as equivalent to the exponential mechanism with ε = 2∆u/T, but does not systematically explore how different temperature settings affect performance.
- Why unresolved: While the paper establishes the theoretical relationship between temperature and ε, it does not experimentally investigate how varying temperature within the same ε budget affects utility and privacy outcomes.
- What evidence would resolve it: Controlled experiments testing different temperature values for fixed ε budgets to measure their impact on utility scores, privacy protection, and the resulting utility-privacy trade-off curves.

## Limitations

- Experimental validation is primarily limited to controlled benchmark datasets (GLUE, Trustpilot, Yelp), which may not generalize to all real-world text rewriting scenarios
- The paper lacks extensive ablation studies examining the impact of specific design choices like clipping values and temperature parameters on the utility-privacy tradeoff
- Privacy analysis relies on theoretical guarantees that assume bounded logit values and independent token replacement, which may not hold perfectly in practice

## Confidence

**High Confidence Claims:**
- DP-MLM achieves better utility preservation than DP-Paraphrase and DP-Prompt on GLUE benchmark tasks
- Encoder-only MLMs provide better utility preservation at lower epsilon levels compared to autoregressive methods
- Sequential token replacement with composition maintains the privacy guarantee

**Medium Confidence Claims:**
- Temperature sampling is equivalent to the exponential mechanism for achieving differential privacy
- The contextualization technique through sentence concatenation significantly improves output quality
- DP-MLM offers practical advantages in terms of balance between utility and privacy preservation

**Low Confidence Claims:**
- DP-MLM's performance would generalize to all real-world text rewriting scenarios
- The specific clipping values and temperature parameters are optimal for all use cases
- The privacy guarantees hold perfectly under all operational conditions

## Next Checks

1. **Sensitivity Analysis on Clipping Values**: Conduct an ablation study varying the logit clipping thresholds systematically to determine their impact on both utility preservation and privacy guarantees. Test clipping values at 0.5, 1.0, 1.5, and 2.0 standard deviations from the mean logit values to establish the optimal tradeoff point.

2. **Cross-Dataset Generalization Test**: Evaluate DP-MLM on diverse text domains beyond GLUE, Trustpilot, and Yelp, including social media posts, medical notes, and legal documents. Measure performance consistency across these domains to assess whether the utility-privacy balance holds universally or is dataset-dependent.

3. **Privacy Boundary Testing**: Design adversarial experiments to test the limits of DP-MLM's privacy guarantees by attempting to reconstruct original text from rewritten outputs under varying privacy budgets. Use both white-box attacks (knowing the model architecture) and black-box attacks (only observing inputs and outputs) to stress-test the theoretical privacy bounds.