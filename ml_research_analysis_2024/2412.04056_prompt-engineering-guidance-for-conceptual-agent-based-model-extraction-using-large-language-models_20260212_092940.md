---
ver: rpa2
title: Prompt Engineering Guidance for Conceptual Agent-based Model Extraction using
  Large Language Models
arxiv_id: '2412.04056'
source_url: https://arxiv.org/abs/2412.04056
tags:
- information
- prompt
- json
- data
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a systematic approach to extracting information
  from conceptual agent-based models using question-answering techniques with Large
  Language Models (LLMs). The method employs a series of 9 structured prompts targeting
  model aim, agent sets, environment, and model execution information.
---

# Prompt Engineering Guidance for Conceptual Agent-based Model Extraction using Large Language Models

## Quick Facts
- arXiv ID: 2412.04056
- Source URL: https://arxiv.org/abs/2412.04056
- Authors: Siamak Khatami; Christopher Frantz
- Reference count: 12
- Key outcome: Systematic approach using 9 structured prompts to extract agent-based model information from conceptual descriptions via LLMs

## Executive Summary
This paper presents a systematic methodology for extracting information from conceptual agent-based models using question-answering techniques with Large Language Models. The approach employs a series of 9 structured prompts designed to target specific model components including model aim, agent sets, environment, and model execution information. Each prompt follows a consistent format requesting JSON-formatted outputs without supplementary text, enabling both human analysis and auto-code generation. The method addresses the challenge of automating agent-based model generation by providing a structured framework for information extraction that can be applied to conceptual model text.

## Method Summary
The methodology uses a series of 9 structured prompts following a consistent JSON output format without supplementary text. The prompts target specific model components: model purpose (description, research questions, system boundaries, outcome variables), agent sets (agent lists, variables, data types, initial values, equations), environment information (space type, variables, equations), and model execution details (variables, execution information). The approach requires a QA model (like ChatGPT) with provided instructions to process conceptual model text through each prompt sequentially. The prompts are designed to extract specific model components including descriptions, variables, data types, initial values, equations, and execution details while avoiding nested prompts that could reduce accuracy.

## Key Results
- Systematic approach using 9 structured prompts for extracting agent-based model information
- JSON-formatted outputs enable both human analysis and auto-code generation
- Method addresses automation challenges in agent-based model generation
- Prompts target model aim, agent sets, environment, and execution information
- Consistent format requests outputs without supplementary text

## Why This Works (Mechanism)
The approach works by leveraging LLMs' ability to parse natural language descriptions and structure the extracted information into standardized JSON formats. By breaking down the extraction process into 9 focused prompts, each targeting specific model components, the method reduces cognitive load on the LLM and improves accuracy. The consistent format and avoidance of nested prompts help prevent hallucination and ensure that extracted information directly corresponds to what's explicitly stated in the source material.

## Foundational Learning
- JSON data structuring: Why needed - Enables machine-readable output for auto-code generation; Quick check - Verify outputs are valid JSON with expected fields
- Prompt engineering best practices: Why needed - Maximizes LLM extraction accuracy; Quick check - Test prompts with sample texts and verify completeness
- Agent-based modeling concepts: Why needed - Ensures proper extraction of model-specific elements; Quick check - Confirm extracted components align with standard ABM terminology
- QA model instruction following: Why needed - Critical for obtaining structured rather than conversational outputs; Quick check - Verify model adheres to "no supplementary text" requirement
- Conceptual model interpretation: Why needed - Enables translation from natural language to structured data; Quick check - Test extraction from models with varying levels of detail
- Information hierarchy understanding: Why needed - Guides the sequential prompt structure; Quick check - Verify that subsequent prompts build on information from previous ones

## Architecture Onboarding
Component map: Conceptual Model Text -> Prompt 1 (Model Aim) -> Prompts 2-4 (Agent Sets) -> Prompts 5-7 (Environment) -> Prompts 8-9 (Model Execution) -> JSON Outputs

Critical path: The sequential execution of all 9 prompts in order, as each builds upon the information context established by previous prompts.

Design tradeoffs: The approach sacrifices potential efficiency gains from combining prompts to maintain accuracy. While nested prompts could reduce the total number needed, experiments showed this decreased accuracy.

Failure signatures: Incomplete JSON outputs, hallucinated data not present in source text, missing nested information, or failure to follow JSON formatting requirements.

Three first experiments:
1. Test Prompt 1 on a simple conceptual model to verify model purpose extraction accuracy
2. Apply Prompts 2-4 sequentially to verify agent set information extraction completeness
3. Run the complete 9-prompt sequence on a multi-agent model to test end-to-end functionality

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the actual accuracy and reliability of the extracted information when using these prompts across different types of agent-based models?
- Basis in paper: The paper mentions that experiments reported "a lack of accuracy for the current LLMs" when using nested prompts, but does not provide specific performance metrics or quantitative accuracy results for the proposed prompt structure.
- Why unresolved: The paper focuses on presenting the prompt engineering methodology rather than evaluating its performance. No quantitative results, benchmark comparisons, or error rate analysis are provided.
- What evidence would resolve it: Empirical evaluation results showing precision, recall, and F1 scores for each prompt category across multiple test models with varying complexity.

### Open Question 2
- Question: How does the prompt engineering approach handle ambiguous or incomplete conceptual model descriptions where certain model elements are not explicitly stated?
- Basis in paper: The instructions explicitly state "Do not summarize information; neither truncate nor auto-generate and augment any data if data and information are not presented in the provided document," suggesting this is a known limitation.
- Why unresolved: While the paper acknowledges this constraint, it doesn't explore how the approach performs with real-world conceptual models that often contain gaps, ambiguities, or implicit assumptions.
- What evidence would resolve it: Case studies demonstrating performance on conceptual models with varying levels of completeness and ambiguity, including examples of how missing information is handled.

### Open Question 3
- Question: What is the optimal sequence and number of prompts for extracting model information, and can certain prompts be effectively combined without loss of accuracy?
- Basis in paper: The paper states that "Prompt 4 can be merged with prompt 3 to reduce the quantity of prompts" but chose not to due to accuracy concerns, and notes that "experiments presented in [4] reported a lack of accuracy for the current LLMs" when using nested prompts.
- Why unresolved: The paper separates information extraction into 9 distinct prompts but doesn't empirically determine the minimal set of prompts needed or identify which combinations work best without compromising accuracy.
- What evidence would resolve it: Systematic experiments comparing different prompt sequencing strategies, including ablation studies showing which prompts are essential and which can be combined or eliminated while maintaining extraction quality.

## Limitations
- No specific performance metrics or accuracy evaluation results provided
- Specific QA model used in experiments not identified
- Potential for LLM hallucination when information is missing from source text
- Avoidance of nested prompts may limit extraction of complex hierarchical information
- Dependence on quality and completeness of source conceptual model descriptions

## Confidence
- Medium confidence in methodology framework based on clear prompt specifications
- Low confidence in accuracy and reliability due to absence of empirical validation data
- Medium confidence in JSON output structure based on explicit format requirements

## Next Checks
1. Test the prompt series on multiple conceptual agent-based model descriptions with varying complexity levels to evaluate extraction consistency and completeness
2. Compare LLM-generated JSON outputs against manually extracted information from the same source texts to quantify accuracy and identify hallucination patterns
3. Experiment with different QA model architectures (GPT-4, Claude, LLaMA variants) using the same prompt templates to determine optimal model selection for this task