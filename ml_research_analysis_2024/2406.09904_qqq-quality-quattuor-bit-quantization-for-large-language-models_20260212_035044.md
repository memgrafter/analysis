---
ver: rpa2
title: 'QQQ: Quality Quattuor-Bit Quantization for Large Language Models'
arxiv_id: '2406.09904'
source_url: https://arxiv.org/abs/2406.09904
tags:
- quantization
- gemm
- w4a8
- arxiv
- per-group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accelerating both the prefill
  and decoding stages of large language model (LLM) inference while maintaining model
  performance. The authors propose QQQ, a quality quattuor-bit quantization method
  with 4-bit weights and 8-bit activations.
---

# QQQ: Quality Quattuor-Bit Quantization for Large Language Models

## Quick Facts
- arXiv ID: 2406.09904
- Source URL: https://arxiv.org/abs/2406.09904
- Authors: Ying Zhang; Peng Zhang; Mincong Huang; Jingyang Xiang; Yujie Wang; Chao Wang; Yineng Zhang; Lei Yu; Chuan Liu; Wei Lin
- Reference count: 4
- Primary result: QQQ achieves 2.24x speedup over FP16 on LLaMA-2-13B with maintained accuracy

## Executive Summary
QQQ introduces a novel 4-bit weight and 8-bit activation quantization method for large language models that achieves significant inference speedups while maintaining model performance. The approach combines adaptive smoothing to handle activation outliers with Hessian-based compensation to minimize quantization loss in weights. Specialized W4A8 GEMM kernels are developed to leverage INT8 Tensor Cores for efficient inference. Experimental results demonstrate QQQ's effectiveness across various LLaMA model sizes, achieving up to 2.24x speedup over FP16 with maintained perplexity and accuracy.

## Method Summary
QQQ employs a two-stage quantization pipeline that first applies adaptive smoothing to activation channels with outliers, then uses Hessian-based compensation to minimize weight quantization loss. The method uses 4-bit weights and 8-bit activations, with specialized W4A8 GEMM kernels optimized for both per-channel and per-group quantization patterns. Adaptive smoothing selectively smooths problematic activation channels while leaving others intact, improving quantization accuracy. Hessian-based compensation then adjusts weights based on second-order information to correct for quantization-induced errors. The approach is implemented within vLLM using optimized INT8 Tensor Core operations.

## Key Results
- Achieves 2.24x speedup over FP16, 2.10x over W8A8, and 1.25x over W4A16 on LLaMA-2-13B
- Maintains perplexity on WikiText2 comparable to higher-bit baselines
- Improves zero-shot accuracy on PIQA, HellaSwag, WinoGrande, and ARC tasks
- Outperforms state-of-the-art quantization methods across model sizes (7B, 13B, 30B, 70B variants)

## Why This Works (Mechanism)

### Mechanism 1
QQQ maintains model performance while significantly accelerating inference by combining adaptive smoothing with Hessian-based quantization compensation. Adaptive smoothing selectively smooths activation channels with outliers, improving quantization accuracy. Hessian-based compensation then minimizes quantization loss in weights. Core assumption: Quantization error can be reduced by targeting problematic channels rather than applying uniform smoothing. Evidence anchors: [abstract] "QQQ employs adaptive smoothing and Hessian-based compensation, significantly enhancing the performance of quantized models without extensive training." [section] "QQQ adaptively smooths activation channels with significant outliers while leaving other channels intact, enabling better activation quantization."

### Mechanism 2
QQQ achieves substantial speedups by implementing specialized W4A8 GEMM kernels optimized for both per-channel and per-group quantization. INT8 Tensor Cores are leveraged through optimized INT4-to-INT8 weight conversion and fused dequantization steps, reducing memory access and computational overhead. Core assumption: INT8 Tensor Cores provide superior throughput compared to FP16 for compute-bound operations. Evidence anchors: [abstract] "Our specialized per-channel W4A8 GEMM and per-group W4A8 GEMM achieve impressive speed increases of 3.67× and 3.29× over FP16 GEMM." [section] "To achieve fast inference speed, QQQ incorporates highly efficient W4A8 GEMM kernels tailored for both per-channel and per-group quantization."

### Mechanism 3
QQQ's two-stage quantization process improves accuracy by first smoothing activations and then compensating for weight quantization loss. Adaptive smoothing transforms problematic activations into more quantization-friendly representations. Hessian-based compensation then corrects for the quantization-induced weight loss. Core assumption: Separating the quantization challenges of activations and weights allows for more effective optimization of each. Evidence anchors: [section] "After transferring the quantization challenge from activations to weights through adaptive smoothing, it becomes crucial to enhance the quantization technique for weights." [section] "QQQ tackles the challenges of weight quantization by employing Hessian-based compensation, which effectively minimizes the loss incurred during the quantization process."

## Foundational Learning

- Concept: Activation outlier detection and smoothing
  - Why needed here: Activation outliers can cause significant quantization errors. Identifying and smoothing only problematic channels improves overall quantization accuracy.
  - Quick check question: How would you determine which activation channels require smoothing, and what smoothing method would you use?

- Concept: Hessian-based weight quantization compensation
  - Why needed here: Weight quantization introduces approximation errors. Hessian-based compensation minimizes these errors by adjusting weights based on second-order information.
  - Quick check question: How does the Hessian matrix inform the weight compensation process, and why is this approach effective for minimizing quantization error?

- Concept: INT8 Tensor Core utilization and optimization
  - Why needed here: INT8 Tensor Cores provide significant speedup for matrix operations. Optimizing GEMM kernels for INT8 operations is crucial for achieving QQQ's performance gains.
  - Quick check question: What are the key considerations when designing GEMM kernels for INT8 Tensor Cores, and how do they differ from FP16 kernel design?

## Architecture Onboarding

- Component map: Adaptive smoothing module -> Hessian-based compensation module -> W4A8 GEMM kernel library
- Critical path: The critical path is the quantization pipeline: activation smoothing → weight quantization compensation → INT8 GEMM execution
- Design tradeoffs: Per-channel vs. per-group quantization. Per-channel offers more precise scaling but higher overhead. Per-group reduces overhead but may sacrifice some accuracy
- Failure signatures: Significant performance degradation indicates issues with outlier detection/smoothing or compensation algorithm. Unexpected speedups may suggest inefficient FP16 implementation rather than INT8 optimization
- First 3 experiments:
  1. Implement and validate adaptive smoothing on a small model, measuring impact on activation quantization error
  2. Integrate Hessian-based compensation with smoothing, evaluating overall quantization accuracy on validation tasks
  3. Implement per-channel W4A8 GEMM kernel and benchmark against FP16 baseline on representative matrix sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does QQQ's performance scale with model size beyond the tested LLaMA-2-70B?
- Basis in paper: [explicit] The paper states that "QQQ's performance enhancement is more pronounced on larger models, as evidenced by the greater speedup achieved on LLaMA-2-13B compared to LLaMA-2-7B." However, the largest model tested was LLaMA-2-70B.
- Why unresolved: The paper does not provide data for models larger than LLaMA-2-70B, and the scalability of QQQ's performance on even larger models remains unexplored.
- What evidence would resolve it: Experimental results showing QQQ's performance (speedup and accuracy) on models larger than LLaMA-2-70B, such as GPT-3 175B or larger, would provide insight into how well QQQ scales.

### Open Question 2
- Question: What is the impact of different outlier thresholds (σ) on QQQ's performance?
- Basis in paper: [explicit] The paper mentions that "we introduce an outlier threshold σ, which is determined through a grid search within the range [0, max(abs(X))]."
- Why unresolved: The paper does not provide details on how different values of σ affect the performance of QQQ, nor does it discuss the sensitivity of the method to this parameter.
- What evidence would resolve it: A systematic study of QQQ's performance with various σ values, showing the relationship between outlier threshold and model accuracy/speedup, would clarify the impact of this parameter.

### Open Question 3
- Question: How does QQQ perform on non-English languages or multilingual tasks?
- Basis in paper: [inferred] The paper focuses on evaluating QQQ on English language tasks (WikiText2, PIQA, ARC, etc.) and does not mention any experiments on non-English or multilingual datasets.
- Why unresolved: The effectiveness of QQQ on languages other than English is not addressed, and it is unclear whether the adaptive smoothing and Hessian-based compensation techniques generalize well to other languages.
- What evidence would resolve it: Experimental results demonstrating QQQ's performance on multilingual datasets or non-English language tasks would indicate its effectiveness across different languages.

## Limitations

- Adaptive smoothing mechanism relies heavily on proper outlier detection threshold tuning, which is not fully specified
- Hessian-based compensation assumes second-order information remains valid after activation smoothing, but this relationship is not empirically validated across diverse model architectures
- Claimed speedups depend on specific hardware (NVIDIA A100 with INT8 Tensor Cores) that may not be universally available

## Confidence

- **High confidence**: The W4A8 GEMM kernel implementation and associated speed measurements (3.67x and 3.29x speedups) are verifiable through the provided code and can be benchmarked independently
- **Medium confidence**: The two-stage quantization approach (adaptive smoothing followed by Hessian-based compensation) is theoretically sound, but the paper lacks ablation studies showing the individual contribution of each component to the final performance
- **Medium confidence**: The perplexity and zero-shot accuracy comparisons with baselines are reported, but the paper does not provide statistical significance testing or variance across multiple runs

## Next Checks

1. Implement QQQ without the adaptive smoothing component and measure the degradation in perplexity and accuracy on WikiText2 and zero-shot tasks to quantify the contribution of smoothing to overall performance

2. Systematically vary the outlier threshold σ across a range of values (e.g., 0.05, 0.1, 0.2) and measure the impact on both accuracy and inference speed to determine the robustness of the method to hyperparameter choices

3. Apply QQQ to a different LLM architecture (e.g., OPT or Falcon) and evaluate whether the same σ = 0.1 threshold and compensation parameters work effectively, or if architecture-specific tuning is required