---
ver: rpa2
title: Lessons from Studying Two-Hop Latent Reasoning
arxiv_id: '2411.16353'
source_url: https://arxiv.org/abs/2411.16353
tags:
- reasoning
- two-hop
- facts
- training
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models can perform
  two-hop reasoning (combining two separately learned facts) without explicit chain-of-thought
  (CoT) reasoning. The authors create a controlled experimental setup using synthetic
  facts to eliminate memorization and reasoning shortcuts as explanations for model
  performance.
---

# Lessons from Studying Two-Hop Latent Reasoning

## Quick Facts
- arXiv ID: 2411.16353
- Source URL: https://arxiv.org/abs/2411.16353
- Reference count: 40
- Primary result: Models fail at latent two-hop reasoning over synthetic facts but succeed when facts co-occur or involve natural pretraining knowledge

## Executive Summary
This paper investigates whether large language models can perform two-hop reasoning (combining two separately learned facts) without explicit chain-of-thought reasoning. The authors create a controlled experimental setup using synthetic facts to eliminate memorization and reasoning shortcuts as explanations for model performance. The key findings show that models completely fail to compose two synthetic facts learned through fine-tuning without CoT, achieving only chance-level accuracy despite perfect recall of individual facts. However, models succeed at two-hop reasoning when facts co-occur in training documents or when one fact is naturally acquired during pretraining while the other is synthetic.

## Method Summary
The authors create a synthetic dataset with 10,000 three-part facts (head, relation, tail) to study two-hop reasoning. They fine-tune models on individual facts and test whether models can compose them to answer questions requiring two-hop reasoning without chain-of-thought prompting. The experimental design isolates reasoning ability by ensuring facts are never co-present in training documents. They test various model sizes (3B, 8B, 70B parameters) and explore interventions like forcing correct fact storage order across transformer layers and providing activation supervision. The controlled setup eliminates common confounders like memorization and reasoning shortcuts that plague natural language datasets.

## Key Results
- Models achieve only chance-level accuracy on two-hop reasoning tasks when facts are learned separately through fine-tuning
- Perfect individual fact recall but complete failure to compose facts demonstrates the gap between memorization and reasoning
- Models succeed at two-hop reasoning when facts co-occur in training documents or when one fact comes from natural pretraining
- Layer-wise supervision and activation control interventions fail to enable latent reasoning over synthetic facts

## Why This Works (Mechanism)
The paper's experimental design fundamentally shapes conclusions about LLM reasoning capabilities, with apparent limitations or successes potentially being artifacts of evaluation methodology rather than fundamental constraints. The synthetic fact structure creates a clean test bed for isolating reasoning from memorization, but the artificial nature of the learning task may prevent models from developing the compositional representations needed for two-hop reasoning. The failure suggests that models may require explicit training signals for fact composition or that the transformer architecture has difficulty representing and composing knowledge that is learned in isolation.

## Foundational Learning
- Synthetic fact datasets: Why needed - to eliminate confounders like memorization and reasoning shortcuts; Quick check - verify facts are never co-present in training documents
- Two-hop reasoning evaluation: Why needed - to test composition of separately learned facts; Quick check - ensure test questions require combining exactly two facts
- Chain-of-thought prompting: Why needed - to distinguish latent from explicit reasoning; Quick check - compare performance with and without CoT prompts
- Layer-wise supervision: Why needed - to explore whether forcing correct fact storage enables reasoning; Quick check - verify activation patterns match supervision targets
- Fact composition: Why needed - to understand whether models can combine knowledge across domains; Quick check - test with one synthetic and one natural fact

## Architecture Onboarding
Component map: Synthetic facts -> Fine-tuning -> Latent storage -> Two-hop reasoning task
Critical path: Fact learning (fine-tuning) -> Knowledge representation (transformer layers) -> Fact composition (output layer reasoning)
Design tradeoffs: Synthetic vs natural facts (control vs realism), explicit vs latent reasoning (observability vs capability), model size vs training efficiency
Failure signatures: Perfect individual fact recall but chance-level two-hop accuracy, success only when facts co-occur or involve natural knowledge
First experiments: 1) Test whether progressive training (learning facts in sequences) enables latent reasoning, 2) Evaluate larger models (175B+) on same synthetic dataset, 3) Replicate with real-world knowledge triples from Wikidata

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the findings raise several implicit questions about whether the failure to compose synthetic facts reflects fundamental limitations of transformer architectures or artifacts of the experimental design and learning task structure.

## Limitations
- Synthetic facts are simple and structured, potentially not reflecting real-world knowledge complexity or distribution
- Study focuses on narrow model size range (3B, 8B, 70B parameters), limiting conclusions about larger models
- Interventions were exploratory rather than systematically optimized, leaving open possibility that alternative training approaches could enable latent reasoning

## Confidence
High confidence in core empirical finding that models fail at latent two-hop reasoning over synthetic facts
Medium confidence in interpretation that this reflects fundamental limitation rather than experimental artifact
Medium confidence in conclusion about experimental design's role in shaping reasoning capability assessments

## Next Checks
1. Replicate core findings using real-world knowledge triples (e.g., from Wikidata) with controlled training and testing to assess generalizability beyond synthetic facts
2. Test whether progressive training (learning facts in sequences that naturally build toward reasoning tasks) enables latent two-hop reasoning over synthetic facts
3. Evaluate whether larger models (e.g., 175B+ parameters) or different architectural variants show improved latent reasoning capabilities on the same synthetic dataset