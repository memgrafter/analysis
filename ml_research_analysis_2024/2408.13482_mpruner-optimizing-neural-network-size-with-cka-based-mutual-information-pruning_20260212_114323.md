---
ver: rpa2
title: 'MPruner: Optimizing Neural Network Size with CKA-Based Mutual Information
  Pruning'
arxiv_id: '2408.13482'
source_url: https://arxiv.org/abs/2408.13482
tags:
- pruning
- mpruner
- layers
- iter
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MPruner introduces a novel neural network pruning method that leverages
  Centered Kernel Alignment (CKA) similarity to identify and remove redundant layers
  across entire architectures. Unlike previous pruning approaches that focus on local
  layer pairs or weight-level sparsity, MPruner uses multi-layer clustering to globally
  assess layer contributions and prunes entire clusters when they exhibit high similarity.
---

# MPruner: Optimizing Neural Network Size with CKA-Based Mutual Information Pruning

## Quick Facts
- arXiv ID: 2408.13482
- Source URL: https://arxiv.org/abs/2408.13482
- Authors: Seungbeom Hu; ChanJun Park; Andrew Ferraiuolo; Sang-Ki Ko; Jinwoo Kim; Haein Song; Jieung Kim
- Reference count: 38
- Primary result: Achieves up to 50% parameter reduction with minimal accuracy loss on ResNet and transformer models

## Executive Summary
MPruner introduces a novel neural network pruning method that leverages Centered Kernel Alignment (CKA) similarity to identify and remove redundant layers across entire architectures. Unlike previous pruning approaches that focus on local layer pairs or weight-level sparsity, MPruner uses multi-layer clustering to globally assess layer contributions and prunes entire clusters when they exhibit high similarity. The method achieves up to 50% reduction in parameters and memory usage for both CNN (ResNet50/152) and transformer-based models (BERT, CodeT5, T5) with minimal to no accuracy loss.

## Method Summary
MPruner operates in three phases: analysis using CKA similarity to identify redundant layer clusters, optimization through multi-layer pruning based on similarity scores and pruning granularity, and recovery with retraining. The method computes CKA similarity between layer activations to form clusters of redundant layers, then prunes these clusters while maintaining accuracy within a specified threshold. After pruning, a short retraining phase (3 epochs) recovers any performance degradation. The approach demonstrates compatibility with existing magnitude pruning techniques, enabling further compression when combined with tools like Wanda.

## Key Results
- Achieves up to 50% parameter reduction across tested architectures (ResNet50/152, BERT, CodeT5, T5)
- Maintains minimal to no accuracy loss when pruning up to 50% of layers
- Successfully combines with magnitude pruning methods for additional compression gains
- Provides mathematical guarantees of accuracy preservation through CKA-based similarity metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CKA similarity enables layer-level redundancy detection that is more robust than local mutual information methods.
- Mechanism: CKA computes centered Gram matrices for layer activations, capturing global structural similarity beyond adjacent layer pairs. By clustering layers whose CKA scores exceed a threshold, the method identifies entire redundant modules rather than just individual filters or neurons.
- Core assumption: Layers with high CKA similarity perform similar functions and can be collapsed without degrading model accuracy.
- Evidence anchors:
  - [abstract] "MPruner utilizes layer clustering with the Centered Kernel Alignment (CKA) similarity metric, allowing us to incorporate global information from the neural network for more precise and efficient layer-wise pruning."
  - [section] "MPruner introduces a novel multi-architectural layer pruning approach that significantly enhances the use of analytical metrics by incorporating a state-of-the-art layer similarity metric to guide pruning, overcoming the limitations of previous and related methods."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.5, average citations=0.0. Top related titles: Pruning Deep Convolutional Neural Network Using Conditional Mutual Information, Mutual Information Preserving Neural Network Pruning."
- Break condition: If CKA threshold is set too high, no clusters form and pruning fails; if too low, inaccurate clustering may occur.

### Mechanism 2
- Claim: Multi-layer cluster pruning preserves model accuracy by mathematically guaranteeing that removed layers are redundant.
- Mechanism: The method iteratively prunes entire clusters of similar layers rather than individual weights, maintaining structural integrity. After pruning, retraining recovers performance while preserving the mathematical safety guarantees.
- Core assumption: The global contribution of a cluster can be approximated by a single representative layer without loss of functionality.
- Evidence anchors:
  - [abstract] "MPruner uses an accuracy threshold rather than a sparsity level as the pruning criterion. MPruner also determines how many layers can be mathematically and safely pruned while staying within the specified threshold."
  - [section] "Our approach utilizes comprehensive global information from pre-trained networks, incorporates a retraining phase, and is adaptable to various architectures and pruning configurations."
  - [corpus] Weak - no direct evidence in corpus about mathematical guarantees for cluster pruning.
- Break condition: Accuracy drop exceeds the threshold during iterative pruning, triggering adjustment of pruning granularity.

### Mechanism 3
- Claim: Integration with magnitude pruning methods enables further compression without accuracy loss.
- Mechanism: MPruner first identifies optimal layer clusters to prune, then applies fine-grained weight pruning to the remaining structure. This two-stage approach combines global redundancy removal with local sparsity optimization.
- Core assumption: Layer-level pruning creates space for additional weight-level pruning without compromising accuracy.
- Evidence anchors:
  - [abstract] "MPruner also demonstrates compatibility with existing magnitude pruning techniques, enabling further compression when combined with tools like Wanda."
  - [section] "To validate the safety and effectiveness of our pruning results, we conduct stress tests and apply an established pruning technique (Sun et al. 2024) to both the original and pruned networks produced by MPruner."
  - [corpus] Weak - limited evidence in corpus about integration with magnitude pruning methods.
- Break condition: Combined approach causes accuracy degradation beyond acceptable thresholds.

## Foundational Learning

- Concept: Centered Kernel Alignment (CKA) similarity metric
  - Why needed here: CKA provides a mathematically grounded way to measure layer similarity across entire architectures, enabling global redundancy detection rather than local pairwise comparisons.
  - Quick check question: What is the key difference between CKA and traditional correlation-based similarity measures when applied to neural network layers?

- Concept: Gram matrix computation and centering
  - Why needed here: The CKA calculation requires computing Gram matrices of layer activations and centering them using the centering matrix, which is fundamental to the similarity measurement.
  - Quick check question: Why is centering the Gram matrix necessary in the CKA computation?

- Concept: Pruning granularity and cluster-based removal
  - Why needed here: Understanding how to control the number of layers removed from each cluster is critical for balancing compression ratio against accuracy preservation.
  - Quick check question: What happens to model accuracy when pruning granularity is set to 2 versus 1 in MPruner?

## Architecture Onboarding

- Component map: CKA similarity calculator (GetCKAMatrix) -> Cluster identification module (GetCandidates) -> Layer pruning engine (Pruner) -> Retraining controller (Trainer) -> Accuracy monitoring system

- Critical path: CKA calculation -> Cluster identification -> Layer pruning -> Retraining -> Accuracy validation

- Design tradeoffs:
  - Higher CKA thresholds -> fewer clusters -> less aggressive pruning
  - Lower CKA thresholds -> more clusters -> higher compression but risk accuracy loss
  - Pruning granularity (1 vs 2) -> trade-off between compression ratio and accuracy stability

- Failure signatures:
  - No clusters identified -> CKA threshold too high
  - Rapid accuracy degradation -> pruning granularity too aggressive
  - Memory overflow during CKA computation -> dataset size too large for current hardware

- First 3 experiments:
  1. Apply MPruner to a pretrained BERT model on a small text classification dataset with Ï„=99% and observe cluster formation
  2. Test pruning granularity adjustment on ResNet50 to find the optimal balance between compression and accuracy
  3. Combine MPruner output with Wanda pruning to measure additional compression gains on transformer models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MPruner perform on large language models (LLaMa, GPT) compared to existing pruning methods?
- Basis in paper: [explicit] "We have not tested MPruner on large language models, such as LLaMa, but the pruning ratios achieved in our three experiments are comparable to or exceed those reported in previous works"
- Why unresolved: The paper only tested MPruner on smaller models (ResNet50/152, BERT, CodeT5, T5) and makes predictions about LLaMa/GPT performance without experimental evidence.
- What evidence would resolve it: Direct application and evaluation of MPruner on LLaMa or GPT models with quantitative comparison to state-of-the-art pruning methods for LLMs.

### Open Question 2
- Question: What is the optimal CKA threshold for different model architectures and datasets?
- Basis in paper: [inferred] The paper uses fixed thresholds (95%, 98%, 99%) without systematic exploration of their impact on different architectures.
- Why unresolved: The paper uses fixed thresholds without systematic exploration of their impact on different architectures and datasets.
- What evidence would resolve it: Systematic experiments varying CKA thresholds across different model architectures and datasets to identify optimal threshold ranges.

### Open Question 3
- Question: How does MPruner's pruning granularity (k=1 vs k=2) affect model performance and efficiency?
- Basis in paper: [explicit] "We offer the option to adjust the pruning granularity to values other than 1... If the granularity is set to 2, the algorithm will attempt to remove every other layer in the cluster"
- Why unresolved: The paper only briefly mentions granularity adjustment but doesn't provide comprehensive analysis of its effects.
- What evidence would resolve it: Controlled experiments systematically varying pruning granularity across different architectures to quantify its impact on accuracy, efficiency, and optimal settings.

## Limitations
- The mathematical guarantees of accuracy preservation through CKA-based clustering remain partially theoretical, with limited empirical validation across diverse architectures beyond the tested CNN and transformer models.
- Integration with magnitude pruning methods like Wanda shows promise but lacks comprehensive quantitative analysis of combined compression gains across multiple architectures.
- The retraining phase effectiveness for recovering accuracy after cluster pruning needs more systematic study, particularly regarding optimal epoch count and learning rate schedules.

## Confidence
- High confidence: CKA similarity effectively identifies redundant layers and enables global pruning strategies that outperform local methods
- Medium confidence: Multi-layer cluster pruning achieves stated compression ratios (up to 50%) with minimal accuracy loss on tested architectures
- Low confidence: Mathematical safety guarantees for cluster removal and combined approach with magnitude pruning methods require additional validation

## Next Checks
1. **Architecture generalization test**: Apply MPruner to additional model families (RNNs, Vision Transformers) and benchmark compression-accuracy trade-offs against established pruning methods.
2. **Mathematical proof validation**: Conduct controlled experiments to verify the claimed accuracy preservation guarantees when pruning clusters of varying sizes and similarity scores.
3. **Combined pruning analysis**: Systematically measure the incremental benefits of integrating MPruner with Wanda or other magnitude pruning techniques across different model sizes and tasks.