---
ver: rpa2
title: Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications
  to DNA and Protein Design
arxiv_id: '2410.13643'
source_url: https://arxiv.org/abs/2410.13643
tags:
- diffusion
- sequences
- protein
- arxiv
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of fine-tuning discrete diffusion
  models to optimize downstream reward functions while preserving naturalness of generated
  sequences. The authors propose DRAKES, a direct reward backpropagation method that
  uses the Gumbel-Softmax trick to make otherwise non-differentiable diffusion trajectories
  differentiable.
---

# Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design

## Quick Facts
- arXiv ID: 2410.13643
- Source URL: https://arxiv.org/abs/2410.13643
- Reference count: 40
- Key outcome: DRAKES achieves high DNA enhancer activity (5.61 median predicted activity) and protein stability (1.095 median ddG, 78.6% success rate) while maintaining naturalness

## Executive Summary
This work addresses the problem of fine-tuning discrete diffusion models to optimize downstream reward functions while preserving naturalness of generated sequences. The authors propose DRAKES, a direct reward backpropagation method that uses the Gumbel-Softmax trick to make otherwise non-differentiable diffusion trajectories differentiable. The approach formulates reward maximization as a reinforcement learning problem with a KL divergence regularization term against the pretrained model. Theoretical analysis shows that the optimal distribution is proportional to exp(reward/α) times the pretrained distribution. Empirical results demonstrate DRAKES' effectiveness in generating DNA sequences with high enhancer activity and protein sequences with high stability while maintaining naturalness measured by sequence similarity metrics.

## Method Summary
DRAKES fine-tunes discrete diffusion models by directly backpropagating rewards through the sampling process using the Gumbel-Softmax trick. The method treats the fine-tuning as an RL problem where the objective is to maximize expected reward plus a KL divergence term that regularizes the model to stay close to the pretrained distribution. The Gumbel-Softmax trick makes the categorical sampling in the diffusion process differentiable, enabling gradient-based optimization. The theoretical analysis establishes that the optimal fine-tuned distribution is proportional to exp(reward/α) times the pretrained distribution, providing a principled foundation for the approach.

## Key Results
- DNA sequence generation: 5.61 median predicted enhancer activity compared to 3.65 for guidance-based methods
- Protein sequence generation: 1.095 median ddG stability score with 78.6% success rate in generating stable proteins
- Naturalness preservation: DRAKES maintains higher sequence similarity to natural sequences compared to over-optimized alternatives
- Ablation studies: KL regularization is crucial for preventing over-optimization while still enabling effective reward maximization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DRAKES enables direct backpropagation of rewards through discrete diffusion trajectories by making them differentiable using the Gumbel-Softmax trick
- Mechanism: The Gumbel-Softmax trick replaces the non-differentiable categorical sampling with a differentiable softmax operation parameterized by temperature τ. As τ approaches zero, the softmax approximation converges to the true categorical distribution. This allows gradients to flow through the entire trajectory during optimization.
- Core assumption: The generator Qθ can be parameterized as a map from R^N to R, allowing soft representations (like the Gumbel-Softmax output) to be valid inputs for the transition rate calculations.
- Evidence anchors:
  - [abstract]: "DRAKES, that enables direct backpropagation of rewards through entire trajectories generated by diffusion models, by making the originally non-differentiable trajectories differentiable using the Gumbel-Softmax trick"
  - [section 4.2]: "we sample xt+∆t ∼ Cat(πt), where Cat(·) denotes the categorical distribution. However, this procedure is not differentiable with respect to θ, which limits its applicability for optimization. To address this, we first recognize that sampling from the categorical distribution can be reduced to a Gumbel-max operation...we can modify it by replacing the max operation with a softmax"
  - [corpus]: Weak - corpus mentions similar approaches but lacks specific validation of Gumbel-Softmax for discrete diffusion trajectories
- Break condition: If the temperature τ cannot be reduced sufficiently without numerical instability, or if the generator Qθ fundamentally requires hard discrete inputs rather than soft distributions

### Mechanism 2
- Claim: The KL divergence regularization prevents over-optimization and maintains naturalness of generated sequences
- Mechanism: The KL term in the objective function constrains the fine-tuned model to stay close to the pretrained distribution, preventing the model from exploiting flaws in the reward function to generate unrealistic but high-reward sequences. This regularization ensures that generated sequences remain likely under the original conditional distribution.
- Core assumption: The pretrained diffusion model captures a valid distribution over natural sequences, and staying close to this distribution preserves naturalness.
- Evidence anchors:
  - [abstract]: "minimizing the KL divergence against pretrained diffusion models to preserve naturalness"
  - [section 4.1]: "using only this objective could lead to over-optimization, where the model produces unrealistic or unnatural samples that technically achieve a high reward, but are impossible to generate in reality. We address this challenge by constraining the optimized model to remain close to a pretrained diffusion model"
  - [section 6.2]: "Notably, while DRAKES, without KL regularization achieves higher Pred-Activity, this can be attributed to over-optimization"
  - [corpus]: Moderate - corpus mentions similar regularization approaches but lacks direct comparison of KL vs no-KL performance
- Break condition: If the pretrained model itself is biased or unrepresentative of natural sequences, or if the reward function is perfectly aligned with naturalness criteria

### Mechanism 3
- Claim: The optimal distribution after fine-tuning is proportional to exp(reward/α) times the pretrained distribution
- Mechanism: Theoretical analysis shows that the fine-tuned model's marginal distribution at time T is proportional to exp(r(xT)/α)ppre(xT), where r is the reward function and α controls the trade-off between reward optimization and KL regularization. This creates a principled way to balance optimization and naturalness.
- Core assumption: The space of generators {Qθ} is fully nonparametric (realizability holds), allowing the optimal distribution to be achieved exactly.
- Evidence anchors:
  - [abstract]: "Theoretical analysis shows that the optimal distribution is proportional to exp(reward/α) times the pretrained distribution"
  - [section 4.1]: "Theorem 1 (Fine-Tuned Distribution). When {Qθ·,· : θ ∈ Θ} is fully nonparametric (i.e., realizability holds), the generated distribution at time T by (6) is proportional to exp(r(·)/α)ppre(·)"
  - [section 5.1]: "This theorem offers valuable insights. The first term, exp(r(x)), represents high rewards. Additionally, the second term, ppre(·), can be seen as prior information that characterizes the natural sequence"
  - [corpus]: Strong - corpus contains similar theoretical results for continuous diffusion models, providing precedent for this type of analysis
- Break condition: If realizability doesn't hold (the model family can't represent the optimal distribution), or if function approximation errors dominate the optimization process

## Foundational Learning

- Concept: Continuous-time Markov chains (CTMC) and their generators
  - Why needed here: Discrete diffusion models are formulated as CTMCs, where the generator Q(t) defines the transition rates between states over time. Understanding CTMCs is essential for grasping how the diffusion process works and how the generator can be modified during fine-tuning.
  - Quick check question: In a CTMC, what does the generator matrix Q represent, and how does it relate to the transition probabilities over a small time interval dt?

- Concept: Gumbel-Softmax trick for differentiable categorical sampling
  - Why needed here: The Gumbel-Softmax trick is the key mechanism that makes discrete sampling differentiable, enabling gradient-based optimization of reward functions through non-differentiable sampling operations.
  - Quick check question: How does the Gumbel-Softmax distribution approximate the categorical distribution, and what role does the temperature parameter τ play in this approximation?

- Concept: Reinforcement learning with KL regularization
  - Why needed here: The fine-tuning problem is formulated as an RL problem with a reward term and a KL regularization term, similar to maximum entropy RL. Understanding this formulation helps in grasping the theoretical guarantees and the balance between optimization and naturalness.
  - Quick check question: In maximum entropy RL, what is the role of the entropy term, and how does it relate to the KL regularization term used in DRAKES?

## Architecture Onboarding

- Component map:
  Pretrained discrete diffusion model (Qθpre) -> Reward model (r) -> DRAKES optimizer -> Generator Qθ -> Trajectory sampler -> KL divergence calculator

- Critical path:
  1. Sample trajectories from pretrained model
  2. Apply Gumbel-Softmax to make sampling differentiable
  3. Calculate reward and KL divergence
  4. Backpropagate gradients through the trajectory
  5. Update generator parameters θ
  6. Generate final sequences from fine-tuned model

- Design tradeoffs:
  - Temperature τ vs. approximation quality: Lower temperature gives better approximation but may cause numerical instability
  - α (KL strength) vs. optimization: Higher α preserves naturalness but may limit reward optimization
  - Gradient truncation depth vs. computational cost: Deeper truncation gives better gradients but increases memory usage
  - Soft vs. hard sampling: Soft sampling enables differentiability but may produce invalid sequences

- Failure signatures:
  - Over-optimization: High reward but low likelihood under pretrained model, poor naturalness metrics
  - Under-optimization: Reward similar to pretrained model, no improvement over baseline
  - Training instability: NaN or exploding gradients, especially with low temperature or deep truncation
  - Mode collapse: Generated sequences become too similar to each other, diversity metrics drop

- First 3 experiments:
  1. Ablation study: Compare DRAKES with and without KL regularization on DNA enhancer activity task to demonstrate over-optimization prevention
  2. Temperature sensitivity: Vary Gumbel-Softmax temperature τ and measure impact on reward optimization and naturalness preservation
  3. α sensitivity: Test different values of the KL regularization strength α to find the optimal balance between reward and naturalness

## Open Questions the Paper Calls Out
None

## Limitations
- The Gumbel-Softmax approximation introduces bias that isn't fully characterized, and the method's scalability to larger sequence lengths or more complex reward functions remains untested.
- The theoretical analysis assumes realizability holds, which may not be practical for complex sequence generation tasks.
- The computational overhead of backpropagating through entire trajectories could be prohibitive for some applications.

## Confidence
- **High confidence**: The core mechanism of using Gumbel-Softmax for differentiable discrete sampling in diffusion models is well-established in the literature. The empirical results showing DRAKES outperforms guidance-based approaches on DNA and protein design tasks are robust across multiple experiments.
- **Medium confidence**: The theoretical analysis connecting the optimal fine-tuned distribution to exp(reward/α) times the pretrained distribution relies on strong assumptions about function approximation that may not hold in practice. The empirical validation of naturalness preservation is convincing but could benefit from more diverse metrics.
- **Low confidence**: The computational efficiency claims relative to guidance-based methods are based on theoretical arguments rather than systematic benchmarking. The method's performance on tasks beyond DNA/protein design hasn't been demonstrated.

## Next Checks
1. **Ablation study on temperature schedule**: Systematically vary the Gumbel-Softmax temperature τ throughout training to determine optimal annealing schedules and characterize the approximation error as a function of temperature.
2. **Scalability benchmark**: Test DRAKES on progressively longer sequences (e.g., 1000+ nucleotides or proteins) to measure computational overhead and identify breaking points for the trajectory backpropagation approach.
3. **Cross-domain Generalization**: Apply DRAKES to a fundamentally different sequence generation task (e.g., small molecule design or text generation) to evaluate whether the method's effectiveness extends beyond biological sequences.