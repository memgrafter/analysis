---
ver: rpa2
title: 'Bypass Back-propagation: Optimization-based Structural Pruning for Large Language
  Models via Policy Gradient'
arxiv_id: '2406.10576'
source_url: https://arxiv.org/abs/2406.10576
tags:
- pruning
- arxiv
- gradient
- methods
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an optimization-based structural pruning method
  for large language models that eliminates back-propagation through the LLM during
  optimization. The method learns binary pruning masks via an underlying Bernoulli
  distribution and uses policy gradient estimation with only forward passes, achieving
  global and heterogeneous pruning automatically.
---

# Bypass Back-propagation: Optimization-based Structural Pruning for Large Language Models via Policy Gradient

## Quick Facts
- arXiv ID: 2406.10576
- Source URL: https://arxiv.org/abs/2406.10576
- Reference count: 40
- Key outcome: Optimization-based structural pruning method that eliminates back-propagation through LLMs during optimization, achieving state-of-the-art perplexity and zero-shot performance while running 2.7 hours on a single A100 GPU for pruning LLaMA-2-13B.

## Executive Summary
This paper presents a novel optimization-based structural pruning method for large language models that eliminates back-propagation through the LLM during optimization. The method learns binary pruning masks via an underlying Bernoulli distribution and uses policy gradient estimation with only forward passes. By decoupling the Bernoulli parameters from the LLM loss, the approach achieves global and heterogeneous pruning automatically while significantly reducing computational requirements compared to back-propagation-based methods.

## Method Summary
The method optimizes Bernoulli parameters using policy gradient estimators that require only forward passes of the LLM, eliminating back-propagation through the model during training. Pruning masks are sampled from Bernoulli distributions and used to prune the LLM during forward passes. The Bernoulli parameters are updated using policy gradient with a moving average baseline for variance reduction. The approach supports global and heterogeneous pruning across different layers and can be optionally initialized with metric-based pruning methods. After training, deterministic masks are generated from the Bernoulli parameters for evaluation.

## Key Results
- Achieves state-of-the-art perplexity on WikiText2 at 30%, 40%, and 50% pruning rates for multiple LLM architectures
- Outperforms metric-based methods and achieves comparable results to back-propagation-based methods
- Demonstrates 2.7 hours training time using approximately 35GB memory on a single A100 GPU for pruning LLaMA-2-13B
- Maintains zero-shot task performance across PIQA, HellaSwag, WinoGrande, ARC-e, and ARC-c benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The policy gradient estimator enables efficient optimization without back-propagation through the LLM.
- **Mechanism:** By decoupling the Bernoulli parameters from the LLM loss, the method uses forward passes only to update pruning masks, avoiding the computational burden of back-propagation.
- **Core assumption:** The gradients of the Bernoulli distribution parameters can be estimated without back-propagating through the LLM, and this estimation is sufficiently accurate for effective pruning.
- **Evidence anchors:**
  - [abstract]: "Our method eliminates the back-propagation through the LLM per se during optimization, requiring only the forward pass of the LLM."
  - [section]: "We achieve this by learning an underlying Bernoulli distribution to sample binary pruning masks, where we decouple the Bernoulli parameters from LLM loss, facilitating efficient optimization via policy gradient estimator without back-propagation."
- **Break condition:** If the variance of the policy gradient estimator becomes too high, the pruning masks may not converge to an effective configuration, leading to poor model performance.

### Mechanism 2
- **Claim:** Global and heterogeneous pruning is achieved by modeling pruning masks as Bernoulli distributions.
- **Mechanism:** The probabilistic formulation allows each layer or module to have its own pruning probability, automatically determining different redundancy levels across the LLM.
- **Core assumption:** The Bernoulli distribution parameters can effectively represent the importance of each module, and the optimization process can learn these parameters to achieve optimal global sparsity.
- **Evidence anchors:**
  - [abstract]: "Our method can 1) support global and heterogeneous pruning (i.e., automatically determine different redundancy for different layers)."
  - [section]: "The probabilistic modeling of Bernoulli distribution facilitates global and heterogeneous pruning across the LLM."
- **Break condition:** If the optimization process fails to properly learn the Bernoulli parameters, the pruning may become too uniform or miss critical modules, degrading model performance.

### Mechanism 3
- **Claim:** Initialization with metric-based pruning methods improves convergence and performance.
- **Mechanism:** Starting with a reasonable prior pruning mask from a metric-based method provides a better initial guess for the Bernoulli parameters, leading to faster convergence and better final results.
- **Core assumption:** Metric-based pruning methods provide a reasonable estimate of module importance that can be refined by the optimization process.
- **Evidence anchors:**
  - [abstract]: "optionally initialize with a metric-based method (for our Bernoulli distributions)."
  - [section]: "Moreover, our method unifies the pruning of the entire LLM into a probabilistic space (optionally initialized by an arbitrary metric-based approach)."
- **Break condition:** If the metric-based initialization is poor or misleading, it may cause the optimization to converge to a suboptimal solution.

## Foundational Learning

- **Concept: Bernoulli Distribution and Policy Gradient**
  - **Why needed here:** The Bernoulli distribution models the binary nature of pruning decisions, and policy gradient provides a way to optimize these probabilities without back-propagation.
  - **Quick check question:** Why is the Bernoulli distribution appropriate for modeling pruning decisions, and how does policy gradient estimate the gradients of its parameters?

- **Concept: Sparse Optimization and Pruning**
  - **Why needed here:** Pruning reduces model size by removing redundant components, and understanding sparse optimization helps in designing effective pruning strategies.
  - **Quick check question:** How does structured pruning differ from unstructured pruning, and what are the benefits of each approach?

- **Concept: Reinforcement Learning and Policy Gradient**
  - **Why needed here:** The pruning problem can be formulated as a reinforcement learning task, where the policy gradient method learns to select pruning actions based on rewards (model performance).
  - **Quick check question:** How does the pruning problem map to a Markov Decision Process, and what is the role of the reward signal in this context?

## Architecture Onboarding

- **Component map:**
  Bernoulli parameters (s) -> Binary masks (m) -> LLM forward pass -> Loss computation -> Policy gradient estimation -> s update with projection

- **Critical path:**
  1. Initialize Bernoulli parameters (s) from a metric-based method or randomly.
  2. Sample binary masks (m) from Bernoulli distributions.
  3. Forward pass the LLM with masks to compute loss.
  4. Estimate gradients of s using policy gradient.
  5. Update s using gradient descent with projection.
  6. Repeat until convergence.

- **Design tradeoffs:**
  - Global vs. local pruning: Global pruning allows different sparsity levels across layers but requires more complex optimization.
  - Variance reduction: Moving average baseline reduces variance but adds computational overhead.
  - Initialization strategy: Metric-based initialization can improve convergence but may bias the results.

- **Failure signatures:**
  - High variance in policy gradient updates leading to unstable training.
  - Poor convergence to suboptimal pruning masks.
  - Memory issues if the projection operator is not implemented efficiently.

- **First 3 experiments:**
  1. Compare policy gradient with and without moving average baseline on a small LLM to measure variance reduction.
  2. Test different initialization strategies (random, metric-based) on a medium-sized LLM to evaluate convergence speed and final performance.
  3. Validate global vs. local pruning on a large LLM to demonstrate the benefits of heterogeneous sparsity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of policy gradient optimization scale with increasing model size beyond 13B parameters?
- Basis in paper: [inferred] The paper demonstrates results on models up to LLaMA-2-13B and LLaMA-3-8B, showing 2.7 hours training time on A100 GPU with ~35GB memory. The authors note this is a practical limitation for individual practitioners.
- Why unresolved: The paper does not test on larger models like LLaMA-405B or GPT-3 sized architectures, which would better demonstrate scalability limitations and computational requirements.
- What evidence would resolve it: Experiments on models with 30B+ parameters showing training time, memory usage, and perplexity scores compared to metric-based methods at various pruning rates.

### Open Question 2
- Question: What is the optimal window size and mask sampling times for the moving average baseline across different pruning rates?
- Basis in paper: [explicit] The authors tested T=5 and Ns=2 but note that "small T and Ns can already offer promising performance" with only "marginal improvement" from larger values.
- Why unresolved: The paper only explores a limited range (T=3,5,7 and Ns=2,3,4) and doesn't provide systematic analysis of how these hyperparameters affect variance reduction and convergence speed across different pruning scenarios.
- What evidence would resolve it: A comprehensive ablation study showing perplexity and training stability across various T and Ns combinations at multiple pruning rates (10%-50%) with statistical significance testing.

### Open Question 3
- Question: How does the proposed method perform on domain-specific datasets compared to cross-dataset evaluation?
- Basis in paper: [explicit] The authors note that "pruning with only C4 dataset might have a negative influence on certain cross-dataset zero-shot tasks" and that "performance on specific domains/tasks can rely heavily on the availability of domain-specific datasets."
- Why unresolved: All experiments use the challenging cross-dataset setup (C4 for training, WikiText2 for evaluation), but the paper doesn't explore in-domain pruning where calibration and evaluation use the same domain.
- What evidence would resolve it: Experiments comparing cross-dataset vs. in-domain performance on specialized datasets (medical, legal, scientific) showing perplexity and task-specific metrics for both scenarios.

## Limitations

- The variance of the policy gradient estimator is not thoroughly analyzed, and the convergence properties are not formally established.
- The method relies on a moving average baseline for variance reduction, but its effectiveness is not rigorously proven.
- Initialization with metric-based methods may introduce bias, and the projection operator's implementation details are not fully specified.

## Confidence

- **High confidence**: The core claim that policy gradient can optimize pruning masks without back-propagation is supported by the experimental results showing state-of-the-art perplexity and zero-shot performance.
- **Medium confidence**: The claim that global and heterogeneous pruning is achieved automatically through Bernoulli distributions is supported by the experimental results, but the underlying mechanism is not fully explained.
- **Low confidence**: The claim that the moving average baseline effectively reduces variance in policy gradient updates is based on empirical observations, but no formal analysis is provided.

## Next Checks

1. **Variance Analysis**: Perform a systematic analysis of the variance in policy gradient estimates with and without the moving average baseline across different model sizes and pruning rates to quantify the effectiveness of variance reduction.

2. **Convergence Study**: Conduct a convergence analysis by varying the number of mask sampling times and learning rates to determine the optimal configuration for stable training and effective pruning.

3. **Initialization Sensitivity**: Test the method with different initialization strategies (random, metric-based, and no initialization) to evaluate the impact on convergence speed and final pruning performance, particularly for very large models.