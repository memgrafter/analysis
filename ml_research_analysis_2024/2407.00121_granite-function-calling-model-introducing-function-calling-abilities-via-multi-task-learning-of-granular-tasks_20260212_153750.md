---
ver: rpa2
title: 'Granite-Function Calling Model: Introducing Function Calling Abilities via
  Multi-task Learning of Granular Tasks'
arxiv_id: '2407.00121'
source_url: https://arxiv.org/abs/2407.00121
tags:
- function
- calling
- granite
- tasks
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces GRANITE-20B-FUNCTIONCALLING, an open-source
  language model trained to perform function calling tasks using a multi-task learning
  approach. The model is fine-tuned on seven fundamental tasks: nested function calling,
  function chaining, parallel functions, function name detection, parameter-value
  pair detection, next-best function selection, and response generation.'
---

# Granite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning of Granular Tasks

## Quick Facts
- arXiv ID: 2407.00121
- Source URL: https://arxiv.org/abs/2407.00121
- Authors: Ibrahim Abdelaziz; Kinjal Basu; Mayank Agarwal; Sadhana Kumaravel; Matthew Stallone; Rameswar Panda; Yara Rizk; GP Bhargav; Maxwell Crouse; Chulaka Gunasekara; Shajith Ikbal; Sachin Joshi; Hima Karanam; Vineet Kumar; Asim Munawar; Sumit Neelam; Dinesh Raghu; Udit Sharma; Adriana Meza Soria; Dheeraj Sreedhar; Praveen Venkateswaran; Merve Unuvar; David Cox; Salim Roukos; Luis Lastras; Pavan Kapanipathi
- Reference count: 9
- Primary result: GRANITE-20B-FUNCTIONCALLING achieves 4th overall and best performance among open models on Berkeley Function Calling Leaderboard

## Executive Summary
This paper introduces GRANITE-20B-FUNCTIONCALLING, an open-source language model fine-tuned to perform function calling tasks using a multi-task learning approach. The model is trained on seven fundamental tasks including nested function calling, function chaining, and parameter-value pair detection, using a unified JSON format across diverse datasets. GRANITE-20B-FUNCTIONCALLING demonstrates strong performance on the Berkeley Function Calling Leaderboard and shows robust generalization across multiple out-of-domain evaluation datasets.

## Method Summary
GRANITE-20B-FUNCTIONCALLING is created by fine-tuning the pre-trained GRANITE-20B-CODE-INSTRUCT model using a multi-task learning approach on seven granular function calling tasks. The training data from various domains (semantic parsing, task-oriented dialog, personal assistants) is unified into a JSON format and instruction-tuned for function calling. The model is trained using QLoRA with 8-bit quantization on 8 A100_80GB GPUs for 3 epochs with a learning rate of 5e-5. The output is structured in JSON format with function names and arguments, achieving strong performance on both the Berkeley Function Calling Leaderboard and out-of-domain datasets.

## Key Results
- Achieves 4th overall and best performance among open models on Berkeley Function Calling Leaderboard
- Demonstrates strong generalization across multiple out-of-domain datasets including ToolLLM, ToolBench, and API-Bank
- Shows high accuracy in function name detection and low hallucination rates (<0.1)
- Outperforms other open models like Llama-3-70B-Instruct and Qwen2.5-72B-Instruct on BFCL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task training with granular sub-tasks enables better generalization than monolithic function calling
- Mechanism: The model learns to decompose complex function calling into simpler, composable skills. Each sub-task focuses on a specific aspect of the function calling pipeline, allowing the model to develop specialized representations for each.
- Core assumption: Granular tasks capture essential components of function calling that generalize across domains better than end-to-end training
- Evidence anchors: The model is trained using a multi-task training approach on seven fundamental tasks encompassed in function calling; Compared to High-Level tasks, these tasks are simpler because they either request function names (without parameters) or parameter-value pairs for a function
- Break condition: If the granular tasks are too domain-specific, the model may fail to generalize across different API structures and naming conventions

### Mechanism 2
- Claim: JSON format unification across diverse datasets creates consistent representations that improve learning efficiency
- Mechanism: By converting all APIs, tools, and functions from different sources into a unified JSON format, the model receives consistent input-output patterns regardless of the original dataset's format
- Core assumption: Consistent representation format is more important than preserving original dataset-specific features for function calling performance
- Evidence anchors: JSON is a language-independent, human-readable, and widely used data format for code-related tasks; In GRANITE -20B-F UNCTION CALLING , we unify the model output representation of function calls to the following format
- Break condition: If JSON format loses critical information present in original formats, the model may underperform on certain API types

### Mechanism 3
- Claim: Training on diverse domains improves out-of-domain generalization
- Mechanism: Exposure to varied contexts and language patterns during training creates a more robust model that can handle unexpected function calling scenarios
- Core assumption: Function calling skills learned in one domain transfer to other domains when the underlying patterns are similar enough
- Evidence anchors: Training data from diverse domains like semantic parsing, task-oriented dialog, personal assistants, and conversational data; On multiple out-of-domain datasets, GRANITE -20B-F UNCTION CALLING provides the best performance among the models that have open licenses
- Break condition: If domain diversity introduces conflicting patterns or noise, the model may struggle to learn coherent function calling behavior

## Foundational Learning

- Concept: Multi-task learning and task decomposition
  - Why needed here: Function calling involves multiple sub-skills (detecting function names, filling parameters, sequencing calls) that benefit from specialized training
  - Quick check question: Can you list the seven granular tasks used in the training approach and explain why each is important for function calling?

- Concept: JSON data representation and parsing
  - Why needed here: The model uses JSON format for both input function libraries and output function calls, requiring understanding of this structured data format
  - Quick check question: What are the key advantages of using JSON format for function calling tasks, and what information might be lost in this representation?

- Concept: Zero-shot generalization evaluation
  - Why needed here: The model is evaluated on out-of-domain datasets it wasn't trained on, requiring understanding of how to measure generalization performance
  - Quick check question: What metrics are used to evaluate function calling performance on out-of-domain datasets, and why can't the same metrics be used across all evaluation sets?

## Architecture Onboarding

- Component map: Base model (GRANITE-20B-CODE-INSTRUCT) -> Multi-task fine-tuning pipeline -> JSON format unification -> Evaluation on BFCL + out-of-domain datasets -> JSON-structured function call outputs
- Critical path: Model receives natural language query → processes function library → generates sequence of function calls in JSON format → evaluates using F1 scores and AST matching
- Design tradeoffs:
  - Smaller model (20B) vs. performance: Achieves 4th overall on BFCL despite smaller size
  - JSON standardization vs. domain-specific features: May lose some information but gains consistency
  - Multi-task training vs. specialized training: Better generalization but potentially slower convergence
- Failure signatures:
  - High hallucination rate (>0.1) indicates poor function name detection
  - Low F1 scores on parameter-value pairs suggest slot-filling issues
  - Poor AST matching on BFCL indicates syntax generation problems
- First 3 experiments:
  1. Test function name detection on ToolLLM G1 dataset - should achieve >0.8 F1
  2. Evaluate parameter-value pair detection on API-Bank dataset - should achieve >0.6 F1
  3. Measure hallucination rate on out-of-domain datasets - should be <0.1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GRANITE-20B-FUNCTIONCALLING's performance scale with increasing context length beyond 8192 tokens?
- Basis in paper: The paper mentions GRANITE-20B-CODE-INSTRUCT supports up to 8192 context length and discusses potential improvements by exploiting Rotary Position Embedding for longer contexts
- Why unresolved: The paper explicitly states that due to context length limitations, they had to truncate function specifications, suggesting potential performance gains with longer contexts
- What evidence would resolve it: Experiments evaluating GRANITE-20B-FUNCTIONCALLING's performance on tasks requiring longer context lengths with and without Rotary Position Embedding

### Open Question 2
- Question: How does GRANITE-20B-FUNCTIONCALLING's performance compare to proprietary models when fine-tuned on the same datasets?
- Basis in paper: The paper compares GRANITE-20B-FUNCTIONCALLING to proprietary models like GPT-4, Claude-3.5-Sonnet, and Gemini-1.5-Pro but notes that proprietary models like Gorilla were fine-tuned on data similar to test sets
- Why unresolved: The paper does not provide a direct comparison where proprietary models are fine-tuned on the same datasets as GRANITE-20B-FUNCTIONCALLING
- What evidence would resolve it: A controlled experiment where proprietary models are fine-tuned on the same datasets used to train GRANITE-20B-FUNCTIONCALLING and then evaluated on the same out-of-domain datasets

### Open Question 3
- Question: What is the impact of using a multi-task learning approach versus fine-tuning on individual function calling tasks separately?
- Basis in paper: The paper describes using a multi-task learning approach for training GRANITE-20B-FUNCTIONCALLING, combining seven fundamental function calling tasks
- Why unresolved: The paper does not provide a direct comparison between the multi-task learning approach and separate fine-tuning on individual tasks
- What evidence would resolve it: Experiments comparing the performance of GRANITE-20B-FUNCTIONCALLING to versions of the model fine-tuned separately on each of the seven fundamental function calling tasks

## Limitations
- The specific weighting configuration for the training data mixture is only partially specified
- The exact prompt templates and instructions for each of the seven granular tasks are referenced but not fully disclosed
- The paper lacks ablation studies to quantify the individual contribution of each mechanism
- No comparative analysis of training time and convergence speed versus alternative approaches

## Confidence

**High Confidence:** The model's strong performance on the Berkeley Function Calling Leaderboard (4th overall, best among open models) is well-supported by the evaluation methodology and results presented.

**Medium Confidence:** The generalization claims across out-of-domain datasets are supported by evaluation results, but the paper lacks detailed error analysis to understand when and why the model fails.

**Low Confidence:** The paper makes claims about the importance of domain diversity without providing evidence of how each domain contributes to the final performance.

## Next Checks

1. **Ablation Study on Task Composition**: Remove one of the seven granular tasks from the training mixture and evaluate the impact on BFCL performance to quantify the contribution of each task type.

2. **JSON Format Impact Analysis**: Train two versions of the model - one with unified JSON format and one with original dataset-specific formats, then compare their performance on domain-specific function calling tasks.

3. **Domain Contribution Analysis**: Create a series of models trained on subsets of the domain diversity and evaluate their generalization to out-of-domain datasets to reveal which domains are most critical for robust function calling.