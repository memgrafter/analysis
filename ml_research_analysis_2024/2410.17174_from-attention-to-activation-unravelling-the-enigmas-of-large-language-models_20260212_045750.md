---
ver: rpa2
title: 'From Attention to Activation: Unravelling the Enigmas of Large Language Models'
arxiv_id: '2410.17174'
source_url: https://arxiv.org/abs/2410.17174
tags:
- attention
- first
- hidden
- position
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work identifies two anomalies in auto-regressive Transformers:
  (1) first token dominance in attention maps; (2) outlier activations in hidden states.
  To address these, it proposes: (1) softmax-1, a reformulation of softmax that removes
  first token dominance; (2) OrthoAdam, a novel optimizer using orthogonal matrices
  to transform gradients, eliminating outlier activations.'
---

# From Attention to Unravelling the Enigmas of Large Language Models

## Quick Facts
- arXiv ID: 2410.17174
- Source URL: https://arxiv.org/abs/2410.17174
- Reference count: 40
- Key outcome: Reduces first token attention from 65% to 3.3% and activation kurtosis from 1657 to 3.1 while maintaining model performance

## Executive Summary
This work identifies two critical anomalies in auto-regressive Transformers: first token dominance in attention maps and outlier activations in hidden states. The authors propose two solutions - softmax-1, a modified softmax function that eliminates first token bias, and OrthoAdam, a novel optimizer using orthogonal matrix transformations to address activation outliers. The proposed methods significantly improve model compression and training stability while preserving model performance on LLaMA-7B.

## Method Summary
The authors introduce softmax-1, which reformulates the softmax function to eliminate the pathological first token dominance observed in attention maps, reducing first token attention from 65% to 3.3%. They also develop OrthoAdam, an optimizer that transforms gradients using orthogonal matrices to eliminate outlier activations, reducing activation kurtosis from 1657 to 3.1. Both methods are evaluated on LLaMA-7B, demonstrating improved compression performance (perplexity penalty under 4-bit quantization reduced from 3565 to 0.3) while maintaining model accuracy.

## Key Results
- First token attention reduced from 65% to 3.3% using softmax-1
- Activation kurtosis decreased from 1657 to 3.1 using OrthoAdam
- Perplexity penalty under 4-bit weight quantization reduced from 3565 to 0.3
- Model performance maintained while achieving significant compression benefits

## Why This Works (Mechanism)
The proposed solutions address fundamental architectural biases in Transformers. Softmax-1 eliminates the pathological first token dominance by modifying how attention scores are normalized, preventing the concentration of attention probability mass on the first token. OrthoAdam addresses the outlier activation problem by using orthogonal matrix transformations to regularize gradients during optimization, preventing extreme values in hidden states. These modifications stabilize training and improve model robustness without requiring architectural changes to the underlying Transformer structure.

## Foundational Learning

**Attention Mechanisms**
- Why needed: Core component for capturing relationships between tokens
- Quick check: Verify attention weights sum to 1 across sequence dimension

**Gradient Optimization**
- Why needed: Essential for training deep neural networks effectively
- Quick check: Monitor gradient norms and update magnitudes during training

**Orthogonal Matrices**
- Why needed: Preserve vector norms during transformations, useful for regularization
- Quick check: Verify matrix orthogonality (Q^T Q = I) after transformations

## Architecture Onboarding

**Component Map**
Input -> Embedding -> Self-Attention -> Feed-Forward -> Output
                          |
                          -> Softmax-1 (modified)
                          |
                          -> OrthoAdam (optimizer)

**Critical Path**
The self-attention mechanism with softmax-1 transformation is the critical path, as it directly addresses the first token dominance issue that affects model behavior.

**Design Tradeoffs**
- Computational overhead of orthogonal matrix operations in OrthoAdam
- Modified softmax may affect attention interpretability
- New optimizer requires careful hyperparameter tuning

**Failure Signatures**
- First token attention still dominates (>20%)
- Activation outliers persist (kurtosis > 10)
- Training instability or divergence

**First Experiments**
1. Measure first token attention distribution before/after softmax-1
2. Compute activation kurtosis across hidden states
3. Compare perplexity on validation set before/after modifications

## Open Questions the Paper Calls Out
None

## Limitations
- Long-term stability of modified training remains uncertain
- Generalizability across different model architectures needs validation
- Interpretability of OrthoAdam's orthogonal transformations requires deeper investigation

## Confidence
- Identification of attention and activation anomalies: **High**
- Effectiveness of softmax-1 in reducing first token attention: **High**
- Efficacy of OrthoAdam in reducing outlier activations: **Medium**
- Generalizability across model architectures: **Low**
- Long-term stability of modified training: **Low**

## Next Checks
1. Evaluate softmax-1 and OrthoAdam on diverse model architectures (T5, GPT variants, smaller LLaMA models) to assess generalizability
2. Conduct ablation studies isolating the effects of softmax-1 and OrthoAdam components on model performance and training stability
3. Test the compression benefits with alternative quantization schemes (8-bit, 16-bit) and on different hardware platforms to verify practical deployment advantages