---
ver: rpa2
title: Do not think about pink elephant!
arxiv_id: '2404.15154'
source_url: https://arxiv.org/abs/2404.15154
tags:
- prompt
- white
- bear
- phenomenon
- absence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and exploits a vulnerability in large models
  known as the "white bear phenomenon," where models fail to understand negation and
  inadvertently generate prohibited content when prompted with negative instructions.
  The authors analyze this issue through representation space analysis, showing that
  attention-based architectures and linear representation spaces struggle with absence
  concepts.
---

# Do not think about pink elephant!

## Quick Facts
- arXiv ID: 2404.15154
- Source URL: https://arxiv.org/abs/2404.15154
- Authors: Kyomin Hwang; Suyoung Kim; JunHoo Lee; Nojun Kwak
- Reference count: 21
- Models fail to understand negation, inadvertently generating prohibited content when prompted with negative instructions

## Executive Summary
This paper identifies and exploits a vulnerability in large models known as the "white bear phenomenon," where models fail to understand negation and inadvertently generate prohibited content when prompted with negative instructions. The authors analyze this issue through representation space analysis, showing that attention-based architectures and linear representation spaces struggle with absence concepts. They propose a prompt-based attack method that successfully generates policy-violating images, achieving a 75.54% success rate in Stable Diffusion. To counter these attacks, they introduce two defense strategies inspired by cognitive therapy: adding definitions of abstract concepts (34.93% success rate) and substituting alternative concepts (48.22% success rate), significantly improving model robustness without requiring additional training.

## Method Summary
The authors analyze the "white bear phenomenon" through representation space analysis of attention-based architectures. They propose a prompt-based attack method to generate prohibited images by exploiting the model's inability to process negation. The defense strategies involve cognitive therapy-inspired prompt engineering: adding definitions of abstract concepts and substituting alternative concepts. Experiments are conducted on Stable Diffusion v2.1 and DALL-E3, using datasets of abstract and concrete words generated with ChatGPT and the Oxford Dictionary.

## Key Results
- Successfully exploits "white bear phenomenon" in Stable Diffusion with 75.54% success rate
- Prompt-based defense strategies reduce attack success to 34.93% (adding definitions) and 48.22% (substituting concepts)
- CLIP embeddings show "elephant" and "not elephant" have high cosine similarity, demonstrating proximity of presence and absence representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention-based architectures fail to process negation because they aggregate token information via weighted averages without direct subtraction or projection capabilities.
- Mechanism: The model's attention mechanism computes token representations as weighted sums, which cannot inherently represent absence. When encountering "not elephant," the model still encodes elephant information because subtraction isn't a fundamental operation in attention computation.
- Core assumption: The attention mechanism's token aggregation process lacks the mathematical capability to perform negation operations.
- Evidence anchors:
  - [abstract] "we point out that attention-based architectures [12, 15], which recent LMs embody, induce this phenomenon due to its inherent inability to correctly process the 'negation' operation"
  - [section 3] "the attention-based architecture which aggregates token information via weighted averages lacks the capability for direct subtraction or projection between tokens"
  - [corpus] No direct corpus evidence; this is primarily derived from the paper's architectural analysis.
- Break condition: If the model architecture includes explicit negation processing or if token embeddings are modified post-attention to incorporate negation information.

### Mechanism 2
- Claim: Linear representation spaces in well-trained models cannot effectively represent absence because absence requires non-linear operations.
- Mechanism: The model's representation space is trained to be linear and smooth, optimized for tasks like classification and style interpolation. However, representing "not A" requires removing A's information from the embedding, which cannot be achieved through linear operations.
- Core assumption: The linearity of the representation space is both a feature (enabling good performance on standard tasks) and a bug (preventing effective negation representation).
- Evidence anchors:
  - [abstract] "this effect is due to the linearity in a well-trained model's representation space, where concept of absence plays a minor role"
  - [section 3] "The manifold hypothesis posits that data, despite appearing complex in high dimensions, can be simplified in a low-dimensional manifold, benefiting from linear classification and smooth style transformations via linear interpolation. However, this linearity is insufficient for representing absence."
  - [corpus] No direct corpus evidence; this is an inference from the paper's analysis of representation space characteristics.
- Break condition: If the model uses non-linear representation spaces or incorporates non-linear operations specifically for negation handling.

### Mechanism 3
- Claim: The proximity of absence representations to presence representations in CLIP embeddings demonstrates the model's inability to distinguish between "elephant" and "not elephant."
- Mechanism: CLIP embeddings place "elephant" and "not elephant" close together in representation space, showing that the model treats absence as similar to presence rather than orthogonal or distant.
- Core assumption: Distance in embedding space correlates with conceptual distinction - if two concepts are close, the model treats them as similar.
- Evidence anchors:
  - [abstract] "CLIP embeddings are unable to comprehend negative concepts in language. In the CLIP encoder, the representation of an object's absence is similar to its presence."
  - [section 3] "Fig. 3 demonstrates that CLIP embeddings are unable to comprehend negative concepts in language. In the CLIP encoder, the representation of an object's absence is similar to its presence."
  - [corpus] No direct corpus evidence; this is derived from the paper's experimental analysis.
- Break condition: If embedding distances are measured differently or if other embedding methods show better separation between presence and absence.

## Foundational Learning

- Concept: Attention mechanisms and weighted averaging
  - Why needed here: Understanding how attention computes token representations as weighted sums is crucial for grasping why negation is difficult to represent.
  - Quick check question: If attention computes token representations as weighted sums, what mathematical operation would be needed to represent "not elephant" and why can't attention perform this operation?

- Concept: Linear vs non-linear representation spaces
  - Why needed here: The paper argues that linear representation spaces are insufficient for representing absence, which requires non-linear operations.
  - Quick check question: Why would a linear representation space struggle to represent the concept of "absence" compared to a non-linear one?

- Concept: Embedding distance and conceptual similarity
  - Why needed here: The paper uses cosine similarity between embeddings to demonstrate that "elephant" and "not elephant" are close in representation space.
  - Quick check question: If two concepts have high cosine similarity in embedding space, what does this imply about how the model treats these concepts?

## Architecture Onboarding

- Component map: Text prompt → Text encoder (CLIP) → Attention aggregation → Representation space → Image generation (Diffusion/unet model)
- Critical path: Text prompt → Text encoder → Attention aggregation → Representation space → Image generation
- Design tradeoffs: Linear representation spaces enable good performance on standard tasks but prevent effective negation representation; attention mechanisms are efficient but lack negation capabilities.
- Failure signatures: High cosine similarity between presence and absence concepts; successful prompt attacks that generate prohibited content; inability to distinguish between "draw X" and "draw not X."
- First 3 experiments:
  1. Measure cosine similarity between "elephant" and "not elephant" embeddings to verify the paper's claim about representation proximity.
  2. Test whether adding explicit negation tokens (e.g., "never," "avoid") improves negation representation in the embedding space.
  3. Compare representation spaces of different model architectures (attention-based vs. non-attention) to see if the negation problem is specific to attention mechanisms.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "white bear phenomenon" persist in models that employ different architectural paradigms, such as those using retrieval-based methods or symbolic AI, instead of attention-based architectures?
- Basis in paper: [inferred] The paper suggests that the "white bear phenomenon" arises due to the inherent inability of attention-based architectures to process negation and the linearity of the representation space in large models. This implies that alternative architectures might handle negation and absence differently.
- Why unresolved: The paper focuses on attention-based architectures like those in Stable Diffusion and DALL-E3. It does not explore how other architectural paradigms, such as retrieval-based methods or symbolic AI, might handle the "white bear phenomenon."
- What evidence would resolve it: Conducting experiments using models with different architectural paradigms and observing whether they exhibit the "white bear phenomenon" would provide insights into the architectural causes of this issue.

### Open Question 2
- Question: How does the scale of the model (e.g., number of parameters) influence the severity or presence of the "white bear phenomenon"?
- Basis in paper: [explicit] The paper discusses the representation space of large models and their inability to understand negation. However, it does not investigate whether this issue is consistent across models of different scales.
- Why unresolved: The experiments are conducted on specific large models (Stable Diffusion and DALL-E3) without varying the scale or comparing with smaller models. The impact of model scale on the "white bear phenomenon" remains unexplored.
- What evidence would resolve it: Testing models of varying scales for the "white bear phenomenon" and analyzing the relationship between model size and the occurrence or severity of the issue would clarify the role of scale.

### Open Question 3
- Question: Can the "white bear phenomenon" be mitigated through architectural modifications rather than prompt-based strategies?
- Basis in paper: [inferred] The paper proposes prompt-based defense strategies inspired by cognitive therapy techniques. This suggests that while prompt engineering can help, it may not be a fundamental solution, implying that architectural changes might be necessary for a more robust resolution.
- Why unresolved: The paper focuses on prompt-based strategies and does not explore architectural modifications that could inherently improve the model's ability to process negation and absence.
- What evidence would resolve it: Developing and testing models with architectural modifications aimed at better handling negation and absence, and evaluating their effectiveness against the "white bear phenomenon," would determine if architectural changes offer a more fundamental solution.

## Limitations
- Model specificity: Analysis focuses primarily on attention-based architectures without comprehensive testing of alternative architectures
- Dataset scope: Defense strategy evaluation uses a limited dataset that may not capture real-world complexity
- Evaluation metrics: Success rates measured on Stable Diffusion specifically without extensive cross-model validation

## Confidence
- High Confidence: The core mechanism explaining why attention-based architectures struggle with negation (weighted averaging preventing subtraction operations) is well-established and logically sound.
- Medium Confidence: The empirical results showing the vulnerability's existence and the effectiveness of defense strategies are reliable within the tested domain (Stable Diffusion), but may not generalize perfectly to all models or contexts.
- Medium Confidence: The CLIP embedding analysis demonstrating proximity between presence and absence representations is methodologically sound, though the interpretation of embedding distances as conceptual similarity has some uncertainty.

## Next Checks
1. Cross-Model Validation: Test the proposed attack and defense strategies across multiple model architectures (including non-attention-based models) to determine whether the "white bear phenomenon" is specific to attention mechanisms or a broader characteristic of large language models.
2. Real-World Scenario Testing: Evaluate the effectiveness of the defense strategies in practical applications beyond image generation, such as content moderation systems or chatbot safety measures, to assess real-world applicability.
3. Alternative Representation Analysis: Investigate whether alternative representation learning approaches (e.g., contrastive learning with different objectives, or architectures that explicitly model negation) can mitigate the white bear phenomenon without requiring prompt-based defenses.