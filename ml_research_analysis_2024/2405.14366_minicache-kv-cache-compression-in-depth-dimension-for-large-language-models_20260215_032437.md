---
ver: rpa2
title: 'MiniCache: KV Cache Compression in Depth Dimension for Large Language Models'
arxiv_id: '2405.14366'
source_url: https://arxiv.org/abs/2405.14366
tags:
- cache
- minicache
- layers
- merging
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of memory inefficiency in large
  language model (LLM) inference caused by the growing size of Key-Value (KV) caches
  as sequence length increases. The authors propose MiniCache, a novel method that
  compresses KV caches across layers in the depth dimension.
---

# MiniCache: KV Cache Compression in Depth Dimension for Large Language Models

## Quick Facts
- arXiv ID: 2405.14366
- Source URL: https://arxiv.org/abs/2405.14366
- Reference count: 40
- Primary result: Achieves up to 5.02× compression ratio on LLaMA-2-7B with 4-bit MiniCache while maintaining near-lossless performance

## Executive Summary
MiniCache addresses the memory inefficiency problem in large language model (LLM) inference caused by the growing size of Key-Value (KV) caches as sequence length increases. The paper proposes a novel method that compresses KV caches across layers in the depth dimension by leveraging the high similarity of KV cache states between adjacent layers. By disentangling states into magnitude and direction components, MiniCache enables effective interpolation while preserving essential information. A token retention strategy keeps highly distinct state pairs unmerged to minimize performance degradation. The method is training-free and complementary to existing compression strategies.

## Method Summary
MiniCache is a training-free method that compresses KV caches across adjacent layers in the depth dimension. The approach is based on the observation that KV cache states exhibit high similarity between neighboring layers in the middle-to-deep portions of LLMs. The method disentangles state vectors into magnitude and direction components, interpolates the directional component using SLERP (Spherical Linear Interpolation) while preserving the original state norms, and applies a token retention strategy to keep highly distinct state pairs unmerged. During restoration, the method rescales magnitudes and restores retention tokens to reconstruct the original KV cache states for generation.

## Key Results
- Achieves up to 5.02× compression ratio on LLaMA-2-7B with 4-bit MiniCache on ShareGPT dataset
- Enhances inference throughput by approximately 5× while reducing memory footprint by 41% compared to FP16 full cache baseline
- Maintains near-lossless performance across various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral on multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-layer KV cache redundancy allows merging of adjacent layer states without significant performance loss.
- Mechanism: KV cache states exhibit high similarity between adjacent layers in the middle-to-deep portions of LLMs, enabling accurate merging of states into a shared memory space.
- Core assumption: The high similarity of KV cache states between adjacent layers is sufficient to preserve semantic and syntactic properties during merging.
- Evidence anchors:
  - [abstract] "Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs."
  - [section] "We observe that KV cache states exhibit high similarity between neighbouring layers in the middle-to-deep portions of LLMs."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.409" (weak, only 25 related papers with low average neighbor FMR score, indicating limited corpus evidence for this specific mechanism)

### Mechanism 2
- Claim: Disentangling KV cache states into magnitude and direction components enables accurate interpolation while preserving state norms.
- Mechanism: By decomposing state vectors into magnitude and direction components, MiniCache can interpolate the directional component in polar coordinates while preserving the original state norms.
- Core assumption: The decomposition into magnitude and direction components allows for effective interpolation without loss of critical information.
- Evidence anchors:
  - [abstract] "To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged."
  - [section] "This approach allows for effective interpolation of the directional component in polar coordinates while preserving the original state norms to retain as much information as possible."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.409" (weak, limited corpus evidence for this specific decomposition and interpolation mechanism)

### Mechanism 3
- Claim: Token retention strategy minimizes performance degradation by preserving highly distinct state pairs.
- Mechanism: MiniCache identifies and retains a small subset of state pairs with low similarities but significant semantic differences, preventing information loss during merging.
- Core assumption: A small subset of highly distinct state pairs exists that are sensitive to merging and require separate retention to maintain performance.
- Evidence anchors:
  - [abstract] "Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead."
  - [section] "We observe that merging these distinct tokens results in performance degradation, corresponding to γ = 0 row in Table 2."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.409" (weak, limited corpus evidence for this specific retention strategy)

## Foundational Learning

- Concept: KV cache mechanism in LLMs
  - Why needed here: Understanding how KV caches work is fundamental to grasping the problem MiniCache solves and the nature of its compression approach.
  - Quick check question: What is the primary purpose of KV caching in LLM inference and how does it reduce computational redundancy?

- Concept: Vector decomposition and interpolation
  - Why needed here: The core of MiniCache's merging strategy relies on decomposing vectors into magnitude and direction components and interpolating between them.
  - Quick check question: How does disentangling a vector into magnitude and direction components enable more accurate interpolation compared to direct averaging?

- Concept: Cosine similarity and angular distance
  - Why needed here: MiniCache uses cosine similarity and angular distance to measure the similarity between KV cache states and determine which tokens to retain.
  - Quick check question: Why is cosine similarity (or angular distance) preferred over Euclidean distance for measuring similarity between high-dimensional vectors in this context?

## Architecture Onboarding

- Component map: Input KV cache states -> Decompose into magnitude/direction -> Interpolate directions using SLERP -> Apply token retention threshold -> Store merged cache with retention tokens -> During restoration: rescale magnitudes -> Restore retention tokens -> Generate output

- Critical path: Fetch KV cache states → Decompose into magnitude/direction → Interpolate directions using SLERP → Apply token retention threshold → Store merged cache with retention tokens → During restoration: rescale magnitudes → Restore retention tokens → Generate output

- Design tradeoffs:
  - Compression ratio vs. performance: Higher merging (more layers compressed) increases compression but risks more performance degradation
  - Memory overhead vs. accuracy: More retained tokens improves accuracy but reduces compression benefits
  - Interpolation parameter t: Balances influence between current and previous layer states

- Failure signatures:
  - Performance degradation when merging too many layers
  - Memory usage not reducing as expected (retention threshold too high)
  - Model producing nonsensical outputs (incorrect interpolation or restoration)

- First 3 experiments:
  1. Baseline test: Run inference with full cache, record memory usage and performance metrics
  2. Single-layer merging test: Apply MiniCache to merge only the first eligible layer pair, measure compression ratio and performance impact
  3. Retention threshold sweep: Test different retention threshold values (γ) to find optimal balance between compression and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MiniCache change when merging more than two adjacent layers simultaneously, and what is the optimal number of layers to merge for maximizing compression ratio without significant performance loss?
- Basis in paper: [inferred] The paper discusses merging KV caches across adjacent layers but does not explore merging more than two layers simultaneously. It mentions future work on enhancing the compression ratio by cross-multiple-layer merging.
- Why unresolved: The paper does not provide experimental results or analysis on merging more than two layers at a time, leaving the impact on performance and compression ratio unknown.
- What evidence would resolve it: Experimental results comparing the performance and compression ratios of MiniCache when merging two, three, or more layers simultaneously, with analysis of the trade-offs involved.

### Open Question 2
- Question: How does the choice of interpolation parameter t in the SLERP function affect the performance of MiniCache across different LLM architectures and datasets, and can a dynamic t be determined based on layer-wise characteristics?
- Basis in paper: [explicit] The paper discusses the effect of the interpolation parameter t and suggests that a dynamic t could be determined based on the relative magnitude ratio of adjacent layers, but does not provide a method for doing so.
- Why unresolved: The paper identifies the potential for a dynamic t but does not explore or implement this approach, leaving the optimal strategy for choosing t across different contexts unclear.
- What evidence would resolve it: A method for dynamically determining the interpolation parameter t based on layer-wise characteristics, along with experimental validation across various LLM architectures and datasets.

### Open Question 3
- Question: How does MiniCache perform when combined with other KV cache compression techniques, such as quantization and sparsity, and what is the optimal combination strategy for achieving the best trade-off between compression ratio and performance?
- Basis in paper: [explicit] The paper mentions that MiniCache is orthogonal to existing quantization and sparsity methods and demonstrates its effectiveness when combined with KIVI-4bit KV cache quantization, but does not explore other combinations or provide a comprehensive analysis of optimal strategies.
- Why unresolved: The paper provides limited evidence of MiniCache's compatibility with other compression techniques and does not offer a systematic approach to combining methods for optimal results.
- What evidence would resolve it: A comprehensive study comparing the performance of MiniCache when combined with various quantization and sparsity methods, along with an analysis of the optimal combination strategies for different use cases.

## Limitations

- The paper lacks detailed hyperparameter specifications, particularly for the interpolation parameter t and token retention threshold γ, making exact reproduction challenging
- Limited corpus evidence (average neighbor FMR of 0.409) suggests the specific mechanism of depth-dimension compression may not be extensively validated in prior work
- The claim of near-lossless performance is primarily validated on ShareGPT dataset; generalization to other tasks and datasets remains to be fully established

## Confidence

- **High confidence**: The fundamental problem of KV cache memory inefficiency in LLM inference is well-established
- **Medium confidence**: The proposed disentanglement approach using magnitude and direction components is theoretically sound, but empirical validation across diverse scenarios is limited
- **Medium confidence**: The token retention strategy appears effective based on provided results, though the exact criteria for retention decisions could benefit from more rigorous justification

## Next Checks

1. **Ablation study on interpolation parameter**: Systematically vary the interpolation parameter t across a range of values (0.1, 0.3, 0.5, 0.7, 0.9) to quantify its impact on both compression ratio and performance degradation, establishing the sensitivity of the method to this critical hyperparameter

2. **Cross-dataset performance validation**: Evaluate MiniCache on additional benchmark datasets beyond ShareGPT (such as GSM8K, COQA, and TruthfulQA mentioned in the paper) to verify the claim of near-lossless performance across diverse task types and validate generalizability

3. **Comparison with existing KV cache compression methods**: Conduct head-to-head comparisons with other KV cache compression techniques like ZSMerge and Q-Filters on the same model architectures to establish MiniCache's relative performance advantage and identify specific scenarios where it excels or underperforms