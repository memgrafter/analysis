---
ver: rpa2
title: 'GloTSFormer: Global Video Text Spotting Transformer'
arxiv_id: '2401.03694'
source_url: https://arxiv.org/abs/2401.03694
tags:
- video
- tracking
- text
- detection
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Transformer-based global tracking method
  called GloTSFormer for video text spotting. The method introduces a Wasserstein
  distance-based method to conduct positional associations between frames and maintains
  a global embedding pool to store historical tracking embeddings and trajectory information.
---

# GloTSFormer: Global Video Text Spotting Transformer

## Quick Facts
- arXiv ID: 2401.03694
- Source URL: https://arxiv.org/abs/2401.03694
- Authors: Han Wang; Yanjie Wang; Yang Li; Can Huang
- Reference count: 40
- Primary result: Achieves 56.0 MOTA on ICDAR2015 video dataset, improving state-of-the-art by 4.6 absolute points

## Executive Summary
This paper proposes GloTSFormer, a Transformer-based method for video text spotting that introduces global tracking through a sliding window embedding pool and Wasserstein distance for morphological matching. The method addresses the challenge of maintaining consistent text trajectories across video frames by storing historical embeddings and computing long-range temporal associations. Experimental results show state-of-the-art performance on the ICDAR2015 video dataset with significant improvements over previous methods.

## Method Summary
GloTSFormer employs a Transformer architecture with a global embedding pool that maintains historical tracking embeddings across frames using a sliding window approach. For each frame, it computes associations between current frame embeddings and the global pool using cross-attention. The method introduces Wasserstein distance to capture both positional and morphological information between text instances in consecutive frames. Semantic embeddings from text recognition are incorporated to improve tracking discrimination. The system uses a YOLOX-based detector with polygon regression, Rotated RoIAlign for feature extraction, and a recognition head with CNN, LSTM, and fully connected layers.

## Key Results
- Achieves 56.0 MOTA on ICDAR2015 video dataset
- Improves previous state-of-the-art by 4.6 absolute points
- Outperforms previous Transformer-based method by 8.3 MOTA
- Ablation studies demonstrate effectiveness of global association module and Wasserstein distance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Global embedding pool stores historical tracking embeddings and trajectory information, enabling long-range temporal associations
- Mechanism: Sliding window maintains embeddings from previous frames. Transformer architecture computes associations between current embeddings and all embeddings in global pool, producing global association matrix for trajectory tracking
- Core assumption: Text shapes and semantic embeddings remain stable across sliding window for meaningful cross-frame associations
- Evidence anchors: Abstract states global embedding pool maintains historical tracking embeddings; section describes Transformer-based architecture for long-range temporal associations
- Break condition: Fast-moving or significantly deformed texts within sliding window cause global embedding associations to degrade

### Mechanism 2
- Claim: Wasserstein distance improves positional matching by incorporating both location and morphological information
- Mechanism: Polygons modeled as Gaussian distributions; Wasserstein distance computed and converted to similarity score for matching decisions
- Core assumption: Text bounding boxes can be accurately approximated by rotated bounding boxes and modeled as Gaussian distributions
- Evidence anchors: Abstract mentions Gaussian Wasserstein distance for morphological correlation; section describes modeling polygons as Gaussians and measuring similarity via distribution distance
- Break condition: Small or heavily occluded texts cause Gaussian approximation to become inaccurate, providing unreliable matching cues

### Mechanism 3
- Claim: Semantic embeddings from text recognition improve tracking discrimination
- Mechanism: Embeddings extracted via Rotated RoIAlign passed through CNN, LSTM, and fully connected layer to predict word classes; semantically enriched embeddings used in Transformer association
- Core assumption: Semantic class information improves embedding distinctiveness for tracking without significantly increasing computational cost
- Evidence anchors: Section describes introducing semantic information to boost tracking performance through word class prediction
- Break condition: Inaccurate recognition head or misaligned semantic features may lose or reverse discrimination benefit

## Foundational Learning

- Concept: Transformer self-attention mechanism
  - Why needed here: Method relies on cross-attention between current frame embeddings and global embedding pool, not self-attention
  - Quick check question: What is the difference between self-attention and cross-attention in a Transformer layer?

- Concept: Wasserstein distance between Gaussian distributions
  - Why needed here: Method models text polygons as Gaussians and computes Wasserstein distances to capture location and shape similarity
  - Quick check question: How does the Wasserstein distance formula change when comparing two Gaussian distributions with different means and covariances?

- Concept: Hungarian algorithm for assignment
  - Why needed here: Method uses Hungarian algorithm after computing association scores to ensure unique ID assignments
  - Quick check question: What is the time complexity of the Hungarian algorithm for an n×n cost matrix?

## Architecture Onboarding

- Component map: Backbone (ResNet-50 + FPN) → Detection head (YOLOX-style + polygon regression) → Recognition head (Rotated RoIAlign → CNN → LSTM → FC) → Tracking head (Transformer encoder/decoder + Wasserstein distance module) → Global embedding pool (sliding window queue)
- Critical path: Input frame → Detection → Rotated RoIAlign extraction → Semantic embedding generation → Global embedding pool update → Transformer cross-attention → Wasserstein distance computation → Max operation → Hungarian assignment
- Design tradeoffs: Sliding window size vs. temporal coherence (larger window improves tracking but increases computation); semantic embedding vs. pure appearance embedding (semantic improves discrimination but adds recognition overhead); Wasserstein distance vs. IoU (Wasserstein captures shape, IoU is simpler but less robust to motion)
- Failure signatures: ID switches in crowded scenes indicate global embedding pool inadequacy; missed detections indicate detector sensitivity; poor tracking with fast-moving text indicates Wasserstein distance or temporal window inadequacy
- First 3 experiments:
  1. Verify Transformer produces correct association scores by comparing P Asso with ground-truth correspondences in controlled dataset
  2. Test Wasserstein distance against IoU on pairs of consecutive frames with known ground-truth to measure robustness to motion
  3. Measure tracking performance with and without semantic embeddings to quantify their contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the global association module in GloTSFormer handle occlusions and disocclusions in video text spotting?
- Basis in paper: [inferred] Paper mentions global embedding pool maintains historical tracking embeddings but does not explicitly discuss handling of occlusions and disocclusions
- Why unresolved: Paper lacks detailed information on how global association module handles common challenges in video text spotting
- What evidence would resolve it: Further experiments and analysis on model's performance in handling occlusions and disocclusions with detailed explanation of underlying mechanisms

### Open Question 2
- Question: What is the impact of different sliding window sizes on the performance of GloTSFormer in video text spotting?
- Basis in paper: [explicit] Paper mentions sliding window size impacts results and provides ablation studies on sizes (2, 4, 8, 16) but lacks comprehensive analysis
- Why unresolved: Paper only provides limited information on impact of different sliding window sizes
- What evidence would resolve it: Detailed analysis of model's performance with different sliding window sizes including metrics such as MOTA, MOTP, and IDF1

### Open Question 3
- Question: How does the Wasserstein distance-based method for positional associations in GloTSFormer compare to other distance metrics in terms of performance and computational efficiency?
- Basis in paper: [explicit] Paper mentions Wasserstein distance outperforms IoU and other distances in performance but lacks detailed comparison of computational efficiency
- Why unresolved: Paper only provides limited information on computational efficiency of different distance metrics
- What evidence would resolve it: Detailed comparison of computational efficiency of different distance metrics including runtime and memory usage

## Limitations

- Global embedding pool introduces substantial memory overhead during inference by maintaining embeddings for all tracked objects across sliding window
- Gaussian approximation for Wasserstein distance calculation may not accurately represent complex text shapes, particularly for highly skewed or irregular polygons
- Method's performance on datasets beyond ICDAR2015 (such as real-world videos with significant motion blur or camera shake) remains untested

## Confidence

- Mechanism 1 (Global embedding pool): Medium confidence - concept clearly described but implementation details not fully specified
- Mechanism 2 (Wasserstein distance): Low confidence - method appears novel with limited corpus evidence and questionable Gaussian approximations
- Mechanism 3 (Semantic embeddings): Low confidence - contribution to tracking performance not empirically validated in isolation
- Overall performance claims: Medium confidence - substantial MOTA improvements but only evaluated on one dataset

## Next Checks

1. Systematically evaluate tracking performance across different sliding window sizes (2, 4, 8, 16) to identify optimal trade-off between temporal coherence and computational efficiency

2. Test method on additional video text spotting datasets with different characteristics (e.g., Minetto dataset, ICDAR2013 video) to assess robustness to varying text densities, motion patterns, and camera movements

3. Perform controlled experiments varying Wasserstein distance parameters (α, normalization function) and semantic embedding presence to quantify individual contributions to overall performance