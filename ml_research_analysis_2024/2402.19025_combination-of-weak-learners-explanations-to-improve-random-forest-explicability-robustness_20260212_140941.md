---
ver: rpa2
title: Combination of Weak Learners eXplanations to Improve Random Forest eXplicability
  Robustness
arxiv_id: '2402.19025'
source_url: https://arxiv.org/abs/2402.19025
tags:
- robustness
- explanations
- explanation
- shap
- weak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of improving the robustness of
  post-hoc explanations for ensemble models, specifically Random Forest, using SHAP.
  The authors propose a method called AXOM (Averaging on the eXplanations Of the Majority)
  that combines weak learner explanations by only averaging those from models that
  contribute positively to the ensemble's final decision.
---

# Combination of Weak Learners eXplanations to Improve Random Forest eXplicability Robustness

## Quick Facts
- arXiv ID: 2402.19025
- Source URL: https://arxiv.org/abs/2402.19025
- Reference count: 25
- Primary result: AXOM method reduces mean robustness value of Random Forest explanations compared to standard SHAP, with p-values below 0.05 on four datasets

## Executive Summary
This paper addresses the problem of improving the robustness of post-hoc explanations for Random Forest models. The authors propose AXOM (Averaging on the eXplanations Of the Majority), a method that combines SHAP explanations from individual weak learners by only averaging those from models that contribute positively to the ensemble's final decision. By discarding explanations from weak learners whose predictions disagree with the ensemble's output, AXOM reduces variability and improves explanation stability. The method is evaluated on four UCI datasets using a modified Lipschitz continuity metric, showing significant improvements in robustness compared to standard Random Forest SHAP.

## Method Summary
AXOM combines weak learner explanations by averaging only those from models that vote with the ensemble majority. For each prediction, the method generates SHAP explanations for all individual weak learners in the Random Forest, then computes a weighted average using only explanations from learners whose predictions match the ensemble's final decision. This approach addresses the predictive multiplicity problem where multiple models with similar accuracy can produce different outputs. The method is compared against standard Random Forest SHAP and single Decision Tree SHAP using a modified robustness metric that averages incremental ratios rather than taking the maximum variation.

## Key Results
- AXOM significantly reduced mean robustness values compared to Random Forest on all four datasets (WINE, GLASS, SEEDS, BANKNOTE) with p-values below 0.05
- The standard deviation of robustness values was also reduced, indicating more reliable explanations
- AXOM's explanations were more similar to Random Forest's when the ensemble's accuracy was higher, as more weak learners agreed with the final prediction
- Random Forest's averaging of weak learner predictions created smoother decision boundaries, leading to more gradual color changes in SHAP heatmaps compared to Decision Trees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Averaging only the SHAP explanations of weak learners that vote with the ensemble majority improves explanation robustness.
- Mechanism: By discarding explanations from weak learners whose predictions disagree with the ensemble's final decision, the method reduces the variability in explanations. This mitigates the predictive multiplicity problem, where multiple models with similar accuracy can produce different outputs, by restricting the explanation averaging to only the "correct" weak learners.
- Core assumption: Weak learners that contribute positively to the ensemble's final prediction are more likely to provide relevant and robust explanations for that decision.
- Evidence anchors:
  - [abstract]: "we take advantage of the decomposability of the Random Forest ensemble model in order to exploit the explanation values provided by each single weak learner to the purpose of building a more robust global explanation by combination of those weak explanations."
  - [section]: "AXOM proposes to combine the explanations of individual learners of an ensemble to improve robustness and explainability. Specifically, we consider only to combine the explanations of weak learners that positively contribute to the ensemble output (by positively we mean that the classification output of the weak learner matches the one obtained by the random forest ensemble)"
  - [corpus]: The corpus contains related papers on Random Forest and explainability, but lacks direct evidence for the mechanism of discarding minority explanations.
- Break condition: If the ensemble's accuracy is low, then the majority of weak learners might be "wrong," and excluding minority explanations could lead to less robust or less accurate explanations.

### Mechanism 2
- Claim: Random Forest's averaging of weak learner predictions creates smoother decision boundaries, which in turn produce more robust SHAP explanations.
- Mechanism: Individual Decision Trees create hard decision boundaries, leading to abrupt changes in SHAP explanations. Random Forest, by averaging the predictions of multiple trees, creates a smoother overall decision boundary. This smoothness translates to smoother SHAP explanations, as SHAP values are derived from model predictions.
- Core assumption: The averaging process in Random Forest leads to smoother decision boundaries compared to individual Decision Trees.
- Evidence anchors:
  - [section]: "Random Forest relies on the combination of several weak learners to create a smoother decision boundary that better adapts to the real one. Hence, it is expected that the softer boundaries that provide a more robust model will also provide more robust explicabilities."
  - [section]: "Fig. 4 show a comparison on the variation of the value of the explanations for the different studied datasets. It can be observed that the heatmaps related to the explanations produced by Random Forest depict more gradual color changes, which correspond to differences in values, follow also a smoother (thus more desirable) progression."
  - [corpus]: No direct evidence in the corpus for the relationship between averaging and smoother boundaries.
- Break condition: If the individual weak learners are highly correlated or very similar, the averaging process might not significantly smooth the decision boundaries.

### Mechanism 3
- Claim: Using a modified Lipschitz continuity metric that averages incremental ratios, rather than taking the maximum, provides a more balanced and fair assessment of explanation robustness.
- Mechanism: The original Lipschitz continuity metric (taking the maximum variation) can be dominated by extreme values near decision boundaries, especially as the number of estimators increases. The modified metric averages the incremental ratios over a neighborhood, providing a more representative measure of robustness within the local region.
- Core assumption: Averaging the incremental ratios provides a more stable and representative measure of robustness than taking the maximum.
- Evidence anchors:
  - [section]: "The choice of searching for the maximum value results in measurements that are unreliable for the purpose of a balanced and fair calculation of robustness around a given data point...we propose a slightly modified version of eq. (1) to calculate the average value of the incremental ratio between the explanation of xi and the explanation of the xj points around it as the new SHAP robustness criteria"
  - [section]: "Fig. 2 shows, for each of the examined datasets, an example comparing the values obtained by first considering the difference of the SHAP explanation values (top) and then their incremental ratio (bottom). All the heatmaps are produced basing the values obtained from 10000 perturbed xj points...It can be observed that the tendency of the values in the incremental ratio is towards infinity as one approaches the centre of the space whereas the differences in SHAP explanation are sometimes negligible."
  - [corpus]: No direct evidence in the corpus for the superiority of the average over the maximum.
- Break condition: If the neighborhood contains points with drastically different explanations, the average could be skewed and not represent the typical robustness.

## Foundational Learning

- Concept: SHAP (SHapley Additive exPlanations) values
  - Why needed here: The paper uses SHAP as the XAI technique to explain individual predictions of Random Forest and its weak learners. Understanding how SHAP works is crucial to grasp the method's approach to combining weak learner explanations.
  - Quick check question: How does SHAP assign importance values to each feature based on its contribution to the prediction? (Answer: SHAP uses Shapley values from game theory to calculate the average marginal contribution of a feature value across all possible partitions of the feature space.)

- Concept: Random Forest and Decision Trees
  - Why needed here: The paper compares the robustness of explanations from Random Forest (an ensemble of Decision Trees) with those from individual Decision Trees. Understanding the structure and behavior of these models is essential to understand the paper's findings.
  - Quick check question: How does Random Forest create a smoother decision boundary compared to a single Decision Tree? (Answer: Random Forest averages the predictions of multiple Decision Trees, which reduces the impact of individual tree's hard decision boundaries and creates a more continuous overall decision surface.)

- Concept: Lipschitz continuity and robustness metrics
  - Why needed here: The paper defines and uses a modified Lipschitz continuity metric to quantify the robustness of explanations. Understanding this concept is necessary to interpret the experimental results and the paper's claims about improved robustness.
  - Quick check question: What is the difference between the original Lipschitz continuity metric (taking the maximum) and the modified version (averaging incremental ratios) used in the paper? (Answer: The original metric focuses on the maximum variation in explanations, which can be dominated by extreme values near boundaries. The modified metric averages the incremental ratios over a neighborhood, providing a more balanced measure of local robustness.)

## Architecture Onboarding

- Component map:
  - Random Forest model (ensemble of Decision Trees)
  - SHAP explainer for Random Forest
  - SHAP explainer for individual Decision Trees (weak learners)
  - AXOM algorithm (averages SHAP explanations from agreeing weak learners)
  - Robustness metric calculation (modified Lipschitz continuity)
  - Datasets (WINE, GLASS, SEEDS, BANKNOTE)

- Critical path:
  1. Train Random Forest and Decision Tree models on a dataset.
  2. For each test sample, generate SHAP explanations for both models.
  3. Apply AXOM algorithm to get explanations from only the agreeing weak learners.
  4. Calculate robustness metrics for all three explanation methods.
  5. Compare robustness results and statistical significance.

- Design tradeoffs:
  - Using only agreeing weak learners (AXOM) vs. all weak learners (standard Random Forest SHAP) trades off potentially including some "wrong" explanations for increased robustness when the ensemble is accurate.
  - Averaging incremental ratios vs. taking the maximum in the robustness metric trades off sensitivity to extreme values for a more representative measure of local robustness.

- Failure signatures:
  - If the ensemble's accuracy is low, AXOM might perform worse than standard Random Forest SHAP because it discards explanations from some "correct" weak learners.
  - If the dataset has very smooth decision boundaries, the difference in robustness between Decision Trees and Random Forest might be negligible.

- First 3 experiments:
  1. Compare the robustness of SHAP explanations from a Decision Tree and Random Forest on a simple, linearly separable dataset. This should show the impact of smoother decision boundaries.
  2. Apply AXOM to a Random Forest trained on a dataset with high ensemble accuracy and compare its robustness to standard Random Forest SHAP. This should demonstrate the benefit of discarding disagreeing weak learner explanations.
  3. Test the sensitivity of the modified robustness metric by comparing it with the original maximum-based metric on a dataset with known challenging decision boundaries. This should illustrate the advantage of averaging incremental ratios.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas for future research are implied by the methodology and results. The authors mention that AXOM could be extended to other ensemble models beyond Random Forest, suggesting potential research directions in applying this approach to boosting methods like XGBoost or AdaBoost. Additionally, the relationship between ensemble accuracy and AXOM's effectiveness suggests that further investigation into the optimal conditions for using this method would be valuable.

## Limitations

- The method's effectiveness depends on the ensemble's accuracy - when accuracy is low, discarding minority explanations could potentially harm explanation quality
- The computational cost is substantial, requiring 10,000 perturbations per test sample for robustness calculation
- The modified Lipschitz continuity metric, while addressing some limitations of the maximum-based approach, may still be sensitive to the choice of neighborhood radius and number of perturbations

## Confidence

- High: The basic mechanism of AXOM (averaging explanations from agreeing weak learners) is sound and well-supported by the experimental results
- Medium: The relationship between ensemble accuracy and explanation quality, and the general superiority of the modified robustness metric over the original
- Low: The precise conditions under which AXOM outperforms standard Random Forest SHAP, and the sensitivity of results to hyperparameters like the number of perturbations and neighborhood radius

## Next Checks

1. Test AXOM on datasets with varying ensemble accuracy levels to establish the relationship between ensemble performance and explanation robustness
2. Compare the computational efficiency of AXOM with standard Random Forest SHAP, and investigate potential optimizations (e.g., adaptive perturbation sampling)
3. Conduct ablation studies to determine the optimal number of perturbations and neighborhood radius for the modified Lipschitz continuity metric