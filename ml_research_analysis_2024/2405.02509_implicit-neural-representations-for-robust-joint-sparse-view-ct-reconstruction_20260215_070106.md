---
ver: rpa2
title: Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction
arxiv_id: '2405.02509'
source_url: https://arxiv.org/abs/2405.02509
tags:
- reconstruction
- methods
- joint
- neural
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles sparse-view CT reconstruction by exploiting
  statistical regularities across similar scanned objects. It proposes a novel INR-based
  Bayesian framework that uses latent variables to capture shared patterns among multiple
  reconstructions.
---

# Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction

## Quick Facts
- arXiv ID: 2405.02509
- Source URL: https://arxiv.org/abs/2405.02509
- Reference count: 40
- One-line primary result: Novel Bayesian INR framework with latent variables achieves superior sparse-view CT reconstruction, outperforming baselines in PSNR/SSIM while being robust to noise and overfitting.

## Executive Summary
This paper addresses the challenge of sparse-view CT reconstruction by introducing a novel Bayesian framework that leverages implicit neural representations (INRs) with latent variables. The key innovation is capturing statistical regularities across multiple similar CT objects through a shared prior, enabling joint reconstruction that outperforms traditional single-object methods. By using variational inference and EM optimization, the method achieves significant improvements in reconstruction quality metrics while demonstrating robustness to measurement noise and overfitting.

## Method Summary
The proposed method uses J individual SIREN networks (one per object) with variational parameters, all sharing latent variables (ω, σ) representing a common prior. During joint training, each network is optimized with KL divergence regularization toward this prior (E-step), while the prior is updated based on all networks (M-step). This creates a feedback loop where knowledge is shared across objects, improving reconstruction quality beyond what single-object methods can achieve. The learned latent variables can also serve as effective initialization for new objects, accelerating training and improving generalization.

## Key Results
- Consistently outperforms baselines (FBP, SIRT, SingleINR, FedAvg, MAML, INRWild) in PSNR and SSIM metrics across multiple CT datasets
- Demonstrates robustness to measurement noise without degradation in reconstruction quality
- Shows reduced overfitting compared to iterative methods, maintaining consistent performance without early stopping
- Provides effective initialization for new objects, accelerating training by leveraging learned statistical regularities

## Why This Works (Mechanism)

### Mechanism 1
The Bayesian framework uses latent variables to capture statistical regularities across multiple similar CT objects, enabling dynamic regularization during training. The latent variables ω and σ act as a shared prior over the network weights of all J objects. During the E-step, each object's network is optimized with KL divergence regularization toward this prior, allowing knowledge sharing. The M-step updates the prior based on all networks, creating a feedback loop.

### Mechanism 2
The method achieves robustness to overfitting through continuous prior regularization rather than relying on early stopping. The KL divergence term in the loss function continuously pulls each network's weights toward the shared prior mean ω. This prevents individual networks from overfitting to their specific noisy measurements by maintaining a connection to the collective trend.

### Mechanism 3
The learned latent variables can serve as effective initialization for new objects, accelerating training and improving generalization. After joint training, the prior mean ω represents a meta-initialization that captures common features across similar objects. When applied to a new object, this initialization provides a better starting point than random initialization, leading to faster convergence and better final quality.

## Foundational Learning

- **Variational Inference and ELBO**: Used to approximate the intractable posterior distribution over network weights, maximizing the ELBO to optimize the marginal likelihood.
  - Quick check: What is the relationship between the ELBO and the KL divergence term in the loss function?

- **KL Divergence as Regularization**: Acts as a regularization term that enforces similarity across networks by measuring divergence between each network's variational posterior and the shared prior.
  - Quick check: How does the KL divergence term change during the M-step when the prior is updated?

- **Reparameterization Trick**: Enables gradient-based optimization of the variational parameters by expressing the sampling process as a deterministic transformation.
  - Quick check: What is the purpose of using the softplus function when parameterizing the variance?

## Architecture Onboarding

- **Component map**: J SIREN networks with variational parameters → E-step optimizations with KL regularization → M-step prior updates → Reconstruction loss computation
- **Critical path**: M-step updates → E-step optimizations → reconstruction loss computation → parameter updates
- **Design tradeoffs**: Number of EM loops (R) vs. computation time; KL divergence weight β vs. reconstruction accuracy vs. overfitting resistance; Network architecture depth/width vs. representation capacity vs. computational cost
- **Failure signatures**: Poor performance when objects are too dissimilar (prior becomes uninformative); Over-regularization when β is too high (loss of object-specific details); Under-regularization when β is too low (overfitting to individual measurements)
- **First 3 experiments**: 1) Implement basic SIREN network with Fourier feature embedding for single object reconstruction; 2) Extend to joint reconstruction of two similar objects with simple averaging of network parameters; 3) Implement full INR-Bayes framework with variational inference and KL regularization, testing on two similar CT objects from the same dataset

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the statistical regularities captured by latent variables change with different levels of measurement noise?
  - Basis in paper: The paper demonstrates that INR-Bayes is robust to noise in measurements, but does not explore how the statistical regularities adapt to varying noise levels.
  - Why unresolved: The paper does not provide a detailed analysis of how the latent variables respond to different noise intensities or types, which could impact their effectiveness in guiding reconstruction.
  - What evidence would resolve it: Experiments varying noise levels in the measurements and analyzing how the latent variables' distributions and the resulting reconstructions change would provide insights.

- **Open Question 2**: What is the impact of using different neural network architectures (e.g., CNNs, transformers) on the effectiveness of the Bayesian framework?
  - Basis in paper: The paper uses SIREN networks for INR-Bayes but does not explore the impact of using alternative architectures on the performance of the Bayesian framework.
  - Why unresolved: The choice of network architecture could influence the ability to capture and utilize statistical regularities, affecting the overall reconstruction quality and efficiency.
  - What evidence would resolve it: Comparative experiments using different network architectures within the Bayesian framework, evaluating their performance on reconstruction quality, robustness, and computational efficiency.

- **Open Question 3**: How does the proposed method scale with the number of objects in joint reconstruction, and what are the computational implications?
  - Basis in paper: While the paper mentions computational costs and shows some scaling experiments, it does not provide a comprehensive analysis of how the method scales with a large number of objects.
  - Why unresolved: Understanding the scalability is crucial for practical applications where a large number of similar objects need to be reconstructed, and it affects the feasibility of the method in real-world scenarios.
  - What evidence would resolve it: Experiments scaling the number of objects in joint reconstruction, analyzing the impact on reconstruction quality, computational time, and memory usage, would provide insights into the method's scalability.

## Limitations
- Requires multiple similar objects for joint training, limiting applicability when only a single CT scan is available
- Computational cost of EM optimization and joint training may be prohibitive for large-scale clinical applications
- Does not address potential privacy concerns when training on multiple patient datasets

## Confidence
- **High Confidence**: The core mechanism of using variational inference with KL regularization to share statistical knowledge across similar objects is theoretically sound and supported by experimental evidence
- **Medium Confidence**: Claims about robustness to overfitting are supported but could benefit from longer training curves and more diverse noise conditions
- **Medium Confidence**: The transferability of learned priors to new objects is demonstrated but only tested on datasets closely related to the training data

## Next Checks
1. **Ablation Study**: Systematically vary the KL divergence weight β and number of EM loops to quantify their impact on reconstruction quality and overfitting resistance
2. **Generalization Test**: Apply the learned priors to CT objects from completely different domains (e.g., industrial vs. medical) to assess true transferability
3. **Computational Efficiency Analysis**: Compare wall-clock training time and inference latency against baseline methods to evaluate practical deployment feasibility