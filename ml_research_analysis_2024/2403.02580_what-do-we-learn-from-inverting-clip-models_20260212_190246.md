---
ver: rpa2
title: What do we learn from inverting CLIP models?
arxiv_id: '2403.02580'
source_url: https://arxiv.org/abs/2403.02580
tags:
- clip
- images
- nsfw
- prompt
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores model inversion as a tool to examine CLIP models,
  revealing that inversion can generate semantically aligned images from text prompts.
  Key findings include CLIP's ability to blend concepts, gender biases in associations
  (e.g., professions defaulting to male), and instances of NSFW content from innocuous
  prompts, especially celebrity names.
---

# What do we learn from inverting CLIP models?

## Quick Facts
- arXiv ID: 2403.02580
- Source URL: https://arxiv.org/abs/2403.02580
- Authors: Hamid Kazemi; Atoosa Chegini; Jonas Geiting; Soheil Feizi; Tom Goldstein
- Reference count: 15
- One-line primary result: Model inversion of CLIP reveals semantic alignment with prompts, but also exposes biases and NSFW content generation.

## Executive Summary
This paper investigates model inversion as a method to analyze CLIP models, revealing both capabilities and biases. By optimizing random noise images to align with text embeddings, the authors generate semantically meaningful images from prompts. The study uncovers CLIP's ability to blend concepts, gender biases in profession associations, and unexpected NSFW content generation. Results show that inversion quality scales with training dataset size and that prompts are treated more like bags of words than coherent sentences.

## Method Summary
The authors employ gradient-based optimization to invert CLIP models by maximizing the cosine similarity between image and text embeddings. Starting from random noise, they optimize an image to match a given text prompt using CLIP's visual and text encoders. Augmentations and regularization terms are incorporated to improve image quality and prevent degenerate solutions. The process involves iterative updates over thousands of steps, with resolution increases at specific intervals.

## Key Results
- CLIP inversion successfully generates semantically aligned images, demonstrating the model's understanding of concept blending
- Gender biases are evident, with profession-related prompts defaulting to male associations
- Innocuous prompts, particularly celebrity names, can trigger NSFW content generation
- Training dataset scale significantly impacts inversion quality, with larger datasets producing better results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP inversion aligns image embeddings with text embeddings via gradient descent optimization.
- Mechanism: The method optimizes a randomly initialized image to maximize the cosine similarity between the CLIP visual encoder output and the CLIP text encoder output for the given prompt, using augmentations and regularization to improve realism.
- Core assumption: CLIP's contrastive training ensures that image and text embeddings live in a shared semantic space where cosine similarity meaningfully measures alignment.
- Evidence anchors:
  - [abstract] "Our examination reveals that inverting CLIP models results in the generation of images that exhibit semantic alignment with the specified target prompts."
  - [section] "To invert a CLIP model for a prompt p, we solve the following optimization problem starting from a random noise: max x cos(V (A(x)), T (p)) + Reg(x)"
- Break condition: If CLIP's image and text encoders use disjoint embedding spaces or if the contrastive loss does not enforce meaningful alignment, the inversion would fail to produce semantically relevant images.

### Mechanism 2
- Claim: Augmentations act as image priors, enforcing that variations of the optimized image remain semantically consistent with the prompt.
- Mechanism: Multiple augmented versions of the input image are optimized simultaneously so that if an image matches the prompt, its augmentations must also match, preventing the optimizer from exploiting spurious correlations.
- Core assumption: The CLIP model's representations are invariant to the chosen augmentations, so enforcing consistency across them preserves semantic content.
- Evidence anchors:
  - [section] "These augmentations are employed to invert classification models and serve as image priors. Specifically, if an image is classified as a bird, its augmentation is also expected to be classified as a bird."
  - [section] "Similarly, in CLIP inversion, if an image aligns with a given prompt, its augmentations must align with that prompt as well."
- Break condition: If augmentations alter the semantic meaning in CLIP's embedding space, enforcing consistency could distort the output or prevent convergence.

### Mechanism 3
- Claim: Larger training datasets yield better inversions by providing richer and more diverse visual-language associations.
- Mechanism: Models trained on more image-caption pairs can better capture subtle semantic relationships, allowing the inversion process to generate higher quality, more coherent images.
- Core assumption: CLIP's ability to generate meaningful inversions scales with the diversity and volume of training data.
- Evidence anchors:
  - [section] "We investigate the scale of the training data on the quality of the inversions, and we show that more training data leads to better inversions."
  - [section] "When a CLIP model is trained on a limited dataset, the resulting image quality is poor."
- Break condition: If dataset size increases without proportional quality improvement, or if training data contains pervasive biases, larger datasets may amplify unwanted artifacts rather than improve inversion quality.

## Foundational Learning

- Concept: Contrastive learning and embedding alignment
  - Why needed here: Understanding how CLIP's training objective creates a shared embedding space is key to grasping why inversion via cosine similarity works.
  - Quick check question: In CLIP's contrastive loss, what relationship between image and text embeddings is being maximized during training?
- Concept: Gradient-based optimization and regularization
  - Why needed here: The inversion process is fundamentally an optimization problem; regularization prevents degenerate solutions.
  - Quick check question: What role does Total Variation regularization play in CLIP inversion?
- Concept: Data augmentation as a prior
  - Why needed here: Augmentations enforce semantic consistency across image variations, acting as a prior for natural image structure.
  - Quick check question: Why might CLIP inversion benefit from applying different augmentations to each image in a batch?

## Architecture Onboarding

- Component map: Random noise -> Optimizer (Adam) -> CLIP visual encoder (V) -> CLIP text encoder (T) -> Augmentation module (A) -> Regularization function (Reg)
- Critical path: Image initialization → gradient ascent on cosine similarity + augmentations → regularization → iterate until convergence
- Design tradeoffs: More augmentations improve realism but increase compute; larger batch sizes improve robustness but require more memory; stronger regularization yields cleaner images but may suppress fine details
- Failure signatures: Images devolve into noise (over-regularization), prompt and image are semantically misaligned (poor embedding space), or optimization stalls (inappropriate learning rate or augmentations)
- First 3 experiments:
  1. Invert a simple, well-defined prompt ("a red apple") with default augmentations and visualize the progression of the image.
  2. Compare inversion quality across CLIP variants (RN50 vs ViT-B-16) to observe scaling effects.
  3. Disable augmentations to demonstrate their necessity for semantic consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms within CLIP's training process contribute to the generation of NSFW content during model inversion, and can these mechanisms be isolated and addressed to prevent such occurrences?
- Basis in paper: [explicit] The paper discusses the generation of NSFW content during model inversion, even with seemingly innocuous prompts, and mentions the proximity of celebrity names to NSFW words in the embedding space.
- Why unresolved: The paper identifies the presence of NSFW content but does not delve into the underlying mechanisms of how this occurs within the model's training process or how to address it.
- What evidence would resolve it: Detailed analysis of the training data and model architecture to identify specific features or training patterns that lead to NSFW associations, followed by experiments modifying these aspects to reduce NSFW content generation.

### Open Question 2
- Question: How does the scale and diversity of training data quantitatively impact the quality and bias of inverted images in CLIP models, and is there an optimal dataset size that minimizes bias while maintaining high-quality inversions?
- Basis in paper: [explicit] The paper notes that more training data leads to better inversions and that different dataset scales (e.g., OpenAI CLIP training data vs. CC12M vs. yfcc15M) affect image quality and NSFW content.
- Why unresolved: While the paper observes correlations between dataset size and inversion quality, it does not provide a quantitative analysis of how these factors interact or identify an optimal dataset size.
- What evidence would resolve it: Systematic experiments varying dataset sizes and diversity, measuring inversion quality and bias metrics across different scales, to determine the relationship and identify any optimal points.

### Open Question 3
- Question: To what extent does CLIP's treatment of prompts as "bags of words" affect its ability to accurately represent complex or nuanced concepts, and can modifications to the model architecture or training process improve its sentence-level understanding?
- Basis in paper: [explicit] The paper highlights that CLIP treats prompts as bags of words, leading to incorrect associations, such as generating an image of a "big kitten chasing a small dog" for the prompt "A big dog chasing a small kitten."
- Why unresolved: The paper demonstrates the issue but does not explore potential solutions or the full extent of the impact on CLIP's performance with complex prompts.
- What evidence would resolve it: Experiments modifying the model to enhance sentence-level understanding, such as incorporating positional embeddings or training on more contextually rich data, and evaluating performance on nuanced prompts before and after modifications.

## Limitations

- Proprietary training dataset limits reproducibility of observed biases and NSFW content generation
- Impact of different augmentation strategies and regularization strengths on inversion quality not fully explored
- Treatment of prompts as bags of words affects ability to represent complex concepts accurately

## Confidence

- High confidence: The core mechanism of CLIP inversion via gradient-based optimization and the correlation between training data scale and inversion quality
- Medium confidence: The specific instances of NSFW content generation and gender biases, as these are contingent on the unknown training data distribution
- Medium confidence: The claim that prompts are treated as bags of words rather than sentences, based on observed incorrect associations

## Next Checks

1. **Augmentation ablation study**: Systematically vary augmentation strength and types to quantify their impact on inversion quality and semantic consistency.
2. **Cross-dataset bias analysis**: Train a CLIP variant on a different, publicly available dataset and compare the inversion outputs for the same prompts to isolate dataset-specific biases.
3. **Prompt structure sensitivity**: Design experiments to test whether rephrasing prompts (e.g., from "a man in a suit" to "a professional man wearing a suit") affects the inversion output, validating the bag-of-words hypothesis.