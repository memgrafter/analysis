---
ver: rpa2
title: Curvature-Informed SGD via General Purpose Lie-Group Preconditioners
arxiv_id: '2402.04553'
source_url: https://arxiv.org/abs/2402.04553
tags:
- psgd
- preconditioner
- learning
- group
- preconditioners
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes curvature-informed SGD (PSGD) via Lie group
  preconditioners, offering a novel approach to accelerate SGD convergence by leveraging
  curvature information from Hessian-vector products or finite differences. The method
  involves two preconditioners - a matrix-free and a low-rank approximation - updated
  online using a criterion robust to stochastic gradient noise and without line search
  or damping.
---

# Curvature-Informed SGD via General Purpose Lie-Group Preconditioners

## Quick Facts
- arXiv ID: 2402.04553
- Source URL: https://arxiv.org/abs/2402.04553
- Authors: Omead Pooladzandi; Xi-Lin Li
- Reference count: 40
- Key outcome: PSGD outperforms SoTA optimizers on Vision, NLP, and RL tasks with low computational overhead

## Executive Summary
This paper introduces Preconditioned Stochastic Gradient Descent (PSGD) that leverages Lie group preconditioners to accelerate convergence by incorporating curvature information. The method uses Hessian-vector products or finite differences to estimate curvature, then fits preconditioners online within connected Lie groups to preserve symmetry and invertibility without requiring damping. Extensive empirical validation demonstrates PSGD's superiority over standard optimizers across multiple domains and architectures while maintaining low computational overhead.

## Method Summary
PSGD estimates curvature through Hessian-vector products or finite differences, then updates preconditioners on connected Lie groups to preserve symmetry and invertibility. The method employs two preconditioner types: a matrix-free XMat (diagonal + anti-diagonal) and a low-rank approximation (LRA) with configurable rank. Preconditioners are updated online using a noise-robust fitting criterion that automatically handles stochastic gradient noise without line search or damping. The Lie group's equivariance property simplifies updates while maintaining the invariance property that eliminates damping requirements.

## Key Results
- PSGD outperforms Adam, SGD, and Apollo on CIFAR-10 with ResNet18
- Demonstrated better generalization on noisy label and imbalanced datasets
- Achieves 20% overhead vs SGD, negligible vs other second-order methods
- Successfully applied to Vision, NLP, and RL tasks across multiple architectures

## Why This Works (Mechanism)

### Mechanism 1
PSGD recovers the inverse of the "absolute" Hessian asymptotically through Lie group preconditioner fitting, regardless of the Hessian's definiteness. The preconditioner Q is updated online via gradient descent on a connected Lie group, preserving symmetry/invertibility. This ensures P = QTQ converges to |H|^-0.5 even with stochastic noise.

### Mechanism 2
Lie group equivariance eliminates the need for damping by automatically adapting the preconditioner's scale and orientation. Moving a preconditioner around any point on the Lie group behaves the same as moving it around the identity, so the same group generator can update Q from any state without extra damping terms.

### Mechanism 3
PSGD finds flatter, more generalizable minima by maintaining higher entropy and lower confidence predictions compared to first/second-order optimizers. The gradient-noise-robust fitting criterion naturally damps noise without extra hyperparameters, leading to less confident but more generalizable solutions.

## Foundational Learning

- **Lie groups and equivariance/invariance properties**: Why needed - Lie groups provide structured space where preconditioner updates preserve symmetry and invertibility without damping. Quick check - Why does updating on a Lie group automatically preserve the symmetry of P = QTQ?
- **Hessian-vector products and finite differences**: Why needed - These allow curvature estimation without forming the full Hessian, enabling scalable second-order methods. Quick check - How does the Hessian-vector product h = ∂(vT g)/∂θ relate to the true Hessian?
- **Preconditioner fitting criteria robust to stochastic noise**: Why needed - Standard BFGS-style fitting fails with stochastic gradients; the proposed criterion automatically dampens noise. Quick check - What is the form of the fitting criterion c(P) = E[δgT P δg + δθT P^-1 δθ] and why is it noise-robust?

## Architecture Onboarding

- **Component map**: θ update via preconditioned gradient -> Preconditioner update: Q fitting on Lie group using (v,h) pairs -> Curvature estimator: Hessian-vector products or finite differences -> Lie group structures: XMat, LRA, UVd variants
- **Critical path**: 1. Sample mini-batch → compute gradient g 2. With probability p: sample v ~ N(0,I), compute h = ∂(vT g)/∂θ 3. Update Q on Lie group using gradients derived from (v,h) 4. Precondition g → g = QT(Qg) 5. Update θ ← θ - μ1g
- **Design tradeoffs**: Memory vs accuracy: LRA with higher rank captures more curvature but costs more memory; Update frequency: Updating Q every iteration gives better preconditioning but doubles overhead vs p=0.1; Lie group choice: XMat is very cheap but less expressive than LRA
- **Failure signatures**: Q becoming singular or near-singular → check Lie group constraint preservation; Large variance in training → verify p and μ2 are not too high; Divergence with finite differences → ensure ε is small enough
- **First 3 experiments**: 1. Rosenbrock minimization: Verify quadratic convergence and exact recovery of (1,1) minimum 2. MNIST with LeNet5: Compare Hessian spectra and test accuracy vs Adam/SGD 3. CIFAR10 ResNet18: Test robustness to label noise and class imbalance, compare to Adam/SGD

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of rank parameter in PSGD's Low-Rank Approximation (LRA) preconditioner impact its performance on different network architectures and datasets? The paper mentions testing orders r = 0, 1, 2, 5, and 10 for the LRA preconditioner on MNIST and CIFAR-10, observing marginal gains up to r = 5, but does not provide systematic analysis of optimal rank across various architectures.

### Open Question 2
Can PSGD's Lie group framework be extended to handle non-differentiable activation functions or network components? The paper focuses on second-order differentiable problems and does not address potential challenges with non-differentiable components common in modern deep learning architectures.

### Open Question 3
How does PSGD's performance compare to other second-order optimization methods in terms of wall-clock time and memory usage for large-scale deep learning tasks? While the paper mentions 20% overhead compared to SGD, it lacks comprehensive comparison of wall-clock time and memory usage against other second-order optimizers like KFAC, Shampoo, or AdaHessian on truly large-scale tasks.

## Limitations
- Theoretical convergence guarantees under non-stationary conditions remain unproven
- Entropy-generalization correlation lacks rigorous theoretical foundation
- Performance on extremely large-scale models (>1B parameters) and distributed settings untested
- Scalability analysis beyond tested architectures incomplete

## Confidence

**High Confidence**: Empirical superiority on benchmark tasks (CIFAR-10, MNIST, Penn Treebank) with multiple runs and proper statistical reporting; computational efficiency claims reasonable given matrix-free and low-rank implementations.

**Medium Confidence**: Theoretical claims about Lie group properties preserving symmetry and eliminating damping are plausible based on group theory, but direct proofs for stochastic setting lacking; mechanism by which higher entropy leads to better generalization requires more rigorous investigation.

**Low Confidence**: Universal applicability claim across all deep learning domains and architectures needs more diverse testing; scalability beyond tested architectures incomplete.

## Next Checks

1. **Convergence Analysis Under Non-Stationarity**: Test PSGD on continual learning benchmarks to verify preconditioner's ability to adapt to changing curvature in non-stationary data distributions.

2. **Entropy-Generalization Causation**: Design experiment where entropy is explicitly controlled (e.g., through temperature scaling) while keeping other factors constant to establish whether higher entropy directly causes better generalization.

3. **Large-Scale Model Validation**: Implement PSGD on a modern transformer architecture (e.g., BERT-base) for language modeling to evaluate scalability and performance on truly large models.