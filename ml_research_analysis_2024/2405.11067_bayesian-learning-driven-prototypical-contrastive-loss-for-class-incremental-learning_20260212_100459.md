---
ver: rpa2
title: Bayesian Learning-driven Prototypical Contrastive Loss for Class-Incremental
  Learning
arxiv_id: '2405.11067'
source_url: https://arxiv.org/abs/2405.11067
tags:
- class
- learning
- task
- loss
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Bayesian learning-driven prototypical contrastive
  loss for class-incremental learning (BLCL) to address catastrophic forgetting in
  continual learning. The method dynamically balances cross-entropy and contrastive
  losses using Bayesian uncertainty weighting, incorporating a novel contrastive loss
  that reduces intra-class and increases inter-class distances.
---

# Bayesian Learning-driven Prototypical Contrastive Loss for Class-Incremental Learning

## Quick Facts
- **arXiv ID:** 2405.11067
- **Source URL:** https://arxiv.org/abs/2405.11067
- **Reference count:** 40
- **Key outcome:** Proposed BLCL method achieves up to 86.57% accuracy on CIFAR-10, 70.43% on CIFAR-100, 79.73% on ImageNet100, and 96.16% on GNSS dataset while maintaining better cluster separation (lower DB, higher CH scores) compared to state-of-the-art methods.

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental learning by proposing a Bayesian learning-driven prototypical contrastive loss (BLCL). The method dynamically balances cross-entropy and contrastive losses using Bayesian uncertainty weighting while incorporating a novel contrastive loss that reduces intra-class and increases inter-class distances. BLCL employs a ResNet-based architecture with dynamic specialized blocks that adapt to task-specific requirements. Experiments on CIFAR-10, CIFAR-100, ImageNet100, and a GNSS dataset demonstrate superior performance over state-of-the-art methods with improved cluster separation and compactness metrics.

## Method Summary
BLCL combines cross-entropy loss with a contrastive loss weighted by Bayesian uncertainty parameters σ1 and σ2. The method uses a ResNet backbone split into generalized feature extraction layers and dynamic specialized blocks that expand task-specific capacity. New tasks add specialized blocks and average weights with previous tasks to retain knowledge while learning new classes. The contrastive loss operates in a 512-dimensional embedding space using cosine distance to pull positive pairs together and push negative pairs apart. Training uses 300 epochs per task with Adam optimizer, exemplar memory of 2000 samples, and data augmentation including color jitter, Gaussian blur, and random flips.

## Key Results
- CIFAR-10: 86.57% accuracy (vs 74.66% for LwF, 76.50% for EWC)
- CIFAR-100: 70.43% accuracy (vs 55.91% for LwF, 56.51% for EWC)
- ImageNet100: 79.73% accuracy (vs 61.20% for LwF, 65.66% for EWC)
- GNSS dataset: 96.16% accuracy (vs 84.72% for LwF, 85.89% for EWC)
- Lower Davies-Bouldin scores (0.46 on CIFAR-10 vs 0.68 for LwF) indicating better cluster separation
- Higher Calinski-Harabasz scores (5.32 on CIFAR-10 vs 3.89 for LwF) indicating better cluster compactness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian weighting dynamically balances CE and CL losses by modeling their uncertainty, preventing catastrophic forgetting.
- Mechanism: The model learns noise parameters σ1 and σ2 for CE and CL losses. Higher uncertainty (larger σ) reduces the loss weight. This shifts emphasis between representation learning (CL) and classification accuracy (CE) based on training stage.
- Core assumption: Homoscedastic task uncertainty captures the relative importance of CE and CL losses across tasks.
- Evidence anchors:
  - [abstract]: "Our approach dynamically adapts the balance between the cross-entropy and contrastive loss functions with a Bayesian learning technique."
  - [section 3.3]: "We adopt a strategy wherein we concurrently optimize both objectives using homoscedastic task uncertainty... The final objective is to minimize Equation 5 with respect to σ1 and σ2, thereby learning the relative weights of the losses L1(W) and L2(W) in an adaptive manner."
  - [corpus]: Weak; no corpus entries directly discuss Bayesian weighting in CIL.
- Break condition: If the uncertainty modeling fails to capture task-dependent importance, the weighting becomes static and performance degrades.

### Mechanism 2
- Claim: Contrastive loss with prototypical embeddings reduces intra-class distance and increases inter-class distance, improving cluster separation.
- Mechanism: Feature embeddings are mapped into a 512-dimensional space. Positive pairs (same class) are pulled together, negative pairs (different classes) are pushed apart using cosine distance. This creates tighter, more separated class clusters.
- Core assumption: Cosine distance in the embedding space effectively captures class similarity for continual learning.
- Evidence anchors:
  - [abstract]: "We introduce a contrastive loss that incorporates novel classes into the latent representation by reducing intra-class and increasing inter-class distance."
  - [section 3.3]: "For all training samples (f(Xa_i), f(Xp_i), f(Xn_i)) ∈ Φ, our objective is to fulfill the following inequality: d(f(Xa_i), f(Xp_i)) + α < d(f(Xa_i), f(Xn_i)), where d is a distance function, specifically the cosine distance..."
  - [corpus]: Weak; no corpus entries discuss contrastive loss for CIL with prototypical embeddings.
- Break condition: If class clusters overlap significantly, the model cannot distinguish classes and accuracy drops.

### Mechanism 3
- Claim: Dynamic specialized blocks expand task-specific capacity while reusing generalized features, enabling efficient continual learning.
- Mechanism: The ResNet backbone is split into generalized (feature extraction) and specialized (task-specific) components. New tasks add specialized blocks and average weights with previous tasks to retain knowledge while learning new classes.
- Core assumption: Deeper layers capture task-specific features, so adding specialized blocks improves performance without retraining entire network.
- Evidence anchors:
  - [abstract]: "BLCL employs a ResNet-based architecture with dynamic specialized blocks that adapt to task-specific requirements."
  - [section 3.2]: "The specialized component consists of blocks whose weights are averaged with the weights from the previous task... To maintain adaptability to varying complexities of new classes across tasks, we dynamically adjust each task's layer structure..."
  - [corpus]: Weak; no corpus entries discuss dynamic specialized blocks in CIL.
- Break condition: If specialized blocks grow too large, memory and computation costs outweigh benefits.

## Foundational Learning

- Concept: Bayesian learning with homoscedastic uncertainty
  - Why needed here: To dynamically balance CE and CL losses without manual hyperparameter tuning.
  - Quick check question: What does a high σ value imply about a loss term's importance?

- Concept: Contrastive learning with positive/negative pairs
  - Why needed here: To create discriminative feature embeddings that reduce forgetting by maintaining class separation.
  - Quick check question: How does increasing inter-class distance help prevent catastrophic forgetting?

- Concept: Prototypical networks for class representation
  - Why needed here: To define class centers in embedding space for contrastive loss computation.
  - Quick check question: Why is reducing intra-class distance important for classification accuracy?

## Architecture Onboarding

- Component map:
  - ResNet18/32 backbone (13 conv layers) -> Generalized feature extraction
  - Specialized blocks (up to 8 blocks) -> Task-specific layers with weight averaging
  - Average pooling -> Feature aggregation
  - FC layer -> Classification head
  - Bayesian weighting module -> Learns σ1, σ2 for CE and CL losses

- Critical path:
  1. Load task data and exemplars.
  2. Compute class similarity to decide specialized block depth.
  3. Forward pass through generalized + specialized layers.
  4. Compute CE loss and form positive/negative pairs for CL loss.
  5. Apply Bayesian weighting to combine losses.
  6. Backpropagate and update weights.
  7. Average specialized weights with previous tasks.

- Design tradeoffs:
  - More specialized blocks → better task adaptation but higher memory.
  - Stronger CL weighting → better representation but risk of poor classification boundaries.
  - Larger exemplar memory → better retention but less space for new data.

- Failure signatures:
  - Accuracy drops sharply after new tasks → catastrophic forgetting.
  - Confusion matrices show high off-diagonal values → poor class separation.
  - t-SNE shows overlapping clusters → contrastive loss ineffective.

- First 3 experiments:
  1. Train BLCL on CIFAR-10 with 2 classes per task; verify accuracy > 80% after 4 tasks.
  2. Disable Bayesian weighting (use fixed 0.9 CL weight); compare accuracy drop.
  3. Remove contrastive loss entirely; compare cluster separation (DB/CH scores).

## Open Questions the Paper Calls Out
None

## Limitations
- Bayesian weighting mechanism's effectiveness depends on accurate uncertainty modeling without direct validation.
- Dynamic specialized blocks' capacity expansion strategy lacks ablation studies showing optimal block numbers per task.
- Contrastive loss formulation assumes cosine distance is optimal for continual learning without comparing to other distance metrics.

## Confidence

- Bayesian weighting mechanism: **Medium** - Theoretically sound but lacks direct validation
- Contrastive loss effectiveness: **High** - Well-established in representation learning literature
- Dynamic specialized blocks: **Medium** - Novel approach but limited ablation studies
- Overall method performance: **High** - Strong quantitative results across multiple datasets

## Next Checks

1. **Ablation study**: Train BLCL with fixed (0.9, 0.1) CE/CL weights vs Bayesian weighting; measure accuracy difference and σ1/σ2 convergence patterns.
2. **Distance metric comparison**: Replace cosine distance with Euclidean distance in contrastive loss; evaluate impact on DB/CH scores and accuracy.
3. **Block capacity analysis**: Train BLCL with fixed 2 specialized blocks vs dynamic adjustment; measure memory usage, accuracy per task, and forgetting rates.