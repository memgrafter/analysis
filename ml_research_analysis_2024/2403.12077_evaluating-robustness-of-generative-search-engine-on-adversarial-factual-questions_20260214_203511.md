---
ver: rpa2
title: Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions
arxiv_id: '2403.12077'
source_url: https://arxiv.org/abs/2403.12077
tags:
- arena
- search
- language
- generative
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the adversarial robustness of generative search
  engines by designing seven attack strategies applied to factual questions. It demonstrates
  that retrieval-augmented generation systems are more susceptible to factual errors
  than large language models without retrieval, with an average attack success rate
  of 31.6% versus 24.4%.
---

# Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions

## Quick Facts
- arXiv ID: 2403.12077
- Source URL: https://arxiv.org/abs/2403.12077
- Reference count: 35
- Primary result: Retrieval-augmented generation systems are 7.2% more vulnerable to factual errors than LLMs without retrieval

## Executive Summary
This study evaluates the adversarial robustness of generative search engines by designing seven attack strategies applied to factual questions. The research demonstrates that retrieval-augmented generation systems are more susceptible to factual errors than large language models without retrieval, with an average attack success rate of 31.6% versus 24.4%. The experiments reveal that search engines often provide contradictory information within responses and struggle with numerical reasoning despite strong cloze performance. Results highlight the need for rigorous evaluation and fortification of these systems against adversarial threats before deployment.

## Method Summary
The researchers collected 43 factual statements from Wikipedia across diverse domains and expanded them to 534 sentences through manual annotation using seven attack techniques (Multihop Extension, Temporal Modification, Semantic Replacement, Distraction Injection, Facts Exaggeration, Facts Reversal, Numerical Manipulation). Four generative search engines (Bing in three modes, PerplexityAI, YouChat) and three LLMs (Gemini-Pro, GPT-3.5-Turbo-1106, GPT-4-1106-Preview) were evaluated on these adversarial sentences. Human evaluators assessed Accuracy Rate, Factscore, Fluency, Utility, and Citation Quality metrics to compare performance before and after attacks.

## Key Results
- Retrieval-augmented generation exhibits 31.6% attack success rate versus 24.4% for LLMs without retrieval
- Interrogative questions yield 4.4% higher accuracy, 2.9% higher citation recall, and 4.7% higher citation precision than declarative sentences
- Search engines provide contradictory information within responses and struggle with numerical reasoning despite strong cloze performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation systems are more susceptible to factual errors than large language models without retrieval.
- Mechanism: Search engines integrate external knowledge sources and rely on keyword-based retrieval, which can be manipulated by subtle adversarial modifications to the input text. These modifications alter the retrieved context in ways that mislead the generative model into producing incorrect answers, even when the base LLM (e.g., GPT-4) would have answered correctly on its own.
- Core assumption: The retrieval step is the weakest link, and adversarial changes to the query can corrupt the retrieved context without affecting the model's understanding of the original prompt.
- Evidence anchors:
  - [abstract]: "retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval"
  - [section]: "Generative search engines are more likely to be induced by factual errors to produce erroneous results than LLMs without retrieval. On average, the ASR of search engines is 31.6%, which is 7.2% higher than LLM’s ASR of 24.4%."
  - [corpus]: Weak evidence; no direct citations found in corpus. Claim is based on the experimental results within the paper itself.
- Break condition: If the retrieval system uses a robust semantic search that understands the intent and ignores irrelevant adversarial content, or if the generative model independently verifies retrieved facts against its internal knowledge.

### Mechanism 2
- Claim: Adversarial factual questions are highly effective in inducing incorrect responses from generative search engines and LLMs.
- Mechanism: Attack strategies (e.g., Multihop Extension, Temporal Modification, Semantic Replacement) subtly distort factual claims so that they appear plausible but are actually incorrect. Search engines, when retrieving supporting documents, may surface content that corroborates the distorted fact, leading the model to accept and repeat the error. The model's reliance on retrieved context for factuality makes it vulnerable to these manipulations.
- Core assumption: The model trusts retrieved information and lacks independent verification, so if the retrieval system brings back misleading content, the model will generate incorrect answers.
- Evidence anchors:
  - [abstract]: "we demonstrate the effectiveness of adversarial factual questions in inducing incorrect responses"
  - [section]: "Prior to such attacks, all models demonstrated exceptional performance, boasting an average accuracy of 95.8%. However, their performance significantly deteriorates after being exposed to adversarial attacks, resulting in an average attack success rate (ASR) of 25.1%."
  - [corpus]: Weak evidence; the corpus does not contain prior studies on these specific attack methods. This is an original contribution of the paper.
- Break condition: If the model independently cross-checks retrieved facts against its internal knowledge, or if the retrieval system filters out or flags potentially misleading content.

### Mechanism 3
- Claim: The form of the question (interrogative vs. declarative) influences the accuracy and quality of the engine's responses.
- Mechanism: Interrogative sentences better stimulate the retrieval capabilities of generative search engines, leading to more effective extraction of search keywords and more relevant citations. This improved retrieval results in higher citation recall and precision, which in turn leads to more accurate answers. The engine is more likely to detect and correct errors in interrogative form because the retrieval is more targeted.
- Core assumption: The search engine's keyword extraction and retrieval process is more effective for interrogative sentences, leading to better context for the generative model to generate accurate answers.
- Evidence anchors:
  - [abstract]: "Moreover, retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval."
  - [section]: "We found that across all engines, the average ASR, Citation-Recall, and Citation-Precision for interrogative sentences are higher than those for declarative sentences by 4.4%, 2.9%, and 4.7%, respectively."
  - [corpus]: Weak evidence; no direct citations found in corpus. This is an original finding of the paper.
- Break condition: If the retrieval system performs equally well for both interrogative and declarative sentences, or if the generative model does not rely heavily on the form of the question for keyword extraction.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: The paper's core argument is that RAG systems are more vulnerable to adversarial attacks than standalone LLMs. Understanding how RAG works is essential to grasp why the retrieval step introduces vulnerabilities.
  - Quick check question: What are the two main components of a RAG system, and how do they interact to generate a response?

- Concept: Adversarial attack strategies
  - Why needed here: The paper evaluates seven different attack strategies, each designed to exploit specific weaknesses in the RAG system. Understanding these strategies is crucial for understanding the paper's findings and implications.
  - Quick check question: What is the difference between "Multihop Extension" and "Distraction Injection" as attack strategies, and how do they aim to mislead the RAG system?

- Concept: Factual accuracy and verification
  - Why needed here: The paper focuses on the factual accuracy of responses generated by RAG systems and evaluates their ability to verify facts using citations. Understanding how factual accuracy is measured and verified is essential for interpreting the paper's results.
  - Quick check question: How does the paper measure factual accuracy, and what role do citations play in this evaluation?

## Architecture Onboarding

- Component map: User query -> Keyword extraction -> Retrieval system -> Generative model -> Output response with citations
- Critical path: User query → Keyword extraction → Retrieval → Generative model → Response
- Design tradeoffs:
  - Accuracy vs. speed: More thorough retrieval and verification can improve accuracy but may slow down response time.
  - Relevance vs. recall: Retrieving more documents can improve recall but may include irrelevant information that confuses the generative model.
  - Trust vs. verification: Relying on retrieved information can be faster but may introduce errors if the retrieval is manipulated.
- Failure signatures:
  - High attack success rate (ASR): Indicates vulnerability to adversarial attacks.
  - Low citation precision: Indicates that many citations are irrelevant or do not support the statements they are attached to.
  - Contextual contradictions in responses: Indicates that the model is not properly integrating retrieved information with the query.
- First 3 experiments:
  1. Test the impact of different keyword extraction strategies on the accuracy of responses to adversarial queries.
  2. Evaluate the effectiveness of different citation verification methods in detecting and correcting factual errors introduced by adversarial attacks.
  3. Compare the performance of RAG systems with different base LLMs (e.g., GPT-3.5 vs. GPT-4) on adversarial queries to identify the impact of the generative model on vulnerability.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation relies on a relatively small set of 43 factual statements from Wikipedia, which may not represent the full diversity of real-world queries.
- The seven attack strategies, while systematic, represent only a subset of possible adversarial techniques.
- Human evaluation introduces potential subjectivity in scoring metrics like Factscore and Citation Quality.
- The study does not investigate defense mechanisms or the effectiveness of potential mitigations against these attacks.

## Confidence

**Confidence labels:**
- Retrieval vulnerability claim (Medium): Strong experimental support within controlled conditions, but limited to specific attack strategies and factual domains.
- Question form influence (Low): Statistically significant but effect size is modest (4.4% ASR difference) and mechanism remains unclear.
- Citation quality correlation (Medium): Empirical measurements provided, but causal relationship between citations and factual accuracy not fully established.

## Next Checks
1. Test the seven attack strategies on a larger, more diverse set of factual queries (e.g., from news articles, technical documentation) to assess external validity.
2. Implement and evaluate simple defense mechanisms (e.g., fact verification modules, citation filtering) to determine if the identified vulnerabilities can be mitigated.
3. Conduct A/B testing with real users to measure the practical impact of adversarial attacks on user trust and task completion in realistic search scenarios.