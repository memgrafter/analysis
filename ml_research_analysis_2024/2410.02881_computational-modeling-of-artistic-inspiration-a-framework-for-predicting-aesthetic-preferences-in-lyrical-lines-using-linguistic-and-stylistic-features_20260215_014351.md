---
ver: rpa2
title: 'Computational Modeling of Artistic Inspiration: A Framework for Predicting
  Aesthetic Preferences in Lyrical Lines Using Linguistic and Stylistic Features'
arxiv_id: '2410.02881'
source_url: https://arxiv.org/abs/2410.02881
tags:
- poetic
- lines
- word
- imagery
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework to computationally model artistic
  inspiration in lyrical content by identifying key linguistic and stylistic features
  that influence individual aesthetic preferences. The authors propose measuring poetic
  imagery, word energy, level of abstraction, valence, surprisal, contextual entropy,
  NPMI, and banality using large language models, then calibrate these features to
  predict whether a lyric line is inspiring to specific individuals.
---

# Computational Modeling of Artistic Inspiration: A Framework for Predicting Aesthetic Preferences in Lyrical Lines Using Linguistic and Stylistic Features

## Quick Facts
- arXiv ID: 2410.02881
- Source URL: https://arxiv.org/abs/2410.02881
- Authors: Gaurav Sahu; Olga Vechtomova
- Reference count: 19
- Key outcome: Proposed framework achieves 92.2% accuracy in predicting individual aesthetic preferences for lyrical lines, outperforming a 70B-parameter LLaMA-3 model by nearly 18 percentage points

## Executive Summary
This paper introduces a novel computational framework for modeling artistic inspiration in lyrical content by identifying key linguistic and stylistic features that influence individual aesthetic preferences. The authors propose measuring eight specific features—poetic imagery, word energy, abstraction level, valence, surprisal, contextual entropy, NPMI, and banality—using large language models in a zero-shot prompting manner. These features are then calibrated using machine learning models to predict whether a lyric line is inspiring to specific individuals. The framework demonstrates significant improvements over using large language models directly, achieving up to 92.2% accuracy with XGBoost calibration on the EvocativeLines dataset containing 3,025 annotated AI-generated lyric lines.

## Method Summary
The proposed two-step framework first computes linguistic and stylistic features for each lyric line using LLM prompting in a zero-shot manner. Eight features are calculated: poetic imagery, word energy, abstraction level, valence, surprisal, contextual entropy, NPMI, and banality. These features are computed for each word and concatenated to form a temporal feature vector. The second step involves training a calibration network (either LSTM+Attention or XGBoost) on these feature vectors to predict whether a lyric line is inspiring to a specific individual. The model is trained on the EvocativeLines dataset and evaluated using accuracy and AUC metrics, with stratified sampling used for train-test splits.

## Key Results
- XGBoost calibration achieves 92.2% accuracy for user A1 and 87.3% for user A2, outperforming 70B-parameter LLaMA-3 by nearly 18 percentage points
- Different annotators are influenced by different subsets of features, demonstrating the framework's flexibility across diverse preferences
- The feature-based approach outperforms using LLMs out-of-the-box, showing that engineered features combined with appropriate calibration is more effective than raw LLM embeddings for subjective tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linguistic and stylistic features computed via LLM prompting are more predictive of individual artistic preferences than raw LLM embeddings.
- Mechanism: Features like poetic imagery, word energy, abstraction level, valence, surprisal, entropy, NPMI, and banality capture structured dimensions of creative appeal that correlate with inspiration. These are computed by prompting LLaMA-3-70b in zero-shot fashion, avoiding preference bias while leveraging its linguistic knowledge. The calibration network then learns to weight and combine these features for a specific user.
- Core assumption: Human aesthetic judgments on lyrical content can be decomposed into measurable linguistic and stylistic components, and LLMs can reliably estimate these components.
- Evidence anchors:
  - [abstract] "Our framework outperforms an out-of-the-box LLaMA-3-70b, a state-of-the-art open-source language model, by nearly 18 points."
  - [section] "These higher predictive power of our features computed using an LLM compared to using the LLM out-of-the-box shows that while we can leverage the knowledge of LLMs to compute well-established linguistic and stylistic characteristics of a lyric line, the LLM itself has difficulty in capturing the nuanced differences between what is perceived as 'inspiring' and 'not inspiring' by an individual."
  - [corpus] Weak—corpus signals only relate to general aesthetic modeling, not this specific decomposition.
- Break condition: If LLMs fail to assign consistent ratings for the stylistic features, or if human preferences are too idiosyncratic to be captured by the chosen features, the predictive power will degrade.

### Mechanism 2
- Claim: Calibration networks trained on feature vectors achieve higher accuracy than fine-tuned large models or embedding-based classifiers.
- Mechanism: The calibration network (LSTM+Attn or XGBoost) maps the engineered feature vectors (8T + m or 8T + 1 dimensions) to binary labels. Because the features are temporally structured and capture creative attributes, the calibration network can learn user-specific patterns. XGBoost, despite having far fewer parameters (163) than LLaMA-3-70b (70B), outperforms it because it learns precise feature interactions relevant to each user profile.
- Core assumption: The combination of high-quality features and an appropriate calibration architecture is more effective than using a large model directly on raw text or embeddings for subjective tasks.
- Evidence anchors:
  - [abstract] "Our computational model leverages the proposed linguistic and poetic features and applies a calibration network on top of it to accurately forecast artistic preferences among different creative individuals."
  - [section] "Specifically, our XGBoost model with just 163 parameters achieved a test accuracy of 92.2 for A1 and 87.3 for A2, compared to 450-shot LLaMA-3 classifier with 70B parameters that achieves an accuracy of 74.8 and 69.8 for A1 and A2, respectively."
  - [corpus] Weak—corpus signals mention subjective preference frameworks but not this specific calibration approach.
- Break condition: If the calibration network overfits to the small dataset, or if the features do not generalize across users, performance will drop.

### Mechanism 3
- Claim: Individual artistic preferences are diverse but can be modeled by different subsets of the proposed features.
- Mechanism: Statistical interaction testing (logistic regression with interaction terms) shows that different users are influenced by different features. For example, user A1 values entropy and poetic imagery, while user A3 values NPMI and abstraction. This suggests the framework is flexible enough to adapt to diverse preference profiles by learning which features matter most for each user.
- Core assumption: Human aesthetic preferences, while subjective, are not entirely random and can be approximated by combinations of measurable linguistic properties.
- Evidence anchors:
  - [section] "We conduct a similar analysis for the other annotators and highlight the significant features for each of them in Table 3. These results show that each annotator's artistic preferences were influenced by a different set of features, and our framework can be used to explain aesthetic preferences in an interpretable manner."
  - [section] "Our experiments demonstrate that the predictive power of our framework surpasses that of several orders of magnitude larger state-of-the-art language models like LLaMA-3-70b."
  - [corpus] Weak—corpus signals only mention subjective preference modeling in general.
- Break condition: If a user's preferences are too complex or non-linear for the available features to capture, or if the sample size is too small to learn reliable feature weights, the model will fail to generalize.

## Foundational Learning

- Concept: Linguistic feature extraction via LLM prompting.
  - Why needed here: The framework depends on computing stylistic attributes (imagery, energy, abstraction, valence, banality) without access to labeled training data for these tasks. Zero-shot prompting leverages LLMs' world knowledge to produce interpretable scores.
  - Quick check question: How would you design a prompt to have an LLM rate the poetic imagery of a sentence on a 1-5 scale, ensuring the rating is enclosed in tags?

- Concept: Temporal feature aggregation.
  - Why needed here: Lyric lines are sequences; features are computed for each prefix to capture how stylistic properties evolve over the line. This allows the model to learn not just static properties but narrative or emotional arcs.
  - Quick check question: Why does the framework concatenate scores for all subsequences (S_t) instead of using only the full sentence score?

- Concept: Calibration network design.
  - Why needed here: The raw feature vectors must be mapped to user-specific preferences. The calibration network learns which features and interactions are predictive for each user, compensating for the fact that different users value different aspects.
  - Quick check question: What are the trade-offs between using an LSTM+Attn versus XGBoost for the calibration network in terms of parameter count and ability to handle sequential features?

## Architecture Onboarding

- Component map: Feature extractor (LLM prompting) -> Calibration network (LSTM+Attn or XGBoost) -> Prediction
- Critical path: Feature extraction → calibration → prediction. Failures in prompt reliability or calibration overfitting will break the pipeline.
- Design tradeoffs: LSTM+Attn can model temporal dependencies but needs more data and has higher parameter count; XGBoost is fast and interpretable but cannot natively handle sequences.
- Failure signatures: Low inter-annotator agreement, poor calibration accuracy on held-out users, or inconsistent LLM feature scores.
- First 3 experiments:
  1. Run the feature extractor on a held-out lyric line and verify that scores for poetic imagery and word energy are in expected ranges.
  2. Train XGBoost on a small subset and confirm it outperforms majority baseline.
  3. Perform ablation by removing one feature (e.g., valence) and measure impact on accuracy.

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but based on the content, several questions emerge regarding the framework's limitations and potential extensions:

- How does the proposed framework handle the challenge of feature interaction when predicting aesthetic preferences, particularly when combining linguistic, stylistic, and statistical features?
- What is the potential impact of expanding the proposed framework to other forms of artistic expression, such as visual art or music, and how would the feature set need to be adapted?
- How does the proposed framework compare to other existing methods for evaluating creative outputs, such as the Consensual Assessment Technique (CAT) or the Torrance Test of Creative Thinking (TTCT), in terms of capturing the inspirational potential of AI-generated content?

## Limitations
- The framework's performance relies heavily on the reliability of LLM-generated feature scores, but the paper does not validate the consistency of these zero-shot prompts across different LLM instances or random seeds
- The dataset size (3,025 lines) is relatively small for subjective preference modeling, and the study only tests on AI-generated lyrics rather than human-authored content
- The calibration models may be overfitting to the small number of annotators (only 2 primary annotators with 10 in the smaller dataset), limiting generalizability to broader populations

## Confidence
- High confidence: The core methodology of feature engineering + calibration network is sound and well-documented
- Medium confidence: The 18-point improvement over LLaMA-3 is significant but may not generalize to other subjective tasks
- Low confidence: The claim that different users are influenced by different feature subsets needs more validation with larger, more diverse annotator pools

## Next Checks
1. **Prompt consistency validation**: Run the LLM feature computation pipeline 10 times on the same lyric line with different random seeds and measure the variance in poetic imagery, word energy, and abstraction scores
2. **Annotator diversity stress test**: Train separate models for each of the 10 annotators in the smaller dataset and compare the learned feature weights to test if preferences truly differ or if there's a common pattern
3. **Generalization test**: Apply the framework to a held-out set of human-authored lyrics (not in the original dataset) and measure performance degradation compared to AI-generated lyrics