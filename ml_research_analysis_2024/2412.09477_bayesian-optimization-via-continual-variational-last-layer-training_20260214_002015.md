---
ver: rpa2
title: Bayesian Optimization via Continual Variational Last Layer Training
arxiv_id: '2412.09477'
source_url: https://arxiv.org/abs/2412.09477
tags:
- vbll
- iteration
- performance
- bayesian
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of designing effective surrogate
  models for Bayesian optimization (BO), particularly in high-dimensional and non-stationary
  environments where traditional Gaussian processes (GPs) struggle. The core method
  introduces a variational Bayesian last layer (VBLL) neural network approach that
  combines the strengths of GPs (uncertainty quantification, conditioning efficiency)
  with the scalability and flexibility of neural networks.
---

# Bayesian Optimization via Continual Variational Last Layer Training

## Quick Facts
- arXiv ID: 2412.09477
- Source URL: https://arxiv.org/abs/2412.09477
- Authors: Paul Brunzema; Mikkel Jordahn; John Willes; Sebastian Trimpe; Jasper Snoek; James Harrison
- Reference count: 40
- Primary result: VBLLs outperform GPs and other BNN baselines on complex benchmarks, especially in high-dimensional settings

## Executive Summary
This paper addresses the challenge of designing effective surrogate models for Bayesian optimization (BO), particularly in high-dimensional and non-stationary environments where traditional Gaussian processes (GPs) struggle. The core method introduces a variational Bayesian last layer (VBLL) neural network approach that combines the strengths of GPs (uncertainty quantification, conditioning efficiency) with the scalability and flexibility of neural networks. The key innovation is a continual training scheme that interleaves full model retraining with efficient recursive last layer updates, enabling online optimization. Experiments show that VBLLs outperform GPs and other Bayesian neural network baselines on complex benchmarks, especially in high-dimensional settings, and demonstrate robustness across various noise levels.

## Method Summary
The method builds on variational Bayesian last layers (VBLLs), connecting training of these models to exact conditioning in GPs. VBLLs use a neural network backbone for feature extraction, with a variational posterior over the last layer weights that enables recursive updates. The approach combines full model retraining with efficient last layer updates, triggered by log-likelihood thresholds. This enables scalable BO with uncertainty quantification through Thompson sampling, particularly advantageous in multi-objective optimization where traditional acquisition functions face numerical instability.

## Key Results
- VBLLs achieve competitive performance with GPs on standard benchmarks while significantly improving scalability
- The continual learning strategy with event-triggered re-initialization reduces computational cost without sacrificing performance
- Thompson sampling with VBLLs provides numerical stability advantages in multi-objective optimization, successfully navigating HV plateaus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VBLL models achieve competitive performance by combining exact conditioning from GPs with feature learning from neural networks
- Mechanism: The VBLL approach leverages a deterministic variational lower bound that is tight when the variational family contains the true posterior. This allows recursive last layer updates while simultaneously optimizing network features through standard neural network training.
- Core assumption: The true posterior for Bayesian linear regression is contained in the chosen variational family with dense covariances
- Evidence anchors:
  - [abstract]: "We build on variational Bayesian last layers (VBLLs), and connect training of these models to exact conditioning in GPs."
  - [section]: "Theorem 1. Fix θ. Then, the variational posterior parameterized by (w̄∗, S∗) := η∗ = arg maxη L(η, θ) is equivalent to the posterior computed by the recursive least squares inferential procedure described by (2) and (3), iterated over the full dataset."
- Break condition: If the true posterior is not contained in the variational family, the variational lower bound would not be tight

### Mechanism 2
- Claim: Continual learning through event-triggered re-initialization improves runtime efficiency while maintaining performance
- Mechanism: Instead of re-training the entire model at every iteration, VBLLs use recursive rank-1 Cholesky updates to efficiently update the last layer posterior. Full model re-training is triggered only when new data is inconsistent with the current model (measured by log-likelihood threshold).
- Core assumption: Recursive updates maintain sufficient model quality between re-initialization events
- Evidence anchors:
  - [section]: "We consider a loss-triggered method for re-training, in which we re-train the full model if the log predictive likelihood of the new data under the model is below some threshold, and otherwise perform recursive updates."
  - [section]: "In Figure 7, we compare the event-triggered strategy to periodic retraining demonstrating an improved performance on all benchmarks."
- Break condition: If the threshold is set too high, model quality degrades; if too low, computational benefits are lost

### Mechanism 3
- Claim: Thompson sampling with VBLLs provides better numerical conditioning in multi-objective optimization compared to log expected hypervolume improvement
- Mechanism: VBLLs yield Gaussian predictive distributions, enabling Thompson sampling through simple sampling from the variational posterior. For multi-objective problems, this allows generating Pareto front samples that can be optimized analytically.
- Core assumption: Gaussian predictive distributions enable stable Thompson sampling
- Evidence anchors:
  - [abstract]: "VBLL networks significantly outperform GPs and other BNN architectures on tasks with complex input correlations"
  - [section]: "Also for multi-objective optimization problems, we can leverage the parametric form of the VBLLs to do efficient Thompson sampling"
- Break condition: If the variational posterior becomes too concentrated or the model fails to capture uncertainty properly, Thompson sampling may become ineffective

## Foundational Learning

- Concept: Bayesian linear regression and recursive posterior updates
  - Why needed here: VBLLs build directly on Bayesian linear regression principles, where the last layer performs exact Bayesian inference given fixed features
  - Quick check question: What is the recursive update formula for the posterior mean and covariance in Bayesian linear regression?

- Concept: Variational inference and ELBO optimization
  - Why needed here: VBLL training uses a deterministic variational lower bound that approximates the marginal likelihood, requiring understanding of variational inference principles
  - Quick check question: Why is the variational lower bound tight when the true posterior is contained in the variational family?

- Concept: Thompson sampling and acquisition functions in Bayesian optimization
  - Why needed here: VBLLs enable Thompson sampling through their parametric form, which is particularly advantageous in multi-objective settings where traditional acquisition functions face numerical issues
  - Quick check question: How does Thompson sampling with VBLLs differ from Thompson sampling with non-parametric models like GPs?

## Architecture Onboarding

- Component map: Neural network backbone (θ) -> Variational posterior (η) -> Noise variance (σ²) -> Wishart scale parameter
- Critical path: For each new data point, check if log-likelihood exceeds threshold → if yes, perform recursive rank-1 Cholesky update; if no, re-train full model with early stopping based on training loss
- Design tradeoffs: Recursive updates offer computational efficiency but may lag in performance; full re-training provides quality but is expensive; Thompson sampling offers numerical stability but may explore more than needed
- Failure signatures: Poor performance may indicate Wishart scale mis-specification, threshold set incorrectly, or network architecture inadequate for the problem complexity
- First 3 experiments:
  1. Run VBLL on a simple 2D benchmark (e.g., Branin) with different Wishart scales to observe sensitivity
  2. Compare event-triggered vs. periodic re-initialization on Ackley2D to understand the tradeoff between runtime and performance
  3. Test Thompson sampling vs. logEI on a multi-objective problem to observe numerical stability differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of re-initialization strategy (event-triggered vs. scheduled) impact the overall performance and runtime efficiency of VBLL models across diverse problem types?
- Basis in paper: [explicit] The paper discusses different re-initialization strategies and their impact on performance and runtime in Section 3.3 and Appendix B.3.3
- Why unresolved: The paper presents results for specific benchmarks but does not comprehensively evaluate the impact of re-initialization strategy across a wide range of problem types
- What evidence would resolve it: A systematic comparison of re-initialization strategies across diverse problem types, including varying dimensions, noise levels, and objective characteristics

### Open Question 2
- Question: Can VBLL models effectively handle high-dimensional problems with non-stationary objectives when combined with acquisition functions beyond Thompson sampling?
- Basis in paper: [explicit] The paper demonstrates strong performance of VBLL models with Thompson sampling on high-dimensional and non-stationary problems in Section 5.3
- Why unresolved: The paper primarily focuses on Thompson sampling and does not extensively explore the performance of VBLL models with other acquisition functions
- What evidence would resolve it: A comprehensive evaluation of VBLL models with various acquisition functions on high-dimensional and non-stationary problems

### Open Question 3
- Question: How does the VBLL model's performance scale with increasing input dimensionality, and what are the practical limitations in terms of computational cost and model complexity?
- Basis in paper: [explicit] The paper demonstrates strong performance of VBLL models on high-dimensional problems, including a 200D benchmark, in Section 5.3
- Why unresolved: The paper does not provide a detailed analysis of the scaling behavior of VBLL models with respect to input dimensionality
- What evidence would resolve it: An in-depth study of the scaling behavior of VBLL models with input dimensionality, including an analysis of computational cost and model complexity

## Limitations
- VBLLs rely on dense variational posterior approximation that scales cubically with dataset size
- Event-triggered re-initialization introduces hyperparameters that require careful tuning
- Numerical stability advantages of Thompson sampling in multi-objective settings need broader validation

## Confidence
- High: VBLL methodology and connection to GPs
- Medium: Continual learning strategy's efficiency gains
- Medium: Thompson sampling advantages in multi-objective optimization

## Next Checks
1. Test VBLL performance with varying dataset sizes to quantify the cubic scaling impact and validate the effectiveness of recursive updates
2. Conduct ablation studies on the re-initialization threshold to understand its sensitivity and impact on the exploration-exploitation tradeoff
3. Validate the Thompson sampling numerical stability advantage on additional multi-objective benchmarks with varying Pareto front geometries