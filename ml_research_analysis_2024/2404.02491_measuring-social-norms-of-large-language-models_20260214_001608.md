---
ver: rpa2
title: Measuring Social Norms of Large Language Models
arxiv_id: '2404.02491'
source_url: https://arxiv.org/abs/2404.02491
tags:
- answer
- subject
- choices
- index
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new benchmark, Social, to evaluate large
  language models'' (LLMs) understanding of social norms. Unlike existing datasets
  that focus on general social science knowledge, Social emphasizes fine-grained fundamental
  social norm skills, covering 402 skills and 12,383 questions across two subjects:
  social studies and language arts.'
---

# Measuring Social Norms of Large Language Models

## Quick Facts
- arXiv ID: 2404.02491
- Source URL: https://arxiv.org/abs/2404.02491
- Authors: Ye Yuan; Kexin Tang; Jianhao Shen; Ming Zhang; Chenguang Wang
- Reference count: 31
- Primary result: Introduces Social benchmark with 402 skills and 12,383 questions to evaluate LLMs' social norm understanding, achieving human-level performance with SocialAgent multi-agent framework

## Executive Summary
This paper introduces a new benchmark called Social to evaluate large language models' (LLMs) understanding of social norms. Unlike existing datasets that focus on general social science knowledge, Social emphasizes fine-grained fundamental social norm skills, covering 402 skills and 12,383 questions across two subjects: social studies and language arts. The dataset follows the K-12 curriculum, enabling direct comparison with elementary students. Experiments on recent LLMs (GPT-3.5-Turbo and LLaMA2-Chat) show significant improvements in social norm understanding over previous models, though performance still lags behind average elementary students. The authors propose SocialAgent, a multi-agent framework combining retrieval, programming, and reasoning agents, which further enhances LLMs' performance to be on par with human performance.

## Method Summary
The authors created the Social benchmark by curating 12,383 questions across 402 fine-grained social norm skills from the K-12 curriculum, covering social studies and language arts. They evaluated GPT-3.5-Turbo and LLaMA2-Chat using zero-shot prompting with specific templates, and developed SocialAgent, a multi-agent framework that integrates retrieval, programming, and reasoning agents to enhance LLM performance. The framework was tested by comparing its outputs with baseline LLM performance and human elementary student performance on the IXL platform.

## Key Results
- Social dataset covers 402 unique social norm skills with 12,383 questions, the most comprehensive set available
- Baseline LLMs (GPT-3.5-Turbo, LLaMA2-Chat) show significant improvement in social norm understanding compared to previous models
- SocialAgent multi-agent framework achieves performance on par with human elementary students

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset's fine-grained skill structure enables precise measurement of LLMs' understanding of social norms.
- Mechanism: By dividing social norms into 402 unique skills across two subjects, the dataset allows granular evaluation of model performance on specific social norm competencies, identifying strengths and weaknesses.
- Core assumption: Each skill is distinct and measurable, and performance on one skill is independent of others.
- Evidence anchors:
  - [abstract] "Our dataset features the largest set of essential social norm skills with 402 unique skills ranging from rules in language, to culture, economics, laws, and so on."
  - [section 2.1] "Social consists of 12,383 high-quality multi-choice questions belonging to 402 skills, the most comprehensive set of social norm skills."
  - [corpus] Weak - no direct evidence on skill granularity impact on measurement.
- Break condition: If skills overlap significantly or are not well-defined, measurement precision degrades.

### Mechanism 2
- Claim: The K-12 curriculum alignment enables direct comparison of LLM performance with human elementary students.
- Mechanism: By designing the dataset according to the K-12 curriculum, the benchmark provides a standardized evaluation framework that maps model performance to human grade-level understanding.
- Core assumption: The K-12 curriculum accurately represents fundamental social norm understanding at each grade level.
- Evidence anchors:
  - [abstract] "We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students."
  - [section 2.1] "We design our dataset according to the K-12 curriculum...This enables the evaluation of models on social norm fundamentals."
  - [corpus] Weak - no direct evidence on curriculum alignment impact on comparison validity.
- Break condition: If the curriculum does not accurately reflect fundamental social norm understanding, comparisons become invalid.

### Mechanism 3
- Claim: The multi-agent framework enhances LLMs' social norm understanding by integrating external knowledge and reasoning.
- Mechanism: SocialAgent combines retrieval, programming, and reasoning agents to provide LLMs with additional context, symbolic knowledge, and step-by-step reasoning paths, improving their ability to understand and answer social norm questions.
- Core assumption: LLMs can effectively utilize external knowledge and reasoning paths when provided in context.
- Evidence anchors:
  - [section 3] "We propose a multi-agent framework based on large language models to improve the models' ability to understand social norms. This method further improves large language models to be on par with humans."
  - [section 4.1] "With SocialAgent, both LLaMA2-70B-Chat and GPT3.5-Turbo improve their social norm understanding across different social norm skills."
  - [corpus] Weak - no direct evidence on multi-agent framework effectiveness.
- Break condition: If LLMs cannot effectively integrate external knowledge or reasoning paths, performance gains diminish.

## Foundational Learning

- Concept: Social norms
  - Why needed here: Understanding what social norms are and their importance is crucial for evaluating LLMs' ability to comprehend and apply them.
  - Quick check question: What are social norms, and why are they important in human society?

- Concept: Curriculum-based assessment
  - Why needed here: Knowledge of curriculum-based assessment methods is necessary to understand how the dataset's K-12 alignment enables standardized evaluation.
  - Quick check question: How does curriculum-based assessment differ from other evaluation methods?

- Concept: Multi-agent systems
  - Why needed here: Understanding multi-agent systems is essential for comprehending how SocialAgent integrates different specialized agents to enhance LLM performance.
  - Quick check question: What are the benefits and challenges of using multi-agent systems in AI applications?

## Architecture Onboarding

- Component map:
  - Dataset: Social (402 skills, 12,383 questions) -> Baseline LLMs (GPT-3.5-Turbo, LLaMA2-Chat) -> SocialAgent (Retrieval Agent, Programming Agent, Reasoning Agent) -> Enhanced LLM performance

- Critical path:
  1. Load dataset and select questions
  2. Prompt LLMs with questions and answer choices
  3. If using SocialAgent, pass question through each agent
  4. Ensemble agent outputs using LLM decision
  5. Generate final answer
  6. Evaluate accuracy against ground truth

- Design tradeoffs:
  - Dataset size vs. granularity: Larger datasets provide more comprehensive coverage but may be harder to manage
  - Agent complexity vs. performance: More sophisticated agents may improve results but increase computational cost
  - Curriculum alignment vs. generalizability: K-12 alignment enables direct comparison but may limit applicability to broader contexts

- Failure signatures:
  - Low accuracy across all skills: Dataset may be too challenging or models may lack fundamental understanding
  - Inconsistent performance across skills: Dataset may have uneven difficulty or skills may not be well-defined
  - High variance in SocialAgent performance: Agent integration may be unstable or LLMs may not effectively utilize agent outputs

- First 3 experiments:
  1. Evaluate baseline LLM performance on dataset without any additional agents
  2. Test each SocialAgent component individually to assess their individual contributions
  3. Compare ensemble vs. individual agent performance to optimize integration strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific components or techniques could be added to the SocialAgent framework to further improve LLMs' understanding of social norms, particularly for higher-grade questions and complex reasoning scenarios?
- Basis in paper: [inferred] The paper mentions that SocialAgent improves performance but models still underperform humans on higher grades and struggle with long answers and complex reasoning.
- Why unresolved: The paper only explores a basic multi-agent framework with retrieval, programming, and reasoning agents. It does not investigate more advanced or specialized components.
- What evidence would resolve it: Experiments comparing SocialAgent to variants with additional components (e.g., specialized reasoning agents, external knowledge integration, fine-tuning) on higher-grade questions and complex reasoning tasks.

### Open Question 2
- Question: How does the performance of SocialAgent vary across different LLM architectures and sizes, and what are the implications for resource-constrained deployment?
- Basis in paper: [explicit] The paper shows that larger LLaMA2 models perform better and that SocialAgent improves performance across different sizes.
- Why unresolved: The paper does not provide a detailed analysis of performance scaling with model size or compare different architectures (e.g., encoder-decoder vs. decoder-only).
- What evidence would resolve it: Comprehensive benchmarking of SocialAgent across a wide range of LLM architectures and sizes, including analysis of performance-resource trade-offs.

### Open Question 3
- Question: To what extent does the SocialAgent framework generalize to other domains beyond social norms, and what modifications would be needed for optimal performance?
- Basis in paper: [inferred] The paper focuses on social norms but the multi-agent framework could potentially be applied to other domains.
- Why unresolved: The paper does not explore the generalizability of SocialAgent to other tasks or domains.
- What evidence would resolve it: Experiments applying SocialAgent to other benchmark datasets (e.g., MMLU, ScienceQA) and analyzing performance gains and required modifications.

## Limitations

- The evaluation methodology relies on a closed-book format that may not fully capture the practical application of social norms in real-world contexts.
- The comparison with elementary students may not account for the different ways humans and LLMs process and apply social knowledge.
- The effectiveness of the multi-agent framework depends heavily on the quality of the underlying LLMs and the integration of external knowledge sources.

## Confidence

- High confidence in the dataset's comprehensive coverage of social norm skills and its alignment with K-12 curriculum
- Medium confidence in the effectiveness of the multi-agent framework, given the limited analysis of its components and performance variations
- Medium confidence in the comparison with human elementary students, due to potential differences in evaluation methodologies and contexts

## Next Checks

1. Conduct an ablation study on the SocialAgent framework to quantify the individual contributions of each agent and identify potential redundancies or conflicts in their outputs.
2. Perform a detailed analysis of LLM performance across different types of social norm questions (e.g., cultural norms vs. legal norms) to identify specific areas of strength and weakness.
3. Compare the performance of LLMs and elementary students on a subset of questions using identical evaluation methodologies to validate the fairness and accuracy of the comparison.