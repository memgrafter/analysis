---
ver: rpa2
title: 'Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision Models
  For Video Captioning and Summarization'
arxiv_id: '2405.20648'
source_url: https://arxiv.org/abs/2405.20648
tags:
- video
- arxiv
- language
- captioning
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose Shotluck Holmes, a family of small-scale large
  language vision models for video captioning and summarization. They fine-tune existing
  small LLVMs (TinyLLaVA-1.5B and TinyLLaVA-3.1B) on the Shot2Story20K dataset to
  extend their capabilities from understanding single images to understanding sequences
  of frames.
---

# Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision Models For Video Captioning and Summarization

## Quick Facts
- arXiv ID: 2405.20648
- Source URL: https://arxiv.org/abs/2405.20648
- Authors: Richard Luo; Austin Peng; Adithya Vasudev; Rishabh Jain
- Reference count: 31
- Key outcome: A family of small-scale LLVMs (1.5B, 3.1B parameters) that outperform the 7B parameter Shot2Story model on video captioning and summarization tasks while being 78% smaller

## Executive Summary
Shotluck Holmes proposes a family of efficient small-scale large language vision models for video captioning and summarization tasks. By fine-tuning existing small LLVMs (TinyLLaVA-1.5B and TinyLLaVA-3.1B) on the Shot2Story20K dataset, the authors demonstrate that these compact models can achieve state-of-the-art performance on both single-shot video captioning and multi-shot video summarization. The approach replaces Shot2Story's multi-stage pipeline with a unified single-stage architecture, achieving 50-100% performance gains over the larger Shot2Story model while significantly reducing computational complexity.

## Method Summary
The approach involves preprocessing the Shot2Story20K dataset to create shot-level entries with corresponding captions and ASR transcriptions. Video frames are sampled (either uniformly or using head-tail sampling) and converted to tensors using a vision encoder (SigLIP). These tensors are then processed through small-scale LLVMs (TinyLLaVA-1.5B or TinyLLaVA-3.1B) with ASR text as additional context. The models are fine-tuned using Adam8bit optimizer with a global batch size of 128, learning rate of 2e-5, and weight decay of 0 for 1 epoch. The unified instruction template allows simultaneous fine-tuning for both single-shot captioning and multi-shot summarization tasks.

## Key Results
- Shotluck Holmes 1.5B model achieves 50-100% gains over Shot2Story on single-shot video captioning
- The 3.1B model outperforms Shot2Story on multi-shot video summarization while being 78% smaller
- State-of-the-art performance across multiple evaluation metrics (BLEU, METEOR, ROUGE, CIDEr) on both single-shot and multi-shot tasks
- Significant computational efficiency improvements through reduced model size and single-stage architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing Shot2Story20K's multi-stage pipeline with a single fine-tuned TinyLLaVA model reduces computational complexity while maintaining or improving performance.
- Mechanism: The Shot2Story20K pipeline requires separate models for shot detection, captioning, and summarization. By using TinyLLaVA's unified vision-language architecture, the entire pipeline can be condensed into a single forward pass, reducing both memory usage and inference latency.
- Core assumption: The pre-trained TinyLLaVA model has sufficient cross-modal reasoning capabilities that can be adapted through fine-tuning on the Shot2Story20K dataset.
- Evidence anchors:
  - [abstract]: "we show that it is sufficient to replace the final two stages of Shot2Story20K's three-stage vision-language model pipeline with TinyLLaVA"
  - [section]: "By leveraging more efficient small-scale LLVMs like TinyLLaVA, we hope to achieve strong performance on single-shot video captioning and multi-shot video summarization with significantly reduced computational complexity"
  - [corpus]: Weak - no direct comparison of multi-stage vs. single-stage architectures in related work

### Mechanism 2
- Claim: Fine-tuning smaller LLVMs on specialized video datasets can achieve competitive or superior performance compared to larger models.
- Mechanism: The Shot2Story20K dataset provides rich video understanding tasks with ground truth captions and summaries. Fine-tuning smaller models like TinyLLaVA on this task-specific data allows them to learn effective video understanding patterns without requiring the massive parameter count of larger models.
- Core assumption: The quality and specificity of the Shot2Story20K dataset compensates for the smaller model size by providing more targeted training signals.
- Evidence anchors:
  - [abstract]: "Shotluck Holmes achieves better performance than state-of-the-art results on the Shot2Story video captioning and summary task with significantly smaller and more computationally efficient models"
  - [section]: "Shotluck Holmes presents a family of small LLMs finetuned on the Shot2Story20K dataset"
  - [corpus]: Moderate - TinyLLaVA framework shows small-scale LLVMs can achieve better performance than larger models on various vision-language tasks

### Mechanism 3
- Claim: The unified instruction template approach allows simultaneous fine-tuning for both single-shot captioning and multi-shot summarization tasks.
- Mechanism: By using the same instruction format for both tasks during fine-tuning, the model learns to adapt its output style based on the context provided (single shot vs. full video), creating a more flexible system that can handle both tasks without task-specific architectures.
- Core assumption: The model can learn to distinguish between task requirements based on input context rather than requiring separate training procedures.
- Evidence anchors:
  - [section]: "Our dataset included both single-shot and multi-shot video examples and the model was simultaneously finetuned to do both"
  - [section]: "Our dataset included both single-shot and multi-shot video examples and the model was simultaneously finetuned to do both"
  - [corpus]: Weak - limited evidence of multi-task fine-tuning approaches in video understanding literature

## Foundational Learning

- Concept: Vision-language model fine-tuning
  - Why needed here: The paper relies on adapting pre-trained TinyLLaVA models to video understanding tasks through fine-tuning
  - Quick check question: What is the difference between supervised fine-tuning and continued pre-training in the context of vision-language models?

- Concept: Video frame sampling and preprocessing
  - Why needed here: The approach requires converting videos into tensor representations that can be processed by vision encoders
  - Quick check question: Why might uniform sampling be preferred over random sampling for video frame extraction in this context?

- Concept: Multimodal instruction tuning
  - Why needed here: The model needs to learn how to process both visual and textual inputs simultaneously to generate appropriate captions and summaries
  - Quick check question: How does the instruction template structure help the model understand the relationship between video frames and ASR transcriptions?

## Architecture Onboarding

- Component map: Video frames → Vision encoder (SigLIP) → MLP mapping layer → LLM (TinyLlama or Phi-2) → Output text

- Critical path: Video frames → Vision encoder → MLP → LLM → Output text
  The vision encoder and MLP layers are frozen during training, making the LLM the primary trainable component

- Design tradeoffs:
  - Model size vs. performance: Smaller models (1.5B) achieve competitive results with significantly reduced computational requirements
  - Sampling strategy: Trade-off between uniform sampling (simpler) and head-tail sampling (better temporal coverage)
  - Fine-tuning scope: Freezing vision encoder layers vs. full fine-tuning for computational efficiency

- Failure signatures:
  - Hallucination in generated captions when visual information is ambiguous
  - Inconsistent performance between single-shot and multi-shot tasks
  - Degraded performance on videos with complex audio-visual interactions

- First 3 experiments:
  1. Baseline evaluation: Run inference on Shot2Story20K test set using pre-trained TinyLLaVA without fine-tuning to establish baseline performance
  2. Fine-tuning ablation: Compare performance when fine-tuning only the LLM vs. fine-tuning the entire model
  3. Sampling strategy comparison: Evaluate the impact of uniform vs. head-tail frame sampling on final metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would specialized fine-tuning on single-shot and multi-shot tasks separately affect performance compared to the current hybrid approach?
- Basis in paper: [explicit] The authors mention that training on a hybrid dataset prevents specialized fine-tuning on each task, which could result in greater performance gains.
- Why unresolved: The paper only reports results from a hybrid fine-tuning approach and doesn't provide evidence for the potential improvements from task-specific training.
- What evidence would resolve it: Training two separate models with identical architectures but fine-tuned exclusively on either single-shot or multi-shot videos, then comparing their performance metrics to the hybrid model's results.

### Open Question 2
- Question: What is the optimal sampling strategy and frame count for converting videos to tensors for these models?
- Basis in paper: [explicit] The authors experimented with uniform and head-tail sampling methods and chose N=120 frames based on hardware constraints, but note this choice was pragmatic rather than optimal.
- Why unresolved: The paper doesn't explore different sampling strategies systematically or test various frame counts to determine the optimal configuration for video understanding.
- What evidence would resolve it: A comprehensive ablation study varying both sampling methods (including more sophisticated approaches) and frame counts (e.g., 60, 120, 240, 480) while measuring performance and computational efficiency trade-offs.

### Open Question 3
- Question: How would incorporating temporal attention mechanisms between shots improve multi-shot video summarization performance?
- Basis in paper: [inferred] The authors acknowledge that their approach may overemphasize individual shots and suggest that a conversation feed over shots would allow for proper attention mapping over sequences.
- Why unresolved: The current model architecture doesn't explicitly model the temporal relationships between shots, potentially missing important narrative connections.
- What evidence would resolve it: Implementing and comparing models with temporal attention mechanisms (such as positional encoding across shots or transformer-based shot-to-shot attention) against the current architecture on the multi-shot video summarization task.

### Open Question 4
- Question: How does the performance of Shotluck Holmes scale with model size beyond 3.1B parameters?
- Basis in paper: [explicit] The authors only tested 1.5B and 3.1B parameter models and noted that the 1.5B model performed particularly well despite being 78% smaller than Shot2Story.
- Why unresolved: The paper doesn't explore whether even smaller models could be effective or if larger models would provide additional performance gains.
- What evidence would resolve it: Training and evaluating models at various scales (e.g., 0.5B, 7B, 13B) on the same tasks to identify the optimal parameter count for balancing performance and efficiency.

### Open Question 5
- Question: What is the impact of different vision encoder architectures on the overall model performance?
- Basis in paper: [explicit] The authors used SigLIP as their vision encoder based on prior research, but note this was inherited from TinyLLaVA rather than being experimentally determined.
- Why unresolved: The paper doesn't explore alternative vision encoders like CLIP or other architectures that might better capture video-specific features.
- What evidence would resolve it: Replacing the SigLIP encoder with alternative vision models while keeping all other components constant, then measuring the impact on video captioning and summarization performance.

## Limitations

- Performance improvements are evaluated on a single dataset (Shot2Story20K), raising questions about generalizability to other video understanding tasks
- The paper lacks runtime measurements for the different model variants, making it difficult to verify the claimed computational efficiency gains
- The evaluation methodology has gaps, including unclear fairness of comparing models fine-tuned on the same test set

## Confidence

**High confidence**: The technical implementation details (fine-tuning procedure, model architecture, sampling strategies) are clearly specified and reproducible. The quantitative results showing performance improvements over the baseline Shot2Story model are well-documented.

**Medium confidence**: The claimed computational efficiency improvements are supported by model size comparisons but lack runtime measurements. The generalizability of results beyond the Shot2Story20K dataset is uncertain.

**Low confidence**: The qualitative analysis of model outputs and failure modes is minimal. The impact of the unified instruction template approach on task performance is not rigorously validated through ablation studies.

## Next Checks

1. **Ablation study validation**: Conduct a controlled experiment comparing Shotluck Holmes performance when removing each component (shot detection pipeline, caption generation, summarization) versus the full Shot2Story pipeline to isolate the true source of performance gains.

2. **Cross-dataset generalization**: Evaluate Shotluck Holmes on at least two additional video captioning/summarization datasets (such as How2 or TVSum) to assess whether the performance improvements generalize beyond Shot2Story20K.

3. **Runtime benchmarking**: Measure actual inference time and memory usage for Shotluck Holmes versus Shot2Story across different hardware configurations to verify the claimed computational efficiency benefits with empirical data.