---
ver: rpa2
title: 'REPEAT: Improving Uncertainty Estimation in Representation Learning Explainability'
arxiv_id: '2412.08513'
source_url: https://arxiv.org/abs/2412.08513
tags:
- repeat
- uncertainty
- importance
- learning
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REPEAT, a new method for explaining unsupervised
  representations in representation learning XAI (R-XAI). Unlike prior approaches
  that measure variability in importance scores, REPEAT directly models whether a
  pixel is certainly important by treating each pixel as a Bernoulli random variable.
---

# REPEAT: Improving Uncertainty Estimation in Representation Learning Explainability

## Quick Facts
- arXiv ID: 2412.08513
- Source URL: https://arxiv.org/abs/2412.08513
- Reference count: 37
- Primary result: REPEAT achieves 1.000 AUROC for OOD detection on VOC vs 0.000 for RELAX

## Executive Summary
REPEAT introduces a novel method for explaining unsupervised representations in representation learning explainability (R-XAI) by directly modeling pixel importance certainty through Bernoulli random variables. Unlike prior approaches that measure variability in importance scores, REPEAT generates multiple stochastic estimates from base R-XAI methods, thresholds them to create binary samples, and estimates pixel importance probabilities with associated uncertainty. The method demonstrates superior performance in out-of-distribution detection, produces more concise uncertainty estimates, and shows strong generalization across different base R-XAI methods.

## Method Summary
REPEAT treats each pixel as a Bernoulli random variable where the mean represents importance probability and variance represents uncertainty. The method leverages stochastic base R-XAI methods (like RELAX or Kernel SHAP) to generate multiple importance estimates, applies thresholding to convert these into binary samples (important/unimportant), and then estimates pixel importance probabilities from these samples. This framework directly models certainty rather than measuring variability, allowing REPEAT to distinguish between pixels that are always important versus those that fluctuate around importance thresholds.

## Key Results
- REPEAT achieves perfect OOD detection performance (1.000 AUROC) on PASCAL-VOC vs 0.000 for RELAX
- REPEAT produces more concise uncertainty estimates with lower entropy (9.97 vs 10.82 for RELAX)
- Strong performance in detecting poisoned data (0.86 AUROC for uncertainty vs 0.59 for importance)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: REPEAT directly models certainty in pixel importance by treating each pixel as a Bernoulli random variable, rather than measuring variability in importance scores.
- Mechanism: For each pixel, REPEAT generates multiple importance estimates through stochastic base R-XAI methods, then thresholds these scores to create binary samples (important/unimportant). From these samples, it estimates the probability of a pixel being important and calculates uncertainty as the variance of the Bernoulli distribution.
- Core assumption: The stochasticity in base R-XAI methods combined with repeated thresholding provides sufficient diversity in importance estimates to accurately model pixel importance probabilities.
- Evidence anchors:
  - [abstract] "REPEAT directly models whether a pixel is certainly important by treating each pixel as a Bernoulli random variable"
  - [section] "REPEAT leverages the stochasticity of current R-XAI methods to produce multiple estimates of importance, thus considering each pixel in an image as a Bernoulli random variable"

### Mechanism 2
- Claim: REPEAT provides more intuitive uncertainty estimates that better align with human judgment about pixel importance.
- Mechanism: By modeling certainty directly rather than variability, REPEAT can distinguish between pixels that are always important (low uncertainty) versus pixels that fluctuate around the importance threshold (high uncertainty), even if their average importance scores are similar.
- Core assumption: Human intuition about pixel importance correlates more with certainty of importance rather than variability in importance scores.
- Evidence anchors:
  - [abstract] "REPEAT gives certainty estimates that are more intuitive, better at detecting out-of-distribution data"
  - [section] "Consider an estimated importance map, where all pixels with importance scores higher than 2 are considered important. Now, take one pixel with importance value 5.6 ±0.1 and another with importance value 5.6 ±1.2. Due to the higher variance of the second pixel, current R-XAI methods would assign high uncertainty to this pixel. However, since all values within the 95% confidence interval of the pixel would still be above the importance threshold, we would still be certain that this pixel is important"

### Mechanism 3
- Claim: REPEAT generalizes to different base R-XAI methods while maintaining strong performance.
- Mechanism: REPEAT's framework is modular - it takes any stochastic R-XAI method as input, generates binary samples through thresholding, and estimates importance probabilities. This allows it to leverage the strengths of various base methods while adding certainty estimation.
- Core assumption: The thresholding process and Bernoulli modeling are compatible with different base R-XAI methods' output distributions and stochastic behaviors.
- Evidence anchors:
  - [section] "REPEAT is more general and can be used with any stochastic R-XAI method. To illustrate this, we have conducted the same OOD detection and complexity experiments as earlier but with Kernel-SHAP (Lundberg and Lee 2017) as the base stochastic R-XAI method"
  - [section] "The performance for OOD detection is very similar for RELAX compared to Kernel-SHAP, but RELAX gives lower complexity compared to using Kernel-SHAP as the base R-XAI method"

## Foundational Learning

- Concept: Bernoulli random variables and their properties (mean and variance)
  - Why needed here: REPEAT treats each pixel as a Bernoulli RV where the mean represents importance probability and variance represents uncertainty
  - Quick check question: If a pixel has importance probability 0.8, what is its uncertainty according to REPEAT's framework?

- Concept: Stochasticity in machine learning models and its role in uncertainty estimation
  - Why needed here: REPEAT leverages stochastic base R-XAI methods to generate diverse importance estimates for reliable probability estimation
  - Quick check question: Why is stochasticity in the base R-XAI method critical for REPEAT's performance?

- Concept: Image thresholding techniques and their properties
  - Why needed here: REPEAT uses thresholding to convert continuous importance scores into binary samples for Bernoulli modeling
  - Quick check question: How might different thresholding methods (Otsu, mean, triangle) affect the quality of binary samples in REPEAT?

## Architecture Onboarding

- Component map: Input image → Feature extractor → Base R-XAI method (stochastic) → Multiple importance maps → Histogram → Thresholding (mean, Otsu, triangle, Li) → Binary samples → Bernoulli probability estimation → Importance and uncertainty outputs
- Critical path: The sequence from base R-XAI method through thresholding to Bernoulli probability estimation is the core of REPEAT's innovation
- Design tradeoffs: Higher number of repeats (K) improves probability estimation accuracy but increases computational cost; different thresholding methods offer different biases in binary sample generation
- Failure signatures: Poor OOD detection performance, high complexity scores, or uncertainty estimates that don't align with human judgment indicate issues with the Bernoulli modeling or thresholding
- First 3 experiments:
  1. Verify that REPEAT produces reasonable importance maps on a simple dataset with known important pixels
  2. Compare REPEAT's uncertainty estimates against human judgments of pixel importance certainty
  3. Test REPEAT with different base R-XAI methods to confirm generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does REPEAT perform when applied to other types of stochasticity beyond RELAX, such as Dropout-based methods?
- Basis in paper: [explicit] The paper demonstrates REPEAT's flexibility by testing it with Kernel SHAP, but only briefly mentions the potential for other stochastic R-XAI methods.
- Why unresolved: The paper only evaluates Kernel SHAP as an alternative base method, leaving the performance with other stochastic approaches unexplored.
- What evidence would resolve it: Experiments comparing REPEAT's performance across a broader range of stochastic R-XAI methods (e.g., Dropout-based, Monte Carlo methods) would clarify its generalizability.

### Open Question 2
- Question: Can REPEAT's thresholding mechanism be further optimized for specific datasets or tasks?
- Basis in paper: [inferred] The paper uses mean thresholding as a default but mentions that Otsu's method produced higher thresholds in their example, suggesting potential variability in performance.
- Why unresolved: The paper evaluates only four standard thresholding methods without exploring dataset-specific optimizations or adaptive thresholding strategies.
- What evidence would resolve it: Comparative studies of adaptive thresholding methods tailored to specific datasets or tasks would reveal whether optimization improves REPEAT's performance.

### Open Question 3
- Question: How does REPEAT scale with larger datasets or more complex models, such as those used in large-scale vision tasks?
- Basis in paper: [explicit] The paper evaluates REPEAT on datasets like PASCAL-VOC and MS-COCO but does not address scalability to larger datasets or more complex architectures.
- Why unresolved: The computational efficiency and performance of REPEAT on large-scale datasets or advanced models (e.g., Vision Transformers with more parameters) remain untested.
- What evidence would resolve it: Scalability studies involving larger datasets (e.g., ImageNet) and more complex models would provide insights into REPEAT's practical applicability.

## Limitations
- Limited validation on real-world scenarios where certainty about pixel importance might be more nuanced
- Reliance on thresholding introduces sensitivity to parameter selection
- Comparison against only one baseline (RELAX) limits generalizability claims

## Confidence

- **High confidence**: The mathematical framework of treating pixels as Bernoulli random variables is sound and well-specified. The out-of-distribution detection performance (1.000 AUROC) on VOC is impressive and reproducible.
- **Medium confidence**: The claim that REPEAT provides more intuitive uncertainty estimates than variability-based methods is supported by the mechanism description but lacks direct human validation studies.
- **Low confidence**: The generalizability claims across different base R-XAI methods are based on limited testing with only Kernel SHAP, and the method's performance on diverse real-world datasets beyond the tested ones remains unknown.

## Next Checks
1. Conduct a user study comparing REPEAT's uncertainty estimates against human judgments of pixel importance certainty on images with known important features, measuring alignment between algorithmic and human assessments.

2. Systematically test REPEAT with additional base R-XAI methods (e.g., LIME, DeepLIFT) and analyze how different stochastic behaviors affect the quality of Bernoulli probability estimates and overall performance.

3. Evaluate REPEAT on realistic out-of-distribution scenarios including natural distribution shifts, adversarial examples, and domain adaptation tasks to verify that the strong synthetic performance generalizes to practical applications.