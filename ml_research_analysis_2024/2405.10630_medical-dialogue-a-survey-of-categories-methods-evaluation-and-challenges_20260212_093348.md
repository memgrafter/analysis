---
ver: rpa2
title: 'Medical Dialogue: A Survey of Categories, Methods, Evaluation and Challenges'
arxiv_id: '2405.10630'
source_url: https://arxiv.org/abs/2405.10630
tags:
- medical
- arxiv
- dialogue
- systems
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews medical dialogue systems from
  a technical perspective, covering system categories, methods (both pre-LLM and LLM-based),
  evaluation metrics, and datasets. It identifies key challenges including hallucination,
  numerical data processing, adversarial attacks, medical specialization, evaluation
  gaps, multi-modal dialogue, and multidisciplinary treatment.
---

# Medical Dialogue: A Survey of Categories, Methods, Evaluation and Challenges

## Quick Facts
- arXiv ID: 2405.10630
- Source URL: https://arxiv.org/abs/2405.10630
- Authors: Xiaoming Shi; Zeming Liu; Li Du; Yuxuan Wang; Hongru Wang; Yuhang Guo; Tong Ruan; Jie Xu; Shaoting Zhang
- Reference count: 40
- This survey systematically reviews medical dialogue systems from a technical perspective, covering system categories, methods (both pre-LLM and LLM-based), evaluation metrics, and datasets. It identifies key challenges including hallucination, numerical data processing, adversarial attacks, medical specialization, evaluation gaps, multi-modal dialogue, and multidisciplinary treatment. The work provides an organized overview of 325 papers and highlights current limitations in LLM evaluation, particularly the lack of rigorous clinical scenario assessments. The survey aims to bridge the gap between advanced NLP techniques and practical medical requirements while proposing potential solutions for future research directions.

## Executive Summary
This survey provides the first comprehensive technical review of medical dialogue systems, organizing 325 papers into systematic categories covering system types, methods, evaluation approaches, and datasets. The work distinguishes between pre-LLM methods (retrieval-based, pipeline generation, end-to-end generation, hybrid) and LLM-based approaches (prompting, fine-tuning), while identifying grand challenges specific to medical applications. The survey addresses a critical gap in the literature by providing technical rather than application-focused analysis, offering researchers a structured framework for understanding the current state of medical dialogue systems and identifying future research directions.

## Method Summary
The survey systematically reviews medical dialogue systems by first categorizing them into system types (diagnosis, intervention, monitoring, counseling, education, multi-objective), then examining pre-LLM methods (retrieval-based, pipeline generation, end-to-end generation, hybrid approaches) and LLM-based methods (prompting, fine-tuning). It analyzes evaluation metrics and datasets, and identifies grand challenges inherited from general domain and specific to medical applications. The authors organize resources including open-source implementations, corpora, and paper lists to facilitate researcher onboarding. The work draws from 325 papers and provides a structured framework for understanding technical approaches and limitations in medical dialogue systems.

## Key Results
- Comprehensive technical categorization of 325 medical dialogue papers into system types, methods, and evaluation approaches
- Systematic identification of grand challenges including hallucination, numerical data processing, adversarial attacks, medical specialization, evaluation gaps, multi-modal dialogue, and multidisciplinary treatment
- Organization of medical dialogue resources (open-source implementations, corpora, paper lists) to facilitate researcher onboarding
- Critical analysis of LLM limitations in medical contexts, particularly regarding numerical reasoning and medical specialization
- Call for unified evaluation frameworks for medical LLMs, especially for diagnostic capabilities in real clinical scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey provides systematic technical categorization of medical dialogue systems that bridges the gap between advanced NLP techniques and practical medical requirements.
- Mechanism: By organizing 325 papers into system categories, methods (pre-LLM and LLM-based), evaluation metrics, and datasets, the survey creates a structured knowledge framework that enables researchers to understand the current state of the field and identify specific research directions.
- Core assumption: That organizing existing literature into a coherent technical framework will accelerate progress in medical dialogue systems by making it easier for researchers to identify gaps and build upon existing work.
- Evidence anchors:
  - [abstract]: "Although these systems have been surveyed in the medical community from an application perspective, a systematic review from a rigorous technical perspective has to date remained noticeably absent."
  - [section]: The paper explicitly states it is "the first to present a comprehensive survey for medical dialogue systems from a technical perspective, summarizing categories, methods before LLM, and LLM-based methods."
  - [corpus]: Weak - corpus only found 25 related papers with average citations of 0.1, suggesting this type of comprehensive technical survey may be relatively novel in the field.
- Break condition: If the technical categorization becomes outdated as new methods emerge faster than the survey can be updated, or if the categories prove to be too rigid to accommodate novel approaches.

### Mechanism 2
- Claim: The identification of grand challenges provides a roadmap for future research directions in medical dialogue systems.
- Mechanism: By explicitly listing challenges inherited from general domain (hallucination, numerical data processing, adversarial attacks) and medical-specific challenges (medical specialization, evaluation, multi-modal dialogue, multi-disciplinary treatment), the survey creates a prioritized agenda for the research community.
- Core assumption: That clearly articulating the most pressing challenges will focus research efforts on the most impactful problems rather than spreading resources across less critical areas.
- Evidence anchors:
  - [abstract]: "This paper lists the grand challenges of medical dialog systems, especially of large language models."
  - [section]: The paper dedicates an entire section to "Grand Challenge" that systematically categorizes challenges into inherited and medical-specific ones.
  - [corpus]: Weak - corpus doesn't directly address challenge identification, but the novelty of the approach is implied by the lack of similar comprehensive challenge lists in the literature.
- Break condition: If the identified challenges prove to be mischaracterized or if new challenges emerge that weren't anticipated, making the roadmap less relevant.

### Mechanism 3
- Claim: The organization of resources (open-source implementations, corpora, paper lists) lowers the barrier to entry for new researchers in the field.
- Mechanism: By providing a centralized repository of resources, the survey reduces the time and effort required for new researchers to become productive in medical dialogue research, potentially accelerating the field's growth.
- Core assumption: That reducing the onboarding friction for new researchers will lead to increased participation and faster innovation in the field.
- Evidence anchors:
  - [abstract]: "We make the first attempt to organize medical dialogue resources including open-source implementations, corpora, and paper lists, which may help new researchers quickly adapt to this field."
  - [section]: The paper mentions organizing "medical dialogue resources" as part of its contributions.
  - [corpus]: Weak - corpus doesn't directly address resource organization, but the claim of being the "first" to do this suggests it's a novel contribution.
- Break condition: If the organized resources become outdated or if the field becomes too specialized for a general resource list to be useful, reducing its value to new researchers.

## Foundational Learning

- Concept: Medical domain knowledge and terminology
  - Why needed here: Medical dialogue systems require understanding of medical concepts, symptoms, diagnoses, and treatments to function effectively. Without this knowledge, systems cannot accurately interpret patient input or provide appropriate responses.
  - Quick check question: Can you explain the difference between a symptom and a sign in medical terminology?

- Concept: Dialogue system architectures (task-oriented vs. chit-chat vs. mixed-type)
  - Why needed here: Different medical dialogue functions (diagnosis, monitoring, counseling) require different dialogue system architectures. Understanding these distinctions is crucial for selecting appropriate approaches.
  - Quick check question: What are the key architectural differences between task-oriented and chit-chat dialogue systems?

- Concept: Large language model capabilities and limitations
  - Why needed here: The survey emphasizes the impact of LLMs on medical dialogue systems, including their strengths (instruction following, generation quality) and weaknesses (hallucination, numerical reasoning, medical specialization).
  - Quick check question: What are the main reasons LLMs struggle with medical specialization compared to general conversation?

## Architecture Onboarding

- Component map: Medical dialogue systems consist of data collection (corpora), model development (pre-LLM methods: retrieval, pipeline generation, end-to-end generation, hybrid; LLM-based methods: prompting, fine-tuning), evaluation (automatic metrics, human evaluation, benchmarks), and challenge identification. The system categories include diagnosis, intervention, monitoring, counseling, education, and multi-objective functions, with dialogue types being task-oriented, dialogue recommendation, chit-chat, question-answering, and mixed-type.
- Critical path: Data → Model Development → Evaluation → Challenge Resolution. The most critical path is from data collection to model development, as the quality and quantity of medical dialogue data directly impacts model performance.
- Design tradeoffs: Retrieval-based methods offer efficiency but may lack precision; generation-based methods offer flexibility but suffer from hallucination; LLM-based methods offer strong performance but require careful prompting/fine-tuning and face medical specialization challenges. The choice depends on the specific application requirements and available resources.
- Failure signatures: Hallucination in generated responses, inability to process numerical medical data correctly, failure to maintain context in multi-turn dialogues, poor performance on medical specialization tasks, vulnerability to adversarial attacks.
- First 3 experiments:
  1. Implement a simple retrieval-based medical dialogue system using BioASQ dataset to establish baseline performance and identify limitations.
  2. Fine-tune a pre-trained language model on the MedDialog dataset using supervised learning to assess generation capabilities and hallucination tendencies.
  3. Apply chain-of-thought prompting to a large language model for medical question-answering on the CMB dataset to evaluate reasoning capabilities in medical contexts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective evaluation metrics for assessing medical LLMs' diagnostic capabilities in real clinical scenarios?
- Basis in paper: [explicit] The paper identifies that current LLM evaluations lack assessments of capabilities in clinical scenarios and calls for designing unified and comprehensive evaluation criteria such as LLM-Mini-CEX for evaluating diagnostic capabilities in real clinical applications.
- Why unresolved: Current evaluations focus on medical information extraction and question-answering but neglect multi-turn diagnostic interviewing and rigorous diagnostic results assessment.
- What evidence would resolve it: Development and validation of standardized evaluation frameworks that incorporate multi-turn diagnostic dialogues and clinical outcome measures, with demonstrated correlation to actual clinical performance.

### Open Question 2
- Question: How can multi-modal medical dialogue systems effectively integrate image data with text interactions for accurate diagnosis?
- Basis in paper: [explicit] The paper highlights that current systems rely on text-only interactions and lack multi-modal information, potentially leading to incorrect diagnoses, and proposes multi-modal systems to understand and process multi-modal inputs.
- Why unresolved: Technical challenges in combining visual and textual medical data, and lack of large-scale multi-modal medical datasets for training and evaluation.
- What evidence would resolve it: Demonstrated improvement in diagnostic accuracy when using multi-modal inputs compared to text-only systems, validated on diverse medical conditions requiring visual examination.

### Open Question 3
- Question: What are the most effective methods for mitigating numerical data processing errors in medical LLMs?
- Basis in paper: [explicit] The paper identifies that medical dialogue systems involve statistical data processing and LLMs' probability-based generation leads to numerical errors, proposing external tool integration as a potential solution.
- Why unresolved: LLMs' fundamental limitations in handling precise numerical calculations and lack of standardized approaches for integrating external computational tools.
- What evidence would resolve it: Comparative studies showing significant reduction in numerical errors when using LLM-external tool integration versus standalone LLMs, with maintained overall system performance.

## Limitations

- The survey's comprehensive technical categorization may become outdated quickly as new methods emerge faster than the literature can be systematically reviewed
- Limited empirical validation of the organized resource repository's practical utility for new researchers entering the field
- Sparse citation network of related work (average neighbor citations of 0.1) suggests potential novelty but also indicates limited peer validation of the approach

## Confidence

- Technical categorization framework: Medium
- Grand challenges roadmap: Medium
- Resource organization impact: Low

## Next Checks

1. Verify the completeness of the 325-paper corpus by comparing against independent literature searches in major medical NLP venues
2. Test the practical utility of the organized resource repository by having new researchers attempt to use it for project onboarding
3. Validate the challenge identification by surveying active medical dialogue researchers on whether the identified challenges align with their experience and priorities