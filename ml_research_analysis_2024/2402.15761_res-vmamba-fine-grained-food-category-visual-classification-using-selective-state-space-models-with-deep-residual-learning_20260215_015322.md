---
ver: rpa2
title: 'Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective
  State Space Models with Deep Residual Learning'
arxiv_id: '2402.15761'
source_url: https://arxiv.org/abs/2402.15761
tags:
- food
- image
- recognition
- classification
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-grained food category
  classification, which requires distinguishing between visually similar food types.
  The authors introduce Res-VMamba, a novel approach that integrates residual learning
  into the VMamba model, a selective state space model that has shown superior performance
  and computational efficiency compared to traditional transformer architectures.
---

# Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning

## Quick Facts
- arXiv ID: 2402.15761
- Source URL: https://arxiv.org/abs/2402.15761
- Authors: Chi-Sheng Chen; Guan-Ying Chen; Dong Zhou; Di Jiang; Dai-Shi Chen
- Reference count: 23
- Primary result: 79.54% classification accuracy on CNFOOD-241 without pre-trained weights

## Executive Summary
This paper addresses the challenge of fine-grained food category classification by introducing Res-VMamba, which integrates residual learning into the VMamba selective state space model. The approach aims to enhance the model's ability to capture fine-grained details in food images by combining global and local state features. Evaluated on the CNFOOD-241 dataset with 241 categories and nearly 200,000 images, Res-VMamba achieves state-of-the-art performance of 79.54% top-1 accuracy without pre-trained weights, establishing a new benchmark for fine-grained food classification.

## Method Summary
Res-VMamba integrates residual learning into the VMamba model by adding global residual connections from raw input to feature maps after VSS blocks. The model uses a VMamba-S backbone with 150 training epochs, AdamW optimizer (learning rate 1e-3 with cosine decay), batch size 128, weight decay 0.05, label smoothing 0.1, and exponential moving average. The architecture processes 600x600 pixel images through a stem module that partitions inputs into patches while maintaining 2D structure, followed by four stages of Vision State Space blocks with patch merging operations between stages.

## Key Results
- Achieves 79.54% top-1 classification accuracy on CNFOOD-241 test set
- Outperforms state-of-the-art models without using pre-trained weights
- Demonstrates effectiveness of combining global residual connections with selective state space modeling for fine-grained food classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual learning enhances VMamba by enabling direct global-local feature sharing, improving fine-grained classification.
- Mechanism: By integrating raw image data into the feature map via global residual connections, Res-VMamba allows both unprocessed global features and hierarchically learned local features to be processed together, improving representational capacity.
- Core assumption: Global image features complement the sequential state space modeling, and their integration does not disrupt the attention-free efficiency of Mamba.
- Evidence anchors:
  - [abstract]: "pioneer the integration of a residual learning framework within the VMamba model to concurrently harness both global and local state features"
  - [section 3.5]: "we propose a new type of VMamba model, Res-Vmamba, a Mamba with residual learning mechanism... integrates raw data directly into the feature map"
  - [corpus]: Weak evidence - no directly comparable papers; mentions "Selective Feature Extraction" but not residual learning in SSM context.
- Break condition: If residual connections introduce harmful gradient paths or excessive memory usage, or if the sequential Mamba mechanism is unable to process the mixed global/local signal effectively.

### Mechanism 2
- Claim: VMamba's selective state space mechanism (S6) outperforms transformers in fine-grained visual classification by capturing both local and global context efficiently.
- Mechanism: The S6 mechanism uses input-conditioned state matrices to selectively propagate information along both spatial dimensions, enabling long-range dependencies without the quadratic cost of self-attention.
- Core assumption: The input-dependent parameterization of SSM matrices is sufficient to model complex visual relationships in fine-grained datasets.
- Evidence anchors:
  - [abstract]: "VMamba model, which incorporates the Mamba mechanism into image tasks... currently establishes the state-of-the-art (SOTA) on the ImageNet dataset"
  - [section 3.3]: "The VMamba model introduces a novel Selective Scan Mechanism (S6), diverging from traditional LTI State Space Models"
  - [corpus]: No direct corpus evidence for VMamba in fine-grained tasks; closest is DGFamba (domain generalization) and InsectMamba (pest classification).
- Break condition: If selective state spaces cannot capture subtle intra-class variations, or if the model overfits to training data without generalization.

### Mechanism 3
- Claim: CNFOOD-241's high resolution and class imbalance make it a challenging benchmark that stresses models' fine-grained classification ability.
- Mechanism: The dataset's uniform 600x600 resolution preserves detail, while imbalanced class distribution forces models to learn discriminative features rather than relying on frequency bias.
- Core assumption: High resolution is more beneficial than dataset size alone for fine-grained tasks, and imbalance is representative of real-world scenarios.
- Evidence anchors:
  - [section 4.1.1]: "CNFOOD-241 dataset possesses the largest... uniform-sized (600 ×600) image collection among publicly available food datasets"
  - [section 4.1.2]: "CNFOOD-241 exhibits a greater degree of class imbalance compared to other datasets, which consequently enhances the challenging nature of this dataset"
  - [corpus]: No direct corpus evidence; related datasets (e.g., InsectMamba) mention camouflage and species diversity as analogous challenges.
- Break condition: If high resolution causes memory bottlenecks that prevent effective training, or if class imbalance leads to severe performance degradation on minority classes.

## Foundational Learning

- Concept: State Space Models (SSMs) as continuous-time linear systems
  - Why needed here: VMamba is built on SSMs; understanding discretization and state evolution is critical to grasping how the model processes image sequences.
  - Quick check question: What is the main mathematical representation of an SSM, and why is discretization necessary for image tasks?

- Concept: Selective Scan Mechanism (S6) and Cross-Scan Module (CSM)
  - Why needed here: These are core to VMamba's efficiency and spatial integration; without them, the model loses its advantage over transformers.
  - Quick check question: How does the S6 mechanism differ from traditional LTI SSMs, and what role does the CSM play?

- Concept: Residual learning and skip connections
  - Why needed here: Res-VMamba introduces global residual connections; understanding how residuals preserve gradient flow and feature reuse is essential.
  - Quick check question: What problem does residual learning solve in deep networks, and how might it apply differently in SSM-based architectures?

## Architecture Onboarding

- Component map: Input → Stem → VSS Block (Stage 1) → Patch Merge → VSS Block (Stage 2) → Patch Merge → VSS Block (Stage 3) → Patch Merge → VSS Block (Stage 4) → Classifier

- Critical path:
  Input → Stem → VSS Block (Stage 1) → Patch Merge → VSS Block (Stage 2) → Patch Merge → VSS Block (Stage 3) → Patch Merge → VSS Block (Stage 4) → Classifier

- Design tradeoffs:
  - VMamba vs. Transformer: Linear time/space vs. quadratic; less parallelizable but more scalable to long sequences
  - Residual learning: Adds global context but increases memory and may disrupt sequential state evolution
  - Dataset choice: High resolution (600x600) preserves detail but increases compute; imbalance stresses generalization

- Failure signatures:
  - Overfitting: High training accuracy but low validation/test accuracy; check class imbalance handling
  - Memory overflow: During patch merging or large batch training; consider gradient checkpointing
  - Vanishing gradients: In very deep Res-VMamba; monitor gradient norms and consider normalization

- First 3 experiments:
  1. Train VMamba-S on CNFOOD-241 without pretraining; compare top-1 accuracy to baseline CNNs/ViTs
  2. Add global residual connections (Res-VMamba) and evaluate accuracy gain and training stability
  3. Analyze class-wise performance to identify if imbalance or resolution is the main bottleneck

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Res-VMamba's performance scale with model size compared to VMamba?
- Basis in paper: [inferred] The paper mentions that Res-VMamba's effectiveness when scaling up the VMamba model remains uncertain.
- Why unresolved: The authors did not conduct experiments with larger VMamba variants (e.g., VMamba-L or VMamba-H) due to resource constraints.
- What evidence would resolve it: Training and evaluating Res-VMamba with larger VMamba architectures on the CNFOOD-241 dataset would demonstrate scaling behavior.

### Open Question 2
- Question: Does Res-VMamba's global residual mechanism provide benefits beyond fine-grained food classification?
- Basis in paper: [inferred] The global residual mechanism is introduced specifically for fine-grained food classification, but its general applicability is not explored.
- Why unresolved: The paper focuses solely on food classification and does not test Res-VMamba on other fine-grained or general image classification tasks.
- What evidence would resolve it: Evaluating Res-VMamba on other fine-grained datasets (e.g., CUB-200-2011, Stanford Cars) and general image classification benchmarks would reveal its broader effectiveness.

### Open Question 3
- Question: How does the computational efficiency of Res-VMamba compare to other state-of-the-art models for food classification?
- Basis in paper: [inferred] While the paper discusses VMamba's superior computational efficiency compared to transformers, it does not provide detailed efficiency comparisons for Res-VMamba.
- Why unresolved: The paper focuses on accuracy comparisons and does not report on inference speed, memory usage, or computational complexity metrics.
- What evidence would resolve it: Measuring and comparing inference time, memory consumption, and FLOPs for Res-VMamba against other SOTA models would provide a complete efficiency analysis.

## Limitations
- Claims about Res-VMamba's superiority are based on a single dataset without ablation studies or comparisons against non-pretrained baselines
- Paper lacks detailed architectural specifications for global residual connections and doesn't report training stability metrics or class-wise performance analysis
- Assertion that CNFOOD-241's class imbalance meaningfully stresses model generalization is not empirically validated through minority class performance metrics

## Confidence
- High: VMamba architecture details and CNFOOD-241 dataset statistics
- Medium: Res-VMamba's design rationale and overall accuracy improvement claims
- Low: Claims about why specific architectural choices work and dataset difficulty characterization

## Next Checks
1. **Ablation Study Required:** Remove the global residual connections from Res-VMamba and retrain to quantify the exact contribution of this component to the 79.54% accuracy.

2. **Class-Wise Performance Analysis:** Generate confusion matrices and per-class precision/recall metrics to verify if the claimed improvement is uniform across all 241 categories or concentrated in specific food types.

3. **Generalization Testing:** Evaluate Res-VMamba on at least two other fine-grained food classification datasets (e.g., Food-101, VireoFood-172) to validate claims about the architecture's broad applicability beyond CNFOOD-241.