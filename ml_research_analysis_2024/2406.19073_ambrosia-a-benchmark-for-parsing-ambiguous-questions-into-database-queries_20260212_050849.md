---
ver: rpa2
title: 'AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database Queries'
arxiv_id: '2406.19073'
source_url: https://arxiv.org/abs/2406.19073
tags:
- ambiguity
- questions
- queries
- ambiguous
- database
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AMBROSIA is a new benchmark for parsing ambiguous natural language\
  \ questions into SQL database queries. The dataset contains questions showcasing\
  \ three types of ambiguity\u2014scope, attachment, and vagueness\u2014along with\
  \ their interpretations and corresponding SQL queries."
---

# AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database Queries

## Quick Facts
- arXiv ID: 2406.19073
- Source URL: https://arxiv.org/abs/2406.19073
- Authors: Irina Saparina; Mirella Lapata
- Reference count: 40
- Primary result: AMBROSIA is a new benchmark for parsing ambiguous natural language questions into SQL database queries, revealing that even advanced models struggle significantly with ambiguity (31% recall on ambiguous questions vs 66% on unambiguous ones)

## Executive Summary
AMBROSIA introduces a novel benchmark for evaluating text-to-SQL models on their ability to parse ambiguous natural language questions into SQL database queries. The benchmark contains questions showcasing three types of ambiguity—scope, attachment, and vagueness—along with their interpretations and corresponding SQL queries. To create realistic databases that support ambiguity, the authors developed a novel approach involving controlled generation of databases from scratch using large language models. When benchmarked on AMBROSIA, various LLMs including GPT-4o, Llama3-70B, and OpenChat demonstrate substantially lower performance on ambiguous questions compared to unambiguous ones, with the best model achieving only 31% recall on ambiguous questions.

## Method Summary
The AMBROSIA benchmark is created through a three-step process: first, multi-table SQL databases are generated using a large language model to support specific types of ambiguity; second, ambiguous questions are written by crowd workers based on these databases; and third, SQL queries are generated for each interpretation of the ambiguous questions. The dataset contains 846 multi-table databases across 16 domains, with ambiguous questions that showcase scope, attachment, and vagueness types of ambiguity. The evaluation framework uses recall, precision, and AllFound metrics adapted specifically for ambiguous questions, measuring whether models can identify ambiguity and generate all possible interpretations as SQL queries.

## Key Results
- All benchmarked models show substantially higher recall on unambiguous questions (66% average) compared to ambiguous questions (31% average)
- Llama3-70B performs best on ambiguous questions with 31% recall, followed by GPT-4o at 27% recall
- Models tend to overestimate ambiguity, classifying 67% of unambiguous questions as ambiguous
- Explicit instructions acknowledging ambiguity significantly improve performance compared to standard text-to-SQL instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM-based controlled generation approach creates realistic databases that support ambiguity.
- Mechanism: The process generates databases from scratch by first identifying key concepts and relations that support the chosen ambiguity type, then generating SQL statements to build the database structure and populate it with data.
- Core assumption: LLMs can generate syntactically correct and semantically meaningful SQL CREATE TABLE and INSERT INTO statements that result in realistic databases.
- Evidence anchors:
  - [abstract] "To create realistic databases that support ambiguity, the authors developed a novel approach involving controlled generation of databases from scratch using large language models."
  - [section 3.3] "We use a large language model for generation (OpenChat; Wang et al. 2024) and view database creation as a semantic parsing problem."
  - [corpus] Weak - corpus focuses on graph queries and not on database generation methods specifically
- Break condition: If generated SQL statements produce syntax errors or databases that don't support the intended ambiguity type, the approach fails.

### Mechanism 2
- Claim: The benchmark design with multiple ambiguity types and interpretations exposes model limitations in ambiguity resolution.
- Mechanism: By providing ambiguous questions with multiple interpretations and corresponding SQL queries, the benchmark forces models to either recognize ambiguity and generate all possible interpretations or fail by producing incomplete or biased results.
- Core assumption: Models that perform well on standard text-to-SQL benchmarks will be challenged by the requirement to handle ambiguity.
- Evidence anchors:
  - [abstract] "We benchmark various LLMs on AMBROSIA, revealing that even the most advanced models struggle to identify and interpret ambiguity in questions."
  - [section 4.1] "As can be seen, all models demonstrate substantially higher recall on unambiguous questions compared to ambiguous ones"
  - [corpus] Moderate - corpus papers discuss ambiguity in queries but focus more on detection/resolution rather than benchmarking approaches
- Break condition: If models consistently generate all interpretations for ambiguous questions, the benchmark wouldn't expose their limitations.

### Mechanism 3
- Claim: The dataset's structure with multi-table databases in various domains provides realistic evaluation scenarios.
- Mechanism: By creating databases across 16 domains with 846 multi-table databases, the benchmark tests model generalization across different contexts and complexity levels.
- Core assumption: Real-world applications require text-to-SQL systems to handle diverse domains and database structures.
- Evidence anchors:
  - [section 3.2] "We create  with these considerations in mind, following three steps: we select a domain and generate a database that supports one of the above ambiguity types"
  - [section 3.5] "Table 1 shows dataset statistics for  compared to two other text-to-SQL datasets"
  - [corpus] Moderate - corpus papers mention domain-specific queries but don't discuss comprehensive domain coverage
- Break condition: If the generated databases are too simple or don't reflect real-world complexity, the benchmark loses its practical relevance.

## Foundational Learning

- Concept: Types of linguistic ambiguity (scope, attachment, vagueness)
  - Why needed here: The benchmark specifically targets these three types of ambiguity to test model capabilities comprehensively
  - Quick check question: Can you explain the difference between scope ambiguity (which elements a quantifier refers to) and attachment ambiguity (how a modifier is attached to the sentence)?

- Concept: SQL query execution and equivalence
  - Why needed here: The evaluation relies on executing SQL queries to verify they produce distinct results, and understanding when different SQL queries are equivalent
  - Quick check question: How would you determine if two SQL queries are non-equivalent based on their execution results?

- Concept: Text-to-SQL semantic parsing evaluation metrics
  - Why needed here: The benchmark uses recall, precision, and AllFound metrics specifically adapted for ambiguous questions
  - Quick check question: Why is recall particularly important for evaluating performance on ambiguous questions compared to standard text-to-SQL benchmarks?

## Architecture Onboarding

- Component map: Database generation pipeline (LLM-based) -> Question writing (human annotators) -> SQL query creation (template-based/human-written) -> Model evaluation (zero-shot/few-shot)
- Critical path: Database generation → Question writing → SQL query creation → Model evaluation
- Design tradeoffs: Using LLM-generated databases allows scalability but may introduce noise; human annotation ensures quality but is expensive and time-consuming
- Failure signatures: Low recall on ambiguous questions, bias toward specific interpretations, inability to recognize ambiguity
- First 3 experiments:
  1. Test zero-shot performance of Llama3-70B on unambiguous vs ambiguous questions to establish baseline gap
  2. Evaluate different prompting strategies (Prompt vs Beam) to see which better handles ambiguity
  3. Test few-shot learning with varying numbers of examples to measure improvement potential

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of database schema complexity on model performance in parsing ambiguous questions?
- Basis in paper: [inferred] The paper discusses the controlled generation of databases supporting ambiguity and mentions that databases generally have simple and clear names, whereas in reality, they might be incomplete, have abbreviations, and so on.
- Why unresolved: The current experiments use databases with simple schemas, and the paper acknowledges that real-world databases might be more complex.
- What evidence would resolve it: Experiments testing model performance on databases with varying levels of schema complexity, including realistic features like abbreviations and incomplete information.

### Open Question 2
- Question: How does the performance of models vary when asked to detect ambiguity versus parse ambiguous questions into SQL queries?
- Basis in paper: [explicit] The paper includes a section on ambiguity detection where models are asked to identify ambiguous questions, showing that models tend to overestimate ambiguity.
- Why unresolved: The paper shows models overestimate ambiguity but doesn't fully explore how this detection task relates to their parsing performance.
- What evidence would resolve it: Comparative analysis of model performance on ambiguity detection tasks versus actual SQL query generation for ambiguous questions.

### Open Question 3
- Question: What is the effect of prompt format (explicit instructions vs. standard instructions) on model performance for different types of ambiguity?
- Basis in paper: [explicit] The paper experiments with both explicit instructions acknowledging ambiguity and standard text-to-SQL instructions, finding that Llama3-70B performs better with explicit instructions.
- Why unresolved: While the paper shows explicit instructions help, it doesn't explore how different prompt formats affect performance across various ambiguity types.
- What evidence would resolve it: Systematic comparison of different prompt formats (e.g., explicit definitions, in-context examples, temperature settings) across all ambiguity types.

## Limitations

- The LLM-based database generation approach may not capture the full complexity and nuance of real-world databases, potentially limiting the benchmark's practical relevance
- The human annotation process for creating ambiguous questions lacks detailed specification of annotator expertise and quality control measures
- The evaluation framework relies on execution results to verify SQL query equivalence, which may not capture all semantic nuances of query differences

## Confidence

- **High confidence**: The core finding that current LLMs struggle significantly with ambiguous questions (31% recall vs 66% on unambiguous questions) is well-supported by the experimental results and aligns with existing research on ambiguity handling limitations.
- **Medium confidence**: The three-step approach for creating ambiguous questions is methodologically sound, but the actual execution quality depends on factors not fully detailed in the paper (annotator expertise, quality control measures).
- **Medium confidence**: The controlled generation of databases using LLMs is innovative but untested in terms of reproducibility and consistency. The paper doesn't provide sufficient detail on how to validate the quality of generated databases.

## Next Checks

1. **Database Quality Validation**: Test the reproducibility of the LLM-based database generation by attempting to regenerate databases for the same ambiguity types and comparing their structural complexity, data distribution, and ability to support intended ambiguity.

2. **Human Annotation Quality Assessment**: Conduct inter-annotator agreement analysis on a subset of ambiguous questions to quantify consistency and identify potential biases in how different annotators create ambiguous questions and their interpretations.

3. **SQL Query Equivalence Testing**: Verify the robustness of the evaluation framework by testing whether different SQL queries that produce semantically equivalent results are correctly handled, and whether non-equivalent queries with similar structures are properly distinguished.