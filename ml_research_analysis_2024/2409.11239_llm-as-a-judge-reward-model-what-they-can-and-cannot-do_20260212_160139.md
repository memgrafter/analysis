---
ver: rpa2
title: 'LLM-as-a-Judge & Reward Model: What They Can and Cannot Do'
arxiv_id: '2409.11239'
source_url: https://arxiv.org/abs/2409.11239
tags:
- arxiv
- preprint
- korean
- evaluation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-as-a-Judge and reward models are widely used as scalable alternatives
  to human annotators for evaluating large language models, particularly for long-form
  responses and reinforcement learning alignment. However, their effectiveness in
  non-English contexts, factual verification, and challenging prompts remains underexplored.
---

# LLM-as-a-Judge & Reward Model: What They Can and Cannot Do

## Quick Facts
- arXiv ID: 2409.11239
- Source URL: https://arxiv.org/abs/2409.11239
- Reference count: 40
- Key outcome: Automated evaluators show strong English-to-Korean transfer but struggle with factual verification and challenging prompts.

## Executive Summary
This paper conducts a comprehensive analysis of LLM-as-a-Judge and reward models for evaluating large language models, focusing on their performance across languages, factual verification, and challenging prompts. Using the KUDGE bilingual dataset in Korean and English, the authors find that English evaluation capabilities strongly predict performance in other languages, with English-trained models transferring skills to Korean more effectively than Korean-specific benchmarks. However, state-of-the-art models fail to detect factual inaccuracies, cultural misrepresentations, and unwanted language, and struggle with complex reasoning tasks even in English. The results highlight significant limitations in current automated judging systems and underscore the need for improved evaluators.

## Method Summary
The study evaluates 20 LLMs using both pointwise and pairwise evaluation methods, generating responses with 31 LLMs and employing human annotation and review to establish ground truth. The KUDGE dataset provides bilingual evaluation data in Korean and English, with both original and challenge subsets. Statistical analysis examines model performance across languages and prompt types, measuring accuracy, Pearson correlation, evaluation time, agreement rates, and bias detection. The research combines qualitative and quantitative analysis to assess automated evaluators' capabilities and limitations.

## Key Results
- English evaluation capabilities strongly predict performance in other languages, with higher RÂ² values than language-specific benchmarks
- Models fail to detect factual inaccuracies, cultural misrepresentations, and unwanted language, indicating limitations as fact-checkers
- Both LLM-as-a-Judge and reward models struggle with challenging prompts, including complex STEM reasoning tasks, with no model surpassing 70% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: English evaluation capabilities strongly predict performance in other languages, enabling evaluators trained in English to transfer skills to Korean more effectively than Korean-specific benchmarks.
- Mechanism: Language-agnostic evaluation ability allows models to generalize judging skills across languages without requiring explicit training in the target language.
- Core assumption: A significant portion of evaluation ability is language-agnostic, meaning core reasoning and preference modeling skills transfer across languages.
- Evidence anchors:
  - [abstract] "English evaluation capabilities significantly influence language-specific evaluation capabilities, often more than the language proficiency itself"
  - [section 6.1] "Regression against REWARD BENCH scores yields a higher R2 value compared to KMMLU scores, suggesting that models better at English evaluations tend to perform well in Korean contexts"
  - [corpus] Weak evidence - no direct corpus support for this specific mechanism

### Mechanism 2
- Claim: Both LLM-as-a-Judge and reward models struggle with challenging prompts, including complex reasoning tasks in STEM, even in English.
- Mechanism: Complex reasoning tasks require problem-solving capabilities that exceed the model's own reasoning capacity, making it difficult to judge correctness.
- Core assumption: Evaluating a problem necessitates solving it, as the evaluator must determine the correctness of an answer.
- Evidence anchors:
  - [abstract] "state-of-the-art evaluators struggle with challenging prompts, in either English or Korean, underscoring their limitations in assessing or generating complex reasoning questions"
  - [section 8] "No model surpasses a 70% accuracy rate on these harder questions, a concerning performance given that random guessing would result in 50% accuracy"
  - [corpus] Weak evidence - no direct corpus support for this specific mechanism

### Mechanism 3
- Claim: Models fail to detect and penalize factual inaccuracies, cultural misrepresentations, and unwanted language.
- Mechanism: LLM-as-a-Judges are better at assessing logic and coherence than identifying truthfulness or detecting subtle factual errors.
- Core assumption: While models may effectively assess the logic or coherence of responses, they are less suitable for identifying the truthfulness or hallucinations in LLM outputs.
- Evidence anchors:
  - [abstract] "state-of-the-art models fail to detect and penalize factual inaccuracies, cultural misrepresentations, and unwanted language, indicating limitations as fact-checkers"
  - [section 7] "Both models perform best on paragraph-type errors, with Claude-3.5-Sonnet identifying nearly all, likely because these alterations significantly change the overall context, making them easily visible"
  - [corpus] Weak evidence - no direct corpus support for this specific mechanism

## Foundational Learning

- Concept: Language-agnostic evaluation ability
  - Why needed here: To understand why English-trained models can effectively evaluate Korean responses without explicit Korean training
  - Quick check question: What percentage of evaluation ability transfers across languages according to the paper's findings?

- Concept: Complex reasoning task evaluation limitations
  - Why needed here: To understand why even state-of-the-art models struggle with STEM and challenging prompts
  - Quick check question: What accuracy threshold did no model surpass on the GPQA subset?

- Concept: Factual error detection limitations
  - Why needed here: To understand why models struggle to identify and penalize factual inaccuracies
  - Quick check question: What type of factual error (word, sentence, paragraph) did models detect most successfully?

## Architecture Onboarding

- Component map: LLM-as-a-Judge (generative evaluation with feedback generation) -> Reward Models (classification-based scoring) -> Bilingual evaluation datasets (KUDGE original and challenge subsets)
- Critical path: 1) Generate responses for evaluation 2) Apply evaluator (LLM-as-a-Judge or Reward Model) 3) Compare against human annotations 4) Analyze performance across languages and prompt types
- Design tradeoffs: Generative evaluators provide detailed feedback but are resource-intensive, while classification-based reward models are efficient but may lack interpretability
- Failure signatures: Poor performance on challenging prompts, inability to detect subtle factual errors, bias toward English content in non-English evaluations
- First 3 experiments:
  1. Test transfer capability by evaluating Korean responses with English-trained models and comparing to Korean-specific benchmarks
  2. Evaluate model performance on progressively more challenging prompts (easy MMLU vs. hard GPQA) to identify reasoning limitations
  3. Test factual error detection by creating corrupted responses and measuring detection rates across different error types (word, sentence, paragraph)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors contribute to the transferability of evaluation capabilities from English to Korean?
- Basis in paper: [inferred]
- Why unresolved: The paper demonstrates that English evaluation capabilities significantly influence performance in Korean, but does not delve into the underlying reasons for this transferability.
- What evidence would resolve it: An in-depth analysis of the training data, model architectures, and evaluation methodologies that highlights the key factors enabling cross-linguistic transfer of evaluation skills.

### Open Question 2
- Question: How can automated evaluators be improved to effectively detect and penalize factual inaccuracies, cultural misrepresentations, and unwanted language?
- Basis in paper: [explicit]
- Why unresolved: The paper identifies these as critical shortcomings but does not propose specific solutions or methodologies for addressing these issues.
- What evidence would resolve it: Development and testing of new training techniques, datasets, or model architectures that demonstrate improved performance in detecting and penalizing these types of errors.

### Open Question 3
- Question: What are the limitations of current automated evaluators in assessing complex reasoning tasks, and how can they be overcome?
- Basis in paper: [explicit]
- Why unresolved: The paper highlights the struggle of automated evaluators with challenging prompts but does not explore the specific reasons for these limitations or potential solutions.
- What evidence would resolve it: A comprehensive study that identifies the specific challenges faced by evaluators in complex reasoning tasks and proposes innovative approaches to enhance their performance in these areas.

## Limitations

- Reliance on KUDGE dataset may not fully represent real-world evaluation diversity
- Meta-evaluation approach constrained by specific prompt and response types in dataset
- Study limited to Korean-English comparisons, potentially limiting generalizability to other languages

## Confidence

**High Confidence**: English evaluation capabilities predicting cross-linguistic performance (strong statistical support through regression analysis)
**Medium Confidence**: Models struggling with challenging prompts and factual error detection (well-supported by empirical results)
**Low Confidence**: Generalizability to other language pairs and evaluation contexts (study limited to Korean-English comparisons)

## Next Checks

1. **Cross-Lingual Transfer Validation**: Test whether models trained on English evaluation can effectively judge responses in additional languages (e.g., Spanish, Arabic) to determine if the transfer phenomenon extends beyond Korean.

2. **Factual Error Detection Sensitivity**: Systematically vary the subtlety of factual errors in responses to measure detection rates across different types (word, sentence, paragraph modifications) and determine detection thresholds.

3. **Challenging Prompt Generalization**: Evaluate whether improvements in STEM reasoning capabilities through fine-tuning or scaffolding techniques lead to better performance on challenging prompts, or if the limitation is fundamental to the evaluation paradigm itself.