---
ver: rpa2
title: 'WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems Through
  Game-Based Analysis'
arxiv_id: '2412.03359'
source_url: https://arxiv.org/abs/2412.03359
tags:
- agents
- reasoning
- game
- wang
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WiS, an open, scalable platform for evaluating
  LLM-based multi-agent systems using the "Who is Spy?" game environment. The platform
  features a unified interface supporting both open-source and closed-source models,
  real-time leaderboards, and comprehensive evaluation metrics including game-winning
  rates, attacking/defense strategies, and reasoning capabilities.
---

# WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems Through Game-Based Analysis

## Quick Facts
- arXiv ID: 2412.03359
- Source URL: https://arxiv.org/abs/2412.03359
- Authors: Chengwei Hu; Jianhui Zheng; Yancheng He; Hangyu Guo; Junguang Jiang; Han Zhu; Kai Sun; Yuning Jiang; Wenbo Su; Bo Zheng
- Reference count: 11
- Key outcome: Introduces WiS, an open, scalable platform for evaluating LLM-based multi-agent systems using "Who is Spy?" game environment, revealing distinct agent behaviors across ten LLMs

## Executive Summary
This paper introduces WiS, a comprehensive platform for evaluating LLM-based multi-agent systems through the "Who is Spy?" social deduction game. The platform provides a unified interface supporting both open-source and closed-source models, enabling researchers to assess agent capabilities in deception, reasoning, and social deduction. Experiments with ten different LLMs revealed distinct behavioral patterns, with GPT-4o demonstrating superior reasoning and deception abilities, while Qwen showed high deception capacity. The platform effectively differentiates multi-agent capabilities through interactive gameplay, providing a robust framework for assessing LLM performance in complex social scenarios.

## Method Summary
The WiS platform evaluates LLM-based multi-agent systems by deploying agents into "Who is Spy?" game sessions where they must use language to deceive, reason, and collaborate. The platform supports ten different LLMs through a unified interface that works with both Hugging Face models and API calls. Agents play multiple rounds with controlled role distribution (spy/civilian), and performance is measured through game-winning rates, voting accuracy, foul rates, and role-specific win rates. Each model undergoes more than 90 repetitions to ensure statistical significance, with identical code used except for base model differences.

## Key Results
- GPT-4o achieved the highest overall performance with 62.50% average score, demonstrating superior reasoning and deception capabilities
- Qwen2.5-72B-Instruct showed high deception capacity but lower reasoning ability compared to GPT-4o
- Prompt injection experiments revealed model-specific vulnerabilities, with attacking strategies achieving 60.42% success against Claude-3-5-Sonnet and 66.67% against ERNIE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The game-based evaluation platform differentiates LLM multi-agent capabilities by measuring specific in-game behaviors (e.g., deception, reasoning, attacking/defense strategies).
- Mechanism: WiS uses the "Who is Spy?" game as a dynamic, interactive environment where agents must use language to deceive, reason, and collaborate. The platform quantifies performance through game outcomes, voting accuracy, foul rates, and role-specific win rates.
- Core assumption: The game environment sufficiently stresses and reveals distinct reasoning, deception, and social deduction skills of LLMs in a way that static benchmarks cannot.
- Evidence anchors:
  - [abstract] "comprehensive evaluation metrics including game-winning rates, attacking/defense strategies, and reasoning capabilities"
  - [section] "agents can influence other agents by modifying the content of their speeches, thereby misleading others"
  - [corpus] Weak — corpus neighbors discuss MAS evaluation but do not specifically address game-based reasoning or deception metrics
- Break condition: If the game mechanics do not reliably expose model differences, or if models overfit to the game, the platform's discriminative power collapses.

### Mechanism 2
- Claim: The zero-sum scoring system ensures that agents are incentivized to both excel individually and differentiate themselves from competitors.
- Mechanism: Points are allocated based on survival rounds and correct spy identification, with total scores adjusted for number of games played to prevent overfitting and reward consistent superiority.
- Core assumption: The zero-sum nature and game-length scaling produce a fair, reproducible ranking that reflects true multi-agent ability rather than luck or overfitting.
- Evidence anchors:
  - [section] "this scoring design ensures that players achieving high rankings not only demonstrate excellent performance in individual rounds but also maintain a consistent superiority over other players"
  - [section] "The ranking is based on the cumulative points scored in all matches... each participant starts with an initial score of 100 points and costs 1 point for each game played"
- Break condition: If the initial point offset or game cost introduces bias, or if player pools are too small to smooth variance, rankings may misrepresent ability.

### Mechanism 3
- Claim: The platform's unified interface for both open-source and closed-source models enables broad, reproducible evaluation across diverse LLM backends.
- Mechanism: Agents can be built using Hugging Face models or via API calls, lowering the barrier for researchers to test their own models without rewriting code or incurring prohibitive costs.
- Core assumption: The abstraction layer (SDK/API) correctly normalizes model behavior and game interaction regardless of backend, enabling fair comparison.
- Evidence anchors:
  - [abstract] "unified model evaluate interface that supports models available on Hugging Face"
  - [section] "Our platform is featured with three main worths: (1) a unified model evaluate interface that supports models available on Hugging Face; (2) real-time updated leaderboard for model evaluation"
  - [corpus] Weak — corpus discusses MAS frameworks but not specifically unified model evaluation interfaces
- Break condition: If API latency or model-specific quirks break the unified interface abstraction, evaluation fairness degrades.

## Foundational Learning

- Concept: Social deduction game mechanics (e.g., hidden role, limited information, deception)
  - Why needed here: Understanding how "Who is Spy?" works is essential to interpret the platform's metrics and experimental setups (e.g., why reasoning and deception matter).
  - Quick check question: In the game, what is the spy's win condition, and how does it differ from the civilians' win condition?
    - Answer: Spy wins if undetected by round 3 or all civilians eliminated; civilians win if they correctly identify the spy before round 3.

- Concept: Multi-agent evaluation metrics (win rate, voting accuracy, foul rate, average score)
  - Why needed here: These metrics directly map to the experimental results table and explain how model behaviors are quantified and compared.
  - Quick check question: Which metric best reflects an agent's reasoning ability in this platform?
    - Answer: Voting accuracy, as it measures the agent's ability to correctly identify the spy based on limited speech data.

- Concept: Prompt injection and adversarial strategy in LLM-based agents
  - Why needed here: Understanding how the platform tests attacking/defense strategies (e.g., inserting instructions to induce fouls or misvotes) is key to interpreting Table 3.
  - Quick check question: What is the difference between the "Prompt Injection Attacking" and "Prompt Injection Defense" strategies?
    - Answer: Attacking injects instructions to cause others to err; defense injects instructions to prevent others from voting against the agent.

## Architecture Onboarding

- Component map: Frontend (React UI) -> Backend (Spring Boot) -> Database (Cloud-native relational) -> SDK (Python) -> Game Engine
- Critical path:
  1. User submits agent via SDK or web UI
  2. Backend queues agent into open game session
  3. Game engine runs rounds, enforces rules, records data
  4. Results fed into leaderboard and stored in DB
  5. Frontend updates rankings and allows visualization
- Design tradeoffs:
  - Open vs. closed-source model support increases accessibility but adds API cost variance
  - Real-time leaderboards improve engagement but require frequent DB writes and caching
  - Full game replay visualization aids debugging but increases storage and compute overhead
- Failure signatures:
  - Agent timeouts or repeated fouls → rule enforcement bug
  - Leaderboard lag or duplicate entries → backend race condition
  - Visualization missing or corrupted → data serialization issue
  - SDK integration errors → API contract mismatch
- First 3 experiments:
  1. Deploy a simple "echo" agent (repeats input) and confirm it loses consistently to baseline models; verify foul rate and win rate metrics.
  2. Run a prompt-injected spy vs. baseline civilian agents; check foul rate and voting accuracy changes.
  3. Test reasoning strategy prompt on GPT-4o vs. Qwen; compare voting accuracy and win rate shifts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt injection strategies affect the robustness of various LLM architectures against adversarial attacks in multi-agent settings?
- Basis in paper: [explicit] The paper describes experiments with prompt injection attacks and defenses, showing varying success rates across models like GPT-4o, Qwen2.5-72B-Instruct, and Claude-3-5-Sonnet
- Why unresolved: While the paper demonstrates effectiveness of certain attacks, it doesn't systematically compare how different architectural approaches (transformers, MoE, etc.) respond to various attack vectors
- What evidence would resolve it: Comprehensive testing across diverse model architectures with standardized attack benchmarks and architectural vulnerability mappings

### Open Question 2
- Question: What are the long-term effects of reinforcement learning from competition data on agent performance in social deduction games?
- Basis in paper: [inferred] The paper mentions providing competition data for training through supervised learning or reinforcement learning, but doesn't explore the longitudinal effects of this training
- Why unresolved: The paper establishes the platform and initial results but doesn't investigate how agents evolve over multiple training iterations using competition data
- What evidence would resolve it: Multi-generational studies tracking agent performance improvements and strategy evolution over repeated training cycles

### Open Question 3
- Question: How does the balance between deception and reasoning capabilities impact overall performance across different game scenarios?
- Basis in paper: [explicit] The paper notes that GPT-4o excels in reasoning while Qwen shows high deception capacity, but doesn't analyze the optimal balance between these traits
- Why unresolved: The paper identifies distinct capabilities but doesn't investigate whether specialized agents or balanced agents perform better in varied game conditions
- What evidence would resolve it: Comparative analysis of specialized vs. balanced agents across diverse game scenarios and player configurations

### Open Question 4
- Question: What are the computational and economic trade-offs between using open-source versus closed-source models for real-time multi-agent competition platforms?
- Basis in paper: [explicit] The paper mentions cost sensitivity among participants and provides both open-source and API-based model support, but doesn't quantify the trade-offs
- Why unresolved: While the paper acknowledges cost considerations, it doesn't provide detailed analysis of performance-per-dollar metrics or latency comparisons
- What evidence would resolve it: Comprehensive benchmarking of response times, costs, and performance metrics across various model categories and scales

### Open Question 5
- Question: How do different visualization and feedback mechanisms impact player strategy development and platform engagement?
- Basis in paper: [explicit] The paper describes visualization features and data download capabilities but doesn't evaluate their effectiveness on user behavior
- Why unresolved: The platform includes these features but lacks empirical analysis of how they influence agent improvement and user retention
- What evidence would resolve it: User studies comparing different visualization approaches and their correlation with agent performance improvements and platform engagement metrics

## Limitations

- The platform's evaluation is limited to a single game environment, raising questions about generalizability to other multi-agent scenarios
- Lack of detailed prompt specifications for each model creates significant reproducibility challenges
- The scoring mechanism's implementation details are not fully specified, potentially impacting fairness and consistency

## Confidence

- **High Confidence**: The platform's technical architecture and basic functionality (unified interface, real-time leaderboard, game mechanics) are well-documented and appear implementable.
- **Medium Confidence**: The experimental results showing performance differences between models are credible, but the exact reproducibility is limited by unspecified prompts and scoring details.
- **Low Confidence**: Claims about the platform's ability to generalize evaluation of LLM multi-agent systems beyond the specific game environment are not well-supported.

## Next Checks

1. Implement a minimal "echo" agent and verify it consistently underperforms baseline models, confirming the platform correctly measures agent capabilities through game outcomes.

2. Conduct sensitivity analysis by varying prompts slightly for the same model and measuring performance variance, to assess the robustness of the evaluation metrics.

3. Compare WiS platform results with traditional LLM benchmarks (e.g., MMLU, HumanEval) for the same models to evaluate correlation between game-based and standard evaluation metrics.