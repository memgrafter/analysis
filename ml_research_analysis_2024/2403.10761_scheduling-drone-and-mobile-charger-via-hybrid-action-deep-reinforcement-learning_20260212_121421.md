---
ver: rpa2
title: Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning
arxiv_id: '2403.10761'
source_url: https://arxiv.org/abs/2403.10761
tags:
- drone
- learning
- action
- actions
- charger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scheduling a drone and a
  mobile charger to maximize observation utility while minimizing task completion
  time. The problem involves a drone sequentially visiting points of interest (PoIs)
  for data collection, with a mobile charger providing wireless recharging at designated
  charging points.
---

# Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2403.10761
- **Source URL**: https://arxiv.org/abs/2403.10761
- **Reference count**: 40
- **Primary result**: Proposed HaDMC framework outperforms state-of-the-art RL methods (DQN, TD3, HPPO, HyAR) in maximizing observation utility while minimizing task completion time for drone-charger scheduling

## Executive Summary
This paper addresses the challenge of scheduling a drone and mobile charger to maximize observation utility while minimizing task completion time. The problem involves two agents with hybrid discrete-continuous action spaces: the drone must visit points of interest for data collection while managing battery constraints, and the charger must provide wireless recharging at designated locations. The authors propose HaDMC, a hybrid-action deep reinforcement learning framework that uses representation learning to convert the complex hybrid action space into a continuous latent space, enabling efficient off-policy training. The approach demonstrates superior performance compared to state-of-the-art methods through extensive experiments.

## Method Summary
The HaDMC framework tackles the multi-agent scheduling problem by converting discrete-continuous hybrid actions into a continuous latent space for RL training. The method uses a TD3-based policy network to generate latent actions, which are then decoded through an embedding table and adversarial autoencoder (AAE) to produce the original hybrid actions. A mutual learning scheme is embedded in the action decoder to emphasize collaborative actions between the drone and charger. The approach leverages off-policy training with experience replay and includes a pre-training phase for the action decoder components. Extensive experiments demonstrate HaDMC's effectiveness compared to baseline methods across various problem scales.

## Key Results
- HaDMC outperforms DQN, TD3, HPPO, and HyAR baselines in both convergence speed and objective value (observation efficiency)
- Ablation studies show both the AAE pipeline and mutual learning contribute to HaDMC's performance
- The framework effectively handles the interdependence between drone and charger actions through the proposed representation learning approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The embedding table efficiently maps high-dimensional continuous latent vectors to discrete actions while preserving differentiability for policy learning.
- Mechanism: The embedding table learns a continuous representation of discrete actions (qε[i] vectors) and uses arg min Euclidean distance to select the discrete action. This enables the policy network to output continuous vectors that are converted to discrete actions during inference while maintaining differentiability for training.
- Core assumption: The discrete action space is small enough (2m-1 actions) that storing and comparing against embedding vectors is computationally feasible.
- Evidence anchors:
  - [abstract] "An action decoder, comprising an embedding table and an adversarial autoencoder (AAE), translates latent actions into original discrete and continuous actions"
  - [section 5.3.2] "We leverage an embedding table qε with learnable parameter ε, which is pre-trained and can convert z to adis"
- Break condition: If the discrete action space becomes too large (m grows significantly), the embedding table comparison becomes computationally expensive and the learned representations may not generalize well.

### Mechanism 2
- Claim: The mutual learning scheme between embedding table and AAE improves collaborative action generation by sharing information between discrete and continuous action pipelines.
- Mechanism: During pre-training, the embedding table parameters are updated based on AAE's output (through reconstruction loss), while AAE learns from embedding table's output. This creates a feedback loop where both components learn to generate coordinated actions.
- Core assumption: The discrete and continuous actions are interdependent in a way that can be captured through shared learning objectives.
- Evidence anchors:
  - [abstract] "A mutual learning scheme is embedded in the action decoder to emphasize collaborative actions"
  - [section 5.4.2] "To address this issue, we introduce a mutual learning policy to the pre-training process: the embedding table updates its parameters based on the output of AAE, while AAE, in turn, learns from the output of embedding table"
- Break condition: If the interdependence between discrete and continuous actions is weak or non-existent, the mutual learning scheme may not provide additional benefit and could even harm performance.

### Mechanism 3
- Claim: The AAE module effectively learns to generate continuous actions by capturing complex distributions through adversarial training.
- Mechanism: The AAE uses an encoder-decoder structure with a discriminator to learn a latent representation of continuous actions. The adversarial training ensures the latent space follows a prior distribution while maintaining reconstruction capability.
- Core assumption: The continuous action space (observation time, charging time) has a complex distribution that cannot be adequately captured by simpler parametric forms.
- Evidence anchors:
  - [abstract] "an adversarial autoencoder (AAE), translates latent actions into original discrete and continuous actions"
  - [section 5.3.2] "AAE is more powerful than other types of autoencoders, as it is able to effectively learn about unknown distribution"
- Break condition: If the continuous action space is simple and can be well-approximated by a simple distribution, the complexity of AAE may be unnecessary and could lead to overfitting.

## Foundational Learning

- Concept: Representation learning for hybrid action spaces
  - Why needed here: The problem requires simultaneous discrete decisions (fly to PoI or charging point) and continuous decisions (observation time, charging duration), which standard RL approaches cannot handle directly
  - Quick check question: What are the dimensions of the hybrid action space in this problem? (Answer: Discrete actions for drone and charger, plus continuous observation/charging times)

- Concept: Multi-agent cooperative reinforcement learning
  - Why needed here: The drone and charger must coordinate their actions to maximize observation efficiency while ensuring the drone doesn't run out of battery
  - Quick check question: How does the reward function encourage collaboration between the drone and charger? (Answer: Through lookahead incentives and efficiency-based rewards)

- Concept: Actor-critic architecture for continuous control
  - Why needed here: The policy network must generate continuous latent actions that can be decoded into hybrid actions, requiring a framework that handles continuous action spaces effectively
  - Quick check question: What are the key components of the actor-critic structure used in HaDMC? (Answer: Actor generates latent actions, critics evaluate Q-values)

## Architecture Onboarding

- Component map: Policy Network (TD3-based) -> Embedding Table/AAE -> Environment -> Reward -> Policy Update

- Critical path: Policy Network → Embedding Table/AE → Environment → Reward → Policy Update

- Design tradeoffs:
  - Embedding table vs direct discretization: Embedding table provides better gradient flow but requires pre-training
  - AAE vs simpler continuous action generation: AAE captures complex distributions but adds training complexity
  - Mutual learning: Improves collaboration but requires careful balancing of learning rates

- Failure signatures:
  - Poor convergence during training (DQN/TD3 baselines fail)
  - Action decoder not learning meaningful mappings (outputs random actions)
  - Mutual learning causing instability (oscillating training)

- First 3 experiments:
  1. Verify embedding table learns meaningful discrete action mappings by checking if different z vectors produce different adis outputs
  2. Test AAE reconstruction capability by feeding known continuous actions through encoder-decoder and checking reconstruction error
  3. Validate mutual learning by comparing performance with and without mutual learning scheme in pre-training

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but suggests future work directions including extending HaDMC to multiple drones and chargers, and exploring performance in more complex urban environments.

## Limitations
- Pre-training phase requires random sampling from action space, which may be infeasible in real-world scenarios with safety constraints
- Embedding table approach assumes discrete action space remains small (2m-1 actions), limiting scalability
- Mutual learning scheme introduces additional hyperparameters that may require careful tuning

## Confidence

- **High**: Core RL framework (TD3-based policy network), AAE architecture for continuous actions, overall problem formulation
- **Medium**: Embedding table effectiveness, mutual learning benefits, pre-training procedure
- **Low**: Scalability claims, performance in highly dynamic environments

## Next Checks
1. **Scalability Test**: Evaluate HaDMC with increasing numbers of PoIs (m > 20) to verify embedding table performance doesn't degrade
2. **Safety Validation**: Test whether the pre-training phase with random actions can be replaced with safer sampling strategies in constrained environments
3. **Robustness Check**: Evaluate performance when charger mobility is limited or when PoI importance weights change dynamically during execution