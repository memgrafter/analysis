---
ver: rpa2
title: "Risk-sensitive control as inference with R\xE9nyi divergence"
arxiv_id: '2411.01827'
source_url: https://arxiv.org/abs/2411.01827
tags:
- control
- optimal
- risk-sensitive
- policy
- divergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a connection between control as inference\
  \ and risk-sensitive control by extending the variational inference framework with\
  \ R\xE9nyi divergence. The key finding is that control as inference with R\xE9nyi\
  \ divergence variational inference is equivalent to log-probability regularized\
  \ risk-sensitive control, where the order parameter of R\xE9nyi divergence controls\
  \ the risk-sensitivity of the resulting policy."
---

# Risk-sensitive control as inference with Rényi divergence

## Quick Facts
- arXiv ID: 2411.01827
- Source URL: https://arxiv.org/abs/2411.01827
- Authors: Kaito Ito; Kenji Kashima
- Reference count: 40
- This paper establishes that control as inference with Rényi divergence variational inference is equivalent to log-probability regularized risk-sensitive control.

## Executive Summary
This paper presents Risk-sensitive Control as Inference (RCaI), a novel framework that unifies control as inference and risk-sensitive control by extending variational inference with Rényi divergence. The key innovation is that the Rényi divergence order parameter controls the risk-sensitivity of the resulting policy, allowing for both risk-averse and risk-seeking behaviors. The authors prove that the optimal risk-sensitive policy can be obtained by solving a soft Bellman equation and demonstrate that RCaI unifies several existing control frameworks as special cases.

## Method Summary
The method extends standard control as inference by replacing Kullback-Leibler divergence with Rényi divergence in the variational inference framework. The authors derive risk-sensitive policy gradient and soft actor-critic methods based on this framework, where the risk-sensitivity parameter η appears as a coefficient in the gradients. The optimal policy is obtained by solving a soft Bellman equation that incorporates the Rényi divergence parameter. The framework is validated on a pendulum swing-up task with varying system parameters to demonstrate robustness to environmental changes.

## Key Results
- RCaI is theoretically equivalent to log-probability regularized risk-sensitive control
- The risk-sensitive optimal policy can be obtained by solving a soft Bellman equation
- RCaI unifies maximum entropy control, linearly-solvable control, and optimal posterior for control as inference as special cases
- Risk-sensitive policies show improved robustness to system perturbations compared to risk-neutral policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RCaI extends traditional control as inference by using Rényi divergence instead of KL divergence for variational inference
- Mechanism: The Rényi divergence parameter α (or equivalently η = α - 1) controls the risk-sensitivity of the resulting policy, with α > 1 (η > 0) producing risk-averse behavior and α < 1 (η < 0) producing risk-seeking behavior
- Core assumption: The optimal policy can be obtained by solving a soft Bellman equation where the Rényi divergence parameter appears as a coefficient
- Evidence anchors: [abstract] "RCaI is shown to be equivalent to log-probability regularized risk-sensitive control" [section] "For notational simplicity, we will drop = 1 for Ot in the remainder of this paper" [corpus] Weak evidence - no direct mention of Rényi divergence in corpus papers
- Break condition: The soft Bellman equation may not have a solution if the required integral diverges, or if the system dynamics are too complex for the policy to approximate

### Mechanism 2
- Claim: The optimal policy under RCaI takes the form of a Gibbs distribution with energy given by the Q-function
- Mechanism: The Q-function can be computed using a soft Bellman equation that incorporates the Rényi divergence parameter, allowing for both risk-averse and risk-seeking behaviors depending on the parameter value
- Core assumption: The Q-function exists and can be computed recursively, and the policy space contains policies that can approximate the optimal solution
- Evidence anchors: [abstract] "We also prove that the risk-sensitive optimal policy can be obtained by solving a soft Bellman equation" [section] "The recursive computation (4), (5) is similar to the Bellman equation for the risk-seeking control" [corpus] No direct mention of soft Bellman equations in corpus papers
- Break condition: If the Q-function cannot be computed accurately (e.g., due to function approximation errors), the resulting policy may not achieve optimal risk-sensitivity

### Mechanism 3
- Claim: RCaI unifies several control frameworks including maximum entropy control, linearly-solvable control, and the optimal posterior for control as inference
- Mechanism: As the Rényi divergence parameter approaches zero, RCaI recovers the risk-neutral maximum entropy control; for deterministic systems, it becomes linearly-solvable; for stochastic systems with η approaching -1, it recovers the optimal posterior from standard control as inference
- Core assumption: The various control frameworks are special cases of RCaI with specific parameter values
- Evidence anchors: [abstract] "As the risk-sensitivity parameter vanishes, we recover the risk-neutral CaI and RL, which means that RCaI is a unifying framework" [section] "Since Rényi divergence includes the KL divergence, RCaI is a unifying framework of CaI" [corpus] Weak evidence - no direct mention of unifying frameworks in corpus papers
- Break condition: If the parameter values do not converge to the specified limits correctly, the unification properties may not hold

## Foundational Learning

- Concept: Variational inference using Rényi divergence
  - Why needed here: The paper extends control as inference by replacing KL divergence with Rényi divergence, which requires understanding the properties of Rényi divergence and its role in variational inference
  - Quick check question: What is the main difference between Rényi divergence and KL divergence, and how does the order parameter α affect the behavior of the divergence?

- Concept: Risk-sensitive control and exponential utility
  - Why needed here: The paper establishes equivalence between RCaI and risk-sensitive control with exponential utility, which requires understanding how risk-sensitivity is modeled and how it affects control policies
  - Quick check question: How does the risk-sensitivity parameter η affect the behavior of a policy in terms of risk aversion or seeking?

- Concept: Soft Bellman equations and dynamic programming
  - Why needed here: The optimal policy under RCaI is obtained by solving a soft Bellman equation, which requires understanding how soft Bellman equations differ from standard Bellman equations and how they are solved
  - Quick check question: What is the main difference between a soft Bellman equation and a standard Bellman equation, and how does this difference affect the solution?

## Architecture Onboarding

- Component map: System dynamics -> Risk-sensitive SAC algorithm -> Soft Bellman equation solver -> Policy representation (Gibbs distribution) -> Risk-sensitivity parameter controller

- Critical path:
  1. Initialize system dynamics, cost functions, and risk-sensitivity parameter
  2. Compute Q-function using soft Bellman equation solver
  3. Derive policy from Q-function using Gibbs distribution
  4. Evaluate policy performance and adjust risk-sensitivity parameter if needed

- Design tradeoffs:
  - Function approximation accuracy vs. computational efficiency for solving soft Bellman equations
  - Policy expressiveness vs. sample efficiency for learning
  - Risk-sensitivity parameter tuning vs. robustness to environmental changes

- Failure signatures:
  - Numerical instability when |η| is too large (exponential growth in gradients)
  - Poor performance when the policy space cannot approximate the optimal Gibbs distribution
  - Convergence issues when the soft Bellman equation does not have a unique solution

- First 3 experiments:
  1. Compare RCaI with standard control as inference on a simple pendulum environment with different risk-sensitivity parameters
  2. Test the robustness of RCaI policies to system perturbations by changing the pendulum length
  3. Analyze the effect of different Rényi divergence orders on policy exploration and exploitation balance

## Open Questions the Paper Calls Out
None explicitly stated in the provided material.

## Limitations
- The soft Bellman equation solution may become numerically unstable for large |η| values due to exponential terms
- The assumption that the policy space can adequately approximate the optimal Gibbs distribution is not empirically validated
- The claimed unification properties rely on parameter limits that may not be practically achievable

## Confidence
- Theoretical derivation of RCaI framework: High
- Equivalence between RCaI and risk-sensitive control: Medium
- Practical advantages over existing methods: Low

## Next Checks
1. Test RCaI on continuous control tasks beyond pendulum (e.g., HalfCheetah, Walker2D) to evaluate scalability and robustness
2. Conduct ablation studies varying η systematically to characterize the risk-sensitivity behavior across different environments
3. Compare computational efficiency and sample complexity against established risk-sensitive RL methods on benchmark problems