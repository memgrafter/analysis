---
ver: rpa2
title: Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings
arxiv_id: '2402.17135'
source_url: https://arxiv.org/abs/2402.17135
tags:
- reward
- learning
- functions
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Functional Reward Encoding (FRE) approach
  for zero-shot reinforcement learning, enabling an agent to learn from unlabeled
  offline trajectories and immediately adapt to new tasks without additional training.
  The core idea is to encode arbitrary reward functions into a latent space using
  a transformer-based variational auto-encoder, allowing the agent to generalize to
  new tasks by simply encoding a few reward-annotated samples.
---

# Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings

## Quick Facts
- arXiv ID: 2402.17135
- Source URL: https://arxiv.org/abs/2402.17135
- Reference count: 20
- Key outcome: Introduces FRE for zero-shot RL, achieving 52.8±18.2 average score on AntMaze vs 25.8±19.8 for Forward-Backward and 11.8±12.6 for Successor Features

## Executive Summary
This paper presents Functional Reward Encoding (FRE), a method for zero-shot reinforcement learning that enables agents to learn from unlabeled offline trajectories and immediately adapt to new tasks without additional training. FRE uses a transformer-based variational auto-encoder to encode arbitrary reward functions into a compact latent space, allowing the agent to generalize to new tasks by simply encoding a few reward-annotated samples. The approach trains agents on a mixture of random unsupervised reward functions and demonstrates strong performance on standard offline RL benchmarks including ExORL, AntMaze, and Kitchen.

## Method Summary
FRE encodes arbitrary reward functions into a latent space using a transformer-based VAE that maps state-reward pairs to fixed-size embeddings. The encoder processes K unordered state-reward pairs from the offline dataset and outputs a latent vector z that is predictive of other state-reward pairs from the same reward function while being compressive. A policy conditioned on z is trained using offline RL (IQL) to maximize rewards from random unsupervised reward functions including goal-reaching, random linear, and random MLP functions. At test time, new tasks are solved by encoding their reward functions into z and executing the policy directly.

## Key Results
- On AntMaze benchmark, FRE achieved 52.8±18.2 average score across all tasks
- FRE significantly outperformed prior zero-shot RL methods: Forward-Backward (25.8±19.8) and Successor Features (11.8±12.6)
- Demonstrated zero-shot performance on multiple standard offline RL benchmarks (ExORL, AntMaze, Kitchen)
- Showed ability to solve various tasks by encoding just a few reward-annotated samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-based VAE can encode arbitrary reward functions into compact latent space by mapping state-reward pairs to fixed-size embedding
- Mechanism: Encoder takes K state-reward pairs from offline dataset, treats as unordered set, outputs latent vector z maximally predictive of other state-reward pairs from same reward function while being compressive
- Core assumption: Reward functions can be represented as lookup tables over states in dataset, mutual information bottleneck can learn meaningful latent encoding
- Evidence anchors: [abstract] "learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder" [section] "any reward function η can be represented as a lookup table over the set of state-reward pairs: Lη := {(se, η(se)) : se ∈ D}"
- Break condition: If reward function depends on states not in dataset or is too complex for transformer to encode within given latent dimension

### Mechanism 2
- Claim: Policy conditioned on latent encoding z can generalize to arbitrary reward functions without additional training
- Mechanism: FRE-conditioned policy π(a|s, z) and Q(s, a, z) trained using offline RL (IQL) to maximize rewards from mixture of random unsupervised reward functions
- Core assumption: Policy and value functions can learn to generalize across latent space of reward encodings, IQL can effectively learn from offline dataset without online exploration
- Evidence anchors: [abstract] "This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner" [section] "RL components (Q-function, value function, and policy) are all conditioned on z"
- Break condition: If offline dataset is too narrow or poor quality, or random reward functions don't cover space of possible downstream tasks

### Mechanism 3
- Claim: Mixture of random unsupervised reward functions (goal-reaching, linear, MLP) as prior provides diverse domain-agnostic training signal
- Mechanism: FRE agent trained to maximize mixture of random singleton functions (goal-reaching), random linear functions, and random MLP functions
- Core assumption: Uniform mixture of these three function types provides sufficient diversity to learn robust representations that generalize to new tasks
- Evidence anchors: [section] "mixture of random unsupervised functions... random singleton functions (corresponding to 'goal-reaching' rewards), random neural networks (MLPs with two linear layers), and random linear functions" [section] "We study these choices further in Section 5.3"
- Break condition: If downstream tasks have different structure than training reward functions or mixture ratio not optimal for specific domain

## Foundational Learning

- Concept: Mutual information bottleneck
  - Why needed here: To learn compressed yet informative latent representation of reward functions that generalizes to new tasks
  - Quick check question: What is the objective function used to train FRE encoder, and how does it balance informativeness and compression?

- Concept: Transformer-based set encoding
  - Why needed here: To process unordered sets of state-reward pairs and produce fixed-size latent embedding capturing structure of reward function
  - Quick check question: How does transformer encoder in FRE handle input set of state-reward pairs, and what positional encodings or masking are used?

- Concept: Offline reinforcement learning
  - Why needed here: To train policy and value functions on offline dataset without online exploration, crucial for zero-shot setting
  - Quick check question: What offline RL algorithm is used to train FRE-conditioned policy, and how does it handle challenge of learning from fixed dataset?

## Architecture Onboarding

- Component map:
  FRE Encoder -> FRE Decoder -> Policy Network -> Q-Function -> Value Function

- Critical path:
  1. Sample random reward function η from prior
  2. Sample K states from offline dataset and evaluate η on them
  3. Encode resulting state-reward pairs into latent z using FRE encoder
  4. Train policy, Q-function, and value function using IQL with rewards from η
  5. At test time, encode new task's reward function into z and execute policy

- Design tradeoffs:
  - Learned encoder vs. fixed encoder (e.g., linear regression as in FB/SF methods)
  - Training encoder jointly with policy vs. two-stage process
  - Mixture of random reward functions vs. more structured prior
  - Transformer vs. other set encoding architectures

- Failure signatures:
  - Poor performance on goal-reaching tasks: Random reward functions may not include enough goal-reaching tasks during training
  - Instability during training: Two-stage training process may be sensitive to hyperparameters
  - Overfitting to offline dataset: Policy may not generalize well to new states or reward functions

- First 3 experiments:
  1. Train FRE on simple gridworld environment with random reward functions and evaluate on goal-reaching tasks
  2. Compare FRE to FB and SF methods on AntMaze benchmark using same offline dataset and evaluation tasks
  3. Ablate different components of random reward prior (goal-reaching, linear, MLP) and measure impact on performance

## Open Questions the Paper Calls Out
- Question: How does choice of reward prior distribution impact FRE's performance on downstream tasks?
- Basis in paper: [explicit] Paper discusses using mixture of random unsupervised reward functions but notes this is design choice and leaves open question of optimal reward priors
- Why unresolved: Paper only experiments with one specific prior distribution and doesn't explore sensitivity to different choices
- What evidence would resolve it: Systematic experiments comparing FRE performance across various reward prior distributions on multiple downstream tasks

## Limitations
- Limited exploration of how reward prior diversity affects generalization capability
- Performance could be bottlenecked by dataset quality rather than encoding approach
- Zero-shot evaluation doesn't fully address potential distribution shifts between training and test reward functions

## Confidence
- High confidence in methodology's soundness for encoding known state-reward pairs
- Medium confidence in ability to generalize to truly unseen reward structures
- Medium confidence in core mechanism based on empirical results showing strong performance on standard benchmarks

## Next Checks
1. **Distribution Shift Analysis**: Systematically evaluate FRE performance when test reward functions have different statistical properties than training rewards
2. **State Coverage Sensitivity**: Measure how performance degrades as fraction of states with reward annotations decreases during test time
3. **Cross-Domain Transfer**: Test FRE-trained models on entirely different domains without retraining to assess generality of learned reward encodings