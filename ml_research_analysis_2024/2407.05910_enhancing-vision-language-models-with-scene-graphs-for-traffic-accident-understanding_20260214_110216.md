---
ver: rpa2
title: Enhancing Vision-Language Models with Scene Graphs for Traffic Accident Understanding
arxiv_id: '2407.05910'
source_url: https://arxiv.org/abs/2407.05910
tags:
- scene
- traffic
- graph
- accident
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Traffic-Scene-Graph Inference (TSGi), a multimodal
  pipeline for classifying traffic accident types. The method uses scene graphs to
  capture spatial and temporal relationships between objects in traffic scenes, and
  aligns these representations with vision and language modalities within a contrastive
  learning framework.
---

# Enhancing Vision-Language Models with Scene Graphs for Traffic Accident Understanding

## Quick Facts
- arXiv ID: 2407.05910
- Source URL: https://arxiv.org/abs/2407.05910
- Reference count: 40
- 4-class traffic accident classification achieves 57.77% balanced accuracy using scene graphs + vision + language

## Executive Summary
This paper introduces Traffic-Scene-Graph Inference (TSGi), a multimodal pipeline for classifying traffic accident types. The method uses scene graphs to capture spatial and temporal relationships between objects in traffic scenes, and aligns these representations with vision and language modalities within a contrastive learning framework. Scene graphs are generated from video frames using the roadscene2vec tool and encoded using a scene graph encoder. These encodings are aligned with embeddings from CLIP and X-CLIP before being used for accident classification. When trained on 4 classes from the DoTA dataset, TSGi achieves a balanced accuracy of 57.77%, representing a 5 percentage point improvement over using vision and language modalities alone.

## Method Summary
The method follows a multi-stage pipeline: first, video frames are sampled and scene graphs are generated using the rs2v tool with BEV calibration. The scene graph encoder (SGE) uses an MRGCN with attention mechanisms and LSTMs to encode spatial and temporal relations. The SGE is then aligned with frozen CLIP and X-CLIP encoders using symmetric cross-entropy loss. Finally, the three modality embeddings are concatenated and passed through a 2-layer MLP classification head. The approach is trained on the DoTA dataset with 2,163 ego-vehicle videos across 4 accident classes.

## Key Results
- 57.77% balanced accuracy on 4-class traffic accident classification
- 5 percentage point improvement over vision and language modalities alone
- Performance improves with larger batch sizes and longer training times during alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scene graphs capture spatial and temporal relationships between traffic objects that are not fully represented in vision or language modalities alone.
- Mechanism: The scene graph encoder (SGE) uses a multirelational graph convolutional network (MRGCN) with attention mechanisms and LSTMs to encode both spatial relations (distance, direction) and temporal dynamics (sequence of frames) into fixed-length embeddings.
- Core assumption: Traffic accidents can be uniquely characterized by the spatial-temporal interactions of objects (vehicles, pedestrians) in the scene.
- Evidence anchors:
  - [abstract] "We approach the problem by representing a traffic scene as a graph, where objects such as cars can be represented as nodes, and relative distances and directions between them as edges."
  - [section] "a multirelational graph convolutional network (MRGCN) as employed in [9] is used, which includes an attention mechanism along with LSTMs to model the spatial and temporal relations of the scene graphs"
  - [corpus] Weak - corpus neighbors focus on scene graphs for hazard scenarios and task reasoning, but don't directly address traffic accident classification specifically
- Break condition: If the spatial-temporal relationships encoded in scene graphs are redundant with or less informative than the vision and language embeddings, the additional modality provides no benefit.

### Mechanism 2
- Claim: Contrastive alignment training helps the SGE learn to produce embeddings that are compatible with vision and language embeddings for multimodal fusion.
- Mechanism: During alignment training, the SGE is trained to minimize symmetric cross-entropy loss with frozen CLIP and X-CLIP encoders, creating a shared embedding space across all three modalities.
- Core assumption: A shared embedding space allows for effective fusion of information from different modalities through simple operations like concatenation.
- Evidence anchors:
  - [abstract] "the graph representation is further aligned with representations from the vision and language modalities within contrastively-trained foundation models"
  - [section] "we use the CLIP model [10], but replace the CLIP image encoder with X-CLIP [11], a pre-trained video encoder... we then freeze the weights from the video and text encoders and align them to the SGE encoder"
  - [corpus] Weak - corpus papers mention cross-attention and alignment techniques but not specifically for traffic accident understanding with scene graphs
- Break condition: If the domains of the pre-trained vision/language encoders are too different from traffic accident videos, alignment training may not transfer useful knowledge.

### Mechanism 3
- Claim: Late fusion of modality embeddings through concatenation and a small MLP classification head effectively combines information for accident classification.
- Mechanism: After alignment training, the three modality embeddings are concatenated and passed through a 2-layer MLP with ReLU activations to produce accident class predictions.
- Core assumption: Simple late fusion is sufficient to combine complementary information from different modalities without complex architectural changes.
- Evidence anchors:
  - [abstract] "Better results are obtained with a classifier that fuses the scene graph input with visual and textual representations"
  - [section] "we choose the approach of concatenating the vectors from the three modalities as described in [28] and, following [24], train a 2-layer MLP with ReLU activations as our classification head"
  - [corpus] Weak - corpus neighbors discuss fusion techniques but don't provide direct evidence for this specific approach in traffic accident classification
- Break condition: If one modality dominates the concatenated representation or if the simple MLP cannot effectively learn to combine the complementary information, performance will not improve.

## Foundational Learning

- Concept: Multimodal contrastive learning
  - Why needed here: The method relies on aligning embeddings from three different modalities (text, video, scene graphs) in a shared space to enable effective fusion
  - Quick check question: What is the purpose of using symmetric cross-entropy loss during the alignment training phase?

- Concept: Graph neural networks for scene representation
  - Why needed here: Scene graphs are encoded using an MRGCN to capture spatial and temporal relationships between traffic objects
  - Quick check question: How does the MRGCN in the SGE differ from a standard GCN in terms of handling traffic scene graphs?

- Concept: Traffic accident classification evaluation metrics
  - Why needed here: The paper uses balanced accuracy to account for class imbalance in the dataset
  - Quick check question: Why might balanced accuracy be more appropriate than standard accuracy for this traffic accident classification task?

## Architecture Onboarding

- Component map:
  Data pre-processing pipeline (video frame sampling, caption generation, scene graph generation using rs2v) -> Scene graph encoder (MRGCN with attention and LSTMs) -> Alignment training module (symmetric cross-entropy loss) -> Fusion module (concatenation of three modality embeddings) -> Classification head (2-layer MLP with ReLU activations)

- Critical path: Video frames → Scene graph generation → SGE encoding → Alignment training → Fusion → Classification

- Design tradeoffs:
  - Using frozen pre-trained encoders vs. fine-tuning them for better domain adaptation
  - Late fusion vs. early fusion or cross-modal attention mechanisms
  - Simple MLP classifier vs. more complex fusion architectures
  - Using synthetic pre-training data (CARLA) vs. focusing on real traffic data

- Failure signatures:
  - Alignment training shows no improvement over baseline (SGE without alignment)
  - Balanced accuracy remains close to random guessing (25%)
  - Large gap between train and validation/test accuracy (overfitting)
  - One modality's embedding dominates the concatenated representation

- First 3 experiments:
  1. Run SGE pre-training only on the DoTA dataset and evaluate classification accuracy to verify scene graphs capture useful information
  2. Test the full pipeline with alignment training using small batch sizes and limited epochs to establish baseline performance
  3. Increase batch size and training epochs during alignment to test hypothesis that larger-scale training improves performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal batch size and number of epochs for alignment training to maximize performance?
- Basis in paper: [explicit] The paper discusses experiments with different batch sizes (32 to 128) and epochs (8 to 20) for alignment training, noting that larger batch sizes show potential for improved results.
- Why unresolved: The experiments conducted did not fully explore the impact of larger batch sizes and longer training times due to compute constraints.
- What evidence would resolve it: Further experiments with varying batch sizes and training epochs to determine the optimal configuration for alignment training.

### Open Question 2
- Question: How would incorporating captions derived from the videos themselves impact the model's performance?
- Basis in paper: [inferred] The paper mentions that captions were manually generated and paired with videos for training, but notes that these captions were not used during inference.
- Why unresolved: The paper does not explore the use of video-derived captions during inference or fine-tuning.
- What evidence would resolve it: Experiments comparing model performance using manually generated captions versus captions automatically derived from the videos.

### Open Question 3
- Question: Would a more complex scene graph structure, including relations between all objects and not just the ego vehicle, improve classification accuracy?
- Basis in paper: [explicit] The paper notes that the scene graph generator focuses on the ego vehicle and its relations, and suggests that modeling relations between all objects could enhance the signal.
- Why unresolved: The current implementation does not explore the impact of more complex scene graph structures on classification accuracy.
- What evidence would resolve it: Experiments comparing model performance using scene graphs with limited structure versus more comprehensive structures including all object relations.

### Open Question 4
- Question: How would incorporating an additional modality, such as audio, affect the model's performance?
- Basis in paper: [explicit] The paper suggests that incorporating an additional modality could be an interesting avenue for exploration.
- Why unresolved: The current model only uses video, text, and scene graph modalities.
- What evidence would resolve it: Experiments incorporating audio data into the model and comparing performance with and without the additional modality.

## Limitations
- Moderate improvement (5 percentage points) over baseline despite added complexity
- Performance remains relatively low at 57.77% balanced accuracy
- Limited generalizability beyond the specific DoTA dataset and 4 accident classes

## Confidence
- High confidence: The pipeline architecture and methodology are clearly described and reproducible.
- Medium confidence: The 5 percentage point improvement is real but the practical significance for real-world deployment is unclear given the relatively low overall accuracy.
- Low confidence: Claims about scene graphs capturing unique spatial-temporal relationships that vision/language cannot fully represent, as the performance gain could be partially due to the additional modality rather than uniquely valuable information.

## Next Checks
1. Test the approach on a different traffic accident dataset to verify generalizability beyond DoTA.
2. Compare performance against a strong baseline that uses late fusion of CLIP and X-CLIP embeddings without scene graphs to isolate the contribution of the graph modality.
3. Experiment with alternative fusion mechanisms (early fusion, cross-modal attention) to determine if the simple concatenation approach is optimal for this task.