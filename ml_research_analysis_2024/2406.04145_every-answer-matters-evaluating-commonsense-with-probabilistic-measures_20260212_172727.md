---
ver: rpa2
title: 'Every Answer Matters: Evaluating Commonsense with Probabilistic Measures'
arxiv_id: '2406.04145'
source_url: https://arxiv.org/abs/2406.04145
tags:
- human
- answers
- protoqa
- answer
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating commonsense reasoning
  in large language models (LLMs) through a generative task. The authors argue that
  existing multiple-choice question-answering benchmarks fail to capture the probabilistic
  and implicit nature of commonsense.
---

# Every Answer Matters: Evaluating Commonsense with Probabilistic Measures

## Quick Facts
- arXiv ID: 2406.04145
- Source URL: https://arxiv.org/abs/2406.04145
- Authors: Qi Cheng; Michael Boratko; Pranay Kumar Yelugam; Tim O'Gorman; Nalini Singh; Andrew McCallum; Xiang Lorraine Li
- Reference count: 12
- Primary result: Introduces PROBEVAL, a probabilistic evaluation method that shows high correlation with human judgments for commonsense reasoning in LLMs

## Executive Summary
This paper addresses the challenge of evaluating commonsense reasoning in large language models through a generative task approach. The authors argue that existing multiple-choice benchmarks fail to capture the probabilistic and implicit nature of commonsense, and propose Commonsense Frame Completion (CFC) as an alternative. For CFC, context sentences with missing slots are presented, and models must infer the missing information. The evaluation method, PROBEVAL, clusters human answers and compares the resulting distributions using KL divergence, achieving high correlation with human judgments.

## Method Summary
The method involves collecting context sentences from the CommonGen dataset, parsing them using AMR to identify missing slots, and gathering diverse human answers for each slot through Amazon MTurk. The PROBEVAL metric clusters these answers using algorithms like X-means or hierarchical agglomerative clustering, then matches model-generated answers to ground truth clusters using WordNet or embedding-based similarity. Evaluation is performed by calculating KL divergence between the categorical distributions of human and model answers, with Laplace smoothing applied to handle unseen concepts.

## Key Results
- PROBEVAL shows high correlation with human judgments on both CFC and ProtoQA datasets
- PROBEVAL is more sensitive to subtle changes in answer distributions compared to existing metrics
- Even the best LLMs show significant performance gaps compared to humans on the CFC task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The probabilistic evaluation method captures the distributional nature of commonsense by clustering answers and comparing categorical distributions.
- Mechanism: By clustering similar answers into concepts and forming probability distributions over these clusters, the method evaluates whether a model can capture not just the correct answers but their relative likelihoods.
- Core assumption: Common sense is inherently probabilistic with multiple valid answers that vary in likelihood based on context and population.
- Evidence anchors:
  - [abstract] "Common sense is also inherently probabilistic with multiple correct answers. The purpose of 'boiling water' could be making tea and cooking, but it also could be killing germs."
  - [section 1] "In order to avoid the issues in MCQA, many recent benchmarks have proposed generative commonsense evaluations. While generative evaluation avoids the difficulty of generating hard negatives, it does not reflect the fact that there are often multiple correct answers, especially for commonsense questions."
  - [corpus] Weak - the corpus doesn't directly address the probabilistic nature of common sense, though related papers discuss commonsense reasoning evaluation.
- Break condition: If clustering fails to capture semantically similar answers as the same concept, or if the KL divergence becomes insensitive to meaningful differences in answer distributions.

### Mechanism 2
- Claim: The CFC task format effectively evaluates implicit commonsense by requiring models to infer missing information in context sentences.
- Mechanism: By presenting context sentences with missing slots (like purpose, instrument, location) and asking for diverse answers, the task tests whether models can utilize implicit knowledge without explicit prompts.
- Core assumption: Real-world commonsense reasoning involves inferring unstated information rather than answering explicit questions about common knowledge.
- Evidence anchors:
  - [abstract] "Unlike factual question answering tasks, there is no single correct answer... We view the context sentence as a structured semantic frame, identify a missing slot, and ask the model to provide a distribution of potential slot fillers."
  - [section 3] "Given a direction such as 'put the water on the burner to boil', it is common sense which allows us to understand that the water is likely in a kettle and not simply dumped on the burner."
  - [corpus] Weak - while corpus papers discuss commonsense reasoning, none specifically validate the frame completion approach as presented here.
- Break condition: If models can succeed by memorizing common patterns rather than truly understanding implicit relationships.

### Mechanism 3
- Claim: PROBEVAL achieves high correlation with human judgments by using probabilistic measures that capture subtle differences in answer distributions.
- Mechanism: The evaluation method uses KL divergence between human and model answer distributions, which is sensitive to differences in both which answers are present and their relative probabilities.
- Core assumption: Human judgments about commonsense reasoning quality correlate strongly with how well a model's answer distribution matches the human answer distribution.
- Evidence anchors:
  - [section 5.3.4] "Our findings, detailed in Table 2 and 3, indicate strong correlations between human evaluation and PROBEVAL."
  - [section 5.4.2] "PROBEVAL significantly outperforms the ProtoQA evaluator when the prediction includes incorrectly ranked responses."
  - [corpus] Weak - corpus doesn't contain evaluation methods for commonsense reasoning that directly compare to PROBEVAL.
- Break condition: If human judgments are inconsistent or if the KL divergence becomes insensitive to meaningful distribution changes.

## Foundational Learning

- Concept: Probabilistic knowledge representation
  - Why needed here: The paper treats commonsense as probabilistic rather than absolute knowledge, requiring understanding of how to represent and compare probability distributions over answer clusters.
  - Quick check question: If humans give answers "kettle" (60%), "pot" (25%), and "cup" (15%) for boiling water, what KL divergence would indicate a model prediction of "kettle" (50%), "pot" (30%), "cup" (20%)?

- Concept: Clustering algorithms and their evaluation
  - Why needed here: The evaluation method depends on clustering similar answers into concepts, requiring knowledge of clustering techniques and how to evaluate their quality.
  - Quick check question: Given answer strings ["kettle", "teapot", "pot", "cup", "glass"], which clustering algorithm would be most appropriate and why?

- Concept: Semantic frame parsing and slot identification
  - Why needed here: The CFC task requires identifying implicit information in sentences by parsing them into semantic frames with missing slots.
  - Quick check question: For the sentence "She turned on the oven to bake cookies", what are the likely missing slot types and example answers?

## Architecture Onboarding

- Component map: Data collection pipeline (CommonGen → AMR parsing → missing slot identification → human annotation) → Embedding layer (FastText word embeddings) → Clustering module (X-means, G-means, HAC implementations) → Matching module (WordNet matching, Gaussian regression, cosine similarity) → Evaluation engine (KL divergence calculation with Laplace smoothing) → Validation framework (diverse sampling, error type generation)

- Critical path: 1. Context sentence processing → AMR parsing → missing slot identification 2. Human answer collection → embedding → clustering → distribution formation 3. Model answer generation → embedding → matching → distribution formation 4. KL divergence calculation between distributions

- Design tradeoffs:
  - Manual vs automatic clustering: Manual provides better correlation but is expensive; automatic is scalable but may miss nuanced distinctions
  - Embedding choice: FastText vs contextual embeddings - FastText performs better here despite losing context
  - Matching method: WordNet provides semantic matching but is slow; Gaussian regression is faster but may miss rare concepts

- Failure signatures:
  - Low correlation with human judgments despite high KL divergence scores
  - Clustering produces too many/few clusters, losing semantic meaning
  - Matching assigns answers to wrong clusters due to semantic drift
  - KL divergence becomes insensitive to meaningful distribution changes

- First 3 experiments:
  1. Test clustering quality by manually clustering a sample of answers and comparing to automatic clustering results using metrics like BLANC score
  2. Validate embedding quality by checking if semantically similar answers have high cosine similarity in embedding space
  3. Test matching accuracy by creating controlled examples where answers should match specific clusters and measuring correct assignment rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed probabilistic evaluation method, PROBEVAL, compare to existing evaluation metrics in terms of sensitivity to subtle changes in answer distributions?
- Basis in paper: Explicit - The paper states that PROBEVAL is more sensitive to subtle changes in answer distributions compared to existing metrics.
- Why unresolved: The paper provides some qualitative analysis and examples of PROBEVAL's sensitivity, but does not present a comprehensive quantitative comparison with other metrics.
- What evidence would resolve it: A detailed study comparing PROBEVAL's performance to other evaluation metrics across various datasets and scenarios, including both qualitative and quantitative measures of sensitivity.

### Open Question 2
- Question: How does the performance of large language models on the Commonsense Frame Completion (CFC) task compare to their performance on other commonsense reasoning benchmarks?
- Basis in paper: Explicit - The paper reports that humans drastically outperform strong language model baselines on the CFC dataset, indicating a significant performance gap.
- Why unresolved: The paper focuses on the CFC task and does not provide a direct comparison with other commonsense reasoning benchmarks.
- What evidence would resolve it: A comprehensive evaluation of various large language models on both the CFC task and other popular commonsense reasoning benchmarks, allowing for a direct comparison of their performance across different tasks.

### Open Question 3
- Question: How does the number of human annotations collected for each question in the CFC dataset affect the stability and representativeness of the answer distribution?
- Basis in paper: Explicit - The paper uses the Neyman-Pearson lemma to determine that collecting 100 answers per question is sufficient to approximate the true answer distribution with reasonable error rate.
- Why unresolved: The paper does not explore the impact of varying the number of annotations on the quality of the dataset and the evaluation process.
- What evidence would resolve it: A study examining the effect of different numbers of human annotations on the stability and representativeness of the answer distribution, as well as the performance of the PROBEVAL metric.

## Limitations

- The evaluation method's effectiveness depends heavily on the quality of answer clustering, which may struggle with highly nuanced or context-dependent commonsense reasoning.
- The use of static FastText embeddings, while performing well in this context, may miss important semantic relationships that contextual embeddings could capture.
- The dataset size (CFC) may limit generalizability to broader commonsense reasoning scenarios.

## Confidence

- High confidence: The fundamental premise that commonsense is probabilistic and requires evaluation methods beyond multiple-choice formats
- Medium confidence: The effectiveness of PROBEVAL's KL divergence approach in capturing distributional differences in commonsense answers
- Medium confidence: The claim that CFC is more challenging than existing commonsense benchmarks based on current LLM performance gaps

## Next Checks

1. Test PROBEVAL's sensitivity to different clustering algorithms and parameters by systematically varying these and measuring correlation with human judgments across multiple test sets
2. Compare PROBEVAL's performance against human evaluators on a held-out test set where evaluators rate the quality of model-generated commonsense answers without knowing the ground truth distributions
3. Evaluate whether the CFC task's difficulty stems from the probabilistic nature of commonsense or other factors by creating controlled variations with different numbers of valid answers and measuring model performance changes