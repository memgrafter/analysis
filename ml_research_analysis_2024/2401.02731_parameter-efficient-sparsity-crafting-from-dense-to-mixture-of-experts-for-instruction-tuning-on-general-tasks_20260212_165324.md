---
ver: rpa2
title: Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for
  Instruction Tuning on General Tasks
arxiv_id: '2401.02731'
source_url: https://arxiv.org/abs/2401.02731
tags:
- arxiv
- sparse
- experts
- shot
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Parameter-Efficient Sparsity Crafting (PESC),
  a method that converts dense models into sparse mixture-of-experts (MoE) models
  for efficient instruction tuning. PESC incorporates adapters into MoE layers to
  differentiate experts without altering individual expert weights, reducing computational
  costs and memory requirements while maintaining approximation quality.
---

# Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks

## Quick Facts
- arXiv ID: 2401.02731
- Source URL: https://arxiv.org/abs/2401.02731
- Reference count: 15
- Primary result: Converts dense models into sparse MoE models using adapters for efficient instruction tuning, achieving 75.7% accuracy on MMLU with only 35B activated parameters out of 38B total

## Executive Summary
This paper introduces Parameter-Efficient Sparsity Crafting (PESC), a method that converts dense models into sparse mixture-of-experts (MoE) models for efficient instruction tuning. PESC incorporates adapters into MoE layers to differentiate experts without altering individual expert weights, reducing computational costs and memory requirements while maintaining approximation quality. The approach was applied to instruction tuning across general tasks, demonstrating significant performance improvements on various benchmarks. The resulting Camelidae models outperformed other open-source sparse and dense models, showing superior general capabilities compared to GPT-3.5.

## Method Summary
PESC converts dense models into sparse MoE models by initializing expert weights from pre-trained dense FFN layers and inserting lightweight adapter modules after each expert. During fine-tuning, only adapter parameters and QLoRA parameters are updated, while the base expert weights remain frozen. A top-k gate router (typically k=2) selects experts per token, and an auxiliary load balancing loss ensures uniform expert utilization. The method was evaluated on instruction tuning datasets (SlimOrca, Magicoder, MetaMathQA) and benchmarked on MMLU, GSM8K, HumanEval, MBPP, HellaSwag, NaturalQuestions, and MATH.

## Key Results
- Camelidae models achieved 75.7% accuracy on MMLU, 79.4% on GSM8K, 48.8% on HumanEval, and 43.2% on MBPP
- Used only 35 billion activated parameters out of 38 billion total parameters
- Outperformed other open-source sparse and dense models, showing superior general capabilities compared to GPT-3.5
- Increasing experts from 4 to 16 significantly improved performance, with further increases potentially yielding more substantial advancements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PESC reduces computational and memory costs by replacing full weight updates of expert networks with adapter-based parameter updates.
- Mechanism: PESC inserts lightweight adapter modules (W_down, W_up, σ) into each expert in the MoE layer, allowing differentiation of experts without modifying their core weights. The adapter updates are mathematically sufficient to approximate full expert fine-tuning.
- Core assumption: Adapters with universal approximation properties (Funahashi 1989, Leshno et al. 1993, Kidger & Lyons 2020) can effectively differentiate experts without altering the base expert weights.
- Evidence anchors:
  - [abstract]: "PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers."
  - [section 2.2]: "Utilizing the adapters as ωi can effectively ensure the quality of the approximation of ˜Fi(θ+i, ωo)."
  - [corpus]: Weak or missing evidence. No direct citations or experiments in related papers validate the universal approximation claim for this specific adapter-MoE configuration.
- Break condition: If adapter capacity is insufficient for the task complexity, the approximation error ξ in Equation (6) will exceed acceptable bounds, leading to performance degradation.

### Mechanism 2
- Claim: PESC achieves computational efficiency by maintaining sparse activation while using parameter-efficient experts, avoiding redundant full expert weight updates.
- Mechanism: The MoE layer uses a top-k gate router (typically k=2) to activate only a subset of experts per token. PESC further reduces computation by updating only adapter parameters instead of full expert weights, while sharing base expert weights across tokens.
- Core assumption: The top-k routing mechanism combined with adapter-based differentiation preserves the model's representational power while minimizing active parameters.
- Evidence anchors:
  - [section 2.3]: "PESC utilizes adapters to circumvent redundant updates of the expert weights θi... differentiating between experts without altering each expert's original weights θo replicated from the original FFN layer."
  - [section 2.2]: "Given the universal approximation property of MLP layers with general activation functions, the Adapter module is a universal approximator."
  - [corpus]: Weak evidence. Related work (FlyLoRA, MoELoRA) explores MoE + LoRA combinations but does not validate the specific efficiency claims of adapter-based expert differentiation in PESC.
- Break condition: If the top-k routing selects the same experts for most tokens, the diversity and capacity benefits of MoE are lost, and adapter approximation becomes insufficient.

### Mechanism 3
- Claim: PESC preserves model capacity during instruction tuning by reusing dense model weights and fine-tuning only a minimal set of adapter parameters.
- Mechanism: Initial MoE expert weights are copied from the pre-trained dense FFN layers. Adapters are inserted after these shared weights, and QLoRA is applied to update remaining parameters, preserving learned representations while adding specialization.
- Core assumption: Weight copying from pre-trained dense models provides a strong initialization for MoE experts, and adapter fine-tuning is sufficient to adapt them to instruction tasks.
- Evidence anchors:
  - [section 2.1]: "During the initialization phase of sparsity crafting, each expert Ei within the MoE layer is initialized with the FFN layer F."
  - [section 3.1]: "We employed QLoRA techniques for effective fine-tuning of both the Camel and Camelidae models derived from Llama2-7B, Llama2-13B, and Yi-34B."
  - [corpus]: Moderate evidence. Sparse upcycling (Komatsuzaki et al.) validates weight reuse, but PESC's combination with adapters and QLoRA is novel and lacks direct validation in related work.
- Break condition: If the dense model initialization is poor for the target task, adapter fine-tuning alone may be insufficient to recover performance.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: PESC converts dense models into sparse MoE models to increase capacity while maintaining computational efficiency.
  - Quick check question: In an MoE layer with n experts and top-k routing, how many different expert combinations are possible per token?
  - Answer: C(n,k) = n choose k combinations.

- Concept: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: PESC uses adapter-based PEFT to differentiate experts without full weight updates, reducing memory and computation.
  - Quick check question: What is the key difference between LoRA and adapter-based PEFT in terms of parameter placement?
  - Answer: LoRA inserts low-rank matrices into FFN and attention layers, while adapters add small bottleneck modules after layers.

- Concept: Universal approximation theorem for MLPs
  - Why needed here: The paper claims adapters can approximate full expert fine-tuning due to their universal approximation properties.
  - Quick check question: What condition must an activation function satisfy to guarantee universal approximation in MLPs?
  - Answer: The activation function must be non-polynomial (e.g., ReLU, sigmoid).

## Architecture Onboarding

- Component map: Input → Dense layers → MoE layer (experts + router) → Output

- Critical path:
  1. Initialize experts from dense FFN weights
  2. Insert adapters after each expert
  3. Apply top-k routing to select experts per token
  4. Update only adapter parameters + QLoRA parameters during training
  5. Use load balancing loss to ensure uniform expert utilization

- Design tradeoffs:
  - Adapter size vs. approximation quality: Larger adapters improve approximation but reduce efficiency gains.
  - k value in top-k routing: Higher k increases capacity but reduces sparsity benefits.
  - Expert initialization: Better initialization from dense models reduces adapter burden but may limit specialization.

- Failure signatures:
  - Performance plateaus or degrades: Adapter capacity insufficient for task complexity.
  - High variance in expert utilization: Router not balancing load effectively.
  - Memory usage remains high: Adapter size too large or k value too high.

- First 3 experiments:
  1. Implement PESC with minimal adapters (e.g., 64-dim) on a small dense model and compare to full fine-tuning on a simple task.
  2. Vary k in top-k routing (k=1,2,3) and measure computational efficiency vs. performance tradeoff.
  3. Test different adapter dimensions and measure approximation quality on held-out validation data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PESC compare to traditional sparse upcycling with full fine-tuning when the number of experts increases beyond 16?
- Basis in paper: [explicit] The paper mentions that increasing the number of experts from 4 to 16 significantly improves performance, and suggests that further increases might yield more substantial advancements.
- Why unresolved: The study only tested up to 16 experts. The paper acknowledges that "a further increase in the number of experts might yield even more substantial advancements" but doesn't provide empirical evidence for this claim.
- What evidence would resolve it: Comparative experiments testing models with 16, 32, 64, and 128 experts on the same benchmarks to measure performance gains and identify potential diminishing returns.

### Open Question 2
- Question: What is the theoretical upper bound on approximation quality when using adapters for expert differentiation, and how does this compare to the approximation quality of full fine-tuning of expert weights?
- Basis in paper: [explicit] The paper states that adapters can guarantee a "good lower bound ξ" on approximation error and that they are "universal approximators," but doesn't quantify this bound or compare it to full fine-tuning.
- Why unresolved: The paper establishes that adapters provide good approximation quality but doesn't provide mathematical bounds or empirical comparisons of approximation quality between PESC and full fine-tuning approaches.
- What evidence would resolve it: Mathematical analysis of approximation bounds for adapter-based expert differentiation versus full fine-tuning, combined with empirical measurements of approximation error on representative tasks.

### Open Question 3
- Question: How does the computational efficiency of PESC scale with model size and number of experts compared to traditional MoE approaches that use LoRA for expert adaptation?
- Basis in paper: [explicit] The paper contrasts PESC with LoRA-based approaches, noting that PESC avoids "additional computational costs including higher memory usage and slower speed" of LoRA, but doesn't provide quantitative scaling comparisons.
- Why unresolved: While the paper qualitatively describes efficiency advantages, it doesn't provide detailed computational complexity analysis or scaling experiments across different model sizes and expert counts.
- What evidence would resolve it: Detailed benchmarking of memory usage, training time, and inference latency for PESC versus LoRA-based MoE approaches across models ranging from 7B to 70B parameters with varying numbers of experts.

## Limitations

- The universal approximation claim for adapter-based expert differentiation lacks direct empirical validation
- Computational efficiency claims need rigorous FLOPs and memory analysis across different sequence lengths
- Limited ablation studies make it difficult to attribute performance gains specifically to the adapter approach versus other factors

## Confidence

**High Confidence**: The basic architecture of converting dense models to MoE using adapter-based differentiation is technically sound and implementable.

**Medium Confidence**: The claim that PESC achieves competitive performance on instruction tuning benchmarks, though attribution to specific design choices is unclear.

**Low Confidence**: The universal approximation claim for the specific adapter-MoE configuration lacks mathematical proof or empirical validation.

## Next Checks

1. **Approximation Quality Analysis**: Conduct controlled experiments comparing PESC's adapter-based expert differentiation against full expert fine-tuning on a synthetic task where ground truth expert specialization is known. Measure the approximation error ξ and verify it remains within acceptable bounds across different adapter sizes and task complexities.

2. **Efficiency Benchmarking**: Perform detailed computational analysis measuring FLOPs, memory usage, and wall-clock time for PESC versus dense fine-tuning and other MoE approaches across varying sequence lengths (64, 512, 2048 tokens) and batch sizes. Include measurements for both training and inference scenarios.

3. **Router Utilization Study**: Analyze the top-k gate router's load balancing behavior by monitoring expert activation frequencies, entropy of token distribution across experts, and identifying any capacity bottlenecks. Test whether the auxiliary load balancing loss effectively prevents expert collapse in long training runs.