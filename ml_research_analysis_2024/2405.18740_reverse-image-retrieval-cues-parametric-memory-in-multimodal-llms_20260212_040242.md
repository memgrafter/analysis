---
ver: rpa2
title: Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs
arxiv_id: '2405.18740'
source_url: https://arxiv.org/abs/2405.18740
tags:
- image
- knowledge
- visual
- mllms
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how reverse image retrieval (RIR) augmentation
  can improve multimodal large language models (MLLMs) on knowledge-intensive visual
  question answering (VQA). The authors augment MLLMs with web-scale reverse image
  search results, providing additional visual and textual cues without necessarily
  containing the direct answer.
---

# Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs

## Quick Facts
- arXiv ID: 2405.18740
- Source URL: https://arxiv.org/abs/2405.18740
- Reference count: 40
- Primary result: Web-scale reverse image search augmentation improves MLLM performance on knowledge-intensive VQA by 18-43%

## Executive Summary
This paper explores how reverse image retrieval (RIR) augmentation can improve multimodal large language models (MLLMs) on knowledge-intensive visual question answering (VQA). The authors augment MLLMs with web-scale reverse image search results, providing additional visual and textual cues without necessarily containing the direct answer. They find that RIR significantly improves the performance of state-of-the-art MLLMs like GPT-4V, GPT-4 Turbo, and GPT-4o on two challenging VQA benchmarks, INFOSEEK and SnakeCLEF, by 37-43%, 25-27%, and 18-20% respectively. The authors also discover that RIR helps MLLMs access their own world knowledge better, particularly for long-tail concepts and objects.

## Method Summary
The authors implement RIR by capturing reverse image search results as screenshots through a Chromium browser-based API. These screenshots, along with layout explanations, are provided as context to MLLMs to answer knowledge-intensive visual questions. The approach is tested on two VQA benchmarks (INFOSEEK and SnakeCLEF) using state-of-the-art MLLMs including GPT-4V, GPT-4 Turbo, GPT-4o, and Idefics-2. The evaluation measures performance improvements across various metrics including GPT-as-judge Accuracy, Binomial-EM, and Genus-Recall.

## Key Results
- RIR improves GPT-4V, GPT-4 Turbo, and GPT-4o performance on INFOSEEK by 37-43%, 25-27%, and 18-20% respectively
- On SnakeCLEF, RIR improves GPT-4o performance by 2× on Binomial-EM and 27.89% on Genus-Recall
- RIR provides the most benefit for long-tail concepts with less web presence and fine-grained knowledge tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RIR improves MLLM performance by helping the model access its own parametric world knowledge rather than providing new external knowledge.
- Mechanism: When visual questions are augmented with RIR results, the multimodal cues help align the visual input with the textual knowledge already present in the model's parameters, enabling better extraction of relevant information.
- Core assumption: The MLLM already possesses relevant factual knowledge about entities but struggles to access it when prompted with pure visual questions.
- Evidence anchors:
  - [abstract]: "To our surprise, we discover that RIR helps the model to better access its own world knowledge."
  - [section 4.4.1]: "The distinct performance gain of text-only oracle-enhanced variants proves that the MLLM backbones do indeed possess the factual knowledge relevant to answer knowledge-intensive visual questions."
  - [corpus]: Weak - no direct citations found, but related works on multimodal retrieval augmentation suggest similar alignment mechanisms.
- Break condition: If the MLLM lacks sufficient parametric knowledge about the entities in question, or if the RIR results don't contain relevant multimodal cues.

### Mechanism 2
- Claim: RIR is more beneficial for long-tail concepts and objects that have less web presence.
- Mechanism: Entities with fewer web search results are underrepresented in MLLM training data, making RIR's external multimodal context more valuable for these cases.
- Core assumption: Web presence correlates with representation in MLLM training data, and RIR helps bridge this gap.
- Evidence anchors:
  - [section 4.4.3]: "We observe that RIR provides the most benefit when the query image is related to entities that are long tail."
  - [section 4.4.3]: "This suggests that RIR indeed helps more on questions about less common entities."
  - [corpus]: Weak - no direct citations found, but related works on rare entity recognition support this hypothesis.
- Break condition: If the entity is extremely rare with no web presence, or if the RIR results are dominated by irrelevant common entities.

### Mechanism 3
- Claim: RIR helps more on tasks requiring fine-grained knowledge compared to coarse-grained knowledge.
- Mechanism: When tasks demand precise identification or detailed factual knowledge, RIR's multimodal cues provide the specific context needed to distinguish between similar entities.
- Core assumption: Fine-grained knowledge requires more specific contextual information that RIR can provide through visual and textual cues.
- Evidence anchors:
  - [section 4.3]: "On the most fine-grained subtask metric, Binomial-EM, RIR improves the performance of state-of-the-art GPT-4o by more than 2×."
  - [section 4.3]: "In addition, on the most coarse-grained subtask metric, Genus-Recall, RIR gives a 27.89% relative improvement."
  - [corpus]: Weak - no direct citations found, but related works on fine-grained visual recognition support this observation.
- Break condition: If the task requires only general knowledge where the MLLM's parametric knowledge is already sufficient.

## Foundational Learning

- Concept: Multimodal Retrieval Augmentation (RAG)
  - Why needed here: Understanding how external knowledge sources can be integrated with MLLMs is crucial for grasping RIR's approach.
  - Quick check question: How does traditional RAG differ from RIR in terms of the type of external knowledge accessed?

- Concept: Parametric vs. Non-parametric Knowledge
  - Why needed here: The paper distinguishes between knowledge stored in the model's parameters and external knowledge sources.
  - Quick check question: What evidence suggests that RIR primarily helps access parametric knowledge rather than providing new knowledge?

- Concept: Long-tail Distribution in Machine Learning
  - Why needed here: Understanding why RIR helps more with rare entities requires knowledge of how training data distributions affect model performance.
  - Quick check question: Why would entities with less web presence benefit more from RIR augmentation?

## Architecture Onboarding

- Component map: RIR API -> Screenshot capture -> Layout explanation -> MLLM integration -> Evaluation pipeline

- Critical path:
  1. Query image and text are sent to RIR API
  2. RIR performs reverse image search and captures screenshot
  3. Screenshot and layout explanation are sent to MLLM
  4. MLLM generates answer using augmented context
  5. Answer is evaluated against ground truth

- Design tradeoffs:
  - Simplicity vs. fine-grained information extraction: Using screenshots instead of parsing individual search results
  - Web-scale vs. curated data: Accessing billions of images vs. more targeted datasets
  - Real-time vs. cached results: Fresh web data vs. faster responses

- Failure signatures:
  - Poor entity recognition: RIR results don't contain relevant entities
  - Information overload: Too many irrelevant results in the screenshot
  - MLLM confusion: Model struggles to interpret the layout explanation
  - Latency issues: Reverse image search takes too long to respond

- First 3 experiments:
  1. Test RIR on a simple image classification task to verify basic functionality
  2. Compare performance with and without RIR on a few INFOSEEK samples
  3. Analyze RIR results for a rare entity to verify the long-tail hypothesis

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but the authors acknowledge potential negative societal consequences such as surveillance and abuse that require further investigation.

## Limitations
- Evaluation relies heavily on GPT-4 as a judge, introducing potential circularity
- Computational overhead of RIR augmentation process and scalability not explored
- Limited to knowledge-intensive VQA tasks, not tested on broader visual reasoning tasks

## Confidence
- High confidence: The empirical results showing performance improvements across all tested MLLMs and benchmarks
- Medium confidence: The mechanism explaining that RIR helps access parametric knowledge rather than providing external knowledge
- Medium confidence: The observation that RIR benefits long-tail concepts more, though the correlation between web presence and training data representation could be more rigorously established

## Next Checks
1. **Cross-validation of evaluation**: Re-run the experiments using human annotators instead of GPT-4 as a judge to verify the robustness of the performance improvements
2. **Parametric knowledge probe**: Design controlled experiments where the MLLM's parametric knowledge is explicitly tested with and without RIR to validate the mechanism claim
3. **Generalization test**: Evaluate RIR on a broader range of visual reasoning tasks beyond knowledge-intensive VQA to assess its general applicability