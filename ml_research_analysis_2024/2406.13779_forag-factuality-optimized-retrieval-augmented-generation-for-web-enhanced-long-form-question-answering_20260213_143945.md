---
ver: rpa2
title: 'FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced
  Long-form Question Answering'
arxiv_id: '2406.13779'
source_url: https://arxiv.org/abs/2406.13779
tags:
- answer
- which
- factuality
- evaluation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of factuality and lack of clear
  logic in long-form question answering using web-enhanced retrieval-augmented generation
  (RAG). The authors propose a two-pronged solution: an outline-enhanced generator
  to improve the logical structure of generated answers, and a doubly fine-grained
  RLHF framework to optimize factuality at multiple levels of granularity.'
---

# FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering

## Quick Facts
- arXiv ID: 2406.13779
- Source URL: https://arxiv.org/abs/2406.13779
- Reference count: 40
- Key outcome: Outline-enhanced generator + doubly fine-grained RLHF achieves SOTA on LFQA with 1/24 the parameters of WebGPT-175B

## Executive Summary
This paper addresses factuality and logical structure issues in long-form question answering by combining an outline-enhanced generator with a doubly fine-grained RLHF framework. The outline stage improves coherence by enforcing structured reasoning before expansion, while the RLHF component optimizes factuality at multiple granularities. Experiments show the 7B-parameter FoRAG model outperforms WebGPT-175B on coherence, helpfulness, and factuality metrics.

## Method Summary
The method consists of two main components: an outline-enhanced generator that first drafts organizational patterns and outlines before expanding to full answers, and a doubly fine-grained RLHF framework that provides factuality optimization at holistic, sentence-level, and subclaim-level granularities. The approach uses automatic evaluation and reward modeling to avoid expensive human annotation while maintaining high factuality standards.

## Key Results
- FoRAG-L-7B (fine-tuned from Llama2-7B-chat) outperforms WebGPT-175B on all three metrics
- Parameter efficiency: 1/24 the parameters of WebGPT-175B while achieving better performance
- Significant improvements in coherence, helpfulness, and factuality on both English and Chinese benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Outline-enhanced generator improves coherence by enforcing structured reasoning before expansion
- Mechanism: Generator first selects organizational pattern and drafts outline, then expands each perspective into final answer
- Core assumption: Structured planning leads to better-organized output and reduces redundancy
- Evidence anchors: [abstract] "novel outline-enhanced generator to achieve clear logic", [section 4.1] "generator first drafts organizational pattern and outline"
- Break condition: If generator fails to produce meaningful outlines or expansion diverges from outline

### Mechanism 2
- Claim: Doubly fine-grained RLHF optimizes factuality by providing denser reward signals than holistic RLHF
- Mechanism: Framework segments answers into multiple granularities and applies both sequence-level and token-level reward modeling
- Core assumption: Finer-grained evaluation and reward signals improve alignment with factual correctness
- Evidence anchors: [abstract] "factuality optimization method based on...doubly fine-grained RLHF framework", [section 5.2] "generic framework contains several existing fine-grained RLHF methods as special cases"
- Break condition: If segmentation granularity is too coarse/fine or reward model training fails to generalize

### Mechanism 3
- Claim: Combined outline stage and factuality optimization produce SOTA performance with fewer parameters
- Mechanism: Outline stage ensures logical structure; doubly fine-grained RLHF improves factuality without expensive human annotation
- Core assumption: High-quality structure and factuality can offset raw parameter count in long-form QA tasks
- Evidence anchors: [abstract] "FoRAG-L-7B outperforms WebGPT-175B...while number of parameters is much smaller (only 1/24)", [section 6.2] Experimental comparison showing FoRAG-L 7B outperforms WebGPT-175B
- Break condition: If either component underperforms, overall gains relative to larger models disappear

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Paper builds on RAG to inject retrieved web content into LLM generation; understanding base RAG workflow is critical
  - Quick check question: In a RAG system, what is the role of the retriever and what is the role of the generator?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Doubly fine-grained RLHF framework is core factuality optimization mechanism; knowing how RLHF works is essential
  - Quick check question: What is the difference between sequence-level and token-level reward modeling in RLHF?

- Concept: Factuality evaluation in long-form text
  - Why needed here: Paper's novelty lies in automating factuality evaluation at multiple granularities; understanding measurement is key
  - Quick check question: How can you decompose a sentence into subclaims for fine-grained factuality checking?

## Architecture Onboarding

- Component map: User query → Web search engine → Retrieved passages → Outline stage → Expansion stage → Factuality optimization → Factuality-optimized answer
- Critical path:
  1. Query + retrieved passages fed to outline-enhanced generator
  2. Outline and organizational pattern generated
  3. Outline expanded into full answer
  4. Answer evaluated at sentence/subclaim levels for factuality
  5. Rewards used to fine-tune generator via RLHF
- Design tradeoffs:
  - Outline stage adds latency but improves coherence
  - Doubly fine-grained RLHF adds training complexity but removes need for human annotation
  - Smaller model size (1/24 params) trades off against computational efficiency gains
- Failure signatures:
  - Outlines that are too generic → reduced coherence gains
  - Reward model overfitting → poor generalization on new queries
  - Segmentation errors → incorrect factuality signals
- First 3 experiments:
  1. Compare coherence and helpfulness of outline-enhanced vs direct generation without outline stage
  2. Test factuality improvements from doubly fine-grained RLHF vs holistic RLHF
  3. Validate that subclaim-level evaluation yields better factuality than sentence-level alone

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several important ones emerge from the work:

1. How do the outline-enhanced generation and doubly fine-grained RLHF interact when both are applied? Does the outline generation step create any biases or constraints that affect the factuality optimization process?

2. How well does the doubly fine-grained RLHF framework generalize to other long-form generation tasks beyond web-enhanced question answering? What modifications might be needed for different domains?

3. What is the computational overhead of the subclaim-level factuality evaluation during both training and inference, and how does this scale with answer length? Are there more efficient evaluation strategies that maintain similar accuracy?

## Limitations

- The paper doesn't fully specify prompt templates used to construct outline-enhanced datasets or fine-grained factuality evaluation data
- Limited comparative analysis with other fine-grained RLHF approaches or similar outline-based methods
- Performance claims relative to WebGPT-175B based on single comparison, may not generalize across different long-form QA tasks

## Confidence

- Confidence in core claims: Medium
- Confidence in specific implementation details and prompt templates: Low
- Confidence in experimental results demonstrating improvements: Medium

## Next Checks

1. Reproduce the outline-enhanced generation: Implement the outline stage using the provided description and test its impact on coherence and helpfulness compared to direct generation without an outline.

2. Evaluate the doubly fine-grained RLHF framework: Construct fine-grained factuality evaluation data using the described granularities (holistic, sentence-level, subclaim-level) and test the impact of the doubly fine-grained approach on factuality compared to holistic RLHF.

3. Assess parameter efficiency: Fine-tune a smaller model (e.g., Llama2-7B-chat) using the proposed method and compare its performance to larger models (e.g., WebGPT-175B) on long-form QA tasks.