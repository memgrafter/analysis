---
ver: rpa2
title: A Compressive Memory-based Retrieval Approach for Event Argument Extraction
arxiv_id: '2409.09322'
source_url: https://arxiv.org/abs/2409.09322
tags:
- retrieval
- event
- demonstrations
- memory
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a Compressive Memory-based Retrieval (CMR)
  mechanism for Event Argument Extraction (EAE) to address two limitations in existing
  retrieval-augmented methods: (1) input length constraints that limit the diversity
  of retrieved information, and (2) the gap between the retriever and inference model
  that reduces retrieval quality. The proposed CMR uses a dynamic matrix memory to
  cache retrieved demonstrations, allowing the model to overcome input length restrictions
  and adaptively filter information from stored candidates based on the input query.'
---

# A Compressive Memory-based Retrieval Approach for Event Argument Extraction

## Quick Facts
- arXiv ID: 2409.09322
- Source URL: https://arxiv.org/abs/2409.09322
- Reference count: 25
- This paper introduces CMR, a compressive memory-based retrieval mechanism that achieves state-of-the-art performance on three EAE datasets

## Executive Summary
This paper addresses two key limitations in existing retrieval-augmented Event Argument Extraction (EAE): input length constraints and the gap between retriever and inference model. The authors propose a Compressive Memory-based Retrieval (CMR) mechanism that uses a dynamic matrix memory to cache retrieved demonstrations, allowing the model to overcome input length restrictions and adaptively filter information from stored candidates based on the input query. Experiments show that CMR significantly outperforms existing retrieval-based methods on three public EAE datasets (RAMS, WikiEvents, ACE05), demonstrating improved robustness and generalization, particularly when retrieving multiple demonstrations.

## Method Summary
The CMR mechanism works by pre-loading all candidate demonstrations into a fixed-size dynamic matrix memory rather than concatenating them directly into the model input. Demonstrations are embedded into KV pairs and aggregated into this matrix, which is then retrieved using the query via a kernel-based similarity function. The model learns to adaptively gate between vanilla attention and memory-retrieved attention using a trainable scalar γ. Training involves shuffling and reranking data by event type, with memory reset after Max_retrieval instances to expose the model to varying retrieval counts. This end-to-end training approach bridges the gap between retriever and inference model while overcoming input length constraints.

## Key Results
- CMR achieves new state-of-the-art performance on three public EAE datasets (RAMS, WikiEvents, ACE05)
- The method significantly outperforms existing retrieval-based approaches, particularly when retrieving multiple demonstrations
- CMR demonstrates improved robustness and generalization by adaptively filtering information from stored candidates
- Performance peaks around 10 demonstrations and degrades when exceeding Max_retrieval limits

## Why This Works (Mechanism)

### Mechanism 1
CMR overcomes the input length constraint by storing retrieved demonstrations in a dynamic matrix memory rather than concatenating them directly into the model input. Demonstrations are embedded into KV pairs and aggregated into a fixed-size matrix M, which is then retrieved using the query Q via the formula Aret = σ(Q)Mk / σ(Q)nk. The core assumption is that KV aggregation preserves the semantic essence of each demonstration sufficiently for later retrieval by the query.

### Mechanism 2
CMR bridges the retriever–inference gap by training the retrieval filter end-to-end with the inference model rather than using a frozen retriever. The model learns to adaptively gate between vanilla attention and memory-retrieved attention using a trainable scalar γ. The core assumption is that training γ and the memory parameters jointly allows the model to learn which parts of retrieved demonstrations are useful for a given query.

### Mechanism 3
CMR improves retrieval robustness by exposing the model to varying numbers of demonstrations during training. Training data is shuffled and reranked by event type, and memory is reset after Max_retrieval instances, ensuring each instance sees a different retrieval count. The core assumption is that varying retrieval counts during training makes the model robust to differing retrieval volumes at inference.

## Foundational Learning

- **Concept**: Attention mechanism in Transformers
  - Why needed here: CMR builds on the idea of attention but replaces the softmax weighting with a kernel-based similarity σ(Q)σ(K)^T.
  - Quick check question: What is the computational complexity of standard multi-head attention vs. linear attention?

- **Concept**: Event Argument Extraction (EAE) task definition
  - Why needed here: Understanding the task helps reason about why retrieval augmentation is useful and how memory caching fits.
  - Quick check question: In EAE, what is the difference between Arg-I and Arg-C metrics?

- **Concept**: Dense retrieval with SBERT embeddings
  - Why needed here: CMR still uses SBERT to retrieve the top-k demonstrations, so knowing how it works clarifies the retrieval step.
  - Quick check question: Why does SBERT use Siamese architecture for retrieval tasks?

## Architecture Onboarding

- **Component map**: Input → SBERT retriever → Top-k demonstrations → Memory update (KV aggregation) → Query embedding → Memory retrieval (Aret) → Gated fusion (A) → Transformer forward pass → Output
- **Critical path**: Retriever → Memory builder → Query processor → Fusion gate → Final prediction
- **Design tradeoffs**: Memory size vs. semantic collision risk; Batch size of demonstrations vs. inference latency; Max_retrieval value vs. training robustness
- **Failure signatures**: Performance plateaus when demonstration diversity is low; Degradation when γ is stuck near 0 or 1; Memory overflow or underflow if normalization term nk collapses
- **First 3 experiments**:
  1. Compare PAIE-CMR vs. PAIE-R on Arg-C with #N=1,5,10
  2. Measure inference time scaling with demonstration batch size
  3. Ablate the gating mechanism by fixing γ=0.5 and observe performance drop

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding CMR's performance in scenarios requiring long-term context beyond training demonstrations, the theoretical upper bounds on demonstration retrieval capacity, and direct comparisons with other memory-augmented architectures like Mamba or Infini-attention for EAE tasks. These questions highlight areas where further research is needed to fully understand CMR's limitations and potential applications beyond the current experimental scope.

## Limitations

- The paper lacks ablation studies isolating individual contributions of the compressive memory versus adaptive gating mechanisms
- Memory size is fixed (dimension d=768) and may not scale well to extremely diverse or large-scale datasets
- Performance is bounded by the quality of the initial SBERT dense retrieval step
- Experiments only test up to 15 demonstrations, leaving theoretical capacity limits unexplored

## Confidence

- High confidence: The claim that CMR overcomes input length constraints through memory caching is well-supported by the described mechanism and experimental setup
- Medium confidence: The assertion that CMR bridges the retriever-inference gap is plausible given the end-to-end training approach, but lacks direct ablation showing performance without the gating mechanism
- Medium confidence: The robustness claims based on varying retrieval counts during training are reasonable, but evidence is primarily from methodology description rather than systematic ablation studies

## Next Checks

1. Perform an ablation study comparing CMR with and without the adaptive gating mechanism (γ) to isolate its contribution to performance improvements
2. Test memory performance across different dimensionalities (e.g., d=512, d=1024) to identify optimal memory size for various dataset characteristics
3. Evaluate the model's sensitivity to demonstration quality by introducing controlled noise into the retrieved demonstrations and measuring performance degradation