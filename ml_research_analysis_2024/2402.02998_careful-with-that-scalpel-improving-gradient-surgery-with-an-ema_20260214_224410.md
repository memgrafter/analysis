---
ver: rpa2
title: 'Careful with that Scalpel: Improving Gradient Surgery with an EMA'
arxiv_id: '2402.02998'
source_url: https://arxiv.org/abs/2402.02998
tags:
- loss
- lmain
- gmain
- main
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bloop, a method for optimizing two objectives
  where one is prioritized over the other. The method projects the auxiliary gradient
  onto the orthogonal space of the primary gradient to maintain progress on the primary
  objective while incorporating auxiliary information.
---

# Careful with that Scalpel: Improving Gradient Surgery with an EMA

## Quick Facts
- arXiv ID: 2402.02998
- Source URL: https://arxiv.org/abs/2402.02998
- Reference count: 40
- Primary result: Introduces Bloop, a method for optimizing two objectives where one is prioritized over the other using EMA-based gradient projection

## Executive Summary
This paper introduces Bloop, a method for optimizing two objectives where one is prioritized over the other. The method projects the auxiliary gradient onto the orthogonal space of the primary gradient to maintain progress on the primary objective while incorporating auxiliary information. To handle stochastic gradients, an exponential moving average (EMA) of the primary gradient is used in the projection. Theoretical analysis shows that Bloop's stationary points are approximate stationary points of the bilevel problem, and convergence is proven under appropriate conditions. Experiments demonstrate that Bloop achieves better Pareto fronts compared to mixed training and other gradient surgery methods without EMA in various tasks including multi-task learning and joint dataset training.

## Method Summary
Bloop optimizes two objectives by projecting the auxiliary gradient onto the orthogonal space of the primary gradient, ensuring the primary objective's progress is maintained. To handle stochastic gradients, an EMA of the primary gradient is used in the projection. The method updates parameters using a direction that combines the primary gradient with the projected auxiliary gradient, scaled by a hyperparameter λ. Theoretical analysis shows that Bloop's stationary points are approximate stationary points of the bilevel problem, and convergence is proven under appropriate conditions.

## Key Results
- Bloop achieves better Pareto fronts compared to mixed training and other gradient surgery methods without EMA
- The method maintains first-order optimality on the main loss while incorporating auxiliary information
- Theoretical analysis proves that Bloop's stationary points are approximate stationary points of the bilevel problem under local error bound and Lipschitz Hessian assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bloop's direction update maintains first-order optimality on the main loss by projecting the auxiliary gradient orthogonal to the main gradient.
- Mechanism: The update direction d = gmain + λπ(gaux, gmain) ensures ⟨d, gmain⟩ = ∥gmain∥², so first-order Taylor expansion shows Lmain(θ - ηd) ≈ Lmain(θ - ηgmain).
- Core assumption: Gradients are computed exactly (full-batch) or EMA approximates the true main gradient well.
- Evidence anchors:
  - [abstract] "orthogonal projection of the auxiliary gradient to the training gradient"
  - [section] "At the first order in the step-size η, we see that the component of the direction in the direction gmain should be the same as that of gmain"
  - [corpus] Weak: corpus focuses on multi-task gradient surgery but not orthogonal projection specifics.
- Break condition: When ∥gmain∥ ≈ 0, the projection becomes ill-defined; or when EMA fails to track gmain, orthogonality is lost.

### Mechanism 2
- Claim: Using EMA of main gradients prevents the projection from degrading into a simple sum of gradients under stochastic noise.
- Mechanism: EMA(gmain) averages gradients over time, reducing variance; projection onto EMA preserves the critical orthogonality property in expectation.
- Core assumption: EMA decay parameter ρ is chosen larger than the step size η so that EMA tracks the true gradient well.
- Evidence anchors:
  - [abstract] "using a moving average of the training loss gradients, we can carefully maintain this critical orthogonality property"
  - [section] "we propose the direction dbatch = gbatch main + λπ(gbatch aux ; gEMA main )"
  - [corpus] Weak: no direct EMA-stochastic gradient discussion in related works listed.
- Break condition: If ρ is too small, EMA is outdated; if ρ is too large, EMA variance dominates and performance regresses to mixed training.

### Mechanism 3
- Claim: Bloop's stationary points are approximate stationary points of the underlying bilevel problem.
- Mechanism: Under local error bound and Lipschitz Hessian, Bloop's direction implies existence of Lagrange multiplier v such that gaux ≈ ∇²Lmain(θ)v when ∥gmain∥ is small.
- Core assumption: Local error bound holds (e.g., via Polyak-Lojasiewicz inequality) and Hessian is Lipschitz.
- Evidence anchors:
  - [abstract] "stationary points are approximate stationary points of the bilevel problem"
  - [section] "there exists v ∈ Rp such that ∥gaux − ∇²Lmain(θ)v∥ ≤ (λ−1 + M c²∥gaux∥/2)ε"
  - [corpus] Weak: no explicit local error bound or bilevel convergence theory in related works.
- Break condition: When local error bound fails (e.g., non-convex deep nets with flat regions), approximation guarantee breaks.

## Foundational Learning

- Concept: Bilevel optimization (min Laux subject to θ ∈ arg min Lmain)
  - Why needed here: Bloop solves a constrained problem where auxiliary loss is minimized only over minimizers of main loss.
  - Quick check question: What is the Lagrangian of min Laux s.t. ∇Lmain = 0?
- Concept: Exponential Moving Average (EMA)
  - Why needed here: EMA provides a low-variance estimate of the main gradient for stable projection in stochastic settings.
  - Quick check question: If ρ=0.01 and gbatch main = [1,0], what is EMA after one step starting from EMA=0?
- Concept: Gradient projection and orthogonality
  - Why needed here: Projection ensures auxiliary gradient does not interfere with main loss descent direction.
  - Quick check question: Compute π(gaux, gmain) for gaux=[1,1], gmain=[2,0].

## Architecture Onboarding

- Component map: Main gradient (gmain) → EMA update → Auxiliary gradient (gaux) → Projection π(gaux, EMA) → Direction d = gmain + λ·π → Optimizer update → Parameters θ.
- Critical path: EMA ← gbatch main; Projection ← gbatch aux, EMA; Direction ← gmain, projection; Update ← direction.
- Design tradeoffs: Larger ρ → more stable EMA but slower adaptation; smaller ρ → faster adaptation but higher variance; λ controls auxiliary loss priority.
- Failure signatures: Training loss stalls (EMA too outdated); auxiliary loss dominates (λ too large); Pareto front collapses to mixed training line (no EMA or bad ρ).
- First 3 experiments:
  1. Run Bloop vs mixed training on synthetic quadratic with misaligned gradients; verify better Pareto front.
  2. Test EMA sensitivity by sweeping ρ in transformer pretraining; observe trade-off curves.
  3. Validate stationary point approximation by checking ∇Lmain near zero and measuring residual ∥gaux - ∇²Lmain v∥.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does the EMA parameter ρ need to be tuned beyond the simple relationship with the learning rate η to achieve optimal performance?
- Basis in paper: The paper suggests that ρ should be "slightly larger than η" and that "choices between these two extremes (ρ = 0.01, or ρ = 0.1) lead to a tradeoff between main and auxiliary loss." However, it does not provide a detailed analysis of how to optimally tune ρ in different scenarios.
- Why unresolved: The relationship between ρ and η is complex and likely depends on factors such as the specific problem, the noise level in the gradients, and the relative importance of the main and auxiliary objectives. A more thorough investigation is needed to determine the optimal ρ for different settings.
- What evidence would resolve it: Experiments varying ρ across a wide range of values for different tasks and datasets, coupled with a theoretical analysis of the convergence properties of Bloop under different ρ settings, would provide insights into the optimal tuning of ρ.

### Open Question 2
- Question: How does Bloop perform when the main and auxiliary objectives have significantly different scales or when one objective is much more sensitive to changes in the parameters than the other?
- Basis in paper: [inferred] The paper does not explicitly address the issue of scale differences between the main and auxiliary objectives. However, it is a common challenge in multi-objective optimization and could potentially affect the performance of Bloop.
- Why unresolved: Scale differences between objectives can lead to one objective dominating the optimization process, potentially causing the algorithm to neglect the other objective. It is unclear how Bloop handles such situations and whether additional modifications are needed to ensure balanced optimization.
- What evidence would resolve it: Experiments comparing the performance of Bloop on problems with different scales of main and auxiliary objectives, along with an analysis of the convergence behavior in these scenarios, would shed light on how Bloop handles scale differences.

### Open Question 3
- Question: Can Bloop be extended to handle more than two objectives, and if so, how would the orthogonal projection be generalized in such cases?
- Basis in paper: The paper mentions that Bloop can be extended to multi-level optimization with more than two losses, but it only provides details for the case of three objectives. The generalization to an arbitrary number of objectives is not discussed.
- Why unresolved: Extending Bloop to handle more than two objectives is a natural progression, but it requires a clear understanding of how to generalize the orthogonal projection to higher dimensions. The paper does not provide sufficient details on this aspect.
- What evidence would resolve it: A theoretical analysis of how the orthogonal projection can be generalized to higher dimensions, along with experiments demonstrating the performance of Bloop on problems with multiple objectives, would provide insights into the feasibility and effectiveness of such an extension.

## Limitations

- The local error bound assumption (Mechanism 3) may not hold in deep learning settings with flat regions and saddle points
- The choice of EMA parameter ρ is presented as straightforward but may require careful tuning in practice
- The ablation on EMA's necessity could be more thorough - testing without EMA or with alternative gradient averaging methods would strengthen the claims

## Confidence

- Mechanism 1 (orthogonality preserves main loss progress): High
- Mechanism 2 (EMA prevents degradation): Medium
- Mechanism 3 (stationary point approximation): Low to Medium

## Next Checks

1. **Local error bound validation**: For a convolutional network on CIFAR-10, measure the actual error ∥gaux - ∇²Lmain(θ)v∥ at convergence points and compare to the theoretical bound to verify if the approximation holds empirically.

2. **EMA ablation study**: Compare Bloop against versions using simple gradient averaging (batch gradient or constant learning rate schedule) instead of EMA, measuring Pareto front quality to isolate EMA's specific contribution.

3. **Gradient norm sensitivity analysis**: Systematically test Bloop's behavior when ∥gmain∥ approaches zero (e.g., by adding L2 regularization to reduce gradient magnitude), measuring whether the orthogonal projection becomes unstable or if alternative formulations are needed.