---
ver: rpa2
title: 'Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding
  Dimensions'
arxiv_id: '2407.20243'
source_url: https://arxiv.org/abs/2407.20243
tags:
- embedding
- matryoshka-adaptor
- dimensions
- ndcg
- supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the computational cost and latency issues associated
  with high-dimensional embeddings from large language models (LLMs), particularly
  for information retrieval applications. The proposed Matryoshka-Adaptor framework
  modifies pre-trained LLM embeddings to achieve substantial dimensionality reduction
  while maintaining performance.
---

# Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions

## Quick Facts
- arXiv ID: 2407.20243
- Source URL: https://arxiv.org/abs/2407.20243
- Authors: Jinsung Yoon; Raj Sinha; Sercan O Arik; Tomas Pfister
- Reference count: 40
- Primary result: Achieves 2-12x dimensionality reduction for LLM embeddings while maintaining information retrieval performance

## Executive Summary
This paper addresses the computational cost and latency issues associated with high-dimensional embeddings from large language models (LLMs), particularly for information retrieval applications. The proposed Matryoshka-Adaptor framework modifies pre-trained LLM embeddings to achieve substantial dimensionality reduction while maintaining performance. It works in both unsupervised and supervised settings, enhancing the "Matryoshka properties" of embeddings. The method was evaluated across diverse datasets (BEIR, MIRACL, Fashion-200K) and embedding types (text, multimodal, multilingual), showing significant gains in retrieval tasks.

## Method Summary
Matryoshka-Adaptor learns a small residual function that transforms high-dimensional LLM embeddings into lower-dimensional representations while preserving similarity relationships. The adaptor uses a skip-connection architecture where the output is the sum of the original embedding and a learned residual (cei + f(cei)). It employs pairwise and top-k similarity losses to maintain relative distances between embeddings, with optional ranking loss for supervised fine-tuning. The framework treats the base embedding model as a black box, making it applicable to any LLM architecture including those accessible only through APIs.

## Key Results
- Achieves 2-12x dimensionality reduction without compromising nDCG@10 performance
- Demonstrates effectiveness across diverse datasets (BEIR, MIRACL, Fashion-200K)
- Works with multiple embedding types (text, multimodal, multilingual)
- Shows consistent improvements in retrieval tasks across various dimensions
- Maintains performance in both unsupervised and supervised settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Matryoshka-Adaptor improves embeddings by learning small, task-specific residual functions that preserve similarity structure across reduced dimensions
- Mechanism: The adaptor function f takes original embeddings and outputs a residual (cei + f(cei)), where f is trained to minimize pairwise and top-k similarity losses. This allows the adaptor to preserve the relative similarity relationships in lower-dimensional projections without needing to retrain the base LLM
- Core assumption: The residual space contains enough capacity to encode the task-specific similarity adjustments while preserving the base model's semantic structure
- Evidence anchors:
  - [abstract] states "Matryoshka-Adaptor facilitates substantial dimensionality reduction while maintaining comparable performance levels"
  - [section 3.2] describes the loss functions Lpair and Ltopk that explicitly preserve similarity relationships
  - [corpus] provides weak evidence - while the paper shows performance gains, it doesn't provide theoretical analysis of why the residual space is sufficient
- Break condition: If the base embeddings already perfectly capture the task-specific similarity structure, the residual function would be near-zero and provide no benefit

### Mechanism 2
- Claim: The adaptor works for both supervised and unsupervised settings by using different loss combinations that target similarity preservation
- Mechanism: In unsupervised mode, the adaptor uses only corpus embeddings with pairwise and top-k similarity losses. In supervised mode, it adds ranking loss from query-corpus pairs. This dual capability allows adaptation without requiring labeled data when unavailable
- Core assumption: The loss functions are properly balanced (α, β, γ hyperparameters) to achieve the desired trade-off between preserving original structure and optimizing for the target task
- Evidence anchors:
  - [abstract] states "exhibits efficacy in both unsupervised and supervised learning settings"
  - [section 4.2] describes the ranking loss Lrank that aligns rankings across different dimensions
  - [corpus] provides limited evidence - the paper shows both modes work but doesn't deeply analyze the loss function design choices
- Break condition: If the loss function coefficients are poorly chosen, the adaptor could either diverge from the original embeddings or fail to improve performance

### Mechanism 3
- Claim: The adaptor framework is model-agnostic and can work with any embedding model, including those accessed only through APIs
- Mechanism: By treating the embedding model as a black box and only working with the extracted embeddings, the framework can adapt embeddings from any source without requiring access to model parameters or training data
- Core assumption: The embedding model produces consistent, well-formed vectors that can be meaningfully transformed through the adaptor function
- Evidence anchors:
  - [abstract] states "designed to be seamlessly integrated with any LLM architecture, encompassing those accessible exclusively through black-box APIs"
  - [section 3.1] describes the framework using pre-trained embedding model E as a black-box
  - [corpus] provides evidence through experiments with OpenAI and Google APIs, showing the approach works in practice
- Break condition: If the base embedding model produces unstable or inconsistent outputs, the adaptor cannot learn meaningful transformations

## Foundational Learning

- Concept: Similarity preservation in reduced dimensions
  - Why needed here: The core innovation relies on maintaining relative similarity relationships when reducing dimensionality, which is non-trivial when using arbitrary linear or nonlinear transformations
  - Quick check question: Why might simple PCA fail to preserve similarity relationships in high-dimensional embeddings?

- Concept: Residual learning and skip connections
  - Why needed here: The adaptor uses a residual architecture (cei + f(cei)) to ensure the base embedding information is preserved while learning task-specific adjustments
  - Quick check question: What advantage does a residual architecture provide over learning the full transformed embedding?

- Concept: Contrastive learning and ranking losses
  - Why needed here: The supervised mode uses ranking loss to align query-corpus pairs, which requires understanding how to optimize for relative ordering rather than absolute values
  - Quick check question: How does ranking loss differ from classification loss in terms of what it optimizes?

## Architecture Onboarding

- Component map:
  Base embedding model (black box) -> Adaptor function f (learnable MLP with skip connection) -> Loss computation module (pairwise, top-k, ranking losses) -> Training loop with corpus embeddings (and query-corpus pairs for supervised) -> Inference pipeline that applies f to embeddings

- Critical path:
  1. Extract embeddings from corpus (and query-corpus pairs if supervised)
  2. Train adaptor function using appropriate loss functions
  3. Apply trained adaptor to new query and corpus embeddings
  4. Compute cosine similarity in reduced dimensions

- Design tradeoffs:
  - Deeper adaptor networks could capture more complex transformations but risk overfitting and increased latency
  - Different loss function combinations could prioritize different aspects of similarity preservation
  - The choice between unsupervised and supervised modes depends on available labeled data

- Failure signatures:
  - If adaptor produces embeddings that are too far from original (high reconstruction loss), the base model's semantic structure is lost
  - If adaptor doesn't improve performance over baseline, the loss functions may be poorly configured or the residual space insufficient
  - If training is unstable, the loss function coefficients (α, β, γ) may need adjustment

- First 3 experiments:
  1. Apply adaptor in unsupervised mode to a small corpus with a known embedding model, verify similarity preservation
  2. Compare adaptor performance against PCA on the same corpus, measure nDCG@10 across dimensions
  3. Test supervised adaptor with a small labeled dataset, compare against unsupervised results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Matryoshka-Adaptor perform when applied to very high-dimensional embeddings (e.g., 4096 dimensions or more) that are becoming more common with larger LLMs?
- Basis in paper: [inferred] The paper demonstrates effectiveness on embeddings up to 3072 dimensions but doesn't explore higher dimensionalities that are emerging with larger models
- Why unresolved: The current evaluation focuses on embeddings up to 3072 dimensions, but larger models with higher dimensional embeddings are becoming more prevalent in practice
- What evidence would resolve it: Empirical results showing performance on embeddings with 4096+ dimensions across various model architectures and datasets

### Open Question 2
- Question: What is the optimal trade-off between supervised and unsupervised tuning for Matryoshka-Adaptor in practical applications with limited labeled data?
- Basis in paper: [explicit] The paper mentions both approaches work but doesn't provide a systematic comparison of their relative benefits or how to optimally combine them
- Why unresolved: While both methods are evaluated separately, the paper doesn't explore hybrid approaches or provide guidelines for when to use each method
- What evidence would resolve it: Systematic experiments comparing various ratios of supervised to unsupervised data, and analysis of performance degradation with different amounts of labeled data

### Open Question 3
- Question: How does Matryoshka-Adaptor affect downstream task performance beyond information retrieval, such as classification or semantic search in specialized domains?
- Basis in paper: [inferred] The paper focuses primarily on retrieval tasks and mentions generalizability but doesn't empirically validate performance on other downstream applications
- Why unresolved: While the framework is presented as generally applicable, all experimental results focus on retrieval tasks, leaving questions about its effectiveness in other domains
- What evidence would resolve it: Empirical results showing performance on classification, clustering, or domain-specific tasks like medical literature search or legal document analysis

## Limitations

- The residual function may have limited capacity to capture complex task-specific similarity adjustments, particularly for domains where the original embedding space poorly represents the target task
- The adaptor requires substantial computational resources during training (processing all corpus embeddings) and introduces inference latency through the additional transformation step
- The paper lacks theoretical analysis of the residual space capacity and conditions under which the approach might fail

## Confidence

*High confidence* in the core claim that the adaptor achieves 2-12x dimensionality reduction while maintaining nDCG@10 performance, supported by extensive experiments across BEIR, MIRACL, and Fashion-200K datasets with multiple embedding models.

*Medium confidence* in the mechanism claims, particularly regarding why residual learning is sufficient for preserving similarity structure across reduced dimensions. While the loss functions are well-defined and experiments show effectiveness, the paper lacks theoretical analysis of the residual space capacity.

*Medium confidence* in the claim that the framework works seamlessly with API-only models, as experiments with OpenAI and Google APIs provide practical evidence, but the paper doesn't deeply analyze edge cases or failure modes specific to black-box implementations.

## Next Checks

1. **Theoretical capacity analysis**: Conduct an analysis of the residual space capacity relative to the original embedding space, establishing bounds on how much dimensionality reduction is theoretically possible before similarity preservation becomes impossible.

2. **Latency-cost tradeoff evaluation**: Measure end-to-end latency including both adaptor inference time and reduced embedding distance computation time, comparing against baseline to quantify the practical benefit of dimensionality reduction.

3. **Domain robustness testing**: Test the adaptor on datasets where the base embedding model is known to perform poorly, evaluating whether the residual learning can compensate for fundamental representational limitations in the base embeddings.