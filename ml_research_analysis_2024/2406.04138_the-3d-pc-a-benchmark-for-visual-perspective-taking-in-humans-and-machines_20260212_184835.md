---
ver: rpa2
title: 'The 3D-PC: a benchmark for visual perspective taking in humans and machines'
arxiv_id: '2406.04138'
source_url: https://arxiv.org/abs/2406.04138
tags:
- dnns
- depth
- humans
- vpt-basic
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the 3D-PC, a novel benchmark to compare visual
  perspective taking (VPT) between humans and machines. The benchmark uses 3D Gaussian
  Splatting to generate realistic scenes where observers must judge object depth order
  and VPT tasks.
---

# The 3D-PC: a benchmark for visual perspective taking in humans and machines

## Quick Facts
- arXiv ID: 2406.04138
- Source URL: https://arxiv.org/abs/2406.04138
- Reference count: 28
- Primary result: Human participants achieved 87% accuracy on VPT tasks while most DNNs were at chance level, revealing a significant gap in 3D reasoning capabilities

## Executive Summary
This paper introduces the 3D-PC benchmark to compare visual perspective taking (VPT) between humans and machines. Using 3D Gaussian Splatting to generate realistic scenes, the study reveals that humans solve VPT by estimating line-of-sight (achieving 87% accuracy) while most DNNs rely on brittle feature-based shortcuts and perform at chance level. Interestingly, DNNs excel at depth order perception, and this performance correlates strongly with object classification accuracy, suggesting monocular depth cues emerge alongside object recognition. The benchmark and dataset are released to help bridge the gap in 3D perception between humans and machines.

## Method Summary
The 3D-PC benchmark generates 3D scenes using Gaussian Splatting models trained on Co3D videos, rendered in Unity with ground truth depth and visibility labels. Human participants (N=33) and various DNN architectures (CNNs, Transformers, VLMs) are evaluated on depth order and VPT tasks. Models are tested via linear probing of embeddings and fine-tuning, with performance compared to human benchmarks. The VPT-Strategy task specifically tests whether observers estimate line-of-sight by holding camera position fixed while moving target objects.

## Key Results
- Humans achieved 87% accuracy on VPT tasks while most DNNs were at chance level
- DNN depth order performance correlated strongly with object classification accuracy on ImageNet
- Fine-tuning improved DNN VPT-basic performance but led to overfitting, with performance dropping back to chance on VPT-Strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DNNs learn depth perception via implicit monocular cues that correlate with object classification performance
- Mechanism: As DNNs improve at object classification, they implicitly extract depth-related features (occlusion, relative size, lighting gradients) that are useful for both tasks
- Core assumption: The ImageNet training objective indirectly rewards features that encode 3D structure
- Evidence anchors:
  - [abstract] "DNN accuracy on this task correlated with their object recognition performance"
  - [section] "Consistent with our hypothesis, we found a strong and significant correlation between DNN performance on ImageNet and depth order task accuracy"

### Mechanism 2
- Claim: Humans solve VPT by estimating line-of-sight, while DNNs rely on brittle feature-based shortcuts
- Mechanism: Humans mentally place themselves in the green camera's position and simulate visibility; DNNs instead memorize patterns like relative object positions and sizes
- Core assumption: The VPT-Strategy task, which holds camera fixed while moving target objects, reveals whether the observer estimates line-of-sight
- Evidence anchors:
  - [abstract] "Humans, on the other hand, are 87% accurate; they likely estimate line-of-sight"
  - [section] "Since DNNs cannot generalize to VPT-Strategy, it means that they do not learn to estimate line-of-sight to solve VPT"

### Mechanism 3
- Claim: Fine-tuning on VPT-basic improves performance but leads to overfitting to specific visual features rather than generalizable 3D reasoning
- Mechanism: Direct training on VPT-basic allows DNNs to memorize the feature patterns that correlate with VPT labels in the training set, but these patterns don't generalize to novel configurations
- Core assumption: The attribution maps showing strong localization of target objects indicate reliance on specific features rather than abstract 3D reasoning
- Evidence anchors:
  - [abstract] "Fine-tuning DNNs on VPT-basic boosted their performance to near human level. However, the performance of the DNNs — but not humans — dropped back to chance on VPT-Strategy"
  - [section] "These attribution maps revealed that the DNNs we evaluated did indeed learn to attend to the locations of the green camera and red ball objects"

## Foundational Learning

- Concept: Gaussian Splatting for 3D scene representation
  - Why needed here: Enables generation of infinite, realistic 3D scenes with precise ground-truth depth and visibility labels
  - Quick check question: How does Gaussian Splatting differ from NeRF in terms of computational efficiency and visual quality?

- Concept: Linear probing vs. fine-tuning
  - Why needed here: Distinguishes between emergent capabilities from pretraining versus learned capabilities from task-specific training
  - Quick check question: What's the key difference in what each method reveals about DNN capabilities?

- Concept: Visual perspective taking vs. depth perception
  - Why needed here: These are distinct cognitive abilities; depth perception can exist without the ability to reason about another viewpoint
  - Quick check question: Can you solve a VPT task if you can perfectly judge depth order? Why or why not?

## Architecture Onboarding

- Component map: Co3D videos -> Gaussian Splatting models -> Unity scene generation -> image rendering with ground truth labels -> linear probing/fine-tuning -> human psychophysics evaluation

- Critical path:
  1. Generate 3D-PC dataset using Gaussian Splatting and Unity
  2. Evaluate all models on depth order and VPT-basic via linear probing
  3. Fine-tune top models on VPT-basic
  4. Test fine-tuned models on VPT-Strategy
  5. Analyze human vs. model performance differences

- Design tradeoffs:
  - Dataset generation: More scenes vs. more viewpoints per scene
  - Model evaluation: Linear probing (emergent capabilities) vs. fine-tuning (learnable capabilities)
  - VPT-Strategy design: More complex occlusions vs. clearer line-of-sight manipulation

- Failure signatures:
  - DNNs failing VPT-Strategy after fine-tuning → overfitting to specific features
  - Weak correlation between ImageNet accuracy and VPT performance → task-specific reasoning needed
  - Human performance drop on complex VPT tasks → cognitive load or strategy change

- First 3 experiments:
  1. Generate a small 3D-PC dataset (10 scenes, 50 images each) and verify ground truth labels are correct
  2. Linear probe a simple CNN (e.g., ResNet-18) on depth order and VPT-basic to establish baseline performance
  3. Create a controlled VPT-Strategy subset with clear line-of-sight changes and test a fine-tuned model on it

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific monocular depth cues do humans and DNNs rely on when solving the depth order task, and how do these strategies differ?
- Basis in paper: [inferred] The paper notes that while both humans and DNNs are very capable at the depth order task, it's unclear whether they rely on the same cues like relative size, occlusion, lighting, or shadows
- Why unresolved: The authors state that future work is needed to drill down on the specific strategies used by humans and DNNs for perceiving 3D spatial properties
- What evidence would resolve it: Controlled experiments systematically varying individual depth cues (e.g., keeping relative size constant while varying occlusion) while measuring human and DNN performance would reveal which cues each relies on

### Open Question 2
- Question: Can architectural modifications or alternative training paradigms enable DNNs to develop VPT capabilities without direct fine-tuning on VPT tasks?
- Basis in paper: [explicit] The authors found that even after fine-tuning, DNNs rely on brittle feature-based strategies rather than line-of-sight estimation like humans do
- Why unresolved: The study demonstrates current DNN architectures struggle with VPT despite emergent 3D perception capabilities, but doesn't explore architectural modifications
- What evidence would resolve it: Developing and testing novel architectures or training approaches (e.g., incorporating embodied learning or explicit 3D reasoning modules) that achieve human-level VPT without direct task-specific fine-tuning

### Open Question 3
- Question: What is the precise relationship between object recognition accuracy and the development of monocular depth cues in DNNs?
- Basis in paper: [explicit] The authors found strong correlations between ImageNet accuracy and performance on both depth order and VPT-basic tasks, suggesting monocular depth cues emerge alongside object recognition
- Why unresolved: While correlations are observed, the paper notes that "More work is needed to identify a causal relationship between the development of monocular depth cues and object recognition accuracy"
- What evidence would resolve it: Experiments manipulating the relationship between object recognition and depth perception during training (e.g., training on depth tasks before vs. after object recognition) to establish causality

## Limitations
- Dataset Generalization: The 3D-PC benchmark uses synthetic scenes generated from Co3D videos, which may not fully capture real-world complexity and variability
- Human Participant Variability: The human psychophysics study involved 33 participants, which may not capture the full range of individual differences in VPT abilities
- Model Evaluation Scope: The study focuses primarily on standard computer vision models and limited VLMs, potentially missing other architectures or training paradigms

## Confidence
- High Confidence: DNNs can learn depth order perception through implicit monocular cues that correlate with object classification performance
- Medium Confidence: Humans solve VPT by estimating line-of-sight while DNNs rely on brittle feature-based shortcuts
- Medium Confidence: Fine-tuning on VPT-basic leads to overfitting to specific visual features rather than generalizable 3D reasoning

## Next Checks
1. Test the same DNN models on VPT tasks using real-world 3D datasets (e.g., Matterport3D, Replica) to verify whether the observed performance gaps persist across different data distributions and scene complexities

2. Systematically remove different types of depth-related features (occlusion boundaries, relative size, lighting gradients) from the input images to determine which cues are most critical for DNN depth order performance and whether this correlates with object classification accuracy

3. Conduct think-aloud protocols or eye-tracking studies with human participants during VPT tasks to directly observe whether they employ line-of-sight estimation strategies and how this varies with task complexity and individual differences