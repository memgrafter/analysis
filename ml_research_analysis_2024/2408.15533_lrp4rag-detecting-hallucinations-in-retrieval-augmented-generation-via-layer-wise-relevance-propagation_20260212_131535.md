---
ver: rpa2
title: 'LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise
  Relevance Propagation'
arxiv_id: '2408.15533'
source_url: https://arxiv.org/abs/2408.15533
tags:
- context
- relevance
- hallucination
- lrp4rag
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LRP4RAG, a method based on Layer-wise Relevance
  Propagation (LRP) for detecting hallucinations in Retrieval-Augmented Generation
  (RAG). LRP4RAG analyzes the relevance between retrieved context and generated answers,
  then prunes context and checks consistency between the model's internal evidence
  and explicit reasoning to identify hallucinations.
---

# LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation

## Quick Facts
- arXiv ID: 2408.15533
- Source URL: https://arxiv.org/abs/2408.15533
- Authors: Haichuan Hu; Congqing He; Xiaochen Xie; Quanjun Zhang
- Reference count: 19
- Key outcome: LRP4RAG achieves 77.2% accuracy on RAGTruth and 76.2% on Dolly-15k, outperforming strongest baselines by 4.1% and 5.1% respectively

## Executive Summary
This paper introduces LRP4RAG, a method for detecting hallucinations in Retrieval-Augmented Generation (RAG) systems using Layer-wise Relevance Propagation (LRP). The approach analyzes the relevance between retrieved context and generated answers, then prunes context and checks consistency between the model's internal evidence and explicit reasoning to identify hallucinations. Evaluated on two datasets with two model sizes, LRP4RAG demonstrates state-of-the-art performance with 77.2% accuracy on one test set and 76.2% on another.

## Method Summary
LRP4RAG uses Layer-wise Relevance Propagation to compute relevance distributions between context and generated answers in RAG systems. The method has two variants: LRP4RAGClassifier converts relevance matrices to feature vectors for standard ML classification, while LRP4RAGLLM prunes context using LRP relevance and performs consistency checks between internal and explicit evidence. The approach leverages LRP rules adapted for transformers using local Jacobian for relevance distribution computation.

## Key Results
- LRP4RAG achieves 77.2% accuracy on RAGTruth dataset, outperforming strongest baseline by 4.1%
- LRP4RAG achieves 76.2% accuracy on Dolly-15k dataset, outperforming strongest baseline by 5.1%
- LRP4RAGClassifier and LRP4RAGLLM variants provide flexibility between accuracy and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
LRP computes relevance distribution that reflects how much each input token contributes to the model's output. Layer-wise Relevance Propagation backpropagates output relevance through layers using predefined LRP rules (epsilon-LRP), assigning relevance scores to input tokens based on their contribution to the final prediction. The relevance distribution computed by LRP accurately reflects the model's internal reasoning process and can distinguish between normal and hallucinated outputs.

### Mechanism 2
Hallucinations occur when the model's internal evidence (LRP relevance) diverges from its explicit evidence (context the model explicitly deems important). LRP4RAGLLM compares two pruned contexts - one from LRP relevance (internal evidence) and one from LLM self-assessment (explicit evidence) - and checks consistency between both contexts and the generated answer. The model's internal reasoning (reflected in LRP relevance) should align with what the model explicitly identifies as important context.

### Mechanism 3
Context pruning based on LRP relevance improves hallucination detection by focusing on the most relevant parts of the context. LRP4RAG prunes the original context by retaining only top-k% of sentences ranked by sentence-level relevance scores, making it easier for the model to self-verify the correctness of its answer. Shorter, more focused contexts retain sufficient information for the model to accurately assess its own answer's correctness.

## Foundational Learning

- **Concept: Layer-wise Relevance Propagation (LRP) algorithm**
  - Why needed here: LRP is the core mechanism for computing context-to-answer relevance distribution that distinguishes normal from hallucinated samples
  - Quick check question: Can you explain how epsilon-LRP distributes relevance from output layer back to input tokens?

- **Concept: Context pruning and consistency checking**
  - Why needed here: These techniques transform LRP relevance scores into actionable evidence for hallucination detection
  - Quick check question: How would you implement sentence-level relevance aggregation from token-level LRP scores?

- **Concept: Binary classification using feature vectors**
  - Why needed here: LRP4RAGClassifier converts relevance matrix to features for standard ML classification
  - Quick check question: What features would you extract from a relevance matrix to represent context-to-answer relationships?

## Architecture Onboarding

- **Component map**: Retriever → Generator → LRP Engine → LRP4RAGLLM/LRP4RAGClassifier → Hallucination Detector
- **Critical path**: Retriever fetches context → Generator produces answer → LRP backward computes relevance → Context pruning retains relevant parts → Consistency check validates answer → Hallucination detection outputs binary result
- **Design tradeoffs**: LRP4RAGLLM vs LRP4RAGClassifier (accuracy vs computational efficiency), Context pruning ratio (information retention vs noise reduction), Feature vector length (completeness vs computational cost)
- **Failure signatures**: Low recall (missing hallucinated samples), Low precision (false positives), Slow inference (LRP backward bottleneck), Inconsistent results (pruning removes critical information)
- **First 3 experiments**: 1) Run threshold-based detection on RAGTruth with different relevance thresholds, 2) Compare LRP4RAGLLM with LRP4RAGClassifier on small subset, 3) Perform ablation study by removing each consistency check

## Open Questions the Paper Calls Out

### Open Question 1
How does LRP4RAG perform on non-QA RAG tasks such as classification, information extraction, and brainstorming? The paper acknowledges that current evaluation is limited to QA-type benchmarks and performance may not generalize to other RAG tasks. Comprehensive experimental results on non-QA tasks would resolve this.

### Open Question 2
What is the impact of data leakage on LRP4RAG's performance when using larger, black-box LLMs? The authors discuss potential data leakage risks with pre-trained models but don't provide experimental evidence on how this might affect performance with black-box LLMs. Experimental results comparing performance with black-box vs open-source models would resolve this.

### Open Question 3
How sensitive is LRP4RAG's hallucination detection to variations in context pruning thresholds? The paper presents results for specific pruning values (5%, 10%, 20%, 30%, 50%) but doesn't analyze sensitivity to small variations within this range. A detailed sensitivity analysis would resolve this.

## Limitations
- Limited direct evidence for LRP adaptation in RAG hallucination detection - relies on general LRP literature rather than RAG-specific validation
- Context pruning optimization unknown - optimal pruning ratio and sentence selection criteria not specified
- Inconsistent baseline comparison - varying baselines across datasets make absolute performance gains difficult to assess

## Confidence

- **High confidence**: Empirical methodology and evaluation framework are well-specified with measurable performance metrics
- **Medium confidence**: LRP adaptation mechanism is theoretically sound but lacks direct RAG validation evidence
- **Low confidence**: Context pruning effectiveness and optimal parameter selection (relevance thresholds, pruning ratios, feature vector lengths) not thoroughly validated across datasets

## Next Checks

1. **Ablation study on context pruning**: Systematically vary context pruning ratio (10%, 20%, 30% of sentences retained) and measure impact on hallucination detection accuracy to identify optimal pruning threshold

2. **Cross-dataset generalization test**: Evaluate LRP4RAG on additional RAG datasets with different characteristics (multi-hop reasoning, different domains) to assess whether performance gains generalize beyond current datasets

3. **Runtime efficiency analysis**: Measure computational overhead of LRP backward pass (0.131s per token) across different model sizes and contexts to determine practical deployment constraints and identify optimization opportunities