---
ver: rpa2
title: Enabling Real-Time Conversations with Minimal Training Costs
arxiv_id: '2409.11727'
source_url: https://arxiv.org/abs/2409.11727
tags:
- user
- input
- idle
- arxiv
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DUO, a method that equips large language
  models with real-time duplex conversation capabilities at minimal training cost.
  The core idea is a channel-division-multiplexing decoding strategy that enables
  simultaneous autoregressive output generation and input processing through a new
  parallel decoding mechanism.
---

# Enabling Real-Time Conversations with Minimal Training Costs

## Quick Facts
- arXiv ID: 2409.11727
- Source URL: https://arxiv.org/abs/2409.11727
- Authors: Wang Xu; Shuo Wang; Weilin Zhao; Xu Han; Yukun Yan; Yudi Zhang; Zhe Tao; Zhiyuan Liu; Wanxiang Che
- Reference count: 5
- Key outcome: DUO achieves duplex conversation with 10K training samples vs baseline's 5,000K

## Executive Summary
This paper introduces DUO, a method that equips large language models with real-time duplex conversation capabilities at minimal training cost. The core idea is a channel-division-multiplexing decoding strategy that enables simultaneous autoregressive output generation and input processing through a new parallel decoding mechanism. The method uses state tokens to determine when to respond to queries or ignore non-query input, requiring only a small dataset for training.

## Method Summary
DUO implements a channel-division-multiplexing decoding strategy that processes input and output channels cyclically in each time slice. While the output channel generates tokens autoregressively, the input channel prefills the key-value cache and predicts next tokens. The attention mask is modified to prevent cross-channel token attendance while sharing prefix tokens. State tokens (<1> for queries, <2> for non-queries) signal when the model should transition between channels. This approach preserves the original model's capabilities while requiring minimal additional training.

## Key Results
- Human evaluation shows DUO significantly outperforms baseline in responsiveness (77% win rate) and human-likeness (72% win rate)
- DUO maintains comparable factuality and faithfulness to baseline while achieving duplex functionality
- Training efficiency: DUO requires only 10K training samples versus baseline's 5,000K samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Channel-division-multiplexing decoding enables simultaneous autoregressive output generation and input processing without increasing forward passes
- Mechanism: The model processes input and output channels cyclically in each time slice. While the output channel generates tokens autoregressively, the input channel prefills the key-value cache and predicts next tokens. The attention mask is modified to prevent cross-channel token attendance while sharing prefix tokens.
- Core assumption: The key-value cache management allows independent processing of input and output channels without requiring additional forward passes beyond standard decoding
- Evidence anchors:
  - [abstract]: "Our method employs parallel decoding of queries and responses in conversations, effectively implementing a channel-division-multiplexing decoding strategy"
  - [section 2.1]: "DUO processes the input and output channel in each time slice. The model generates output concurrently while the key-value cache of input is prefilled and the next tokens of input are predicted"
  - [corpus]: Weak - corpus contains related work on duplex models but lacks direct evidence of this specific multiplexing mechanism
- Break condition: If key-value cache cannot be efficiently managed to maintain independent channel processing, or if attention mask modifications break autoregressive generation

### Mechanism 2
- Claim: State tokens (α tokens) enable the model to determine when and whether to respond to queries
- Mechanism: The model predicts α tokens to complete the input query in each processing cycle. State tokens signal different behaviors: <1> indicates a user query requiring response (input channel transitions to output), <2> indicates non-query text (model continues original response), otherwise the input is disregarded
- Core assumption: The model can learn to predict these state tokens accurately with minimal training, enabling appropriate channel transitions
- Evidence anchors:
  - [section 2.2]: "We use state tokens to indicate the status of the user's query... State token <1> denotes that the input is a user query... State token <2> denotes that the new input is no-query text"
  - [abstract]: "Furthermore, two state tokens are designed to signal whether a query should be addressed or ignored"
  - [corpus]: Moderate - related duplex models use similar state signaling but with different mechanisms (time-division vs channel-division)
- Break condition: If the model fails to learn state token prediction accurately, leading to inappropriate responses or missed queries

### Mechanism 3
- Claim: Minimal training (10K samples) is sufficient because the method preserves the original model's capabilities
- Mechanism: By using a channel-division-multiplexing approach that maintains the original data format with only state token additions, the model only needs to learn query state recognition rather than complete duplex processing
- Core assumption: The original model's language understanding capabilities are sufficient, and only the decoding strategy needs adaptation
- Evidence anchors:
  - [abstract]: "requiring only a small dataset for training... requiring minimal additional training... requiring minimal training costs"
  - [section 2.2]: "DUO differs from MiniCPM-Duplex... Our proposed method preserves the model's original capabilities to a greater extent. We solely train the model to recognize the state of the user's query, a process that demands minimal computational resources"
  - [corpus]: Strong - the comparison with MiniCPM-Duplex's 5,000K samples vs DUO's 10K samples provides clear evidence of training efficiency
- Break condition: If preserving original capabilities proves insufficient for duplex functionality, requiring more extensive retraining

## Foundational Learning

- Concept: Autoregressive generation and attention mechanisms
  - Why needed here: Understanding how LLMs generate text token-by-token and how attention allows context consideration is fundamental to grasping how DUO's parallel decoding works
  - Quick check question: How does the modified attention mask in DUO prevent cross-channel token attendance while maintaining autoregressive generation?

- Concept: Key-value cache in transformer models
  - Why needed here: DUO's channel-division-multiplexing relies on efficient key-value cache management to process input and output simultaneously without increasing forward passes
  - Quick check question: What role does the key-value cache play in enabling simultaneous input processing and output generation in DUO?

- Concept: State token prediction and classification
  - Why needed here: The model must accurately predict state tokens (<1> for queries, <2> for non-queries) to determine appropriate responses, requiring understanding of classification in LLMs
  - Quick check question: How does the model learn to distinguish between query and non-query inputs using state tokens?

## Architecture Onboarding

- Component map: Input channel → Key-value cache prefilling → Next token prediction → State token detection → Channel transition decision → Output channel → Autoregressive generation
- Critical path: Input token reception → Key-value cache prefilling → Next token prediction → State token detection → Channel transition decision → Output generation (if applicable)
- Design tradeoffs: Channel-division-multiplexing vs time-division-multiplexing (DUO vs MiniCPM-Duplex) - DUO preserves more original capabilities but may have limitations in handling complex overlapping interactions
- Failure signatures: Inappropriate channel transitions (responding to non-queries or ignoring queries), increased latency in response generation, breakdown in autoregressive generation quality
- First 3 experiments:
  1. Test state token prediction accuracy on a validation set of queries and non-queries
  2. Measure response latency and correctness in simple duplex scenarios with controlled interruptions
  3. Benchmark against MiniCPM-Duplex on responsiveness and human-likeness while monitoring factuality and faithfulness

## Open Questions the Paper Calls Out
None provided in the input

## Limitations
- Efficiency gains from channel-division-multiplexing remain theoretical without direct latency measurements comparing DUO to standard turn-based LLMs in real-time settings
- State token prediction accuracy is critical but not thoroughly validated with false positive/negative rates across different conversational contexts
- Evaluation focuses on subjective preference metrics rather than objective measures of conversational quality, task completion rates, or error propagation in overlapping input scenarios

## Confidence
- **High Confidence**: DUO achieves duplex conversation capabilities with significantly less training data than baseline methods (10K vs 5,000K samples)
- **Medium Confidence**: DUO improves responsiveness (77% win rate) and human-likeness (72% win rate) in human evaluations
- **Low Confidence**: DUO maintains factuality and faithfulness comparable to baseline while achieving duplex functionality

## Next Checks
1. Conduct detailed error analysis of state token predictions, measuring false positive and false negative rates across different query types and conversation contexts
2. Measure actual inference latency for DUO versus standard turn-based LLMs under realistic conditions, including token generation rates and memory overhead
3. Test DUO on specific conversational tasks with overlapping inputs to measure objective task completion rates and accuracy compared to turn-based baselines