---
ver: rpa2
title: 'Multilingual Retrieval Augmented Generation for Culturally-Sensitive Tasks:
  A Benchmark for Cross-lingual Robustness'
arxiv_id: '2410.01171'
source_url: https://arxiv.org/abs/2410.01171
tags:
- documents
- languages
- china
- language
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BORDIRLINES, a dataset pairing territorial
  dispute queries with Wikipedia documents in 49 languages to study cross-lingual
  robustness in retrieval-augmented generation (RAG). The authors formalize five multilingual
  IR modes and evaluate how document linguistic composition affects responses from
  several LLMs.
---

# Multilingual Retrieval Augmented Generation for Culturally-Sensitive Tasks: A Benchmark for Cross-lingual Robustness

## Quick Facts
- arXiv ID: 2410.01171
- Source URL: https://arxiv.org/abs/2410.01171
- Reference count: 36
- Primary result: Dataset pairing territorial dispute queries with Wikipedia documents in 49 languages to study cross-lingual robustness in retrieval-augmented generation (RAG)

## Executive Summary
This paper introduces BORDIRLINES, a dataset designed to evaluate cross-lingual robustness in retrieval-augmented generation systems. The dataset pairs territorial dispute queries with Wikipedia documents across 49 languages, enabling systematic study of how linguistic composition of retrieved documents affects LLM responses. The authors formalize five multilingual IR modes and demonstrate that multilingual retrieval (rel_langs) improves cross-lingual consistency and reduces geopolitical bias compared to in-language retrieval (qlang). The study reveals important patterns about citation behavior across languages and highlights the importance of diverse, multilingual document sources for equitable information access.

## Method Summary
The authors construct BORDIRLINES by selecting territorial disputes as culturally sensitive topics and pairing them with Wikipedia articles in 49 languages. They formalize five multilingual IR modes: query-language only (qlang), related-language only (rel_langs), combined query+related (qlang+rel_langs), cross-lingual reranking (clr), and contrastive selection (contrast). The evaluation measures cross-lingual consistency using average normalized Levenshtein similarity, quantifies geopolitical bias by analyzing which entity is mentioned first in responses, and tracks citation rates across languages. Multiple LLMs are evaluated under these conditions to assess how document linguistic composition affects response quality and bias.

## Key Results
- Multilingual retrieval (rel_langs) improves cross-lingual consistency compared to in-language retrieval (qlang)
- Multilingual retrieval reduces geopolitical bias in responses compared to in-language retrieval
- Low-resource languages show higher variability in citation rates across different IR modes

## Why This Works (Mechanism)
The study demonstrates that incorporating documents from related languages during retrieval provides LLMs with broader cultural context and multiple perspectives on sensitive topics. This multilingual document pool helps mitigate the biases inherent in single-language information sources, particularly for geopolitical topics where different language communities may have varying perspectives. The IR systems' preference for query-language documents suggests that even with multilingual retrieval options, systems tend to default to linguistically aligned sources, which can perpetuate cultural biases if not carefully managed.

## Foundational Learning
- **Cross-lingual consistency**: Measuring similarity between responses across languages; needed to assess whether RAG systems produce coherent information regardless of query language
- **Geopolitical bias quantification**: Analyzing which entity is mentioned first in responses to territorial disputes; needed to measure cultural bias in sensitive topics
- **Citation rate analysis**: Tracking how often documents from different languages are cited in responses; needed to understand IR system preferences and potential information gaps
- **Multilingual IR modes**: Formalizing different strategies for document selection across languages; needed to systematically compare retrieval approaches
- **Low-resource language variability**: Observing higher citation rate variability for underrepresented languages; needed to identify equity issues in information access

## Architecture Onboarding
- **Component map**: Query -> IR System (5 modes) -> Retrieved Documents -> LLM Generator -> Response
- **Critical path**: Query formulation → Document retrieval (one of 5 modes) → Document concatenation → Generation prompt → LLM response
- **Design tradeoffs**: Multilingual retrieval provides broader context but increases computational cost; single-language retrieval is efficient but may perpetuate biases
- **Failure signatures**: High inconsistency across languages indicates poor cross-lingual robustness; systematic bias toward certain entities indicates geopolitical bias; low citation rates for certain languages indicate information gaps
- **First experiments**: 1) Compare cross-lingual consistency between qlang and rel_langs modes, 2) Analyze entity mention order differences across IR modes, 3) Track citation rate variability for low-resource vs high-resource languages

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset relies solely on Wikipedia, which may not represent full diversity of information sources across languages and cultures
- Evaluation focuses on territorial disputes, limiting generalizability to other culturally sensitive domains
- Does not address potential amplification of biases through the RAG pipeline where retrieval biases interact with generation model biases

## Confidence
- **High confidence**: Multilingual retrieval improves cross-lingual consistency compared to in-language retrieval
- **Medium confidence**: Multilingual retrieval reduces geopolitical bias (requires validation across more domains)
- **Medium confidence**: Low-resource languages show higher citation rate variability (needs broader validation)

## Next Checks
1. Replicate study using alternative multilingual document sources beyond Wikipedia to verify findings across different information ecosystems
2. Test methodology on additional culturally sensitive domains (medical information, historical narratives) to assess generalizability
3. Implement bias measurement frameworks that account for both retrieval and generation stages to understand bias propagation through the RAG pipeline