---
ver: rpa2
title: The Optimality of (Accelerated) SGD for High-Dimensional Quadratic Optimization
arxiv_id: '2409.09745'
source_url: https://arxiv.org/abs/2409.09745
tags:
- lemma
- have
- learning
- step
- momentum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the optimality of stochastic gradient descent\
  \ (SGD) and its accelerated variant with momentum (ASGD) for high-dimensional quadratic\
  \ optimization problems. The authors establish upper bounds on the last-iterate\
  \ error of momentum-accelerated SGD with exponentially decaying step size schedule,\
  \ and provide information-theoretic lower bounds for general algorithms under \u2113\
  \u221E constraints on the optimal solution."
---

# The Optimality of (Accelerated) SGD for High-Dimensional Quadratic Optimization

## Quick Facts
- arXiv ID: 2409.09745
- Source URL: https://arxiv.org/abs/2409.09745
- Reference count: 40
- Primary result: SGD achieves min-max optimal convergence rates for high-dimensional quadratic optimization when Hessian eigenvalues decay at rate a and optimal solution satisfies source condition with decay rate b, specifically when a ≤ b; momentum acceleration extends this to a ≤ 2b when b < a ≤ 2b.

## Executive Summary
This paper establishes the optimality of stochastic gradient descent (SGD) and its accelerated variant with momentum (ASGD) for high-dimensional quadratic optimization problems. The authors prove that SGD achieves min-max optimal convergence rates when the Hessian eigenvalues decay at rate a and the optimal solution satisfies a source condition with decay rate b, specifically when a ≤ b. Momentum acceleration improves optimality to a ≤ 2b when b < a ≤ 2b. The theoretical analysis reveals that SGD is efficient for "dense" features under ℓ∞ constraints on the optimal solution, performs well for easy problems without saturation effects, and benefits from momentum acceleration when problems are harder. These results are validated through experiments on synthetic quadratic problems and two-layer neural networks in the NTK regime.

## Method Summary
The paper studies quadratic optimization problems with Hessian eigenvalues following polynomial decay λ_i ≍ i^−a and optimal solutions satisfying source conditions λ_i (w_i^∗)^2 ≤ c_i^−b. SGD and ASGD algorithms are analyzed with exponentially decaying step size schedules η_t = η_0/4^(ℓ−1) for stage ℓ. The analysis separates convergence into bias and variance terms, with momentum acceleration shown to improve bias convergence without increasing variance order. Information-theoretic lower bounds are established to prove min-max optimality, and experiments validate the theoretical predictions on synthetic problems and two-layer ReLU networks trained on CIFAR-2 data.

## Key Results
- SGD achieves min-max optimal convergence rates when a ≤ b, where a is the Hessian eigenvalue decay rate and b is the source condition rate
- Momentum acceleration (ASGD) extends optimality to a ≤ 2b when b < a ≤ 2b by accelerating bias convergence without increasing variance
- SGD is instance-optimal for any Hessian subject to ℓ∞ constraints on the optimal solution, achieving dimension-independent rates
- Theoretical results are validated on synthetic quadratic optimization and two-layer neural networks in NTK regime

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGD achieves optimality when a ≤ b through exponentially decaying step size that adaptively selects effective dimension based on solution smoothness
- Core assumption: Hessian eigenvalues λ_i ≍ i^−a and source condition λ_i (w_i^∗)^2 ≤ c_i^−b with a ≤ b
- Evidence anchors: [abstract] shows SGD achieves optimal rates when a ≤ b; [section] demonstrates SGD variants attain optimality when a ≤ 2b
- Break condition: When a > b, problem becomes too ill-conditioned for vanilla SGD to achieve optimal rates

### Mechanism 2
- Claim: Momentum acceleration improves optimality to a ≤ 2b when b < a ≤ 2b by accelerating bias convergence without increasing variance order
- Core assumption: Momentum parameter β = (1 − A · T^−(a−b)/b)^2 where A = 256 log_2 T · ln T
- Evidence anchors: [abstract] shows momentum extends optimality boundary; [section] proves SGD variants achieve optimality when b ≤ a ≤ 2b
- Break condition: When a > 2b, neither SGD nor ASGD can achieve optimality regardless of momentum tuning

### Mechanism 3
- Claim: SGD achieves instance optimality under ℓ∞ constraints on optimal solution, independent of problem dimension
- Core assumption: Optimal solution w^* satisfies ||w^*||_∞ ≤ c in Hessian eigenspace
- Evidence anchors: [abstract] shows SGD is efficient for dense features under ℓ∞ constraints; [section] provides first justification under slow source condition decay
- Break condition: When optimal solution has sparse structure or ℓ∞ constraints are violated

## Foundational Learning

- Concept: Eigenvalue decay rates and source conditions in nonparametric regression
  - Why needed here: The optimality analysis relies on understanding how polynomial eigenvalue decay and source conditions affect convergence rates
  - Quick check question: Given λ_i = i^−3 and λ_i (w_i^∗)^2 = i^−2, what is the relationship between a and b, and which method achieves optimality?

- Concept: Bias-variance decomposition in stochastic optimization
  - Why needed here: The analysis separates convergence into bias (deviation from optimum) and variance (gradient noise) terms
  - Quick check question: In high-dimensional settings with limited queries, which term typically dominates and how does step size affect each?

- Concept: Information-theoretic lower bounds and minimax optimality
  - Why needed here: Lower bounds using min-max framework prove that upper bounds are tight
  - Quick check question: What distinguishes instance-dependent lower bounds from worst-case bounds, and why use the former approach?

## Architecture Onboarding

- Component map: Quadratic optimization framework -> Algorithm variants (SGD/ASGD) -> Step size schedule -> Bias-variance analysis -> Momentum matrix decomposition -> Information-theoretic bounds
- Critical path: 1) Characterize function class using eigenvalue decay a and source condition b, 2) Implement algorithm with decaying step size and momentum, 3) Analyze bias and variance separately, 4) Establish upper bounds, 5) Construct hard instances for lower bounds, 6) Compare upper and lower bounds
- Design tradeoffs: Faster step decay reduces variance but slows bias; larger β accelerates bias but increases variance; η_0 must balance convergence and progress
- Failure signatures: a > 2b prevents optimality regardless of tuning; too large β causes instability; too large η_0 violates convergence; small b degrades SGD performance
- First 3 experiments: 1) Synthetic problem with λ_i = i^−4 and w_i^∗ ∝ i^−0.75 testing SGD vs SHB, 2) Two-layer NTK network on CIFAR-2 comparing methods, 3) Instance optimality test with ℓ∞ constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does momentum-accelerated SGD optimality extend to non-quadratic loss functions like those in deep neural networks?
- Basis in paper: The paper establishes optimality for quadratic objectives and demonstrates practical relevance on two-layer NTK networks, but the analysis is specifically tailored to quadratic structure
- Why unresolved: The theoretical framework relies heavily on quadratic structure and eigen-decomposition, requiring new techniques for non-quadratic losses
- What evidence would resolve it: Rigorous analysis of momentum-accelerated SGD for general convex and non-convex losses, potentially using integral quadratic constraints or NTK techniques

### Open Question 2
- Question: How does momentum parameter β affect the implicit bias of SGD towards dense solutions under ℓ∞ constraints?
- Basis in paper: The paper shows SGD with momentum learns dense features under ℓ∞ constraints but doesn't fully characterize how different momentum values influence this bias
- Why unresolved: While establishing that SGD with momentum achieves optimal rates under ℓ∞ constraints, the specific role of momentum in shaping dense vs sparse solution trade-offs is not fully characterized
- What evidence would resolve it: Empirical studies varying momentum parameters while monitoring ℓ∞ norm of solutions and correlation with generalization across problem classes

### Open Question 3
- Question: Can optimality results extend to non-i.i.d. data distributions or structured noise?
- Basis in paper: The paper assumes i.i.d. data and anisotropic gradient noise satisfying Var(ζt | wt) ⪯ σ²H, while real-world data often exhibits correlations
- Why unresolved: The theoretical analysis relies on independence assumptions to control variance terms and derive clean convergence rates
- What evidence would resolve it: Extension of information-theoretic lower bound framework to handle non-i.i.d. data using statistical learning theory techniques, validated on real-world datasets

## Limitations
- Analysis assumes polynomial eigenvalue decay and source conditions that may not capture all practical problem structures
- Information-theoretic lower bounds rely on constructing specific hard instances that may not represent all problem classes
- Exponentially decaying step size schedule may not be most practical for real-world applications with validation-based tuning

## Confidence
- High confidence: Optimality regions (a ≤ b for SGD, a ≤ 2b for ASGD) as rigorously proven through matching upper and lower bounds
- Medium confidence: Practical implications and experimental validation on synthetic problems and simplified neural network setting
- Low confidence: Broader applicability to general non-quadratic problems and deep learning beyond NTK regime

## Next Checks
1. **Extended Experimental Validation**: Test theoretical predictions on more diverse neural network architectures and real-world datasets beyond CIFAR-2, including scenarios where Hessian structure deviates from assumed polynomial decay
2. **Robustness to Hyperparameter Misspecification**: Evaluate sensitivity of optimality regions to practical considerations like imperfect step size scheduling, adaptive learning rates, or momentum parameters that don't exactly match theoretical recommendations
3. **Generalization to Non-Quadratic Objectives**: Design experiments on non-quadratic loss surfaces to test whether insights about eigenvalue decay and source conditions extend to general optimization problems encountered in practice