---
ver: rpa2
title: Towards Robust Knowledge Representations in Multilingual LLMs for Equivalence
  and Inheritance based Consistent Reasoning
arxiv_id: '2410.14235'
source_url: https://arxiv.org/abs/2410.14235
tags:
- languages
- language
- llms
- across
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether large language models (LLMs) can reason
  using foundational relationships of "equivalence" and "inheritance" across multiple
  languages. The authors introduce novel tasks and benchmarks spanning six languages
  (English, French, Spanish, German, Portuguese, and Hindi) to assess LLM reasoning
  capabilities.
---

# Towards Robust Knowledge Representations in Multilingual LLMs for Equivalence and Inheritance based Consistent Reasoning

## Quick Facts
- arXiv ID: 2410.14235
- Source URL: https://arxiv.org/abs/2410.14235
- Reference count: 36
- Authors: Gaurav Arora; Srujana Merugu; Shreya Jain; Vaibhav Saxena
- Primary result: Current state-of-the-art LLMs produce conflicting answers across languages in 17.3-57.5% of cases

## Executive Summary
This paper evaluates whether large language models (LLMs) can reason using foundational relationships of "equivalence" and "inheritance" across multiple languages. The authors introduce novel tasks and benchmarks spanning six languages (English, French, Spanish, German, Portuguese, and Hindi) to assess LLM reasoning capabilities. Their analysis reveals that current state-of-the-art LLMs produce conflicting answers across languages in 17.3-57.5% of cases and violate inheritance constraints in up to 37.2% of cases. To address these inconsistencies, the authors propose a novel "Compositional Representations" method where tokens are represented as compositions of equivalent tokens across languages. This approach reduces conflicts by up to 4.7% compared to baselines, demonstrating the benefits of shared representations for improved knowledge sharing across languages.

## Method Summary
The authors evaluate multiple SOTA LLMs including Claude v1/v2/v3 Sonnet, BLOOMZ-7B, and XGLM-7.5B on novel tasks spanning six languages. They create a dataset of 88,334 factual questions on well-known named entities, translated across these languages, along with synthetic controlled experiments and inheritance reasoning datasets. For their Compositional Representations (CoRe) method, they modify the attention mechanism to incorporate proximal tokens from other languages based on compatibility scores, building compositional representations that bridge distant representation spaces across languages.

## Key Results
- LLMs exhibit reasoning by equivalence with dependency on input and output language in 17.3-57.5% of cases
- LLM answers violate inheritance constraints in up to 37.2% cases across these languages
- CoRe reduces conflicts by up to 4.7% compared to baselines
- Conflict rates reduce with increase in model strength, with open-source models lagging behind closed-source models significantly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoRe improves knowledge consistency across languages by constructing compositional representations where each token's embedding is a weighted sum of its equivalent tokens across all languages.
- Mechanism: For each token in the input sequence, CoRe selects top-n proximal tokens from each language's vocabulary based on compatibility scores (dot product of query and key matrices). These proximal tokens are then used to build a compositional representation via modified attention, effectively bridging distant representation spaces.
- Core assumption: The compatibility scores between query and key vectors can effectively identify semantically equivalent tokens across languages, and incorporating these into the representation space will improve cross-lingual consistency.
- Evidence anchors:
  - [abstract] "We propose novel 'Compositional Representations' where tokens are represented as composition of equivalent tokens across languages, with resulting conflict reduction (up to -4.7%) indicating benefits of shared LLM representations."
  - [section 5] "Z′ becomes a composition of equivalent tokens from all the languages yielding more consistent responses."
  - [corpus] Weak - while related work exists on multilingual representation learning (e.g., Conneau et al., 2020), there's no direct evidence in corpus neighbors specifically about compositional representations for equivalence reasoning.

### Mechanism 2
- Claim: LLMs exhibit reasoning by equivalence when their representations of equivalent concepts across languages are sufficiently similar, allowing knowledge transfer without duplication.
- Mechanism: When LLMs learn representations where equivalent concepts across languages are mapped to similar or identical vector spaces, property transfer becomes possible through the shared representation rather than requiring separate knowledge copies in each language.
- Core assumption: LLMs can learn meaningful semantic representations that capture cross-linguistic equivalence, and these representations can be leveraged for consistent reasoning across languages.
- Evidence anchors:
  - [section 3.2.2] "For instance, conflict rate for En-De is much lower than that of En-HiEn, which in turn is lower than that of En-Hi" - showing that languages with similar typology and script have lower conflict rates, suggesting representation similarity enables knowledge transfer.
  - [section 2] "LLMs use distributional semantics, i.e., a word's meaning stems from the training data context" - establishing the mechanism by which LLMs build representations.
  - [corpus] Moderate - MultiNRC and MMLU-ProX benchmarks also evaluate cross-linguistic reasoning, suggesting this is a recognized challenge, but don't specifically address the representation similarity mechanism.

### Mechanism 3
- Claim: Reasoning by inheritance requires LLMs to maintain consistent ontological knowledge where specific instances inherit properties from their abstract parent concepts across all languages.
- Mechanism: LLMs must encode hierarchical relationships in their representations such that properties associated with abstract concepts automatically transfer to their instances, regardless of the language of expression.
- Core assumption: LLMs can capture and maintain ontological relationships (like "is-a") in their distributed representations, and these relationships are preserved across languages in the training data.
- Evidence anchors:
  - [section 4] "Our results indicate that LLM answers violate inheritance constraints in up to 37.2% cases across these languages" - demonstrating that inheritance-based reasoning fails in current LLMs.
  - [section 4] "This indicates that the ability to reason by inheritance likely depends on the amount of specific language data during training LLMs" - suggesting the mechanism depends on training data coverage.
  - [corpus] Moderate - MultiNRC specifically evaluates multilingual reasoning, which would include inheritance relationships, though not explicitly framed this way.

## Foundational Learning

- Concept: Distributional semantics vs denotational semantics
  - Why needed here: Understanding that LLMs build meaning from context (distributional) rather than direct object correspondence (denotational) is crucial for why cross-lingual equivalence is challenging.
  - Quick check question: If "apple" and "सेब" (seb) refer to the same object, why might their representations differ in an LLM trained primarily on monolingual data?

- Concept: Attention mechanism in transformers
  - Why needed here: CoRe modifies the attention mechanism to incorporate proximal tokens from other languages, so understanding standard attention is prerequisite.
  - Quick check question: In standard transformer attention, what determines which tokens in the sequence influence the representation of a given token?

- Concept: Cross-lingual representation alignment
  - Why needed here: The paper's core contribution relies on aligning representations across languages, which requires understanding how multilingual models typically handle this.
  - Quick check question: What challenges arise when trying to align representations of equivalent concepts across languages with different scripts and typologies?

## Architecture Onboarding

- Component map: Transformer decoder (XGLM-4.5B) augmented with CoRe. CoRe adds proximal token selection module and modifies attention mechanism to incorporate compositional representations.
- Critical path: For each token: (1) Compute query, key, value vectors; (2) Select top-n proximal tokens from each language; (3) Build compatibility matrix with selected tokens; (4) Apply modified attention with compositional representation; (5) Pass result to subsequent layers.
- Design tradeoffs: CoRe increases computational cost by considering vocabulary-wide token compatibility rather than just sequence tokens, but this is mitigated by selecting only top-n candidates. The approach trades efficiency for improved cross-lingual consistency.
- Failure signatures: High conflict rates across languages despite CoRe application, inconsistent results when varying n, failure to improve on downstream tasks despite reduced conflicts, or degraded performance on monolingual tasks.
- First 3 experiments:
  1. Implement baseline XGLM-4.5B and verify it reproduces the conflict rates reported in Tables 1-3 on the parallel QA dataset.
  2. Add CoRe with n=5 and evaluate conflict reduction on the controlled synthetic dataset from Section 3.2.1, comparing to baseline.
  3. Test CoRe with different values of n (5, 10, 15) on En-De parallel data to identify optimal parameter setting and observe how conflict rates change with n.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model sizes affect reasoning consistency across languages beyond the models tested?
- Basis in paper: [explicit] The paper shows that conflict rates reduce with increase in model strength, with open-source models lagging behind closed-source models significantly.
- Why unresolved: The paper only tested specific model families (Claude v1/v2/v3, BLOOMZ, XGLM) across different sizes. The relationship between model size and reasoning consistency across languages may not be linear and could vary based on architecture or training data composition.
- What evidence would resolve it: Systematic testing of models across a broader range of sizes (e.g., 1B, 3B, 7B, 13B, 70B+ parameters) while controlling for architecture and training data composition would clarify how model size affects cross-lingual reasoning consistency.

### Open Question 2
- Question: Can CoRe be effectively extended to asymmetric and transitive relationships beyond equivalence?
- Basis in paper: [explicit] The authors note that CoRe is currently applicable only to symmetric relationships such as "equivalence" but mention the possibility of extending to asymmetric and transitive relationships using hyperbolic representations.
- Why unresolved: The paper proposes CoRe specifically for equivalence relationships and only mentions the theoretical possibility of extending it to other relationship types without demonstrating this capability.
- What evidence would resolve it: Implementation and evaluation of CoRe for asymmetric relationships (like parent-child inheritance) and transitive relationships would demonstrate whether the approach generalizes beyond symmetric relationships.

### Open Question 3
- Question: What is the impact of CoRe on long-form generation tasks that require multi-step reasoning?
- Basis in paper: [explicit] The paper focuses on objective, factoid-based questions for evaluation due to clarity and automated assessment feasibility, explicitly noting that reasoning tasks also encompass subjective, long-form generation tasks like summarization and problem solving.
- Why unresolved: The authors performed a small-scale experiment on NLI tasks but acknowledge that long-form generation tasks would require human-in-the-loop evaluations, which they did not perform due to cost and complexity.
- What evidence would resolve it: Human evaluation of CoRe's impact on long-form generation tasks across multiple languages would clarify whether the consistency improvements observed in factoid QA translate to more complex reasoning scenarios.

## Limitations

- The conflict reduction achieved by CoRe (up to 4.7%) is modest and may not justify the increased computational overhead in practical applications.
- The evaluation framework relies heavily on automated conflict detection using an LLM judge, which may introduce measurement noise.
- The paper doesn't address whether reduced conflict rates translate to improved downstream performance.

## Confidence

- Methodology: Medium - Experimental design is sound but relies on automated evaluation that may have precision issues
- Core claims about cross-lingual inconsistency: High - Well-supported by extensive evaluation across multiple models and languages
- CoRe mechanism effectiveness: Medium - Theoretically motivated but lacks extensive ablation studies and downstream task validation
- Conflict rate measurements: Medium - Automated detection using LLM judge introduces potential measurement noise

## Next Checks

1. Conduct human evaluation of a random sample of identified conflicts to establish ground truth precision of the LLM judge and calibrate reported conflict rates
2. Perform systematic ablation studies on CoRe components (proximal token selection, compatibility scoring, compositional representation) to identify which mechanisms drive improvement
3. Test CoRe on downstream multilingual reasoning tasks to verify that conflict reduction translates to meaningful performance gains beyond the synthetic evaluation settings