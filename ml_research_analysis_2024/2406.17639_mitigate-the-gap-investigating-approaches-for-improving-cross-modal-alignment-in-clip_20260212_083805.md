---
ver: rpa2
title: 'Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment
  in CLIP'
arxiv_id: '2406.17639'
source_url: https://arxiv.org/abs/2406.17639
tags:
- clip
- image
- alignclip
- modality
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the modality gap in CLIP's embedding space,
  where image and text embeddings are sparsely distributed in separate regions. The
  authors propose AlignCLIP to address this issue by sharing the parameter space between
  vision and language encoders (SharedCLIP) and introducing an intra-modality separation
  objective (IMSep).
---

# Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP

## Quick Facts
- arXiv ID: 2406.17639
- Source URL: https://arxiv.org/abs/2406.17639
- Authors: Sedigheh Eslami; Gerard de Melo
- Reference count: 15
- Primary result: AlignCLIP improves cross-modal alignment in CLIP, increasing cosine similarity between paired embeddings from 0.47 to 0.67

## Executive Summary
This paper investigates the modality gap in CLIP's embedding space, where image and text embeddings are sparsely distributed in separate regions. The authors propose AlignCLIP to address this issue by sharing the parameter space between vision and language encoders (SharedCLIP) and introducing an intra-modality separation objective (IMSep). IMSep encourages semantically dissimilar embeddings within each modality to be pushed apart, thereby reducing the modality gap. Experiments show that AlignCLIP achieves noticeable improvements in cross-modal alignment, with average cosine similarity between paired image-text embeddings increasing from 0.47 to 0.67. This improvement translates to enhanced performance across various downstream tasks, including zero-shot and fine-tuned image classification, as well as cross-modal retrieval.

## Method Summary
The authors propose AlignCLIP, which consists of two main components: SharedCLIP and Intra-Modality Separation (IMSep). SharedCLIP shares the transformer encoder and projection layer between vision and language encoders, reducing the modality gap by aligning optimization trajectories. IMSep introduces an objective function that enforces reasonable distances within each modality by separating semantically dissimilar embeddings. The approach is evaluated on the CC12M dataset using a ViT-B-16 backend, with training conducted for 30 epochs using AdamW optimizer and cosine scheduler.

## Key Results
- AlignCLIP improves average cosine similarity between paired image-text embeddings from 0.47 to 0.67
- Enhanced performance across downstream tasks: zero-shot and fine-tuned image classification, cross-modal retrieval
- Improved robustness to natural distribution shifts (ImageNetV2, ImageNet-R, ImageNet-A, ImageNetSketch)
- Alignment score improvements translate to better R@1, R@5, and R@10 retrieval metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharing the transformer encoder and projection layer between vision and language encoders reduces the modality gap by aligning the optimization trajectories of the two modalities.
- Mechanism: When each modality has its own separate encoder parameters, the learned functions map inputs to different subregions of the embedding space. By sharing parameters, both modalities are forced to map to similar regions, reducing the distance between their embeddings.
- Core assumption: The same parameter initialization and shared architecture can adequately encode both modalities without losing modality-specific information.
- Evidence anchors:
  - [abstract] "We design AlignCLIP, in order to answer these questions and through extensive experiments, we show that AlignCLIP achieves noticeable enhancements in the cross-modal alignment of the embeddings"
  - [section] "we suspect that one of the main reasons that the modality gap exists in the original CLIP's embedding space is the fact that each modality has a separate disentangled set of parameters for optimization"
  - [corpus] Weak evidence. The corpus contains related papers about modality gaps but none directly confirm the parameter-sharing mechanism as described.

### Mechanism 2
- Claim: Intra-Modality Separation (IMSep) reduces the modality gap by pushing semantically dissimilar embeddings within each modality apart, causing each modality to expand toward the other.
- Mechanism: IMSep adds a vision-to-vision contrastive loss that treats image-text pairs as positives and image-image pairs as negatives. This separates dissimilar images, causing the visual embeddings to spread out and move toward the more compact text embeddings (or vice versa).
- Core assumption: The text descriptions provide sufficient semantic supervision to determine which images should be pushed apart.
- Evidence anchors:
  - [section] "We hypothesize that this phenomenon is a direct result of the cross-modal contrastive objective in CLIP, which merely optimizes the relative distances of image embeddings and text embeddings"
  - [section] "We define an objective function that enforces reasonable distances within each modality by separating the uni-modal embeddings that are semantically dissimilar"
  - [corpus] Weak evidence. While corpus papers discuss modality gaps, none provide direct evidence for this specific intra-modality separation mechanism.

### Mechanism 3
- Claim: The re-scaling mechanism in IMSep prevents over-separation of semantically similar samples by adjusting the strength of separation based on semantic similarity.
- Mechanism: The pairwise semantic similarity S is calculated from text embeddings, and the distance D = 1 - S is used to re-scale the image-image cosine similarities V. This reduces the separation force for semantically similar image pairs.
- Core assumption: The semantic similarity between texts accurately reflects the semantic similarity between their paired images.
- Evidence anchors:
  - [section] "In order to calculate the semantic similarity within the image samples, we utilize their pairing texts as the semantic supervision signal"
  - [section] "One should notice that while enforcing intra-modality separation, by minimizing the denominator in Eq. 6, some samples might indeed be semantically similar to each other and therefore, must not be separated immensely"
  - [corpus] Weak evidence. No direct confirmation in corpus papers, though related work on semantic supervision exists.

## Foundational Learning

- Concept: Contrastive learning and Info-NCE loss
  - Why needed here: The paper builds on CLIP's contrastive training framework, and understanding how Info-NCE works is essential to grasp why the modality gap exists and how proposed solutions modify it.
  - Quick check question: In Info-NCE loss, what happens to the gradient when a positive pair is very similar versus when it's dissimilar?

- Concept: Multi-modal embedding spaces and alignment metrics
  - Why needed here: The core problem is about cross-modal alignment in embedding space, and the paper measures success using cosine similarity between paired embeddings.
  - Quick check question: If two modalities are perfectly aligned, what should be the average cosine similarity between paired embeddings?

- Concept: Transformer architecture and parameter sharing
  - Why needed here: The proposed SharedCLIP architecture shares transformer parameters between modalities, requiring understanding of how transformers process different input types and how parameter sharing affects learning.
  - Quick check question: What modifications are needed to adapt a transformer originally designed for text to process image patches?

## Architecture Onboarding

- Component map: Image/text input -> Patch extraction/tokenization -> [CLS] token prepending -> Shared transformer encoder (12 layers, 12 heads) -> [CLS] token pooling (vision) / max-pooling (text) -> Shared projection layer (768-dim) -> IMSep semantic encoding (for images) -> Loss computation (CLIP + IMSep) -> Backpropagation

- Critical path:
  1. Input preprocessing (vision patches or text tokens)
  2. Shared transformer encoding
  3. Modality-specific pooling ([CLS] for vision, max-pool for text)
  4. Shared projection to 768-dim space
  5. IMSep semantic encoding (for images only)
  6. Loss computation (CLIP loss + IMSep loss)
  7. Backpropagation through shared parameters

- Design tradeoffs:
  - Shared vs. separate parameters: Tradeoff between modality gap reduction and potential loss of modality-specific representation power
  - IMSep strength (β parameter): Balance between maintaining semantic structure and achieving separation
  - Semantic encoder choice: Tradeoff between semantic accuracy and computational overhead

- Failure signatures:
  - Mode collapse: Both modalities map to identical representations, losing discriminative power
  - Over-separation: Embeddings become too sparse, harming nearest-neighbor retrieval
  - Semantic misalignment: Re-scaling fails, causing semantically similar samples to be pushed apart

- First 3 experiments:
  1. Train SharedCLIP without IMSep on CC12M, measure alignment scores and classification performance
  2. Add IMSep with varying β values, analyze impact on alignment vs. downstream task performance
  3. Replace SBERT semantic encoder with a smaller/faster alternative, measure performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does sharing the parameter space between vision and language encoders consistently reduce the modality gap across different pre-training datasets and model architectures?
- Basis in paper: [explicit] The authors investigate this question for CLIP using CC12M dataset and ViT-B-16 backend, showing noticeable improvements.
- Why unresolved: The study only examines one specific dataset (CC12M) and one architecture (ViT-B-16). Different datasets may have varying information imbalance between modalities, and other architectures might respond differently to parameter sharing.
- What evidence would resolve it: Systematic experiments testing SharedCLIP across multiple datasets (e.g., LAION, YFCC100M) and architectures (e.g., ConvNets, Swin Transformers) would clarify generalizability.

### Open Question 2
- Question: What is the optimal trade-off between cross-modal alignment and unimodal representation quality when applying intra-modality separation?
- Basis in paper: [inferred] The authors introduce IMSep to push apart semantically dissimilar embeddings within each modality, but don't systematically explore the balance between alignment and unimodal quality.
- Why unresolved: While IMSep improves cross-modal alignment, it may distort unimodal representations if pushed too far. The paper doesn't investigate this trade-off or provide guidance on optimal parameter settings.
- What evidence would resolve it: Comprehensive ablation studies varying the IMSep strength and evaluating both cross-modal alignment metrics and unimodal task performance would identify optimal trade-offs.

### Open Question 3
- Question: How does the modality gap affect robustness to distribution shifts beyond natural distribution shifts?
- Basis in paper: [explicit] The authors evaluate robustness to natural distribution shifts (ImageNetV2, ImageNet-R, ImageNet-A, ImageNetSketch) and find AlignCLIP improves performance.
- Why unresolved: The study only examines robustness to natural distribution shifts. Other types of distribution shifts (e.g., adversarial attacks, domain shifts to medical images, satellite imagery) may behave differently.
- What evidence would resolve it: Evaluating AlignCLIP on benchmarks for different types of distribution shifts (e.g., ImageNet-A for natural adversarial examples, WILDS for domain shifts, adversarial attack benchmarks) would reveal generalizability of modality gap reduction benefits.

## Limitations
- The proposed mechanisms for modality gap reduction lack direct empirical validation and rely on post-hoc reasoning
- The assumption that text-based semantic similarity accurately reflects image semantic relationships is not rigorously tested
- Limited ablation studies prevent isolating the specific contributions of parameter sharing versus IMSep

## Confidence
- **High Confidence**: Experimental results showing improved alignment scores and downstream task performance are well-documented and reproducible
- **Medium Confidence**: The overall framework of reducing modality gap through shared parameters and intra-modality separation is plausible and consistent with multi-modal learning principles
- **Low Confidence**: Specific mechanisms explaining why each component works (parameter sharing alignment, semantic-based separation, re-scaling effectiveness) lack direct supporting evidence

## Next Checks
1. **Ablation study**: Train a version with separate but equal-capacity encoders to isolate the effect of parameter sharing from architectural differences
2. **Semantic alignment test**: Compare semantic similarity rankings from text embeddings versus direct image similarity to quantify the assumption underlying IMSep
3. **Over-separation analysis**: Measure embedding sparsity and nearest-neighbor preservation before and after IMSep to detect potential over-separation issues