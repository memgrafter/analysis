---
ver: rpa2
title: Automata-based constraints for language model decoding
arxiv_id: '2407.08103'
source_url: https://arxiv.org/abs/2407.08103
tags:
- tokens
- token
- language
- outlines
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel automata-based method for constraining
  language model decoding to generate valid formal language outputs. The core idea
  is to reformulate detokenization as a finite-state transducer (FST) and use FST-FSA/PDA
  composition to adapt character-based automata to token-based inputs.
---

# Automata-based constraints for language model decoding

## Quick Facts
- **arXiv ID**: 2407.08103
- **Source URL**: https://arxiv.org/abs/2407.08103
- **Reference count**: 40
- **One-line primary result**: Automata-based method compiles constraints ~7,000x faster than prior work while achieving perfect output conformance

## Executive Summary
This paper introduces a novel approach for constraining language model decoding using automata theory. The core innovation reformulates detokenization as a finite-state transducer (FST) and leverages FST-FSA/PDA composition to adapt character-based automata to token-based inputs. This elegantly handles tokenization mismatches between formal languages and subword tokenizers while maintaining theoretical correctness.

The method is evaluated on both speed and correctness metrics, demonstrating compilation speeds approximately 7,000x faster than previous approaches while achieving perfect output conformance on structured data generation tasks. The approach is extensible to both regular expressions and deterministic context-free grammars, providing a general solution for enforcing formal language constraints on language model outputs.

## Method Summary
The method reformulates detokenization as a finite-state transducer (FST) that maps sequences of subword tokens back to character sequences. This FST is then composed with either a finite-state automaton (FSA) for regular languages or a push-down automaton (PDA) for context-free grammars. The composition operation handles tokenization mismatches automatically, allowing constraints to operate on tokens while preserving formal language structure. The paper also introduces practical extensions including terminal labels for wildcard matching and syntactic sugar for common patterns, all implemented within the automata framework without breaking theoretical guarantees.

## Key Results
- Compilation speed approximately 7,000x faster than previous work (Willard & Louf, 2023)
- Perfect output conformance on structured data generation tasks
- General solution applicable to both regular expressions and deterministic context-free grammars

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Detokenization reformulated as FST enables efficient mapping from token sequences to formal language constraints.
- **Mechanism**: The paper shows that detokenization can be represented as a finite-state transducer (FST) that converts sequences of subword tokens back into character sequences. This FST can then be composed with either a finite-state automaton (FSA) for regular languages or a push-down automaton (PDA) for context-free grammars, allowing the constraint to operate on tokens while preserving formal language structure.
- **Core assumption**: Tokenization ambiguities can be handled through FST composition without requiring manual handling of special cases.
- **Evidence anchors**:
  - [abstract]: "We solve these issues through the application of automata theory, deriving an efficient closed-form solution for the regular languages"
  - [section 2.3]: "Our first contribution is a reformulation of detokenization (i.e., the process of converting token sequences back into text) as an FST"
  - [corpus]: Weak - the corpus neighbors discuss automata extraction and expressivity but don't directly address the detokenization-as-FST mechanism
- **Break condition**: If the FST construction algorithm fails to correctly handle token boundaries or character sequences, the composition will not produce valid formal language constraints.

### Mechanism 2
- **Claim**: FST-FSA/PDA composition provides a general solution that is faster, provably correct, and more extensible than previous approaches.
- **Mechanism**: By reformulating the entire constraint problem in terms of automata, the paper leverages well-optimized FST composition algorithms that automatically handle tokenization mismatches. This replaces previous bespoke solutions with a single, general composition operation that works for both regular expressions and context-free grammars.
- **Core assumption**: Well-optimized FST composition algorithms exist and can be efficiently applied to this problem.
- **Evidence anchors**:
  - [abstract]: "Previous work on this topic (Willard & Louf, 2023) layers bespoke solutions onto automata, leading to problems with speed, correctness, and extensibility. Instead, we reformulate the entire task in terms of automata so we can leverage well-studied and well-optimized algorithms"
  - [section 6.1]: "The primary reason for the large compilation speedup is that we use OpenFST's highly-optimized implementations of FST operations"
  - [corpus]: Weak - corpus papers discuss automata but don't specifically address FST composition for language model constraints
- **Break condition**: If FST composition becomes a bottleneck or fails to handle certain tokenization patterns, the speed and correctness advantages disappear.

### Mechanism 3
- **Claim**: Extensions like wildcard matching and syntactic sugar address practical efficiency and usability concerns while preserving theoretical correctness.
- **Mechanism**: The paper introduces terminal labels that map to pre-computed token masks for wildcard patterns, and syntactic sugar for common patterns like substring matching. These extensions are implemented as special cases within the automata framework without breaking the underlying composition approach.
- **Core assumption**: Wildcard patterns can be efficiently handled through pre-computed token masks without requiring explicit FST edges for each token.
- **Evidence anchors**:
  - [section 2.5.1]: "We mitigate this issue by defining terminal labels, which are token IDs disjoint from V that map to pre-computed masks of valid tokens"
  - [section 2.5.2]: "We therefore define the syntactic sugar /(?P<SUBSTRING OF>abc)/ to match any substring of 'abc'"
  - [corpus]: Weak - corpus neighbors discuss automata expressivity but not practical extensions for efficiency
- **Break condition**: If terminal labels or syntactic sugar extensions conflict with each other or break the automata composition, the system becomes unreliable.

## Foundational Learning

- **Concept: Finite-state automata (FSA)**
  - Why needed here: FSAs provide the theoretical foundation for representing regular languages and enable composition with FSTs for token-based constraints
  - Quick check question: What is the key difference between deterministic and non-deterministic FSAs, and why does this matter for constraint compilation?

- **Concept: Finite-state transducers (FST)**
  - Why needed here: FSTs model the detokenization process and enable composition with FSAs/PDAs to handle tokenization mismatches
  - Quick check question: How does FST-FSA composition work, and why does it preserve the formal language properties of the original FSA?

- **Concept: Push-down automata (PDA)**
  - Why needed here: PDAs extend the approach to context-free languages, enabling constraints on structured data like JSON and Python code
  - Quick check question: What is the relationship between PDAs and context-free grammars, and why are deterministic PDAs sufficient for most programming language syntax?

## Architecture Onboarding

- **Component map**: Vocabulary FST -> Constraint FSA/PDA -> Composition -> Constraint application during decoding
- **Critical path**: Vocabulary FST → Constraint FSA/PDA → Composition → Constraint application during decoding
- **Design tradeoffs**:
  - Pre-computation vs. runtime computation of FSTs
  - Memory usage for storing large vocabulary FSTs
  - Complexity of extensions vs. theoretical purity
  - Determinism requirements for PDAs vs. expressivity

- **Failure signatures**:
  - Compilation failures: Invalid regex/grammar or tokenization mismatches
  - Runtime errors: Missing FST edges or composition failures
  - Constraint violations: Logit masking not working correctly
  - Performance issues: Slow composition or large memory usage

- **First 3 experiments**:
  1. Simple regex constraint (e.g., /foo|bar/) to verify basic composition works
  2. JSON schema constraint to test PDA integration and nested structure handling
  3. Wildcard pattern with terminal labels to verify extension mechanism performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the practical limitations and trade-offs of using deterministic context-free grammars (DCFGs) versus regular expressions for constraining language model outputs?
- Basis in paper: The paper mentions that DCFGs can express more complex languages than regular expressions but require careful grammar writing to avoid non-determinism. It also notes that for shorter outputs, FSAs (regular languages) can be competitive with PDAs (DCFGs) in terms of expressiveness.
- Why unresolved: The paper doesn't provide concrete examples or quantitative analysis of when to use DCFGs versus regular expressions. It also doesn't discuss the potential performance implications of using DCFGs.
- What evidence would resolve it: A study comparing the expressiveness, performance, and ease of use of DCFGs versus regular expressions for various constraint types would help clarify when to use each approach.

### Open Question 2
- Question: How does the proposed method handle very large vocabularies, and what are the scalability limits?
- Basis in paper: The paper mentions that popular LMs use sub-word tokenizers with vocabularies exceeding 100k tokens, which can lead to states with almost |V| outbound edges in the composed FSA. It proposes using terminal labels to mitigate this issue but doesn't discuss scalability limits.
- Why unresolved: The paper doesn't provide data on how the method performs with extremely large vocabularies or discuss potential bottlenecks.
- What evidence would resolve it: Experiments measuring compilation and per-step overhead for increasingly large vocabularies would help determine the scalability limits of the method.

### Open Question 3
- Question: Can the proposed method be extended to handle non-deterministic context-free grammars (NCFGs)?
- Basis in paper: The paper focuses on DCFGs and mentions that non-deterministic PDAs (which accept context-free languages) are not equivalent to deterministic PDAs. It doesn't explore the possibility of handling NCFGs.
- Why unresolved: The paper doesn't discuss the challenges or potential solutions for extending the method to handle NCFGs.
- What evidence would resolve it: A study investigating the feasibility and potential approaches for extending the method to handle NCFGs would clarify whether this is a viable direction for future work.

## Limitations
- Tokenization mismatch handling scope remains unclear for complex tokenization scenarios
- Determinism constraints limit expressivity for certain language features
- Runtime performance scaling impact on decoding speed and memory usage is unquantified

## Confidence
- **High confidence**: Core automata composition approach is well-grounded in formal language theory with verifiable implementation using OpenFST
- **Medium confidence**: Correctness claims rely on assumptions about FST composition handling all tokenization edge cases
- **Low confidence**: Practical extensions may have undocumented interactions that could break theoretical guarantees

## Next Checks
1. **Edge case tokenization testing**: Systematically test with tokenizers that produce problematic tokenization patterns to verify FST composition handles all edge cases
2. **Non-deterministic grammar validation**: Attempt to use with non-deterministic context-free grammars to document exactly where and why the system fails
3. **Large-scale runtime benchmarking**: Measure impact on decoding speed and memory usage across different model sizes and constraint complexity levels