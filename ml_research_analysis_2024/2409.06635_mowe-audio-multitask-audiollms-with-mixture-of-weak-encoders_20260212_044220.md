---
ver: rpa2
title: 'MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders'
arxiv_id: '2409.06635'
source_url: https://arxiv.org/abs/2409.06635
tags:
- audio
- encoder
- encoders
- mowe
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving Audio Large Language
  Models' (AudioLLMs) performance on diverse audio tasks. Pre-trained audio encoders
  often lack the capacity to capture features for new tasks and datasets, limiting
  the generalizability of AudioLLMs.
---

# MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders

## Quick Facts
- arXiv ID: 2409.06635
- Source URL: https://arxiv.org/abs/2409.06635
- Authors: Wenyu Zhang, Shuo Sun, Bin Wang, Xunlong Zou, Zhuohan Liu, Yingxu He, Geyu Lin, Nancy F. Chen, Ai Ti Aw
- Reference count: 40
- One-line primary result: MoWE achieves METEOR 25.49 on AudioCaps, outperforming WavLLM (6.70) and SALMONN (22.79)

## Executive Summary
MoWE-Audio introduces a novel approach to improving Audio Large Language Models (AudioLLMs) by incorporating a mixture of weak encoders alongside a strong base encoder. The key innovation is a data-dependent routing mechanism that selectively activates the most relevant weak encoder for each audio input, enhancing feature extraction without significantly increasing model size. This approach addresses the challenge of limited encoder capacity in AudioLLMs when dealing with diverse audio tasks and datasets.

The proposed MoWE architecture demonstrates substantial improvements across multiple audio tasks, including Automatic Speech Recognition, Emotion Recognition, Audio Question Answering, Speech Question Answering, and Audio Captioning. By leveraging specialized weak encoders and intelligent routing, MoWE achieves competitive performance compared to state-of-the-art models while maintaining parameter efficiency through selective activation of only the most relevant encoders for each input.

## Method Summary
MoWE (Mixture of Weak Encoders) integrates a pool of lightweight encoders into the AudioLLM framework to supplement a strong base encoder. The architecture employs both data-independent and data-dependent routing strategies to selectively activate the most relevant weak encoders for each audio input. During training, a routing loss with entropy and diversity terms ensures confident and diverse encoder selection while preventing routing collapse. The model concatenates embeddings from the selected encoders along the feature dimension, followed by downsampling and projection to the LLM input space. Multi-task training is performed using AdamW optimizer with next-token prediction objective, achieving improved performance across diverse audio tasks without significantly increasing model size.

## Key Results
- MoWE achieves METEOR score of 25.49 on AudioCaps, outperforming WavLLM (6.70) and SALMONN (22.79)
- Significant improvements in Emotion Recognition, Audio Question Answering, and Audio Captioning compared to baseline models
- Maintains competitive performance on Automatic Speech Recognition while enhancing multi-task capabilities
- Parameter-efficient design with only slight reduction in throughput despite adding multiple weak encoders

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoWE improves multi-task performance by selectively activating weak encoders specialized for specific audio characteristics
- Mechanism: The data-dependent router evaluates base encoder embeddings and routes to the most suitable weak encoder for each audio input, leveraging specialized representations for diverse audio types
- Core assumption: Different audio tasks require different feature representations that a single base encoder cannot capture effectively
- Evidence anchors: [abstract] "MoWE supplements a base encoder with a pool of relatively light weight encoders, selectively activated based on the audio input to enhance feature extraction"; [section] "We observe evident specialization of the encoders. LibriSpeech-Clean and Spoken-SQuAD consist speech mostly in neutral and consistent tone, and are mainly processed by two individual Whisper-tiny encoders that are pre-trained to specialize in ASR"

### Mechanism 2
- Claim: The mixture of weak encoders increases overall model capacity without significant parameter increase
- Mechanism: Combining multiple lightweight encoders (9M-95M parameters) with a strong base encoder (637M parameters) achieves broader feature extraction capability while keeping active parameters manageable through selective routing
- Core assumption: The computational overhead of routing decisions is negligible compared to the benefits of having specialized encoders available
- Evidence anchors: [abstract] "selectively activated based on the audio input to enhance feature extraction without significantly increasing model size"; [section] "Adding the mixture of weak encoders only slightly reduces throughput, as the parameter counts of the weak encoders are significantly smaller in comparison with the LLM (9-95M vs. 8B)"

### Mechanism 3
- Claim: MoWE provides effective regularization through entropy and diversity losses that prevent routing collapse
- Mechanism: The routing loss includes entropy terms that encourage confident routing decisions and a diversity term that prevents all samples from being routed to the same encoder, maintaining model robustness
- Core assumption: Without explicit diversity regularization, the data-dependent router would converge to using only the most generally useful encoder, eliminating the benefits of having multiple options
- Evidence anchors: [section] "The entropy losses in Equation 7 and 8 encourages confident routing decisions, and the diversity loss in Equation 9 prevents data-dependent router decisions from collapsing to a single encoder choice"; [section] "We smooth the weight ri,dep by ri,dep = 0.9 * ri,dep + 0.1 * ǫ with ǫ = 0.1/M to prevent training from being heavily biased towards specific encoders"

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Understanding how MoE works in language models helps grasp how MoWE adapts this concept to audio encoders instead of transformer layers
  - Quick check question: What is the key difference between traditional MoE in LLMs and MoWE's approach to audio encoders?

- Concept: Self-supervised learning in audio representation
  - Why needed here: Many weak encoders (HuBERT, WavLM) are pre-trained using self-supervised methods, which is critical for understanding their strengths in capturing general audio features
  - Quick check question: How do self-supervised audio encoders like HuBERT differ from supervised encoders like Whisper in their feature extraction approach?

- Concept: Audio embedding fusion techniques
  - Why needed here: MoWE concatenates embeddings from multiple encoders along the feature dimension, requiring understanding of how to properly combine heterogeneous representations
  - Quick check question: What are the advantages and disadvantages of concatenating versus weighted averaging when combining embeddings from multiple audio encoders?

## Architecture Onboarding

- Component map: Audio input → Base encoder → Data-dependent router → Weak encoder selection → Embedding concatenation → Adapter → Projection → LLM → Text output

- Critical path: Audio input → Whisper-large-v3 base encoder → Data-dependent router → Selected Whisper-tiny encoder → Concatenated embeddings → Modality adapter (downsamples to 100 tokens) → Linear projection → Llama-3-8B-Instruct → Text output

- Design tradeoffs:
  - Number of weak encoders vs. routing complexity
  - Encoder diversity vs. training data requirements
  - Router sophistication vs. inference latency
  - Parameter sharing across encoders vs. specialization

- Failure signatures:
  - Poor routing decisions (all samples routed to same encoder)
  - Inconsistent outputs across similar audio samples
  - Training instability (loss spikes during routing updates)
  - Memory bottlenecks (too many active encoders)

- First 3 experiments:
  1. Baseline comparison: Run with only the strong base encoder to establish performance baseline
  2. Single weak encoder addition: Add one Whisper-tiny encoder with data-independent routing to verify basic MoWE integration
  3. Router ablation: Compare data-independent only, data-dependent only, and combined routing strategies to understand their individual contributions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but the limitations section identifies several areas requiring further investigation.

## Limitations
- Generalization uncertainty: Performance benefits may be specific to the particular encoder combinations used rather than a universal property of the MoWE architecture
- Routing interpretability: Limited analysis of what features drive routing decisions, making it unclear whether routing choices are semantically meaningful or task-optimal
- Multi-task interference: Potential for negative transfer between tasks with conflicting feature requirements during multi-task training

## Confidence
- High Confidence: The core claim that MoWE improves performance across multiple audio tasks is well-supported by experimental results
- Medium Confidence: Claims about specialized feature extraction through selective routing and maintaining manageable model size are supported but require further validation
- Low Confidence: Claims about superior generalization to new tasks compared to single-encoder approaches are largely theoretical with limited experimental support

## Next Checks
1. **Encoder Replacement Ablation Study:** Systematically replace individual weak encoders with alternative pre-trained encoders while keeping the MoWE architecture constant to test whether performance gains are due to specific encoders or the MoWE architecture itself

2. **Routing Decision Analysis:** Implement activation maximization techniques to visualize what features trigger routing to specific encoders and assess consistency and task-relevance of routing patterns across similar audio samples

3. **Cross-task Transfer Analysis:** Design experiments where MoWE is first trained on ASR and Emotion Recognition tasks, then evaluated on transfer performance for Audio Question Answering and Audio Captioning to quantify positive or negative transfer effects