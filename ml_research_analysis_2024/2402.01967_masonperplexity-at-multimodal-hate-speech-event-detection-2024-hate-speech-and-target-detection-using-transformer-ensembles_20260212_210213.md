---
ver: rpa2
title: 'MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate Speech
  and Target Detection Using Transformer Ensembles'
arxiv_id: '2402.01967'
source_url: https://arxiv.org/abs/2402.01967
tags:
- hate
- speech
- sub-task
- detection
- proceedings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MasonPerplexity's submission to the Multimodal
  Hate Speech Event Detection shared task at CASE 2024. The task involves detecting
  hate speech and identifying targets in text-embedded images from political events.
---

# MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate Speech and Target Detection Using Transformer Ensembles

## Quick Facts
- arXiv ID: 2402.01967
- Source URL: https://arxiv.org/abs/2402.01967
- Authors: Amrita Ganguly; Al Nahian Bin Emran; Sadiya Sayara Chowdhury Puspo; Md Nishat Raihan; Dhiman Goswami; Marcos Zampieri
- Reference count: 6
- Primary result: 0.8347 F1-score on hate speech detection (sub-task A), 0.6741 F1-score on target detection (sub-task B)

## Executive Summary
This paper presents MasonPerplexity's submission to the CASE 2024 Multimodal Hate Speech Event Detection shared task. The task involves detecting hate speech and identifying targets in text-embedded images from political events. The authors employed XLM-RoBERTa-large for hate speech detection (sub-task A) and an ensemble of XLM-RoBERTa-base, BERTweet-large, and BERT-base for target detection (sub-task B). Their approach achieved 0.8347 F1-score in sub-task A and 0.6741 F1-score in sub-task B, ranking 3rd on both sub-tasks.

## Method Summary
The authors used XLM-RoBERTa-large for sub-task A (hate speech detection) and an ensemble of XLM-RoBERTa-base, BERTweet-large, and BERT-base for sub-task B (target detection). Text was extracted from images using Google Vision API. For sub-task B, they employed back-translation data augmentation, converting training data through various language pairs to improve model performance. Ensemble predictions for sub-task B were combined using majority voting.

## Key Results
- XLM-RoBERTa-large achieved 0.8347 F1-score on hate speech detection (sub-task A)
- Ensemble approach combining three models achieved 0.6741 F1-score on target detection (sub-task B)
- Back-translation augmentation improved performance from 0.65 to 0.67 F1-score
- Ranked 3rd on both sub-tasks in the competition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XLM-RoBERTa-large's strong performance on hate speech detection is due to its multilingual pretraining on diverse web text, enabling robust generalization across languages in political event contexts.
- Mechanism: Pretraining on large-scale multilingual corpora allows XLM-RoBERTa-large to capture shared linguistic structures and semantics, which helps detect hate speech patterns even when they are expressed in different languages or cultural contexts.
- Core assumption: The political event dataset contains multilingual content, and hate speech patterns are linguistically transferable across languages.
- Evidence anchors:
  - [abstract] "We use an XLM-roBERTa-large model for sub-task A"
  - [section] "XLM-R shows the best F1 score" for sub-task A
  - [corpus] Weak evidence: corpus neighbors focus on similar hate speech tasks but not directly on multilingual pretraining effects.
- Break condition: If the dataset is monolingual or if hate speech patterns are highly language-specific, XLM-RoBERTa-large's multilingual advantage may diminish.

### Mechanism 2
- Claim: Ensemble approach combining multiple transformer models (XLM-RoBERTa-base, BERTweet-large, BERT-base) improves target detection by leveraging diverse linguistic representations and reducing individual model biases.
- Mechanism: Different transformer architectures capture complementary features of text-embedded images, and majority voting over ensemble predictions increases robustness against class imbalance and labeling noise.
- Core assumption: Each base model has different strengths in processing political discourse, and their combination mitigates individual weaknesses.
- Evidence anchors:
  - [abstract] "an ensemble approach combining XLM-roBERTa-base, BERTweet-large, and BERT-base for sub-task B"
  - [section] "our ensemble approach was provides the best F1-score of 0.67" for sub-task B
  - [corpus] Weak evidence: related papers discuss ensemble methods but not specifically for hate speech target detection.
- Break condition: If models in the ensemble are highly correlated or if the dataset is too small to benefit from ensemble diversity.

### Mechanism 3
- Claim: Back-translation augmentation improves model performance by expanding the training data with paraphrased examples in culturally diverse languages, reducing overfitting and improving generalization.
- Mechanism: Translating training examples to languages with low cultural overlap (e.g., Xosha, Twi, Lao, Pashto, Yoruba) and back introduces linguistic variability that forces the model to learn more robust, language-agnostic hate speech features.
- Core assumption: Cultural and linguistic diversity in back-translation exposes the model to a wider range of hate speech expressions, improving robustness.
- Evidence anchors:
  - [section] "we employed back translation, converting the training data through Xosha to Twi to English and Lao to Pashto to Yoruba to English. This significantly improves overall model performance from 0.65 to 0.67."
  - [abstract] No direct mention of back-translation.
  - [corpus] Weak evidence: corpus neighbors do not discuss back-translation in hate speech contexts.
- Break condition: If back-translation introduces noise or if the augmented examples are too dissimilar from the original data distribution.

## Foundational Learning

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: The models (XLM-RoBERTa, BERT, BERTweet) are transformer-based; understanding self-attention and pretraining is essential for model selection and fine-tuning.
  - Quick check question: How does the self-attention mechanism in transformers allow models to weigh the importance of different words in a sentence for tasks like hate speech detection?

- Concept: Class imbalance and its impact on model performance
  - Why needed here: Sub-task B shows significant label imbalance (Individual: 42.38%, Community: 17.25%, Organization: 40.37%), requiring techniques like ensemble voting or data augmentation.
  - Quick check question: What are two strategies to handle class imbalance in multi-class classification problems, and how do they apply to the hate speech target detection task?

- Concept: Multimodal learning and text extraction from images
  - Why needed here: The task involves text-embedded images; OCR (Google Vision API) is used to extract text, which is then classified. Understanding OCR limitations and multimodal feature fusion is important.
  - Quick check question: What are potential sources of error when using OCR to extract text from text-embedded images for hate speech detection?

## Architecture Onboarding

- Component map: Image -> OCR (Google Vision API) -> Extracted text -> Model inference -> Final prediction
- Critical path:
  1. Input image → OCR extraction
  2. Extracted text → Model inference (XLM-RoBERTa-large for A, ensemble for B)
  3. Sub-task A: Direct prediction (HATE/NO-HATE)
  4. Sub-task B: Ensemble voting → Final label (Individual/Community/Organization)
- Design tradeoffs:
  - Single strong model (XLM-RoBERTa-large) vs. ensemble (better robustness but higher complexity)
  - Back-translation augmentation vs. potential noise introduction
  - OCR accuracy vs. manual annotation cost
- Failure signatures:
  - Low recall on minority classes in sub-task B (e.g., Community label)
  - High OCR error rates degrading input text quality
  - Ensemble disagreement indicating model uncertainty or ambiguous cases
- First 3 experiments:
  1. Compare single XLM-RoBERTa-large vs. ensemble performance on a held-out validation set to quantify ensemble benefit.
  2. Test back-translation with different source languages to find optimal augmentation strategy.
  3. Analyze confusion matrices to identify which classes are most confused and adjust model or data accordingly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different multimodal fusion strategies (e.g., early fusion, late fusion, attention-based fusion) compare in their effectiveness for hate speech detection in text-embedded images?
- Basis in paper: [inferred] The paper mentions the challenges of handling multimodal content but does not explore different fusion strategies.
- Why unresolved: The paper focuses on using pre-trained models like XLM-RoBERTa-large and ensembles, but does not investigate various fusion approaches.
- What evidence would resolve it: A comparative study of different fusion strategies using the same dataset and evaluation metrics.

### Open Question 2
- Question: How does the performance of the proposed models generalize to other types of political events or domains beyond the Russia-Ukraine conflict?
- Basis in paper: [inferred] The dataset used in the study is specific to the Russia-Ukraine conflict, and the paper does not discuss the generalizability of the models.
- Why unresolved: The models are trained and evaluated on a specific dataset, and their performance on other domains or events is unknown.
- What evidence would resolve it: Testing the models on datasets from different political events or domains and comparing their performance.

### Open Question 3
- Question: How do the proposed models handle code-switching and transliterations in text-embedded images, and what is their impact on performance?
- Basis in paper: [inferred] The paper mentions the use of back-translation for data augmentation but does not explicitly discuss the handling of code-switching and transliterations.
- Why unresolved: The impact of code-switching and transliterations on model performance is not explored in the paper.
- What evidence would resolve it: Analyzing the performance of the models on text-embedded images containing code-switching and transliterations, and comparing it to their performance on standard text.

## Limitations
- Lack of detailed hyperparameter specifications and training configurations for each model
- Vague back-translation implementation details without exact language pairs and quality control measures
- No ablation studies to quantify individual contributions of ensemble members or data augmentation
- OCR extraction process without error rate analysis or quality metrics
- Dataset characteristics not fully disclosed, limiting generalizability understanding

## Confidence

**High confidence:** The claim that XLM-RoBERTa-large achieved 0.8347 F1-score on hate speech detection is well-supported by the results section and the clear methodology description. The ensemble approach for target detection achieving 0.6741 F1-score is also well-documented with specific model combinations and voting mechanisms.

**Medium confidence:** The mechanism explaining XLM-RoBERTa-large's multilingual advantage relies on reasonable assumptions about language transfer but lacks direct experimental validation. The ensemble benefit is demonstrated but without ablation studies to isolate individual contributions. The back-translation improvement from 0.65 to 0.67 is reported but the exact implementation details and quality control measures are unclear.

**Low confidence:** The assumption that cultural and linguistic diversity in back-translation uniformly improves model robustness is not experimentally validated. The claim that different transformer architectures capture truly complementary features lacks quantitative evidence from correlation analysis or feature importance studies.

## Next Checks

1. Conduct an ablation study comparing XLM-RoBERTa-large performance with and without back-translation augmentation on a held-out validation set to quantify the exact contribution of data augmentation versus model architecture.

2. Perform correlation analysis between ensemble member predictions to verify that the models capture genuinely complementary features rather than redundant information, and test ensemble performance with different voting strategies (weighted vs. majority).

3. Implement a controlled experiment testing OCR quality impact by comparing model performance on original images versus ground-truth text to establish the performance gap attributable to OCR extraction errors.