---
ver: rpa2
title: Self-Attention Limits Working Memory Capacity of Transformer-Based Models
arxiv_id: '2409.10715'
source_url: https://arxiv.org/abs/2409.10715
tags:
- position
- index
- epoch
- attention
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transformer-based models show working memory capacity limits in
  N-back tasks, with performance declining as N increases. This study investigates
  why this occurs by training simple decoder-only Transformers on N-back tasks.
---

# Self-Attention Limits Working Memory Capacity of Transformer-Based Models

## Quick Facts
- arXiv ID: 2409.10715
- Source URL: https://arxiv.org/abs/2409.10715
- Reference count: 40
- Transformer models show working memory capacity limits in N-back tasks, with performance declining as N increases

## Executive Summary
This study investigates why Transformer-based models exhibit working memory capacity limits when performing N-back tasks, where performance should decline as N increases. Through systematic experiments with simplified decoder-only Transformers on synthetic N-back tasks, the authors demonstrate that self-attention mechanisms are both essential for task performance and simultaneously responsible for creating capacity constraints. The research reveals that as the required memory span (N) increases, attention scores become more dispersed across positions, leading to higher entropy and reduced predictive accuracy.

## Method Summary
The researchers trained vanilla decoder-only Transformers on synthetic N-back tasks with sequences of 24 characters from a 24-letter alphabet. Models were trained for 10 epochs with cross-entropy loss, using simplified architectures without FFN or layer normalization layers. The study systematically varied N (from 1 to 6), the number of layers (1-2), and attention heads (1-4), analyzing attention score evolution, entropy measurements, and accuracy correlations across 50 independent training runs for each configuration.

## Key Results
- Transformer performance on N-back tasks shows consistent decline as N increases
- Attention scores gradually aggregate to N-back positions during training, indicating learned task strategy
- Total entropy of attention score matrices increases with N, suggesting dispersion causes capacity limits
- Prediction accuracy at position i correlates positively with attention score at position i-N

## Why This Works (Mechanism)

### Mechanism 1
Attention scores gradually aggregate to N-back positions during training. During N-back training, the model learns to strengthen attention weights between the current position and the position N steps back, as this correlation is essential for correct predictions. Core assumption: The model can learn to selectively attend to the N-back position through gradient-based optimization. Evidence: Attention scores evolve from uniform distribution to focus on N-back positions over training epochs.

### Mechanism 2
Total entropy of attention scores increases as N increases, causing performance decline. As N increases, the model struggles to maintain focused attention on the distant N-back position, leading to more dispersed attention weights across multiple positions. Core assumption: Higher entropy in attention distributions correlates with reduced predictive accuracy. Evidence: Entropy measurements show systematic increase with N values, corresponding to accuracy drops.

### Mechanism 3
Attention score at position i-N increases with test accuracy at position i. For accurate predictions at position i, the model must learn to allocate most attention to position i-N, creating a positive correlation between attention strength and accuracy. Core assumption: The model's prediction accuracy depends directly on the strength of attention to the correct reference position. Evidence: Correlation analysis shows attention scores at N-back positions increase alongside accuracy over training.

## Foundational Learning

- **N-back task structure and purpose**: Understanding the task design is crucial for interpreting model behavior and evaluating whether the model has learned the correct strategy. Quick check: In an N-back task with N=3, which previous position should the model attend to when making a prediction at position 10?

- **Entropy as a measure of attention dispersion**: The paper uses entropy to quantify how focused vs. dispersed the model's attention is, which is central to understanding the capacity limit. Quick check: If attention scores are uniformly distributed across all positions, what would the entropy value be compared to when all attention is concentrated on one position?

- **Self-attention mechanism in transformers**: The entire study investigates how self-attention limitations cause working memory capacity constraints. Quick check: In a causal transformer with masking, which positions can position i attend to when computing its output?

## Architecture Onboarding

- **Component map**: Input character embedding → Positional embedding → Masked self-attention → Attention score aggregation → Unembedding layer → Match/non-match logits

- **Critical path**: 1) Character embedding + positional embedding 2) Masked self-attention computation 3) Attention score aggregation to N-back position 4) Unembedding to match/non-match logits 5) Cross-entropy loss with ground truth

- **Design tradeoffs**: Simplified architecture (no FFN/layer norm) improves interpretability but may limit performance; Single attention head simplifies analysis but may not capture complex patterns; Fixed 24-character sequences allow controlled experiments but limit generalizability

- **Failure signatures**: Uniform attention distribution across all positions indicates failure to learn task strategy; Low entropy with poor accuracy suggests incorrect attention focus; Accuracy plateauing early in training suggests insufficient capacity or learning issues

- **First 3 experiments**: 1) Train 1-layer 1-head model on 1-back task and visualize attention evolution over epochs 2) Compare attention entropy and accuracy across different N values (1, 2, 3) 3) Test performance degradation when increasing sequence length beyond 24 positions

## Open Questions the Paper Calls Out

### Open Question 1
How does the dispersion of attention scores across different positions in the sequence affect the model's working memory capacity for tasks beyond N-back? The study finds that total entropy of attention scores increases as N increases, suggesting dispersion of attention as a cause of capacity limits in N-back tasks. This remains unresolved because the current study focuses specifically on N-back tasks and does not explore how attention dispersion affects performance on other cognitive tasks that also require working memory.

### Open Question 2
Can architectural modifications to Transformer models (e.g., additional layers, different attention mechanisms) reduce the dispersion of attention scores and improve working memory capacity? The study notes that models with more layers or attention heads achieve higher accuracy but still show slight declines as N increases, suggesting that current modifications are insufficient. This remains unresolved because the study only briefly tests a few architectural variants and does not systematically explore how different designs affect attention dispersion and working memory capacity.

### Open Question 3
What is the relationship between the number of training samples and the model's ability to manage attention dispersion in high-N tasks? The study trains models on a fixed dataset size and does not investigate how varying the amount of training data affects the model's ability to handle attention dispersion as N increases. This remains unresolved because the paper does not explore the impact of training data quantity on the model's working memory capacity or its ability to aggregate attention effectively.

## Limitations

- The synthetic N-back task may not fully capture real-world working memory demands, limiting ecological validity
- Simplified Transformer architecture (no FFN/layer norm) may behave differently from standard models used in practice
- Focus on decoder-only Transformers leaves open questions about encoder-decoder or encoder-only architectures

## Confidence

**High Confidence**: The core empirical observation that Transformer performance on N-back tasks declines as N increases is well-supported by experimental results.

**Medium Confidence**: The interpretation that attention dispersion (entropy) directly causes capacity limitations is plausible but not definitively proven.

**Low Confidence**: The broader claim that these findings generalize to all Transformer-based models' working memory capacity in practical applications remains speculative.

## Next Checks

1. **Cross-Architecture Validation**: Replicate experiments using standard Transformer architectures (with FFN layers and layer normalization) to determine whether observed capacity limits persist in more realistic model configurations.

2. **Sequence Length Scaling**: Extend experiments to longer sequence lengths (e.g., 48, 96 positions) to test how attention entropy and accuracy scale beyond the current 24-position limit.

3. **Alternative Task Structures**: Test whether similar attention-based capacity limits appear in more complex working memory tasks requiring semantic or structural reasoning beyond simple position-based matching.