---
ver: rpa2
title: Flexible and Effective Mixing of Large Language Models into a Mixture of Domain
  Experts
arxiv_id: '2408.17280'
source_url: https://arxiv.org/abs/2408.17280
tags:
- expert
- experts
- each
- training
- router
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a toolkit for creating Mixture-of-Domain-Experts
  (MOE) models from trained large language models (LLMs) at low cost. The approach
  enables mixing pre-trained and fine-tuned expert models with a base model by replacing
  FFN layers and optionally adding a router.
---

# Flexible and Effective Mixing of Large Language Models into a Mixture of Domain Experts

## Quick Facts
- arXiv ID: 2408.17280
- Source URL: https://arxiv.org/abs/2408.17280
- Reference count: 38
- Primary result: MOE models created by swapping FFN layers can outperform individual experts on various tasks

## Executive Summary
This paper presents a toolkit for creating Mixture-of-Domain-Experts (MOE) models from trained large language models at low cost. The approach enables mixing pre-trained and fine-tuned expert models with a base model by replacing FFN layers and optionally adding a router. The toolkit supports flexible architectures including Gate-less MOE (equal expert weighting), Noisy MOE (randomized top-K expert selection), and router training options. Experiments using Merlinite and llama3-8B models show that MOE models can outperform individual experts and baselines on various tasks including math, general knowledge, and medical domains.

## Method Summary
The method involves swapping the FFN layers of expert models into a base model's architecture, with optional router/gate integration. Three MOE variants are proposed: Gate-less MOE (equal weighting of experts), Noisy MOE (randomized top-K expert selection), and router-trained MOE. The approach supports both full FFN layer replacement and LoRA adapter mixing. Implementation details include handling tensor shapes during layer swapping and optional router training using combined datasets from all experts.

## Key Results
- MOE models can outperform individual expert models and baselines on math, general knowledge, and medical tasks
- Gate-less and Noisy MOE variants achieve competitive results without requiring training
- Router training provides modest benefits primarily on math tasks
- Noisy MOE offers faster inference when using multiple experts compared to gate-less approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing FFN layers with domain-expert FFNs preserves expert capabilities while creating a unified MOE model
- Mechanism: The toolkit swaps each expert's FFN layers into the base model's architecture, maintaining the original expert's learned representations while integrating them into a shared model structure
- Core assumption: The FFN layers contain the domain-specific knowledge that makes each expert effective
- Evidence anchors:
  - [abstract]: "The approach enables mixing pre-trained and fine-tuned expert models with a base model by replacing FFN layers"
  - [section]: "Our MOE Model Mixing toolkit swaps the FFN layers of each expert model, along with a gate, in place of the FFN layers of a base model"
  - [corpus]: Weak evidence - corpus neighbors focus on different MoE mechanisms rather than FFN swapping
- Break condition: If FFN layers don't capture domain-specific knowledge, or if architectural incompatibilities prevent proper layer integration

### Mechanism 2
- Claim: Equal weighting of experts (Gate-less MOE) can outperform trained router-based approaches
- Mechanism: By assigning equal probability to each expert rather than learning complex routing, the model avoids over-specialization and maintains balanced performance across domains
- Core assumption: Domain experts have complementary capabilities that don't require sophisticated routing when used in combination
- Evidence anchors:
  - [abstract]: "Gate-less MOE (equal expert weighting)" and "The Gate-less and Noisy MOE variants achieve competitive results without requiring training"
  - [section]: "We thus propose creating a Gate-less MOE, which assigns an equal weight to each expert. We show that when the number of expert models is small, this can be an optimal strategy"
  - [corpus]: No direct evidence in corpus about equal weighting effectiveness
- Break condition: When expert capabilities overlap significantly or when domain specialization is highly unbalanced

### Mechanism 3
- Claim: Noisy MOE with random top-K selection achieves near-optimal performance with reduced computational cost
- Mechanism: Random white noise-based routing selects K experts per token, maintaining performance while reducing the number of active parameters during inference
- Core assumption: Random selection of competent experts is sufficient for good performance, and sparsity reduces computational overhead without significant quality loss
- Evidence anchors:
  - [abstract]: "Noisy MOE (randomized top-K expert selection)" and "Noisy MOE offering faster inference when using multiple experts"
  - [section]: "To reduce the inference cost of the Gate-less MOE when the number of expert modules increases, we also propose a Noisy MOE" and "This Noisy MOE works almost as well as the Gate-less MOE and provides faster inference time"
  - [corpus]: No corpus evidence about random routing effectiveness
- Break condition: When random selection frequently chooses inappropriate experts for specific tokens, or when K is too small to capture necessary expertise

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Understanding how MoE differs from dense models is crucial for grasping why swapping FFN layers works
  - Quick check question: How does an MoE model activate fewer parameters than a dense model while maintaining or improving performance?

- Concept: Router/gating mechanisms in neural networks
  - Why needed here: The paper contrasts different routing approaches (gate-less, noisy, trained), requiring understanding of how routers function
  - Quick check question: What is the primary purpose of a router in an MoE model, and how does it differ from the equal weighting approach?

- Concept: Transfer learning and fine-tuning principles
  - Why needed here: The work builds on pre-trained and fine-tuned models, so understanding how domain expertise is encoded is essential
  - Quick check question: Why might a fine-tuned medical domain model outperform a general model on medical tasks, and how does this relate to the MoE approach?

## Architecture Onboarding

- Component map: Base model (merlinite-7b) ← FFN layers replaced by expert FFN layers + router/gate (optional) + embedding layers (optional training)
- Critical path: Model loading → FFN layer swapping → (optional) router training → inference routing → output generation
- Design tradeoffs:
  - Equal weighting vs. trained routing: Simplicity and cost vs. potential performance gains
  - Full FFN replacement vs. LoRA adapters: Performance preservation vs. parameter efficiency
  - Number of experts: Coverage of domains vs. inference cost and memory usage
- Failure signatures:
  - Performance worse than individual experts: Indicates poor integration or routing
  - Memory overflow with multiple experts: Suggests architectural scaling issues
  - Inconsistent results across tasks: May indicate imbalanced expert capabilities or routing
- First 3 experiments:
  1. Create 2x MOE with merlinite + metamath, test on GSM8K and MMLU to verify basic functionality
  2. Compare gate-less vs. noisy MOE with 4 experts on math and medical tasks to observe routing impact
  3. Test router training on 2x MOE using combined metamath+pubmedQA dataset, measure performance changes on domain-specific tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does router training provide the most benefit in MOE models, and how can we predict when it will be valuable?
- Basis in paper: [explicit] The paper states "We find that the decrease in loss is moderate during router training, implying that the ability of the router to learn is somewhat limited" and shows router training provides modest benefits primarily on math tasks
- Why unresolved: The paper shows router training is not always needed and provides mixed results across different tasks and model architectures, but doesn't establish clear criteria for when router training will be most beneficial
- What evidence would resolve it: Systematic experiments varying dataset size, task types, number of experts, and expert specialization to identify conditions where router training provides significant improvement versus when it doesn't

### Open Question 2
- Question: How does the choice of base model domain expertise impact MOE performance across different task types?
- Basis in paper: [explicit] The ablation study shows "The base model used for the MOE has a noticeable impact, as can be seen from bars 4-6... The MOE with a math-trained base performs the best on the GSM8K math test and the MOE with a medical-trained base performs best on the medical tests"
- Why unresolved: While the paper demonstrates base model choice affects performance, it doesn't establish whether this is due to domain alignment or other factors, or how to optimally select base models for different applications
- What evidence would resolve it: Controlled experiments varying base model domains against diverse task types while controlling for other variables to determine optimal base model selection strategies

### Open Question 3
- Question: What is the optimal strategy for mixing different types of expert modules (FFN layers vs LoRA adapters) in MOE architectures?
- Basis in paper: [explicit] The paper states "we also enable creating an MOE from LoRA adapter experts" and shows "FFN mixing is best overall but LoRA adapter MOE mixing is competitive" but doesn't provide clear guidelines for when to use each
- Why unresolved: The paper demonstrates both approaches work but doesn't establish clear criteria for selecting between FFN layers and LoRA adapters or how to optimally combine them
- What evidence would resolve it: Systematic comparison of FFN vs LoRA mixing strategies across different model sizes, task types, and expert configurations to identify optimal mixing strategies for different scenarios

## Limitations

- Implementation-specific limitations: The paper describes the MOE mixing toolkit but provides limited details about the exact implementation of FFN layer swapping and router integration
- Empirical scope constraints: All experiments were conducted using the merlinite-7B model family and specific domain experts, limiting generalizability to other architectures and scales
- Evaluation breadth limitations: While multiple tasks are evaluated, the focus is primarily on classification-style benchmarks, with limited assessment of generation tasks

## Confidence

**High confidence** in the core technical approach: The mechanism of swapping FFN layers and the conceptual framework for different MOE variants are well-described and technically sound

**Medium confidence** in performance claims: The paper demonstrates that MOE models can outperform individual experts on many tasks, but the performance gains are modest in some cases

**Low confidence** in generalizability: The results are based on a specific set of models and tasks, limiting claims about effectiveness across arbitrary domains or model scales

## Next Checks

1. Test the MOE mixing approach with different base model architectures (e.g., mixtral, deepseek) and varying model sizes to verify if performance benefits hold across architectures and scales

2. Systematically vary the number of experts (K) in Noisy MOE and compare with different routing strategies across a broader set of tasks to better understand when and why different routing approaches succeed or fail

3. Evaluate the MOE models on open-ended generation tasks and long-form reasoning benchmarks to determine if the approach maintains performance advantages beyond classification-style evaluations