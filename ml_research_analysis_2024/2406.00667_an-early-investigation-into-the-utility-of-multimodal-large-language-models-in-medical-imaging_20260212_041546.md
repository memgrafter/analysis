---
ver: rpa2
title: An Early Investigation into the Utility of Multimodal Large Language Models
  in Medical Imaging
arxiv_id: '2406.00667'
source_url: https://arxiv.org/abs/2406.00667
tags:
- images
- image
- gemini
- medical
- gpt-4v
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the use of multimodal large language models\
  \ (MLLMs) for medical image analysis, focusing on classification and interpretation\
  \ of lung X-ray and retinal fundoscopy images. Two models\u2014Gemini and GPT-4V\u2014\
  were tested on real and synthetic images using NERIF-based prompts."
---

# An Early Investigation into the Utility of Multimodal Large Language Models in Medical Imaging

## Quick Facts
- arXiv ID: 2406.00667
- Source URL: https://arxiv.org/abs/2406.00667
- Authors: Sulaiman Khan; Md. Rafiul Biswas; Alina Murad; Hazrat Ali; Zubair Shah
- Reference count: 21
- Primary result: Gemini outperforms GPT-4V in medical image classification and interpretation, with better human expert alignment

## Executive Summary
This study investigates the use of multimodal large language models (MLLMs) for medical image analysis, focusing on classification and interpretation of lung X-ray and retinal fundoscopy images. Two models—Gemini and GPT-4V—were tested on real and synthetic images using NERIF-based prompts. Gemini demonstrated superior performance in both classification accuracy and interpretation quality compared to GPT-4V, with human expert alignment. While GPT-4V frequently declined image analysis tasks or provided generic responses, Gemini offered more confident and clinically aligned interpretations. The results suggest MLLMs hold potential for supporting diagnostic workflows in specialized medical imaging tasks, though current limitations—such as model refusals and limited dataset size—warrant further research.

## Method Summary
The study employed NERIF-based prompts to guide Gemini and GPT-4V in classifying and interpreting lung X-ray and retinal fundoscopy images. A mixed dataset of real and synthetic images was curated, and both models were evaluated on classification accuracy and interpretation quality. Human expert alignment was assessed through qualitative evaluation by a trained medical practitioner. The methodology focused on comparing model performance across different image types and prompt formulations.

## Key Results
- Gemini achieved higher classification accuracy than GPT-4V for both real and synthetic medical images
- Gemini provided more confident and clinically aligned interpretations compared to GPT-4V's frequent refusals or generic responses
- Human expert evaluation showed better alignment with Gemini's outputs than GPT-4V's

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gemini outperforms GPT-4V in classifying real vs synthetic medical images due to superior multimodal integration and fewer refusals.
- Mechanism: Gemini's architecture leverages a unified multimodal embedding space and longer context length, enabling better handling of complex visual patterns in medical images without defaulting to generic refusals.
- Core assumption: The dataset and prompt formulation are sufficient for the models to differentiate real from synthetic images, and Gemini's training data includes relevant medical imaging patterns.
- Evidence anchors:
  - [abstract] "Gemini demonstrated superior performance in both classification accuracy and interpretation quality compared to GPT-4V"
  - [section] "Gemini leads in the performance of classifying images (both real and synthetic images)... GPT-4V responded with 'Sorry, I can't help with identifying or making assumptions about medical images'"
  - [corpus] Weak evidence: related papers focus on general Gemini/GPT-4V comparisons but lack specific medical image classification studies.
- Break condition: If prompt design fails to guide the model toward task-specific outputs, or if the models' training data lacks exposure to medical imaging, performance drops regardless of architecture.

### Mechanism 2
- Claim: NERIF-based prompts enhance MLLM performance by providing structured, expert-aligned guidance for image interpretation tasks.
- Mechanism: NERIF introduces notation-enhanced rubrics and iterative refinement, aligning model outputs with domain-specific expectations and reducing ambiguous or generic responses.
- Core assumption: Structured prompts with scoring rubrics and domain-specific instructions can significantly improve model accuracy in specialized tasks like medical image analysis.
- Evidence anchors:
  - [section] "we adopted the NERIF method proposed by Lee and Zhai... to formulate an appropriate input prompt for the MLLMs"
  - [abstract] "Gemini offered more confident and clinically aligned interpretations"
  - [corpus] No direct evidence; assumption based on cited NERIF method in general AI contexts.
- Break condition: If the rubric does not capture essential clinical reasoning or if the model cannot interpret rubric instructions, output quality degrades.

### Mechanism 3
- Claim: Human expert alignment is a practical measure of MLLM utility in medical imaging when formal benchmark datasets are lacking.
- Mechanism: Qualitative evaluation by a trained practitioner provides context-sensitive validation of model interpretations, supplementing quantitative metrics.
- Core assumption: A single trained doctor's evaluations are representative enough to gauge alignment, despite small sample size and potential bias.
- Evidence anchors:
  - [section] "we adopted a qualitative evaluation approach and presented randomly selected images along with the interpretation of the MLLMs to a trained doctor"
  - [abstract] "human expert alignment"
  - [corpus] No direct evidence; reliance on small-scale human validation is acknowledged as a limitation in the paper.
- Break condition: If the evaluator's expertise or biases do not align with broader clinical standards, the validation may misrepresent real-world utility.

## Foundational Learning

- Concept: Multimodal embedding spaces and attention mechanisms
  - Why needed here: Understanding how Gemini and GPT-4V process combined text and image inputs is essential for diagnosing performance differences.
  - Quick check question: How does multi-query attention differ from standard self-attention in multimodal models, and why might it help with long medical images?

- Concept: Prompt engineering and few-shot learning techniques
  - Why needed here: NERIF-based prompts are central to the study's methodology; knowing how rubric instructions shape model outputs is key to reproducing or improving results.
  - Quick check question: What are the core components of a NERIF prompt, and how does iterative refinement improve alignment with human expectations?

- Concept: Synthetic data generation and detection
  - Why needed here: The study compares real vs synthetic medical images; understanding diffusion models and synthetic image artifacts informs both dataset curation and model evaluation.
  - Quick check question: What visual cues typically distinguish synthetic from real medical images, and how might these be learned or missed by MLLMs?

## Architecture Onboarding

- Component map: Real medical images (lung X-ray, retinal fundoscopy) + synthetic counterparts → prompt + MLLM → classification + interpretation → human validation
- Core models: Gemini (gemini-1.0-pro-vision-latest), GPT-4V (gpt-4-vision-preview)
- Evaluation layer: Human doctor labels and qualitative assessment

- Critical path: 1. Image curation (real + synthetic) → 2. NERIF prompt formulation → 3. MLLM inference (classification/interpretation) → 4. Human expert alignment check → 5. Performance summary

- Design tradeoffs:
  - Small dataset vs model generalizability: Limited samples may not expose model weaknesses
  - Qualitative vs quantitative evaluation: Human alignment provides nuance but lacks statistical power
  - Prompt specificity vs model flexibility: Highly structured prompts may improve task alignment but reduce general applicability

- Failure signatures:
  - High refusal rate (GPT-4V) → model safety/filtering misfires
  - Low confidence or generic outputs → prompt misalignment or insufficient training data
  - Poor human alignment despite high accuracy → metric-task mismatch

- First 3 experiments:
  1. Run classification task with minimal prompts to establish baseline model behavior without NERIF guidance
  2. Test with synthetic-only dataset to measure model's synthetic image detection capability in isolation
  3. Vary context length (if possible) to assess impact of input size on interpretation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do MLLMs like Gemini and GPT-4V perform on real-time medical image analysis tasks, especially in scenarios requiring immediate diagnostic decisions?
- Basis in paper: [inferred] The paper mentions that the study did not evaluate long-form question answering or real-time data analysis, which are critical in clinical settings.
- Why unresolved: The study focused on a limited dataset and did not simulate real-time clinical scenarios where immediate diagnostic decisions are required.
- What evidence would resolve it: Conducting experiments with larger, real-time datasets and simulating clinical environments to assess the models' performance under time constraints.

### Open Question 2
- Question: What is the impact of model refusals (e.g., declining to analyze certain medical images) on the overall utility of MLLMs in clinical workflows?
- Basis in paper: [explicit] The paper notes that GPT-4V frequently declined to analyze medical images, which affected its classification accuracy.
- Why unresolved: The study did not explore the frequency or reasons behind model refusals in detail, nor did it assess how these refusals impact clinical decision-making.
- What evidence would resolve it: Analyzing the patterns of model refusals across diverse medical image datasets and evaluating their impact on clinical outcomes.

### Open Question 3
- Question: How do MLLMs compare to traditional machine learning models in terms of accuracy, interpretability, and clinical applicability for medical image analysis?
- Basis in paper: [inferred] The paper highlights the superior performance of Gemini over GPT-4V but does not compare MLLMs to traditional ML models.
- Why unresolved: The study focused solely on comparing two MLLMs without benchmarking against established ML approaches in medical imaging.
- What evidence would resolve it: Conducting comparative studies between MLLMs and traditional ML models using the same medical image datasets and evaluation metrics.

## Limitations
- Small dataset size with only 10 real lung X-ray images and 11 real retinal fundoscopy images
- Single medical expert evaluation introduces potential bias and lacks statistical robustness
- No comparison with established medical imaging models or radiologist performance
- Specific prompt formulations not fully detailed, hindering exact replication

## Confidence
- **High Confidence**: Gemini's superior performance in classification accuracy and interpretation quality compared to GPT-4V is well-supported by experimental results and expert alignment
- **Medium Confidence**: The effectiveness of NERIF-based prompts in enhancing model performance is plausible but lacks direct comparative evidence
- **Low Confidence**: The broader clinical utility of MLLMs in medical imaging workflows remains uncertain due to limited dataset size and lack of statistical validation

## Next Checks
1. Expand the evaluation dataset to include a larger and more diverse set of medical images across multiple imaging modalities
2. Conduct a multi-expert evaluation with a panel of medical professionals to reduce individual bias
3. Perform head-to-head comparisons between MLLMs and traditional medical imaging models or radiologist performance on identical tasks