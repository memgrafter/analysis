---
ver: rpa2
title: Personalized Image Generation with Large Multimodal Models
arxiv_id: '2410.14170'
source_url: https://arxiv.org/abs/2410.14170
tags:
- image
- personalized
- generation
- images
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Pigeon is a personalized image generation framework that leverages\
  \ large multimodal models (LMMs) to capture user preferences from noisy history\
  \ images and multimodal instructions. It uses a two-stage preference alignment process\u2014\
  masked preference reconstruction and pairwise preference alignment\u2014to train\
  \ the model without supervised data."
---

# Personalized Image Generation with Large Multimodal Models

## Quick Facts
- arXiv ID: 2410.14170
- Source URL: https://arxiv.org/abs/2410.14170
- Authors: Yiyan Xu; Wenjie Wang; Yang Zhang; Biao Tang; Peng Yan; Fuli Feng; Xiangnan He
- Reference count: 40
- Primary result: Pigeon achieves 7-31% improvement in personalization metrics over baselines while maintaining semantic alignment and fidelity

## Executive Summary
Pigeon is a personalized image generation framework that leverages large multimodal models (LMMs) to capture user preferences from noisy history images and multimodal instructions. The framework addresses the challenge of limited supervised data by introducing a two-stage preference alignment process that includes masked preference reconstruction and pairwise preference alignment using Direct Preference Optimization. Pigeon significantly outperforms baseline methods in personalization metrics while maintaining comparable semantic alignment and fidelity, as validated through both quantitative evaluation and human studies.

## Method Summary
Pigeon employs a two-stage preference alignment scheme to train LMMs for personalized image generation without supervised data. The first stage uses masked preference reconstruction to fine-tune the model to reconstruct target images from history and masked reference images. The second stage employs pairwise preference alignment using Direct Preference Optimization to enhance personalization based on preference pairs. The framework uses LaVIT as the base LMM with a mask generation module for token-level noise filtering, a personalization module for multimodal instruction encoding, and an image generation module (Stable Diffusion XL) to convert personalized tokens into final images.

## Key Results
- Pigeon achieves 7-31% improvement in personalization metrics (CLIP Image Score, DINO Image Score, LPIPS, MS-SSIM) over baselines
- Maintains comparable semantic alignment with reference images (CLIP Score, FID)
- Validated through both quantitative evaluation and human studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pigeon effectively captures user preferences from noisy history images using a token-level masking strategy
- Mechanism: The mask generation module uses a Transformer encoder to encode history and reference images, then computes cosine similarity between hidden states and original visual token embeddings to identify and mask low-score tokens that are likely to contain noise
- Core assumption: User-interacted history images contain both relevant personalized information and noisy signals, with the relevant information having higher similarity to the reference image
- Evidence anchors:
  - [abstract] "Mask generation module incorporates a mask generator to create token-level masks for reference-aware history filtering, effectively removing noisy signals from the history images at the feature level"
  - [section 2.2.1] "To quantify the importance of each token, we compute the cosine similarity between the hidden states and the original visual token embeddings"
- Break condition: If user history images are extremely diverse or the reference image is not representative of user preferences, the masking strategy may filter out relevant information or retain noise

### Mechanism 2
- Claim: Pigeon achieves effective multimodal instruction encoding by extracting both high-level visual and textual semantic features
- Mechanism: The personalization module uses a caption model to generate textual descriptions of the reference image and a tokenizer decoder with average pooling to extract dense visual features, which are then aligned through an adapter layer
- Core assumption: High-level semantic features extracted from the reference image can effectively guide the generation of personalized target images while preserving the multimodal instruction context
- Evidence anchors:
  - [abstract] "Personalization module integrates masked history tokens and encodes multimodal instructions with the transformed semantic features of the reference image to generate personalized tokens"
  - [section 2.2.2] "To enrich the semantics of the reference image ùíô0 and enhance the comprehension of multimodal instructions in LMMs, we utilize a caption model... to generate a textual description"
- Break condition: If the caption model fails to accurately describe the reference image or the visual feature extraction misses critical semantic information, the multimodal instruction encoding will be ineffective

### Mechanism 3
- Claim: Pigeon's two-stage preference alignment process effectively adapts LMMs for personalized image generation without supervised data
- Mechanism: Stage 1 uses masked preference reconstruction to fine-tune the model to reconstruct target images from history and masked reference images, while Stage 2 employs pairwise preference alignment using Direct Preference Optimization to enhance personalization based on preference pairs
- Core assumption: User history images, despite containing noise, partially reflect user preferences, and the model can learn to distinguish between chosen and rejected generated images through preference scoring
- Evidence anchors:
  - [abstract] "To alleviate the data scarcity, we introduce a two-stage preference alignment scheme, comprising masked preference reconstruction and pairwise preference alignment"
  - [section 2.2.4] "Stage-1: Masked Preference Reconstruction... treats the last one as the personalized target image, while the preceding images are treated as history images"
  - [section 2.2.4] "Stage-2: Pairwise Preference Alignment... generates multiple target images... ranks them using a preference reward strategy"
- Break condition: If the preference reward strategy fails to accurately rank generated images or the model cannot learn meaningful preferences from the pseudo-labeled data, the two-stage alignment will not improve personalization

## Foundational Learning

- Concept: Large Multimodal Models (LMMs) and their capabilities
  - Why needed here: Pigeon is built on LMMs, specifically LaVIT, to leverage their multimodal understanding and generation capabilities
  - Quick check question: What are the key components of LMMs like LaVIT that enable both visual and textual processing?

- Concept: Diffusion Models (DMs) for image generation
  - Why needed here: Pigeon uses a DM (Stable Diffusion XL) to convert personalized tokens into final images
  - Quick check question: How do diffusion models progressively refine noisy images into clear outputs?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Pigeon employs DPO in the second stage of preference alignment to optimize the model based on chosen vs rejected image pairs
  - Quick check question: What is the mathematical formulation of DPO and how does it differ from other preference learning methods?

## Architecture Onboarding

- Component map:
  Mask Generator (Transformer encoder) ‚Üí Creates token-level masks for history and reference images ‚Üí Personalization Module (Caption model + Tokenizer decoder + Adapter) ‚Üí Extracts multimodal semantic features ‚Üí LMM (LaVIT) ‚Üí Personalized tokens ‚Üí Image Generation Module (DM: Stable Diffusion XL) ‚Üí Personalized target image

- Critical path: User history images + Reference image + Multimodal instructions ‚Üí Mask Generator ‚Üí Masked history tokens + Encoded multimodal features ‚Üí LMM (LaVIT) ‚Üí Personalized tokens ‚Üí Image Generation Module ‚Üí Personalized target image

- Design tradeoffs:
  - Using LMMs provides strong multimodal capabilities but increases computational complexity
  - Two-stage alignment avoids need for supervised data but requires careful design of preference reward strategy
  - Token-level masking provides fine-grained noise filtering but may lose some relevant information

- Failure signatures:
  - Poor personalization: History mask may be too aggressive or preference reward strategy ineffective
  - Poor semantic alignment: Multimodal instruction encoding fails or reference mask ratio inappropriate
  - Low fidelity: Image generation module (DM) not properly conditioned on personalized tokens

- First 3 experiments:
  1. Test mask generator effectiveness by varying history mask ratio and measuring personalization metrics
  2. Validate multimodal instruction encoding by comparing with and without visual/textual feature extraction
  3. Evaluate two-stage alignment by comparing performance after stage 1 vs stage 2 on a held-out validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Pigeon's performance change when applied to lifelong user histories instead of fixed interaction sequences?
- Basis in paper: [inferred] The paper mentions potential future work on "developing efficient strategies to manage lifelong user history for superior personalization."
- Why unresolved: The current implementation uses fixed-length interaction sequences (5 history images), but real-world applications would involve continuously evolving user histories
- What evidence would resolve it: Comparative experiments showing personalization metrics (CIS, LPIPS, etc.) when using sliding windows of different lengths over extended user histories

### Open Question 2
- Question: Can Pigeon's preference alignment scheme be adapted to work with other LMM architectures beyond LaVIT?
- Basis in paper: [explicit] "Pigeon can also be applied to more LMMs, which is left for future exploration."
- Why unresolved: The paper only demonstrates Pigeon with LaVIT, despite claiming it can work with other LMMs
- What evidence would resolve it: Empirical results showing Pigeon's performance when applied to LLaVA, Gemini, or other LMMs, with comparative metrics to LaVIT-based results

### Open Question 3
- Question: How does Pigeon's performance degrade when the reference image is semantically dissimilar to user preferences?
- Basis in paper: [inferred] The paper focuses on cases where reference images align with user preferences, but doesn't explore edge cases of semantic mismatch
- Why unresolved: The experiments use reference images from the same theme/genre as user history, but real-world applications may involve arbitrary reference images
- What evidence would resolve it: Controlled experiments where reference images are deliberately chosen to be semantically dissimilar from user history, measuring the trade-off between personalization and semantic alignment

## Limitations

- The framework's performance on more diverse user preferences (combining abstract art, photography, and graphic design) remains untested
- The mechanism for distinguishing relevant from noisy tokens in history images through cosine similarity scoring lacks rigorous validation
- The preference reward strategy used to rank generated images hasn't been validated against human expert judgment beyond the human study

## Confidence

- High confidence: The two-stage preference alignment framework is technically sound and the implementation using established methods (LaVIT, Stable Diffusion XL, DPO) is well-grounded in existing literature
- Medium confidence: The quantitative improvements (7-31% over baselines) are promising but the evaluation metrics may not fully capture the nuanced quality of personalized generation
- Low confidence: The mechanism for distinguishing relevant from noisy tokens in history images through cosine similarity scoring lacks rigorous validation, and the break conditions for this mechanism are not thoroughly explored

## Next Checks

1. **Ablation study on mask ratios**: Systematically vary the history mask ratio (15%, 30%, 45%, 60%) and reference mask ratio (50%, 70%, 90%) during both training and inference to quantify their impact on personalization metrics and identify optimal values for different preference distributions

2. **Cross-domain generalization test**: Evaluate Pigeon on a multi-domain dataset combining user preferences from at least three visually distinct categories (e.g., abstract art, product photography, and architectural visualization) to assess robustness beyond the current single-domain evaluations

3. **Preference reward strategy audit**: Conduct a controlled experiment comparing the model's preferred images (based on the reward strategy) against a blind human preference test with professional designers to validate that the automated ranking aligns with expert judgment of personalization quality