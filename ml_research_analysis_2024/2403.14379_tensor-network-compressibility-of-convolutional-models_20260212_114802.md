---
ver: rpa2
title: Tensor network compressibility of convolutional models
arxiv_id: '2403.14379'
source_url: https://arxiv.org/abs/2403.14379
tags:
- convolution
- tensor
- truncation
- kernel
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates why tensorization\u2014compressing CNN\
  \ convolution kernels using low-rank tensor decompositions\u2014does not hurt accuracy.\
  \ The authors analyze dense (untensorized) CNNs by truncating their convolution\
  \ kernels using SVD and CP decomposition, then measuring the impact on classification\
  \ accuracy."
---

# Tensor network compressibility of convolutional models

## Quick Facts
- arXiv ID: 2403.14379
- Source URL: https://arxiv.org/abs/2403.14379
- Reference count: 40
- Primary result: CNNs can be significantly compressed via tensor decompositions without accuracy loss, with truncated models recovering original accuracy after only a few retraining epochs

## Executive Summary
This paper investigates why tensorization—compressing CNN convolution kernels using low-rank tensor decompositions—does not hurt accuracy. The authors analyze dense (untensorized) CNNs by truncating their convolution kernels using SVD and CP decomposition, then measuring the impact on classification accuracy. They find that kernels can often be truncated significantly (up to 50% norm loss) without large accuracy loss, especially in deeper layers. Compressed models recover original accuracy after only a few epochs of re-training, suggesting truncations do not move the model to worse minima. This indicates CNNs inherently encode information in a compressible way, explaining why tensorization works effectively for compression without sacrificing performance.

## Method Summary
The authors analyze pre-trained dense CNNs (ResNet-50 and vanilla CNN) by applying SVD and CP decomposition-based truncation to convolution kernels. They reshape kernels according to different bipartitions (single-mode and two-mode) and truncate by discarding small singular values or reducing CP rank. The study measures norm loss, correlation loss, and accuracy impact across multiple layers and truncation magnitudes. Truncated models are then retrained to assess recovery speed and verify that truncations don't move models to worse minima.

## Key Results
- Convolution kernels can be truncated by up to 50% of their norm without significant accuracy loss, especially in deeper layers
- SVD-based truncation across the OUT mode is more effective than across other modes
- Truncated models recover original accuracy after only a few retraining epochs, suggesting no movement to worse minima
- CIFAR-10 and CIFAR-100 classification tasks show minimal accuracy degradation with significant compression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor network compressibility is an intrinsic feature of dense CNNs, not just a byproduct of explicit low-rank training bias.
- Mechanism: The learned convolution kernels already encode redundancy that allows significant truncation without large accuracy loss, especially in deeper layers.
- Core assumption: CNN training naturally discovers low-rank structures in kernels because redundant information is not necessary for task performance.
- Evidence anchors:
  - [abstract] "kernels can often be truncated significantly (up to 50% norm loss) without large accuracy loss, especially in deeper layers."
  - [section 4.3] "truncation up to 50% of the norm has little impact on the model's accuracy."
  - [corpus] No direct match; corpus discusses compression techniques but not inherent compressibility evidence.
- Break condition: If deeper layers lose accuracy significantly upon truncation, or if retraining does not recover accuracy.

### Mechanism 2
- Claim: SVD-based truncation across the OUT mode is more effective than across other modes.
- Mechanism: The OUT mode bipartition exposes more compressible correlations because it groups many dimensions together, creating larger singular value spectra with small values.
- Core assumption: Grouping more modes in a bipartition increases the chance of finding compressible singular values.
- Evidence anchors:
  - [section 4.3] "We find that truncation up to 50% of the norm has little impact on the model's accuracy... especially for OUT mode."
  - [section 4.2] "However, the spectrum for bipartition OUT , KW contains several small singular values that can be discarded without significant norm loss."
  - [corpus] No direct match; corpus neighbors discuss compression but not mode-specific effectiveness.
- Break condition: If truncating across OUT mode results in large accuracy loss compared to other modes.

### Mechanism 3
- Claim: Post-truncation accuracy recovery after only a few epochs indicates the model does not move to a worse minimum.
- Mechanism: Truncation removes redundant correlations but preserves essential information, so retraining can quickly restore performance without exploring worse regions of the loss landscape.
- Core assumption: The loss surface near the original minimum is relatively flat in the truncated directions, allowing quick recovery.
- Evidence anchors:
  - [abstract] "Compressed models recover original accuracy after only a few epochs of re-training, suggesting truncations do not move the model to worse minima."
  - [section 4.5] "Remarkably, the model recovered the pre-truncation accuracy only after a few training epochs."
  - [corpus] No direct match; corpus neighbors do not discuss post-truncation recovery dynamics.
- Break condition: If retraining requires many epochs or fails to recover original accuracy.

## Foundational Learning

- Concept: Tensor network contractions as a framework for understanding convolutions
  - Why needed here: The paper uses tensor network language to describe convolution operations and compression, making it essential to follow the analysis.
  - Quick check question: How would you express a 2D convolution as a tensor contraction between the input image tensor and kernel tensor?

- Concept: SVD-based correlation truncation
  - Why needed here: The paper's main experimental method involves truncating convolution kernels by discarding small singular values from reshaped kernel matrices.
  - Quick check question: What happens to the Frobenius norm of a matrix when you discard its smallest singular values?

- Concept: CP decomposition for tensor compression
  - Why needed here: The paper compares SVD-based truncation with CP decomposition-based truncation as an alternative compression method.
  - Quick check question: How does the CP rank control the level of compression in a tensor decomposition?

## Architecture Onboarding

- Component map:
  - Pre-trained dense CNN -> SVD/CP truncation applied to convolution kernels -> Truncated model with measured norm loss, accuracy loss, and retraining recovery

- Critical path:
  1. Load pre-trained dense CNN
  2. Select convolution layers for truncation
  3. For each kernel, reshape according to chosen bipartition
  4. Perform SVD or CP decomposition
  5. Truncate by discarding smallest singular values or reducing CP rank
  6. Measure norm loss and accuracy impact
  7. Retrain truncated model and measure recovery speed

- Design tradeoffs:
  - SVD vs CP: SVD is more stable and effective for most truncations, while CP can be unstable for high ranks but may capture different correlation structures
  - Bipartition choice: Larger bipartitions (like OUT) generally allow more compression but may have different accuracy impacts
  - Layer selection: Deeper layers are more compressible but truncating too many layers simultaneously reduces robustness

- Failure signatures:
  - Large accuracy drop immediately after truncation indicates critical information loss
  - Slow or incomplete retraining recovery suggests the model moved to a worse minimum
  - Numerical instability during CP decomposition indicates rank is too high for stable approximation

- First 3 experiments:
  1. Apply single-mode SVD truncation to OUT mode of conv1 in ResNet-50, measure norm loss vs accuracy impact
  2. Apply two-mode SVD truncation to OUT-KW mode of conv4 in ResNet-50, compare with OUT mode results
  3. Apply CP decomposition truncation with rank 20 to all convolution layers, measure compression ratio and accuracy retention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the accuracy of truncated CNNs recover so quickly after only a few epochs of re-training?
- Basis in paper: [explicit] The authors observe that aggressively truncated models recover pre-truncation accuracy after only a few epochs, suggesting truncations do not move models to worse minima, but the mechanism remains unclear.
- Why unresolved: The paper proposes potential explanations (e.g., preserved classifier layers, residual connections) but does not conclusively identify the underlying cause or mechanism.
- What evidence would resolve it: Controlled ablation studies systematically varying which layers are truncated, comparing models with/without residual connections, and analyzing loss landscape topology before/after truncation could clarify whether recovery stems from preserved information, favorable optimization paths, or other factors.

### Open Question 2
- Question: Is the robustness against correlation truncation observed in CNNs generalizable to other neural architectures like transformers or MLPs?
- Basis in paper: [explicit] The authors hypothesize that robustness should apply to other layers and architectures (fully connected, attention layers, transformers, etc.) but do not test this claim experimentally.
- Why unresolved: The study focuses exclusively on CNN convolution layers; no experiments were conducted on transformers, MLPs, or other architectures to verify if similar robustness exists.
- What evidence would resolve it: Direct experiments applying SVD/CP-based truncation to attention matrices in transformers, weight matrices in MLPs, and other architectures, measuring accuracy impact and post-truncation recovery across diverse tasks.

### Open Question 3
- Question: Does tensorization of CNNs capture all the accuracy benefits of correlation truncation, or are there scenarios where truncation without tensorization performs better?
- Basis in paper: [inferred] The paper shows correlation truncation preserves accuracy but does not compare this directly to tensorized models' performance on the same tasks.
- Why unresolved: While the paper demonstrates truncation's effectiveness, it does not benchmark whether explicitly tensorized models (trained with tensor decomposition constraints) achieve similar or better accuracy than models that are truncated post-training.
- What evidence would resolve it: Systematic comparison experiments training tensorized models from scratch versus truncating dense models post-training, measuring final accuracy, convergence speed, and compression ratios for identical tasks and architectures.

## Limitations
- The study focuses exclusively on CNN architectures and does not test whether the compressibility phenomenon generalizes to transformers, MLPs, or other neural architectures
- The mechanism behind rapid post-truncation accuracy recovery remains unclear and unexplained
- The analysis relies on post-hoc truncation of pre-trained models rather than training with explicit tensor decomposition constraints from the start

## Confidence

**High confidence**: SVD truncation across OUT mode is more effective than other modes (supported by specific experimental observations)

**Medium confidence**: CNNs inherently encode information in a compressible way (inferred from truncation results but not definitively proven)

**Medium confidence**: Post-truncation accuracy recovery indicates no movement to worse minima (supported by retraining results but alternative explanations exist)

## Next Checks

1. Test truncation on randomly initialized models (before training) to determine if compressibility is present from initialization or emerges during training
2. Compare truncation effectiveness across different CNN architectures (ResNet vs vanilla CNN) to assess universality of the compressibility phenomenon
3. Measure loss landscape curvature near original and post-truncation minima to verify the claim about not moving to worse minima