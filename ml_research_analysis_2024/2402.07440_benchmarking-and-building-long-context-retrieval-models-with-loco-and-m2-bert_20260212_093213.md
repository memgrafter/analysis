---
ver: rpa2
title: Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT
arxiv_id: '2402.07440'
source_url: https://arxiv.org/abs/2402.07440
tags:
- retrieval
- m2-bert
- locov1
- length
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of long-context retrieval, where
  existing retrieval systems struggle to handle documents of 10K tokens or more that
  require synthesizing information across the entire text. The authors introduce LoCoV1,
  a novel 12-task benchmark designed to evaluate long-context retrieval performance,
  and present M2-BERT, an 80M parameter state-space model based on the Monarch Mixer
  architecture capable of processing up to 32K token sequences.
---

# Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT

## Quick Facts
- arXiv ID: 2402.07440
- Source URL: https://arxiv.org/abs/2402.07440
- Authors: Jon Saad-Falcon; Daniel Y. Fu; Simran Arora; Neel Guha; Christopher Ré
- Reference count: 40
- One-line primary result: M2-BERT outperforms competitive Transformer-based models by at least 23.3 points on LoCoV1 despite having 90× fewer parameters

## Executive Summary
This paper introduces M2-BERT, a novel state-space model for long-context retrieval that can handle documents up to 32K tokens. The model uses Monarch Mixer architecture for subquadratic scaling and orthogonal projection loss for efficient fine-tuning with single-sample batches. M2-BERT achieves significant improvements over Transformer-based models on the LoCoV1 benchmark while being substantially more parameter-efficient.

## Method Summary
The paper presents M2-BERT, an 80M parameter model based on the Monarch Mixer architecture that can process up to 32K token sequences. The model is pretrained on a mixture of short and long text sequences from C4, Wikipedia, and BookCorpus using masked language modeling. For fine-tuning, the authors use orthogonal projection loss (OPL) to enable effective training with single-sample batches, avoiding the need for large negative mining. The LoCoV1 benchmark is introduced with 12 tasks from real-world datasets spanning diverse domains.

## Key Results
- M2-BERT outperforms competitive Transformer-based models by at least 23.3 points on LoCoV1
- Achieves 3-676× better efficiency in embedding generation compared to the next state-of-the-art model
- Maintains superior performance despite having 90× fewer parameters than baseline models
- Successfully handles 32K token sequences while competitive models struggle with long-context retrieval

## Why This Works (Mechanism)

### Mechanism 1
M2-BERT's Monarch Mixer architecture enables subquadratic scaling with input context length by using Monarch matrices as a subquadratic primitive, replacing the quadratic attention mechanism in Transformers.

### Mechanism 2
Orthogonal Projection Loss (OPL) enables effective fine-tuning with single-sample batches by optimizing the distance between query-document pairs to be aligned for positives and orthogonal for negatives.

### Mechanism 3
Pretraining with mixed short and long sequences enables handling both query and document contexts by allowing the model to learn representations for both short queries and long documents.

## Foundational Learning

- Concept: Masked Language Modeling (MLM) pretraining objective
  - Why needed here: MLM pretraining allows the model to learn language understanding from unlabeled text, which is essential for building a general-purpose retrieval encoder
  - Quick check question: What is the purpose of masking tokens during pretraining, and how does predicting them help the model learn language representations?

- Concept: Contrastive learning and embedding geometry
  - Why needed here: Understanding how contrastive losses like MNRL and OPL shape the embedding space is crucial for designing effective fine-tuning strategies
  - Quick check question: How does MNRL create alignment between positive pairs and uniformity across the embedding hypersphere, and why is this geometry important for retrieval?

- Concept: Subquadratic architectures and their scaling properties
  - Why needed here: Understanding how Monarch Mixer and other SSM architectures achieve subquadratic scaling is essential for appreciating why M2-BERT can handle longer contexts than Transformers
  - Quick check question: What is the computational complexity of attention in Transformers versus the Monarch matrices in M2-BERT, and how does this difference enable longer context handling?

## Architecture Onboarding

- Component map: Tokenized text sequences -> Monarch Mixer layers -> Embedding projection -> Loss calculation
- Critical path: 1. Tokenization and input preparation 2. Monarch Mixer processing through all layers 3. Embedding projection and normalization 4. Loss calculation and backpropagation
- Design tradeoffs: Monarch Mixer vs. attention (better scaling vs. potential cost in representational power), OPL vs. MNRL (single-sample training vs. potentially better geometry), mixed-sequence pretraining vs. uniform-length (ability to handle both short and long contexts vs. potentially simpler pretraining)
- Failure signatures: Poor retrieval accuracy (could indicate issues with fine-tuning, embedding projection, or insufficient representational capacity), slow training (may suggest inefficient Monarch matrix operations or suboptimal hyperparameters), memory issues (could indicate problems with sequence length handling or batch size configuration)
- First 3 experiments: 1. Validate Monarch Mixer forward pass by comparing outputs with a small Transformer baseline 2. Test OPL fine-tuning on a small retrieval dataset to verify single-sample batch training works 3. Evaluate mixed-sequence pretraining by training on short sequences only, long sequences only, and the mixture to confirm the mixture performs best

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does M2-BERT perform on non-English retrieval tasks compared to other state-of-the-art models?
- Basis in paper: [inferred] The paper primarily focuses on English retrieval tasks and mentions testing M2-BERT on the MTEB benchmark for English datasets. However, it does not provide results for non-English retrieval tasks.
- Why unresolved: The paper does not explore the performance of M2-BERT on non-English retrieval tasks, leaving a gap in understanding its cross-lingual capabilities.
- What evidence would resolve it: Experiments comparing M2-BERT's performance on non-English retrieval tasks against other state-of-the-art models, such as BGE-Large and E5-Mistral, would provide insights into its cross-lingual retrieval capabilities.

### Open Question 2
- Question: How does the performance of M2-BERT vary with different pretraining data mixtures and sequence length distributions?
- Basis in paper: [explicit] The paper discusses the importance of pretraining data mixture for M2-BERT, but does not provide a detailed analysis of how different pretraining data mixtures and sequence length distributions affect its performance.
- Why unresolved: The paper only presents one pretraining data mixture and sequence length distribution, leaving questions about the optimal configuration for M2-BERT's performance.
- What evidence would resolve it: Experiments varying the pretraining data mixtures and sequence length distributions, and comparing their impact on M2-BERT's performance on LoCoV1 and other benchmarks, would provide insights into the optimal pretraining configuration.

### Open Question 3
- Question: How does M2-BERT's performance scale with increasing sequence lengths beyond 32K tokens?
- Basis in paper: [inferred] The paper presents M2-BERT models with sequence lengths up to 32K tokens, but does not explore its performance on even longer sequences.
- Why unresolved: The paper does not investigate the scalability of M2-BERT beyond 32K tokens, leaving questions about its ability to handle extremely long documents.
- What evidence would resolve it: Experiments evaluating M2-BERT's performance on retrieval tasks with documents longer than 32K tokens would provide insights into its scalability and potential limitations.

## Limitations

- Limited evaluation scope: The evaluation is constrained to relatively narrow domains (legal, scientific, news, and general knowledge) and lacks extensive testing on real-world retrieval scenarios
- Architecture scaling questions: The paper doesn't fully explore how M2-BERT's performance degrades as context length increases beyond 32K tokens
- Single-sample batch limitations: The effectiveness of orthogonal projection loss compared to large-batch contrastive methods requires further investigation

## Confidence

- High Confidence (80-100%): The experimental results demonstrating M2-BERT's superior performance on LoCoV1 (23.3+ point improvements) are well-supported by the presented data
- Medium Confidence (50-80%): The efficiency claims (3-676× improvement in embedding generation) are based on reported metrics but lack comprehensive benchmarking against multiple competitive systems
- Low Confidence (0-50%): The generalization capabilities of M2-BERT to truly diverse real-world retrieval scenarios remain uncertain due to the limited evaluation domains

## Next Checks

1. **Architectural ablation study**: Conduct controlled experiments comparing M2-BERT against an identical model using Transformer attention layers, with all other factors (pretraining, fine-tuning, hyperparameters) held constant
2. **Extended context length evaluation**: Test M2-BERT's performance on sequences beyond 32K tokens (e.g., 64K, 128K) to understand the practical limits of its subquadratic scaling
3. **Real-world deployment benchmark**: Evaluate M2-BERT on a production retrieval dataset with diverse query distributions, document types, and retrieval patterns to assess its generalization beyond the controlled LoCoV1 benchmark tasks