---
ver: rpa2
title: 'SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression'
arxiv_id: '2412.12204'
source_url: https://arxiv.org/abs/2412.12204
tags:
- embedding
- compression
- sememe
- entanglement
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes SEE (Sememe Entanglement Encoding), a method
  for compressing transformer-based models by integrating semantic knowledge from
  HowNet with quantum entanglement principles. The approach represents basic semantic
  units (sememes) as low-dimensional vectors and reconstructs them into high-dimensional
  word embeddings through tensor products.
---

# SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression

## Quick Facts
- arXiv ID: 2412.12204
- Source URL: https://arxiv.org/abs/2412.12204
- Authors: Jing Zhang; Shuzhen Sun; Peng Zhang; Guangxing Cao; Hui Gao; Xindian Ma; Nan Xu; Yuexian Hou
- Reference count: 5
- One-line primary result: Achieves up to 80x compression of embedding parameters while maintaining translation performance within 0.6% of original models

## Executive Summary
This paper proposes Sememe Entanglement Encoding (SEE), a method for compressing transformer-based models by integrating semantic knowledge from HowNet with quantum entanglement principles. The approach represents basic semantic units (sememes) as low-dimensional vectors and reconstructs them into high-dimensional word embeddings through tensor products, achieving dual compression by reducing both vocabulary size and embedding dimensions. Experiments on WMT17 ZH-EN and IWSLT17 ZH-EN translation tasks demonstrate stable performance with up to 80x compression of embedding parameters, while application to Phi3-3B achieves 5x compression with only 0.6% performance loss on ARC benchmarks.

## Method Summary
SEE modifies the embedding layer by replacing direct word embeddings with a computation graph that first looks up sememe and morpheme vectors from HowNet, applies tensor products based on semantic structure, then sums results to reconstruct high-dimensional embeddings. The approach achieves dual compression by reducing vocabulary size (|V|) and embedding dimensions (d) through low-rank approximation of semantic units. A multi-stage distillation method with MSE loss on embedding parameters and hidden states, followed by logits-based distillation and cross-entropy loss, enables effective knowledge transfer from large models to compressed versions.

## Key Results
- Achieves up to 80x compression of embedding parameters on translation tasks
- Maintains translation performance within 0.6% of original models on ARC benchmarks
- Reduces Phi3-3B embedding layer by 5x while preserving accuracy

## Why This Works (Mechanism)

### Mechanism 1
The approach represents basic semantic units (sememes) as low-dimensional vectors and reconstructs high-dimensional word embeddings through tensor products. This achieves dual compression by reducing both vocabulary size and embedding dimensions while incorporating expert-derived semantic knowledge. Core assumption: Semantic meaning can be preserved when decomposed into basic semantic units (sememes) and reconstructed through quantum entanglement principles.

### Mechanism 2
Incorporating HowNet semantic knowledge improves model understanding of metaphors and semantic interactions beyond what pure structural decomposition provides. By explicitly modeling semantic units through expert-annotated HowNet sememes, the model learns deeper semantic interactions at the level of human expert understanding, enhancing metaphor comprehension. Core assumption: Expert-derived semantic knowledge provides more effective semantic representations than learned embeddings alone, especially for metaphorical language.

### Mechanism 3
The multi-stage distillation method with MSE loss on both embedding parameters and hidden states enables effective knowledge transfer from large models to compressed versions. The approach uses MSE loss between original and compressed embedding parameters and hidden states in the initial stage, followed by logits-based distillation and cross-entropy loss in subsequent stages. Core assumption: The compressed model can effectively learn the knowledge representation of the original model through staged distillation that targets both structural parameters and functional outputs.

## Foundational Learning

- Concept: Tensor product decomposition for matrix factorization
  - Why needed here: Understanding how tensor products can decompose high-dimensional embeddings into lower-dimensional components that can be reconstructed is fundamental to SEE's compression mechanism
  - Quick check question: How does the tensor product operation differ from simple matrix multiplication when decomposing embedding matrices?

- Concept: Quantum entanglement principles in information representation
  - Why needed here: The paper leverages quantum entanglement concepts to model non-separable relationships between semantic units, requiring understanding of how quantum systems represent complex correlations
  - Quick check question: In what way does quantum entanglement modeling differ from traditional linear combinations when representing semantic relationships?

- Concept: HowNet semantic knowledge base structure
  - Why needed here: Understanding the organization and scope of HowNet's sememe annotations is crucial for implementing and tuning SEE's semantic decomposition
  - Quick check question: What is the approximate ratio of unique sememes to total vocabulary words in HowNet, and why does this ratio matter for compression?

## Architecture Onboarding

- Component map: Token → sememe/morpheme lookup → tensor product computation → embedding reconstruction → transformer processing
- Critical path: Token → sememe/morpheme lookup → tensor product computation → embedding reconstruction → transformer processing. The bottleneck is the lookup and tensor computation during initialization.
- Design tradeoffs: SEE trades computational overhead during initialization for reduced memory footprint during inference. The semantic lookup table increases model complexity but enables higher compression ratios.
- Failure signatures: Poor BLEU scores on translation tasks despite high compression ratios suggest semantic relationships aren't being adequately captured. High memory usage despite compression claims indicates tensor products aren't reducing dimensions as expected.
- First 3 experiments:
  1. Implement sememe lookup and tensor product computation for a small vocabulary, verify output dimensions match expected embeddings
  2. Test translation performance with various rank and order parameters to find sweet spot between compression and quality
  3. Compare inference latency and memory usage against baseline transformer to quantify overhead costs

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of sememe representation (e.g., different sememe embeddings or clustering approaches) affect the compression ratio and translation quality? The paper mentions using sememes from HowNet but does not explore alternative sememe representations or clustering strategies.

### Open Question 2
What is the optimal balance between rank, order, and m (number of low-dimensional vectors per basic unit) for different model sizes and tasks? The paper mentions sensitivity tests on rank and order but does not provide a comprehensive analysis of the optimal balance for different model sizes and tasks.

### Open Question 3
How does the proposed method compare to other knowledge injection methods (e.g., knowledge graphs, ontologies) in terms of compression ratio and model performance? The paper focuses on sememe-based knowledge injection but does not compare it to other knowledge injection methods.

### Open Question 4
How does the proposed method handle out-of-vocabulary (OOV) words or rare words during inference? The paper does not explicitly address how the proposed method handles OOV or rare words during inference.

## Limitations
- The quantum entanglement framing may overstate the actual quantum information theory principles applied
- Results are primarily demonstrated on Chinese-English translation tasks and may not generalize to other language pairs or task types
- The long-term viability of HowNet-based semantic decomposition across evolving language use is uncertain

## Confidence

**High Confidence**: The core compression mechanism (reducing embedding parameters through sememe decomposition) is well-specified and theoretically sound. The dual compression claim (vocabulary size and dimensionality) is mathematically verifiable.

**Medium Confidence**: The performance claims on translation tasks are supported by experimental results, but the ablation studies for individual components (semantic knowledge, tensor products, distillation) are incomplete. The 5x compression on Phi3-3B with 0.6% performance retention is specific to ARC benchmarks and may not generalize.

**Low Confidence**: The quantum entanglement application appears to use standard tensor products rather than true quantum mechanical entanglement. The computational overhead costs during inference are not fully characterized.

## Next Checks
1. Implement a version of SEE that uses random sememe assignments instead of HowNet annotations, keeping all other components identical. Compare performance degradation to quantify the actual contribution of expert-derived semantic knowledge versus the compression mechanism alone.

2. Apply SEE to non-translation tasks (sentiment analysis, summarization, code generation) using the same compression settings. Measure whether the 0.6% performance retention threshold holds across diverse benchmarks and whether different tasks require different rank/order parameter tuning.

3. Measure inference latency and memory usage for models compressed with SEE across different batch sizes and sequence lengths. Compare against both uncompressed baselines and other compression methods to determine the practical deployment trade-offs beyond parameter count reduction.