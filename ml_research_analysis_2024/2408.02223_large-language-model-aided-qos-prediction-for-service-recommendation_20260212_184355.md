---
ver: rpa2
title: Large Language Model Aided QoS Prediction for Service Recommendation
arxiv_id: '2408.02223'
source_url: https://arxiv.org/abs/2408.02223
tags:
- prediction
- service
- llmqos
- rmse
- services
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel QoS prediction model for web service
  recommendation that leverages large language models (LLMs) to extract descriptive
  features from user and service attributes. The proposed llmQoS model constructs
  natural language descriptions of users and services from their attributes (such
  as country and autonomous system), uses pre-trained LLMs like RoBERTa or Phi3mini
  to extract rich text features from these descriptions, and combines them with traditional
  collaborative filtering approaches to predict QoS values.
---

# Large Language Model Aided QoS Prediction for Service Recommendation

## Quick Facts
- arXiv ID: 2408.02223
- Source URL: https://arxiv.org/abs/2408.02223
- Reference count: 40
- One-line result: llmQoS achieves up to 28.93% improvement in MAE and 9.68% improvement in RMSE for throughput prediction using Phi3mini

## Executive Summary
This paper introduces llmQoS, a novel approach that leverages large language models (LLMs) to extract descriptive features from user and service attributes for QoS prediction in web service recommendation. The model constructs natural language descriptions from user attributes (country, autonomous system) and service attributes (URL, provider, country, autonomous system), then uses pre-trained LLMs like RoBERTa or Phi3mini to extract rich semantic features from these descriptions. These LLM features are combined with traditional collaborative filtering approaches to predict QoS values, effectively addressing the data sparsity problem inherent in QoS prediction.

## Method Summary
The llmQoS model constructs descriptive sentences from user and service attributes, extracts features using pre-trained LLMs, projects these features to match ID embedding dimensions, and concatenates them with user/service ID embeddings. The combined features are processed through an MLP network to predict QoS values. The model is trained using Huber loss with the Adam optimizer and evaluated on the WSDream dataset for throughput and response time prediction across different data sparsity levels (5%, 10%, 15%, 20%).

## Key Results
- Phi3mini-based llmQoS achieves up to 28.93% improvement in MAE and 9.68% improvement in RMSE for throughput prediction
- Phi3mini-based llmQoS achieves up to 21.40% improvement in MAE and 2.01% improvement in RMSE for response time prediction
- Larger LLMs like Phi3mini consistently outperform smaller ones like RoBERTa across all sparsity levels and evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM features provide complementary information that mitigates data sparsity in QoS prediction
- Mechanism: Descriptive sentences constructed from user and service attributes are input to pre-trained LLMs, which extract rich semantic features capturing latent relationships between users/services not present in historical interaction data alone
- Core assumption: Attributes (country, autonomous system, URL, service provider) contain meaningful proximity and similarity signals that LLMs can effectively extract and encode
- Evidence anchors:
  - [abstract]: "The model effectively addresses the data sparsity problem inherent in QoS prediction by incorporating complementary information from user and service attributes"
  - [section]: "The descriptive LLM features provide information that is complementary to the historical user-service interactions"
  - [corpus]: Weak - the corpus contains related QoS prediction papers but none specifically about LLM-aided approaches, suggesting this is a novel mechanism

### Mechanism 2
- Claim: Larger LLMs like Phi3mini outperform smaller ones like RoBERTa due to higher capacity and training on more data
- Mechanism: The larger parameter count (3.5B vs 120M) and training on significantly more tokens (3.3T vs 160B) allows Phi3mini to capture more complex patterns and generalize better from the descriptive text features
- Core assumption: The increased model capacity and training data directly translate to better feature extraction quality for the QoS prediction task
- Evidence anchors:
  - [abstract]: "The results show that larger LLMs like Phi3mini consistently outperform smaller ones like RoBERTa"
  - [section]: "Phi3mini has 3.5 × 109 parameters, which is significantly larger than RoBERTa's 1.2 × 108 million parameters"
  - [corpus]: Weak - no direct comparison of LLM sizes in QoS tasks in the corpus, though general LLM literature supports this claim

### Mechanism 3
- Claim: LLM features can be integrated into various QoS prediction architectures and still improve performance
- Mechanism: The descriptive text features are added as additional input alongside traditional ID embeddings, and this combination improves prediction regardless of the underlying architecture (simple MLP or complex graph-based models like BGCL)
- Core assumption: The LLM features contain generalizable information that enhances any QoS prediction model, not just the specific architecture proposed
- Evidence anchors:
  - [section]: "LLM features can be integrated into more complex QoS prediction models and can still increase the performance significantly and consistently"
  - [section]: "We keep all other features and network architecture unchanged for BGCL, with the only addition of the LLM feature vectors"
  - [corpus]: Weak - the corpus shows various QoS prediction approaches but none combining them with LLM features, indicating this is an unexplored integration

## Foundational Learning

- Concept: Quality of Service (QoS) prediction
  - Why needed here: The paper's core task is predicting QoS values (throughput, response time) for user-service pairs based on historical data and attributes
  - Quick check question: What are the two main QoS metrics evaluated in this paper, and how are they measured?

- Concept: Collaborative filtering and matrix factorization
  - Why needed here: The paper builds upon traditional CF methods and combines them with LLM features for improved prediction
  - Quick check question: What are the two main categories of collaborative filtering methods mentioned, and how do they differ in their approach?

- Concept: Large Language Models (LLMs) and feature extraction
  - Why needed here: The paper's novel contribution is using LLMs to extract semantic features from descriptive text about users and services
  - Quick check question: What is the dimensionality of the Phi3mini feature vector before and after projection in the proposed model?

## Architecture Onboarding

- Component map:
  - User/Service attributes → Descriptive sentence → LLM feature extraction → Projection → Concatenation with ID embeddings → MLP network → Prediction layer
  - Service attributes (URL, provider, country, AS) → Descriptive sentence → LLM feature extraction → Projection → Concatenation with Service ID embeddings → MLP network → Prediction layer

- Critical path: Attribute → Descriptive sentence → LLM feature extraction → Projection → Concatenation with ID embeddings → MLP → Prediction

- Design tradeoffs:
  - LLM choice: Larger models (Phi3mini) give better accuracy but higher computational cost
  - Feature dimensionality: Higher dimensionality captures more information but increases model complexity
  - MLP architecture: Simple architecture keeps focus on LLM contribution but may limit performance ceiling
  - Loss function: Huber loss is more robust to outliers than MSE but may converge slower

- Failure signatures:
  - Poor performance: Check if LLM features are actually being used (ablation test), verify attribute quality, examine if model is overfitting
  - Training instability: Adjust learning rate, check input normalization, verify loss function implementation
  - Slow convergence: Increase batch size, adjust learning rate schedule, verify data pipeline efficiency

- First 3 experiments:
  1. Train with only ID embeddings (no LLM features) to establish baseline performance
  2. Add RoBERTa LLM features to compare improvement from small LLM
  3. Replace with Phi3mini LLM features to verify larger model benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of llmQoS scale with increasingly larger LLMs beyond Phi3mini (e.g., Phi4, GPT-4, or Llama-3)?
- Basis in paper: [explicit] The paper discusses that larger LLMs like Phi3mini outperform smaller ones like RoBERTa, and mentions that larger LLMs have higher capacity and can learn more information from larger text corpora
- Why unresolved: The experiments only compare Phi3mini with RoBERTa. The paper does not test performance with LLMs that have hundreds of billions of parameters
- What evidence would resolve it: Comparative experiments showing MAE/RMSE performance of llmQoS using various larger LLMs (GPT-4, Llama-3, Phi4) on the WSDream dataset

### Open Question 2
- Question: What is the optimal methodology for constructing descriptive sentences from user and service attributes to maximize LLM feature extraction quality?
- Basis in paper: [explicit] The paper constructs descriptive sentences from attributes like country and autonomous system but does not explore alternative constructions or conduct sensitivity analysis on sentence structure
- Why unresolved: The paper uses a specific sentence construction method but does not investigate whether different formulations (e.g., including/excluding certain attributes, reordering information) would yield better LLM features
- What evidence would resolve it: Systematic experiments varying the descriptive sentence construction methodology while keeping all other factors constant, measuring the resulting QoS prediction performance

### Open Question 3
- Question: How does llmQoS perform on datasets from different domains or with different types of user/service attributes?
- Basis in paper: [inferred] The paper only evaluates on the WSDream dataset with specific attributes (country, autonomous system, etc.). The generalizability to other datasets or attribute types is not demonstrated
- Why unresolved: The experiments are limited to a single dataset with specific attributes. The paper does not test whether the LLM feature extraction approach works equally well for other types of user/service attributes or in different application domains
- What evidence would resolve it: Performance evaluation of llmQoS on multiple datasets from different domains (e.g., e-commerce recommendations, content delivery networks) with varying attribute types and structures

## Limitations

- The paper's evaluation is limited to a single dataset (WSDream), raising questions about generalizability to other QoS prediction scenarios with different attribute structures or service types
- The descriptive sentence construction method is not fully specified, particularly for handling numerical attributes like IP addresses and coordinates, which could impact feature quality
- No ablation studies are provided to quantify the individual contribution of user vs service LLM features to overall performance improvements

## Confidence

- **High confidence**: The mechanism that LLM features provide complementary information to historical interaction data, as this is directly demonstrated through improved performance metrics across all test scenarios
- **Medium confidence**: The superiority of larger LLMs (Phi3mini vs RoBERTa) for this task, as while results show consistent improvement, the paper doesn't explore whether this scales with even larger models or if it's specific to the WSDream dataset characteristics
- **Low confidence**: The claim that LLM features can be seamlessly integrated into various QoS prediction architectures, as only two architectures (simple MLP and BGCL) were tested without exploring the full spectrum of potential models

## Next Checks

1. Test the model on additional QoS prediction datasets with different attribute types and service domains to verify generalizability beyond WSDream
2. Conduct ablation studies to isolate the contribution of user attributes vs service attributes in the LLM feature extraction, and determine if one source of information is more valuable than the other
3. Implement and evaluate the model with even larger LLMs (e.g., LLaMA-2, Mistral) to determine if the performance gains continue to scale with model size or if diminishing returns occur