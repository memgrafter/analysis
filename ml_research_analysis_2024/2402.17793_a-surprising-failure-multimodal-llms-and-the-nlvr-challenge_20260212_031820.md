---
ver: rpa2
title: A Surprising Failure? Multimodal LLMs and the NLVR Challenge
arxiv_id: '2402.17793'
source_url: https://arxiv.org/abs/2402.17793
tags:
- sentence
- image
- 'false'
- gpt-4v
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates three state-of-the-art multimodal large language\
  \ models\u2014GPT-4V, Gemini Pro, and IDEFICS\u2014on the NLVR task, which requires\
  \ compositional and spatial reasoning. Despite their strong performance on other\
  \ vision-language tasks, all three models show poor accuracy on NLVR, with GPT-4V\
  \ achieving the highest score of 59.9% using zero-shot prompting."
---

# A Surprising Failure? Multimodal LLMs and the NLVR Challenge

## Quick Facts
- arXiv ID: 2402.17793
- Source URL: https://arxiv.org/abs/2402.17793
- Reference count: 11
- Primary result: Current multimodal LLMs achieve only 49.9-59.9% accuracy on NLVR task, far below human (95.4%) and specialized architectures (78.3%)

## Executive Summary
This study evaluates three state-of-the-art multimodal large language models—GPT-4V, Gemini Pro, and IDEFICS—on the NLVR task, which requires compositional and spatial reasoning. Despite their strong performance on other vision-language tasks, all three models show poor accuracy on NLVR, with GPT-4V achieving the highest score of 59.9% using zero-shot prompting. Gemini Pro performs worst at 49.9%, while IDEFICS reaches 55.9% zero-shot but improves to 59.7% after fine-tuning. The models struggle particularly with compositional reasoning and spatial relations, failing to match human-level performance (95.4%) or previous specialized architectures (78.3%). The study highlights that current multimodal models still struggle with fundamental visual reasoning challenges despite their impressive capabilities on other benchmarks.

## Method Summary
The paper evaluates three MLLMs (GPT-4V, Gemini Pro, IDEFICS) on NLVR Test-P split (5,940 examples) using zero-shot and five-shot prompting. IDEFICS is also fine-tuned on NLVR training data using QLoRA with 4-bit quantization. The evaluation compares performance across image types (Tower vs Scatter) and analyzes failure modes in compositional reasoning.

## Key Results
- GPT-4V achieves highest zero-shot accuracy at 59.9%, significantly below human (95.4%) and previous state-of-the-art (78.3%)
- Gemini Pro performs worst at 49.9% zero-shot, showing the largest gap to human performance
- IDEFICS improves from 55.9% zero-shot to 59.7% after fine-tuning, demonstrating limited benefit from additional training
- All models perform better on structured Tower images than scattered Scatter images, indicating visual encoding biases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLMs rely heavily on semantic shortcuts rather than compositional visual reasoning.
- Mechanism: The models fail to parse fine-grained spatial relationships and compositional descriptions because they shortcut to pattern matching on object labels and superficial correlations.
- Core assumption: NLVR was constructed to avoid semantic biases, so failures indicate fundamental compositional reasoning gaps.
- Evidence anchors:
  - [abstract] "NLVR was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases."
  - [section 1] "NLVR uses simple geometrical shapes, and was generated to be robust to systematic and semantic biases."
  - [corpus] Weak - neighbors focus on compositional generalization but no direct NLVR evaluation.
- Break condition: Performance improves if models are explicitly trained or prompted to decompose sentences into spatial relations before comparing to visual features.

### Mechanism 2
- Claim: Zero-shot prompting with chain-of-thought reasoning slightly improves compositional accuracy.
- Mechanism: Chain-of-thought prompts force intermediate reasoning steps, which may help models attend to compositional details rather than surface cues.
- Core assumption: Intermediate reasoning annotations would further improve accuracy, but are unavailable.
- Evidence anchors:
  - [section 3.2.1] "For GPT-4V, the best prompt uses chain-of-thought...but as we don't have the intermediate reasoning annotation for the training examples, only the label is provided."
  - [section 4] "GPT-4V with zero-shot prompting achieves the best overall performance, similar to the accuracy of the fine-tuned IDEFICS."
- Break condition: If the model still fails after adding intermediate reasoning steps in prompts, the bottleneck is likely visual feature extraction, not reasoning structure.

### Mechanism 3
- Claim: Image type affects compositional reasoning difficulty (Tower vs Scatter).
- Mechanism: Structured, regular layouts (Tower) are easier for the model to parse than irregular layouts (Scatter) because the former allows simpler spatial relation extraction.
- Core assumption: The model's visual encoder is better at processing regular grid-like structures than scattered, variable arrangements.
- Evidence anchors:
  - [section 4] "In the zero-shot setting, both GPT-4V and IDEFICS perform better on Tower, which is simpler and has a more structured visual representation."
  - [section 2] "There are two types of images: Tower...and Scatter, which contains scattered objects of different sizes..."
- Break condition: If accuracy equalizes across image types after fine-tuning, the visual encoder's inductive bias toward structure is the primary factor.

## Foundational Learning

- Concept: Compositional reasoning
  - Why needed here: NLVR requires combining multiple spatial relations (e.g., "small black triangle touching the wall") into a coherent truth evaluation.
  - Quick check question: Can the model correctly answer "There is a small red circle above a large blue square" given a synthetic image with those objects in correct spatial arrangement?

- Concept: Visual feature extraction for synthetic geometric shapes
  - Why needed here: Unlike natural images, NLVR uses simple shapes; models must reliably detect and distinguish shapes, sizes, and positions without semantic context.
  - Quick check question: Does the model correctly identify "a small black triangle" vs "a large black triangle" in a Tower image?

- Concept: Prompt engineering for reasoning tasks
  - Why needed here: The paper tests zero- and few-shot prompts; understanding how to structure instructions and chain-of-thought is critical for performance.
  - Quick check question: Does adding "Think step by step" to the prompt improve accuracy on a held-out NLVR sample?

## Architecture Onboarding

- Component map:
  - Text sentence → Text encoder → Parsed compositional clauses
  - Image → Visual encoder → Bounding boxes, shape, size, position
  - Parsed text relations + Visual features → Fusion module → Aligned spatial relations
  - Aligned relations → Classifier → True/False output

- Critical path:
  - Image → visual features → spatial relation extraction → alignment with text relations → truth evaluation

- Design tradeoffs:
  - High-resolution image input vs. model inference speed
  - Chain-of-thought prompt length vs. token limits
  - Fine-tuning data size vs. risk of overfitting on synthetic NLVR domain

- Failure signatures:
  - Consistently wrong on "only" or "exactly" clauses → text parsing issue
  - Fails on scattered objects but passes on towers → visual encoder bias
  - Degrades with few-shot examples → prompt conditioning problem

- First 3 experiments:
  1. Run zero-shot with chain-of-thought prompt on 10 NLVR samples; log intermediate reasoning steps.
  2. Compare accuracy on Tower vs Scatter subsets to confirm structural bias.
  3. Fine-tune IDEFICS on NLVR training set; evaluate Test-P accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to multimodal LLMs could improve their performance on compositional and spatial reasoning tasks like NLVR?
- Basis in paper: Explicit - The paper shows that current MLLMs struggle with NLVR despite their strong performance on other tasks, and mentions that specialized architectures (78.3% accuracy) outperform MLLMs (59.9% max).
- Why unresolved: The paper only evaluates existing models without exploring architectural modifications. It identifies the performance gap but doesn't investigate what architectural changes might bridge it.
- What evidence would resolve it: Systematic experiments comparing different architectural modifications (attention mechanisms, spatial reasoning modules, etc.) on NLVR task performance, showing clear improvements over baseline MLLMs.

### Open Question 2
- Question: How does the compositionality of visual elements in NLVR images affect MLLM performance compared to simpler image-text tasks?
- Basis in paper: Explicit - The paper notes that NLVR was specifically designed to require compositional and spatial reasoning, and that even small details can be crucial for correct answers.
- Why unresolved: While the paper shows poor performance on NLVR, it doesn't systematically analyze which types of compositional reasoning are most challenging for MLLMs or how this differs from their performance on less compositional tasks.
- What evidence would resolve it: Detailed error analysis breaking down MLLM failures by type of compositional reasoning required (spatial relations, object attributes, logical combinations) and comparison with performance on less compositional vision-language tasks.

### Open Question 3
- Question: Could fine-tuning strategies other than standard supervised fine-tuning improve MLLM performance on NLVR without extensive manual annotation?
- Basis in paper: Explicit - The paper mentions that fine-tuning IDEFICS improved performance from 55.9% to 59.7%, but this improvement was modest.
- Why unresolved: The paper only explores standard supervised fine-tuning with the full training set, not alternative fine-tuning strategies like few-shot learning, curriculum learning, or self-supervised pre-training specific to compositional reasoning.
- What evidence would resolve it: Experiments comparing various fine-tuning strategies on MLLMs for NLVR task, demonstrating which approaches yield the most significant performance improvements with minimal additional annotations.

### Open Question 4
- Question: How do semantic biases in MLLMs affect their performance on tasks like NLVR that were specifically designed to minimize such biases?
- Basis in paper: Explicit - The paper discusses that previous vision-language models often relied on semantic biases rather than robust visual processing, and that NLVR was constructed to be robust to such biases.
- Why unresolved: While the paper shows MLLMs struggle with NLVR, it doesn't systematically analyze whether and how semantic biases in the training data of MLLMs affect their performance on bias-resistant tasks.
- What evidence would resolve it: Analysis of MLLM predictions on NLVR showing whether errors correlate with semantic features that were meant to be irrelevant, and comparison with models trained on more balanced datasets.

## Limitations
- The study identifies performance gaps but doesn't definitively establish whether the bottleneck is in visual feature extraction, text parsing, or fusion/alignment
- The relatively small Test-P split (5,940 examples) limits statistical power for detecting performance differences between models
- The paper doesn't explore architectural modifications or alternative fine-tuning strategies that might improve performance

## Confidence
- High confidence: GPT-4V outperforms other models in zero-shot setting; all models significantly underperform human and previous specialized architectures
- Medium confidence: Compositional reasoning and spatial relations are the primary failure modes; structured layouts (Tower) are easier than scattered ones (Scatter)
- Low confidence: The specific mechanisms of failure (visual vs. reasoning bottleneck) and whether intermediate reasoning steps would meaningfully improve performance

## Next Checks
1. Conduct ablation studies by systematically removing spatial relation components from prompts to isolate whether text parsing or visual reasoning is the primary bottleneck
2. Evaluate models on intermediate NLVR validation splits to identify specific error patterns (e.g., failure on "only" clauses vs. spatial relations)
3. Test whether adding explicit spatial relation annotations in few-shot examples improves accuracy, directly validating the compositional reasoning hypothesis