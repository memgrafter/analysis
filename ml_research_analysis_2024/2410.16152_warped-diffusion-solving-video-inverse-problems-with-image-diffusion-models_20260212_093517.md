---
ver: rpa2
title: 'Warped Diffusion: Solving Video Inverse Problems with Image Diffusion Models'
arxiv_id: '2410.16152'
source_url: https://arxiv.org/abs/2410.16152
tags:
- diffusion
- noise
- video
- arxiv
- warping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a principled method for applying image diffusion
  models to video inverse problems such as inpainting and super-resolution. The key
  insight is that videos can be viewed as continuous warping transformations between
  frames, and that image models must be equivariant with respect to these transformations
  to maintain temporal consistency.
---

# Warped Diffusion: Solving Video Inverse Problems with Image Diffusion Models

## Quick Facts
- arXiv ID: 2410.16152
- Source URL: https://arxiv.org/abs/2410.16152
- Reference count: 40
- Primary result: Introduces a principled method for applying image diffusion models to video inverse problems while maintaining temporal consistency through warping equivariance

## Executive Summary
This paper presents a principled approach for extending image diffusion models to video inverse problems like inpainting and super-resolution. The key insight is that videos can be viewed as continuous warping transformations between frames, and image models must be equivariant with respect to these transformations to maintain temporal consistency. The authors introduce a function space diffusion framework that incorporates spatially correlated noise through Gaussian processes and use an equivariance self-guidance technique during sampling to enforce temporal consistency without additional training. Their method, Warped Diffusion, demonstrates near-zero warping error while maintaining competitive restoration performance on both synthetic and real videos.

## Method Summary
The authors propose a function space diffusion framework that treats video frames as functions on a 2D domain and uses optical flow to estimate the warping between frames. They introduce Gaussian process noise for spatially correlated perturbations and implement an equivariance self-guidance mechanism during sampling that uses the model's own predictions to maintain temporal consistency. The approach is built on top of latent diffusion models like Stable Diffusion XL and doesn't require retraining, making it practical for real-world applications. The method involves fine-tuning the base model with GP noise, estimating optical flow between frames, and applying the self-guidance during sampling to ensure consistent temporal evolution across frames.

## Key Results
- Achieves near-zero warping error while maintaining competitive restoration performance
- Outperforms previous approaches on both synthetic and real video datasets (COYO and FETV)
- Works with existing latent diffusion models like Stable Diffusion XL without requiring model retraining
- Demonstrates improved temporal consistency across various video inverse problems including inpainting and super-resolution

## Why This Works (Mechanism)
The method works by recognizing that videos are continuous warping transformations of frames, and that for temporal consistency, image diffusion models must be equivariant with respect to these transformations. The Gaussian process noise introduces spatially correlated perturbations that respect the underlying video structure, while the equivariance self-guidance mechanism ensures that the model's predictions remain consistent across time steps. By leveraging optical flow to estimate the warping between frames, the approach can maintain temporal coherence without requiring explicit video training data or architectural modifications to the underlying image diffusion model.

## Foundational Learning
**Function Space Diffusion**: Treats diffusion as operating on function spaces rather than pixel spaces - needed to reason about continuous transformations and spatial correlations; quick check: verify the noise covariance matrix is positive definite
**Gaussian Process Noise**: Uses spatially correlated noise instead of i.i.d. noise - needed to respect the underlying video structure and maintain consistency; quick check: confirm the GP kernel parameters produce reasonable correlation lengths
**Optical Flow Estimation**: Computes pixel-level motion between frames - needed to accurately model the warping transformations; quick check: verify flow consistency using forward-backward consistency checks
**Equivariance**: Ensures transformations commute with the model - needed for temporal consistency across frames; quick check: test that applying a warp then denoising equals denoising then applying the warp
**Latent Diffusion**: Operates in compressed latent space - needed for computational efficiency with large models; quick check: verify that reconstruction from latents preserves image quality

## Architecture Onboarding

Component map:
SDXL model -> Gaussian Process noise layer -> Optical flow estimator -> Warped Diffusion sampling loop with equivariance self-guidance

Critical path:
1. Optical flow estimation between consecutive frames
2. Warping of noise and predictions according to estimated flow
3. Equivariance self-guidance during sampling to enforce temporal consistency

Design tradeoffs:
- Using existing latent diffusion models provides accessibility but limits the ability to enforce equivariance at the architectural level
- GP noise provides spatial correlation but requires careful kernel parameter tuning
- Optical flow estimation adds computational overhead but is necessary for accurate warping
- Self-guidance during sampling avoids retraining but may have limitations compared to fully equivariant architectures

Failure signatures:
- Temporal inconsistency manifests as flickering or jittering between frames
- Poor optical flow estimation leads to incorrect warping and temporal artifacts
- Overly strong equivariance guidance can degrade spatial quality
- GP noise parameters that are too aggressive can destabilize the sampling process

First experiments:
1. Test equivariance property by applying synthetic warps to single frames and measuring consistency
2. Evaluate temporal consistency on simple translation/rotation sequences before moving to complex videos
3. Compare performance with and without equivariance guidance to isolate its contribution

## Open Questions the Paper Calls Out

Open Question 1: How does the choice of Gaussian process kernel parameters (particularly length scale) affect the quality and consistency of video generation across different types of spatial transformations? While the paper provides specific kernel parameters used in their experiments, they don't explore how different parameter choices might affect performance for different types of transformations or video content.

Open Question 2: Can the function space diffusion model framework be extended to handle 3D spatial transformations for video generation, and how would this affect computational complexity? The paper currently treats video frames as functions on a 2D domain and uses 2D spatial transformations. The function space framework could theoretically extend to higher dimensions.

Open Question 3: What is the theoretical limit of temporal consistency that can be achieved through inference-time equivariance guidance without retraining the underlying diffusion model? The paper demonstrates near-zero warping error through their equivariance self-guidance technique but doesn't explore the theoretical bounds of what can be achieved.

## Limitations
- Assumes video data follows continuous warping transformations, which may not hold for all video types with complex object interactions or non-rigid deformations
- Computational overhead from optical flow estimation during sampling could limit real-time applications
- Memory constraints from optical flow computation may limit scalability to long videos
- Performance may not generalize well to video domains outside the COYO and FETV datasets used in experiments

## Confidence
High confidence: The core theoretical framework connecting image diffusion models to video inverse problems through warping equivariance is well-established and mathematically sound.

Medium confidence: The empirical results demonstrating improved temporal consistency and competitive restoration performance are convincing, though limited to specific datasets and tasks.

Low confidence: The practical applicability to diverse real-world video scenarios and long-form video content remains uncertain.

## Next Checks
1. Test Warped Diffusion on videos with complex dynamics (e.g., sports, crowd scenes) to evaluate robustness beyond simple warping assumptions
2. Measure end-to-end inference time and memory usage for processing long videos (1+ minutes) to assess practical scalability
3. Compare performance against state-of-the-art video-specific diffusion models on benchmark datasets to contextualize the claimed improvements