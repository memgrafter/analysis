---
ver: rpa2
title: Vision-Language Models Create Cross-Modal Task Representations
arxiv_id: '2410.22330'
source_url: https://arxiv.org/abs/2410.22330
tags:
- task
- examples
- image
- text
- cross-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLMs compress tasks into shared task vectors that are invariant
  to modality (text/image) and format (examples/instructions), enabling cross-modal
  transfer via activation patching. The task vector, derived from a single compressed
  representation, significantly outperforms full few-shot prompting when applied across
  modalities.
---

# Vision-Language Models Create Cross-Modal Task Representations

## Quick Facts
- arXiv ID: 2410.22330
- Source URL: https://arxiv.org/abs/2410.22330
- Reference count: 40
- Primary result: VLMs compress tasks into modality-invariant task vectors enabling cross-modal transfer via activation patching

## Executive Summary
This paper introduces a method for extracting compressed task representations (task vectors) from vision-language models (VLMs) that enable cross-modal transfer between text and image inputs. The key insight is that VLMs can represent tasks in a modality-agnostic way, allowing task vectors derived from one modality to be applied to another. This approach significantly outperforms traditional few-shot prompting and demonstrates transfer between base LLMs and their fine-tuned VLM counterparts. The work also shows that task vectors can be defined from instructions alone, improving sample efficiency for new tasks.

## Method Summary
The authors develop a task vector extraction method that compresses task demonstrations (either text or image) into a single representation using activation patching. They apply this to VLMs by extracting task vectors from the middle layers of the model, where modality-specific representations are integrated into a shared task space. The method involves freezing the model parameters and transferring only the task vector between modalities. They evaluate this approach across 8 different tasks and two base LLM architectures (FlanT5 and Vicuna), comparing performance against full few-shot prompting and baseline methods.

## Key Results
- Task vectors derived from a single modality significantly outperform full few-shot prompting when applied across modalities
- Task vectors transfer between base LLMs and their fine-tuned VLM counterparts
- Instruction-only task vectors improve sample efficiency compared to demonstration-only approaches
- VLMs map text and image examples to similar task vectors in middle layers while maintaining distinct embeddings in context

## Why This Works (Mechanism)
VLMs integrate cross-modal information through a hierarchical representation process where lower layers maintain modality-specific embeddings while middle layers converge to shared task representations. This architectural property allows task vectors to capture the essential task-relevant information while discarding modality-specific details. The activation patching mechanism works because VLMs have learned to separate task knowledge from input modality during training, creating a natural interface for cross-modal transfer.

## Foundational Learning
- **Activation patching**: A technique for transferring model states between different inputs or conditions; needed for isolating and transferring task representations without fine-tuning entire models; quick check: verify that patching only the task vector produces the desired effect
- **Cross-modal transfer**: Moving knowledge between different input modalities; required for applying text-derived task knowledge to visual inputs; quick check: measure performance drop when applying task vectors across modalities
- **Task vector compression**: Reducing task demonstrations to a single vector representation; essential for efficient knowledge transfer; quick check: verify compressed vectors retain task-relevant information
- **Modality invariance**: The property of task representations being independent of input format; critical for cross-modal applicability; quick check: compare task vectors across different modalities
- **Middle layer integration**: The point in model architecture where cross-modal information converges; key to where task vectors should be extracted; quick check: analyze representation similarity across layers
- **Zero-shot transfer**: Applying learned representations without task-specific training; demonstrates the generality of task vectors; quick check: evaluate performance on held-out tasks

## Architecture Onboarding
- **Component map**: Input (text/image) -> VLM layers (modality-specific) -> Middle layers (task integration) -> Task vector extraction -> Activation patching -> Output
- **Critical path**: Task vector extraction from middle layers → Activation patching → Cross-modal application → Performance evaluation
- **Design tradeoffs**: Single compressed representation vs. multiple vectors; frozen model parameters vs. fine-tuning; instruction-only vs. demonstration-based vectors
- **Failure signatures**: Performance degradation when task vectors extracted from wrong layers; modality-specific information leakage in task vectors; inability to transfer between base LLM and VLM
- **3 first experiments**: 1) Extract task vectors from different VLM layers to find optimal extraction point; 2) Apply task vectors across all 8 tasks to verify cross-modal transfer; 3) Compare instruction-only vs. demonstration-based task vectors on sample efficiency

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The study only uses GPT4V as the VLM model, raising questions about generalizability to other vision-language models
- Analysis is limited to 8 tasks and two base LLM architectures, constraining the breadth of evaluation
- The cross-modal alignment claims lack validation through real-world applications or semantic grounding analysis
- Activation patching has not been tested in end-to-end fine-tuning scenarios

## Confidence
- **High confidence**: VLMs produce task representations that are modality-invariant and transferable between base LLMs and their fine-tuned VLM counterparts
- **Medium confidence**: Task vectors extracted from a single modality can successfully transfer to the other modality for zero-shot inference
- **Low confidence**: Cross-modal alignment achieved through task vectors represents true semantic grounding or can be effectively applied in end-to-end fine-tuning scenarios

## Next Checks
1. Test task vector transfer across multiple VLMs (CLIP, Flamingo, LLaVA) to verify findings are not model-specific
2. Evaluate task vectors in a supervised fine-tuning setting where they are used to initialize or guide model training
3. Conduct a semantic grounding analysis to determine whether cross-modal task alignment corresponds to meaningful visual-textual concept mapping rather than superficial representation similarity