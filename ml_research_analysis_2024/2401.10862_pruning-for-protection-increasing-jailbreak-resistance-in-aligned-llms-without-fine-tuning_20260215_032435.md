---
ver: rpa2
title: 'Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without
  Fine-Tuning'
arxiv_id: '2401.10862'
source_url: https://arxiv.org/abs/2401.10862
tags:
- pruned
- pruning
- jailbreaking
- unpruned
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how model compression affects Large Language
  Model (LLM) safety, specifically resistance to jailbreak attacks. The authors prune
  LLaMA-2 Chat, Vicuna 1.3, and Mistral Instruct v0.2 to 10%, 20%, and 30% sparsity
  using Wanda pruning without fine-tuning.
---

# Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning

## Quick Facts
- **arXiv ID**: 2401.10862
- **Source URL**: https://arxiv.org/abs/2401.10862
- **Reference count**: 40
- **Primary result**: Moderate pruning (10-20%) improves jailbreak resistance in aligned LLMs without fine-tuning

## Executive Summary
This study investigates how model compression through pruning affects Large Language Model safety, specifically resistance to jailbreak attacks. The authors prune LLaMA-2 Chat, Vicuna 1.3, and Mistral Instruct v0.2 to 10%, 20%, and 30% sparsity using Wanda pruning without fine-tuning. Results show that moderate pruning (10-20%) improves jailbreak resistance by 10-20 percentage points across most malicious categories, with LLaMA-2 Chat showing the most improvement due to its stronger initial safety alignment. Attention analysis reveals pruned models concentrate more on task-relevant tokens, suggesting sharper attention patterns enhance safety detection. The pruned models maintained competitive performance on standard benchmarks including ARC, MMLU, and GSM8K.

## Method Summary
The authors applied Wanda pruning to three 7B parameter models (LLaMA-2 Chat, Vicuna 1.3, and Mistral Instruct v0.2) at 10%, 20%, and 30% sparsity levels. They evaluated jailbreak resistance using a curated dataset of 2,250 prompts across five malicious categories and 10 jailbreak types. Safety improvements were measured by refusal rates, while attention patterns were analyzed using IntraTaskRank and EndTaskRank metrics. Standard benchmarks (ARC, MMLU, GSM8K, TruthfulQA, Winogrande, HellaSwag, AltQA, and WikiText perplexity) were used to ensure capability retention after pruning.

## Key Results
- Moderate pruning (10-20%) increased jailbreak refusal rates by 10-20 percentage points across most malicious categories
- LLaMA-2 Chat showed the highest safety improvement due to stronger initial alignment training
- Excessive pruning (30%) degraded safety and increased perplexity from 6.94 to 7.33
- Pruned models exhibited sharper attention patterns, concentrating more on task-relevant tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Moderate pruning (10-20%) enhances jailbreak resistance by sharpening attention patterns on task-relevant tokens.
- Mechanism: Pruning reduces parameter redundancy, causing the model to focus more attention on tokens related to the malicious task while reducing focus on jailbreak pretext tokens.
- Core assumption: Attention concentration on task tokens improves detection of malicious intent.
- Evidence anchors:
  - [abstract] "pruning aids LLMs in concentrating attention on task-relevant tokens"
  - [section] "pruned models exhibit sharper attention and increased sensitivity to artificial jailbreak constructs"
- Break condition: If pruning removes critical parameters for safety detection, or if attention patterns become too sparse to capture malicious intent.

### Mechanism 2
- Claim: Pruning amplifies the effects of initial safety alignment training.
- Mechanism: Models with stronger initial safety training (like LLaMA-2 Chat) show greater improvement after pruning because pruning reinforces existing safety patterns.
- Core assumption: Pruning doesn't degrade safety-aligned parameters but rather enhances their relative influence.
- Evidence anchors:
  - [abstract] "pruning benefits correlate with initial model safety levels"
  - [section] "LLaMA-2 Chat, the safest model initially, showed the highest safety improvement after pruning"
- Break condition: If pruning disproportionately affects safety-aligned parameters or if the initial safety training is too weak to be amplified.

### Mechanism 3
- Claim: Attention pattern changes explain jailbreak resistance improvements.
- Mechanism: Pruned models show higher IntraTaskRank (attention between task tokens) and lower attention to jailbreak pretext tokens, making them less susceptible to jailbreak prompts.
- Core assumption: The model's ability to distinguish between task and pretext tokens determines jailbreak success.
- Evidence anchors:
  - [section] "IntraTaskRank and EndTaskRank metrics" and "pruned models consistently process tokens from the original malicious task paying less attention to the jailbreak pretext"
- Break condition: If jailbreak prompts evolve to embed tasks more deeply within pretext, or if attention metrics don't generalize to new attack types.

## Foundational Learning

- Concept: Large Language Model Safety Alignment
  - Why needed here: Understanding how models are trained to refuse harmful content is crucial for interpreting why pruning affects safety.
  - Quick check question: What is the primary method used to align LLMs with human values in this study?
  - Answer: Reinforcement Learning with Human Feedback (RLHF)

- Concept: Model Pruning and Compression
  - Why needed here: The study uses pruning as the primary compression technique, so understanding how it works is essential.
  - Quick check question: What pruning method is used in this study, and what percentage of parameters are pruned?
  - Answer: Wanda pruning to 10%, 20%, and 30% sparsity

- Concept: Jailbreak Attack Types
  - Why needed here: The study evaluates resistance to different jailbreak methods, so understanding these attack types is important.
  - Quick check question: What are the three types of jailbreak attacks considered in this study?
  - Answer: Role-playing, Attention-shifting, and Privileged execution

## Architecture Onboarding

- Component map: LLaMA-2 Chat, Vicuna 1.3, Mistral Instruct v0.2 (7B parameters each) with transformer decoder blocks containing attention mechanisms and feed-forward networks
- Critical path: Pruning → Safety evaluation → Attention analysis → Benchmark testing
- Design tradeoffs: Moderate pruning (10-20%) improves safety but excessive pruning (30%) degrades it; attention concentration helps safety but may affect other capabilities
- Failure signatures: Decreased jailbreak resistance at high sparsity levels, inconsistent safety improvements across models
- First 3 experiments:
  1. Apply Wanda pruning at 10% sparsity to LLaMA-2 Chat and measure jailbreak resistance on the curated dataset
  2. Compare attention patterns between pruned and unpruned models using IntraTaskRank and EndTaskRank metrics
  3. Benchmark pruned models on standard tasks (ARC, MMLU, GSM8K) to ensure capability retention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the attention concentration effect from pruning generalize to other safety domains beyond jailbreak resistance, such as preventing harmful content generation or reducing hallucinations?
- Basis in paper: Explicit - The authors speculate that attention concentration aids models in detecting malicious tasks and state "these observations suggest that moderate parameter pruning can potentially enhance the desired behaviors in LLMs under threat models."
- Why unresolved: The study focuses specifically on jailbreak resistance. The authors note this is an open question about whether pruning effects extend to other safety behaviors, but do not test this hypothesis.
- What evidence would resolve it: Experiments testing pruning effects on other safety metrics (e.g., hallucination rates, harmful content generation) across multiple pruning levels and model architectures.

### Open Question 2
- Question: What is the optimal pruning level for balancing safety improvements with performance preservation across different model architectures and sizes?
- Basis in paper: Explicit - The authors find that 20% pruning provides optimal jailbreak resistance for their tested models, but note that "too much will negatively affect the alignment training" and suggest this needs further investigation.
- Why unresolved: The study only tests 7B parameter models with Wanda pruning. The authors acknowledge this is a limited scope and suggest exploring larger models and different compression techniques.
- What evidence would resolve it: Systematic evaluation of pruning effects across multiple model sizes (from 1B to 70B parameters) and architectures using various pruning methods, measuring both safety metrics and standard benchmark performance.

### Open Question 3
- Question: How does the location of pruned parameters (attention vs MLP layers) affect safety improvements, and why does full pruning outperform MLP-only pruning?
- Basis in paper: Explicit - The authors compare full pruning vs MLP-only pruning and find "the fully pruned model is more resistant to jailbreaking prompts," but do not explain why this occurs.
- Why unresolved: The study observes this difference but does not investigate the mechanism or explore different pruning strategies for different layer types.
- What evidence would resolve it: Ablation studies testing different pruning ratios for attention vs MLP layers, combined with attention pattern analysis to understand how parameter location affects safety-relevant attention mechanisms.

## Limitations

- Relatively small scale of base models (7B parameters) may limit generalizability to larger models
- Absence of fine-tuning after pruning means safety improvements may not be optimal
- Attention analysis relies on indirect metrics that haven't been validated across diverse jailbreak strategies

## Confidence

- **High confidence**: Moderate pruning (10-20%) improves jailbreak resistance is well-supported by empirical data across multiple models and attack types
- **Medium confidence**: The mechanism explaining improved safety through attention concentration on task-relevant tokens is plausible but relies on indirect evidence
- **Medium confidence**: The claim that excessive pruning (30%) degrades safety is supported by the data, though the specific threshold may vary

## Next Checks

1. Test the pruning approach on larger models (70B+ parameters) to assess scalability and whether the 10-20% optimal range holds for more complex architectures
2. Conduct fine-tuning experiments on the pruned models to determine if safety gains can be maintained or enhanced while recovering any lost capabilities
3. Evaluate the pruned models against emerging jailbreak strategies, particularly those that embed malicious tasks more deeply within pretext, to test the robustness of attention-based safety mechanisms