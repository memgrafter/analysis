---
ver: rpa2
title: 'Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved
  Contexts When Knowledge Conflicts?'
arxiv_id: '2401.11911'
source_url: https://arxiv.org/abs/2401.11911
tags:
- contexts
- llms
- generated
- context
- retrieved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how large language models (LLMs) merge\
  \ generated and retrieved contexts, especially when they conflict. The researchers\
  \ developed a controlled framework to create datasets where questions are paired\
  \ with conflicting contexts\u2014one containing the correct answer and the other\
  \ incorrect."
---

# Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?

## Quick Facts
- arXiv ID: 2401.11911
- Source URL: https://arxiv.org/abs/2401.11911
- Reference count: 40
- Primary result: LLMs show significant bias toward favoring generated contexts over retrieved ones, even when generated contexts contain incorrect information.

## Executive Summary
This study investigates how large language models (LLMs) merge generated and retrieved contexts when they conflict, revealing a systematic bias toward generated contexts regardless of their accuracy. The researchers developed a controlled framework where questions are paired with conflicting contexts - one containing the correct answer and one incorrect. Through experiments across multiple LLMs including GPT-4/3.5 and Llama2, they found that generated contexts are preferred even when wrong, due to higher textual similarity to questions and incomplete semantic structure of retrieved contexts. The findings highlight critical challenges in LLM context integration and suggest the need for improved methods to balance information from different sources.

## Method Summary
The researchers constructed context-conflicting datasets by pairing questions with both generated and retrieved contexts, ensuring only one context type contained the correct answer. They used NQ and TQA datasets, generating contexts via GPT-4, GPT-3.5, and Llama2 models, while retrieving contexts from Wikipedia using Contriever. The evaluation employed Exact Match (EM) scores to measure answer accuracy and a novel DiffGR metric to quantify bias toward generated versus retrieved contexts. LLMs were tested in a zero-shot setting using hybrid approaches that combined both context types.

## Key Results
- LLMs consistently prefer generated contexts over retrieved ones, even when generated contexts contain incorrect information
- Generated contexts show higher textual similarity to questions, increasing their selection likelihood
- Retrieved contexts suffer from semantic incompleteness due to segmentation, reducing their utility
- The bias persists across different LLM families and remains even when generated contexts contradict the model's parametric memory

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generated contexts are selected more often because they are textually more similar to questions than retrieved contexts.
- **Mechanism:** The model uses textual similarity as a proxy for relevance, and generated contexts are produced by LLMs that inherently optimize for this similarity.
- **Core assumption:** Higher similarity between context and question correlates with higher likelihood of selection.
- **Evidence anchors:**
  - [abstract] "contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection"
  - [section] "generated contexts typically exhibit a higher degree of similarity to questions even on AIR subsets, where generated contexts contain incorrect information"
  - [corpus] Weak evidence - corpus only shows related work on context pruning but no direct evidence about similarity selection bias
- **Break condition:** If similarity is artificially controlled to be equal between generated and retrieved contexts, the selection bias should diminish.

### Mechanism 2
- **Claim:** Retrieved contexts are semantically incomplete due to segmentation, reducing their utility in LLMs.
- **Mechanism:** Retrieved contexts are often truncated to fit fixed-length passages, breaking semantic coherence and sentence completeness.
- **Core assumption:** Semantic completeness is necessary for effective context utilization by LLMs.
- **Evidence anchors:**
  - [abstract] "the segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs"
  - [section] "current retrieval systems typically employ fixed-length truncation to divide a complete article into multiple passages... This truncation often results in retrieved contexts with incomplete semantic meaning"
  - [corpus] Weak evidence - corpus shows related work on context pruning but doesn't directly test completeness effects
- **Break condition:** If retrieved contexts are presented with restored semantic completeness, their selection likelihood should increase.

### Mechanism 3
- **Claim:** LLMs prefer self-generated contexts even when generated by other LLMs, indicating a broader bias beyond confirmation bias.
- **Mechanism:** The preference for generated contexts persists even when generated by different LLMs, suggesting the bias is structural rather than based on knowledge alignment.
- **Core assumption:** The bias exists independently of the specific LLM generating the context.
- **Evidence anchors:**
  - [abstract] "regardless of whether the generated text was produced internally or by other LLMs, which persists even when the generated contexts offer incorrect information"
  - [section] "LLMs maintain a high DiffGR when the generated contexts are replaced with counter-memory contexts... LLMs retain a preference for generated contexts, even when these contexts contain information that contradicts the LLMs' parametric memory"
  - [corpus] Weak evidence - corpus shows related work on knowledge conflicts but doesn't directly test cross-LLM generation preferences
- **Break condition:** If the generator and reader are from entirely different model families with no shared training data, the bias should still persist.

## Foundational Learning

- **Concept:** Exact Match (EM) metric
  - Why needed here: Used to evaluate whether generated/retrieved contexts actually contain the correct answer
  - Quick check question: What threshold would you use to determine if a context "contains" an answer?

- **Concept:** BERTScore for semantic similarity
  - Why needed here: Used to measure semantic similarity between contexts and questions beyond simple lexical overlap
  - Quick check question: How would BERTScore differ from Jaccard similarity in this context?

- **Concept:** Controlled experimental design with conflicting contexts
  - Why needed here: The core methodology for isolating whether LLMs prefer generated vs retrieved contexts
  - Quick check question: What would happen if both contexts contained the correct answer?

## Architecture Onboarding

- **Component map:** Question → Generator (LLM) → Retrieved context (retrieval system) → Reader (LLM) → Answer
- **Critical path:** Context generation/retrieval → Context selection → Answer generation
- **Design tradeoffs:** Length control vs. semantic completeness; similarity optimization vs. factual accuracy
- **Failure signatures:** High DiffGR values on AIR subsets; low EM scores when correct answer is in retrieved context
- **First 3 experiments:**
  1. Replicate the DiffGR calculation on NQ-AIR subset with random context ordering
  2. Test semantic completeness by comparing truncated vs complete generated contexts
  3. Measure similarity gap impact by creating synthetic contexts with controlled similarity differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise mechanisms by which LLMs merge generated and retrieved contexts, especially when they conflict?
- Basis in paper: Explicit
- Why unresolved: The paper identifies a bias towards generated contexts but does not fully elucidate the internal decision-making processes of LLMs when integrating diverse information sources.
- What evidence would resolve it: Further experiments using techniques like ablation studies, attention analysis, or controlled interventions to isolate the contribution of different factors (e.g., text similarity, semantic completeness) to the merging process.

### Open Question 2
- Question: How does the bias towards generated contexts generalize across different types of knowledge and question domains?
- Basis in paper: Inferred
- Why unresolved: The study focuses on open-domain QA using NQ and TQA datasets. It remains unclear if the observed bias persists in other domains like reasoning, creative writing, or factual verification.
- What evidence would resolve it: Replicating the experiments on diverse datasets spanning various knowledge domains and question types to assess the generalizability of the bias.

### Open Question 3
- Question: What are the potential negative consequences of the bias towards generated contexts in real-world applications of LLMs?
- Basis in paper: Explicit
- Why unresolved: The paper highlights the risk of generated misinformation but does not extensively explore the downstream impacts of this bias on user trust, decision-making, or societal implications.
- What evidence would resolve it: Case studies or user studies examining the effects of LLM-generated misinformation in specific application contexts, such as education, healthcare, or news generation.

## Limitations
- The study relies on specific datasets (NQ, TQA) which may not generalize to other domains
- The filtering process for ensuring traceability may introduce selection bias
- The Contriever retrieval system's specific characteristics may influence results

## Confidence
- High: The observed DiffGR bias toward generated contexts is reproducible and statistically significant
- Medium: The textual similarity mechanism as primary driver (requires controlled similarity experiments)
- Medium: The segmentation incompleteness as a major factor (needs semantic completeness restoration tests)
- Low: The cross-LLM generation preference mechanism (lacks direct cross-family model testing)

## Next Checks
1. **Controlled Similarity Test:** Create synthetic contexts with controlled similarity levels to definitively prove/disprove the similarity selection mechanism
2. **Semantic Completeness Restoration:** Experiment with providing retrieved contexts with restored semantic completeness (e.g., context expansion) to measure impact on selection bias
3. **Cross-Family Model Test:** Test the generation preference bias using LLMs from entirely different families with no shared training data to isolate structural vs. parametric factors