---
ver: rpa2
title: 'LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark
  for Chinese Large Language Models'
arxiv_id: '2403.12601'
source_url: https://arxiv.org/abs/2403.12601
tags:
- llms
- questions
- chinese
- lhmke
- school
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LHMKE, a comprehensive benchmark for evaluating
  Chinese large language models (LLMs) across a wide range of subjects and educational
  levels. LHMKE covers 30 subjects, 75 tasks, and 10,465 questions, including both
  objective and subjective question types.
---

# LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models

## Quick Facts
- arXiv ID: 2403.12601
- Source URL: https://arxiv.org/abs/2403.12601
- Authors: Chuang Liu; Renren Jin; Yuqi Ren; Deyi Xiong
- Reference count: 0
- Primary result: LHMKE is a comprehensive Chinese LLM benchmark covering 30 subjects, 75 tasks, and 10,465 questions with both objective and subjective types, revealing current models' struggles particularly on professional certification exams.

## Executive Summary
LHMKE is a novel benchmark designed to comprehensively evaluate Chinese large language models across a wide spectrum of educational levels and subjects. Unlike existing Chinese benchmarks that primarily focus on multiple-choice questions, LHMKE includes both objective and subjective question types, providing a more holistic assessment of LLM knowledge capabilities. The benchmark covers 30 subjects from primary school through professional certification exams, totaling 10,465 questions across 75 tasks.

The authors evaluated 11 Chinese LLMs on LHMKE and found that current models struggle to achieve high scores, particularly in professional certification exams. They also explored using GPT-4 with carefully designed prompts as an automatic evaluator for subjective questions, finding it to be the most effective method compared to traditional metrics and multi-agent debates. This approach addresses the scalability challenge of manually grading subjective responses while maintaining reasonable alignment with human evaluation.

## Method Summary
The LHMKE benchmark was constructed by collecting questions from public test databases across 30 subjects spanning primary school, secondary school, college, and career development levels. Questions were categorized as either objective (multiple-choice) or subjective (requiring written responses). The benchmark was evaluated using zero-shot inference on 11 Chinese LLMs, with GPT-4 serving as an automatic evaluator for subjective questions through detailed prompt engineering. Human evaluators established baseline scores for subjective questions, which were then compared against GPT-4's automated scoring to assess consistency and reliability.

## Key Results
- Current Chinese LLMs struggle on LHMKE, with performance particularly weak on professional certification exams
- GPT-4 with detailed prompts provides the most effective automatic scoring for subjective questions, outperforming traditional metrics and multi-agent debates
- LLMs perform better on elementary and secondary school exams compared to college and professional certification exams
- The benchmark reveals significant performance gaps between objective and subjective question types

## Why This Works (Mechanism)

### Mechanism 1
Including both objective and subjective question types in a benchmark better reveals the full knowledge acquisition and application capabilities of LLMs. Objective questions test recognition and recall via selection, while subjective questions require generation, explanation, and reasoning, exposing deeper understanding. The core assumption is that LLM performance varies systematically across question formats; subjective questions expose limitations hidden by objective-only benchmarks. Evidence shows that LHMKE's inclusion of subjective questions offers a more holistic evaluation of LLM knowledge levels compared to benchmarks focusing solely on multiple-choice questions.

### Mechanism 2
Using GPT-4 with carefully designed prompts as an automatic evaluator for subjective questions provides scoring consistency comparable to human raters. GPT-4 is instructed to mimic human reviewer roles with explicit scoring rubrics, reducing subjective variance and enabling scalable evaluation. The core assumption is that GPT-4 can accurately interpret reference answers and align its scoring with human judgment when given structured prompts. Experimental results show that when given detailed scoring prompts, GPT-4's scores are closer to human evaluators than other methods, with average scores of 5.0 compared to 5.92 for alternative approaches.

### Mechanism 3
Evaluating LLMs across a broad educational spectrum (primary to professional certification) exposes performance gaps that subject-specific or level-specific benchmarks miss. Multi-level tasks require adaptation across knowledge complexity and format, revealing strengths and weaknesses in different domains. The core assumption is that LLM performance correlates with educational level and domain specialization; breadth of subjects amplifies discriminative power. Results demonstrate that LLMs encounter significant difficulties with professional domain knowledge, showing markedly subpar performance in career development groups compared to elementary and secondary school exams.

## Foundational Learning

- **Zero-shot evaluation setting**: Ensures assessment reflects real-world usage without task-specific fine-tuning, making results generalizable. Quick check: Does the LLM produce responses without any task-specific training examples?

- **Automatic subjective scoring with LLMs**: Manual grading is infeasible at scale; automated scoring must maintain human-level consistency. Quick check: Does the scoring rubric clearly define point allocation for correct, partial, and incorrect answers?

- **Benchmark standardization via uniform scoring**: Different exams have different total scores; normalization enables cross-subject and cross-level comparison. Quick check: Are all subjects mapped to a common score range before aggregation?

## Architecture Onboarding

- **Component map**: Dataset collection → Task categorization (primary, secondary, college, career) → Question annotation (objective/subjective) → Reference answer generation → LLM evaluation pipeline → Automatic scoring (GPT-4) → Result aggregation → Comparative analysis
- **Critical path**: Data collection → Annotation → Prompt engineering → Evaluation execution → Result analysis
- **Design tradeoffs**: Broader subject coverage increases benchmark robustness but raises data collection and annotation costs; including subjective questions improves depth but complicates automated evaluation
- **Failure signatures**: Low inter-annotator agreement → inconsistent human scoring; high variance in GPT-4 scores → prompt inadequacy; performance clustering → lack of discriminative power
- **First 3 experiments**:
  1. Run a subset of objective questions across all LLMs to establish baseline accuracy
  2. Evaluate a small sample of subjective questions using both human and GPT-4 scoring to compare alignment
  3. Test scaling by running full LHMKE on one representative LLM to identify bottlenecks in processing and scoring

## Open Questions the Paper Calls Out

### Open Question 1
How well do current Chinese LLMs perform on subjective questions compared to objective questions in LHMKE? While the paper compares performance between objective and subjective questions, it does not provide a detailed breakdown of how well LLMs perform specifically on subjective questions. This requires a detailed analysis showing LLM performance scores specifically for subjective questions in LHMKE, broken down by question type and subject.

### Open Question 2
What are the main factors contributing to the performance gap between Chinese LLMs on elementary/secondary school exams versus professional certification exams in LHMKE? The paper identifies a performance gap but does not delve into the specific factors causing this discrepancy. This requires an analysis identifying key differences in question complexity, subject matter, and required knowledge depth between elementary/secondary school and professional certification exams.

### Open Question 3
How effective is GPT-4 as an automatic evaluator for subjective questions compared to human evaluators in LHMKE? While the paper compares GPT-4 to other evaluation methods, it does not provide a direct comparison with human evaluators' scores. This requires a detailed comparison of GPT-4's scores with human evaluators' scores on the same set of subjective questions, including correlation coefficients and average score differences.

## Limitations

- The exact prompts and instructions used for GPT-4 evaluation are partially described but not fully detailed in the paper, requiring reconstruction from supplementary materials
- The specific versions and sources of all 11 Chinese LLMs evaluated are not fully specified, making exact replication challenging
- Performance differences between Chinese LLMs may be influenced by their instruction-tuning approaches (SFT vs RLHF) and specific training data, which are not comprehensively characterized

## Confidence

- **High Confidence**: The benchmark design covering 30 subjects across multiple educational levels is clearly specified and reproducible
- **Medium Confidence**: The zero-shot evaluation methodology and baseline performance metrics are well-documented, though exact model versions and configurations may vary
- **Medium Confidence**: The GPT-4 automatic scoring approach shows promise but requires careful prompt engineering and validation against human benchmarks

## Next Checks

1. Reconstruct and test the GPT-4 scoring prompts on a subset of subjective questions to verify alignment with reported human evaluation scores
2. Run the complete LHMKE benchmark on at least two different Chinese LLMs to verify the performance trends and identify any implementation dependencies
3. Compare the performance gap between objective and subjective question types across multiple models to validate the benchmark's discriminative power