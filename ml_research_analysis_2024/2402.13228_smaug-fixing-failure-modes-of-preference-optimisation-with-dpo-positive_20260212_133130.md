---
ver: rpa2
title: 'Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive'
arxiv_id: '2402.13228'
source_url: https://arxiv.org/abs/2402.13228
tags:
- dpop
- preferred
- loss
- datasets
- completions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a failure mode in Direct Preference Optimisation
  (DPO) where fine-tuning can reduce the probability of preferred completions, especially
  in datasets with low edit distances between preference pairs. The authors propose
  DPO-Positive (DPOP), a new loss function that adds a term to incentivise maintaining
  high log-likelihood of preferred completions, addressing this issue.
---

# Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive

## Quick Facts
- **arXiv ID**: 2402.13228
- **Source URL**: https://arxiv.org/abs/2402.13228
- **Reference count**: 40
- **Primary result**: DPOP fixes a failure mode in DPO where preferred completions' probability can decrease, creating Smaug-72B which surpasses 80% accuracy on HuggingFace Open LLM Leaderboard

## Executive Summary
This paper identifies a critical failure mode in Direct Preference Optimisation (DPO) where the fine-tuning process can actually reduce the probability of preferred completions, particularly when the edit distance between preference pairs is low. The authors propose DPO-Positive (DPOP), a new loss function that adds a term to maintain high log-likelihood of preferred completions while preserving the relative preference structure. Using DPOP, they create Smaug-34B and Smaug-72B models that achieve state-of-the-art performance on the HuggingFace Open LLM Leaderboard, with Smaug-72B becoming the first open-source LLM to surpass 80% average accuracy.

## Method Summary
The paper introduces DPOP, a modified version of DPO that addresses the failure mode where DPO can reduce the probability of preferred completions. DPOP adds a regularization term to the standard DPO loss that penalizes reductions in the log-probability of preferred completions below their reference model values. The method involves fine-tuning large language models using preference datasets with paired completions (preferred and dispreferred) and evaluating on multiple benchmarks including GSM8K, ARC, HellaSwag, and the HuggingFace Open LLM Leaderboard.

## Key Results
- DPOP outperforms DPO across multiple datasets including MetaMath, ARC, and HellaSwag
- Smaug-72B achieves an average accuracy of 80% on the HuggingFace Open LLM Leaderboard, improving by nearly 2% over the second-best open-source model
- DPOP shows superior performance even on datasets with high edit distances between preference pairs
- The method is effective across different model sizes from 7B to 72B parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DPO loss can reduce log-likelihood of preferred completions if relative probability increases
- **Mechanism**: The DPO loss minimizes based on difference in log-ratios, allowing both log-ratios to decrease as long as their difference increases. With low edit distance, tokens after the differing token receive gradients that decrease their log-probabilities
- **Core assumption**: Reference model is reasonably optimized with well-calibrated logits
- **Evidence anchors**: Abstract statement about reduction of preferred examples' likelihood; section 3 showing gradients decrease correct token logits and increase incorrect token logits
- **Break condition**: When edit distance is large enough that gradients from differing tokens don't significantly affect subsequent tokens

### Mechanism 2
- **Claim**: DPOP adds a penalty term to maintain high log-likelihood of preferred completions
- **Mechanism**: DPOP adds max(0, log πref(yw|x)/πθ(yw|x)) to the loss, which is zero when πratio(yw|x) ≥ 1 and increases as the ratio goes below 1, incentivizing maintenance of high probability for preferred completions
- **Core assumption**: Reference model's probability of preferred completions is a reasonable baseline to maintain
- **Evidence anchors**: Abstract claim that DPOP avoids the failure mode; section 4 describing the new term that penalizes reducing probability of positive completions
- **Break condition**: When λ is set too low to effectively penalize reductions in preferred completion likelihood

### Mechanism 3
- **Claim**: DPOP ensures positive gradients for correct tokens and negative gradients for incorrect tokens
- **Mechanism**: When πratio < 1, DPOP's gradients become λ(1 - s{y<k w,x} j) + s{y<k l,x} j - s{y<k w,x} j for correct tokens and -(λ + 1)s{y<k w,x} j + s{y<k l,x} j for incorrect tokens. Large enough λ guarantees positive gradients for correct tokens and negative for incorrect tokens
- **Core assumption**: Vocabulary index i of token tk is correctly identified
- **Evidence anchors**: Section 4's gradient analysis showing guaranteed positive/negative gradients for large λ; abstract claim about DPOP outperforming DPO across datasets
- **Break condition**: When λ is not sufficiently large to overcome base DPO gradients

## Foundational Learning

- **Concept**: Bradley-Terry model for pairwise preference ranking
  - **Why needed here**: DPO and DPOP are based on modeling relative probability of preferred vs dispreferred completions using this model
  - **Quick check question**: What does the Bradley-Terry model assume about the probability of preferring yw over yl given context x?

- **Concept**: Edit distance (Hamming distance) between sequences
  - **Why needed here**: The failure mode of DPO occurs specifically when edit distance between preferred and dispreferred completions is low
  - **Quick check question**: How does the location of differing tokens affect gradient calculations in DPO?

- **Concept**: Contrastive learning loss functions
  - **Why needed here**: DPOP can be viewed as similar to contrastive loss with margin, which helps understand its theoretical motivation
  - **Quick check question**: What are the two main terms in contrastive loss formulations and how does DPOP relate to them?

## Architecture Onboarding

- **Component map**: Pairwise preference data -> Model initialization -> Preference optimization (DPO/DPOP) -> Evaluation on downstream tasks
- **Critical path**: Data preprocessing → Model initialization → Preference optimization (DPO/DPOP) → Evaluation on downstream tasks
- **Design tradeoffs**: DPO is simpler but can fail on low edit distance datasets; DPOP adds complexity with additional hyperparameter λ but mitigates failure modes; higher λ values provide stronger regularization but may slow convergence
- **Failure signatures**: DPO: Decreasing log-probabilities of preferred completions during training, especially for tokens after differing positions; DPOP: Stable or increasing log-probabilities of preferred completions throughout training
- **First 3 experiments**: 1) Run DPO on MetaMath dataset and monitor log-probabilities of preferred completions - expect degradation; 2) Run DPOP on same dataset with λ=50 - expect stable log-probabilities; 3) Compare final model performance on GSM8K test set - expect DPOP to outperform DPO

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the choice of lambda (λ) in DPOP affect performance across different model sizes and datasets?
  - **Basis in paper**: [explicit] The paper mentions an ablation study on the value of λ, testing λ ∈ {5, 50, 500} and finding that performance is relatively unaffected by the choice of λ on MetaMath and ARC datasets
  - **Why unresolved**: The ablation study was limited to smaller models (7B) and specific datasets (MetaMath and ARC). The paper did not conduct a full ablation study on larger models (34B and 72B) due to computational constraints
  - **What evidence would resolve it**: Conducting a comprehensive ablation study on larger models (34B and 72B) across a wider variety of datasets with different edit distances between preference pairs would provide insights into the sensitivity of DPOP's performance to the choice of λ

- **Open Question 2**: Can DPOP be effectively applied to non-English datasets, and how does its performance compare to English datasets?
  - **Basis in paper**: [inferred] The paper mentions that the effectiveness of DPOP on non-English datasets is an exciting area for future work, but the current study only demonstrated its performance on six English-language datasets
  - **Why unresolved**: The paper did not include any non-English datasets in its experiments, leaving the question of DPOP's effectiveness on non-English data unanswered
  - **What evidence would resolve it**: Applying DPOP to a diverse set of non-English datasets and comparing its performance to that on English datasets would provide insights into its generalizability across different languages

- **Open Question 3**: How does DPOP perform on preference datasets with high edit distances between completions compared to DPO?
  - **Basis in paper**: [explicit] The paper mentions that DPOP outperforms DPO even on datasets with high edit distances between completions, as shown in the experiments on ARC dataset
  - **Why unresolved**: While the paper demonstrates DPOP's superiority over DPO on ARC, a high edit distance dataset, it does not provide a comprehensive comparison across multiple datasets with varying edit distances
  - **What evidence would resolve it**: Conducting experiments on a wide range of datasets with varying edit distances between preference pairs and comparing the performance of DPOP and DPO would provide a clearer understanding of DPOP's effectiveness in different scenarios

## Limitations

- The analysis focuses primarily on low edit distance cases, and generalizability to other preference alignment scenarios needs further validation
- The hyperparameter λ's optimal value is not thoroughly explored, and its sensitivity to dataset characteristics is unclear
- The paper does not provide a detailed ablation study on the components of DPOP's loss function

## Confidence

- **High Confidence**: The theoretical analysis of DPO's failure mode when edit distance is low, supported by gradient calculations and the Bradley-Terry model; the empirical evidence showing DPOP's effectiveness on multiple benchmarks
- **Medium Confidence**: The claim that DPOP outperforms DPO across a wide variety of datasets and downstream tasks, as the paper presents results on a limited number of benchmarks
- **Low Confidence**: The assertion that DPOP is the first open-source LLM to surpass an average accuracy of 80% on the HuggingFace Open LLM Leaderboard, as the paper doesn't provide comprehensive analysis of leaderboard history

## Next Checks

1. **Ablation Study**: Conduct an ablation study on DPOP's loss function components to determine the contribution of each term to the overall performance improvement, helping understand the importance of the regularization term and margin factor

2. **Hyperparameter Sensitivity**: Investigate the sensitivity of DPOP's performance to the hyperparameter λ and its interaction with other hyperparameters like learning rate and batch size, providing insights into robustness across different datasets and model architectures

3. **Generalization to Other Preference Alignment Scenarios**: Evaluate DPOP's effectiveness on preference alignment tasks beyond pairwise completion ranking, such as preference ranking of longer sequences or preference alignment with multiple preference levels, assessing broader applicability to various preference alignment scenarios