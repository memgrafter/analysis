---
ver: rpa2
title: Optimizing LLM Queries in Relational Data Analytics Workloads
arxiv_id: '2403.05821'
source_url: https://arxiv.org/abs/2403.05821
tags:
- cache
- query
- column
- dataset
- movie
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of invoking Large
  Language Models (LLMs) within relational data analytics workloads. The authors propose
  novel techniques to optimize LLM queries by reordering rows and fields to maximize
  key-value cache reuse, deduplicating redundant inference requests, and estimating
  LLM operator costs for query optimization.
---

# Optimizing LLM Queries in Relational Data Analytics Workloads

## Quick Facts
- arXiv ID: 2403.05821
- Source URL: https://arxiv.org/abs/2403.05821
- Reference count: 40
- Primary result: Up to 3.4x improvement in job completion time and 32% cost savings on Llama 3 models in Spark workloads

## Executive Summary
This paper addresses the high computational cost of invoking Large Language Models (LLMs) within relational data analytics workloads. The authors propose novel techniques to optimize LLM queries by reordering rows and fields to maximize key-value cache reuse, deduplicating redundant inference requests, and estimating LLM operator costs for query optimization. Their approach, implemented in Apache Spark with vLLM as the model serving backend, achieves significant performance improvements and cost savings on a benchmark of diverse LLM-based queries.

## Method Summary
The authors developed optimization techniques specifically for LLM queries in relational data analytics contexts. Their approach focuses on three main strategies: optimizing the order of rows and fields to maximize key-value cache reuse across similar requests, implementing deduplication mechanisms to eliminate redundant inference requests, and creating cost estimation models for LLM operators to enable intelligent query optimization. These techniques were implemented as extensions to Apache Spark, leveraging vLLM for model serving, and evaluated using Llama 3 models across various query workloads.

## Key Results
- Up to 3.4x improvement in job completion time
- 32% cost savings under OpenAI and Anthropic pricing models
- Demonstrated effectiveness on diverse LLM-based queries using Llama 3 models

## Why This Works (Mechanism)
The optimization works by exploiting workload information inherent in relational analytics to enhance the KV cache hit rate and reduce the number of LLM invocations needed. By reordering rows and fields based on similarity patterns, the system can maximize reuse of cached computations across similar queries. The deduplication mechanism prevents redundant processing of identical or near-identical requests, while the cost estimation framework enables the query optimizer to make informed decisions about execution plans that minimize LLM invocation costs.

## Foundational Learning
- **Key-Value Cache Optimization**: Understanding how KV caches work in transformer models and how cache hits can be maximized through intelligent request ordering - needed to reduce redundant computation; quick check: measure cache hit rate before and after optimization
- **Query Optimization in Relational Systems**: Applying traditional database optimization techniques to LLM workloads - needed to integrate LLM optimizations into existing data processing frameworks; quick check: compare execution plans with and without LLM-aware optimizations
- **Deduplication Mechanisms**: Implementing efficient duplicate detection for LLM requests - needed to eliminate redundant model invocations; quick check: count unique vs. duplicate requests in typical workloads

## Architecture Onboarding

**Component Map**: Spark SQL Planner -> LLM Operator Rewriter -> vLLM Model Server -> KV Cache Manager -> Deduplication Layer

**Critical Path**: Query optimization occurs at planning time, with actual LLM invocations happening during execution. The KV cache manager and deduplication layer operate in real-time during query execution to optimize ongoing requests.

**Design Tradeoffs**: The system trades additional preprocessing (row/field reordering) and memory overhead (for deduplication tracking) against reduced computation time. The field reordering approach may impact response quality but improves cache utilization.

**Failure Signatures**: Performance degradation occurs when request patterns lack similarity, making cache reuse ineffective. Memory pressure can arise from storing deduplication state. Query optimization may fail to find cost-effective plans if cost estimates are inaccurate.

**First Experiments**:
1. Measure cache hit rate and overall latency with and without field reordering on a controlled dataset
2. Evaluate deduplication effectiveness by counting unique vs. duplicate requests in sample workloads
3. Compare execution costs under different pricing models with and without the cost estimation framework

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on Llama 3 models, raising questions about generalizability to other model families
- Does not address potential accuracy degradation from optimization techniques, particularly field reordering
- Cost model assumes static pricing from OpenAI and Anthropic, which may not reflect real-world pricing fluctuations

## Confidence
- **High confidence**: The technical implementation details of KV cache optimization and deduplication mechanisms are well-described and technically sound
- **Medium confidence**: The claimed performance improvements (3.4x speedup, 32% cost reduction) are based on benchmark results that may not fully represent production workloads
- **Low confidence**: The absence of accuracy impact analysis and limited model diversity in evaluation reduce confidence in real-world applicability

## Next Checks
1. Conduct controlled experiments measuring response quality degradation when applying field reordering and other optimizations across diverse query types
2. Test optimization effectiveness with smaller models (Llama 3 8B) and larger models (Llama 3 70B), as well as non-Llama architectures, to assess KV cache behavior across model families
3. Evaluate cost savings under varying token pricing scenarios and enterprise pricing models to verify robustness of the cost optimization claims