---
ver: rpa2
title: Generative Model for Less-Resourced Language with 1 billion parameters
arxiv_id: '2410.06898'
source_url: https://arxiv.org/abs/2410.06898
tags:
- slovene
- language
- gams
- https
- wechsel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GaMS 1B, a 1-billion-parameter generative
  language model for Slovene. Since Slovene lacks sufficient training data for building
  a large model from scratch, the authors continued pretraining the English OPT model
  with Slovene, Croatian, and Serbian texts.
---

# Generative Model for Less-Resourced Language with 1 billion parameters

## Quick Facts
- arXiv ID: 2410.06898
- Source URL: https://arxiv.org/abs/2410.06898
- Authors: Domen Vreš; Martin Božič; Aljaž Potočnik; Tomaž Martinčič; Marko Robnik-Šikonja
- Reference count: 0
- Key outcome: Introduces GaMS 1B, a 1-billion-parameter generative language model for Slovene, achieving comparable or better performance than GPT-3.5-Turbo on sentence simplification but lagging behind fine-tuned BERT models on classification tasks.

## Executive Summary
This paper presents GaMS 1B, a 1-billion-parameter generative language model for Slovene, a low-resource language. Since Slovene lacks sufficient training data for building a large model from scratch, the authors continued pretraining the English OPT model with Slovene, Croatian, and Serbian texts. They trained a new tokenizer tailored for Slovene and used embedding transfer methods (WECHSEL, FOCUS) to adapt the English embeddings to the new vocabulary. The model was evaluated on Slovene classification tasks and a sentence simplification task, showing strong generative capabilities despite limitations in classification performance.

## Method Summary
The authors continued pretraining an English OPT model on Slovene, Croatian, Serbian, and English texts, training a new Slovene tokenizer to improve efficiency. They used embedding transfer methods (WECHSEL, FOCUS) to initialize the embedding layer, preserving semantic relationships across languages. The model was trained for multiple epochs to compensate for the limited Slovene corpus size, using a vocabulary of 80,000 tokens and standard transformer architecture with 24 layers and 32 attention heads.

## Key Results
- GaMS 1B achieved comparable or better performance than GPT-3.5-Turbo on the sentence simplification task SENTA using SARI score.
- On Slovene classification tasks, GaMS lagged behind fine-tuned BERT-type models, with low percentage of valid predictions.
- The WECHSEL and FOCUS embedding transfer methods improved training stability and reduced validation loss compared to random initialization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning from English OPT to Slovene is feasible because embedding similarity methods (WECHSEL, FOCUS) preserve semantic structure across languages.
- Mechanism: The methods align embeddings in a shared vector space (FastText or CroSloEngual BERT) and transfer them via nearest-neighbor weighted averaging. This maintains the distribution of semantic relationships when vocabulary changes.
- Core assumption: The semantic similarity structure is preserved under linear or sparsemax alignment between languages.
- Evidence anchors:
  - [abstract]: "used embedding initialization methods FOCUS and WECHSEL to transfer the embeddings from the English OPT model"
  - [section 4.2]: Detailed equations for cosine similarity and convex combination.
  - [corpus]: Weak—corpus contains Slovene, Croatian, Serbian, English but no direct semantic similarity validation.
- Break condition: If the semantic similarity structure differs significantly between English and Slovene (e.g., polysemy patterns), the alignment fails and model performance degrades.

### Mechanism 2
- Claim: Vocabulary expansion with Slovene-tailored tokenizer improves Slovene language handling without catastrophic forgetting of English.
- Mechanism: A larger Slovene vocabulary reduces token count for Slovene text (2x fewer tokens than OPT tokenizer) and allows better context window utilization. Embedding transfer methods mitigate the negative impact of random initialization.
- Core assumption: More fine-grained tokenization for Slovene compensates for model parameter limitations and preserves contextual representation.
- Evidence anchors:
  - [section 4.1]: "tokenizing the same amount of Slovene text with OPT tokenizer results in twice as many tokens as tokenizing it with Slovene tokenizer"
  - [section 4.2]: "WECHSEL/FOCUS improve the model performance compared to random initialization of the embedding matrix"
  - [corpus]: Weak—no direct performance comparison with random initialization without transfer.
- Break condition: If the new vocabulary introduces too many rare tokens or the model overfits to Slovene, English performance drops significantly.

### Mechanism 3
- Claim: Multi-epoch training with repeated data compensates for limited Slovene corpus size and stabilizes model convergence.
- Mechanism: Repeated exposure to the same tokens increases effective training steps and allows embedding and output layers to stabilize before fine-tuning hidden layers.
- Core assumption: Data repetition does not cause overfitting because the corpus is small and diverse enough.
- Evidence anchors:
  - [section 4.3]: "training the model for multiple epochs" and "we train the WECHSEL CSE GaMS model... for 4 epochs"
  - [section 4.3]: "using multiple epochs actually reduces the validation loss"
  - [corpus]: Weak—no ablation showing single vs. multi-epoch performance without repeated data.
- Break condition: If the corpus contains highly repetitive or low-quality data, multi-epoch training amplifies noise and harms generalization.

## Foundational Learning

- Concept: Subword tokenization and vocabulary size trade-offs
  - Why needed here: Tokenization efficiency directly impacts model context window usage and training cost.
  - Quick check question: If Slovene words are split into more tokens than English words with the same tokenizer, what happens to the effective context window for Slovene?

- Concept: Embedding alignment and transfer learning
  - Why needed here: Transferring embeddings preserves learned semantic relationships when adapting to a new language.
  - Quick check question: What happens to the model's semantic understanding if embeddings are randomly initialized instead of transferred?

- Concept: Cross-entropy loss and vocabulary distribution
  - Why needed here: Loss comparison across vocabularies with different token distributions is invalid.
  - Quick check question: Why can't we directly compare validation loss between OPT_GaMS and GaMS models?

## Architecture Onboarding

- Component map: Input text -> Slovene/Croatian/Serbian/English tokenizer (SentencePiece BPE) -> Embedding layer (WECHSEL/FOCUS transferred or random initialization) -> 24 transformer layers (32 heads, 2048 dim, Pre-LayerNorm, ReLU) -> Output layer (80k token vocabulary) -> Cross-entropy loss
- Critical path: Tokenizer → Embedding transfer → Multi-epoch training → Evaluation
- Design tradeoffs: Larger vocabulary improves Slovene efficiency but increases parameters and training time; multi-epoch training helps small corpora but risks overfitting.
- Failure signatures: High percentage of invalid predictions in classification tasks indicates poor task understanding; increased validation loss in later epochs indicates overfitting.
- First 3 experiments:
  1. Compare validation loss curves for random vs. WECHSEL/FOCUS embedding initialization.
  2. Measure token count ratio (Slovene/OPT tokenizer) on held-out Slovene text.
  3. Evaluate classification task performance with increasing few-shot examples to find the saturation point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does instruction-tuning affect the performance of GaMS models on classification tasks?
- Basis in paper: [inferred] The paper states that the models are not instruction-tuned and that this could improve their understanding of tasks, suggesting future work to develop an instruction-following dataset and instruction-tune the models.
- Why unresolved: The paper only evaluates the models using few-shot in-context learning without instruction-tuning, and it is explicitly stated that this approach is not suitable for classification tasks.
- What evidence would resolve it: Training and evaluating GaMS models with instruction-tuning on classification tasks and comparing their performance to the current few-shot in-context learning approach and fine-tuned BERT-type models.

### Open Question 2
- Question: What is the impact of using different embedding initialization methods (WECHSEL, FOCUS, and their combinations with CSE BERT) on the model's performance in downstream tasks?
- Basis in paper: [explicit] The paper compares different embedding initialization methods and shows that they reduce training and validation loss, but it notes that the final validation losses differ by less than 0.02, showing no significant difference in performance.
- Why unresolved: The paper concludes that there is no significant difference in the performance of these methods, but it does not explore whether these differences might become more pronounced with larger models or in different tasks.
- What evidence would resolve it: Training and evaluating larger GaMS models with different embedding initialization methods on a wider range of downstream tasks to determine if the differences in performance become significant.

### Open Question 3
- Question: How does the performance of GaMS models on generative tasks like sentence simplification compare to other state-of-the-art generative models for Slovene?
- Basis in paper: [explicit] The paper evaluates GaMS models on the sentence simplification task SENTA and finds that they perform similarly or better than GPT-3.5-Turbo, but it also notes that the differences in SARI scores are not significant.
- Why unresolved: The paper only compares GaMS models to GPT-3.5-Turbo and some T5 models, but it does not provide a comprehensive comparison with other state-of-the-art generative models for Slovene.
- What evidence would resolve it: Evaluating GaMS models on a broader range of generative tasks and comparing their performance to other state-of-the-art generative models for Slovene, such as those based on the Llama or Falcon architectures.

## Limitations
- Lack of direct comparison between embedding transfer methods and random initialization makes it difficult to isolate their contribution to model performance.
- Corpus composition and quality remain underspecified, particularly regarding the balance between Slovene, Croatian, Serbian, and English texts.
- Evaluation framework relies on few-shot in-context learning without instruction tuning, which may not fully leverage the model's capabilities for classification tasks.

## Confidence
- **High confidence**: Multi-epoch training reduces validation loss and the vocabulary expansion with Slovene tokenizer improves tokenization efficiency.
- **Medium confidence**: Embedding transfer methods improve model performance compared to random initialization.
- **Low confidence**: GaMS model's sentence simplification performance being "comparable or better" than GPT-3.5-Turbo.

## Next Checks
1. Conduct ablation study comparing model performance with WECHSEL/FOCUS embedding transfer versus random initialization, keeping all other factors constant.
2. Perform statistical significance testing on sentence simplification results using multiple evaluation metrics (BLEU, ROUGE, human evaluation) to validate the claim of GPT-3.5-Turbo comparison.
3. Analyze catastrophic forgetting by evaluating English task performance before and after Slovene pretraining to quantify language retention.