---
ver: rpa2
title: Discovering Preference Optimization Algorithms with and for Large Language
  Models
arxiv_id: '2406.08414'
source_url: https://arxiv.org/abs/2406.08414
tags:
- torch
- loss
- floattensor
- logits
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for automatically discovering new
  preference optimization algorithms using large language models (LLMs). The approach
  involves iteratively prompting an LLM to propose and implement new preference optimization
  loss functions based on their previously evaluated performance.
---

# Discovering Preference Optimization Algorithms with and for Large Language Models

## Quick Facts
- arXiv ID: 2406.08414
- Source URL: https://arxiv.org/abs/2406.08414
- Reference count: 40
- Primary result: Automatically discovered preference optimization algorithms, including DiscoPOP, that outperform existing methods on multiple alignment tasks

## Executive Summary
This paper introduces a novel approach for automatically discovering preference optimization algorithms using large language models (LLMs). The method involves iteratively prompting an LLM to propose and implement new preference optimization loss functions based on their previously evaluated performance. Through this process, the authors discover several high-performing algorithms, including DiscoPOP, which adaptively blends logistic and exponential losses. Experiments demonstrate that DiscoPOP achieves state-of-the-art performance on multiple held-out tasks, including multi-turn dialogue (AlpacaEval 2.0), controlled sentiment generation (IMDb), and summarization (TL;DR), outperforming existing preference optimization algorithms.

## Method Summary
The method uses an iterative LLM prompting loop where the model generates new PyTorch-based loss functions based on previously evaluated performance metrics. Each proposed loss is unit-tested for validity, then trained on a preference dataset using a fixed pipeline, and evaluated on downstream benchmarks. The resulting fitness score (MT-Bench score) is fed back into the prompt context, allowing the LLM to refine subsequent proposals. This process discovers novel algorithms like DiscoPOP, which adaptively combines logistic and exponential losses based on the log-ratio difference between chosen and rejected completions.

## Key Results
- DiscoPOP achieves SOTA performance on AlpacaEval 2.0 (71.4% win rate) and outperforms existing methods on IMDb and TL;DR tasks
- The discovered algorithm shows significant improvements over traditional convex losses, particularly in challenging multi-turn dialogue scenarios
- DiscoPOP exhibits non-convex behavior with local minima that correlate with noisier preference labels in the training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative LLM-driven discovery with in-context performance feedback enables exploration of non-obvious loss function designs that outperform hand-crafted objectives.
- Mechanism: The LLM generates new loss functions, which are evaluated on MT-Bench; the resulting fitness score is fed back into the prompt context, allowing the LLM to refine subsequent proposals based on empirical performance rather than only theoretical considerations.
- Core assumption: The LLM's generative knowledge and reasoning are sufficient to propose diverse, syntactically valid PyTorch implementations that can be evaluated quickly and provide meaningful gradients for further search.
- Evidence anchors: [abstract] "Specifically, we iteratively prompt an LLM to propose and implement new preference optimization loss functions based on previously-evaluated performance metrics." [section] "After each training run we provide the LLM with the corresponding validation accuracy and query it for the next PyTorch-based [Paszke et al., 2017] candidate objective function."
- Break condition: If the LLM repeatedly proposes invalid code or gets stuck proposing minor variations without improvement, or if the inner-loop evaluation becomes too slow/costly.

### Mechanism 2
- Claim: Blending logistic and exponential losses in a log-ratio modulated fashion (DiscoPOP) yields better alignment across held-out tasks than using either loss alone.
- Mechanism: The DiscoPOP loss dynamically weights logistic and exponential components based on the sign and magnitude of the log-ratio difference, giving the exponential term more influence when the model strongly prefers chosen completions and the logistic term more when rejecting completions.
- Core assumption: The adaptive weighting scheme improves alignment because it handles both high-confidence correct predictions (via exponential) and moderate-correct/rejected pairs (via logistic) more effectively than a fixed loss.
- Evidence anchors: [abstract] "The best performing of these we call Discovered Preference Optimization (DiscoPOP), a novel algorithm that adaptively blends logistic and exponential losses." [section] "Mathematically, the LRML function can be described with a temperature parameter τ = 0.05 as follows: flrml(βρ) = ( σ(βρ/τ ) − 1) · fdpo(βρ) + σ(βρ/τ ) · fexp(βρ)"
- Break condition: If DiscoPOP fails to converge for extreme β values or shows instability due to its non-convexity, or if simpler baselines consistently match its performance.

### Mechanism 3
- Claim: Meta-optimization over objective functions (rather than model parameters) can discover generally applicable preference optimization algorithms without human intervention.
- Mechanism: By treating the loss function itself as the meta-parameter γ, the discovery process searches the space of possible convex/non-convex loss forms rather than tuning existing ones, yielding novel algorithmic structures.
- Core assumption: The space of possible loss functions is rich enough that useful new algorithms exist, and the LLM can effectively explore this space given only performance feedback.
- Evidence anchors: [section] "The goal of meta-optimization (optimizing the optimization process) is to uncover novel learning algorithms using a data-driven process." [section] "We catalogue the best-performing loss functions... including one called Discovered Preference Optimization (DiscoPOP), which adaptively blends logistic and exponential losses."
- Break condition: If discovered losses do not generalize beyond the discovery task, or if the search space proves too large/inefficient to navigate with LLM prompting.

## Foundational Learning

- Concept: Preference optimization as binary classification over log-ratio differences.
  - Why needed here: All proposed losses operate on the scalar ρ = log πθ(yw|x)/πref(yw|x) − log πθ(yl|x)/πref(yl|x); understanding this framing is critical to designing or debugging loss functions.
  - Quick check question: What is the interpretation of a positive ρ in the context of preference optimization?

- Concept: Bradley-Terry model and its connection to the BT likelihood formulation used in DPO.
  - Why needed here: The derivation of the DPO loss and all its variants relies on assuming a BT model for pairwise preferences; misapplying it leads to incorrect loss shapes.
  - Quick check question: How does the BT model translate pairwise preferences into a probability over log-ratio differences?

- Concept: Convex vs non-convex loss behavior and its implications for optimization stability.
  - Why needed here: DiscoPOP is explicitly non-convex in parts of its domain; recognizing when non-convexity is helpful vs harmful is essential for diagnosing training issues.
  - Quick check question: Why might a non-convex loss be beneficial in a curriculum-like training regime?

## Architecture Onboarding

- Component map: LLM prompt template -> Code generation and parsing -> Unit test validation -> Inner-loop training pipeline -> Evaluation harness -> Feedback loop

- Critical path:
  1. Prompt LLM with current context (known losses + their fitness)
  2. Parse and validate generated code
  3. Train model with new loss on preference dataset
  4. Evaluate on held-out benchmark
  5. Append result to context and repeat

- Design tradeoffs:
  - Speed vs exploration: Faster training (fewer epochs) allows more iterations but may miss subtle performance differences
  - Context length limits: Must balance including enough prior results for guidance vs staying within LLM context window
  - Code generality: Generated losses must work with arbitrary input shapes; overly specific code fails validation

- Failure signatures:
  - Stuck in local optima: LLM repeatedly proposes minor variants of the same loss without fitness improvement
  - Invalid code: Frequent parse errors or shape mismatches in generated functions
  - Evaluation noise: MT-Bench scores vary too much across seeds to guide the search effectively

- First 3 experiments:
  1. Verify the discovery pipeline by regenerating known losses (DPO, SLiC) and checking if they match baseline fitness scores
  2. Run a small sweep over τ in DiscoPOP to confirm the theoretical non-convexity manifests in practice
  3. Test whether removing the "thought" field from the context degrades the quality of generated proposals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the discovered non-convex loss functions (like DiscoPOP) affect optimization dynamics compared to convex baselines during training?
- Basis in paper: [explicit] The paper explicitly identifies DiscoPOP as non-convex and shows it has negative gradients at ρ=0
- Why unresolved: The paper only provides initial analysis of DiscoPOP's properties but doesn't thoroughly examine how its non-convexity affects training dynamics, convergence, or optimization landscape
- What evidence would resolve it: Detailed analysis of training trajectories, gradient distributions, and optimization behavior when using DiscoPOP vs convex alternatives

### Open Question 2
- Question: What is the relationship between the local minima in DiscoPOP's loss function and the quality of preference labels in the training data?
- Basis in paper: [explicit] The paper observes that 1.35% of training samples fall within DiscoPOP's local minima and shows these samples have lower reward differences between chosen and rejected completions
- Why unresolved: While the paper identifies this correlation, it doesn't explore whether this indicates noisy labels or if the local minima serve a useful regularization function
- What evidence would resolve it: Analysis of how removing or weighting these local minimum samples differently affects final model performance

### Open Question 3
- Question: Can the LLM-driven discovery process be scaled to discover more complex loss functions with multiple hyperparameters that can be tuned independently?
- Basis in paper: [inferred] The paper notes that DiscoPOP repurposes β in the traditional sense, affecting both functional behavior and KL penalty, and suggests future work could study different forms with multiple parameters
- Why unresolved: The current discovery process only explored loss functions with single-parameter variants of β, leaving open the question of whether more complex multi-parameter losses could be discovered
- What evidence would resolve it: Successful discovery of multi-parameter loss functions that outperform single-parameter alternatives across tasks

## Limitations

- The LLM-driven discovery process requires significant computational resources for iterative training and evaluation, making it less accessible than traditional hand-crafted algorithm design
- The effectiveness of DiscoPOP and other discovered algorithms depends heavily on the quality and diversity of the preference datasets used during discovery
- The non-convexity of DiscoPOP, while potentially beneficial, may introduce optimization challenges not fully explored in this work

## Confidence

- **High Confidence**: The core discovery mechanism (iterative LLM prompting with performance feedback) is well-documented and reproducible. The empirical results showing DiscoPOP's superior performance on held-out tasks are supported by specific metrics (AlpacaEval 2.0 win rates, MT-Bench scores).
- **Medium Confidence**: The theoretical justification for DiscoPOP's adaptive blending mechanism is sound, but the practical benefits over simpler alternatives warrant further investigation across more diverse tasks and model scales.
- **Low Confidence**: The scalability of the discovery process to larger models (beyond 7B parameters) and its robustness to different preference dataset characteristics remain unclear without additional experiments.

## Next Checks

1. **Generalization Test**: Evaluate DiscoPOP on a completely different task domain (e.g., code generation or mathematical reasoning) using the same base model and preference dataset to verify that its performance gains extend beyond dialogue and summarization.

2. **Ablation Study**: Systematically remove components of DiscoPOP's adaptive weighting scheme to quantify the contribution of each element to overall performance, determining whether the full complexity is necessary.

3. **Discovery Process Robustness**: Repeat the discovery process with different random seeds and varying context window sizes to assess whether the same algorithms are consistently discovered and whether the search process is sensitive to these hyperparameters.