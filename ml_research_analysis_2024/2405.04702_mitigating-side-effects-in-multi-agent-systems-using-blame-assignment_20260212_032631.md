---
ver: rpa2
title: Mitigating Side Effects in Multi-Agent Systems Using Blame Assignment
arxiv_id: '2405.04702'
source_url: https://arxiv.org/abs/2405.04702
tags:
- agents
- nses
- penalty
- joint
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of mitigating unintended negative
  side effects (NSEs) that occur when independently trained robots are deployed in
  a shared environment and their joint actions produce undesirable outcomes. The authors
  formulate the problem as a bi-objective lexicographic decentralized Markov decision
  process (MASE-MDP), where the primary objective is task completion and the secondary
  objective is minimizing NSEs.
---

# Mitigating Side Effects in Multi-Agent Systems Using Blame Assignment

## Quick Facts
- arXiv ID: 2405.04702
- Source URL: https://arxiv.org/abs/2405.04702
- Reference count: 32
- Primary result: A metareasoning approach that decomposes joint negative side effect penalties into individual agent penalties using counterfactual blame assignment, enabling decentralized policy updates that reduce unintended consequences while maintaining task performance

## Executive Summary
This paper addresses the challenge of unintended negative side effects (NSEs) in multi-agent systems where independently trained robots interact in shared environments. The authors propose a novel framework that formulates the problem as a bi-objective lexicographic decentralized Markov decision process, where task completion is prioritized over NSE minimization. The key innovation is RECON, a counterfactual-based blame assignment algorithm that decomposes the joint NSE penalty into individual agent penalties, enabling decentralized policy computation. The method is evaluated across three simulation domains (salp-inspired robots, Overcooked, and warehouse) and demonstrated on real mobile robots, showing significant reduction in NSE encounters while maintaining task performance.

## Method Summary
The paper introduces a metareasoning approach for mitigating side effects in multi-agent systems by formulating the problem as a bi-objective lexicographic decentralized MDP (MASE-MDP). The framework decomposes the joint NSE penalty into individual agent penalties using a counterfactual-based blame assignment algorithm called RECON. The algorithm uses optimal value functions to quantify how much each agent's action contributes to the overall NSE, enabling decentralized policy updates. The approach assumes transition and reward independence among agents, allowing for independent policy computation. Agents can update their policies either fully or in subsets, with the method showing effectiveness even with partial updates.

## Key Results
- RECON reduces NSE encounters by up to 75% compared to naive policies in real robot experiments
- The method maintains task performance while significantly reducing side effects across all tested domains
- Partial policy updates (as few as 10% of agents) can achieve substantial NSE reduction, though full updates perform better
- The framework scales to different numbers of agents and various domain types including continuous state spaces

## Why This Works (Mechanism)
The method works by assigning individual blame to each agent for their contribution to negative side effects through counterfactual reasoning. By decomposing the joint NSE penalty using optimal value functions, each agent can compute its individual penalty without needing global coordination. This enables decentralized policy updates where agents independently adjust their policies to reduce their assigned blame while still prioritizing task completion. The counterfactual evaluation compares actual outcomes with what would have happened without each agent's action, providing a principled way to attribute responsibility.

## Foundational Learning
- **Decentralized MDPs**: Why needed - To enable independent decision-making by agents without central coordination; Quick check - Verify transition and reward independence assumptions hold
- **Lexicographic objectives**: Why needed - To ensure task completion is prioritized over NSE minimization; Quick check - Confirm primary objective dominates secondary objective in all cases
- **Counterfactual reasoning**: Why needed - To attribute blame for side effects without requiring global state information; Quick check - Validate counterfactual outcomes match expectations
- **Value function decomposition**: Why needed - To enable individual agents to compute their contribution to joint penalties; Quick check - Ensure decomposition accurately reflects individual impact

## Architecture Onboarding

Component Map:
MASE-MDP Formulation -> RECON Blame Assignment -> Individual Penalty Computation -> Decentralized Policy Update -> Environment Interaction

Critical Path:
The critical path flows from the bi-objective formulation through RECON's blame assignment to individual penalty computation, then to decentralized policy updates. The most computationally intensive step is typically the counterfactual evaluation within RECON, which requires solving multiple MDPs to generate counterfactual scenarios.

Design Tradeoffs:
The framework trades computational complexity (solving multiple counterfactual MDPs) for the benefit of decentralized operation and scalability. While full policy updates provide optimal NSE reduction, partial updates offer a practical compromise for large systems. The independence assumptions simplify computation but may limit applicability to heterogeneous agent scenarios.

Failure Signatures:
- Poor blame assignment leading to suboptimal policy updates
- Computational bottlenecks when scaling to many agents or complex environments
- Degradation in task performance if NSE minimization interferes with primary objectives
- Inconsistent counterfactual evaluations due to stochastic environment dynamics

First 3 Experiments:
1. Compare RECON against Difference Reward baseline across all three simulation domains
2. Vary the percentage of agents undergoing policy updates (10%, 50%, 100%) to assess scalability
3. Test performance on real mobile robots in controlled environments to validate simulation results

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of RECON scale when agents have heterogeneous transition dynamics or reward structures, violating the independence assumptions?
- Basis in paper: The authors note their framework currently supports Dec-MDPs with transition and reward independence and aim to relax this assumption in future work
- Why unresolved: The paper only evaluates scenarios with independent transitions and rewards; no experiments test heterogeneous agent models
- What evidence would resolve it: Empirical comparison of RECON's performance in both independent and non-independent transition/reward settings across multiple domains

### Open Question 2
- Question: What is the optimal trade-off between the percentage of agents undergoing policy updates and overall NSE mitigation effectiveness in different domain types?
- Basis in paper: The paper varies update percentages from 10-100% but notes this is "practically infeasible for large systems"
- Why unresolved: While the paper explores different update percentages, it doesn't provide guidance on how to determine the optimal subset of agents to update
- What evidence would resolve it: Systematic analysis showing NSE reduction vs. number of updated agents across different problem sizes and domain characteristics

### Open Question 3
- Question: How does the choice of counterfactual evaluation method affect blame assignment accuracy and subsequent NSE mitigation?
- Basis in paper: The authors compare RECON with Difference Reward baseline, but don't explore alternative counterfactual methods
- Why unresolved: Only one counterfactual-based method is evaluated; the paper doesn't compare different counterfactual generation strategies
- What evidence would resolve it: Comparative analysis of multiple counterfactual evaluation methods measuring both blame assignment accuracy and final NSE reduction

## Limitations
- Limited to discrete action spaces and environments where agents can be independently trained before deployment
- Assumes agents have access to the same state information, which may not hold in partially observable environments
- Experimental validation limited to simulation environments and a single mobile robot platform
- Does not address scenarios with heterogeneous agent capabilities or changing environment dynamics

## Confidence
- Claims about reducing NSEs: Medium - results are promising but limited to controlled experimental settings
- Claims about real-time effectiveness on mobile robots: High - supported by direct experimental validation
- Theoretical guarantees of blame assignment algorithm: Low - no formal proofs of convergence or optimality provided

## Next Checks
1. Test RECON in continuous control environments with more complex dynamics to assess generalizability
2. Evaluate performance in partially observable settings where agents have limited state information
3. Scale up experiments to larger numbers of agents and more complex task scenarios to assess computational efficiency and effectiveness at scale