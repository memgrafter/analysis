---
ver: rpa2
title: 'A Novel Interpretability Metric for Explaining Bias in Language Models: Applications
  on Multilingual Models from Southeast Asia'
arxiv_id: '2410.15464'
source_url: https://arxiv.org/abs/2410.15464
tags:
- bias
- biased
- more
- less
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel interpretability metric, the bias
  attribution score, to quantify token-level contributions to biased behavior in pretrained
  language models (PLMs). The method leverages information theory to calculate scores
  that reveal which words in a sentence push a model toward biased or less biased
  outputs.
---

# A Novel Interpretability Metric for Explaining Bias in Language Models: Applications on Multilingual Models from Southeast Asia

## Quick Facts
- arXiv ID: 2410.15464
- Source URL: https://arxiv.org/abs/2410.15464
- Authors: Lance Calvin Lim Gamboa; Mark Lee
- Reference count: 14
- Primary result: Introduces bias attribution score to quantify token-level contributions to biased behavior in PLMs, revealing sexual orientation bias is stronger than gender bias in multilingual models

## Executive Summary
This paper introduces a novel interpretability metric called the bias attribution score to quantify how individual tokens contribute to biased behavior in pretrained language models. The method uses information theory to measure the contribution of each word in a sentence to the model's preference for biased over less biased outputs. Applied to multilingual models including Southeast Asian ones, the approach reveals that sexual orientation bias is stronger than gender bias, with words related to crime, intimate relationships, and helping being most associated with bias-inducing behavior.

## Method Summary
The method computes token-level bias attribution scores using Jensen-Shannon distance to measure how much each word pushes a model toward biased or less biased outputs. It masks each token in CrowS-Pairs benchmark sentences one-at-a-time, calculates probability distributions for both biased and less biased conditions, and measures distance from ground truth. The approach works for both masked and causal language models by conditioning appropriately on context. Semantic analysis using pymusas tagger identifies which semantic categories (crime, intimate relationships, helping) are most associated with bias-inducing words.

## Key Results
- Sexual orientation bias is significantly stronger than gender bias across all evaluated models
- SEALION models show lower bias scores compared to BERT, ALBERT, and GPT-2 architectures
- Words related to crime, intimate relationships, and helping are most strongly associated with bias-inducing behavior
- The bias attribution score successfully identifies which specific tokens contribute most to biased model outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bias attribution score reveals token-level contributions to biased behavior in PLMs.
- Mechanism: Masks each token one-at-a-time, calculates probability distributions under biased and less biased conditions, measures distance from ground truth using Jensen-Shannon distance. Tokens contributing more to biased condition receive negative scores.
- Core assumption: Jensen-Shannon distance accurately captures difference in model behavior between biased and less biased conditions.
- Evidence anchors: Abstract states method "draws from information theory to measure token-level contributions to biased behavior"; corpus shows moderate relevance in interpretability literature with FMR=0.47.
- Break condition: If Jensen-Shulian distance doesn't accurately capture behavioral differences, scores will be misleading.

### Mechanism 2
- Claim: The bias attribution score can identify semantic categories most associated with bias-inducing words.
- Mechanism: Tags tokens with semantic categories using pymusas tagger after computing bias attribution scores. Proportion of tokens with negative scores within each category reveals which semantic fields are most associated with bias.
- Core assumption: Semantic tagger accurately categorizes tokens into meaningful fields that correlate with bias.
- Evidence anchors: Abstract identifies "crime, intimate relationships, and helping" as key bias domains; pymusas characterizes English words according to 232 field tags.
- Break condition: If semantic tagger misclassifies tokens or categories don't capture true nature of bias-inducing words, analysis will be incorrect.

### Mechanism 3
- Claim: The bias attribution score method is generalizable to both masked and causal language models.
- Mechanism: For masked models, conditions on all tokens before and after masked token. For causal models, conditions only on tokens before masked token. All other calculation steps remain the same.
- Core assumption: Core logic of measuring distance from ground truth applies equally to both model types.
- Evidence anchors: Abstract states method works for "masked and causal language models"; method generalizes similar to Felkner et al. (2023).
- Break condition: If conditioning mechanism for causal models doesn't capture relevant context for bias attribution, scores will be inaccurate.

## Foundational Learning

- Concept: Jensen-Shannon distance
  - Why needed here: Measures distance between probability distributions obtained from model and ground truth distribution
  - Quick check question: What is the range of values for Jensen-Shannon distance, and what do the extremes represent?

- Concept: Information theory
  - Why needed here: The bias attribution score draws from information theory to quantify token-level contributions to biased behavior
  - Quick check question: How does information theory help in measuring the contribution of individual tokens to overall model behavior?

- Concept: Semantic tagging
  - Why needed here: Categorizes tokens into semantic fields to identify which categories are most associated with bias-inducing words
  - Quick check question: What is the purpose of using a semantic tagger in the context of bias attribution analysis?

## Architecture Onboarding

- Component map: CrowS-Pairs dataset -> Bias evaluation component -> Bias attribution score calculation -> Semantic analysis component -> Results interpretation
- Critical path: 1) Load CrowS-Pairs dataset 2) Evaluate model bias using SJSD score 3) Calculate bias attribution scores for each token 4) Tag tokens with semantic categories 5) Analyze proportions of bias-inducing tokens in each category
- Design tradeoffs: Using English benchmark for multilingual models may not capture all nuances of bias in Southeast Asian languages; semantic tagger designed for English may limit accuracy for multilingual models
- Failure signatures: Bias attribution scores don't correlate with known biases; semantic categories don't align with expected bias domains; method doesn't generalize to different model types
- First 3 experiments:
  1. Run bias attribution score method on simple masked language model with known bias to verify it identifies correct tokens
  2. Compare bias attribution scores across different model sizes and training paradigms to validate consistency
  3. Apply semantic analysis to subset of tokens to ensure tagger correctly categorizes bias-inducing words

## Open Questions the Paper Calls Out

- Does the bias attribution score method generalize effectively to evaluate bias in multilingual models trained on languages beyond English and Southeast Asian contexts?
- What are the most effective bias mitigation strategies for language models when considering semantic categories most associated with bias?
- How does the presence of bias in language models impact downstream applications in sensitive domains?

## Limitations

- Reliance on English benchmark datasets (CrowS-Pairs) for evaluating multilingual models may not capture all nuances of bias in Southeast Asian languages
- Jensen-Shannon distance calculation assumes ground truth one-hot distribution accurately represents unbiased behavior
- Semantic tagger analysis depends on accuracy and completeness of English-language tagging system, which may not generalize well to multilingual contexts

## Confidence

- High confidence: Sexual orientation bias is stronger than gender bias across evaluated models; SEALION models show lower bias than other architectures
- Medium confidence: Specific semantic categories identified as most associated with bias (crime, intimate relationships, helping), given dependence on English-language semantic tagging
- Low confidence: Generalizability of bias attribution method to truly capture token-level contributions in Southeast Asian languages due to benchmark and tagging limitations

## Next Checks

1. Replicate bias attribution analysis using Southeast Asian language-specific bias benchmark to verify whether identified semantic categories remain consistent across different cultural contexts
2. Conduct ablation studies where known biased tokens are systematically removed or modified to confirm bias attribution scores correctly identify most influential words
3. Apply method to simpler, controlled dataset with artificially injected bias to verify scores accurately capture token-level contributions to biased behavior