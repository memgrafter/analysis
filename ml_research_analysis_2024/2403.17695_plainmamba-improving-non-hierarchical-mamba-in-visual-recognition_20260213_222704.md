---
ver: rpa2
title: 'PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition'
arxiv_id: '2403.17695'
source_url: https://arxiv.org/abs/2403.17695
tags:
- plainmamba
- vision
- visual
- arxiv
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PlainMamba, a non-hierarchical state space
  model for visual recognition. The authors adapt Mamba's selective scanning to 2D
  images by introducing continuous 2D scanning to ensure spatial continuity and direction-aware
  updating to encode directional information.
---

# PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition

## Quick Facts
- arXiv ID: 2403.17695
- Source URL: https://arxiv.org/abs/2403.17695
- Reference count: 40
- One-line primary result: PlainMamba achieves 77.9-82.3% ImageNet-1K top-1 accuracy and outperforms previous non-hierarchical SSMs while being more efficient on high-resolution inputs

## Executive Summary
PlainMamba adapts the Mamba state space model for visual recognition by introducing continuous 2D scanning and direction-aware updating mechanisms. The architecture maintains constant width across all layers and eliminates CLS tokens, creating a simple non-hierarchical design. It achieves competitive performance on ImageNet classification, semantic segmentation, and object detection while demonstrating efficiency advantages over transformers at high resolutions.

## Method Summary
PlainMamba extends Mamba's selective scanning to 2D images through continuous 2D scanning that ensures spatial adjacency and direction-aware updating that encodes positional information. The architecture stacks identical blocks with constant width throughout all layers, removing hierarchical structures and CLS tokens. The model uses a convolutional tokenizer for downsampling, positional encoding to retain spatial information, and PlainMamba blocks that integrate the 2D scanning mechanisms before a task-specific head.

## Key Results
- Achieves 77.9-82.3% ImageNet-1K top-1 accuracy across different model sizes
- Obtains 44.1-49.1 mIoU on ADE20K semantic segmentation
- Demonstrates 41.7-46.8 COCO AP on object detection and segmentation
- Shows significant efficiency gains over DeiT at 4096Ã—4096 resolution with lower FLOPs and memory usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous 2D Scanning ensures spatial and semantic continuity in visual token processing.
- Mechanism: The scanning order always moves to an adjacent token in the 2D space, avoiding jumps that would create discontinuities. When reaching the end of a row/column, the next token is its adjacent neighbor rather than jumping to the opposite end.
- Core assumption: Spatial continuity of scanned tokens translates to semantic continuity in learned representations.
- Evidence anchors:
  - [abstract]: "a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence"
  - [section]: "Our Continuous 2D Scanning addresses this challenge by ensuring a scanned visual token is always adjacent (in the 2D space) to the previously scanned token"
  - [corpus]: Weak - No direct corpus evidence found for this specific mechanism

### Mechanism 2
- Claim: Direction-Aware Updating encodes 2D positional information into the selective scanning process.
- Mechanism: A set of learnable parameters {ðš¯â‚–} representing four cardinal directions plus a BEGIN direction are combined with data-dependent updating parameters to explicitly inject directional information into the SSM.
- Core assumption: The relative 2D position of each token can be effectively encoded through directional parameters and learned during training.
- Evidence anchors:
  - [abstract]: "direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information"
  - [section]: "Drawing inspiration from the relative positional encoding mechanisms in vision transformers [23], we employ a set of learnable parameters {ðš¯â‚– âˆˆ RáµÃ—â‚}..."
  - [corpus]: Weak - No direct corpus evidence found for this specific directional encoding mechanism

### Mechanism 3
- Claim: Maintaining constant width across all layers simplifies feature integration and scaling.
- Mechanism: The architecture stacks identical PlainMamba blocks with constant width throughout all layers, removing the need for CLS tokens and hierarchical structures.
- Core assumption: Constant-width, non-hierarchical architectures are easier to integrate across modalities and downstream tasks.
- Evidence anchors:
  - [abstract]: "Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers"
  - [section]: "This design choice considers the recent progress made in various visual foundation models [46,65,67] where the plain non-hierarchical ViT is used rather than its hierarchical counterparts"
  - [corpus]: Weak - No direct corpus evidence found for this specific architectural simplification

## Foundational Learning

- Concept: State Space Models (SSMs) and their relationship to linear recurrent networks
  - Why needed here: PlainMamba is built on Mamba's SSM foundation, so understanding how SSMs process sequential data is crucial for adapting them to 2D visual inputs
  - Quick check question: What is the fundamental difference between how SSMs and Transformers process sequences?

- Concept: Selective scanning mechanism in Mamba
  - Why needed here: The paper builds upon Mamba's selective scanning but adapts it for 2D images, so understanding the original 1D scanning process is essential
  - Quick check question: How does Mamba's selective scanning differ from traditional attention mechanisms in terms of computational complexity?

- Concept: Positional encoding in vision transformers
  - Why needed here: The Direction-Aware Updating mechanism draws inspiration from positional encoding, so understanding how relative positions are encoded in ViTs is important
  - Quick check question: What is the key difference between absolute and relative positional encoding in vision transformers?

## Architecture Onboarding

- Component map: Convolutional tokenizer -> Positional encoding -> PlainMamba blocks -> Task-specific head
- Critical path: Input image â†’ Convolutional tokenizer â†’ Positional encoding â†’ PlainMamba blocks â†’ Task-specific head
- Design tradeoffs:
  - Non-hierarchical vs hierarchical: Simpler integration and scaling vs potentially better hierarchical feature capture
  - Constant width vs variable width: Easier feature integration vs potentially more expressive representations
  - No CLS token vs with CLS token: Simpler architecture vs potential loss of global information gathering mechanism
- Failure signatures:
  - Poor spatial understanding: Check if continuous 2D scanning is implemented correctly
  - Inefficient scaling: Verify constant-width design is maintained across all layers
  - Directional confusion: Test if direction-aware updating parameters are being learned properly
- First 3 experiments:
  1. Test continuous 2D scanning by visualizing the scanning order on sample images
  2. Validate direction-aware updating by checking if the model can distinguish between different scanning directions
  3. Compare performance with and without CLS tokens to verify the simplification doesn't hurt accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PlainMamba's performance scale when applied to very large input resolutions beyond 4096Ã—4096, and what is the theoretical limit of its efficiency gains compared to transformers?
- Basis in paper: [explicit] The paper demonstrates PlainMamba's efficiency advantage over DeiT at high resolutions (4096Ã—4096), showing significantly lower FLOPs and memory consumption.
- Why unresolved: The paper only tests up to 4096Ã—4096 resolution. The scaling behavior and theoretical efficiency limits at even larger resolutions remain unexplored.
- What evidence would resolve it: Systematic benchmarking of PlainMamba and DeiT/ViT variants at resolutions exceeding 4096Ã—4096, measuring FLOPs, memory usage, and accuracy trade-offs.

### Open Question 2
- Question: What is the impact of different spatial scanning patterns (beyond the four cardinal directions used in PlainMamba) on model performance and efficiency?
- Basis in paper: [explicit] The paper introduces Continuous 2D Scanning with four distinct orders to ensure spatial continuity and direction-aware updating.
- Why unresolved: The paper only explores four scanning directions. The potential benefits or drawbacks of alternative scanning patterns (e.g., diagonal, spiral, or adaptive patterns) remain unexplored.
- What evidence would resolve it: Comparative experiments testing PlainMamba with various scanning patterns, measuring accuracy, efficiency, and sensitivity to input image content.

### Open Question 3
- Question: How does PlainMamba's performance generalize to other visual modalities beyond standard image classification, such as medical imaging, satellite imagery, or video analysis?
- Basis in paper: [explicit] The paper evaluates PlainMamba on ImageNet-1K, semantic segmentation (ADE20K), and object detection/segmentation (COCO), but doesn't explore specialized visual domains.
- Why unresolved: The experiments focus on general-purpose visual recognition tasks. The model's effectiveness for domain-specific applications with unique characteristics (e.g., high-resolution medical scans, multi-spectral satellite images, or temporal video data) is unknown.
- What evidence would resolve it: Comprehensive benchmarking of PlainMamba on specialized datasets representative of different visual domains, measuring performance relative to domain-specific architectures.

## Limitations
- Weak empirical grounding for core mechanisms through lack of ablation studies
- Limited task scope focused primarily on standard benchmarks without exploring complex visual tasks
- Scaling assumptions unverified for larger models and cross-domain generalization
- Training configuration opacity with incomplete implementation details

## Confidence
- High Confidence: Claims about competitive performance on established benchmarks (ImageNet-1K top-1 accuracy of 77.9-82.3%, ADE20K mIoU of 44.1-49.1%, COCO AP of 41.7-46.8%)
- Medium Confidence: The assertion that PlainMamba outperforms previous non-hierarchical SSMs is credible given the results, though direct comparisons with specific prior work would strengthen this claim
- Low Confidence: The theoretical mechanisms (continuous 2D scanning improving spatial continuity, direction-aware updating encoding positional information) lack experimental validation through ablation studies or qualitative analysis of learned representations

## Next Checks
1. **Ablation validation:** Train PlainMamba variants without continuous 2D scanning (using standard raster scanning) and without direction-aware updating (using standard Mamba parameters) to quantify their individual contributions to performance.

2. **Scanning order visualization:** Implement visualization tools to trace the actual scanning order on sample images and verify that the continuous 2D scanning mechanism maintains adjacency as claimed, checking for any boundary condition violations.

3. **Cross-resolution robustness:** Test the model on varying input resolutions beyond the standard 224x224 to validate whether the direction-aware updating parameters generalize effectively across different spatial scales and whether the constant-width design maintains performance.