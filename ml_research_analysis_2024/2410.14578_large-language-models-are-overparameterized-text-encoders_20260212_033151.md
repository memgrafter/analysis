---
ver: rpa2
title: Large Language Models Are Overparameterized Text Encoders
arxiv_id: '2410.14578'
source_url: https://arxiv.org/abs/2410.14578
tags:
- pruning
- text
- training
- layers
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) are
  overparameterized for text embedding tasks. The authors propose pruning the last
  p% layers of LLMs before supervised contrastive training, achieving proportional
  reductions in memory and inference time.
---

# Large Language Models Are Overparameterized Text Encoders

## Quick Facts
- **arXiv ID:** 2410.14578
- **Source URL:** https://arxiv.org/abs/2410.14578
- **Reference count:** 34
- **Key outcome:** Up to 80% of LLM layers can be pruned for text embedding tasks with only modest performance drops, and up to 30% with negligible impact.

## Executive Summary
This paper investigates whether large language models are overparameterized for text embedding tasks. The authors propose pruning the last p% layers of LLMs before supervised contrastive training, achieving proportional reductions in memory and inference time. They evaluate four state-of-the-art LLMs (LLaMA-3-8B, Mistral-7B, Qwen2-7B, Phi3-4B) on text embedding tasks and find that up to 30% of layers can be pruned with negligible impact on performance, and up to 80% with only a modest drop. They also introduce L3Prune, a novel layer-pruning strategy based on initial model loss, which provides two optimal pruning configurations: a large variant with negligible performance loss and a small variant for resource-constrained settings.

## Method Summary
The authors propose pruning the last p% layers of LLMs before supervised contrastive training, achieving proportional reductions in memory and inference time. They evaluate four state-of-the-art LLMs (LLaMA-3-8B, Mistral-7B, Qwen2-7B, Phi3-4B) on text embedding tasks and find that up to 30% of layers can be pruned with negligible impact on performance, and up to 80% with only a modest drop. They also introduce L3Prune, a novel layer-pruning strategy based on initial model loss, which provides two optimal pruning configurations: a large variant with negligible performance loss and a small variant for resource-constrained settings.

## Key Results
- Up to 30% of LLM layers can be pruned with negligible impact on text embedding performance
- Up to 80% of layers can be pruned with only a modest performance drop
- L3Prune strategy identifies two optimal configurations: 21% parameter reduction with -0.3 performance drop (large variant), and 74% parameter reduction with -5.1 performance drop (small variant)

## Why This Works (Mechanism)
The paper demonstrates that the final layers of LLMs contain redundant parameters for text embedding tasks, allowing for effective pruning without significant performance degradation.

## Foundational Learning
1. **Text Embedding:** Dense vector representations of text for semantic tasks; needed for retrieval, clustering, and similarity comparison
2. **Layer Pruning:** Removing model layers to reduce computational cost; quick check: measure parameter reduction vs. performance impact
3. **Supervised Contrastive Learning:** Training method that maximizes similarity between related samples; quick check: verify loss convergence during training
4. **Model Overparameterization:** When models have more parameters than necessary for a task; quick check: assess performance with reduced parameters

## Architecture Onboarding
**Component Map:** Input -> Encoder Layers -> Pooling Layer -> Output Embeddings
**Critical Path:** Input text → Encoder layers → Pooling → Final embedding
**Design Tradeoffs:** Layer pruning reduces memory and inference time but may impact task-specific performance
**Failure Signatures:** Over-pruning leads to performance degradation; under-pruning misses optimization opportunities
**First Experiments:** 1) Baseline performance without pruning, 2) Gradual layer pruning from 10% to 80%, 3) Compare L3Prune vs. uniform pruning

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to 4-8B parameter models, may not generalize to larger models
- Pruning strategy focuses only on trailing layers, other pruning methods not explored
- Evaluation metrics and downstream tasks not fully specified

## Confidence
**High** in core finding that LLMs contain redundant parameters for text embedding tasks
**Medium** in specific pruning percentages due to potential variability across architectures
**Medium** in L3Prune strategy's optimality claims requiring broader validation

## Next Checks
1. Test pruning strategies on larger LLM variants (70B+ parameters) to assess scalability
2. Evaluate pruned models across diverse embedding tasks including retrieval, clustering, and semantic similarity
3. Compare L3Prune against alternative pruning methods (magnitude-based, attention-based, or layer-wise importance metrics)