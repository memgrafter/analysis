---
ver: rpa2
title: Estimating the Local Learning Coefficient at Scale
arxiv_id: '2402.03698'
source_url: https://arxiv.org/abs/2402.03698
tags:
- learning
- coefficient
- loss
- parameter
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to estimate the Local Learning Coefficient
  (LLC), a measure of model complexity derived from Bayesian statistics and singular
  learning theory. While the LLC has theoretical value, it has been challenging to
  estimate for modern deep learning architectures.
---

# Estimating the Local Learning Coefficient at Scale

## Quick Facts
- arXiv ID: 2402.03698
- Source URL: https://arxiv.org/abs/2402.03698
- Reference count: 40
- The paper introduces a scalable method to estimate the Local Learning Coefficient (LLC) for deep learning models using stochastic-gradient MCMC.

## Executive Summary
This paper presents a method to estimate the Local Learning Coefficient (LLC), a measure of model complexity from singular learning theory, for modern deep learning architectures. The authors adapt stochastic gradient Langevin dynamics (SGLD) to approximate the tempered posterior expectations required by the LLC formula, enabling estimation at scales up to 100 million parameters. They demonstrate accuracy on deep linear networks by recovering theoretical predictions and show the estimator can detect internal structural changes during training. The method also achieves invariance to parameter-space symmetries when properly preconditioned, addressing a key limitation of previous approaches.

## Method Summary
The method uses stochastic-gradient Langevin dynamics (SGLD) to sample from a localized, tempered posterior distribution in parameter space. The inverse temperature β is set to 1/log(n) as dictated by Watanabe's theory, and a confinement term keeps exploration localized around minima without distorting the volume scaling exponent. The preconditioning matrix A scales gradients inversely to parameter rescaling factors, removing sensitivity to symmetry-induced degeneracies. This approach enables approximation of the LLC formula's expectations at scale while maintaining accuracy and invariance properties.

## Key Results
- Stochastic-gradient LLC estimator recovers theoretical learning coefficient for deep linear networks up to 100M parameters
- Estimator achieves invariance to parameter-space symmetries like rescaling in ReLU networks with proper preconditioning
- LLC estimates can detect internal structural changes during training, such as rank transitions in deep linear networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The stochastic-gradient LLC estimator recovers the theoretical learning coefficient for deep linear networks up to 100M parameters.
- Mechanism: The estimator uses stochastic gradient Langevin dynamics (SGLD) to approximate the tempered posterior expectation required by the LLC formula, with the inverse temperature β set to 1/log(n) as dictated by Watanabe's theory.
- Core assumption: The tempered posterior can be accurately approximated by SGLD even at large scales, and the confinement term keeps exploration localized around minima without distorting the volume scaling exponent.
- Evidence anchors:
  - [abstract] "Using a method developed in {\tt arXiv:2308.12108 [stat.ML]} we empirically show how the LLC may be measured accurately and self-consistently for deep linear networks (DLNs) up to 100M parameters."
  - [section] "We show that stochastic-gradient LLC estimator can accurately recover the theoretically predicted learning coefficient for DLNs up to at least 100M parameters."
- Break condition: If the confinement strength γ is set too high or too low, the estimated volume scaling could be distorted; if the step size is mis-tuned, SGLD samples will not approximate the tempered posterior.

### Mechanism 2
- Claim: The LLC estimator is invariant to parameter-space symmetries like rescaling in ReLU networks when properly preconditioned.
- Mechanism: By setting the preconditioning matrix A to scale gradients inversely to the parameter rescaling factors, the estimator's sensitivity to symmetry-induced degeneracies is removed.
- Core assumption: The preconditioning matrix can be constructed to cancel out the Jacobian effects of symmetry transformations.
- Evidence anchors:
  - [section] "With proper preconditioning, stochastic-gradient LLC estimator is invariant to parameter-space symmetries, like rescaling symmetries in ReLU networks."
  - [corpus] No direct evidence in neighbors; the claim is new to this paper.
- Break condition: If preconditioning is omitted or mis-specified, symmetry-induced degeneracies will lead to inconsistent LLC estimates across rescaled models.

### Mechanism 3
- Claim: The LLC estimator can detect internal structural changes in neural networks during training, such as rank transitions in deep linear networks.
- Mechanism: As the network's effective rank changes along the training trajectory, the geometry of the loss landscape near the current parameter changes, altering the volume scaling exponent λ(w*) and thus the LLC estimate.
- Core assumption: Structural changes in the network (e.g., rank increases) correspond to detectable changes in the local loss landscape geometry.
- Evidence anchors:
  - [section] "We notice that the estimated LLC can measure changes in a model’s internal structure... This is conceptually similar to the experiments of Chen et al. (2023) and Hoogland et al. (2024)."
  - [corpus] No direct evidence in neighbors; the claim is new to this paper.
- Break condition: If the confinement is too weak or the step size too large, the SGLD chain may not remain localized enough to detect fine-grained structural transitions.

## Foundational Learning

- Concept: Singular Learning Theory (SLT) and the Learning Coefficient (λ)
  - Why needed here: The LLC is defined in terms of SLT as the asymptotic volume scaling exponent of low-loss parameter regions; understanding this definition is necessary to interpret estimator outputs.
  - Quick check question: What does the multiplicity m in the volume scaling law V(ε) ∝ ε^λ (−log ε)^{m−1} represent?

- Concept: Bayesian Generalization Error and the Free Energy Formula
  - Why needed here: The LLC appears in the asymptotic free energy formula, linking it to Bayesian generalization error; this motivates why measuring it matters beyond pure geometry.
  - Quick check question: How does the term λ log n in the free energy formula relate to the learning coefficient?

- Concept: Stochastic Gradient MCMC (SGLD) and Tempered Posteriors
  - Why needed here: The estimator approximates expectations over tempered posteriors using SGLD; knowing how SGLD works is key to understanding estimator behavior and tuning.
  - Quick check question: What role does the inverse temperature β = 1/log(n) play in the SGLD-based LLC estimator?

## Architecture Onboarding

- Component map:
  - Input: Deep linear network (DLN) architecture and parameters, training dataset
  - Core estimator: SGLD sampling of the localized tempered posterior to compute the expectation in the LLC formula
  - Output: Estimated LLC λ̂(w*) for each parameter of interest
  - Validation: Comparison against theoretical λ for DLNs and invariance tests under symmetries

- Critical path:
  1. Generate or load a DLN and dataset
  2. Set SGLD hyperparameters (step size, batch size, confinement γ)
  3. Run SGLD to collect samples from the localized tempered posterior
  4. Compute the LLC estimate from the sampled expectations
  5. Compare with theoretical or baseline values

- Design tradeoffs:
  - Step size vs. accuracy: Larger step sizes speed sampling but risk bias; smaller sizes are accurate but slow.
  - Confinement strength γ vs. localization: Too weak and the estimator drifts; too strong and the volume scaling is distorted.
  - Batch size vs. gradient variance: Larger batches reduce variance but increase cost.

- Failure signatures:
  - Loss trace not flattening → insufficient step count or step size too high
  - Low mean MALA acceptance probability (< 0.9) → step size too large
  - λ̂(w*) inconsistent under known symmetries → preconditioning missing or wrong
  - Large variance across runs → insufficient sampling or poor mixing

- First 3 experiments:
  1. Run estimator on a small DLN (≤ 10k params) with known theoretical λ; verify λ̂(w*) ≈ λ.
  2. Test invariance: rescale a ReLU network and confirm λ̂(w*) unchanged with proper preconditioning.
  3. Detect structural change: train a DLN with increasing rank and confirm λ̂(w*) tracks rank transitions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the stochastic-gradient LLC estimator (ˆλ(w*)) accurate for nonlinear neural networks beyond deep linear networks?
- Basis in paper: [explicit] The authors state "While this does not necessarily mean that it is an accurate estimator of the LLC for nonlinear neural networks, this question remains out of reach until there is substantial new mathematical progress on deriving theoretical values for these quantities."
- Why unresolved: Deep linear networks are the only model class where theoretical learning coefficient values are available, making it impossible to directly verify accuracy for nonlinear networks.
- What evidence would resolve it: Deriving theoretical learning coefficient values for nonlinear networks or finding an alternative ground truth method to validate estimates.

### Open Question 2
- Question: Can the LLC be used to predict generalization error for neural networks outside the Bayesian setting?
- Basis in paper: [explicit] The authors note "We expect the utility of the learning coefficient as a geometric measure to apply beyond the Bayesian setting, but whether the connection with generalization will continue to hold is unknown."
- Why unresolved: The theoretical connection between LLC and generalization error has only been proven for Bayesian learning, not for standard neural network training.
- What evidence would resolve it: Empirical studies showing correlation between LLC and generalization error across various neural network architectures and tasks.

### Open Question 3
- Question: Does the LLC capture model-independent complexity, or is it purely a measure of model-specific complexity?
- Basis in paper: [explicit] The authors discuss "It is worth clarifying what we mean here — or rather, what we do not mean... Measures of model-independent complexity, like Kolmogorov complexity, describe the complexity inherent to the task itself."
- Why unresolved: While the LLC is theoretically a model complexity measure, the authors speculate it might have model-independent properties for neural networks.
- What evidence would resolve it: Experiments showing that LLC values are consistent across different neural network architectures trained on the same task, or theoretical proofs establishing model-independent properties.

## Limitations
- Scalability to truly large-scale architectures like transformers remains unproven, and computational cost may become prohibitive
- The method relies on accurate approximation of tempered posterior expectations, which may be challenging for complex loss landscapes
- No direct validation of accuracy for nonlinear neural networks beyond deep linear networks

## Confidence

- **High confidence**: The LLC estimator accurately recovers theoretical values for deep linear networks up to 100M parameters
- **Medium confidence**: The estimator is invariant to parameter-space symmetries with proper preconditioning
- **Medium confidence**: The estimator can detect internal structural changes in neural networks during training

## Next Checks
1. **Scalability Test**: Apply the LLC estimator to a transformer architecture (e.g., a small BERT variant) and validate that the estimated LLC is consistent with theoretical expectations or full-gradient methods.
2. **Hyperparameter Sensitivity**: Systematically vary SGLD hyperparameters (step size, batch size, confinement strength) across multiple runs and architectures to quantify their impact on LLC estimates and identify robust settings.
3. **Non-Convexity Robustness**: Evaluate the estimator on a non-linear, non-convex architecture (e.g., a ResNet) to assess whether the LLC estimates remain meaningful and stable in the presence of complex loss landscapes.