---
ver: rpa2
title: A Training Data Recipe to Accelerate A* Search with Language Models
arxiv_id: '2407.09985'
source_url: https://arxiv.org/abs/2407.09985
tags:
- search
- heuristic
- training
- learning
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how to efficiently learn heuristic functions
  for A search using large language models (LLMs). The key insight is that both A
  and LLM learning benefit from focusing training data on nodes near the goal state.
---

# A Training Data Recipe to Accelerate A* Search with Language Models

## Quick Facts
- arXiv ID: 2407.09985
- Source URL: https://arxiv.org/abs/2407.09985
- Authors: Devaansh Gupta; Boyang Li
- Reference count: 36
- Primary result: Up to 15× reduction in search iterations and 5× wall-clock speedup for A* search using LLM-learned heuristics with planner-aware training data sampling

## Executive Summary
This paper addresses the challenge of learning effective heuristic functions for A* search using large language models. The key insight is that both A* search and LLM training benefit from focusing on nodes near the goal state. The authors develop a planner-aware sampling strategy that prioritizes these nodes, which can be combined with existing coreset selection methods. Their approach achieves significant improvements in search efficiency across three domains (maze, Sokoban, sliding tile puzzles) while requiring fewer training samples than full-data approaches.

## Method Summary
The authors propose a planner-aware sampling strategy for training LLMs to predict heuristic values for A* search. The method involves training an LLM to predict residual distances by sampling nodes from solved problems, with a focus on nodes near the goal. The sampling strategy uses a distribution D(n,τ) that exponentially favors nodes closer to the goal, with temperature parameter τ controlling the degree of prioritization. This sampling can be combined with existing coreset selection methods through a union-based resampling approach. The trained LLM serves as a heuristic function that can be integrated into A* search, providing significant improvements in search efficiency while maintaining solution optimality.

## Key Results
- Up to 15× reduction in search iterations compared to uniform sampling baselines
- Up to 5× wall-clock speedup in A* search with LLM heuristics
- Improved sample efficiency with planner-aware sampling reducing required training data
- Maintained or improved solution optimality across all tested domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Nodes closer to the goal are more valuable for both LLM training and A* search efficiency
- **Mechanism**: The paper demonstrates that accurate heuristic predictions near the goal have outsized impact on search performance, while also being easier for LLMs to learn due to simpler distance-to-goal relationships
- **Core assumption**: The relationship between node position and learning difficulty/generalization is monotonic and consistent across domains
- **Evidence anchors**:
  - [abstract]: "A* requires more accurate predictions on search nodes near the goal, and LLMs need the same set of nodes for effective generalisation"
  - [section]: "If we can only minimize errors of the heuristic function on one section of the search trajectory, we should choose the end section, which is closest to the goal"

### Mechanism 2
- **Claim**: Planner-aware sampling can be combined with existing coreset selection methods to improve performance
- **Mechanism**: The proposed algorithm first samples nodes using any baseline method, then resamples from the union with D(n,τ) sampling, giving nodes appearing in both sets double weight
- **Core assumption**: The union-based resampling approach maintains diversity while prioritizing goal-proximal nodes
- **Evidence anchors**:
  - [section]: "This procedure is summarised in Algorithm 2" and "SD augmented with D(n, τ) outperforms all other methods"
  - [abstract]: "Our technique outperforms uniform pruning and existing baselines in extensive experiments"

### Mechanism 3
- **Claim**: Large language models can serve as effective heuristic functions when trained on properly sampled data
- **Mechanism**: The LLM learns to predict residual distances by training on node-distance pairs from solved problems, with the sampling strategy ensuring optimal training distribution
- **Core assumption**: LLMs can effectively learn regression tasks for heuristic prediction when provided with appropriate data distribution
- **Evidence anchors**:
  - [section]: "Our goal is train a language model θ, that, given a node n, can predict the residual d∗(n) = h∗(n) − h(n)"
  - [abstract]: "Our technique reduces the number of iterations required to find the solutions by up to 15×, with a wall-clock speed-up of search up to 5×"

## Foundational Learning

- **Concept: A* Search Algorithm**
  - Why needed here: Understanding A* is fundamental to grasping why heuristic learning matters and how the proposed sampling strategy works
  - Quick check question: What condition must a heuristic satisfy to guarantee A* finds optimal solutions?

- **Concept: Heuristic Learning for Planning**
  - Why needed here: The paper builds on previous work in learning heuristics, so understanding this field's foundations is crucial
  - Quick check question: How do traditional neural network approaches to heuristic learning differ from the LLM-based approach proposed here?

- **Concept: Coreset Selection**
  - Why needed here: The paper's core contribution involves selecting optimal training subsets, which is a specific application of coreset selection
  - Quick check question: What is the primary goal of coreset selection in machine learning, and how does it apply to this heuristic learning context?

## Architecture Onboarding

- **Component map**: Problem domain generators -> A* with oracle heuristic -> Training data sampling -> LLM training -> A* with LLM heuristic -> Evaluation metrics
- **Critical path**: Data generation → A* with oracle → Training data sampling → LLM training → A* with LLM heuristic → Performance evaluation
- **Design tradeoffs**:
  - Sampling temperature τ vs. training data difficulty
  - LLM model size vs. overfitting risk in low-data regime
  - Sampling strategy complexity vs. performance gains
  - Wall-clock time for LLM inference vs. search efficiency
- **Failure signatures**:
  - High validation error despite proper sampling → Model architecture or training configuration issue
  - No improvement in ILR despite better LLM performance → A* implementation bug or incorrect heuristic integration
  - ITR < 1 despite good ILR → LLM inference overhead too high relative to search improvements
- **First 3 experiments**:
  1. Run A* with oracle heuristic on maze domain to verify basic functionality and collect training data
  2. Train LLM with uniform sampling on maze domain and verify it can predict distances
  3. Implement planner-aware sampling with D(n,τ) and compare performance against uniform sampling on maze domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed planner-aware sampling strategy perform on domains beyond classical puzzles (maze, Sokoban, sliding tile puzzles)?
- Basis in paper: [explicit] The authors acknowledge their study is restricted to classical puzzle domains and suggest experimental verification would be necessary to confirm generalization to other problems.
- Why unresolved: The current experiments are limited to three specific domains, leaving uncertainty about the method's effectiveness on more complex or different types of planning problems.
- What evidence would resolve it: Conducting experiments on diverse planning domains such as robotic path planning, logistics, or scheduling problems would provide empirical evidence of the method's generalizability.

### Open Question 2
- Question: What is the optimal balance between data difficulty and quantity for training LLM heuristics in the low-data regime?
- Basis in paper: [inferred] The authors observe that larger models cause overfitting in the low-data regime and note that studying the effects of scaling up data with parameters is left for future work.
- Why unresolved: The paper does not explore the trade-off between model size and training data quantity, nor does it investigate optimal data sampling strategies that balance difficulty and coverage.
- What evidence would resolve it: Systematic experiments varying model sizes, training data quantities, and sampling strategies would help identify the optimal balance for effective heuristic learning.

### Open Question 3
- Question: How does the choice of C(n) function affect the performance of the planner-aware sampling strategy, and what is the theoretically justified choice?
- Basis in paper: [explicit] The authors discuss that while any monotonically increasing function can theoretically be used for C(n), practical considerations limit the choices, and they perform ablation studies with different functions.
- Why unresolved: The paper uses log(|π*|/(|π*|-g(n))) as the default choice for C(n) but acknowledges that choosing the best performing or most theoretically justified one is left for future works.
- What evidence would resolve it: A comprehensive analysis comparing various C(n) functions on multiple domains and their theoretical justifications would help determine the optimal choice.

## Limitations
- Limited to three classical puzzle domains (maze, Sokoban, sliding tile puzzles)
- Computational overhead of LLM inference during search not fully characterized
- Temperature parameter τ requires careful tuning and may not scale well across diverse problem spaces

## Confidence
- **High confidence**: Core claim that planner-aware sampling improves LLM training for heuristics
- **Medium confidence**: 5× wall-clock speedup claim due to potential variability in implementation efficiency
- **Medium confidence**: Scalability claims beyond the tested domains and problem sizes

## Next Checks
1. Test the sampling strategy on additional planning domains (e.g., blocks world, logistics) to verify domain generalization
2. Conduct a thorough analysis of LLM inference overhead vs. search time savings across different hardware configurations
3. Perform systematic hyperparameter sensitivity analysis for the temperature parameter τ and sampling budget α