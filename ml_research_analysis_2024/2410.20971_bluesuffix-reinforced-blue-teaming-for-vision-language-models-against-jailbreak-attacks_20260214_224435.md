---
ver: rpa2
title: 'BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak
  Attacks'
arxiv_id: '2410.20971'
source_url: https://arxiv.org/abs/2410.20971
tags:
- defense
- arxiv
- jailbreak
- purifier
- suffix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes BlueSuffix, a black-box defense framework
  for Vision-Language Models (VLMs) against jailbreak attacks. The method combines
  three components: a diffusion-based image purifier, an LLM-based text purifier,
  and a reinforcement-learning fine-tuned blue-team suffix generator.'
---

# BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks

## Quick Facts
- arXiv ID: 2410.20971
- Source URL: https://arxiv.org/abs/2410.20971
- Reference count: 14
- Key outcome: BlueSuffix reduces multimodal jailbreak attack success rates by ~70% on open-source VLMs and ~50% on commercial VLMs while maintaining benign input performance

## Executive Summary
BlueSuffix introduces a novel black-box defense framework that protects Vision-Language Models (VLMs) from multimodal jailbreak attacks through a three-component approach. The system combines diffusion-based image purification, LLM-based text rewriting, and reinforcement learning fine-tuned blue-team suffix generation. Tested against state-of-the-art bimodal adversarial prompts, BlueSuffix demonstrates substantial reduction in Attack Success Rate across 13 safety categories while preserving performance on benign inputs, showing strong transferability and robustness against adaptive attacks.

## Method Summary
BlueSuffix employs a three-stage defense pipeline: first, a diffusion-based image purifier removes adversarial perturbations from jailbreak images; second, an LLM-based text purifier rewrites malicious prompts to be more explicit and identifiable as harmful; third, a reinforcement learning fine-tuned GPT-2 suffix generator produces safety-enhancing suffixes that are appended to the purified input. The suffix generator is optimized using PPO to maximize safety scores from an LLM judge while maintaining alignment with a reference policy through KL divergence penalties.

## Key Results
- Achieves ~70% reduction in Attack Success Rate on open-source VLMs (LLaVA, MiniGPT-4) and ~50% on commercial VLMs (Gemini)
- Maintains high Benign Passing Rate with only 3.60% drop on benign inputs
- Demonstrates strong transferability across unseen datasets and robustness against adaptive attacks

## Why This Works (Mechanism)

### Mechanism 1
The blue-team suffix generator uses reinforcement learning to optimize for safety scores given by an LLM judge, thereby enhancing cross-modal robustness. The suffix generator fine-tunes a lightweight LLM (GPT-2) to maximize the expected safety score (1 for benign, 0 for harmful) given by an LLM judge (e.g., GPT-4o or Llama 3). The optimization uses reinforcement learning with a KL divergence penalty to prevent mode collapse and maintain alignment with the reference policy.

### Mechanism 2
The diffusion-based image purifier effectively removes adversarial perturbations from jailbreak images, reducing their ability to trigger harmful responses. The image purifier uses a diffusion model to iteratively denoise the adversarial image. The diffusion process progressively corrupts the image with noise, and the reverse diffusion process iteratively removes the noise to recover a clean sample.

### Mechanism 3
The LLM-based text purifier rewrites jailbreak prompts to make them more detailed and explicit, enabling the target VLM to identify harmful content without altering the original meaning. The text purifier uses an LLM (e.g., GPT-4o or Llama 3) to rewrite the adversarial textual prompt by adding more detailed descriptions and emphasizing potential risks.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) and their vulnerabilities to jailbreak attacks.
  - Why needed here: Understanding how VLMs process multimodal inputs and their susceptibility to adversarial attacks is crucial for designing effective defense mechanisms.
  - Quick check question: What are the key components of a typical VLM architecture, and how do jailbreak attacks exploit their vulnerabilities?

- Concept: Reinforcement learning and its application in training LLM-based suffix generators.
  - Why needed here: The blue-team suffix generator relies on reinforcement learning to optimize for safety scores, requiring knowledge of RL algorithms and their implementation.
  - Quick check question: How does the KL divergence penalty in the RL objective help prevent mode collapse and maintain alignment with the reference policy?

- Concept: Diffusion models and their use in image denoising and purification.
  - Why needed here: The image purifier uses a diffusion model to remove adversarial perturbations, necessitating an understanding of diffusion processes and their application in image processing.
  - Quick check question: How does the reverse diffusion process iteratively remove noise from the corrupted image to recover a clean sample?

## Architecture Onboarding

- Component map: Jailbreak Input -> Diffusion-based Image Purifier -> LLM-based Text Purifier -> GPT-2 Suffix Generator -> LLM Judge -> Target VLM
- Critical path:
  1. Receive multimodal jailbreak prompts (image and text)
  2. Purify the image using the diffusion-based image purifier
  3. Rewrite the text using the LLM-based text purifier
  4. Generate a blue-team suffix using the suffix generator
  5. Append the suffix to the rewritten text and input the purified multimodal prompts into the target VLM
  6. Evaluate the VLM's response using the LLM judge and update the suffix generator's reward signal

- Design tradeoffs:
  - Unimodal vs. bimodal defense: Unimodal defenses are simpler but may not fully exploit cross-modal information
  - Model-agnostic vs. model-specific defense: Model-agnostic defenses are more flexible but may not be as effective
  - Detection vs. mitigation: Detection-based defenses are simpler but may have higher false positive rates

- Failure signatures:
  - High ASR on benign prompts: Indicates overly aggressive defense blocking legitimate requests
  - Low ASR reduction on jailbreak prompts: Suggests ineffective mitigation
  - Degradation in performance on benign inputs: Implies unwanted side effects on normal functioning

- First 3 experiments:
  1. Evaluate ASR reduction on jailbreak prompts using only the image purifier
  2. Evaluate ASR reduction on jailbreak prompts using only the text purifier
  3. Evaluate ASR reduction on jailbreak prompts using the full BlueSuffix defense

## Open Questions the Paper Calls Out

### Open Question 1
How does the cross-modal optimization strategy balance maximizing safety scores with preserving the model's original capabilities on benign inputs? The paper mentions the suffix generator is fine-tuned to maximize expected safety scores while incorporating a KL divergence penalty, but doesn't provide detailed analysis of this trade-off beyond reporting a 3.60% drop in Benign Passing Rate.

### Open Question 2
Can the defense framework be extended to handle more complex multimodal jailbreak attacks that combine multiple modalities beyond just text and images? The current framework is specifically designed for text and image inputs, and extending it to handle additional modalities would require significant architectural modifications.

### Open Question 3
How does the effectiveness of the defense vary across different types of harmful content, and can the system be adapted to handle context-specific safety concerns? The paper shows varying effectiveness across different jailbreak categories but doesn't analyze why these differences exist or how the defense might be adapted for different types of harmful content.

## Limitations
- Heavy dependence on LLM judge reliability for reinforcement learning reward signals
- Unclear mechanism for how text-only suffixes achieve cross-modal robustness
- Significant computational overhead from multiple purification and evaluation steps

## Confidence

**High Confidence**: Core methodology of combining multiple purification layers with reinforcement-learned suffix generation is well-specified, with consistently demonstrated ASR reductions across different benchmarks.

**Medium Confidence**: Transferability claims to unseen datasets are supported but evaluation scope is limited; adaptive attack robustness is demonstrated but attack space explored is relatively narrow.

**Low Confidence**: Exact mechanism of cross-modal robustness achievement is not fully explained; dependence on LLM judge reliability is not thoroughly validated across all safety categories.

## Next Checks

1. **Judge Reliability Audit**: Systematically evaluate GPT-4o judge's consistency and potential biases across all 13 safety categories to establish confidence thresholds for reliable reward signals.

2. **Cross-Modal Attack Battery**: Develop and test adaptive attacks specifically designed to probe cross-modal robustness claims, including attacks that exploit interaction between purified image and text components.

3. **Resource Overhead Characterization**: Measure end-to-end latency and computational costs of BlueSuffix compared to baseline defenses, evaluating performance under different hardware constraints.