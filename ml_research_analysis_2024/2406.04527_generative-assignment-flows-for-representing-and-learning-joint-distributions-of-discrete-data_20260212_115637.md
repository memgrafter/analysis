---
ver: rpa2
title: Generative Assignment Flows for Representing and Learning Joint Distributions
  of Discrete Data
arxiv_id: '2406.04527'
source_url: https://arxiv.org/abs/2406.04527
tags:
- flow
- assignment
- distribution
- which
- distributions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel generative model for representing
  and learning joint probability distributions of discrete random variables. The approach
  uses measure transport by randomized assignment flows on the statistical submanifold
  of factorizing distributions, enabling efficient representation and sampling from
  any target distribution and likelihood assessment of unseen data points.
---

# Generative Assignment Flows for Representing and Learning Joint Distributions of Discrete Data

## Quick Facts
- arXiv ID: 2406.04527
- Source URL: https://arxiv.org/abs/2406.04527
- Authors: Bastian Boll; Daniel Gonzalez-Alvarado; Stefania Petra; Christoph Schnörr
- Reference count: 9
- Key outcome: KL divergence of 0.12-0.14 for 160 classes on structured image labelings

## Executive Summary
This paper introduces a novel generative model for discrete joint probability distributions using measure transport on a statistical submanifold of factorizing distributions. The approach employs randomized assignment flows and conditional Riemannian flow matching to efficiently represent and sample from complex distributions without simulation-based training objectives. The method demonstrates superior scaling to many classes compared to recent normalizing flow approaches for discrete data.

## Method Summary
The generative model represents discrete joint distributions by embedding factorizing distributions into a meta-simplex and using measure transport via assignment flows. Training is performed through conditional Riemannian flow matching, where geodesics on the assignment manifold encode training data. The model uses a UNet to parameterize the affinity function of the assignment flow system, and employs geometric numerical integration for sampling and evaluation.

## Key Results
- Achieves KL divergence of 0.12-0.14 for 160 classes on structured image labelings
- Outperforms Dirichlet flow matching and linear flow matching baselines in scaling to many classes
- Demonstrates computational efficiency by working with submanifold of factorizing distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The generative model represents discrete joint distributions using measure transport on a submanifold of factorizing distributions, enabling efficient representation and sampling.
- Mechanism: The approach embeds the assignment manifold into the meta-simplex of all joint distributions. This embedding maps factorizing distributions to the submanifold, allowing the model to represent complex distributions via convex combinations of these factorized points. The assignment flow then transports a simple reference measure on the manifold to approximate the target distribution.
- Core assumption: Factorizing distributions form a tractable submanifold within the high-dimensional space of all discrete joint distributions.
- Evidence anchors:
  - [abstract] "The approach uses measure transport by randomized assignment flows on the statistical submanifold of factorizing distributions"
  - [section 2.2] "The embedding (1.13) yields the submanifold of factorizing discrete distributions in ∆N"
  - [corpus] Weak evidence; no direct corpus support for this specific embedding approach
- Break condition: If the statistical dependencies in the target distribution cannot be well-approximated by convex combinations of factorizing distributions, or if the embedding map does not preserve key geometric properties needed for flow matching.

### Mechanism 2
- Claim: Training is performed via Riemannian flow matching using conditional probability paths, avoiding the need for sampling-based training objectives.
- Mechanism: The model learns by matching the assignment flow vector field to conditional vector fields that encode training data as geodesics on the assignment manifold. These conditional fields generate paths that converge to Dirac measures at training points in the limit. This simulation-free approach avoids the instability of direct density estimation on discrete spaces.
- Core assumption: Conditional probability paths can be constructed in closed form and effectively encode the training data distribution.
- Evidence anchors:
  - [section 3.2.2] "the marginal probability path νt represents the target data distribution p in the limit t → ∞"
  - [section 3.2.3] "The conditional vector field that generates (3.27) is given by ut(W |β) = RW [λVβ]"
  - [corpus] Moderate evidence; similar flow matching approaches exist but not specifically for discrete distributions
- Break condition: If the conditional paths do not adequately represent the training distribution, or if the flow matching objective fails to converge to meaningful parameters.

### Mechanism 3
- Claim: The approach scales effectively to many classes by using infinite integration time and gradual probability mass transport.
- Mechanism: By choosing infinite integration time and appropriate rate parameters λ, the model avoids pathological early class decisions that plague finite-time approaches. The conditional probability paths maintain full support on the manifold throughout integration, allowing gradual refinement of class assignments over time.
- Core assumption: Gradual probability mass transport prevents early commitment to incorrect class assignments, particularly in high-dimensional settings.
- Evidence anchors:
  - [section 3.2.4] "by opting for large integration time t → ∞ and a corresponding construction (3.27) of conditional probability paths, our approach is able to scale to many classes c ≫ 1"
  - [section 4.1] "Our proposed approach (red) is able to outperform... Dirichlet flow matching [SJW+24] in terms of scaling to many classes c"
  - [corpus] Strong evidence; directly compares to related work showing superior scaling
- Break condition: If the gradual transport is too slow to be practical, or if numerical precision limits prevent accurate long-time integration.

## Foundational Learning

- Concept: Information geometry and the Fisher-Rao metric
  - Why needed here: The approach relies on the Riemannian structure of the assignment manifold to define meaningful flow dynamics and geodesics that encode probability distributions
  - Quick check question: How does the Fisher-Rao metric differ from the Euclidean metric when measuring distances between probability distributions?

- Concept: Normalizing flows and measure transport
  - Why needed here: The generative model fundamentally uses measure transport to push forward a simple reference distribution to the complex target distribution
  - Quick check question: What is the relationship between the Jacobian determinant of the transport map and the change in probability density?

- Concept: Algebraic statistics and the geometry of discrete distributions
  - Why needed here: Understanding the structure of factorizing distributions and their embedding in the space of all joint distributions is crucial for the approach
- Quick check question: Why are factorizing distributions represented as points on a submanifold rather than filling the entire space of joint distributions?

## Architecture Onboarding

- Component map: Assignment flow ODE -> Embedding map -> Flow matching loss -> Parameter update -> Convergence check
- Critical path: Training involves computing conditional vector fields → evaluating the flow matching loss → updating parameters → checking convergence. Sampling involves drawing from the reference distribution → integrating the learned assignment flow → extracting the resulting distribution.
- Design tradeoffs: Using infinite integration time provides better scaling but requires more computation per sample. The embedding approach enables efficient representation but may limit expressiveness if the target distribution is too far from the submanifold of factorizing distributions.
- Failure signatures: Poor training convergence may indicate issues with the conditional paths or flow matching objective. Sampling artifacts may indicate numerical instability in long-time integration or insufficient support of the reference distribution.
- First 3 experiments:
  1. Verify the embedding map correctly represents factorizing distributions on a small toy example (e.g., two binary variables)
  2. Test flow matching training on a simple discrete distribution with known solution
  3. Evaluate sampling quality and likelihood computation on a small-scale problem before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed approach scale to extremely high-dimensional joint distributions (e.g., with thousands of random variables)?
- Basis in paper: [inferred] The paper mentions that the approach is computationally efficient due to working with the submanifold of factorizing distributions, but does not provide explicit scalability analysis for very high dimensions.
- Why unresolved: The paper focuses on demonstrating performance with up to 160 classes and does not explore the limits of the approach with respect to the number of random variables.
- What evidence would resolve it: Experiments evaluating the approach on joint distributions with hundreds or thousands of random variables, measuring performance and computational requirements.

### Open Question 2
- Question: Can the proposed approach handle non-uniform prior distributions on the assignment manifold?
- Basis in paper: [explicit] The paper assumes a simple reference measure on the assignment manifold, but does not discuss the impact of using different prior distributions.
- Why unresolved: The choice of reference measure could affect the learned generative model and its ability to represent complex joint distributions.
- What evidence would resolve it: Experiments comparing the performance of the approach with different prior distributions on the assignment manifold.

### Open Question 3
- Question: How does the proposed approach compare to other generative models for discrete data, such as variational autoencoders (VAEs) or generative adversarial networks (GANs)?
- Basis in paper: [explicit] The paper focuses on comparing the approach to recent related work on normalizing flows for discrete data, but does not provide a comprehensive comparison to other generative models.
- Why unresolved: A thorough comparison to other generative models would provide a better understanding of the strengths and weaknesses of the proposed approach.
- What evidence would resolve it: Experiments evaluating the performance of the proposed approach against VAEs and GANs on various discrete data sets.

## Limitations
- The embedding into a submanifold of factorizing distributions may limit expressiveness for highly structured or long-range dependent data.
- Simulation-free training requires closed-form conditional probability paths that may not generalize to all discrete structures.
- Numerical precision challenges increase for many classes due to long integration times and probability constraint maintenance.

## Confidence

- **High confidence**: The geometric framework and embedding approach are well-grounded in information geometry theory. The superiority over Dirichlet flow matching in scaling to many classes is demonstrated empirically.
- **Medium confidence**: The simulation-free training via conditional Riemannian flow matching is theoretically sound but lacks extensive empirical validation across diverse discrete distributions. The closed-form expressions for conditional paths may not hold for all problem structures.
- **Medium confidence**: The computational advantages for large-scale problems are demonstrated on image labeling tasks but require validation on other discrete data types with different dependency structures.

## Next Checks

1. Test the model's ability to capture long-range dependencies in discrete sequences (e.g., text or molecular structures) where factorizing distributions may be insufficient.
2. Compare training stability and convergence rates between the simulation-free approach and sampling-based alternatives across different discrete distribution families.
3. Evaluate numerical precision requirements and potential breakdown points for integration time and parameter λ when scaling to hundreds or thousands of classes.