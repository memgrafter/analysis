---
ver: rpa2
title: 'M3TCM: Multi-modal Multi-task Context Model for Utterance Classification in
  Motivational Interviews'
arxiv_id: '2404.03312'
source_url: https://arxiv.org/abs/2404.03312
tags:
- therapist
- classification
- utterance
- client
- m3tcm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of accurately classifying utterances
  in motivational interviews, which are crucial for understanding the quality and
  dynamics of client-therapist interactions. The authors present M3TCM, a Multi-modal
  Multi-task Context Model for utterance classification that employs multi-task learning
  to model both therapist and client behavior simultaneously.
---

# M3TCM: Multi-modal Multi-task Context Model for Utterance Classification in Motivational Interviews

## Quick Facts
- arXiv ID: 2404.03312
- Source URL: https://arxiv.org/abs/2404.03312
- Reference count: 0
- Primary result: Achieves 0.66 F1 (client) and 0.83 F1 (therapist) on AnnoMI dataset, outperforming previous approaches by 20% and 15% relative improvement respectively

## Executive Summary
This paper addresses utterance classification in motivational interviews through M3TCM, a Multi-modal Multi-task Context Model that leverages multi-task learning to simultaneously model therapist and client behavior patterns. The model integrates text and speech modalities using RoBERTa and AST embeddings, while incorporating conversation context through a shared self-attention layer. In evaluations on the AnnoMI dataset, M3TCM demonstrates significant improvements over previous approaches, achieving 0.66 F1 for client and 0.83 F1 for therapist utterance classification.

## Method Summary
M3TCM employs a multi-task learning framework with shared self-attention processing sequences of k utterances from both therapist and client roles. The model uses RoBERTa Large for text embeddings and Audio Spectrogram Transformer (AST) for audio embeddings, concatenating these modalities before feeding them to the shared self-attention layer. Separate classification networks handle therapist and client tasks respectively. Training uses 5-fold cross-validation with focal loss to address class imbalance, optimized with AdamW (learning rate 1e-5).

## Key Results
- Achieves 0.66 F1 score for client utterance classification (20% relative improvement over previous approaches)
- Achieves 0.83 F1 score for therapist utterance classification (15% relative improvement over previous approaches)
- Context modeling with 10-utterance windows provides optimal performance, with clear increases observed when expanding from 1 to 10 utterances

## Why This Works (Mechanism)

### Mechanism 1
Multi-task learning with shared self-attention enables joint modeling of therapist and client behavior patterns by processing both utterance types simultaneously through a shared self-attention layer. This allows the model to learn cross-role dependencies and shared conversational dynamics that improve classification accuracy for both roles.

### Mechanism 2
Multi-modal fusion of text and audio embeddings captures both semantic content and emotional/prosodic cues essential for MI utterance classification. Text embeddings from RoBERTa capture semantic meaning while audio embeddings from AST capture prosodic features like pitch, tempo, and volume changes, creating a richer representation.

### Mechanism 3
Conversation context modeling with extended input windows (10 utterances) significantly improves classification accuracy by capturing longer-term conversational dependencies and topic evolution that are crucial for understanding utterance meaning in context.

## Foundational Learning

- Concept: Multi-task learning and shared representations
  - Why needed here: Enables the model to learn both therapist and client classification simultaneously while sharing common conversational patterns
  - Quick check question: What is the key architectural difference between single-task and multi-task learning in this model?

- Concept: Attention mechanisms and sequence modeling
  - Why needed here: The self-attention layer captures dependencies between utterances across the conversation timeline
  - Quick check question: How does self-attention differ from simple concatenation of utterance embeddings?

- Concept: Multi-modal fusion techniques
  - Why needed here: Combines text and audio information to capture both semantic content and prosodic/emotional cues
  - Quick check question: What are the advantages of concatenating embeddings versus using attention-based fusion?

## Architecture Onboarding

- Component map: Input layer (k therapist + k client utterances) → Embedding layer (RoBERTa + AST) → Shared Self-Attention → Task-specific Classification → Output layer
- Critical path: Text/Audio → Embeddings → Shared Self-Attention → Task-specific Classification
- Design tradeoffs: Single-task vs multi-task (simpler but misses cross-role patterns), context window size (larger captures more context but increases computational cost), embedding fusion method (concatenation is simple but may lose interaction information)
- Failure signatures: Poor performance on minority classes (may indicate class imbalance), similar performance for single-task vs multi-task (suggests insufficient shared patterns), no improvement with context windows > 3 (may indicate context modeling isn't capturing useful dependencies)
- First 3 experiments: 1) Compare single-task vs multi-task learning performance, 2) Test different context window sizes (1, 3, 5, 10 utterances), 3) Evaluate text-only vs audio-only vs multi-modal performance

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal context size for utterance classification in motivational interviews, and does it vary based on specific utterance types or speaker roles? The study only tested up to 10 utterances without investigating whether different utterance types might benefit from different context sizes.

### Open Question 2
How does the performance of M3TCM compare when using only video modality versus combining it with text and audio modalities? The authors mention video modality as potential future work but didn't test this experimentally.

### Open Question 3
Can the multi-task learning approach used in M3TCM be effectively applied to other conversational domains with asymmetrical roles, such as psychiatric interactions, sales conversations, or teacher-student interactions? While proposed as a possibility, the authors didn't test the approach on any other conversational domains.

## Limitations

- Limited generalizability due to evaluation on single AnnoMI dataset without external validation on alternative MI datasets or other therapeutic conversation types
- Optimal architecture choices (self-attention configuration, fusion method) are empirically determined but lack theoretical justification for why these specific choices work best
- Multi-modal fusion approach concatenates embeddings without exploring alternative strategies that might capture more complex interactions between modalities

## Confidence

- High Confidence: Multi-task learning framework with shared self-attention layer demonstrates consistent performance improvements over single-task baselines
- Medium Confidence: Multi-modal fusion approach's effectiveness is supported by comparative results, though specific contribution of audio features is not fully quantified
- Low Confidence: Generalizability to other therapeutic contexts or languages remains uncertain due to single-dataset evaluation

## Next Checks

1. Evaluate M3TCM performance on alternative MI datasets (e.g., MITI, AMRITA) or therapeutic conversation datasets from different domains to assess generalizability beyond the AnnoMI corpus

2. Implement and compare attention-based fusion methods versus simple concatenation for combining text and audio embeddings, and test whether modality-specific attention weights improve performance

3. Conduct detailed analysis of what patterns the shared self-attention layer learns by visualizing attention weights and comparing feature representations between therapist and client classification tasks to understand the mechanism behind multi-task benefits