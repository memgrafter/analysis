---
ver: rpa2
title: Autonomous Option Invention for Continual Hierarchical Reinforcement Learning
  and Planning
arxiv_id: '2412.16395'
source_url: https://arxiv.org/abs/2412.16395
tags:
- options
- learning
- state
- abstract
- option
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for autonomous option invention
  in continual reinforcement learning settings. The core idea is to leverage learned
  state abstractions, represented as Conditional Abstraction Trees (CATs), to identify
  significant changes in context-specific abstractions as cues for defining option
  endpoints.
---

# Autonomous Option Invention for Continual Hierarchical Reinforcement Learning and Planning

## Quick Facts
- arXiv ID: 2412.16395
- Source URL: https://arxiv.org/abs/2412.16395
- Authors: Rashmeet Kaur Nayyar; Siddharth Srivastava
- Reference count: 12
- Primary result: Autonomous option invention using learned state abstractions outperforms state-of-the-art RL baselines in sample efficiency across challenging domains

## Executive Summary
This paper introduces CHiRP, a method for autonomous option invention in continual reinforcement learning settings that leverages learned state abstractions to identify significant changes in context-specific abstractions as cues for defining option endpoints. The approach automatically invents options with symbolic representations that satisfy composability, reusability, and mutual independence, enabling efficient hierarchical planning and knowledge transfer across tasks. By interleaving planning with learning and using a search process to compose options while inventing new option signatures to bridge gaps, CHiRP demonstrates substantially better sample efficiency than state-of-the-art RL baselines while requiring fewer hyperparameters and less tuning.

## Method Summary
CHiRP operates by maintaining a universal CAT (Conditional Abstraction Tree) across tasks, using CAT+RL to learn and refine state abstractions and option policies, and employing an option inventor that creates new abstract options based on context-specific abstraction changes. The planner composes options using A* search over option endpoints, augmented with lifted transitions at higher abstraction levels. The method identifies option endpoints by detecting significant changes in context-specific abstractions, creates symbolic option signatures for composability and reuse, and ensures mutual independence through distinct variable sets affected by each option. Knowledge is transferred across tasks by maintaining and updating the universal CAT, allowing learned options to be reused in new problem instances.

## Key Results
- CHiRP demonstrates substantially better sample efficiency than state-of-the-art RL baselines across diverse challenging domains
- The approach requires fewer hyperparameters and less tuning while effectively learning and transferring abstract knowledge
- Invented options satisfy key desiderata: composability for planning, reusability across tasks, and mutual independence to reduce interference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-specific abstractions are used to identify when to start and end options by detecting significant changes in the Q-function structure.
- Mechanism: The algorithm generates Context-Specific CATs (C-CATs) conditioned on context-variables, then measures structural differences between consecutive C-CATs using a context-specific distance function. When this distance exceeds a threshold, it signals a significant change in the task's nature, marking an option endpoint.
- Core assumption: Variables that remain relatively stable (low frequency of change) but have high refinement in the CAT are the best indicators of context changes that warrant new options.
- Evidence anchors:
  - [abstract] "Our approach continually learns and maintains an interpretable state abstraction, and uses it to invent high-level options with abstract symbolic representations."
  - [section] "The core idea is to capture notions of context-specific abstractions that depend on and change with the current state by identifying salient variable values responsible for greatest variation in the Q-function, and to use changes in these abstractions as a cue for defining option endpoints."
  - [corpus] Weak - neighboring papers focus on hierarchical options but don't discuss CAT-based context-specific abstractions for endpoint detection.
- Break condition: If context-variables don't exist (all variables change frequently) or the threshold tuning fails to capture meaningful task transitions.

### Mechanism 2
- Claim: Symbolic option signatures enable composability and reuse by providing interpretable initiation and termination conditions that can be efficiently searched and combined.
- Mechanism: Options are represented as ⟨Io, βo⟩ pairs where Io and βo are sets of abstract states. The planner uses A* search over these symbolic endpoints, augmented with lifted transitions at higher abstraction levels, to compose plans that bridge learned options with new option signatures when needed.
- Core assumption: Symbolic representations of option endpoints are sufficient for effective hierarchical planning without requiring low-level policy execution during planning.
- Evidence anchors:
  - [abstract] "These options meet three key desiderata: (1) composability for solving tasks effectively with lookahead planning, (2) reusability across problem instances for minimizing the need for relearning, and (3) mutual independence for reducing interference among options."
  - [section] "The invented options have symbolic abstract descriptions that directly support composability and reusability through high-level planning."
  - [corpus] Weak - neighboring papers discuss hierarchical options but don't emphasize symbolic representations for planning efficiency.
- Break condition: If the symbolic representation becomes too coarse to distinguish between useful option compositions, or if the search space explodes due to too many option signatures.

### Mechanism 3
- Claim: Mutual independence of options is achieved through distinct sets of variables and value ranges affected by each option, reducing interference and enabling independent learning.
- Mechanism: The option invention process identifies option endpoints based on changes in context-specific abstractions, and each option maintains its own encapsulated CAT-based state abstraction. This ensures that options primarily affect different state variables and value ranges, with minimal overlap in their core affected variables.
- Core assumption: Task decomposition naturally leads to options affecting distinct variable sets, and the CAT structure can capture these differences effectively.
- Evidence anchors:
  - [abstract] "These options meet three key desiderata: (1) composability, (2) reusability, and (3) mutual independence for reducing interference among options."
  - [section] "The options have stronger effects on different sets of variables and/or values, which reduces mutual interference."
  - [corpus] Weak - neighboring papers don't discuss mutual independence as a design criterion for option invention.
- Break condition: If the learned options end up affecting overlapping variable sets despite the independence design goal, or if the abstraction refinement doesn't capture variable-specific effects adequately.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and factored state representations
  - Why needed here: The paper operates on goal-oriented MDPs with factored states where each state is defined by variable assignments. Understanding this is crucial for grasping how CATs represent abstractions over variable domains.
  - Quick check question: In a factored MDP with variables V = {x, y, l}, where x and y are continuous coordinates and l is discrete, how would you represent the state s = ⟨1.2, 3.4, 2⟩ in terms of variable assignments?

- Concept: Reinforcement Learning with sparse rewards and long horizons
  - Why needed here: The paper specifically targets challenging RL settings with unknown transition functions, sparse rewards, and long effective horizons where standard RL methods struggle. This context explains why hierarchical abstraction approaches are necessary.
  - Quick check question: Why do standard Q-learning or DQN approaches struggle in environments with sparse rewards and long horizons compared to dense-reward, short-horizon tasks?

- Concept: Abstraction hierarchies and temporal abstraction (options)
  - Why needed here: The paper builds on the options framework where temporally extended behaviors are composed to solve tasks hierarchically. Understanding how options work and how they can be composed is fundamental to grasping the proposed approach.
  - Quick check question: What are the three components of an option ⟨Io, βo, πo⟩, and how do they enable temporal abstraction in reinforcement learning?

## Architecture Onboarding

- Component map: New task -> abstract initial/goal states -> computeOptionPlan() -> generate MDPs for needed options -> CAT+RL learning -> inventOptions() -> update universal CAT -> execute plan with active replanning if needed -> transfer learned options to next task
- Critical path: New task → abstract initial/goal states → computeOptionPlan() → generate MDPs for needed options → CAT+RL learning → inventOptions() → update universal CAT → execute plan with active replanning if needed → transfer learned options to next task
- Design tradeoffs: Symbolic vs neural representations (interpretable but potentially less expressive), single vs multiple option invention per task (completeness vs efficiency), context-independent vs context-dependent distance metrics (decomposition quality vs computational cost)
- Failure signatures: Option explosion (too many options created), planner failure to find paths between option endpoints, learned options that don't generalize across tasks, poor state abstraction refinement leading to ineffective option identification
- First 3 experiments:
  1. Implement CAT+RL on a simple grid world to verify state abstraction refinement works before adding option invention
  2. Add basic option invention with fixed thresholds on the grid world to test if meaningful option endpoints are identified
  3. Implement the full planner with option composition on a domain with known hierarchical structure (like taxi) to verify the complete pipeline works end-to-end

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the approach be extended to handle continuous parameterized actions?
- Basis in paper: [explicit] The paper explicitly identifies this as a future research direction, stating "An interesting future research direction is to extend our approach to settings with continuous parameterized actions."
- Why unresolved: The current framework assumes discrete actions, and the paper does not explore how the option invention process would work with continuous action spaces.
- What evidence would resolve it: A modified version of CHiRP that successfully invents options in environments with continuous parameterized actions, demonstrating comparable performance to discrete action settings.

### Open Question 2
- Question: What are the optimality guarantees of the invented options in terms of cumulative reward or task completion time?
- Basis in paper: [inferred] While the paper discusses the desiderata of composability, reusability, and mutual independence, it does not provide theoretical or empirical analysis of the optimality of the invented options.
- Why unresolved: The paper focuses on the invention process and empirical performance, but does not analyze whether the invented options lead to optimal or near-optimal policies.
- What evidence would resolve it: Theoretical proofs or empirical studies comparing the performance of policies using invented options against optimal policies or upper bounds on cumulative reward.

### Open Question 3
- Question: How does the choice of hyperparameters (e.g., δthre, σthre) affect the quality and diversity of invented options across different domains?
- Basis in paper: [explicit] The paper mentions that "These values are robust across domains" and provides default values, but does not conduct an in-depth sensitivity analysis.
- Why unresolved: While the paper provides default values, it does not explore how sensitive the option invention process is to variations in these hyperparameters or how they interact with different domain characteristics.
- What evidence would resolve it: A systematic sensitivity analysis showing how variations in δthre and σthre affect the number, diversity, and performance of invented options across a range of domains with different characteristics.

## Limitations
- Limited empirical validation of CAT-based state abstraction refinement across diverse domains
- Claims about superior sample efficiency lack comprehensive benchmarking against modern deep RL baselines
- Performance in truly lifelong learning settings with significant domain shifts is unclear

## Confidence
- **High**: The theoretical framework connecting context-specific abstractions to option endpoints is well-developed and internally consistent
- **Medium**: The three desiderata (composability, reusability, mutual independence) are theoretically sound, but their practical satisfaction requires empirical validation
- **Low**: Claims about superior sample efficiency compared to state-of-the-art RL baselines lack comprehensive benchmarking against modern methods

## Next Checks
1. Implement ablation studies comparing CAT-based option invention against random option generation and fixed-horizon option termination to isolate the contribution of abstraction-based endpoint detection
2. Test the method's robustness to varying threshold parameters (δthre, σthre) across different domains to assess hyperparameter sensitivity and practical usability
3. Evaluate transfer performance when options are invented in task A but applied to structurally different task B to measure genuine knowledge transfer versus task-specific memorization