---
ver: rpa2
title: 'FROC: Building Fair ROC from a Trained Classifier'
arxiv_id: '2412.14724'
source_url: https://arxiv.org/abs/2412.14724
tags:
- figure
- fairness
- classifier
- fair
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel fairness measure called \u03B5p-Equalized\
  \ ROC, which ensures fairness across all possible classification thresholds. The\
  \ authors propose a post-processing method, FROC, that transforms an unfair classifier's\
  \ output into a probabilistic fair classifier."
---

# FROC: Building Fair ROC from a Trained Classifier

## Quick Facts
- arXiv ID: 2412.14724
- Source URL: https://arxiv.org/abs/2412.14724
- Authors: Avyukta Manjunatha Vummintala; Shantanu Das; Sujit Gujar
- Reference count: 40
- Primary result: Introduces FROC post-processing method achieving ε₁-Equalized ROC fairness with 7-8% fairness improvement and at most 2% accuracy drop

## Executive Summary
This paper introduces FROC, a post-processing method that transforms an unfair classifier's output into a probabilistic fair classifier achieving ε₁-Equalized ROC fairness. The approach approximates ROC curves using threshold queries and transports them within ε distance of each other to minimize AUC loss. Unlike existing fairness methods that enforce equalized odds at a single threshold, FROC ensures fairness across all possible classification thresholds by randomizing scores across feasible classifiers. The method achieves 7-8% improvement in fairness metrics while maintaining at most 2% drop in accuracy compared to existing approaches.

## Method Summary
FROC is a post-processing algorithm that takes an unfair classifier's scores and protected attribute labels as input, then constructs a probabilistic fair classifier. The method works by querying the ROC curves at k equidistant thresholds, creating piece-wise linear approximations, and then shifting the ROC points of the less-favored group within the norm boundary of the fairer group until all thresholds satisfy the ε₁ fairness bound. The shifted ROC points are used to construct a probabilistic classifier via randomization over the ROC space. The algorithm is designed to minimize AUC loss while achieving ε₁-Equalized ROC fairness, which bounds the L₁ norm between false positive rates and true positive rates across all thresholds and groups.

## Key Results
- FROC achieves ε₁-Equalized ROC fairness, improving fairness metrics by 7-8% compared to existing methods
- The method maintains at most 2% drop in accuracy while achieving fairness improvements
- Theoretical analysis proves FROC is optimal under certain conditions, with AUC loss bounded and decreasing as threshold queries increase
- Empirical results on Adult and COMPAS datasets demonstrate practical effectiveness of the approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FROC transforms an unfair classifier's ROC curve to achieve ε₁-Equalized ROC without retraining.
- Mechanism: The algorithm approximates the ROC curves for each protected group using k threshold queries, constructs piece-wise linear approximations (PLAs), and then shifts the ROC points of the less-favored group (ROCup) within the norm boundary of the fairer group (ROCdown) until all thresholds satisfy the ε₁ fairness bound. The shifted ROC points are then used to construct a probabilistic classifier via randomization over the ROC space.
- Core assumption: The ROC curves of both protected groups are sufficiently close so that the norm boundary does not collapse to a single point, and the threshold query model with k equidistant points gives a good enough approximation.
- Evidence anchors: [abstract]: "FROC, that transforms an unfair classifier's output into a probabilistic fair classifier." [section 3.3]: "we propose a ROC transport, FROC, a post-processing algorithm (Algorithm 1)." [corpus]: Weak—no direct mention of FROC; neighbor papers discuss fairness in different settings but not threshold-invariant fairness.
- Break condition: If the two ROC curves diverge heavily (large baseline unfairness), the norm boundary becomes too small to contain feasible classifiers, or if Assumption 4.2 (spacing and intersection constraints) is violated.

### Mechanism 2
- Claim: The AUC loss induced by FROC is bounded and decreases with more threshold queries.
- Mechanism: Two sources of AUC loss: (1) PLA approximation loss LP A, bounded as ≤ 1/(2k) uT uF; (2) transport loss LAUC from shifting ROC points. The algorithm chooses optimal transport points to minimize LAUC.
- Core assumption: The slopes of TPR and FPR with respect to thresholds are bounded by constants uT and uF, ensuring LP A → 0 as k → ∞.
- Evidence anchors: [section 4.1]: "Theorem 4.1...LP LA ≤ 1/2 × uT uF /k" [section 4.2]: "Theorem 4.2...under certain theoretical conditions, FROC achieves the theoretical optimal guarantees." [corpus]: Weak—no neighbor papers address AUC loss bounds in fairness post-processing.
- Break condition: If the classifier's TPR/FPR rates change too rapidly with thresholds (unbounded uT or uF), LP A does not vanish and AUC loss grows.

### Mechanism 3
- Claim: The geometric optimality of FROC relies on all optimal fair points lying on the norm boundary.
- Mechanism: By contradiction, any interior point in the norm set can be shifted to a boundary point with lower AUC loss; CutShift, UpShift, and LeftShift operations select the boundary point that minimizes the loss.
- Core assumption: The ROCup can intersect the norm boundary at most twice (Assumption 4.2), and the baseline classifier satisfies spacing constraints (FPRdown_{i-1} ≤ FPRup_i ≤ FPRdown_{i+1}).
- Evidence anchors: [section 4.3]: "Theorem 4.3...all optimally fair points must lie on some Norm Boundary." [section 4.3]: "Theorem 4.4...CutShift operation must be performed." [corpus]: Weak—no neighbor papers discuss geometric optimality proofs in post-processing.
- Break condition: If ROC curves intersect more than twice or the spacing assumption fails, CutShift may not be optimal and FROC may not achieve minimal AUC loss.

## Foundational Learning

- Concept: ROC curves and AUC as threshold-invariant performance metrics.
  - Why needed here: FROC's fairness notion and transport are defined geometrically over ROC curves, not just at a single threshold.
  - Quick check question: Given two classifiers with identical accuracy but different ROC shapes, which one is preferable for a practitioner who needs to adjust thresholds dynamically?

- Concept: Lp norms as fairness metrics.
  - Why needed here: ε₁-Equalized ROC uses L₁ norm to bound differences in TPR/FPR across thresholds and groups.
  - Quick check question: How does using L₁ norm differ from enforcing equalized odds at a single threshold?

- Concept: Post-processing vs. in-processing fairness methods.
  - Why needed here: FROC is a post-processing method; understanding the trade-offs (no retraining, model-agnostic, limited AUC loss) is key to evaluating its practicality.
  - Quick check question: What are the main practical advantages of post-processing over in-processing for fairness?

## Architecture Onboarding

- Component map: Input scores and protected attributes -> Query model (k threshold samples) -> PLA construction -> Transport within norm boundary -> BuildClassifier (randomization) -> Output fair probabilistic classifier
- Critical path: Query -> PLA -> Transport -> BuildClassifier -> Output fair probabilistic classifier
- Design tradeoffs: More threshold queries (larger k) -> smaller LP A but higher computation; stricter ε -> larger AUC loss but better fairness; relying on spacing assumptions -> potential suboptimality if violated
- Failure signatures: (1) ROC curves far apart -> norm boundary too small; (2) Violations of Assumption 4.2 -> suboptimal AUC; (3) Insufficient k -> large LP A; (4) Classifier with very steep ROC -> large LP A regardless of k
- First 3 experiments:
  1. Run FROC with k=100, ε=0.05 on a simple logistic regression trained on Adult dataset; plot original vs. post-processed ROC curves
  2. Vary ε from 0.01 to 0.1; measure fairness improvement (ε₁-Equalized ROC gap) and accuracy drop
  3. Increase k from 50 to 500; confirm LP A decreases and AUC loss stabilizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FROC perform when the ROC curves of different protected groups intersect multiple times, rather than just twice as assumed in the theoretical analysis?
- Basis in paper: [inferred] The authors mention that FROC can still produce fair outputs even when ROC curves intersect more than twice, but note that the optimality theorems do not apply in such scenarios.
- Why unresolved: The theoretical analysis assumes a maximum of two intersections, and the authors explicitly state that the optimality guarantees are not proven for cases with more intersections.
- What evidence would resolve it: Empirical results comparing FROC's performance (in terms of AUC loss and fairness metrics) on datasets where ROC curves intersect multiple times versus the theoretical bounds for the two-intersection case.

### Open Question 2
- Question: What is the impact of the number of query points (k) on the trade-off between PLA loss and computational efficiency in FROC?
- Basis in paper: [explicit] The authors mention that increasing k reduces PLA loss but do not provide specific guidelines or empirical results on the optimal number of query points.
- Why unresolved: While the authors prove that PLA loss decreases with k, they do not explore the practical implications of choosing different values of k or provide empirical evidence on the trade-off between accuracy and computational cost.
- What evidence would resolve it: A systematic study varying k across multiple datasets, showing how PLA loss, computational time, and fairness metrics change with different numbers

## Limitations
- Theoretical guarantees depend on Assumptions 4.1 and 4.2 regarding ROC curve spacing and intersection properties
- The approach assumes bounded slopes for TPR/FPR with respect to thresholds, which may not hold for all classifier types
- When ROC curves intersect more than twice, the optimality theorems no longer apply though the algorithm still produces fair outputs

## Confidence

**Mechanism 1**: High confidence - the algorithm is clearly specified and the post-processing approach is well-defined
**Mechanism 2**: Medium confidence - theoretical bounds are proven but depend on assumptions that may not always hold in practice
**Mechanism 3**: Medium confidence - optimality proofs are conditional on spacing and intersection assumptions that require validation

## Next Checks

1. **Assumption validation**: Test FROC on datasets where Assumption 4.2 (ROC intersection at most twice) is violated to measure real-world performance degradation
2. **Bounded slope verification**: Quantify the actual bounds uT and uF for different classifier types to validate the PLA approximation error bounds
3. **Scaling study**: Evaluate FROC's performance as the initial unfairness gap between groups increases to identify the practical limits of the approach