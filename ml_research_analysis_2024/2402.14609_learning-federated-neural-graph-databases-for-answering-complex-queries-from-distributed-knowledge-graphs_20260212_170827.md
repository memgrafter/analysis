---
ver: rpa2
title: Learning Federated Neural Graph Databases for Answering Complex Queries from
  Distributed Knowledge Graphs
arxiv_id: '2402.14609'
source_url: https://arxiv.org/abs/2402.14609
tags:
- graph
- queries
- query
- knowledge
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a privacy-preserving framework, FedNGDB, for
  answering complex queries across distributed knowledge graphs without exposing raw
  data. FedNGDB employs federated learning with secret aggregation to collaboratively
  train local neural graph database models while protecting sensitive embeddings.
---

# Learning Federated Neural Graph Databases for Answering Complex Queries from Distributed Knowledge Graphs

## Quick Facts
- arXiv ID: 2402.14609
- Source URL: https://arxiv.org/abs/2402.14609
- Reference count: 27
- Key outcome: FedNGDB achieves comparable or superior performance to baselines like FedE and FedR, particularly in cross-graph query answering, while preserving privacy through homomorphic encryption and differential privacy.

## Executive Summary
This paper proposes FedNGDB, a privacy-preserving framework for answering complex queries across distributed knowledge graphs without exposing raw data. The approach uses federated learning with secret aggregation to collaboratively train local neural graph database models while protecting sensitive embeddings. The framework enables both in-graph and cross-graph query answering by leveraging global operators for encoding and local embeddings for scoring, achieving strong performance on three real-world datasets.

## Method Summary
FedNGDB employs federated learning with secret aggregation to train local neural graph database models across distributed clients. The method uses homomorphic encryption and perturbed parameter sharing to aggregate entity embeddings while preventing server access to raw data. For query answering, complex queries are decomposed into sub-queries, sent to relevant clients for encoding, and then aggregated using global operator networks. The framework supports multiple base query encoding models and applies differential privacy to gradients for enhanced privacy protection.

## Key Results
- FedNGDB achieves comparable or superior performance to baselines like FedE and FedR, particularly in cross-graph query answering.
- The framework effectively improves retrieval accuracy over local-only models while preserving privacy through techniques such as homomorphic encryption and differential privacy.
- FedNGDB scales well with increasing numbers of clients and supports multiple base query encoding models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedNGDB enables cross-graph query answering without exposing sensitive embeddings.
- Mechanism: Secret aggregation uses homomorphic encryption and perturbed parameter sharing to aggregate entity embeddings while preventing the server from accessing raw embeddings.
- Core assumption: Homomorphic encryption preserves the privacy of parameters during aggregation and prevents server-side reconstruction attacks.
- Evidence anchors:
  - [abstract]: "leveraging federated learning with secret aggregation to collaboratively train local neural graph database models while protecting sensitive embeddings."
  - [section]: "Secret Aggregation" section describes the use of homomorphic encryption and perturbed parameters.
  - [corpus]: No direct evidence; corpus neighbors focus on federated learning and NGDBs but not secret aggregation.
- Break condition: If homomorphic encryption is broken or if the perturbation scheme is insufficient to prevent reconstruction attacks.

### Mechanism 2
- Claim: FedNGDB improves retrieval accuracy over local-only models by leveraging global operator networks.
- Mechanism: The server decomposes complex queries into sub-queries, sends them to relevant clients, aggregates sub-query embeddings using global operator networks, and retrieves answers from distributed NGDBs.
- Core assumption: The global operator network can effectively combine sub-query embeddings to represent the full complex query.
- Evidence anchors:
  - [abstract]: "The approach enables both in-graph and cross-graph query answering by leveraging global operators for encoding and local embeddings for scoring."
  - [section]: "Query Retrieval" section describes the decomposition and aggregation process.
  - [corpus]: No direct evidence; corpus neighbors do not specifically address global operator networks for complex query answering.
- Break condition: If the global operator network cannot effectively combine sub-query embeddings, leading to inaccurate query representation and poor retrieval performance.

### Mechanism 3
- Claim: FedNGDB scales well with increasing numbers of clients due to its federated learning architecture.
- Mechanism: FedNGDB uses federated learning to train local models on each client's data, reducing communication overhead compared to centralized training.
- Core assumption: The communication overhead in federated learning is manageable even with a large number of clients.
- Evidence anchors:
  - [abstract]: "The framework supports multiple base query encoding models and scales well with increasing numbers of clients."
  - [section]: "Model Learning" section describes the federated learning process.
  - [corpus]: No direct evidence; corpus neighbors do not specifically address scalability in federated NGDBs.
- Break condition: If communication overhead becomes prohibitive as the number of clients increases, leading to slow training and poor performance.

## Foundational Learning

- Concept: Federated Learning
  - Why needed here: Enables collaborative model training across distributed clients without sharing raw data, preserving privacy.
  - Quick check question: What is the main difference between federated learning and centralized learning?

- Concept: Homomorphic Encryption
  - Why needed here: Allows computations on encrypted data without decrypting it, preserving privacy during aggregation.
  - Quick check question: What is the key property of homomorphic encryption that makes it suitable for federated learning?

- Concept: Complex Query Answering
  - Why needed here: Enables retrieval of answers to logical queries over knowledge graphs, which is the primary task of FedNGDB.
  - Quick check question: What are the main types of logical expressions used in complex queries?

## Architecture Onboarding

- Component map:
  - Central server -> Clients -> Operator networks -> Entity embeddings

- Critical path:
  1. Clients train local NGDB models on their data.
  2. Clients upload perturbed parameters to the server.
  3. Server aggregates parameters using secret aggregation.
  4. Server updates global parameters and distributes them to clients.
  5. For query retrieval, server decomposes queries and sends sub-queries to relevant clients.
  6. Clients score candidate entities and upload scores to the server.
  7. Server aggregates scores and retrieves answers.

- Design tradeoffs:
  - Privacy vs. performance: Secret aggregation adds overhead but preserves privacy.
  - Communication overhead vs. model accuracy: More frequent communication can improve accuracy but increases overhead.
  - Local model complexity vs. global model performance: More complex local models may lead to better global performance but increase training time.

- Failure signatures:
  - Poor query retrieval performance: May indicate issues with query encoding, scoring, or aggregation.
  - Slow training: May indicate communication overhead or inefficient local training.
  - Privacy breaches: May indicate vulnerabilities in secret aggregation or differential privacy mechanisms.

- First 3 experiments:
  1. Evaluate FedNGDB's performance on in-graph queries compared to local training.
  2. Evaluate FedNGDB's performance on cross-graph queries compared to central training.
  3. Evaluate FedNGDB's scalability by increasing the number of clients and measuring communication overhead and performance.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important areas remain unexplored based on the content:

### Open Question 1
- Question: How does the performance of FedNGDB scale when the number of clients increases beyond 10, and what are the practical limits of this scaling?
- Basis in paper: [explicit] The paper evaluates performance with 3, 5, and 10 clients, showing improvements over local training but does not explore scalability beyond this point.
- Why unresolved: The paper does not provide data or analysis for scenarios with more than 10 clients, leaving the scalability limits unexplored.
- What evidence would resolve it: Empirical results showing performance metrics (e.g., HR@3, MRR) for systems with 20, 50, or 100 clients would clarify scalability boundaries.

### Open Question 2
- Question: What is the impact of relation overlap between local graph databases on the retrieval performance of FedNGDB, and how does it compare to scenarios with no overlap?
- Basis in paper: [explicit] The paper evaluates scenarios with no relation overlap but does not extensively analyze cases with varying degrees of overlap.
- Why unresolved: The experiments focus on non-overlapping relations, and the effects of partial or full overlap are not investigated.
- What evidence would resolve it: Comparative experiments measuring retrieval performance across datasets with different levels of relation overlap (e.g., 10%, 50%, 90%) would quantify the impact.

### Open Question 3
- Question: How does the privacy-utility tradeoff in FedNGDB change with different configurations of the privacy budget (ε), and what is the optimal balance for real-world applications?
- Basis in paper: [explicit] The paper applies differential privacy with a fixed ε-DP setting but does not explore how varying ε affects performance.
- Why unresolved: The experiments use a single privacy configuration, leaving the relationship between privacy strength and retrieval accuracy unexplored.
- What evidence would resolve it: A systematic study varying ε (e.g., 0.1, 1, 10) and reporting corresponding HR@3 and MRR metrics would reveal the tradeoff curve.

### Open Question 4
- Question: How does FedNGDB handle dynamic graph updates, such as adding or removing entities and relations, without retraining the entire model?
- Basis in paper: [inferred] The paper describes a static training process but does not address incremental updates to the graph data.
- Why unresolved: The framework is designed for initial training on static datasets, and mechanisms for dynamic updates are not discussed.
- What evidence would resolve it: Experiments or theoretical analysis demonstrating FedNGDB’s ability to incorporate new data efficiently (e.g., via incremental aggregation) would address this gap.

## Limitations

- The evaluation focuses on three specific knowledge graph datasets (FB15k, FB15k-237, NELL995) which may not generalize to all graph types or domains.
- The paper does not provide extensive analysis of how the framework performs under different privacy budgets or with varying levels of Laplacian noise.
- The communication overhead analysis is limited to just comparing round numbers, without detailed bandwidth or latency measurements across different network conditions.

## Confidence

- **High confidence**: The core mechanism of using federated learning with secret aggregation for privacy preservation is well-supported by the described architecture and experimental results.
- **Medium confidence**: The scalability claims are supported by the experimental setup but lack detailed analysis of communication overhead under various network conditions and client numbers.
- **Medium confidence**: The cross-graph query answering performance improvements are demonstrated but the exact mechanisms of query decomposition and routing could benefit from more detailed explanation.

## Next Checks

1. **Privacy Analysis Validation**: Test the framework with varying levels of differential privacy noise (ε values) to quantify the privacy-utility tradeoff and verify that the claimed privacy guarantees hold under different attack scenarios.

2. **Communication Overhead Benchmarking**: Implement detailed measurements of actual bandwidth usage and latency during federated training rounds, particularly focusing on the secret aggregation protocol's communication costs across different client counts.

3. **Cross-Graph Query Decomposition Testing**: Conduct ablation studies that isolate the query decomposition and routing mechanisms to verify that the improved cross-graph performance is indeed due to the proposed architecture rather than other factors.