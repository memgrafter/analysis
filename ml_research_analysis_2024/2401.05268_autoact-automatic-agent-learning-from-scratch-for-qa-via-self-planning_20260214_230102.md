---
ver: rpa2
title: 'AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning'
arxiv_id: '2401.05268'
source_url: https://arxiv.org/abs/2401.05268
tags:
- agent
- task
- autoact
- tool
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AUTOACT introduces an automatic agent learning framework that synthesizes
  planning trajectories from scratch without relying on large-scale annotated data
  or closed-source models. It employs a division-of-labor strategy where a meta-agent
  differentiates into specialized sub-agents (task decomposition, tool invocation,
  reflection) based on target task information.
---

# AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning

## Quick Facts
- arXiv ID: 2401.05268
- Source URL: https://arxiv.org/abs/2401.05268
- Reference count: 25
- Authors: Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen
- Primary result: AUTOACT achieves better or comparable performance to strong baselines across complex QA tasks using different LLMs, with Llama-2-13b matching GPT-3.5-Turbo performance

## Executive Summary
AUTOACT introduces an automatic agent learning framework that synthesizes planning trajectories from scratch without relying on large-scale annotated data or closed-source models. It employs a division-of-labor strategy where a meta-agent differentiates into specialized sub-agents (task decomposition, tool invocation, reflection) based on target task information. Experiments show AUTOACT achieves better or comparable performance to strong baselines across complex QA tasks using different LLMs, with Llama-2-13b matching GPT-3.5-Turbo performance. Human evaluation confirms superior trajectory quality in action type selection, parameter determination, and logical coherence.

## Method Summary
AUTOACT is an automatic agent learning framework for QA that synthesizes planning trajectories from scratch using self-instruction and self-planning. The framework begins with target task information and a small set of example Q&A pairs, then uses self-instruct to augment this data. The meta-agent automatically selects appropriate tools from a comprehensive library and synthesizes planning trajectories in zero-shot manner. These trajectories are filtered and used to fine-tune three specialized sub-agents (Plan-Agent, Tool-Agent, Reflect-Agent) using parameter-efficient LoRA fine-tuning. The differentiated sub-agents then collaborate through group planning to process new questions and obtain desired outcomes.

## Key Results
- AUTOACT achieves better or comparable performance to strong baselines across complex QA tasks
- Llama-2-13b achieves comparable performance to GPT-3.5-Turbo
- Human evaluation confirms superior trajectory quality in action type selection, parameter determination, and logical coherence
- Division-of-labor strategy effectively differentiates meta-agent into specialized sub-agents for improved performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Division-of-labor strategy improves performance by reducing cognitive load on individual agents
- Mechanism: The meta-agent differentiates into specialized sub-agents (plan-agent, tool-agent, reflect-agent) based on target task information and synthesized trajectories. Each sub-agent handles a specific function, allowing focused optimization.
- Core assumption: Breaking down complex planning tasks into specialized subtasks allows each component to optimize for its specific role without interference from other responsibilities.
- Evidence anchors:
  - [abstract]: "AUTOACT leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task."
  - [section]: "we propose the division-of-labor strategy which resembles cell differentiation based on the self-synthesized trajectories (genes), where the META-AGENT acts as a stem cell (Colman, 2008) and differentiates into three sub-agents with distinct functions"
  - [corpus]: Weak evidence - no direct citations about division-of-labor in the corpus
- Break condition: When task complexity requires too many specialized agents, coordination overhead may outweigh benefits

### Mechanism 2
- Claim: Self-planning enables automatic agent learning without relying on large-scale annotated data or closed-source models
- Mechanism: The meta-agent uses self-instruct to augment limited task data from scratch, then synthesizes planning trajectories in zero-shot manner using available tool library, eliminating need for human annotations or GPT-4-generated trajectories.
- Core assumption: A sufficiently capable language model can generate high-quality synthetic training data and trajectories through self-instruction and self-planning without external supervision.
- Evidence anchors:
  - [abstract]: "AUTOACT first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models"
  - [section]: "we enable the META-AGENT to synthesize planning trajectories on its own. Equipped with Ts, we instruct the META-AGENT to synthesize trajectories in a zero-shot manner"
  - [corpus]: Weak evidence - corpus neighbors focus on other agent approaches but don't directly validate self-planning capability
- Break condition: When self-generated trajectories lack diversity or quality, leading to poor agent performance

### Mechanism 3
- Claim: Parameter-efficient fine-tuning (LoRA) enables lightweight and low-consumption differentiation
- Mechanism: Instead of full fine-tuning, AUTOACT applies LoRA to create specialized sub-agents, making the differentiation process lightweight while maintaining performance.
- Core assumption: Low-rank adaptation can effectively specialize models for different sub-tasks without the computational cost of full fine-tuning.
- Evidence anchors:
  - [section]: "Our differentiation is a parameter-efficient fine-tuning process to achieve lightweight and low consumption. Particularly, for each sub-agent, we train a specific LoRA (Hu et al., 2022)."
  - [section]: "We fine-tune all our models with LoRA (Hu et al., 2022) in the format proposed in Alpaca (Taori et al., 2023)"
  - [corpus]: No direct evidence in corpus about LoRA usage
- Break condition: When task complexity exceeds what can be captured through low-rank adaptations

## Foundational Learning

- Concept: Self-instruction for data augmentation
  - Why needed here: AUTOACT starts with limited task data and needs to generate sufficient training examples without external labeled data
  - Quick check question: How does self-instruction generate new question-answer pairs from existing examples?

- Concept: Zero-shot trajectory synthesis
  - Why needed here: AUTOACT must generate planning trajectories without access to closed-source models or human-annotated data
  - Quick check question: What prompt format does the meta-agent use to synthesize Thought-Action-Observation trajectories?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: Differentiating into multiple sub-agents requires training multiple models efficiently without full fine-tuning costs
  - Quick check question: How does LoRA enable specialization while keeping computational costs low?

## Architecture Onboarding

- Component map:
  Meta-Agent -> Self-Instruction -> Tool Selection -> Trajectory Synthesis -> Filtering -> LoRA Fine-tuning -> Sub-agents (Plan-Agent, Tool-Agent, Reflect-Agent) -> Group Planning

- Critical path:
  1. Self-instruct augments limited task data
  2. Automatic tool selection identifies relevant tools
  3. Zero-shot trajectory synthesis generates training data
  4. Filtering removes incorrect trajectories
  5. LoRA fine-tuning creates specialized sub-agents
  6. Group planning executes tasks using sub-agent collaboration

- Design tradeoffs:
  - Self-planning vs. supervised learning: Sacrifices potential quality for independence from closed-source models
  - Division-of-labor vs. single-agent: Better performance but increased coordination complexity
  - LoRA vs. full fine-tuning: Lower cost but potentially limited specialization capacity

- Failure signatures:
  - Poor tool selection leading to irrelevant trajectories
  - Insufficient data diversity causing overfitting
  - Over-differentiation creating coordination overhead
  - Self-generated trajectories lacking complexity for hard tasks

- First 3 experiments:
  1. Test self-instruction capability on a simple QA task with 2-3 examples
  2. Verify tool selection mechanism with a known task and tool library
  3. Evaluate trajectory synthesis quality by comparing self-generated vs. reference trajectories

## Open Questions the Paper Calls Out
- How does AUTOACT perform on benchmarks beyond complex question-answering tasks, such as web interaction, household tasks, or robotics?
- What specific mechanisms could improve knowledge extraction through self-instruct in AUTOACT?
- How would incorporating iterative self-improvement techniques affect AUTOACT's performance?

## Limitations
- Performance limited by model's ability to access internal knowledge through self-instruct
- Excessive fine-grained division of labor may not lead to better performance
- Only evaluated on complex question-answering tasks, not broader interactive domains

## Confidence
- High Confidence: Experimental results showing comparable performance across multiple QA tasks using different model sizes
- Medium Confidence: Division-of-labor strategy and LoRA fine-tuning approach are conceptually sound but require more rigorous validation
- Low Confidence: Self-planning mechanism's ability to generate high-quality trajectories without external supervision has weakest supporting evidence

## Next Checks
1. Systematically evaluate the diversity and complexity of self-generated trajectories compared to human-annotated or GPT-4-generated trajectories
2. Conduct ablation experiments varying the number and specialization level of sub-agents to identify optimal granularity
3. Evaluate the framework's performance when starting from progressively smaller seed datasets to determine minimum viable training data