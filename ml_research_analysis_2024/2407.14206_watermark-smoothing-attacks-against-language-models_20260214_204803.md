---
ver: rpa2
title: Watermark Smoothing Attacks against Language Models
arxiv_id: '2407.14206'
source_url: https://arxiv.org/abs/2407.14206
tags:
- text
- watermark
- tokens
- attack
- green
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel watermark removal method called the
  Smoothing Attack against language models. The attack leverages the relationship
  between model confidence and watermark detectability by selectively smoothing watermarked
  content to erase watermark traces while preserving text quality.
---

# Watermark Smoothing Attacks against Language Models

## Quick Facts
- arXiv ID: 2407.14206
- Source URL: https://arxiv.org/abs/2407.14206
- Authors: Hongyan Chang; Hamed Hassani; Reza Shokri
- Reference count: 19
- This paper introduces a novel watermark removal method called the Smoothing Attack against language models, demonstrating effectiveness across multiple model sizes and watermark types.

## Executive Summary
This paper presents the Smoothing Attack, a novel method for removing watermarks from language model outputs by exploiting the relationship between model confidence and watermark detectability. The attack selectively smooths watermarked tokens that exhibit lower confidence than their unwatermarked counterparts, effectively erasing watermark traces while maintaining text quality. The authors demonstrate that this approach can reconstruct watermarked tokens with over 0.9 AUC using only a few hundred queries, completely defeating watermark detection across multiple model architectures.

## Method Summary
The Smoothing Attack leverages the observation that watermarked tokens often have lower model confidence compared to their unwatermarked counterparts. The method identifies these low-confidence tokens and applies smoothing techniques to reconstruct them, effectively removing the watermark signature. The attack operates by querying the model to compare confidence scores between watermarked and unwatermarked outputs, then selectively modifying tokens where the confidence gap is significant. This approach requires access to unwatermarked model outputs for comparison but can be executed efficiently with minimal computational overhead.

## Key Results
- The attack achieves over 0.9 AUC in reconstructing watermarked tokens across models from 1.3B to 30B parameters
- Successfully bypasses watermark detection for 10 different watermarking schemes tested
- Generated text maintains comparable utility to unwatermarked model outputs while completely defeating watermark detection
- Requires only a few hundred queries to effectively remove watermarks from model-generated text

## Why This Works (Mechanism)
The attack exploits a fundamental vulnerability in watermarking schemes: the deliberate introduction of lower-confidence tokens to embed watermark information. By identifying and smoothing these tokens, the attack reverses the watermark embedding process. The method capitalizes on the fact that watermarked tokens are statistically distinguishable from regular tokens through their confidence scores, allowing targeted reconstruction that preserves text quality while removing the watermark signature.

## Foundational Learning
- **Watermark Detection AUC**: Area Under the Curve metric for measuring detection performance - needed to quantify attack effectiveness
  *Quick check*: AUC values above 0.9 indicate strong watermark reconstruction capability
- **Model Confidence Calibration**: The relationship between predicted probabilities and true likelihoods - needed to identify watermarked tokens
  *Quick check*: Watermarked tokens consistently show lower confidence scores than unwatermarked equivalents
- **Token Smoothing Techniques**: Methods for reconstructing token probabilities while maintaining text coherence - needed to preserve output quality
  *Quick check*: Smoothed tokens maintain semantic and grammatical consistency with surrounding text

## Architecture Onboarding

**Component Map**: Model outputs -> Confidence Analysis -> Watermarked Token Detection -> Token Smoothing -> Clean Text Output

**Critical Path**: The attack follows a sequential process of analyzing model outputs, detecting low-confidence watermarked tokens, applying smoothing reconstruction, and producing watermark-free text that maintains utility.

**Design Tradeoffs**: The attack balances watermark removal effectiveness against text quality preservation. Higher smoothing aggressiveness improves watermark removal but risks degrading text quality. The method must also manage query efficiency to remain practical while maintaining effectiveness.

**Failure Signatures**: The attack may fail when watermarked tokens do not exhibit distinguishable confidence patterns, when models have different confidence calibration properties, or when watermarks are distributed across the vocabulary in ways that resist smoothing techniques.

**3 First Experiments**:
1. Test confidence score distributions for watermarked vs unwatermarked tokens across different model sizes
2. Measure smoothing effectiveness on tokens with varying confidence gaps
3. Evaluate text quality preservation using multiple utility metrics after smoothing

## Open Questions the Paper Calls Out
None

## Limitations
- The attack depends on access to unwatermarked model outputs for comparison, which may not always be available
- Effectiveness may vary across different watermarking schemes, particularly non-unidiff approaches
- The method's generalizability to specialized domain models and multilingual architectures requires further validation
- Task-specific performance impact after smoothing is not comprehensively evaluated

## Confidence

**High confidence**: The fundamental relationship between model confidence and watermark detectability is well-established. The smoothing mechanism itself is technically sound and the experimental methodology for evaluating AUC performance is rigorous.

**Medium confidence**: The generalizability of the attack across different watermarking schemes and model architectures. While tested on 10 different watermarks, the diversity of these schemes and whether they represent the full spectrum of watermarking techniques remains unclear.

**Medium confidence**: The claim about maintaining text utility equivalent to unwatermarked models. The evaluation focuses on general text quality rather than task-specific performance metrics.

## Next Checks
1. Test the smoothing attack against a broader range of watermarking schemes including signature-based and syntax-based methods to verify cross-scheme effectiveness and identify potential attack limitations.

2. Evaluate the attack's performance on specialized domain models (medical, legal, technical) and multilingual models to assess whether confidence-calibration differences affect attack success rates.

3. Conduct comprehensive task-specific utility analysis by testing the smoothed text on downstream applications (summarization, question answering, code generation) to verify that quality preservation holds beyond general text coherence metrics.