---
ver: rpa2
title: 'BiVLC: Extending Vision-Language Compositionality Evaluation with Text-to-Image
  Retrieval'
arxiv_id: '2406.09952'
source_url: https://arxiv.org/abs/2406.09952
tags:
- negative
- images
- image
- bivlc
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BiVLC, a new dataset extending the vision-language
  compositionality (VLC) evaluation to bidirectional retrieval (image-to-text and
  text-to-image). The authors generate synthetic negative images for the SUGAR CREPE
  dataset using text-to-image models and filter ambiguous cases through human annotation,
  resulting in 2,933 instances with 11,732 retrieval examples.
---

# BiVLC: Extending Vision-Language Compositionality Evaluation with Text-to-Image Retrieval

## Quick Facts
- arXiv ID: 2406.09952
- Source URL: https://arxiv.org/abs/2406.09952
- Reference count: 40
- The paper introduces BiVLC, a new dataset extending vision-language compositionality evaluation to bidirectional retrieval

## Executive Summary
This paper introduces BiVLC, a novel dataset for evaluating vision-language compositionality in bidirectional retrieval tasks (image-to-text and text-to-image). The authors extend the existing SUGAR CREPE dataset by generating synthetic negative images using text-to-image models and filtering ambiguous cases through human annotation. The resulting dataset contains 2,933 instances with 11,732 retrieval examples. Experiments demonstrate that while humans perform similarly in both retrieval directions, current multimodal models struggle significantly more with text-to-image retrieval, highlighting the challenge of bidirectional compositionality evaluation.

## Method Summary
The authors created BiVLC by extending the SUGAR CREPE dataset with synthetic negative images generated using text-to-image models. They employed a two-stage human annotation process: first to identify and remove ambiguous cases where synthetic images were difficult to distinguish from real ones, and second to validate the remaining instances. The final dataset comprises 2,933 compositionality instances, each providing both image-to-text and text-to-image retrieval pairs. The authors evaluated multiple multimodal models on this dataset and introduced CLIP TROHN-I MG, a model trained with hard negative images that achieved state-of-the-art performance on BiVLC.

## Key Results
- Humans perform similarly in both image-to-text and text-to-image retrieval directions on BiVLC
- Current multimodal models show significant performance gaps in text-to-image retrieval compared to image-to-text retrieval
- CLIP TROHN-I MG, trained with hard negative synthetic images, achieves the best performance on BiVLC but still falls short of human performance
- The synthetic negative images provide valuable training signal, though the performance gap with humans remains substantial

## Why This Works (Mechanism)
The bidirectional evaluation framework works by leveraging synthetic negative images generated from text descriptions to create challenging compositionality tasks. The text-to-image models generate plausible negative examples that share semantic components with target images but violate compositional rules, creating hard negative samples. The human filtering process ensures that these synthetic negatives are sufficiently challenging while remaining distinguishable, creating a high-quality evaluation benchmark that reveals directional asymmetries in current multimodal models' understanding of compositionality.

## Foundational Learning
1. **Vision-language compositionality**: Understanding how models combine visual and linguistic concepts - why needed: Core concept being evaluated; quick check: Can models correctly identify when visual elements contradict textual descriptions
2. **Bidirectional retrieval**: Evaluating both image→text and text→image tasks - why needed: Reveals asymmetric model capabilities; quick check: Compare performance across directions to identify bottlenecks
3. **Synthetic negative generation**: Using text-to-image models to create challenging negatives - why needed: Enables scalable dataset creation; quick check: Validate that generated negatives are semantically related but compositionally incorrect
4. **Hard negative mining**: Selecting challenging negative examples - why needed: Improves model robustness; quick check: Measure if models can distinguish between similar positive and negative pairs
5. **Human-in-the-loop annotation**: Using human judgment to filter ambiguous cases - why needed: Ensures dataset quality; quick check: Verify inter-annotator agreement and consistency
6. **Multimodal model evaluation**: Assessing vision-language model performance - why needed: Benchmarks current capabilities; quick check: Compare against human baselines to establish difficulty

## Architecture Onboarding

**Component Map:**
Human Annotation -> Synthetic Image Generation -> BiVLC Dataset Construction -> Model Training -> Bidirectional Evaluation

**Critical Path:**
Text descriptions → Text-to-image generation → Human filtering → Positive/negative pair creation → Model training with hard negatives → Bidirectional retrieval evaluation

**Design Tradeoffs:**
- Synthetic negatives provide scalability but may introduce distribution shift
- Human filtering ensures quality but limits dataset size
- Bidirectional evaluation reveals directional weaknesses but doubles annotation effort

**Failure Signatures:**
- Models failing text-to-image retrieval more than image-to-text suggests semantic understanding gaps
- High error rates on synthetic negatives indicate poor generalization from training data
- Performance close to random selection reveals fundamental compositionality understanding failures

**First Experiments:**
1. Compare model performance on synthetic versus real negative images to quantify synthetic data impact
2. Evaluate model performance on subsets of BiVLC with varying levels of synthetic negative difficulty
3. Test whether fine-tuning on BiVLC improves performance on other compositionality benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic negative images may introduce distribution shift affecting model generalization
- Human annotation relied on a small team (3 members), potentially limiting interpretation diversity
- Study focuses on single base dataset (SUGAR CREPE), limiting generalizability to other domains

## Confidence

**Major Claim Clusters and Confidence Levels:**
- **BiVLC dataset represents a meaningful extension of VLC evaluation** (High confidence): Clear methodology and substantial dataset size
- **Current multimodal models show significant directional asymmetry in VLC** (Medium confidence): Results show differences but synthetic negatives and limited human baseline detail introduce uncertainty
- **CLIP TROHN-I MG achieves state-of-the-art performance on BiVLC** (Medium confidence): Well-supported claim but advantage of synthetic negatives not fully isolated

## Next Checks
1. Conduct controlled experiment comparing model performance using synthetic versus real negative images to quantify synthetic data impact
2. Expand human evaluation to include larger, more diverse annotation team with documented participant selection criteria
3. Test generalizability by applying bidirectional evaluation framework to additional vision-language datasets beyond SUGAR CREPE