---
ver: rpa2
title: Automated Computational Energy Minimization of ML Algorithms using Constrained
  Bayesian Optimization
arxiv_id: '2407.05788'
source_url: https://arxiv.org/abs/2407.05788
tags:
- energy
- performance
- optimization
- consumption
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of minimizing computational
  energy consumption during ML model training while maintaining predictive performance.
  The authors propose using Constrained Bayesian Optimization (CBO) to minimize wall-clock
  runtime subject to a constraint on predictive performance (MSE for regression, accuracy
  for classification).
---

# Automated Computational Energy Minimization of ML Algorithms using Constrained Bayesian Optimization

## Quick Facts
- **arXiv ID**: 2407.05788
- **Source URL**: https://arxiv.org/abs/2407.05788
- **Reference count**: 27
- **Primary result**: CBO consistently finds hyperparameter configurations that meet performance constraints while achieving lower energy consumption compared to unconstrained approaches.

## Executive Summary
This paper addresses the challenge of minimizing computational energy consumption during ML model training while maintaining predictive performance. The authors propose using Constrained Bayesian Optimization (CBO) to minimize wall-clock runtime subject to a constraint on predictive performance (MSE for regression, accuracy for classification). They compare CBO against unconstrained Bayesian Optimization with a penalty term across various ML models and datasets. The key finding is that CBO consistently finds hyperparameter configurations that meet the performance constraint while achieving lower energy consumption compared to the unconstrained approach.

## Method Summary
The authors formulate energy minimization as a constrained optimization problem where the objective is to minimize wall-clock runtime (proxy for energy consumption) subject to a constraint on predictive performance. They use CBO with a joint acquisition function combining Expected Improvement (EI) and Probability of Feasibility (PoF). The method employs independent Gaussian Processes with Matérn 5/2 kernel to model both the objective function (log-transformed runtime) and constraint function (log-transformed performance metrics). The approach is evaluated across regression and classification tasks using various ML models on California Housing and 20-Newsgroups datasets.

## Key Results
- CBO achieved lower MSE while maintaining similar or lower training times compared to unconstrained BO, which often violated the performance constraint
- The constrained approach consistently found feasible solutions that met performance requirements while minimizing energy consumption
- CBO demonstrated superior performance compared to unconstrained BO with penalty terms across all tested models and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The joint acquisition function combining Expected Improvement (EI) and Probability of Feasibility (PoF) effectively learns feasible regions while optimizing energy consumption.
- Mechanism: By multiplying EI and PoF, the acquisition function ensures that candidate points are evaluated based on both their potential to minimize energy consumption and their likelihood of satisfying the performance constraint. This joint learning approach allows the optimizer to explore the search space more efficiently by avoiding regions that violate constraints while still seeking optimal energy consumption.
- Core assumption: The constraint function and objective function can be modeled independently using separate Gaussian Processes with independent kernels.
- Evidence anchors:
  - [abstract] "We define a joint acquisition function, where the feasible regions are learnt jointly for both objective and constraint functions."
  - [section 3.2] "Here we use a joint acquisition function which is a product of EI and PoF. Therefore, the feasible regions for a sampling of the next point of the given CBO will be learnt jointly with the optimal regions of both EI and PoF."
- Break condition: This mechanism breaks if the constraint function and objective function are strongly correlated, as the independent GP assumption becomes invalid.

### Mechanism 2
- Claim: Transforming the positive-only runtime and performance metrics using logarithmic scaling enables effective Gaussian Process modeling.
- Mechanism: By defining f(x) = log τ(x) - log τb and transforming performance metrics similarly, the authors create distributions that can take both positive and negative values, which are necessary for Gaussian Process modeling. This transformation allows the surrogate models to capture the relationships between hyperparameters and both energy consumption and performance.
- Core assumption: The transformed metrics can be effectively modeled by Gaussian Processes despite the non-linear relationship between hyperparameters and the original metrics.
- Evidence anchors:
  - [section 3.1] "To get both positive and negative samples, we define f(x) as f(x) = log τ(x) − log τb... Similarly c(x) i.e. the error metric for regression models or accuracy metric for classification models are always positive for all values of x. Therefore, to be modelled well by GP, c(x) is defined as..."
- Break condition: This mechanism breaks if the logarithmic transformation distorts the relationship between hyperparameters and the original metrics beyond what Gaussian Processes can effectively model.

### Mechanism 3
- Claim: Using wall-clock runtime as a proxy for energy consumption is effective for application-level optimization.
- Mechanism: Since energy is defined as power integrated over time, and assuming constant power resources, minimizing runtime directly minimizes energy consumption. This simplification allows the authors to focus on optimizing computational time rather than attempting to directly measure energy consumption, which is complex and hardware-dependent.
- Core assumption: Power consumption remains relatively constant across different hyperparameter configurations, making runtime a valid proxy for energy.
- Evidence anchors:
  - [section 3] "Thus, for a constant power resource, the only variable is consumed time and for simplicity we re-frame the problem to minimization of run time consumption."
- Break condition: This mechanism breaks if different hyperparameter configurations lead to significantly different power consumption patterns, making runtime an invalid proxy for energy.

## Foundational Learning

- **Concept**: Bayesian Optimization
  - Why needed here: BO is the fundamental framework used for hyperparameter optimization, allowing efficient exploration of the hyperparameter space when function evaluations are expensive.
  - Quick check question: What is the primary advantage of Bayesian Optimization over grid search or random search for hyperparameter optimization?

- **Concept**: Gaussian Processes
  - Why needed here: GPs serve as the surrogate model for both the objective function (energy consumption) and constraint function (performance), enabling probabilistic predictions and uncertainty quantification.
  - Quick check question: Why are Gaussian Processes particularly well-suited for modeling expensive-to-evaluate black-box functions?

- **Concept**: Expected Improvement and Probability of Feasibility acquisition functions
  - Why needed here: EI guides the search toward regions that improve the objective function, while PoF ensures that sampled points are likely to satisfy the performance constraint.
  - Quick check question: How does the product of EI and PoF acquisition functions balance the dual objectives of minimizing energy consumption while meeting performance constraints?

## Architecture Onboarding

- **Component map**: Hyperparameter configuration -> Performance evaluation -> Runtime measurement -> Log-transformed metrics -> Independent GPs -> Joint acquisition function (EI × PoF) -> Next hyperparameter selection

- **Critical path**:
  1. Initialize with default hyperparameters
  2. Evaluate performance and runtime for initial points
  3. Fit surrogate models to transformed metrics
  4. Compute joint acquisition function
  5. Select next hyperparameters to evaluate
  6. Repeat steps 2-5 until convergence

- **Design tradeoffs**:
  - Independent GP modeling simplifies implementation but may miss correlations between energy consumption and performance
  - Using wall-clock time as energy proxy is practical but hardware-dependent
  - Logarithmic transformation enables GP modeling but may introduce non-linear distortions

- **Failure signatures**:
  - Surrogate models fail to converge or make poor predictions
  - Acquisition function consistently samples invalid points (constraint violations)
  - Runtime measurements are inconsistent or noisy
  - Performance constraint is too strict or too loose for the problem domain

- **First 3 experiments**:
  1. Implement the CBO framework with a simple regression model (e.g., Lasso) on a small dataset to verify the optimization loop works correctly
  2. Compare CBO performance against unconstrained BO with penalty on the same problem to validate the constrained approach
  3. Test the framework with different kernel functions in the Gaussian Process to assess sensitivity to kernel choice

## Open Questions the Paper Calls Out

- **Open Question 1**: How do dependencies between energy consumption and predictive performance vary across different hyperparameter choices, and can these dependencies be effectively captured by joint surrogate models?
  - Basis in paper: [inferred] The paper notes that hyperparameter choices (like preprocessing settings, regularization parameters, or neural network architectures) can simultaneously impact energy consumption and predictive performance, suggesting the current assumption of independent surrogate models is a limitation.
  - Why unresolved: The authors acknowledge this limitation but do not provide evidence or methods for modeling these dependencies, leaving the effectiveness of joint modeling unexplored.
  - What evidence would resolve it: Experiments comparing independent versus joint surrogate models on the same datasets and tasks, showing whether joint modeling leads to better energy efficiency without sacrificing predictive performance.

- **Open Question 2**: How sensitive are the results to the choice of baseline predictive performance threshold, and how can optimal thresholds be determined for different tasks and domains?
  - Basis in paper: [explicit] The authors assume a predefined threshold for predictive accuracy but note this is a limitation, as default hyperparameters may be suboptimal and thresholds may need to be task-specific.
  - Why unresolved: The paper uses fixed thresholds based on default settings but does not explore how varying thresholds affect optimization outcomes or provide guidance on threshold selection.
  - What evidence would resolve it: A sensitivity analysis showing how different threshold values impact energy consumption and constraint satisfaction, along with recommendations for threshold selection based on task characteristics.

- **Open Question 3**: How does the computational overhead of CBO compare to unconstrained BO with penalty terms, and does this overhead negate energy savings in practice?
  - Basis in paper: [inferred] The paper compares CBO and unconstrained BO results but does not discuss the computational overhead of CBO itself, which could affect real-world applicability.
  - Why unresolved: While the paper demonstrates CBO's effectiveness, it does not quantify the additional computational cost of CBO's acquisition function calculations compared to simpler penalty-based approaches.
  - What evidence would resolve it: Detailed runtime analysis comparing CBO and unconstrained BO, including both optimization time and training time, to determine if CBO's overhead is offset by energy savings in the trained models.

## Limitations
- The runtime-to-energy proxy assumption may not hold across different hardware configurations or when power usage varies significantly with hyperparameter choices
- Independent Gaussian Process modeling assumes no correlation between energy consumption and predictive performance, which may be violated in practice
- Specific hyperparameter search spaces and performance constraint thresholds are not detailed, making exact reproduction challenging

## Confidence

- **High confidence**: The general framework of using CBO for constrained optimization in ML hyperparameter tuning is sound and well-established
- **Medium confidence**: The specific application to energy minimization is novel, but the runtime proxy assumption needs validation across different hardware
- **Medium confidence**: The empirical results showing improved energy efficiency while maintaining performance constraints appear promising but are limited to specific datasets and models

## Next Checks
1. **Hardware dependence validation**: Test the runtime-to-energy proxy assumption across different hardware configurations (CPU vs GPU, different architectures) to verify the generality of the approach.

2. **Correlation analysis**: Investigate the relationship between energy consumption and predictive performance across different hyperparameter configurations to validate the independent GP modeling assumption.

3. **Constraint sensitivity analysis**: Systematically vary the performance constraint thresholds to understand how sensitive the optimization results are to constraint specification and identify potential trade-offs.