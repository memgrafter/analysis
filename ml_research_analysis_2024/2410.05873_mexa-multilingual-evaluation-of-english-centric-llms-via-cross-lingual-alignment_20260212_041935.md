---
ver: rpa2
title: 'MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment'
arxiv_id: '2410.05873'
source_url: https://arxiv.org/abs/2410.05873
tags:
- latn
- language
- languages
- mexa
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEXA is a method to estimate multilingual coverage of English-centric
  LLMs by measuring cross-lingual alignment of sentence embeddings between English
  and other languages using parallel sentences. It uses a robust binary alignment
  score that compares similarity of parallel sentence pairs against non-parallel ones
  to avoid hubness issues.
---

# MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment

## Quick Facts
- arXiv ID: 2410.05873
- Source URL: https://arxiv.org/abs/2410.05873
- Authors: Amir Hossein Kargaran; Ali Modarressi; Nafiseh Nikeghbal; Jana Diesner; François Yvon; Hinrich Schütze
- Reference count: 40
- One-line primary result: MEXA achieves 0.90 Pearson correlation between predicted alignment scores and actual multilingual task performance

## Executive Summary
MEXA provides a novel method to evaluate the multilingual coverage of English-centric large language models by measuring cross-lingual alignment between English and other languages using parallel sentences. The approach leverages the observation that English-centric LLMs use English as a pivot language in their intermediate layers, computing alignment scores that compare semantic similarity of parallel sentence pairs against non-parallel ones. Experiments demonstrate that MEXA achieves strong correlation with actual downstream task performance across multiple languages and models, offering a reliable and scalable way to assess multilingual capabilities, particularly for low-resource languages.

## Method Summary
MEXA computes cross-lingual alignment scores by measuring semantic similarity between English and non-English sentence embeddings extracted from English-centric LLMs. The method uses parallel sentence pairs from datasets like FLORES-200 and Bible, calculating a binary alignment score that compares the cosine similarity of parallel sentences against non-parallel ones to avoid hubness issues. MEXA scores are aggregated across model layers using mean pooling and correlated with actual multilingual task performance on benchmarks like Belebele, m-MMLU, and m-ARC.

## Key Results
- MEXA achieves an average Pearson correlation of 0.90 with actual multilingual task performance
- Weighted-average token embeddings with mean pooling yield the best correlation results
- MEXA reliably distinguishes between high and low resource languages across multiple model families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MEXA uses English as a pivot to evaluate cross-lingual alignment in English-centric LLMs.
- Mechanism: MEXA measures semantic similarity between English and non-English embeddings using parallel sentences, leveraging the observation that English-centric LLMs use English as a latent language in middle layers.
- Core assumption: The English-centric LLM's intermediate layers encode semantic information in English for multilingual queries.
- Evidence anchors:
  - [abstract] "MEXA leverages that English-centric LLMs use English as a pivot language in their intermediate layers."
  - [section 2] "Zhong et al. (2024) extend this analysis to multiple tokens, also showing that an LLM dominated by both English and Japanese uses both languages as internal latent languages."
- Break condition: If the model does not use English as a pivot language in middle layers, MEXA scores would not correlate with actual multilingual performance.

### Mechanism 2
- Claim: MEXA avoids hubness problems by using a binary alignment score instead of absolute cosine similarity values.
- Mechanism: MEXA assigns 1 if parallel sentence similarity exceeds all non-parallel pairs, otherwise 0, sidestepping absolute similarity issues.
- Core assumption: Anisotropy and hubness in transformer models make absolute cosine similarity unreliable for multilingual alignment.
- Evidence anchors:
  - [abstract] "MEXA computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages."
  - [section 3] "This approach sidesteps the hubness problem since the absolute cosine similarity values themselves are not directly used."
- Break condition: If the anisotropy problem is not present or is negligible, the binary scoring approach may be unnecessarily conservative.

### Mechanism 3
- Claim: MEXA achieves high correlation with downstream task performance across multiple languages and models.
- Mechanism: MEXA scores, computed using weighted average embeddings and mean pooling, show Pearson correlation of 0.90 with actual task performance.
- Core assumption: The alignment score computed by MEXA is linearly related to the model's actual multilingual performance.
- Evidence anchors:
  - [abstract] "Our results show that MEXA, in its default settings, achieves an average Pearson correlation of 0.90 between its predicted scores and actual task performance across languages."
  - [section 5.1] "Across all settings, the best overall result (higher correlation) is achieved when embeddings are computed using the weighted average, with mean pooling as the pooling method."
- Break condition: If the relationship between alignment and performance is non-linear or task-dependent, MEXA's correlation may not hold across all scenarios.

## Foundational Learning

- Concept: Cross-lingual alignment
  - Why needed here: Understanding how embeddings from different languages relate to each other is fundamental to MEXA's approach.
  - Quick check question: What is the difference between absolute cosine similarity and relative alignment scores in multilingual settings?

- Concept: Token-level vs. sentence-level embeddings
  - Why needed here: MEXA explores different methods for computing sentence embeddings from token embeddings.
  - Quick check question: Why might using only the last token embedding be problematic for representing an entire sentence?

- Concept: Layer-wise pooling strategies
  - Why needed here: MEXA aggregates alignment scores across model layers using different pooling methods.
  - Quick check question: How do mean pooling and max pooling differ in their treatment of alignment scores across layers?

## Architecture Onboarding

- Component map: MEXA consists of parallel sentence datasets (FLORES-200, Bible), LLM models (Llama, Gemma, Mistral, OLMo), embedding computation methods (weighted average, last token), layer pooling strategies (mean, max), and correlation analysis with downstream tasks (Belebele, m-MMLU, m-ARC).

- Critical path: For a new engineer to implement MEXA, the critical path involves loading parallel sentences, computing embeddings for each layer, calculating MEXA scores for each language pair, and correlating these scores with downstream task performance.

- Design tradeoffs: MEXA trades off computational efficiency (using 100 sentences per language) against potential accuracy, and uses binary scoring to avoid hubness at the cost of potentially losing fine-grained information.

- Failure signatures: MEXA would fail if the English-centric LLM does not use English as a pivot language, if the parallel sentence data is poor quality or misaligned, or if the relationship between alignment and performance is non-linear.

- First 3 experiments:
  1. Implement MEXA on a small subset of FLORES-200 with a single LLM to verify basic functionality.
  2. Compare MEXA scores using weighted average vs. last token embeddings to identify the better approach.
  3. Test MEXA correlation with downstream tasks using both mean and max pooling to determine the optimal pooling strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would MEXA scores correlate with generative task performance across different languages?
- Basis in paper: [inferred] The paper explicitly limits scope to non-generative tasks and acknowledges this as a limitation, suggesting the need for evaluation on generative tasks.
- Why unresolved: MEXA was only validated against non-generative benchmarks (Belebele, m-MMLU, m-ARC), and generative capabilities may depend on different factors than semantic alignment.
- What evidence would resolve it: Experimental results showing Pearson correlation between MEXA scores and performance on generative tasks (translation, summarization, story completion) across multiple languages.

### Open Question 2
- Question: Would MEXA scores change significantly if computed using multilingual models (e.g., BLOOM, mBERT) rather than English-centric models?
- Basis in paper: [explicit] The paper focuses on English-centric models and uses English as the pivot language, but doesn't explore how results might differ with truly multilingual models.
- Why unresolved: The methodology assumes English as pivot, but multilingual models may use different internal language representations that could affect alignment measurements.
- What evidence would resolve it: MEXA scores computed for multilingual models using both English and non-English pivot languages, compared against downstream task performance.

### Open Question 3
- Question: How sensitive are MEXA scores to the size and quality of parallel datasets across different language pairs?
- Basis in paper: [explicit] The paper uses two parallel datasets (FLORES-200 with 100 sentences per language, Bible with 103 sentences) but doesn't systematically explore how dataset size affects reliability.
- Why unresolved: While the paper argues 100 sentences is sufficient for robustness, it doesn't test this claim or examine how results vary with dataset size/quality across different language pairs.
- What evidence would resolve it: Systematic experiments varying parallel dataset sizes and quality (noisy vs clean translations) across multiple language pairs, measuring impact on MEXA-score correlation with downstream tasks.

## Limitations
- MEXA relies on the assumption that English-centric models use English as a pivot language, which is not directly validated for all tested models.
- The method uses only 100 parallel sentences per language, and its robustness to dataset size and quality is not thoroughly explored.
- MEXA is limited to non-generative tasks and does not evaluate cross-lingual alignment for generative capabilities like translation or summarization.

## Confidence
- High: MEXA achieves 0.90 Pearson correlation with multilingual task performance across multiple models and datasets.
- Medium: The mechanism of English serving as a pivot language relies on prior work and lacks direct validation.
- Medium: The binary scoring approach's effectiveness in avoiding hubness is not directly compared to absolute similarity methods.

## Next Checks
1. Test MEXA's correlation with multilingual performance on models explicitly trained without English as a pivot (e.g., models with balanced multilingual pretraining).
2. Vary the number of parallel sentences used for MEXA calculation (e.g., 10, 50, 200) and assess the stability of correlations.
3. Compare MEXA scores using binary alignment versus absolute cosine similarity thresholds to quantify the benefit of the binary approach in avoiding hubness.