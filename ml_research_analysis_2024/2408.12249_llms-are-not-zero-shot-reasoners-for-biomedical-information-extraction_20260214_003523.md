---
ver: rpa2
title: LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction
arxiv_id: '2408.12249'
source_url: https://arxiv.org/abs/2408.12249
tags:
- tasks
- arxiv
- performance
- biomedical
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are increasingly used in healthcare
  but their performance on structured biomedical information extraction tasks remains
  unclear. This paper systematically benchmarks LLM performance on medical classification
  and Named Entity Recognition (NER) tasks, evaluating task knowledge, reasoning capabilities,
  and external knowledge enhancement.
---

# LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction

## Quick Facts
- arXiv ID: 2408.12249
- Source URL: https://arxiv.org/abs/2408.12249
- Reference count: 40
- Primary result: Standard prompting outperforms complex reasoning and knowledge-augmentation techniques for biomedical structured prediction tasks

## Executive Summary
This paper systematically benchmarks LLM performance on medical classification and Named Entity Recognition (NER) tasks, evaluating various prompting techniques including standard prompting, chain-of-thought reasoning, self-consistency, and retrieval-augmented generation with PubMed and Wikipedia corpora. The study reveals that standard prompting consistently outperforms more complex techniques across both tasks, suggesting that reasoning and knowledge-enhancing methods developed for knowledge-intensive tasks are not easily transferable to structured prediction tasks requiring precise outputs. Parametric knowledge capacity (model size) was found to be the primary driver of zero-shot performance, with the 70B model showing notable gains over the 7B model (5.4% for classification, 2.2% for NER Span F1).

## Method Summary
The study evaluates three open models (BioMistral-7B, Llama-2-70B, Llama-2-7B) on 14 classification datasets and 12 NER datasets from the BigBio collection using "true" zero-shot evaluation without any examples. Five prompting techniques are tested: VANILLA (standard prompting), COT (chain-of-thought reasoning), SC (self-consistency), RAG-P (retrieval-augmented generation with PubMed), and RAG-W (RAG with Wikipedia). The evaluation uses constrained decoding to ensure structured outputs and measures performance using Micro-F1 scores for classification and Span-F1/full F1 for NER tasks.

## Key Results
- Standard prompting consistently outperforms complex techniques like chain-of-thought and RAG across both classification and NER tasks
- Model size (parametric knowledge capacity) is the primary driver of zero-shot performance, with 70B models showing 5.4% and 2.2% gains over 7B models
- Retrieval-augmented generation with PubMed and Wikipedia does not improve information extraction performance and may introduce irrelevant information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard prompting outperforms complex reasoning and knowledge-augmentation techniques for structured biomedical information extraction tasks.
- Mechanism: Simple task-specific instructions allow LLMs to focus on extracting structured outputs without the overhead of reasoning chains or external knowledge retrieval that may introduce irrelevant information.
- Core assumption: For structured prediction tasks requiring precise outputs, additional reasoning steps or retrieved context introduce noise rather than helpful information.
- Evidence anchors:
  - [abstract]: "our results reveal that standard prompting consistently outperforms more complex techniques across both tasks"
  - [section]: "Counter intuitively, our results reveal that standard prompting consistently outperforms more complex techniques across both tasks"
  - [corpus]: Weak evidence - corpus shows related work on retrieval-augmented generation and biomedical LLMs, but doesn't directly support this mechanism
- Break condition: If tasks require broader contextual understanding or reasoning beyond the immediate input, standard prompting may fail and more sophisticated techniques could become necessary.

### Mechanism 2
- Claim: Model size (parametric knowledge capacity) is the primary driver of zero-shot performance in biomedical structured prediction tasks.
- Mechanism: Larger models have more parameters to encode domain-specific knowledge and linguistic patterns, enabling better performance without additional prompting techniques.
- Core assumption: The increased parameter count directly translates to better domain knowledge representation and task understanding.
- Evidence anchors:
  - [abstract]: "parametric knowledge capacity, i.e., model size, is the primary driver of zero-shot performance"
  - [section]: "Consistent with prior findings, the 70B model shows notable gains over the 7B model (5.4% for classification, 2.2% for NER Span F1)"
  - [corpus]: Weak evidence - corpus contains related papers on biomedical LLMs but doesn't directly support this mechanism
- Break condition: If task complexity exceeds what can be captured by parameter scaling alone, or if specific domain knowledge requires targeted training rather than general parameter scaling.

### Mechanism 3
- Claim: Advanced prompting techniques like Chain-of-Thought and Retrieval-Augmented Generation are designed for knowledge-intensive tasks and don't transfer well to structured prediction tasks requiring precise outputs.
- Mechanism: Techniques developed for QA or reasoning tasks assume the need for broad knowledge synthesis, but structured prediction tasks require precise understanding of task semantics and domain-specific vocabulary rather than general reasoning ability.
- Core assumption: The underlying assumptions of CoT and RAG (need for broad knowledge integration) don't match the requirements of structured prediction tasks.
- Evidence anchors:
  - [abstract]: "advanced prompting methods developed for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are not easily portable to biomedical tasks where precise structured outputs are required"
  - [section]: "These tasks rely less on broad knowledge from biomedical papers or Wikipedia and more on domain-specific application within the given input"
  - [corpus]: Weak evidence - corpus shows related work on RAG for biomedical tasks but doesn't directly support this mechanism
- Break condition: If structured prediction tasks evolve to require more reasoning or external knowledge integration, these techniques might become more effective.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The paper evaluates models' ability to perform tasks without any task-specific examples, which is crucial for understanding real-world applicability where annotated data is scarce.
  - Quick check question: What distinguishes "true" zero-shot learning from few-shot learning in the context of this paper?

- Concept: Parametric vs. non-parametric knowledge
  - Why needed here: The paper distinguishes between knowledge embedded in model weights versus knowledge retrieved from external sources, which is central to understanding why certain techniques work or don't work.
  - Quick check question: How does the paper define parametric knowledge versus external knowledge, and why is this distinction important for biomedical information extraction?

- Concept: Named Entity Recognition (NER) vs. classification
  - Why needed here: The paper benchmarks both tasks, which have different output requirements and challenges in the biomedical domain.
  - Quick check question: What are the key differences between NER and classification tasks in terms of output structure and evaluation metrics used in this study?

## Architecture Onboarding

- Component map:
  Dataset loading and preprocessing pipeline -> Model loading and configuration (BioMistral-7B, Llama-2-70B, Llama-2-7B) -> Prompting technique modules (VANILLA, COT, SC, RAG-P, RAG-W) -> Evaluation metrics computation (Micro-F1 for classification, Span-F1 and full F1 for NER) -> Result aggregation and analysis

- Critical path:
  1. Load dataset and preprocess text
  2. Select model and prompting technique
  3. Generate predictions with constrained decoding
  4. Compute evaluation metrics
  5. Aggregate results across datasets

- Design tradeoffs:
  - Using open-source models enables constrained decoding but limits model choice compared to proprietary models
  - Focusing on "true" zero-shot setting provides real-world relevance but may underestimate potential with few-shot learning
  - Using only 10 labels per task simplifies evaluation but may not capture full task complexity

- Failure signatures:
  - Poor performance across all models and techniques suggests dataset leakage or fundamental task difficulty
  - Specific techniques failing on certain datasets may indicate technique-dataset mismatch
  - Large performance gaps between 7B and 70B models may indicate insufficient parametric knowledge in smaller models

- First 3 experiments:
  1. Run VANILLA prompting on a simple classification dataset (e.g., CAS) to establish baseline performance
  2. Test COT on the same dataset to verify if reasoning techniques degrade performance
  3. Evaluate constrained decoding effectiveness by comparing outputs with and without constraints on a small NER dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications would enable LLMs to better handle domain-specific vocabulary, jargon, and syntactic nuances in biomedical structured prediction tasks?
- Basis in paper: [inferred] The paper notes that models struggle with specialized vocabulary, jargon, acronyms, and synonyms varying across subfields, as well as ambiguity, polysemy, and syntactic nuances in biomedical concepts.
- Why unresolved: The paper demonstrates these challenges but does not propose or test specific architectural solutions to address them.
- What evidence would resolve it: Empirical results comparing baseline LLMs with modified architectures incorporating specialized biomedical token embeddings, hierarchical attention mechanisms for domain-specific contexts, or structured output decoders trained on biomedical corpora.

### Open Question 2
- Question: How can retrieval-augmented generation be effectively adapted for structured prediction tasks rather than just knowledge-intensive QA tasks?
- Basis in paper: [explicit] The paper shows RAG does not help information extraction and suggests irrelevant information misleads models, but doesn't explore alternative RAG implementations for structured tasks.
- Why unresolved: The paper tests standard RAG approaches but doesn't investigate domain-specific corpora, fine-grained retrieval strategies, or RAG modifications tailored to structured output requirements.
- What evidence would resolve it: Results from experiments testing domain-specific retrieval corpora (e.g., biomedical ontologies), hierarchical retrieval strategies that first identify relevant entity types before retrieving examples, or RAG approaches that incorporate structured output constraints during retrieval.

### Open Question 3
- Question: What are the fundamental limitations of current prompting techniques for structured prediction tasks, and how can they be overcome?
- Basis in paper: [explicit] The paper demonstrates that chain-of-thought, self-consistency, and RAG techniques developed for knowledge-intensive tasks do not transfer well to structured prediction requiring precise outputs.
- Why unresolved: While the paper identifies this limitation, it doesn't explore alternative prompting strategies, task decomposition methods, or prompt engineering techniques specifically designed for structured biomedical tasks.
- What evidence would resolve it: Comparative results showing the effectiveness of novel prompting strategies such as task-specific decomposition prompts, template-based structured output generation, or iterative refinement prompting approaches against current methods on the same benchmark datasets.

## Limitations
- The study focuses on "true" zero-shot learning without any examples, which may underestimate the potential of advanced prompting techniques that could benefit from few-shot demonstrations
- The evaluation uses only 10 labels per task for NER, which may not capture the full complexity of biomedical entity recognition
- Retrieval-augmented generation experiments use only PubMed and Wikipedia as knowledge sources, potentially missing domain-specific biomedical knowledge bases

## Confidence
**High confidence**: The finding that parametric knowledge capacity (model size) is the primary driver of zero-shot performance, supported by consistent 5.4% and 2.2% performance gaps between 7B and 70B models across both classification and NER tasks. The observation that standard prompting consistently outperforms complex techniques is also well-supported, with multiple datasets showing this pattern.

**Medium confidence**: The claim that advanced prompting techniques are not easily transferable to structured prediction tasks. While the empirical results support this, the mechanism explaining why (that these techniques assume broader knowledge integration than needed) is somewhat speculative and would benefit from further investigation.

**Low confidence**: The assertion that reasoning and knowledge-enhancing methods developed for knowledge-intensive tasks are fundamentally incompatible with structured prediction tasks. The study shows current techniques don't work well, but doesn't conclusively prove that better-designed variants couldn't succeed.

## Next Checks
1. **Cross-domain validation**: Test whether the standard prompting advantage holds in other structured prediction domains (e.g., legal document analysis, financial statement extraction) to determine if the findings are specific to biomedical tasks or represent a more general principle about structured prediction.

2. **Hybrid prompting experiment**: Design a controlled experiment comparing standard prompting against carefully optimized versions of chain-of-thought and retrieval-augmented generation specifically tailored for structured output tasks, rather than applying these techniques directly from their original domains.

3. **Knowledge source diversity test**: Evaluate RAG performance using biomedical-specific knowledge sources (e.g., clinical guidelines, drug databases, medical ontologies) rather than general Wikipedia and PubMed to determine if the knowledge source choice, rather than the RAG technique itself, explains the poor performance.