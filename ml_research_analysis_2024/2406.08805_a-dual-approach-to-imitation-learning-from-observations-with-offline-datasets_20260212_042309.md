---
ver: rpa2
title: A Dual Approach to Imitation Learning from Observations with Offline Datasets
arxiv_id: '2406.08805'
source_url: https://arxiv.org/abs/2406.08805
tags:
- expert
- learning
- dilo
- offline
- imitation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DILO is a novel offline imitation learning method for learning
  from observation data without expert actions. It directly learns a multi-step utility
  function quantifying the effect of state transitions on divergence from expert behavior,
  bypassing intermediate model learning.
---

# A Dual Approach to Imitation Learning from Observations with Offline Datasets

## Quick Facts
- **arXiv ID**: 2406.08805
- **Source URL**: https://arxiv.org/abs/2406.08805
- **Reference count**: 40
- **Key outcome**: DILO achieves improved performance over prior LfO methods across diverse datasets including high-dimensional observations and real-robot tasks

## Executive Summary
DILO addresses the challenge of imitation learning from observation (LfO) when expert actions are unavailable but suboptimal interaction data is accessible. The method directly learns a multi-step utility function quantifying how state transitions impact divergence from expert behavior, bypassing intermediate model learning that introduces compounding errors. By converting the constrained distribution matching problem to its dual form, DILO reduces to a single-player optimization solvable by actor-critic methods similar to offline RL. Experiments demonstrate consistent gains in normalized returns across MuJoCo environments and superior performance in real-world manipulation tasks compared to baselines like BCO and SMODICE.

## Method Summary
DILO is an offline imitation learning method that learns from observation-only expert demonstrations and suboptimal interaction data. It directly learns a multi-step utility function V(s,s') that measures divergence from expert visitation distributions, using a dual formulation of distribution matching with χ² divergence. The method extracts policies via value-weighted regression on offline data and employs orthogonal gradient updates to handle the challenging optimization. DILO can leverage arbitrary suboptimal data through mixture distribution matching, allowing it to learn recovery behaviors and improve performance even with limited expert demonstrations.

## Key Results
- DILO achieves 35.0% improvement over BC on Walker2d-medium-v0 with mixed datasets
- Consistently outperforms baselines (SMODICE, ORIL, IQ-Learn, ReCOIL) on D4RL locomotion tasks
- Demonstrates 87% success rate on Door task and 75% on Lift task in real-robot experiments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DILO bypasses compounding errors from intermediate models by directly learning a multi-step utility function that quantifies how state transitions impact divergence from expert visitation.
- **Mechanism**: Instead of learning one-step models (discriminators or IDMs) that introduce errors at each timestep, DILO frames imitation learning as joint state-next state distribution matching. This allows it to learn a utility function V(ps, s1) that measures the long-term effect of transitioning to s1 from s on divergence from expert behavior. The dual formulation converts this into an unconstrained optimization that can be solved by an actor-critic approach.
- **Core assumption**: The next state contains sufficient information about the missing action to enable effective distribution matching without explicitly learning inverse dynamics or reward functions.
- **Evidence anchors**:
  - [abstract]: "DILO directly learns a multi-step utility function quantifying the effect of state transitions on divergence from expert behavior, bypassing intermediate model learning."
  - [section]: "We overcome these limitations by directly learning a multi-step utility function that quantifies how each action impacts the agent's divergence from the expert's visitation distribution."
  - [corpus]: No direct evidence found in corpus neighbors about multi-step utility function learning. Weak corpus support for this mechanism.
- **Break condition**: If the next state does not adequately encode information about the missing action, or if the distribution mismatch between agent and expert is too large for joint state-next state matching to be effective.

### Mechanism 2
- **Claim**: DILO reduces imitation learning to a single-player optimization problem similar to offline RL, improving stability and performance.
- **Mechanism**: By converting the constrained distribution matching objective to its dual form, DILO creates an unconstrained optimization problem that can be solved with standard actor-critic methods. This eliminates the need for min-max optimization between discriminator and policy, and avoids the distribution shift issues that plague traditional offline RL methods.
- **Core assumption**: The dual formulation provides a stable and effective optimization objective that can be solved with standard RL techniques without requiring on-policy data collection.
- **Evidence anchors**:
  - [abstract]: "DILO reduces the learning from observations problem to that of simply learning an actor and a critic, bearing similar complexity to vanilla offline RL."
  - [section]: "DILO solves a single-player objective, making the learning stable and more performant."
  - [corpus]: No direct evidence in corpus neighbors about single-player optimization or comparison to offline RL complexity. Weak corpus support.
- **Break condition**: If the dual objective becomes unstable during optimization, or if standard actor-critic methods cannot effectively solve the resulting optimization problem.

### Mechanism 3
- **Claim**: DILO can leverage arbitrary suboptimal data through mixture distribution matching, improving sample efficiency and robustness.
- **Mechanism**: DILO incorporates offline interaction data through a convex mixture distribution matching objective with linear constraints. This allows the algorithm to use suboptimal data to learn recovery behaviors and improve performance even when expert demonstrations are limited.
- **Core assumption**: The mixture distribution matching objective preserves the fixed point of optimization and provides a principled way to incorporate offline data of arbitrary quality.
- **Evidence anchors**:
  - [abstract]: "DILO (Dual Imitation Learning from Observations), an algorithm that can leverage arbitrary suboptimal data to learn imitating policies without requiring expert actions."
  - [section]: "The mixture distribution matching objective preserves the fixed point of optimization d*π(s, s1, a1) = dE(s, s1, a1) irrespective of mixing parameter β."
  - [corpus]: No direct evidence in corpus neighbors about mixture distribution matching or leveraging arbitrary suboptimal data. Weak corpus support.
- **Break condition**: If the suboptimal data is too poor quality or too different from expert behavior, or if the mixture ratio β cannot effectively balance expert and suboptimal data.

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs)
  - **Why needed here**: The paper operates within the MDP framework, defining states, actions, transitions, rewards, and policies. Understanding MDPs is crucial for grasping how DILO frames imitation learning as a distribution matching problem.
  - **Quick check question**: What is the difference between state-action visitation distribution dπ(s,a) and joint state-next state visitation distribution dπ(s,s1,a1)?

- **Concept**: f-divergences and distribution matching
  - **Why needed here**: DILO uses f-divergences to measure the difference between agent and expert visitation distributions. Understanding how f-divergences work and how they relate to distribution matching is essential for understanding the objective function.
  - **Quick check question**: How does the Pearson chi-square divergence differ from KL divergence, and why might it be preferred in this context?

- **Concept**: Lagrangian duality and convex optimization
  - **Why needed here**: The core insight of DILO is converting the constrained distribution matching problem to its dual form. Understanding duality theory and how it applies to optimization problems is crucial for understanding why DILO works.
  - **Quick check question**: What are the KKT conditions, and how do they ensure that the dual problem provides the same solution as the primal?

## Architecture Onboarding

- **Component map**:
  - Value network Vϕ -> Policy network πψ -> Orthogonal gradient update -> Value-weighted regression

- **Critical path**:
  1. Sample s, s1, s2 from mixture of expert and offline datasets
  2. Update Vϕ using orthogonal gradient update on dual objective
  3. Update πψ using value-weighted regression on offline dataset
  4. Evaluate policy and repeat

- **Design tradeoffs**:
  - Mixture ratio β controls balance between expert and offline data
  - Temperature τ in value-weighted regression controls policy conservatism
  - Conservatism parameter λ in image experiments controls optimism/pessimism
  - Orthogonal gradient descent parameter η controls stability

- **Failure signatures**:
  - V(ps, s1) values diverging to infinity or becoming NaN
  - Policy πψ collapsing to a single action or becoming random
  - Performance degrading on expert-only tasks
  - Training instability or oscillations

- **First 3 experiments**:
  1. Run DILO on Hopper with random+expert dataset to verify basic functionality
  2. Test on Walker2d with medium+few-expert dataset to verify performance on limited expert data
  3. Evaluate on Pen manipulation task to verify performance on high-dimensional observations

## Open Questions the Paper Calls Out

- **Question**: How does DILO's performance scale with dataset size and expert trajectory quality in high-dimensional observation spaces like image-based manipulation tasks?
- **Basis in paper**: [explicit] The paper mentions that SMODICE struggles with high-dimensional observations due to discriminator overfitting, while DILO shows improved performance without extensive hyperparameter tuning. However, the scaling relationship with dataset size is not explicitly explored.
- **Why unresolved**: The paper only tests DILO on a limited set of image-based tasks with fixed dataset sizes. The relationship between dataset size, expert trajectory quality, and performance in high-dimensional spaces remains unexplored.
- **What evidence would resolve it**: Controlled experiments varying dataset sizes and expert trajectory quality in image-based manipulation tasks, comparing DILO's performance to baselines across different dataset regimes.

- **Question**: Can DILO be extended to learn from expert demonstrations in a semantic representation space rather than raw observation space, and how would this affect its performance?
- **Basis in paper**: [inferred] The authors acknowledge this as a limitation, noting that DILO assumes matching visitation distributions in the observation space rather than a meaningful semantic space. They suggest this could be addressed with better universal representations.
- **Why unresolved**: The paper does not explore learning from semantic representations, focusing instead on raw observations. The potential benefits and challenges of semantic space matching are not investigated.
- **What evidence would resolve it**: Implementation of DILO with semantic representations (e.g., learned embeddings) and comparison to raw observation performance across multiple tasks and datasets.

- **Question**: How does DILO's conservatism parameter τ affect its ability to handle distribution shift and extrapolation in real-world robotics tasks?
- **Basis in paper**: [explicit] The authors note that DILO uses a conservatism parameter τ to regulate extrapolation and mention that adaptively selecting τ is an area of active investigation. They also observe failure modes like "dead zone" behavior in low-dim observations.
- **Why unresolved**: The paper only tests a fixed τ value across all tasks and does not explore the parameter's impact on distribution shift handling or real-world performance.
- **What evidence would resolve it**: Systematic study of τ's effect on DILO's performance in tasks with varying levels of distribution shift, including real-world robotics scenarios with significant state space differences from training data.

## Limitations

- The paper lacks explicit comparison of DILO's performance on expert-only datasets versus mixed datasets, making it difficult to isolate the benefits of leveraging suboptimal data
- The orthogonal gradient update mechanism is mentioned as crucial but not thoroughly explained or empirically validated for its necessity
- The claim that DILO's utility function learning is more stable than discriminator-based methods is not directly tested with ablation studies comparing to discriminator-only variants

## Confidence

- **High confidence**: Empirical results showing DILO's superiority over baselines on D4RL and Robomimic datasets
- **Medium confidence**: Theoretical claims about bypassing compounding errors through direct utility function learning, as empirical evidence is primarily comparative
- **Low confidence**: Practical utility of the orthogonal gradient update mechanism without more detailed explanation or ablation studies

## Next Checks

1. Run ablation studies removing the orthogonal gradient update to quantify its impact on stability and performance
2. Test DILO on expert-only datasets to isolate the benefits of leveraging suboptimal data
3. Compare DILO's performance to a discriminator-only baseline that learns one-step models to validate claims about avoiding compounding errors