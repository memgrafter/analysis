---
ver: rpa2
title: 'HAF-RM: A Hybrid Alignment Framework for Reward Model Training'
arxiv_id: '2407.04185'
source_url: https://arxiv.org/abs/2407.04185
tags:
- uni00000011
- reward
- uni00000014
- uni00000018
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid alignment framework for training
  reward models in large language models. The method combines sequence-level reward
  optimization with token-level policy supervision, sharing an internal preference
  model between both components.
---

# HAF-RM: A Hybrid Alignment Framework for Reward Model Training

## Quick Facts
- arXiv ID: 2407.04185
- Source URL: https://arxiv.org/abs/2407.04185
- Reference count: 35
- Primary result: Hybrid framework combining sequence-level reward optimization with token-level policy supervision achieves 76.52% accuracy vs 73.16% baseline

## Executive Summary
HAF-RM introduces a hybrid alignment framework for training reward models by combining sequence-level reward optimization with token-level policy supervision. The method shares an internal preference model between both components, enabling more effective calibration of the preference space. Experiments on five datasets demonstrate improved accuracy over baseline and DPO methods, with HAF achieving up to 76.52% accuracy compared to 73.16% for baseline. The approach shows better generalization to out-of-distribution data and superior performance in downstream tasks like best-of-N sampling and RLHF.

## Method Summary
HAF-RM trains reward models using a hybrid approach that combines reward loss at the sequence level with policy loss at the token level, sharing an internal preference model between both components. The framework treats reward models as decoupled architectures with an internal preference model outputting token-level representations and a reward layer mapping these to sequence-level rewards. The policy model, which can serve as an implicit reward model through generation probabilities, provides token-level supervision that acts as regularization to prevent internal representation degradation. The method integrates DPO loss to combine concentrated data-fitting characteristics with generalization capabilities.

## Key Results
- HAF achieves 76.52% accuracy on average across five datasets compared to 73.16% for baseline methods
- HAF shows superior performance in best-of-N sampling tasks with 75.87% win rate versus 72.18% for baseline
- In RLHF experiments, HAF-trained models achieve 64.07% win rates compared to 55.50% for baseline methods
- HAF demonstrates better generalization to out-of-distribution data than both baseline and DPO approaches

## Why This Works (Mechanism)

### Mechanism 1
Adding token-level policy supervision improves reward model calibration and generalization by introducing an additional policy loss that supervises the shared internal preference model at the token level, while the reward model optimizes sequence-level rewards. This dual supervision calibrates the preference space more effectively than sequence-level supervision alone. The core assumption is that the internal preference model captures similar representations for both reward and policy predictions, and token-level supervision provides complementary signal to sequence-level supervision.

### Mechanism 2
Policy loss acts as regularization preventing internal representation degradation by serving as a regularization term that maintains the quality of the internal preference model's representations during training. This prevents the model from degrading when optimizing for sequence-level rewards. The core assumption is that the policy model's token-level probability prediction requires maintaining high-quality internal representations, which benefits the reward model when shared.

### Mechanism 3
HAF captures both concentrated data-fitting and generalization capabilities by integrating DPO loss into the hybrid framework. This combines DPO's concentrated data-fitting characteristics with the baseline method's generalization ability, achieving better performance on both individual and mixed preference distributions. The core assumption is that DPO's concentrated data-fitting approach provides benefits for certain preference types while the baseline method provides better generalization, and combining both approaches yields superior overall performance.

## Foundational Learning

- **Bradley-Terry model for preference modeling**: Why needed - The reward model training uses the Bradley-Terry model to transform reward prediction into a probability optimization problem, fundamental to understanding how pairwise comparisons are converted to loss functions. Quick check - What mathematical transformation does the Bradley-Terry model apply to convert reward differences into win probabilities?

- **Decoupled reward model architecture (internal preference model + reward layer)**: Why needed - HAF's effectiveness relies on understanding that reward models can be split into an internal preference model that outputs token-level representations and a reward layer that maps these to sequence-level rewards. Quick check - How does the internal preference model differ from the final reward prediction in the HAF architecture?

- **Policy model as implicit reward model**: Why needed - Understanding that policy models can serve as reward models through their generation probabilities is crucial for grasping why HAF can share the internal preference model between reward and policy components. Quick check - What mathematical relationship connects policy model probabilities to implicit reward values?

## Architecture Onboarding

- **Component map**: Input → Internal preference model → (Reward layer + Policy layer) → Dual losses → Parameter updates
- **Critical path**: Input → Internal preference model → (Reward layer + Policy layer) → Dual losses → Parameter updates
- **Design tradeoffs**: 
  - Sharing vs. separate internal preference models for reward and policy
  - Weighting between reward loss and policy loss (α hyperparameter)
  - Choice of policy loss function (DPO vs alternatives)
  - Model size vs. training efficiency (full-parameter vs. LoRA)
- **Failure signatures**:
  - Reward loss dominates → Poor generalization, overfitting to specific preference patterns
  - Policy loss dominates → Loss of sequence-level reward quality, excessive focus on token-level accuracy
  - Imbalanced internal representations → Inconsistent preferences between reward and policy outputs
  - Training instability → Oscillating losses or gradient explosions
- **First 3 experiments**:
  1. Train baseline reward model (reward loss only) on a single dataset and evaluate accuracy
  2. Train HAF with various α values on the same dataset and compare performance curves
  3. Test both models on out-of-distribution data to measure generalization differences

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal policy loss ratio (α) for HAF training across different types of datasets and model architectures? The paper mentions that a policy ratio of 0.2 was used, but notes that even a mere 0.1x of policy loss can significantly impact results, and explores different ratios in Figure 7. This remains unresolved because the paper shows sensitivity to the policy ratio but does not determine optimal values for different scenarios. Systematic experiments testing multiple policy ratios across different dataset types and model architectures with statistical analysis would resolve this.

### Open Question 2
How does the HAF framework perform when extended to multi-modal reward models that need to evaluate both text and image inputs? The paper focuses on text-based reward modeling, but the framework's decoupling of internal preference modeling from reward prediction could theoretically extend to other modalities. This remains unresolved because the paper does not explore applications beyond text-based language models. Experiments applying HAF to multi-modal datasets would resolve this.

### Open Question 3
What is the relationship between the internal preference model learned by HAF and human interpretability of preferences? The paper discusses the internal preference model as a shared component but does not analyze its interpretability or alignment with human-understandable features. This remains unresolved because the paper demonstrates improved performance but does not investigate whether the learned preferences correspond to human-understandable concepts. Analysis techniques like probing or human studies would resolve this.

### Open Question 4
How does HAF compare to alternative regularization approaches for reward model training, such as entropy regularization or contrastive learning? The paper presents HAF as superior to baseline and DPO methods but does not compare against other regularization techniques that could serve similar purposes. This remains unresolved because the paper establishes HAF's advantages over specific baselines but does not explore whether other regularization methods could achieve similar or better results. Comparative experiments testing HAF against reward models trained with various regularization techniques would resolve this.

## Limitations

- Architecture Generalization: The framework's performance on highly specialized domains or multi-modal data remains untested
- Hyperparameter Sensitivity: The α parameter balancing reward and policy losses is critical but receives limited sensitivity analysis
- Computational Overhead: The paper doesn't quantify the training time increase relative to baseline methods

## Confidence

- **High Confidence**: The claim that HAF-RM achieves improved accuracy over baseline methods (76.52% vs 73.16%) is well-supported by experimental results across multiple datasets and model sizes
- **Medium Confidence**: The mechanism explanation for why hybrid supervision works (calibration of shared preference space) is plausible but relies on assumptions about internal representation alignment that aren't directly verified
- **Low Confidence**: The claim about HAF inheriting both DPO's concentrated data-fitting and baseline's generalization capability is asserted but not empirically validated through controlled experiments

## Next Checks

1. **Ablation Study on Architecture Components**: Systematically remove either the reward layer or policy layer while keeping the shared internal preference model, then evaluate performance degradation to quantify how much each component contributes to the hybrid advantage

2. **Cross-Domain Transfer Experiment**: Train HAF-RM on one domain (e.g., Dahoas) and test on completely different preference distributions (e.g., medical or legal preference data) to validate claimed generalization improvements and identify limitations

3. **Policy Loss Sensitivity Analysis**: Conduct a grid search over α values (0.1 to 0.9) and policy loss formulations to identify the optimal configuration and determine whether performance improvements are robust to implementation details