---
ver: rpa2
title: Characterizing Truthfulness in Large Language Model Generations with Local
  Intrinsic Dimension
arxiv_id: '2402.18048'
source_url: https://arxiv.org/abs/2402.18048
tags:
- intrinsic
- dimension
- language
- truthfulness
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using local intrinsic dimension (LID) of model
  activations to characterize and predict the truthfulness of large language model
  (LLM) outputs. The key idea is that truthful outputs have lower LIDs than untruthful
  ones, because they are closer to natural language structure.
---

# Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension

## Quick Facts
- arXiv ID: 2402.18048
- Source URL: https://arxiv.org/abs/2402.18048
- Authors: Fan Yin; Jayanth Srinivasa; Kai-Wei Chang
- Reference count: 40
- Primary result: LID-based methods detect LLM hallucinations with up to 8% better AUROC than uncertainty-based and trained classifier approaches

## Executive Summary
This paper introduces a novel approach for detecting hallucinations in large language model (LLM) outputs by leveraging local intrinsic dimension (LID) of model activations. The key insight is that truthful outputs have lower LIDs than untruthful ones because they are closer to natural language structure. The authors develop a Maximum Likelihood Estimation (MLE) method with distance-aware corrections to estimate LID values, then use these as features for binary truthfulness classification. Experiments on four QA datasets with Llama-2 models show that LID-based methods outperform uncertainty-based and trained classifier baselines, achieving up to 8% improvement in AUROC.

## Method Summary
The approach involves extracting intermediate layer representations from LLM-generated text, computing LID values using a corrected MLE estimator that accounts for non-linearity in language representations, and using these LID values as features for truthfulness prediction. The method involves selecting the optimal layer based on maximizing summed LID values (with a "shift behind" adjustment), then calculating LID values for the last token's representation. The LID estimation uses a Poisson process approximation with distance-aware corrections to improve accuracy. The resulting LID features are used in a binary classifier to predict whether outputs are truthful or hallucinated.

## Key Results
- LID-based methods achieve up to 8% improvement in AUROC over uncertainty-based and trained classifier baselines for hallucination detection
- Truthful outputs consistently exhibit lower LID values than untruthful outputs across all tested datasets
- LID values show a characteristic "hunchback" shape across transformer layers, with optimal performance typically occurring one layer after maximum LID sum
- Cross-dataset neighbor usage (training on one dataset, testing on another) shows only slight performance degradation, suggesting robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Truthful outputs have lower local intrinsic dimension (LID) because they are closer to natural language structure, while untruthful outputs mix human and model distributions leading to higher LID.
- Mechanism: LID measures the minimal number of activations needed to characterize a point in the activation manifold without significant information loss. Truthful outputs lie in a more structured manifold (lower dimension), while hallucinations create a mixed distribution (higher dimension).
- Core assumption: LLM representations lie in lower-dimensional manifolds due to model inductive bias and natural language structure.
- Evidence anchors: Abstract states "truthful outputs have lower LIDs than untruthful ones, because they are closer to natural language structure"; section 1 explains LLM representations are believed to lie in lower-dimensional manifolds.

### Mechanism 2
- Claim: The maximum likelihood estimation (MLE) method for LID is well-suited for individual sample estimation and can be corrected for non-linearity in language representations.
- Mechanism: MLE fits a Poisson process to neighbor counts around a sample, parameterized by LID. The paper proposes corrections to account for non-linearity and optimal representation selection.
- Core assumption: The density function around a sample can be approximated and corrected for non-uniformity.
- Evidence anchors: Section 3.2 describes MLE approximating neighbor counts with a Poisson process; section 3.3 explains correction function replacement based on manifold geometric properties.

### Mechanism 3
- Claim: The layer selection strategy based on maximizing summed LID values provides the most informative feature for truthfulness detection.
- Mechanism: The paper observes that LID values correlate with detection performance but exhibit a "shift behind" effect, so it selects the layer with maximum summed LID values plus one.
- Core assumption: The relationship between LID values and detection performance is consistent across different layers.
- Evidence anchors: Section 3.3 notes representations from the last layer might not yield the most informative feature and performance correlates with absolute value of summed LIDs; section 5.1 observes "shift behind" phenomenon.

## Foundational Learning

- Concept: Intrinsic dimension estimation
  - Why needed here: Understanding how to measure the dimensionality of data manifolds is crucial for implementing LID-based truthfulness detection.
  - Quick check question: What is the difference between local and global intrinsic dimension estimation, and why does the paper prefer local estimation?

- Concept: Maximum likelihood estimation (MLE) for Poisson processes
  - Why needed here: The MLE framework forms the basis for the LID estimation method used in the paper.
  - Quick check question: How does the MLE method approximate neighbor counts with a Poisson process, and what is the role of the rate parameter Î»(t)?

- Concept: Layer-wise representations in transformers
  - Why needed here: The paper operates on intermediate representations at different layers, requiring understanding of how information flows through transformer layers.
  - Quick check question: How do representations change across transformer layers, and why might the last layer not always be optimal for truthfulness detection?

## Architecture Onboarding

- Component map: LLM-generated text -> intermediate layer representations -> LID estimation with corrected MLE -> truthfulness prediction -> AUROC evaluation

- Critical path:
  1. Generate text with LLM
  2. Extract representations from selected layer
  3. Compute LID values using corrected MLE
  4. Calculate truthfulness scores
  5. Evaluate with AUROC

- Design tradeoffs:
  - Neighbor count T vs. computational cost and variance
  - Layer selection vs. detection performance
  - Correction complexity vs. estimation accuracy
  - Cross-dataset neighbor usage vs. performance degradation

- Failure signatures:
  - LID values don't correlate with truthfulness (check correlation plots)
  - Performance doesn't improve with more neighbors (check neighbor robustness)
  - Layer selection doesn't match expected patterns (check hunchback shape)
  - Cross-dataset neighbors perform significantly worse (check domain shift)

- First 3 experiments:
  1. Verify LID values differ between truthful and untruthful samples on a small dataset
  2. Test neighbor count robustness by varying T and observing performance changes
  3. Compare layer selection by plotting LID vs. performance curves across layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the intrinsic dimension vary across different model architectures beyond Llama-2 (e.g., GPT-3, OPT, or smaller models)?
- Basis in paper: The paper shows results primarily for Llama-2 models and mentions testing on Llama, OPT, and other models in preliminary experiments, but does not provide a comprehensive comparison.
- Why unresolved: The paper does not systematically compare intrinsic dimension behavior across different model architectures, leaving uncertainty about whether the observed patterns are universal or specific to Llama-2.
- What evidence would resolve it: Conducting a systematic study measuring intrinsic dimensions across multiple model architectures (GPT-3, OPT, Bloom, etc.) and comparing their layer-wise and position-wise patterns would clarify generalizability.

### Open Question 2
- Question: What is the relationship between intrinsic dimension and the specific content or complexity of the generated text (e.g., factual vs. opinion-based, simple vs. complex language)?
- Basis in paper: The paper focuses on detecting hallucinations in factual question-answering tasks but does not explore how intrinsic dimension correlates with the semantic content or complexity of the text.
- Why unresolved: While the paper shows that truthful answers have lower intrinsic dimensions, it does not investigate whether this is due to content type (factual vs. opinion) or linguistic complexity, which could confound the results.
- What evidence would resolve it: Analyzing intrinsic dimensions across diverse text types (factual, opinion, creative writing, etc.) and controlling for linguistic complexity would reveal whether the observed patterns are content-specific or generalizable.

### Open Question 3
- Question: How does the intrinsic dimension evolve during the training process of a model, and does it correlate with specific training objectives or data distributions?
- Basis in paper: The paper investigates how intrinsic dimensions change during instruction tuning and correlates them with model performance, but does not explore the evolution during initial pre-training or fine-tuning on specific datasets.
- Why unresolved: The paper only examines instruction tuning on a single dataset (SUPER-NI) and does not explore how intrinsic dimensions evolve during pre-training or fine-tuning on other datasets or objectives.
- What evidence would resolve it: Tracking intrinsic dimensions across different stages of model training (pre-training, fine-tuning, instruction tuning) and correlating them with specific training objectives or data distributions would clarify their role in model development.

### Open Question 4
- Question: Can intrinsic dimension be used to detect other types of model errors or biases beyond hallucinations, such as toxic outputs or adversarial attacks?
- Basis in paper: The paper discusses potential extensions to detecting harmful prompts or adversarial data but does not empirically explore these applications.
- Why unresolved: While the paper suggests that intrinsic dimension could be a general tool for understanding model behavior, it does not test its effectiveness in detecting other types of errors or biases.
- What evidence would resolve it: Experimenting with intrinsic dimension as a detection tool for toxic outputs, adversarial attacks, or other biases would validate its broader applicability beyond hallucination detection.

## Limitations

- The generalizability of LID-based truthfulness detection across different model architectures remains uncertain, as experiments primarily focus on Llama-2 models.
- The relationship between LID values and truthfulness is correlational rather than causally established, lacking deeper theoretical explanation for the observed patterns.
- Performance gains on smaller datasets like TruthfulQA are less pronounced, suggesting the method may be less effective in certain domains.

## Confidence

- Confidence in the core claims: Medium-High (experimental results are compelling but generalizability is uncertain)
- Confidence in the mathematical framework: High (MLE-based LID estimation with corrections is well-established)
- Confidence in practical implementation details: Medium (some implementation specifics like correction parameters need further validation)

## Next Checks

1. Cross-architecture validation: Test the LID-based truthfulness detection on non-transformer architectures (e.g., RNNs, CNNs) and different model families (e.g., GPT, Claude) to assess generalizability.

2. Ablation study on correction terms: Systematically remove or modify the distance-aware corrections in the MLE method to quantify their individual contributions to performance gains.

3. Adversarial robustness evaluation: Design adversarial examples specifically targeting the LID detection mechanism to identify potential vulnerabilities or failure modes in the approach.