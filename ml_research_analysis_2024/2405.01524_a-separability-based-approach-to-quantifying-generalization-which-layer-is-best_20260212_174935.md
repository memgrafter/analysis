---
ver: rpa2
title: 'A separability-based approach to quantifying generalization: which layer is
  best?'
arxiv_id: '2405.01524'
source_url: https://arxiv.org/abs/2405.01524
tags:
- classes
- unseen
- seen
- generalization
- generalizability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a separability-based approach to quantify
  how well deep learning models generalize to unseen classes within the same domain.
  The authors argue that high classification accuracy on seen classes does not guarantee
  strong generalization, and that the ability to cluster unseen classes in latent
  space is a more meaningful measure of generalization power.
---

# A separability-based approach to quantifying generalization: which layer is best?

## Quick Facts
- arXiv ID: 2405.01524
- Source URL: https://arxiv.org/abs/2405.01524
- Authors: Luciano Dyballa; Evan Gerritz; Steven W. Zucker
- Reference count: 40
- Primary result: The paper introduces a separability-based approach to quantify how well deep learning models generalize to unseen classes within the same domain, finding that different architectures and layers vary significantly in generalization ability.

## Executive Summary
This paper proposes a novel method to quantify generalization in deep learning models by measuring the separability of embeddings from unseen classes across intermediate layers. The authors argue that high classification accuracy on seen classes does not guarantee strong generalization, and that the ability to cluster unseen classes in latent space is a more meaningful measure of generalization power. Their method involves fine-tuning pre-trained models on a subset of classes and measuring the separability of embeddings from withheld classes across intermediate layers using clustering metrics. The key finding is that different network architectures and layers within the same network vary significantly in their generalization ability, with some generalizing best in early layers and others in later layers.

## Method Summary
The method involves fine-tuning pre-trained models (ViT, Swin, PViT, CvT, PoolFormer, ConvNeXt V2) on a subset of classes (seen) from CIFAR-100 and a Chinese calligraphy dataset, while withholding 5 classes for testing generalization. The models are fine-tuned for 500 epochs with learning rate 2e-4, batch size 72, and AdamW optimizer. Embeddings are extracted from intermediate layers for unseen class examples, and generalization metrics (NMI, kNN, linear probe) are computed for each layer. The results are averaged over three random seeds to assess consistency.

## Key Results
- Different network architectures and layers within the same network vary significantly in their generalization ability to unseen classes.
- Some architectures generalize best in early layers while others in later layers, challenging the assumption that deeper layers always provide the most useful representations.
- The method reveals an intrinsic capacity of different layers of a model to generalize, independent of specific datasets, as indicated by consistent trends across datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separability of unseen-class embeddings in latent space is a better indicator of generalization than classification accuracy on seen classes.
- Mechanism: By fine-tuning on a subset of classes and measuring the clustering quality (via NMI, kNN) of withheld classes across intermediate layers, the method quantifies how well the model can represent novel categories using features learned from seen classes.
- Core assumption: The feature space learned for seen classes contains sufficient and transferable structure to separate unseen classes, even without explicit training on them.
- Evidence anchors:
  - [abstract] "generalize best? We provide a new method for evaluating the capacity of networks to represent a sampled domain, regardless of whether the network has been trained on all classes in that domain."
  - [section] "generalization power is quantified as a function of the latent embeddings of unseen data from intermediate layers for both unsupervised and supervised settings."
  - [corpus] Weak evidence; corpus papers focus on domain adaptation and layer selection, but do not directly support the separability-based generalization claim.
- Break condition: If unseen classes require features not present in the seen-class representation space, or if the embedding space collapses due to over-regularization.

### Mechanism 2
- Claim: Different network architectures and different layers within the same network vary significantly in their generalization ability to unseen classes.
- Mechanism: By comparing g_i (generalizability index) across layers and architectures, the method reveals that some models generalize best in early layers while others in later layers, challenging the assumption that deeper layers always provide the most useful representations.
- Core assumption: The inductive biases and representational capacities of different architectures cause them to encode transferable features at different depths.
- Evidence anchors:
  - [abstract] "different network architectures and different layers within the same network vary significantly in their generalization ability, with some architectures generalizing best in early layers and others in later layers."
  - [section] "Working throughout all stages of the network, we find that... deeper layers in a model do not always generalize the best, which has implications for pruning."
  - [corpus] Moderate evidence; corpus papers on layer selection and domain adaptation support the idea that layer choice matters for generalization.
- Break condition: If the dataset domains are too dissimilar, or if fine-tuning does not adequately expose the model to relevant features.

### Mechanism 3
- Claim: The proposed method can reveal an intrinsic capacity of different layers of a model to generalize, independent of specific datasets.
- Mechanism: Consistency of g_i curves across different datasets suggests that the observed patterns reflect the architecture's inherent representational properties rather than dataset-specific quirks.
- Core assumption: The qualitative similarity of generalization curves across datasets indicates that the method captures architecture-dependent generalization capacity.
- Evidence anchors:
  - [abstract] "Since the trends observed across datasets are largely consistent, we conclude that our approach reveals (a function of) the intrinsic capacity of the different layers of a model to generalize."
  - [section] "Comparing across datasets, the layer generalization curves are qualitatively similar, indicating that our metric captures an intrinsic aspect of the architecture."
  - [corpus] Weak evidence; corpus does not directly support the claim of dataset-independent intrinsic capacity, but papers on architecture design and generalization bounds provide indirect context.
- Break condition: If the method is sensitive to hyperparameters or if the datasets used are not sufficiently diverse to reveal true architectural differences.

## Foundational Learning

- Concept: Few-shot learning and zero-shot learning
  - Why needed here: The paper's scenario—evaluating how well a model can organize unseen classes without explicit labels—relates closely to few-shot and zero-shot learning paradigms.
  - Quick check question: Can you explain the difference between few-shot and zero-shot learning, and how they relate to the paper's "open set" generalization task?

- Concept: Clustering metrics (NMI, kNN) and linear probe classifiers
  - Why needed here: The paper uses these methods to quantify the separability of unseen-class embeddings in latent space, which is central to its generalization assessment.
  - Quick check question: How do NMI, kNN, and linear probe classifiers each measure the quality of clustering or separability in a feature space?

- Concept: Domain adaptation and out-of-distribution generalization
  - Why needed here: The paper's setting—fine-tuning on some classes and evaluating on others from the same domain—shares similarities with domain adaptation and OOD generalization, providing context for the method's novelty.
  - Quick check question: How does the paper's approach differ from standard domain adaptation or OOD detection, and why is this distinction important?

## Architecture Onboarding

- Component map:
  - Pre-trained models (ViT, Swin, PViT, CvT, PoolFormer, ConvNeXt V2) -> Fine-tuning pipeline (subset of classes as seen, rest as unseen) -> Embedding extraction from intermediate layers -> Clustering/separability metrics (NMI, kNN, linear probe) -> Evaluation and comparison across architectures and layers

- Critical path:
  1. Fine-tune pre-trained model on seen classes
  2. Extract embeddings from intermediate layers for unseen classes
  3. Compute clustering metrics (NMI, kNN, linear probe) for each layer
  4. Identify best generalizing layer (max g_i)
  5. Compare across architectures and datasets

- Design tradeoffs:
  - Choice of clustering metric (unsupervised vs. supervised)
  - Number of classes withheld for unseen evaluation
  - Depth and size of intermediate layers considered
  - Hyperparameters for fine-tuning and clustering

- Failure signatures:
  - Low g_i across all layers: model fails to generalize to unseen classes
  - Inconsistent g_i across datasets: method may be dataset-dependent
  - Best layer varies wildly across runs: high variance, possibly due to randomness or poor fine-tuning

- First 3 experiments:
  1. Reproduce results: Fine-tune ViT and Swin on CIFAR-100, compare g_i across layers and datasets
  2. Ablation study: Remove intermediate layers, observe impact on g_i
  3. Cross-dataset validation: Use calligraphy and CIFAR-100, check consistency of best generalizing layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed generalization metric correlate with downstream task performance when applied to unseen classes in practical applications?
- Basis in paper: [explicit] The authors note that their metric can be used to guide architectural design and improvements, suggesting potential practical applications, but do not empirically validate this claim with downstream tasks.
- Why unresolved: The paper focuses on quantifying generalization through separability metrics but does not demonstrate whether higher g-values actually translate to better performance on real-world downstream tasks involving unseen classes.
- What evidence would resolve it: Experimental validation showing that models with higher g-values consistently outperform lower-g models on practical few-shot learning, domain adaptation, or category discovery tasks.

### Open Question 2
- Question: How does the generalization power of intermediate layers change when models are trained with different objectives like contrastive learning or self-supervised learning?
- Basis in paper: [inferred] The authors mention future work could explore different training paradigms, and their experiments are limited to standard supervised fine-tuning, suggesting this remains an open question.
- Why unresolved: The paper only evaluates generalization after supervised fine-tuning, leaving the impact of alternative training objectives on intermediate layer generalization unexplored.
- What evidence would resolve it: Comparative experiments training models with contrastive learning, masked autoencoding, or other self-supervised methods and measuring how g-values differ across layers compared to standard supervised training.

### Open Question 3
- Question: Is there an optimal number of classes in the training set that maximizes generalization to unseen classes, or does more always help?
- Basis in paper: [explicit] The authors deliberately withheld classes during training to test generalization, but only used a fixed ratio (15 seen, 5 unseen) without exploring how this ratio affects results.
- Why unresolved: The paper uses a specific split of seen versus unseen classes but does not investigate whether this ratio or the absolute number of training classes affects the generalization patterns observed.
- What evidence would resolve it: Systematic experiments varying the number of seen classes while keeping total classes constant, measuring how g-values and optimal layer positions change across different training set sizes.

## Limitations

- The method's sensitivity to fine-tuning hyperparameters and the specific choice of classes withheld for generalization evaluation is not thoroughly explored.
- The claim of dataset-independent intrinsic capacity is based on qualitative similarity across datasets, but quantitative robustness checks are lacking.
- The paper does not address the impact of class imbalance or domain shift between seen and unseen classes.

## Confidence

- **High confidence**: The core methodology (fine-tuning, embedding extraction, clustering metrics) is sound and reproducible.
- **Medium confidence**: The claim that separability of unseen-class embeddings is a better indicator of generalization than classification accuracy is supported by experimental results, but alternative explanations (e.g., model capacity, fine-tuning quality) are not fully ruled out.
- **Low confidence**: The assertion that the method reveals an architecture's intrinsic generalization capacity independent of dataset is based on limited cross-dataset validation and qualitative observations.

## Next Checks

1. **Hyperparameter Sensitivity**: Systematically vary fine-tuning hyperparameters (learning rate, batch size, epochs) and assess the impact on g_i curves across layers and architectures.
2. **Cross-Dataset Quantitative Analysis**: Perform statistical tests (e.g., paired t-tests) to compare g_i curves across multiple datasets, quantifying the consistency and significance of observed patterns.
3. **Class Distribution Robustness**: Evaluate the method's performance under varying degrees of class imbalance and domain shift between seen and unseen classes, and assess its sensitivity to these factors.