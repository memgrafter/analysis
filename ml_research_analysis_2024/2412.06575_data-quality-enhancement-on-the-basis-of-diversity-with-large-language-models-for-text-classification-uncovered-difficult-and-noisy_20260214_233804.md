---
ver: rpa2
title: 'Data Quality Enhancement on the Basis of Diversity with Large Language Models
  for Text Classification: Uncovered, Difficult, and Noisy'
arxiv_id: '2412.06575'
source_url: https://arxiv.org/abs/2412.06575
tags:
- data
- dataset
- text
- training
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving large language models'
  (LLMs) performance in text classification tasks by enhancing data quality. The proposed
  Data Quality Enhancement (DQE) method uses a greedy algorithm to sample the dataset
  and then categorizes incorrectly predicted unsampled data into uncovered, difficult,
  and noisy data based on textual similarity.
---

# Data Quality Enhancement on the Basis of Diversity with Large Language Models for Text Classification: Uncovered, Difficult, and Noisy

## Quick Facts
- arXiv ID: 2412.06575
- Source URL: https://arxiv.org/abs/2412.06575
- Reference count: 7
- Improves LLM text classification accuracy by 0.48% to 1% using diversity-based data quality enhancement

## Executive Summary
This paper addresses the challenge of improving large language models' performance in text classification tasks by enhancing data quality. The proposed Data Quality Enhancement (DQE) method uses a greedy algorithm to sample the dataset and then categorizes incorrectly predicted unsampled data into uncovered, difficult, and noisy data based on textual similarity. This approach ensures data diversity, selects beneficial samples for model fine-tuning, and eliminates noisy data. Experimental results demonstrate that DQE significantly improves LLMs' performance in text classification tasks while saving nearly half of the training time.

## Method Summary
The DQE method enhances LLM text classification by first preprocessing the data to remove duplicates and clean inconsistent labels. It then applies K-Center-Greedy algorithm to select 50% of the data based on semantic diversity using vector embeddings. The LLM is fine-tuned on this sampled data, then used to predict the unsampled data. Incorrectly predicted samples are categorized into uncovered (new patterns), difficult (challenging patterns), or noisy (likely mislabeled) based on cosine similarity with correctly classified samples. Noisy data is verified using GPT-4o and removed, while uncovered and difficult data are merged with the sampled set for final training.

## Key Results
- Achieved state-of-the-art results in several open-source classification tasks
- Accuracy improvements ranging from 0.48% to over 1% compared to baseline models
- Nearly halved training time while improving model performance
- Successfully identified and removed noisy data while preserving valuable samples

## Why This Works (Mechanism)

### Mechanism 1
- Greedy sampling ensures data diversity by selecting semantically distant samples
- Uses K-Center-Greedy algorithm to iteratively select samples farthest from already chosen ones
- Core assumption: Samples with large cosine distances between embeddings represent diverse and informative data points
- Break condition: If embedding model fails to capture semantic differences, diversity is not ensured

### Mechanism 2
- Categorizing incorrectly predicted data into three types improves training efficiency
- Identifies errors as uncovered (lack of exposure), difficult (inherent difficulty), or noisy (labeling errors)
- Core assumption: Similar texts should have similar labels; if they don't, one is likely mislabeled
- Break condition: If dataset contains genuinely ambiguous cases where similar texts should have different labels

### Mechanism 3
- Removing noisy data while adding uncovered and difficult data improves model performance
- Eliminates data identified as noisy and supplements with uncovered/difficult patterns
- Core assumption: Noisy data disrupts model convergence while uncovered/difficult data enhance learning
- Break condition: If noisy data detection incorrectly flags clean data, or if detection misses important patterns

## Foundational Learning

- **Cosine similarity for text comparison**
  - Why needed here: Used to find most similar sample to incorrectly predicted data for categorization
  - Quick check question: How would you compute cosine similarity between two text vectors, and what does a high value indicate about their relationship?

- **Greedy algorithms for subset selection**
  - Why needed here: K-Center-Greedy algorithm selects diverse subset maximizing feature space coverage
  - Quick check question: What is the greedy choice property in K-Center-Greedy algorithm, and how does it ensure diversity?

- **Supervised fine-tuning of LLMs**
  - Why needed here: Method involves fine-tuning LLMs on sampled data subset before predicting unsampled data
  - Quick check question: What are key differences between pre-training and fine-tuning of LLMs, and why is fine-tuning necessary?

## Architecture Onboarding

- **Component map:**
  Data preprocessing -> Vector embedding generation -> K-Center-Greedy sampling (50%) -> LLM fine-tuning -> Prediction of unsampled data -> Categorization (uncovered/difficult/noisy) -> Noisy data verification (GPT-4o) -> Final dataset assembly -> Final LLM fine-tuning

- **Critical path:**
  1. Preprocess data
  2. Generate vector embeddings
  3. Apply K-Center-Greedy sampling (50% of data)
  4. Fine-tune LLM on sampled data
  5. Predict unsampled data
  6. For each incorrect prediction, find most similar sample via cosine similarity and categorize
  7. Verify suspected noisy data with GPT-4o
  8. Assemble final training set
  9. Fine-tune LLM on final dataset

- **Design tradeoffs:**
  - Sampling ratio (50%) vs. coverage of data space vs. noise inclusion
  - Computational cost of finding similar samples vs. accuracy of categorization
  - Reliance on GPT-4o for verification vs. potential biases/costs
  - Using cosine similarity vs. more sophisticated similarity measures

- **Failure signatures:**
  - Poor performance despite enhancement → embedding model not capturing semantic differences
  - High proportion of data categorized as noisy → dataset has many ambiguous cases or strict similarity measure
  - Performance worse than baseline → incorrect noisy data detection or missed patterns

- **First 3 experiments:**
  1. Run full pipeline on small dataset with logging to verify components and measure step times
  2. Compare performance with different sampling ratios (30%, 50%, 70%) to find optimal balance
  3. Test sensitivity of categorization to similarity threshold by varying it and measuring changes in categorization proportions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal sampling ratio for balancing data diversity and training efficiency across different text classification tasks?
- **Basis in paper:** The paper mentions using 50% sampling but acknowledges that too little may lead to significant uncovered data while too much might increase noisy data selection.
- **Why unresolved:** Only tested 50% ratio without exploring other ratios or providing systematic method for determining optimal ratio.
- **What evidence would resolve it:** Comparative experiments testing various sampling ratios (25%, 50%, 75%) across diverse tasks and dataset sizes, measuring performance gains vs. training efficiency.

### Open Question 2
- **Question:** How does DQE performance compare when using different vector models for semantic representation?
- **Basis in paper:** Uses all-mpnet-base-v2 model for vector representation, acknowledging that model quality affects semantic distances.
- **Why unresolved:** Does not explore impact of alternative vector models or fine-tuning vector models for specific tasks.
- **What evidence would resolve it:** Experiments comparing DQE performance using different pre-trained vector models (BERT, RoBERTa) and fine-tuned versions, measuring impact on categorization accuracy and classification performance.

### Open Question 3
- **Question:** Can DQE be effectively extended to other natural language processing tasks beyond text classification?
- **Basis in paper:** Focuses specifically on text classification and acknowledges challenges in applying data quality research to text classification problems.
- **Why unresolved:** Applicability to other NLP tasks (sentiment analysis, NER, QA) is not explored.
- **What evidence would resolve it:** Application of DQE to various NLP tasks with task-specific adaptations and evaluation against task-specific data quality techniques.

## Limitations
- Paper lacks comprehensive ablation studies to isolate impact of each component (sampling ratio, similarity threshold, noisy data verification)
- Computational cost analysis is limited to training time savings without considering categorization overhead
- Method's generalizability to domains outside text classification or languages other than English remains untested

## Confidence

- **High confidence:** Core claim that DQE improves model performance on benchmark datasets is well-supported by experimental results showing consistent accuracy improvements across multiple datasets
- **Medium confidence:** Mechanism by which DQE improves performance (removing noisy data while adding uncovered/difficult samples) is plausible but not fully validated through ablation studies or error analysis
- **Low confidence:** Claim that DQE "saves nearly half of the training time" is questionable given additional computational overhead of categorization and GPT-4o verification

## Next Checks

1. Conduct ablation study to quantify individual contributions of sampling ratio, similarity threshold, and noisy data verification to overall performance improvements
2. Measure complete end-to-end training time including all categorization steps to verify claimed time savings and identify computational bottlenecks
3. Test method's performance on dataset with known label noise characteristics to validate effectiveness of noisy data detection mechanism and determine optimal thresholds for different noise levels