---
ver: rpa2
title: 'Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the
  Importance of 2D Representation, Positions, and Objects'
arxiv_id: '2410.06405'
source_url: https://arxiv.org/abs/2410.06405
tags:
- tasks
- task
- reasoning
- positional
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A vanilla Vision Transformer (ViT) fails on the ARC dataset, achieving
  only 18% accuracy despite one million training examples per task. This failure stems
  from the ViT's inability to model spatial relationships between objects and grid
  boundaries.
---

# Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the Importance of 2D Representation, Positions, and Objects

## Quick Facts
- arXiv ID: 2410.06405
- Source URL: https://arxiv.org/abs/2410.06405
- Reference count: 40
- Primary result: ViTARC achieves 75% accuracy on ARC tasks vs 18% for vanilla ViT

## Executive Summary
Vision Transformers struggle with the Abstraction and Reasoning Corpus (ARC) because they cannot effectively model spatial relationships between objects and grid boundaries. The authors introduce ViTARC, an architecture that uses 2D padding with border tokens, 2D sinusoidal positional encoding, and object-based positional encoding to significantly improve performance from 18% to 75% accuracy. The key insight is that ARC tasks require explicit 2D spatial modeling that standard ViT tokenization fails to capture.

## Method Summary
The authors developed ViTARC by addressing the fundamental limitation of vanilla ViTs in modeling spatial relationships. They implemented 2D padding with border tokens to maintain grid structure, used 2D sinusoidal absolute positional encoding for precise spatial location tracking, and added object-based positional encoding to capture object relationships. A positional encoding mixer was introduced to balance absolute and relative positional information, and 2D relative positional encoding was added for fine-grained spatial relationships. The model was trained from scratch on 1 million examples per task using supervised learning.

## Key Results
- Vanilla ViT achieves only 18% accuracy on ARC despite 1 million training examples per task
- ViTARC with 2D padding, border tokens, and 2D sinusoidal positional encoding reaches 66% accuracy
- Full ViTARC with positional encoding mixing and 2D relative positional encoding achieves 75% accuracy
- Over half of tasks solved at 95% accuracy or higher with complete ViTARC architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision Transformers fail on ARC because they cannot model spatial relationships between objects and grid boundaries.
- Mechanism: ViTs flatten 2D grids into 1D sequences, losing positional context that is critical for tasks requiring spatial reasoning and boundary detection.
- Core assumption: The ARC tasks fundamentally require understanding 2D spatial relationships that are not captured by standard ViT tokenization and positional encoding.
- Evidence anchors:
  - [abstract] "A vanilla Vision Transformer (ViT) fails on the ARC dataset, achieving only 18% accuracy despite one million training examples per task. This failure stems from the ViT's inability to model spatial relationships between objects and grid boundaries."
  - [section] "We hypothesize that the vanilla ViT fails because it cannot accurately model spatial relationships between the objects in an ARC grid and the grid boundaries."

### Mechanism 2
- Claim: 2D visual tokens with padding and border tokens significantly improve ViT performance on ARC.
- Mechanism: Using 2D padding tokens and border tokens (newline, end-of-grid) creates explicit spatial structure that helps the model understand grid boundaries and dimensions.
- Core assumption: Explicit 2D representation with boundary markers provides sufficient spatial context for the model to learn ARC transformations.
- Evidence anchors:
  - [abstract] "ViTARC, a new architecture, introduces pixel-level input representation, 2D padding with border tokens, 2D sinusoidal absolute positional encoding, and object-based positional encoding. These changes improve performance to 66% accuracy."
  - [section] "2D padding...We implemented 2D padding, where<pad> tokens are applied to the image first before being flattened...Border tokens...introduce border tokens to explicitly define the grid boundaries."

### Mechanism 3
- Claim: Positional encoding mixing and 2D relative positional encoding further enhance spatial reasoning capabilities.
- Mechanism: Learning to balance absolute and relative positional information through PEmixer, combined with 2D-RPE, provides more precise spatial modeling for complex visual structures.
- Core assumption: The model needs to dynamically adjust the importance of different positional cues based on task requirements.
- Evidence anchors:
  - [abstract] "Further enhancements including positional encoding mixing and 2D relative positional encoding boost accuracy to 75%, with over half of tasks solved at 95% accuracy or higher."
  - [section] "To better balance the importance of positional information and tokens, we modify Equation (1) by learning weight vectors for the encodings, i.e., h0i = α⊙Epi + β⊙Eposi."

## Foundational Learning

- Concept: 2D vs 1D representation of spatial data
  - Why needed here: ARC tasks are inherently 2D grid-based problems where spatial relationships are crucial for solving.
  - Quick check question: Why does flattening a 2D grid into a 1D sequence cause problems for spatial reasoning tasks?

- Concept: Positional encoding in transformers
  - Why needed here: Transformers are permutation-invariant without positional information, making positional encoding critical for spatial tasks.
  - Quick check question: What is the difference between absolute and relative positional encoding, and when would each be more appropriate?

- Concept: Object-based reasoning vs pixel-based reasoning
  - Why needed here: ARC tasks often involve recognizing and transforming objects rather than individual pixels.
  - Quick check question: How does object-based positional encoding differ from grid-based positional encoding in its ability to capture spatial relationships?

## Architecture Onboarding

- Component map: Input grid → 2D padding → border tokens → positional encodings → transformer → output grid reconstruction

- Critical path: Input grid → 2D padding → border tokens → positional encodings → transformer → output grid reconstruction

- Design tradeoffs:
  - Fixed vs variable grid sizes (2D padding provides fixed schema)
  - Learned vs sinusoidal positional encoding (sinusoidal provides better generalization)
  - Absolute vs relative positional encoding (combination provides best performance)

- Failure signatures:
  - Incorrect grid dimensions in output
  - Failure to recognize object boundaries
  - Confusion between similarly colored pixels in different positions
  - Over-reliance on color similarity over spatial relationships

- First 3 experiments:
  1. Implement 2D padding with border tokens and measure impact on grid dimension accuracy
  2. Add 2D sinusoidal positional encoding and compare performance with learned positional encoding
  3. Implement PEmixer and test whether it improves performance on tasks requiring fine spatial distinctions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would ViTARC perform on larger grids beyond the current ARC size constraints, and what architectural modifications would be needed?
- Basis in paper: [explicit] The paper notes that ViTARC was designed for ARC's pixel-level precision and abstract reasoning requirements, and mentions that resizing or cropping is infeasible for such tasks. It also states that more advanced segmentors like SAM could be integrated for larger grid cells or complex real-world settings.
- Why unresolved: The current ViTARC architecture was optimized for the small grid sizes in ARC tasks. The paper only suggests that more advanced segmentors could be integrated for larger grids, but doesn't test or specify what modifications would be necessary for scalability.
- What evidence would resolve it: Experiments testing ViTARC on progressively larger grid sizes with and without architectural modifications like hierarchical transformers or adaptive patch sizes would show performance scaling and identify necessary changes.

### Open Question 2
- Question: Can the positional encoding mixing strategy (PEmixer) be generalized to work with other vision transformer variants beyond ViTARC, and what would be the optimal mixing approach?
- Basis in paper: [explicit] The paper introduces PEmixer as a learnable weighting between input embeddings and positional encodings, testing several variants including scalar-based, vector-based, and normalization-based approaches, finding weighted_sum_no_norm_vec to be optimal.
- Why unresolved: While PEmixer showed benefits for ViTARC, the paper only tested it within their specific architecture. The optimal mixing strategy might vary depending on the transformer variant, task domain, or whether learned vs. fixed positional encodings are used.
- What evidence would resolve it: Systematic ablation studies applying different PEmixer variants to various vision transformer architectures (Swin, ConvNeXt, etc.) across multiple vision tasks would reveal which mixing approaches generalize best and under what conditions.

### Open Question 3
- Question: What is the relationship between the object-based positional encoding (OPE) performance and the quality of the underlying segmentation algorithm, and can self-supervised object discovery replace external segmentation?
- Basis in paper: [explicit] The paper uses OpenCV contour detection for OPE and notes that more advanced segmentors like SAM could be integrated, suggesting that OPE's effectiveness depends on segmentation quality. It also mentions that fine-grained distinctions at the instance level can be addressed by the model's attention mechanism.
- Why unresolved: The paper demonstrates OPE's effectiveness with simple contour detection but doesn't explore how different segmentation qualities affect performance or whether the model could learn object representations without external segmentors.
- What evidence would resolve it: Comparative experiments using OPE with varying segmentation qualities (OpenCV, SAM, self-supervised methods) and testing whether attention-only models can implicitly learn object boundaries would quantify the dependency on segmentation quality and explore self-supervised alternatives.

## Limitations

- Evaluation limited to 400 public ARC tasks, which may not represent full diversity of abstract reasoning problems
- Performance gains substantial but still fall short of human-level performance on many tasks
- Computational cost of generating 1 million training examples per task is substantial
- Study focuses on pixel-level and object-based reasoning without addressing more abstract conceptual reasoning

## Confidence

- **High confidence**: The baseline ViT failure (18% accuracy) and the fundamental claim that spatial relationship modeling is critical for ARC tasks
- **Medium confidence**: The specific architectural improvements (2D padding, border tokens, PEmixer) and their relative contributions to performance
- **Medium confidence**: The claim that object-based positional encoding is necessary for ARC success

## Next Checks

1. **Ablation on spatial reasoning requirements**: Systematically categorize ARC tasks by their spatial reasoning complexity (simple patterns, grid boundaries, object relationships, hierarchical structures) and measure ViTARC performance on each category to validate which spatial modeling components are most critical for different reasoning types.

2. **Cross-dataset generalization**: Evaluate ViTARC on other visual reasoning datasets (e.g., PGM, SVRT) to determine whether the architectural improvements generalize beyond ARC, or if they are specifically tuned to ARC's grid-based structure.

3. **Efficiency-accuracy tradeoff analysis**: Compare the performance of ViTARC against smaller-scale training (fewer examples per task) and against transfer learning approaches to quantify whether the 1 million examples per task are necessary, or if similar performance could be achieved with more efficient training strategies.