---
ver: rpa2
title: Transformers represent belief state geometry in their residual stream
arxiv_id: '2405.15943'
source_url: https://arxiv.org/abs/2405.15943
tags:
- belief
- state
- states
- geometry
- structure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that transformers trained on next-token
  prediction learn to represent the geometry of belief states in their residual stream.
  The authors connect this finding to the theory of optimal prediction from computational
  mechanics, showing that transformers don't just learn the hidden structure of data-generating
  processes but also how to update beliefs about these hidden states.
---

# Transformers represent belief state geometry in their residual stream

## Quick Facts
- arXiv ID: 2405.15943
- Source URL: https://arxiv.org/abs/2405.15943
- Reference count: 40
- Primary result: Transformers trained on next-token prediction learn to represent the geometry of belief states in their residual stream, even for processes with fractal belief state structures.

## Executive Summary
This paper demonstrates that transformers trained on next-token prediction develop geometric representations of belief states in their residual stream. The authors show that these models don't just learn the hidden structure of data-generating processes but also how to update beliefs about these hidden states, connecting this finding to the theory of optimal prediction from computational mechanics. The core method involves training transformers on data from hidden Markov models with known belief state geometries, then using linear regression to identify if and how these belief state geometries are represented in the residual stream.

## Method Summary
The methodology involves training transformers on data generated from hidden Markov models (HMMs) with known belief state geometries, then analyzing whether the residual stream activations linearly represent these belief state structures. The approach uses linear regression to find affine maps from residual stream activations to ground-truth belief states, testing this on processes with both simple and complex fractal belief state geometries. The experiments include both simple processes where belief states are concentrated in the final layer, and more complex cases where the representation is distributed across multiple layers.

## Key Results
- For the Mess3 process with fractal belief state geometry, the final residual stream linearly represents this structure with mean squared error of 0.0004
- For the RRXOR process where distinct belief states have identical next-token predictions, the belief state geometry is distributed across multiple layers rather than concentrated in the final layer
- The representation of belief state geometry correlates with better next-token prediction performance
- Cross-validation and shuffling controls confirm the findings aren't artifacts of the regression procedure

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transformers trained on next-token prediction learn to represent the geometry of belief states in their residual stream, where belief states are probability distributions over hidden states of the data-generating process.
- **Mechanism:** The mixed-state presentation (MSP) describes how an optimal predictor updates beliefs about hidden states as it observes tokens. This belief updating process forms a Markov chain where each belief state is a point in a probability simplex. Transformers learn to map their residual stream activations to this belief state geometry through linear regression.
- **Core assumption:** The belief state geometry is linearly embeddable in the residual stream dimensions, and transformers can learn this mapping during training on next-token prediction.
- **Evidence anchors:**
  - [abstract] "belief states are linearly represented in the residual stream of transformers, even in cases where the predicted belief state geometry has highly nontrivial fractal structure"
  - [section] "Using linear regression, we identified a 2D subspace of the 64-dimensional residual activations that best matched the ground-truth belief distributions from the MSP"
  - [corpus] Weak evidence - the corpus neighbors discuss belief states and residual streams but don't directly address the linear representation mechanism
- **Break condition:** If the belief state geometry cannot be linearly embedded in the residual stream (e.g., requires nonlinear mapping), or if transformers cannot learn this mapping through standard training.

### Mechanism 2
- **Claim:** The representation of belief state geometry correlates with better next-token prediction performance.
- **Mechanism:** Belief states contain information about entire future sequences, not just the next token. Transformers that better represent this geometry can make more informed predictions by maintaining distinctions between belief states that have the same next-token probability but different future implications.
- **Core assumption:** Maintaining the full belief state geometry (rather than collapsing equivalent next-token predictions) provides predictive advantage for future tokens beyond the immediate next token.
- **Evidence anchors:**
  - [abstract] "the inferred belief states contain information about the entire future, beyond the local next-token prediction that the transformers are explicitly trained on"
  - [section] "we ask if the pairwise distances between belief state representations in the transformer can be explained by pairwise distances in the next-token predictions, or if they are better explained by ground truth belief state distances"
  - [corpus] Moderate evidence - neighbors discuss belief state geometry and prediction but don't directly address the correlation with performance
- **Break condition:** If the computational cost of maintaining full belief state geometry outweighs the predictive benefit, or if the training objective doesn't reward this extended foresight.

### Mechanism 3
- **Claim:** For processes where distinct belief states have identical next-token predictions, the belief state geometry is distributed across multiple layers rather than concentrated in the final layer.
- **Mechanism:** When belief states are degenerate in their next-token predictions, distinctions can be lost before the unembedding layer. To preserve this information, transformers distribute the representation across multiple layers, with different layers capturing different aspects of the belief state geometry.
- **Core assumption:** The transformer architecture can distribute and recombine information across layers to maintain belief state distinctions that would otherwise be lost.
- **Evidence anchors:**
  - [abstract] "In some cases the geometry of belief state updating is found in the final residual stream, while in others it is spread out across the residual streams of multiple layers"
  - [section] "when we take the residual stream activations of all layers and concatenate them, we find a veridical representation" for the RRXOR process
  - [corpus] Weak evidence - corpus neighbors discuss residual streams and representations but don't specifically address layer-wise distribution of belief states
- **Break condition:** If the layer-wise distribution cannot be effectively recombined, or if intermediate layers lose the necessary information before it can be distributed.

## Foundational Learning

- **Concept: Hidden Markov Models (HMMs) and their mathematical structure**
  - Why needed here: The paper's theoretical framework is built on HMMs as the data-generating processes, and understanding their transition matrices, emission probabilities, and state spaces is crucial for interpreting the belief state geometry.
  - Quick check question: Given a 3-state HMM with transition matrices T(0) and T(1), how would you compute the probability of being in state 2 after observing the sequence "010"?

- **Concept: Probability simplex and geometric representations of distributions**
  - Why needed here: Belief states are probability distributions over hidden states, and the paper shows these lie in a probability simplex. Understanding simplex geometry is essential for interpreting how belief states are represented.
  - Quick check question: For a 3-state HMM, what are the vertices of the 2-simplex representing the belief states, and what do they represent in terms of certainty about hidden states?

- **Concept: Linear regression and subspace identification**
  - Why needed here: The core experimental method involves using linear regression to find affine mapping from residual stream activations to belief states, and understanding this technique is crucial for evaluating the methodology.
  - Quick check question: If you have 64-dimensional residual activations and want to find a 2D subspace representing belief states, what is the mathematical form of the regression model you would use?

## Architecture Onboarding

- **Component map:**
  - HMM Generator -> Transformer Model -> Linear Regression Module -> Analysis Pipeline

- **Critical path:**
  1. Generate training data from HMM with known belief state geometry
  2. Train transformer on next-token prediction task
  3. Extract residual stream activations at all context positions and layers
  4. Use linear regression to map activations to ground-truth belief states
  5. Visualize and quantify the geometry in the projected subspace
  6. Perform controls (cross-validation, shuffling, training progression)

- **Design tradeoffs:**
  - Single-layer vs. multi-layer analysis: Simpler but may miss distributed representations
  - Full vs. partial context: More context provides better belief state inference but increases computational cost
  - Linear vs. nonlinear regression: Linear is simpler and more interpretable but may miss complex relationships

- **Failure signatures:**
  - High mean squared error in regression indicates poor belief state representation
  - Shuffling controls collapsing to simplex center indicates spurious correlations
  - Cross-validation showing different geometries suggests overfitting
  - Training progression not showing emergence suggests architecture limitations

- **First 3 experiments:**
  1. Train on simple 3-state HMM (like Z1R) and verify belief states are represented in final layer
  2. Train on process with next-token degenerate belief states (like RRXOR) and check if geometry is distributed across layers
  3. Apply shuffling control to confirm findings aren't artifacts of the regression procedure

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on synthetic HMMs with controlled structures, which may not fully capture the complexity of natural language data
- The linear regression approach assumes affine relationships between activations and belief states, potentially missing nonlinear representations
- The experiments use relatively small transformers (4 layers, 64 dimensions), raising questions about scalability to larger models and real-world applications

## Confidence
- High confidence: Core finding that transformers develop representations of belief state geometry in residual stream, supported by multiple controls
- Medium confidence: Claim that representation correlates with better next-token prediction (correlation shown but causation not established)
- Medium confidence: Layer-wise distribution finding for RRXOR (evidence shows distributed representations exist but mechanism remains less clear)

## Next Checks
1. **Natural Language Extension**: Apply the methodology to transformers trained on real language data, mapping their residual stream activations to belief states derived from linguistic structures (e.g., syntactic or semantic states). This would test whether the findings generalize beyond synthetic HMMs.

2. **Scale-Up Experiment**: Repeat the experiments with larger transformers (more layers, higher dimensions) and varying context window sizes to determine if the belief state representation scales and how it changes with model capacity.

3. **Intervention Study**: Implement a modified training objective that explicitly rewards maintaining belief state geometry representations, then measure if this improves next-token prediction performance beyond standard training, establishing a causal link between belief state representation and predictive capability.