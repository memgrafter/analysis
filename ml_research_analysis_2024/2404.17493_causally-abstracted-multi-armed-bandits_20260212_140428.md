---
ver: rpa2
title: Causally Abstracted Multi-armed Bandits
arxiv_id: '2404.17493'
source_url: https://arxiv.org/abs/2404.17493
tags:
- cmab
- abstraction
- regret
- abstracted
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for transferring information
  between causal multi-armed bandits (CMABs) defined on related but different variables
  through causal abstraction. The authors define the problem of causally abstracted
  MABs (CAMABs) where two CMABs are related by an abstraction map, allowing information
  transfer despite differing variable sets and granularity.
---

# Causally Abstracted Multi-armed Bandits

## Quick Facts
- arXiv ID: 2404.17493
- Source URL: https://arxiv.org/abs/2404.17493
- Reference count: 40
- This paper introduces a framework for transferring information between causal multi-armed bandits (CMABs) defined on related but different variables through causal abstraction.

## Executive Summary
This paper introduces a framework for transferring information between causal multi-armed bandits (CMABs) defined on related but different variables through causal abstraction. The authors define the problem of causally abstracted MABs (CAMABs) where two CMABs are related by an abstraction map, allowing information transfer despite differing variable sets and granularity. They propose algorithms for learning in CAMABs including TOpt (transferring optimal action), IMIT (transferring actions), and TExp (transferring expected values), and analyze their regret performance. The key results show that while naive approaches like TOpt can fail without preserving optima, TExp can effectively leverage abstraction maps to improve learning efficiency, as demonstrated on both synthetic and realistic online advertising scenarios.

## Method Summary
The paper introduces causally abstracted multi-armed bandits (CAMABs) where two causal MABs are related via a causal abstraction map. The authors propose three algorithms for learning in CAMABs: TOpt (transferring optimal action), IMIT (transferring actions), and TExp (transferring expected values). These algorithms leverage the abstraction to improve learning efficiency by transferring information from a base CMAB to an abstracted CMAB. The methods are analyzed theoretically and validated on synthetic examples and an online advertising scenario.

## Key Results
- TOpt can fail when the optimal action is not preserved under abstraction, even with zero IC error
- IMIT performs best when actions with similar expected rewards are mapped together by the abstraction
- TExp effectively leverages abstraction maps to improve learning efficiency through expected value transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Abstractions allow transfer of expected rewards across CMABs defined on different variables.
- Mechanism: The abstraction map α transports the expected reward from the base CMAB to the abstracted CMAB via interpolation (αE), enabling warm-start learning in the abstracted model.
- Core assumption: The abstraction is sufficiently consistent (low IC error) and the interpolation function αE accurately maps reward values between domains.
- Evidence anchors:
  - [abstract]: "propose algorithms for learning in CAMABs including TOpt (transferring optimal action), IMIT (transferring actions), and TExp (transferring expected values)"
  - [section 5.3]: "We now consider the possibility of transferring the expected value of the rewards in the base CMAB in order to warm-start the abstracted CMAB"
  - [corpus]: Weak - no direct neighbor evidence found for this specific mechanism.
- Break condition: High IC error or poor interpolation quality causes large bias in transferred expected rewards, making the warm-start ineffective.

### Mechanism 2
- Claim: Action aggregation via abstraction can improve confidence in the abstracted CMAB.
- Mechanism: Multiple base actions mapping to a single abstract action allow the abstracted CMAB to aggregate samples, improving estimation confidence for that abstract action.
- Core assumption: Actions with similar expected rewards are mapped together by the abstraction, reducing variance in the aggregated estimate.
- Evidence anchors:
  - [section 5.2]: "If many actions aj with small gaps ∆(aj) map onto a′i, then IMIT will oversample a′i and be overconfident in its estimation"
  - [section 5.10]: "IMIT performs best when the optimal action a∗ together with other actions aj with small gaps ∆(aj) are mapped onto a′∗"
  - [corpus]: Weak - no direct neighbor evidence found for this specific mechanism.
- Break condition: Actions with very different expected rewards are mapped to the same abstract action, leading to high bias in the aggregated estimate.

### Mechanism 3
- Claim: Optimal action preservation is not guaranteed under abstraction, even with zero IC error.
- Mechanism: The abstraction may change the relative ordering of expected rewards between base and abstract models, meaning the optimal action in the base CMAB does not necessarily map to the optimal action in the abstracted CMAB.
- Core assumption: The abstraction preserves the structure of the reward distribution in a way that maintains the optimal action mapping.
- Evidence anchors:
  - [section 5.1]: "it is not guaranteed that α(ao) = a′∗" and provides counterexamples
  - [abstract]: "While naive approaches like TOpt can fail without preserving optima"
  - [corpus]: Weak - no direct neighbor evidence found for this specific mechanism.
- Break condition: The abstraction introduces a non-linear transformation of the reward space that changes the optimal action ordering.

## Foundational Learning

- Concept: Causal abstraction and its properties (interventional consistency, reward discrepancy)
  - Why needed here: The algorithms rely on understanding how abstractions relate two CMABs and how to measure the quality of this relationship.
  - Quick check question: What is the difference between interventional consistency error and reward discrepancy, and how do they affect transfer learning?

- Concept: Multi-armed bandit algorithms (UCB, Thompson sampling) and their regret bounds
  - Why needed here: The paper builds upon standard MAB algorithms and analyzes their performance in the CAMAB setting.
  - Quick check question: How does the cumulative regret of UCB scale with the number of timesteps, and what factors affect this scaling?

- Concept: Transfer learning in reinforcement learning and bandits
  - Why needed here: The paper extends transfer learning concepts to the causal MAB setting with different variables.
  - Quick check question: What are the key challenges in transferring knowledge between related but different decision-making problems?

## Architecture Onboarding

- Component map:
  Base CMAB -> Abstracted CMAB -> Abstraction map (α) -> Transfer algorithms (TOpt, IMIT, TExp)

- Critical path:
  1. Define the base and abstracted CMABs and their relationship via an abstraction map
  2. Choose a transfer algorithm based on the properties of the abstraction and the desired trade-offs
  3. Implement the transfer algorithm, handling the translation of actions, rewards, or expected values as needed
  4. Analyze the performance of the algorithm in terms of regret and compare it to direct learning on the abstracted CMAB

- Design tradeoffs:
  - TOpt: Computationally efficient but may fail if the optimal action is not preserved under abstraction
  - IMIT: Leverages action trajectories but may suffer from overconfidence if actions are poorly aggregated
  - TExp: Provides warm-start via expected value transfer but requires accurate interpolation and low IC error

- Failure signatures:
  - High regret compared to direct learning on the abstracted CMAB
  - Convergence to a suboptimal action in the abstracted CMAB
  - Unstable or biased estimates of expected rewards in the abstracted CMAB

- First 3 experiments:
  1. Implement a simple CAMAB with a known abstraction and compare the performance of TOpt, IMIT, and TExp against direct learning on the abstracted CMAB
  2. Vary the properties of the abstraction (IC error, action aggregation) and observe their effects on the transfer algorithms' performance
  3. Apply the transfer algorithms to a realistic online advertising scenario and analyze the trade-offs between computational efficiency and regret reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical regret bounds for TExp in the general CAMAB setting?
- Basis in paper: [explicit] The authors state "We leave theoretical regret guarantees of TExp as an open problem" in Section 5.3.
- Why unresolved: The paper provides extensive analysis for TOpt and IMIT but does not derive regret bounds for TExp, which transfers expected values rather than actions.
- What evidence would resolve it: A mathematical proof showing regret bounds for TExp under various CAMAB conditions (exact vs non-exact abstractions, different interpolation errors).

### Open Question 2
- Question: How can we efficiently learn abstraction maps and estimate IC error for CAMABs in practice?
- Basis in paper: [explicit] The authors note "Notice that TExp relies on explicit knowledge of κi which might not be available. In such cases, we are unable to restrict actions to I'+" in Section 5.3.
- Why unresolved: The paper assumes a known abstraction map, but in real-world applications, abstractions must be learned from data.
- What evidence would resolve it: An algorithm that jointly learns the abstraction map and estimates IC error while solving the CAMAB, with theoretical guarantees on convergence and accuracy.

### Open Question 3
- Question: What is the relationship between CAMABs and structured/regional bandits?
- Basis in paper: [inferred] The discussion section mentions "we leave the relationship between CAMABs and other specialised frameworks for MABs, such as regional and structured bandits" as a potential direction for future work.
- Why unresolved: The paper introduces CAMABs as a new framework but does not formally connect it to existing structured bandit approaches.
- What evidence would resolve it: A formal reduction showing how CAMABs can be expressed as special cases of structured bandits, or vice versa, with proofs of regret equivalence under certain conditions.

## Limitations

- The paper lacks empirical validation of the transfer algorithms on real-world problems beyond the synthetic examples, limiting confidence in practical applicability.
- The analysis of failure conditions is theoretical; no systematic experiments are presented to demonstrate when and why the algorithms fail.
- The complexity of computing the abstraction map and interpolation function αE is not discussed, which may limit scalability to large SCMs.

## Confidence

- **High**: The theoretical framework for CAMABs and the definition of causal abstraction error (IC error) are well-established and mathematically rigorous.
- **Medium**: The analysis of the three transfer algorithms (TOpt, IMIT, TExp) and their regret bounds is sound, but the practical performance depends on the quality of the abstraction and interpolation.
- **Low**: The paper's claims about the effectiveness of the transfer algorithms in realistic scenarios are not fully supported by empirical evidence.

## Next Checks

1. Implement the transfer algorithms on a more complex and realistic SCM, such as a healthcare decision-making problem, to assess their performance and robustness in practice.
2. Conduct a systematic study of the algorithms' failure modes by varying the properties of the abstraction (IC error, action aggregation) and observing the impact on regret and convergence.
3. Compare the computational efficiency of the transfer algorithms with direct learning on the abstracted CMAB, considering the overhead of computing the abstraction map and interpolation function.