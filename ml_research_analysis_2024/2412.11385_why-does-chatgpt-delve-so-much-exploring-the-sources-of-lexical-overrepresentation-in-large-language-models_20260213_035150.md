---
ver: rpa2
title: Why Does ChatGPT "Delve" So Much? Exploring the Sources of Lexical Overrepresentation
  in Large Language Models
arxiv_id: '2412.11385'
source_url: https://arxiv.org/abs/2412.11385
tags:
- words
- items
- focal
- abstracts
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study formalizes a reproducible method to identify 21 "focal
  words" overused in scientific abstracts and by ChatGPT-3.5, with "delve" showing
  a 6697% increase in usage (2020-2024). Analysis of Llama models revealed that RLHF
  contributes to lexical overrepresentation, as the chat variant was less surprised
  by AI-generated abstracts containing these words.
---

# Why Does ChatGPT "Delve" So Much? Exploring the Sources of Lexical Overrepresentation in Large Language Models

## Quick Facts
- arXiv ID: 2412.11385
- Source URL: https://arxiv.org/abs/2412.11385
- Reference count: 28
- "delve" usage increased 6697% in scientific abstracts (2020-2024)

## Executive Summary
This study identifies 21 "focal words" that have spiked in usage in scientific abstracts, particularly in AI-generated text. Using ChatGPT-3.5 and various corpora, the authors found that "delve" exemplifies this phenomenon with a 6697% increase in usage. The research explores whether Reinforcement Learning from Human Feedback (RLHF) contributes to this lexical overrepresentation by comparing Llama model variants. An exploratory experiment found participants prefer abstracts without "delve" in the first sentence, suggesting sensitivity to this word.

## Method Summary
The authors conducted a three-step corpus analysis using PubMed scientific abstracts (5.2B tokens, 26.7M abstracts, 1975-2024) to identify words with significant usage spikes (2020-2024). They generated 10,000 AI abstracts using ChatGPT-3.5 and compared word usage between human and AI abstracts. Focal words were identified by cross-referencing spiking words with AI-overrepresented words. The study validated findings using alternative datasets (Arxiv, Wikipedia, ICE) and tested Llama 2 and 3 models to explore RLHF's role. An online study with 132 participants rated 528 abstracts to assess human preferences for focal words.

## Key Results
- 21 focal words identified with significant usage spikes in scientific abstracts (2020-2024)
- "delve" shows 6697% increase in usage, appearing 1360 times per million in AI-generated abstracts vs 20 times per million in human abstracts
- Llama 2-Chat (with RLHF) is less "surprised" by AI-generated abstracts containing focal words compared to Llama 2-Base
- Participants preferred abstracts without "delve" in the first sentence (mean rating 3.97 vs 3.61)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Focal words spike in usage due to reinforcement learning from human feedback (RLHF).
- Mechanism: Human evaluators rate outputs, and models learn to produce text similar to highly-rated exemplars. If evaluators prefer abstracts containing certain words, the model will overuse those words.
- Core assumption: Human evaluators assess quality based partly on word choice rather than solely on content.
- Evidence anchors:
  - The paper notes that RLHF involves training models to align with human preferences, and the Llama model comparison suggests RLHF may contribute to lexical overrepresentation.
  - Llama 2-Chat, which includes RLHF, is less "surprised" by AI-generated abstracts containing focal words, suggesting RLHF's role in overrepresentation.
  - The focal words are overrepresented in AI-generated abstracts compared to human-generated ones, indicating a potential link to evaluation criteria.
- Break condition: If human evaluators base ratings solely on content and not word choice, RLHF would not cause lexical overrepresentation.

### Mechanism 2
- Claim: Model architecture or algorithmic choices do not primarily drive lexical overrepresentation.
- Mechanism: If architecture or algorithms were the cause, both Llama 2-Base and Llama 2-Chat would show similar entropy patterns. The difference in entropy between the two models suggests other factors are at play.
- Core assumption: Model architecture and many algorithms are held constant across Llama 2-Base and Llama 2-Chat.
- Evidence anchors:
  - Llama 2-Chat is considerably less "surprised" by AI-generated abstracts than Llama 2-Base, indicating that factors beyond architecture or algorithms contribute to lexical overrepresentation.
  - The entropy analysis shows a significant difference between the models, supporting the idea that architecture is not the primary cause.
- Break condition: If future studies show similar entropy patterns across models with different architectures, the claim would be invalid.

### Mechanism 3
- Claim: Focal words are not overrepresented in training data.
- Mechanism: If focal words were common in training data, they would appear frequently in model outputs without being overused. The analysis of various corpora shows focal words are not especially prevalent in pre-LLM era data.
- Core assumption: The training data for LLMs does not contain an unusually high frequency of focal words.
- Evidence anchors:
  - The analysis of Arxiv abstracts, the Leipzig Corpus Collective, and Wikipedia articles shows that focal words are not overrepresented in these datasets.
  - The opm of focal words in ChatGPT-generated abstracts far exceeds their opm in the examined datasets, suggesting focal words are not overrepresented in training data.
- Break condition: If future studies find focal words are common in LLM training data, the claim would be invalid.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding RLHF is crucial to grasp how human preferences shape LLM outputs, particularly the overuse of focal words.
  - Quick check question: How does RLHF influence the lexical choices of LLMs, and why might it lead to overrepresentation of certain words?

- Concept: Entropy and Surprise in Language Models
  - Why needed here: Entropy measures the "surprise" of a model when encountering text, helping to identify whether focal words are overrepresented due to model architecture or other factors.
  - Quick check question: How does comparing per-word entropy between different models help determine the source of lexical overrepresentation?

- Concept: Corpus Analysis and Word Frequency
  - Why needed here: Analyzing word frequency across different corpora helps identify whether focal words are overrepresented due to training data or other factors.
  - Quick check question: Why is it important to compare the frequency of focal words in AI-generated abstracts with their frequency in human-generated abstracts and other datasets?

## Architecture Onboarding

- Component map: PubMed abstracts -> Corpus analysis -> Focal word identification -> ChatGPT-3.5 and 4.0 testing -> Llama model comparison -> Human preference experiment
- Critical path: Identify focal words → Analyze their usage in different corpora → Compare model entropy → Conduct experiments on human preferences
- Design tradeoffs: Balancing model performance with interpretability, managing computational resources for large-scale corpus analysis, and ensuring ethical recruitment for human experiments
- Failure signatures: Inability to identify focal words, inconclusive entropy comparisons, or unexpected human preferences in experiments
- First 3 experiments:
  1. Analyze the entropy of different LLM variants when processing human- and AI-generated abstracts.
  2. Conduct an online study to assess human preferences for abstracts with and without focal words.
  3. Compare the frequency of focal words across multiple corpora to determine their prevalence in training data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does RLHF contribute to lexical overrepresentation in LLMs beyond the case of "delve"?
- Basis in paper: The paper suggests RLHF might play a role based on Llama model testing, but experimental results with ChatGPT were inconclusive.
- Why unresolved: The exploratory experiment had methodological issues and focused heavily on "delve" items, making it unclear if findings generalize to other focal words.
- What evidence would resolve it: A larger, better-designed experiment testing participant preferences for abstracts containing various focal words (not just "delve") would clarify RLHF's role.

### Open Question 2
- Question: Do other LLMs besides ChatGPT exhibit similar patterns of lexical overrepresentation?
- Basis in paper: The study only tested ChatGPT-3.5 and ChatGPT-4.0, but suggests future work could examine other models.
- Why unresolved: The paper did not test a diverse range of LLMs, so it's unknown if the phenomenon is model-specific or widespread.
- What evidence would resolve it: Systematic testing of focal word usage across multiple LLM architectures (GPT, Claude, Gemini, etc.) would reveal if the pattern is universal or model-dependent.

### Open Question 3
- Question: Are focal words overused because they serve as quality signals for human evaluators during RLHF?
- Basis in paper: The authors speculate that rushed evaluators might use certain words as proxies for quality, influencing model training.
- Why unresolved: This is presented as a hypothesis but not directly tested; the experimental results were inconclusive.
- What evidence would resolve it: Analyzing the actual human feedback data from RLHF training (if accessible) to see if evaluators rate outputs containing focal words more highly would test this hypothesis.

## Limitations
- The study cannot definitively attribute focal word overrepresentation to RLHF alone due to limited model transparency and the exploratory nature of human preference experiments.
- The human study involved only 132 participants rating 528 abstracts, limiting generalizability of the findings.
- The entropy analysis, while suggestive, cannot isolate RLHF as the sole causal factor without access to models trained with and without RLHF on identical data.

## Confidence

- **High confidence**: The identification of focal words through corpus analysis and their statistical overrepresentation in AI-generated abstracts is methodologically sound and reproducible.
- **Medium confidence**: The role of RLHF in contributing to lexical overrepresentation is supported by entropy comparisons but not definitively proven due to limited model transparency.
- **Low confidence**: The human preference experiment provides only suggestive evidence about focal word sensitivity, with results that could be influenced by experimental design or participant selection.

## Next Checks
1. Replicate the entropy analysis with open-source models that have both RLHF and non-RLHF variants trained on identical data to isolate RLHF's specific contribution.
2. Conduct a larger-scale human preference study with randomized abstract presentation and diverse participant pools to validate sensitivity to focal words.
3. Analyze focal word usage across multiple scientific domains and publication types to determine if overrepresentation is field-specific or pervasive across academic writing.