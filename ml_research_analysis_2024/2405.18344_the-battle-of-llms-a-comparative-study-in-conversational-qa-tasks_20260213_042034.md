---
ver: rpa2
title: 'The Battle of LLMs: A Comparative Study in Conversational QA Tasks'
arxiv_id: '2405.18344'
source_url: https://arxiv.org/abs/2405.18344
tags:
- cotton
- these
- conversational
- language
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares five large language models (LLMs) \u2014 ChatGPT-3,\
  \ GPT-4, Gemini, Mixtral, and Claude \u2014 on conversational question-answering\
  \ tasks using four datasets (CoQA, DialFact, FaVIQ, and CoDAH). The study uses BLEU,\
  \ ROUGE-L, METEOR, TER, Jaccard, BART, and NIST scores to evaluate the models' outputs."
---

# The Battle of LLMs: A Comparative Study in Conversational QA Tasks

## Quick Facts
- arXiv ID: 2405.18344
- Source URL: https://arxiv.org/abs/2405.18344
- Authors: Aryan Rangapur; Aman Rangapur
- Reference count: 11
- Key outcome: GPT-4 and Claude consistently outperformed other models on conversational QA tasks using BLEU, ROUGE-L, METEOR, TER, Jaccard, BART, and NIST scores

## Executive Summary
This paper presents a comprehensive comparative evaluation of five large language models (ChatGPT-3, GPT-4, Gemini, Mixtral, and Claude) on conversational question-answering tasks. The study uses four benchmark datasets (CoQA, DialFact, FaVIQ, and CoDAH) and multiple evaluation metrics to assess model performance. GPT-4 and Claude emerged as the top performers, demonstrating superior accuracy, relevance, and consistency in generating responses to conversational queries.

The evaluation pipeline employs a modular design with question generation and response generation components, enabling scalable and flexible processing of conversational QA tasks. The study identifies significant performance gaps between models, with GPT-4 and Claude showing marked advantages in handling the complexities of conversational contexts, while also revealing limitations such as potential gender bias and response inconsistencies in some models.

## Method Summary
The study evaluates five LLMs on four conversational QA datasets using a pipeline approach with question generation and response generation modules. Each model generates responses to questions from CoQA, DialFact, FaVIQ, and CoDAH datasets, which are then evaluated using multiple metrics including BLEU, ROUGE-L, METEOR, TER, Jaccard, BART, and NIST scores. The evaluation also incorporates Chain of Thought, Zero Shot, and Few Shot learning assessments to examine reasoning capabilities and adaptability across different conversational scenarios.

## Key Results
- GPT-4 and Claude consistently outperformed ChatGPT-3, Gemini, and Mixtral across all evaluation metrics
- Mixtral achieved comparable performance to ChatGPT-3, while Claude matched GPT-4's capabilities
- The pipeline demonstrated scalability and flexibility in handling diverse conversational QA tasks
- All models showed varying degrees of gender bias and response inconsistencies across different prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pipeline leverages human-in-the-loop fine-tuning to improve the model's understanding of nuanced conversational instructions.
- Mechanism: The pipeline incorporates human feedback during supervised fine-tuning, allowing the model to iteratively refine its ability to comprehend and respond to complex conversational cues.
- Core assumption: Human feedback provides meaningful guidance that generalizes to new conversational scenarios.
- Evidence anchors:
  - [abstract]: "The 'human-in-the-loop' phase follows, incorporating human feedback to enhance the model's ability to comprehend and respond effectively to nuanced instructions."
  - [section]: "In the supervised fine-tuning stage, the models undergo further training on labeled datasets, refining their understanding and adapting to specific tasks."
- Break condition: If human feedback is noisy, inconsistent, or fails to cover the diversity of real-world conversational contexts, the model's performance may degrade on unseen scenarios.

### Mechanism 2
- Claim: GPT-4 and Claude outperform other models on conversational QA tasks due to their superior ability to generate relevant, specific, and consistent responses.
- Mechanism: These models likely benefit from larger model capacity, better fine-tuning, or more diverse training data, enabling them to better understand context and maintain coherence across multi-turn conversations.
- Core assumption: Model architecture and training data directly influence conversational reasoning and response quality.
- Evidence anchors:
  - [abstract]: "Our evaluation results showed that GPT-4 and Claude outperformed ChatGPT-3, Gemini, and Mixtral in terms of accuracy, relevance, and consistency."
  - [section]: "Both models demonstrated significant improvements in generating coherent and contextually relevant responses, making them promising candidates for conversational QA tasks."
- Break condition: If the tasks or datasets used for evaluation do not adequately capture the nuances of real-world conversation, the observed superiority may not generalize.

### Mechanism 3
- Claim: The evaluation pipeline is scalable and flexible, enabling large-scale response generation and comparison across diverse conversational QA datasets.
- Mechanism: The pipeline uses a modular design with separate question generation and response generation modules, allowing for efficient processing of large datasets and easy adaptation to new tasks.
- Core assumption: Modular design and efficient processing enable the pipeline to handle increasing data volumes and task complexity.
- Evidence anchors:
  - [section]: "Our pipeline exhibited a commendable level of scalability and flexibility, underscoring its ability to adeptly handle a diverse array of conversational QA tasks."
  - [section]: "The robustness and scalability of our pipeline position it as a versatile solution with applications ranging from virtual assistants to customer service chatbots, conversational agents, and creative content generation."
- Break condition: If the pipeline encounters extremely large or rapidly changing datasets, or if the modular design introduces bottlenecks or integration issues, scalability and flexibility may be compromised.

## Foundational Learning

- Concept: Evaluation metrics for natural language generation (BLEU, ROUGE-L, METEOR, TER, Jaccard, BART, NIST)
  - Why needed here: These metrics provide a quantitative assessment of the generated responses' quality, fluency, and relevance, enabling comparison between different models and datasets.
  - Quick check question: What is the main difference between BLEU and ROUGE-L, and when would you use each metric?

- Concept: Chain of Thought prompting, Zero-shot learning, and Few-shot learning
  - Why needed here: These techniques allow the models to demonstrate their reasoning abilities, adaptability to new tasks, and ability to maintain context over multiple turns in a conversation.
  - Quick check question: How does Chain of Thought prompting differ from traditional prompting, and what are its potential benefits for conversational QA tasks?

- Concept: Hallucination in language models and its impact on response quality
  - Why needed here: Understanding and quantifying hallucination helps identify potential areas for improvement in the models and ensures the reliability of their responses.
  - Quick check question: What is the Hallucination Vulnerability Index (HVI), and how does it help assess the extent of hallucination in language model responses?

## Architecture Onboarding

- Component map: Question generation -> Response generation -> Evaluation -> Comparison
- Critical path: Question generation → Response generation → Evaluation → Comparison
- Design tradeoffs:
  - Balancing the complexity and diversity of generated questions with the computational resources required for large-scale processing.
  - Choosing the appropriate evaluation metrics to capture the nuances of conversational QA tasks while ensuring fair and meaningful comparisons between models.
  - Ensuring the pipeline's scalability and flexibility while maintaining the quality and reliability of the generated responses.
- Failure signatures:
  - Low evaluation scores across multiple metrics may indicate issues with the model's understanding of context, relevance, or coherence.
  - Inconsistent or misleading responses to the same question based on the same context suggest potential problems with the model's reasoning or training data.
  - High hallucination rates, as measured by the HVI, may indicate a need for improved fact-checking or grounding mechanisms.
- First 3 experiments:
  1. Evaluate the models on a small subset of the conversational QA datasets using the chosen evaluation metrics to establish a baseline performance.
  2. Compare the performance of different models on the same dataset to identify the most promising candidates for further investigation.
  3. Analyze the generated responses for examples of hallucination, inconsistency, or irrelevance to gain insights into potential areas for improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the evaluated LLMs perform on conversational QA tasks with complex or multi-hop reasoning requirements?
- Basis in paper: [inferred] The paper mentions that conversational QA introduces a level of ambiguity and uncertainty not typically found in regular QA scenarios, and the models may face challenges in contexts not covered by the selected datasets.
- Why unresolved: The study's datasets and evaluation focus on general conversational QA tasks, but do not specifically assess the models' performance on complex or multi-hop reasoning scenarios.
- What evidence would resolve it: Additional experiments using datasets that explicitly test complex or multi-hop reasoning in conversational QA, and a detailed analysis of the models' performance on such tasks.

### Open Question 2
- Question: What is the impact of fine-tuning the LLMs on domain-specific conversational QA corpora, such as medical or legal domains?
- Basis in paper: [inferred] The paper mentions that future research could consider investigating alternative approaches for fine-tuning these models for conversational QA tasks, and it highlights the importance of fact-checking the generated text from these models.
- Why unresolved: The study does not explore the effects of fine-tuning the LLMs on domain-specific conversational QA corpora, which could potentially improve their performance and accuracy in specialized domains.
- What evidence would resolve it: Experiments comparing the performance of the LLMs on domain-specific conversational QA tasks before and after fine-tuning on relevant corpora, along with an analysis of the impact on accuracy and fact-checking capabilities.

### Open Question 3
- Question: How do the LLMs handle long-term context and memory in multi-turn conversational QA tasks?
- Basis in paper: [inferred] The paper mentions that conversational QA involves follow-up questions and clarification, and the models may face challenges in contexts not covered by the selected datasets.
- Why unresolved: The study does not explicitly assess the LLMs' ability to maintain long-term context and memory across multiple turns in a conversation, which is crucial for effective conversational QA.
- What evidence would resolve it: Experiments using datasets with long conversational threads and an evaluation of the LLMs' ability to consistently understand and respond to context-dependent queries over multiple turns.

## Limitations
- The study identified gender bias in model responses, particularly regarding stereotypes about emotional labor and communication.
- Models showed inconsistencies in outputs for similar prompts, suggesting potential training data or fine-tuning gaps.
- The relatively small size of some evaluation datasets (particularly CoDAH) may limit generalizability to broader conversational scenarios.
- The study does not address potential confounding factors such as temperature settings, prompt engineering, or model versions.

## Confidence

- Model performance rankings (GPT-4/Claude > Mixtral > ChatGPT-3/Gemini): **High** - Consistent across multiple metrics and datasets
- Claim of pipeline scalability and flexibility: **Medium** - Based on synthetic evaluation rather than empirical scaling tests
- Gender bias findings: **Medium** - Limited to observed patterns without statistical significance testing
- Claims about mechanism superiority: **Medium** - Correlational evidence but lacking ablation studies

## Next Checks
1. Conduct statistical significance testing on performance differences between models across datasets to determine if observed gaps are meaningful
2. Perform ablation studies varying temperature settings and prompt engineering to isolate their effects on model performance
3. Expand evaluation to include additional conversational QA datasets and real-world conversational data to test generalizability of findings