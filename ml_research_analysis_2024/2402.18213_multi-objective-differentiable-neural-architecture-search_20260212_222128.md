---
ver: rpa2
title: Multi-objective Differentiable Neural Architecture Search
arxiv_id: '2402.18213'
source_url: https://arxiv.org/abs/2402.18213
tags:
- modnas
- search
- latency
- devices
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MODNAS introduces a scalable method for multi-objective differentiable
  neural architecture search (NAS) that efficiently profiles Pareto fronts across
  multiple hardware devices. By leveraging a hypernetwork conditioned on hardware
  features and user preference vectors, MODNAS generates diverse, Pareto-optimal architectures
  without requiring multiple search runs.
---

# Multi-objective Differentiable Neural Architecture Search

## Quick Facts
- arXiv ID: 2402.18213
- Source URL: https://arxiv.org/abs/2402.18213
- Reference count: 0
- Primary result: MODNAS enables efficient multi-objective NAS profiling across multiple hardware devices using a single search run

## Executive Summary
MODNAS introduces a scalable method for multi-objective differentiable neural architecture search that efficiently profiles Pareto fronts across multiple hardware devices. By leveraging a hypernetwork conditioned on hardware features and user preference vectors, MODNAS generates diverse, Pareto-optimal architectures without requiring multiple search runs. The approach combines meta-learning with multiple gradient descent to optimize for conflicting objectives like accuracy, latency, and energy consumption simultaneously.

## Method Summary
MODNAS uses a MetaHypernetwork that conditions on hardware features and user preference vectors to parameterize joint architectural distributions across devices and objectives. The method employs Multiple Gradient Descent (MGD) to optimize across all objectives and devices simultaneously, with a MetaPredictor providing fast hardware objective estimation without expensive training. This framework enables zero-shot transferability to new devices while maintaining high Pareto front quality across diverse search spaces and hardware configurations.

## Key Results
- Outperforms existing methods in hypervolume metrics while reducing search costs from O(T) to O(1)
- Achieves state-of-the-art results on tasks ranging from image classification to language modeling
- Demonstrates effectiveness across 19 devices and 4 search spaces including CNNs and Transformers

## Why This Works (Mechanism)

### Mechanism 1
A single search run can generate Pareto-optimal architectures for multiple hardware devices without retraining. The MetaHypernetwork conditions on both a user preference vector and a hardware embedding, generating a joint architectural distribution across devices. This distribution is sampled via a differentiable Architect, enabling gradient-based optimization in a single search run.

### Mechanism 2
Multiple Gradient Descent ensures simultaneous improvement across all objectives for all devices in each update. MGD computes a weighted sum of gradients across devices and objectives, where weights are found via a Frank-Wolfe solver. This guarantees Pareto improvement: all objectives improve or remain unchanged in each step.

### Mechanism 3
A frozen MetaPredictor enables fast hardware objective estimation without expensive training. For objectives like latency or energy, a lightweight regression model predicts values from architecture and hardware embeddings. This avoids running architectures on hardware during search, drastically reducing cost.

## Foundational Learning

- Hypernetworks and their use in architecture generation: Why needed here - Hypernetworks generate architectural parameters conditioned on hardware and preferences, enabling a single model to cover many architectures and devices. Quick check - What is the role of the hypernetwork in MODNAS, and how does it differ from a standard NAS supernet?

- Multi-objective optimization and Pareto fronts: Why needed here - MODNAS seeks diverse Pareto-optimal solutions balancing accuracy, latency, and energy across devices. Quick check - How does the preference vector influence the scalarization of objectives, and what does this mean for the generated Pareto front?

- Differentiable architecture sampling (ReinMax/GDAS): Why needed here - Discrete architecture choices must be made differentiable for gradient-based search. Quick check - Why is the Straight-Through Estimator or ReinMax needed in the Architect, and what problem do they solve?

## Architecture Onboarding

- Component map: MetaHypernetwork → Architect → (Supernetwork + MetaPredictor) → MGD → MetaHypernetwork
- Critical path: MetaHypernetwork → Architect → (Supernetwork + MetaPredictor) → MGD → MetaHypernetwork
- Design tradeoffs:
  - Using MetaPredictor vs. direct hardware evaluation: Faster but less accurate
  - MGD vs. mean gradient: More complex but better Pareto improvement
  - ReinMax vs. GDAS: ReinMax more accurate but slightly more complex
- Failure signatures:
  - Poor Pareto front: Likely MetaHypernetwork or MGD not capturing device/objective relationships
  - Slow convergence: Could be MGD not working or Supernetwork training issues
  - Inaccurate predictions: MetaPredictor needs more training data or better architecture
- First 3 experiments:
  1. Train MetaHypernetwork with random preference vectors and one device, evaluate generated architectures' accuracy vs. ground truth.
  2. Replace MGD with mean gradient, compare Pareto front quality on a simple 2-device, 2-objective setup.
  3. Swap ReinMax for GDAS in Architect, measure impact on final Pareto front quality.

## Open Questions the Paper Calls Out

### Open Question 1
How does MODNAS perform on search spaces with non-convex Pareto fronts? The paper mentions testing on HW-GPT-Bench with non-convex Pareto fronts but only provides results for one specific space. Additional experiments on various non-convex search spaces would demonstrate robustness.

### Open Question 2
What is the impact of the number of objectives (M) on MODNAS's performance? While the paper mentions handling three objectives and claims scalability beyond two, it lacks detailed analysis of performance changes as objectives increase.

### Open Question 3
How does the choice of the MetaHypernetwork architecture affect MODNAS's performance? The paper describes the architecture but doesn't explore different MetaHypernetwork designs or their impact on various search spaces.

## Limitations

- MetaPredictor accuracy for hardware metrics is critical but not thoroughly validated, potentially leading to suboptimal architecture selection
- Scalability to a large number of diverse hardware devices remains untested, with potential challenges in capturing hardware similarities
- MGD optimization complexity increases with more devices and objectives, potentially causing convergence issues

## Confidence

- High confidence: The overall framework combining hypernetworks, meta-learning, and MGD for multi-objective NAS is sound and well-motivated
- Medium confidence: The effectiveness of the MetaPredictor and the scalability of the MetaHypernetwork to many devices are reasonable but require more validation
- Low confidence: The impact of non-smooth Pareto fronts on MGD convergence and the performance on highly diverse hardware sets need further investigation

## Next Checks

1. Evaluate MetaPredictor's predictions on held-out architectures and devices, comparing predicted vs. actual hardware metrics to quantify accuracy and search impact
2. Extend search to 30+ diverse hardware devices (CPUs, GPUs, FPGAs, ASICs) to assess MetaHypernetwork scalability and MGD stability
3. Introduce non-smooth Pareto fronts with conflicting objectives or diverse hardware to test MGD robustness and convergence behavior