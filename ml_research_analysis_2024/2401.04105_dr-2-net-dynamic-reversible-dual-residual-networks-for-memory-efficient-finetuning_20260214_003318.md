---
ver: rpa2
title: 'Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient
  Finetuning'
arxiv_id: '2401.04105'
source_url: https://arxiv.org/abs/2401.04105
tags:
- finetuning
- pretrained
- video
- network
- reversible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dr2Net, a novel approach for memory-efficient
  fine-tuning of large pre-trained models. Dr2Net addresses the problem of high memory
  consumption during end-to-end fine-tuning, especially for tasks with high-resolution
  data like video understanding and object detection.
---

# Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning

## Quick Facts
- arXiv ID: 2401.04105
- Source URL: https://arxiv.org/abs/2401.04105
- Reference count: 40
- Key result: Achieves 46.1-79.5% memory reduction across video tasks while maintaining comparable performance to conventional fine-tuning

## Executive Summary
Dr$^2$Net introduces a novel approach for memory-efficient fine-tuning of large pre-trained models by transforming them into reversible dual-residual networks. The method addresses the high memory consumption challenge in fine-tuning for tasks with high-resolution data like video understanding and object detection. By introducing a second set of residual connections with dynamic coefficients, Dr$^2$Net enables seamless initialization from pre-trained non-reversible models while achieving significant memory savings through reversible computation. Experimental results demonstrate substantial memory reduction (46.1% to 79.5%) across multiple computer vision tasks while maintaining competitive performance.

## Method Summary
Dr$^2$Net converts pre-trained non-reversible networks into reversible dual-residual networks by adding new residual connections and introducing coefficients (α and β) to control the network's proximity to either architecture. The method employs a dynamic training strategy that transitions from the pre-trained non-reversible model to a reversible network with higher numerical precision. The dual-residual design allows for direct parameter initialization from pre-trained models while the reversible computation eliminates the need to store intermediate activations, significantly reducing memory consumption during fine-tuning.

## Key Results
- Memory reduction of 46.1%, 56.6%, and 79.5% for three video tasks (temporal action detection, video object segmentation, action recognition)
- Memory savings of 30.6% and 44.4% for point cloud segmentation and object detection tasks
- Achieves comparable performance to conventional fine-tuning while significantly reducing memory usage
- Demonstrates effectiveness across multiple pre-trained architectures including Swin, ViT, Video Swin, and Video ViT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual-residual design allows seamless initialization from pretrained non-reversible networks.
- Mechanism: Dr$^2$Net introduces a second set of residual connections with coefficient β while keeping the original residual structure with coefficient α. At initialization (α=1, β=0), the network is mathematically equivalent to the pretrained model, enabling direct parameter transfer.
- Core assumption: The pretrained network has residual connections or can be expressed in a form compatible with the Dr$^2$Net module structure.
- Evidence anchors: [abstract] "Dr$^2$Net contains two types of residual connections, one maintaining the residual structure in the pretrained models, and the other making the network reversible." [section] "We use two coefficients on either type of residual connections respectively, and introduce a dynamic training strategy that seamlessly transitions the pretrained model to a reversible network"

### Mechanism 2
- Claim: Dynamic adjustment of α and β coefficients balances initialization proximity and numerical precision.
- Mechanism: The coefficients start at (α=1, β=0.1) to maintain pretrained model equivalence, then transition to values like (α=0, β=1) to achieve optimal numerical precision for reversible computation. This creates a trade-off between preserving pretrained initialization and minimizing gradient error.
- Core assumption: The gradient error landscape varies predictably with α and β values, and there exists a point with acceptable error that also maintains reasonable performance.
- Evidence anchors: [abstract] "We use two coefficients on either type of residual connections respectively, and introduce a dynamic training strategy that seamlessly transitions the pretrained model to a reversible network" [section] "We identify two key factors that impact the effectiveness of Dr$^2$Net finetuning: (1) the model's proximity to the pretrained network $M_n$, and (2) gradient precision of the customized back-propagation." [section] "In Fig. 3, we plot the gradient error levels... We can see that the error level is the lowest $10^{-12}$ when α=0 and β=1... The error level in the middle area is around $10^{-7}$, which is already acceptable."

### Mechanism 3
- Claim: Reversible computation eliminates the need to store intermediate activations, reducing memory consumption.
- Mechanism: By making the network reversible, intermediate activations can be reconstructed from output during backpropagation, allowing them to be cleared from memory during the forward pass. This provides constant memory usage regardless of network depth.
- Core assumption: The network's reversibility is mathematically sound and numerical errors during reconstruction remain within acceptable bounds.
- Evidence anchors: [abstract] "Due to its reversibility, intermediate activations, which can be reconstructed from output, are cleared from memory during training." [section] "Due to the reversibility of each module, all the intermediate activations $x_i$ and $y_i$ can be reconstructed from the output, and hence don't need to be cached in memory" [section] "in reversible networks, though the activations can be exactly reconstructed when done in exact arithmetic, numerical error may be accumulated during back propagation due to floating point computation with limited precision."

## Foundational Learning

- Concept: Reversible network computation
  - Why needed here: Understanding how intermediate activations can be reconstructed from output rather than stored is fundamental to grasping the memory efficiency mechanism.
  - Quick check question: If a reversible network has N layers and each layer's output can be reconstructed from the final output, how much memory is needed to store intermediate activations?

- Concept: Jacobian determinant and invertibility
  - Why needed here: The mathematical proof of reversibility relies on showing the Jacobian determinant is non-zero, which ensures the network function is invertible.
  - Quick check question: Given the Jacobian matrix J for Dr$^2$Net's ith module, what condition must β satisfy to ensure the network is reversible?

- Concept: Numerical precision in floating point arithmetic
  - Why needed here: Understanding how floating point errors accumulate during reversible reconstruction is critical for appreciating the dynamic coefficient adjustment mechanism.
  - Quick check question: Why might using β=0 (to exactly match the pretrained network) be problematic in practice despite being mathematically correct?

## Architecture Onboarding

- Component map: Input → Fi blocks with α-weighted residuals → β-weighted reversible path → Output
- Critical path: Forward pass: Input → Fi blocks with α-weighted residuals → β-weighted reversible path → Output. Reverse pass: Output → Reconstruct intermediates using Eq. 2 → Compute gradients.
- Design tradeoffs: The dual-residual design adds computational overhead (estimated 33% more operations) but provides memory savings that scale with network depth. The dynamic coefficient adjustment adds training complexity but enables initialization from pretrained models.
- Failure signatures: If gradient errors become too large (e.g., >10^-4), training will fail. If α transitions too quickly to 0, the model may lose beneficial pretrained initialization. If β is too small, numerical errors in reconstruction will corrupt gradients.
- First 3 experiments:
  1. Implement a simple Dr$^2$Net module with a single Fi block and verify reversibility by reconstructing inputs from outputs.
  2. Test gradient error levels across different α and β values using a random input tensor to validate the error landscape shown in Fig. 3.
  3. Implement the dynamic coefficient update schedule on a small pretrained model (e.g., ViT-small) and verify memory savings during finetuning on a simple downstream task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different updating frequencies (η) affect the convergence speed and final accuracy of Dr$^2$Net across various tasks?
- Basis in paper: [explicit] The paper mentions comparing different η values for the task of action recognition with VideoMAE pretrained model and notes that smaller η values result in higher performance. However, the impact on convergence speed and other tasks is not explored.
- Why unresolved: The experiments only provide results for one specific task and do not analyze the effect on convergence speed or other downstream tasks.
- What evidence would resolve it: Conducting experiments with varying η values across multiple tasks and measuring both convergence speed and final accuracy would provide a comprehensive understanding of the impact of updating frequency.

### Open Question 2
- Question: What is the optimal balance between numerical precision and architectural similarity to the pretrained model during the dynamic finetuning process?
- Basis in paper: [explicit] The paper discusses the tradeoff between numerical precision and architectural similarity, but does not provide a clear framework for determining the optimal balance.
- Why unresolved: The paper suggests that the importance of these factors changes over the finetuning process, but does not offer a method to quantify or optimize this balance.
- What evidence would resolve it: Developing a theoretical framework or empirical study that quantifies the impact of numerical precision and architectural similarity on final accuracy would help determine the optimal balance.

### Open Question 3
- Question: How does Dr$^2$Net perform on tasks beyond computer vision, such as natural language processing or audio analysis?
- Basis in paper: [explicit] The paper mentions the potential for extending Dr$^2$Net to other domains, but does not provide any experimental results or analysis.
- Why unresolved: The paper focuses solely on computer vision tasks and does not explore the applicability or performance of Dr$^2$Net in other domains.
- What evidence would resolve it: Implementing Dr$^2$Net for tasks in natural language processing or audio analysis and comparing its performance to conventional methods would demonstrate its broader applicability.

## Limitations

- The exact implementation details for the dynamic coefficient schedule are underspecified, making faithful reproduction challenging
- The paper doesn't address whether the computational overhead (estimated 33% more operations) offsets memory benefits in practical deployment scenarios
- Experiments only show relative improvements without absolute memory consumption numbers, limiting practical deployment assessment

## Confidence

- **High confidence**: The memory reduction claims are well-supported by experimental results across multiple tasks and datasets. The reversible computation mechanism for eliminating intermediate activation storage is mathematically sound.
- **Medium confidence**: The performance preservation claims are supported but could benefit from more ablation studies on the dynamic coefficient scheduling. The initialization equivalence claim assumes pretrained models have compatible residual structures.
- **Low confidence**: The exact implementation details for the dynamic training strategy are underspecified, making faithful reproduction challenging without additional experimentation.

## Next Checks

1. **Coefficient Schedule Validation**: Implement and test multiple dynamic coefficient update schedules (linear, exponential, step-wise) to determine which provides the best trade-off between initialization proximity and numerical precision across different model architectures.

2. **Cross-Architecture Compatibility**: Test Dr$^2$Net initialization from non-residual architectures (e.g., Vision Transformers without residual connections) to validate whether the dual-residual adaptation works universally or is limited to models with compatible structures.

3. **Memory-Computation Tradeoff Analysis**: Measure both memory consumption and computational overhead (FLOPs) across different network depths to determine the break-even point where memory savings justify the additional computation cost.