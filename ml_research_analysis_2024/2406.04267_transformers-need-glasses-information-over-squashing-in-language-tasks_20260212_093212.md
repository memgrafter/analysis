---
ver: rpa2
title: Transformers need glasses! Information over-squashing in language tasks
arxiv_id: '2406.04267'
source_url: https://arxiv.org/abs/2406.04267
tags:
- sequence
- token
- which
- collapse
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies fundamental limitations in decoder-only Transformers
  that cause representational collapse and over-squashing, leading to failure on simple
  counting and copying tasks. The authors prove that distinct input sequences can
  yield arbitrarily close representations in the final token, especially with low-precision
  floating-point formats, making the model provably unable to distinguish between
  them.
---

# Transformers need glasses! Information over-squashing in language tasks

## Quick Facts
- arXiv ID: 2406.04267
- Source URL: https://arxiv.org/abs/2406.04267
- Authors: Federico Barbero, Andrea Banino, Steven Kapturowski, Dharshan Kumaran, João G. M. Araújo, Alex Vitvitskyi, Razvan Pascanu, Petar Veličković
- Reference count: 40
- Primary result: Decoder-only Transformers suffer from representational collapse and over-squashing, making them provably unable to distinguish between certain distinct input sequences, especially for long sequences and in low-precision arithmetic.

## Executive Summary
This paper identifies fundamental limitations in decoder-only Transformers that cause representational collapse and over-squashing, leading to failure on simple counting and copying tasks. The authors prove that distinct input sequences can yield arbitrarily close representations in the final token, especially with low-precision floating-point formats, making the model provably unable to distinguish between them. They also show that earlier tokens in a sequence have exponentially more influence on the final representation due to the causal attention mechanism, leading to information loss. Empirical validation on Gemini 1.5 and Gemma 7B demonstrates these effects occur in practice, with representational collapse happening around sequence length 50-300.

## Method Summary
The paper combines theoretical analysis with empirical validation to demonstrate representational collapse in decoder-only Transformers. The theoretical framework analyzes signal propagation through Transformer layers, showing how softmax attention and positional encoding decay lead to information loss. The empirical component tests contemporary LLMs (Gemini 1.5, Gemma 7B) on controlled counting and copying tasks with sequences of varying lengths. The authors also propose and validate simple solutions including separator tokens and local sliding window attention to mitigate these issues.

## Key Results
- Distinct input sequences can yield arbitrarily close final token representations under low-precision arithmetic, making them indistinguishable
- Earlier tokens in a sequence have exponentially more influence on final representations than later tokens due to causal attention
- Gemini 1.5 and Gemma 7B fail on counting tasks for sequences longer than approximately 50-300 tokens
- Simple solutions like separator tokens or local sliding window attention can mitigate these issues

## Why This Works (Mechanism)

### Mechanism 1: Representational Collapse in Decoder-Only Transformers
- Claim: Distinct input sequences can yield arbitrarily close final token representations, making them indistinguishable under finite precision.
- Mechanism: As sequence length grows, the softmax attention weights between nearly identical sequences converge, causing their final representations to become arbitrarily close. This is exacerbated by low-precision floating-point formats used in modern LLMs.
- Core assumption: Positional encodings decay to zero with increasing distance, and sequences differ only in their final token(s).
- Evidence anchors: [abstract]: "we prove that certain distinct sequences of inputs to the Transformer can yield arbitrarily close representations in the final token"

### Mechanism 2: Over-Squashing Due to Unidirectional Attention Flow
- Claim: Earlier tokens in a sequence have exponentially more influence on the final representation than later tokens due to the causal attention mechanism.
- Mechanism: The decoder-only architecture creates a topological bottleneck where information from later tokens must traverse more attention steps to reach the final representation. This leads to information loss, particularly for tokens near the end of long sequences.
- Core assumption: Attention weights can be treated as approximately independent of input values for analysis purposes.
- Evidence anchors: [abstract]: "we show that decoder-only Transformer language models can lose sensitivity to specific tokens in the input"

### Mechanism 3: Counting Limitations Due to Softmax Normalization
- Claim: Transformer models struggle with counting tasks because softmax normalization inherently loses information about sequence length and magnitude.
- Mechanism: The softmax operation normalizes attention weights to sum to one, effectively discarding information about the absolute number of tokens. This makes it impossible to count without additional mechanisms like positional encodings or causal masking.
- Core assumption: The softmax normalization is the primary mechanism for computing attention weights.
- Evidence anchors: [section 6]: "the normalisation of the softmax makes it hard for a model to take into account the length of a sequence"

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how information flows through decoder-only transformers is crucial for analyzing representational collapse and over-squashing
  - Quick check question: How does the causal attention mechanism in decoder-only transformers differ from bidirectional attention in encoder-only models?

- Concept: Floating-point precision and numerical stability
  - Why needed here: The paper demonstrates how low-precision arithmetic exacerbates representational collapse
  - Quick check question: What is the difference between bf16 and fp32 precision, and how might this affect representational capacity?

- Concept: Graph neural networks and over-squashing
  - Why needed here: The paper draws parallels between transformer limitations and well-studied GNN phenomena
  - Quick check question: What is the relationship between commute time in graphs and information propagation in transformers?

## Architecture Onboarding

- Component map: Input embeddings with positional encodings -> Multi-head causal attention layers -> Feed-forward networks -> Layer normalization -> Final output projection for next-token prediction

- Critical path: Input sequence → Positional encodings → Multi-head attention → Feed-forward → Layer norm → Output projection

- Design tradeoffs:
  - Causal vs. bidirectional attention (affects over-squashing)
  - Precision level (bf16 vs fp32) affects representational collapse
  - Positional encoding type (decay vs absolute) affects counting capability

- Failure signatures:
  - Poor performance on counting and copying tasks for long sequences
  - Inability to distinguish between sequences that differ only in final tokens
  - U-shaped performance curve where early and late tokens are more retrievable than middle tokens

- First 3 experiments:
  1. Implement synthetic experiments to verify representational collapse with different positional encodings
  2. Test counting performance on sequences of varying lengths with different precision formats
  3. Measure attention weight distributions to quantify over-squashing effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between representational collapse and total variation distance in softmax distributions under different positional encoding schemes?
- Basis in paper: [explicit] The paper mentions total variation in the context of representational collapse proofs and notes connections to information theory.
- Why unresolved: The paper provides theoretical bounds for certain positional encoding types but doesn't establish a complete characterization across all common positional encodings like RoPE, Alibi, and sinusoidal encodings.
- What evidence would resolve it: A comprehensive analysis showing how total variation bounds vary with different positional encoding schemes, with empirical validation on multiple LLMs showing when and how representational collapse occurs under each scheme.

### Open Question 2
- Question: How does the depth of a decoder-only Transformer affect the rate of representational collapse and information over-squashing?
- Basis in paper: [inferred] The paper mentions that information loss increases with sequence length and discusses the effect of layers in over-squashing analysis, but doesn't systematically study the effect of depth.
- Why unresolved: While the paper provides theoretical bounds on information loss, it doesn't empirically investigate how increasing the number of layers impacts the severity and onset of representational collapse.
- What evidence would resolve it: Systematic experiments varying the depth of decoder-only Transformers while measuring representational collapse and over-squashing across different sequence lengths, with theoretical analysis of how depth affects the convergence of representations.

### Open Question 3
- Question: What is the optimal window size for local sliding window attention that minimizes representational collapse while maintaining performance on language tasks?
- Basis in paper: [explicit] The paper mentions local sliding window attention as a potential solution to representational collapse and over-squashing in the context of Gemma 2.
- Why unresolved: The paper identifies local sliding window attention as a promising solution but doesn't provide empirical analysis of how different window sizes affect the trade-off between information preservation and task performance.
- What evidence would resolve it: Empirical studies varying window sizes in local attention mechanisms across different sequence lengths and tasks, measuring both representational collapse and task performance to identify optimal window sizes.

## Limitations

- Theoretical proofs demonstrate representational collapse under specific conditions, but empirical validation relies on observing performance degradation rather than directly measuring representational distances
- Analysis assumes attention weights become independent of input values, which may not hold in well-trained models
- The connection between theoretical framework and observed empirical failures is suggestive but not rigorously established

## Confidence

**High Confidence:** The empirical observations of performance degradation on counting and copying tasks for long sequences in contemporary LLMs (Gemini 1.5, Gemma 7B) are well-supported and reproducible.

**Medium Confidence:** The theoretical proof of representational collapse under the stated conditions is mathematically sound, but its practical relevance depends on whether real LLMs satisfy these conditions.

**Low Confidence:** The specific claim that softmax normalization inherently prevents counting is not well-supported, as modern LLMs may learn compensatory mechanisms.

## Next Checks

**Validation Check 1:** Conduct controlled experiments comparing representational distances in synthetic Transformers with different positional encoding schemes (decaying vs absolute) and precision levels (bf16 vs fp32). Measure the L1 distance between final token representations of sequences that differ only in their last token.

**Validation Check 2:** Test the counting performance of Gemini 1.5 and Gemma 7B on sequences with varying patterns (constant ones vs alternating zeros and ones) and with explicit positional hints in the prompts.

**Validation Check 3:** Implement the proposed sliding window attention mechanism and conduct ablation studies to isolate which aspects of the solution address the theoretical limitations. Compare performance with standard attention on counting and copying tasks.