---
ver: rpa2
title: Learning Granger Causality from Instance-wise Self-attentive Hawkes Processes
arxiv_id: '2402.03726'
source_url: https://arxiv.org/abs/2402.03726
tags:
- event
- causal
- causality
- granger
- instance-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning Granger causality
  from asynchronous, multi-type event sequences at the instance level. The proposed
  method, Instance-wise Self-Attentive Hawkes Processes (ISAHP), leverages a transformer-based
  self-attention mechanism to capture complex instance-level causal structures while
  maintaining an additive structure in the intensity function that enables direct
  Granger causality inference.
---

# Learning Granger Causality from Instance-wise Self-attentive Hawkes Processes

## Quick Facts
- arXiv ID: 2402.03726
- Source URL: https://arxiv.org/abs/2402.03726
- Authors: Dongxia Wu; Tsuyoshi Idé; Aurélie Lozano; Georgios Kollias; Jiří Navrátil; Naoki Abe; Yi-An Ma; Rose Yu
- Reference count: 14
- Primary result: ISAHP achieves AUC of 0.967 on Synergy and 0.835 on MT for type-level causal discovery

## Executive Summary
This paper addresses the challenge of learning Granger causality from asynchronous, multi-type event sequences at the instance level. The proposed Instance-wise Self-Attentive Hawkes Processes (ISAHP) method leverages transformer-based self-attention to capture complex instance-level causal structures while maintaining an additive structure in the intensity function that enables direct Granger causality inference. ISAHP achieves state-of-the-art performance on two proxy tasks: type-level causal discovery and instance-level event type prediction, demonstrating its ability to capture synergistic causal effects that classical linear Hawkes models cannot handle.

## Method Summary
ISAHP uses a transformer-based self-attention mechanism to parameterize the Hawkes process intensity function, allowing it to capture complex instance-level causal relationships while maintaining the additive structure necessary for interpretable Granger causality. The model computes attention scores between events to determine causal influences, then aggregates these instance-level effects to infer type-level causal patterns. Training is performed using maximum likelihood estimation with type-level regularization to ensure both instance-level and type-level performance.

## Key Results
- Type-level causal discovery: AUC of 0.967 on Synergy dataset, 0.835 on MT dataset
- Instance-level event type prediction: Accuracy of 0.471 on Synergy, 0.974 on MT
- Successfully captures synergistic causal effects that classical linear Hawkes models cannot handle
- Outperforms other neural point process baselines by directly parameterizing intensity function with attention scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The additive structure in the intensity function preserves instance-level causal interpretability while allowing complex interactions through self-attention.
- Mechanism: By maintaining the Hawkes process intensity as a sum over historical events, each term α(x, xj)ϕ(t - tj | x, xj) can be directly interpreted as the causal influence of event j on event i. The self-attention mechanism computes these α values based on event embeddings, allowing nonlinear interactions without losing the additive interpretability.
- Core assumption: The causal influence between events can be decomposed into additive components that can be parameterized by attention-based functions.
- Evidence anchors:
  - [abstract]: "leverages the self-attention mechanism of the transformer to align with the principles of Granger causality"
  - [section]: "maintains an additive structure over the historical events in the intensity function"
  - [corpus]: Weak evidence - corpus lacks specific validation of this mechanism
- Break condition: If the additive structure assumption fails or attention scores cannot be properly interpreted as causal strengths.

### Mechanism 2
- Claim: Instance-level causal inference is achieved through attention-based kernel functions that capture both type-level patterns and individual event interactions.
- Mechanism: The kernel function α(x, xj) is computed as a function of the attention score A(x, xj) and the value embedding vj, allowing it to capture both the similarity between events (through attention) and the specific relevance to the target event type (through learned parameters). This creates a bridge between instance-level and type-level causality.
- Core assumption: Event embeddings capture sufficient information about both temporal context and event type characteristics to enable meaningful similarity computations.
- Evidence anchors:
  - [section]: "the attention weight A(x, xj) represents the similarity between x and xj"
  - [section]: "α(x, xj) → A(x, xj)w̃αk,j where w̃αk,j ≜ (wαk)⊤vj"
  - [corpus]: No direct evidence in corpus about this specific mechanism
- Break condition: If attention similarity scores don't correlate with actual causal relationships or embeddings don't capture relevant event characteristics.

### Mechanism 3
- Claim: Type-level causality emerges naturally from instance-level causal strengths through aggregation, creating a coherent framework that benefits both tasks.
- Mechanism: The type-level causal matrix ᾱk,k' is computed as the average of instance-level causal strengths αs,i,j for events with types k and k'. This aggregation creates a natural link between the two levels of analysis and allows the model to learn from both perspectives simultaneously.
- Core assumption: Averaging instance-level causal strengths produces meaningful type-level causal estimates that align with ground truth.
- Evidence anchors:
  - [section]: "ᾱk,k' ≜ 1/Nk,k ∑s,i,j δks,j,k δks,i,k αs,i,j"
  - [section]: "our experiments also show that ISAHP achieves state-of-the-art performance in two proxy tasks, one involving type-level causal discovery and another involving instance-level event type prediction"
  - [corpus]: No corpus evidence supporting this aggregation mechanism
- Break condition: If type-level aggregation doesn't capture meaningful patterns or creates spurious correlations.

## Foundational Learning

- Concept: Granger causality in point processes
  - Why needed here: The entire paper builds on extending Granger causality from regular time series to asynchronous event sequences using point process models.
  - Quick check question: What distinguishes Granger causality in point processes from traditional time series Granger causality?

- Concept: Hawkes process intensity function structure
  - Why needed here: Understanding why the additive structure is critical for causal interpretation and how it differs from other point process models.
  - Quick check question: How does the additive structure in Hawkes processes enable causal interpretation that other point process models lack?

- Concept: Self-attention mechanism in transformers
  - Why needed here: The paper's innovation relies on using self-attention to parameterize causal influences while maintaining interpretability.
  - Quick check question: What properties of self-attention make it suitable for capturing instance-level causal relationships in event sequences?

## Architecture Onboarding

- Component map: Input (timestamps, event types) → Embedding layer (MLP on time difference and one-hot type) → Self-attention mechanism (key-value-query with learned parameters) → Kernel function (α and γ parameterized by attention scores) → Intensity function (additive structure with background intensity) → Output (likelihood for MLE training)
- Critical path: Event embedding → Attention computation → Kernel parameterization → Intensity calculation → Likelihood evaluation
- Design tradeoffs: The additive structure enables interpretability but may limit expressiveness compared to fully connected architectures; attention-based parameterization allows complex interactions but requires careful regularization to prevent overfitting.
- Failure signatures: Poor type-level performance indicates issues with the aggregation mechanism; poor instance-level prediction suggests embedding or attention parameterization problems; unstable training points to regularization issues.
- First 3 experiments:
  1. Verify that attention scores correlate with known causal relationships on a simple synthetic dataset with ground truth.
  2. Test the effect of regularization parameters on type-level vs instance-level performance trade-offs.
  3. Compare performance on sequences with synergistic vs non-synergistic causal patterns to validate the model's ability to capture complex interactions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ISAHP perform when the underlying data generation mechanism does not adhere to the linearity assumption of type-level intensity functions?
- Basis in paper: [explicit] The paper mentions that Synergy dataset involves synergistic effects between multiple causes and MT is based on a real-world dataset including non-linear effects. The underlying data generation mechanisms do not adhere to the linearity assumption of the type-level intensity functions of the MHP baselines (HExp, HSG, CRHG).
- Why unresolved: The paper does not provide a direct comparison of ISAHP's performance on datasets with non-linear causal effects versus datasets with linear causal effects.
- What evidence would resolve it: Empirical results comparing ISAHP's performance on datasets with linear and non-linear causal effects.

### Open Question 2
- Question: How does the inclusion of type-level regularization (TLR) in ISAHP affect its performance on datasets with different characteristics?
- Basis in paper: [explicit] The paper conducts an ablation study on TLR and shows that including TLR improves the model performance for both Synergy and MT datasets.
- Why unresolved: The paper does not explore the impact of TLR on datasets with different characteristics, such as datasets with a large number of event types or datasets with long-range temporal dependencies.
- What evidence would resolve it: Empirical results comparing ISAHP's performance with and without TLR on datasets with varying characteristics.

### Open Question 3
- Question: How does ISAHP's performance on instance-level causal discovery compare to other methods when the ground truth data on instance-level causality is available?
- Basis in paper: [explicit] The paper mentions that the main motivation of ISAHP is instance-level Granger-causal analysis, but it includes proxy tasks involving type-level inference as well as instance-level event type prediction due to the scarcity of ground truth data on instance-level causality.
- Why unresolved: The paper does not provide a direct comparison of ISAHP's performance on instance-level causal discovery with other methods when the ground truth data on instance-level causality is available.
- What evidence would resolve it: Empirical results comparing ISAHP's performance on instance-level causal discovery with other methods on datasets with available ground truth data on instance-level causality.

## Limitations

- Core claims about instance-level causal inference rely on untested assumptions about the additive structure and attention-causality correlation
- Empirical validation depends heavily on proxy tasks rather than direct causal inference evaluation
- No ablation studies showing the impact of removing key components like type-level regularization
- Performance varies significantly across datasets, with instance-level prediction much better on MT than Synergy

## Confidence

- **Type-level causal discovery claims (AUC 0.967/0.835)**: Medium confidence - strong performance metrics but based on synthetic data with unknown generation process
- **Instance-level event prediction (accuracy 0.471/0.974)**: Medium confidence - results show improvement over baselines but performance varies significantly across datasets
- **Synergistic effect capture**: Low confidence - claims are supported by a single ratio metric without deeper analysis of what these effects represent
- **Attention as causal attribution**: Medium confidence - theoretically sound but lacks rigorous validation that attention weights correspond to actual causal strength

## Next Checks

1. **Ablation study on additive structure**: Remove the Hawkes process additive constraint and replace with a fully connected neural network to quantify the expressiveness trade-off versus interpretability benefit

2. **Attention-causality correlation analysis**: On a synthetic dataset with known ground truth, measure the correlation between learned attention weights and actual causal strengths across different event types and temporal distances

3. **Out-of-distribution generalization test**: Evaluate the model on sequences with significantly different event type distributions or temporal patterns than training data to assess robustness of causal inference