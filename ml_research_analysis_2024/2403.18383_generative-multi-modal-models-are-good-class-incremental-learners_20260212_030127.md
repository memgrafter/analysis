---
ver: rpa2
title: Generative Multi-modal Models are Good Class-Incremental Learners
arxiv_id: '2403.18383'
source_url: https://arxiv.org/abs/2403.18383
tags:
- learning
- tasks
- generative
- arxiv
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in class-incremental
  learning by proposing a generative multi-modal model (GMM) framework. Instead of
  using discriminative models with expanding classification heads, GMM directly generates
  textual labels for images using an adapted generative model.
---

# Generative Multi-modal Models are Good Class-Incremental Learners

## Quick Facts
- arXiv ID: 2403.18383
- Source URL: https://arxiv.org/abs/2403.18383
- Authors: Xusheng Cao; Haori Lu; Linlan Huang; Xialei Liu; Ming-Ming Cheng
- Reference count: 40
- This paper proposes using generative multi-modal models for class-incremental learning, achieving at least 14% better accuracy than state-of-the-art methods in few-shot settings

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental learning by proposing a generative multi-modal model (GMM) framework. Instead of using discriminative models with expanding classification heads, GMM directly generates textual labels for images using an adapted generative model. These generated texts are then compared with actual category names using a text encoder to determine the classification prediction. This approach avoids the bias towards current tasks seen in discriminative models. The method achieves significantly better results in long-sequence task scenarios and improves accuracy by at least 14% over state-of-the-art methods in few-shot CIL settings, with substantially less forgetting.

## Method Summary
The proposed method uses pre-trained generative multi-modal models (EVi-T-CLIP, MiniGPT-4) to generate textual descriptions of images, which are then compared to category names using a text encoder for classification. During training, only a linear projection layer is fine-tuned on the image encoder output for 1-2 epochs per task. At inference, generated text descriptions are compared to category names using cosine similarity. The approach uses cross-entropy loss on generated text labels and avoids the bias towards current tasks that plagues discriminative models.

## Key Results
- Achieves at least 14% better accuracy than state-of-the-art methods in few-shot class-incremental learning settings
- Significantly reduces catastrophic forgetting compared to discriminative approaches
- Performs exceptionally well in long-sequence task scenarios where traditional methods struggle

## Why This Works (Mechanism)
The generative approach works because it treats class-incremental learning as a retrieval problem rather than a classification problem. By generating textual descriptions of images and comparing them to category names, the method avoids the bias that discriminative models develop towards recently learned classes. The text-based representation provides a more stable feature space that doesn't degrade as new tasks are learned, unlike the feature representations in traditional discriminative models.

## Foundational Learning
- **Class-Incremental Learning (CIL)**: Learning new classes sequentially while retaining knowledge of previous classes; needed because real-world scenarios often involve learning from streaming data
- **Catastrophic Forgetting**: When learning new tasks causes dramatic performance degradation on previous tasks; critical to understand because it's the core problem being addressed
- **Generative Multi-modal Models**: Models that can generate text from images; essential because they provide the text-based feature representation
- **Text Encoder Similarity Matching**: Using cosine similarity between text embeddings for classification; fundamental to the retrieval-based approach
- **Cross-Entropy Loss**: Standard classification loss used during fine-tuning; needed for training the linear projection layer

## Architecture Onboarding

Component Map:
Pre-trained Generative Model (EVi-T-CLIP/MiniGPT-4) -> Image Encoder -> Linear Projection Layer -> Text Encoder -> Cosine Similarity -> Classification

Critical Path:
Image -> Generative Model (text generation) -> Text Encoder (feature extraction) -> Similarity matching with category names -> Classification prediction

Design Tradeoffs:
- Uses pre-trained features vs fine-tuning entire model: The method relies heavily on pre-trained features, which limits adaptability but ensures stability
- Text generation vs direct classification: Text generation provides more stable features but adds computational overhead during inference
- Linear probe only vs full fine-tuning: Linear probe preserves pre-trained knowledge but may limit task-specific adaptation

Failure Signatures:
- Poor early task performance indicates insufficient pre-trained feature quality
- Unstable text generation leads to inconsistent predictions
- High computational cost during inference due to text generation requirements

First Experiments:
1. Linear probe baseline on CIFAR100 to verify pre-trained feature quality
2. Ablation study comparing different template formats for text generation
3. Computational overhead measurement comparing inference time to traditional methods

## Open Questions the Paper Calls Out
### Open Question 1
How can the generated text descriptions from generative models be made more concise and task-specific while maintaining high classification accuracy in class-incremental learning? The paper mentions that without fine-tuning, the generated text can be lengthy and include unnecessary details, as shown in Figure 4 where MiniGPT-4 provides verbose descriptions compared to the concise "This is a photo of [CLS]" format used in the proposed method.

### Open Question 2
Can the proposed generative approach be extended to more complex class-incremental learning scenarios beyond image classification, such as object detection or semantic segmentation? The paper focuses specifically on image classification and uses a straightforward text generation and similarity matching approach.

### Open Question 3
How does the performance of generative models for class-incremental learning compare to discriminative models when dealing with highly imbalanced class distributions within and across tasks? The paper focuses on standard CIL settings with balanced class distributions, but does not explore scenarios with significant class imbalance.

## Limitations
- Heavy reliance on pre-trained generative models limits generalizability to non-natural image domains
- Computational overhead during inference due to text generation requirements
- Limited ablation studies on template designs and text encoder configurations

## Confidence
- **High Confidence**: The core methodology of using generative models to produce textual labels and matching with text encoders is technically sound and well-implemented
- **Medium Confidence**: Claims about superior performance over state-of-the-art methods are supported by experimental results but limited to specific dataset configurations and may not generalize
- **Low Confidence**: The assertion that this approach completely eliminates catastrophic forgetting is overstated, as performance degradation still occurs, particularly in early tasks

## Next Checks
1. Cross-domain validation: Test the method on non-natural image datasets (medical images, satellite imagery) to assess generalizability beyond the natural image domain
2. Ablation study: Systematically evaluate the impact of different template formats, text encoders, and generative model choices on final performance
3. Computational overhead analysis: Measure and report inference time and memory requirements for the text generation and matching process compared to traditional discriminative approaches