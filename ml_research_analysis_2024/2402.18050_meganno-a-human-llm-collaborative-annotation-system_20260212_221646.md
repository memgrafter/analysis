---
ver: rpa2
title: 'MEGAnno+: A Human-LLM Collaborative Annotation System'
arxiv_id: '2402.18050'
source_url: https://arxiv.org/abs/2402.18050
tags:
- annotation
- data
- llms
- labels
- meganno
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEGAnno+ is a system for collaborative human-LLM annotation, enabling
  efficient data labeling for NLP tasks. It automates LLM annotation and human verification
  workflows, addressing challenges like prompt management, error handling, and metadata
  extraction.
---

# MEGAnno+: A Human-LLM Collaborative Annotation System

## Quick Facts
- arXiv ID: 2402.18050
- Source URL: https://arxiv.org/abs/2402.18050
- Reference count: 11
- Key outcome: MEGAnno+ is a system for collaborative human-LLM annotation, enabling efficient data labeling for NLP tasks. It automates LLM annotation and human verification workflows, addressing challenges like prompt management, error handling, and metadata extraction. The system supports customizable LLM agents, robust error handling, and selective human verification based on confidence scores. A use case on natural language inference demonstrated its effectiveness in streamlining annotation tasks. MEGAnno+ bridges the gap between LLM automation and human oversight, ensuring high-quality labeled data for downstream applications.

## Executive Summary
MEGAnno+ is a collaborative annotation system designed to streamline the process of labeling NLP datasets by leveraging the strengths of both LLMs and human annotators. The system automates LLM annotation workflows, including prompt management, error handling, and metadata extraction, while enabling selective human verification based on confidence scores and other metadata. By providing a user-friendly interface within Jupyter notebooks, MEGAnno+ allows users to efficiently create, manage, and verify annotations, ensuring high-quality labeled data for downstream NLP tasks.

## Method Summary
MEGAnno+ is a collaborative annotation system that automates LLM annotation workflows and enables selective human verification. It provides a Jupyter notebook client with UI widgets for creating agents (LLM model + prompt template), running annotation jobs, and verifying results. The system handles error management for LLM API calls, stores annotations and metadata in a backend database, and supports exporting verified labels. A key feature is the ability to filter and sort annotations by confidence scores, allowing users to prioritize low-confidence predictions for human review. The system was demonstrated on a natural language inference task, showcasing its effectiveness in streamlining the annotation process.

## Key Results
- Automates LLM annotation workflows with structured error handling and metadata extraction.
- Enables selective human verification guided by confidence scores and metadata.
- Supports reusable agents for consistent annotation across datasets and systematic performance comparison.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automating LLM annotation workflows reduces manual overhead while maintaining quality through structured error handling.
- Mechanism: The system wraps LLM API calls in a controller that pre-processes prompts, retries on known failures, and post-processes outputs into valid schema-compliant labels. This prevents users from handling raw API errors and format inconsistencies.
- Core assumption: LLM API failures are either transient (retryable) or require user-side intervention, and post-processing can reliably extract valid labels from free-text outputs.
- Evidence anchors:
  - [abstract] "convenient and robust LLM annotation"
  - [section 4.3.3] "errors encountered during API calls are handled in two ways: handle within our system or delegate to users"
  - [section 4.3.4] "MEGAnno+ conducts an automated post-processing step on LLM responses"
- Break condition: If LLM outputs are too unstructured or schema violations are frequent, the automated extraction fails and manual intervention becomes necessary.

### Mechanism 2
- Claim: Selective human verification guided by confidence scores and metadata improves label quality while minimizing human effort.
- Mechanism: The system stores token logits and other metadata from LLM responses, then allows users to filter and sort annotations by confidence before reviewing. Low-confidence predictions are surfaced first for human inspection.
- Core assumption: Confidence scores derived from token logits correlate with label correctness, and human reviewers can efficiently prioritize suspicious cases.
- Evidence anchors:
  - [abstract] "selective, exploratory verification of LLM labels by humans"
  - [section 4.4] "Our widget facilitates this process by presenting metadata, such as model confidence or token logit scores, in a separate column"
  - [section 5] "she sorts the annotations in an ascending order of confidence and manually verifies low confidence (< 95%) annotations"
- Break condition: If confidence scores are poorly calibrated or metadata is unavailable, the filtering mechanism loses effectiveness.

### Mechanism 3
- Claim: Reusable agents (model+prompt configurations) enable systematic comparison and consistent annotation across datasets.
- Mechanism: Agents are stored with their model configs and prompt templates, allowing users to reuse or swap agents across jobs and compare performance on the same subset.
- Core assumption: Prompt templates and model configs that work well on one subset generalize reasonably to others, and storage of these artifacts is feasible.
- Evidence anchors:
  - [abstract] "effective management of LLM agents, annotations, and artifacts"
  - [section 4.2] "Used agents are stored in our database and can be queried based on model configuration"
  - [section 5] "She creates another agent, GPT-3 with temperature with zero and re-runs annotation on the same subset"
- Break condition: If prompt effectiveness is highly dataset-specific, reusing agents leads to degraded performance.

## Foundational Learning

- Concept: Prompt engineering and template design
  - Why needed here: Effective LLM annotation depends on crafting prompts that elicit correct, schema-compliant outputs.
  - Quick check question: What happens if the prompt template does not specify output format constraints?

- Concept: Error handling strategies for external APIs
  - Why needed here: LLM annotation workflows must handle transient failures and rate limits without manual intervention.
  - Quick check question: How does the system distinguish between retryable and non-retryable API errors?

- Concept: Metadata extraction and usage
  - Why needed here: Confidence scores and other artifacts guide human verification and performance analysis.
  - Quick check question: Which metadata is available for a given LLM model, and how is it stored?

## Architecture Onboarding

- Component map:
  - Frontend: Jupyter notebook client with UI widgets
  - Backend: Web service and database storing records, labels, agents, jobs, metadata
  - LLM controller: Manages agent registration, job execution, error handling
  - Verification widget: Interactive UI for exploring and correcting LLM labels

- Critical path:
  1. User creates/selects agent (model + prompt)
  2. User selects subset to annotate
  3. LLM controller runs job (pre-process → API calls → post-process → store)
  4. User verifies low-confidence labels via widget
  5. Export verified labels for downstream use

- Design tradeoffs:
  - Storing prompt templates vs. generated prompts: Templates are smaller and reusable; prompts are data-heavy.
  - Confidence score availability: Some models (e.g., GPT-4) do not expose token logits, limiting verification prioritization.
  - Retry logic granularity: Retrying on timeouts vs. skipping problematic records affects job completion time.

- Failure signatures:
  - High invalid label rate → prompt template or schema mismatch
  - Frequent API connection errors → network or service outage
  - Empty confidence column → model does not provide logits
  - Slow job progress → large input size or rate limiting

- First 3 experiments:
  1. Run a small annotation job with a simple prompt template and inspect the generated prompts and confidence scores.
  2. Verify a subset of LLM labels using the table view, sorting by confidence, and confirm corrections are stored.
  3. Create a second agent with modified prompt/model and compare label distributions on the same subset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reliability of LLM annotations vary across different task types and domains?
- Basis in paper: [explicit] The paper mentions that LLM performance varies across different tasks, datasets, and labels, and that LLMs struggle with nuanced contexts, necessitating human involvement.
- Why unresolved: While the paper acknowledges these variations, it does not provide a systematic study comparing LLM performance across a wide range of task types and domains.
- What evidence would resolve it: A comprehensive empirical study evaluating LLM annotation performance across diverse NLP tasks (e.g., sentiment analysis, named entity recognition, summarization) and domains (e.g., medical, legal, social media).

### Open Question 2
- Question: What are the most effective strategies for handling LLM annotation errors and inconsistencies?
- Basis in paper: [explicit] The paper discusses the need for post-processing LLM responses to extract valid labels and handle errors, but does not provide a detailed analysis of error handling strategies.
- Why unresolved: The paper mentions the importance of error handling but does not explore specific techniques or compare their effectiveness in mitigating LLM annotation errors.
- What evidence would resolve it: An empirical comparison of different error handling strategies (e.g., rule-based, model-based, hybrid) and their impact on the quality and consistency of LLM annotations.

### Open Question 3
- Question: How can the human verification process be optimized to minimize time and effort while ensuring annotation quality?
- Basis in paper: [explicit] The paper introduces a selective verification approach using confidence scores and metadata, but does not explore optimization techniques for the human verification process.
- Why unresolved: While the paper presents a verification framework, it does not investigate methods to further streamline the process or determine the optimal balance between verification effort and annotation quality.
- What evidence would resolve it: An empirical study comparing different verification strategies (e.g., threshold-based, active learning, uncertainty sampling) and their impact on verification time, effort, and annotation quality.

## Limitations

- System performance and robustness in diverse, real-world annotation scenarios are not well-evaluated.
- Dependency on specific LLM APIs introduces potential brittleness if model behaviors or access patterns change.
- Lack of quantitative evaluation metrics (e.g., label accuracy, human effort reduction) limits effectiveness assessment.

## Confidence

- High: Automated error handling and metadata extraction for supported LLM models.
- Medium: Selective human verification workflow and agent management.
- Low: System performance and robustness in diverse, real-world annotation scenarios.

## Next Checks

1. **Prompt Template Robustness**: Test the same prompt template across multiple datasets (e.g., NLI, sentiment analysis) and measure label consistency and schema compliance rates.
2. **Confidence Score Calibration**: Compare confidence scores against actual label accuracy for different LLM models and datasets to assess prioritization effectiveness.
3. **Scalability Testing**: Run annotation jobs on progressively larger datasets (e.g., 1K, 10K, 100K records) and measure job completion time, error rates, and memory usage.