---
ver: rpa2
title: 'DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented Generation
  for Question-Answering'
arxiv_id: '2406.07348'
source_url: https://arxiv.org/abs/2406.07348
tags:
- documents
- query
- document
- arxiv
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DR-RAG addresses the problem of retrieving dynamic-relevant documents
  in multi-hop QA tasks where static-relevant documents are easily retrieved but dynamic-relevant
  documents are often missed due to low relevance with the query. The core method
  uses a two-stage retrieval framework that first retrieves static-relevant documents,
  then concatenates them with the query to retrieve dynamic-relevant documents, followed
  by a classifier-based selection strategy (forward or inverse selection) to optimize
  document relevance and minimize redundancy.
---

# DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented Generation for Question-Answering

## Quick Facts
- arXiv ID: 2406.07348
- Source URL: https://arxiv.org/abs/2406.07348
- Reference count: 40
- Improves answer accuracy by 6.17-9.36% and reduces retrieval time by 74.2% on multi-hop QA datasets

## Executive Summary
DR-RAG addresses the challenge of retrieving dynamic-relevant documents in multi-hop question answering tasks where static-relevant documents are easily retrieved but critical dynamic-relevant documents are often missed due to low query relevance. The method employs a two-stage retrieval framework that first retrieves static-relevant documents, then concatenates them with the query to uncover dynamic relevance, followed by a classifier-based selection strategy to optimize document relevance while minimizing redundancy. Experiments on HotpotQA, 2Wiki, and MuSiQue datasets demonstrate significant improvements in answer accuracy (6.17%, 7.34%, and 9.36% respectively) while reducing retrieval time by 74.2% compared to adaptive RAG frameworks, achieving 98% recall rate on 2Wiki with only 6 documents retrieved.

## Method Summary
DR-RAG uses a two-stage retrieval framework for multi-hop QA. First, it retrieves k1 static-relevant documents using similarity matching. Then, each static document is concatenated with the query to form new context for retrieving k2 dynamic-relevant documents. A binary classifier evaluates document pairs to determine relevance contribution, applying either forward selection (including only positive documents) or inverse selection (excluding negative documents). The final document set is fed to an LLM in a single inference call. This approach specifically targets the problem where dynamic-relevant documents have low relevance to the query but are semantically linked to static-relevant documents through shared entities or relationships.

## Key Results
- Improves answer accuracy by 6.17%, 7.34%, and 9.36% on HotpotQA, 2Wiki, and MuSiQue datasets respectively
- Achieves 98% recall rate on 2Wiki dataset with only 6 documents retrieved
- Reduces retrieval time by 74.2% compared to adaptive RAG frameworks
- Maintains single LLM call while improving accuracy and efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage retrieval with query-document concatenation uncovers dynamic relevance that single-stage retrieval misses.
- Mechanism: Static-relevant documents are first retrieved via similarity matching. These are then concatenated with the query to form new context that enables retrieval of dynamic-relevant documents which are low relevance to the query alone but semantically linked through shared entities or relationships.
- Core assumption: Static-relevant documents contain bridging information that, when combined with the query, increases the semantic overlap with dynamic-relevant documents.
- Evidence anchors:
  - [abstract] "We have found that even though there is low relevance between some critical documents and query, it is possible to retrieve the remaining documents by combining parts of the documents with the query."
  - [section 2.2.1] "When q and d_stat are concatenated, the query contains both the 'Johan Ludvig Heiberg' and the relationship 'spouse/wife', which is essentially similar to d_dyn."
  - [corpus] Found 25 related papers. Average neighbor FMR=0.545. Limited direct evidence on query-document concatenation improving dynamic relevance.
- Break condition: If static-relevant documents fail to contain bridging information, concatenation yields no improvement in dynamic relevance retrieval.

### Mechanism 2
- Claim: Classifier-based selection (forward and inverse) reduces redundancy and improves precision by filtering documents that do not contribute to answering the query.
- Mechanism: A binary classifier evaluates whether document pairs (or document-query pairs) contribute to answering the query. In forward selection, only documents classified as positive are included. In inverse selection, documents classified as negative are excluded from the final context.
- Core assumption: The classifier can accurately distinguish between documents that are useful versus redundant for answering the query.
- Evidence anchors:
  - [section 2.2.2] "We design a classifier that determines whether the retrieved documents contribute to the current query by a predefined threshold."
  - [section 4.2] "CIS method is devised to validate this hypothesis... CIS method is effective in removing redundant information."
  - [corpus] Found 25 related papers. Average neighbor FMR=0.545. Limited direct evidence on classifier effectiveness in RAG redundancy reduction.
- Break condition: If classifier accuracy is low, forward selection may exclude needed documents and inverse selection may retain irrelevant ones.

### Mechanism 3
- Claim: Single LLM call with optimized context improves efficiency while maintaining accuracy compared to multi-step approaches.
- Mechanism: By retrieving all necessary documents through the two-stage process and classifier filtering before calling the LLM, DR-RAG avoids multiple inference passes, reducing computational cost.
- Core assumption: The retrieved and filtered document set is sufficient for the LLM to generate accurate answers in a single pass.
- Evidence anchors:
  - [abstract] "Meanwhile, DR-RAG call the LLMs only once, which significantly improves the efficiency of the experiment."
  - [section 4.1] "DR-RAG is also less than other RAG frameworks in terms of the number of LLMs responses and the time consumed in QA systems."
  - [corpus] Found 25 related papers. Average neighbor FMR=0.545. Some evidence in related works on single-pass vs multi-pass efficiency.
- Break condition: If the final document set is incomplete or the LLM requires iterative refinement, single-call approach may degrade answer quality.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) framework
  - Why needed here: DR-RAG builds upon and modifies the standard RAG pipeline by introducing two-stage retrieval and classifier-based selection.
  - Quick check question: What are the main components of a standard RAG system and how does it typically retrieve documents?

- Concept: Multi-hop Question Answering (QA)
  - Why needed here: The datasets used (HotpotQA, 2Wiki, MuSiQue) require reasoning across multiple documents, which is the target use case for DR-RAG.
  - Quick check question: What distinguishes multi-hop QA from single-hop QA in terms of document retrieval and reasoning requirements?

- Concept: Static vs Dynamic document relevance
  - Why needed here: DR-RAG specifically targets the problem of retrieving dynamic-relevant documents that are low relevance to the query but necessary for the answer.
  - Quick check question: How do static-relevant and dynamic-relevant documents differ in terms of their relationship to the query and their role in answering multi-hop questions?

## Architecture Onboarding

- Component map:
  - Retriever (similarity matching or BM25) -> Static-relevant document pool -> Query-document concatenation module -> Dynamic-relevant document retriever -> Binary relevance classifier (forward/inverse selection) -> LLM inference engine -> Evaluation metrics (EM, F1, Acc, recall rate)

- Critical path:
  1. Retrieve k1 static-relevant documents via similarity matching
  2. Concatenate each with query and retrieve k2 dynamic-relevant documents
  3. Apply classifier-based selection (forward or inverse)
  4. Feed final document set to LLM for single inference
  5. Evaluate answer quality and retrieval efficiency

- Design tradeoffs:
  - Two-stage retrieval increases recall but adds computational overhead in the first stage
  - Classifier selection reduces redundancy but may exclude needed documents if accuracy is low
  - Single LLM call improves efficiency but assumes completeness of retrieved context
  - Parameter choices (k1, k2, classifier threshold) affect balance between recall and precision

- Failure signatures:
  - Low recall despite high k values indicates static documents lack bridging information
  - Decreased accuracy after classifier filtering suggests poor classifier performance
  - High time consumption despite single LLM call may indicate inefficient retriever implementation
  - Answer quality degradation suggests missing critical documents in final set

- First 3 experiments:
  1. Implement baseline RAG with single-stage retrieval (BM25 or SM) and measure recall and accuracy on MuSiQue
  2. Add query-document concatenation stage and compare recall improvement on 2Wiki dataset
  3. Implement classifier forward selection and measure impact on redundancy reduction and answer quality on HotpotQA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold for the classifier to determine document relevance without excessive false positives or negatives?
- Basis in paper: [explicit] The paper mentions a classifier is used with "a predefined threshold" but doesn't specify what this threshold should be or how to optimize it.
- Why unresolved: The authors don't provide experimental results showing how different threshold values affect performance, nor do they discuss methods for threshold optimization.
- What evidence would resolve it: Systematic experiments varying the classification threshold and measuring its impact on recall, precision, and final answer accuracy across different datasets.

### Open Question 2
- Question: How does DR-RAG's performance scale when applied to knowledge bases with significantly different characteristics (size, document length, domain specificity)?
- Basis in paper: [inferred] The paper only evaluates on three multi-hop QA datasets without discussing how the method might perform on knowledge bases of different scales or domains.
- Why unresolved: The authors don't provide analysis of performance across varying knowledge base sizes or document characteristics, nor do they discuss potential limitations when scaling to larger or more diverse knowledge sources.
- What evidence would resolve it: Experiments testing DR-RAG on knowledge bases with different sizes (orders of magnitude larger/smaller), document lengths (varying by domain), and levels of domain specificity.

### Open Question 3
- Question: What is the computational overhead of training the classifier for different types of knowledge bases and how does this compare to the time savings from reduced LLM calls?
- Basis in paper: [explicit] The paper mentions a "small binary-classification model" is trained but doesn't provide details on training time, computational requirements, or a cost-benefit analysis.
- Why unresolved: No information is provided about classifier training time, computational resources needed, or whether the upfront training cost is justified by the efficiency gains during inference.
- What evidence would resolve it: Detailed measurements of classifier training time and computational requirements across different knowledge bases, compared to the time saved during inference by avoiding multiple LLM calls.

## Limitations

- Classifier selection strategy lacks detailed implementation specifications, making it difficult to assess whether reported improvements stem from the specific algorithm design or general classifier-based filtering approaches
- No ablation studies isolating the contribution of each component (two-stage retrieval, query-document concatenation, classifier selection) to overall performance gains
- Limited evaluation on only three multi-hop QA datasets without testing on knowledge bases of different scales or domains

## Confidence

**High Confidence**: The fundamental problem identification - that static-relevant documents are easily retrieved while dynamic-relevant documents are missed in multi-hop QA - is well-supported by the literature on retrieval-augmented generation. The experimental results showing accuracy improvements (6.17-9.36%) and recall improvements (98% on 2Wiki) are internally consistent.

**Medium Confidence**: The mechanism explaining how query-document concatenation uncovers dynamic relevance is logically sound but lacks extensive empirical validation. The efficiency claims (74.2% reduction in retrieval time) are plausible given the single LLM call architecture but would benefit from detailed timing breakdowns.

**Low Confidence**: The classifier selection strategy's effectiveness is claimed but not thoroughly validated. Without details on classifier architecture, training methodology, or performance metrics, it's difficult to assess whether this component genuinely contributes to the reported improvements or could introduce new failure modes.

## Next Checks

1. Implement ablation studies to isolate the contribution of each component (two-stage retrieval, query-document concatenation, classifier selection) to the overall performance gains, measuring both accuracy and efficiency metrics independently.

2. Conduct error analysis on classifier selection failures by examining cases where relevant documents were incorrectly filtered out, and measure the impact on answer quality when including these documents in the final context.

3. Test the approach on additional multi-hop QA datasets beyond the three reported (HotpotQA, 2Wiki, MuSiQue) to assess generalizability, particularly on datasets with different document distributions and query patterns.