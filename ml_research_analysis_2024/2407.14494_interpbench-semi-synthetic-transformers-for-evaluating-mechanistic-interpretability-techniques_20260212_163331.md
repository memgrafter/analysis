---
ver: rpa2
title: 'InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability
  Techniques'
arxiv_id: '2407.14494'
source_url: https://arxiv.org/abs/2407.14494
tags:
- siit
- circuit
- nodes
- link
- tracr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InterpBench, a benchmark of 86 semi-synthetic
  transformers with known circuits for evaluating mechanistic interpretability techniques.
  The key method is Strict Interchange Intervention Training (SIIT), which extends
  IIT by also training nodes not in the high-level graph, ensuring the low-level model
  correctly implements the circuit.
---

# InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques

## Quick Facts
- arXiv ID: 2407.14494
- Source URL: https://arxiv.org/abs/2407.14494
- Reference count: 40
- One-line primary result: ACDC outperforms Subnetwork Probing and edgewise SP on InterpBench with statistical significance

## Executive Summary
This paper introduces InterpBench, a benchmark of 86 semi-synthetic transformers with known circuits for evaluating mechanistic interpretability techniques. The key innovation is Strict Interchange Intervention Training (SIIT), which extends IIT by also training nodes not in the high-level graph, ensuring the low-level model correctly implements the circuit. The primary result is that SIIT-generated transformers maintain the original circuit while being more realistic than Tracr-generated ones, as shown by node effect and KL divergence analysis. On InterpBench, ACDC outperforms Subnetwork Probing and edgewise SP in circuit discovery, with statistical significance, while EAP with integrated gradients is competitive with ACDC.

## Method Summary
The paper introduces InterpBench, a benchmark of 86 semi-synthetic transformers with known circuits for evaluating mechanistic interpretability techniques. The main method, Strict Interchange Intervention Training (SIIT), is an extension of IIT that also trains nodes not in the high-level graph, ensuring the low-level model correctly implements the circuit. SIIT uses interchange interventions on both aligned and non-aligned nodes during training, with a strictness loss that penalizes the model when intervening on non-aligned nodes changes the output. This forces non-aligned nodes to be constant and not contribute to task computation. The resulting transformers maintain the original circuit while being more realistic than Tracr-generated ones, as shown by node effect and KL divergence analysis.

## Key Results
- SIIT-generated transformers maintain the original circuit while being more realistic than Tracr-generated ones, as shown by node effect and KL divergence analysis
- On InterpBench, ACDC outperforms Subnetwork Probing and edgewise SP in circuit discovery, with statistical significance
- EAP with integrated gradients is competitive with ACDC on the benchmark

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SIIT ensures that only aligned nodes are responsible for the output computation by intervening on non-aligned nodes during training.
- **Mechanism**: SIIT extends IIT by sampling non-aligned low-level variables and applying a strictness loss that penalizes the model when intervening on these variables changes the output. This forces non-aligned nodes to be constant and not contribute to task computation.
- **Core assumption**: Non-aligned nodes can be effectively constrained to be constant through interchange interventions without disrupting the high-level circuit implementation.
- **Evidence anchors**:
  - [abstract] "SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model's output."
  - [section 3] "SIIT improves upon IIT by also intervening on subcomponents of the low-level model that are not mapped to any high-level node."
  - [corpus] Weak - corpus neighbors discuss causal intervention frameworks but don't specifically address the strictness loss mechanism.
- **Break condition**: If non-aligned nodes cannot be constrained to be constant (e.g., due to architectural limitations or optimization issues), the strictness loss would fail to enforce the desired behavior.

### Mechanism 2
- **Claim**: SIIT-generated transformers are more realistic than Tracr models because they retain non-zero weights and superposition while implementing known circuits.
- **Mechanism**: By training with SIIT, transformers maintain a realistic weight distribution similar to naturally trained models (as shown by KL divergence analysis), unlike Tracr's sparse, zeroed-out weights.
- **Core assumption**: The weight distribution of SIIT models closely approximates that of naturally trained models while preserving the ground-truth circuit.
- **Evidence anchors**:
  - [section 4] "Using IIT's terminology, the Tracr-generated transformers are the high-level models, the SIIT-generated transformers are the low-level ones..."
  - [section 5] "Another proxy for realism is: do the weights of 'natural' and SIIT models follow similar distributions? Figure 2 shows a histogram of the weights..."
  - [corpus] Weak - corpus neighbors discuss interpretability frameworks but don't compare weight distributions between synthetic and natural models.
- **Break condition**: If SIIT training fails to produce realistic weight distributions or introduces artifacts that make models behave unnaturally, the benchmark would lose its utility for evaluating interpretability techniques.

### Mechanism 3
- **Claim**: INTERPBENCH enables statistically significant evaluation of circuit discovery techniques by providing ground-truth circuits in realistic models.
- **Mechanism**: By having known circuits implemented in semi-synthetic transformers, INTERPBENCH allows rigorous testing of methods like ACDC, Subnetwork Probing, and Edge Attribution Patching, revealing performance differences that weren't apparent with manually discovered circuits.
- **Core assumption**: Ground-truth circuits in SIIT models are sufficiently faithful representations of the intended algorithms for meaningful evaluation.
- **Evidence anchors**:
  - [abstract] "INTERPBENCH is a collection of 86 semi-synthetic yet realistic transformers with known circuits for evaluating these techniques."
  - [section 5] "INTERPBENCH can be used to evaluate mechanistic interpretability techniques, and has yielded unexpected results: ACDC is significantly better than SP and edgewise SP..."
  - [corpus] Weak - corpus neighbors discuss evaluation frameworks but don't specifically address benchmarks with ground-truth circuits.
- **Break condition**: If the ground-truth circuits in SIIT models don't accurately represent the intended algorithms or if the models are too unrealistic, the benchmark would fail to provide meaningful evaluation.

## Foundational Learning

- **Concept**: Interchange Intervention Training (IIT)
  - Why needed here: Understanding IIT is essential because SIIT is an extension of this method, and the paper's contribution relies on understanding how IIT works and its limitations.
  - Quick check question: What is the key limitation of IIT that SIIT addresses, according to the paper?

- **Concept**: Causal consistency in mechanistic interpretability
  - Why needed here: The paper's evaluation of SIIT models relies on measuring causal consistency through node effects and KL divergence, which are core concepts in MI.
  - Quick check question: How does the paper measure whether SIIT models correctly implement the desired circuits?

- **Concept**: Ground-truth circuit evaluation
  - Why needed here: INTERPBENCH's value proposition is that it provides models with known ground-truth circuits, enabling rigorous evaluation of MI techniques.
  - Quick check question: What advantage does INTERPBENCH have over previous evaluation methods that relied on manually discovered circuits?

## Architecture Onboarding

- **Component map**: High-level model (Tracr-generated transformer) -> Alignment map -> Low-level model (SIIT-trained transformer) -> Evaluation metrics
- **Critical path**:
  1. Define high-level circuit (Tracr program)
  2. Map high-level nodes to low-level components
  3. Train low-level model with SIIT until 100% IIA and SIIA
  4. Validate that non-aligned nodes have no effect on output
  5. Use models to evaluate MI techniques
- **Design tradeoffs**:
  - Strictness loss vs. model performance - higher strictness may hurt accuracy
  - Realism vs. ground-truth fidelity - more realistic models may have more complex, less clean circuits
  - Granularity of alignment - training at attention head level vs. QKV subcomponents
- **Failure signatures**:
  - Low SIIA despite high IIA indicates non-aligned nodes are affecting output
  - High node effects for non-circuit nodes suggests SIIT failed to constrain them
  - Poor performance on MI technique evaluation suggests models are unrealistic or circuits are not faithful
- **First 3 experiments**:
  1. Train a simple Tracr circuit with SIIT and verify 100% IIA and SIIA
  2. Compare weight distributions of SIIT model vs. naturally trained model
  3. Run ACDC on SIIT model and measure AUROC against ground-truth circuit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the realism of SIIT-trained transformers compare to naturally trained transformers when scaled to larger architectures with more complex tasks?
- Basis in paper: [inferred] The paper notes that INTERPBENCH models are "very small and have very little functionality" and that "results on INTERPBENCH may not accurately represent the results of the larger models."
- Why unresolved: The current benchmark only includes small transformers with single algorithmic circuits, limiting generalizability to larger models.
- What evidence would resolve it: Training and evaluating SIIT transformers with larger architectures (more layers, heads, and features) on multi-task benchmarks would demonstrate scalability of realism.

### Open Question 2
- Question: Does SIIT training produce more interpretable circuits than IIT when the high-level model is unknown and must be discovered from the low-level model?
- Basis in paper: [explicit] The paper states that SIIT "improves upon IIT by also intervening on subcomponents of the low-level model that are not mapped to any high-level node."
- Why unresolved: The current evaluation assumes the high-level circuit is known and uses it to train SIIT, not testing discovery of unknown circuits.
- What evidence would resolve it: Applying SIIT to train transformers on tasks without predefined circuits, then using interpretability methods to discover and validate the resulting circuits.

### Open Question 3
- Question: What is the relationship between zero ablation effects and the presence of non-linear feature interactions in SIIT-trained models?
- Basis in paper: [explicit] The paper notes that "zero ablations indicate that there are nodes not in the circuit with significant effects" and suggests this "may indicate that INTERPBENCH circuits are not entirely true."
- Why unresolved: The paper observes the phenomenon but does not investigate the underlying mechanism.
- What evidence would resolve it: Analyzing the distribution of zero ablation effects across different circuit types and comparing with models trained to minimize non-linear interactions.

## Limitations
- The evaluation relies on node effect and KL divergence metrics which may not capture all aspects of circuit fidelity or model realism
- The benchmark focuses on relatively simple circuits, limiting generalizability to complex, real-world scenarios
- The comparison between SIIT and Tracr models doesn't establish a clear threshold for what constitutes "realistic enough" for meaningful interpretability evaluation

## Confidence
- **High confidence**: SIIT successfully constrains non-aligned nodes to have minimal effect on output (supported by SIIA measurements and node effect analysis)
- **Medium confidence**: SIIT-generated models are meaningfully more realistic than Tracr models (supported by weight distribution analysis but limited by scope of comparison)
- **Medium confidence**: ACDC outperforms Subnetwork Probing and edgewise SP on InterpBench (statistically significant results but benchmark scope may limit generalizability)

## Next Checks
1. **Circuit fidelity stress test**: Apply SIIT to progressively more complex circuits (beyond the current 85 Tracr circuits) and measure whether IIA and SIIA remain at 100% while maintaining realistic weight distributions. This would validate whether SIIT scales to more sophisticated algorithmic tasks.

2. **Spurious correlation detection**: Design adversarial circuit variants where the ground-truth circuit contains specific features, then check whether SIIT models learn only the intended circuit or also develop shortcut solutions. This would reveal whether SIIT truly enforces faithful circuit implementation.

3. **Cross-technique validation**: Run the same circuit discovery techniques (ACDC, Subnetwork Probing, Edge Attribution Patching) on naturally trained transformers solving the same tasks as the SIIT models, then compare performance. This would establish whether InterpBench's advantage is due to ground-truth availability or model differences.