---
ver: rpa2
title: Spatial Reasoning and Planning for Deep Embodied Agents
arxiv_id: '2409.19479'
source_url: https://arxiv.org/abs/2409.19479
tags:
- learning
- policy
- agent
- training
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis addresses the challenge of solving spatial reasoning
  and planning tasks in a data-driven manner, aiming to enhance learning efficiency,
  interpretability, and transferability across novel scenarios. Four key contributions
  are presented: (1) CALVIN, a differentiable planner that learns interpretable models
  of the world for long-term planning, successfully navigating partially observable
  3D environments by learning rewards and state transitions from expert demonstrations;
  (2) SOAP, a reinforcement learning algorithm that discovers options unsupervised
  for long-horizon tasks, showing robust performances on history-conditional corridor
  tasks and classical benchmarks; (3) LangProp, a code optimisation framework using
  LLMs to solve embodied agent problems by treating code as learnable policies, generating
  interpretable code with comparable or superior performance to human-written experts
  in the CARLA autonomous driving benchmark; (4) Voggite, an embodied agent with a
  vision-to-action transformer backend that solves complex tasks in Minecraft, achieving
  third place in the MineRL BASALT Competition by identifying action triggers to segment
  tasks into multiple stages.'
---

# Spatial Reasoning and Planning for Deep Embodied Agents

## Quick Facts
- arXiv ID: 2409.19479
- Source URL: https://arxiv.org/abs/2409.19479
- Authors: Shu Ishida
- Reference count: 0
- Primary result: This thesis presents four key contributions to spatial reasoning and planning for deep embodied agents: CALVIN for differentiable planning, SOAP for unsupervised option discovery, LangProp for LLM-based code optimization, and Voggite for Minecraft task execution.

## Executive Summary
This thesis addresses the challenge of solving spatial reasoning and planning tasks in a data-driven manner, aiming to enhance learning efficiency, interpretability, and transferability across novel scenarios. The work presents four key contributions: CALVIN, a differentiable planner that learns interpretable models of the world for long-term planning; SOAP, a reinforcement learning algorithm that discovers options unsupervised for long-horizon tasks; LangProp, a code optimization framework using LLMs to solve embodied agent problems; and Voggite, an embodied agent with a vision-to-action transformer backend that solves complex tasks in Minecraft. Each contribution tackles specific challenges in spatial reasoning and planning, from navigation in partially observable environments to autonomous driving and task execution in complex game worlds.

## Method Summary
The thesis presents four distinct approaches to spatial reasoning and planning for deep embodied agents. CALVIN introduces a differentiable planner with structural constraints on value iteration to explicitly model impossible actions and noisy motion, enabling long-term planning in partially observable 3D environments. SOAP extends the concept of Generalised Advantage Estimate (GAE) to propagate option advantages through time, facilitating unsupervised option discovery for long-horizon tasks. LangProp treats code as learnable policies and uses LLMs as optimizers, enabling iterative code improvement in a data-driven way. Voggite employs a vision-to-action transformer backend to solve complex tasks in Minecraft by identifying action triggers to segment tasks into multiple stages. These methods collectively address challenges in learning efficiency, interpretability, and transferability across novel scenarios.

## Key Results
- CALVIN successfully navigates partially observable 3D environments by learning rewards and state transitions from expert demonstrations
- SOAP shows robust performances on history-conditional corridor tasks and classical benchmarks through unsupervised option discovery
- LangProp generates interpretable code with comparable or superior performance to human-written experts in the CARLA autonomous driving benchmark
- Voggite achieves third place in the MineRL BASALT Competition by solving complex tasks in Minecraft

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CALVIN learns accurate models of the world by imposing structural constraints on value iteration, explicitly modeling illegal actions and task termination.
- Mechanism: CALVIN decomposes the transition model P(s'|s,a) into two components: a translation-invariant motion model bP(s'-s|a) and an observation-dependent action availability predictor bA(s,a). This ensures that Q-values for illegal actions (e.g., moving into obstacles) are properly penalized by setting bA(s,a) = 0 for those actions.
- Core assumption: The environment has local and translation-invariant motion dynamics, and obstacles can be identified from observations.
- Evidence anchors:
  - [abstract]: "CALVIN thus imposes a structural constraint on the value iteration, which explicitly learns to model impossible actions and noisy motion."
  - [section 3.2.4]: "Since the VIN allows all actions at all states (A does not depend on s), collisions must be modelled as states with low rewards... In practice, the VIN does not learn to forbid such actions completely, resulting in the propagation of values from states which cannot be reached directly due to collision along the way."
  - [corpus]: Weak - corpus evidence focuses on spatial reasoning in general, not specifically on VIN limitations.
- Break condition: If the environment has non-local or non-translation-invariant dynamics, or if obstacles cannot be reliably identified from observations.

### Mechanism 2
- Claim: LangProp treats code as learnable policies and uses LLMs as optimizers, enabling iterative code improvement in a data-driven way.
- Mechanism: LangProp defines a LangProp model with setup prompts, update prompts, and executable code (policies). During training, the LLM generates and updates policy scripts, which are evaluated against a training objective. The policies are ranked by performance, and the best-performing ones are selected for updates and inference.
- Core assumption: Code can be represented as parameters of a model, and LLMs can generate and update code based on feedback.
- Evidence anchors:
  - [abstract]: "LangProp, a framework for iteratively optimising code generated by Large Language Models... By adopting a metric- and data-driven training paradigm for this code optimisation procedure, one could easily adapt findings from traditional machine learning techniques such as Behavioural Cloning, DAgger, and RL."
  - [section 5.3.1]: "The LangProp model consists of a setup prompt, an update prompt, and a collection of executable code generated by the LLM, which this work will refer to as policies."
  - [corpus]: Weak - corpus evidence mentions multi-modal LLMs for embodied agents but not specifically code optimization.
- Break condition: If the code cannot be effectively represented as a parameter of the model, or if LLMs cannot generate meaningful updates based on feedback.

### Mechanism 3
- Claim: SOAP learns temporally consistent options by analytically evaluating the policy gradient for an optimal option assignment, avoiding the need for backward pass information.
- Mechanism: SOAP extends the concept of Generalised Advantage Estimate (GAE) to propagate option advantages through time. It analytically evaluates the policy gradient of the option policy to derive an option advantage function, which facilitates temporal propagation of the policy gradients without requiring the backward pass of the EM algorithm.
- Core assumption: The option policy only needs to be conditional on the history of the agent, not on future trajectories.
- Evidence anchors:
  - [abstract]: "SOAP evaluates the policy gradient for an optimal option assignment. It extends the concept of the Generalised Advantage Estimate (GAE) to propagate option advantages through time, which is an analytical equivalent to performing temporal back-propagation of option policy gradients."
  - [section 4.5.1]: "The maximisation objective J[π] for the agent can be defined as: J[πΘ] = Eτ∼π[R(τ)] = ∫τ R(τ)p(τ|Θ)dτ. Taking the gradient of the maximisation objective..."
  - [corpus]: Weak - corpus evidence focuses on multi-modal LLMs and long context reasoning, not specifically on option learning.
- Break condition: If the option policy requires knowledge of future trajectories to make optimal decisions, or if the environment is not a POMDP.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: MDP provides the mathematical framework for sequential decision-making under uncertainty, which is fundamental to reinforcement learning and planning.
  - Quick check question: What are the key components of an MDP and how do they relate to the agent's decision-making process?

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: POMDP extends MDP to handle environments where the agent only has partial information about the underlying state, which is common in real-world scenarios.
  - Quick check question: How does a POMDP differ from an MDP, and why is it important for modeling real-world environments?

- Concept: Value Iteration and Value Iteration Networks (VIN)
  - Why needed here: Value iteration is a classical planning algorithm, and VIN extends it to learnable neural networks, providing a foundation for CALVIN's approach to differentiable planning.
  - Quick check question: What are the key differences between value iteration and VIN, and how does CALVIN improve upon VIN?

## Architecture Onboarding

- Component map:
  - CALVIN: VIN backbone, LPN backbone, motion model, action availability predictor, reward predictor, value iteration step
  - LangProp: LangProp model (setup prompt, update prompt, policies), LLM optimizer, LangProp trainer, policy tracker
  - SOAP: Option policy, option advantage function, policy gradient weighting, PPO objective, value regression

- Critical path:
  - CALVIN: Observations → LPN → Map embeddings → VIN/GPPN → Action selection
  - LangProp: Setup prompt → LLM generation → Policy execution → Evaluation → Update prompt → LLM update
  - SOAP: Observations → Option policy → Action selection → Reward → Option advantage propagation → Policy update

- Design tradeoffs:
  - CALVIN: Tradeoff between model complexity and interpretability. More complex models may learn better but be less interpretable.
  - LangProp: Tradeoff between code optimization and execution safety. More aggressive optimization may lead to code that is less safe or harder to debug.
  - SOAP: Tradeoff between option expressiveness and learning complexity. More expressive options may be harder to learn but more effective.

- Failure signatures:
  - CALVIN: Poor navigation performance, incorrect reward/value maps, failure to handle obstacles
  - LangProp: Code that doesn't compile, code that produces incorrect outputs, code that is too slow or inefficient
  - SOAP: Options that don't segment tasks effectively, options that don't lead to good performance, failure to learn in POMDP environments

- First 3 experiments:
  1. CALVIN: Test on simple 2D grid environments with obstacles and targets to verify basic navigation and obstacle avoidance
  2. LangProp: Test on simple code generation tasks like Sudoku or CartPole to verify basic code optimization capabilities
  3. SOAP: Test on simple POMDP environments like corridors to verify option learning and temporal consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can CALVIN's navigation capabilities be extended to dynamic environments with moving obstacles and changing layouts?
- Basis in paper: [inferred] The paper mentions that CALVIN's current formulation assumes a static environment and that extending it to dynamic environments would require a learnable dynamic mapping system and modelling of temporally and policy-dependent dynamics.
- Why unresolved: The paper does not explore this extension and the challenges of adapting VIN-like architectures to dynamic environments are not addressed.
- What evidence would resolve it: Experiments demonstrating CALVIN's performance in dynamic environments with moving obstacles and changing layouts, compared to baseline methods.

### Open Question 2
- Question: Can SOAP be extended to work with continuous or multi-discrete latent variables as options, instead of the current discrete formulation?
- Basis in paper: [inferred] The paper mentions that the current SOAP formulation uses discrete options, which may be less expressive compared to latent variables in recurrent policies and transformers, and that further research is needed to extend the derivations to work with continuous or multi-discrete variables.
- Why unresolved: The paper does not explore this extension and the challenges of adapting SOAP to continuous or multi-discrete latents are not addressed.
- What evidence would resolve it: Experiments demonstrating SOAP's performance with continuous or multi-discrete latent variables as options, compared to the discrete formulation and other baselines.

### Open Question 3
- Question: How can LangProp be adapted to support chaining of multiple modules with a full backpropagation algorithm, instead of the current single-layer feedback operation?
- Basis in paper: [inferred] The paper mentions that the current implementation is limited to a single module and that supporting multiple chained modules with a full backpropagation algorithm is a foreseeable and natural extension to the framework.
- Why unresolved: The paper does not explore this extension and the challenges of implementing a full backpropagation algorithm for chained modules are not addressed.
- What evidence would resolve it: Implementation of a LangProp framework that supports chaining of multiple modules with a full backpropagation algorithm, and experiments demonstrating its performance compared to the current single-layer feedback operation.

## Limitations

- CALVIN's performance may degrade in environments with non-local or non-translation-invariant dynamics
- LangProp's reliance on LLM-generated code raises concerns about safety and reliability in safety-critical applications
- SOAP's temporal consistency assumption may break down in highly stochastic or partially observable environments

## Confidence

- **High**: The CALVIN mechanism for learning interpretable world models through structural constraints is well-supported by theoretical analysis and experimental results on MiniWorld and AVD benchmarks.
- **Medium**: SOAP's approach to unsupervised option discovery shows promise but relies heavily on the assumption that option policies only need history conditioning, which may not hold in all POMDP scenarios.
- **Medium**: LangProp's code optimization framework demonstrates effectiveness on simple tasks but the scalability to complex embodied agent problems remains uncertain.
- **Low**: Voggite's Minecraft agent performance is based on a single competition result without detailed ablation studies or comparison to other state-of-the-art approaches.

## Next Checks

1. Test CALVIN on environments with non-translation-invariant dynamics and complex obstacle configurations to verify the robustness of the motion model and action availability predictor.
2. Conduct a thorough evaluation of LangProp's generated code on a diverse set of embodied agent tasks, including stress tests for edge cases and safety-critical scenarios.
3. Evaluate SOAP's option learning in environments with varying levels of partial observability and stochasticity to assess the limits of its temporal consistency assumption.