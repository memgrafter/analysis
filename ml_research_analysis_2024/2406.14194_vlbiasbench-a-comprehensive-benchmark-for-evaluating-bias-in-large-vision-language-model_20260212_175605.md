---
ver: rpa2
title: 'VLBiasBench: A Comprehensive Benchmark for Evaluating Bias in Large Vision-Language
  Model'
arxiv_id: '2406.14194'
source_url: https://arxiv.org/abs/2406.14194
tags:
- bias
- image
- dataset
- evaluation
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLBiasBench introduces a comprehensive benchmark for evaluating
  biases in Large Vision-Language Models (LVLMs) using synthetic data. The dataset
  covers 11 bias categories and includes both open-ended and close-ended question
  types.
---

# VLBiasBench: A Comprehensive Benchmark for Evaluating Bias in Large Vision-Language Model

## Quick Facts
- arXiv ID: 2406.14194
- Source URL: https://arxiv.org/abs/2406.14194
- Authors: Sibo Wang; Xiangkui Cao; Jie Zhang; Zheng Yuan; Shiguang Shan; Xilin Chen; Wen Gao
- Reference count: 40
- Primary result: Open-source LVLMs show higher bias across 11 categories compared to closed-source models like GPT-4o and Gemini

## Executive Summary
VLBiasBench introduces a comprehensive benchmark for evaluating biases in Large Vision-Language Models (LVLMs) using synthetic data. The dataset covers 11 bias categories and includes both open-ended and close-ended question types. The benchmark was evaluated on 15 open-source and two closed-source models. Results show that open-source models like Shikra-7b exhibit higher bias across multiple categories, while closed-source models like GPT-4o and Gemini demonstrate lower bias levels. The study reveals that LVLMs still struggle with bias, particularly when dealing with intersectional categories and ambiguous contexts.

## Method Summary
VLBiasBench uses Stable Diffusion XL to generate 46,848 synthetic images aligned to 11 bias categories via CLIP similarity filtering (>0.7). The dataset includes 82,494 questions divided into open-ended and close-ended types, further partitioned into Base/Scene/Text/Scene Text subsets to isolate modality effects. Models are evaluated using VADER sentiment analysis for open-ended responses and accuracy metrics for close-ended questions. The benchmark tests 17 models (15 open-source, 2 closed-source) across multiple bias dimensions including race, gender, age, and socioeconomic status.

## Key Results
- Open-source models like Shikra-7b exhibit higher bias across multiple categories compared to closed-source models
- LVLMs struggle with intersectional bias categories and ambiguous contexts
- Model bias generally decreases with increasing parameter scale within the same architecture
- Base/Scene subset differences reveal modality-specific bias patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic image generation via SDXL with prompt injection reduces training-data leakage and provides controlled bias conditions
- Mechanism: Generated images are semantically aligned to bias categories using CLIP similarity filtering (>0.7) and combined with question templates that isolate specific bias dimensions
- Core assumption: Synthetic data captures relevant bias signal without introducing confounding attributes, and CLIP-based filtering reliably ensures semantic alignment
- Evidence anchors: [abstract], [section], [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.472, average citations=0.0
- Break condition: If CLIP similarity drops below 0.7 or synthetic prompts introduce uncontrolled confounders

### Mechanism 2
- Claim: Multi-perspective evaluation (open-ended + close-ended, Base/Scene/Text/Scene Text) isolates impact of textual vs. visual modalities on bias expression
- Mechanism: Base dataset pairs neutral images with rich textual context; Scene dataset does the reverse. Text and Scene Text datasets add comparative group questions
- Core assumption: Four subsets sufficiently disentangle modality effects so differences in model performance can be attributed to specific bias sources
- Evidence anchors: [section], [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.472, average citations=0.0
- Break condition: If subset definitions overlap in confounding ways or questions don't cleanly isolate intended modality effect

### Mechanism 3
- Claim: Sentiment polarity and gender polarity metrics (VADER-based) provide reproducible, rule-based bias quantification without human annotation costs
- Mechanism: Model-generated responses scored with VADER for sentiment; subgroup ranges and thresholds (>0.5 positive, <-0.3 negative) identify bias magnitude
- Core assumption: Sentiment analysis captures social bias tendencies adequately and chosen thresholds filter out neutral noise
- Evidence anchors: [abstract], [section], [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.472, average citations=0.0
- Break condition: If VADER scoring poorly aligns with human judgment on bias-laden text or thresholds poorly chosen

## Foundational Learning

- Concept: Semantic alignment via CLIP similarity filtering
  - Why needed here: Ensures generated images faithfully represent bias categories and removes out-of-distribution noise that could confound bias evaluation
  - Quick check question: If CLIP similarity drops below 0.7, should the image be kept or discarded?

- Concept: Threshold-based sentiment filtering
  - Why needed here: Prevents neutral responses from diluting bias signal, making the metric more discriminative between models
  - Quick check question: What threshold values are used to classify positive vs. negative vs. neutral sentiment?

- Concept: Disambiguated vs. ambiguous context design
  - Why needed here: Allows measurement of whether models overconfidently answer when evidence is insufficient, a key bias failure mode
  - Quick check question: In ambiguous contexts, what answer option is considered correct?

## Architecture Onboarding

- Component map: SDXL (image generator) → CLIP encoder (semantic filter) → Prompt library (bias category + style) → Question templates (open/closed, Base/Scene/Text/Scene Text) → Model hub (15 open-source + 2 closed-source LVLMs) → Evaluation metrics (VADER range, gender polarity, accuracy)
- Critical path: Prompt construction → Image generation → CLIP filtering → Dataset assembly → Model inference → Metric computation → Result aggregation
- Design tradeoffs: Synthetic data avoids leakage but may miss real-world complexity; rule-based sentiment metrics are fast but less nuanced than human evaluation; four-way dataset split increases coverage but also annotation and maintenance burden
- Failure signatures: Low CLIP similarity scores; model outputs that are too brief or off-topic; accuracy drop when switching from Base to Scene; inconsistent rankings across sentiment vs. accuracy metrics
- First 3 experiments:
  1. Generate a small set of synthetic images for one bias category, run CLIP filtering, manually inspect semantic alignment
  2. Apply the open-ended question template to a held-out LVLM, compute VADER ranges, compare with manual sentiment scores
  3. Test one model on Base vs. Scene subsets for a single bias category, verify modality attribution logic by inspecting model answers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are current sentiment analysis methods like VADER in detecting biases in LLM-generated text, and can alternative metrics improve bias detection accuracy?
- Basis in paper: [explicit] The authors acknowledge that using sentiment analysis as an evaluation metric is an indirect way to reflect bias, building on and improving the approach established in previous work, BOLD [10]
- Why unresolved: While the authors validate the feasibility of the VADER metric through correlation experiments with human evaluation and GPT sentiment scores, the paper does not explore other potential evaluation metrics or compare their effectiveness in detecting biases
- What evidence would resolve it: Comparative studies using alternative bias detection metrics (e.g., model-based or human-involved) alongside VADER, showing their relative accuracy and efficiency in detecting biases across different bias categories

### Open Question 2
- Question: What are the long-term effects of using synthetic data to train or fine-tune LVLMs, and can it effectively mitigate bias issues in these models?
- Basis in paper: [explicit] The authors mention that synthetic data has been adopted for model training and fine-tuning in NLP and CV, and they believe VLBiasBench has the potential to serve as a viable training dataset to mitigate bias issues in large models
- Why unresolved: The paper focuses on using synthetic data for evaluation rather than training. The authors acknowledge that they have not explored the possibility of using their benchmark data for model training due to computational resource limitations
- What evidence would resolve it: Empirical studies demonstrating the impact of training or fine-tuning LVLMs using synthetic data on their bias levels, compared to models trained on real data or not trained with synthetic data

### Open Question 3
- Question: How do different model architectures and parameter scales influence the bias exhibited by LVLMs, and can increasing model size consistently reduce bias?
- Basis in paper: [explicit] The authors analyze the impact of parameter scale on output bias and find that, in most cases, the degree of bias tends to decrease as the number of parameters increases within the same architecture
- Why unresolved: While the authors observe a general trend of decreasing bias with increasing model size, they note an anomalous phenomenon where InstructBlip-flan-t5-xxl exhibits significant bias differences due to variations in language model size
- What evidence would resolve it: Comprehensive studies across various model architectures and parameter scales, examining the correlation between model size and bias levels, and identifying factors that influence this relationship

## Limitations
- Reliance on synthetic data may not fully capture real-world complexity and edge cases
- Rule-based sentiment analysis (VADER) may not perfectly align with human judgment on bias-laden text
- Limited exploration of intersectional bias disentanglement methodology

## Confidence
- Measuring intersectional biases: Medium
- Comparative ranking of open vs. closed-source models: High
- Synthetic data semantic alignment reliability: Medium

## Next Checks
1. Conduct human evaluation on a subset of model responses to validate VADER-based sentiment scoring accuracy for bias detection
2. Test model performance on real-world images from diverse sources to assess synthetic data limitations
3. Implement additional intersectional bias metrics to better capture model performance on overlapping demographic categories