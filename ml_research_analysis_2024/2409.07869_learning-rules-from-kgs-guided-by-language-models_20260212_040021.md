---
ver: rpa2
title: Learning Rules from KGs Guided by Language Models
arxiv_id: '2409.07869'
source_url: https://arxiv.org/abs/2409.07869
tags:
- rules
- rule
- language
- learning
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes leveraging large language models (LMs) for
  ranking rules extracted from knowledge graphs (KGs) in the context of KG completion.
  The core idea is to integrate a LM-based score with traditional statistical metrics
  like confidence when evaluating the quality of rules.
---

# Learning Rules from KGs Guided by Language Models

## Quick Facts
- arXiv ID: 2409.07869
- Source URL: https://arxiv.org/abs/2409.07869
- Authors: Zihang Peng; Daria Stepanova; Vinh Thinh Ho; Heike Adel; Alessandra Russo; Simon Ott
- Reference count: 14
- One-line primary result: Preliminary experiments show that incorporating non-fine-tuned LM scores can improve the average precision of top-ranked rules for KG completion, particularly with moderate weights (0.4-0.6) for the LM score.

## Executive Summary
This paper proposes leveraging large language models (LMs) for ranking rules extracted from knowledge graphs (KGs) in the context of KG completion. The core idea is to integrate a LM-based score with traditional statistical metrics like confidence when evaluating the quality of rules. Specifically, the authors construct prompts from predicted triples, query the LM to rank potential entities, and use the reciprocal rank of correct entities as an external quality measure. Their hybrid scoring function combines the traditional confidence with the LM-based score using a weighted average. Preliminary experiments on the Wiki44K dataset using BERT without fine-tuning show that incorporating LM scores can improve the average precision of top-ranked rules, particularly when using a moderate weight (0.4-0.6) for the LM score.

## Method Summary
The method involves extracting rules from a KG using association rule mining (AMIE+/AnyBurl) with thresholds for support (‚â• 10), confidence (‚â• 0.1), and head coverage (‚â• 0.01). These rules are then ranked using a hybrid scoring function that combines traditional confidence metrics with an LM-based score. The LM score is computed as the reciprocal rank of the correct entity predicted by the LM when queried with a masked prompt constructed from the rule's head and relation. The hybrid score is a weighted average of the statistical confidence and the LM score, with the weight parameter ùúô determining the contribution of each component. Experiments were conducted on the Wiki44K dataset using BERT without fine-tuning.

## Key Results
- Incorporating LM scores improves average precision of top-ranked rules compared to using confidence alone
- Optimal performance achieved with moderate LM weight (0.4-0.6) in the hybrid scoring function
- Non-fine-tuned BERT still provides meaningful LM-based scores for rule ranking
- The approach shows potential for using LMs in rule evaluation for KG completion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using reciprocal rank of correct entities from LM as an external quality measure captures predictive validity of rules that statistical metrics miss.
- Mechanism: For each rule prediction, construct a masked prompt (e.g., "John lives in [MASK]"), query LM for top-k predictions, and if the correct entity appears, use its reciprocal rank as a quality score. This directly measures how well the rule's prediction aligns with LM's knowledge.
- Core assumption: LMs encode factual knowledge that correlates with true KG facts, even without fine-tuning.
- Evidence anchors:
  - [abstract] "use the reciprocal rank of correct entities as an external quality measure"
  - [section] "The external rule quality measurement ùúá2 is the reciprocal rank of the correct answer assigned by the language model ‚Ñí given the head and relation of a triple predicted by a rule"
- Break condition: LM's factual knowledge is outdated, biased, or uncorrelated with true KG facts.

### Mechanism 2
- Claim: Hybrid scoring function combining statistical confidence with LM-based score improves rule ranking over using either alone.
- Mechanism: The weighted average formula ùúá(ùëü) = (1 ‚àí ùúô) * ùúá1(ùëü) + ùúô * ùúá2(ùëü) balances traditional confidence (descriptive quality) with LM-based reciprocal rank (predictive quality).
- Core assumption: Descriptive quality (confidence) and predictive quality (LM score) capture orthogonal aspects of rule quality.
- Evidence anchors:
  - [abstract] "Our hybrid scoring function combines the traditional confidence with the LM-based score using a weighted average"
  - [section] "the rule evaluation formula [2] combines these scores using a weighted average"
- Break condition: When ùúô is too high or too low, or when LM scores don't correlate with actual rule performance.

### Mechanism 3
- Claim: Non-fine-tuned BERT can still improve rule ranking because it captures general world knowledge.
- Mechanism: Even without KG-specific fine-tuning, BERT's pre-training on large text corpora provides factual knowledge that can validate rule predictions.
- Core assumption: Pre-trained LMs retain factual knowledge about entities and relationships without domain adaptation.
- Evidence anchors:
  - [abstract] "Preliminary experiments... show that incorporating LM scores can improve the average precision of top-ranked rules, particularly when using a moderate weight (0.4-0.6) for the LM score"
  - [section] "Our preliminary experiments... have shown a potential for using LMs for rule evaluation"
- Break condition: When the LM's factual knowledge is insufficient or contradictory to KG facts.

## Foundational Learning

- Concept: Knowledge Graph Completion
  - Why needed here: The entire approach depends on understanding that KGs are often incomplete and need completion methods
  - Quick check question: What are the two main approaches to KG completion mentioned in the paper?

- Concept: Association Rule Mining
  - Why needed here: Rules are extracted using association rule mining techniques like those in AMIE+
  - Quick check question: What are the three key metrics used to filter rules during extraction?

- Concept: Hybrid Scoring Systems
  - Why needed here: The method combines two different quality measures using weighted averaging
  - Quick check question: How does the weight parameter ùúô affect the final rule ranking?

## Architecture Onboarding

- Component map: KG ‚Üí Rule Extraction (AMIE+/AnyBurl) ‚Üí Rule Ranking (Hybrid Score) ‚Üí Predictions ‚Üí Evaluation
- Critical path: Rule extraction ‚Üí LM-based ranking ‚Üí Top-k rules ‚Üí Prediction generation
- Design tradeoffs: Fine-tuned vs non-fine-tuned LMs (tradeoff between accuracy and deployment complexity)
- Failure signatures: Low LM score correlation with actual prediction accuracy; high variance in reciprocal ranks
- First 3 experiments:
  1. Run rule extraction with standard confidence only, record baseline precision
  2. Run rule extraction with LM-based ranking only (ùúô=1), compare performance
  3. Run hybrid ranking with varying ùúô values (0.2, 0.4, 0.6, 0.8), find optimal weight

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LM-guided rule ranking change when using fine-tuned LMs versus out-of-the-box LMs?
- Basis in paper: [explicit] The paper mentions that preliminary experiments were done with BERT without fine-tuning, showing potential for LMs in rule evaluation.
- Why unresolved: The experiments only used non-fine-tuned BERT, leaving the impact of fine-tuning on LMs unexplored.
- What evidence would resolve it: Experiments comparing rule ranking performance using fine-tuned LMs against non-fine-tuned LMs on the same dataset.

### Open Question 2
- Question: Can the hybrid scoring function be extended to include other statistical metrics beyond confidence and PCA confidence?
- Basis in paper: [inferred] The paper discusses using a weighted average of statistical metrics and LM-based scores, suggesting potential for other metrics.
- Why unresolved: The experiments focused on standard and PCA confidence, without exploring other possible metrics.
- What evidence would resolve it: Experiments testing the hybrid scoring function with additional statistical metrics like support or conviction.

### Open Question 3
- Question: What is the impact of the parameter 'n' (number of top results from LM) on the quality of rule ranking?
- Basis in paper: [explicit] The paper mentions using top-n results from LM to determine correct predictions, but does not explore the impact of varying 'n'.
- Why unresolved: The choice of 'n' is not investigated, leaving its effect on rule ranking quality unknown.
- What evidence would resolve it: Experiments varying 'n' and measuring its impact on rule ranking performance.

### Open Question 4
- Question: How does the performance of LM-guided rule learning scale with larger and more complex knowledge graphs?
- Basis in paper: [inferred] The experiments were conducted on a relatively small dataset (Wiki44K), suggesting the need to test scalability.
- Why unresolved: The current experiments are limited to a small dataset, not addressing scalability concerns.
- What evidence would resolve it: Experiments applying the method to larger and more complex KGs to evaluate scalability and performance.

## Limitations

- Experiments conducted on a single small-scale dataset (Wiki44K) limiting generalizability
- Use of non-fine-tuned BERT may not capture domain-specific knowledge as effectively as fine-tuned models
- Entity-to-token mapping relies on cosine similarity which could introduce noise for ambiguous entities

## Confidence

- High Confidence: Using reciprocal rank from LM predictions as external quality measure
- Medium Confidence: Non-fine-tuned BERT can effectively rank rules; orthogonal nature of descriptive and predictive quality measures
- Low Confidence: Optimal weight range of 0.4-0.6 for LM score in hybrid function generalizes across different contexts

## Next Checks

1. **Dataset Generalization Test**: Replicate the experiments on at least two additional KGs of varying scales and domains (e.g., YAGO, DBpedia) to validate the robustness of the hybrid scoring approach and optimal weight parameters across different data characteristics.

2. **Fine-tuning Impact Analysis**: Compare the performance of non-fine-tuned BERT against domain-adapted versions (BERT fine-tuned on KG triples or domain-specific text) to quantify the tradeoff between deployment simplicity and ranking accuracy.

3. **Entity Mapping Validation**: Systematically evaluate the accuracy of the entity-to-token mapping mechanism by measuring the correlation between cosine similarity scores and actual entity representation quality, particularly for entities with ambiguous or multi-word names.