---
ver: rpa2
title: 'LLMs Are Not Intelligent Thinkers: Introducing Mathematical Topic Tree Benchmark
  for Comprehensive Evaluation of LLMs'
arxiv_id: '2406.05194'
source_url: https://arxiv.org/abs/2406.05194
tags:
- llms
- reasoning
- mathematical
- topics
- choices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Mathematical Topics Tree (MaTT) benchmark,
  a comprehensive dataset with 1,958 math questions across 12 major topics from pure
  and applied mathematics. The authors evaluate GPT-4, ChatGPT, and Mistral on MaTT
  and find that GPT-4 achieves only 54% accuracy on multiple-choice questions.
---

# LLMs Are Not Intelligent Thinkers: Introducing Mathematical Topic Tree Benchmark for Comprehensive Evaluation of LLMs

## Quick Facts
- arXiv ID: 2406.05194
- Source URL: https://arxiv.org/abs/2406.05194
- Reference count: 5
- Key outcome: GPT-4 achieves only 54% accuracy on MaTT multiple-choice questions, with performance dropping 24.2 percentage points when choices are removed

## Executive Summary
This paper introduces the Mathematical Topics Tree (MaTT) benchmark, a comprehensive dataset with 1,958 math questions across 12 major topics from pure and applied mathematics. The authors evaluate GPT-4, ChatGPT, and Mistral on MaTT and find that GPT-4 achieves only 54% accuracy on multiple-choice questions. Chain-of-thought prompting provides little improvement, and accuracy drops significantly when choices are removed. Manual analysis of GPT-4's explanations shows that in only 53.3% of correct answers does the model engage in genuine reasoning; the rest rely on strategies like choice engineering, theorem use, or memorization. These results indicate that LLMs still struggle with deep mathematical reasoning and that their performance is highly dependent on the presence of answer choices.

## Method Summary
The authors constructed the MaTT benchmark by curating 1,958 mathematical questions across 12 topics from pure and applied mathematics. They evaluated three LLMs (GPT-4, ChatGPT, Mistral) on these questions using both standard and Chain-of-Thought prompting. Performance was measured with and without multiple-choice options. For questions answered correctly by GPT-4, the authors manually analyzed the model's explanations to categorize the reasoning strategies employed, distinguishing between complete reasoning, choice/weak reasoning, and no/wrong reasoning.

## Key Results
- GPT-4 achieved only 54% accuracy on multiple-choice questions in the MaTT benchmark
- LLM accuracy dropped by up to 24.2 percentage points when multiple-choice options were removed
- Only 53.3% of GPT-4's correct answers showed genuine reasoning; the rest relied on choice engineering, theorem use, circular reasoning, or memorization
- Chain-of-thought prompting provided minimal improvement in model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple-choice format enables LLMs to bypass genuine reasoning by exploiting answer patterns and proximity heuristics
- Mechanism: When choices are present, LLMs use choice engineering strategies such as selecting the closest numerical value, removing implausible options, or exploiting patterns in how choices are constructed. This allows them to arrive at correct answers without engaging in the full reasoning chain
- Core assumption: LLMs prioritize answer selection over reasoning when choices are provided, and can achieve high accuracy without completing the underlying logic
- Evidence anchors: [abstract] "LLMs accuracy dramatically reduced by up to 24.2 percentage point when the questions were presented without providing choices"

### Mechanism 2
- Claim: Chain-of-thought prompting fails to improve LLM performance on complex mathematical reasoning tasks because the reasoning steps required exceed simple decomposition
- Mechanism: CoT prompting is designed to elicit step-by-step reasoning, but when the problem requires multiple interdependent steps, creative problem-solving, or non-trivial theorem application, the model cannot effectively generate the intermediate reasoning chain, resulting in no improvement
- Core assumption: The effectiveness of CoT is limited to problems solvable in a few straightforward steps, and does not extend to complex multi-step reasoning
- Evidence anchors: [abstract] "even when employing Chain-of-Thought prompting, we observe mostly no notable improvement"

### Mechanism 3
- Claim: LLM accuracy varies significantly across closely related mathematical sub-topics due to differences in exposure, complexity, and reasoning requirements
- Mechanism: Even within the same broad mathematical area, sub-topics differ in their reliance on specific theorems, calculation complexity, or creative insight. LLMs may have better coverage or memorization for some sub-topics, leading to inconsistent performance
- Core assumption: LLMs' knowledge is unevenly distributed across mathematical domains, and performance is not solely determined by overall topic difficulty
- Evidence anchors: [abstract] "Further detailed analysis of the LLMs' performance across a range of topics showed significant discrepancy even for closely related subtopics within the same general mathematical area"

## Foundational Learning

- Concept: Mathematical reasoning hierarchy (from arithmetic to abstract reasoning)
  - Why needed here: Understanding the level of reasoning required for different mathematical problems helps explain why LLMs perform well on some topics but poorly on others
  - Quick check question: Can you classify a given math problem as requiring procedural calculation, theorem application, or creative insight?

- Concept: Chain-of-thought prompting and its limitations
  - Why needed here: CoT is a common technique to improve LLM reasoning, but its effectiveness depends on the problem structure and the model's ability to generate intermediate steps
  - Quick check question: When would you expect CoT prompting to fail for a mathematical problem?

- Concept: Multiple-choice exploitation strategies
  - Why needed here: LLMs often use answer choice patterns to bypass reasoning, so understanding these strategies is crucial for designing robust benchmarks
  - Quick check question: How might an LLM use answer choices to arrive at a correct answer without genuine reasoning?

## Architecture Onboarding

- Component map: MaTT Benchmark -> LLM Evaluators -> Manual Analysis Pipeline -> Performance Metrics
- Critical path:
  1. Construct MaTT benchmark from reference books and topical hierarchy
  2. Generate multiple-choice options for each question
  3. Evaluate LLMs on MaTT with and without choices and CoT
  4. Manually analyze GPT-4 explanations for reasoning completeness
  5. Identify performance patterns and reasoning strategies

- Design tradeoffs:
  - Breadth vs. depth: Covering many topics vs. deep evaluation of a few
  - Question difficulty: Balancing challenge with solvability for LLMs
  - Choice construction: Making choices plausible but distinguishable
  - Manual annotation: Ensuring consistency and objectivity in reasoning evaluation

- Failure signatures:
  - High accuracy with choices but low without suggests choice exploitation
  - No improvement with CoT suggests complex reasoning requirements
  - Inconsistent performance across sub-topics suggests uneven knowledge distribution
  - Explanations with circular reasoning or unsupported theorem use indicate lack of genuine reasoning

- First 3 experiments:
  1. Evaluate LLM performance on MaTT with and without choices to measure choice dependency
  2. Compare LLM performance with and without CoT prompting to assess reasoning step decomposition
  3. Manually analyze GPT-4 explanations for reasoning completeness and identify common strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the true extent of large language models' ability to perform mathematical reasoning without relying on answer choices or memorization strategies?
- Basis in paper: Explicit - The paper states that GPT-4 only achieved 54% accuracy on multiple-choice questions and that its performance dropped significantly when choices were removed, indicating heavy reliance on choice engineering and other strategies rather than genuine reasoning
- Why unresolved: The paper provides evidence of LLMs' dependency on choices and their use of strategies like choice engineering, theorem use, circular reasoning, and memorization. However, it does not conclusively determine the models' ability to reason independently
- What evidence would resolve it: Comprehensive testing of LLMs on a wider range of mathematical problems without answer choices, coupled with rigorous analysis of their reasoning processes, would provide clearer insights into their independent reasoning capabilities

### Open Question 2
- Question: How do different prompting strategies, beyond Chain-of-Thought, affect the performance of large language models on complex mathematical problems?
- Basis in paper: Explicit - The paper mentions that Chain-of-Thought prompting did not notably improve model performance, suggesting that more sophisticated prompting techniques might be necessary for complex mathematical reasoning
- Why unresolved: The paper only explores the impact of Chain-of-Thought prompting and does not investigate other prompting strategies that could potentially enhance LLMs' performance on mathematical tasks
- What evidence would resolve it: Systematic evaluation of various prompting strategies, including Tree-of-Thought and Self-Verification, on a diverse set of mathematical problems would reveal which approaches are most effective in eliciting genuine reasoning from LLMs

### Open Question 3
- Question: What specific mathematical topics or types of problems do large language models struggle with the most, and why?
- Basis in paper: Explicit - The paper highlights significant discrepancies in LLMs' performance across different mathematical topics, indicating that models have inconsistent reasoning capabilities even within closely related subtopics
- Why unresolved: While the paper identifies performance gaps across various topics, it does not delve into the underlying reasons for these discrepancies or provide a detailed analysis of the specific types of problems that pose the greatest challenges to LLMs
- What evidence would resolve it: In-depth analysis of LLMs' errors and reasoning processes on different types of mathematical problems, coupled with comparisons to human problem-solving approaches, would elucidate the specific areas where models struggle and the reasons behind these difficulties

## Limitations

- The MaTT benchmark dataset remains unpublished, preventing independent verification of question quality and topic coverage
- Manual analysis of GPT-4 explanations involves subjective categorization of reasoning strategies with inter-annotator agreement not reported
- The sample size of manually analyzed explanations (57 out of 360 correct answers) may not fully capture the distribution of reasoning strategies across all topics

## Confidence

- High confidence: The observed performance drop (24.2 percentage points) when answer choices are removed is robust and well-supported by the data presented
- Medium confidence: The claim that only 53.3% of correct answers show genuine reasoning is supported by manual analysis but depends on subjective categorization criteria
- Medium confidence: The assertion that CoT prompting provides minimal improvement is supported by the data, though the complexity of mathematical problems makes this conclusion reasonable but not definitive

## Next Checks

1. Publish and conduct independent evaluation of the MaTT benchmark on additional LLM models to verify the observed performance patterns and choice dependency effects across different architectures
2. Perform inter-annotator reliability testing on the manual explanation analysis to establish consistency in categorizing reasoning strategies and complete the analysis for all correct answers
3. Design and test a modified version of MaTT with answer choices constructed to minimize exploitation patterns (e.g., using numerical ranges rather than discrete options) to quantify the specific contribution of choice engineering to LLM performance