---
ver: rpa2
title: 'LEASE: Offline Preference-based Reinforcement Learning with High Sample Efficiency'
arxiv_id: '2412.21001'
source_url: https://arxiv.org/abs/2412.21001
tags:
- reward
- offline
- data
- preference
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LEASE, an offline preference-based reinforcement
  learning algorithm with high sample efficiency. LEASE addresses the challenge of
  acquiring sufficient preference labels for offline PbRL by leveraging a learned
  transition model to generate unlabeled preference data and introducing an uncertainty-aware
  mechanism to select high-quality data.
---

# LEASE: Offline Preference-based Reinforcement Learning with High Sample Efficiency

## Quick Facts
- arXiv ID: 2412.21001
- Source URL: https://arxiv.org/abs/2412.21001
- Authors: Xiao-Yin Liu; Guotao Li; Xiao-Hu Zhou; Zeng-Guang Hou
- Reference count: 40
- Primary result: Achieves comparable performance to URLHF using fewer preference data without online interaction, with 3.9% average performance improvement

## Executive Summary
LEASE addresses the challenge of acquiring sufficient preference labels for offline preference-based reinforcement learning by combining data augmentation through a learned transition model with an uncertainty-aware selection mechanism. The method generates unlabeled preference data using the transition model and filters high-quality data using confidence and uncertainty thresholds before training the reward model. Experimental results on D4RL benchmark tasks show LEASE achieves strong performance with limited preference data, particularly improving on tasks like walker2d-m-e, pen-expert, and hammer-expert. The paper also provides theoretical guarantees for both reward model generalization and policy improvement in the preference-based setting.

## Method Summary
LEASE is an offline preference-based reinforcement learning algorithm that addresses the challenge of limited preference data by leveraging a learned transition model to generate additional unlabeled preference data. The method uses ensemble models for both transition and reward learning, with an uncertainty-aware selection mechanism that filters generated data based on confidence (preference model output probability) and variance (ensemble prediction variance). The algorithm trains a transition model on offline data, pre-trains a reward model on limited preference data, generates and selects unlabeled data, updates the reward model, and trains a policy using offline RL algorithms like CQL or IQL.

## Key Results
- LEASE achieves comparable performance to URLHF using fewer preference data without online interaction
- Average performance improvement of 3.9% over URLHF using the same amount of data
- Demonstrates effectiveness across locomotion and manipulation tasks, with significant improvements on walker2d-m-e, pen-expert, and hammer-expert
- Shows better data efficiency compared to baselines in offline PbRL setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation via transition model combined with uncertainty-aware selection improves reward model accuracy under limited preference data.
- Mechanism: LEASE trains a transition model on offline data to generate unlabeled preference data, then uses a selection mechanism based on high confidence (preference model output probability) and low variance (ensemble prediction variance) to filter noisy pseudo-labels before reward model training.
- Core assumption: Generated unlabeled data follows similar distribution to real preference data and contains useful signal; the selection mechanism effectively filters out incorrect pseudo-labels.
- Evidence anchors:
  - [abstract] "where a learned transition model is leveraged to generate unlabeled preference data... an uncertainty-aware mechanism to select high-quality data"
  - [section] "motivated by model-based offline RL... we also train the transition model to generate more unlabeled preference data... the generated preference labels may contain errors"
  - [corpus] Weak evidence - no direct comparison of data augmentation methods in corpus
- Break condition: If transition model has large errors, generated data distribution diverges significantly from real data, making selection mechanism ineffective.

### Mechanism 2
- Claim: Generalization bound for reward model establishes theoretical guarantee for performance under limited data.
- Mechanism: Theorem 1 provides a bound on expected reward error combining empirical error, pseudo-label error, Rademacher complexity, and sample size terms.
- Core assumption: Reward model belongs to a function class with bounded Rademacher complexity; pseudo-label error can be bounded by selection mechanism.
- Evidence anchors:
  - [abstract] "the generalization bound of reward model to analyze the factors influencing reward accuracy"
  - [section] "we develop a new generalization bound for the reward model and provide a theory of policy improvement guarantee"
  - [corpus] Weak evidence - corpus papers mention theoretical analysis but don't provide specific generalization bounds
- Break condition: If function class complexity is too high or pseudo-label error is unbounded, bound becomes vacuous.

### Mechanism 3
- Claim: Policy improvement guarantee extends offline RL theory to preference-based setting using state-action pairs.
- Mechanism: Theorem 2 provides performance bound combining offline RL gap (ξ) and reward model gap, showing how both components affect final policy quality.
- Core assumption: Concentrability coefficient for PbRL can be bounded using state-action pair analysis; reward function class is bounded.
- Evidence anchors:
  - [abstract] "demonstrate that the policy learned by LEASE has theoretical improvement guarantee"
  - [section] "develop a theoretical guarantee of policy improvement for LEASE... giving the bound for J(µ, R*) - J(bπ, R*)"
  - [corpus] Weak evidence - corpus papers mention theoretical analysis but don't provide specific policy improvement guarantees
- Break condition: If concentrability coefficient grows unbounded or offline RL gap dominates, improvement guarantee becomes weak.

## Foundational Learning

- Concept: Semi-supervised learning with noisy labels
  - Why needed here: LEASE uses unlabeled data with pseudo-labels from reward model, which may be noisy
  - Quick check question: How does the selection mechanism reduce the impact of incorrect pseudo-labels?

- Concept: Model-based reinforcement learning
  - Why needed here: Transition model generates additional data for augmentation, similar to model-based RL approaches
  - Quick check question: What happens to generated data quality if transition model has high prediction error?

- Concept: Ensemble methods for uncertainty estimation
  - Why needed here: Multiple reward and transition models provide variance estimates for data selection
  - Quick check question: How does prediction variance relate to label confidence in the selection mechanism?

## Architecture Onboarding

- Component map:
  Transition model ensemble (N_T=7, elite=5) -> Reward model ensemble (N_R=3) -> Selection mechanism -> Reward model update -> Policy learner -> Evaluation

- Critical path: Transition model training → Reward model pre-training → Data generation → Selection → Reward update → Policy learning → Evaluation

- Design tradeoffs:
  - More ensemble members → better uncertainty estimates but higher computational cost
  - Stricter selection thresholds → higher quality data but fewer samples
  - More generated data → better coverage but potential for noisy labels
  - Choice of offline RL algorithm affects policy learning but not reward learning

- Failure signatures:
  - Reward model training collapse: Likely due to too many incorrect pseudo-labels
  - Poor policy performance: Could indicate reward model inaccuracies or insufficient data
  - High variance in results: May suggest instability in selection mechanism or model training

- First 3 experiments:
  1. Verify transition model generates plausible trajectories by visualizing a few samples
  2. Test selection mechanism by checking pseudo-label accuracy with/without filtering on a small validation set
  3. Compare reward model performance with different confidence/uncertainty thresholds on a held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LEASE scale with the amount of offline data used for training the transition model?
- Basis in paper: [explicit] The paper mentions that offline datasets are used to train the transition model, but does not explore how varying the size of this offline dataset affects the quality of generated preference data and subsequent policy performance.
- Why unresolved: The experiments only use fixed offline datasets from D4RL without varying their size or quality.
- What evidence would resolve it: Experiments showing policy performance with different sizes of offline datasets used for transition model training, particularly for tasks where current performance is lower than URLHF.

### Open Question 2
- Question: What is the impact of different ensemble sizes for transition and reward models on LEASE's performance?
- Basis in paper: [explicit] The paper uses 7 transition models and 3 reward models but does not systematically explore how varying these ensemble sizes affects performance or computational efficiency.
- Why unresolved: The experimental results use fixed ensemble sizes without ablation studies on different configurations.
- What evidence would resolve it: Performance comparisons using different numbers of models in the ensembles (e.g., 3, 5, 7, 10) across multiple tasks to find optimal trade-offs between performance and computational cost.

### Open Question 3
- Question: How does LEASE perform on tasks with more complex or sparse reward structures beyond those tested in the D4RL benchmark?
- Basis in paper: [inferred] The paper focuses on Mujoco and Adroit tasks with relatively straightforward reward structures, but does not test on environments with highly complex or sparse rewards.
- Why unresolved: The experimental validation is limited to standard benchmark tasks without exploring more challenging reward landscapes.
- What evidence would resolve it: Performance evaluations on environments with highly sparse rewards, delayed rewards, or multi-objective reward structures to assess LEASE's generalization to more complex reward learning scenarios.

## Limitations
- Theoretical bounds rely heavily on concentrability coefficient assumptions that may not hold in practice
- No empirical validation of theoretical claims or sensitivity analysis of selection thresholds
- Limited ablation studies on selection mechanism effectiveness and ensemble size impact
- Experiments only on standard D4RL benchmarks without testing on more complex reward structures

## Confidence
- Mechanism 1 (Data augmentation with uncertainty selection): Medium - Strong empirical results but limited ablation studies on selection mechanism effectiveness
- Mechanism 2 (Generalization bound): Low - Theoretical derivation present but no empirical verification of bound tightness
- Mechanism 3 (Policy improvement guarantee): Low - Theoretical guarantee derived but untested on real-world performance impact

## Next Checks
1. Conduct ablation studies systematically varying selection thresholds to understand their impact on performance and pseudo-label quality
2. Measure concentrability coefficients empirically across tasks to validate theoretical assumptions about policy improvement bounds
3. Perform sensitivity analysis on ensemble size (N_T, N_R) to determine minimum effective ensemble configurations and computational tradeoffs