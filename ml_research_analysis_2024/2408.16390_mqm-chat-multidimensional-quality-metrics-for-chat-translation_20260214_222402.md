---
ver: rpa2
title: 'MQM-Chat: Multidimensional Quality Metrics for Chat Translation'
arxiv_id: '2408.16390'
source_url: https://arxiv.org/abs/2408.16390
tags:
- translation
- chat
- errors
- data
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MQM-Chat introduces a refined evaluation metric tailored for chat\
  \ translation, addressing the limitations of existing metrics in capturing the nuances\
  \ of stylized and ambiguous content. The metric includes seven error types\u2014\
  mistranslation, omission or addition, terminology or proper noun issues, unnatural\
  \ style, ambiguity and disambiguation, buzzword or loanword issues, and dialogue\
  \ inconsistency\u2014with three severity levels."
---

# MQM-Chat: Multidimensional Quality Metrics for Chat Translation

## Quick Facts
- arXiv ID: 2408.16390
- Source URL: https://arxiv.org/abs/2408.16390
- Reference count: 25
- MQM-Chat introduces a refined evaluation metric tailored for chat translation, addressing limitations of existing metrics in capturing nuances of stylized and ambiguous content.

## Executive Summary
MQM-Chat introduces a refined evaluation metric tailored for chat translation, addressing the limitations of existing metrics in capturing the nuances of stylized and ambiguous content. The metric includes seven error types—mistranslation, omission or addition, terminology or proper noun issues, unnatural style, ambiguity and disambiguation, buzzword or loanword issues, and dialogue inconsistency—with three severity levels. Experiments using five models (GPT-4, LLaMA3, DeepL, SKIM, and Facebook) on Chinese⇒English and Japanese⇒English datasets revealed that GPT-4 outperformed others, particularly in handling dialogue consistency and buzzwords. Traditional NMT models struggled with ambiguity and stylized content, while LLMs occasionally added explanatory text for culturally specific terms. The findings highlight the importance of preserving stylized content and dialogue consistency in chat translation, emphasizing the need for tailored training and evaluation methods.

## Method Summary
The study evaluated five translation models (GPT-4, LLaMA3, DeepL, SKIM, and Facebook) on Chinese⇒English and Japanese⇒English chat datasets using MQM-Chat, a refined error categorization framework. Datasets included 200 Chinese chats (LCCC-base), 200 Japanese chats (Open2ch), and 100 long Chinese (NaturalConv) and 100 long Japanese (BPersona-chat) for comparison. Bilingual annotators labeled translations with MQM-Chat error types and severity levels using Label Studio. Overall Quality Scores (OQS) were calculated using absolute penalty totals and evaluation word counts, with detailed error type distributions analyzed across models and datasets.

## Key Results
- GPT-4 outperformed other models in handling dialogue consistency and buzzwords in chat translation.
- Traditional NMT models struggled with ambiguity and stylized content in chat translations.
- LLMs occasionally added explanatory text for culturally specific terms, impacting translation quality.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MQM-Chat captures chat-specific translation errors that traditional metrics miss.
- Mechanism: By introducing error types tailored for chats (ambiguity and disambiguation, buzzword/loanword issues, dialogue inconsistency), MQM-Chat evaluates translations based on semantic accuracy and stylized content preservation rather than just fluency and grammar.
- Core assumption: Chat translations require different evaluation criteria than formal documents due to their ambiguity, colloquialisms, and context dependency.
- Evidence anchors:
  - [abstract] "The complexities of chats pose significant challenges for machine translation models... a refined error categorization framework assessing semantic accuracy while preserving the speaker's stylized nuances"
  - [section] "MQM-Chat focuses on seven error types: mistranslation, omission or addition, terminology or proper noun issues, unnatural style, ambiguity and disambiguation, buzzwords or loanwords issues, and dialogue inconsistency, where the latter three are customized typologies tailored for chat translation"
  - [corpus] Weak - no direct corpus evidence linking MQM-Chat to improved evaluation outcomes
- Break condition: If chat content becomes more standardized or if models achieve perfect accuracy on stylized content, the specialized error types become unnecessary.

### Mechanism 2
- Claim: Severity levels in MQM-Chat provide nuanced quality assessment.
- Mechanism: Three severity levels (major, minor, neutral) with penalty multipliers allow for weighted scoring that reflects the impact of different error types on overall translation quality.
- Core assumption: Not all translation errors equally impact the quality and understandability of chat translations.
- Evidence anchors:
  - [abstract] "We provided three levels of severity for each error to evaluate the translations comprehensively: major for errors that significantly impact the understandability of the content; minor for errors that do not impact the overall understandability but detract from the quality; neutral for errors requiring additional revision but do not pose significant risks"
  - [section] "Severity penalty multipliers are 5 for major, 1 for minor, and 0 for neutral"
  - [corpus] Weak - no direct corpus evidence demonstrating the effectiveness of severity-based scoring
- Break condition: If severity distinctions become too subjective or if the penalty system fails to correlate with human judgment of translation quality.

### Mechanism 3
- Claim: MQM-Chat enables comparative analysis of different translation models' strengths and weaknesses.
- Mechanism: By applying MQM-Chat to evaluate multiple models (GPT-4, LLaMA3, DeepL, SKIM, Facebook), researchers can identify specific areas where each model excels or struggles in chat translation.
- Core assumption: Different translation models have distinct capabilities and limitations when handling chat-specific challenges.
- Evidence anchors:
  - [abstract] "Through the experiments of five models using MQM-Chat, we observed that all models generated certain fundamental errors, while each of them has different shortcomings, such as omission, overly correcting ambiguous source content, and buzzword issues"
  - [section] "The Overall Quality Score (OQS) calculations indicate that GPT-4 outperformed the other models. On the other hand, the severity percentage of each error type shows that there are usually more severe mistranslations, buzzword or loanword issues, and dialogue inconsistency errors in chat translations"
  - [corpus] Weak - no direct corpus evidence showing improved model selection or development based on MQM-Chat results
- Break condition: If all models converge to similar performance levels or if MQM-Chat fails to differentiate between models in meaningful ways.

## Foundational Learning

- Concept: Multidimensional Quality Metrics (MQM) framework
  - Why needed here: MQM-Chat builds upon the existing MQM framework, adapting it for chat translation scenarios.
  - Quick check question: What are the seven error types in MQM-Chat and which three are specifically customized for chats?

- Concept: Severity-based quality scoring
  - Why needed here: The severity levels and penalty multipliers are crucial for calculating the Overall Quality Score (OQS) and providing nuanced evaluation.
  - Quick check question: How do the severity penalty multipliers (5 for major, 1 for minor, 0 for neutral) affect the final OQS calculation?

- Concept: Chat translation challenges
  - Why needed here: Understanding the unique complexities of chat translation (ambiguity, stylized content, cultural nuances) is essential for appreciating the need for MQM-Chat.
  - Quick check question: Why are ambiguity and disambiguation errors particularly important in chat translation compared to formal document translation?

## Architecture Onboarding

- Component map: MQM-Chat error taxonomy (7 types with 3 severity levels) -> Annotation tool (Label Studio) -> Translation models (GPT-4, LLaMA3, DeepL, SKIM, Facebook) -> Datasets (Chinese→English and Japanese→English chats) -> Overall Quality Score (OQS) calculation formula

- Critical path: Prepare chat datasets in source languages -> Generate translations using multiple models -> Annotate translations using MQM-Chat error types and severity levels -> Calculate OQS for each model and dataset combination -> Analyze error distributions and model performance

- Design tradeoffs: Specialized error types vs. general applicability -> Subjective severity assessment vs. objective scoring -> Crowdsourced annotation quality vs. expert annotation cost

- Failure signatures: Low inter-annotator agreement on error types or severity levels -> OQS not correlating with human judgment of translation quality -> Error distributions not aligning with known chat translation challenges

- First 3 experiments: Compare OQS results between short (noisy) and long (clean) chat datasets to validate the metric's sensitivity to chat-specific challenges -> Analyze the distribution of error types across different models to identify common weaknesses and strengths -> Test the impact of different severity assessment guidelines on inter-annotator agreement and final OQS calculations

## Open Questions the Paper Calls Out

The paper acknowledges that the evaluation was limited to translations from Chinese and Japanese to English, stating "With data limited to translations from Chinese and Japanese to English, the result of our experiments is relatively narrow." This limitation raises questions about how MQM-Chat would perform in different linguistic contexts and whether the error taxonomy generalizes to other language pairs.

## Limitations

- The study relies heavily on crowdsourced annotations without reporting inter-annotator agreement statistics, raising questions about the reliability of error severity assessments.
- The comparison across five translation models uses different prompting strategies (few-shot vs. zero-shot) and model sizes, making it difficult to isolate the impact of model architecture versus prompt design on performance differences.
- The error taxonomy, while tailored for chats, may not generalize to other informal communication contexts like social media posts or forum discussions.

## Confidence

- **High confidence**: The identification of specific error types unique to chat translation (ambiguity/disambiguation, buzzword/loanword issues, dialogue inconsistency) and their observation across multiple models.
- **Medium confidence**: The overall superiority of GPT-4 in handling dialogue consistency and buzzwords, given the varying experimental conditions across models.
- **Low confidence**: The severity-based scoring system's effectiveness without reported inter-annotator agreement or correlation with human judgment of translation quality.

## Next Checks

1. Conduct inter-annotator agreement analysis to establish reliability thresholds for MQM-Chat error labeling and severity assessment.
2. Replicate experiments using identical prompting strategies across all models to isolate architectural differences from implementation variations.
3. Test MQM-Chat's generalizability by applying it to evaluate translations of other informal text types (social media, forums) to validate the error taxonomy's broader applicability.