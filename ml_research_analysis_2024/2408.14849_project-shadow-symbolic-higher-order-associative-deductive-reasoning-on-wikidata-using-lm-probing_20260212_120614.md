---
ver: rpa2
title: 'Project SHADOW: Symbolic Higher-order Associative Deductive reasoning On Wikidata
  using LM probing'
arxiv_id: '2408.14849'
source_url: https://arxiv.org/abs/2408.14849
tags:
- knowledge
- relation
- language
- shadow
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SHADOW, a fine-tuned language model for knowledge
  base construction using Wikidata triple completion. The approach combines LM probing
  with associative deductive reasoning to improve relation-type disambiguation.
---

# Project SHADOW: Symbolic Higher-order Associative Deductive reasoning On Wikidata using LM probing

## Quick Facts
- arXiv ID: 2408.14849
- Source URL: https://arxiv.org/abs/2408.14849
- Authors: Hanna Abi Akl
- Reference count: 11
- F1 score: 68.72% on LM-KBC 2024 challenge

## Executive Summary
SHADOW is a fine-tuned language model for knowledge base construction using Wikidata triple completion. The approach combines LM probing with associative deductive reasoning to improve relation-type disambiguation. SHADOW was trained on a small Wikidata triple dataset using template-based SPARQL query generation. Evaluated on the LM-KBC 2024 challenge, it achieved an F1 score of 68.72%, outperforming the baseline by 20%. Performance varied by relation type, with strong results on awardWonBy but challenges with seriesHasNumberOfEpisodes.

## Method Summary
SHADOW fine-tunes a pre-trained T5 model to generate template IDs instead of full SPARQL queries, simplifying the knowledge base completion task. The model learns to associate subject-relation pairs with numerical template IDs that correspond to pre-defined SPARQL patterns. Trained on 377 Wikidata triples across 5 relation types, SHADOW leverages transfer learning from the base T5 model while focusing on associative deductive reasoning principles. The approach shifts from generating correct SPARQL queries to retrieving relevant objects through template-based reasoning.

## Key Results
- F1 score of 68.72% on LM-KBC 2024 challenge, outperforming baseline by 20%
- Strong performance on awardWonBy relation (F1: 0.88) despite data scarcity
- Variable performance across relations: seriesHasNumberOfEpisodes achieved perfect precision but zero recall

## Why This Works (Mechanism)

### Mechanism 1
SHADOW leverages associative deductive reasoning to disambiguate relation types and implicitly learn SPARQL query patterns. The model is trained to generate template IDs instead of full SPARQL queries. By mapping subject-relation pairs to numerical template IDs, the model learns to associate each relation type with the correct knowledge graph completion pattern. This reduces the complexity of the generation task while maintaining semantic accuracy.

### Mechanism 2
Transfer learning from a base T5 model enables SHADOW to leverage pre-existing generative capabilities for knowledge base construction. By fine-tuning a pre-trained T5 model on the intermediate task of template ID generation, SHADOW inherits the model's ability to generate structured outputs while adapting to the specific domain of Wikidata triples. The small dataset size (377 training triples) is compensated by the model's prior knowledge.

### Mechanism 3
The template-based approach reduces the generation complexity while maintaining semantic accuracy for knowledge base completion. Instead of generating full SPARQL queries, SHADOW learns to generate template IDs that correspond to pre-defined SPARQL patterns. This simplifies the task from complex query generation to categorical classification, while the templates handle the actual query construction.

## Foundational Learning

- Concept: Associative Deductive Reasoning
  - Why needed here: The approach is inspired by associative deductive reasoning to disambiguate relation types and learn SPARQL query patterns.
  - Quick check question: How does associative deductive reasoning differ from pure deductive reasoning in the context of knowledge base construction?

- Concept: Template-Based Knowledge Graph Completion
  - Why needed here: The model uses templates to simplify the generation task from complex SPARQL queries to template ID classification.
  - Quick check question: What are the advantages and disadvantages of using templates versus generating full SPARQL queries?

- Concept: Transfer Learning from Pre-trained Language Models
  - Why needed here: SHADOW leverages a pre-trained T5 model to inherit generative capabilities for structured knowledge tasks.
  - Quick check question: How does transfer learning help in situations with limited training data for specialized tasks?

## Architecture Onboarding

- Component map:
  Input -> Fine-tuned T5 small model -> Template ID output -> SPARQL template -> Wikidata query -> Completed triple

- Critical path:
  1. Input processing: Parse Wikidata triples
  2. Template ID generation: Model predicts correct template ID
  3. SPARQL query execution: Retrieve object entities using template
  4. Output formatting: Return completed triples

- Design tradeoffs:
  - Template-based approach: Reduces generation complexity but limits flexibility
  - Small training dataset: Leverages transfer learning but may not capture all relation nuances
  - Template ID classification: Simplifies task but requires accurate template design

- Failure signatures:
  - Low precision on certain relations (e.g., countryLandBordersCountry)
  - Poor recall on specific relation types (e.g., seriesHasNumberOfEpisodes)
  - Inconsistent performance across different relation categories

- First 3 experiments:
  1. Test template ID generation on a held-out validation set to verify model understanding of relation types.
  2. Execute generated SPARQL queries to validate template accuracy and completeness.
  3. Analyze performance differences across relation types to identify potential data or template design issues.

## Open Questions the Paper Calls Out

### Open Question 1
How can the performance of SHADOW on the countryLandBordersCountry relation be improved to reduce false positives from sea borders? The paper notes that the countryLandBordersCountry relation uses property P47 which includes both land and sea borders, leading to high recall but low precision. The paper identifies this as a limitation but doesn't propose solutions for filtering sea borders or finding a more precise property.

### Open Question 2
What factors contribute to SHADOW's inability to learn the seriesHasNumberOfEpisodes relation despite fine-tuning? The paper notes that this relation shows perfect precision but zero recall, suggesting SHADOW didn't learn to associate this relation with its correct template. The paper hypothesizes that the purely numerical nature of this relation might be challenging, but doesn't systematically investigate why the model failed to learn it.

### Open Question 3
How does the quality of pre-training data affect SHADOW's performance on underrepresented relations like awardWonBy? The paper notes that SHADOW performed well on the underrepresented awardWonBy relation and suggests this could be due to the quality of pre-training data. The paper only hypothesizes about pre-training data quality but doesn't empirically verify this relationship or explore what makes certain relations easier to learn.

### Open Question 4
Would allowing the model to generate SPARQL queries directly rather than using fixed templates improve performance across relation types? The paper mentions that using fixed templates sacrifices flexibility in favor of correct syntax, which may have impacted results particularly for the countryLandBordersCountry relation. The paper chose fixed templates to ensure correct syntax but acknowledges this tradeoff without testing the alternative approach.

## Limitations
- Template rigidity sacrifices flexibility and may miss nuanced relation patterns
- Small dataset (377 training triples) limits generalization to new relation types
- Significant performance variation across relation types suggests potential overfitting

## Confidence
- High Confidence: The core methodology of using template-based SPARQL generation for knowledge base completion is sound and reproducible.
- Medium Confidence: The claimed 20% improvement over baseline is credible given the evaluation setup, but external validation on different datasets is needed.
- Medium Confidence: The transfer learning mechanism is plausible but the extent to which pre-training contributes versus the small dataset learning is unclear.

## Next Checks
1. **Cross-relation Generalization Test**: Evaluate SHADOW on a held-out set of Wikidata relations not seen during training to assess whether the model learns relation-agnostic knowledge extraction patterns or merely memorizes specific template mappings.

2. **Template Flexibility Analysis**: Compare SHADOW's performance against a variant that generates full SPARQL queries without templates to quantify the semantic expressiveness trade-off and identify cases where template rigidity limits performance.

3. **Data Efficiency Study**: Train SHADOW on progressively smaller subsets of the training data to determine the minimum viable dataset size and assess whether the claimed transfer learning benefits hold under more severe data constraints.