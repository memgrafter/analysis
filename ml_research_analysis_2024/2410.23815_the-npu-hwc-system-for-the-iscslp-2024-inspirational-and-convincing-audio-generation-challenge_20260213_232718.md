---
ver: rpa2
title: The NPU-HWC System for the ISCSLP 2024 Inspirational and Convincing Audio Generation
  Challenge
arxiv_id: '2410.23815'
source_url: https://arxiv.org/abs/2410.23815
tags:
- speech
- audio
- speaker
- track
- background
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents the NPU-HWC system for the ISCSLP 2024 Inspirational
  and Convincing Audio Generation Challenge, which addresses the challenge of generating
  high-quality, realistic audio close to real-world sound. The system consists of
  two modules: a speech generator for Track 1 and a background audio generator for
  Track 2.'
---

# The NPU-HWC System for the ISCSLP 2024 Inspirational and Convincing Audio Generation Challenge

## Quick Facts
- arXiv ID: 2410.23815
- Source URL: https://arxiv.org/abs/2410.23815
- Authors: Dake Guo; Jixun Yao; Xinfa Zhu; Kangxiang Xia; Zhao Guo; Ziyu Zhang; Yao Wang; Jie Liu; Lei Xie
- Reference count: 0
- One-line primary result: Second place in Track 1 (MOS 3.63) and first place in Track 2 (MOS 3.67) of the ISCSLP 2024 Inspirational and Convincing Audio Generation Challenge

## Executive Summary
This paper presents the NPU-HWC system for the ISCSLP 2024 Inspirational and Convincing Audio Generation Challenge, addressing the challenge of generating high-quality, realistic audio close to real-world sound. The system consists of two modules: a speech generator for Track 1 and a background audio generator for Track 2. For Track 1, the authors employ Single-Codec to tokenize speech into discrete tokens and use a language-model-based approach to achieve zero-shot speaking style cloning. This approach effectively decouples timbre and speaking style at the token level, reducing the acoustic modeling burden on the autoregressive language model. Additionally, they use DSPGAN to upsample 16 kHz mel-spectrograms to high-fidelity 48 kHz waveforms. For Track 2, the authors propose a background audio generator based on large language models (LLMs), which produces scene-appropriate accompaniment descriptions, synthesizes background audio with Tango 2, and integrates it with the speech generated by their Track 1 system.

## Method Summary
The NPU-HWC system addresses two tracks of the ISCSLP 2024 challenge: Track 1 focuses on zero-shot speaker and style cloning using Single-Codec for speech tokenization and a language model-based approach, while Track 2 involves background audio generation using an LLM-based cascaded system. For Track 1, the system employs Single-Codec to convert speech into discrete tokens, which are then processed by a NanoGPT language model for speech generation, followed by DSPGAN vocoding to produce high-fidelity 48 kHz waveforms. Track 2 uses DeepSeek-V2 LLM to generate scene-appropriate accompaniment descriptions based on speech transcripts, which are then synthesized into background audio using Tango 2 and integrated with the Track 1 output. The system is trained on over 80,000 hours of Mandarin speech data and evaluated using subjective metrics including Mean Opinion Score (MOS) for quality, similarity, emotion, and background audio matching degree.

## Key Results
- Achieved second place in Track 1 with an average Mean Opinion Score (MOS) of 3.63
- Achieved first place in Track 2 with an average MOS of 3.67
- Successfully implemented zero-shot speaking style cloning using Single-Codec and language model approach
- Developed an effective LLM-based system for background audio generation that integrates with speech synthesis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-Codec effectively decouples timbre and speaking style at the token level
- Mechanism: Single-Codec uses a decoupled Vector Quantized VAE (VQ-VAE) that separates speech into timbre/time-invariant embeddings and discrete sequences related to pronunciation. This explicit separation reduces the acoustic modeling burden on the autoregressive language model.
- Core assumption: Token-level decoupling allows the language model to focus on semantic modeling while preserving speaker characteristics in separate embeddings
- Evidence anchors:
  - [abstract]: "The Single-Codec effectively decouples timbre and speaking style at the token level, reducing the acoustic modeling burden on the autoregressive language model"
  - [section 3.1.1]: "It employs a decoupled Vector Quantized VAE (VQ-VAE), segmenting the speech into timbre, acoustic environment-related time-invariant embeddings, and a discrete sequence related to pronunciation"
  - [corpus]: Weak - no direct corpus evidence supporting this specific decoupling mechanism
- Break condition: If the decoupled embeddings fail to capture sufficient speaker characteristics or if the language model cannot effectively integrate the separated information streams

### Mechanism 2
- Claim: LLM-based background audio generation creates contextually appropriate scenes through semantic understanding
- Mechanism: The system uses DeepSeek-V2 LLM to generate scene-appropriate accompaniment descriptions based on speech transcripts, then synthesizes background audio with Tango 2. This cascading approach leverages LLM's semantic understanding capabilities to bridge text-to-audio generation.
- Core assumption: LLMs can effectively understand speech content and generate relevant scene descriptions that map to appropriate background audio
- Evidence anchors:
  - [section 3.2.1]: "The goal is to generate descriptive texts of potential scenes corresponding to the speech transcripts using the semantic understanding capabilities of LLM"
  - [section 3.2]: "This system produces scene-appropriate accompaniment descriptions, synthesizes background audio with Tango 2, and integrates it with the speech generated by our Track 1 system"
  - [corpus]: Weak - no corpus evidence directly supporting the effectiveness of this specific LLM-to-audio pipeline
- Break condition: If the LLM generates irrelevant or overly complex descriptions that Tango 2 cannot effectively translate into appropriate background audio

### Mechanism 3
- Claim: DSPGAN upsampling from 16 kHz to 48 kHz significantly improves audio quality
- Mechanism: DSPGAN vocoder processes mel-spectrograms to produce high-fidelity 48 kHz waveforms, bridging the gap between low-resolution training data and high-quality audio generation requirements.
- Core assumption: The DSPGAN model trained on high-fidelity data can effectively upsample lower-resolution inputs without introducing artifacts
- Evidence anchors:
  - [abstract]: "we use DSPGAN to upsample 16 kHz mel-spectrograms to high-fidelity 48 kHz waveforms"
  - [section 3.1.3]: "We utilize DSPGAN [8] to upsample the 16 kHz mel-spectrogram to a high-fidelity 48 kHz waveform which effectively bridges the gap between the inherent limitations of the training data and the demand for high-quality speech generation"
  - [corpus]: Weak - no corpus evidence quantifying the quality improvement from upsampling
- Break condition: If upsampling introduces artifacts or fails to improve perceptual quality compared to native 48 kHz generation

## Foundational Learning

- Concept: Discrete speech tokenization and its role in speech synthesis
  - Why needed here: Understanding how Single-Codec converts continuous speech into discrete tokens that can be processed by language models
  - Quick check question: What is the advantage of using discrete tokens instead of continuous representations for speech synthesis with language models?

- Concept: Large language model prompting strategies for content understanding
  - Why needed here: The system relies on LLMs to understand speech content and generate appropriate scene descriptions for background audio
  - Quick check question: How does the prompt refinement strategy address the challenge of generating simple, relevant descriptions for abstract text?

- Concept: Latent diffusion models for audio generation
  - Why needed here: Tango 2 uses latent diffusion to generate background audio from text descriptions
  - Quick check question: What is the relationship between text prompts and the diffusion process in Tango 2's audio generation?

## Architecture Onboarding

- Component map:
  - Track 1: Text tokenizer (BPE) → Single-Codec → NanoGPT language model → DSPGAN vocoder → High-quality speech output
  - Track 2: LLM (DeepSeek-V2) → Scene description generation → Tango 2 → Background audio synthesis → Audio mixing with Track 1 output

- Critical path: Text → Speech generation (Track 1) → Background audio generation (Track 2) → Final audio output
- Design tradeoffs:
  - Single-Codec vs traditional tokenizers: Decoupling reduces language model burden but requires careful embedding integration
  - LLM-based vs direct TTA: LLM provides semantic understanding but adds complexity and potential description-generation errors
  - Upsampling vs native high-resolution: Uses available data efficiently but may introduce artifacts

- Failure signatures:
  - Speaker similarity issues: May indicate Single-Codec embedding problems or insufficient speaker representation in training data
  - Background audio mismatch: Could indicate LLM prompt issues or Tango 2 generation failures
  - Audio quality degradation: May indicate DSPGAN upsampling problems or training data limitations

- First 3 experiments:
  1. Test Single-Codec's ability to reconstruct speech from its discrete tokens using reference audio from training set
  2. Evaluate LLM's scene description generation with controlled text inputs (concrete vs abstract content)
  3. Verify DSPGAN's upsampling quality by comparing 16 kHz vs 48 kHz outputs with human listeners

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Single-Codec's decoupling of timbre and speaking style at the token level affect the long-term stability and consistency of generated speech across extended dialogues or narratives?
- Basis in paper: [explicit] The paper mentions that Single-Codec effectively decouples timbre and speaking style at the token level, reducing the acoustic modeling burden on the autoregressive language model.
- Why unresolved: The paper does not provide evidence or analysis of how this decoupling affects the consistency of speech generation over longer sequences or in more complex scenarios.
- What evidence would resolve it: Comparative analysis of speech generated using Single-Codec versus other methods over extended dialogues, focusing on consistency and stability of style and timbre.

### Open Question 2
- Question: To what extent does the DSPGAN vocoder's frequency band expansion improve the perceptual quality of generated speech, and how does this improvement vary with different input audio qualities?
- Basis in paper: [explicit] The paper describes the use of DSPGAN to upsample 16 kHz mel-spectrograms to high-fidelity 48 kHz waveforms, bridging the gap between low-resolution training data and high-quality speech generation.
- Why unresolved: While the paper claims improvement in speech quality, it does not quantify the perceptual benefits or explore variations with different input qualities.
- What evidence would resolve it: Detailed perceptual studies comparing speech quality with and without DSPGAN processing across various input audio qualities.

### Open Question 3
- Question: How does the LLM-based background audio generation system handle complex or ambiguous textual inputs, and what are the limitations in terms of scene relevance and audio-text alignment?
- Basis in paper: [explicit] The paper discusses challenges faced by the LLM in generating scene descriptions, particularly for abstract texts or those lacking concrete elements.
- Why unresolved: The paper does not provide a comprehensive analysis of the system's performance with complex or ambiguous inputs or the limitations in scene relevance.
- What evidence would resolve it: Empirical studies testing the system's performance on a diverse set of complex and ambiguous textual inputs, assessing scene relevance and alignment.

## Limitations

- Lack of corpus evidence supporting the effectiveness of Single-Codec's decoupling mechanism
- Insufficient detail on DSPGAN training methodology and its impact on audio quality
- No comprehensive analysis of LLM performance with complex or ambiguous textual inputs

## Confidence

- **High Confidence**: Overall system architecture, cascading approach for Track 2, use of established models (DeepSeek-V2, Tango 2), and clear presentation of competition results
- **Medium Confidence**: Single-Codec tokenization approach and claimed benefits for decoupling timbre and speaking style
- **Low Confidence**: Specific effectiveness of DSPGAN upsampling, LLM prompting strategy for background generation, and integration of separated embeddings

## Next Checks

1. **Single-Codec Decoupling Validation**: Conduct a controlled experiment reconstructing speech from Single-Codec tokens using reference audio from the training set, measuring both reconstruction quality and the preservation of speaker characteristics to verify the claimed decoupling mechanism.

2. **LLM Scene Description Evaluation**: Test the LLM's ability to generate appropriate scene descriptions by providing controlled text inputs with varying levels of concreteness and abstractness, then evaluate whether Tango 2 can effectively translate these descriptions into relevant background audio.

3. **DSPGAN Upsampling Quality Assessment**: Perform a comparative listening test where human evaluators rate the quality of audio generated through DSPGAN upsampling versus native 48 kHz generation, to determine if upsampling introduces artifacts or genuinely improves perceptual quality.