---
ver: rpa2
title: Spectral Graph Pruning Against Over-Squashing and Over-Smoothing
arxiv_id: '2404.04612'
source_url: https://arxiv.org/abs/2404.04612
tags: []
core_contribution: This paper addresses the problems of over-squashing and over-smoothing
  in graph neural networks by proposing a spectral graph pruning approach inspired
  by the Braess paradox. The key idea is that deleting edges can simultaneously increase
  the spectral gap (mitigating over-squashing) and slow down the rate of smoothing.
---

# Spectral Graph Pruning Against Over-Squashing and Over-Smoothing
## Quick Facts
- **arXiv ID:** 2404.04612
- **Source URL:** https://arxiv.org/abs/2404.04612
- **Reference count:** 40
- **Primary result:** Spectral graph pruning can simultaneously mitigate over-squashing and over-smoothing in GNNs, achieving state-of-the-art performance on long-range and heterophilic graph benchmarks.

## Executive Summary
This paper introduces a spectral graph pruning framework that addresses two fundamental limitations of graph neural networks: over-squashing and over-smoothing. Drawing inspiration from the Braess paradox, the authors propose that strategically deleting edges can paradoxically improve information flow by increasing the spectral gap and slowing down the smoothing process. They develop two efficient algorithms, PROXY DELETE for edge deletions and PROXYADD for edge additions, which leverage matrix perturbation theory to approximate spectral gap changes without full eigendecomposition. The approach demonstrates superior performance on challenging graph benchmarks and reveals connections to graph lottery tickets at initialization.

## Method Summary
The authors propose a spectral graph pruning approach that targets the Fiedler value (spectral gap) of the graph Laplacian to address over-squashing and over-smoothing simultaneously. They develop two algorithms: PROXY DELETE, which efficiently identifies edges to remove based on their contribution to the spectral gap using perturbation theory, and PROXYADD, which adds edges to improve connectivity. Both methods approximate the impact of edge modifications on the Fiedler value without requiring expensive eigendecompositions. The pruning process can be applied at initialization to discover lottery ticket-like sparse graph structures that maintain or improve GNN performance while reducing computational requirements.

## Key Results
- PROXY DELETE and PROXYADD outperform existing graph rewiring methods on long-range graph benchmarks and large heterophilic datasets
- Spectral pruning at initialization can discover effective graph lottery tickets with comparable or better performance than state-of-the-art methods
- The approach is particularly effective in heterophilic settings, where it deletes inter-class edges to prevent detrimental smoothing while maintaining connectivity
- PROXY ADD achieves superior performance on heterophilic datasets by strategically adding edges to improve connectivity without increasing over-smoothing

## Why This Works (Mechanism)
The proposed approach works by leveraging spectral graph theory to optimize the graph structure for GNN training. By increasing the spectral gap (Fiedler value), edge deletions mitigate over-squashing by allowing information to propagate more efficiently across distant nodes. Simultaneously, slower smoothing rates prevent node representations from becoming indistinguishable, addressing over-smoothing. The Braess paradox-inspired insight is that removing certain edges can improve overall information flow by eliminating bottlenecks. The perturbation-based approximations enable efficient identification of edges whose removal maximally increases the spectral gap, making the approach scalable to large graphs.

## Foundational Learning
- **Spectral Graph Theory:** Understanding eigenvalues of graph Laplacians is essential for grasping how graph structure affects information propagation in GNNs
  - *Why needed:* The core mechanism relies on spectral properties (Fiedler value) to quantify information flow bottlenecks
  - *Quick check:* Verify that graphs with larger spectral gaps exhibit better long-range information propagation

- **Over-squashing and Over-smoothing:** These phenomena describe how GNNs struggle with distant information and node representations become indistinguishable
  - *Why needed:* The paper directly targets these two complementary problems through graph structure optimization
  - *Quick check:* Confirm that increasing spectral gap reduces over-squashing while controlled edge deletion prevents over-smoothing

- **Matrix Perturbation Theory:** Used to efficiently approximate changes in eigenvalues when edges are added or removed
  - *Why needed:* Enables scalable computation of spectral gap changes without expensive full eigendecompositions
  - *Quick check:* Validate that perturbation approximations accurately predict actual spectral gap changes

- **Graph Lottery Tickets:** Sparse graph structures that can be identified at initialization and maintain or improve GNN performance
  - *Why needed:* The paper extends the lottery ticket hypothesis to graph structures rather than just weights
  - *Quick check:* Compare performance of pruned graphs against original dense graphs across multiple seeds

- **Heterophily in Graphs:** Settings where connected nodes tend to have different labels, contrasting with homophily
  - *Why needed:* The approach shows particular effectiveness in heterophilic settings by removing inter-class edges
  - *Quick check:* Measure performance improvement specifically on heterophilic versus homophilic datasets

- **Graph Rewiring vs Pruning:** Distinguishing between modifying existing edges versus removing edges entirely
  - *Why needed:* The paper focuses on deletion (pruning) rather than rewiring, with different theoretical implications
  - *Quick check:* Compare results of edge deletion versus edge rewiring on the same benchmark datasets

## Architecture Onboarding
- **Component map:** Graph G -> Laplacian L -> Fiedler value λ₂ -> Edge impact scores -> Edge selection (delete/add) -> Pruned graph G' -> GNN training
- **Critical path:** Spectral gap calculation → Edge impact approximation → Edge selection → Graph pruning → GNN training and evaluation
- **Design tradeoffs:** The perturbation approximation trades some accuracy for computational efficiency, enabling scalability to large graphs
- **Failure signatures:** If the perturbation approximation is inaccurate, the method may select suboptimal edges to delete/add, potentially worsening over-squashing or over-smoothing
- **First experiments:**
  1. Compute Fiedler values before and after applying PROXY DELETE on synthetic graphs with known bottlenecks
  2. Compare over-squashing metrics (e.g., receptive field capacity) between pruned and unpruned graphs
  3. Evaluate GNN performance on heterophilic datasets with and without spectral pruning

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The theoretical foundation linking edge deletions to both over-squashing and over-smoothing could be more rigorously established
- The perturbation-based approximation methods rely on assumptions about spectral properties that may not hold in all graph topologies
- Experimental validation focuses primarily on specific benchmark datasets, with generalizability to other graph types remaining uncertain

## Confidence
- **High confidence:** The core observation that spectral gap increases can mitigate over-squashing is well-supported by spectral graph theory
- **Medium confidence:** Empirical demonstrations of performance improvements over existing rewiring methods are convincing but limited in scope
- **Low confidence:** Broader claims about lottery ticket discovery and computational efficiency improvements need more rigorous validation across diverse scenarios

## Next Checks
1. Test the spectral pruning approach on additional graph types including temporal graphs, biological networks, and knowledge graphs to assess generalizability beyond the current benchmark selection
2. Conduct controlled experiments comparing PROXY DELETE/PROXYADD with other pruning methods on identical hardware to quantify actual computational savings versus claimed reductions in training time and memory usage
3. Perform ablation studies isolating the effects of spectral gap increases versus other potential benefits (like reduced parameter count) to verify that improvements stem specifically from addressing over-squashing and over-smoothing rather than general model simplification