---
ver: rpa2
title: 'ALOHa: A New Measure for Hallucination in Captioning Models'
arxiv_id: '2404.02904'
source_url: https://arxiv.org/abs/2404.02904
tags:
- object
- objects
- aloha
- hallucination
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ALOHa, a new measure for object hallucination
  in image captioning models. ALOHa leverages large language models to extract visually
  grounded objects from captions, computes semantic similarity between candidate and
  reference objects, and uses Hungarian matching to produce a final hallucination
  score.
---

# ALOHa: A New Measure for Hallucination in Captioning Models

## Quick Facts
- arXiv ID: 2404.02904
- Source URL: https://arxiv.org/abs/2404.02904
- Authors: Suzanne Petryk; David M. Chan; Anish Kachinthaya; Haodi Zou; John Canny; Joseph E. Gonzalez; Trevor Darrell
- Reference count: 40
- Primary result: ALOHa achieves 13.6% higher hallucination detection accuracy than CHAIR and 20.3% localization accuracy on HAT dataset

## Executive Summary
This paper introduces ALOHa, a new measure for detecting object hallucinations in image captioning models. ALOHa leverages large language models to extract visually grounded objects from captions, computes semantic similarity between candidate and reference objects using S-BERT embeddings, and employs Hungarian matching to produce a final hallucination score. Evaluated on the newly introduced HAT dataset, ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR, the previous state-of-the-art metric, and achieves 20.3% localization accuracy.

## Method Summary
ALOHa extracts visually grounded objects from candidate and reference captions using an LLM with in-context learning. These objects are filtered and augmented using spaCy, then represented as semantic embeddings using S-BERT. A similarity matrix is constructed between candidate and reference objects, and the Hungarian algorithm finds the optimal assignment to maximize similarity. The minimum similarity in this assignment (ALOHa score) measures the degree of hallucination. The method also produces ALOHao scores for individual objects.

## Key Results
- ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT dataset
- Achieves 20.3% localization accuracy in identifying specific hallucinated objects
- Detects 30.8% more hallucinations on nocaps-FOIL compared to CHAIR
- Maintains performance when transferring from MS COCO to out-of-domain nocaps data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using large language models for open-vocabulary object extraction enables ALOHa to generalize beyond fixed object sets.
- Mechanism: LLMs parse visually grounded objects from captions using in-context learning, avoiding the limitations of fixed vocabulary matching.
- Core assumption: LLMs can accurately identify visually grounded objects from captions without explicit supervision.
- Evidence anchors:
  - [abstract] "we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations."
  - [section 2] "we leverage the capability of zero-shot in-context learning in large language models."
  - [corpus] Weak evidence - related papers discuss LLMs for hallucination detection but don't directly validate the parsing accuracy claim.
- Break condition: If LLM parsing accuracy drops significantly on out-of-domain data or complex captions, the generalization advantage disappears.

### Mechanism 2
- Claim: Semantic similarity matching using text embeddings identifies hallucinations more accurately than exact string matching.
- Mechanism: Candidate and reference objects are embedded using S-BERT, and cosine similarity scores determine object matches, allowing fuzzy matching of semantically related concepts.
- Core assumption: Semantic embeddings capture visual similarity between objects even when surface forms differ.
- Evidence anchors:
  - [section 2] "often it is useful to understand a continuous scale of hallucination – e.g., for a reference object 'dog', hallucinating 'wolf' should be penalized less than 'potato'."
  - [section 3] "Table 1 further ablates the choice of image detections and indicates that ALOHa is robust to missing detections."
  - [corpus] Moderate evidence - S-BERT embeddings are well-established, but their effectiveness specifically for visual object similarity in captioning isn't directly validated.
- Break condition: If embeddings fail to capture visual similarity (e.g., "wolf" vs "dog" have low similarity), the fuzzy matching advantage is lost.

### Mechanism 3
- Claim: Hungarian matching optimally assigns candidate objects to reference objects based on semantic similarity.
- Mechanism: A cost matrix of cosine similarities is constructed and solved using the Hungarian algorithm to find maximum-similarity assignments.
- Core assumption: Optimal assignment based on pairwise similarities produces better hallucination detection than greedy or threshold-based methods.
- Evidence anchors:
  - [section 2] "We then use the Hungarian method (Kuhn, 1955) to find an optimal maximum-similarity assignment Mi,j between candidate and reference sets of objects."
  - [section 2] "The minimum similarity in this linear assignment (the ALOHa score) measures the degree of hallucination of the caption."
  - [corpus] No direct evidence - the paper cites Hungarian method for assignment but doesn't validate it's superior to alternatives for this specific task.
- Break condition: If the similarity matrix contains many near-equal values, the Hungarian assignment may produce arbitrary results.

## Foundational Learning

- Concept: Hungarian algorithm for optimal assignment
  - Why needed here: ALOHa needs to pair each candidate object with the most similar reference object to determine hallucination scores
  - Quick check question: What optimization problem does the Hungarian algorithm solve, and why is it appropriate for object matching?

- Concept: Cosine similarity for semantic comparison
  - Why needed here: ALOHa uses semantic embeddings to measure object similarity beyond exact string matching
  - Quick check question: How does cosine similarity differ from other distance metrics, and why is it suitable for comparing text embeddings?

- Concept: In-context learning with LLMs
  - Why needed here: ALOHa relies on LLMs to extract visually grounded objects from captions without fine-tuning
  - Quick check question: What are the key components of an effective in-context learning prompt, and how do they influence LLM output?

## Architecture Onboarding

- Component map: LLM-based object extraction → Object filtering (spaCy augmentation, conjunction handling) → Semantic embedding (S-BERT) → Hungarian matching → ALOHao and ALOHa score computation
- Critical path: Object extraction → Filtering → Matching → Scoring (each stage depends on successful completion of the previous)
- Design tradeoffs: Open vocabulary (generalization) vs. precision (risk of parsing errors); semantic similarity (robustness) vs. computational cost (embedding generation)
- Failure signatures: High parsing error rate indicates LLM extraction issues; low average precision suggests matching problems; inconsistent ALOHao scores may indicate embedding quality issues
- First 3 experiments:
  1. Validate LLM parsing accuracy on a small set of annotated captions
  2. Test semantic similarity matching on pairs of objects with known visual relationships
  3. Run Hungarian matching on a small similarity matrix to verify optimal assignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would ALOHa perform on datasets with significantly different object categories or domains, such as medical imaging or satellite imagery?
- Basis in paper: [inferred] The paper mentions that ALOHa generalizes better to out-of-domain data compared to CHAIR, but does not provide extensive testing on datasets with completely different object categories or domains.
- Why unresolved: The paper primarily focuses on generalizing from MS COCO to nocaps, which still involves everyday objects. Testing on entirely different domains would require a new dataset and potentially different parsing strategies.
- What evidence would resolve it: Evaluating ALOHa on a dataset with unique object categories, such as medical imaging or satellite imagery, and comparing its performance to other hallucination detection methods would provide insights into its generalizability.

### Open Question 2
- Question: Can ALOHa be adapted to detect non-object hallucinations, such as incorrect actions, quantities, or abstract concepts?
- Basis in paper: [explicit] The paper mentions that detecting complex hallucinations in actions, quantities, and abstract concepts remains an exciting and challenging task for future exploration.
- Why unresolved: ALOHa is designed to focus on object hallucination detection using visually grounded objects. Detecting non-object hallucinations would require a different approach, possibly involving more complex language understanding or additional modalities.
- What evidence would resolve it: Developing and evaluating an extension of ALOHa that can handle non-object hallucinations, such as incorrect actions or quantities, would demonstrate its potential to address a broader range of hallucination types.

### Open Question 3
- Question: How does the choice of language model impact ALOHa's performance, and can smaller or open-source models achieve comparable results?
- Basis in paper: [explicit] The paper explores the performance of different language models for parsing within ALOHa, finding that GPT-3.5 performs best but Koala (an open-source model) still functions with degraded performance.
- Why unresolved: The paper only compares a few language models and does not extensively explore the impact of model size, architecture, or training data on ALOHa's performance.
- What evidence would resolve it: Conducting a comprehensive study comparing ALOHa's performance with various language models, including different sizes, architectures, and training datasets, would provide insights into the trade-offs between performance and model characteristics.

## Limitations

- Heavy reliance on LLM-based object extraction without rigorous validation of parsing accuracy across diverse caption styles and domains
- Computational cost of generating embeddings for every object pair may limit scalability to large datasets
- Effectiveness of S-BERT embeddings for visual object similarity in captioning contexts isn't directly validated

## Confidence

- **High Confidence**: The Hungarian matching algorithm for optimal assignment (well-established method with clear theoretical guarantees)
- **Medium Confidence**: Semantic similarity approach using S-BERT embeddings (solid foundation but not validated for visual object comparison)
- **Low Confidence**: LLM-based open-vocabulary object extraction (lacks empirical validation and depends heavily on prompt quality)

## Next Checks

1. Conduct a detailed error analysis of LLM object extraction on a subset of HAT annotations to quantify parsing accuracy and identify failure patterns across different caption types.

2. Perform ablation studies comparing ALOHa with exact matching baselines using identical object sets to isolate the contribution of semantic similarity versus open-vocabulary extraction.

3. Test ALOHa's performance on captions with varying levels of visual complexity (simple objects vs. complex scenes) to identify domain-specific limitations and establish confidence bounds for different use cases.