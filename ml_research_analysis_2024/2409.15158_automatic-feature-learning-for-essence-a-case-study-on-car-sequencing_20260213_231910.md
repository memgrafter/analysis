---
ver: rpa2
title: 'Automatic Feature Learning for Essence: a Case Study on Car Sequencing'
arxiv_id: '2409.15158'
source_url: https://arxiv.org/abs/2409.15158
tags:
- algorithm
- features
- instance
- constraint
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an automatic feature learning approach for
  Essence, a high-level constraint modelling language. The key idea is to use a language
  model (specifically a Longformer-based neural network) to extract features directly
  from the textual representation of Essence problem instances, bypassing the need
  for low-level model translations.
---

# Automatic Feature Learning for Essence: a Case Study on Car Sequencing

## Quick Facts
- arXiv ID: 2409.15158
- Source URL: https://arxiv.org/abs/2409.15158
- Reference count: 40
- Primary result: NN-based feature learning from Essence text achieves competitive algorithm selection performance with 150x faster feature extraction vs fzn2feat

## Executive Summary
This paper presents an automatic feature learning approach for the Essence constraint modelling language that bypasses traditional low-level model translations. Using a Longformer-based neural network, the approach extracts features directly from the textual representation of Essence problem instances, achieving competitive algorithm selection performance while dramatically reducing feature extraction time. The study focuses on the car sequencing problem using a portfolio of 12 algorithms (3 models × 4 solvers), demonstrating that the learned features perform comparably to existing fzn2feat features while being significantly faster to compute (0.02s median vs 5.38s).

## Method Summary
The approach uses a Longformer language model to process raw Essence text, producing feature vectors that capture semantic meaning of problem instances. These features are then used for algorithm selection through either AutoFolio or K-means clustering. The training uses 10-fold cross-validation on 10,214 car sequencing instances, with a two-phase approach that separates feature learning from algorithm selection. The model is trained using weighted binary cross-entropy loss to predict algorithm competitiveness, avoiding the challenges of multi-class classification with imbalanced data.

## Key Results
- NN-based feature extraction: 0.02s median time vs 5.38s for fzn2feat (150x faster)
- Competitive PAR10 scores: 3.33 (NN-based) vs 3.26 (fzn2feat) on tuned AutoFolio
- Split learning approach outperforms combined neural network approach
- K-means clustering with NN features achieves best performance (PAR10: 3.04)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language model feature learning works because it captures high-level semantic structure directly from Essence specifications, avoiding the need for low-level model translation.
- Mechanism: The Longformer-based neural network tokenizes the raw Essence text and produces a feature vector that encodes the semantic meaning of the problem instance, which can then be used for algorithm selection without requiring fzn2feat feature extraction.
- Core assumption: The high-level textual representation of an Essence instance contains sufficient information for predicting algorithm performance across different model-solver combinations.
- Evidence anchors:
  - [abstract]: "Our contribution is automatic learning of instance features directly from the high-level representation of a problem instance using a language model."
  - [section 3.1]: "We adopt a language model... to learn a set of features that will be later used to select an algorithm... The input of such a model is the raw text of the Essence instance in tokenized form... and the output is a feature vector that describes the semantic meaning of the input."
  - [corpus]: Weak - no direct citations, but related papers on constraint modeling and automated feature learning suggest this is an active research area.
- Break condition: If the high-level representation loses critical information needed for algorithm selection compared to low-level FlatZinc representations.

### Mechanism 2
- Claim: Splitting feature learning and algorithm selection into two phases is more effective than combining them in a single neural network.
- Mechanism: The language model focuses solely on extracting meaningful features from text, while a separate ML-based algorithm selector (like AutoFolio or K-means) makes the final selection based on these features, avoiding the complexity of multi-class classification with imbalanced data.
- Core assumption: The feature learning task and the algorithm selection task have different optimization objectives that are better served by separate models.
- Evidence anchors:
  - [section 5.2]: "The training data is potentially highly imbalanced... This observation highlights the challenges of training a combined learning approach for the AAS task... One possible explanation... we mitigate this issue in our split learning approach by replacing the multi-class classification task with a multi-label classification task."
  - [section 5.2]: "In fact, this change allows us to train the network more effectively. As illustrated in Figure 6, the accuracy and F1 score now improve steadily during the training process."
  - [corpus]: Weak - no direct citations, but the approach aligns with common ML practice of separating feature extraction from prediction.
- Break condition: If the two-phase approach introduces significant latency or if the learned features don't generalize well across different algorithm portfolios.

### Mechanism 3
- Claim: NN-based feature extraction is significantly faster than traditional fzn2feat extraction while maintaining competitive performance.
- Mechanism: Once trained, the neural network can extract features from an Essence instance in milliseconds (0.02s median vs 5.38s for fzn2feat), though this requires GPU acceleration for the NN computation.
- Core assumption: The computational cost of running a constraint solver to extract fzn2feat features is substantially higher than running a pre-trained neural network.
- Evidence anchors:
  - [section 5.4]: "A significant advantage of the NN-based approach is the time required to extract features from an instance. It consistently took less than 0.38 seconds to produce a result, whereas fzn2feat took up to 33 seconds."
  - [section 5.4]: "Median Mean Max Min fzn2feat 6.71 5.38 33.68 0.80 NN 0.02 0.02 0.38 0.02"
  - [corpus]: Weak - no direct citations, but the speed comparison is clearly stated in the experimental results.
- Break condition: If GPU resources are unavailable or if the speed advantage is negated by the need to retrain the model for different problem domains.

## Foundational Learning

- Concept: Constraint Programming and Modeling Languages
  - Why needed here: Understanding Essence as a high-level constraint modeling language and its relationship to low-level solvers is fundamental to grasping why feature extraction from high-level specifications is valuable.
  - Quick check question: What is the difference between Essence Prime and FlatZinc representations in terms of abstraction level?

- Concept: Algorithm Selection and Portfolio Methods
  - Why needed here: The paper's core contribution is about automatically selecting the best algorithm (model-solver combination) from a portfolio, so understanding the basics of algorithm selection is essential.
  - Quick check question: What does PAR10 score measure in the context of algorithm selection?

- Concept: Neural Networks and Language Models
  - Why needed here: The feature learning approach uses a Longformer-based neural network, so understanding how language models process text and generate feature vectors is crucial.
  - Quick check question: What is the difference between softMax and sigmoid activation functions in the context of this paper's two-phase approach?

## Architecture Onboarding

- Component map:
  - Essence instance text → Longformer language model → Feature vector → (Optional) Linear layer → Probability scores → ML algorithm selector → Best algorithm recommendation
  - Alternative path: Feature vector → AutoFolio/K-means clustering → Best algorithm recommendation

- Critical path:
  1. Load and tokenize Essence instance text
  2. Pass through pre-trained Longformer model
  3. Apply linear layer (if using NN-based selector)
  4. Generate probability scores for algorithm competitiveness
  5. Feed features to ML algorithm selector
  6. Return selected algorithm

- Design tradeoffs:
  - GPU vs CPU: NN-based feature extraction requires GPU for speed, while fzn2feat works on CPU but is slower
  - Single vs split learning: Combining feature learning and selection in one NN is simpler but harder to train; splitting them is more effective but requires two models
  - High vs low dimensional features: NN-based features are high-dimensional (783 features) which can cause issues with classical ML models like random forests

- Failure signatures:
  - Training stagnation in combined learning approach (as observed in Figure 5)
  - High variance in tuned AutoFolio performance with NN-based features
  - GPU dependency causing performance degradation on CPU-only systems

- First 3 experiments:
  1. Compare feature extraction time: Run the NN-based feature extraction on a small set of Essence instances and measure the time per instance, comparing it to fzn2feat extraction time.
  2. Test split vs combined learning: Train both the combined NN approach and the split approach on a small subset of the dataset and compare their PAR10 scores on a validation set.
  3. Validate feature effectiveness: Use the extracted features with both AutoFolio and K-means clustering on a small dataset to see if they can identify the best algorithm with better than random accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does splitting feature learning and algorithm selection into two phases consistently outperform combining them in a single neural network across different problem domains and constraint modelling languages?
- Basis in paper: [explicit] The paper demonstrates that splitting feature learning and algorithm selection into two phases is more effective than combining them in a single neural network for the Essence car sequencing problem
- Why unresolved: The experiments only tested this on one specific problem (car sequencing) and one constraint modelling language (Essence). The conclusion may not generalize to other problem domains or languages.
- What evidence would resolve it: Comparative experiments applying both approaches (split vs combined) to a diverse range of constraint problems across multiple constraint modelling languages.

### Open Question 2
- Question: How can the instability of tuned AutoFolio with high-dimensional neural network features be mitigated to achieve consistent performance improvements?
- Basis in paper: [explicit] The paper observes that tuned AutoFolio shows high variance in performance when using the high-dimensional (783 features) neural network features, unlike with the 95-feature fzn2feat set
- Why unresolved: The paper only identifies the problem but does not test solutions. The proposed approaches (NN-based selector or feature compression) were not implemented and evaluated.
- What evidence would resolve it: Experimental results showing the performance of an NN-based selector versus tuned AutoFolio, or showing the impact of feature compression techniques on the stability and performance of tuned AutoFolio.

### Open Question 3
- Question: Can the feature extraction cost advantage of neural network features be maintained when computational resources are limited (e.g., CPU-only environments)?
- Basis in paper: [explicit] The paper shows neural network feature extraction is significantly faster (0.02s median) than fzn2feat (5.38s median) but only when using a GPU
- Why unresolved: The paper acknowledges that the speed advantage depends on GPU availability but does not explore how the approach performs without GPU acceleration
- What evidence would resolve it: Comparative experiments measuring feature extraction times for both methods on CPU-only systems, and potentially exploring lightweight neural network architectures optimized for CPU inference

### Open Question 4
- Question: What is the optimal neural network architecture and training schedule for learning features from high-level constraint models that generalizes across different solver portfolios?
- Basis in paper: [inferred] The paper uses a specific architecture (Longformer) and training schedule but notes this was based on small manual tuning, suggesting room for optimization
- Why unresolved: The paper does not systematically explore the hyperparameter space or compare alternative architectures beyond the chosen Longformer model
- What evidence would resolve it: Systematic ablation studies testing different neural network architectures, training schedules, and hyperparameters across multiple constraint problems and solver portfolios to identify optimal configurations

## Limitations
- The approach requires GPU acceleration for feature extraction, limiting deployment in resource-constrained environments
- Performance results are based on a single problem domain (car sequencing), raising questions about generalizability
- High-dimensional features (783 vs 95) cause instability in AutoFolio tuning with high variance in performance

## Confidence
**High Confidence**: The mechanism of faster feature extraction through pre-trained neural networks is well-supported by the experimental results showing median extraction time of 0.02s versus 5.38s for fzn2feat. The split learning approach showing better performance than combined learning is also well-demonstrated through training curve analysis.

**Medium Confidence**: The claim that NN-based features are "competitive" with fzn2feat features is supported by the results, but the PAR10 score differences, while small, suggest that fzn2feat may still have an edge in some cases. The computational cost savings are clear, but the practical implications for different deployment scenarios need further investigation.

**Low Confidence**: The generalizability of the approach to other constraint programming domains beyond car sequencing remains unproven. The impact of high-dimensional features (783 vs 95) on different algorithm selectors is not fully characterized.

## Next Checks
1. **Domain Generalization Test**: Apply the trained feature learning model to a different constraint programming problem domain (e.g., nurse rostering or scheduling) and evaluate whether the features remain predictive of algorithm performance.

2. **GPU-Free Performance Evaluation**: Benchmark the feature extraction speed and quality on CPU-only systems to quantify the practical impact of the GPU requirement and explore potential optimizations for CPU deployment.

3. **Feature Dimensionality Impact Study**: Systematically vary the number of features retained from the neural network output and measure the impact on algorithm selector performance, particularly focusing on the variance observed with AutoFolio.