---
ver: rpa2
title: Method of data forward generation with partial differential equations for machine
  learning modeling in fluid mechanics
arxiv_id: '2501.03300'
source_url: https://arxiv.org/abs/2501.03300
tags:
- data
- field
- generated
- equations
- boundary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a high-efficiency data forward generation method
  from PDEs to address the critical challenge of expensive or inaccessible high-fidelity
  datasets for AI in fluid mechanics. The method generates flow field solutions following
  either Gaussian random fields (GRF) or physical laws (spectra), then computes source
  terms, boundary conditions, and initial conditions to satisfy PDEs, forming training
  data pairs.
---

# Method of data forward generation with partial differential equations for machine learning modeling in fluid mechanics

## Quick Facts
- arXiv ID: 2501.03300
- Source URL: https://arxiv.org/abs/2501.03300
- Reference count: 21
- Key outcome: This study proposes a high-efficiency data forward generation method from PDEs to address the critical challenge of expensive or inaccessible high-fidelity datasets for AI in fluid mechanics. The method generates flow field solutions following either Gaussian random fields (GRF) or physical laws (spectra), then computes source terms, boundary conditions, and initial conditions to satisfy PDEs, forming training data pairs. Two neural networks—Poisson-NN embedded in projection methods and WTCNN embedded in multigrid simulations—are proposed for solving incompressible Navier-Stokes equations. Results show that generated data enables training models with excellent generalization and accuracy without DNS data. Spectra-constrained data significantly outperforms GRF-based data, improving convergence rates and accuracy. For instance, WTCNN-MG reduces relative residuals to 10⁻⁶ in about 50 iterations, achieving 10.33× speedup over traditional MG methods.

## Executive Summary
This paper addresses the fundamental challenge of expensive or inaccessible high-fidelity datasets for training AI models in fluid mechanics. The authors propose a novel data forward generation method that creates training data by first generating plausible flow field solutions (either through Gaussian random fields or spectral constraints) and then computing the corresponding source terms, boundary conditions, and initial conditions to satisfy partial differential equations. This approach eliminates the need for costly direct numerical simulations (DNS) while ensuring physical consistency. The method is demonstrated by training two neural network architectures—Poisson-NN embedded in projection methods and WTCNN embedded in multigrid simulations—for solving incompressible Navier-Stokes equations, achieving excellent generalization and significant computational speedup.

## Method Summary
The method generates flow field solutions either following Gaussian random fields (GRF) or physical laws (spectra), then computes source terms, boundary conditions, and initial conditions to satisfy PDEs, forming training data pairs. Two neural networks are proposed: Poisson-NN embedded in projection methods and WTCNN embedded in multigrid simulations, both for solving incompressible Navier-Stokes equations. The forward generation approach creates data pairs of PDE solutions and their corresponding source terms, boundary conditions, and initial conditions, enabling training without expensive DNS data.

## Key Results
- Generated data enables training models with excellent generalization and accuracy without DNS data
- Spectra-constrained data significantly outperforms GRF-based data, improving convergence rates and accuracy
- WTCNN-MG reduces relative residuals to 10⁻⁶ in about 50 iterations, achieving 10.33× speedup over traditional MG methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forward generation creates training data by solving PDEs in reverse: generate solution first, then compute source terms, boundary conditions, and initial conditions.
- Mechanism: Instead of solving PDEs forward to obtain flow field data, the method samples plausible flow fields from statistical distributions (GRF) or spectral constraints, then uses the PDEs themselves to calculate the corresponding source terms, boundary conditions, and initial conditions. This ensures the generated data is physically consistent.
- Core assumption: The generated flow field solutions, whether from GRF or spectra, are statistically representative enough of real flow fields to train generalizable models.
- Evidence anchors:
  - [abstract] "the solutions of the PDEs are first generated either following a random field (e.g. Gaussian random field , GRF... ) or physical laws (e.g. a kind of spectra... ) then the source terms, boundary conditions and initial conditions are computed to satisfy PDEs"
  - [section] "Our way of thinking is to forward generate flow field solution, then the source terms, boundary conditions and initial conditions are obtained via PDEs, forming data pairs"
- Break condition: If the statistical distribution or spectral constraints used to generate the flow fields do not capture the essential physics of the target flow regime, the resulting training data will not generalize well.

### Mechanism 2
- Claim: Spectral-constraint-based data generation outperforms GRF-based generation in accuracy and convergence.
- Mechanism: Flow fields generated using spectral constraints (following a target energy spectrum like k^-7/3) naturally embed the correct statistical properties in frequency space. This ensures that the training data reflects the underlying physics more accurately than GRF-based data, which can overestimate energy across scales due to wide sampling of kernel parameters.
- Core assumption: The spectral distribution of the flow field is a critical feature for learning accurate PDE solvers.
- Evidence anchors:
  - [section] "The spectrum of pGRF exhibits higher energy, indicating that the GRF-based method tends to overestimate the energy across scales... In contrast, pspectrum aligns closely with the reference spectrum... indicating that the spectral-constrained method effectively captures the target energy distribution"
  - [section] "The results indicate that even without any DNS data, the generated data can train these two models with excellent generalization and accuracy. The data following spectrum can significantly improve the convergence rate, generalization and accuracy than that generated following GRF."
- Break condition: If the assumed target spectrum does not match the actual flow physics (e.g., different flow regimes), spectral-constraint data generation may not provide benefits.

### Mechanism 3
- Claim: Embedding neural networks into traditional numerical solvers (projection method, multigrid) accelerates convergence by providing accurate initial guesses or error corrections.
- Mechanism: Neural networks trained on the generated data learn to approximate complex PDE solutions or error corrections. When embedded into solvers, they provide highly accurate initial approximations (e.g., for BiCGSTAB solver) or perform low-frequency error correction (e.g., in WTCNN-MG), dramatically reducing the number of iterations needed for convergence.
- Core assumption: The neural networks can learn the underlying solution structure well enough from the generated data to provide useful approximations.
- Evidence anchors:
  - [section] "it serves as the initial approximation very close to exact solution for the regular Biconjugate gradient stabilized (BiCGSTAB) solver, which can significantly reduce the number of iterations required and remarkably accelerate the overall solution process"
  - [section] "WTCNN-MG has the fastest convergence performance, reducing the relative residual to 10−5 in about 20 iterations and to 10−6 in about 50 iterations, significantly outperforming other models"
- Break condition: If the neural network architecture is not expressive enough to capture the solution structure, or if the generated data is not representative, the embedded networks will not provide accurate approximations.

## Foundational Learning

- Concept: Gaussian Random Fields (GRF) and their generation using Fast Fourier Transform.
  - Why needed here: GRF is one method used to generate flow field solutions with statistical properties, forming the basis of one type of training data.
  - Quick check question: How does the choice of kernel function (e.g., Matern) and its parameters (correlation length, smoothness) affect the statistical properties of the generated flow field?

- Concept: Spectral representation of turbulent flows and energy spectra.
  - Why needed here: Spectral-constraint data generation relies on generating flow fields that follow a target energy spectrum, which is fundamental to turbulent flow physics.
  - Quick check question: What is the physical significance of the k^-7/3 spectrum in turbulent flows, and why is it used as a reference spectrum in this work?

- Concept: Multigrid methods and their components (smoothing, restriction, prolongation).
  - Why needed here: The study embeds a wavelet transform CNN into a multigrid solver, so understanding the multigrid algorithm is crucial.
  - Quick check question: What is the role of smoothing in multigrid methods, and why might a learned smoothing operator be more efficient than a classical one?

## Architecture Onboarding

- Component map: GRF generator (FFT-based) -> Spectra generator -> Compute source terms/BCs/ICs -> Form data pairs -> Train Poisson-NN/WTCNN -> Embed in solvers
- Critical path: Generate flow field solution → Compute source terms/BCs/ICs → Form data pairs → Train neural networks → Embed networks into numerical solvers → Accelerate convergence
- Design tradeoffs:
  - GRF vs Spectra: GRF is computationally cheaper (O(NlogN)) but may not capture physical spectra accurately; Spectra is more expensive (O(N*M)) but provides physically consistent data
  - Network Architecture: Simple MLPs might be cheaper to train but less expressive; Complex CNNs/Wavelet networks are more expressive but require more data and compute
- Failure signatures:
  - Poor generalization: Model performs well on training data but poorly on unseen flows
  - Divergence: Solver fails to converge with neural network embedded
  - Overfitting: Model memorizes training data (e.g., CNN-MG stagnating on certain spectra)
- First 3 experiments:
  1. Generate a small dataset (e.g., 100 samples) using GRF with different kernel parameters, train a simple network to solve the Poisson equation, and measure convergence speed vs. traditional solver
  2. Repeat experiment 1 with spectra-constraint data generation and compare performance
  3. Train a network to predict the source term of the Poisson equation given a pressure field, and evaluate the accuracy of the predicted source term against the true source term computed from the PDE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed data generation methods scale to higher Reynolds numbers and more complex flow geometries beyond the 2D Kolmogorov flow and 3D isotropic turbulence cases studied?
- Basis in paper: [inferred] The paper demonstrates the method on relatively simple canonical flows, but acknowledges the need for future studies on anisotropic turbulent flows with separation, attachment, and transition phenomena.
- Why unresolved: The current validation is limited to specific canonical flows. Real-world applications often involve higher Reynolds numbers, complex geometries, and additional physical phenomena not captured in these test cases.
- What evidence would resolve it: Systematic validation across a range of Reynolds numbers, different geometries (airfoils, pipes, complex domains), and flow regimes (laminar-turbulent transition, separated flows, etc.) with quantitative comparisons to DNS data.

### Open Question 2
- Question: What is the impact of the choice of wavelet transform in WTCNN-MG on its performance compared to other transform bases or filter designs?
- Basis in paper: [explicit] The paper uses discrete wavelet transforms in the WTCNN-MG framework, but notes this as one possible design choice among others.
- Why unresolved: While wavelets are shown to work well, the paper doesn't explore alternative bases (e.g., Fourier, curvelets) or different wavelet families, leaving uncertainty about whether the specific choice is optimal.
- What evidence would resolve it: Comparative studies of WTCNN-MG performance using different transform bases, wavelet families (Daubechies, Coiflets, etc.), or filter designs, with systematic analysis of how these choices affect convergence and generalization.

### Open Question 3
- Question: How sensitive are the Poisson-NN and WTCNN models to hyperparameter choices, and what are the optimal training strategies for different types of flow problems?
- Basis in paper: [inferred] The paper uses specific network architectures and training procedures, but doesn't systematically explore the hyperparameter space or provide guidelines for different flow regimes.
- Why unresolved: The reported results depend on specific choices of network depth, learning rates, training epochs, and other hyperparameters. The sensitivity to these choices and their optimal values for different problems remains unexplored.
- What evidence would resolve it: Comprehensive hyperparameter sensitivity studies, including network architecture search, learning rate schedules, batch sizes, and training duration. Guidelines or adaptive methods for selecting optimal hyperparameters based on flow characteristics would be particularly valuable.

## Limitations

- The method's generalization to higher Reynolds numbers and complex flow geometries beyond the tested canonical flows remains unproven
- The O(NM) computational complexity for spectra generation may become prohibitive for high-resolution 3D problems
- The assumption that PDE-satisfying data pairs are sufficient for training generalizable solvers may not hold for flows with strong non-linearities or transient phenomena

## Confidence

- High Confidence: The fundamental concept of forward generation (generating solutions first, then computing source terms) is mathematically sound and clearly demonstrated in the methodology section. The embedding of neural networks into traditional solvers (Poisson-NN in projection, WTCNN in multigrid) follows established practices.
- Medium Confidence: The comparative advantage of spectra-constraint over GRF-based generation is supported by quantitative results showing better alignment with reference spectra and improved convergence rates. However, the generality of this advantage across different flow types requires further validation.
- Medium Confidence: The reported speedup factors (10.33× for WTCNN-MG) are based on specific test cases and numerical settings. The dependency on problem size, Reynolds number, and grid resolution is not fully characterized.

## Next Checks

1. Test generalization across flow regimes: Generate data for different Reynolds numbers and flow types (e.g., laminar vs turbulent), then evaluate whether models trained on one regime maintain accuracy on another.

2. Characterize scaling behavior: Systematically vary grid resolution and problem size to measure how the O(NM) computational cost of spectra generation impacts practical feasibility for 3D problems.

3. Ablation study on network components: Remove the neural network from the embedded solvers and compare convergence rates to isolate the contribution of learned components versus the underlying numerical method.