---
ver: rpa2
title: Does Self-Attention Need Separate Weights in Transformers?
arxiv_id: '2412.00359'
source_url: https://arxiv.org/abs/2412.00359
tags:
- shared
- weight
- arxiv
- attention
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a shared weight self-attention mechanism
  for BERT models that uses a single weight matrix for Keys, Queries, and Values instead
  of separate matrices. This approach reduces the parameter count by 66.53% in the
  attention block and 12.94% in the total BERT model while maintaining competitive
  performance across GLUE benchmark tasks.
---

# Does Self-Attention Need Separate Weights in Transformers?

## Quick Facts
- arXiv ID: 2412.00359
- Source URL: https://arxiv.org/abs/2412.00359
- Reference count: 8
- 66.53% reduction in attention block parameters, 12.94% reduction in total BERT parameters while maintaining competitive performance

## Executive Summary
This paper proposes a shared weight self-attention mechanism for BERT models that replaces three separate weight matrices (for Keys, Queries, and Values) with a single shared weight matrix plus diagonal scaling matrices. The approach achieves a 66.53% reduction in attention block parameters and 12.94% reduction in total BERT parameters while maintaining competitive performance on GLUE benchmark tasks. The method also demonstrates improved generalization on noisy and out-of-domain data compared to traditional self-attention approaches, with accuracy improvements of 0.38%, 5.81%, and 1.06% over standard, symmetric, and pairwise attention-based BERT models respectively.

## Method Summary
The method replaces traditional self-attention's three separate weight matrices (Wq, Wk, Wv) with a single shared weight matrix Ws and three diagonal scaling matrices (Dq, Dk, Dv). The shared transformation S = XWs creates a base representation that is then modulated by the diagonal matrices to produce Q = SDq, K = SDk, V = SDv. This factorization reduces parameters from 3d² to d² + 3d, achieving a 66.53% reduction in attention block parameters. The model is pre-trained on BooksCorpus and English Wikipedia for 20 epochs, then fine-tuned on GLUE benchmark tasks with specific hyperparameters per task.

## Key Results
- 66.53% reduction in attention block parameters and 12.94% reduction in total BERT parameters
- Accuracy improvements of 0.38%, 5.81%, and 1.06% over standard, symmetric, and pairwise attention-based BERT models on GLUE tasks
- 15-20% faster training speed compared to traditional self-attention
- Better generalization on noisy and out-of-domain data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single shared weight matrix with diagonal scaling matrices can capture the same representational power as three separate weight matrices.
- Mechanism: The shared weight matrix Ws acts as a base representation S = XWs. Diagonal matrices Dq, Dk, Dv modulate S to produce Q = SDq, K = SDk, V = SDv, effectively factoring the original weight matrices.
- Core assumption: Essential features for semantic understanding can be captured in a single shared representation, with diagonal scaling providing sufficient differentiation.
- Evidence anchors: Abstract states the shared weight matrix enables efficient feature capture without overhead of managing multiple matrices.

### Mechanism 2
- Claim: Parameter reduction through shared weights improves computational efficiency without sacrificing performance.
- Mechanism: Using a single weight matrix Ws instead of three separate matrices reduces parameters from 3d² to d² + 3d.
- Core assumption: The relationship between parameter count and performance is not strictly linear.
- Evidence anchors: Abstract notes cuts training parameters by more than half and significantly reduces training time.

### Mechanism 3
- Claim: Shared weight self-attention provides better generalization on noisy and out-of-domain data.
- Mechanism: The regularization effect of sharing weights creates a more robust representation that is less sensitive to noise and domain shifts.
- Core assumption: Shared weights act as implicit regularization that improves model's ability to handle perturbations.
- Evidence anchors: Abstract states the method achieves strong performance on GLUE benchmark tasks, even outperforming standard BERT baseline in handling noisy and out-of-domain data.

## Foundational Learning

- Concept: Self-attention mechanism in Transformers
  - Why needed here: Understanding traditional self-attention is essential to grasp why shared weight self-attention is a simplification.
  - Quick check question: How does traditional self-attention compute attention scores using Keys, Queries, and Values?

- Concept: Matrix factorization and dimensionality reduction
  - Why needed here: The shared weight approach relies on factorizing three weight matrices into a shared base matrix and diagonal scaling matrices.
  - Quick check question: How does the factorization Wq = WsDq relate to traditional weight matrix approach in terms of parameter count?

- Concept: Regularization techniques in neural networks
  - Why needed here: The shared weight mechanism acts as a form of regularization, key to understanding its improved generalization properties.
  - Quick check question: How does parameter sharing in neural networks act as a regularization technique?

## Architecture Onboarding

- Component map:
  Input embeddings X -> Shared transformation S = XWs -> Diagonal scaling to produce Q = SDq, K = SDk, V = SDv -> Attention computation: softmax(QKT/√d)V -> Output processing

- Critical path:
  1. Input embeddings X
  2. Shared transformation S = XWs
  3. Diagonal scaling to produce Q, K, V
  4. Attention computation: softmax(QKT/√d)V
  5. Output processing through BERT layers

- Design tradeoffs:
  - Parameter reduction vs. potential loss of expressiveness
  - Simplified implementation vs. potential optimization challenges
  - Improved generalization vs. possible underfitting on complex tasks

- Failure signatures:
  - Performance degradation on tasks requiring highly specialized attention patterns
  - Inability to capture long-range dependencies effectively
  - Over-regularization leading to underfitting on clean, in-domain data

- First 3 experiments:
  1. Replace standard self-attention with shared weight self-attention in a small BERT model and compare GLUE performance
  2. Measure parameter reduction and training time improvements
  3. Test robustness on noisy versions of GLUE datasets to verify generalization claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the shared weight self-attention mechanism maintain its performance advantages when scaled to larger models beyond 100 million parameters?
- Basis in paper: Authors note improved training efficiency was only observed for smaller BERT-like models with approximately 100 million parameters.
- Why unresolved: Authors explicitly state scalability to much larger models requires further analysis and was not tested.
- What evidence would resolve it: Experimental results demonstrating performance in large-scale models like BERT-large or GPT-2.

### Open Question 2
- Question: How does the shared weight self-attention mechanism perform on decoder-based models or pure language modeling tasks?
- Basis in paper: Authors explicitly state results don't necessarily translate to decoder models or pure language modeling tasks.
- Why unresolved: Paper focuses specifically on encoder-based models and doesn't test on decoder architectures.
- What evidence would resolve it: Comparative studies evaluating shared weight self-attention in decoder-only models and autoregressive language modeling tasks.

### Open Question 3
- Question: What alternative strategies could improve the shared weight self-attention mechanism's performance on more complex datasets?
- Basis in paper: Authors note reliance on single softmax weight may not be optimal for more complex datasets.
- Why unresolved: While authors identify this limitation, they don't explore specific alternative strategies.
- What evidence would resolve it: Implementation and evaluation of modified versions using multiple softmax weights or adaptive weighting schemes.

## Limitations

- Claims about parameter reduction and performance improvements are based on limited empirical validation without direct comparisons to other parameter-efficient transformer variants.
- Mechanism for how diagonal scaling matrices provide sufficient differentiation is theoretically proposed but not rigorously proven.
- Claim of improved generalization on noisy and out-of-domain data lacks systematic experimentation across diverse datasets.
- Absence of ablation studies makes it difficult to isolate specific contributions of shared weights versus diagonal scaling.

## Confidence

- High confidence: Parameter reduction claims (66.53% in attention block, 12.94% in total BERT parameters) are mathematically straightforward and verifiable.
- Medium confidence: Performance claims on GLUE tasks are supported by reported results but lack statistical significance testing and comparison against recent strong baselines.
- Low confidence: Claim of better generalization on noisy and out-of-domain data is supported only by qualitative statements without rigorous experimental validation.

## Next Checks

1. Implement the shared weight attention mechanism and verify the claimed parameter reduction through systematic counting of learnable parameters.

2. Conduct controlled experiments comparing the shared weight model against standard BERT baseline and recent parameter-efficient transformer variants on GLUE tasks, including statistical significance testing.

3. Systematically evaluate robustness to noise and domain shift by testing the model on corrupted versions of GLUE datasets with various noise types and intensities.