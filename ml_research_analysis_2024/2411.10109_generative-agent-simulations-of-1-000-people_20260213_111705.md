---
ver: rpa2
title: Generative Agent Simulations of 1,000 People
arxiv_id: '2411.10109'
source_url: https://arxiv.org/abs/2411.10109
tags:
- affiliation
- correl
- page
- what
- p-value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a generative agent architecture that simulates
  the attitudes and behaviors of over 1,000 real individuals by leveraging qualitative
  interviews and large language models. The agents accurately predict participants'
  responses on the General Social Survey (85% normalized accuracy), Big Five personality
  traits (80% normalized correlation), and economic games (66% normalized correlation).
---

# Generative Agent Simulations of 1,000 People

## Quick Facts
- arXiv ID: 2411.10109
- Source URL: https://arxiv.org/abs/2411.10109
- Reference count: 40
- One-line primary result: Generative agents accurately simulate individual attitudes and behaviors, achieving 85% GSS accuracy and 80% Big Five correlation

## Executive Summary
This paper presents a generative agent architecture that simulates the attitudes and behaviors of over 1,000 real individuals by leveraging qualitative interviews and large language models. The agents accurately predict participants' responses on the General Social Survey (85% normalized accuracy), Big Five personality traits (80% normalized correlation), and economic games (66% normalized correlation). They also replicate experimental treatment effects comparably to human participants. The approach reduces accuracy biases across racial and ideological groups compared to agents using demographic descriptions. This work demonstrates the potential of generative agents to serve as a tool for investigating individual and collective behavior across various social science domains.

## Method Summary
The researchers collected 1,052 individual interview transcripts averaging 6,491 words through AI-conducted semi-structured interviews. These transcripts were used to create generative agents by conditioning large language models with an expert reflection module. The agents completed various tasks including the General Social Survey, Big Five personality assessments, economic games, and replication studies of experimental treatments. Performance was evaluated against participants' actual responses using normalized accuracy metrics and compared to baseline agents using demographic descriptions or persona summaries.

## Key Results
- Generative agents achieved 85% normalized accuracy predicting GSS survey responses
- Agents demonstrated 80% normalized correlation for Big Five personality trait predictions
- Agents replicated experimental treatment effects with 99% normalized correlation compared to human participants
- Interview-based agents reduced demographic bias compared to demographic-based agents across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative agents achieve high predictive accuracy by conditioning large language models on rich, individualized interview transcripts rather than relying on demographics or persona descriptions.
- Mechanism: The interview transcripts provide nuanced, context-rich information about an individual's life experiences, beliefs, and behaviors, which the language model uses to generate responses that closely match the individual's actual survey and experimental responses.
- Core assumption: Language models can effectively extract and utilize complex patterns from lengthy interview transcripts to simulate individual behavior across diverse tasks.
- Evidence anchors:
  - [abstract] The agents replicate participants' responses on the General Social Survey with 85% normalized accuracy and Big Five personality traits with 80% normalized correlation, significantly outperforming agents using demographic descriptions.
  - [section] "The generative agents achieved a normalized correlation of 0.80 (std=1.88), based on a raw correlation of r = 0.78 (std=0.70) divided by participants' replication correlation of 0.95 (std=0.76)."
  - [corpus] Weak - the corpus papers focus on agent-based simulations but don't directly address the use of qualitative interviews as training data for generative agents.
- Break condition: If language models cannot effectively process and extract relevant information from lengthy interview transcripts, or if the interview data is not sufficiently representative of the individual's attitudes and behaviors.

### Mechanism 2
- Claim: The agent architecture reduces accuracy biases across racial and ideological groups compared to agents using demographic descriptions.
- Mechanism: By using individualized interview data instead of demographic attributes, the agents avoid stereotyping and overgeneralization that can occur when relying on group-level characteristics.
- Core assumption: Demographic attributes are insufficient to accurately predict individual attitudes and behaviors, and can lead to biased predictions.
- Evidence anchors:
  - [abstract] "Our architecture reduces accuracy biases across racial and ideological groups compared to agents given demographic descriptions."
  - [section] "Interview-based agents consistently reduced biases across tasks compared to demographic-based agents. For political ideology, we observed that in the GSS, the DPD dropped from 12.35% for demographic-based generative agents to 7.85% for interview-based generative agents."
  - [corpus] Weak - the corpus papers mention bias in LLM simulations but don't specifically address the use of interview data to reduce bias.
- Break condition: If demographic attributes are sufficient to accurately predict individual attitudes and behaviors, or if interview data introduces its own biases.

### Mechanism 3
- Claim: The agent architecture can predict population-level treatment effects in experimental replications comparably to human participants.
- Mechanism: The agents, when aggregated, exhibit responses to experimental interventions that mirror the patterns observed in human samples, indicating that the architecture captures relevant behavioral mechanisms.
- Core assumption: The individual-level predictive accuracy of the agents extends to capturing aggregate behavioral responses to interventions.
- Evidence anchors:
  - [abstract] "They also replicate experimental treatment effects comparably to human participants."
  - [section] "The effects sizes estimated from the generative agents were highly correlated with those of the participants (r=0.98), compared to the participants' internal consistency correlation of 0.99, yielding a normalized correlation of 0.99."
  - [corpus] Weak - the corpus papers focus on agent-based simulations but don't specifically address the prediction of experimental treatment effects.
- Break condition: If the individual-level predictive accuracy of the agents does not extend to capturing aggregate behavioral responses to interventions, or if the experimental conditions are not adequately represented in the interview data.

## Foundational Learning

- Concept: Natural Language Processing and Language Models
  - Why needed here: The agent architecture relies on large language models to process interview transcripts and generate responses.
  - Quick check question: Can you explain how language models like GPT-4 are trained and how they generate text based on input prompts?

- Concept: Agent-Based Modeling and Simulation
  - Why needed here: The paper presents a generative agent architecture for simulating human behavior, which builds on the principles of agent-based modeling.
  - Quick check question: What are the key components of an agent-based model, and how do they interact to produce emergent behavior?

- Concept: Social Science Research Methods
  - Why needed here: The paper evaluates the agent architecture using established social science constructs like the General Social Survey and Big Five personality traits.
  - Quick check question: What are the strengths and limitations of using surveys and experiments to study human attitudes and behaviors?

## Architecture Onboarding

- Component map:
  - AI interviewer -> Interview transcripts -> LLM with expert reflection -> Survey/experiment responses

- Critical path:
  1. Conduct interviews with participants to generate transcripts
  2. Create generative agents by conditioning language models on interview transcripts
  3. Have agents complete surveys and experiments
  4. Compare agents' responses to participants' responses to assess predictive accuracy

- Design tradeoffs:
  - Interview length vs. information richness: Longer interviews provide more data but increase cost and participant burden
  - Model complexity vs. interpretability: More complex models may capture nuanced patterns but are harder to interpret and debug
  - Data privacy vs. research utility: Sharing individual-level data enables more research but raises privacy concerns

- Failure signatures:
  - Low predictive accuracy: Agents' responses do not match participants' responses on surveys and experiments
  - High bias: Agents exhibit systematic errors in predicting responses for certain demographic groups
  - Poor generalizability: Agents perform well on some tasks but poorly on others, indicating limited robustness

- First 3 experiments:
  1. Test the predictive accuracy of agents created from shorter interview transcripts to assess the impact of interview length
  2. Compare the performance of agents using different language models (e.g., GPT-4 vs. GPT-3.5) to assess the impact of model choice
  3. Evaluate the agents' ability to predict responses to new, unseen survey questions to assess generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the predictive accuracy of generative agents change when using longer interview transcripts versus shorter ones?
- Basis in paper: [explicit] The paper mentions that random lesion experiments showed a linear decline in performance as portions of the interview were removed, with accuracy dropping from 0.85 to 0.79 when 80% of the utterances were excluded.
- Why unresolved: The paper does not provide a detailed analysis of how different lengths of interview transcripts affect predictive accuracy.
- What evidence would resolve it: Conducting experiments with varying lengths of interview transcripts to measure the change in predictive accuracy would provide evidence on the optimal interview length for generative agents.

### Open Question 2
- Question: To what extent do linguistic features in interview transcripts contribute to the predictive power of generative agents?
- Basis in paper: [explicit] The paper notes that summary agents, which convert interview transcripts into bullet-pointed summaries, performed slightly below full interview-based agents, suggesting that some information may be lost from linguistic cues during summarization.
- Why unresolved: The paper does not quantify the specific contribution of linguistic features to predictive accuracy.
- What evidence would resolve it: Comparative studies analyzing the predictive performance of agents using full transcripts, summarized content, and linguistically annotated data would clarify the role of linguistic features.

### Open Question 3
- Question: How do generative agents perform in predicting behaviors and attitudes in domains not covered by the current evaluation metrics?
- Basis in paper: [inferred] The current evaluation focuses on the General Social Survey, Big Five personality traits, economic games, and experimental studies. The paper does not explore performance in other domains.
- Why unresolved: The paper does not assess the generalizability of generative agents to other social science domains.
- What evidence would resolve it: Applying generative agents to predict behaviors and attitudes in additional domains, such as health behaviors or environmental attitudes, would test their broader applicability.

## Limitations
- The study relies on a single interview per participant, which may not capture the full complexity of individual attitudes and behaviors
- The evaluation focuses on self-reported survey responses and controlled experimental tasks, which may not fully represent real-world behavior
- The LLM architecture details are not fully specified, particularly the expert reflection module implementation, making exact replication challenging

## Confidence

- **High confidence**: The core finding that interview-based agents outperform demographic-based agents for individual prediction across multiple tasks
- **Medium confidence**: The bias reduction claims and the generalizability of results to other social science domains
- **Medium confidence**: The architecture's ability to capture aggregate behavioral responses to experimental interventions

## Next Checks
1. Test agent performance with multiple interviews per participant over time to assess temporal stability and capture dynamic attitude changes
2. Conduct a pre-registered replication with a different population sample and alternative interview protocols to test robustness
3. Evaluate agent predictions against objective behavioral outcomes (e.g., actual voting records, purchase data) rather than self-reported survey responses to assess ecological validity