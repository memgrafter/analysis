---
ver: rpa2
title: 'Attention Guided CAM: Visual Explanations of Vision Transformer Guided by
  Self-Attention'
arxiv_id: '2402.04563'
source_url: https://arxiv.org/abs/2402.04563
tags:
- attention
- each
- image
- class
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a gradient-based method to generate visual explanations
  for Vision Transformers (ViT) by selectively aggregating gradients propagated from
  the classification output to self-attention matrices. The gradients are guided by
  self-attention scores normalized with sigmoid to alleviate peak intensities and
  supplement patch-level context information.
---

# Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention

## Quick Facts
- arXiv ID: 2402.04563
- Source URL: https://arxiv.org/abs/2402.04563
- Authors: Saebom Leem; Hyunseok Seo
- Reference count: 16
- The paper proposes a gradient-based method to generate visual explanations for Vision Transformers (ViT) by selectively aggregating gradients propagated from the classification output to self-attention matrices, achieving state-of-the-art performance in weakly-supervised localization tasks.

## Executive Summary
This paper introduces Attention Guided CAM, a method for generating visual explanations of Vision Transformer models by leveraging self-attention mechanisms. The approach selectively aggregates gradients from the classification output to self-attention matrices, guided by sigmoid-normalized self-attention scores. This technique effectively addresses peak intensity artifacts common in ViT explainability methods while maintaining model faithfulness. The method demonstrates superior performance in weakly-supervised object localization across ImageNet, Pascal VOC 2012, and CUB 200 datasets, outperforming previous state-of-the-art approaches.

## Method Summary
The method extracts gradients from the classification output through skip connections to each self-attention matrix in the ViT encoder blocks. These gradients are then guided by self-attention scores normalized with sigmoid to reduce peak intensities while preserving spatial information. The gradients and normalized self-attention feature maps are combined through element-wise multiplication, and the results are aggregated across all layers and attention heads to generate the final class activation map (CAM). This approach captures high-level semantic contributions while maintaining patch-level context information, resulting in more accurate and continuous heatmaps for object localization.

## Key Results
- Achieves 73.41% pixel accuracy, 52.12% IoU, and 65.15% dice coefficient on ImageNet
- Outperforms previous state-of-the-art explainability methods for ViT across all tested datasets
- Demonstrates better faithfulness in pixel perturbation tests with an ABPC score of 36.91% on ImageNet
- Consistent improvements across ImageNet, Pascal VOC 2012, and CUB 200 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Normalizing self-attention scores with sigmoid instead of softmax reduces peak intensity artifacts while preserving model faithfulness
- Mechanism: Sigmoid normalization suppresses extreme values in self-attention matrices that would otherwise create peak intensities through softmax amplification, while maintaining the monotonic relationship needed for gradient backpropagation
- Core assumption: Self-attention scores exhibit smooth varying properties that satisfy the approximation condition for gradient equivalence
- Evidence anchors:
  - [abstract]: "we normalize the self-attention matrices with sigmoid, which is a monotonically increasing function as well as softmax"
  - [section]: "softmax tends to amplify the local large values in the process of converting the self-attention scores into probabilities. Consequently, it generates a peak intensity that highlights the specific point of a homogeneous background"
  - [corpus]: Weak - no direct corpus evidence found for sigmoid vs softmax normalization in ViT explainability methods
- Break condition: If self-attention scores are highly concentrated with extreme outliers, the sigmoid normalization may not sufficiently suppress peak intensities, or if the approximation condition in the supplementary proof fails

### Mechanism 2
- Claim: Selective gradient aggregation through skip connections captures high-level semantic contributions while preserving spatial information
- Mechanism: Gradients propagated from classification output through skip connections to each encoder block's self-attention matrices encode the contribution of image features to the final prediction, while the first-row elements maintain patch position information
- Core assumption: The skip connections in ViT preserve spatial relationships and allow gradients to flow from later to earlier layers while maintaining patch correspondence
- Evidence anchors:
  - [abstract]: "Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location"
  - [section]: "the MLP head is directly connected not only to AK_r2,1 in the last encoder block but also to all Ak_h,1s in the previous blocks, as shown in Figure 2"
  - [corpus]: Weak - corpus contains related methods like "Dynamic Accumulated Attention Map" but no direct evidence for skip-connection gradient aggregation
- Break condition: If skip connections are modified or removed, or if the MLP head architecture changes significantly (e.g., using average pooling instead of [class] token)

### Mechanism 3
- Claim: Feature map-gradient Hadamard product produces continuous heatmaps that cluster contributions on target objects
- Mechanism: Combining normalized self-attention feature maps with ReLU-activated gradients through element-wise multiplication creates continuous activation maps where contributions cluster together rather than scattering across isolated patches
- Core assumption: The combination of feature maps (containing patch correlation patterns) with their corresponding gradients (containing contribution weights) produces meaningful spatial aggregation
- Evidence anchors:
  - [abstract]: "These gradients are additionally guided by the normalized self-attention scores... They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism"
  - [section]: "the feature maps reflect the interaction among the pixels from multiple layers of the model and the gradients are the contributions of these high-level image features"
  - [corpus]: Weak - no direct corpus evidence for Hadamard product combining in ViT explainability
- Break condition: If the relationship between gradients and feature maps becomes non-correlated due to architectural changes, or if ReLU activation removes too many relevant negative contributions

## Foundational Learning

- Concept: Vision Transformer architecture and self-attention mechanism
  - Why needed here: Understanding how ViT processes patches, maintains positional information, and uses self-attention is crucial for implementing the gradient aggregation approach
  - Quick check question: What is the size relationship between input patches and self-attention matrices in a ViT-base model?

- Concept: Gradient-based explainability methods (CAM, Grad-CAM)
  - Why needed here: The method builds on gradient-based visualization by adapting it to ViT's unique architecture and combining it with attention scores
  - Quick check question: How does Grad-CAM differ from the proposed method in terms of feature map selection and normalization?

- Concept: Weakly-supervised object localization metrics (pixel accuracy, IoU, Dice coefficient)
  - Why needed here: The method is evaluated using these metrics to demonstrate localization performance improvements over existing methods
  - Quick check question: What is the relationship between precision and recall in the context of bounding box generation from heatmaps?

## Architecture Onboarding

- Component map:
  - Input: Image patches → Patch embeddings → Encoder blocks (self-attention + MLP + skip connections) → MLP head → Classification output
  - Key components: Self-attention matrices (Ak_h), Skip connection matrices (Ek_r1, Ek_r2), Classification output (yc), Gradients (βk,c, αk,c_h), Feature maps (F_k_h)
  - Output: Class activation map (CAM) generated from aggregated feature map-gradient products

- Critical path: Classification output → Backpropagate gradients through skip connections → Compute gradients for each self-attention matrix → Combine with normalized self-attention feature maps → Aggregate across layers/heads → Generate CAM

- Design tradeoffs:
  - Sigmoid vs softmax normalization: Sigmoid reduces peak intensity but requires approximation proof for gradient equivalence
  - First-row selection vs full matrix: Using first row maintains spatial correspondence but discards some attention information
  - ReLU activation: Removes negative contributions but may eliminate meaningful negative correlations

- Failure signatures:
  - Peak intensity artifacts dominating heatmaps: Indicates sigmoid normalization insufficient or approximation condition failing
  - Scattered contributions across image: Suggests gradient aggregation not properly capturing semantic features
  - Missing object instances: Indicates feature map-gradient combination not effectively localizing complete objects

- First 3 experiments:
  1. Compare heatmaps generated with sigmoid vs softmax normalization on homogeneous background images to verify peak intensity reduction
  2. Test gradient aggregation with and without skip connections to validate spatial information preservation
  3. Evaluate localization performance on single-instance vs multi-instance images to assess complete object capture capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sigmoid normalization of self-attention scores compare to other normalization techniques in mitigating peak intensities while preserving model faithfulness?
- Basis in paper: [explicit] The paper demonstrates that sigmoid normalization effectively reduces peak intensities compared to softmax, and proves that under certain assumptions, gradients with sigmoid normalization approximate those with softmax normalization.
- Why unresolved: The paper only compares sigmoid to softmax. Other normalization techniques like layer normalization or instance normalization could potentially offer better trade-offs between peak reduction and gradient faithfulness.
- What evidence would resolve it: Systematic comparison of different normalization techniques applied to self-attention scores, evaluating both peak intensity reduction and localization performance across multiple datasets.

### Open Question 2
- Question: What is the impact of using different aggregation methods (e.g., weighted sum vs. simple average) across self-attention heads and layers on the quality of the generated CAMs?
- Basis in paper: [inferred] The paper uses a simple sum aggregation across heads and layers, but doesn't explore alternative aggregation strategies that might better capture the contribution of different attention heads.
- Why unresolved: The choice of aggregation method could significantly impact the localization performance and faithfulness of the explanations, but this aspect hasn't been thoroughly investigated.
- What evidence would resolve it: Comparative analysis of different aggregation methods (e.g., weighted sum based on gradient magnitudes, attention head importance scores) on localization performance and faithfulness metrics.

### Open Question 3
- Question: How does the proposed method perform on other ViT variants (e.g., Swin Transformer, DeiT) or on models with different patch sizes and numbers?
- Basis in paper: [inferred] The method is evaluated only on ViT-base with a specific patch size and number, but its generalizability to other ViT architectures remains unexplored.
- Why unresolved: Different ViT variants have varying architectural details and patch configurations, which could affect the effectiveness of the proposed gradient aggregation and attention normalization techniques.
- What evidence would resolve it: Evaluation of the method on multiple ViT variants and configurations, comparing localization performance and faithfulness across different architectures.

## Limitations

- The theoretical proof for gradient equivalence under sigmoid normalization requires rigorous validation beyond the supplementary material
- The method's effectiveness on multi-instance images and complex scenes remains untested
- The approach depends on specific ViT architectural details that may vary across implementations

## Confidence

- **High Confidence**: Performance metrics on standard datasets (pixel accuracy, IoU, Dice coefficient)
- **Medium Confidence**: Mechanism of gradient aggregation through skip connections and self-attention normalization
- **Low Confidence**: Theoretical proofs for gradient equivalence under sigmoid normalization and spatial preservation claims

## Next Checks

1. **Gradient Equivalence Verification**: Test the approximation condition by comparing gradients under sigmoid vs softmax normalization across different attention score distributions to validate the theoretical claims.
2. **Spatial Information Preservation**: Conduct ablation studies removing skip connections to quantify the impact on spatial localization accuracy and heatmap continuity.
3. **Multi-Instance Robustness**: Evaluate the method on images containing multiple instances of the same object class to assess whether it can localize all relevant instances rather than just the most prominent one.