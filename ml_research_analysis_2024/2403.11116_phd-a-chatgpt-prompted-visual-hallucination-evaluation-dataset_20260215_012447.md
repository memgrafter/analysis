---
ver: rpa2
title: 'PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset'
arxiv_id: '2403.11116'
source_url: https://arxiv.org/abs/2403.11116
tags: []
core_contribution: 'This paper introduces PhD, a dataset for evaluating visual hallucinations
  in multimodal large language models (MLLMs). The dataset is designed to assess hallucinations
  across five tasks: object recognition, attribute recognition (shape, material, color,
  counting, position, sentiment, utility), and two modes: normal and hallucinatory.'
---

# PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset

## Quick Facts
- arXiv ID: 2403.11116
- Source URL: https://arxiv.org/abs/2403.11116
- Reference count: 40
- Primary result: Introduces PhD dataset for evaluating visual hallucinations in multimodal large language models across 5 tasks and 4 hallucination modes

## Executive Summary
This paper introduces PhD, a comprehensive dataset designed to evaluate visual hallucinations in multimodal large language models (MLLMs). The dataset is structured along two dimensions - task and mode - encompassing five evaluation tasks (object recognition, attribute recognition including shape, material, color, counting, position, sentiment, and utility) and four hallucination modes (normal, specious context, incorrect context, and counter-common-sense). PhD contains over 14,000 daily images, 750 counter-common-sense images, and 102,000 visual question answering triplets.

The authors propose an automated pipeline leveraging ChatGPT to construct the dataset, reducing human annotation burden while maintaining hallucination diversity. The dataset reveals considerable variability in MLLMs' performance across tasks and modes, offering insights into the nature of hallucination. PhD serves as a potent tool for visual hallucination evaluation and may contribute to the refinement of MLLMs by exposing systematic weaknesses in multimodal integration.

## Method Summary
The PhD dataset is constructed using a ChatGPT-assisted semi-automated pipeline with four pivotal modules: task-specific hallucinatory item selection, hitem-embedded question generation, specious/incorrect context generation, and counter-common-sense image generation. The pipeline extracts objects and attributes from visual annotations, generates contextually coherent confusing items using ChatGPT, and constructs hallucinatory questions automatically. The dataset evaluates MLLMs across five tasks (object, shape, material, color, counting, position, sentiment, utility) and four hallucination modes (normal, specious context, incorrect context, counter-common-sense) using accuracy metrics.

## Key Results
- PhD dataset contains 14k daily images, 750 counter-common-sense images, and 102k VQA triplets
- MLLMs show considerable variability in performance across different tasks and hallucination modes
- The structured evaluation reveals systematic weaknesses in multimodal integration, particularly in counting and position tasks
- Automated pipeline successfully generates diverse hallucinatory items while reducing human annotation burden

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The automated pipeline using ChatGPT reduces human annotation burden while maintaining hallucination diversity.
- Mechanism: The pipeline leverages ChatGPT to extract objects and attributes from visual annotations, generate confusing items with contextual relevance, and construct hallucinatory questions automatically. This semi-automated approach scales dataset creation without sacrificing quality.
- Core assumption: ChatGPT can reliably generate contextually coherent confusing items that are semantically related to the original image content but incorrect.
- Evidence anchors:
  - [abstract] "We construct PhD by a ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules: task-specific hallucinatory item (hitem) selection, hitem-embedded question generation, specious / incorrect context generation, and counter-common-sense (CCS) image generation."
  - [section] "Leveraging the object-attribute information extracted, we utilize ChatGPT to create confusion-inducing items. We have crafted specific prompts that allow ChatGPT to produce confusing items (objects and attributes) that are logically consistent and accompanied by reasoning."
- Break condition: If ChatGPT generates items that are too obvious or unrelated to the image context, the hallucination quality degrades and models can easily distinguish correct from incorrect information.

### Mechanism 2
- Claim: Structuring evaluation along task and mode dimensions reveals systematic weaknesses in MLLMs' multimodal integration.
- Mechanism: By categorizing evaluation into five tasks (object, shape, material, color, counting, position, sentiment, utility) and four modes (normal, specious context, incorrect context, counter-common-sense), the dataset exposes where models fail to properly integrate visual and textual information.
- Core assumption: Different MLLM architectures will show consistent patterns of failure across specific task-mode combinations.
- Evidence anchors:
  - [abstract] "Depending on what to ask (objects, attributes, sentiment, etc.) and how the questions are asked, we structure PhD along two dimensions, i.e. task and mode."
  - [section] "We categorize it into four types (shown in Fig. 1) as follows: • Object hallucination... • Attribute hallucination... • Multi-modal conflicting hallucination... • Counter-common-sense hallucination."
- Break condition: If MLLMs show random or inconsistent performance across tasks and modes, the structured evaluation approach loses diagnostic value.

### Mechanism 3
- Claim: Including "hallucination elements" in the dataset enables targeted analysis of model failure modes.
- Mechanism: Each question-answer pair includes explicit annotation of why hallucination occurs, allowing researchers to trace specific failure patterns and develop targeted mitigation strategies.
- Core assumption: Understanding the specific reasons for hallucination enables more effective model improvement than general performance metrics alone.
- Evidence anchors:
  - [abstract] "PhD provides images, question-answer pairs, and identifies the hallucinatory elements within the questions."
  - [section] "We document the rationale behind the selection and formulation of these confusing items as the components contributing to hallucination."
- Break condition: If the hallucination elements are not consistently accurate or miss key failure modes, the dataset loses its diagnostic value for model improvement.

## Foundational Learning

- Concept: Multimodal hallucination mechanisms
  - Why needed here: Understanding how LLMs' internal knowledge conflicts with visual input is essential for interpreting PhD evaluation results
  - Quick check question: What is the fundamental difference between intrinsic and extrinsic hallucinations in MLLMs?

- Concept: Automated data generation pipelines
  - Why needed here: The PhD dataset relies on ChatGPT-assisted generation, requiring understanding of prompt engineering and quality control
  - Quick check question: How does the pipeline ensure generated confusing items are contextually relevant to the original image?

- Concept: VQA evaluation metrics
  - Why needed here: PhD uses accuracy metrics, but understanding their limitations is crucial for proper interpretation
  - Quick check question: Why might accuracy metrics be insufficient for evaluating multimodal hallucination performance?

## Architecture Onboarding

- Component map: Visual encoder (ViT) -> Feature extractor -> LLM adapter -> Bridges visual features to text space -> Frozen LLM -> Generates responses based on combined input -> Context generator -> Creates specious/incorrect contexts

- Critical path:
  1. Image input -> Visual feature extraction
  2. Combined visual-text features -> LLM processing
  3. LLM generation -> Response output
  4. Hallucination occurs when LLM's internal knowledge overrides visual features

- Design tradeoffs:
  - Visual model complexity vs. LLM dominance
  - Frozen LLM benefits vs. hallucination risks
  - Dataset size vs. hallucination quality

- Failure signatures:
  - High performance on positive evaluation but low on hallucinatory tasks
  - Task-specific weaknesses (e.g., counting vs. color recognition)
  - Sensitivity to question phrasing and context

- First 3 experiments:
  1. Test model performance on positive vs. hallucinatory datasets to establish baseline hallucination rate
  2. Ablation study varying question phrasing (Yes/No vs. open-ended) to identify sensitivity patterns
  3. Task-mode combination analysis to identify specific failure patterns across different evaluation dimensions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific architectural modifications that could effectively balance the visual and textual modalities in LVLMs to reduce hallucinations?
- Basis in paper: explicit
- Why unresolved: The paper identifies that the current LVLM architectures, which rely heavily on LLMs, lead to an imbalance between visual and textual modalities, resulting in hallucinations. However, it does not propose specific architectural modifications to address this imbalance.
- What evidence would resolve it: Developing and testing new LVLM architectures that incorporate mechanisms to better balance visual and textual modalities, followed by empirical evaluation of their performance on hallucination tasks.

### Open Question 2
- Question: How can we develop a comprehensive evaluation framework that not only detects but also quantifies the severity of different types of hallucinations in LVLMs?
- Basis in paper: explicit
- Why unresolved: The paper introduces a dataset for evaluating hallucinations but does not provide a framework for quantifying the severity of different types of hallucinations. This would be useful for understanding the impact of hallucinations and prioritizing mitigation strategies.
- What evidence would resolve it: Creating a multi-dimensional evaluation framework that includes metrics for severity, frequency, and impact of hallucinations, and applying it to various LVLMs to generate comparative data.

### Open Question 3
- Question: What are the underlying mechanisms in LLMs that cause them to generate hallucinations when processing visual information, and how can these be mitigated?
- Basis in paper: inferred
- Why unresolved: The paper suggests that the internal knowledge and reasoning processes of LLMs contribute to hallucinations, but it does not delve into the specific mechanisms within LLMs that lead to this behavior. Understanding these mechanisms is crucial for developing targeted interventions.
- What evidence would resolve it: Conducting detailed analyses of LLM behavior, such as attention mechanisms and knowledge representation, to identify the specific factors that lead to hallucinations, and then designing targeted interventions to mitigate these factors.

## Limitations

- The dataset construction heavily relies on ChatGPT's capabilities, introducing potential biases in hallucinatory item generation that may not generalize to human annotators
- The evaluation focuses primarily on accuracy metrics, which may not capture nuanced aspects of multimodal reasoning failures
- The dataset's reliance on daily images and counter-common-sense examples may limit generalizability to specialized domains

## Confidence

- **High Confidence**: The automated pipeline methodology and dataset structure are well-defined and reproducible
- **Medium Confidence**: Claims about MLLM performance patterns across tasks and modes, as these depend on specific model implementations
- **Low Confidence**: Generalizability of findings to all MLLM architectures and real-world applications

## Next Checks

1. **Cross-annotator validation**: Have human experts evaluate a subset of generated hallucinatory items to assess ChatGPT's reliability and identify systematic biases

2. **Domain transfer study**: Test model performance on PhD tasks using images from specialized domains (medical, technical) to assess generalizability

3. **Metric expansion**: Implement additional evaluation metrics beyond accuracy (e.g., calibration, uncertainty estimation) to capture more nuanced aspects of multimodal hallucination