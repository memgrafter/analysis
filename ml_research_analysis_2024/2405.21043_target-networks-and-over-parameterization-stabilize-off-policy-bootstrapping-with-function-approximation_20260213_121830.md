---
ver: rpa2
title: Target Networks and Over-parameterization Stabilize Off-policy Bootstrapping
  with Function Approximation
arxiv_id: '2405.21043'
source_url: https://arxiv.org/abs/2405.21043
tags:
- target
- learning
- data
- off-policy
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability of temporal difference (TD)
  learning when combining off-policy data, function approximation, and bootstrapping
  - known as the deadly triad. The authors prove that using both a target network
  and over-parameterized linear function approximation provides convergence guarantees
  even with off-policy data, relaxing conditions that typically cause divergence.
---

# Target Networks and Over-parameterization Stabilize Off-policy Bootstrapping with Function Approximation

## Quick Facts
- arXiv ID: 2405.21043
- Source URL: https://arxiv.org/abs/2405.21043
- Authors: Fengdi Che; Chenjun Xiao; Jincheng Mei; Bo Dai; Ramki Gummadi; Oscar A Ramirez; Christopher K Harris; A. Rupam Mahmood; Dale Schuurmans
- Reference count: 40
- One-line primary result: Proves that combining target networks with over-parameterized linear function approximation provides convergence guarantees for off-policy TD learning

## Executive Summary
This paper addresses the instability of temporal difference (TD) learning when combining off-policy data, function approximation, and bootstrapping - known as the deadly triad. The authors prove that using both a target network and over-parameterized linear function approximation provides convergence guarantees even with off-policy data, relaxing conditions that typically cause divergence. Their primary result shows that the combination ensures convergence when the infinity norm of the projected transition matrix is bounded by one, a condition naturally satisfied with full state coverage or trajectory data. They demonstrate empirically on Baird's counterexample and a Four-room task that their method converges faster than alternatives like residual minimization or gradient TD methods, while avoiding the divergence issues of standard TD.

## Method Summary
The paper introduces Over-parameterized Target TD (OTTD), which combines a target network with over-parameterized linear function approximation for stable off-policy learning. The algorithm updates parameters using a semi-gradient of the expected mean-squared Bellman error (EMSBE) with a slowly-updated target network, while over-parameterization ensures the solution is independent of the state distribution. The method theoretically guarantees convergence when the infinity norm of the projected transition matrix is bounded by one, which naturally holds with trajectory data. The approach extends to Q-learning by constraining maximization to actions seen in the dataset.

## Key Results
- Proves convergence of OTTD to the TD fixed point when infinity norm of projected transition matrix is bounded by one
- Shows OTTD converges faster than residual minimization and gradient TD methods on Baird's counterexample
- Demonstrates similar convergence properties for Q-learning when constraining maximization to seen actions
- First practical algorithm proven convergent and empirically effective for off-policy TD learning with function approximation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The combination of a target network and over-parameterization stabilizes TD learning by eliminating the divergence-inducing condition on the spectral radius of the projected transition matrix.
- **Mechanism:** Over-parameterization removes the dependency of the solution on the state distribution, while the target network breaks the contractivity requirement that causes divergence in the deadly triad. Together, they ensure convergence to the TD fixed point when the infinity norm of the projected transition matrix is bounded by one.
- **Core assumption:** The state-action space is sufficiently covered by the offline data, or the data consists of complete trajectories.
- **Evidence anchors:**
  - [abstract] "The authors prove that using both a target network and over-parameterized linear function approximation provides convergence guarantees even with off-policy data"
  - [section 3.2] "Theorem 3.2 illustrates the efficacy of incorporating a target network in stabilizing bootstrapping with function approximation. This is apparent in the convergence of OTTD to the TD fixed point, eliminating the condition on the spectral radius of I−η(M−γN)M ⊤Dk to be bounded by one."
  - [corpus] Weak evidence - no direct mentions of target networks or over-parameterization in related papers.
- **Break condition:** If the offline data does not cover the state-action space sufficiently, or if the infinity norm of the projected transition matrix exceeds one.

### Mechanism 2
- **Claim:** Over-parameterization ensures the existence of a TD fixed point independent of the state collection distribution, eliminating the need for state distribution correction.
- **Mechanism:** By having more parameters than distinct data points, the model can satisfy all Bellman consistency constraints exactly, making the solution invariant to the data distribution.
- **Core assumption:** The function approximation has dimension d > k, where k is the support of the empirical data.
- **Evidence anchors:**
  - [abstract] "Over-parameterization is another approach for ensuring the existence of a TD fixed point (Xiao et al. 2021, Thomas 2022)."
  - [section 3.1] "In the over-parameterized regime, the Moore-Penrose pseudoinverse of the feature matrix, denoted as Φ†, equals Φ⊤(ΦΦ⊤)−1."
  - [corpus] Weak evidence - no direct mentions of over-parameterization in related papers.
- **Break condition:** If the function approximation is under-parameterized (d ≤ k), the model becomes dependent on the state collection distribution.

### Mechanism 3
- **Claim:** Normalized importance sampling (NIS) correction removes the remaining convergence condition by preventing overestimation on out-of-dataset value estimates.
- **Mechanism:** NIS correction reduces the variance of importance sampling ratios, making the projected transition matrix stochastic with infinity norm equal to one, which naturally satisfies the convergence condition.
- **Core assumption:** The behavior policies cover the support of the target policy, and the dataset consists of trajectory data.
- **Evidence anchors:**
  - [section 4.1] "The algorithm with NIS correction is realized by weighting each transition proportional to its IS ratio ρ(a′i|si′)"
  - [section 4.1] "Therefore, the algorithm OTTD with NIS correction effectively addresses the deadly triad for off-policy tasks with trajectory data"
  - [corpus] Weak evidence - no direct mentions of NIS correction in related papers.
- **Break condition:** If the behavior policies do not cover the support of the target policy, or if the dataset does not consist of trajectory data.

## Foundational Learning

- **Concept:** Markov Decision Process (MDP)
  - Why needed here: The paper's analysis is built on MDP theory, including value functions, Bellman equations, and transition dynamics.
  - Quick check question: What is the Bellman equation for the Q-value function, and how does it relate to the value function?

- **Concept:** Temporal Difference (TD) Learning
  - Why needed here: TD learning is the core algorithm being stabilized, using bootstrapping to update value estimates.
  - Quick check question: How does TD learning update its value estimates using the TD error, and what is the role of the discount factor?

- **Concept:** Linear Function Approximation
  - Why needed here: The paper focuses on linear function approximation for value estimation, using feature matrices and parameter vectors.
  - Quick check question: How does linear function approximation represent the value function, and what is the role of the feature matrix?

## Architecture Onboarding

- **Component map:**
  - Feature matrix Φ -> Student parameter θ -> Target parameter θtarg -> Transition matrix ˆPπ -> Importance sampling ratios ρ

- **Critical path:**
  1. Collect offline data (state-action-reward-next state-next action tuples)
  2. Compute feature matrix Φ and transition matrix ˆPπ
  3. Initialize student and target parameters
  4. Update student parameter using semi-gradient of EMSBE with target parameter
  5. Periodically update target parameter by copying student parameter
  6. Repeat until convergence

- **Design tradeoffs:**
  - Target network update frequency (m): Higher frequency reduces divergence but may slow convergence
  - Over-parameterization dimension (d): Higher dimension ensures solution independence but increases computation
  - NIS correction: Reduces variance but may introduce bias in continuing tasks

- **Failure signatures:**
  - Divergence: Check if the infinity norm of the projected transition matrix exceeds one
  - Slow convergence: Check if the target network update frequency is too high or the learning rate is too low
  - High variance: Check if NIS correction is needed for off-policy data

- **First 3 experiments:**
  1. Test convergence on Baird's counterexample with varying over-parameterization dimensions
  2. Test convergence on Four-room task with and without NIS correction for off-policy data
  3. Test convergence on Q-learning with limiting arg max to seen actions in the dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal window size m for target network updates in over-parameterized TD learning?
- Basis in paper: [inferred] The paper shows convergence for m ≥ m̄ but does not specify the optimal value, noting that m̄ depends on problem-specific constants.
- Why unresolved: The analytical form of m̄ depends on the norm constraint applied to NM†, and the paper only provides a bound for the infinity norm case. Finding the optimal m would require empirical tuning or additional theoretical analysis.
- What evidence would resolve it: Empirical studies comparing convergence speed across different m values, or theoretical analysis deriving tighter bounds on m̄.

### Open Question 2
- Question: Can the convergence results be extended to neural network function approximation?
- Basis in paper: [explicit] The authors state "While our study is currently confined to linear function approximation, it offers compelling evidence for convergence guarantees...Extending these results to neural networks would be a crucial next step."
- Why unresolved: Neural networks have different properties than linear function approximation, including non-convex optimization landscapes and feature learning, which may affect the convergence conditions.
- What evidence would resolve it: Empirical demonstrations of convergence in neural network settings, or theoretical analysis establishing analogous convergence conditions for neural networks.

### Open Question 3
- Question: How does the convergence rate of over-parameterized target TD compare to residual minimization in practice?
- Basis in paper: [explicit] The paper notes that "OTTD also mitigates the slow convergence rates of alternative algorithms" and shows OTTD converges faster than RM on Baird's counterexample, but doesn't provide comprehensive comparisons.
- Why unresolved: The paper provides limited empirical comparisons and doesn't quantify the convergence rate differences across diverse problems.
- What evidence would resolve it: Systematic empirical studies comparing convergence rates of OTTD and RM across multiple benchmark problems, measuring both speed and final error.

## Limitations
- Theoretical guarantees rely on infinity norm condition being naturally satisfied with trajectory data, which may not hold in all practical scenarios
- Empirical validation limited to specific environments (Baird's counterexample and Four-room task) without testing on more complex or continuous control problems
- Practical implications and computational costs of high-dimensional feature spaces for over-parameterization are not thoroughly explored

## Confidence
- High confidence: The core theoretical results regarding target network stabilization and the elimination of spectral radius conditions
- Medium confidence: The empirical demonstrations on Baird's counterexample and Four-room task showing faster convergence than alternatives
- Medium confidence: The extension to Q-learning with seen-action constraints

## Next Checks
1. Test convergence on continuous control environments (e.g., MuJoCo tasks) to verify the method's scalability beyond tabular/grid-world settings
2. Conduct ablation studies varying over-parameterization dimensions to quantify the trade-off between solution independence and computational efficiency
3. Evaluate the method's performance when the infinity norm condition is violated to understand its robustness to assumption violations