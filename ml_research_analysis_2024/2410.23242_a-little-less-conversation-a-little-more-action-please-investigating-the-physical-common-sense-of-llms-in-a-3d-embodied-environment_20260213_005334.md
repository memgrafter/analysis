---
ver: rpa2
title: 'A little less conversation, a little more action, please: Investigating the
  physical common-sense of LLMs in a 3D embodied environment'
arxiv_id: '2410.23242'
source_url: https://arxiv.org/abs/2410.23242
tags:
- arxiv
- llms
- physical
- reasoning
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-AAI, a framework for evaluating the physical
  common-sense reasoning capabilities of LLMs in a 3D virtual environment using the
  Animal-AI Testbed. The authors demonstrate that state-of-the-art multimodal LLMs
  can complete tasks in this setting, but are outperformed by human children and competition
  agents across all tested levels.
---

# A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment

## Quick Facts
- arXiv ID: 2410.23242
- Source URL: https://arxiv.org/abs/2410.23242
- Reference count: 28
- LLMs with no fine-tuning can complete tasks in the Animal-AI environment but are outperformed by human children and competition agents

## Executive Summary
This paper introduces LLM-AAI, a framework for evaluating the physical common-sense reasoning capabilities of large language models (LLMs) in a 3D virtual environment using the Animal-AI Testbed. The authors demonstrate that state-of-the-art multimodal LLMs can complete tasks in this setting, but are outperformed by human children and competition agents across all tested levels. The LLM-AAI framework enables direct comparison between LLMs, humans, and other AI agents on ecologically valid experiments drawn from cognitive science, providing a novel approach for assessing physical reasoning that goes beyond traditional text/image-based benchmarks.

## Method Summary
The LLM-AAI framework connects LLMs to the Animal-AI Testbed environment, allowing them to control an agent using a simple scripting language (Go, Turn, Think commands). The LLM receives observations from the environment and generates action scripts to complete tasks. The framework was tested with three prominent multimodal LLMs (Claude 3.5 Sonnet, GPT-4o, Gemini 1.5 Pro) on 40 tasks from the Animal-AI Testbed levels 1-10, comparing their performance to human children and competition agents.

## Key Results
- State-of-the-art multimodal LLMs can complete tasks in the Animal-AI environment without fine-tuning
- LLMs are outperformed by human children and competition agents across all tested levels
- The LLM-AAI framework enables direct comparison of LLMs with other embodied agents and humans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM-AAI framework enables direct comparison between LLMs and other agents by situating them in the same 3D environment.
- Mechanism: By providing LLMs with control of an agent in the Animal-AI environment, the framework creates a standardized setting where performance can be directly measured and compared across different types of agents.
- Core assumption: The Animal-AI environment provides a valid and reliable test of physical common-sense reasoning that is comparable across different agent types.
- Evidence anchors:
  - [abstract]: "Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals."
  - [section]: "Our approach situates LLMs in a realistic physical environment (ecologically valid), draws on testing materials that have been independently validated on humans and other animals (construct valid)."
  - [corpus]: Weak evidence. While the corpus mentions related work on LLM agents in games, it does not specifically address direct comparison in a standardized 3D environment.

### Mechanism 2
- Claim: LLMs can perform tasks in the Animal-AI environment using a simple scripting language.
- Mechanism: The framework provides LLMs with a basic scripting language (Go, Turn, Think) that allows them to control an agent and interact with the environment to complete tasks.
- Core assumption: LLMs can effectively translate their understanding of the environment and tasks into appropriate action scripts.
- Evidence anchors:
  - [abstract]: "We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task."
  - [section]: "LLMs can act in the environment using a simple scripting language. The LLMs have access to three functions: Go, Turn, and Think."
  - [corpus]: Weak evidence. While the corpus mentions LLM agents in various environments, it does not specifically address the use of a simple scripting language for physical reasoning tasks.

### Mechanism 3
- Claim: The LLM-AAI framework provides a more ecologically valid and construct valid evaluation of physical common-sense reasoning than traditional benchmarks.
- Mechanism: By situating LLMs in a realistic 3D environment and using tasks drawn from cognitive science research, the framework better captures the complexity of real-world physical reasoning and has been validated on humans and animals.
- Core assumption: Traditional text/image-based benchmarks do not adequately capture the complexity and nuance of real-life physical processes.
- Evidence anchors:
  - [abstract]: "However, these benchmarks do not capture the complexity and nuance of real-life physical processes."
  - [section]: "Our approach situates LLMs in a realistic physical environment (ecologically valid), draws on testing materials that have been independently validated on humans and other animals (construct valid)."
  - [corpus]: Weak evidence. While the corpus mentions the limitations of traditional benchmarks, it does not provide direct evidence for the ecological and construct validity of the LLM-AAI approach.

## Foundational Learning

- Concept: Physical common-sense reasoning
  - Why needed here: Understanding the concept is crucial for grasping the purpose and significance of the LLM-AAI framework, which aims to evaluate this specific capability in LLMs.
  - Quick check question: What are some key components of physical common-sense reasoning, and why is it important for LLMs to possess this capability?

- Concept: Embodied cognition
  - Why needed here: The LLM-AAI framework is based on the idea of "embodying" LLMs by granting them control of an agent in a 3D environment, which relates to the concept of embodied cognition.
  - Quick check question: How does the concept of embodied cognition relate to the LLM-AAI framework, and why might it be important for evaluating physical reasoning in LLMs?

- Concept: Cognitive science research methods
  - Why needed here: The LLM-AAI framework draws on experiments and tasks from cognitive science research, so understanding these methods is important for interpreting the results and comparing them to human and animal performance.
  - Quick check question: What are some common methods used in cognitive science research to study physical reasoning, and how are these methods applied in the LLM-AAI framework?

## Architecture Onboarding

- Component map:
  - LLM -> LLM-AAI framework -> Animal-AI environment
  - Prompt -> LLM (provides context and instructions)

- Critical path:
  1. Environment provides observation (image, reward, health)
  2. LLM-AAI framework combines observation with prompt and previous interactions
  3. LLM generates action script (Go, Turn, Think commands)
  4. LLM-AAI framework parses script into low-level actions
  5. Actions are executed in Animal-AI environment
  6. New observation is generated, and the cycle repeats

- Design tradeoffs:
  - Using a simple scripting language vs. more complex control schemes
  - Providing a single image observation per action vs. more frequent observations
  - Limiting the number of action scripts per episode vs. allowing more flexibility

- Failure signatures:
  - LLM generates invalid or nonsensical action scripts
  - LLM fails to progress in the environment or collect rewards
  - LLM gets stuck in loops or repetitive behaviors
  - LLM's performance is significantly worse than expected based on its capabilities

- First 3 experiments:
  1. Evaluate LLM performance on simple navigation tasks in the Animal-AI environment
  2. Compare LLM performance to human children and DRL agents on a subset of tasks
  3. Test the impact of providing in-context examples on LLM performance

## Open Questions the Paper Calls Out

- Open Question 1: Does fine-tuning multimodal LLMs on AAI tasks improve performance?
  - Basis in paper: [explicit] Mentioned in Limitations section
  - Why unresolved: Paper only tested out-of-the-box LLMs without fine-tuning
  - What evidence would resolve it: Training experiments showing performance gains after fine-tuning on AAI navigation tasks

- Open Question 2: How does frame-by-frame control compare to script-based control for LLM-AAI performance?
  - Basis in paper: [explicit] Suggested in Limitations section as alternative control scheme
  - Why unresolved: Study used coarse script-based control due to cost constraints
  - What evidence would resolve it: Direct comparison experiments using both control schemes

- Open Question 3: What is the impact of increased action budget on LLM performance in AAI?
  - Basis in paper: [explicit] Mentioned as limitation - LLMs restricted to 30 action-scripts per episode vs unlimited time for humans/DRL agents
  - Why unresolved: Study imposed action limit due to cost considerations
  - What evidence would resolve it: Experiments with varying action limits showing performance changes

## Limitations
- The ecological validity of the Animal-AI environment as a proxy for real-world physical reasoning remains untested across diverse physical scenarios beyond the current testbed
- The sample size for human children (n=59) may not fully represent population-level performance across different age groups or cultural contexts
- The generalization of results to other LLM architectures or domains of physical reasoning is unclear, as only three prominent multimodal models were evaluated

## Confidence

**Major Uncertainties:**
- The ecological validity of the Animal-AI environment as a proxy for real-world physical reasoning remains untested across diverse physical scenarios beyond the current testbed
- The generalization of results to other LLM architectures or domains of physical reasoning is unclear, as only three prominent multimodal models were evaluated
- The sample size for human children (n=59) may not fully represent population-level performance across different age groups or cultural contexts

**Confidence Labels:**
- **High confidence**: The LLM-AAI framework successfully enables direct comparison of LLMs with other agents in a standardized 3D environment
- **Medium confidence**: The claim that LLMs are outperformed by human children and competition agents across all tested levels, given the limited sample of tasks and models tested
- **Low confidence**: The assertion that traditional text/image-based benchmarks cannot capture real-world physical complexity, as this claim is not directly validated against alternative benchmark approaches

## Next Checks

1. Test the LLM-AAI framework with a broader range of LLM architectures and physical reasoning tasks to assess generalizability
2. Conduct controlled experiments comparing performance on traditional benchmarks versus the Animal-AI environment for the same physical reasoning concepts
3. Validate the Animal-AI environment's ecological validity by testing human performance across diverse physical reasoning scenarios not currently represented in the testbed