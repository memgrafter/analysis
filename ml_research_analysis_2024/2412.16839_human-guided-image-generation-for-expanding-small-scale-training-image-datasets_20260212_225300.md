---
ver: rpa2
title: Human-Guided Image Generation for Expanding Small-Scale Training Image Datasets
arxiv_id: '2412.16839'
source_url: https://arxiv.org/abs/2412.16839
tags:
- images
- generated
- image
- labels
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of expanding small-scale training
  image datasets for computer vision tasks, particularly in scenarios where collecting
  large numbers of images is impractical (e.g., rare wildlife observation). The authors
  propose DataCrafter, a human-guided image generation method that improves upon automatic
  dataset expansion by allowing users to refine prompts based on sample-level feedback.
---

# Human-Guided Image Generation for Expanding Small-Scale Training Image Datasets

## Quick Facts
- arXiv ID: 2412.16839
- Source URL: https://arxiv.org/abs/2412.16839
- Reference count: 40
- Key outcome: Human-guided image generation method (DataCrafter) improves classification accuracy from 48.45% to 81.80% on small-scale datasets

## Executive Summary
This paper addresses the challenge of expanding small-scale training image datasets for computer vision tasks, particularly in scenarios where collecting large numbers of images is impractical (e.g., rare wildlife observation). The authors propose DataCrafter, a human-guided image generation method that improves upon automatic dataset expansion by allowing users to refine prompts based on sample-level feedback. The core of their approach is a multi-modal projection method (M2M) that projects both original and generated images into a 2D plane along with descriptive content labels, enabling efficient exploration and identification of quality issues. The sample-level prompt refinement method allows users to provide feedback on undesired images rather than directly editing prompts, making the process more accessible. Evaluation on real-world datasets (Pets and COCO) demonstrates that M2M outperforms existing methods in preserving both inter- and intra-modal relationships. A case study shows that DataCrafter improves classification accuracy from 48.45% to 81.80% on the Pets dataset, and the method extends to object detection tasks, improving mAP from 92.4% to 94.5%. The approach effectively addresses quality issues in generated images while making prompt refinement more user-friendly.

## Method Summary
DataCrafter is a human-guided image generation framework for expanding small-scale training datasets. It uses a multi-modal projection method (M2M) to visualize both original and generated images in a 2D space along with content labels extracted by GPT-4. Users can explore this visualization to identify quality issues in generated images. The sample-level prompt refinement method allows users to provide feedback on undesired images, which triggers an evolutionary algorithm to automatically refine the prompts. This iterative process continues until the user is satisfied with the generated images, which are then used to train downstream models. The approach leverages pre-trained diffusion models (like Stable Diffusion) for image generation and incorporates three key metrics - informativeness, diversity, and distance - to guide the generation process and evaluate quality.

## Key Results
- Classification accuracy improves from 48.45% to 81.80% on the Pets dataset
- Object detection mAP improves from 92.4% to 94.5% on the COCO17 dataset
- M2M method outperforms existing approaches in preserving both inter- and intra-modal relationships
- Sample-level prompt refinement successfully generates images that better match desired content while removing undesired elements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal projection method (M2M) enables efficient exploration of original and generated images by projecting them into a 2D plane along with content labels
- Mechanism: The method uses contrastive learning to preserve both inter- and intra-modal relationships, placing similar images together and matching images with their corresponding content labels
- Core assumption: The contrastive loss approach can effectively handle the many-to-many setting where images can have multiple labels
- Evidence anchors:
  - [abstract]: "We develop a multi-modal projection method with theoretical guarantees to facilitate the exploration of both the original and generated images"
  - [section 5.2.1]: "To find more suitable losses for the many-to-many setting, we get inspiration from multi-modal learning. In this field, contrastive loss has become one of the most popular methods for multi-modal learning tasks"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.504, average citations=0.0. Top related titles include papers on prompt engineering and interactive image generation
- Break condition: If the contrastive learning approach fails to preserve the relative distance orders between images and labels, the exploration effectiveness would be compromised

### Mechanism 2
- Claim: Sample-level prompt refinement lowers the barrier for novice users to improve prompt quality
- Mechanism: Users provide feedback on undesired images, and an evolutionary algorithm automatically refines the prompts based on this feedback rather than requiring direct prompt editing
- Core assumption: Users can more easily identify undesired images than modify complex prompts, and the evolutionary algorithm can effectively translate this feedback into improved prompts
- Evidence anchors:
  - [abstract]: "Since directly refining the prompts is challenging for novice users, we develop a sample-level prompt refinement method to make it easier"
  - [section 5.3.1]: "The current sample-level prompt refinement method primarily focuses on providing feedback for a single set of generated images"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.504, average citations=0.0. Top related titles include papers on prompt engineering and interactive image generation
- Break condition: If users struggle to identify which images are truly undesired or if the evolutionary algorithm cannot effectively translate this feedback into meaningful prompt improvements

### Mechanism 3
- Claim: The combination of metric visualization and multi-modal distribution visualization provides comprehensive oversight of the generation process
- Mechanism: Metric visualization shows informativeness, diversity, and distance metrics over iterations, while multi-modal distribution visualization displays images and content labels in a 2D space for detailed exploration
- Core assumption: Users need both high-level metrics to track generation progress and detailed visualizations to identify specific quality issues
- Evidence anchors:
  - [section 5.1]: "The metric visualization gives an overview of the generated images... we identify three metrics: informativeness, diversity, and distance"
  - [section 5.2]: "To address these issues, inspired by a recent work [32], we utilize multi-modal language models (e.g., GPT-4) to extract descriptive labels from images"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.504, average citations=0.0. Top related titles include papers on prompt engineering and interactive image generation
- Break condition: If the metrics don't accurately reflect image quality or if the visualization becomes too cluttered to be useful

## Foundational Learning

- Concept: Multi-modal learning and contrastive loss
  - Why needed here: The method uses contrastive loss to handle the many-to-many setting in multi-modal projection
  - Quick check question: How does contrastive loss differ from the multi-modal distance order loss used in previous methods, and why is it more suitable for the many-to-many setting?

- Concept: Evolutionary algorithms
  - Why needed here: The sample-level prompt refinement method uses an evolutionary algorithm to automatically refine prompts based on user feedback
  - Quick check question: What are the key components of an evolutionary algorithm (selection, mutation, evaluation) and how are they applied in the context of prompt refinement?

- Concept: Diffusion models and latent perturbation
  - Why needed here: The paper builds on dataset expansion methods that use pre-trained diffusion models with latent perturbation
  - Quick check question: How does latent perturbation in diffusion models help ensure that generated images are similar but not identical to original images?

## Architecture Onboarding

- Component map:
  Content label extraction module -> Multi-modal projection module (M2M) -> Metric visualization module and Multi-modal distribution visualization -> Sample-level prompt refinement module -> Image generation module -> Content label extraction module

- Critical path:
  1. Extract content labels from original and generated images using GPT-4
  2. Project images and labels into 2D space using M2M method
  3. Display results in metric visualization and multi-modal distribution visualization
  4. User provides feedback on undesired images
  5. Evolutionary algorithm refines prompts based on feedback
  6. Generate new images with refined prompts
  7. Repeat until satisfaction, then train downstream model

- Design tradeoffs:
  - Using GPT-4 for content label extraction provides rich semantic information but adds computational cost and potential for hallucination
  - The evolutionary algorithm approach for prompt refinement is more user-friendly but may be slower than direct prompt editing
  - Multi-modal projection balances exploration efficiency with preservation of relationships but requires careful tuning of contrastive loss weights

- Failure signatures:
  - Content labels fail to accurately represent image content (hallucination or omission)
  - Multi-modal projection fails to preserve inter-modal relationships (images not matched with correct labels)
  - Metric visualization doesn't correlate with actual image quality
  - Evolutionary algorithm fails to improve prompts despite user feedback
  - Visualizations become too cluttered with large numbers of images/labels

- First 3 experiments:
  1. Test content label extraction quality by comparing GPT-4 labels with human annotations on a small dataset
  2. Validate M2M projection by checking if similar images are placed close together and images are matched with correct labels
  3. Test evolutionary algorithm effectiveness by providing known bad prompts and verifying if the algorithm can improve them based on feedback

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DataCrafter's efficiency compare to existing automatic dataset expansion methods in terms of computational resources and time?
- Basis in paper: [inferred] The paper demonstrates DataCrafter's effectiveness in improving model performance but does not provide a direct comparison of computational efficiency with automatic methods.
- Why unresolved: The case study focuses on effectiveness and usability but does not measure or compare the computational resources or time required by DataCrafter versus automatic methods.
- What evidence would resolve it: A controlled experiment comparing the runtime and resource usage of DataCrafter versus automatic methods like GIF [65] on the same datasets would provide the necessary evidence.

### Open Question 2
- Question: Can DataCrafter's multi-modal projection method (M2M) be effectively extended to handle non-natural images, such as medical or satellite imagery?
- Basis in paper: [explicit] The paper mentions that DataCrafter can be used for other image types as long as corresponding generative models are available, but notes that generative models for non-natural images are generally less developed.
- Why unresolved: The paper does not provide empirical evidence or experiments demonstrating M2M's effectiveness on non-natural image types, only theoretical speculation about its potential.
- What evidence would resolve it: Experiments applying DataCrafter to non-natural image datasets (e.g., medical imaging datasets) and evaluating the quality of the multi-modal projections would resolve this question.

### Open Question 3
- Question: What is the optimal balance between effectiveness and efficiency in DataCrafter's human-guided image generation process?
- Basis in paper: [explicit] The paper acknowledges that efficiency is crucial for practical applications but does not conduct user studies to explore the interplay between effectiveness and efficiency.
- Why unresolved: The case study focuses on demonstrating effectiveness but does not measure or optimize for efficiency, leaving the optimal balance between these factors unexplored.
- What evidence would resolve it: User studies measuring both the improvement in model performance and the time/effort required by users to achieve these improvements would provide the necessary evidence to determine the optimal balance.

## Limitations
- The method relies heavily on GPT-4 for content label extraction, introducing potential bias and computational overhead
- The evolutionary algorithm for prompt refinement, while user-friendly, may be slower than direct prompt editing approaches
- The evaluation is primarily focused on classification and object detection tasks, with limited exploration of other computer vision applications

## Confidence
- **High Confidence**: The core claim that human-guided image generation with sample-level feedback improves dataset quality and downstream task performance is well-supported by experimental results showing accuracy improvements from 48.45% to 81.80% on the Pets dataset.
- **Medium Confidence**: The effectiveness of the multi-modal projection method (M2M) is supported by comparison with existing methods, but the theoretical guarantees mentioned in the abstract are not fully detailed in the paper.
- **Medium Confidence**: The sample-level prompt refinement method is shown to be effective, but the evaluation focuses on a single iterative refinement process rather than exploring multiple refinement cycles or comparing directly with expert prompt engineering.

## Next Checks
1. **Content Label Quality Validation**: Conduct a human annotation study comparing GPT-4 extracted labels with ground truth labels on a subset of images to quantify hallucination rates and label accuracy.
2. **Scalability Testing**: Evaluate the method's performance and visualization effectiveness when scaling up to larger original datasets (e.g., 100+ classes) and higher numbers of generated images to identify potential bottlenecks.
3. **Alternative Prompt Refinement Comparison**: Implement and compare the sample-level feedback approach with a direct prompt editing interface where users can modify prompts themselves, measuring both quality improvements and user effort/time.