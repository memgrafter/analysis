---
ver: rpa2
title: Guiding and Diversifying LLM-Based Story Generation via Answer Set Programming
arxiv_id: '2406.00554'
source_url: https://arxiv.org/abs/2406.00554
tags:
- story
- generation
- narrative
- stories
- outline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a neurosymbolic approach to large language
  model (LLM)-based story generation that combines the open-domain capabilities of
  LLMs with the structural benefits of symbolic planning. The method uses answer set
  programming (ASP) to generate diverse story outlines by defining narrative functions
  and constraints, then employs an LLM to expand these outlines into complete stories.
---

# Guiding and Diversifying LLM-Based Story Generation via Answer Set Programming

## Quick Facts
- arXiv ID: 2406.00554
- Source URL: https://arxiv.org/abs/2406.00554
- Authors: Phoebe J. Wang; Max Kreminski
- Reference count: 7
- Primary result: ASP-guided LLM approach produces more diverse stories than unguided baseline across six premise prompts

## Executive Summary
This paper presents a neurosymbolic approach to story generation that combines large language models (LLMs) with answer set programming (ASP) for structural guidance. The method generates diverse story outlines using ASP constraints while leveraging LLM capabilities for open-domain content generation. The approach addresses the tension between the structural benefits of symbolic planning and the flexibility of neural generation, demonstrating improved diversity and compactness compared to unguided LLM baselines.

## Method Summary
The approach uses ASP to define narrative functions and constraints that generate diverse story outlines, which are then expanded into complete stories by an LLM. The system requires only fifteen simple constraints to produce over 400,000 distinct outlines, offering improved compactness compared to traditional narrative planning methods. Evaluation employs semantic similarity analysis to compare diversity between ASP-guided and unguided LLM generations across six different premise prompts, measuring paragraph-by-paragraph homogeneity scores.

## Key Results
- ASP-guided stories show consistently lower paragraph-by-paragraph homogeneity scores than unguided LLM outputs
- The approach generates over 400,000 distinct outlines using only fifteen simple constraints
- Semantic similarity analysis demonstrates improved diversity across six different premise prompts

## Why This Works (Mechanism)
The neurosymbolic approach leverages ASP's ability to encode complex narrative constraints while maintaining the open-domain generation capabilities of LLMs. By separating structural planning from content generation, the system can produce diverse outlines that satisfy specific narrative requirements while allowing the LLM to focus on coherent and contextually appropriate text generation. This division of labor enables the combination of symbolic precision with neural flexibility.

## Foundational Learning
- Answer Set Programming: Logic-based declarative programming for knowledge representation and reasoning
  - Why needed: Provides formal constraint system for generating diverse, structurally sound story outlines
  - Quick check: Can encode narrative functions and constraints to produce multiple valid story structures

- Large Language Models: Neural networks trained on vast text corpora for open-domain text generation
  - Why needed: Enables coherent and contextually appropriate story content expansion
  - Quick check: Can transform structured outlines into fluent narrative text

- Semantic Similarity Analysis: Computational method for measuring text similarity
  - Why needed: Provides quantitative metric for evaluating story diversity
  - Quick check: Can distinguish between homogeneous and diverse story generations

- Narrative Planning: Traditional approach using detailed symbolic representations
  - Why needed: Establishes baseline for comparing compactness and flexibility
  - Quick check: Requires extensive representations compared to ASP approach

- Constraint Specification: Definition of rules governing narrative structure
  - Why needed: Enables control over story outline diversity and coherence
  - Quick check: Simple constraint sets can generate large variety of valid outlines

- Homogeneity Metrics: Measures of similarity within text segments
  - Why needed: Quantifies diversity at paragraph level
  - Quick check: Lower scores indicate greater diversity between paragraphs

## Architecture Onboarding

Component Map:
ASP Constraint Engine -> Story Outline Generator -> LLM Content Expander -> Final Story

Critical Path:
1. Define narrative functions and constraints in ASP
2. Generate diverse story outlines satisfying constraints
3. Feed outlines to LLM for content expansion
4. Output complete stories

Design Tradeoffs:
- Symbolic precision vs. neural flexibility
- Constraint expressiveness vs. computational efficiency
- Diversity vs. coherence control
- Compact representation vs. detailed planning

Failure Signatures:
- Unrealistic or contradictory story outlines from ASP
- LLM producing incoherent text despite valid outlines
- Limited diversity despite constraint specification
- Semantic similarity metrics failing to capture narrative quality

First Experiments:
1. Test ASP engine with simple narrative constraints to verify outline generation
2. Validate LLM expansion with hand-crafted outlines before automated generation
3. Compare semantic similarity scores on small scale before full evaluation

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding evaluation methods and controllability. It acknowledges that semantic similarity metrics may not fully capture qualitative aspects of narrative quality such as coherence, emotional engagement, or reader satisfaction. The current approach lacks user-directed customization mechanisms, with the authors suggesting future work on interactive tools for constraint specification.

## Limitations
- Evaluation relies primarily on semantic similarity metrics rather than human judgment
- Limited controllability with no interactive user-directed constraint specification
- May not fully capture qualitative narrative aspects like coherence or emotional engagement
- Generalizability beyond tested prompts and LLM configurations remains unproven

## Confidence

| Claim | Confidence |
|-------|------------|
| ASP-guided generation produces more diverse stories than unguided LLMs | Medium |
| Improved compactness and flexibility compared to traditional narrative planning | Medium |
| Semantic similarity analysis adequately captures story diversity | Low |

## Next Checks
1. Conduct human evaluation studies comparing reader preferences and perceived story quality between ASP-guided and unguided LLM outputs across diverse narrative genres
2. Test the approach with additional LLMs and prompt variations to assess generalizability beyond the current experimental setup
3. Implement and evaluate interactive constraint specification tools to measure the practical utility of user-directed narrative control in real-world creative writing scenarios