---
ver: rpa2
title: 'LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large Language
  Models'
arxiv_id: '2406.09008'
source_url: https://arxiv.org/abs/2406.09008
tags:
- topic
- document
- words
- word
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces W ALM, a new evaluation method for topic
  modeling that jointly assesses the semantic quality of both document representations
  and topics. The core idea is to measure the agreement between topical words generated
  by topic models and those from Large Language Models (LLMs), treating the LLM as
  a proxy for human judgment.
---

# LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large Language Models

## Quick Facts
- arXiv ID: 2406.09008
- Source URL: https://arxiv.org/abs/2406.09008
- Authors: Xiaohao Yang; He Zhao; Dinh Phung; Wray Buntine; Lan Du
- Reference count: 26
- Key outcome: Introduces W ALM, a new evaluation method for topic modeling that jointly assesses semantic quality of document representations and topics by measuring agreement between topical words from topic models and LLM-generated keywords

## Executive Summary
This paper introduces W ALM, a novel evaluation method for topic modeling that leverages Large Language Models (LLMs) to automatically assess the semantic quality of both document representations and topics. The core innovation is using LLMs as a proxy for human judgment by comparing their keyword suggestions with topical words generated by topic models. Through experiments on multiple datasets and topic models, W ALM demonstrates strong correlation with human judgment and provides complementary evaluation capabilities to existing methods.

## Method Summary
W ALM evaluates topic models by generating topical words from a document's topic distribution and comparing them with keywords suggested by LLMs. The method uses two main approaches: overlap-based metrics (word overlap and synset overlap) and embedding-based metrics (optimal assignment and optimal transport using GloVe or contextualized embeddings). The paper also introduces topic-aware keyword suggestion, where LLMs first identify corpus-level topics relevant to each document before generating keywords. This creates a comprehensive framework for evaluating both document-topic quality and topic coherence simultaneously.

## Key Results
- W ALM metrics show strong correlation with human judgment, outperforming several traditional topic model evaluation metrics
- Overlap-based metrics demonstrate better sensitivity in handling topic model variations, while embedding-based metrics show smaller evaluation gaps
- Topic-aware keyword suggestion consistently improves evaluation quality by incorporating higher-level conceptual information
- W ALM effectively captures overall topic model performance across different model types and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: W ALM captures overall topic model performance by jointly evaluating document representations and topics through topical word agreement with LLM-generated keywords
- Mechanism: Generates topical words from topic model by combining document-topic proportions with topic-word distributions, then compares these words to LLM-generated keywords using overlap and embedding-based metrics
- Core assumption: LLM-generated keywords serve as a reliable proxy for human judgment of document relevance
- Evidence anchors: Abstract statement about capturing semantic summary of documents; section 4.2 description of generative process; weak corpus evidence
- Break condition: If LLM-generated keywords systematically diverge from human judgment or fail to capture document semantics accurately

### Mechanism 2
- Claim: Embedding-based metrics provide more flexible semantic similarity measurement than overlap-based metrics
- Mechanism: Uses word embeddings (GloVe or contextualized from LLM) to calculate semantic distances between topical words and LLM keywords, allowing for semantic similarity even when exact word matches don't occur
- Core assumption: Pre-trained word embeddings capture semantic relationships between words effectively
- Evidence anchors: Section 4.4 description of optimal transport formulation; section 5.3 discussion of contextualized variants; weak corpus evidence
- Break condition: If word embeddings fail to capture relevant semantic relationships or if contextualized embeddings are too computationally expensive

### Mechanism 3
- Claim: Topic-aware keyword suggestion improves evaluation by incorporating corpus-level topic information
- Mechanism: LLM first identifies relevant corpus-level topics for each document, then generates keywords based on those specific topics, providing higher-level conceptual information beyond individual document content
- Core assumption: Collection-level topics provide meaningful context that improves keyword generation quality
- Evidence anchors: Section 4.3 description of topic-aware prompting; section 5.2 results showing improvement; weak corpus evidence
- Break condition: If corpus-level topic identification fails or if topic-aware keywords don't improve correlation with human judgment

## Foundational Learning

- Concept: Topic modeling fundamentals (LDA, document-topic distributions, topic-word distributions)
  - Why needed here: Understanding how topic models generate topical words is essential for implementing W ALM
  - Quick check question: How does a topic model generate a document's topical word distribution from its document-topic proportions and global topics?

- Concept: Large Language Model prompting techniques
  - Why needed here: W ALM relies on LLMs to generate keywords and topic-aware keywords through specific prompt formats
  - Quick check question: What information should be included in prompts to generate both regular and topic-aware keywords?

- Concept: Word embedding similarity and optimal transport
  - Why needed here: W ALM uses word embeddings and optimal transport to measure semantic similarity between word sets
  - Quick check question: How does optimal transport differ from simple word overlap in measuring similarity between two sets of words?

## Architecture Onboarding

- Component map: Input documents → Topic model inference → Topical word generation → LLM keyword generation → Agreement scoring (overlap/OT/embedding-based) → Evaluation metrics
- Critical path: Document → Topic model inference → Topical words → LLM keywords → Agreement calculation
- Design tradeoffs: Overlap-based metrics are computationally cheaper but less flexible; embedding-based metrics are more semantic but computationally expensive; topic-aware keywords provide better context but require additional LLM calls
- Failure signatures: Low W ALM scores across all models may indicate poor LLM keyword quality; inconsistent rankings across metrics may suggest sensitivity issues; high computational cost may indicate need for optimization
- First 3 experiments:
  1. Run W ALM with basic keyword suggestion on a small dataset to verify basic functionality
  2. Compare overlap-based vs embedding-based metrics on the same dataset to understand sensitivity differences
  3. Test topic-aware keyword suggestion on a dataset with clear topical structure to evaluate improvement over basic keywords

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive are W ALM metrics to different pre-trained word embeddings (GloVe vs contextualized embeddings)?
- Basis in paper: [explicit] The paper conducts a case study comparing static GloVe embeddings with contextualized embeddings from LLMs for the embedding-based W ALM variants (Soa and Sot).
- Why unresolved: The case study only tests one model (NVDM) on 100 documents from each dataset, which may not be representative of all topic models and datasets.
- What evidence would resolve it: Systematic experiments across multiple topic models, datasets, and embedding types would clarify the sensitivity and potential benefits of contextualized embeddings.

### Open Question 2
- Question: Does the performance of W ALM metrics correlate with human judgment across different document types and domains?
- Basis in paper: [explicit] The paper conducts a case study with human annotations on 200 documents and computes evaluation gaps, showing moderate correlation with human judgment.
- Why unresolved: The case study is limited to one topic model (NVDM) and two datasets (20News and DBpedia), and uses a small sample size for human annotations.
- What evidence would resolve it: Large-scale human evaluation studies across diverse document types, domains, and topic models would establish the generalizability of W ALM's correlation with human judgment.

### Open Question 3
- Question: How does the choice of LLM affect the consistency and reliability of W ALM metrics?
- Basis in paper: [explicit] The paper tests different LLMs (LLAMA3-8B-Instruct, Mistral-7B-Instruct-v0.3, Phi-3-Mini-128K-Instruct, and Yi-1.5-9B-Chat) and observes that overlap-based metrics show minimal variation.
- Why unresolved: The study only examines four LLMs and focuses on overlap-based metrics, leaving questions about the impact of LLM choice on embedding-based metrics and other potential LLM variations.
- What evidence would resolve it: Comprehensive testing across a wider range of LLMs, including different model sizes and architectures, would reveal the sensitivity of W ALM to LLM choice and identify the most reliable options.

## Limitations

- Evaluation framework relies heavily on LLM-generated keywords as proxy for human judgment, introducing potential biases
- Computational expense is significant, particularly for embedding-based metrics requiring large matrix computations
- Evaluation limited to English corpora (20News and DBpedia), restricting generalizability to other languages
- Choice of 10 topical words and 5 LLM keywords is arbitrary and may affect sensitivity
- Paper doesn't extensively explore impact of different word embedding models beyond GloVe

## Confidence

- High confidence: Core mechanism that W ALM can measure topic model quality through agreement with LLM keywords
- Medium confidence: Relative performance of overlap-based versus embedding-based metrics
- Medium confidence: Topic-aware keyword suggestion improvement
- Low confidence: Cross-linguistic applicability

## Next Checks

1. **Cross-LLM validation**: Run W ALM using multiple different LLMs (beyond the four tested) on the same datasets to verify consistency of rankings and identify potential LLM-specific biases

2. **Human judgment correlation**: Conduct a new human evaluation study specifically designed to test whether W ALM scores predict human assessments of document-topic relevance and topic coherence

3. **Multilingual extension**: Apply W ALM to non-English corpora (e.g., multilingual news datasets or Wikipedia in different languages) to test generalizability and identify any language-specific limitations