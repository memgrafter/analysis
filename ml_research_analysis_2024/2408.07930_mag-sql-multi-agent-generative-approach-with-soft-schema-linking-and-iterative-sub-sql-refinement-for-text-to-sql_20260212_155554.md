---
ver: rpa2
title: 'MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative
  Sub-SQL Refinement for Text-to-SQL'
arxiv_id: '2408.07930'
source_url: https://arxiv.org/abs/2408.07930
tags:
- schema
- database
- sub-sql
- table
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAG-SQL is a multi-agent generative approach for text-to-SQL that
  addresses complex database schema and difficult questions. It combines soft schema
  linking with table summarization and value retrieval, a novel targets-conditions
  decomposition method, and an iterative Sub-SQL generation and refinement module.
---

# MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL

## Quick Facts
- arXiv ID: 2408.07930
- Source URL: https://arxiv.org/abs/2408.07930
- Authors: Wenxuan Xie; Gaochen Wu; Bowen Zhou
- Reference count: 25
- Primary result: MAG-SQL achieves 61.08% execution accuracy on BIRD benchmark with GPT-4

## Executive Summary
MAG-SQL introduces a multi-agent generative approach for text-to-SQL that addresses the challenges of complex database schemas and difficult questions. The system combines soft schema linking with table summarization and value retrieval, a novel targets-conditions decomposition method, and an iterative Sub-SQL generation and refinement module. By breaking down complex questions into manageable sub-questions and using external execution feedback for refinement, MAG-SQL achieves significant improvements over baseline approaches on the BIRD benchmark.

## Method Summary
MAG-SQL employs a four-agent pipeline for text-to-SQL generation: a Soft Schema Linker that extracts relevant columns through entity-based linking with table summaries and value examples; a Targets-Conditions Decomposer that breaks complex questions into simpler sub-questions based on targets and conditions; a Sub-SQL Generator that creates SQL queries with chain-of-thought reasoning; and a Sub-SQL Refiner that iteratively corrects errors through external execution feedback. The approach uses zero-shot prompting with GPT-4 or GPT-3.5 without fine-tuning, making it adaptable to different database schemas and question types.

## Key Results
- Achieves 61.08% execution accuracy on BIRD benchmark with GPT-4
- Outperforms vanilla GPT-4 (46.35%) by 14.73 percentage points
- Surpasses MAC-SQL (57.56%) by 3.52 percentage points

## Why This Works (Mechanism)

### Mechanism 1: Soft Schema Linking
The Entity-based Schema Linking method reduces noise in the database schema by extracting entities from the question and finding the most relevant columns for each entity. The system first extracts entities from the natural language question, then uses the extracted entities along with table summaries and value examples to find the most relevant columns. This creates a "soft schema" that includes only the columns most likely to be needed.

### Mechanism 2: Targets-Conditions Decomposition
The Targets-Conditions Decomposition method breaks down complex questions into simpler sub-questions that can be answered sequentially. The system decomposes the question into "targets" (what needs to be retrieved) and "conditions" (filters that must be applied). Each sub-question adds one condition to the previous sub-question, creating a cascade of increasingly specific queries.

### Mechanism 3: Iterative Refinement
The iterative Sub-SQL generation and refinement with external execution feedback corrects errors at each step before they propagate. After generating each Sub-SQL, the system executes it to verify correctness. If errors are found, the system refines the SQL before proceeding to the next sub-question. This prevents error accumulation.

## Foundational Learning

- **Entity extraction from natural language**
  - Why needed here: To identify which database columns are relevant to the question
  - Quick check question: Given the question "What are the names of students who scored above 90 in Math?", what entities should be extracted?

- **SQL query decomposition into targets and conditions**
  - Why needed here: To break down complex questions into simpler sub-queries that can be solved sequentially
  - Quick check question: Decompose the question "Find the average salary of employees in the Sales department who have been with the company for more than 5 years" into targets and conditions

- **Iterative refinement based on execution feedback**
  - Why needed here: To correct errors in generated SQL before they propagate to subsequent steps
  - Quick check question: If a generated SQL query returns an empty result when it shouldn't, what are the most common types of errors that should be checked?

## Architecture Onboarding

- **Component map**: Question → Entity extraction → Schema selection → Decomposition → Sub-SQL generation → Execution → Refinement → Next sub-question
- **Critical path**: Natural language question flows through entity extraction, schema linking, decomposition, SQL generation, execution, and refinement to produce final answer
- **Design tradeoffs**:
  - Soft schema vs. hard schema selection: Soft schema retains full context but adds noise; hard schema is cleaner but may lose relevant information
  - Sequential decomposition vs. parallel decomposition: Sequential reduces complexity but may accumulate errors; parallel is faster but more complex
  - Execution-based refinement vs. model-based refinement: Execution is more reliable but slower; model-based is faster but may miss certain errors
- **Failure signatures**:
  - Schema linking failures: Irrelevant columns selected, relevant columns missed
  - Decomposition failures: Sub-questions cannot be answered independently, information lost in decomposition
  - Generation failures: SQL syntax errors, semantic errors, missing joins
  - Refinement failures: Errors persist after multiple refinement attempts
- **First 3 experiments**:
  1. Test entity extraction accuracy on questions with multiple entities and overlapping names
  2. Test decomposition quality on questions with nested conditions and multiple targets
  3. Test refinement effectiveness on queries with common error patterns (missing joins, incorrect filters)

## Open Questions the Paper Calls Out

- **Open Question 1**: How would MAG-SQL perform with open-source LLMs instead of closed-source ones? The paper mentions this as a limitation and suggests it could further improve results. Direct evaluation with fine-tuned open-source LLMs on BIRD and Spider datasets would resolve this.

- **Open Question 2**: Can the MAG-SQL workflow be made more flexible through dynamic agent invocation? The authors identify this as a limitation, noting that the current fixed workflow reduces flexibility. A multi-agent system with autonomous planning capabilities would resolve this.

- **Open Question 3**: What methods could improve the stability of LLM outputs in MAG-SQL? The authors note that LLM outputs are probability-based and not stable. Implementation and evaluation of techniques like ensemble methods or confidence scoring would resolve this.

- **Open Question 4**: How much does the Targets-Conditions Decomposition method contribute to performance gains? The ablation study shows removing the decomposer reduces accuracy, but doesn't isolate its specific contribution. Ablation experiments that keep the iterative generation but remove only the decomposition logic would resolve this.

## Limitations
- The paper lacks ablation studies to isolate the contribution of each mechanism, making it unclear which components drive the 14.73% improvement over vanilla GPT-4
- Evaluation is conducted exclusively on the BIRD benchmark, limiting generalizability to other text-to-SQL datasets like Spider or WikiSQL
- Several critical implementation details are omitted, including specific prompt templates and exact refinement procedures, hindering independent reproduction

## Confidence

**High confidence**: The core methodology of using multi-agent decomposition for text-to-SQL is sound and well-explained.

**Medium confidence**: The reported execution accuracy of 61.08% on BIRD is plausible given the complexity of the benchmark, but the absence of variance metrics makes it difficult to assess consistency.

**Low confidence**: The claim that MAG-SQL significantly outperforms MAC-SQL (57.56%) is questionable without knowing whether both methods used the same underlying LLM model and whether hyperparameters were properly tuned.

## Next Checks

1. **Ablation study**: Conduct controlled experiments to measure the individual contribution of each mechanism (soft schema linking, targets-conditions decomposition, and iterative refinement) by disabling them one at a time and measuring the impact on execution accuracy.

2. **Cross-dataset validation**: Evaluate MAG-SQL on at least two additional text-to-SQL benchmarks (e.g., Spider and WikiSQL) to assess generalization beyond the BIRD dataset, particularly focusing on whether the soft schema linking approach performs well on schemas with different structural characteristics.

3. **Efficiency benchmarking**: Measure and report the average inference time per question, total API token usage, and wall-clock time for MAG-SQL compared to baseline approaches, as the iterative refinement process may introduce significant computational overhead that affects practical applicability.