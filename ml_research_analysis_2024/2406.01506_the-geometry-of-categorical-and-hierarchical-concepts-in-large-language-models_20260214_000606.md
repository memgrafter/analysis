---
ver: rpa2
title: The Geometry of Categorical and Hierarchical Concepts in Large Language Models
arxiv_id: '2406.01506'
source_url: https://arxiv.org/abs/2406.01506
tags:
- representation
- vector
- representations
- binary
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the linear representation hypothesis for large
  language models by showing how to represent categorical and hierarchical concepts
  as vectors and polytopes in the representation space. The authors formalize vector
  representations of binary features, prove that categorical concepts are represented
  as polytopes, and demonstrate that hierarchical structure is encoded as orthogonality
  between representations.
---

# The Geometry of Categorical and Hierarchical Concepts in Large Language Models

## Quick Facts
- arXiv ID: 2406.01506
- Source URL: https://arxiv.org/abs/2406.01506
- Reference count: 40
- Key outcome: This paper extends the linear representation hypothesis for large language models by showing how to represent categorical and hierarchical concepts as vectors and polytopes in the representation space.

## Executive Summary
This paper extends the linear representation hypothesis for large language models by showing how to represent categorical and hierarchical concepts as vectors and polytopes in the representation space. The authors formalize vector representations of binary features, prove that categorical concepts are represented as polytopes, and demonstrate that hierarchical structure is encoded as orthogonality between representations. Empirically validating on Gemma and LLaMA-3 models using WordNet data, they estimate representations for 900+ hierarchically related concepts, finding that the geometric structure of representations aligns with semantic hierarchy. Specifically, they show that test word projections are near 1, random words near 0, and cosine similarities between related concepts reflect WordNet structure.

## Method Summary
The authors estimate vector representations for binary features using Linear Discriminant Analysis (LDA) on vocabulary sets derived from WordNet synsets. They then construct polytope representations for categorical concepts as convex hulls of these vector representations. To validate hierarchical relationships, they use a whitening transformation to align embedding and unembedding spaces, then measure cosine similarities between parent and child concept representations to verify orthogonality encoding.

## Key Results
- Hierarchical semantics in LLMs are linearly represented as orthogonal polytopes in the representation space
- Test word projections are near 1, random words near 0 when projected onto estimated vector representations
- Cosine similarities between related concepts reflect WordNet structure, validating the geometric encoding of hierarchy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Categorical concepts in LLMs are represented as polytopes where each vertex corresponds to a vector representation of one element in the concept.
- Mechanism: The paper extends the linear representation hypothesis by showing that binary features can be represented as vectors (not just directions) with magnitudes derived from the separation between attribute and non-attribute tokens in the representation space. Once binary features have vector representations, categorical concepts can be constructed as convex hulls of these vectors, forming polytopes.
- Core assumption: Vector representations of binary features exist and can be estimated via linear discriminant analysis using vocabulary sets that contain the attribute.
- Evidence anchors: [abstract] "This allows us to immediately formalize the representation of categorical concepts as polytopes in the representation space."
- Break condition: If vector representations of binary features cannot be estimated with sufficient separation between attribute and non-attribute tokens, the polytope construction fails.

### Mechanism 2
- Claim: Hierarchical relationships between concepts are encoded as orthogonality in the representation space.
- Mechanism: The paper proves that if concept Z is subordinate to concept W, then the vector representation of W is orthogonal to the difference between the vector representations of Z and W. This orthogonality captures the semantic hierarchy by ensuring that manipulating one concept doesn't affect the relative probabilities of related concepts.
- Core assumption: The causal inner product can be estimated via whitening transformation, aligning embedding and unembedding spaces.
- Evidence anchors: [abstract] "Specifically, they show that test word projections are near 1, random words near 0, and cosine similarities between related concepts reflect WordNet structure."
- Break condition: If the whitening transformation doesn't properly align the spaces, or if the cosine similarities don't reflect the WordNet structure, the orthogonality encoding fails.

### Mechanism 3
- Claim: The linear representation hypothesis can be extended from binary concepts to general categorical concepts by representing binary features as vectors.
- Mechanism: The paper shows that binary features can be represented as vectors with magnitudes derived from the separation between attribute and non-attribute tokens. These vector representations can then be composed to represent categorical concepts as polytopes and hierarchical relationships as orthogonalities.
- Core assumption: Binary features have linear representations that can be estimated via linear discriminant analysis.
- Evidence anchors: [abstract] "We show how to extend the formalization of the linear representation hypothesis to represent features (e.g., is_animal) as vectors."
- Break condition: If binary features don't have linear representations with the required properties, the extension to categorical concepts fails.

## Foundational Learning

- Concept: Linear Representation Hypothesis
  - Why needed here: The entire paper builds on this hypothesis, extending it from binary concepts to categorical and hierarchical concepts.
  - Quick check question: What is the difference between representing a concept as a direction versus a vector in the context of LLMs?

- Concept: Causal Inner Product
  - Why needed here: The paper relies on transforming the representation spaces to use the causal inner product, which respects the semantics of language.
  - Quick check question: How does the whitening transformation estimate the causal inner product?

- Concept: Polytope Representation
  - Why needed here: The paper shows that categorical concepts are represented as polytopes, which is a key contribution.
  - Quick check question: What is the relationship between the vertices of a polytope representation and the elements of a categorical concept?

## Architecture Onboarding

- Component map: Representation Space (Λ) -> Unembedding Space (Γ) -> Whitening Transformation -> Linear Discriminant Analysis -> WordNet Hierarchy

- Critical path:
  1. Transform representation spaces using whitening to align with causal inner product
  2. Estimate vector representations of binary features using LDA
  3. Validate that test word projections are near 1 and random words near 0
  4. Show that cosine similarities between related concepts reflect WordNet structure
  5. Demonstrate that hierarchical relations are encoded as orthogonality

- Design tradeoffs:
  - Using LDA for estimation vs other methods (tradeoff between accuracy and computational efficiency)
  - Representing concepts as polytopes vs other geometric structures (tradeoff between expressiveness and simplicity)
  - Using WordNet vs other semantic hierarchies (tradeoff between availability and relevance)

- Failure signatures:
  - If test word projections are not near 1, vector representations may not exist
  - If cosine similarities don't reflect WordNet structure, orthogonality encoding may fail
  - If whitening transformation doesn't align spaces, causal inner product may not be properly estimated

- First 3 experiments:
  1. Validate that binary features have vector representations by checking test word projections
  2. Show that cosine similarities between related concepts reflect WordNet structure
  3. Demonstrate that hierarchical relations are encoded as orthogonality in the representation space

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the linear representation hypothesis be extended to capture more complex semantic relationships beyond hierarchical structures, such as analogies or causal relationships?
- Basis in paper: [explicit] The paper discusses extending the linear representation hypothesis from binary concepts to categorical and hierarchical concepts, but does not explore other types of semantic relationships.
- Why unresolved: The paper focuses specifically on hierarchical and categorical concepts, leaving open the question of how the hypothesis might apply to other forms of semantic relationships.
- What evidence would resolve it: Experimental validation showing how linear representations can capture analogies or causal relationships in language models, potentially through empirical studies or theoretical extensions of the current framework.

### Open Question 2
- Question: Can the geometric representation of hierarchical concepts be applied to improve interpretability methods for language models, such as sparse autoencoders?
- Basis in paper: [explicit] The paper discusses implications for interpretability methods, suggesting that hierarchical structure should be respected in methods like sparse autoencoders.
- Why unresolved: While the paper hints at potential applications, it does not provide concrete methods or experiments demonstrating how hierarchical geometric representations could be integrated into existing interpretability frameworks.
- What evidence would resolve it: Implementation of interpretability methods that explicitly account for hierarchical geometry, with experiments showing improved interpretability or performance compared to existing methods.

### Open Question 3
- Question: How can the causal inner product be estimated or approximated for internal layers of language models, beyond the final softmax layer?
- Basis in paper: [inferred] The paper relies on transforming representation spaces using the causal inner product, but notes that this technique only works for the final layer, leaving internal layers unaddressed.
- Why unresolved: The paper does not provide a method for extending the causal inner product to internal layers, which is crucial for understanding the geometry of the entire model.
- What evidence would resolve it: Development of a method to estimate or approximate the causal inner product for internal layers, validated through experiments showing consistent geometric representations across layers.

## Limitations
- The whitening transformation assumption is critical but only empirically supported through WordNet validation, with no theoretical proof of perfect alignment
- Experimental validation is limited to noun synsets from WordNet, not testing generalization to other semantic domains or handling of polysemy
- The paper demonstrates geometric representations but doesn't show practical utility for model interpretability or control

## Confidence
- High Confidence: The theoretical framework extending linear representation hypothesis to categorical and hierarchical concepts
- Medium Confidence: The empirical validation showing cosine similarities align with WordNet structure
- Low Confidence: The practical utility of these geometric representations for model interpretability or control

## Next Checks
1. **Cross-Domain Validation**: Test the polytope and orthogonality representations on verb hierarchies and abstract concept hierarchies (e.g., emotions, events) to verify that the geometric encoding generalizes beyond concrete nouns.

2. **Temporal Stability Analysis**: Track how the geometric representations of hierarchical concepts evolve during model fine-tuning or across different model checkpoints to reveal whether the orthogonal polytope structure is stable.

3. **Intervention Experiment**: Design a controlled experiment where manipulating the vector representations of parent concepts affects the representations of child concepts according to the orthogonality constraints to demonstrate causal meaningfulness.