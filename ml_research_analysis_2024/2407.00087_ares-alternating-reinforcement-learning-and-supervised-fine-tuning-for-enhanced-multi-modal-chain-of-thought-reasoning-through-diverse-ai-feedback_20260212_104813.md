---
ver: rpa2
title: 'ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced
  Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback'
arxiv_id: '2407.00087'
source_url: https://arxiv.org/abs/2407.00087
tags:
- rationale
- feedback
- reasoning
- ares
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ARES, a two-stage framework that alternates
  reinforcement learning (RL) and supervised fine-tuning (SFT) to improve multi-modal
  Chain-of-Thought reasoning. ARES uses an advanced AI model as a teacher to provide
  sentence-level feedback for RL, allowing granular reward shaping, and correction
  feedback for SFT to stabilize the RL process.
---

# ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback

## Quick Facts
- arXiv ID: 2407.00087
- Source URL: https://arxiv.org/abs/2407.00087
- Authors: Ju-Seung Byun; Jiyun Chun; Jihyung Kil; Andrew Perrault
- Reference count: 25
- Key outcome: ~70% win rate against baselines judged by GPT-4o, 2.5% average increase in inference answer accuracy

## Executive Summary
ARES is a two-stage framework that alternates reinforcement learning (RL) and supervised fine-tuning (SFT) to improve multi-modal Chain-of-Thought reasoning. The method uses an advanced AI model (Claude 3 Haiku) as a teacher to provide sentence-level feedback for RL and correction feedback for SFT. Experiments on ScienceQA and A-OKVQA datasets show ARES achieves approximately 70% win rate against baselines judged by GPT-4o and increases inference answer accuracy by 2.5% on average.

## Method Summary
ARES implements a hybrid algorithm that alternates between RL fine-tuning using sentence-level feedback from Claude 3 Haiku and SFT using correction feedback from the same teacher model. The RL stage uses PPO with sentence-level scores (0.0-1.0) indicating each sentence's contribution to problem-solving, while the SFT stage corrects errors identified in RL-generated rationales. This approach addresses the degeneration problems of pure RL while allowing the model to explore new reasoning patterns.

## Key Results
- Achieves ~70% win rate against baselines in rationale quality judged by GPT-4o
- Improves inference answer accuracy by 2.5% on average across ScienceQA and A-OKVQA
- Successfully generates more coherent and accurate multi-modal Chain-of-Thought reasoning chains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating RL and SFT stages allows the model to explore new reasoning patterns through RL while stabilizing them through SFT.
- Mechanism: RL fine-tuning encourages deviation from original distribution to maximize rewards, while SFT corrects errors like repetitive or incomplete sentences and aligns the model with intended reasoning patterns without degeneration.
- Core assumption: Teacher model's correction feedback accurately identifies and corrects errors in RL-generated rationales.
- Evidence: Abstract states "With the correction feedback, we stabilize the RL fine-tuned model through SFT."

### Mechanism 2
- Claim: Sentence-level feedback from teacher model provides more granular rewards than ranking-based feedback, allowing for more nuanced learning.
- Mechanism: Individual sentence scoring enables understanding which parts of reasoning are most valuable and adjusting accordingly, rather than treating entire generations as monolithic units.
- Core assumption: Teacher model can reliably evaluate contribution of each sentence to solving the problem.
- Evidence: Abstract mentions "request the Teacher to score how much each sentence contributes to solving the problem in a Chain-of-Thought (CoT)."

### Mechanism 3
- Claim: Two-stage approach addresses limitations of both pure RL and pure SFT by combining their strengths.
- Mechanism: RL allows exploration and discovery of new reasoning patterns, while SFT provides stability and error correction, resulting in rationales that are both innovative and well-formed.
- Core assumption: Combination of RL and SFT produces better results than either method alone.
- Evidence: Paper proposes "a hybrid algorithm ARES that Alternates REinforcement Learning and Supervised Fine-Tuning."

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed: Understanding RLHF basics is crucial for grasping how ARES uses AI feedback to fine-tune the model.
  - Quick check: What is the main difference between RLHF and RLAIF?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed: ARES specifically targets multi-modal CoT reasoning, so understanding this concept is essential for understanding the problem domain.
  - Quick check: How does CoT reasoning differ from direct answer generation?

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed: SFT is the second stage in ARES and is used to stabilize the model after RL fine-tuning.
  - Quick check: What are the main advantages of SFT over RL for fine-tuning language models?

## Architecture Onboarding

- Component map: Teacher model (Claude 3 Haiku) -> RL fine-tuning stage -> SFT fine-tuning stage -> LoRA adapter
- Critical path: Generate rationales using baseline model → Request sentence-level scores from teacher model → Perform RL fine-tuning using these scores → Request correction feedback from teacher model → Perform SFT using corrected data → Repeat until convergence
- Design tradeoffs: API usage incurs costs and has usage limits but eliminates need for reward model training; alternating RL/SFT provides stability but may slow down learning; LoRA is faster but may have slightly lower performance than separate inference model
- Failure signatures: Model generates repetitive or incomplete sentences after RL stage; teacher model provides inconsistent or inaccurate feedback; model fails to improve rationale quality despite multiple ARES iterations
- First 3 experiments: 1) Run ARES pipeline once on small ScienceQA subset and compare rationale quality to baseline; 2) Test different learning rates for RL stage to find optimal hyperparameters; 3) Compare performance of ARES with and without SFT stage to quantify stabilization benefits

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided text.

## Limitations
- Reliance on Claude 3 Haiku for sentence-level feedback may introduce inconsistencies
- Alternating RL/SFT approach may suffer from suboptimal hyperparameter balancing
- Evaluation methodology using GPT-4o as judge raises questions about potential biases

## Confidence
- High confidence: Fundamental concept of alternating RL and SFT for stability, experimental methodology showing 2.5% accuracy improvement, win rate metrics against baselines
- Medium confidence: Effectiveness of sentence-level feedback compared to ranking-based approaches, specific contribution of each ARES component to overall performance
- Low confidence: Generalizability of ARES to other reasoning tasks beyond ScienceQA and A-OKVQA, long-term stability of alternating training approach

## Next Checks
1. **Teacher model consistency audit**: Evaluate inter-rater reliability of Claude 3 Haiku's sentence-level scores by having multiple teacher models score same rationales and measuring agreement rates.
2. **Component ablation study**: Run controlled experiments comparing ARES performance when removing either RL stage, SFT stage, or both to quantify specific contribution of each component.
3. **Cross-dataset generalization test**: Apply ARES to additional multi-modal reasoning datasets (such as MMMU or MMBench) to assess whether framework generalizes beyond initial ScienceQA and A-OKVQA benchmarks.