---
ver: rpa2
title: 'Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee
  Discussions'
arxiv_id: '2405.20267'
source_url: https://arxiv.org/abs/2405.20267
tags:
- questions
- evaluation
- peer
- auto-arena
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Auto-Arena introduces a fully automated LLM evaluation framework
  using LLM-powered agents for question generation, peer battles, and committee discussions.
  It addresses limitations of static benchmarks (inflexibility, contamination) and
  human evaluations (manual effort) by simulating human-like evaluation processes.
---

# Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions

## Quick Facts
- arXiv ID: 2405.20267
- Source URL: https://arxiv.org/abs/2405.20267
- Reference count: 40
- Key outcome: Auto-Arena achieves 92.14% Spearman correlation with human preferences, surpassing all previous benchmarks without manual efforts

## Executive Summary
Auto-Arena introduces a fully automated LLM evaluation framework that uses LLM-powered agents for question generation, peer battles, and committee discussions. It addresses limitations of static benchmarks (inflexibility, contamination) and human evaluations (manual effort) by simulating human-like evaluation processes. In experiments with 15 LLMs, Auto-Arena achieves 92.14% Spearman correlation with human preferences—surpassing all previous benchmarks without manual efforts. The peer battle mechanism reveals deeper capabilities and performance gaps, while committee discussions reduce single-model bias. Auto-Arena can be easily extended to other domains/languages and offers timely, trustworthy evaluations for evolving LLMs.

## Method Summary
Auto-Arena operates through a three-stage process: an LLM examiner dynamically generates questions, two LLM candidates engage in multi-round peer battles based on these questions, and a committee of LLM judges collaboratively discusses and decides the winner. The framework uses a swiss-style pairing mechanism and Elo rating system to produce a final leaderboard. Unlike static benchmarks, Auto-Arena's dynamic nature reduces contamination risks while providing richer evaluation data through interactive debates. The committee discussion mechanism enhances fairness by reducing individual judge bias through collective intelligence.

## Key Results
- Achieves 92.14% Spearman correlation with human preferences from Chatbot Arena
- Outperforms all previous benchmarks in ranking LLMs without manual evaluation efforts
- Reduces contamination percentage to 2% compared to static datasets
- Improves agreement among judges from 0.43 to 0.54 through committee discussions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Peer battles reveal deeper capabilities and performance gaps compared to static benchmarks
- Mechanism: Multi-round debate format forces candidates to demonstrate reasoning, strategizing, and adaptive capabilities beyond simple question-answering
- Core assumption: Performance gaps become more visible when models must engage in competitive interactions and defend their responses
- Evidence anchors:
  - [abstract] "two LLM candidates engage in a multi-round peer battle based on individual questions, aiming at revealing their true performance differences"
  - [section 2.2] "the performance gaps between candidates become more visible: Candidate B is able to provide a more elaborate and helpful response when explaining the theories behind the initial answer"
  - [corpus] Weak - no direct citations found in neighboring papers
- Break condition: If models become too similar in capability, peer battles may not differentiate performance effectively

### Mechanism 2
- Claim: Committee discussions reduce single-model bias and improve judgment quality
- Mechanism: Multiple judge agents with diverse capabilities collaborate and reach consensus, reducing individual model-specific biases
- Core assumption: Collective intelligence from diverse viewpoints produces more reliable evaluations than single-model judgments
- Evidence anchors:
  - [abstract] "a committee of LLM judges collaboratively discusses and decides the winner, reducing bias and enhancing fairness"
  - [section 2.3] "agreement increased to an average of 0.54, which indicates moderate agreement" and "Agreement probability is defined as the mean probability of two random judges agreeing with each other. After committee discussion, the agreement increases by 11%"
  - [section 3.3] "agreement increased to an average of 0.54, which indicates moderate agreement"
- Break condition: If committee members are too homogeneous in capability, the diversity benefit is lost

### Mechanism 3
- Claim: Dynamic question generation reduces contamination and self-enhancement bias
- Mechanism: Questions are generated by an examiner agent rather than drawn from static datasets, reducing exposure risk and examiner's advantage
- Core assumption: Dynamically generated questions are less likely to have been seen during training compared to static benchmark questions
- Evidence anchors:
  - [abstract] "instead of the simple and one-round question-answering scheme, Auto-Arena introduces a dynamic multi-round peer battle, which displays deeper abilities of LLMs, such as reasoning, interacting, and strategizing. The dynamic nature of peer battles also reduces contamination risks"
  - [section 2.1] "as using a static dataset could incur data contamination concerns and result in unfair evaluations, we ask an LLM examiner agent to dynamically generate questions"
  - [appendix C] "Auto-Arena, by generating fresh questions, does alleviate the contamination issue. Compared to static datasets, Auto-Arena's contamination percentage (2%) according to the exact match is significantly lower"
- Break condition: If question generation becomes predictable or follows patterns that models can exploit

## Foundational Learning

- Concept: Spearman correlation as evaluation metric
  - Why needed here: Measures monotonic relationship between Auto-Arena rankings and human preferences from Chatbot Arena
  - Quick check question: If Auto-Arena achieves 92.14% Spearman correlation with human preferences, what does this indicate about the ranking order?

- Concept: Elo rating system for model ranking
  - Why needed here: Provides a dynamic ranking mechanism that updates as models compete, similar to chess rankings
  - Quick check question: How does the Elo system update ratings after a debate where a lower-ranked model defeats a higher-ranked model?

- Concept: Contamination detection in LLM evaluation
  - Why needed here: Ensures test questions haven't been seen during model training, which would invalidate performance measurements
  - Quick check question: What are the two detection methods used to measure contamination, and how do they differ in approach?

## Architecture Onboarding

- Component map: Examiner (question generation) → Candidates (peer battles) → Committee (judgment) → Ranking (Elo scores)
- Critical path: Question generation → Peer battle execution → Committee discussion → Winner determination → Elo score update
- Design tradeoffs: Dynamic generation vs. contamination risk; single judge vs. committee bias; complexity vs. interpretability
- Failure signatures: Low inter-judge agreement; inconsistent rankings across runs; questions that are too easy/hard
- First 3 experiments:
  1. Run with GPT-4 vs. Claude-3 on synthetic math questions, verify ranking matches expectations
  2. Test committee discussion impact by comparing agreement before/after discussion
  3. Validate contamination analysis by comparing Auto-Arena questions against static benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the peer battle mechanism in Auto-Arena consistently improve evaluation quality across different LLM families and capabilities, or are there specific conditions where it may not be effective?
- Basis in paper: [explicit] The paper shows that peer battles improve Spearman correlation with human preferences by 5% compared to simple question-answering, but this was tested on a specific set of 15 LLMs.
- Why unresolved: The effectiveness of peer battles may vary depending on the specific LLM architectures, training methods, or capability levels involved. The paper only tested on a limited set of popular models.
- What evidence would resolve it: Systematic testing of Auto-Arena across a broader and more diverse set of LLMs (including smaller models, specialized models, and models with different training paradigms) to determine if peer battles consistently improve evaluation quality or if there are edge cases where they may not be beneficial.

### Open Question 2
- Question: How does the quality of Auto-Arena evaluations change when using different committee sizes (e.g., 3 vs 5 vs 7 judges) and what is the optimal committee size for balancing evaluation quality with computational cost?
- Basis in paper: [inferred] The paper uses a committee of 5 judges based on current rankings, but doesn't explore how committee size affects evaluation quality or agreement levels.
- Why unresolved: The paper demonstrates that committee discussions improve agreement and evaluation quality, but doesn't investigate whether this effect scales with committee size or if there's a point of diminishing returns.
- What evidence would resolve it: Comparative studies measuring evaluation quality metrics (correlation with human preferences, inter-judge agreement) and computational costs across different committee sizes to identify optimal tradeoffs.

### Open Question 3
- Question: Can the peer battle mechanism in Auto-Arena be adapted to evaluate LLMs on specialized tasks beyond general conversation, such as code generation, mathematical theorem proving, or domain-specific knowledge?
- Basis in paper: [explicit] The paper mentions that Auto-Arena can be easily adapted to other domains or languages by altering prompts, and provides a Chinese language case study.
- Why unresolved: While the framework shows promise for general evaluation, its effectiveness for specialized domains with unique evaluation criteria hasn't been demonstrated. The current peer battle format may need modification for certain specialized tasks.
- What evidence would resolve it: Implementation and validation of Auto-Arena variants for specific specialized domains, measuring performance against established benchmarks and human experts in those domains.

## Limitations

- Contamination analysis based on small test set (31 questions) without addressing other contamination sources
- Claims of "superior performance" based on correlation with Chatbot Arena rather than direct comparison on same questions
- Effectiveness of peer battles not systematically tested across diverse LLM architectures and capabilities

## Confidence

**High confidence**: The core architectural design of Auto-Arena (examiner-candidates-committee) is clearly specified and reproducible. The Elo rating system implementation and swiss-style pairing mechanism are standard approaches with well-understood properties.

**Medium confidence**: The claim that committee discussions reduce bias is supported by observed agreement increases (from 0.43 to 0.54), but the interpretation of "moderate agreement" and whether this actually improves evaluation quality requires further validation.

**Low confidence**: The assertion that Auto-Arena achieves "superior performance" to all previous benchmarks is based on correlation with Chatbot Arena rather than direct comparison on the same questions, making it difficult to isolate Auto-Arena's specific contributions.

## Next Checks

1. **Cross-dataset validation**: Test Auto-Arena's rankings against human preferences on completely separate datasets (beyond Chatbot Arena) to verify the 92.14% correlation generalizes across different domains and question types.

2. **Ablation study on committee size**: Systematically vary the number of judge agents in the committee (1, 3, 5, 7) and measure the impact on inter-judge agreement, ranking stability, and correlation with human preferences to identify the optimal committee size.

3. **Dynamic question quality analysis**: Track question difficulty distribution over time to determine if the examiner agent tends to generate questions that become easier or harder as it gains experience, which could indicate emergent bias in the question generation process.