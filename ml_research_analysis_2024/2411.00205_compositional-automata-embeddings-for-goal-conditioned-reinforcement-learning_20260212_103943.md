---
ver: rpa2
title: Compositional Automata Embeddings for Goal-Conditioned Reinforcement Learning
arxiv_id: '2411.00205'
source_url: https://arxiv.org/abs/2411.00205
tags:
- cdfa
- task
- figure
- tasks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces compositional DFAs (cDFAs) as a goal representation
  for temporal tasks in goal-conditioned reinforcement learning. The authors address
  the challenge of conditioning RL policies on complex temporal goals by encoding
  cDFAs using graph attention networks (GATv2) and pre-training on reach-avoid derived
  (RAD) cDFAs.
---

# Compositional Automata Embeddings for Goal-Conditioned Reinforcement Learning

## Quick Facts
- arXiv ID: 2411.00205
- Source URL: https://arxiv.org/abs/2411.00205
- Authors: Beyazit Yalcinkaya; Niklas Lauffer; Marcell Vazquez-Chanlatte; Sanjit A. Seshia
- Reference count: 40
- Primary result: Policies trained with frozen pre-trained cDFA encoders achieve over 95% task satisfaction in discrete environments and strong generalization across different cDFA task classes

## Executive Summary
This paper introduces compositional DFAs (cDFAs) as a goal representation for temporal tasks in goal-conditioned reinforcement learning. The authors address the challenge of conditioning RL policies on complex temporal goals by encoding cDFAs using graph attention networks (GATv2) and pre-training on reach-avoid derived (RAD) cDFAs. They demonstrate that pre-training on RAD cDFAs enables zero-shot generalization to various cDFA task classes and accelerates policy specialization. The primary results show that policies trained with frozen pre-trained cDFA encoders achieve over 95% task satisfaction in discrete environments and strong generalization across different task classes, including those with up to 10 conjunctions or 10 symbols per DFA. The approach outperforms hierarchical methods and achieves comparable results to LTL-based approaches while offering advantages in interpretability and formal semantics.

## Method Summary
The method uses graph attention networks (GATv2) to encode compositional DFAs into embeddings that condition RL policies. The approach involves pre-training the cDFA encoder on reach-avoid derived (RAD) DFAs in a dummy MDP, then using the frozen pre-trained encoder during policy training. The cDFA encoder processes the DFA graph structure with 32-dimensional hidden states, 8 message passing steps, and 4 attention heads. The policy network (actor-critic with 64-unit hidden layers) receives concatenated state observations and cDFA embeddings. Pre-training occurs on RAD cDFA distributions in a single-state dummy MDP for 10M timesteps, followed by policy training on target tasks for 20M timesteps (Letterworld) or 50M timesteps (Zones).

## Key Results
- Policies trained with frozen pre-trained cDFA encoders achieve over 95% task satisfaction in discrete environments
- Zero-shot generalization from RAD pre-training to cRAR, cRA, and cReach task classes
- Outperforms hierarchical methods and achieves comparable results to LTL-based approaches
- Handles cDFAs with up to 10 conjunctions or 10 symbols per DFA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on reach-avoid derived (RAD) cDFAs enables zero-shot generalization to other cDFA task classes.
- Mechanism: The RAD cDFA distribution contains a rich variety of DFAs that include the local reach-avoid structure present in many temporal tasks. By pre-training the cDFA encoder on this diverse distribution, the model learns to recognize and encode the structural patterns common across different task classes. This learned representation generalizes to new cDFA classes because they share underlying reach-avoid substructures.
- Core assumption: RAD cDFAs adequately represent the structural diversity needed to generalize to other cDFA classes.
- Evidence anchors:
  - [abstract]: "we observe that all paths through a DFA correspond to a series of reach-avoid tasks and propose pre-training graph neural network embeddings on 'reach-avoid derived' DFAs"
  - [section 4]: "Building on the observation that paths of a DFA correspond to SRA tasks, we define reach-avoid derived DFAs as a generalization of SRA"
  - [corpus]: Weak evidence - no direct citations to support RAD generalization claims
- Break condition: If the target cDFA classes contain structures not present in RAD (e.g., complex nested dependencies or patterns beyond reach-avoid), the pre-trained encoder may fail to capture these nuances.

### Mechanism 2
- Claim: Freezing the pre-trained cDFA encoder during policy training improves performance compared to allowing gradient updates.
- Mechanism: The pre-trained encoder learns a rich, domain-independent representation of cDFAs on the RAD distribution. Freezing it prevents catastrophic forgetting of this learned representation while the policy network adapts to the specific MDP dynamics. Allowing gradient updates may cause the encoder to overfit to the specific task distribution, losing the generalization benefits.
- Core assumption: The RAD pre-trained representation is sufficiently rich to handle the target MDP without fine-tuning.
- Evidence anchors:
  - [section 5]: "Notably, non-frozen pre-training performs significantly worse for RGCN than its frozen counterpart, showing that freezing the MPNN module after pre-training actually helps performance"
  - [abstract]: "policies trained with frozen pre-trained cDFA encoders achieve over 95% task satisfaction"
  - [corpus]: Weak evidence - no direct citations comparing frozen vs. non-frozen approaches
- Break condition: If the target MDP has significantly different characteristics than the RAD distribution (e.g., continuous vs. discrete), freezing may prevent necessary adaptation.

### Mechanism 3
- Claim: Using graph attention networks (GATv2) for cDFA embedding is more effective than relational graph convolutional networks (RGCN).
- Mechanism: GATv2's attention mechanism allows the model to dynamically weight the importance of different structural elements in the cDFA graph during message passing. This enables better capture of the semantic relationships between states, transitions, and conjunctions in the cDFA, leading to more informative embeddings for policy conditioning.
- Core assumption: The attention mechanism in GATv2 is necessary to capture the semantic nuances in cDFA structures.
- Evidence anchors:
  - [section 5]: "Notably, non-frozen pre-training performs significantly worse for RGCN than its frozen counterpart, showing that freezing the MPNN module after pre-training actually helps performance. We further note that the wall clock training time was significantly faster due to not propagating gradients through the MPNN"
  - [abstract]: "policies trained with frozen pre-trained cDFA encoders achieve over 95% task satisfaction"
  - [corpus]: Weak evidence - no direct citations comparing GATv2 vs RGCN performance
- Break condition: If the cDFA structures are relatively simple or if the attention mechanism introduces unnecessary complexity, RGCN might perform comparably with less computational overhead.

## Foundational Learning

- Concept: Deterministic Finite Automata (DFA) and their Boolean compositions (cDFAs)
  - Why needed here: The entire method relies on representing temporal tasks as compositions of DFAs, so understanding DFA structure, transitions, and Boolean operations is fundamental.
  - Quick check question: Given two DFAs A and B, what is the maximum number of states in their conjunction (A ∧ B)?

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: The cDFA encoder uses a GATv2 (a type of GNN) to convert cDFA graph structures into embeddings. Understanding how message passing works and how attention mechanisms operate is crucial.
  - Quick check question: In a GATv2, what does the attention score αvu represent between nodes v and u?

- Concept: Reinforcement Learning (RL) and goal-conditioned policies
  - Why needed here: The method uses RL to learn policies that can satisfy cDFA-specified goals. Understanding how goal-conditioned RL differs from standard RL and how policies condition on goals is essential.
  - Quick check question: In goal-conditioned RL, how does the policy architecture typically incorporate the goal representation?

## Architecture Onboarding

- Component map: MDP environment -> cDFA task generator -> cDFA encoder (GATv2) -> Policy network (PPO) -> Environment
- Critical path: 1. Generate cDFA task 2. Encode cDFA using pre-trained (frozen) GATv2 to get embedding 3. Concatenate embedding with state observation 4. Pass through policy network to get action 5. Execute action in environment 6. Update state and DFA (transition DFA based on observation) 7. Repeat until goal reached or timeout
- Design tradeoffs:
  - Pre-training vs. training from scratch: Pre-training provides generalization but requires additional computation and may not adapt well to specific domains
  - Frozen vs. non-frozen encoder: Freezing preserves generalization benefits but may limit adaptation to specific MDP characteristics
  - GATv2 vs. RGCN: GATv2 potentially captures richer semantics but with higher computational cost
- Failure signatures:
  - Poor generalization: If policies fail on out-of-distribution cDFA classes, the encoder may not have learned sufficient structural patterns
  - Slow learning: If pre-training doesn't help, the encoder representation may not be informative enough
  - Overfitting: If non-frozen training performs better, the pre-trained representation may be too general
- First 3 experiments:
  1. Verify encoder pre-training: Test the pre-trained cDFA encoder on the dummy MDP to ensure it can correctly classify cDFA acceptance/rejection
  2. Validate policy learning: Train a policy on a simple RAD task class (e.g., cReach.1.1.1.3) and verify it achieves high task satisfaction
  3. Test generalization: Use a policy trained on RAD to solve a different task class (e.g., cRAR) and measure performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RAD pre-training approach scale to more complex temporal tasks with longer sequences or larger compositions of DFAs?
- Basis in paper: [explicit] The paper mentions that policies trained on RAD cDFAs generalize well to tasks with up to 10 conjunctions or 10 symbols per DFA, but does not explore tasks beyond these limits.
- Why unresolved: The paper does not provide evidence of the approach's performance on tasks with longer sequences or larger compositions of DFAs, leaving open the question of scalability.
- What evidence would resolve it: Experiments demonstrating the performance of the approach on tasks with longer sequences or larger compositions of DFAs would provide evidence of its scalability.

### Open Question 2
- Question: Can the RAD pre-training approach be extended to handle non-deterministic finite automata (NFA) or other types of automata?
- Basis in paper: [inferred] The paper focuses on deterministic finite automata (DFA) and does not explore the use of other types of automata, such as NFA.
- Why unresolved: The paper does not provide evidence of the approach's performance on tasks involving NFA or other types of automata, leaving open the question of its applicability to other automata.
- What evidence would resolve it: Experiments demonstrating the performance of the approach on tasks involving NFA or other types of automata would provide evidence of its applicability to other automata.

### Open Question 3
- Question: How does the choice of geometric distribution parameters (pn and pk) affect the performance of the RAD pre-training approach?
- Basis in paper: [explicit] The paper mentions that the geometric distribution parameters (pn and pk) are set to 0.5, but does not explore the effect of different parameter choices on the approach's performance.
- Why unresolved: The paper does not provide evidence of the approach's performance with different geometric distribution parameters, leaving open the question of the optimal parameter choice.
- What evidence would resolve it: Experiments demonstrating the performance of the approach with different geometric distribution parameters would provide evidence of the optimal parameter choice.

## Limitations
- Limited empirical validation on real-world robotic or continuous control tasks beyond simplified environments
- RAD pre-training assumption lacks rigorous testing against alternative pre-training distributions
- Alphabet mapping function from MDP states to DFA symbols is underspecified
- Limited comparison to LTL-based methods, focusing on specific cases rather than comprehensive benchmarks

## Confidence
- **High Confidence:** The core mechanism of using graph neural networks to embed cDFA structures is technically sound and the basic experimental results (95%+ task satisfaction) are reproducible given the specifications
- **Medium Confidence:** The generalization claims across cDFA classes are supported by experiments but rely heavily on the RAD distribution assumption without exploring alternatives
- **Low Confidence:** The superiority claims over hierarchical methods and LTL-based approaches are based on limited comparisons that may not generalize to other domains

## Next Checks
1. **Distribution Validation:** Test the pre-trained encoder on cDFA classes not derived from reach-avoid structure to verify the RAD generalization assumption
2. **Alternative Architectures:** Compare GATv2 performance against simpler GNN variants (RGCN, GCN) on the same tasks to quantify the attention mechanism's contribution
3. **Alphabet Sensitivity:** Systematically vary the alphabet mapping function to determine how sensitive performance is to this critical but underspecified component