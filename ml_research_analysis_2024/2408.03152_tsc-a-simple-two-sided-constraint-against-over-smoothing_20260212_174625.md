---
ver: rpa2
title: 'TSC: A Simple Two-Sided Constraint against Over-Smoothing'
arxiv_id: '2408.03152'
source_url: https://arxiv.org/abs/2408.03152
tags:
- layer
- node
- over-smoothing
- nodes
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses over-smoothing in graph convolutional networks
  (GCNs) caused by high-order neighbor aggregation leading to indistinguishable node
  representations. The proposed Two-Sided Constraint (TSC) method combines random
  masking on representation matrix columns to regulate information aggregation and
  prevent convergence, with contrastive constraints on rows to enhance node discriminability.
---

# TSC: A Simple Two-Sided Constraint against Over-Smoothing

## Quick Facts
- **arXiv ID**: 2408.03152
- **Source URL**: https://arxiv.org/abs/2408.03152
- **Reference count**: 40
- **Primary result**: TSC combines random column masking and row-wise contrastive constraints to effectively mitigate over-smoothing in deep GCNs, achieving state-of-the-art performance on 5 real-world graph datasets.

## Executive Summary
This paper addresses the over-smoothing problem in graph convolutional networks (GCNs) where deep networks cause node representations to become indistinguishable. The proposed Two-Sided Constraint (TSC) method uniquely combines random masking on representation matrix columns to prevent convergence with contrastive constraints on rows to enhance node discriminability. The method is implemented as a plug-in module for both SGC and GCN architectures and shows significant improvements in classification accuracy while reducing representation convergence across multiple benchmark datasets.

## Method Summary
TSC combines two complementary mechanisms to combat over-smoothing: column-wise random masking that regulates information aggregation from neighbors to prevent representation convergence, and row-wise contrastive constraints that maintain node discriminability by pushing different nodes apart. The method is applied as a plug-in module to both SGC and GCN architectures, with masking applied at multiple layers and contrastive constraints added either at every layer (SGC) or just the final layer (GCN). The approach addresses both causes of over-smoothing simultaneously - neighbor quantity issues through masking and neighbor quality issues through contrastive learning.

## Key Results
- TSC effectively mitigates over-smoothing in deep GCNs, maintaining node discriminability even with 32 layers
- The method achieves state-of-the-art classification accuracy on Cora, Citeseer, Pubmed, CoauthorCS, and AmazonPhoto datasets
- TSC reduces Mean Average Distance (MAD) metrics, demonstrating improved representation diversity compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random masking on representation matrix columns prevents representation convergence while preserving node individuality.
- Mechanism: By randomly dropping columns of aggregated information and replacing them with the previous layer's representation, the model avoids complete homogenization of node features that occurs with deep aggregation.
- Core assumption: Column-wise masking maintains enough structural information while breaking the perfect convergence pattern.
- Evidence anchors:
  - [abstract]: "The random masking acts on the representation matrix's columns to regulate the degree of information aggregation from neighbors, thus preventing the convergence of node representations."
  - [section]: "The masking strategy is only applied after layer 3 (ð‘™ â‰¥ 3). In deeper layers, nodes' neighbors become more similar, making over-smoothing more likely to occur."
  - [corpus]: Weak. Corpus doesn't directly mention column-wise masking approaches.
- Break condition: If masking rate is too high, the model may lose too much structural information; if too low, convergence still occurs.

### Mechanism 2
- Claim: Contrastive constraints on representation matrix rows enhance node discriminability by pushing different nodes apart.
- Mechanism: The model treats identical nodes across consecutive layers as positive pairs and different nodes as negative pairs, creating a contrastive loss that maintains diversity.
- Core assumption: Nodes that are truly distinct should have representations that remain distinguishable across layers.
- Evidence anchors:
  - [abstract]: "the contrastive constraint, applied to the representation matrix's rows, enhances the discriminability of the nodes."
  - [section]: "the contrastive constraint not only brings the node's representation close to its last layer but also separates it from other nodes' representations."
  - [corpus]: Weak. While contrastive learning is mentioned in related papers, none specifically address row-wise contrastive constraints for over-smoothing.
- Break condition: If temperature coefficient Ï„ is too high or low, the contrastive effect becomes ineffective.

### Mechanism 3
- Claim: The two-sided constraint approach (column + row operations) addresses both causes of over-smoothing simultaneously.
- Mechanism: Column masking handles neighbor quantity issues while row-wise contrastive learning addresses neighbor quality issues.
- Core assumption: Over-smoothing has two distinct causes that require different interventions.
- Evidence anchors:
  - [abstract]: "Current solutions mainly focus on one of the above causes and seldom consider both at once."
  - [section]: "We provide a new perspective of graph over-smoothing in view of neighbor overlapping and individuality overwhelmed."
  - [corpus]: Weak. Corpus papers focus on either filtering or enhancement approaches, not both simultaneously.
- Break condition: If one side dominates (too much masking or too strong contrastive constraint), the balance is lost.

## Foundational Learning

- Concept: Graph Convolutional Networks and message passing
  - Why needed here: The paper builds directly on GCN and SGC architectures, so understanding how information propagates through graph layers is essential.
  - Quick check question: How does the aggregation operation in GCN differ from standard convolution, and why does this lead to over-smoothing in deep layers?

- Concept: Over-smoothing phenomenon in deep GNNs
  - Why needed here: The entire paper is motivated by and addresses over-smoothing, so understanding its causes and effects is critical.
  - Quick check question: What happens to node representations after many layers of message passing, and why do they become indistinguishable?

- Concept: Contrastive learning and positive/negative pair construction
  - Why needed here: The contrastive constraint component relies on understanding how to construct and optimize contrastive losses.
  - Quick check question: In contrastive learning, what makes a good positive pair versus a negative pair, and how does this apply to node representations across layers?

## Architecture Onboarding

- Component map:
  Input -> Feature transformation -> Layer-wise message passing with masking -> Contrastive constraint application -> Classification output

- Critical path:
  1. Initial feature transformation (for SGC)
  2. Layer-wise message passing with column masking
  3. Contrastive constraint application (SGC: every layer; GCN: last layer only)
  4. Final classification

- Design tradeoffs:
  - Masking rate schedule: Early layers preserve more information, later layers increase masking to prevent convergence
  - Contrastive constraint placement: SGC applies to all layers, GCN only to last layer due to computational constraints
  - Temperature coefficient Ï„: Balances the strength of contrastive learning

- Failure signatures:
  - Too much masking: Representations become too sparse and lose graph structure information
  - Too strong contrastive constraint: Nodes may be pushed too far apart, losing meaningful similarity
  - Imbalanced approach: Focusing only on masking or only on contrastive learning won't address both over-smoothing causes

- First 3 experiments:
  1. Baseline test: Run SGC with varying depth (4, 8, 16, 32 layers) to confirm over-smoothing without any constraints
  2. Masking test: Apply column masking with different masking rates to observe convergence behavior
  3. Contrastive test: Add contrastive constraints to masked representations and measure discriminability improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the random masking strategy in TSC compare to other neighbor filtering methods (e.g., DropMessage, DropEdge) in terms of computational efficiency and effectiveness in mitigating over-smoothing?
- Basis in paper: [explicit] The paper discusses the differences between random masking and other neighbor filtering methods like DropMessage, highlighting that masking in columns can maintain all node's representation against convergence in layer-wise, whereas dropping nodes only discard few information from neighbors.
- Why unresolved: The paper provides a theoretical analysis and comparison but does not include empirical results comparing the computational efficiency of TSC's random masking with other methods.
- What evidence would resolve it: Experimental results comparing the computational time and memory usage of TSC's random masking with other neighbor filtering methods like DropMessage and DropEdge.

### Open Question 2
- Question: What is the impact of the temperature coefficient Ï„ in the contrastive constraint on the overall performance of TSC, and how sensitive is the model to changes in this parameter?
- Basis in paper: [explicit] The paper mentions that the temperature coefficient Ï„ is a hyperparameter in the contrastive constraint, and it provides some analysis on its impact on the accuracy and MAD metrics.
- Why unresolved: While the paper provides some analysis, it does not thoroughly explore the sensitivity of the model to different values of Ï„ or provide a detailed discussion on its impact on performance.
- What evidence would resolve it: A comprehensive sensitivity analysis of TSC's performance with respect to different values of Ï„, including visualizations and statistical tests to quantify the impact on accuracy and MAD.

### Open Question 3
- Question: How does TSC perform on large-scale graphs with millions of nodes and edges, and what are the scalability challenges?
- Basis in paper: [inferred] The paper mentions that TSC is implemented on both SGC and GCN architectures and shows effectiveness on various real-world graph datasets, but it does not explicitly discuss performance on large-scale graphs.
- Why unresolved: The experiments conducted in the paper are on relatively small to medium-sized graphs, and the scalability of TSC to large-scale graphs is not addressed.
- What evidence would resolve it: Experimental results on large-scale graph datasets, including performance metrics, computational time, and memory usage, to evaluate the scalability of TSC.

## Limitations

- The paper relies heavily on empirical validation rather than rigorous mathematical proofs for the effectiveness of the two-sided constraint approach
- The masking strategy's effectiveness depends on proper rate scheduling, which isn't fully explored across different graph types and structures
- The exact interplay between column masking and row-wise contrastive learning could benefit from more formal analysis to understand optimal parameter settings

## Confidence

- **High confidence**: The column masking mechanism for preventing representation convergence is well-supported by the experimental results and aligns with established knowledge about information bottleneck principles
- **Medium confidence**: The contrastive constraint's effectiveness in enhancing node discriminability is demonstrated empirically but could benefit from more ablation studies to isolate its specific contributions
- **Medium confidence**: The two-sided approach's superiority over single-mechanism solutions is supported by experiments but the theoretical justification for why both mechanisms are necessary could be stronger

## Next Checks

1. **Ablation study**: Remove either the masking or contrastive components to quantify their individual contributions to over-smoothing mitigation across different graph datasets and network depths

2. **Theoretical analysis**: Derive mathematical bounds on how the masking rate and contrastive temperature affect the convergence rate of node representations in deep GCN layers

3. **Generalization test**: Apply TSC to additional graph types beyond citation networks (e.g., biological networks, social networks) to evaluate its robustness across different structural patterns and feature distributions