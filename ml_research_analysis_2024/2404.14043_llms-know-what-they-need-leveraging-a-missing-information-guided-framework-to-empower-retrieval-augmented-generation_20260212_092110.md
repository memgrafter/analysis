---
ver: rpa2
title: 'LLMs Know What They Need: Leveraging a Missing Information Guided Framework
  to Empower Retrieval-Augmented Generation'
arxiv_id: '2404.14043'
source_url: https://arxiv.org/abs/2404.14043
tags:
- information
- knowledge
- question
- director
- film
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of complex multi-hop queries
  in Retrieval-Augmented Generation (RAG) by leveraging Large Language Models (LLMs)
  to identify missing information and guide targeted knowledge retrieval. The proposed
  Missing Information Guided Retrieve-Extraction-Solving paradigm (MIGRES) uses LLMs
  to decompose queries, extract useful information from documents, and iteratively
  retrieve relevant knowledge.
---

# LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID:** 2404.14043
- **Source URL:** https://arxiv.org/abs/2404.14043
- **Authors:** Keheng Wang; Feiyu Duan; Peiguang Li; Sirui Wang; Xunliang Cai
- **Reference count:** 13
- **Primary result:** MIGRES framework improves multi-hop QA performance by identifying missing information and guiding targeted retrieval

## Executive Summary
This paper addresses the challenge of complex multi-hop queries in Retrieval-Augmented Generation (RAG) systems by leveraging Large Language Models to identify missing information and guide targeted knowledge retrieval. The proposed Missing Information Guided Retrieve-Extraction-Solving paradigm (MIGRES) decomposes queries, extracts useful information from documents, and iteratively retrieves relevant knowledge to answer complex questions. The framework includes sentence-level filtering to reduce noise and LLM-based knowledge prompting when external retrieval fails.

## Method Summary
MIGRES uses LLMs to decompose complex queries into sub-questions and iteratively extract missing information through targeted retrieval. The framework employs a four-stage process: (1) query decomposition using LLM to identify sub-questions, (2) targeted retrieval based on identified missing information, (3) sentence-level filtering to reduce noise and improve precision, and (4) LLM-based knowledge prompting when external retrieval fails. The method combines information extraction with retrieval to provide context for answering complex queries, optimizing both retrieval quality and computational efficiency by consuming fewer tokens than baseline approaches.

## Key Results
- MIGRES outperforms state-of-the-art baselines on multi-hop QA, open-domain QA, and commonsense reasoning tasks
- Achieves superior performance with fewer tokens consumed compared to baseline methods
- Demonstrates effectiveness across multiple benchmark datasets for complex query answering

## Why This Works (Mechanism)
The framework works by leveraging LLM's ability to identify knowledge gaps and guide targeted retrieval. By decomposing complex queries into manageable sub-questions, MIGRES reduces the search space and improves retrieval precision. The sentence-level filtering mechanism reduces noise by focusing only on relevant information, while the LLM-based knowledge prompting serves as a fallback when external retrieval fails, ensuring robustness in information acquisition.

## Foundational Learning
1. **Multi-hop QA:** Understanding complex questions requiring reasoning across multiple documents
   - Why needed: MIGRES specifically targets multi-hop queries that are challenging for traditional RAG systems
   - Quick check: Can identify when a query requires information from multiple sources

2. **Sentence-level filtering:** Reducing noise by selecting only relevant sentences from retrieved documents
   - Why needed: Improves precision and reduces computational overhead
   - Quick check: Can distinguish between relevant and irrelevant information within documents

3. **LLM-guided retrieval:** Using LLM to identify missing information and guide targeted knowledge acquisition
   - Why needed: Enables more precise retrieval by focusing on specific knowledge gaps
   - Quick check: Can accurately identify what information is missing to answer a query

## Architecture Onboarding

**Component Map:** Query Decomposition -> Targeted Retrieval -> Sentence Filtering -> Knowledge Prompting -> Answer Generation

**Critical Path:** The most critical path is Query Decomposition -> Targeted Retrieval -> Sentence Filtering, as these components directly impact retrieval quality and answer accuracy. The Knowledge Prompting stage serves as a fallback mechanism when retrieval fails.

**Design Tradeoffs:** The framework trades computational overhead from iterative LLM calls for improved retrieval precision and answer quality. Sentence-level filtering adds processing time but significantly reduces noise and improves relevance. The LLM-based knowledge prompting provides robustness but depends on LLM quality and may introduce additional latency.

**Failure Signatures:** The system may fail when LLM decomposition is inaccurate, leading to incorrect sub-question formulation. Retrieval failures can occur when the missing information is too obscure or domain-specific for available knowledge sources. Sentence filtering may miss relevant information if the LLM's understanding of context is limited.

**First 3 Experiments:**
1. Test query decomposition accuracy on benchmark multi-hop QA datasets to validate the initial breakdown of complex queries
2. Measure sentence filtering effectiveness by comparing precision-recall metrics with and without the filtering component
3. Evaluate knowledge prompting success rate when external retrieval fails to assess the fallback mechanism's reliability

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Lacks detailed computational overhead analysis for iterative LLM calls
- Evaluation focuses on retrieval quality without comprehensive ablation studies
- Limited generalizability testing to specialized domains beyond general knowledge
- Dependency on LLM quality introduces variability based on model choice

## Confidence
- **Medium** for core performance claims due to positive but incomplete analysis
- **Low** for computational efficiency claims due to limited resource requirement discussion
- **Medium** for practical applicability given promising results but narrow domain coverage

## Next Checks
1. Conduct ablation studies to quantify the individual contribution of each MIGRES component (decomposition, extraction, sentence filtering, knowledge prompting) to overall performance improvements.

2. Measure and report detailed computational overhead including processing time and token consumption breakdown for each query type to validate the efficiency claims.

3. Test the method on specialized domain datasets (e.g., biomedical, legal, technical) to assess generalizability beyond general knowledge domains.