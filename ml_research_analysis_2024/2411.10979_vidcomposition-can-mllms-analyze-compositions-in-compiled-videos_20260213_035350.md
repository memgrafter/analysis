---
ver: rpa2
title: 'VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?'
arxiv_id: '2411.10979'
source_url: https://arxiv.org/abs/2411.10979
tags:
- video
- arxiv
- mllms
- understanding
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VIDCOMPOSITION introduces a benchmark to evaluate MLLMs on fine-grained
  video composition understanding, focusing on compiled videos with cinematic-level
  annotations. It includes 982 videos and 1706 multiple-choice questions across 15
  tasks in 5 categories like cinematography, character, narrative, scene, and making
  analysis.
---

# VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?

## Quick Facts
- arXiv ID: 2411.10979
- Source URL: https://arxiv.org/abs/2411.10979
- Reference count: 40
- Primary result: VIDCOMPOSITION benchmark reveals significant performance gaps between human and MLLM capabilities in understanding fine-grained video composition in compiled videos

## Executive Summary
VIDCOMPOSITION introduces a benchmark to evaluate MLLMs on fine-grained video composition understanding, focusing on compiled videos with cinematic-level annotations. It includes 982 videos and 1706 multiple-choice questions across 15 tasks in 5 categories like cinematography, character, narrative, scene, and making analysis. Evaluation of 33 MLLMs shows a significant performance gap between human and model capabilities, highlighting limitations in understanding complex video compositions. The benchmark provides insights for improving MLLMs and potential applications in video generation.

## Method Summary
VIDCOMPOSITION evaluates MLLMs using a standardized prompt template that feeds 794 frames per video into models to obtain single-character predictions (A-D) for multiple-choice questions. The benchmark consists of 982 compiled videos with 1,706 human-annotated questions across 15 tasks in 5 categories. Models are evaluated without training or fine-tuning, using overall accuracy (ACC) and per-task accuracy metrics calculated as the ratio of correct predictions to total questions. The evaluation includes systematic analysis of factors affecting performance including input frames, visual encoder resolution, language decoder size, and training data volume.

## Key Results
- MLLMs show significant performance gaps compared to human capabilities on video composition tasks
- Models struggle particularly with cinematography analysis and narrative understanding tasks
- Input frames don't contribute to performance improvements; higher resolution visual encoders show significant benefits
- Larger language models and more training data generally improve performance, but fundamental limitations remain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VIDCOMPOSITION improves MLLM evaluation by focusing on fine-grained compositional understanding in compiled videos.
- Mechanism: The benchmark provides cinematic-level annotations and multiple-choice questions that require models to interpret nuanced visual elements like camera movements, angles, and narrative structures across compiled video segments.
- Core assumption: Current MLLMs rely on broader, coarse-grained interpretations and lack training on compiled video contexts that demand shot-by-shot analysis.
- Evidence anchors:
  - [abstract] "lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interact within highly compiled video contexts."
  - [section 1] "Shot-by-shot analysis, a technique where creators meticulously break down the elements of a video, serves as a vital tool for understanding video composition in depth."
  - [corpus] Weak - related benchmarks focus on robustness or multi-video understanding but don't specifically target fine-grained composition in compiled videos.

### Mechanism 2
- Claim: VIDCOMPOSITION reveals specific weaknesses in MLLMs through systematic task categorization.
- Mechanism: The benchmark organizes 15 sub-tasks across 5 categories (Cinematography Analysis, Character Understanding, Narrative Understanding, Scene Perception, Making Analysis), allowing detailed performance analysis across different compositional aspects.
- Core assumption: Task-specific evaluation reveals more actionable insights than overall accuracy metrics alone.
- Evidence anchors:
  - [abstract] "covering various compositional aspects such as camera movement, angle, shot size, narrative structure, character actions and emotions, etc."
  - [section 2.1] "includes 5 main categories: Cinematography Analysis (CA), Character Understanding (CU), Narrative Understanding (NU), Scene Perception (SP), and Making Analysis (MA); and 15 sub-tasks"
  - [corpus] Weak - other benchmarks exist but don't provide this level of granular task breakdown for video composition.

### Mechanism 3
- Claim: VIDCOMPOSITION provides diagnostic insights for model improvement through controlled factor analysis.
- Mechanism: The benchmark enables systematic analysis of factors affecting MLLM performance (number of input frames, resolution, language decoder size, training data volume) by maintaining consistent evaluation conditions.
- Core assumption: Isolating and testing individual factors reveals which architectural choices most impact compositional understanding.
- Evidence anchors:
  - [section 4] "We focus on four factors: the number of input frames (#frm), the resolution of the visual encoder (Res.), the size of the language decoder (LLM size), and the volume of training data (Data volume)"
  - [section 4.1] "Across all the models, we consistently observe that the input frames don't contribute to the performance"
  - [corpus] Weak - diagnostic analysis is mentioned but not as systematically applied to compositional understanding.

## Foundational Learning

- Concept: Compiled videos vs natural-shot videos
  - Why needed here: VIDCOMPOSITION specifically evaluates models on compiled videos (edited/integrated multiple clips) rather than natural-shot videos, which require different understanding capabilities.
  - Quick check question: What distinguishes compiled videos from natural-shot videos in terms of the compositional understanding required?

- Concept: Shot-by-shot analysis in video composition
  - Why needed here: The benchmark requires understanding how visual elements combine and interact within compiled video contexts, which is fundamentally a shot-by-shot analytical approach.
  - Quick check question: Why is shot-by-shot analysis essential for understanding compiled video compositions rather than general scene recognition?

- Concept: Multiple-choice question evaluation for video understanding
  - Why needed here: VIDCOMPOSITION uses multiple-choice questions with distractor options, requiring models to not just recognize but discriminate between compositional elements.
  - Quick check question: How does the multiple-choice format with distractors help reveal fine-grained understanding versus simple recognition?

## Architecture Onboarding

- Component map: Video collection → Annotation → Quality control → Model evaluation → Diagnostic analysis → Performance insights
- Critical path: Video collection → Annotation → Quality control → Model evaluation → Diagnostic analysis → Performance insights
- Design tradeoffs: High-quality human annotation ensures benchmark reliability but requires significant resources; multiple-choice format enables consistent evaluation but may not capture all aspects of compositional understanding
- Failure signatures: Models perform well on action/character recognition but struggle with cinematography analysis and narrative tasks; predictions show bias toward certain answer options in smaller models
- First 3 experiments:
  1. Evaluate a model on VIDCOMPOSITION to establish baseline performance across all 15 sub-tasks
  2. Test the same model with varying numbers of input frames to confirm the finding that more frames don't improve performance
  3. Compare model performance with different visual encoder resolutions while keeping other factors constant to validate the resolution impact findings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do varying input frame counts (#frm) affect MLLM performance on fine-grained video composition tasks?
- Basis in paper: [explicit] The paper states that models with different input frame counts show no clear trends in performance, with results either remaining stable or fluctuating randomly as the number of frames increases.
- Why unresolved: The paper indicates that while more frames provide more information, this small amount of useful information is mixed with a large amount of duplicate information, and the model cannot effectively extract it.
- What evidence would resolve it: A controlled experiment varying #frm while keeping all other factors constant, measuring task-specific performance changes.

### Open Question 2
- Question: What is the relative contribution of higher resolution visual encoders versus larger language models to improved performance on video composition tasks?
- Basis in paper: [explicit] The paper observes that higher-resolution visual encoders significantly improve performance, but it is impossible to determine how much of this improvement is due to the higher-resolution visual encoder versus the different models themselves.
- Why unresolved: The paper notes that while resolution is unchangeable for one specific model, the comparison is confounded by using different models.
- What evidence would resolve it: An ablation study using the same model architecture with different visual encoder resolutions while keeping other components constant.

### Open Question 3
- Question: How does the volume of training data in the SFT stage impact MLLM performance on complex narrative understanding tasks?
- Basis in paper: [explicit] The paper shows that larger data volumes lead to improved performance in most cases, but notes that models still struggle with tasks requiring understanding of the gap between scripts and actual video presentation.
- Why unresolved: While the paper demonstrates a correlation between data volume and performance, it does not explain why models still struggle with narrative tasks despite increased training data.
- What evidence would resolve it: A detailed analysis of model predictions on narrative tasks across different training data volumes, identifying specific failure modes that persist regardless of data quantity.

## Limitations
- The benchmark's reliance on high-quality human annotation raises questions about scalability and generalizability to other video domains
- Multiple-choice format may not fully capture the complexity of compositional understanding
- The evaluation doesn't clearly distinguish between fundamental architectural limitations and insufficient training data specific to compiled video contexts

## Confidence
- High Confidence: The benchmark's methodology and dataset construction are sound, with clear task categorization and systematic evaluation procedures
- Medium Confidence: The conclusion about MLLM limitations is supported by the data, but the extent to which these limitations are fundamental versus addressable through better training remains unclear
- Low Confidence: The diagnostic analysis linking specific architectural factors (input frames, resolution, decoder size) to performance gaps needs more rigorous validation across diverse model families

## Next Checks
1. Test whether fine-tuning MLLMs on a small subset of VIDCOMPOSITION data improves performance on held-out questions, distinguishing between architectural limitations and data scarcity
2. Evaluate whether replacing the multiple-choice format with open-ended compositional analysis questions changes the performance gap between human and model capabilities
3. Apply the same diagnostic factor analysis (frames, resolution, decoder size) to a broader range of MLLMs beyond the initial 33 models to verify the consistency of the observed patterns