---
ver: rpa2
title: Multi-Label Bayesian Active Learning with Inter-Label Relationships
arxiv_id: '2411.17941'
source_url: https://arxiv.org/abs/2411.17941
tags:
- label
- learning
- data
- correlation
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces CRAB, a multi-label active learning framework
  that addresses two main challenges: leveraging inter-label correlations and handling
  data imbalance. CRAB constructs dynamic positive and negative correlation matrices
  to capture label co-occurrence and exclusivity, enabling holistic uncertainty assessment
  rather than treating labels in isolation.'
---

# Multi-Label Bayesian Active Learning with Inter-Label Relationships

## Quick Facts
- arXiv ID: 2411.17941
- Source URL: https://arxiv.org/abs/2411.17941
- Authors: Yuanyuan Qi; Jueqing Lu; Xiaohao Yang; Joanne Enticott; Lan Du
- Reference count: 27
- Primary result: CRAB consistently outperforms five state-of-the-art baselines in micro-F1 across four datasets

## Executive Summary
CRAB introduces a novel multi-label active learning framework that addresses two critical challenges: leveraging inter-label correlations and handling data imbalance. The method constructs dynamic positive and negative correlation matrices to capture label co-occurrence and exclusivity, enabling holistic uncertainty assessment rather than treating labels in isolation. CRAB employs ensemble pseudo-labeling and beta scoring rules to mitigate class imbalance, alongside clustering to ensure diversity. Experiments on four datasets (RCV1, UKLEX, EURLEX, MIMIC3) show CRAB consistently outperforms five state-of-the-art baselines in micro-F1, with robust performance on imbalanced and highly correlated data.

## Method Summary
CRAB is an ensemble-based Bayesian active learning framework that leverages inter-label correlations through dynamic positive and negative correlation matrices. The method uses three-stage sampling: label-wise selection with asymmetric correlation filtering, negative correlation sampling to capture mutually exclusive labels, and hard-to-learn sample selection based on pseudo-label confidence. Beta scoring rules with tunable parameters address class imbalance by adjusting the loss surface for minority labels. The framework iteratively refines the unlabeled pool, computes correlation-aware Beta scores with attention mechanisms, and uses K-means clustering to select diverse, informative samples. CRAB was tested on four benchmark multi-label datasets with three different backbone architectures.

## Key Results
- CRAB achieves superior micro-F1 scores compared to five state-of-the-art baselines across all four datasets
- The method shows robust performance on highly imbalanced datasets, with MeanIR scores indicating effective imbalance handling
- Ablation studies confirm the effectiveness of correlation-aware sampling and asymmetric label handling in improving model performance

## Why This Works (Mechanism)

### Mechanism 1
CRAB leverages asymmetric label correlations through dynamic positive and negative correlation matrices to refine sampling and avoid redundant label selection. During each iteration, CRAB constructs a positive correlation matrix (A) capturing co-occurrence probabilities between labels and a negative correlation matrix (N egA) capturing mutual exclusivity. For label-wise sampling, CRAB identifies asymmetric correlations (high A(m,n) but low A(n,m)) and removes the less informative label from the sampling space, ensuring independence and avoiding overlap in hierarchical structures. This works under the assumption that label correlations are stable enough within each AL iteration to be captured by counts from labeled data.

### Mechanism 2
CRAB addresses class imbalance by integrating Beta scoring rules into the acquisition function, adjusting α and β to weight loss for underrepresented classes. The acquisition score uses a Beta proper scoring rule parameterized by α and β (set to 0.1 and 3 from greedy search). These parameters shape the loss surface to penalize misclassifications of minority labels more heavily, balancing the contribution of imbalanced labels during informativeness evaluation. This approach assumes that adjusting Beta distribution parameters can effectively rebalance the impact of label frequency on the scoring without requiring explicit resampling.

### Mechanism 3
CRAB improves robustness by sampling hard-to-learn and negatively correlated instances to focus on difficult decision boundaries and avoid model bias. Hard-to-learn samples are defined as those with no pseudo-label confidence above 0.5, indicating model uncertainty. Negatively correlated samples are those whose pseudo-label pairs exceed a threshold in N egA. Both sets are added to a refined unlabeled pool and selected based on correlation-aware Beta scores, forcing the model to learn ambiguous and mutually exclusive cases. This works under the assumption that instances with low confidence or predicted negative correlations are indeed harder to learn and beneficial for model generalization.

## Foundational Learning

- Concept: Multi-label classification vs. multi-class classification
  - Why needed here: CRAB must handle multiple simultaneous labels per instance and their correlations, unlike multi-class where only one label applies
  - Quick check question: What distinguishes a multi-label from a multi-class problem, and how does this affect the design of an acquisition function?

- Concept: Proper scoring rules and Beta scoring
  - Why needed here: The acquisition function must evaluate probabilistic predictions fairly and handle imbalance; Beta scoring generalizes log-loss and Brier score
  - Quick check question: How do the parameters α and β in the Beta scoring rule influence the loss for positive vs. negative class predictions?

- Concept: Bayesian active learning and expected loss reduction
  - Why needed here: CRAB uses an ensemble to approximate posterior distributions and selects samples that maximize expected score improvement
  - Quick check question: How does the Monte Carlo approximation of P(θ|L) via ensemble predictions enable uncertainty quantification in this setting?

## Architecture Onboarding

- Component map: Labeled pool L -> Ensemble of classifiers ΘE -> Correlation matrices A, N egA -> Pseudo-labels y* -> Three refined sampling subsets (label-wise, negatively correlated, hard-to-learn) -> Correlation-aware Beta scoring SAB -> K-means clustering -> Selected samples

- Critical path:
  1. Train ensemble on L
  2. Update correlation matrices from L
  3. Generate pseudo-labels y*
  4. Refine U into three subsets via label-wise, negative, and hard sampling
  5. Compute SAB scores for refined subset
  6. Cluster and select N most informative samples
  7. Add to L, remove from U

- Design tradeoffs:
  - Dynamic correlation matrices vs. static global correlations: dynamic allows adaptation but requires more computation per iteration
  - Beta scoring parameters tuned on one dataset: may not generalize; risk of overfitting
  - Polynomial decay for hard-to-learn sampling: balances early focus vs. later diversity, but decay rate is heuristic

- Failure signatures:
  - Poor performance on highly imbalanced data: Beta parameters may need retuning
  - Stale correlation estimates: too few updates or biased labeled pool
  - Overfitting to pseudo-labels: ensemble may be too confident, leading to bad hard-to-learn sampling

- First 3 experiments:
  1. Run CRAB on a small multi-label text dataset (e.g., RCV1 subset) with TextCNN backbone; verify correlation matrices update correctly
  2. Compare Beta scoring with standard cross-entropy in acquisition; check micro-F1 difference on imbalanced synthetic labels
  3. Ablation: disable negative correlation sampling; observe change in performance on datasets with known label exclusivity

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CRAB scale with increasing label cardinality in datasets with high inter-label correlations? The paper demonstrates robust performance on highly correlated MIMIC dataset but doesn't systematically explore performance across varying label cardinalities. This remains unresolved because experiments focus on four specific datasets without controlled analysis of how increasing label cardinality affects CRAB's performance relative to baselines.

### Open Question 2
Can the negative correlation matrix be effectively learned from limited annotated data, or does it require a minimum dataset size to be reliable? The paper constructs negative correlation matrices from iteratively updated labeled data but doesn't discuss minimum sample requirements or early-stage reliability. This is unresolved because the paper demonstrates effectiveness on datasets with thousands of samples but doesn't address whether negative correlations can be meaningfully estimated when starting with very few labeled examples.

### Open Question 3
How does CRAB's performance compare when using different backbone architectures beyond the three tested (TextCNN, TextRNN, DistilBERT)? While the paper tests three specific architectures and acknowledges parameter adjustability across different structures, it doesn't explore whether certain backbone types might synergize particularly well with the correlation-aware approach. This remains unresolved because the paper doesn't systematically test across diverse architecture families.

## Limitations

- Dynamic correlation matrices may become unreliable with small or biased labeled pools, leading to poor sample selection
- Beta scoring parameters tuned on one dataset may not generalize across different data distributions and imbalance patterns
- The framework's complexity and multiple tunable components make it challenging to reproduce and optimize without extensive hyperparameter tuning

## Confidence

- High: CRAB's architecture and methodology are clearly described; the use of dynamic correlation matrices and Beta scoring is innovative and well-motivated
- Medium: Experimental results show CRAB outperforms baselines on average, but the robustness across different dataset characteristics and hyperparameter settings is not fully established
- Low: The claim that CRAB robustly handles all types of imbalance and correlation without careful parameter tuning is not sufficiently supported

## Next Checks

1. **Correlation Stability Test:** Run CRAB on a small multi-label dataset with known label correlations (e.g., RCV1 subset) and monitor how correlation matrix estimates change with each AL iteration. Verify that updates are meaningful and stable, and check if stale or noisy estimates lead to poor sample selection.

2. **Beta Scoring Ablation:** Implement a version of CRAB that replaces Beta scoring with standard cross-entropy or a fixed-class weighting scheme. Compare micro-F1 scores on imbalanced synthetic datasets to quantify the actual benefit of Beta scoring for imbalance mitigation.

3. **Hard-to-Learn Sampling Sensitivity:** Disable or modify the hard-to-learn sampling mechanism and rerun CRAB on datasets with varying levels of pseudo-label noise (e.g., by adjusting ensemble size or dropout). Measure the impact on final model performance to assess the robustness of this component.