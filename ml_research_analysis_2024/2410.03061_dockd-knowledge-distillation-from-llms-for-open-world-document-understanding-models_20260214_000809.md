---
ver: rpa2
title: 'DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding
  Models'
arxiv_id: '2410.03061'
source_url: https://arxiv.org/abs/2410.03061
tags:
- document
- form
- text
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DocKD, a framework that improves the generalizability
  of visual document understanding (VDU) models by distilling knowledge from large
  language models (LLMs) using external document knowledge. The key idea is to provide
  LLMs with enriched document information (e.g., key-value pairs, layouts, descriptions)
  to generate high-quality training data for downstream tasks.
---

# DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models

## Quick Facts
- **arXiv ID:** 2410.03061
- **Source URL:** https://arxiv.org/abs/2410.03061
- **Reference count:** 40
- **Primary result:** DocKD framework improves VDU model generalizability by distilling LLM knowledge using external document knowledge.

## Executive Summary
This paper introduces DocKD, a framework that improves the generalizability of visual document understanding (VDU) models by distilling knowledge from large language models (LLMs) using external document knowledge. The key idea is to provide LLMs with enriched document information (e.g., key-value pairs, layouts, descriptions) to generate high-quality training data for downstream tasks. Experiments on three document understanding tasks—visual question answering, entity extraction, and classification—demonstrate that DocKD significantly outperforms a direct knowledge distillation baseline. Student models trained solely on DocKD-generated data achieve comparable performance to those trained with human-annotated data on in-domain tasks and notably outperform them on out-of-domain tasks, showcasing the framework's potential for open-world document understanding.

## Method Summary
DocKD uses LLMs to generate training data by providing document text and external knowledge (layout, key-value pairs, descriptions). For VQA, linearized OCR text and LLM-generated QA pairs are used. For entity extraction, key-value pairs and iterative LLM prompts generate field names. For classification, document descriptions help LLMs generate diverse class labels. Student VDU models (e.g., DocFormerv2) are trained on the generated data and evaluated on in-domain and out-of-domain tasks.

## Key Results
- DocKD significantly outperforms direct knowledge distillation baselines on VQA, entity extraction, and classification tasks.
- Student models trained solely on DocKD-generated data achieve comparable performance to those trained with human-annotated data on in-domain tasks.
- DocKD-generated data notably outperforms human-annotated data on out-of-domain tasks, demonstrating superior generalizability.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** External document knowledge (layout, key-value pairs, descriptions) improves LLM-generated data quality for VDU tasks.
- **Mechanism:** LLMs often fail to understand unstructured OCR text and lack awareness of visual document elements. By providing structured layout information (linearized OCR), detected key-value pairs, and document descriptions, the LLM can generate more accurate and diverse annotations.
- **Core assumption:** LLMs can effectively utilize provided structured document elements to generate higher-quality task-specific annotations.
- **Evidence anchors:**
  - [abstract] "We identify that directly prompting LLMs often fails to generate informative and useful data. In response, we present a new framework (called DocKD) that enriches the data generation process by integrating external document knowledge."
  - [section 3.1] "One limitation of the LLM's QA generation lies on its text-to-text framework, where it requires the text to be organized in a semantically meaningful order. However, OCR text is a simple sequence of words typically ordered by raster scanning, which ignores the important layout and structural information of document pages."
  - [corpus] Weak - no direct citation, but related work on LLM prompting and layout awareness supports this mechanism.

### Mechanism 2
- **Claim:** Iterative key-value pair presentation improves field name generation for entity extraction.
- **Mechanism:** LLMs struggle to enumerate diverse entities, especially when many exist. By iteratively presenting each detected key-value pair and leveraging the output from the previous iteration, the LLM focuses on generating field names for the current entity while maintaining consistency with previous outputs.
- **Core assumption:** LLMs can maintain context across iterative generations and generate consistent field names for related entities.
- **Evidence anchors:**
  - [section 3.2] "Although LLMs can identify entities from documents to a certain extent, we notice that they are unable to sufficiently enumerate the entities. They tend to list mostly the major ones, especially when there are many potential entities in the document, and fail to identify diverse types."
  - [section 3.2] "Because there exist multiple KV pairs, we iteratively present each KV entity line by line to the LLM, with the previous line's output appended."
  - [corpus] Weak - no direct citation, but iterative prompting strategies in LLM literature support this mechanism.

### Mechanism 3
- **Claim:** Document descriptions improve the diversity and quality of generated class labels for document classification.
- **Mechanism:** LLMs tend to generate overly general class labels when directly prompted. By providing document descriptions, the LLM can better summarize the document and generate more diverse and detailed class labels.
- **Core assumption:** LLMs can effectively utilize document descriptions to generate more specific and relevant class labels.
- **Evidence anchors:**
  - [section 3.3] "We notice that when an LLM is directly prompted to predict document classes, it frequently generates class labels that are overly general, resulting in low diversity."
  - [section 3.3] "To address this, we incorporate document descriptions to pgen which we find can facilitate LLMs to better summarize a document and generate more diverse class labels."
  - [corpus] Weak - no direct citation, but document summarization and classification literature supports this mechanism.

## Foundational Learning

- **Concept:** Large Language Models (LLMs) and their capabilities in text generation and understanding.
  - **Why needed here:** The entire framework relies on leveraging LLMs to generate high-quality annotations for VDU tasks.
  - **Quick check question:** What are the key differences between LLMs and traditional language models, and how do these differences enable them to generate more informative annotations?

- **Concept:** Optical Character Recognition (OCR) and its limitations in document understanding.
  - **Why needed here:** The framework uses OCR text as input to LLMs, and understanding OCR limitations is crucial for improving the generated annotations.
  - **Quick check question:** What are the main challenges in OCR text extraction, and how do these challenges affect the quality of LLM-generated annotations?

- **Concept:** Document layouts and their importance in visual document understanding.
  - **Why needed here:** The framework leverages layout information to improve LLM-generated annotations, and understanding layout elements is crucial for effective document understanding.
  - **Quick check question:** What are the key layout elements in documents, and how do these elements contribute to the overall understanding of the document's content?

## Architecture Onboarding

- **Component map:** Document Input -> OCR Extraction -> Layout Linearization & KV Detection -> LLM Prompt Construction -> LLM Annotation Generation -> Student Model Training
- **Critical path:**
  1. Document input (image or text)
  2. OCR extraction (if image input)
  3. Layout linearization and key-value detection
  4. LLM prompt construction with external document knowledge
  5. LLM annotation generation
  6. Student model training on generated annotations
- **Design tradeoffs:**
  - LLM size vs. annotation quality: Larger LLMs may generate better annotations but are more computationally expensive.
  - External document knowledge vs. annotation diversity: Providing more structured elements may improve annotation quality but reduce diversity.
  - Iterative vs. simultaneous generation: Iterative approaches may improve consistency but increase generation time.
- **Failure signatures:**
  - LLM-generated annotations are irrelevant or inaccurate.
  - Student models perform poorly on downstream tasks.
  - External document knowledge does not improve annotation quality.
- **First 3 experiments:**
  1. Compare annotation quality with and without external document knowledge (e.g., layout linearization).
  2. Evaluate the impact of iterative vs. simultaneous key-value pair presentation on field name generation.
  3. Assess the effect of document descriptions on the diversity and quality of generated class labels.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several are implied:

1. How does DocKD perform on documents with complex visual elements like intricate figures, diagrams, or dense equations?
2. What is the optimal strategy for integrating LLMs with document expert models and large multimodal models like GPT-4V to synthesize visually-rich, informative annotations?
3. How does the performance of DocKD vary with the size and quality of the teacher LLM and student VDU model?
4. What is the optimal data volume and quality trade-off for DocKD, and how does it impact the performance of student models?
5. How does DocKD generalize to different languages and domains beyond the ones explored in the paper?

## Limitations
- The framework's effectiveness heavily depends on the quality of external document knowledge extraction (layout linearization, key-value detection) which is not thoroughly validated.
- The computational cost of using large LLMs for data generation at scale is not discussed.
- The framework assumes LLMs can effectively utilize provided document elements, but this capability may vary significantly across different LLM architectures and sizes.

## Confidence
- **High confidence:** The core mechanism of using external document knowledge to improve LLM-generated annotations is well-supported by ablation studies and comparative results.
- **Medium confidence:** The claim that DocKD-generated data matches human-annotated data quality for in-domain tasks, as this comparison relies on a limited set of tasks and datasets.
- **Low confidence:** The assertion that DocKD significantly outperforms direct knowledge distillation baselines, as the comparison is made against a single baseline and may not generalize to other distillation approaches.

## Next Checks
1. **External Knowledge Quality Assessment:** Systematically evaluate how variations in the quality of extracted layout information, key-value pairs, and document descriptions affect the final annotation quality and downstream task performance.
2. **LLM Architecture Sensitivity:** Test the framework with different LLM sizes and architectures (e.g., smaller models, open-source alternatives) to determine the minimum effective LLM requirements for maintaining annotation quality.
3. **Computational Efficiency Analysis:** Measure the wall-clock time and resource consumption of the DocKD framework across different document volumes and compare it with traditional data annotation approaches to establish practical scalability limits.