---
ver: rpa2
title: Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of Large
  Language Models
arxiv_id: '2407.16221'
source_url: https://arxiv.org/abs/2407.16221
tags:
- answer
- given
- option
- question
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a standardized black-box evaluation methodology\
  \ for assessing Large Language Models' (LLMs) Abstention Ability (AA)\u2014the capacity\
  \ to withhold responses when uncertain. The authors create Abstain-QA, a 2,900-sample\
  \ Multiple-Choice Question Answering (MCQA) dataset spanning answerable/unanswerable,\
  \ well-represented/under-represented, and fact-based/reasoning tasks, including\
  \ Carnatic-QA (900 samples from Carnatic music)."
---

# Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of Large Language Models

## Quick Facts
- arXiv ID: 2407.16221
- Source URL: https://arxiv.org/abs/2407.16221
- Reference count: 14
- Key outcome: Introduces standardized black-box evaluation methodology for LLMs' Abstention Ability using Abstain-QA dataset and AUCM framework

## Executive Summary
This paper addresses a critical gap in LLM evaluation by introducing a standardized black-box methodology to assess abstention ability - the capacity of LLMs to withhold responses when uncertain. The authors develop Abstain-QA, a 2,900-sample Multiple-Choice Question Answering dataset spanning answerable/unanswerable, well-represented/under-represented, and fact-based/reasoning tasks. They propose the Answerable-Unanswerable Confusion Matrix (AUCM) and associated metrics (AR, AAC, UAC) to systematically evaluate how well LLMs can detect when they should abstain from answering.

The study reveals that while LLMs demonstrate strong abstention ability on simple fact-based questions, they struggle significantly with reasoning tasks and under-represented domains. Chain-of-Thought prompting and Strict Prompting consistently improve both abstention ability and accuracy, while verbal confidence thresholding shows mixed results. Enhanced abstention clauses particularly boost performance for larger models, highlighting the potential for targeted prompt engineering to improve LLM reliability.

## Method Summary
The authors create the Abstain-QA dataset with 2,900 samples across answerable/unanswerable, well-represented/under-represented, and fact-based/reasoning tasks, including Carnatic-QA (900 samples). They implement a black-box evaluation framework using three prompting strategies (Base, Verbal Confidence, Chain-of-Thought) and three abstention clause types (Standard, Abstain Clause, Extreme Abstain Clause). The AUCM evaluation framework tracks four outcomes: True Positive, True Negative, False Positive, and False Negative, calculating metrics including Answerable Accuracy (AAC), Unanswerable Accuracy (UAC), Abstention Rate (AR), and Precision (P). The methodology is applied across six LLMs (GPT-4, Mixtral, Mistral) in a zero-shot setting.

## Key Results
- LLMs show strong abstention ability on simple fact-based MCQs but weak performance on reasoning and under-represented domains
- Chain-of-Thought prompting and Strict Prompting improve both abstention ability and accuracy
- Enhanced abstention clauses (AC, EAC) boost performance, especially for larger models
- Verbal confidence thresholding shows inconsistent results across different LLM architectures
- Correct answer position has marginal impact on performance, with edges showing slight improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Abstention clauses improve AA by increasing model sensitivity to uncertainty through penalty framing
- Mechanism: Clauses introduce explicit consequences for incorrect answers, shifting the model's loss landscape to penalize wrong answers more heavily
- Core assumption: LLMs can interpret and act on task-specific instructions that frame abstention as safer behavior
- Evidence anchors: Abstract mentions Strict prompting and CoT enhance AA; section 4.1 describes how clauses introduce consequences for incorrect responses

### Mechanism 2
- Claim: Chain-of-Thought prompting improves AA by forcing explicit reasoning before answering
- Mechanism: CoT requires verbalizing intermediate reasoning steps, surfacing uncertainty earlier and allowing detection of insufficient knowledge
- Core assumption: LLMs generate more calibrated responses when required to show reasoning process
- Evidence anchors: Abstract states CoT improves AA and accuracy; section 4.2 explains CoT mandates step-by-step reasoning

### Mechanism 3
- Claim: Verbal Confidence Thresholding improves AA by converting low-confidence predictions to abstentions
- Mechanism: Model generates confidence score; predictions below threshold become "I Don't Know," filtering uncertain responses
- Core assumption: Model's self-reported confidence correlates with actual uncertainty
- Evidence anchors: Abstract shows mixed results; section 4.2 defines Verbal Confidence experiment and thresholding process

## Foundational Learning

- Concept: Confusion matrices and their extension to abstention scenarios (AUCM)
  - Why needed here: AUCM is the core evaluation framework distinguishing answerable/unanswerable questions and model abstentions vs. answers
  - Quick check question: What four outcomes does the AUCM track, and how do they differ from a standard binary confusion matrix?

- Concept: Prompt engineering and instruction following in LLMs
  - Why needed here: Study relies on varying task prompts and abstention clauses to manipulate model behavior
  - Quick check question: How does the "Abstain Clause" differ structurally from the "Extreme Abstain Clause" in the task prompt?

- Concept: Zero-shot evaluation and controlled dataset design
  - Why needed here: Experiments conducted in zero-shot setting using Abstain-QA dataset with balanced answerable/unanswerable splits
  - Quick check question: Why does the Abstain-QA dataset include an equal split of answerable and unanswerable questions?

## Architecture Onboarding

- Component map: Dataset generation -> Prompt template application -> Model inference -> AUCM calculation -> Result aggregation
- Critical path: Dataset generation -> Prompt templating -> Model execution -> AUCM calculation -> Result aggregation
- Design tradeoffs: MCQA tasks offer precise control but may not reflect open-ended real-world uncertainty; abstention clauses improve AA but may reduce accuracy on answerable questions
- Failure signatures: High False Negatives in answerable questions indicate over-abstention; low True Negatives in unanswerable questions indicate poor uncertainty detection
- First 3 experiments:
  1. Run Base experiment with Standard clause on small MMLU subset to verify AUCM metric computation
  2. Apply AC to same subset and compare AR and AAC changes
  3. Introduce CoT prompting and measure shifts in UAC and AR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Abstention Ability (AA) of Large Language Models (LLMs) vary across different domains and question types?
- Basis in paper: [explicit] Paper introduces standardized black-box evaluation methodology for assessing LLMs' AA across varied question types, domains, and task types
- Why unresolved: Paper presents initial findings showing LLMs perform well on simple facts but struggle with reasoning and under-represented domains, but lacks comprehensive analysis across different domains and question types
- What evidence would resolve it: Detailed analysis of AA performance across various domains and question types with comparison of results from different datasets and experiments

### Open Question 2
- Question: What is the impact of different prompting strategies on the Abstention Ability (AA) of Large Language Models (LLMs)?
- Basis in paper: [explicit] Paper explores impact of three prompting strategies - Strict Prompting, Verbal Confidence Thresholding, and Chain-of-Thought (CoT) - on improving AA
- Why unresolved: Paper provides initial results showing CoT and Strict Prompting enhance AA, but doesn't fully explore impact across different models and datasets
- What evidence would resolve it: Comprehensive evaluation of prompting strategies' impact on AA across various models, datasets, and experimental setups

### Open Question 3
- Question: How does the position of the correct answer in multiple-choice questions affect the Abstention Ability (AA) of Large Language Models (LLMs)?
- Basis in paper: [explicit] Paper investigates effect of correct answer positioning on AA by performing experiments with different answer positions
- Why unresolved: Paper provides initial findings showing marginal improvements when correct answer is at edges, but doesn't fully explore impact of answer positioning on AA
- What evidence would resolve it: Detailed analysis of answer positioning impact on AA across various models, datasets, and experimental setups with comparison of results from different answer positions

## Limitations
- AA improvements may not generalize across different task domains, particularly for under-represented and reasoning tasks
- Verbal confidence thresholding calibration shows inconsistent results, suggesting potential domain-specific dependencies
- Tradeoff between abstention ability and accuracy on answerable questions requires further investigation

## Confidence
- High: AUCM framework and basic AA measurement methodology are well-defined and reproducible
- Medium: Effectiveness of Chain-of-Thought prompting for improving AA is supported but shows variability across models
- Low: Verbal confidence thresholding mechanism lacks consistent calibration across different LLM architectures

## Next Checks
1. Conduct ablation studies on abstention clauses to isolate their specific contribution to AA improvements across different task types
2. Test AUCM framework with additional LLM architectures (e.g., Claude, Llama) to assess generalizability
3. Evaluate AA performance on open-ended question formats to validate findings beyond the MCQA format