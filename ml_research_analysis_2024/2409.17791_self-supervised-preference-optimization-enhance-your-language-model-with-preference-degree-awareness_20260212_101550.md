---
ver: rpa2
title: 'Self-supervised Preference Optimization: Enhance Your Language Model with
  Preference Degree Awareness'
arxiv_id: '2409.17791'
source_url: https://arxiv.org/abs/2409.17791
tags:
- preference
- llms
- methods
- self-supervised
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue that existing human preference alignment
  methods for large language models (LLMs) do not account for varying degrees of preference
  within responses. To solve this, the authors propose a novel Self-supervised Preference
  Optimization (SPO) framework that constructs a self-supervised preference degree
  loss combined with the alignment loss.
---

# Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness

## Quick Facts
- **arXiv ID:** 2409.17791
- **Source URL:** https://arxiv.org/abs/2409.17791
- **Reference count:** 20
- **Key result:** SPO improves LLM preference alignment by teaching models to understand varying preference degrees without additional annotation costs

## Executive Summary
Existing human preference alignment methods for large language models (LLMs) fail to account for varying degrees of preference within responses. This paper introduces Self-supervised Preference Optimization (SPO), a novel framework that constructs self-supervised preference degree loss combined with alignment loss. By selectively removing key content from LLM outputs and classifying responses with different preference intensities, SPO enables models to better understand preference degrees. Extensive experiments on the Anthropic HH and TL;DR summarization datasets demonstrate that SPO can be seamlessly integrated with existing preference optimization methods like DPO, achieving state-of-the-art results.

## Method Summary
SPO works by extracting key content from LLM outputs using RAKE, then incrementally removing these key elements to generate responses with varying degrees of preference. These responses are used in a self-supervised classification task where the model learns to distinguish between different preference levels. The classification loss is integrated with the original alignment loss (DPO, IPO, or KTO) to jointly optimize the LLM. The method employs two parallel self-supervised classification modules - one for preferred responses and one for dis-preferred responses - to facilitate the sequential integration of preferred and dis-preferred intensity into LLMs.

## Key Results
- SPO integrated with DPO achieves 88.6% win rate on TL;DR dataset, surpassing LLaMA-13B with DPO alone
- LLaMA-7B optimized with DPO+SPO outperforms LLaMA-13B optimized with DPO alone on TL;DR
- Significant performance boosts across multiple base models (LLaMA-7B, LLaMA-13B, LLaMA3-8B, Mistral-7B) and datasets

## Why This Works (Mechanism)

### Mechanism 1
The removal of key content creates responses with varying degrees of preference that help the model learn preference intensity. RAKE extracts key content from LLM outputs, and incremental removal generates responses with different preference intensities. These are used in self-supervised classification where the model learns to distinguish between preference levels. Core assumption: key content is closely associated with preference information.

### Mechanism 2
The self-supervised classification loss integrated with alignment loss improves preference degree understanding. After key content removal, remaining tokens' hidden states are augmented with positional encoding and fed into an MLP for classification. This combined loss allows the model to learn preference degrees while maintaining alignment. Core assumption: positional encoding helps the classifier understand preference when semantic coherence is compromised.

### Mechanism 3
Using two separate self-supervised modules (for preferred and dis-preferred responses) provides better performance than a single module. The method employs two parallel classification modules, each handling its respective response type, with their losses combined with the main alignment loss. Core assumption: concurrent application of both modules facilitates sequential integration of preference intensity without excessive classes.

## Foundational Learning

- **Concept:** Direct Preference Optimization (DPO) and its variants
  - Why needed here: SPO is designed to enhance existing preference optimization methods like DPO
  - Quick check question: How does DPO differ from traditional RLHF approaches in terms of reward modeling?

- **Concept:** Self-supervised learning and contrastive learning
  - Why needed here: SPO uses self-supervised classification to teach preference degrees without additional labeled data
  - Quick check question: What is the key difference between supervised and self-supervised learning in representation learning?

- **Concept:** Keyword extraction methods (RAKE, YAKE, PositionRank)
  - Why needed here: The method relies on RAKE for key content extraction
  - Quick check question: How does RAKE determine which phrases are considered key content in a document?

## Architecture Onboarding

- **Component map:** LLM -> RAKE keyword extractor -> Key content removal module -> Two self-supervised classification modules -> Loss combination layer -> Main alignment loss

- **Critical path:**
  1. LLM generates response
  2. RAKE extracts key content
  3. Key content is removed in varying amounts to create self-supervised samples
  4. Classification modules process the modified responses
  5. Classification losses are combined with main alignment loss
  6. Joint optimization updates the LLM

- **Design tradeoffs:**
  - Using key content removal vs. other methods for creating preference degrees
  - Two separate classification modules vs. a single multi-class classifier
  - Fixed number of preference levels vs. continuous preference intensity

- **Failure signatures:**
  - Low classification accuracy indicates self-supervised modules are not learning preference degrees effectively
  - Performance degradation compared to baseline methods suggests additional complexity is not beneficial
  - Unstable training or divergence indicates incorrect loss scaling or hyperparameter settings

- **First 3 experiments:**
  1. Apply SPO to DPO on TL;DR dataset and measure win rate improvement
  2. Compare RAKE vs. YAKE vs. PositionRank for key content extraction
  3. Test different numbers of preference levels (N) to find optimal setting

## Open Questions the Paper Calls Out
None

## Limitations
- Key assumption that RAKE accurately captures preference-relevant content is only empirically validated, not theoretically justified
- Dual-module design adds complexity without clear theoretical justification; improvement may be dataset-specific
- Performance gains evaluated through GPT-4 win rates may introduce evaluation bias and show dataset-dependent variation

## Confidence
- **High Confidence:** Integration with existing preference optimization methods is technically sound; observation that key content removal creates meaningful preference variations is supported
- **Medium Confidence:** Claims of "significant boosts" require context as incremental gains over strong baselines; RAKE superiority demonstrated but not mechanistically explained
- **Low Confidence:** Theoretical justification for two separate classification modules is weak; assumption of linear preference scale from key content removal is not validated beyond observation

## Next Checks
1. Systematically corrupt key content extraction by using random tokens instead of RAKE-identified content, then measure whether SPO performance degrades as expected
2. Apply SPO to a third, qualitatively different preference learning dataset (e.g., conversational AI safety preferences) to test generalization beyond dialogue and summarization
3. Replace the dual-module architecture with a unified multi-class classifier that predicts all preference levels simultaneously, then compare performance and computational efficiency