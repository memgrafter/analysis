---
ver: rpa2
title: 'From RAGs to riches: Utilizing large language models to write documents for
  clinical trials'
arxiv_id: '2402.16406'
source_url: https://arxiv.org/abs/2402.16406
tags:
- clinical
- writing
- trials
- large
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the use of large language models (LLMs) to
  generate clinical trial documents. The authors assess GPT-4, both in its standard
  form and when enhanced with retrieval-augmented generation (RAG), for drafting clinical
  trial protocol sections.
---

# From RAGs to riches: Utilizing large language models to write documents for clinical trials

## Quick Facts
- arXiv ID: 2402.16406
- Source URL: https://arxiv.org/abs/2402.16406
- Authors: Nigel Markey; Ilyass El-Mansouri; Gaetan Rensonnet; Casper van Langen; Christoph Meier
- Reference count: 25
- Primary result: RAG-augmented LLMs achieve over 80% accuracy in clinical thinking and logic for clinical trial protocol writing

## Executive Summary
This study evaluates the effectiveness of large language models (LLMs), specifically GPT-4, for generating clinical trial protocol documents. The research compares standard GPT-4 against a retrieval-augmented generation (RAG) version that incorporates external regulatory guidance and clinical trial data. Results demonstrate that while standard GPT-4 performs adequately on content relevance and medical terminology, it falls short on clinical logic and lacks proper referencing. The RAG-enhanced approach significantly improves performance across all measured dimensions, particularly achieving over 80% accuracy in clinical thinking and logic while providing appropriate citations.

## Method Summary
The study implements a RAG framework that retrieves regulatory guidance documents, clinicaltrials.gov data, and scientific literature to augment GPT-4 during clinical trial protocol generation. The evaluation framework assesses four dimensions: clinical thinking and logic, medical terminology and completeness, transparency and references, and readability. The research tests the approach on 14 different diseases across various clinical trial phases, comparing standard GPT-4 performance against the RAG-augmented version.

## Key Results
- Standard GPT-4 performs well on content relevance and medical terminology but poorly on clinical logic
- RAG-augmented LLMs achieve over 80% accuracy in clinical thinking and logic
- RAG significantly improves transparency and referencing, with citations being correct and appropriate nearly 80% of the time
- The approach demonstrates effectiveness across 14 different diseases and multiple trial phases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG improves LLM accuracy in clinical trial document writing by providing external, current regulatory and clinical data
- Mechanism: The RAG framework retrieves relevant documents (regulatory guidance, clinicaltrials.gov data) and feeds them to the LLM during generation, supplementing the model's internal knowledge
- Core assumption: The retrieved documents are relevant and accurate enough to significantly improve the LLM's output quality
- Evidence anchors:
  - [abstract]: "RAG provides the model with external regulatory guidance and clinical trial data to improve accuracy."
  - [section]: "RAG is a methodology for incorporating knowledge from external databases...involves providing the LLM with external sources of knowledge to supplement the model's internal representation of information."
  - [corpus]: Weak evidence. The corpus contains related RAG research but no direct evaluation of clinical trial document writing
- Break condition: If retrieved documents are outdated, irrelevant, or the LLM fails to properly synthesize the information, accuracy gains will be minimal or non-existent

### Mechanism 2
- Claim: RAG significantly improves LLM performance on clinical thinking and logic by grounding outputs in current regulatory guidance
- Mechanism: By providing the LLM with access to current regulatory documents, RAG ensures that generated content adheres to the latest guidelines and avoids common errors
- Core assumption: Regulatory documents contain the necessary information to guide clinical thinking and logic in trial protocols
- Evidence anchors:
  - [abstract]: "RAG significantly improves performance, achieving over 80% accuracy in clinical thinking and logic..."
  - [section]: "Specifically we analyzed four dimensions: Clinical thinking and logic, which measures how closely recommendations from regulatory guidance documents were included in the generated section..."
  - [corpus]: Weak evidence. The corpus contains RAG research but no specific evaluation of clinical thinking and logic in trial protocols
- Break condition: If the regulatory documents are incomplete or ambiguous, or if the LLM misinterprets the guidance, clinical thinking and logic scores may not improve

### Mechanism 3
- Claim: RAG improves LLM transparency and referencing by including appropriate citations to source documents
- Mechanism: The RAG framework retrieves source documents and provides the LLM with the necessary information to include proper citations in the generated text
- Core assumption: The retrieved documents contain the necessary information for the LLM to generate appropriate citations
- Evidence anchors:
  - [abstract]: "RAG significantly improves performance, achieving over 80% accuracy in clinical thinking and logic, as well as proper referencing."
  - [section]: "Regarding transparency and references, the RAG-augmented LLM (by design) includes references, which we show to be correct and appropriate nearly 80% of the time."
  - [corpus]: Weak evidence. The corpus contains related RAG research but no specific evaluation of transparency and referencing in clinical trial documents
- Break condition: If the retrieved documents do not contain the necessary information for citations, or if the LLM fails to properly attribute the information, transparency and referencing scores may not improve

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: Understanding how LLMs work is crucial for evaluating their use in clinical trial document writing and for understanding the limitations of off-the-shelf LLMs
  - Quick check question: What are the key limitations of using off-the-shelf LLMs for clinical trial document writing, according to the paper?
- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is the key technique used in this study to improve LLM performance in clinical trial document writing. Understanding how RAG works is essential for understanding the results
  - Quick check question: How does RAG address the limitations of off-the-shelf LLMs in clinical trial document writing?
- Concept: Clinical Trial Protocols
  - Why needed here: The paper focuses on evaluating LLMs for generating sections of clinical trial protocols. Understanding the structure and requirements of these documents is crucial for evaluating the results
  - Quick check question: What are the key sections of a clinical trial protocol that were evaluated in this study?

## Architecture Onboarding

- Component map: User Query -> RAG Framework (retrieves regulatory documents, clinicaltrials.gov data, scientific literature) -> LLM (GPT-4) -> Generated Protocol Section -> Evaluation Framework
- Critical path:
  1. User provides a query to generate a specific section of a clinical trial protocol
  2. RAG framework retrieves relevant documents based on the query
  3. Retrieved documents are provided to the LLM
  4. LLM generates the requested section of the protocol
  5. Generated text is evaluated using the evaluation framework
- Design tradeoffs:
  - Off-the-shelf LLM vs. RAG-augmented LLM: Off-the-shelf LLMs are simpler to use but may lack accuracy and proper referencing. RAG-augmented LLMs are more complex but offer improved accuracy and transparency
  - Choice of external data sources: The quality and relevance of the retrieved documents directly impact the performance of the RAG-augmented LLM
- Failure signatures:
  - Low scores in clinical thinking and logic: Indicates that the LLM is not properly incorporating regulatory guidance into the generated text
  - Low scores in transparency and references: Indicates that the LLM is not properly citing sources or including appropriate references
  - Irrelevant or inaccurate retrieved documents: Indicates that the RAG framework is not properly identifying and retrieving relevant information
- First 3 experiments:
  1. Generate a clinical trial protocol section using an off-the-shelf LLM and evaluate the results using the evaluation framework
  2. Implement a basic RAG framework using a small set of regulatory documents and evaluate its impact on LLM performance
  3. Experiment with different configurations of the RAG framework (e.g., different retrieval algorithms, different sets of external data sources) and evaluate their impact on LLM performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RAG-augmented LLMs vary across different clinical trial phases and disease types?
- Basis in paper: [explicit] The paper evaluates LLMs on protocols for 14 diseases across different phases but does not provide detailed performance breakdowns by disease type or phase
- Why unresolved: The study provides overall performance metrics but lacks granular analysis of how well the RAG-augmented LLMs perform for specific disease types or trial phases
- What evidence would resolve it: A detailed analysis of RAG-augmented LLM performance metrics segmented by disease type and trial phase would clarify this

### Open Question 2
- Question: What are the long-term impacts of using LLMs on the quality and consistency of clinical trial documents?
- Basis in paper: [inferred] The paper discusses the potential of LLMs to improve writing efficiency but does not address long-term impacts on document quality and consistency
- Why unresolved: The study focuses on immediate performance improvements but does not explore how consistent and high-quality outputs are maintained over time
- What evidence would resolve it: Longitudinal studies tracking the quality and consistency of LLM-generated documents over multiple trial cycles would provide insights

### Open Question 3
- Question: How can LLMs be integrated into existing clinical trial workflows without disrupting current processes?
- Basis in paper: [explicit] The paper mentions the need for responsible human oversight and the challenges of integrating LLMs into existing workflows
- Why unresolved: While the paper acknowledges the potential of LLMs, it does not provide specific strategies for seamless integration into current clinical trial workflows
- What evidence would resolve it: Case studies or pilot programs demonstrating successful integration of LLMs into existing clinical trial workflows would offer practical solutions

## Limitations
- The evaluation framework assesses only four dimensions of clinical trial protocol writing, potentially missing critical aspects of protocol quality
- Results are based on GPT-4 specifically, leaving uncertainty about generalizability to other large language models
- The study does not demonstrate whether improved evaluation scores translate to actual clinical trial success or regulatory approval

## Confidence

**High Confidence:** The finding that RAG-augmented LLMs achieve over 80% accuracy in clinical thinking and logic compared to standard GPT-4's poor performance. This is directly supported by the evaluation framework and consistent with established RAG literature showing improved accuracy through external knowledge retrieval.

**Medium Confidence:** The claim that RAG-augmented LLMs are "highly effective for generating high-quality clinical trial documents." While the evaluation shows improvements in specific dimensions, the study does not demonstrate that these improvements translate to actual clinical trial success or regulatory approval. The evaluation framework, while comprehensive for the assessed dimensions, may not capture all critical aspects of protocol quality.

**Low Confidence:** The assertion that RAG-augmented LLMs can replace human expertise in clinical trial protocol writing. The study does not address the role of human oversight, the need for domain expertise in protocol development, or the potential for LLM-generated content to introduce subtle errors that could impact trial outcomes.

## Next Checks
1. **Real-world regulatory assessment:** Submit RAG-augmented LLM-generated protocol sections to regulatory experts for formal review and assessment of approval likelihood, comparing results against human-written protocols
2. **Cross-model validation:** Replicate the study using alternative LLMs (e.g., Claude, Gemini) to determine whether the observed improvements are specific to GPT-4 or generalizable across language models
3. **Longitudinal study:** Track the performance of RAG-augmented LLM-generated protocols through actual clinical trial phases to assess whether improved evaluation scores correlate with better trial outcomes, participant safety, and data quality