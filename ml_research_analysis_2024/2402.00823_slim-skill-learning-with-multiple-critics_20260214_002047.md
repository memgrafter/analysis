---
ver: rpa2
title: 'SLIM: Skill Learning with Multiple Critics'
arxiv_id: '2402.00823'
source_url: https://arxiv.org/abs/2402.00823
tags:
- slim
- learning
- skill
- discovery
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of learning reusable manipulation
  skills without task-specific supervision, a problem where existing mutual-information-based
  methods struggle to generate both diverse and safe behaviors in complex robotic
  manipulation tasks. The proposed solution, SLIM, employs a multi-critic actor-critic
  architecture to combine three complementary reward signals: a reaching bonus to
  encourage interaction with objects, a Lipschitz-constrained discovery reward for
  diversity, and a safety constraint to ensure compliant behavior.'
---

# SLIM: Skill Learning with Multiple Critics

## Quick Facts
- arXiv ID: 2402.00823
- Source URL: https://arxiv.org/abs/2402.00823
- Reference count: 34
- Key outcome: Discovers diverse and safe manipulation skills via multi-critic architecture that outperforms mutual-information-based baselines

## Executive Summary
SLIM addresses the challenge of unsupervised skill discovery in robotic manipulation by introducing a multi-critic actor-critic architecture that combines three complementary reward signals: reaching bonus, Lipschitz-constrained discovery reward, and safety constraint. The method successfully generates diverse and safe manipulation behaviors that transfer effectively to downstream tasks including position matching, trajectory following, and multi-object rearrangement.

## Method Summary
SLIM trains a skill-conditioned policy using PPO with three separate critics, each corresponding to a specific reward component: a reaching bonus to encourage object interaction, a Lipschitz-constrained discovery reward for diversity, and a safety constraint to ensure compliant behavior. The key innovation is normalizing advantages from each critic before combining them for policy updates, which prevents interference between rewards of different scales and enables stable learning of diverse, safe manipulation skills.

## Key Results
- Discovers skills with higher state coverage and safety rate than state-of-the-art mutual-information-based baselines
- Skills enable significantly faster learning of downstream tasks (position/orientation matching, trajectory following)
- Can be effectively sequenced for multi-object rearrangement tasks
- Demonstrates practical utility in hierarchical control and planning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple critics avoid reward interference during skill discovery.
- Mechanism: Separate critic networks per reward component learn value estimates at their own scale, preventing one reward's magnitude from dominating updates.
- Core assumption: Reward interference is the primary reason naive reward summation fails to produce useful and safe behaviors.
- Evidence anchors:
  - [abstract] "Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery"
  - [section] "We propose to use a multi-critic actor learning architecture... to learn a latent variable skill-conditioned policy... with three critics for the above reward functions"
  - [corpus] Weak anchor - corpus neighbors don't directly discuss multi-critic architectures for skill discovery.

### Mechanism 2
- Claim: Batch normalization of advantages balances contribution of each reward.
- Mechanism: Normalized advantages from each critic ensure that during policy updates, each reward component contributes proportionally regardless of its original scale.
- Core assumption: Without normalization, reward scale differences would cause some components to dominate policy updates.
- Evidence anchors:
  - [section] "One key component in our implementation is that we learn the value functions for each reward function using their respective reward scales but perform a batch normalization of the advantages computed from each critic before combining them with weights for actor learning"
  - [section] "This scheme has the advantages of (i) fostering unperturbed critic learning per reward component and (ii) easing the burden of choosing appropriate weights"
  - [corpus] No direct evidence - corpus doesn't discuss advantage normalization in multi-critic settings.

### Mechanism 3
- Claim: Dedicated safety reward component ensures learned skills respect predefined constraints.
- Mechanism: Safety indicator function provides negative rewards for unsafe states, and its separate critic ensures this signal is preserved during policy learning.
- Core assumption: Mutual information maximization alone cannot ensure safety in robotic manipulation.
- Evidence anchors:
  - [abstract] "As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful and safe manipulation behaviors"
  - [section] "Furthermore, ensuring the safety of skill discovery is another critical problem recently introduced in [5]"
  - [section] "rsafety = −I(st), where I is a safety indicator function over the states encoding predefined safety constraints"
  - [corpus] No direct evidence - corpus neighbors don't discuss safety in skill discovery.

## Foundational Learning

- Concept: Mutual information maximization for skill discovery
  - Why needed here: Forms the foundation of how latent skills relate to visited states in unsupervised skill learning
  - Quick check question: What is the relationship between skill z and state s in standard mutual information skill discovery?

- Concept: Actor-critic reinforcement learning framework
  - Why needed here: Provides the optimization mechanism for learning both the policy and value functions
  - Quick check question: How does the policy gradient update differ between single-critic and multi-critic approaches?

- Concept: Hierarchical reinforcement learning
  - Why needed here: Enables leveraging discovered skills as motor primitives for downstream tasks
  - Quick check question: What is the role of the high-level controller in HRL when using pre-trained skills?

## Architecture Onboarding

- Component map:
  State vector -> State representation ϕ(s) -> Three critics (Vreach, Vdiscovery, Vsafety) -> Batch-normalized advantages -> Actor policy π(a|s,z) -> Skill-conditioned actions -> Environment

- Critical path:
  1. Sample skill sequence (z1,...,zn)
  2. Collect trajectories using current policy
  3. Update state representation ϕ to maximize displacement
  4. Update each critic with its respective reward component
  5. Compute and batch-normalize advantages from all critics
  6. Update actor policy using combined advantages

- Design tradeoffs:
  - Multiple critics add parameter count but reduce interference
  - Separate normalization per advantage vs. global reward scaling
  - Equal weighting of normalized advantages vs. learned weighting

- Failure signatures:
  - If critics diverge significantly in magnitude: check reward scales and normalization
  - If policy learns only one type of behavior: check if all rewards are being combined properly
  - If safety is violated frequently: check safety reward scaling and critic learning

- First 3 experiments:
  1. Test with only reaching reward - verify basic object interaction
  2. Test with reaching + discovery - verify diversity in object manipulation
  3. Test with all three rewards - verify safe and diverse behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the interference between multiple reward components manifest during skill discovery, and can we develop more principled methods to identify and mitigate such interference beyond the multi-critic architecture?
- Basis in paper: [explicit] The paper states "We empirically demonstrated that by integrating multiple critics and associated reward functions, the resulting skill-conditioned policy acquires safe and diverse manipulation skills" and discusses interference between reward signals as a motivation for the multi-critic approach.
- Why unresolved: The paper uses multi-critics as a solution but doesn't deeply analyze the nature of interference between rewards or explore alternative mitigation strategies.
- What evidence would resolve it: Empirical studies comparing different methods of handling reward interference (weighted combinations, normalization schemes, alternative architectures) with detailed analysis of how interference manifests in training dynamics.

### Open Question 2
- Question: Can the bonus reward function for encouraging object interaction be replaced with a learned intrinsic reward that adapts to the specific manipulation task and environment without manual design?
- Basis in paper: [explicit] The paper mentions "One limitation of our approach is that we assume an easy-to-design and reasonably generic bonus reward function to help with encouraging object interaction. A natural extension for future work is to replace this bonus with another intrinsic reward function that serves the same purpose of easing exploration."
- Why unresolved: The paper uses a hand-designed reaching bonus but acknowledges this as a limitation without exploring automated alternatives.
- What evidence would resolve it: Demonstrations of skill discovery performance using learned intrinsic rewards (e.g., from curiosity, empowerment, or task-agnostic exploration objectives) compared to the hand-designed reaching bonus.

### Open Question 3
- Question: How do skills discovered by SLIM transfer to real-world robotic manipulation scenarios, and what are the key sim-to-real gaps that need to be addressed?
- Basis in paper: [explicit] The paper states "Lastly, sim2real deployment of our learned skill policies and assessing the benefits of applying our approach in other fields such as locomotion and navigation holds potential for fruitful exploration."
- Why unresolved: All experiments are conducted in simulation (IsaacGym), and the paper explicitly identifies sim-to-real transfer as an open direction.
- What evidence would resolve it: Successful deployment of SLIM-learned skills on a physical robot system, with analysis of performance degradation and necessary adaptations for real-world dynamics, sensing noise, and actuation limitations.

## Limitations
- Assumes an easy-to-design and reasonably generic bonus reward function to help with encouraging object interaction
- All experiments conducted in simulation; sim-to-real transfer remains unexplored
- Theoretical guarantees for convergence and optimality of multi-critic skill discovery are not provided

## Confidence
- High confidence in the multi-critic architecture design and its ability to combine heterogeneous rewards
- Medium confidence in the effectiveness of advantage normalization for balancing reward contributions
- Medium confidence in the safety reward's ability to constrain learned behaviors

## Next Checks
1. Ablation study isolating the multi-critic architecture vs. naive reward summation with normalization to directly measure interference effects
2. Sensitivity analysis of safety constraint strength to determine if current parameters are optimal or conservative
3. Extended evaluation on downstream tasks beyond single-object manipulation to test scalability of discovered skills