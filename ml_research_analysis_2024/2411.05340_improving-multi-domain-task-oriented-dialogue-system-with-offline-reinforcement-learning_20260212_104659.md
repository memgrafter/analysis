---
ver: rpa2
title: Improving Multi-Domain Task-Oriented Dialogue System with Offline Reinforcement
  Learning
arxiv_id: '2411.05340'
source_url: https://arxiv.org/abs/2411.05340
tags:
- dialogue
- system
- language
- pre-trained
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of fine-tuning pre-trained
  language models for task-oriented dialogue (TOD) systems, specifically exposure
  bias and token loss problems that hinder user task completion. The authors propose
  an end-to-end TOD system that fine-tunes GPT-2 using both supervised learning and
  reinforcement learning (RL).
---

# Improving Multi-Domain Task-Oriented Dialogue System with Offline Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2411.05340
- **Source URL**: https://arxiv.org/abs/2411.05340
- **Reference count**: 35
- **Key outcome**: Increases inform rate by 1.60% and success rate by 3.17% on MultiWOZ2.1

## Executive Summary
This paper addresses exposure bias and token loss problems in fine-tuning pre-trained language models for task-oriented dialogue (TOD) systems. The authors propose an end-to-end TOD system that fine-tunes GPT-2 using both supervised learning and reinforcement learning (RL). The RL component uses a non-differentiable reward function based on success rate and BLEU metrics to guide the model toward generating coherent, fluent responses that complete user tasks. Experimental results on MultiWOZ2.1 demonstrate consistent improvements over baseline methods, particularly for multi-domain dialogues.

## Method Summary
The proposed method fine-tunes GPT-2 on dialogue-session level using a combined supervised learning and reinforcement learning approach. The model processes user utterances, belief states, system acts, and system responses across all dialogue turns in the input sequence. Supervised learning uses cross-entropy loss for next token prediction, while RL employs REINFORCE with a reward function combining success rate (task completion) and BLEU score (fluency). The reward is calculated as: Reward(y, ŷ) = α × Success(y, ŷ) + (1 - α) × BLEU(yu, ŷu) + 1, where α controls the trade-off between task completion and response quality.

## Key Results
- Increases inform rate by 1.60% and success rate by 3.17% compared to UBAR baseline on MultiWOZ2.1
- Demonstrates consistent improvements across different dialogue turn sizes
- Particularly effective for multi-domain dialogues while maintaining competitive performance on BLEU and combined score metrics

## Why This Works (Mechanism)

### Mechanism 1
Offline reinforcement learning with a non-differentiable reward function mitigates exposure bias and token loss in TOD systems. The RL component updates the language model parameters to maximize a reward composed of success rate and BLEU score, steering the model toward generating sequences that complete user tasks rather than merely predicting the next token.

### Mechanism 2
Dialogue session-level fine-tuning provides richer context for the model to learn multi-turn task-oriented dialogue patterns. By including user utterance, belief state, system act, and system response across all dialogue turns in the input sequence, the model learns dependencies between dialogue states and responses over the entire conversation.

### Mechanism 3
The weighted reward function balances task completion and response quality. The reward combines success rate (measuring task completion) and BLEU score (measuring fluency/coherence) with a tunable parameter α to find an optimal balance between these objectives.

## Foundational Learning

- **Reinforcement learning fundamentals (policy gradient methods, reward maximization)**: Why needed here - The paper employs REINFORCE loss to optimize the model parameters based on the non-differentiable reward signal. Quick check question: How does the REINFORCE algorithm update model parameters when the reward is not differentiable?

- **Pre-trained language model fine-tuning strategies**: Why needed here - The method builds upon GPT-2 and requires understanding how supervised fine-tuning differs from RL-based fine-tuning. Quick check question: What is the key difference between supervised learning loss and REINFORCE loss in this context?

- **Task-oriented dialogue system evaluation metrics**: Why needed here - The reward function uses success rate and BLEU, which are standard evaluation metrics for TOD systems. Quick check question: How do inform rate, success rate, and BLEU differ in measuring TOD system performance?

## Architecture Onboarding

- **Component map**: User utterance → Belief state generation → Database query → System action generation → System response generation, with RL optimizing the entire path

- **Critical path**: User utterance → Belief state generation → Database query → System action generation → System response generation, with RL optimizing the entire path

- **Design tradeoffs**: Greedy decoding vs. more sophisticated decoding strategies (faster but potentially less optimal outputs); Fixed reward weights vs. adaptive weighting (simpler but may not generalize well); Non-differentiable reward vs. differentiable approximation (more accurate but harder to optimize)

- **Failure signatures**: Low success rate with high BLEU (model prioritizes fluency over task completion); High success rate with low BLEU (model completes tasks but generates unnatural responses); Performance degradation on multi-domain dialogues vs. single-domain

- **First 3 experiments**: 1) Implement the baseline UBAR model to establish a performance reference; 2) Add the RL component with fixed reward weights (α=3, β=0.8) to verify the basic RL mechanism works; 3) Test different reward weightings (α values) to find the optimal balance between success rate and BLEU

## Open Questions the Paper Calls Out

- How does the performance of the proposed RL approach compare to other advanced RL techniques such as PPO or A2C when applied to the same task-oriented dialogue system?

- What is the impact of varying the dialogue context length on the model's performance, and is there an optimal context length for different types of tasks?

- How does the model perform on datasets with more diverse domains or more complex user queries, and what adaptations are necessary for such scenarios?

## Limitations

- Weak empirical validation based solely on MultiWOZ2.1 without testing on additional datasets or domains
- Non-differentiable reward complexity introduces optimization challenges that are not fully explored
- Limited evaluation metrics without human evaluation or qualitative assessments of response quality

## Confidence

- **Multi-domain effectiveness**: High confidence - Demonstrates consistent improvements across dialogue turn sizes
- **RL contribution to task completion**: Medium confidence - Ablation study comparing supervised-only vs. supervised+RL is not provided
- **Generalizability to other TOD systems**: Low confidence - Evaluation limited to single dataset and architecture (GPT-2)

## Next Checks

1. Conduct ablation study on reward weighting by systematically testing different values of α in the reward function to determine sensitivity to this hyperparameter

2. Evaluate the trained model on an alternative TOD benchmark (such as MultiWOZ2.2 or Schema-Guided Dialogue Dataset) to assess generalizability beyond MultiWOZ2.1

3. Train a version of the model using only supervised learning (without RL) under identical conditions to quantify the specific contribution of the RL component to the observed performance gains