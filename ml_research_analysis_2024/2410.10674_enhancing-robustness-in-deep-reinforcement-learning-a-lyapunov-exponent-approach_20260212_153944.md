---
ver: rpa2
title: 'Enhancing Robustness in Deep Reinforcement Learning: A Lyapunov Exponent Approach'
arxiv_id: '2410.10674'
source_url: https://arxiv.org/abs/2410.10674
tags:
- state
- reward
- control
- policies
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of deep reinforcement learning
  policies to small state perturbations in continuous control tasks, demonstrating
  that learned policies can exhibit deterministic chaos with unstable non-linear dynamics.
  Using Lyapunov Exponents, the authors show that even high-performing policies in
  complex environments like Walker and Cheetah can produce chaotic state and reward
  trajectories, where small perturbations lead to exponentially diverging outcomes.
---

# Enhancing Robustness in Deep Reinforcement Learning: A Lyapunov Exponent Approach

## Quick Facts
- arXiv ID: 2410.10674
- Source URL: https://arxiv.org/abs/2410.10674
- Authors: Rory Young; Nicolas Pugeault
- Reference count: 39
- This paper investigates the robustness of deep reinforcement learning policies to small state perturbations in continuous control tasks

## Executive Summary
This paper investigates the robustness of deep reinforcement learning policies to small state perturbations in continuous control tasks, demonstrating that learned policies can exhibit deterministic chaos with unstable non-linear dynamics. Using Lyapunov Exponents, the authors show that even high-performing policies in complex environments like Walker and Cheetah can produce chaotic state and reward trajectories, where small perturbations lead to exponentially diverging outcomes. To address this, they propose a novel Maximal Lyapunov Exponent regularization method for the Dreamer V3 architecture, which estimates local state divergence using the Recurrent State Space Model and incorporates it into the policy loss. Experiments show that this regularization reduces chaotic dynamics and improves robustness to observation noise, with regularized policies achieving better performance under Gaussian noise perturbations compared to baseline methods.

## Method Summary
The authors propose a Maximal Lyapunov Exponent (MLE) regularization method for Dreamer V3 that estimates local state divergence using the Recurrent State Space Model (RSSM) and incorporates it into the policy loss. The method generates multiple trajectory samples from the same initial state using the RSSM, calculates the variance across these trajectories as a proxy for local state divergence, and adds this variance to the policy loss. This encourages the policy to produce stable state trajectories that achieve high rewards. The approach is tested on DeepMind Control Suite environments with SAC, TD3, and Dreamer V3 agents, and robustness is evaluated under Gaussian observation noise.

## Key Results
- Learned policies in complex environments like Walker and Cheetah exhibit chaotic dynamics with positive Maximal Lyapunov Exponents
- MLE regularization reduces chaotic state dynamics and improves robustness to observation noise
- Regularized policies achieve better performance under Gaussian noise perturbations compared to baseline methods
- The variance across RSSM trajectory samples effectively estimates local state divergence for regularization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLE regularization reduces chaotic state dynamics by penalizing variance between predicted trajectories.
- Mechanism: The regularization term adds a loss component that encourages the policy to minimize variance across multiple sampled future trajectories. By reducing trajectory divergence, the policy becomes less sensitive to small state perturbations, thereby lowering the Maximal Lyapunov Exponent.
- Core assumption: The variance across sampled trajectories is a good proxy for the local state divergence used to estimate the Lyapunov Exponent.
- Evidence anchors:
  - [abstract]: "This new approach reduces the chaotic state dynamics, rendering the learnt policies more resilient to sensor noise or adversarial attacks"
  - [section]: "the variance between trajectories (VarL(·)) thus provides an estimation of the local state divergence"
  - [corpus]: Weak evidence - corpus papers mention Lyapunov exponents but none directly validate variance-as-proxy approach for RL policy regularization

### Mechanism 2
- Claim: Chaotic reward surfaces create high-frequency oscillations that make policies vulnerable to adversarial attacks.
- Mechanism: When λ₁ > 0, small state perturbations cause exponentially diverging reward trajectories. This creates a fractal return surface where tiny changes in state lead to drastically different total rewards, providing opportunities for adversarial attacks to exploit these high-frequency variations.
- Core assumption: The reward function is Lipschitz continuous with respect to state trajectories when λ₁ < 0, but becomes fractal when λ₁ > 0.
- Evidence anchors:
  - [section]: "small state perturbations produce significantly different total rewards" and "adversarial attack methods could leverage these high-frequency oscillations"
  - [corpus]: Weak evidence - corpus papers mention robustness and Lyapunov exponents but don't specifically address fractal reward surfaces in RL

### Mechanism 3
- Claim: MLE regularization maintains performance while improving robustness by balancing reward maximization with stability.
- Mechanism: The policy loss combines the standard Dreamer V3 objective with the MLE regularization term. This joint optimization encourages the policy to find solutions that achieve high expected returns while maintaining stable state dynamics, rather than optimizing return at the expense of stability.
- Core assumption: The weighting between the standard loss and regularization term can be tuned to maintain performance while achieving stability gains.
- Evidence anchors:
  - [section]: "we propose incorporating the regularisation term...into the policy loss" and "This incentivises the policy to produce stable state trajectories which attain high rewards"
  - [corpus]: Weak evidence - corpus papers discuss robustness improvements but don't provide specific evidence for this combined optimization approach

## Foundational Learning

- Concept: Lyapunov Exponents as stability metrics
  - Why needed here: The paper uses MLE and SLE to quantify chaos in RL policies, which requires understanding how these exponents measure exponential divergence rates
  - Quick check question: What does a positive MLE indicate about a dynamical system's behavior?

- Concept: Recurrent State Space Models (RSSM)
  - Why needed here: The MLE regularization relies on RSSM to predict multiple future trajectories whose variance estimates the local state divergence
  - Quick check question: How does the RSSM generate multiple plausible future trajectories from the same initial state?

- Concept: Model-based reinforcement learning
  - Why needed here: Dreamer V3 is a model-based RL algorithm, and understanding its architecture is crucial for implementing the MLE regularization
  - Quick check question: What are the key components of Dreamer V3's architecture that enable trajectory prediction?

## Architecture Onboarding

- Component map: Encoder -> Decoder -> Dynamics Predictor -> Sequence Model -> Policy + Value Head, with MLE regularization loss term using RSSM components to generate L trajectory samples for variance calculation.

- Critical path: Forward pass through RSSM → generate L trajectory samples → calculate variance across samples → add to policy loss → backward pass with combined gradients.

- Design tradeoffs: Higher L values give better divergence estimates but increase computation. The regularization weight must balance stability gains against performance degradation. Assumes RSSM accurately models environment dynamics.

- Failure signatures: Performance degradation without robustness improvement suggests regularization is too strong. Persistent positive MLE indicates insufficient regularization or poor RSSM modeling. Unexpected reward drops may indicate poor balance between objectives.

- First 3 experiments:
  1. Verify that variance calculation correctly estimates divergence by comparing against ground truth MLE on simple chaotic systems
  2. Test different regularization weights on Walker Walk environment to find optimal balance
  3. Compare robustness to Gaussian noise between baseline and regularized policies across all environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MLE regularization method scale to more complex, high-dimensional environments beyond the DeepMind Control Suite tasks?
- Basis in paper: [inferred] The paper demonstrates the method on relatively standard benchmark tasks and notes that high-performing policies in complex environments exhibit chaotic dynamics.
- Why unresolved: The experiments were limited to specific environments, and the paper doesn't provide evidence of performance on more complex real-world applications.
- What evidence would resolve it: Testing the method on more complex, higher-dimensional environments or real-world robotic control tasks would demonstrate scalability.

### Open Question 2
- Question: What is the computational overhead of calculating the MLE regularization term during training, and how does it impact training time?
- Basis in paper: [explicit] The paper mentions that increasing the number of trajectories (L) improves accuracy but requires more computational resources, and they set L=3 to maintain similar training time to baseline Dreamer V3.
- Why unresolved: The paper doesn't provide quantitative data on the additional computational cost or training time impact.
- What evidence would resolve it: Empirical measurements comparing training time and computational resources with and without the MLE regularization term.

### Open Question 3
- Question: How sensitive is the MLE regularization to the choice of hyperparameters such as the variance weighting in the loss function?
- Basis in paper: [explicit] The paper presents the regularization term but doesn't explore sensitivity to hyperparameter choices or provide ablation studies for the weighting coefficient.
- Why unresolved: The paper doesn't explore the sensitivity of the method to different hyperparameter settings.
- What evidence would resolve it: Systematic ablation studies varying the weighting coefficient and other relevant hyperparameters to show how performance changes.

### Open Question 4
- Question: Can the MLE regularization method be adapted to other model-based RL algorithms beyond Dreamer V3?
- Basis in paper: [inferred] The method is specifically designed for Dreamer V3's RSSM architecture, but the underlying concept of using state variance to estimate chaos could potentially apply to other methods.
- Why unresolved: The paper only demonstrates the method on Dreamer V3 and doesn't explore generalization to other architectures.
- What evidence would resolve it: Implementing and testing the method on other model-based RL algorithms to verify its broader applicability.

## Limitations

- The variance-as-proxy approach for estimating MLE may be unreliable when the RSSM poorly models true state dynamics
- The optimal regularization weight balance between stability and performance remains largely empirical
- The analysis focuses on continuous control tasks and may not extend to discrete action spaces

## Confidence

**High Confidence Claims:**
- MLE and SLE can quantify chaos in RL policies
- Positive MLE values indicate unstable dynamics in trained policies
- RSSM-based trajectory sampling is feasible for variance estimation

**Medium Confidence Claims:**
- MLE regularization improves robustness to observation noise
- Chaotic reward surfaces create vulnerability to adversarial attacks
- The combined optimization approach maintains performance while improving stability

**Low Confidence Claims:**
- Variance across RSSM trajectories reliably estimates true MLE
- The specific regularization weight of 0.01 is optimal across all environments
- Chaotic dynamics are the primary cause of poor robustness (vs. other factors)

## Next Checks

1. **Ground truth validation**: Compare RSSM-based MLE estimates against analytical MLE values on simple chaotic systems (Lorenz, logistic map) to verify the variance-as-proxy method.

2. **Cross-environment robustness**: Test the regularized policy's performance under varying noise levels and distributions (not just Gaussian) across all seven environments to assess generalization of robustness improvements.

3. **Ablation study**: Systematically vary the regularization weight and L (number of trajectory samples) to map the performance-robustness tradeoff space and identify optimal hyperparameters.