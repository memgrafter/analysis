---
ver: rpa2
title: Is Context Helpful for Chat Translation Evaluation?
arxiv_id: '2403.08314'
source_url: https://arxiv.org/abs/2403.08314
tags:
- translation
- context
- metrics
- quality
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how well existing automatic metrics evaluate
  machine-translated chats. It finds that chat translations are less error-prone than
  news texts and that error types differ between domains.
---

# Is Context Helpful for Chat Translation Evaluation?

## Quick Facts
- arXiv ID: 2403.08314
- Source URL: https://arxiv.org/abs/2403.08314
- Reference count: 19
- Chat translations have fewer errors than news and benefit from context in evaluation

## Executive Summary
This paper investigates how well automatic metrics evaluate machine-translated chats and whether context can improve their performance. The authors benchmark multiple metrics on human-annotated chat data, finding that reference-based neural metrics like COMET-22 achieve the highest correlation with human judgments. They then explore augmenting metrics with conversational context, showing that reference-free metrics benefit significantly while reference-based metrics do not. Finally, they propose CONTEXT-MQM, an LLM-based metric that uses bilingual context, which outperforms both non-contextual LLM metrics and COMET-22 on imperfect translations.

## Method Summary
The study evaluates automatic metrics for chat translation quality using MQM annotations from WMT 2022 Chat Shared Task (7120 conversations, English-German). The authors benchmark existing metrics (BLEU, CHRF, BERTSCORE, BLEURT, COMET variants, XCOMET, METRICX) and extend COMET with contextual information by prepending conversation history. They implement CONTEXT-MQM using GPT-4 with bilingual context prompts and compare correlation with human judgments across different scenarios.

## Key Results
- Chat translations contain fewer errors than news texts, with fluency errors being more frequent
- Reference-based COMET-22 achieves highest correlation with human judgments across all language pairs
- Reference-free metrics benefit significantly from context in out-of-English settings
- CONTEXT-MQM with bilingual context outperforms both non-contextual LLM metrics and COMET-22 on imperfect translations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context improves translation quality evaluation for short, ambiguous sentences
- Mechanism: Short sentences lack sufficient information for disambiguation; adding conversation context provides necessary disambiguation
- Core assumption: The added context is relevant and correct
- Evidence anchors: Abstract shows context helps reference-free metrics and out-of-English settings; section shows context most helpful for sentences â‰¤20 characters
- Break condition: If context is irrelevant, incorrect, or noisy, it can introduce ambiguity and hurt evaluation

### Mechanism 2
- Claim: Reference-free metrics benefit more from context than reference-based metrics
- Mechanism: Reference-based metrics already have reference context; adding more context may introduce noise
- Core assumption: Reference-based metrics are less reliant on additional context for disambiguation
- Evidence anchors: Abstract shows COMET-22 doesn't benefit while COMET-20-QE does; section shows reference-based metrics don't benefit on average
- Break condition: If reference translation lacks necessary context, reference-based metrics might also benefit

### Mechanism 3
- Claim: LLM-based metrics can benefit from context in evaluating chat translation quality
- Mechanism: LLMs can leverage reasoning and in-context learning to identify errors when provided with conversation context
- Core assumption: LLMs can effectively utilize context for translation quality evaluation
- Evidence anchors: Abstract shows CONTEXT-MQM improves quality assessment with bilingual context; section confirms preliminary experiments show improvements
- Break condition: If LLM cannot effectively utilize context or if context is noisy, performance might not improve

## Foundational Learning

- Concept: Multidimensional Quality Metrics (MQM)
  - Why needed here: Used to annotate and evaluate machine-translated chat quality with detailed error categorization
  - Quick check question: What are the main categories of errors in MQM, and how are they weighted?

- Concept: Spearman rank correlation
  - Why needed here: Measures correlation between automatic metric scores and human judgments
  - Quick check question: What is the range of Spearman rank correlation, and what does a high value indicate?

- Concept: Contextual Neural Machine Translation
  - Why needed here: Understanding how contextual information is incorporated into NMT models is crucial for designing context-aware evaluation metrics
  - Quick check question: How do contextual NMT models utilize previous sentences in conversation to improve translation quality?

## Architecture Onboarding

- Component map: Data preprocessing -> Metric evaluation -> Context augmentation -> LLM-based evaluation -> Analysis and reporting
- Critical path: Prepare chat translation data with context -> Evaluate automatic metrics -> Extend metrics with context -> Implement and evaluate CONTEXT-MQM -> Analyze results
- Design tradeoffs: Reference-based vs reference-free metrics (context reliance vs reference availability), context window size (information vs noise), LLM model selection (capability differences)
- Failure signatures: Low correlation with human judgments, performance degradation with context, inconsistent results across language pairs
- First 3 experiments: 1) Evaluate COMET-22 correlation on all chat translation data, 2) Extend COMET-20-QE with context for out-of-English pairs, 3) Implement CONTEXT-MQM and compare to non-contextual LLM metrics on English-German subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific types of errors occur most frequently in chat translations that are not captured by current metrics?
- Basis in paper: [explicit] Chat translations have different error types than news, with fluency errors being more frequent
- Why unresolved: Study identifies error type differences but lacks detailed analysis of specific categories current metrics miss
- What evidence would resolve it: Detailed breakdown of error frequencies by specific MQM categories in chat vs news domains

### Open Question 2
- Question: How much context is optimal for improving chat translation quality estimation?
- Basis in paper: [explicit] Paper shows context helps but doesn't determine optimal amount needed
- Why unresolved: Experiments vary context window sizes but don't establish clear threshold or optimal number of sentences
- What evidence would resolve it: Systematic analysis showing correlation improvements plateau at specific context window size

### Open Question 3
- Question: Why do reference-based metrics not benefit from context while reference-free metrics do?
- Basis in paper: [explicit] Paper observes COMET-22 doesn't benefit while COMET-20-QE does
- Why unresolved: Paper hypothesizes about references containing necessary information but doesn't explain underlying mechanism
- What evidence would resolve it: Comparative analysis of metric architectures and how they process contextual information

## Limitations

- Findings primarily based on English-German chat translation data, limiting generalizability to other language pairs
- CONTEXT-MQM evaluation is preliminary with limited examples (11 reference translations, 4-5 context sentences)
- Paper lacks detailed error analysis for cases where context addition decreases correlation with human judgments

## Confidence

**High Confidence**: Chat translations are less error-prone than news texts; reference-based COMET-22 achieves highest correlation with human judgments

**Medium Confidence**: Reference-free metrics benefit more from context than reference-based metrics in out-of-English settings

**Low Confidence**: Effectiveness of CONTEXT-MQM for evaluating imperfect translations due to limited evaluation scope

## Next Checks

1. Replicate context-augmentation experiments across additional language pairs (e.g., English-French, English-Chinese) to verify if reference-free metrics consistently benefit more than reference-based metrics

2. Conduct ablation studies on CONTEXT-MQM by systematically removing context components to isolate contribution of each element to performance improvements

3. Compare CONTEXT-MQM against other contextual LLM-based metrics using the same evaluation framework to establish relative performance