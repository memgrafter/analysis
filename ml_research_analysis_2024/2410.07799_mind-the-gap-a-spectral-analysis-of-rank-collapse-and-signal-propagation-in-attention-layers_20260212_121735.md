---
ver: rpa2
title: 'Mind the Gap: a Spectral Analysis of Rank Collapse and Signal Propagation
  in Attention Layers'
arxiv_id: '2410.07799'
source_url: https://arxiv.org/abs/2410.07799
tags:
- attention
- rank
- matrix
- collapse
- matrices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes signal propagation in softmax-based attention
  layers using random matrix theory. The authors identify a previously unknown phenomenon
  called "rank collapse in width," which occurs when the context length increases,
  causing all tokens to converge to a single representation.
---

# Mind the Gap: a Spectral Analysis of Rank Collapse and Signal Propagation in Attention Layers

## Quick Facts
- arXiv ID: 2410.07799
- Source URL: https://arxiv.org/abs/2410.07799
- Authors: Thiziri Nait Saada; Alireza Naderi; Jared Tanner
- Reference count: 40
- Key outcome: Identifies rank collapse in width as a new attention layer failure mode caused by spectral gaps between singular values, and proposes removing outlier eigenvalues as a fix

## Executive Summary
This paper analyzes signal propagation in softmax-based attention layers using random matrix theory, uncovering a previously unknown phenomenon called "rank collapse in width" that occurs when context length increases. The authors prove that this spectral collapse is caused by a spectral gap between the two largest singular values of the attention matrix. They propose a simple solution—removing the outlier eigenvalue(s) from the attention matrix—which provably mitigates rank collapse in width and reduces gradient explosion. Experiments on commercial models like BERT validate these findings, showing the fix effectively prevents rank collapse in width while slowing rank collapse in depth.

## Method Summary
The authors use random matrix theory to analyze softmax attention matrices, proving that a spectral gap exists between the largest singular value (always 1) and the bulk of eigenvalues. They demonstrate that this gap causes rank collapse in width as context length increases, where all token representations converge to a single point. The proposed solution modifies the attention matrix by subtracting the all-ones matrix (1/T 1T×T), eliminating the dominant eigenvector direction. Theoretical analysis is validated empirically on commercial transformer models, comparing vanilla attention with the modified version across different context lengths.

## Key Results
- Rank collapse in width occurs as context length increases due to a spectral gap between the two largest singular values
- The spectral gap causes all tokens to converge toward a single representation along the dominant eigenvector direction
- Removing the outlier eigenvalue(s) provably mitigates rank collapse in width and reduces gradient explosion
- Experiments on BERT models validate that the proposed fix prevents rank collapse in width and slows rank collapse in depth

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Rank collapse in width is caused by a spectral gap between the two largest singular values of the attention matrix.
- **Mechanism**: As context length increases, the softmax-based attention matrix develops a spectral gap where the largest eigenvalue remains 1 while the bulk of eigenvalues shrink, causing all tokens to converge toward a single representation.
- **Core assumption**: The input to softmax has i.i.d. Gaussian entries and the input tokens are orthonormal (X0X0⊤ = I).
- **Evidence anchors**:
  - [abstract]: "uncovers a spectral gap between the two largest singular values of the attention matrix as the cause of (iii)"
  - [section 2.3]: "Since the rows sum to unity, there is an eigenvalue of 1 corresponding to the eigenvector of all-ones."
- **Break condition**: If input tokens are not orthonormal or if the attention matrix is not derived from i.i.d. Gaussian key/query matrices, the spectral gap may not form as described.

### Mechanism 2
- **Claim**: Removing the outlier eigenvalue(s) from the attention matrix provably mitigates rank collapse in width.
- **Mechanism**: By subtracting the all-ones matrix (1/T 1T×T) from the attention matrix, the dominant eigenvector direction is eliminated, preventing token representations from collapsing to a single point.
- **Core assumption**: The rank-one perturbation 1/T 1T×T is the source of the spectral gap.
- **Evidence anchors**:
  - [abstract]: "propose a novel yet simple practical solution to mitigate rank collapse in width by removing the outlier eigenvalue(s)"
  - [section 2.4]: "we can slightly modify the attention mechanism to eliminate the outlier—and thus the gap—simply by replacing A with A⊥"
- **Break condition**: If the attention matrix has multiple outliers or the perturbation is not rank-one, simply removing the largest eigenvalue may not fully resolve rank collapse.

### Mechanism 3
- **Claim**: Rank collapse in depth is exacerbated by rank collapse in width through repeated matrix multiplications.
- **Mechanism**: When rank collapse in width occurs within each attention layer, subsequent matrix multiplications in deeper layers amplify this effect, causing faster convergence to a single representation across all tokens.
- **Core assumption**: Rank collapse in depth naturally arises from repeated matrix multiplications and is accelerated when each layer already suffers from rank collapse in width.
- **Evidence anchors**:
  - [abstract]: "we identify an additional and previously unknown challenge unique to softmax attention layers: (iii) rank collapse in width"
- **Break condition**: If architectural modifications like skip connections or LayerNorm sufficiently slow rank collapse in depth independently, the interaction between width and depth collapse may be less significant.

## Foundational Learning

- **Concept**: Random Matrix Theory (RMT) and spectral analysis
  - **Why needed here**: The paper relies on RMT to analyze the eigenvalue/singular value distributions of random attention matrices and prove the existence of spectral gaps
  - **Quick check question**: What is the limiting distribution of eigenvalues for a large i.i.d. Gaussian matrix (Wigner's semicircle law)?

- **Concept**: Free probability and asymptotic freeness
  - **Why needed here**: Used to analyze the product of multiple random matrices and compute moments of limiting distributions for covariance and Jacobian matrices
  - **Quick check question**: How does free multiplicative convolution differ from classical convolution when combining the spectra of random matrices?

- **Concept**: Submultiplicativity of operator norms
  - **Why needed here**: Critical for bounding the singular values of products of matrices and proving results about exploding gradients
  - **Quick check question**: If A and B are matrices with ||A|| ≤ c1 and ||B|| ≤ c2, what is the bound on ||AB||?

## Architecture Onboarding

- **Component map**: Input tokens (X0) -> Key/query/value matrices (WQ, WK, WV) -> Attention matrix (A) -> Output signal (X)
- **Critical path**: X0 → A(X0) → WV → X, where A(X0) is the softmax attention matrix and WV transforms the attended values
- **Design tradeoffs**:
  - Orthonormal vs. non-orthonormal inputs: Theorem 1 assumes orthonormal inputs for mathematical rigor, but experiments show results hold more generally
  - Single vs. multiple outliers: The paper focuses on removing the largest spectral gap, but experiments reveal additional outliers emerge in deeper layers
  - Computational cost: Removing all outliers requires SVD at each layer, while removing only the largest is much cheaper
- **Failure signatures**:
  - Stable rank sr(Σ) → 1 as T increases (rank collapse in width)
  - Gradient norm ∥∂XL/∂WV∥F growing as T^(L-1) (exploding gradients)
  - Spectral gap between largest and second-largest singular values in attention matrix
- **First 3 experiments**:
  1. **Verify spectral gap existence**: Generate random key/query matrices, compute attention matrix A(X0), and plot eigenvalues to confirm gap between λ1=1 and λ2=O(1/√T)
  2. **Test rank collapse in width**: Pass isotropic input through single attention layer, compute stable rank sr(XX⊤), and verify it converges to 1 as T increases
  3. **Validate the fix**: Apply the "remove gap" modification (A⊥ = A - 1/T 1T×T), repeat experiment 2, and confirm stable rank scales linearly with T instead of collapsing

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the precise relationship between spectral gap removal and improved training dynamics in deeper layers?
- **Basis in paper**: [explicit] The authors note their theoretical framework provides insights into recent empirical studies showing training benefits from various fixes, including "centering" self-attention layers and "differentiating" attention mechanisms, which can be interpreted as implicit efforts to address spectral gap issues.
- **Why unresolved**: The paper focuses on initialization dynamics and theoretical analysis, while training dynamics are beyond its scope.
- **What evidence would resolve it**: Systematic training experiments comparing standard attention, gap-removed attention, and alternative attention mechanisms across multiple architectures and tasks, measuring both convergence speed and final performance.

### Open Question 2
- **Question**: How do additional outliers that emerge in deeper layers affect rank collapse dynamics, and what is their precise mathematical characterization?
- **Basis in paper**: [explicit] The authors observe that additional outliers emerge from the spectrum with depth in an intricate way and conduct experiments comparing removing the largest outlier versus all outliers.
- **Why unresolved**: The paper does not provide a theoretical explanation for why these additional outliers emerge or their mathematical characterization beyond empirical observation.
- **What evidence would resolve it**: A theoretical analysis of how the product of i.i.d. Markov matrices with removed outliers affects spectral properties across layers, combined with experiments identifying patterns in when and how additional outliers appear.

### Open Question 3
- **Question**: Can alternative attention mechanisms be designed that maintain favorable signal propagation properties while avoiding spectral gap issues?
- **Basis in paper**: [inferred] The authors analyze various attention variants (ReLU, sigmoid) that fall within the class of attention matrices presenting a spectral gap, and note their analysis is relevant to these variants.
- **Why unresolved**: While the paper identifies the spectral gap as problematic and analyzes its effects, it does not systematically explore the design space of attention mechanisms that could avoid this issue while maintaining performance.
- **What evidence would resolve it**: A comprehensive analysis of attention mechanisms that modify the softmax function or key-query dot product structure to eliminate or mitigate spectral gaps, followed by empirical evaluation of their signal propagation and task performance.

## Limitations
- Theoretical analysis assumes i.i.d. Gaussian key/query matrices and orthonormal input tokens, which may not hold in practice
- The proposed fix may be insufficient for very deep networks where additional outliers emerge in deeper layers
- Computational tradeoff between removing all outliers (requiring SVD) versus only the largest gap is not fully explored

## Confidence
**High confidence**: The existence of rank collapse in depth is well-established in the literature and the paper's contribution in identifying rank collapse in width is supported by both theoretical analysis and empirical validation. The spectral gap mechanism and its connection to the softmax normalization are mathematically sound.

**Medium confidence**: The claim that removing the spectral gap provably mitigates rank collapse in width is theoretically justified under the paper's assumptions, but the practical effectiveness depends on how closely real attention matrices match the idealized model. The experimental validation on commercial models provides supporting evidence but doesn't conclusively prove the mechanism works universally.

**Low confidence**: The assertion that the proposed fix is the "most effective" solution with "lowest computational cost" compared to alternatives is not rigorously proven. The paper mentions that removing all outliers could provide additional benefits but doesn't quantify this or compare it systematically against the single-gap removal strategy.

## Next Checks
1. **Generalization to non-orthogonal inputs**: Test the theoretical predictions on input tokens that are not perfectly orthonormal to verify if the spectral gap phenomenon persists under more realistic conditions.

2. **Deep network ablation study**: Systematically compare the performance of removing only the largest gap versus removing all detected outliers across multiple layers to quantify the tradeoff between effectiveness and computational cost.

3. **Cross-architecture validation**: Apply the analysis and proposed fix to attention mechanisms beyond standard softmax, such as linear attention or other variants, to determine if the spectral gap phenomenon is specific to softmax or more general.