---
ver: rpa2
title: A Two-Scale Complexity Measure for Deep Learning Models
arxiv_id: '2401.09184'
source_url: https://arxiv.org/abs/2401.09184
tags:
- lower
- training
- where
- fisher
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel complexity measure, the two-scale effective
  dimension (2sED), for statistical models based on the effective dimension. The measure
  provably bounds the generalization error under mild assumptions on the model.
---

# A Two-Scale Complexity Measure for Deep Learning Models

## Quick Facts
- arXiv ID: 2401.09184
- Source URL: https://arxiv.org/abs/2401.09184
- Authors: Massimiliano Datres; Gian Paolo Leonardi; Alessio Figalli; David Sutter
- Reference count: 40
- Key outcome: Introduces a novel two-scale effective dimension (2sED) complexity measure that provably bounds generalization error and can be efficiently approximated for deep learning models

## Executive Summary
This paper introduces the two-scale effective dimension (2sED) as a complexity measure for statistical models, particularly deep learning architectures. The measure combines global parameter space dimensions with local complexity measures through a convex combination, allowing it to provide non-vacuous generalization bounds even for over-parameterized models. For Markovian models, the authors develop an efficient layerwise iterative approach to approximate 2sED from below, making it computationally tractable for deep networks with many parameters.

## Method Summary
The method defines 2sED as a convex combination of the ambient dimension and effective dimension, controlled by a meso-scale parameter Î¶. For Markovian models, it computes a lower bound through layerwise iteration by exploiting the block diagonal structure of the Fisher information matrix. The approach uses Monte Carlo integration to estimate the required integrals, making it scalable to high-dimensional parameter spaces. The measure is validated through experiments on standard datasets (Covertype, MNIST, CIFAR10) with various neural network architectures (MLPs, CNNs).

## Key Results
- 2sED provides provable generalization bounds under mild assumptions on the model
- Lower 2sED approximation enables efficient computation for deep learning models
- Experimental results show strong correlation between 2sED and training error across different architectures
- The measure satisfies key properties of complexity measures: correlation with model expressiveness and ability to distinguish between model complexities

## Why This Works (Mechanism)

### Mechanism 1
The two-scale effective dimension provides tighter generalization bounds than the original effective dimension by using a convex combination of ambient dimension and effective dimension. This adjustment allows the bound to remain non-vacuous even when models are over-parameterized, as it can downweight the contribution of small eigenvalues that dominate the original measure. The mechanism relies on the Fisher information matrix being L-Lipschitz with bounded eigenvalues.

### Mechanism 2
The lower 2sED provides an efficient approximation for Markovian models by exploiting the block structure of the Fisher information matrix. Each block corresponds to a layer, and the iterative computation takes expectations over previous layers' parameters. This approach is possible because of the concavity of the logarithm, which allows swapping expectation and logarithm operations. The Markovian property ensures each layer's distribution depends only on the previous layer's output.

### Mechanism 3
Experimental results validate 2sED as a complexity measure by demonstrating correlation with training error. Models with higher lower 2sED values tend to achieve lower training losses, indicating the measure captures the model's expressive power and ability to fit data. This correlation is established through experiments on various neural network architectures and datasets, though it relies on the validity of stochastic perturbations as approximations for deterministic models.

## Foundational Learning

- Concept: Fisher information matrix
  - Why needed here: 2sED is defined in terms of the Fisher information matrix, and the lower 2sED approximation relies on its block structure for Markovian models
  - Quick check question: What is the Fisher information matrix, and how is it related to the curvature of the log-likelihood function?

- Concept: Markovian property
  - Why needed here: The lower 2sED approximation is specifically designed for Markovian models with sequential, feed-forward structure
  - Quick check question: What is the Markovian property, and how does it relate to the structure of the Fisher information matrix for sequential models?

- Concept: Covering numbers and VC dimension
  - Why needed here: 2sED relates to covering numbers, which measure function class complexity, helping interpret its meaning in statistical learning theory
  - Quick check question: How are covering numbers related to the VC dimension, and what role do they play in statistical learning theory?

## Architecture Onboarding

- Component map: Compute Fisher information matrix -> Compute 2sED or lower 2sED -> Use as complexity measure for model selection or generalization bound
- Critical path: 1) Compute Fisher information matrix for the model 2) Compute 2sED or lower 2sED using paper formulas 3) Use as complexity measure for model selection or generalization bounds
- Design tradeoffs: Accuracy vs. efficiency (lower 2sED vs. full 2sED), generality vs. specificity (2sED for all models vs. lower 2sED for Markovian models), theoretical guarantees vs. empirical performance
- Failure signatures: If Fisher information matrix is not well-defined or cannot be computed accurately, 2sED cannot be computed; if model doesn't satisfy Markovian property, lower 2sED approximation is not applicable; if Monte Carlo estimation is inaccurate, 2sED may not be reliable
- First 3 experiments: 1) Compute 2sED for simple linear regression and verify theoretical prediction 2) Compute lower 2sED for small Markovian model (e.g., hidden Markov model) and verify approximation quality 3) Compute lower 2sED for small neural network and verify correlation with training error on simple dataset

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but acknowledges limitations regarding the applicability to non-Markovian models and the need for further exploration of the measure's properties across different model families and datasets.

## Limitations
- Theoretical claims depend on strong assumptions about Fisher information matrix properties (Lipschitz continuity, bounded eigenvalues) that may not hold for all deep learning models
- Empirical validation relies on stochastic perturbations of deterministic networks, which may not fully capture true model complexity
- Monte Carlo estimation procedures for Fisher information matrix are not fully specified, making exact reproduction challenging
- The measure's predictive power for generalization performance beyond training error is not extensively explored

## Confidence

- Mechanism 1 (Generalization Bound): Medium confidence - Theoretical derivation appears sound but relies on assumptions that may not generalize to all architectures
- Mechanism 2 (Lower 2sED Approximation): Medium confidence - Iterative approach is mathematically elegant but approximation quality depends heavily on Markovian structure
- Mechanism 3 (Empirical Correlation): Low-Medium confidence - Correlation with training error demonstrated but experiments limited in scope and don't test predictive power for generalization

## Next Checks

1. **Theoretical stress test**: Systematically examine cases where Fisher information matrix is not Lipschitz (e.g., ReLU networks with large weight magnitudes) to determine when generalization bound becomes vacuous

2. **Approximation error analysis**: Compute both full 2sED and lower 2sED for small Markovian models where exact computation is feasible, quantifying approximation error across different architectures and dataset sizes

3. **Generalization prediction test**: Use 2sED measure to predict test set performance on held-out data, comparing predictive accuracy against established complexity measures like PAC-Bayes bounds and sharpness-based measures