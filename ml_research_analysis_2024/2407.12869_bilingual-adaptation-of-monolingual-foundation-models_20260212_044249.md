---
ver: rpa2
title: Bilingual Adaptation of Monolingual Foundation Models
arxiv_id: '2407.12869'
source_url: https://arxiv.org/abs/2407.12869
tags:
- llama
- arabic
- tokens
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents an efficient method for adapting monolingual
  large language models to another language, focusing on adapting Llama 2 to Arabic.
  The authors propose a two-stage approach: expanding the vocabulary and training
  only the embeddings matrix, followed by full model continual pre-training on a bilingual
  corpus.'
---

# Bilingual Adaptation of Monolingual Foundation Models

## Quick Facts
- arXiv ID: 2407.12869
- Source URL: https://arxiv.org/abs/2407.12869
- Reference count: 16
- Primary result: Two-stage vocabulary extension and continual pre-training effectively adapts Llama models to Arabic/Hindi while preserving English capabilities

## Executive Summary
This paper presents a cost-effective method for adapting monolingual large language models to new languages, specifically demonstrating adaptation of Llama 2 to Arabic and Llama 3 8B to Hindi. The authors propose a two-stage approach: first extending the vocabulary and training only the embeddings matrix, followed by full model continual pre-training on a bilingual corpus. This approach achieves significant improvements in the target language while maintaining or slightly enhancing performance in English. The method addresses key challenges including catastrophic forgetting and tokenizer limitations, and includes detailed ablation studies on embedding initialization techniques, data mix ratios, and learning rates.

## Method Summary
The method employs a two-stage approach to adapt monolingual LLMs to new languages. First, the vocabulary is extended by adding target language tokens (100% of original vocabulary size for Arabic, 25% for Hindi), and the embedding layer is trained on a bilingual corpus mix (9:1 target:English) while keeping other parameters frozen. Second, the full model undergoes continual pre-training on the same bilingual corpus mix with optimal hyperparameters. Embedding initialization uses techniques like embedding space transformation (projecting Jais embeddings to Llama space via least squares) and similarity-based initialization. The approach includes careful mitigation of catastrophic forgetting through strategic English-Arabic data mixing during pre-training.

## Key Results
- Llama 2-extend100 tokenizer reduces Arabic fertility by 72.17% while maintaining English fertility
- 1:9 English-Arabic data mix ratio effectively mitigates catastrophic forgetting
- Significant Arabic performance improvements with slight English enhancements
- Generalizable to multiple model sizes (7B, 13B, 70B) and languages (Arabic, Hindi)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vocabulary extension reduces tokenization imbalance between languages, lowering inference cost and improving performance.
- Mechanism: Expanding the base tokenizer vocabulary with frequent tokens from the target language creates a more balanced multilingual tokenizer. This reduces the number of subwords needed to represent the same text in the target language, improving efficiency and model capability.
- Core assumption: A tokenizer with balanced fertility across languages leads to better model performance and lower computational cost.
- Evidence anchors:
  - [section]: "We experiment with two methods, vocabulary replacement and vocabulary extension, to create balanced tokenizers for English and Arabic."
  - [section]: "Llama 2-extend100 reduces the fertility of Llama 2's tokenizer by 72.17% while maintaining the fertility in English."
  - [corpus]: Weak evidence - no direct citations on tokenization efficiency.
- Break condition: If the extended vocabulary doesn't significantly reduce the fertility gap between languages, or if it introduces tokens that don't improve representation of the target language.

### Mechanism 2
- Claim: Proper embedding initialization aligns semantic representations across languages, enabling knowledge transfer.
- Mechanism: Using techniques like embedding space transformation and similarity-based initialization ensures that tokens representing similar concepts in different languages have similar embeddings. This alignment facilitates cross-lingual knowledge transfer during continual pre-training.
- Core assumption: Aligned embeddings in the latent space enable the model to transfer knowledge and reasoning capabilities across languages.
- Evidence anchors:
  - [section]: "We explore three techniques for initializing newly added token embeddings... We use OpenAI's text-embedding-3-large embeddings for their superior quality and multilingual performance."
  - [section]: "In this initialization method, we leverage the pre-trained embedding vectors of Jais-30B... We find a linear transformation to project EJais to ELlama2's space by solving for W and b using the least squares method."
  - [corpus]: Weak evidence - no direct citations on embedding alignment techniques.
- Break condition: If the embedding initialization techniques fail to produce semantically meaningful alignments, leading to poor cross-lingual transfer.

### Mechanism 3
- Claim: Continual pre-training on a bilingual corpus with optimal data mix ratios mitigates catastrophic forgetting and enables cross-lingual transfer.
- Mechanism: By continually pre-training the model on a mix of the target language corpus and the original language corpus, the model retains its proficiency in the original language while acquiring capabilities in the target language. The optimal data mix ratio ensures a balance between learning the new language and preserving the original language capabilities.
- Core assumption: A carefully chosen mix of original and target language data during continual pre-training prevents catastrophic forgetting and enables effective cross-lingual transfer.
- Evidence anchors:
  - [section]: "We conduct exhaustive experiments to find a minimum proportion of Pile data that should be mixed with AraV5 to mitigate forgetting. Table 2 shows results from the experiments with different data mixes."
  - [section]: "Interestingly, increasing the amount of English data while keeping Arabic tokens constant improves Arabic performance, indicating cross-lingual capability transfer."
  - [corpus]: Weak evidence - no direct citations on optimal data mix ratios for continual pre-training.
- Break condition: If the data mix ratio is not optimal, leading to either catastrophic forgetting of the original language or poor performance in the target language.

## Foundational Learning

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding catastrophic forgetting is crucial for designing effective continual pre-training strategies that preserve the model's original capabilities while adapting to a new language.
  - Quick check question: What is catastrophic forgetting, and how does it affect the adaptation of monolingual models to new languages?

- Concept: Tokenization and Vocabulary
  - Why needed here: A solid understanding of tokenization techniques and vocabulary management is essential for creating balanced multilingual tokenizers and ensuring efficient model training and inference.
  - Quick check question: How does the choice of tokenizer and vocabulary size impact the model's performance and efficiency in multilingual settings?

- Concept: Embedding Alignment
  - Why needed here: Proper embedding initialization and alignment techniques are critical for enabling cross-lingual knowledge transfer and ensuring that the model can effectively utilize its existing capabilities in the new language.
  - Quick check question: What are the key considerations for initializing and aligning embeddings when adapting a monolingual model to a new language?

## Architecture Onboarding

- Component map: Tokenizer -> Embedding Layer -> Transformer Decoder Blocks
- Critical path: 1) Extend vocabulary and train embedding layer, 2) Continually pre-train full model on bilingual corpus, 3) Fine-tune on downstream tasks
- Design tradeoffs: Vocabulary size (representation vs. computational cost), embedding initialization method (alignment quality vs. efficiency), data mix ratio (cross-lingual transfer vs. original language preservation)
- Failure signatures: Catastrophic forgetting of original language, poor target language performance, inefficient tokenization or embedding alignment
- First 3 experiments:
  1. Vocabulary extension and embedding initialization: Extend base vocabulary with target language tokens and initialize new embeddings using embedding space transformation or similarity-based methods
  2. Continual pre-training with optimal data mix: Train on bilingual corpus with varying data mix ratios to find optimal balance between cross-lingual transfer and original language preservation
  3. Fine-tuning on downstream tasks: Evaluate adapted model on downstream tasks in both original and target languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of English to Arabic data during continual pre-training for maintaining English capabilities while acquiring Arabic proficiency?
- Basis in paper: [explicit] The authors found that mixing 1 part English with 9 parts Arabic (1:9 En:Ar) is sufficient to mitigate forgetting, but they also note that increasing the amount of English data while keeping Arabic tokens constant improves Arabic performance.
- Why unresolved: The paper does not explore the full range of possible English-Arabic data mix ratios or provide a definitive optimal ratio.
- What evidence would resolve it: Experiments testing a wider range of English-Arabic data mix ratios and their impact on both English and Arabic downstream task performance would help determine the optimal ratio.

### Open Question 2
- Question: How does the choice of embedding initialization method affect the cross-lingual transfer capabilities of the adapted model?
- Basis in paper: [explicit] The authors explore three techniques for initializing newly added token embeddings and find that the Embedding Space Transformation method performs better than similarity-based initialization.
- Why unresolved: The paper does not provide a comprehensive comparison of all embedding initialization methods or their impact on cross-lingual transfer capabilities.
- What evidence would resolve it: A detailed ablation study comparing the performance of models with different embedding initialization methods on cross-lingual transfer tasks would help determine the optimal method.

### Open Question 3
- Question: Can the proposed adaptation approach be extended to low-resource languages with limited pre-existing bilingual corpora?
- Basis in paper: [explicit] The authors mention their intention to extend this approach to low-resource languages in the future.
- Why unresolved: The paper does not explore the application of the proposed method to low-resource languages or discuss potential challenges in such scenarios.
- What evidence would resolve it: Experiments adapting the method to low-resource languages and evaluating its effectiveness would provide insights into the generalizability of the approach.

## Limitations
- Evaluation focuses primarily on Arabic and Hindi adaptation from English without testing generalization to other language pairs
- Relies heavily on proprietary model weights and datasets that may not be fully reproducible
- Embedding initialization techniques lack direct empirical validation of cross-lingual semantic alignment quality
- Optimal data mix ratios determined through limited ablation studies without extensive hyperparameter search
- Cross-lingual transfer evaluation is indirect rather than through explicit cross-lingual task transfer

## Confidence
**High Confidence**: The two-stage adaptation approach is well-established in the literature and experimental results show consistent improvements across multiple model sizes and languages.

**Medium Confidence**: Specific embedding initialization techniques and their claimed benefits for cross-lingual transfer are plausible but lack direct empirical validation.

**Low Confidence**: Generalizability to other language pairs beyond Arabic and Hindi is not tested, and cross-lingual transfer evaluation is indirect.

## Next Checks
1. **Cross-Lingual Transfer Validation**: Design experiments that explicitly test cross-lingual transfer capabilities through parallel data training and zero-shot cross-lingual inference tasks.

2. **Embedding Alignment Quality Assessment**: Implement direct measures of embedding space alignment quality such as bilingual lexicon induction or cross-lingual similarity retrieval tasks.

3. **Generalization to Other Language Pairs**: Adapt the methodology to a different language pair (e.g., Spanish or French to English) and evaluate performance on both languages to assess generalizability beyond Arabic and Hindi.