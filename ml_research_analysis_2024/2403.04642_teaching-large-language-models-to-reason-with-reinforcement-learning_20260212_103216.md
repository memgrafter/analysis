---
ver: rpa2
title: Teaching Large Language Models to Reason with Reinforcement Learning
arxiv_id: '2403.04642'
source_url: https://arxiv.org/abs/2403.04642
tags:
- performance
- training
- https
- semanticscholar
- corpusid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates reinforcement learning (RL) methods to improve
  large language models' (LLMs) reasoning capabilities. It compares three RL algorithms
  - Expert Iteration (EI), Proximal Policy Optimization (PPO), and Return-Conditioned
  RL - on math word problems using different reward schemes and model initializations.
---

# Teaching Large Language Models to Reason with Reinforcement Learning

## Quick Facts
- arXiv ID: 2403.04642
- Source URL: https://arxiv.org/abs/2403.04642
- Reference count: 37
- Primary result: RL fine-tuning improves both maj@1 and pass@96 metrics simultaneously, unlike supervised fine-tuning which trades off between these metrics

## Executive Summary
This paper investigates reinforcement learning methods to improve large language models' reasoning capabilities on math word problems. The authors compare three RL algorithms - Expert Iteration (EI), Proximal Policy Optimization (PPO), and Return-Conditioned RL - across different model sizes and reward schemes. Surprisingly, EI consistently outperforms PPO and RCRL, achieving strong performance with low sample complexity. The study finds that RL fine-tuning improves both greedy accuracy (maj@1) and best-of-96 accuracy (pass@96) simultaneously, unlike supervised fine-tuning which typically trades off between these metrics. The authors identify limited exploration as a key bottleneck, noting that models fail to discover solutions beyond what supervised fine-tuning can achieve.

## Method Summary
The authors implement three RL algorithms on Llama-2 models for math reasoning tasks. Expert Iteration samples K solutions per question, filters incorrect/duplicate responses, and uses high-reward samples for training. PPO uses a KL-penalized objective with LoRA adaptation, while RCRL conditions policies on return values. They test both sparse rewards (correct/incorrect) and dense rewards (step-by-step solution matching). Models are evaluated on GSM8K and SVAMP benchmarks using multiple metrics including maj@1, pass@96, and rerank@96. Training uses either pretrained checkpoints or SFT checkpoints as initialization.

## Key Results
- EI consistently outperforms PPO and RCRL across model sizes and reward schemes
- RL fine-tuning improves both maj@1 and pass@96 simultaneously, unlike SFT
- All algorithms converge quickly (~60,000 rollouts) even without SFT initialization
- Exploration bottleneck limits further improvement - models don't discover solutions beyond SFT capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EI outperforms PPO and RCRL because it uses high-quality synthetic data generated by the student model itself, which is more diverse than static supervised datasets.
- Mechanism: EI samples K times per question, filters out incorrect or duplicate solutions, and uses the remaining high-reward samples as training data. This exploration generates a more diverse set of solution paths than SFT data, which improves both maj@1 and pass@96 simultaneously.
- Core assumption: The student model's own samples are sufficiently diverse and of high enough quality to act as a proxy for an "optimal policy."
- Evidence anchors:
  - [abstract]: "RL fine-tuning improves both maj@1 and pass@96 metrics simultaneously, unlike supervised fine-tuning which trades off between these metrics. The authors attribute this to RL's ability to generate diverse training data during exploration."
  - [section 4.1]: "EI gives a sizable improvement of around 7% over the SFT baseline... EI mostly improves maj@1 scores relative to PPO."
- Break condition: If the student model cannot generate diverse solutions beyond what is already present in SFT data, EI's advantage disappears.

### Mechanism 2
- Claim: PPO converges as quickly as EI despite being more sophisticated because reasoning tasks have deterministic dynamics, which favors behavior cloning approaches like EI.
- Mechanism: In deterministic environments, the optimal policy can be approximated by directly cloning observed high-reward trajectories. PPO's additional machinery for handling stochasticity and exploration is unnecessary.
- Core assumption: The reasoning tasks studied are fully deterministic, so there is no advantage to PPO's on-policy exploration strategies.
- Evidence anchors:
  - [section 4.1]: "Both algorithms achieve comparable pass@96 scores across model sizes... This suggests even with RL training our best models are not discovering solutions beyond what can be discovered with (light) supervised fine-tuning given the same rollout budget."
  - [section 5]: "Crucial in our setting is the usage of a pretrained model imparting a strong exploration prior... This is a particularly surprising finding when compared to the performance of EI and PPO on more classical RL problems training a neural network from scratch."
- Break condition: If reasoning tasks had stochastic elements (e.g., requiring search over multiple possible interpretations), PPO's exploration would likely outperform EI.

### Mechanism 3
- Claim: The sample complexity of EI and PPO is low because pretrained models already encode strong reasoning priors, limiting the need for extensive exploration.
- Mechanism: Pretrained LLMs already possess general reasoning capabilities. RL fine-tuning primarily adjusts these existing capabilities rather than learning them from scratch, requiring fewer samples to converge.
- Core assumption: The pretrained model's knowledge is sufficiently aligned with the reasoning task to enable rapid fine-tuning.
- Evidence anchors:
  - [abstract]: "Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of 10^6 samples to converge from a pretrained checkpoint."
  - [section 4.1]: "Both EI and PPO converge relatively quickly even without supervised fine-tuning, requiring only ~60,000 model rollouts."
- Break condition: If the reasoning task required capabilities far outside the pretrained model's distribution, sample complexity would increase dramatically.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation of reasoning tasks
  - Why needed here: Understanding how reasoning problems can be framed as sequential decision-making allows application of RL algorithms.
  - Quick check question: How would you represent a math word problem as an MDP where tokens are both actions and state?

- Concept: Reward shaping and its impact on exploration
  - Why needed here: Different reward schemes (sparse vs. dense) significantly affect model performance and sample complexity.
  - Quick check question: Why might dense rewards that match intermediate steps to reference solutions lead to overfitting?

- Concept: Exploration-exploitation tradeoff in RL
  - Why needed here: The paper identifies limited exploration as a key bottleneck for further improvement.
  - Quick check question: What techniques could encourage a pretrained model to explore beyond solutions already generated by SFT?

## Architecture Onboarding

- Component map: Pretrained model -> SFT (if available) -> RL exploration phase -> Policy improvement -> Evaluation

- Critical path: Pretrained model → SFT (if available) → RL exploration phase → Policy improvement → Evaluation

- Design tradeoffs:
  - EI vs PPO: EI simpler, more sample-efficient for deterministic tasks; PPO better for stochastic environments
  - Dense vs sparse rewards: Dense rewards provide more guidance but may encourage overfitting to training solutions
  - Temperature scheduling: Higher temperatures for exploration, lower for exploitation

- Failure signatures:
  - Model collapse: Output becomes degenerate or repetitive
  - No improvement in pass@96 despite maj@1 gains: Overfitting to specific solution patterns
  - Slow convergence: Insufficient exploration or poor reward signal

- First 3 experiments:
  1. Run EI with K=96 samples per question to establish baseline performance
  2. Compare PPO with and without LoRA to assess memory vs performance tradeoff
  3. Test effect of reducing K to 4 samples per question to study sample complexity requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the exploration bottleneck persist even with larger models and more diverse training data?
- Basis in paper: Explicit - The paper notes that "models fail to explore significantly beyond solutions already produced by SFT models" and this limitation impacts PPO's potential advantages over EI.
- Why unresolved: While the paper observes this bottleneck, it doesn't provide a definitive explanation for why exploration remains limited despite model size increases and the RL's data generation capabilities.
- What evidence would resolve it: Experimental results showing whether increasing model size, using more diverse training data, or modifying the reward structure could overcome the exploration bottleneck would be needed.

### Open Question 2
- Question: Can more sophisticated prompting strategies like Tree of Thought (Yao et al., 2023) and combining LLM generative abilities with evolutionary algorithms (Lehman et al., 2022) effectively encourage complex, rich exploration of reasoning problems?
- Basis in paper: Explicit - The paper suggests that "More sophisticated prompting strategies such as Tree of Thought (Yao et al., 2023) and combining LLM generative abilities with evolutionary algorithms (Lehman et al., 2022) have already begun to make progress in this direction."
- Why unresolved: The paper mentions these strategies as promising directions but doesn't provide empirical evidence of their effectiveness in encouraging exploration.
- What evidence would resolve it: Experimental results comparing the exploration capabilities and reasoning performance of models using these advanced prompting strategies versus standard prompting would be needed.

### Open Question 3
- Question: How does the performance of RL algorithms on reasoning tasks compare to their performance on tasks involving human preferences, such as RLHF?
- Basis in paper: Explicit - The paper notes that "prior work in RLHF finds PPO outperforms EI type approaches in human preference satisfaction and instruction following (Gulcehre et al., 2023; Dubois et al., 2023; Kirk et al., 2023)."
- Why unresolved: The paper observes this difference in performance but doesn't provide a detailed analysis of the factors contributing to it.
- What evidence would resolve it: A comparative study of RL algorithms on both reasoning tasks and human preference tasks, analyzing factors such as reward reliability, exploration requirements, and task complexity, would be needed.

## Limitations
- Conclusions limited to math word problems (GSM8K, SVAMP), may not generalize to broader reasoning tasks
- Deterministic nature of tasks may artificially favor simpler algorithms like EI over PPO
- Pretrained model's strong prior knowledge appears critical but limits understanding of RL's standalone capabilities

## Confidence

**High Confidence**: The empirical finding that EI outperforms PPO and RCRL on the tested benchmarks, with improved maj@1 and pass@96 metrics simultaneously. The sample complexity results showing rapid convergence (~60,000 rollouts) are also well-supported by the experimental data.

**Medium Confidence**: The attribution of EI's success to its ability to generate diverse training data during exploration. While the evidence shows improved performance, the specific mechanism linking exploration to diversity and then to performance requires more rigorous analysis of the solution space coverage.

**Low Confidence**: The claim that pretrained models' strong reasoning priors are the primary reason for low sample complexity. The paper provides correlational evidence but lacks ablation studies comparing models with and without pretraining.

## Next Checks
1. **Exploration Analysis**: Implement solution diversity metrics to quantify the actual diversity of solutions generated by EI versus PPO during training. Compare this diversity to the diversity present in SFT data to validate the exploration hypothesis.

2. **Stochastic Task Generalization**: Test EI and PPO on reasoning tasks with stochastic elements (e.g., multi-step planning with probabilistic outcomes) to determine whether the deterministic-task advantage of EI holds more broadly.

3. **Pretraining Ablation**: Train LLMs from scratch on reasoning tasks and compare sample complexity to pretrained models undergoing RL fine-tuning. This would directly test whether pretraining is responsible for the observed rapid convergence.