---
ver: rpa2
title: 'AdaFish: Fast low-rank parameter-efficient fine-tuning by using second-order
  information'
arxiv_id: '2403.13128'
source_url: https://arxiv.org/abs/2403.13128
tags:
- matrix
- information
- adafish
- fisher
- low-rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaFish, an efficient second-order optimization
  algorithm designed to accelerate parameter-efficient fine-tuning within low-rank
  decomposition frameworks. The key innovation lies in leveraging the inherent low-rank
  structure of weight updates to approximate Hessian information efficiently using
  a generalized Fisher information matrix.
---

# AdaFish: Fast low-rank parameter-efficient fine-tuning by using second-order information

## Quick Facts
- arXiv ID: 2403.13128
- Source URL: https://arxiv.org/abs/2403.13128
- Reference count: 40
- AdaFish achieves faster convergence with half the epochs compared to AdamW while maintaining or improving final performance

## Executive Summary
AdaFish is an efficient second-order optimization algorithm that accelerates parameter-efficient fine-tuning within low-rank decomposition frameworks. The key innovation is leveraging the inherent low-rank structure of weight updates to approximate Hessian information efficiently using a generalized Fisher information matrix. AdaFish constructs an adaptive Fisher information matrix via exponential moving average, serving as a nondiagonal preconditioner that captures second-order momentum without significant computational overhead. Empirical results demonstrate that AdaFish achieves faster convergence and superior performance on both image classification and natural language processing tasks compared to AdamW.

## Method Summary
AdaFish operates by constructing an adaptive Fisher information matrix that approximates the Hessian of the loss function while exploiting the low-rank structure of parameter updates in PEFT methods. The algorithm maintains an exponential moving average of the generalized Fisher information matrix, which serves as a preconditioner to accelerate convergence. By capturing second-order momentum through this approximation, AdaFish achieves faster convergence rates without the computational burden of full second-order methods. The low-rank decomposition framework allows the method to scale efficiently to large models while maintaining theoretical convergence guarantees.

## Key Results
- Reduces required epochs by half compared to AdamW on tested tasks
- Maintains or improves final performance across vision and NLP benchmarks
- Demonstrates superior test accuracy and training efficiency on VTab-1K image classification
- Outperforms AdamW across all Rouge metrics on MIMIC-CXR medical summarization task

## Why This Works (Mechanism)
AdaFish works by exploiting the low-rank structure inherent in parameter-efficient fine-tuning to approximate second-order information efficiently. The generalized Fisher information matrix captures curvature information about the loss landscape, which traditional first-order methods like AdamW cannot access. By maintaining an exponential moving average of this matrix, AdaFish creates an adaptive preconditioner that adjusts the optimization trajectory based on local geometry. This allows for more informed parameter updates that converge faster while requiring minimal additional computation compared to standard PEFT methods.

## Foundational Learning
- **Low-rank decomposition in PEFT**: Why needed - reduces the number of trainable parameters while maintaining model capacity; Quick check - verify rank values used and their impact on parameter count reduction
- **Generalized Fisher information matrix**: Why needed - approximates second-order curvature information efficiently; Quick check - confirm the matrix captures relevant Hessian information without full computation
- **Exponential moving average for preconditioners**: Why needed - provides stability and adapts to changing loss landscape; Quick check - verify update frequency and decay rate settings
- **Nondiagonal preconditioning**: Why needed - captures correlations between parameters for more effective updates; Quick check - compare against diagonal approximations to measure improvement

## Architecture Onboarding

**Component Map**
Input -> Fisher Matrix Update -> Low-rank Parameter Update -> Model Weights -> Loss Computation

**Critical Path**
The critical path involves computing the generalized Fisher information matrix, applying exponential moving average updates, and using this as a preconditioner for low-rank parameter updates. The Fisher matrix update and low-rank decomposition are the most computationally intensive components.

**Design Tradeoffs**
- Accuracy vs. computational overhead in Fisher matrix approximation
- Rank reduction level vs. approximation quality of Hessian information
- Update frequency of Fisher matrix vs. stability and convergence speed
- Memory requirements for storing Fisher information vs. benefit in convergence

**Failure Signatures**
- Divergence or oscillation when Fisher approximation becomes inaccurate
- Plateau in convergence when rank reduction is too aggressive
- Increased memory usage if Fisher matrix updates are too frequent
- Suboptimal performance when model architecture doesn't align with low-rank assumptions

**First Experiments**
1. Compare convergence curves of AdaFish vs AdamW on a small subset of VTab-1K
2. Ablation study varying the rank of low-rank decomposition and measuring impact on performance
3. Wall-clock time analysis comparing AdaFish against AdamW including Fisher matrix computation overhead

## Open Questions the Paper Calls Out
None

## Limitations
- Performance generality across diverse model architectures and tasks remains unproven
- Computational overhead analysis focuses on epoch reduction rather than absolute training time
- Theoretical convergence guarantees rely on idealized assumptions about optimization landscape
- Limited evaluation scope to specific datasets (VTab-1K and MIMIC-CXR) and pretrained model families

## Confidence
- **High confidence**: Core algorithmic contribution of using low-rank structure for efficient second-order approximation is well-established
- **Medium confidence**: Convergence guarantees and theoretical bounds rely on idealized assumptions
- **Medium confidence**: Claim of halving required epochs compared to AdamW needs broader validation

## Next Checks
1. Evaluate AdaFish on standard NLP benchmarks (GLUE, SuperGLUE, SQuAD) using BERT, RoBERTa, and GPT-family models
2. Conduct wall-clock time analysis comparing AdaFish against AdamW and other baselines, including Fisher matrix update costs
3. Perform ablation studies systematically varying low-rank approximation rank and Fisher matrix update frequency