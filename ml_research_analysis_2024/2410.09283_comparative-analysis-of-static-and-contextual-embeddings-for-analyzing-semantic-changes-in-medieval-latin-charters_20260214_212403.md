---
ver: rpa2
title: Comparative Analysis of Static and Contextual Embeddings for Analyzing Semantic
  Changes in Medieval Latin Charters
arxiv_id: '2410.09283'
source_url: https://arxiv.org/abs/2410.09283
tags:
- embeddings
- latin
- word
- semantic
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares static and contextual embeddings for analyzing
  semantic changes in Medieval Latin charters, focusing on shifts due to the Norman
  Conquest. Using the DEEDS corpus, contextual embeddings (BERT trained from scratch)
  outperformed static embeddings in detecting semantic changes, with stronger correlations
  between change labels and cosine similarity.
---

# Comparative Analysis of Static and Contextual Embeddings for Analyzing Semantic Changes in Medieval Latin Charters

## Quick Facts
- arXiv ID: 2410.09283
- Source URL: https://arxiv.org/abs/2410.09283
- Reference count: 25
- Primary result: Contextual embeddings trained from scratch outperform static embeddings in detecting semantic changes in Medieval Latin charters

## Executive Summary
This study compares static and contextual embeddings for analyzing semantic changes in Medieval Latin charters, focusing on shifts due to the Norman Conquest. Using the DEEDS corpus, contextual embeddings (BERT trained from scratch) outperformed static embeddings in detecting semantic changes, with stronger correlations between change labels and cosine similarity. Static embeddings, particularly Incremental and External initialization, showed moderate results but were less effective. The findings highlight the adaptability of contextual embeddings even in scarce, historical datasets, offering insights into linguistic transformations during the Norman Conquest.

## Method Summary
The study uses the DEEDS corpus containing 17k Medieval Latin charters spanning 589-1272 CE, split into three periods (ANG, NOR, PLA). Static embeddings were trained using FastText with three initialization strategies (Incremental, Internal, External), while contextual embeddings were trained from scratch (MLatin-BERT) and adapted from pre-trained models (Ada-BERT-Bam, Ada-BERT-Vas). Word embeddings were extracted and cosine similarities calculated across periods to measure semantic change. Performance was evaluated using manual semantic change labels for 338 target words, with metrics including mean difference in cosine similarity (δµ) and Pearson correlation (ρ) with semantic change labels.

## Key Results
- Contextual embeddings outperformed static embeddings in detecting semantic changes, with stronger correlations between change labels and cosine similarity
- Training contextual embeddings from scratch (MLatin-BERT) performed better than adapting pre-trained models for historical languages
- The Norman Conquest period showed more significant semantic change than the later Plantagenet period

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextual embeddings capture semantic shifts better than static embeddings in scarce historical corpora.
- Mechanism: Contextual embeddings generate unique representations for each word instance based on its context, allowing them to distinguish polysemous words and capture subtle semantic changes that static embeddings cannot.
- Core assumption: The Medieval Latin corpus has sufficient contextual diversity within its limited size to allow contextual embeddings to learn meaningful word representations.
- Evidence anchors:
  - [abstract] "contextual embeddings (BERT trained from scratch) outperformed static embeddings in detecting semantic changes"
  - [section 6.1] "All contextual embeddings demonstrated statistically significant δµ values" and "The correlation coefficient further highlighted the better performance of contextual embeddings"
  - [corpus] Corpus has 3M tokens and 17k charters, which while small compared to modern corpora, may provide enough context diversity
- Break condition: If the corpus is too small or lacks sufficient contextual diversity, contextual embeddings may not learn meaningful representations and could perform worse than static embeddings.

### Mechanism 2
- Claim: Training contextual embeddings from scratch on the target corpus yields better results than adapting pre-trained models for historical languages.
- Mechanism: Pre-trained models may carry biases from modern corpora that interfere with learning historical language patterns, while training from scratch allows the model to learn directly from the historical data distribution.
- Core assumption: The historical language patterns in Medieval Latin are sufficiently different from modern patterns that pre-trained models cannot adequately capture them.
- Evidence anchors:
  - [section 6.1] "both static and contextual models trained from scratch (Incremental and MLatin-BERT) performed better than those adapted from pre-trained embeddings"
  - [section 2.2] Discussion of histBERT and MacBERTh showing training from scratch can outperform adaptation
  - [corpus] Corpus contains "local dialects and borrowings from other languages" that may differ significantly from modern training data
- Break condition: If historical language patterns are similar enough to modern patterns, or if the corpus is too small to learn effectively from scratch, adaptation might perform better.

### Mechanism 3
- Claim: The Norman Conquest period shows more significant semantic change than the later Plantagenet period.
- Mechanism: The Norman Conquest introduced profound administrative, cultural, and linguistic shifts that caused more dramatic word meaning changes than the relatively stable Plantagenet period that followed.
- Core assumption: The Norman Conquest had a more transformative impact on language use than the subsequent period of Norman rule.
- Evidence anchors:
  - [abstract] "This paper presents the first computational analysis of semantic change pre- and post-Norman Conquest"
  - [section 6.2] "We expect the AN period to have a smaller mean value across all words, a larger mean difference between changed and unchanged words, and a more negative correlation between COSN P and semantic change labels than for NP period, based on the assumption that the semantic change from the Anglo-Saxon period to the Norman period is more significant"
  - [corpus] Corpus spans 589-1272 CE, covering both periods with sufficient data for comparison
- Break condition: If subsequent historical events or cultural changes during the Plantagenet period caused comparable or greater semantic shifts, this assumption would be invalid.

## Foundational Learning

- Concept: Static vs contextual embeddings
  - Why needed here: The paper compares these two approaches to determine which better captures semantic change in historical corpora
  - Quick check question: What is the key difference between static and contextual embeddings in how they represent word meanings?

- Concept: Cosine similarity as a semantic change metric
  - Why needed here: The paper uses cosine similarity between word embeddings across periods to quantify semantic change
  - Quick check question: How does cosine similarity between word embeddings from different time periods indicate semantic change?

- Concept: Embedding initialization strategies
  - Why needed here: The paper employs different initialization strategies (incremental, internal, external) for static embeddings to address data scarcity
  - Quick check question: What problem does embedding initialization solve in diachronic embedding analysis, and how do the different strategies address it?

## Architecture Onboarding

- Component map:
  Corpus preprocessing -> Static embedding training (3 strategies) -> Contextual embedding training (scratch + adapted) -> Word embedding extraction -> Cosine similarity calculation -> Semantic change evaluation

- Critical path:
  1. Preprocess Medieval Latin charters and split into three periods
  2. Train static embeddings using three initialization strategies
  3. Train contextual embeddings (from scratch and adapted)
  4. Extract word embeddings from contextual models
  5. Calculate cosine similarities across periods
  6. Compare performance using manual semantic change labels

- Design tradeoffs:
  - Training from scratch vs. adaptation: Scratch requires more resources but avoids modern language biases; adaptation is faster but may not capture historical patterns
  - Embedding size: Larger embeddings may capture more nuance but require more data; smaller embeddings are more efficient but may miss subtle changes
  - Training epochs: More epochs may improve quality but risk overfitting on small corpora

- Failure signatures:
  - Static embeddings show high correlation between unchanged words and semantic change labels (ρ close to 0)
  - Contextual embeddings show similar distributions for changed and unchanged words
  - All models show poor δµ values (small difference between changed and unchanged word groups)
  - Models trained from scratch perform worse than adapted models

- First 3 experiments:
  1. Compare cosine similarity distributions for changed vs unchanged words using a single static embedding method (e.g., Incremental) to establish baseline
  2. Train MLatin-BERT from scratch and compare its performance against the static baseline
  3. Test different embedding sizes (100 vs 300) for the Incremental static embedding method to optimize hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would larger contextual embedding models (beyond the 12-layer BERT used in this study) perform on the Medieval Latin charters dataset for semantic change detection?
- Basis in paper: [inferred] The paper notes that larger model sizes tend to perform better based on scaling laws, but resource constraints limited their exploration to only the standard BERT configuration.
- Why unresolved: The authors only tested a single BERT model size due to resource constraints and did not explore larger variants that might be more effective.
- What evidence would resolve it: Training and evaluating larger BERT models (e.g., 24-layer variants) on the same dataset and comparing their performance metrics (δµ and ρ) against the current model.

### Open Question 2
- Question: Would developing a continuous semantic change index (rather than binary labels) improve the evaluation of embedding models for historical semantic change detection?
- Basis in paper: [explicit] The authors explicitly state they were limited to binary labels due to resource constraints and suggest that a continuous index from zero to one would allow for more informative quantitative evaluations.
- Why unresolved: The study relied on binary labels created by three Latin specialists, which limits the granularity of evaluation and prevents more nuanced analysis of semantic change.
- What evidence would resolve it: Creating a continuous scale for semantic change through expert annotation and re-evaluating all embedding models using this richer label set to compare against binary results.

### Open Question 3
- Question: Are there alternative distance metrics beyond cosine similarity (such as APD or PRT mentioned in the paper) that would be more effective for modeling semantic change in historical corpora?
- Basis in paper: [explicit] The authors suggest in the conclusion that cosine similarity may not be the most appropriate measure and mention APD and PRT as potential alternatives from previous studies.
- Why unresolved: The study exclusively used cosine similarity to measure semantic change, despite acknowledging that this might not be optimal for the task.
- What evidence would resolve it: Implementing APD and PRT metrics on the same dataset and comparing their performance against cosine similarity across all embedding models using the same evaluation framework.

## Limitations

- Corpus size and representativeness: The 3M token corpus is relatively small for training contextual embeddings from scratch, potentially constraining model learning
- Manual labeling subjectivity: Semantic change labels from Latin specialists may introduce subjectivity and inter-annotator variability
- Historical period boundaries: The three-period division may not optimally capture semantic shifts, particularly around the Norman Conquest transition

## Confidence

- High confidence: The comparative performance advantage of contextual embeddings over static embeddings is well-supported by correlation metrics and mean difference scores
- Medium confidence: The superiority of training from scratch versus adaptation for historical language modeling is demonstrated but based on limited comparison
- Low confidence: The assumption that the Norman Conquest period shows more significant semantic change than the Plantagenet period is primarily theoretical and not fully validated by results

## Next Checks

1. Cross-validation with expanded labeling: Replicate the semantic change labeling process with additional Latin specialists and a broader set of words to assess inter-annotator agreement and establish confidence intervals for performance metrics.

2. Period boundary sensitivity analysis: Systematically vary the temporal boundaries between ANG, NOR, and PLA periods to determine whether the observed semantic change patterns are robust to different periodization schemes.

3. Modern language comparison: Apply the same static versus contextual embedding methodology to a modern language corpus with known semantic shifts to determine whether the observed performance differences are specific to historical language processing or represent a general pattern.