---
ver: rpa2
title: 'FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation
  from In-The-Wild Clothing Images'
arxiv_id: '2410.01801'
source_url: https://arxiv.org/abs/2410.01801
tags:
- texture
- image
- images
- garment
- material
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FabricDiffusion introduces a method to extract normalized, tileable
  fabric textures and prints from in-the-wild clothing images for application to 3D
  garments. Unlike prior approaches that synthesize textures directly on 3D meshes,
  FabricDiffusion treats the problem as generating distortion-free texture maps that
  can be mapped onto garment UV space, inspired by real-world garment construction.
---

# FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images

## Quick Facts
- arXiv ID: 2410.01801
- Source URL: https://arxiv.org/abs/2410.01801
- Authors: Cheng Zhang; Yuanhao Wang; Francisco Vicente Carrasco; Chenglei Wu; Jinlong Yang; Thabo Beeler; Fernando De la Torre
- Reference count: 16
- Primary result: Achieves FID of 12.44 and CLIP-s of 0.97 on texture transfer tasks, significantly outperforming state-of-the-art methods

## Executive Summary
FabricDiffusion introduces a novel method for extracting normalized, tileable fabric textures and prints from in-the-wild clothing images for application to 3D garments. Unlike prior approaches that synthesize textures directly on 3D meshes, FabricDiffusion treats the problem as generating distortion-free texture maps that can be mapped onto garment UV space, inspired by real-world garment construction. The method trains a diffusion model on a large synthetic dataset of paired distorted and flat rendered texture images, enabling correction of geometric distortions, lighting, and shadows. FabricDiffusion generates both fabric textures and transparent prints with alpha channels, and integrates with PBR material estimation for realistic relighting. Evaluated on both synthetic and real images, FabricDiffusion significantly outperforms state-of-the-art methods while preserving fine details and material properties.

## Method Summary
FabricDiffusion extracts high-fidelity fabric textures from in-the-wild clothing images by training a diffusion model on paired distorted and flat texture images. The method treats texture generation as a 2D image-to-image translation problem, predicting distortion-free textures that can be mapped to 3D garment UV space. A synthetic dataset of paired images is created by rendering garments with various textures and poses, then applying geometric distortions. The diffusion model learns to reverse these distortions, correcting for lighting, shadows, and geometric warping. The approach generates both fabric textures and transparent prints with alpha channels, and integrates with PBR material estimation for realistic relighting. The method outperforms state-of-the-art approaches on texture transfer tasks while preserving fine details and material properties.

## Key Results
- Achieves FID of 12.44 and CLIP-s of 0.97 on texture transfer tasks
- Significantly outperforms state-of-the-art methods in both quantitative metrics and visual quality
- Successfully generalizes to unseen textures and garment shapes while preserving fine details
- Generates both fabric textures and transparent prints with alpha channels for realistic relighting

## Why This Works (Mechanism)
FabricDiffusion succeeds by treating texture generation as a 2D image-to-image translation problem rather than direct 3D synthesis. By training on paired distorted and flat texture images, the diffusion model learns to correct geometric distortions, lighting, and shadows that occur when fabric is photographed in real-world conditions. The method leverages the garment UV space concept, treating texture maps as distortion-free representations that can be seamlessly mapped to 3D surfaces. This approach separates the complex 3D geometry from the texture generation problem, allowing the model to focus on correcting 2D image artifacts. The use of diffusion models enables high-fidelity texture generation with fine detail preservation, while the synthetic training data provides controlled pairs for learning the distortion-correction mapping.

## Foundational Learning
- **Diffusion Models**: Why needed - Generate high-fidelity textures with fine detail preservation. Quick check - Verify the model can denoise corrupted texture images effectively.
- **UV Space Mapping**: Why needed - Provides a distortion-free representation of fabric patterns that can be seamlessly applied to 3D surfaces. Quick check - Confirm generated textures map correctly to garment meshes without visible seams or distortions.
- **PBR Material Estimation**: Why needed - Enables realistic relighting of generated textures by estimating material properties like specularity and roughness. Quick check - Validate that materials respond realistically to different lighting conditions.
- **Paired Image Translation**: Why needed - Learn the mapping between distorted real-world images and ideal flat texture representations. Quick check - Ensure the model can consistently correct geometric distortions and lighting artifacts.
- **Synthetic Data Generation**: Why needed - Create controlled training pairs of distorted and undistorted textures for model learning. Quick check - Verify the synthetic dataset covers diverse garment types, poses, and texture patterns.

## Architecture Onboarding

**Component Map:**
Raw Image -> Garment Segmentation -> Texture Extraction -> Diffusion Model -> Corrected Texture -> PBR Material Estimation -> Final 3D Texture

**Critical Path:**
Raw Image → Garment Segmentation → Texture Extraction → Diffusion Model → Corrected Texture → PBR Material Estimation → Final 3D Texture

**Design Tradeoffs:**
The method trades computational complexity for higher fidelity by using diffusion models rather than simpler generative approaches. The synthetic data approach provides controlled training but may limit real-world robustness. Separating texture generation from 3D geometry simplifies the problem but requires accurate UV mapping.

**Failure Signatures:**
- Visible seams or discontinuities when tiling generated textures
- Persistent lighting artifacts or shadows in corrected textures
- Inaccurate material property estimation leading to unrealistic relighting
- Poor performance on highly reflective or complex fabric patterns

**First 3 Experiments:**
1. Test texture correction on a controlled set of synthetic distorted images with known ground truth
2. Evaluate generalization to completely unseen fabric patterns and garment types
3. Assess the quality of PBR material estimation by rendering textures under different lighting conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic training data may limit real-world robustness for complex fabric patterns and extreme lighting conditions
- Performance on highly irregular garment geometries and extreme poses remains unclear
- Assumes access to garment segmentation masks, limiting fully automatic applications
- Potential boundary artifacts when tiling generated texture outputs

## Confidence

**High Confidence:**
- Core methodology of using diffusion models for texture distortion correction
- Superior quantitative performance on standard benchmarks (FID, CLIP scores)

**Medium Confidence:**
- Claims of outperforming state-of-the-art methods based on quantitative metrics
- Generalization to unseen textures and garment shapes

**Low Confidence:**
- Real-world robustness across diverse, challenging conditions not extensively validated
- Performance on highly complex fabric patterns and extreme lighting scenarios

## Next Checks
1. Test robustness on challenging real-world images with complex fabric patterns (sequins, fur, metallics) and extreme lighting conditions
2. Evaluate performance on highly irregular garment geometries and extreme poses to assess boundary artifact issues
3. Conduct user study comparing visual quality and realism against state-of-the-art methods across diverse garment types and fabric materials