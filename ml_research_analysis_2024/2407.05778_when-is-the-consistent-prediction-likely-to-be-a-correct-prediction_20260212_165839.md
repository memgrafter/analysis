---
ver: rpa2
title: When is the consistent prediction likely to be a correct prediction?
arxiv_id: '2407.05778'
source_url: https://arxiv.org/abs/2407.05778
tags:
- reasoning
- answer
- consistent
- length
- zeroshot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study challenges the assumption that the most consistent answer\
  \ from large language models (LLMs) is most likely correct. It demonstrates that\
  \ consistent answers derived from longer reasoning texts\u2014those involving more\
  \ output tokens and computational effort\u2014are more likely to be accurate."
---

# When is the consistent prediction likely to be a correct prediction?

## Quick Facts
- arXiv ID: 2407.05778
- Source URL: https://arxiv.org/abs/2407.05778
- Authors: Alex Nguyen; Dheeraj Mekala; Chengyu Dong; Jingbo Shang
- Reference count: 17
- One-line primary result: Consistent answers from longer reasoning texts (not just the most frequent answer) are more likely to be correct.

## Executive Summary
This paper challenges the assumption that the most consistent answer from large language models is necessarily the most accurate. The authors demonstrate that answers derived from longer reasoning texts—which involve more computation and often contain chain-of-thought reasoning—are more likely to be correct than simply the most frequent answer across all outputs. By sampling multiple responses from Mixtral-8x7B and focusing on those exceeding a certain length threshold, they achieve 86% of the self-consistency performance obtained through zero-shot CoT prompting on GSM8K and MultiArith datasets.

## Method Summary
The authors sample multiple reasoning texts per question using Mixtral-8x7B with temperature=1.2 and top-k=40, then filter responses based on token length (≥60 for longer responses). They extract answers from each reasoning text and compute consistency across answers, selecting the most consistent answer above a minimum threshold. For self-consistency with length, they sample until the most frequent answer meets a minimum threshold (e.g., 12) and compare accuracy against baseline ZEROSHOT. Reasoning texts are classified as CoT or blurt using a few-shot prompt, and the authors analyze correctness patterns across different response types and lengths.

## Key Results
- Consistent answers from longer reasoning texts (≥60 tokens) achieve 86% of self-consistency performance with zero-shot CoT prompting
- LLMs autonomously generate chain-of-thought reasoning when producing longer responses, without custom prompts
- The probability of LLMs generating longer responses by default is quite low, necessitating decoding strategies conditioned on output length

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistent answers from longer reasoning texts are more likely to be correct.
- Mechanism: LLMs generate longer responses through more computation, which increases the probability of producing chain-of-thought (CoT) reasoning. CoT reasoning leads to more accurate solutions.
- Core assumption: CoT reasoning correlates with higher accuracy in LLM reasoning tasks.
- Evidence anchors:
  - [abstract] "consistent answers derived through more computation i.e. longer reasoning texts, rather than simply the most consistent answer across all outputs, are more likely to be correct."
  - [section 3] "we observe an increase in the percentage of CoTs detected in a manner similar to the performance trends observed in Figures 2 and 5."
  - [corpus] Weak: No direct corpus evidence for the CoT-accuracy correlation; relies on the paper's internal figures.
- Break condition: If longer responses are generated without producing CoT reasoning, the accuracy advantage may not hold.

### Mechanism 2
- Claim: LLMs can autonomously generate CoT reasoning without custom prompts when producing longer responses.
- Mechanism: During longer response generation, LLMs spontaneously produce step-by-step reasoning breakdowns, which are not elicited by specific prompt prefixes.
- Core assumption: The length of the response is correlated with the depth of reasoning.
- Evidence anchors:
  - [abstract] "we demonstrate that LLMs can autonomously produce chain-of-thought (CoT) style reasoning with no custom prompts merely while generating longer responses."
  - [section 3] "we observe the spontaneous appearance of CoTs without any specific prompts."
  - [corpus] Weak: No external corpus evidence provided; claim based on the paper's internal analysis.
- Break condition: If the model generates long responses that are repetitive or off-topic rather than CoT reasoning, the mechanism fails.

### Mechanism 3
- Claim: The probability of generating longer responses is low, necessitating decoding strategies conditioned on output length.
- Mechanism: LLMs have a bias toward shorter responses, often "blurting out" the answer in the initial tokens, making longer CoT responses rare without specific length constraints.
- Core assumption: The default generation behavior favors brevity over detailed reasoning.
- Evidence anchors:
  - [abstract] "we demonstrate that the probability of LLMs generating a longer response is quite low, highlighting the need for decoding strategies conditioned on output length."
  - [section 6] "we observe a substantial discrepancy between the likelihood of generating a shorter text and a longer text."
  - [corpus] No direct corpus evidence; based on the paper's likelihood analysis.
- Break condition: If decoding strategies effectively increase the probability of longer responses without sacrificing quality, the need for length conditioning may diminish.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT reasoning is central to understanding why longer responses lead to more accurate answers. It involves breaking down problems into step-by-step solutions.
  - Quick check question: Can you explain how CoT reasoning differs from direct answer generation in LLMs?

- Concept: Self-consistency method
  - Why needed here: Self-consistency involves sampling multiple responses and selecting the most frequent answer. Understanding this method is key to grasping the paper's critique and proposed refinement.
  - Quick check question: How does self-consistency improve over single-shot prompting, and what limitation does the paper identify?

- Concept: Decoding strategies (temperature, top-k sampling)
  - Why needed here: The paper uses specific decoding parameters to encourage diversity in responses. Knowledge of these strategies is essential for replicating or extending the experiments.
  - Quick check question: What effect does increasing the sampling temperature have on the diversity of generated responses?

## Architecture Onboarding

- Component map: Question -> Reasoning extraction -> Answer extraction -> Consistency evaluation -> Selected answer
- Critical path:
  1. Prompt the LLM with the question
  2. Generate multiple reasoning texts using sampling
  3. Filter responses based on length threshold
  4. Extract answers from each reasoning text
  5. Compute consistency across answers
  6. Select the most consistent answer above the minimum threshold

- Design tradeoffs:
  - Sampling more responses increases the chance of finding longer, more accurate CoT reasoning but raises computational cost.
  - Setting a high minimum consistency threshold improves reliability but may reduce coverage if not enough consistent answers are found.
  - Length thresholds filter out short, potentially less accurate responses but may discard valid concise answers.

- Failure signatures:
  - If the model rarely generates responses above the length threshold, the method may fail to improve accuracy.
  - If the model produces long but irrelevant or repetitive text, the consistency metric may be misleading.
  - If the minimum consistency threshold is too high, the method may not return any answer.

- First 3 experiments:
  1. Vary the length threshold and measure the impact on accuracy to find the optimal cutoff.
  2. Compare the performance of length-thresholded sampling against standard self-consistency on multiple datasets.
  3. Analyze the types of reasoning (CoT vs blurt vs noisy) generated at different response lengths to confirm the correlation with accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific decoding strategies could be developed to encourage LLMs to generate longer reasoning texts more frequently?
- Basis in paper: Inferred from the observation that LLMs rarely generate longer responses by default and the need for decoding strategies conditioned on output length.
- Why unresolved: The paper identifies the need for such strategies but does not propose specific methods to achieve this.
- What evidence would resolve it: Experimental results showing improved performance in generating longer reasoning texts using proposed decoding strategies.

### Open Question 2
- Question: How does the performance of self-consistency methods vary across different types of reasoning tasks (e.g., mathematical, logical, commonsense)?
- Basis in paper: Inferred from the paper's focus on mathematical reasoning tasks and the potential for generalization to other reasoning tasks.
- Why unresolved: The paper only evaluates the method on mathematical reasoning datasets and does not explore its effectiveness on other types of reasoning tasks.
- What evidence would resolve it: Comparative performance analysis of self-consistency methods across diverse reasoning tasks.

### Open Question 3
- Question: What is the relationship between the length of reasoning texts and the complexity of the questions being answered?
- Basis in paper: Inferred from the observation that longer reasoning texts are more likely to contain CoT-style reasoning and lead to more accurate answers.
- Why unresolved: The paper does not investigate how the complexity of questions influences the length and quality of reasoning texts.
- What evidence would resolve it: Analysis of the correlation between question complexity, reasoning text length, and answer accuracy.

### Open Question 4
- Question: How does the frequency of CoT-style reasoning in longer texts impact the overall performance of LLMs on reasoning tasks?
- Basis in paper: Inferred from the finding that longer reasoning texts are more likely to contain CoT-style reasoning and lead to more accurate answers.
- Why unresolved: The paper does not quantify the impact of CoT-style reasoning frequency on LLM performance.
- What evidence would resolve it: Statistical analysis of the relationship between CoT frequency, reasoning text length, and task performance.

### Open Question 5
- Question: What are the limitations of using self-consistency methods with minimum consistency thresholds in real-world applications?
- Basis in paper: Inferred from the paper's experimental setup and the potential for practical deployment challenges.
- Why unresolved: The paper focuses on controlled experimental settings and does not address real-world application limitations.
- What evidence would resolve it: Case studies or experiments evaluating the method's effectiveness in real-world scenarios with varying conditions.

## Limitations
- The paper's core claims rest on internal empirical observations rather than external validation
- The relationship between response length and CoT reasoning quality needs testing across diverse model architectures and domains
- The method's performance gains were achieved on specific arithmetic and language tasks with particular model configurations

## Confidence

**High Confidence**: The observation that LLMs can generate CoT reasoning without custom prompts during longer response generation is supported by direct textual analysis in the paper. The mechanism that sampling multiple responses and filtering by length can recover performance close to zero-shot CoT prompting is empirically demonstrated with clear metrics.

**Medium Confidence**: The claim that longer responses contain more computation leading to better reasoning is mechanistically plausible but requires more rigorous testing. The assertion that LLMs rarely generate longer responses by default is supported by likelihood analysis but may depend on specific decoding configurations and model architectures.

**Low Confidence**: The generalizability of the method to domains beyond arithmetic reasoning and to models outside the Mixtral-8x7B and Llama-2 70B family remains uncertain. The optimal length thresholds and minimum consistency requirements may be task-specific rather than universal.

## Next Checks

1. **Cross-model validation**: Test whether the length-based filtering approach achieves similar performance gains on open-source models like Llama-3, Mistral, and Qwen, and compare against proprietary models like GPT-4 and Claude.

2. **Domain transfer analysis**: Evaluate the method on non-arithmetic reasoning tasks including commonsense reasoning (StrategyQA), symbolic reasoning (Last Letter Concat), and scientific problem-solving.

3. **Length threshold sensitivity**: Systematically vary the minimum token length threshold across a wider range (30-200 tokens) and measure the accuracy trade-off curve.