---
ver: rpa2
title: Multiple Realizability and the Rise of Deep Learning
arxiv_id: '2405.13231'
source_url: https://arxiv.org/abs/2405.13231
tags:
- multiple
- cognitive
- https
- realizability
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines the philosophical implications of deep learning
  for the multiple realizability thesis in cognitive science. The authors argue against
  the common inference that multiple realizability implies methodological autonomy
  from implementation-level studies.
---

# Multiple Realizability and the Rise of Deep Learning

## Quick Facts
- arXiv ID: 2405.13231
- Source URL: https://arxiv.org/abs/2405.13231
- Reference count: 0
- Primary result: Deep learning challenges assumptions about methodological autonomy in cognitive science

## Executive Summary
This paper examines the philosophical implications of deep learning for the multiple realizability thesis in cognitive science. The authors argue against the common inference that multiple realizability implies methodological autonomy from implementation-level studies. Through analysis of key philosophical works, they demonstrate that while multiple realizability may entail ontological autonomy, it does not preclude the relevance of implementation-level details to higher-level cognitive inquiry. The paper concludes that deep neural networks, despite being implementation-level models, can provide crucial insights into cognitive processes and challenge traditional top-down approaches to cognitive science.

## Method Summary
The paper employs philosophical analysis and argumentation, examining the logic of key philosophical works (Fodor & Pylyshyn 1988; Bechtel & Mundale 1999) and their assumptions about the relationship between multiple realizability and methodological autonomy. Rather than computational or empirical studies, it uses philosophical reasoning to clarify the distinction between ontological and methodological autonomy, and explores how deep neural networks might inform cognitive theory despite being implementation-level models.

## Key Results
- Multiple realizability implies ontological autonomy but not methodological autonomy
- Deep neural networks can provide implementation-level insights that inform cognitive theory
- The "bitter lesson" of deep learning challenges top-down cognitive science approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The multiple realizability thesis implies ontological autonomy but not methodological autonomy
- **Mechanism**: Multiple realizability establishes that psychological states cannot be reduced to specific brain states due to one-to-many mapping between mental types and physical implementations. This creates an autonomous domain of psychological entities that exist independently of particular physical substrates.
- **Core assumption**: Psychological constructs have explanatory value that transcends their physical implementations
- **Evidence anchors**:
  - [abstract] "multiple realizability implies ontological autonomy, but it does not imply that lower-level details are irrelevant to higher-level inquiry"
  - [section] "Multiple realizability thereby provides a direct argument for the irreducibility of psychological constructs and, in so doing, secures an autonomous domain of entities for cognitive scientists to study"
  - [corpus] Weak evidence - no corpus papers directly address this philosophical distinction

### Mechanism 2
- **Claim**: Deep neural networks can provide implementation-level insights that inform cognitive theory
- **Mechanism**: DNNs serve as artificial implementations of cognitive capacities that can be reverse-engineered to reveal functional structures. By interpreting these networks, researchers can identify psychological constructs and refine theoretical understanding of how these constructs operate.
- **Core assumption**: Implementation-level models can be meaningfully interpreted to extract functional characterizations
- **Evidence anchors**:
  - [abstract] "deep neural networks may play a crucial role in formulating and evaluating hypotheses about cognition, even if they are interpreted as implementation-level models"
  - [section] "Neural network models can help us to address each of these questions...advances in this area are already underway"
  - [corpus] Weak evidence - corpus papers focus on technical applications rather than philosophical implications

### Mechanism 3
- **Claim**: The "bitter lesson" of deep learning challenges top-down cognitive science approaches
- **Mechanism**: Scaling models through increased computational power and data access often yields better results than attempts to encode human cognitive theories directly. This suggests that traditional top-down approaches may be too brittle or idealized to capture complex cognitive functions.
- **Core assumption**: The most effective solutions to cognitive problems may not align with human intuitions about cognitive architecture
- **Evidence anchors**:
  - [abstract] "the 'bitter lesson' of deep learning suggests that scaling models may be more effective than attempting to encode human cognitive theories directly"
  - [section] "The history of AI suggests that progress towards engineering alternate realizations of psychological capacities depends not on thinking up clever theories that human modelers might try to implement, but on simply scaling the models up"
  - [corpus] Weak evidence - corpus papers do not discuss this methodological implication

## Foundational Learning

- **Concept**: Distinction between ontological and methodological autonomy
  - Why needed here: This distinction is central to the paper's argument about what multiple realizability actually entails for cognitive science methodology
  - Quick check question: Can a domain of entities be ontologically autonomous (existing independently) while still being empirically informed by study of their physical implementations?

- **Concept**: Implementation-level interpretation of neural networks
  - Why needed here: The paper argues that DNNs can be informative implementations even if they don't directly encode human cognitive theories
  - Quick check question: What does it mean for a neural network to be an "informative implementation" versus a "mere implementation" of cognitive processes?

- **Concept**: The "bitter lesson" in AI development
  - Why needed here: This concept challenges traditional approaches to cognitive science and suggests a new methodological paradigm
  - Quick check question: How does the success of scaling approaches in AI relate to the traditional goal of cognitive science to understand human-like intelligence?

## Architecture Onboarding

- **Component map**: Philosophical analysis of multiple realizability -> Analysis of its implications -> Examination of DNN capabilities -> Derivation of methodological conclusions
- **Critical path**: From establishing multiple realizability → analyzing its implications → examining DNN capabilities → deriving methodological conclusions
- **Design tradeoffs**: Balancing philosophical rigor with empirical relevance; maintaining neutrality between competing cognitive theories while demonstrating DNN utility
- **Failure signatures**: Overclaiming implications of multiple realizability; underestimating interpretability challenges for DNNs; dismissing either philosophical or empirical contributions
- **First 3 experiments**:
  1. Analyze a specific DNN implementation to identify whether it implements known cognitive constructs or reveals new ones
  2. Compare performance of scaled DNNs versus theory-driven models on a cognitive task
  3. Design an experiment testing whether neuroscience insights improve DNN interpretation for cognitive modeling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can neural network models definitively prove or disprove the multiple realizability thesis, or are they merely suggestive evidence?
- Basis in paper: [explicit] The paper discusses how DNNs provide "the most plausible examples to date of alternate realizations of human psychological capacities" but acknowledges this requires "a more detailed technical discussion, as well as further engagement with subtle philosophical issues"
- Why unresolved: The relationship between artificial implementations and philosophical claims about multiple realizability remains debated. The paper notes that conclusively demonstrating DNNs as alternative realizations "will require a more detailed technical discussion" and philosophical engagement.
- What evidence would resolve it: Systematic demonstrations of DNNs implementing specific cognitive capacities that map to known psychological functions, combined with philosophical analysis of whether these constitute genuine multiple realizations rather than mere simulations.

### Open Question 2
- Question: Does the "bitter lesson" of deep learning fundamentally challenge traditional top-down cognitive science approaches, or can these approaches coexist with implementation-level models?
- Basis in paper: [explicit] The authors discuss Richard Sutton's "bitter lesson" that "building in how we think we think does not work" and ask whether "the success of deep learning may ultimately present a stronger challenge than has yet been discussed"
- Why unresolved: The paper acknowledges this as a "more ambitious thesis" that requires further investigation. It remains unclear whether idealizations in traditional theories are "necessary or useful from an explanatory perspective" or whether bottom-up approaches will completely supplant top-down methods.
- What evidence would resolve it: Comparative studies showing whether top-down approaches continue to generate novel insights or whether implementation-level models consistently outperform them in predicting cognitive phenomena.

### Open Question 3
- Question: How should we distinguish between "mere implementation" and "informative implementation" of cognitive functions in neural networks?
- Basis in paper: [explicit] The authors critique Fodor and Pylyshyn's "mere implementation" objection and argue that "not all implementation is 'mere'" but acknowledge the need for a framework to evaluate when implementation provides genuine insights
- Why unresolved: The paper identifies the problem of distinguishing these types of implementation but doesn't provide a complete resolution. The authors note that psychological constructs are often "distinctly underspecified" leaving "room for a successful 'implementation' to diverge in illuminating ways"
- What evidence would resolve it: A principled framework for evaluating when neural network implementations provide genuine theoretical insights versus when they simply replicate functions without illuminating underlying mechanisms.

## Limitations

- The paper assumes familiarity with philosophical literature on multiple realizability and functionalism
- Technical details of referenced empirical studies are not fully elaborated
- The relationship between artificial implementations and philosophical claims about multiple realizability remains debated

## Confidence

- **High confidence**: Distinction between ontological and methodological autonomy
- **Medium confidence**: DNNs can inform cognitive theory through reverse-engineering
- **Low confidence**: Generality of the "bitter lesson" across all cognitive domains

## Next Checks

1. **Implementation-level interpretation**: Select a pre-trained DNN and attempt to identify specific cognitive constructs it implements, documenting the interpretation process and its challenges.

2. **Scaling versus theory comparison**: Conduct a controlled experiment comparing performance of scaled DNNs against theory-driven models on a well-defined cognitive task, measuring both accuracy and interpretability.

3. **Neuroscience-DNN synergy**: Design an experiment testing whether neuroscience-informed constraints improve DNN performance or interpretability for cognitive modeling tasks.