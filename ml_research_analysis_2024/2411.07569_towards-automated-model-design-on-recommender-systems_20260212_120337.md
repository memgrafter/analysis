---
ver: rpa2
title: Towards Automated Model Design on Recommender Systems
arxiv_id: '2411.07569'
source_url: https://arxiv.org/abs/2411.07569
tags:
- search
- dense
- sparse
- space
- recommender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing efficient deep
  learning models for recommender systems, which involve heterogeneous data modalities
  and complex feature interactions. The authors propose a novel paradigm called NASRec
  that utilizes weight-sharing neural architecture search (WS-NAS) to explore abundant
  solution spaces for both model architecture and hardware co-design.
---

# Towards Automated Model Design on Recommender Systems

## Quick Facts
- arXiv ID: 2411.07569
- Source URL: https://arxiv.org/abs/2411.07569
- Reference count: 40
- Primary result: NASRec achieves 2x FLOPs efficiency, 1.8x energy efficiency, and 1.5x performance improvements in recommender models

## Executive Summary
This paper addresses the challenge of designing efficient deep learning models for recommender systems, which involve heterogeneous data modalities and complex feature interactions. The authors propose a novel paradigm called NASRec that utilizes weight-sharing neural architecture search (WS-NAS) to explore abundant solution spaces for both model architecture and hardware co-design. NASRec constructs a large supernet incorporating diverse building operators, dense connectivity, and dimension search options, while also considering Processing-In-Memory (PIM) configurations for hardware efficiency. Experimental results on three Click-Through Rate (CTR) prediction benchmarks demonstrate that NASRec-crafted models outperform both manually designed and AutoML-crafted models, achieving state-of-the-art performance.

## Method Summary
NASRec employs weight-sharing neural architecture search to design efficient deep learning models for recommender systems. The method constructs a supernet containing choice blocks with various operators (FC, Gating, Sum, Dot-Product, EFC, Attention), dense connectivity patterns, and dimension search capabilities. To handle the scale and heterogeneity of the search space, NASRec introduces Single-operator Any-connection sampling, operator-balancing interaction modules, and post-training fine-tuning techniques. The search process involves supernet training with specialized sampling strategies, followed by regularized evolution search to identify optimal subnetworks, and concludes with fine-tuning to optimize final model performance.

## Key Results
- NASRec-crafted models outperform manually designed and AutoML-crafted models on three CTR prediction benchmarks
- From a co-design perspective, NASRec achieves 2x FLOPs efficiency, 1.8x energy efficiency, and 1.5x performance improvements
- The framework demonstrates effective handling of heterogeneous data modalities and complex feature interactions in recommender systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight-sharing neural architecture search (WS-NAS) with single-operator any-connection sampling enables efficient exploration of large, heterogeneous search spaces in recommender systems.
- Mechanism: By sampling one operator per block while allowing any connection pattern, the supernet trains efficiently and covers diverse model architectures without co-adaptation between operators.
- Core assumption: Separating operator sampling from connection sampling maintains training stability while enabling broad architectural coverage.
- Evidence anchors:
  - [abstract] "Our solution space's scale, heterogeneity, and complexity pose several challenges, which we address by proposing various techniques for training and evaluating the supernet."
  - [section] "We propose Single-operator Any-connection sampling to decouple operator selections and increase connection coverage"
- Break condition: If operator co-adaptation still occurs despite single-operator sampling, or if connection coverage becomes too sparse to represent important architectural motifs.

### Mechanism 2
- Claim: Operator-balancing interaction modules address the over-parameterization problem in Dot-Product operations, enabling fair training across heterogeneous operators.
- Mechanism: By projecting sparse inputs to √2·dimd dimensions before Dot-Product, the number of parameters grows linearly rather than quadratically with the number of sparse features, balancing convergence rates across operators.
- Core assumption: Quadratic parameter growth in Dot-Product operations creates convergence rate disparities that can be mitigated through dimensionality reduction.
- Evidence anchors:
  - [section] "We insert a simple EFC as a projection layer before the Dot-Product to mitigate such over-parameterization demonstrated in Figure 4."
  - [section] "As a result, the over-parameterization in Dot-Product leads to an increased convergence rate for the Dot-Product operator"
- Break condition: If the linear projection degrades Dot-Product effectiveness or if other operators develop similar imbalance issues.

### Mechanism 3
- Claim: Post-training fine-tuning re-calibrates subnet weights corrupted during supernet training, improving ranking accuracy and final model performance.
- Mechanism: Fine-tuning only the last fully connected layer on the target dataset for a few training steps re-adapts weights to their specific subnet configuration after supernet training.
- Core assumption: Weights become corrupted during supernet training due to co-adaptation, but can be partially recovered through targeted fine-tuning.
- Evidence anchors:
  - [section] "To address this issue, we propose a post-training fine-tuning technique that re-adapts the weights of each standalone subnet back to its specific configuration after supernet training."
  - [section] "This novel post-training fine-tuning technique comes with only marginal additional search cost and significantly boosts the ranking of subnets"
- Break condition: If fine-tuning becomes computationally prohibitive or if it introduces new overfitting issues on small datasets.

## Foundational Learning

- Concept: Weight Sharing Neural Architecture Search (WS-NAS)
  - Why needed here: Traditional NAS trains each candidate model separately, which is computationally prohibitive for the large, heterogeneous search space in recommender systems.
  - Quick check question: How does weight sharing enable training of billions of potential architectures in a single forward/backward pass?

- Concept: Multi-modality Feature Representation
  - Why needed here: Recommender systems must handle both dense numerical features and sparse categorical embeddings, requiring specialized operators and connectivity patterns.
  - Quick check question: What distinguishes dense inputs (R^B×dimd) from sparse inputs (R^B×Ns×dims) in terms of tensor dimensions and processing requirements?

- Concept: Hardware-Aware Co-Design
  - Why needed here: Processing-In-Memory (PIM) architectures have different computational characteristics than traditional hardware, requiring architectural modifications for optimal performance.
  - Quick check question: How does quantization search interact with ReRAM-based PIM hardware to optimize both accuracy and energy efficiency?

## Architecture Onboarding

- Component map: Supernet -> Choice blocks (FC, Gating, Sum, Dot-Product, EFC, Attention) -> Dense connectivity -> Dimension search -> Quantization options -> Post-training fine-tuning
- Critical path: Supernet training → Subnet evaluation → Regularized evolution search → Fine-tuning → Model deployment
- Design tradeoffs: Larger search spaces provide better performance but require more sophisticated training techniques and longer search times
- Failure signatures: Poor ranking correlation indicates supernet training issues, while convergence failures suggest sampling strategy problems
- First 3 experiments:
  1. Train NASRec-Small supernet on Criteo with Single-operator Any-connection sampling and measure Kendall's tau ranking correlation against ground truth
  2. Compare operator-balancing interaction vs standard Dot-Product on training efficiency and ranking quality in NASRec-Full
  3. Evaluate post-training fine-tuning impact on subnet performance across different sampling strategies using Pearson's correlation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can NASRec be extended to handle more diverse and complex data modalities beyond the current dense and sparse inputs?
- Basis in paper: [explicit] The paper mentions that NASRec addresses challenges related to data multi-modality and heterogeneity in the recommendation domain.
- Why unresolved: The paper focuses on dense and sparse inputs, but does not explore other potential data modalities that could be present in real-world recommender systems.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of NASRec on datasets with additional data modalities, such as sequential data or graph-structured data.

### Open Question 2
- Question: How can NASRec be adapted to handle dynamic and evolving recommendation scenarios, where the underlying data distribution changes over time?
- Basis in paper: [inferred] The paper does not explicitly address the issue of dynamic data distributions, which is a common challenge in real-world recommender systems.
- Why unresolved: The current NASRec framework assumes a static data distribution and does not provide mechanisms for adapting to changes over time.
- What evidence would resolve it: Experimental results showing the performance of NASRec on datasets with temporal shifts or concept drift, and demonstrating the ability of the framework to adapt to these changes.

### Open Question 3
- Question: How can NASRec be scaled to handle extremely large-scale recommender systems with billions of users and items?
- Basis in paper: [inferred] The paper does not explicitly discuss the scalability of NASRec to handle large-scale recommender systems.
- Why unresolved: The current NASRec framework may face computational and memory constraints when applied to extremely large-scale systems.
- What evidence would resolve it: Experimental results demonstrating the performance and efficiency of NASRec on large-scale recommender systems, and providing insights into potential optimizations and trade-offs.

## Limitations

- Search space representativeness may be limited by architectural priors and existing design patterns
- Hardware co-design validation relies on ReRAM hardware simulation rather than empirical measurements on actual PIM hardware
- Generalization beyond CTR prediction tasks remains unproven and untested

## Confidence

**High Confidence**: The core weight-sharing neural architecture search framework and basic training methodology are well-established. The post-training fine-tuning technique is conceptually sound and likely to provide consistent improvements.

**Medium Confidence**: The Single-operator Any-connection sampling approach and operator-balancing interaction modules show promise but may have dataset-specific effectiveness. The empirical validation across three datasets provides some generalizability, though larger-scale studies would strengthen confidence.

**Low Confidence**: The specific hardware co-design benefits (2x FLOPs efficiency, 1.8x energy efficiency, 1.5x performance improvements) are based on simulations rather than measurements on actual PIM hardware, making real-world performance uncertain.

## Next Checks

1. **Cross-Dataset Transferability**: Evaluate whether NASRec models trained on one dataset (e.g., Criteo) maintain performance when transferred to datasets with different characteristics (different feature distributions, different domain), testing the robustness of the search process.

2. **Hardware Simulation Validation**: Implement the NASRec quantization and operator choices on actual ReRAM-based PIM hardware (or the closest available analog) to validate that simulated performance gains translate to real hardware, particularly measuring energy consumption and latency.

3. **Architectural Ablation Study**: Systematically remove or modify key architectural priors (dense connectivity, specific operator types) to determine whether NASRec's performance gains come from the search algorithm itself or from the well-designed initial search space, separating algorithmic contribution from architectural bias.