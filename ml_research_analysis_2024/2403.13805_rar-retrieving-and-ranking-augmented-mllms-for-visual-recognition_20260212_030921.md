---
ver: rpa2
title: 'RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition'
arxiv_id: '2403.13805'
source_url: https://arxiv.org/abs/2403.13805
tags:
- image
- mllms
- datasets
- clip
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAR improves few-shot/zero-shot visual recognition by augmenting
  MLLMs with a retrieval-and-ranking pipeline. It uses CLIP to build a memory of image-text
  embeddings, retrieves top-k candidates, and ranks them with MLLMs for final predictions.
---

# RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition

## Quick Facts
- arXiv ID: 2403.13805
- Source URL: https://arxiv.org/abs/2403.13805
- Authors: Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang
- Reference count: 40
- One-line primary result: RAR improves few-shot/zero-shot visual recognition by augmenting MLLMs with a retrieval-and-ranking pipeline, achieving +6.2% average 4-shot accuracy and +6.4% AP on LVIS.

## Executive Summary
RAR addresses the limitations of both CLIP and MLLMs in visual recognition by combining their strengths through a retrieval-and-ranking pipeline. The method builds a memory of image-text embeddings using CLIP, retrieves top-k candidates for each input image, and uses MLLMs to rank these candidates for final predictions. This approach significantly improves performance on fine-grained benchmarks (+9.9% accuracy on average), few-shot datasets (+6.2% average), and object detection tasks (+6.4% AP on LVIS). The key innovation lies in addressing MLLM context window limitations by reducing the candidate space through efficient retrieval.

## Method Summary
RAR constructs a multimodal retriever using CLIP to create and store explicit memory of image-text embeddings for different categories. During inference, it retrieves the top-k most similar results from this memory using HNSW indexing and employs MLLMs to rank these candidates for final predictions. The method can use either fine-tuned MLLMs or in-context learning for ranking. For object detection tasks, RAR applies preprocessing (cropping and blurring) to handle the detection scenario. The approach effectively combines CLIP's broad recall capability with MLLM's fine-grained semantic understanding, addressing the context window limitations of MLLMs when dealing with large vocabularies.

## Key Results
- Achieves 9.9% average improvement on 5 fine-grained visual recognition benchmarks
- Improves few-shot classification by 6.2% average accuracy across 11 datasets (4-shot setting)
- Increases LVIS detection AP by 6.4% and V3Det mAP by 1.5% with 13,204 categories
- Outperforms both CLIP+KNN baseline and MLLM fine-tuning alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's broad associations hinder fine-grained recognition, but MLLMs excel at fine-grained categories when context is limited.
- Mechanism: CLIP provides high recall via broad similarity search; MLLMs provide high precision by ranking the top-k candidates.
- Core assumption: The top-k candidates retrieved by CLIP contain the correct answer with high probability.
- Evidence anchors: [abstract] states CLIP's focus on broad associations hinders precision in distinguishing subtle differences; [section] describes using CLIP for retrieval then MLLMs for ranking.

### Mechanism 2
- Claim: MLLMs struggle with large vocabularies due to limited context window, but RAR overcomes this by reducing the candidate set.
- Mechanism: Retrieval narrows the vast category space to a manageable subset, allowing MLLMs to rank effectively within their context limits.
- Core assumption: The context window limitation is the primary bottleneck for MLLMs on large-vocabulary tasks.
- Evidence anchors: [abstract] notes MLLM performance declines with category numbers due to limited context window size; [section] describes RAR preserving comprehensive knowledge while addressing limitations.

### Mechanism 3
- Claim: Fine-tuning MLLMs on ranking tasks significantly improves their ability to select correct categories from retrieved candidates.
- Mechanism: Fine-tuning on ranking format data teaches MLLMs to follow prompt format and make selections within the given candidate list.
- Core assumption: MLLMs can learn ranking behavior through supervised fine-tuning on a relatively small dataset.
- Evidence anchors: [section] explores fine-tuning with ranking format data to bolster MLLMs' ranking performance; [section] states fine-tuning's goal was to enable MLLMs to improve ranking ability.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Understanding how CLIP learns to align image and text embeddings through contrastive loss.
  - Quick check question: What is the main difference between contrastive learning and supervised learning in the context of CLIP?

- Concept: Multimodal embeddings
  - Why needed here: Understanding how CLIP projects images and text into a shared embedding space for comparison.
  - Quick check question: How does CLIP ensure that similar images and text descriptions are close in the embedding space?

- Concept: k-NN retrieval
  - Why needed here: Understanding how the retrieval module finds the most similar categories to an input image.
  - Quick check question: What is the computational complexity of brute-force k-NN search versus approximate methods like HNSW?

## Architecture Onboarding

- Component map: Input image → CLIP embedding → HNSW retrieval → Top-k categories → MLLM ranking prompt → Final prediction

- Critical path: Input image → CLIP embedding → HNSW retrieval → Top-k categories → MLLM ranking prompt → Final prediction

- Design tradeoffs:
  - k value vs. retrieval accuracy vs. MLLM ranking load
  - Fine-tuning vs. in-context learning for MLLM ranking capability
  - Memory size vs. retrieval speed vs. storage requirements
  - Crop scale and blurring parameters vs. detection accuracy

- Failure signatures:
  - Low top-1 accuracy but high top-k accuracy suggests retrieval is good but ranking is poor
  - Consistent misclassification on rare categories suggests memory lacks sufficient examples
  - Performance drops on large-vocabulary datasets suggests k is too small or MLLM ranking is insufficient

- First 3 experiments:
  1. Test baseline CLIP+KNN on a small fine-grained dataset to establish baseline performance
  2. Implement RAR with k=5 and test on same dataset to verify ranking improvement
  3. Vary k from 3 to 7 to find optimal trade-off between retrieval coverage and MLLM ranking load

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RAR performance scale with increasingly large vocabulary sizes beyond V3Det's 13,204 categories?
- Basis in paper: [explicit] The paper mentions RAR achieved +1.5% APall on V3Det with 13,204 categories and notes this was "particularly significant given the complexity" of this dataset, suggesting potential scaling challenges.
- Why unresolved: The paper only tests up to 13,204 categories and doesn't explore performance on vocabularies an order of magnitude larger, which would be relevant for real-world applications with millions of classes.
- What evidence would resolve it: Testing RAR on synthetic or real datasets with 100K+ categories while measuring accuracy degradation, retrieval time, and ranking effectiveness.

### Open Question 2
- Question: What is the precise mechanism by which MLLM ranking improves over CLIP's initial retrieval results?
- Basis in paper: [inferred] The paper claims MLLMs excel at "linguistic and semantic analysis to assess the contextual appropriateness of each class name" but doesn't provide quantitative evidence showing what specific types of errors MLLMs correct versus CLIP.
- Why unresolved: The ablation studies focus on hyperparameters (k values, fine-tuning datasets) rather than analyzing what error types each component fixes.
- What evidence would resolve it: A detailed error analysis categorizing prediction errors into types (e.g., synonym confusion, fine-grained distinctions, context mismatches) and showing which component corrects each type.

### Open Question 3
- Question: How does RAR performance compare to training a specialized model on the target dataset when limited training data is available?
- Basis in paper: [explicit] The paper emphasizes RAR's few-shot capabilities but only compares against CLIP+KNN and MLLM fine-tuning, not against traditional few-shot learning methods like meta-learning or metric learning approaches.
- Why unresolved: The paper establishes RAR outperforms baselines but doesn't position it within the broader few-shot learning literature or quantify the trade-off between its retrieval+ranking approach versus training with limited data.
- What evidence would resolve it: Head-to-head comparisons of RAR against state-of-the-art few-shot methods (e.g., ProtoNets, RelationNets, or modern few-shot transformers) on the same datasets with equivalent data budgets.

## Limitations

- The paper's claims about MLLM context window limitations are plausible but not empirically validated through systematic ablation studies.
- The fine-tuning procedure is described briefly without hyperparameter details, making it difficult to assess whether improvements come from fine-tuning quality or the retrieval mechanism itself.
- The comparison with GPT4V and Gemini1.5 is based on only 4-shot ImageNet results, which is insufficient to establish general superiority.

## Confidence

**High Confidence**: The retrieval mechanism (CLIP + HNSW) is well-established and the reported improvements on multiple datasets (6.2% average on few-shot, 6.4% AP on LVIS) are statistically significant and reproducible with standard implementations.

**Medium Confidence**: The claim that RAR specifically addresses MLLM context window limitations is supported by ablation studies showing performance drops with larger k values, but the causal mechanism isn't fully proven - it could be that ranking improves with more candidates regardless of context constraints.

**Low Confidence**: The assertion that fine-tuning significantly improves MLLM ranking performance lacks sufficient evidence - the paper mentions exploring fine-tuning but doesn't provide comprehensive comparisons between fine-tuned and in-context learning approaches.

## Next Checks

1. **Context Window Ablation**: Systematically vary k from 3 to 15 on a representative dataset to determine the exact point where MLLM context limitations begin affecting performance, validating the core hypothesis about context window constraints.

2. **Fine-tuning vs In-context Learning**: Implement both fine-tuned and in-context learning versions of RAR on the same datasets to quantify the actual contribution of fine-tuning versus the retrieval mechanism itself.

3. **Memory Size Scaling**: Test RAR performance with progressively larger category memories (10k, 50k, 100k categories) to verify whether the retrieval precision advantage over CLIP holds as vocabulary size increases, addressing the claim about handling large vocabularies.