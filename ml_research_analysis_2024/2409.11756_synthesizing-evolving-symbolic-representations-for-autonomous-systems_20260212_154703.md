---
ver: rpa2
title: Synthesizing Evolving Symbolic Representations for Autonomous Systems
arxiv_id: '2409.11756'
source_url: https://arxiv.org/abs/2409.11756
tags:
- agent
- learning
- planning
- knowledge
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an autonomous learning system that combines
  low-level and high-level representations to synthesize evolving symbolic knowledge.
  The system discovers options (temporally extended actions) from primitive actions
  using intrinsic motivation, explores the environment using these options, abstracts
  the collected experience into a probabilistic planning domain definition language
  (PPDDL) representation, and plans to reach goals.
---

# Synthesizing Evolving Symbolic Representations for Autonomous Systems

## Quick Facts
- arXiv ID: 2409.11756
- Source URL: https://arxiv.org/abs/2409.11756
- Reference count: 40
- Primary result: Autonomous system synthesizes evolving symbolic knowledge through option discovery, curiosity-driven exploration, and PPDDL abstraction

## Executive Summary
This paper presents an autonomous learning system that combines low-level and high-level representations to synthesize evolving symbolic knowledge. The system discovers options (temporally extended actions) from primitive actions using intrinsic motivation, explores the environment using these options, abstracts the collected experience into a probabilistic planning domain definition language (PPDDL) representation, and plans to reach goals. Tested in a treasure game environment with increasing complexity, the system demonstrates that planning-driven exploration outperforms random exploration in larger domains, successfully generating valid high-level models and plans to solve tasks.

## Method Summary
The system employs an iterative pipeline combining option discovery, exploration, abstraction, and planning. Option discovery uses surprise-based intrinsic motivation to identify new policies from primitive actions. The exploration phase executes these options and collects initiation and transition data tuples. An abstraction procedure converts raw sensor data into PPDDL representation by partitioning options, estimating preconditions and effects using classifiers, and synthesizing the domain. Planning at the high level enables efficient task solving by chaining abstract options without low-level trial-and-error. The system uses curiosity-driven exploration to target less-visited states, improving knowledge acquisition efficiency.

## Key Results
- Planning-driven exploration outperforms random exploration in larger domains with higher success rates
- System successfully generates valid high-level PPDDL models and plans to solve tasks
- Planning strategies show particular effectiveness in complex environments with increasing domain sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system achieves autonomous symbolic knowledge synthesis by iteratively discovering options, collecting transition data, and abstracting into PPDDL.
- Mechanism: Option discovery driven by surprise-based intrinsic motivation identifies new policies. Exploration collects initiation and transition data tuples. Abstraction clusters options, learns preconditions/effects, and synthesizes PPDDL operators.
- Core assumption: The abstraction algorithm (Konidaris et al. [59]) reliably converts raw sensor data into valid PPDDL domains.
- Evidence anchors:
  - [abstract]: "The abstraction procedure converts raw sensor data into a probabilistic planning domain definition language (PPDDL) representation by partitioning options, estimating preconditions and effects, and synthesizing the domain."
  - [section]: "The abstraction procedure executes the following five steps: (1) Options partition, (2) Precondition estimation, (3) Effect estimation, (4) PPDDL Domain synthesis."
  - [corpus]: Weak evidence - no direct mentions of PPDDL or similar abstraction pipelines in neighbor titles/abstracts.

### Mechanism 2
- Claim: Curiosity-driven exploration at the high level improves knowledge acquisition efficiency by targeting less-visited states.
- Mechanism: Goal Selector implements Goal Babbling and Distance-based Goal Babbling to choose target states at the frontier of visited states. This biases exploration toward novel configurations, increasing symbolic diversity.
- Core assumption: Novel states are more likely to reveal new options and symbolic concepts.
- Evidence anchors:
  - [abstract]: "The system employs two types of intrinsic motivation: surprise-based motivation for option discovery and curiosity-based motivation for exploration."
  - [section]: "Distance-based Goal Babbling... models the curiosity towards the less explored states as being influenced by the goal's distance from its starting location."
  - [corpus]: No direct evidence of curiosity-driven exploration in neighbors; weak indirect support from "self-evolving" themes.

### Mechanism 3
- Claim: Planning at high level enables efficient task solving by chaining abstract options without low-level trial-and-error.
- Mechanism: PPDDL domain encodes causal relationships between options. Planner searches for action sequences to reach symbolic goals. This avoids repeated primitive-level exploration.
- Core assumption: The PPDDL representation is sufficiently expressive to capture the true causal dependencies in the environment.
- Evidence anchors:
  - [abstract]: "The system discovers options... explores the environment using these options, abstracts the collected experience into a PPDDL representation... and plans to reach goals."
  - [section]: "Using symbols Σ and high-level actions A as building blocks, it is possible to define the model of environment D... Planning is the field of research studying formal methods to automatically find solutions, also called plans, to tasks requiring a sequence of actions."

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and semi-MDPs (SMDPs)
  - Why needed here: Options extend primitive actions; understanding the SMDP formalism is essential for grasping how temporally extended actions are modeled.
  - Quick check question: What is the key difference between an MDP and an SMDP in terms of action representation?

- Concept: Classical Planning and PDDL
  - Why needed here: PPDDL is built on planning domain concepts (symbols, operators, preconditions/effects). Knowledge of planning theory is required to understand abstraction and planning phases.
  - Quick check question: How do preconditions and effects in a PDDL operator relate to the initiation set and termination condition of an option?

- Concept: Intrinsic Motivation (IM) in RL
  - Why needed here: Two IM types (surprise, curiosity) drive option discovery and exploration. Understanding IM frameworks is necessary to grasp agent behavior.
  - Quick check question: What distinguishes knowledge-based IM from competence-based IM in the context of option discovery vs. exploration?

## Architecture Onboarding

- Component map: Option Discovery -> Exploration -> Abstraction -> Planning -> Goal Selector -> Option Discovery

- Critical path:
  1. Option Discovery → 2. Exploration → 3. Abstraction → 4. Planning → 5. Goal Selector → back to Option Discovery
  Any failure in abstraction or planning blocks the cycle.

- Design tradeoffs:
  - Abstraction quality vs. data volume: more data yields better PPDDL but increases computation
  - Exploration strategy: random vs. curiosity-driven impacts learning speed and completeness
  - Symbolic expressiveness vs. generalization: richer symbols improve planning but may overfit

- Failure signatures:
  - Option discovery fails: no new options after many cycles → check surprise signal implementation
  - Abstraction produces noisy symbols: transition data insufficient or clustering unstable → increase data or adjust clustering parameters
  - Planning fails: PPDDL domain incomplete → inspect precondition/effect estimation

- First 3 experiments:
  1. Run the system on domain1 with only Action Babbling; verify PPDDL domain generation and plan success rate
  2. Switch to Goal Babbling on domain2; compare success rates and symbolic coverage
  3. Use Distance-based Goal Babbling on domain5; measure exploration efficiency and final plan success

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the abstraction procedure be made truly incremental rather than requiring recomputation from scratch each cycle?
- Basis in paper: [explicit] The paper states "the abstraction module has no real incremental nature, but the abstraction procedure is executed over all the data, each cycle from scratch" and this is identified as a key limitation.
- Why unresolved: Current implementation is computationally inefficient and doesn't reflect human learning processes.
- What evidence would resolve it: A demonstration of a system that incrementally updates the PPDDL representation as new data arrives, with computational complexity analysis showing improvement over the batch approach.

### Open Question 2
- Question: Can the symbolic representation be extended beyond propositional logic to support first-order logic with arguments and predicates?
- Basis in paper: [explicit] The paper explicitly states "the system proposed is limited by the propositional logic representation, which does not permit the inclusion of arguments in operators and symbols" and suggests this as a future direction.
- Why unresolved: Current propositional representation limits expressiveness and transfer capabilities.
- What evidence would resolve it: Implementation of an abstraction procedure using first-order logic that successfully generates lifted representations, demonstrated through transfer learning tasks across different environments.

### Open Question 3
- Question: What is the optimal strategy for filtering or structuring collected knowledge data to minimize abstraction computation time while maintaining representation quality?
- Basis in paper: [inferred] The paper discusses that "the abstraction time seems being linear with respect to the number transition tuples" and suggests finding solutions to mitigate this aspect.
- Why unresolved: The current approach doesn't scale well with increasing amounts of data, and the paper suggests this as a key challenge.
- What evidence would resolve it: Comparative analysis showing that a specific data filtering or structuring strategy reduces abstraction computation time by X% while maintaining Y% of the original representation's planning success rate.

## Limitations

- Abstraction algorithm requires recomputation from scratch each cycle, lacking true incremental capability
- Propositional logic representation limits expressiveness and prevents operator arguments or predicates
- Computational complexity scales linearly with transition data volume, potentially limiting scalability

## Confidence

- Abstraction procedure reliability: Medium confidence - theoretical grounding but limited empirical validation
- Curiosity-driven exploration effectiveness: Medium confidence - demonstrated in simulation only
- Planning efficiency claims: Medium confidence - supported by comparative results but limited complexity environments

## Next Checks

1. Test the abstraction procedure on a domain with significantly higher state space complexity (10x current size) to assess scalability limits
2. Validate option discovery and planning in a physical robot environment with real sensor noise and uncertainty
3. Implement ablation studies removing each intrinsic motivation component to quantify their individual contributions to learning efficiency