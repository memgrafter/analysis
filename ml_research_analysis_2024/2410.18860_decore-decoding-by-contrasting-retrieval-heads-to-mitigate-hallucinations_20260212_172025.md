---
ver: rpa2
title: 'DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations'
arxiv_id: '2410.18860'
source_url: https://arxiv.org/abs/2410.18860
tags:
- heads
- retrieval
- decore
- masked
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of hallucinations in large language
  models, where models generate unfaithful or factually incorrect outputs. The core
  method, DeCoRe, involves masking retrieval heads in the Transformer architecture
  to induce hallucinations and then contrasting the outputs of the base LLM and the
  masked LLM using conditional entropy to amplify accurate predictions.
---

# DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations

## Quick Facts
- arXiv ID: 2410.18860
- Source URL: https://arxiv.org/abs/2410.18860
- Reference count: 40
- One-line primary result: DeCoRe improves faithfulness in retrieval-augmented LLMs by 18.6% on XSum summarization, 10.9% on MemoTrap instruction following, and 5.5% on NQ-Swap open-book QA

## Executive Summary
This paper addresses hallucinations in large language models by introducing DeCoRe (Decoding by Contrasting Retrieval heads), a method that contrasts outputs from masked and unmasked retrieval heads to amplify faithful predictions. The approach induces hallucinations by masking retrieval heads during decoding, then uses conditional entropy to determine when to trust the base model versus the masked model's output. DeCoRe demonstrates significant improvements in contextual faithfulness across multiple benchmarks, particularly in retrieval-augmented tasks.

## Method Summary
DeCoRe works by strategically masking retrieval heads in Transformer architectures during the decoding process, which induces hallucinatory outputs from the masked model. The method then compares the next-token distributions from both the original (unmasked) and masked models using conditional entropy. When the masked model's distribution shows higher entropy, indicating more uncertainty, the system favors the base model's prediction. Conversely, when the masked model's distribution shows lower entropy, its predictions are amplified. This contrastive approach dynamically adjusts based on the model's confidence levels, leading to more grounded and reliable responses in tasks requiring contextual faithfulness.

## Key Results
- XSum summarization: 18.6% improvement in faithfulness metrics
- MemoTrap instruction following: 10.9% improvement in faithfulness
- NQ-Open and NQ-Swap: 2.4% and 5.5% improvements in open-book QA tasks

## Why This Works (Mechanism)
The mechanism exploits the fact that retrieval heads in LLMs serve as fact-checking mechanisms during generation. By masking these heads, DeCoRe can induce controlled hallucinations, creating a contrast between faithful (unmasked) and unfaithful (masked) outputs. The conditional entropy comparison acts as a signal to determine which output to trust - when the masked model is uncertain (high entropy), it's likely hallucinating, so the base model's prediction is favored. When the masked model is confident despite missing retrieval information, it may indicate the base model is introducing noise, so the masked model's output is amplified. This creates a self-correcting mechanism that improves overall faithfulness.

## Foundational Learning
- **Retrieval heads in Transformers**: Specialized attention heads that attend to retrieved context; needed for understanding how masking induces hallucinations; quick check: verify presence of retrieval-specific heads in model architecture
- **Conditional entropy in probability distributions**: Measures uncertainty in predicted distributions; needed for determining when to trust masked vs unmasked outputs; quick check: calculate entropy differences between base and masked models
- **Contrastive decoding**: Using multiple model variants to improve generation quality; needed for the core mechanism of DeCoRe; quick check: implement basic contrastive decoding with two model variants
- **Retrieval-augmented generation (RAG)**: Combining retrieval with generation; needed for understanding the context where DeCoRe is most effective; quick check: set up a simple RAG pipeline with a base LLM
- **Hallucination detection metrics**: Evaluating faithfulness of generated text; needed for validating DeCoRe's effectiveness; quick check: apply standard hallucination metrics to sample outputs
- **Transformer masking techniques**: Methods for selectively disabling model components; needed for inducing controlled hallucinations; quick check: implement retrieval head masking in a transformer model

## Architecture Onboarding
- **Component map**: Base LLM -> Retrieval augmented generation -> DeCoRe (Retrieval head masking + Conditional entropy contrast) -> Output selection
- **Critical path**: Retrieval retrieval -> Context encoding -> Masked vs unmasked decoding -> Entropy comparison -> Output selection
- **Design tradeoffs**: Computational overhead from dual decoding vs faithfulness improvement; masking introduces controlled hallucinations but may miss other hallucination types
- **Failure signatures**: Over-reliance on masked model in low-retrieval scenarios; incorrect entropy thresholds leading to wrong output selection; performance degradation on non-retrieval tasks
- **First experiments**: 1) Baseline hallucination rate on standard benchmarks, 2) Ablation study on entropy threshold sensitivity, 3) Cross-model validation across different LLM architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Method focuses primarily on retrieval-augmented scenarios, with limited validation on non-retrieval tasks
- Computational overhead from running parallel masked and unmasked decoding could be prohibitive for real-time applications
- Performance gains vary significantly across tasks (18.6% for XSum vs 2.4% for NQ-Open)

## Confidence
- **High confidence**: The general approach of using retrieval head masking to induce hallucinations and contrastive decoding to improve faithfulness is methodologically sound and demonstrates consistent improvements across multiple benchmarks
- **Medium confidence**: The specific performance improvements and their generalizability across different model families and task types require further validation
- **Medium confidence**: The relationship between conditional entropy and faithfulness holds well for retrieval-augmented tasks but may need refinement for broader applications

## Next Checks
1. Cross-model validation: Test DeCoRe across different LLM architectures (e.g., LLaMA, Mistral, Claude) to assess whether the retrieval head masking approach generalizes beyond the tested model families
2. Ablation on entropy thresholds: Systematically vary the conditional entropy thresholds used for contrastive selection to determine optimal parameters and assess robustness to different threshold choices
3. Non-retrieval task evaluation: Apply DeCoRe to tasks without retrieval components (e.g., closed-book QA, creative writing) to evaluate whether the method's benefits extend beyond retrieval-augmented scenarios