---
ver: rpa2
title: 'VoxHakka: A Dialectally Diverse Multi-speaker Text-to-Speech System for Taiwanese
  Hakka'
arxiv_id: '2409.01548'
source_url: https://arxiv.org/abs/2409.01548
tags:
- hakka
- speech
- data
- speaker
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VoxHakka is a multi-speaker, dialectally diverse TTS system for
  Taiwanese Hakka, a critically under-resourced language. Leveraging YourTTS, it achieves
  high naturalness and accuracy across six dialects.
---

# VoxHakka: A Dialectally Diverse Multi-speaker Text-to-Speech System for Taiwanese Hakka

## Quick Facts
- **arXiv ID**: 2409.01548
- **Source URL**: https://arxiv.org/abs/2409.01548
- **Reference count**: 0
- **Primary result**: Multi-speaker TTS system for Taiwanese Hakka achieving high naturalness and accuracy across six dialects

## Executive Summary
VoxHakka addresses the critical under-resourcing of Taiwanese Hakka by implementing a multi-speaker, dialectally diverse TTS system. Leveraging YourTTS technology, it achieves zero-shot synthesis for unseen speakers and supports six distinct Hakka dialects. The system addresses data scarcity through an innovative web scraping pipeline combined with ASR-based cleaning techniques. Subjective CMOS tests demonstrate significant superiority over existing Hakka TTS systems in pronunciation accuracy, tone correctness, and overall naturalness. The system is released under CC-BY 4.0 license for open accessibility.

## Method Summary
VoxHakka implements a YourTTS-based architecture with dialect and speaker embeddings for Taiwanese Hakka TTS. The system employs a web scraping pipeline to collect speech data from government sources, followed by ASR-based cleaning using a trained acoustic model to refine ill-transcribed data. Forced alignment and silence trimming prepare 77.72% of scraped speech for TTS training. G2P conversion maps Hakka characters to phonemes using a six-dialect lexicon. The model incorporates dialect-specific embeddings and speaker embeddings from a pre-trained model, enabling zero-shot multi-speaker synthesis. Training uses AdamW optimizer with 200k warmup steps and 800k total steps. HiFi-GAN vocoder generates the final speech output.

## Key Results
- Subjective CMOS tests show VoxHakka significantly outperforms existing Hakka TTS systems in pronunciation accuracy and naturalness
- System achieves zero-shot multi-speaker synthesis capability across six Hakka dialects
- Web scraping pipeline with ASR cleaning successfully acquires 180.53 hours of high-quality training data
- Sixian dialect evaluation demonstrates superior tone correctness compared to commercial alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VoxHakka achieves zero-shot multi-speaker synthesis through speaker embeddings derived from a pre-trained speaker model
- Mechanism: YourTTS architecture incorporates speaker embeddings generated by a separate speaker model, which are summed with text encoder outputs and decoder outputs
- Core assumption: Speaker model effectively captures speaker-specific characteristics that generalize to unseen speakers
- Evidence anchors: [abstract] "enabling the synthesis of speech for unseen speakers"; [section 3] "speaker embeddings generated by a separate speaker model [8] based on the G2P output"
- Break condition: Speaker embeddings fail to generalize when speakers have unique vocal characteristics not represented in training data

### Mechanism 2
- Claim: Web scraping with ASR-based cleaning enables acquisition of sufficient high-quality data for under-resourced language TTS
- Mechanism: Pipeline scrapes audio/transcriptions from government sources, uses trained ASR system to refine ill-transcribed data, applies forced alignment and silence trimming
- Core assumption: ASR system trained on well-transcribed data can accurately clean ill-transcribed data from similar sources
- Evidence anchors: [abstract] "cost-effective approach utilizing a web scraping pipeline coupled with automatic speech recognition (ASR)-based data cleaning techniques"; [section 4.1] "77.72% of the scraped speech is deemed suitable for TTS training"
- Break condition: ASR system accuracy degrades significantly when audio quality varies beyond training distribution

### Mechanism 3
- Claim: Dialect-specific embeddings enable accurate synthesis across all six Hakka dialects
- Mechanism: YourTTS incorporates 4-dimensional trainable language/dialect embeddings that capture pronunciation variations across dialects
- Core assumption: Dialect embeddings can effectively encode the phonetic and tonal differences between all six Hakka dialects
- Evidence anchors: [abstract] "training the model with dialect-specific data, allowing for the generation of speaker-aware Hakka speech"; [section 3] "dialect embeddings capturing pronunciation variations across dialects"
- Break condition: Dialect embeddings cannot adequately represent highly divergent dialects, leading to synthesis errors

## Foundational Learning

- Concept: Grapheme-to-Phoneme (G2P) conversion
  - Why needed here: Taiwanese Hakka uses Chinese characters without phonetic annotations, requiring conversion to phonetic representations for TTS synthesis
  - Quick check question: How does the G2P system handle characters with multiple pronunciations across different Hakka dialects?

- Concept: Speaker embedding extraction and utilization
  - Why needed here: VoxHakka needs to generate speech for unseen speakers, requiring a mechanism to capture and apply speaker characteristics from reference audio
  - Quick check question: What features does the speaker model extract to represent speaker identity and how are they integrated into the TTS pipeline?

- Concept: Dialectal variation in tone and phonology
  - Why needed here: Taiwanese Hakka has six major dialects with distinct tonal patterns and phonological features that must be accurately represented in synthesized speech
  - Quick check question: How do the tonal systems differ across the six Hakka dialects and how does the model capture these differences?

## Architecture Onboarding

- Component map: Web scraping pipeline → ASR cleaning → Data preprocessing (forced alignment, silence trimming) → G2P conversion → YourTTS model (text encoder, dialect embedding, speaker embedding, decoder, vocoder) → HiFi-GAN vocoder → Pre-trained speaker model for embedding extraction → Custom Hakka lexicon for ASR training

- Critical path: Text → G2P → Phoneme embedding + Dialect embedding + Speaker embedding → Text encoder → Decoder → Vocoder → Speech

- Design tradeoffs: Zero-shot capability vs. synthesis quality for seen speakers; CPU deployment vs. inference speed; dialect coverage vs. data scarcity for individual dialects

- Failure signatures:
  - Pronunciation errors → G2P conversion or dialect embedding issues
  - Speaker identity mismatch → Speaker embedding extraction problems
  - Unnatural prosody → Decoder or vocoder issues
  - Dialect confusion → Dialect embedding confusion or insufficient dialect-specific data

- First 3 experiments:
  1. Test G2P conversion accuracy on a sample of Hakka characters with known pronunciations across dialects
  2. Evaluate speaker embedding extraction by synthesizing speech for a held-out speaker and comparing to reference audio
  3. Test dialect embedding effectiveness by synthesizing the same text in all six dialects and having native speakers identify the dialect accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of VoxHakka vary across the six Hakka dialects, and what factors contribute to these differences?
- Basis in paper: [explicit] The paper mentions that there is a substantial disparity in the amount of speech data available for Dapu, Raoping, Zhaoan, and Nansixian dialects compared to Sixian and Hailu
- Why unresolved: The paper only evaluates the Sixian dialect in the subjective listening tests, as there are no publicly available TTS systems for the other dialects
- What evidence would resolve it: Conducting subjective listening tests with native speakers of each dialect to evaluate the naturalness, pronunciation accuracy, and tone correctness of VoxHakka's synthesized speech for all six dialects

### Open Question 2
- Question: What are the specific limitations of VoxHakka's pronunciation accuracy, and how can they be addressed?
- Basis in paper: [explicit] The paper states that both VoxHakka versions exhibit slightly lower pronunciation accuracy scores compared to Cyberon
- Why unresolved: The paper does not provide a detailed analysis of the specific pronunciation errors made by VoxHakka or propose concrete solutions for improvement
- What evidence would resolve it: Conducting a detailed error analysis of VoxHakka's pronunciation errors, identifying the underlying causes, and developing targeted solutions to address these limitations

### Open Question 3
- Question: How can VoxHakka be extended to support emotional expressiveness and prosodic control in Hakka speech synthesis?
- Basis in paper: [explicit] The paper mentions that future research aims to expand the system's capabilities by incorporating emotional expressiveness and prosodic control
- Why unresolved: The paper does not provide any details on how emotional expressiveness and prosodic control can be achieved in VoxHakka
- What evidence would resolve it: Conducting research on incorporating emotion and prosody modeling techniques into the VoxHakka framework, evaluating the effectiveness of different approaches

## Limitations

- Lack of detailed quantitative evaluation of dialect embedding effectiveness across all six Hakka dialects
- No rigorous testing of speaker embedding generalization capability for unseen speakers
- Limited validation of ASR-based cleaning pipeline accuracy beyond post-cleaning data statistics
- Subjective comparison to commercial systems without objective metrics like MOS or pronunciation accuracy scores

## Confidence

**High Confidence Claims:**
- VoxHakka successfully implements a working TTS system for Taiwanese Hakka that outperforms existing systems in subjective tests
- The web scraping + ASR cleaning pipeline effectively increases usable training data from under-resourced sources
- YourTTS architecture can incorporate dialect and speaker embeddings for multi-speaker, multi-dialect synthesis

**Medium Confidence Claims:**
- The zero-shot speaker synthesis capability generalizes effectively to unseen speakers
- Dialect embeddings accurately capture and reproduce all six Hakka dialect variations
- The system achieves "high naturalness and accuracy" as claimed in the abstract

**Low Confidence Claims:**
- The specific quality improvements over commercial systems are quantifiable and consistent across all evaluation dimensions
- The ASR-based cleaning achieves sufficient accuracy for all data sources and quality levels
- The G2P conversion handles polyphones and dialectal variations without systematic errors

## Next Checks

1. **Dialect Embedding Validation**: Conduct a controlled experiment synthesizing the same text in all six dialects and have native speakers from each dialect region rate both dialect identification accuracy and pronunciation correctness

2. **Speaker Embedding Generalization Test**: Systematically evaluate speaker embedding quality by synthesizing speech for speakers in a held-out test set (never seen during training) and comparing MOS scores to seen speakers, including objective speaker verification metrics

3. **ASR Cleaning Accuracy Assessment**: Implement a rigorous evaluation of the ASR-based cleaning pipeline by sampling transcripts before and after cleaning, having human annotators rate transcription accuracy, and calculating word error rate reduction