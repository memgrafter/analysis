---
ver: rpa2
title: Aligning Large Language Models by On-Policy Self-Judgment
arxiv_id: '2402.11253'
source_url: https://arxiv.org/abs/2402.11253
tags:
- judge
- response
- policy
- judgment
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SELF-JUDGE, a novel alignment framework that
  trains a single model to both generate responses and judge pairwise preferences,
  enabling on-policy learning without requiring a separate reward model. The approach
  uses Judge-augmented Supervised Fine-tuning (JSFT) to treat the pairwise judgment
  task as a special case of instruction-following, allowing the model to judge its
  own responses during self-training.
---

# Aligning Large Language Models by On-Policy Self-Judgment

## Quick Facts
- arXiv ID: 2402.11253
- Source URL: https://arxiv.org/abs/2402.11253
- Reference count: 23
- Key outcome: SELF-JUDGE outperforms RLHF, DPO, RSO, ReST, and RAFT on preference benchmarks, achieving 44.88% winning rate on AlpacaEval, 76.25% on VicunaEval, and 4.80 score on MT-Bench.

## Executive Summary
This paper introduces SELF-JUDGE, a novel alignment framework that trains a single model to both generate responses and judge pairwise preferences, enabling on-policy learning without requiring a separate reward model. The approach uses Judge-augmented Supervised Fine-tuning (JSFT) to treat the pairwise judgment task as a special case of instruction-following, allowing the model to judge its own responses during self-training. Experimental results show that SELF-JUDGE achieves state-of-the-art performance on preference benchmarks while being parameter efficient.

## Method Summary
SELF-JUDGE introduces Judge-augmented Supervised Fine-tuning (JSFT) where a single model is trained on both standard supervised fine-tuning and pairwise judgment tasks. The model learns to interpret judgment templates as instructions, outputting a judge token (A or B) plus optional rationale. During self-training, the same model samples responses and judges them using a frozen reference version, avoiding distribution shift issues. Inference uses tournament rejection sampling where the model judges its own outputs to select the best response.

## Key Results
- Achieves 44.88% winning rate on AlpacaEval benchmark
- Achieves 76.25% winning rate on VicunaEval benchmark  
- Achieves 4.80 score on MT-Bench
- Outperforms existing methods including RLHF, DPO, RSO, ReST, and RAFT
- Demonstrates effective self-rejection at inference without additional evaluators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating pairwise judgment as instruction-following enables a single model to serve dual roles
- Mechanism: The model learns to interpret structured judgment templates as instructions, producing a judge token (A or B) plus optional rationale. This allows it to generate responses AND evaluate them in the same forward pass.
- Core assumption: LLMs can generalize from SFT-style instruction patterns to comparative judgment tasks without requiring separate reward modeling.
- Evidence anchors: [abstract] "view the pairwise judgment task, choosing the better response from a response pair, as a special case of the instruction-following task"

### Mechanism 2
- Claim: On-policy self-judgment improves performance by reducing distribution shift between policy and evaluator
- Mechanism: The same model samples responses and judges them, so the evaluator's distribution matches the policy's, avoiding the covariate shift seen with separate reward models.
- Core assumption: A model's own judgments are sufficiently calibrated to guide policy updates without external validators.
- Evidence anchors: [section 3.2] "we regard the reference policy πref as a judge to evaluate the preference order between generated samples from current policy"

### Mechanism 3
- Claim: Principle-aware judgment with rationale improves judging quality without sacrificing policy performance
- Mechanism: Adding structured principles and rationale to the judgment template biases the model toward finer-grained reasoning, which transfers to better policy updates.
- Core assumption: Training on richer judgment targets (principles + rationales) enhances the model's ability to detect subtle preference signals.
- Evidence anchors: [section 5.3] "involving principles and rationale improves JM's performance as a judge but not as a policy"

## Foundational Learning

- Concept: Pairwise Bradley-Terry preference modeling
  - Why needed here: The model must output a preference between two responses; understanding the probabilistic choice model underpins the training objective.
  - Quick check question: What does a judge token likelihood represent in terms of relative preference?

- Concept: Instruction-tuning as multi-task learning
  - Why needed here: JSFT fuses SFT and judgment into one training objective; the model must treat judgment prompts as another instruction format.
  - Quick check question: How does a judgment template differ structurally from a standard instruction prompt?

- Concept: On-policy vs. off-policy alignment
  - Why needed here: SELF-JUDGE's advantage stems from sampling and judging from the current policy; knowing the bias/variance trade-offs is key.
  - Quick check question: Why does on-policy sampling reduce distribution shift compared to static preference datasets?

## Architecture Onboarding

- Component map: JSFT dataset builder -> Judge Model (JM) <- LoRA adapters -> Self-training loop -> Self-rejection module
- Critical path:
  1. Build JSFT dataset (SFT + pairwise judgment)
  2. Fine-tune base LLM with JSFT → JM
  3. Initialize policy and frozen reference from JM
  4. Self-training: sample pairs → reference judge → DPO update
  5. Inference: tournament sampling using JM judgments
- Design tradeoffs:
  - Single-model vs. separate RM: saves parameters, but relies on self-calibration
  - Tournament rejection vs. Best-of-N: O(N) vs. O(N²) judgments, slight quality loss
  - Principle-aware vs. generic: richer training signal vs. possible task bias
- Failure signatures:
  - Policy collapse: self-judgments become overly confident or degenerate
  - Slow convergence: on-policy updates need many samples for stable gradients
  - Tournament instability: small N leads to noisy rejection
- First 3 experiments:
  1. JSFT ablation: compare JM trained only on SFT, only on judgment, and combined
  2. Calibration check: measure agreement between JM's judge-token likelihood and human preference accuracy
  3. Rejection scaling: sweep N in tournament sampling to find diminishing returns

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of SELF-JUDGE scale with model size, and what are the limitations of using smaller models?
- Open Question 2: How does SELF-JUDGE perform on tasks requiring complex reasoning or domain-specific knowledge compared to specialized models?
- Open Question 3: What is the impact of different sampling strategies (temperature, top-p, etc.) on the performance of SELF-JUDGE during self-training and inference?
- Open Question 4: How does SELF-JUDGE handle conflicting principles in the judgment task, and what is the impact on final model behavior?
- Open Question 5: What is the computational overhead of SELF-JUDGE compared to traditional RLHF, considering both training and inference time?

## Limitations

- The instruction-following framing of judgment tasks may not generalize beyond specific templates used
- Self-calibration assumption underlying on-policy learning isn't thoroughly validated with calibration curves
- Computational efficiency claims lack quantitative runtime comparisons against baseline methods
- The O(N²) complexity of tournament rejection may become prohibitive for larger N values

## Confidence

**High Confidence**: Empirical results showing SELF-JUDGE outperforming baselines on preference benchmarks are well-supported by the data presented.

**Medium Confidence**: Mechanism explanations (instruction-following framing, on-policy benefits) are plausible but rely on behavioral observations rather than mechanistic understanding.

**Low Confidence**: Scalability claims and computational efficiency assertions lack quantitative support and runtime measurements.

## Next Checks

1. **Calibration Analysis**: Measure the judge model's calibration by comparing judge-token likelihood scores against human preference accuracy on held-out data. Plot reliability diagrams to identify overconfidence or underconfidence patterns.

2. **Template Ablation Study**: Systematically vary the judgment template structure (e.g., different instruction formats, rationale requirements) and measure impact on both judging capability and policy performance to validate the instruction-following mechanism.

3. **Runtime Benchmarking**: Measure wall-clock time for SELF-JUDGE's tournament rejection versus baseline methods' inference procedures, including memory usage analysis for the LoRA adapters versus separate reward models.