---
ver: rpa2
title: Accelerating Full Waveform Inversion By Transfer Learning
arxiv_id: '2408.00695'
source_url: https://arxiv.org/abs/2408.00695
tags:
- learning
- nn-based
- transfer
- conventional
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a transfer learning approach to accelerate neural
  network (NN)-based full waveform inversion (FWI). By pretraining a U-Net with gradients
  from the first iteration of conventional FWI, the method provides a better initial
  guess for the subsequent NN-based FWI optimization.
---

# Accelerating Full Waveform Inversion By Transfer Learning

## Quick Facts
- arXiv ID: 2408.00695
- Source URL: https://arxiv.org/abs/2408.00695
- Reference count: 40
- The paper presents a transfer learning approach to accelerate neural network (NN)-based full waveform inversion (FWI) by pretraining with gradients from conventional FWI.

## Executive Summary
This paper introduces a transfer learning approach to accelerate neural network-based full waveform inversion (FWI). By pretraining a U-Net with gradients from the first iteration of conventional FWI, the method provides a better initial guess for subsequent NN-based FWI optimization. The approach leverages supervised pretraining to map adjoint gradients to true material fields, simplifying network architecture and improving convergence speed and reconstruction quality.

## Method Summary
The method pretrains a U-Net to map adjoint gradients from the first iteration of conventional FWI to true material fields, then uses this pretrained network as initialization for NN-based FWI. Training data consists of pairs of first-iteration adjoint gradients and corresponding true material fields from synthetic domains with elliptical voids. The pretrained U-Net provides a geometrically aligned initial guess that accelerates convergence of the subsequent NN-based FWI optimization.

## Key Results
- Transfer learning NN-based FWI outperforms conventional FWI and standard NN-based FWI in reconstruction quality
- The method achieves faster convergence and better artifact reduction compared to other approaches
- Pretraining with adjoint gradients provides geometrically aligned initial guesses that simplify network architecture

## Why This Works (Mechanism)

### Mechanism 1
Pretraining with first-iteration adjoint gradients provides a geometrically aligned initial guess for NN-based FWI. The first-iteration adjoint gradient approximates the shape of the defect, so mapping it directly to the material field creates an initialization much closer to the true solution than random weights. This works because the adjoint gradient shape is sufficiently similar to the true material distribution to serve as a good coarse prior. However, if the damage is very small relative to grid resolution, the adjoint gradient will be noisy and poorly aligned to the true shape.

### Mechanism 2
NN-based FWI benefits from spectral bias, preferentially learning low-frequency content and avoiding high-frequency artifacts. Overparameterized NNs implicitly regularize the optimization by smoothing high-frequency noise in reconstructions while still allowing sharp material transitions. This works because the spectral bias of deep nets is strong enough to suppress cycle-skipping artifacts common in conventional FWI. However, if the damage has fine-scale features that are critical for accuracy, the low-frequency preference may under-resolve them.

### Mechanism 3
Transfer learning removes dependence on random weight initialization, stabilizing convergence across different damage shapes. Pretrained U-Net weights encode learned mappings from adjoint gradients to plausible material fields, so the optimizer starts from a meaningful basin in parameter space rather than a random one. This works because the pretraining data distribution (elliptical voids) is broad enough that the learned mapping generalizes to unseen damage geometries. However, if the damage geometry is far outside the pretraining distribution, the initial guess can be poor and require many more iterations.

## Foundational Learning

- **Adjoint method for gradient computation in PDE-constrained optimization**: Used to compute ∇γL for both conventional FWI and as pretraining input. Quick check: In the adjoint method, why do we solve a backward-in-time wave equation after the forward solve?

- **Neural network parameterization of material fields**: Replaces piecewise constant discretization with an overparameterized NN, enabling implicit regularization. Quick check: What is the shape of the NN output tensor relative to the finite-difference grid?

- **Transfer learning via supervised pretraining**: Maps adjoint gradients to material fields so the downstream FWI starts from a better basin. Quick check: What loss function is used during pretraining and why is it appropriate?

## Architecture Onboarding

- **Component map**: Forward wave solver -> Adjoint solver -> Pretraining U-Net (gradient → material field) -> NN-based FWI generator (random tensor → material field) -> Loss aggregation and optimizer (Adam)

- **Critical path**: 1. Generate pretraining data: run conventional FWI one iteration → adjoint gradient + true material field. 2. Train U-Net to map gradient → material field. 3. For each FWI case: compute first-iteration adjoint gradient → feed to pretrained U-Net → use output as initialization → run NN-based FWI updates.

- **Design tradeoffs**: Pretraining cost vs. convergence speed: pretraining requires extra adjoint solves but yields faster downstream convergence. Network capacity vs. generalization: larger U-Net can fit training data better but may overfit to narrow damage shapes. Random input tensor vs. structured input: random tensor gives more flexibility but may slow convergence; adjoint gradient is structured and easier to map.

- **Failure signatures**: Poor initial guess: adjoint gradient too noisy or damage too small → pretraining cannot generalize. Overfitting: training loss low but validation loss high → pretraining U-Net memorized shapes. Spectral bias too strong: reconstructions overly smooth → fine damage features missing.

- **First 3 experiments**: 1. Run conventional FWI one iteration on a simple void → inspect adjoint gradient shape. 2. Train U-Net for 10 epochs on 50 synthetic samples → check loss curve for overfitting. 3. Run transfer learning NN-based FWI on a test case → compare MSE after 5, 10, 15 iterations to baseline NN-based FWI.

## Open Questions the Paper Calls Out

- **Scaling performance with domain size and complexity**: How does the performance of transfer learning NN-based FWI scale with increasing domain size and complexity of damage patterns? The paper mentions that larger domains make pretraining data generation and hyperparameter tuning more challenging, but only tests on 2D domains with simple elliptical voids. Systematic experiments on progressively larger 2D domains and full 3D domains with complex damage patterns would resolve this.

- **Optimal pretraining architecture and strategy**: What is the optimal architecture and training strategy for the pretraining network to maximize generalization to out-of-distribution damage cases? While the paper uses a U-Net and provides some hyperparameter tuning details, it does not explore alternative architectures or systematically investigate the impact of training data diversity on generalization. Comparative studies of different network architectures and training strategies would resolve this.

- **Benefits of regularization and hybrid approaches**: Can transfer learning NN-based FWI benefit from incorporating regularization techniques or hybrid approaches that combine physics-based and data-driven methods? The paper focuses on comparing transfer learning NN-based FWI with conventional FWI and NN-based FWI without pretraining, but does not explore combining these approaches with regularization or hybrid methods. Experiments comparing performance with various regularization techniques or hybrid approaches would resolve this.

## Limitations
- The method relies on adjoint gradients being sufficiently informative for pretraining, which degrades for very small defects relative to grid resolution
- Numerical experiments are limited to 2D elliptical voids; generalization to 3D or irregular geometries remains unproven
- Computational overhead of pretraining may offset gains for very small problems

## Confidence
- **High** that pretraining improves initialization quality, as evidenced by the direct mapping between adjoint gradients and true material fields
- **Medium** that the observed acceleration is primarily due to spectral bias of NNs and not confounded by other factors
- **Low** that the method will generalize to damage shapes far outside the elliptical training distribution without retraining

## Next Checks
1. Test the method on 2D domains with rectangular voids or intersecting cracks to assess pretraining generalization limits
2. Compare convergence curves and final MSE for varying damage sizes (small vs. large) to quantify the threshold where adjoint gradients become too noisy
3. Perform a runtime analysis comparing total wall-clock time (including pretraining) versus conventional FWI and baseline NN-based FWI for different problem sizes