---
ver: rpa2
title: 'ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven
  Scientific Discovery'
arxiv_id: '2410.05080'
source_url: https://arxiv.org/abs/2410.05080
tags:
- data
- task
- dataset
- https
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ScienceAgentBench is a new benchmark designed to rigorously evaluate
  language agents on individual tasks within data-driven scientific discovery workflows.
  The benchmark comprises 102 tasks extracted from 44 peer-reviewed publications across
  four scientific disciplines: Bioinformatics, Computational Chemistry, Geographical
  Information Science, and Psychology & Cognitive Neuroscience.'
---

# ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery

## Quick Facts
- arXiv ID: 2410.05080
- Source URL: https://arxiv.org/abs/2410.05080
- Reference count: 40
- 102 tasks extracted from 44 peer-reviewed publications across four scientific disciplines, with top-performing agent (Claude-3.5-Sonnet with self-debug) achieving 32.4% success rate without expert knowledge

## Executive Summary
ScienceAgentBench introduces a rigorous benchmark for evaluating language agents on individual tasks within data-driven scientific discovery workflows. The benchmark comprises 102 tasks extracted from 44 peer-reviewed publications across Bioinformatics, Computational Chemistry, Geographical Information Science, and Psychology & Cognitive Neuroscience. Each task is formulated as a code generation problem requiring Python programs that perform specific scientific analyses. The evaluation methodology employs multiple metrics including Valid Execution Rate, Success Rate, CodeBERTScore, and API Cost, along with rubric-based human evaluation for fine-grained assessment. Experiments with five LLMs and three frameworks reveal that current language agents struggle with scientific task completion, achieving only 32.4% success rate even with expert-provided knowledge.

## Method Summary
ScienceAgentBench constructs a benchmark by extracting 102 scientific tasks from 44 peer-reviewed publications across four disciplines. Each task is validated by subject matter experts and formulated as a code generation problem with annotated Python programs as targets. The evaluation uses five LLMs (Llama-3.1-70B/405B, Mistral-Large-2, GPT-4o, Claude-3.5-Sonnet) with three frameworks (direct prompting, OpenHands CodeAct, self-debug). Programs are executed in isolated environments and assessed using outcome-based metrics (VER, SR, CBS, Cost) plus rubric-based human evaluation. Two data contamination mitigation strategies are employed: removing test samples and using dummy labels for model development tasks.

## Key Results
- Claude-3.5-Sonnet with self-debug framework achieved the highest success rate of 32.4% without expert knowledge and 34.3% with expert knowledge
- Data processing and model development emerged as major failure points, particularly for tasks involving heterogeneous data types
- Expert-provided knowledge improved performance on some metrics but agents struggled to effectively utilize provided information
- Significant performance variation across scientific domains, with Bioinformatics showing highest success rates and Psychology lowest

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ScienceAgentBench rigorously evaluates LLM-based agents on individual tasks rather than end-to-end automation claims.
- Mechanism: By extracting 102 tasks from 44 peer-reviewed publications and validating them with 9 subject matter experts, the benchmark ensures scientific authenticity and real-world relevance.
- Core assumption: Tasks from peer-reviewed publications with expert validation accurately represent realistic data-driven discovery workflows.
- Evidence anchors: [abstract] "we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation"; [section] "The construction of ScienceAgentBench follows three key design principles... (1) Scientific authenticity through co-design with subject matter experts"
- Break condition: If peer-reviewed publications don't capture realistic workflows or expert validation introduces bias.

### Mechanism 2
- Claim: Task-specific success criteria and rubric-based evaluation provide granular assessment beyond binary outcomes.
- Mechanism: Success criteria are implemented as executable scripts and rubrics are structured into five stages (Data Loading, Data Processing, Modeling/Visualization, Output formatting, Output Saving).
- Core assumption: Fine-grained evaluation criteria can better differentiate agent capabilities than outcome-only metrics.
- Evidence anchors: [section] "We also provide step-by-step rubrics specific to each task to enable graded evaluation"; [section] "Outcome-based evaluation metrics...can sometimes be too stringent...As a complement...we introduce rubric-based evaluation"
- Break condition: If rubric scoring becomes subjective or fails to correlate with task completion.

### Mechanism 3
- Claim: Mitigation strategies address data contamination and shortcut concerns in benchmark evaluation.
- Mechanism: Two strategies: (1) removing test set samples to break automatic data loaders, (2) replacing test labels with dummy values for model development tasks.
- Core assumption: These dataset modifications effectively prevent agents from memorizing or cheating.
- Evidence anchors: [section] "we devise two strategies to modify the datasets... (1) For each dataset, we randomly remove five data points... (2) For tasks involving model development, we re-split the dataset"; [section] "These two strategies effectively mitigate data contamination and agent shortcut concerns"
- Break condition: If agents find alternative ways to cheat or dataset modifications break task validity.

## Foundational Learning

- Concept: Code generation as task formulation
  - Why needed here: ScienceAgentBench unifies all tasks to output self-contained Python programs for verifiable, usable results
  - Quick check question: Why does unifying outputs as Python programs enable better evaluation than natural language descriptions?

- Concept: Scientific workflow decomposition
  - Why needed here: Tasks are extracted from real publications representing sub-tasks like data processing, model development, and visualization
  - Quick check question: How does decomposing end-to-end discovery into individual tasks help assess agent capabilities?

- Concept: Expert validation protocol
  - Why needed here: 9 subject matter experts validate tasks to ensure scientific authenticity and domain relevance
  - Quick check question: What specific validation steps do experts perform to ensure task quality?

## Architecture Onboarding

- Component map:
  Task extraction pipeline → Expert validation → Program annotation → Success criteria implementation → Rubric generation → Evaluation framework
  LLMs + frameworks (direct, OpenHands, self-debug) → Program generation → Environment setup → Execution → Metric calculation
  Human evaluation interface → Rubric-based scoring → Quality control

- Critical path:
  1. Task extraction from publications
  2. Expert validation and knowledge annotation
  3. Program annotation and success criteria
  4. Environment setup for evaluation
  5. Program execution and metric calculation
  6. Rubric-based human evaluation

- Design tradeoffs:
  - Python-only vs multi-language support (chose Python for annotator familiarity)
  - Medium scale (102 tasks) vs comprehensiveness (balances quality vs coverage)
  - Open-source only vs proprietary datasets (ensures accessibility)
  - Outcome-based vs rubric-based evaluation (provides both binary and granular assessment)

- Failure signatures:
  - Low VER/SR across agents → Benchmark too difficult or agents insufficient
  - High CBS but low SR → Programs structurally similar but functionally incorrect
  - Inconsistent rubric scoring → Need for better rubric standardization
  - Environment setup failures → Package dependency issues or domain-specific tool requirements

- First 3 experiments:
  1. Run baseline direct prompting with GPT-4o on 10 diverse tasks to establish performance floor
  2. Compare OpenHands vs self-debug frameworks using Claude-3.5-Sonnet on same tasks
  3. Test expert-provided knowledge impact by running self-debug with and without knowledge on 20 tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language agents perform on data-driven discovery tasks outside the four disciplines covered in ScienceAgentBench?
- Basis in paper: [inferred] The paper notes that ScienceAgentBench focuses on four specific disciplines (Bioinformatics, Computational Chemistry, Geographical Information Science, and Psychology & Cognitive Neuroscience) and acknowledges this as a limitation.
- Why unresolved: The benchmark's current scope limits understanding of agent performance in other scientific domains like physics, astronomy, or materials science.
- What evidence would resolve it: Extending ScienceAgentBench to include tasks from additional scientific disciplines and evaluating agent performance across this expanded benchmark.

### Open Question 2
- Question: What is the impact of different knowledge injection strategies on language agent performance in scientific tasks?
- Basis in paper: [explicit] The paper evaluates expert-provided knowledge and finds it helps with some metrics but not others, noting that agents sometimes struggle to effectively utilize provided knowledge.
- Why unresolved: The study only tests one approach to knowledge injection (providing up to three pieces of expert knowledge). Other strategies like iterative knowledge refinement or knowledge retrieval during execution were not explored.
- What evidence would resolve it: Comparative experiments testing various knowledge injection methods, including retrieval-augmented generation and adaptive knowledge selection based on task requirements.

### Open Question 3
- Question: What are the key bottlenecks preventing language agents from successfully completing complex scientific tasks?
- Basis in paper: [explicit] The paper identifies data processing and model development as major failure points, particularly for tasks involving heterogeneous data types like cell images, molecules, and genes.
- Why unresolved: While specific failure modes are identified, the paper doesn't provide a systematic analysis of which sub-tasks or reasoning capabilities are most limiting across the benchmark.
- What evidence would resolve it: Detailed error analysis breaking down failure rates by specific sub-task categories, data types, and reasoning requirements, combined with ablation studies on agent components.

## Limitations

- The benchmark covers only four scientific disciplines, limiting generalizability to other domains
- Rubric-based evaluation introduces potential subjectivity and variability in fine-grained assessment
- Data contamination mitigation strategies may not fully prevent agents from exploiting dataset-specific patterns
- Current evaluation methodology may not scale effectively to larger or more complex scientific tasks

## Confidence

**High Confidence** (Extensive evidence, strong theoretical grounding):
- The benchmark construction methodology following scientific authenticity principles
- The technical implementation of success criteria as executable scripts
- The basic evaluation framework using VER, SR, CBS, and Cost metrics

**Medium Confidence** (Some evidence but with limitations):
- The effectiveness of data contamination mitigation strategies
- The reliability of rubric-based evaluation for fine-grained assessment
- The claim that task-level evaluation better informs agent capabilities than end-to-end claims

**Low Confidence** (Limited evidence or significant assumptions):
- The generalizability of results across all scientific discovery workflows
- The sufficiency of 102 tasks to represent realistic scientific discovery
- The scalability of the evaluation methodology to larger or more complex tasks

## Next Checks

1. **Cross-Domain Performance Validation**: Run a systematic study testing the same agent across all four scientific domains using identical evaluation protocols to quantify performance variation and identify domain-specific challenges.

2. **Rubric Reliability Assessment**: Conduct inter-rater reliability analysis by having multiple evaluators score the same set of programs using the rubrics, measuring inter-rater agreement (Cohen's kappa) to quantify scoring consistency.

3. **Data Contamination Robustness Test**: Design a controlled experiment where agents are tested on both original datasets and modified versions to measure how much performance degrades, validating the effectiveness of the contamination mitigation strategies.