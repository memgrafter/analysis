---
ver: rpa2
title: 'GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?'
arxiv_id: '2412.10056'
source_url: https://arxiv.org/abs/2412.10056
tags:
- questions
- question
- zhang
- llms
- difficulty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GAOKAO-Eval is a comprehensive benchmark based on China\u2019\
  s National College Entrance Examination, designed to evaluate Large Language Models\
  \ (LLMs) in a secure, closed-book setting with expert human grading. Contrary to\
  \ expectations, high scores on GAOKAO-Eval do not reliably indicate human-aligned\
  \ capabilities; models show semi difficulty-invariant scoring and high variance\
  \ in performance on questions of similar difficulty."
---

# GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?

## Quick Facts
- arXiv ID: 2412.10056
- Source URL: https://arxiv.org/abs/2412.10056
- Authors: Zhikai Lei; Tianyi Liang; Hanglei Hu; Jin Zhang; Yunhua Zhou; Yunfan Shao; Linyang Li; Chenchui Li; Changbo Wang; Hang Yan; Qipeng Guo
- Reference count: 38
- Key outcome: High scores on GAOKAO-Eval do not reliably indicate human-aligned capabilities due to semi difficulty-invariant scoring and inconsistent grading

## Executive Summary
GAOKAO-Eval is a comprehensive benchmark based on China's National College Entrance Examination, designed to evaluate Large Language Models (LLMs) in a secure, closed-book setting with expert human grading. Contrary to expectations, high scores on GAOKAO-Eval do not reliably indicate human-aligned capabilities; models show semi difficulty-invariant scoring and high variance in performance on questions of similar difficulty. Unique error patterns and inconsistent grading between models and human teachers were also identified. The findings challenge the validity of current benchmarks and highlight the need for LLM-aligned difficulty analysis.

The study reveals that reasoning-based metrics, such as OpenAI o1's reasoning tokens, can better align difficulty assessments with LLM performance, offering a path toward more accurate evaluation. This work suggests that current LLM evaluation frameworks may need significant revision to properly assess true model capabilities rather than just pattern matching abilities.

## Method Summary
GAOKAO-Eval evaluates Large Language Models using China's National College Entrance Examination (Gaokao) questions across multiple subjects including Chinese, Mathematics, English, Physics, Chemistry, Biology, History, Geography, and Politics. The benchmark includes 3.95k questions with an average length of 674.01 words, featuring both textual and multimodal questions. Models are evaluated in a closed-book setting with expert human grading for subjective questions. The Rasch model is used to analyze scoring patterns and difficulty alignment, while the Elo rating system assesses question difficulty. The evaluation focuses on models released before the Gaokao exam date to maintain the closed-book constraint.

## Key Results
- Models exhibit semi difficulty-invariant scoring behavior, where high scores do not reliably indicate human-aligned capabilities
- High variance in performance on questions of similar difficulty suggests pattern matching without genuine understanding
- Inconsistent grading between models and human teachers reveals potential flaws in current evaluation frameworks
- Reasoning-based metrics like OpenAI o1's reasoning tokens show promise for better difficulty alignment with LLM performance

## Why This Works (Mechanism)
The benchmark works by leveraging the standardized and comprehensive nature of Gaokao exams to create a consistent evaluation framework. The closed-book setting ensures that models cannot access external information during evaluation, while expert human grading provides ground truth for subjective questions. The Rasch model analysis reveals how model performance deviates from human-aligned difficulty curves, exposing the semi difficulty-invariant behavior. This approach identifies fundamental limitations in how current benchmarks assess LLM capabilities.

## Foundational Learning
- **Rasch Model Analysis**: Needed to understand how scoring patterns relate to question difficulty and identify deviation from human-aligned performance. Quick check: Verify R-squared values indicate good model fit to performance data.
- **Elo Rating System**: Required for difficulty assessment that incorporates both human expertise and LLM judgments. Quick check: Confirm Elo ratings correlate with both human and model performance patterns.
- **Expert Human Grading Protocols**: Essential for establishing ground truth on subjective questions and identifying inconsistent grading patterns. Quick check: Calculate Inconsistent Score Rate (ISR) across different subjects and models.
- **Closed-Book Evaluation**: Critical for ensuring fair comparison and preventing model access to external information. Quick check: Verify all models were released before exam date and cannot access current information.
- **Multimodal Question Handling**: Important for comprehensive assessment including image-based questions. Quick check: Confirm multimodal models can properly process and respond to visual content.
- **Difficulty-Invariant Behavior Analysis**: Central to understanding why high scores don't reflect true capabilities. Quick check: Examine variance in performance across questions of similar difficulty.

## Architecture Onboarding

**Component Map**: GAOKAO-Exam Questions -> Expert Human Grading -> Rasch Model Analysis -> Difficulty Assessment -> LLM Evaluation -> Performance Comparison

**Critical Path**: The most critical evaluation path is: Question Selection → Expert Grading → Rasch Analysis → LLM Testing → Performance Evaluation. This sequence ensures proper difficulty calibration and fair comparison between human and model performance.

**Design Tradeoffs**: The closed-book constraint ensures fairness but may disadvantage models with strong reasoning capabilities that rely on implicit knowledge. Expert human grading provides gold standard assessment but introduces subjectivity and potential inconsistency. The comprehensive subject coverage ensures broad evaluation but increases complexity in maintaining grading standards across domains.

**Failure Signatures**: Inconsistent Score Rates (ISR) exceeding 20% indicate significant grading problems. Poor Rasch model fit (R-squared < 0.7) suggests LLM performance doesn't align with human difficulty patterns. High variance in performance on similar difficulty questions indicates pattern matching rather than understanding.

**Three First Experiments**:
1. Test a small set of questions (50-100) with multiple human graders to establish baseline consistency and calculate initial ISR.
2. Run a single representative LLM model on the full dataset to establish baseline performance patterns and identify the most problematic question types.
3. Apply the Rasch model to both human and model performance data to quantify the initial difficulty alignment and identify the magnitude of difficulty-invariant behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the semi difficulty-invariant scoring behavior of LLMs relate to their internal architecture and training objectives?
- Basis in paper: Explicit
- Why unresolved: The paper identifies the phenomenon but does not explore the underlying mechanisms or architectural features that cause it.
- What evidence would resolve it: Systematic ablation studies varying model size, architecture, and training objectives to correlate with the strength of the difficulty-invariant behavior.

### Open Question 2
- Question: Can reasoning tokens be reliably used as a proxy for LLM-aligned difficulties across diverse subject areas and question types?
- Basis in paper: Explicit
- Why unresolved: The paper shows initial promise but only tests this approach on a limited set of models and subjects.
- What evidence would resolve it: Large-scale experiments applying reasoning-token-based difficulty alignment to models across all GAOKAO-Eval subjects, comparing performance improvements.

### Open Question 3
- Question: What specific error patterns in LLM responses most significantly contribute to the mismatch between high scores and human-aligned capabilities?
- Basis in paper: Explicit
- Why unresolved: The paper identifies unique error patterns but does not quantify their impact on overall capability assessment.
- What evidence would resolve it: Detailed error analysis correlating specific mistake types with deviations from human-aligned performance curves.

## Limitations
- The benchmark is based on a single examination system (Gaokao) and may not generalize to all LLM evaluation scenarios
- High variance in performance on questions of similar difficulty indicates potential limitations in the evaluation methodology
- Inconsistent grading between models and human teachers suggests current frameworks may not accurately capture true model capabilities

## Confidence
- Benchmark validity: Medium - The approach is comprehensive but limited to one examination system
- Difficulty analysis: High - Rasch model provides strong statistical foundation for difficulty assessment
- Generalization claims: Low - Findings are based on Chinese Gaokao and may not apply to other evaluation contexts

## Next Checks
1. Test the benchmark across multiple years of Gaokao exams to assess temporal consistency and rule out year-specific patterns
2. Apply the same evaluation methodology to international standardized tests to verify generalizability beyond Chinese examination system
3. Conduct ablation studies on the impact of reasoning tokens versus traditional scoring metrics on model performance assessment to validate the proposed improvement approach