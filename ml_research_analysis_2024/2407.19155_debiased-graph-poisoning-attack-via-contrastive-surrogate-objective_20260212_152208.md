---
ver: rpa2
title: Debiased Graph Poisoning Attack via Contrastive Surrogate Objective
arxiv_id: '2407.19155'
source_url: https://arxiv.org/abs/2407.19155
tags:
- nodes
- attack
- graph
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that meta-gradient-based graph adversarial
  attacks are biased towards training nodes, resulting in uneven perturbations that
  mainly target edges involving labeled nodes and neglect potential attack opportunities
  between unlabeled nodes. This bias stems from the training procedure of the surrogate
  model, which only uses labeled nodes.
---

# Debiased Graph Poisoning Attack via Contrastive Surrogate Objective

## Quick Facts
- arXiv ID: 2407.19155
- Source URL: https://arxiv.org/abs/2407.19155
- Authors: Kanghoon Yoon; Yeonjun In; Namkyeong Lee; Kibum Kim; Chanyoung Park
- Reference count: 40
- Key outcome: The paper identifies that meta-gradient-based graph adversarial attacks are biased towards training nodes, resulting in uneven perturbations that mainly target edges involving labeled nodes and neglect potential attack opportunities between unlabeled nodes. To address this, the authors propose Metacon, a new attack method using contrastive surrogate objectives. Experiments on multiple datasets show that Metacon significantly outperforms existing meta-gradient-based attacks by considering a broader range of edges for attacks, achieving better node classification accuracy degradation.

## Executive Summary
This paper addresses a critical limitation in meta-gradient-based graph adversarial attacks: their bias towards training nodes. The authors discover that existing methods predominantly target edges connected to labeled nodes, neglecting potential attacks between unlabeled nodes. To solve this, they propose Metacon, which incorporates contrastive learning objectives into the surrogate model training. Metacon employs two approaches: sample contrastive loss (Metacon-S) and dimension contrastive loss (Metacon-D). Both methods include unlabeled nodes in the surrogate loss while aligning with the goals of the victim GNN. Experiments demonstrate that Metacon significantly outperforms existing attacks by considering a broader range of edges, achieving superior node classification accuracy degradation.

## Method Summary
The proposed method, Metacon, addresses the bias in meta-gradient-based attacks by incorporating contrastive surrogate objectives into the surrogate model training. The key innovation is using contrastive learning to include unlabeled nodes in the training procedure, which mitigates the bias towards training nodes. Metacon employs two approaches: sample contrastive loss (Metacon-S) which considers all nodes as negatives, and dimension contrastive loss (Metacon-D) which contrasts nodes dimension-wise. The surrogate model is trained with both cross-entropy loss on labeled nodes and contrastive loss on unlabeled nodes. The attack loss is then computed on unlabeled nodes, and the meta-gradient is used to determine which edges to perturb sequentially.

## Key Results
- Metacon-S achieves state-of-the-art attack performance on Cora and Citeseer datasets, significantly degrading node classification accuracy
- Metacon-D offers a computationally efficient alternative with comparable performance on larger datasets
- The method effectively considers a broader range of edges for attacks, not just those connected to labeled nodes
- Metacon demonstrates effectiveness against robust GNN models and scales to larger graphs like Reddit

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Meta-gradient-based attacks are biased towards training nodes due to the surrogate model's training procedure.
- Mechanism: The surrogate model in meta-gradient-based attacks is trained only on labeled nodes. This causes the meta-gradient to favor perturbations that affect edges connected to labeled nodes, neglecting potential attacks between unlabeled nodes.
- Core assumption: The effectiveness of an attack depends on the surrogate model's training procedure.
- Evidence anchors:
  - [abstract] "we find that in fact the prevalent meta-gradient-based attacks, which utilizes the gradient of the loss w.r.t the adjacency matrix, are biased towards training nodes."
  - [section] "The bias manifests as an uneven perturbation, connecting two nodes when at least one of them is a labeled node, i.e., training node, while it is unlikely to connect two unlabeled nodes."

### Mechanism 2
- Claim: Contrastive surrogate objectives can mitigate the bias towards training nodes.
- Mechanism: By incorporating unlabeled nodes into the surrogate loss using contrastive learning, the attack method considers a broader range of edges for potential attacks, not just those connected to labeled nodes.
- Core assumption: The surrogate loss that includes both labeled and unlabeled nodes aligns with the goals of the victim GNN.
- Evidence anchors:
  - [abstract] "Metacon employs two approaches: sample contrastive loss (Metacon-S) and dimension contrastive loss (Metacon-D), both designed to include unlabeled nodes in the surrogate model training while aligning with the goals of the victim GNN."
  - [section] "we propose a new attack method called Metacon, which alleviates the bias in meta-gradient using a new surrogate loss that includes both labeled (training) and unlabeled nodes in the training procedure."

### Mechanism 3
- Claim: The sample contrastive surrogate objective (Metacon-S) outperforms the dimension contrastive surrogate objective (Metacon-D) in terms of attack effectiveness.
- Mechanism: Metacon-S considers all nodes as negatives in the contrastive loss, leading to a more comprehensive attack search space compared to Metacon-D, which only contrasts nodes dimension-wise.
- Core assumption: A more comprehensive attack search space leads to more effective attacks.
- Evidence anchors:
  - [abstract] "Metacon significantly outperforms existing meta-gradient-based attacks by considering a broader range of edges for attacks, achieving better node classification accuracy degradation."
  - [section] "We observe that Metacon-S achieves the state-of-art attack performance on Cora and Citeseer in terms of degrading the node classification accuracy, while showing comparable performances on Cora ML."

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their vulnerability to adversarial attacks
  - Why needed here: Understanding how GNNs work and their vulnerabilities is crucial for designing effective attack methods.
  - Quick check question: What are the main types of adversarial attacks on GNNs, and how do they differ in terms of the attacker's knowledge and goals?

- Concept: Meta-gradient-based attacks and their limitations
  - Why needed here: Recognizing the limitations of existing meta-gradient-based attacks, such as the bias towards training nodes, is essential for developing improved attack methods.
  - Quick check question: How does the surrogate model's training procedure in meta-gradient-based attacks contribute to the bias towards training nodes?

- Concept: Contrastive learning and its application in GNNs
  - Why needed here: Understanding contrastive learning and how it can be applied to include unlabeled nodes in the surrogate loss is key to developing effective bias-mitigated attack methods.
  - Quick check question: What are the main differences between sample contrastive learning and dimension contrastive learning, and how do they affect the attack's effectiveness and computational complexity?

## Architecture Onboarding

- Component map:
  Surrogate model -> Contrastive surrogate objectives -> Attack loss -> Meta-gradient computation -> Edge perturbation

- Critical path:
  1. Train the surrogate model using the contrastive surrogate objective
  2. Compute the meta-gradient of the attack loss with respect to the adjacency matrix
  3. Select and flip the edge with the largest absolute meta-gradient value
  4. Repeat steps 1-3 until the perturbation budget is exhausted

- Design tradeoffs:
  - Sample contrastive loss (Metacon-S) vs. Dimension contrastive loss (Metacon-D):
    - Metacon-S: More effective attacks but higher computational cost
    - Metacon-D: Less effective attacks but lower computational cost
  - Incorporating unlabeled nodes in the surrogate loss vs. Focusing only on labeled nodes:
    - Incorporating unlabeled nodes: Broader attack search space but potentially more complex surrogate loss
    - Focusing only on labeled nodes: Narrower attack search space but simpler surrogate loss

- Failure signatures:
  - Attack effectiveness decreases when the surrogate model's contrastive objectives do not align with the victim GNN's goals
  - Computational cost becomes prohibitive for large graphs when using Metacon-S
  - Attack performance degrades when the perturbation budget is too small or too large

- First 3 experiments:
  1. Compare the attack effectiveness of Metacon-S and Metacon-D on a small graph with a limited perturbation budget
  2. Evaluate the impact of varying the coefficient of the contrastive loss (λ) on the attack performance
  3. Test the transferability of the attacked graph generated by Metacon to different GNN architectures (e.g., GraphSAGE, GAT)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the bias towards training nodes manifest differently across various GNN architectures beyond GCN, such as GAT or GraphSAGE?
- Basis in paper: [explicit] The authors note that the bias affects meta-gradient-based attacks, which are widely used, but do not extensively compare how this bias impacts different GNN architectures in their experiments.
- Why unresolved: The study primarily focuses on GCN models and their variants, leaving a gap in understanding how other architectures might be more or less susceptible to this bias.
- What evidence would resolve it: Empirical studies comparing the impact of the bias on a range of GNN architectures, including GAT, GraphSAGE, and others, under identical experimental conditions.

### Open Question 2
- Question: What are the implications of the bias towards training nodes for real-world applications of GNNs, such as social network analysis or recommendation systems?
- Basis in paper: [inferred] The paper discusses the theoretical and experimental findings of the bias but does not explore its practical implications in real-world scenarios.
- Why unresolved: While the paper provides a theoretical framework and experimental results, it does not address how these findings translate to practical applications where GNNs are used.
- What evidence would resolve it: Case studies or experiments applying the findings to real-world datasets in fields like social network analysis or recommendation systems to observe the practical impact of the bias.

### Open Question 3
- Question: How can the proposed Metacon method be adapted or extended to improve its effectiveness against adaptive attacks that specifically target its weaknesses?
- Basis in paper: [explicit] The authors demonstrate Metacon's effectiveness against standard meta-gradient-based attacks but do not explore its robustness against adaptive attacks designed to exploit its specific characteristics.
- Why unresolved: The study does not investigate how Metacon performs when adversaries are aware of its mechanisms and adapt their strategies accordingly.
- What evidence would resolve it: Experiments where adversaries are given information about Metacon's mechanisms and attempt to design attacks specifically to undermine its effectiveness, followed by iterative improvements to Metacon based on these findings.

## Limitations

- The experimental scope is limited to six relatively small datasets, with only Reddit representing a larger graph
- The method's scalability to massive graphs with millions of nodes remains unproven
- The contrastive objectives' hyperparameters are tuned empirically but sensitivity analysis is limited to a narrow range
- The paper assumes white-box access to the surrogate model, which may not reflect realistic attack scenarios

## Confidence

**High confidence**: The identification of bias towards training nodes in meta-gradient attacks is well-supported by empirical evidence and intuitive reasoning. The proposed contrastive objectives are technically sound and the theoretical framework is coherent.

**Medium confidence**: The effectiveness claims for Metacon-S and Metacon-D are supported by experiments on multiple datasets, but the magnitude of improvement varies significantly across datasets. The computational complexity analysis suggests Metacon-D is more efficient, but the trade-off between effectiveness and efficiency is not fully quantified.

**Low confidence**: The transferability claims to robust GNN models and large-scale graphs are based on limited experiments. The paper shows effectiveness against specific robust models (GCN-Jaccard, GCN-SVD) but doesn't explore a broader range of defense mechanisms or provide theoretical guarantees about transferability.

## Next Checks

1. **Scalability Test**: Evaluate Metacon on graphs with 100K+ nodes to verify computational efficiency claims and assess whether the bias-mitigation mechanism scales effectively to larger graphs.

2. **Hyperparameter Sensitivity**: Conduct a comprehensive grid search for λ values across different graph types (heterophilic, homophilic, varying densities) to establish guidelines for hyperparameter selection rather than relying on dataset-specific tuning.

3. **Defense Robustness**: Test Metacon against a broader range of defense mechanisms including preprocessing defenses, adversarial training, and certified defenses to assess the robustness of the attack method in realistic scenarios where defenses are deployed.