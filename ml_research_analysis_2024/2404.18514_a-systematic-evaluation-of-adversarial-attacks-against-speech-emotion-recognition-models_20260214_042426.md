---
ver: rpa2
title: A Systematic Evaluation of Adversarial Attacks against Speech Emotion Recognition
  Models
arxiv_id: '2404.18514'
source_url: https://arxiv.org/abs/2404.18514
tags:
- attack
- attacks
- ravdess
- emovo
- emodb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the vulnerability of speech emotion recognition
  (SER) systems to adversarial attacks across multiple languages and genders. A CNN-LSTM
  model is trained on log Mel-spectrograms extracted from German (EmoDB), Italian
  (EMOVO), and English (Ravdess) datasets.
---

# A Systematic Evaluation of Adversarial Attacks against Speech Emotion Recognition Models

## Quick Facts
- arXiv ID: 2404.18514
- Source URL: https://arxiv.org/abs/2404.18514
- Authors: Nicolas Facchinetti; Federico Simonetta; Stavros Ntalampiras
- Reference count: 40
- Primary result: All seven tested adversarial attacks significantly degrade SER model accuracy from ~90% to 1-20%

## Executive Summary
This study systematically evaluates the vulnerability of speech emotion recognition (SER) systems to adversarial attacks across German, Italian, and English datasets. The authors train a CNN-LSTM model on log Mel-spectrograms and test seven attack methods (FGSM, BIM, DeepFool, JSMA, C&W, PixelAttack, BoundaryAttack). All attacks achieve substantial accuracy degradation, with JSMA and PixelAttack proving most effective at inducing misclassification with minimal perturbations. The evaluation reveals no major differences in attack efficacy across languages, though minor gender-based variations exist. The results highlight SER systems' susceptibility to adversarial examples and emphasize the need for robust defenses.

## Method Summary
The authors construct a CNN-LSTM model for SER using log Mel-spectrograms as input features. Audio data from three datasets (EmoDB, EMOVO, Ravdess) is preprocessed by resampling to 16kHz, removing silence, and splitting into 3-second segments. Log Mel-spectrograms are extracted with 128 Mel bands, FFT window size 368, and hop size 184. The model architecture consists of three Conv2D layers (16, 32, 64 filters) with max pooling, followed by bidirectional LSTM with 256 units and a softmax output layer. Seven adversarial attack methods are implemented using the ART library with various parameter configurations to evaluate model vulnerability across languages and genders.

## Key Results
- All seven adversarial attacks (FGSM, BIM, DeepFool, JSMA, C&W, PixelAttack, BoundaryAttack) significantly reduce SER accuracy from ~90% to 1-20%
- JSMA and PixelAttack achieve the highest attack success rates with minimal perturbations
- White-box attacks are generally more successful than black-box attacks, though BoundaryAttack also performs strongly
- No substantial differences in attack efficacy across German, Italian, and English languages
- Minor gender-based variations observed, with male samples often more vulnerable to white-box attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Log-Mel spectrograms provide a compact yet discriminative representation of speech emotion that CNNs can effectively process
- Mechanism: The 2D structure of spectrograms allows convolutional filters to detect local spectral patterns associated with emotional cues while temporal dependencies are captured by LSTM layers
- Core assumption: Emotional information is sufficiently preserved in the spectral-temporal representation and is extractable via learned filters
- Evidence anchors:
  - [abstract] "We first propose a suitable methodology for audio data processing, feature extraction, and CNN-LSTM architecture."
  - [section 2.2] "We finally computed the log-Mel spectrograms of the obtained audio excerpts, a choice motivated by prior research [8]."
  - [corpus] Weak evidence; related work focuses on STAA-Net, emoDARTS, and EmoBox but does not directly validate spectrogram choice
- Break condition: If emotional cues are distributed across frequency bands in ways that violate local spatial assumptions of convolution, or if temporal dependencies are too long-range for LSTM capacity

### Mechanism 2
- Claim: Adversarial attacks degrade SER model accuracy by introducing imperceptible perturbations that exploit gradient-based decision boundaries
- Mechanism: Attack algorithms compute gradients of the loss w.r.t. input spectrograms, then add small perturbations in the direction that maximizes misclassification
- Core assumption: The model's decision boundary is differentiable and sensitive to small input changes
- Evidence anchors:
  - [abstract] "all the considered adversarial attacks are able to significantly reduce the performance of the constructed models."
  - [section 2.4] Multiple attacks (FGSM, BIM, DeepFool, etc.) explicitly rely on gradient-based or gradient-free optimization to find perturbations
  - [corpus] No direct evidence; related work focuses on adversarial attacks but not specific to SER spectrograms
- Break condition: If model uses non-differentiable operations, defenses like adversarial training or input quantization, or if perturbations exceed human perceptibility thresholds

### Mechanism 3
- Claim: The CNN-LSTM architecture generalizes across languages because emotional acoustic patterns are universal enough to be learned from spectrograms
- Mechanism: Shared CNN feature extractors learn language-agnostic spectral features while LSTMs model temporal dynamics
- Core assumption: Emotional expression in speech has cross-linguistic acoustic correlates that are detectable in the spectral domain
- Evidence anchors:
  - [abstract] "minor differences were noted between the languages analyzed" and evaluation across German, Italian, and English
  - [section 2.2] Use of the same CNN-LSTM architecture across all three datasets without language-specific adaptation
  - [corpus] Related work (EmoBox, cross-lingual SER) supports multilingual generalization but does not validate CNN-LSTM specifically
- Break condition: If emotional expression is too culturally specific or if language-specific phonetic content interferes with emotional feature extraction

## Foundational Learning

- Concept: Log-Mel spectrogram computation
  - Why needed here: Provides the input representation that the CNN-LSTM model processes; understanding its parameters (128 Mel bands, window size 368, hop size 184) is critical for interpreting attack perturbations
  - Quick check question: Why use 128 Mel bands instead of raw FFT bins, and how does the window size affect temporal resolution?

- Concept: CNN-LSTM model architecture
  - Why needed here: The vulnerability to attacks depends on the model's depth, filter sizes, LSTM units, and normalization
  - Quick check question: How does changing from unidirectional to bidirectional LSTM affect the model's sensitivity to perturbations?

- Concept: Adversarial attack types (white-box vs. black-box)
  - Why needed here: Different attacks require different levels of access to the model (gradients vs. output only), and their effectiveness varies based on the attack method and distance metric used
  - Quick check question: What is the practical difference between L0, L2, and L∞ perturbations in terms of human perceptibility and attack success?

## Architecture Onboarding

- Component map: Input (log Mel-spectrograms) -> 3 Conv2D layers with max pooling -> BatchNormalization -> Bidirectional LSTM -> Dense softmax output
- Critical path: Spectrogram extraction → normalization → model input → CNN feature extraction → temporal modeling via LSTM → classification → loss computation
- Design tradeoffs: Small CNN vs. deeper networks (chosen for efficiency and to avoid overfitting), bidirectional LSTM (better context but more parameters), standardization vs. max normalization (affects feature scaling)
- Failure signatures: High validation loss with low training loss (overfitting), slow convergence or oscillation (learning rate/batch size issues), attacks succeed easily (lack of adversarial robustness)
- First 3 experiments:
  1. Train base M0 model on one dataset (e.g., EmoDB) with different normalizations; compare accuracy and loss curves
  2. Add BatchNormalization to M0 and measure impact on training stability and final accuracy
  3. Implement FGSM attack on trained M0; vary eps parameter and observe accuracy degradation and perturbation magnitude

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the vulnerability of SER models to adversarial attacks depend more on the model architecture or the underlying speech data representation?
- Basis in paper: [inferred] The authors found that all tested attacks significantly degraded SER model performance regardless of language or gender, but noted differences in effectiveness across attack types and model configurations
- Why unresolved: The study focused on a single CNN-LSTM architecture and log Mel-spectrograms, limiting the ability to isolate the impact of architectural choices versus data representation
- What evidence would resolve it: Comparing attack effectiveness across multiple model architectures (e.g., CNN, LSTM, Transformer) and speech representations (e.g., raw waveforms, MFCCs, spectrograms) would clarify the relative contributions of architecture and data representation to vulnerability

### Open Question 2
- Question: How do real-world noise conditions and speaker variability affect the transferability and effectiveness of adversarial examples in SER?
- Basis in paper: [inferred] The study used clean, controlled datasets and did not evaluate attacks under realistic conditions or with diverse speaker populations
- Why unresolved: The experiments were conducted in an idealized setting without accounting for real-world factors that could influence attack success
- What evidence would resolve it: Testing attacks on noisy, multi-speaker datasets with varying recording conditions would reveal how environmental and speaker factors impact adversarial example effectiveness

### Open Question 3
- Question: Can adversarial training or other defense mechanisms effectively mitigate attacks on SER models without significantly degrading clean data performance?
- Basis in paper: [explicit] The authors suggest that exploring more robust models or alternative training data is needed to enhance system robustness, but do not investigate specific defense strategies
- Why unresolved: The study focused on attack vulnerability but did not evaluate potential defenses or their impact on model performance
- What evidence would resolve it: Implementing and evaluating defense mechanisms (e.g., adversarial training, input preprocessing, model ensemble) on SER models, while measuring both attack resilience and clean data accuracy, would clarify their practical viability

## Limitations

- The study uses only three datasets, limiting generalizability across languages and emotional expression styles
- Attack configurations are fixed based on prior work rather than being optimized for the specific SER task
- The evaluation does not account for potential perceptual thresholds of adversarial perturbations in audio
- Analysis of gender-based differences is preliminary and lacks statistical significance testing

## Confidence

- **High confidence**: CNN-LSTM model achieves strong baseline performance (>90% accuracy) across languages when trained on log Mel-spectrograms
- **Medium confidence**: All seven adversarial attack methods significantly degrade SER accuracy; JSMA and PixelAttack are most effective
- **Medium confidence**: No substantial differences in attack efficacy across languages, with minor variations observed
- **Low confidence**: Gender-based vulnerability differences require further statistical validation

## Next Checks

1. **Replicate across model architectures**: Test the same attack suite on alternative SER models (e.g., transformer-based, attention mechanisms) to determine if vulnerability patterns are architecture-specific

2. **Implement perceptual validation**: Conduct listening tests to establish human detection thresholds for adversarial perturbations and correlate with attack success rates

3. **Statistical significance analysis**: Perform rigorous statistical testing on gender-based vulnerability differences and cross-linguistic performance variations to validate observed trends