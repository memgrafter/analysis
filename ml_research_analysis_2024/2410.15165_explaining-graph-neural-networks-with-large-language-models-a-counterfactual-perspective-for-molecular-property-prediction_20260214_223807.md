---
ver: rpa2
title: 'Explaining Graph Neural Networks with Large Language Models: A Counterfactual
  Perspective for Molecular Property Prediction'
arxiv_id: '2410.15165'
source_url: https://arxiv.org/abs/2410.15165
tags:
- graph
- counterfactual
- text
- llm-gce
- validity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-GCE, a novel framework for explaining
  Graph Neural Networks (GNNs) in molecular property prediction tasks using Large
  Language Models (LLMs). The method addresses the challenge of generating human-comprehensible
  counterfactual explanations for GNN predictions by leveraging LLMs' reasoning abilities
  and domain knowledge.
---

# Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective for Molecular Property Prediction

## Quick Facts
- arXiv ID: 2410.15165
- Source URL: https://arxiv.org/abs/2410.15165
- Reference count: 24
- Key outcome: LLM-GCE framework generates chemically feasible counterfactual explanations for GNN molecular property predictions using LLM reasoning and dynamic feedback

## Executive Summary
This paper introduces LLM-GCE, a novel framework for explaining Graph Neural Networks (GNNs) in molecular property prediction tasks using Large Language Models (LLMs). The method addresses the challenge of generating human-comprehensible counterfactual explanations for GNN predictions by leveraging LLMs' reasoning abilities and domain knowledge. LLM-GCE employs a counterfactual autoencoder to generate graph structures from text pairs produced by LLMs, while incorporating a dynamic feedback mechanism to mitigate hallucinations. Extensive experiments on five real-world molecular datasets demonstrate that LLM-GCE outperforms state-of-the-art baselines in terms of validity and proximity metrics, particularly when chemical feasibility is considered.

## Method Summary
LLM-GCE combines LLM reasoning with GNN explainability through a three-module architecture. First, a contrastive pretraining phase aligns text encoder embeddings with GT-GNN embeddings using symmetric cross-entropy loss. Second, a counterfactual autoencoder generates molecular graph structures from text pairs, using the aligned embeddings to guide the graph decoder. Third, a dynamic CTP feedback module iteratively refines counterfactual text pairs by incorporating GT-GNN predictions, reducing hallucinations and improving quality. The framework uses GPT-4 to generate text pairs describing graph structure, labels, and significant subgraphs, which are then converted to counterfactual molecular graphs through the autoencoder while ensuring chemical feasibility via valence-bond theory checks.

## Key Results
- LLM-GCE outperforms state-of-the-art baselines in validity and proximity metrics across five molecular datasets
- The dynamic CTP feedback mechanism significantly reduces hallucination rates in generated counterfactuals
- Chemical feasibility constraints improve the practical utility of counterfactual explanations in molecular domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-GCE leverages LLM's strong reasoning ability to generate chemically feasible counterfactuals that are more faithful to the original molecule structure.
- Mechanism: The CTP dynamic feedback module iteratively refines counterfactual text pairs by incorporating GT-GNN predictions, reducing hallucinations and improving the quality of generated counterfactuals.
- Core assumption: LLM's extensive pretraining provides sufficient domain knowledge to generate chemically meaningful counterfactuals when guided properly.
- Evidence anchors:
  - [abstract]: "LLM-GCE employs a dynamic feedback mechanism to mitigate hallucinations"
  - [section 4.4]: "To remedy this, a CTP dynamic feedback scheme is designed to calibrate CTP with the CA-generated counterfactuals"
  - [corpus]: No direct evidence - this is a novel contribution not covered in related work

### Mechanism 2
- Claim: The counterfactual autoencoder bridges the gap between text-based LLM reasoning and graph-based molecular structures.
- Mechanism: The CA uses pretrained text encoder embeddings aligned with GT-GNN embeddings to guide graph decoder in generating structurally accurate counterfactuals.
- Core assumption: Text encoder embeddings contain sufficient structural information when aligned with graph embeddings through contrastive pretraining.
- Evidence anchors:
  - [section 4.3.3]: "the embedding of the CTP approximates ˆG's GT-GNN embedding, which guides counterfactual decoding"
  - [section 4.2]: "we employ a contrastive learning strategy to align the text encoder's embeddings with the GT-GNN's embeddings"
  - [corpus]: Related work shows GNNs excel at molecular property prediction, supporting the need for graph-based representation

### Mechanism 3
- Claim: Incorporating domain-specific knowledge through valence bond theory improves the validity of generated counterfactuals.
- Mechanism: The feasibility check filters counterfactuals that violate chemical stability rules, ensuring only chemically valid molecules are considered.
- Core assumption: Chemical feasibility constraints are essential for meaningful counterfactual explanations in molecular domains.
- Evidence anchors:
  - [abstract]: "The approach shows promising results in generating more faithful and chemically stable counterfactuals"
  - [section 5.1.1]: "we only calculate the two metrics to the set of feasible counterfactuals ˆGs, i.e., those counterfactuals that are chemically stable according to valence-bond theory"
  - [corpus]: Related work on molecular property prediction emphasizes chemical validity

## Foundational Learning

- Concept: Graph Neural Networks for molecular property prediction
  - Why needed here: GNNs are the black-box models being explained by counterfactual methods
  - Quick check question: What is the primary advantage of using GNNs for molecular graphs compared to traditional ML methods?

- Concept: Large Language Models and their reasoning capabilities
  - Why needed here: LLMs provide the reasoning ability to generate chemically meaningful counterfactuals
  - Quick check question: How do LLMs differ from traditional language models in terms of reasoning and domain knowledge?

- Concept: Counterfactual explanations and their evaluation metrics
  - Why needed here: The paper's contribution is a new method for generating graph counterfactual explanations
  - Quick check question: What are the key differences between validity and proximity metrics in counterfactual explanation evaluation?

## Architecture Onboarding

- Component map: Text Encoder → Contrastive Pretraining → Text Encoder → Counterfactual Autoencoder → Graph Decoder → Feasibility Check → Valid Counterfactual
- Critical path: Input graph → CTP generation → Text encoder embedding → Latent space combination → Graph decoder → Feasibility check → Valid counterfactual
- Design tradeoffs: Using LLMs adds computational overhead but provides better domain knowledge integration; using contrastive pretraining improves alignment but requires additional training data
- Failure signatures: Low validity indicates issues with CTP generation or autoencoder; low proximity suggests poor structural preservation; high hallucination rates indicate problems with feedback mechanism
- First 3 experiments:
  1. Test basic counterfactual generation without feedback mechanism to establish baseline performance
  2. Evaluate impact of contrastive pretraining on validity and proximity metrics
  3. Measure hallucination rates with and without dynamic feedback iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-GCE be adapted to incorporate 3D molecular structures rather than just 2D graphs?
- Basis in paper: [explicit] The paper mentions "we leave the GCE for 3-D molecule graphs for future work" and discusses the importance of 3D structure in determining molecular properties.
- Why unresolved: The current framework focuses on 2D molecular graphs, which may limit its accuracy and applicability in real-world scenarios where 3D structure is crucial.
- What evidence would resolve it: Successful implementation and experimental validation of LLM-GCE on 3D molecular structures, demonstrating improved accuracy and interpretability compared to 2D-only approaches.

### Open Question 2
- Question: What techniques can be developed to improve the computational efficiency and scalability of LLM-GCE for large-scale graphs?
- Basis in paper: [inferred] The paper discusses the computational cost of using large language models and mentions that "LLM-GCE can be time consuming when dealing with extremely large-scale graphs, such as some big proteins."
- Why unresolved: The current implementation may be computationally expensive and time-consuming for large-scale graphs, limiting its practical applicability.
- What evidence would resolve it: Development and demonstration of techniques (e.g., knowledge distillation, model compression) that significantly reduce the computational cost and improve the scalability of LLM-GCE without compromising its performance.

### Open Question 3
- Question: How can the interpretability and usefulness of the generated counterfactual explanations be evaluated through human studies?
- Basis in paper: [inferred] The paper mentions the importance of generating human-comprehensible counterfactuals and suggests "future work could involve human evaluation and user studies."
- Why unresolved: While the paper demonstrates the effectiveness of LLM-GCE through quantitative metrics, it does not provide insights into how domain experts perceive and utilize the generated counterfactual explanations.
- What evidence would resolve it: Results from human evaluation studies and user feedback, demonstrating the interpretability, usefulness, and potential impact of LLM-GCE's counterfactual explanations on decision-making processes in molecular property prediction tasks.

## Limitations
- LLM hallucination remains a fundamental challenge despite the dynamic feedback mechanism, potentially limiting the reliability of generated counterfactuals
- The framework's dependence on GPT-4 introduces computational overhead and API access constraints that may limit practical deployment
- Chemical feasibility evaluation relies solely on valence-bond theory, which may not capture all relevant molecular stability considerations

## Confidence
- **High confidence**: The overall framework architecture and experimental methodology are sound and well-justified
- **Medium confidence**: The effectiveness of the CTP dynamic feedback mechanism in reducing hallucinations is demonstrated but could benefit from additional ablation studies
- **Medium confidence**: The claim of superior performance over baselines is supported by experiments but depends on specific implementation choices

## Next Checks
1. Conduct systematic ablation studies to quantify the contribution of each component (contrastive pretraining, dynamic feedback, feasibility check) to overall performance
2. Test the framework's robustness to different LLM models and prompt variations to assess dependency on specific GPT-4 implementations
3. Evaluate the generated counterfactuals with domain experts to verify their chemical meaningfulness and explanatory value beyond automated metrics