---
ver: rpa2
title: Leveraging Large Language Models for Medical Information Extraction and Query
  Generation
arxiv_id: '2410.23851'
source_url: https://arxiv.org/abs/2410.23851
tags:
- clinical
- retrieval
- medical
- query
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates that small, open-source large language\
  \ models (LLMs) can effectively generate queries for clinical trial retrieval, achieving\
  \ performance comparable to medical experts and larger closed-source models like\
  \ GPT3.5. Using six LLMs\u2014four open-source and two closed-source\u2014the researchers\
  \ found that models such as Qwen2-7B-Instruct and Phi3-medium-4k-Instruct generated\
  \ queries with retrieval effectiveness (nDCG@10, P@10, Bpref) on par with expert-created\
  \ queries and superior to standard baselines."
---

# Leveraging Large Language Models for Medical Information Extraction and Query Generation

## Quick Facts
- arXiv ID: 2410.23851
- Source URL: https://arxiv.org/abs/2410.23851
- Reference count: 40
- Small open-source LLMs can generate clinical trial queries matching expert performance

## Executive Summary
This study demonstrates that small, open-source large language models can effectively generate queries for clinical trial retrieval, achieving performance comparable to medical experts and larger closed-source models like GPT3.5. Using six LLMs—four open-source and two closed-source—the researchers found that models such as Qwen2-7B-Instruct and Phi3-medium-4k-Instruct generated queries with retrieval effectiveness on par with expert-created queries and superior to standard baselines. These models also offered fast response times and produced manageable query lengths, making them practical for real-world deployment. The approach addresses privacy concerns by running locally within medical institutions, allowing expert oversight while maintaining data security.

## Method Summary
The researchers generated queries from synthetic clinical notes using six LLMs (four open-source: Qwen2-7B-Instruct, Phi3-medium-4k-Instruct, Phi3-mini-4k-Instruct, Medical-Llama3-8B; two closed-source: GPT3.5, GPT4). Queries were processed through comma-separated keyword extraction, cleaned with Porter stemming, and concatenated with original queries before BM25 retrieval. Performance was evaluated using nDCG@10, P@10, MRR, Bpref, and R@25 metrics on clinical trial documents, comparing against expert-created queries and standard baselines.

## Key Results
- Qwen2-7B-Instruct and Phi3-medium-4k-Instruct achieved retrieval performance comparable to expert-created queries
- Open-source models outperformed standard baselines and matched larger closed-source models
- Query concatenation consistently improved retrieval performance over using generated queries alone
- Models demonstrated fast response times (1.7–8 seconds) and reasonable query lengths (15–63 terms)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small open-source LLMs can generate effective clinical trial queries comparable to expert-created queries
- Mechanism: By fine-tuning or prompting LLMs with structured medical information, they can extract relevant keywords and generate search queries that capture patient eligibility criteria
- Core assumption: The clinical notes contain sufficient structured information that LLMs can parse to create meaningful queries
- Evidence anchors:
  - The paper states that models such as Qwen2-7B-Instruct and Phi3-medium-4k-Instruct generated queries with retrieval effectiveness on par with expert-created queries
  - "Open-source models, particularly Qwen2-7B-Instruct and Phi3-medium-4k-Instruct, achieve a retrieval performance comparable to that of both larger closed-source models like GPT3.5 and several experts"
- Break condition: If clinical notes lack sufficient structured information or contain too much ambiguity, LLMs may fail to generate effective queries

### Mechanism 2
- Claim: Running LLMs locally within medical institutions addresses privacy concerns
- Mechanism: By processing patient data locally, sensitive information is not transmitted to external servers, maintaining compliance with data privacy regulations
- Core assumption: Local deployment of LLMs is feasible within the computational constraints of medical institutions
- Evidence anchors:
  - The paper mentions that the approach addresses privacy concerns by running locally within medical institutions
  - "Our approach aims to overcome resource constraints and infrastructure limitations often encountered in these settings. Specifically, our experiments utilized a maximum of 30GB of memory running on a machine equipped with an A6000 GPU"
- Break condition: If local computational resources are insufficient or if regulatory requirements change, this mechanism may fail

### Mechanism 3
- Claim: Combining LLM-generated queries with original queries improves retrieval performance
- Mechanism: Concatenating the original clinical note with the LLM-generated query provides a richer input for the IR system, enhancing the retrieval of relevant clinical trials
- Core assumption: The original clinical note contains additional relevant information that complements the LLM-generated query
- Evidence anchors:
  - "The concatenation approach consistently outperforms the LLM-generated queries alone"
  - "In the Tables I, II, and III, we