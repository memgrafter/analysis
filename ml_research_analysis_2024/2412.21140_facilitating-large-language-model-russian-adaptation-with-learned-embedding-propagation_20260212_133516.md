---
ver: rpa2
title: Facilitating large language model Russian adaptation with Learned Embedding
  Propagation
arxiv_id: '2412.21140'
source_url: https://arxiv.org/abs/2412.21140
tags:
- language
- embedding
- which
- original
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Learned Embedding Propagation (LEP), a novel
  method for adapting large language models (LLMs) to new languages without access
  to high-quality instruction-tuning data. Unlike traditional approaches that require
  full model retraining or instruction-tuning, LEP minimizes disruption to existing
  LLM knowledge by directly implanting new language knowledge through an ad-hoc embedding
  propagation procedure.
---

# Facilitating large language model Russian adaptation with Learned Embedding Propagation

## Quick Facts
- arXiv ID: 2412.21140
- Source URL: https://arxiv.org/abs/2412.21140
- Reference count: 40
- Primary result: LEP achieves competitive performance to instruction-tuned models for Russian adaptation without requiring instruction-tuning data

## Executive Summary
This paper introduces Learned Embedding Propagation (LEP), a novel method for adapting large language models to new languages without access to high-quality instruction-tuning data. Unlike traditional approaches that require full model retraining or instruction-tuning, LEP minimizes disruption to existing LLM knowledge by directly implanting new language knowledge through an ad-hoc embedding propagation procedure. The method was evaluated by adapting LLaMa-3-8B and Mistral-7B for Russian using four different vocabulary adaptation strategies. Results showed that LEP achieves competitive performance compared to state-of-the-art instruction-tuned models like OpenChat 3.5 and LLaMa-3-8B-Instruct, with further improvements possible through self-calibration and continued tuning. The approach offers a cost-effective alternative for language adaptation while maintaining or exceeding the performance of contemporary LLMs.

## Method Summary
Learned Embedding Propagation (LEP) is a novel approach for adapting large language models to new languages without requiring instruction-tuning data. The method works by training a base model on new language data, then using an ad-hoc embedding propagation procedure to transfer this knowledge to an instruction-tuned model. LEP employs linear transformations to align embeddings from continued pre-training with instruction-tuned embeddings, assuming that the optimal intermediate layer composition remains unchanged during adaptation. The approach includes four vocabulary adaptation strategies: BPE (full vocabulary substitution), Unigram (full substitution with different algorithm), Extension (merging new tokens with existing vocabulary), and Optimization (similar to extension but with learned token weights). After embedding propagation, optional self-calibration using synthetic instruction data can recover performance losses from language adaptation.

## Key Results
- LEP-adapted models achieved 85.2% accuracy on Russian MultiTurn, compared to 86.2% for LLaMa-3-8B-Instruct and 85.3% for OpenChat 3.5
- Extended and Optimized tokenization variants showed the highest language-adaptation scores, outperforming full substitution methods
- Self-calibration improved DaruCopy task performance from 79.6% to 97.8% accuracy on LLaMa-3-8B-Instruct
- LEP provided a cost-effective alternative to full instruction-tuning while maintaining competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learned Embedding Propagation (LEP) transfers language adaptation knowledge from a pre-adapted base model to an instruction-tuned target model through linear transformation of embeddings.
- Mechanism: The method assumes that the optimal intermediate layer composition Wru→inst ≈ Winst, allowing embedding propagation via linear transformations Dru_inst and Uru_inst that align embeddings from continued pre-training with instruction-tuned embeddings.
- Core assumption: The fine-tuning trajectory can be approximated by linear transformations of base model parameters, and the optimal intermediate layers remain unchanged during adaptation.
- Evidence anchors:
  - [abstract] "LEP employs an innovative embedding propagation technique, bypassing the need for instruction-tuning and directly integrating new language knowledge into any instruct-tuned LLM variant"
  - [section] "Ilharco et al. (2023) showed that the fine-tuning trajectory may be approximated with linear transformations of base model parameters"
  - [corpus] Weak evidence - only 0 related citations found, suggesting this approach is novel
- Break condition: If the assumption that Wru→inst ≈ Winst fails, the embedding propagation will produce poor alignment and degraded performance.

### Mechanism 2
- Claim: Vocabulary adaptation through extension and optimization methods provides more stable embedding initialization compared to full vocabulary substitution.
- Mechanism: Extended and Optimized tokenization variants merge new Russian tokens with existing English vocabulary, creating a gradual transition that maintains semantic coherence while adding language-specific tokens.
- Core assumption: Partial vocabulary extension preserves more of the original embedding semantics than complete substitution, reducing the optimization difficulty during continued pre-training.
- Evidence anchors:
  - [section] "Extended and Optimized tokenization variants...merge with top 64000 most common tokens from new BPE vocabulary"
  - [section] "vocabulary extension methods such as Extended and Optimized have the lowest optimization difficulty as they show the highest language-adaptation scores"
  - [corpus] Weak evidence - only 0 related citations found, suggesting this approach is novel
- Break condition: If the merged vocabulary becomes too large or contains conflicting tokens, the optimization process may fail to converge or produce suboptimal embeddings.

### Mechanism 3
- Claim: Self-calibration using synthetic instruction data generated by the original instruction-tuned model can recover performance losses from language adaptation.
- Mechanism: The method generates calibration examples using prompts from existing instruction datasets and greedy decoding from the instruction-tuned LLM, then fine-tunes the language-adapted model on this synthetic data.
- Core assumption: The synthetic examples generated by the original instruction-tuned model maintain sufficient quality and diversity to effectively calibrate the adapted model.
- Evidence anchors:
  - [section] "we used prompts from Saiga instruction dataset and used greedy decoding to get the most likely answer from instruct-tuned LLM viewpoint"
  - [section] "the performance of DaruCopy tasks improved substantially, practically reaching the perfect reliability levels"
  - [corpus] Weak evidence - only 0 related citations found, suggesting this approach is novel
- Break condition: If the synthetic data lacks diversity or quality, the calibration process may reinforce biases rather than improve performance.

## Foundational Learning

- Concept: Tokenization algorithms and their impact on language model performance
  - Why needed here: The paper evaluates four different tokenization strategies (BPE, Unigram, Extension, Optimization) and their effects on Russian adaptation performance
  - Quick check question: How does tokenization affect the number of tokens per word and what impact does this have on model performance?

- Concept: Embedding initialization and its role in transfer learning
- Concept: Continued pre-training and its relationship to fine-tuning
  - Why needed here: The paper uses continued pre-training of embeddings only as a middle ground between full model training and instruction-tuning
  - Quick check question: Why might continued pre-training of embeddings only be more efficient than full model training for language adaptation?

## Architecture Onboarding

- Component map: Tokenization training -> Embedding initialization -> Continued pre-training -> LEP transformation -> Optional calibration
- Critical path: Tokenization → Embedding initialization → Continued pre-training → LEP transformation → Optional calibration

## Open Questions the Paper Calls Out
None

## Limitations

- The fundamental assumption that embedding propagation through linear transformations can adequately capture complex linguistic relationships may not generalize to languages with significantly different morphological structures or character systems
- Vocabulary adaptation strategy effectiveness may be specific to Russian language and particular English base models used, with untested effectiveness for agglutinative languages or non-Latin scripts
- Self-calibration relies on greedy decoding from original instruction-tuned model, which may limit diversity and quality of synthetic data, potentially reinforcing existing biases

## Confidence

- LEP embedding propagation mechanism: Medium - shows promise but lacks extensive validation across diverse language families
- Vocabulary strategy superiority claims: Medium - primarily empirical evidence without theoretical guarantees
- Self-calibration effectiveness: Low - limited evaluation scope and potential for synthetic data to reinforce biases

## Next Checks

1. Cross-linguistic validation: Test LEP on languages with vastly different morphological complexity (e.g., Turkish, Finnish, or Japanese) to verify whether the embedding propagation mechanism generalizes beyond Slavic languages with Cyrillic scripts.

2. Vocabulary size sensitivity: Systematically vary the number of new tokens added during extension/optimization and measure the trade-off between performance gains and vocabulary management complexity across different base model sizes.

3. Long-context evaluation: Assess LEP-adapted models on tasks requiring extended context (beyond 4096 tokens) to determine whether the embedding propagation approach maintains coherence in long-form generation scenarios.