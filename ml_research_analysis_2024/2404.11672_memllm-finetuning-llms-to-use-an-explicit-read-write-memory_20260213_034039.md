---
ver: rpa2
title: 'MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory'
arxiv_id: '2404.11672'
source_url: https://arxiv.org/abs/2404.11672
tags:
- memory
- relation
- language
- knowledge
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MemLLM introduces a novel approach to enhancing large language
  models by integrating a structured, explicit read-write memory module. Unlike traditional
  LLMs that rely solely on parametric memory, MemLLM enables dynamic interaction with
  an interpretable and editable memory component, improving performance on knowledge-intensive
  tasks and reducing hallucinations.
---

# MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory

## Quick Facts
- **arXiv ID**: 2404.11672
- **Source URL**: https://arxiv.org/abs/2404.11672
- **Reference count**: 40
- **Primary result**: Finetuning Mistral-7B with structured triple-based memory improves language modeling perplexity and knowledge editing performance

## Executive Summary
MemLLM introduces a novel approach to enhancing large language models by integrating a structured, explicit read-write memory module. Unlike traditional LLMs that rely solely on parametric memory, MemLLM enables dynamic interaction with an interpretable and editable memory component, improving performance on knowledge-intensive tasks and reducing hallucinations. The model is trained to extract knowledge from text and write it to memory using finetuning with a structured triple-based schema, and to retrieve relevant information when needed. Evaluations on the Re-DocRED dataset demonstrate significant improvements in language modeling perplexity, particularly for named entities, and strong performance in knowledge editing tasks, outperforming state-of-the-art methods.

## Method Summary
The method involves finetuning a Mistral-7B model using LoRA to interact with an explicit read-write memory module. The memory stores information in structured triples (subject, relation, object) extracted from text using a memory-write model. A memory-read model generates queries to retrieve relevant information from the memory during text generation. The training data is created from annotated corpora like Re-DocRED, with structured API commands for memory operations. The model learns to determine when to read from memory and how to incorporate retrieved information into its outputs.

## Key Results
- MemLLM achieves significant improvements in language modeling perplexity, especially for named entities (ENTITY PPL)
- Strong performance in knowledge editing tasks, outperforming state-of-the-art methods on reliability, generalization, and locality metrics
- Memory efficiency benefits demonstrated by structured triple storage compared to proposition-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning with structured triple-based schema teaches LLMs to extract and write knowledge into explicit memory
- Mechanism: The model learns to parse focus sentences and generate API calls that store extracted relations in a structured triple format (subject, relation, object)
- Core assumption: Structured schema enables better interpretability and scalability compared to parametric memory
- Evidence anchors:
  - [abstract]: "Our experiments indicate that MemLLM enhances the LLM's performance and interpretability, in language modeling in general and knowledge-intensive tasks in particular"
  - [section 3.3]: "We create these API training data from corpora annotated with entities and relations, including Re-DocRED"
  - [corpus]: Weak - no direct corpus evidence for schema effectiveness
- Break condition: If the structured schema cannot capture the complexity of natural language relations or becomes too rigid for practical use

### Mechanism 2
- Claim: Memory-read API calls improve language modeling by retrieving relevant information at optimal points
- Mechanism: The model learns to identify when to initiate memory reads and generate effective queries that retrieve helpful information
- Core assumption: Retrieving relevant information during generation reduces hallucinations and improves factual accuracy
- Evidence anchors:
  - [abstract]: "improving the LLM's capabilities in using stored knowledge"
  - [section 3.2]: "The LLM has both read and write access to the explicit memory component"
  - [corpus]: Weak - limited evidence of memory-read effectiveness in real applications
- Break condition: If memory reads become too frequent and disrupt natural text flow, or if queries return too much irrelevant information

### Mechanism 3
- Claim: Memory-write model creates a comprehensive knowledge base from text that improves retrieval performance
- Mechanism: The model extracts relations from documents and stores them in structured triples, creating a scalable knowledge base
- Core assumption: Structured triple storage is more efficient and interpretable than proposition-based storage
- Evidence anchors:
  - [section 4.2]: "Memory redundancy reduction benefits. Instead of storing facts in a structured triple format, each fact could be stored as its own proposition"
  - [section 3.1]: "The memory stores information in relation triples"
  - [corpus]: Weak - no direct evidence comparing triple vs proposition storage efficiency
- Break condition: If the memory-write process has low precision/recall, leading to incomplete or incorrect knowledge base

## Foundational Learning

- Concept: Structured data modeling (relational databases)
  - Why needed here: Understanding how to design and query structured triple schemas is essential for implementing the memory component
  - Quick check question: How would you design a database schema to store the fact "Paris is the capital of France"?

- Concept: Natural language processing for relation extraction
  - Why needed here: The memory-write model must accurately extract relations from text, requiring understanding of NLP techniques
  - Quick check question: What are the challenges in extracting relations from sentences with pronouns or implicit relationships?

- Concept: Information retrieval and vector similarity
  - Why needed here: The memory-read process uses vector similarity to find relevant entities and relations, requiring understanding of IR concepts
  - Quick check question: How does cosine similarity between entity embeddings help in retrieving relevant information?

## Architecture Onboarding

- Component map:
  - LLM (Mistral-7B) -> Memory-write model -> Triple Memory (entities, relations, triples tables) -> Memory-read model -> LLM

- Critical path:
  1. Extract relations from text using memory-write model
  2. Store relations in structured triple format
  3. Generate memory-read queries at optimal points
  4. Retrieve relevant information using vector similarity
  5. Incorporate retrieved information into text generation

- Design tradeoffs:
  - Structured vs unstructured memory: Structured provides better interpretability but requires more complex schema design
  - Precision vs recall in relation extraction: Higher precision reduces noise but may miss important information
  - Memory size vs query efficiency: Larger memory provides more knowledge but slower retrieval

- Failure signatures:
  - Memory-write errors: Missing relations, incorrect extractions, redundant storage
  - Memory-read failures: Poor query generation, irrelevant results, empty returns
  - Integration issues: Model ignoring memory results, excessive memory reads, incorrect incorporation

- First 3 experiments:
  1. Test relation extraction on sample sentences to verify memory-write accuracy
  2. Verify memory-read query generation and result retrieval on known triples
  3. Evaluate perplexity improvement on small text corpus with populated memory

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the similarity thresholds (τe, τt, τr) impact the precision and recall of the memory retrieval, and what is the optimal threshold setting for different knowledge domains?
- Basis in paper: [explicit] The paper discusses the impact of similarity thresholds on memory retrieval in section 4.2, noting that a larger τe increases explicitness but limits the number of similarly mentioned entities that can be retrieved.
- Why unresolved: The paper does not provide a systematic study on the impact of different threshold settings on retrieval performance. It only mentions that the thresholds were set based on initial tests and the 95th percentile of sentences in the Re-DocRED dataset.
- What evidence would resolve it: A comprehensive ablation study varying the similarity thresholds and evaluating their impact on precision, recall, and overall retrieval performance across different knowledge domains.

### Open Question 2
- Question: How does the performance of MemLLM scale with the size of the memory, and what is the maximum size of the memory that can be effectively managed by the model?
- Basis in paper: [explicit] The paper discusses the memory efficiency of storing structured triples vs. proposition-based storage in section 4.2, noting that the structured triple approach results in significantly lower memory usage.
- Why unresolved: The paper does not provide a systematic study on the impact of memory size on model performance. It only mentions that there is a relatively small negative effect of memory size when comparing the full Wikipedia memory with the memory stripped down to Re-DocRED test relations.
- What evidence would resolve it: An evaluation of MemLLM's performance on language modeling and knowledge editing tasks as the size of the memory increases, identifying the point at which the model's performance plateaus or degrades.

### Open Question 3
- Question: How does the performance of MemLLM compare to other memory-augmented LLMs that use unstructured storage, such as Retrieval-Augmented Generation (RAG) methods?
- Basis in paper: [explicit] The paper mentions that RAG methods can provide updated facts but their unstructured storage complicates fact editing in the introduction.
- Why unresolved: The paper does not include a direct comparison between MemLLM and RAG methods in terms of language modeling and knowledge editing performance.
- What evidence would resolve it: A head-to-head comparison of MemLLM and RAG methods on a common benchmark, evaluating their performance on language modeling perplexity, knowledge editing reliability, generalization, and locality.

## Limitations
- Evaluation scope limited to single dataset (Re-DocRED) and knowledge editing tasks, lacking broader applicability testing
- No analysis of computational overhead or latency introduced by memory operations for practical deployment
- Model's behavior with incomplete or contradictory knowledge in memory not explored, raising robustness concerns

## Confidence
- **High Confidence**: The core architecture design (separating memory-write and memory-read components) is well-specified and theoretically sound. The improvements in perplexity for named entities and knowledge editing performance are directly measurable and demonstrated.
- **Medium Confidence**: The mechanism claims about how structured triples improve interpretability and scalability are supported by design arguments but lack direct empirical comparison with alternative memory structures. The assumption that memory-read queries improve factual accuracy is plausible but not extensively validated across diverse scenarios.
- **Low Confidence**: Claims about general reduction of hallucinations beyond named entities are not thoroughly tested. The long-term scalability of the approach with continuously growing memory is not addressed.

## Next Checks
1. **Memory Scaling Experiment**: Evaluate performance as memory size grows from 1K to 1M triples to measure retrieval efficiency degradation and identify when memory operations become bottlenecks.
2. **Adversarial Knowledge Test**: Create test cases with contradictory or outdated information in memory to evaluate how the model handles inconsistencies and whether it can identify and resolve conflicts.
3. **Cross-Domain Evaluation**: Test the finetuned model on at least three diverse knowledge-intensive datasets (e.g., WikiHop for multi-hop reasoning, MedQA for medical knowledge, and a commonsense reasoning benchmark) to validate generalizability beyond the Re-DocRED domain.