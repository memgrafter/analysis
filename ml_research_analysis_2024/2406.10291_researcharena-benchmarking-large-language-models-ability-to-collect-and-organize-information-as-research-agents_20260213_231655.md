---
ver: rpa2
title: 'ResearchArena: Benchmarking Large Language Models'' Ability to Collect and
  Organize Information as Research Agents'
arxiv_id: '2406.10291'
source_url: https://arxiv.org/abs/2406.10291
tags:
- 'true'
- lidar
- survey
- 'false'
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ResearchArena benchmarks LLMs' abilities to conduct academic surveys
  by decomposing the process into information discovery, selection, and organization
  tasks. It provides an offline environment with 12M full-text papers and 7.9K survey
  papers, enabling systematic evaluation without distributing copyrighted content.
---

# ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents

## Quick Facts
- arXiv ID: 2406.10291
- Source URL: https://arxiv.org/abs/2406.10291
- Authors: Hao Kang; Chenyan Xiong
- Reference count: 11
- LLMs underperform keyword-based methods in academic survey research tasks

## Executive Summary
ResearchArena is a benchmark that evaluates Large Language Models' (LLMs) ability to conduct academic surveys by decomposing the process into three tasks: information discovery, selection, and organization. The benchmark uses an offline environment built from the Semantic Scholar Open Research Corpus (S2ORC), containing approximately 12 million full-text papers and 7.9K survey papers. Rather than redistributing copyrighted materials, the authors provide code to construct the environment, ensuring ethical compliance while enabling systematic evaluation. The benchmark reveals that LLM-based approaches underperform simpler keyword-based retrieval methods, though recent reasoning models like DeepSeek-R1 show modest improvements in zero-shot performance.

## Method Summary
ResearchArena constructs a benchmark environment from the S2ORC corpus by filtering for survey papers (using keyword "survey" plus GPT-4 validation), extracting their references, and creating JSON-encoded mind-maps from survey figures. The benchmark evaluates three tasks: information discovery (identifying relevant literature), information selection (evaluating papers' relevance and impact), and information organization (structuring knowledge into hierarchical frameworks like mind-maps). Baseline methods include TITLE (using survey titles as queries), ZERO-SHOT (direct LLM generation), DECOMPOSER (decomposed prompting), and clustering approaches. Evaluation uses standard information retrieval metrics including recall, precision, nDCG, and MRR for discovery and selection, plus tree-based metrics for organization.

## Key Results
- LLM-based approaches underperform keyword-based retrieval methods in discovery and selection tasks
- Using survey titles as retrieval queries yields superior recall and precision compared to LLM-driven methods
- Information organization remains challenging for LLMs, especially without oracle guidance
- DeepSeek-R1 shows only modest gains over other LLM approaches in zero-shot performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ResearchArena uses an offline environment to avoid copyright issues while enabling systematic evaluation of LLM research capabilities.
- Mechanism: Instead of redistributing copyrighted materials, ResearchArena provides code to construct the benchmark environment from the Semantic Scholar Open Research Corpus (S2ORC), allowing users to build reproducible datasets.
- Core assumption: S2ORC contains sufficient full-text papers and bibliographic references to create a meaningful evaluation environment.
- Evidence anchors:
  - [abstract]: "To ensure ethical compliance, we do not redistribute copyrighted materials; instead, we provide code to construct the environment from the Semantic Scholar Open Research Corpus (S2ORC)."
  - [section]: "To ensure compliance, we provide tools and code that enable users to reproduce the dataset using publicly accessible S2ORC."
  - [corpus]: The corpus contains approximately 12 million full-text academic papers and 7.9K survey papers, providing substantial data for evaluation.
- Break condition: If S2ORC lacks sufficient full-text papers or bibliographic data, the offline environment cannot adequately support the benchmark tasks.

### Mechanism 2
- Claim: The benchmark decomposes research survey tasks into three distinct stages to evaluate LLM capabilities systematically.
- Mechanism: ResearchArena models the survey process as information discovery, selection, and organization, with mind-map construction as a bonus task, reflecting the real-world research workflow.
- Core assumption: These three stages capture the essential components of academic survey research and can be evaluated independently.
- Evidence anchors:
  - [abstract]: "ResearchArena models the process in three stages: (1) information discovery, identifying relevant literature; (2) information selection, evaluating papers' relevance and impact; and (3) information organization, structuring knowledge into hierarchical frameworks such as mind-maps."
  - [section]: "It comprises three sub-tasks for evaluation: information discovery, information selection, and information organization."
  - [corpus]: The dataset includes survey papers with their references, enabling evaluation of all three stages.
- Break condition: If the stages are not independent or do not adequately represent the research process, the benchmark may not accurately assess LLM capabilities.

### Mechanism 3
- Claim: LLM-based approaches underperform simpler keyword-based retrieval methods in discovery and selection tasks.
- Mechanism: The benchmark reveals that using survey titles as retrieval queries yields superior recall and precision compared to LLM-driven information discovery and selection tasks.
- Core assumption: Keyword-based methods can effectively capture the core topic of survey papers without requiring complex reasoning.
- Evidence anchors:
  - [abstract]: "Preliminary evaluations reveal that LLM-based approaches underperform compared to simpler keyword-based retrieval methods, though recent reasoning models such as DeepSeek-R1 show slightly better zero-shot performance."
  - [section]: "For example, using survey titles as retrieval queries consistently yields superior recall and precision compared to LLM-driven information discovery and selection tasks."
  - [corpus]: Performance metrics from experiments show TITLE method outperforming ZERO-SHOT and DECOMPOSER approaches.
- Break condition: If LLM reasoning capabilities improve significantly, this performance gap may narrow or reverse.

## Foundational Learning

- Concept: Information retrieval metrics (Recall, Precision, nDCG, MRR)
  - Why needed here: These metrics provide quantitative evaluation of how well LLMs can discover and select relevant academic papers.
  - Quick check question: What does Recall@10 measure in the context of information discovery?

- Concept: Hierarchical knowledge representation
  - Why needed here: Information organization task requires constructing mind-maps that capture relationships between research concepts.
  - Quick check question: How does Heading Soft Recall differ from traditional recall metrics in evaluating mind-maps?

- Concept: Citation analysis and influence classification
  - Why needed here: Information selection requires distinguishing between influential and non-influential citations to rank papers by importance.
  - Quick check question: Why is it important to classify citations as influential versus non-influential in academic research?

## Architecture Onboarding

- Component map: S2ORC → Survey selection → Reference linking → Mind-map extraction → Benchmark construction
- Critical path: Survey selection → Reference linking → Discovery task evaluation → Selection task evaluation → Organization task evaluation
- Design tradeoffs:
  - Offline vs. online evaluation: Offline ensures reproducibility but may miss recent papers
  - Survey keyword filtering vs. GPT-4 validation: Balancing scale with accuracy in survey identification
  - Abstract indexing vs. full-text: Balancing informativeness with computational tractability
- Failure signatures:
  - Low recall in discovery tasks indicates insufficient query generation or retrieval capabilities
  - Poor nDCG scores suggest inability to distinguish influential from non-influential citations
  - Low Heading Soft Recall reveals difficulties in understanding hierarchical knowledge structures
- First 3 experiments:
  1. Run TITLE baseline on BM25 retriever to establish performance floor
  2. Evaluate ZERO-SHOT with GPT-4 on same retrieval task to measure LLM improvement
  3. Test DECOMPOSER approach to assess benefits of decomposed prompting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-based methods be improved to match or exceed the performance of keyword-based retrieval methods in information discovery tasks?
- Basis in paper: [explicit] The paper states that LLM-based approaches underperform compared to simpler keyword-based retrieval methods in information discovery tasks, with recent reasoning models like DeepSeek-R1 showing only modest gains in zero-shot performance.
- Why unresolved: Current LLM-based methods struggle to effectively leverage external tools and generate relevant queries for information discovery, as evidenced by their consistently low recall and precision scores compared to TITLE methods.
- What evidence would resolve it: Comparative studies measuring the performance of advanced LLM-based methods against keyword-based retrieval across diverse research topics, with detailed analysis of query generation strategies and tool utilization patterns.

### Open Question 2
- Question: What architectural or training modifications could enable LLMs to construct coherent hierarchical knowledge structures in information organization tasks without oracle guidance?
- Basis in paper: [explicit] The paper notes that LLMs face challenges in constructing coherent structures during information organization, particularly when references are not provided, and that CLUSTERING methods outperform LLM-based agents in structural alignment.
- Why unresolved: Current LLM-based approaches lack the capability to autonomously organize information into meaningful hierarchies, as demonstrated by their poor performance on tree semantic distance metrics compared to traditional clustering methods.
- What evidence would resolve it: Experimental results comparing various architectural modifications (such as hierarchical attention mechanisms or graph neural networks) against baseline methods, measuring performance improvements in both content similarity and structural alignment metrics.

### Open Question 3
- Question: How can the dataset construction methodology be enhanced to improve reference coverage and ensure more comprehensive representation of influential literature in survey papers?
- Basis in paper: [inferred] The paper acknowledges that approximately 17.18% of survey papers have at least 50% reference coverage, and that the reference linking process is inherently imperfect due to missing references and misclassified citation structures.
- Why unresolved: Current automated approaches to reference linking and citation analysis cannot guarantee perfect recall of influential references, and the text-only nature of the S2ORC corpus limits the availability of full-text references for many survey papers.
- What evidence would resolve it: Comparative studies evaluating different reference extraction techniques (including domain-specific heuristics and human annotation) against ground truth citation data, measuring improvements in reference coverage ratios and influence classification accuracy across disciplines.

## Limitations

- Benchmark relies on S2ORC corpus from February 2024, potentially missing recent academic literature
- Exact prompt specifications for GPT-4 validation during survey selection are not fully detailed
- Performance gaps between LLM-based and keyword methods may reflect implementation choices rather than fundamental limitations

## Confidence

**High confidence** in: The benchmark design and task decomposition (information discovery, selection, organization) - this follows established academic research workflows and uses standard evaluation metrics.

**Medium confidence** in: The comparative performance results between baseline methods - while the experimental setup appears sound, implementation details for LLM approaches could affect outcomes.

**Low confidence** in: Generalizability of findings to other domains beyond computer science - the S2ORC corpus and survey paper distribution may not represent all academic fields equally.

## Next Checks

1. Re-run the benchmark with a more recent S2ORC snapshot to assess whether performance patterns hold for newer literature and whether temporal factors influence LLM effectiveness in information discovery.

2. Apply the same benchmark methodology to specialized corpora (e.g., biomedical literature from PubMed) to determine if LLM underperformance is domain-agnostic or specific to the computer science literature represented in S2ORC.

3. Systematically vary prompting strategies, few-shot examples, and retrieval configurations for the LLM-based approaches to identify whether performance gaps stem from fundamental limitations or sub-optimal implementations.