---
ver: rpa2
title: 'AdaRank: Disagreement Based Module Rank Prediction for Low-rank Adaptation'
arxiv_id: '2408.09015'
source_url: https://arxiv.org/abs/2408.09015
tags:
- module
- ranks
- adarank
- rank
- modules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaRank introduces a simple disagreement-based approach to predict
  layerwise ranks for low-rank adaptation in large language models. By measuring model
  output disagreements induced by random module perturbations, it estimates module
  criticality and allocates higher ranks to more critical layers while using lower
  ranks elsewhere.
---

# AdaRank: Disagreement Based Module Rank Prediction for Low-rank Adaptation

## Quick Facts
- arXiv ID: 2408.09015
- Source URL: https://arxiv.org/abs/2408.09015
- Authors: Yihe Dong
- Reference count: 5
- Key outcome: Disagreement-based rank prediction improves low-rank adaptation generalization, particularly for smaller datasets

## Executive Summary
AdaRank introduces a simple disagreement-based approach to predict layerwise ranks for low-rank adaptation in large language models. By measuring model output disagreements induced by random module perturbations, it estimates module criticality and allocates higher ranks to more critical layers while using lower ranks elsewhere. This method improves generalization compared to uniform rank allocation, particularly for smaller datasets, without requiring additional objectives or regularizers. Experiments on BERT using Trec, Yelp, and AG News datasets demonstrate consistent performance gains, with average rank 8 achieving 97.27% accuracy on Trec and 99.54% AUC on Yelp. AdaRank also generalizes well using generic text for rank prediction, making it broadly applicable across tasks.

## Method Summary
AdaRank predicts module criticality by measuring model output disagreements induced by random perturbations to each module. The method perturbs a module with random noise, creates two perturbed model instances, and measures the ℓ1 difference in their logits. Modules with higher disagreement receive higher ranks during low-rank adaptation, allowing more critical modules to learn more complex features while restricting degrees of freedom in less critical modules. The approach is task-agnostic and can use generic text for rank prediction, leaving both pretraining and adaptation stages completely intact.

## Key Results
- Average rank 8 with AdaRank achieves 97.27% accuracy on Trec and 99.54% AUC on Yelp
- AdaRank outperforms uniform rank allocation on all tested datasets, with greatest gains on smaller datasets
- Generic text performs comparably to task-specific text for rank prediction, with 0.1-0.3% performance difference
- AdaRank maintains performance advantages even when using a single sentence for rank prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disagreement between perturbed model instances predicts module criticality for rank allocation
- Mechanism: Perturbing a module with random noise and measuring the ℓ1 difference between two perturbed model instances' logits approximates how sensitive the overall model output is to changes in that module. More sensitive modules (higher disagreement) receive higher ranks during adaptation.
- Core assumption: The ℓ1 difference in logits between perturbed model instances correlates with module criticality
- Evidence anchors:
  - [abstract] "We develop a simple model disagreement based technique to predict the rank of a given module relative to the other modules"
  - [section] "To approximate its importance score, we 1) add a random noise tensor of the same shape to m's pretrained weights, while keeping all other modules frozen, 2) do this twice, and 3) take the ℓ1 difference between model logits produced by these two perturbed model instances"
  - [corpus] Weak evidence - the corpus mentions related work on rank pruning and low-rank adapters but doesn't directly address disagreement-based criticality measurement

### Mechanism 2
- Claim: Lower disagreement rates indicate more generalizable features, requiring lower ranks during adaptation
- Mechanism: The smaller the disagreement is between different model instances after perturbing the module, the better the model would be able to generalize, meaning the features learned by that module during pretraining are robust and generalizable, and hence a lower rank is needed during adaptation.
- Core assumption: Model disagreement correlates with generalization capability
- Evidence anchors:
  - [abstract] "Inspired by the theory and observations around feature learning and module criticality"
  - [section] "Another motivation behind AdaRank stems from the relation between model agreement and model generalization [Jiang et al., 2022, Nakkiran and Bansal, 2020], which found that the smaller the disagreement is between multiple pretrained instances of a model on a test set, the better the model generalizes to that test set"
  - [corpus] Weak evidence - corpus mentions parameter-efficient methods but doesn't directly address the disagreement-generalization relationship

### Mechanism 3
- Claim: Layerwise rank customization improves generalization compared to uniform rank allocation
- Mechanism: Allocating higher ranks to more critical modules allows them to learn more complex features during adaptation, while restricting degrees of freedom in less critical modules reduces overfitting. This contrasts with uniform rank allocation across all layers.
- Core assumption: Different layers learn different types of features with varying information-theoretic content
- Evidence anchors:
  - [abstract] "Empirically, AdaRank generalizes notably better on unseen data than using uniform ranks with the same number of parameters"
  - [section] "This strongly suggests that different layers should use different ranks during adaptation, which has the potential to not only improve expressiveness by allowing some layers to learn more complex features, but also reduce overfitting by restricting the degrees of freedom in the remaining layers"
  - [corpus] Weak evidence - corpus mentions various low-rank adaptation methods but doesn't specifically address layerwise rank customization

## Foundational Learning

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: AdaRank builds upon LoRA by customizing ranks per module rather than using uniform ranks
  - Quick check question: What is the primary advantage of LoRA over full fine-tuning of large language models?

- Concept: Module criticality
  - Why needed here: AdaRank uses module criticality (measured via disagreement) to determine rank allocation
  - Quick check question: How does perturbing a module with random noise help estimate its criticality?

- Concept: Model generalization and calibration
  - Why needed here: The core hypothesis is that lower disagreement indicates better generalization, which drives the rank prediction mechanism
  - Quick check question: According to the paper, what is the relationship between model disagreement and generalization performance?

## Architecture Onboarding

- Component map:
  - Pretrained model P (base model)
  - Perturbed model instances (Pm, P'm) - created by adding noise to individual modules
  - Module importance scoring function (ℓ1 difference in logits)
  - Rank scaling function (Algorithm 3)
  - Adaptation stage (LoRA with predicted ranks)

- Critical path:
  1. Calculate module importance scores using Algorithm 2
  2. Scale importance scores to ranks using Algorithm 3
  3. Apply LoRA adaptation with predicted ranks
  4. Evaluate generalization performance

- Design tradeoffs:
  - Using generic text vs. task-specific text for rank prediction (Section 3.4)
  - Number of perturbed model instances vs. number of input samples for disagreement calculation
  - Fixed average rank vs. adaptive rank determination without a priori constraints

- Failure signatures:
  - Random ranks perform worse than uniform ranks (Table 7)
  - Task-specific text doesn't always outperform generic text for rank prediction
  - Advantages of AdaRank over uniform ranks appear to saturate at higher ranks

- First 3 experiments:
  1. Implement Algorithm 2 to calculate module importance scores on a small BERT model using generic text
  2. Implement Algorithm 3 to convert importance scores to ranks for a fixed average rank
  3. Apply LoRA with AdaRank-predicted ranks on Trec dataset and compare with uniform rank baseline

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implicit questions arise from the discussion:
- How does the number of perturbed model instances affect the quality of module importance scores?
- Can AdaRank be extended to other model architectures beyond BERT?
- What is the optimal trade-off between using generic versus task-specific text for rank prediction?

## Limitations
- Theoretical foundation connecting disagreement rates to feature robustness and generalization is not rigorously established
- The choice of using generic text for rank prediction, while showing good results, lacks strong theoretical justification compared to task-specific text
- The claim that generic text performs comparably to task-specific text for rank prediction has low confidence based on limited evidence

## Confidence
- **High**: The empirical demonstration that AdaRank outperforms uniform rank allocation on multiple datasets
- **Medium**: The theoretical connection between disagreement and generalization capability
- **Low**: The claim that generic text performs comparably to task-specific text for rank prediction

## Next Checks
1. Conduct ablation studies to isolate the contribution of disagreement-based rank prediction versus the specific LoRA implementation, by comparing AdaRank with other rank prediction methods (e.g., random, uniform, or importance-based from other metrics)

2. Test the transferability hypothesis by using text from one domain/task to predict ranks for adaptation on a different domain/task, measuring how much performance degrades compared to using task-specific text

3. Analyze the relationship between disagreement rates and actual generalization performance across different modules by conducting controlled experiments that measure both metrics simultaneously, potentially revealing nonlinear or threshold effects in the disagreement-generalization relationship