---
ver: rpa2
title: Setting the Record Straight on Transformer Oversmoothing
arxiv_id: '2401.04301'
source_url: https://arxiv.org/abs/2401.04301
tags:
- layer
- transformer
- eigenvalues
- layers
- latexit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the widely held belief that Transformer models
  inevitably oversmooth (i.e., features become increasingly similar) as depth increases.
  Through theoretical analysis and empirical experiments on pre-trained models, the
  authors demonstrate that oversmoothing is not inevitable.
---

# Setting the Record Straight on Transformer Oversmoothing

## Quick Facts
- arXiv ID: 2401.04301
- Source URL: https://arxiv.org/abs/2401.04301
- Reference count: 40
- Primary result: Challenges the belief that Transformers inevitably oversmooth, showing smoothing behavior depends on eigenspectrum of value and projection weights

## Executive Summary
This paper challenges the widely held belief that Transformer models inevitably oversmooth (i.e., features become increasingly similar) as depth increases. Through theoretical analysis and empirical experiments on pre-trained models, the authors demonstrate that oversmoothing is not inevitable. They show that the smoothing behavior depends on the eigenspectrum of the value and projection weights, and that the sign of layer normalization weights can influence this effect. The paper introduces a simple reparameterization of the Transformer weights that allows one to influence smoothing behavior, enabling both smoothing and sharpening effects depending on the parameter settings.

## Method Summary
The paper analyzes Transformer oversmoothing by examining the eigenspectrum of the value and projection weights in the attention mechanism. It introduces a reparameterization that directly controls these eigenvalues to influence whether the model smooths or sharpens features. The method involves parameterizing the weight matrix H as H = VHΛHVH⁻¹ where ΛH is constrained to either [-1, 0) for sharpening or [0, 1] for smoothing. This reparameterization is implemented via a clipped parameterization that learns the eigenvalues directly.

## Key Results
- Theoretically shows that oversmoothing depends on the eigenspectrum of value and projection weights, not an inevitable property of deep Transformers
- Introduces a simple reparameterization method that can control smoothing behavior by constraining eigenvalues of weight matrices
- Demonstrates that layer normalization weights can reverse expected smoothing behavior, particularly in Pre-LN architectures
- Empirical results show the reparameterization can reduce or increase smoothing depending on dataset and model architecture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The eigenspectrum of the value and projection weights controls whether Transformer features converge or diverge.
- **Mechanism:** When the eigenvalues of H fall in [-1, 0), the term (1 + λH * λA1) dominates and prevents input convergence, reducing oversmoothing. When eigenvalues fall in (0, ∞), the term (1 + λH * λAn) dominates, leading to input convergence and oversmoothing.
- **Core assumption:** The attention matrix A is positive and diagonalizable, and its eigenvalues are bounded within (-1, 1].
- **Evidence anchors:** Theoretical analysis showing that smoothing behavior depends on eigenspectrum; empirical validation on pre-trained models.
- **Break condition:** If the attention matrix becomes non-positive or non-diagonalizable, or if layer normalization weights become negative (which can reverse the expected filtering behavior), the mechanism breaks.

### Mechanism 2
- **Claim:** A simple reparameterization of the Transformer weights (controlling the eigenspectrum of H) allows one to influence smoothing behavior.
- **Mechanism:** By parameterizing H as H = VHΛHVH⁻¹ and constraining diag(ΛH) to either [-1, 0) (sharpening) or [0, 1] (smoothing), one can directly control whether the model smooths or sharpens features.
- **Core assumption:** Learning the eigenvalues of H through this parameterization is stable and effective.
- **Evidence anchors:** Theoretical derivation of the reparameterization; empirical results showing controlled smoothing behavior.
- **Break condition:** If the clipped parameterization becomes too restrictive or if training instability occurs (e.g., eigenvalues sampled too large), the mechanism breaks.

### Mechanism 3
- **Claim:** The sign of layer normalization weights can influence the smoothing behavior, potentially reversing the expected effect.
- **Mechanism:** When layer normalization weights are negative in Pre-LN architecture, the normalization happens after attention but before the residual connection, which can reverse the expected filtering behavior of the layer.
- **Core assumption:** The interaction between layer normalization and the residual connection in the order of operations is the primary cause of this reversal.
- **Evidence anchors:** Empirical observations across multiple models showing reversed behavior with negative LN weights in Pre-LN.
- **Break condition:** If the architecture changes (e.g., different LN placement) or if the LN weights are constrained to be positive, this mechanism breaks.

## Foundational Learning

- **Concept:** Eigenspectrum analysis of linear transformations
  - **Why needed here:** The core theoretical insight relies on understanding how the eigenvalues of matrices like H and A control the long-term behavior of repeated linear operations in Transformers.
  - **Quick check question:** What is the dominant eigenvalue in the repeated application of a matrix, and how does it determine convergence behavior?

- **Concept:** Singular Value Decomposition (SVD) and its relationship to rank
  - **Why needed here:** Rank collapse is one of the three definitions of oversmoothing used, and SVD is the standard tool to compute and analyze rank in practice.
  - **Quick check question:** How does the effective rank (exponential of the entropy of the normalized singular values) differ from the standard rank, and why is it preferred for analyzing oversmoothing?

- **Concept:** Layer normalization mechanics in Transformer architectures
  - **Why needed here:** The paper shows that the sign and placement of layer normalization weights can reverse the expected smoothing behavior, which is critical for understanding when the proposed reparameterization works or fails.
  - **Quick check question:** How does the order of operations (attention → residual → LN vs. attention → LN → residual) affect the overall transformation applied to the input?

## Architecture Onboarding

- **Component map:** Attention module → Value and projection weights → Residual connection → Layer normalization → Feed-forward network

- **Critical path:** A → WV, Wproj → H → (I + H ⊗ A) → Xℓ (repeated application)
  - The critical path for oversmoothing is the repeated application of the linear transformation (I + H ⊗ A) to the input.

- **Design tradeoffs:**
  - Controlling the eigenspectrum of H gives fine-grained control over smoothing but requires careful parameterization and initialization.
  - Layer normalization can enhance or reverse the desired smoothing effect depending on weight signs and placement.
  - The asymptotic analysis is powerful but may not fully capture finite-depth behavior.

- **Failure signatures:**
  - Training instability (exploding/vanishing gradients) when eigenvalues of H are not well-constrained.
  - Unexpected smoothing/sharpening behavior when layer normalization weights become negative (especially in Pre-LN).
  - Rank collapse or input convergence even when the reparameterization is designed to prevent it.

- **First 3 experiments:**
  1. **Baseline comparison:** Measure HFC/LFC, cosine similarity, and effective rank on a pre-trained ViT model to establish the current smoothing behavior.
  2. **Reparameterization ablation:** Apply the eigendecomposition reparameterization with diag(ΛH) constrained to [-1, 0) and [0, 1] separately, and compare the resulting smoothing metrics.
  3. **Layer normalization ablation:** Train models with Pre-LN and Post-LN, both with positive and negative LN weights, and observe the effect on the smoothing metrics to verify the reversal effect.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Under what specific conditions do the eigenvalues of H consistently fall within the sharpening range [-1, 0) versus the smoothing range (0, ∞) in pre-trained Transformer models?
- **Basis in paper:** The paper introduces a reparameterization where diag(ΛH) := clip(ψ, [-1, 0]) encourages sharpening and diag(ΛH) := clip(ψ, [0, 1]) encourages smoothing, based on the theoretical finding that these eigenvalue ranges lead to different dominating eigenvalues.
- **Why unresolved:** The paper observes that different models (ViT, DeiT, Crammed BERT) and datasets (CIFAR100, ImageNet, The Pile) show varying smoothing behaviors, and the reparameterization has mixed effects across these cases.
- **What evidence would resolve it:** Systematic experiments varying model architecture (depth, width, attention mechanism), dataset properties (size, domain, class balance), and training procedures (initialization, optimization, regularization) while tracking the distribution of H's eigenvalues and corresponding smoothing behavior.

### Open Question 2
- **Question:** How do layer normalization weights influence the smoothing behavior of Transformers, and can this effect be theoretically characterized?
- **Basis in paper:** The paper notes that the sign of layer normalization weights can influence smoothing behavior, with negative weights in Pre-LN layers potentially reversing the expected filtering behavior.
- **Why unresolved:** The paper states that layer normalization breaks the theoretical analysis and cannot be easily characterized without further assumptions.
- **What evidence would resolve it:** A theoretical framework extending the current analysis to include layer normalization, possibly by introducing specific assumptions about the relationship between H, the layer normalization parameters, and the attention matrix. Empirical validation on models with varied layer normalization configurations (Pre-LN vs Post-LN, different initialization schemes, fixed vs learned weights).

### Open Question 3
- **Question:** What are the rates of convergence for the different smoothing definitions (input convergence, angle convergence, rank collapse) in Transformers, and how do they depend on the eigenspectrum of H and A?
- **Basis in paper:** The paper's theoretical analysis is asymptotic, applying in the limit as the number of layers ℓ → ∞.
- **Why unresolved:** The paper explicitly states that understanding the rates of convergence is a limitation of the current theoretical analysis.
- **What evidence would resolve it:** Theoretical analysis deriving explicit bounds on the convergence rates for each smoothing definition in terms of the eigenvalues of H and A. Empirical validation measuring the convergence of smoothing metrics across layers in various Transformer models and datasets, correlating the observed rates with the theoretical predictions.

## Limitations
- Analysis focuses on simplified Transformer architecture without feed-forward network, limiting generalizability
- Theoretical results rely on eigenvalue analysis assuming positive attention matrices and diagonalizable weight matrices
- Empirical validation uses pre-trained models rather than training from scratch with proposed reparameterization

## Confidence

- **High confidence**: The theoretical analysis showing that oversmoothing depends on the eigenspectrum of H is mathematically sound within the stated assumptions. The observation that layer normalization weight signs affect smoothing behavior is empirically demonstrated across multiple models.

- **Medium confidence**: The reparameterization method's practical effectiveness is supported by empirical results but lacks extensive ablation studies on different initialization strategies and hyperparameter choices.

- **Low confidence**: Claims about the method's ability to control oversmoothing in diverse real-world scenarios are limited by the absence of training-from-scratch experiments and the simplified theoretical framework.

## Next Checks
1. Train full Transformer models from scratch using the proposed reparameterization with both sharpening and smoothing variants, measuring convergence and final performance.

2. Conduct systematic ablation studies varying the initialization range for ΛH eigenvalues and measuring the stability and effectiveness of smoothing control.

3. Test the method across a broader range of architectures including those with complex attention patterns (e.g., sparse attention, axial attention) and different normalization placements.