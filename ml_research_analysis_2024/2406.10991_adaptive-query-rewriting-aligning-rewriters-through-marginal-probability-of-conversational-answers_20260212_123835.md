---
ver: rpa2
title: 'Adaptive Query Rewriting: Aligning Rewriters through Marginal Probability
  of Conversational Answers'
arxiv_id: '2406.10991'
source_url: https://arxiv.org/abs/2406.10991
tags:
- retrieval
- rewrite
- query
- preference
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaQR, a method for training query rewriting
  models for conversational question answering with limited annotations. The approach
  fine-tunes a compact language model on a small portion of labeled data, then generates
  rewrite candidates for each query.
---

# Adaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers

## Quick Facts
- arXiv ID: 2406.10991
- Source URL: https://arxiv.org/abs/2406.10991
- Reference count: 34
- One-line primary result: AdaQR achieves comparable or better performance than state-of-the-art methods while using significantly fewer annotations for conversational question answering.

## Executive Summary
This paper introduces AdaQR, a method for training query rewriting models for conversational question answering with limited annotations. The approach fine-tunes a compact language model on a small portion of labeled data, then generates rewrite candidates for each query. A novel reward calculation method assesses the retriever's preference for these candidates by the marginal probability of answers conditioned on the conversational query and retrieved passages. This reward guides further optimization using Direct Preference Optimization (DPO), requiring no additional rewrite or retrieval annotations. Experiments on four datasets demonstrate that AdaQR improves both in-domain and out-of-domain performance, achieving comparable or better results than state-of-the-art methods while using significantly fewer annotations. The approach also shows robustness across different retrievers and language models.

## Method Summary
AdaQR employs a two-stage approach for conversational query rewriting. First, a compact language model is fine-tuned on limited seed data (approximately 10% of training split from QReCC and TopiOCQA datasets) using supervised fine-tuning. For each conversation example, the fine-tuned model generates three rewrite candidates. These candidates are then evaluated by an off-the-shelf retriever (BM25 or dense models like ANCE and E5) that retrieves top-K passages for each rewrite. A pre-trained CQA model estimates the probability of the correct answer given each retrieved passage, and the marginal probability of the answer across all top-K passages serves as the reward for each rewrite candidate. Preference pairs are constructed based on reward differences exceeding a threshold, and Direct Preference Optimization aligns the rewriter with retriever preferences using these rewards. This approach requires only conversation answers rather than passage labels, making it annotation-efficient while improving both in-domain and out-of-domain performance.

## Key Results
- AdaQR achieves comparable or better performance than state-of-the-art methods while using significantly fewer annotations
- The method demonstrates robust performance across both sparse (BM25) and dense (ANCE, E5) retrievers
- Out-of-domain experiments show effective adaptation to new datasets without requiring domain-specific annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Marginal probability of answers serves as effective retriever preference signal
- Mechanism: By marginalizing the probability of the answer over top-K retrieved passages, the method captures the retriever's implicit preference for rewrites that lead to passages more likely to generate the correct answer
- Core assumption: The retriever's ranking quality directly correlates with the probability of generating the correct answer from retrieved passages
- Evidence anchors:
  - [abstract] "A novel approach is then proposed to assess retriever's preference for these candidates by the probability of answers conditioned on the conversational query by marginalizing the Top-K passages"
  - [section 2.4] "By marginalizing the top K passages, we calculate the marginal probability of answer ei as the retrievers' preference for rewrite candidate ˆri"
  - [corpus] Weak evidence - corpus shows related works on conversational query rewriting but none specifically using marginal probability of answers as preference signal
- Break condition: If the retriever consistently ranks irrelevant passages highly, or if the CQA model generates answers with low probability from relevant passages despite good ranking

### Mechanism 2
- Claim: Preference optimization with weak supervision improves out-of-domain performance
- Mechanism: Using limited seed dataset annotations to fine-tune an initial model, then applying preference optimization with conversation answers as reward allows adaptation to new domains without requiring domain-specific annotations
- Core assumption: The rewriting patterns learned from seed datasets transfer sufficiently to new domains, and preference optimization can correct domain-specific deficiencies
- Evidence anchors:
  - [abstract] "AdaQR not only enhances the in-domain capabilities of the rewriter with limited annotation requirement, but also adapts effectively to out-of-domain datasets"
  - [section 4] "For out-of-domain scenarios across four datasets, Ours that began with a heterogeneous-seed SFT and then underwent preference optimization on target datasets... still exceeds most of baselines"
  - [corpus] Weak evidence - corpus shows related preference optimization approaches but limited evidence for out-of-domain adaptation with minimal annotations
- Break condition: If domain shift is too large for the initial SFT model to provide useful rewrites, or if conversation answers are not informative enough to guide preference optimization

### Mechanism 3
- Claim: Sampling multiple rewrite candidates and selecting pairs based on reward gap provides effective preference pairs
- Mechanism: Generating 3 rewrite candidates per query and forming preference pairs where the reward difference exceeds a threshold creates training data that highlights meaningful differences in rewrite quality
- Core assumption: The reward calculation produces sufficiently discriminative values to identify meaningful quality differences between rewrites
- Evidence anchors:
  - [section 2.5] "For each conversation example, we construct pairwise preference data {(H, q, ˆrw, ˆrl)} by selecting pairs of rewrites (ˆrw, ˆrl) from {ˆri}3 i=1 such that ew − el > δ"
  - [section 5.2] "Direct preference optimization across all candidate values of K significantly improves the retrieval performance over the SFT version"
  - [corpus] No direct evidence - corpus doesn't mention this specific approach to generating preference pairs
- Break condition: If reward values are too similar across candidates, or if the threshold δ eliminates too many or too few pairs

## Foundational Learning

- Concept: Marginalization of probabilities
  - Why needed here: To aggregate evidence from multiple retrieved passages when assessing rewrite quality
  - Quick check question: If a rewrite retrieves 5 passages with answer probabilities [0.1, 0.3, 0.2, 0.4, 0.1] and retrieval scores [1.0, 0.8, 0.9, 0.7, 0.6], what's the marginal probability? (Assume softmax on retrieval scores)

- Concept: Preference optimization (DPO)
  - Why needed here: To align the query rewriter with the retriever's implicit preferences without requiring explicit human annotations
  - Quick check question: In DPO, what's the relationship between the preferred rewrite and dispreferred rewrite in the loss function?

- Concept: Weak supervision
  - Why needed here: To enable training with limited annotations by using readily available conversation answers instead of expensive passage labels
  - Quick check question: What's the key difference between weak supervision and semi-supervised learning in this context?

## Architecture Onboarding

- Component map: Conversational query → SFT Rewriter → Candidate Generator → Retriever → CQA Model → Reward Calculator → Preference Pair Constructor → DPO Trainer → Aligned Rewriter
- Critical path: Conversational query → SFT Rewriter → Candidate Generator → Retriever → CQA Model → Reward Calculator → DPO Trainer → Aligned Rewriter
- Design tradeoffs:
  - Using conversation answers vs. passage labels: More accessible but potentially noisier signal
  - Top-K value: Larger K provides more robust estimates but increases computation and potential noise
  - Number of candidates (3): Balances diversity with computational cost
- Failure signatures:
  - Poor retrieval metrics despite high reward: Retriever-ranker misalignment
  - High variance in rewards: Insufficient top-K passages or noisy CQA model
  - No improvement over SFT: Insufficient reward signal or poor initial SFT model
- First 3 experiments:
  1. Verify SFT model produces reasonable rewrites on seed dataset
  2. Test reward calculation with known good/bad rewrites to validate signal quality
  3. Run full pipeline with small subset to verify preference optimization improves retrieval

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of rewrite labels affect the performance of AdaQR during the supervised fine-tuning stage?
- Basis in paper: [inferred] The paper mentions using limited rewrite labels (~10% of training set) from seed datasets but does not explore how label quality impacts overall performance.
- Why unresolved: The authors state that exploring the impact of label quality and quantity remains an avenue for potential enhancement, but they do not conduct experiments on this aspect due to computational constraints.
- What evidence would resolve it: Conducting experiments with varying quality and quantity of rewrite labels during the SFT stage and measuring the downstream performance of AdaQR on both in-domain and out-of-domain datasets.

### Open Question 2
- Question: How does AdaQR perform on datasets with different answer types, such as factual vs. free-form responses?
- Basis in paper: [explicit] The paper notes that TopiOCQA uses free-form responses as answers, while QReCC exhibits larger word-level overlap with supporting passages. However, it doesn't thoroughly analyze performance across different answer types.
- Why unresolved: The analysis focuses on comparing word-level overlap vs. semantic-level approaches but doesn't systematically evaluate performance across diverse answer types.
- What evidence would resolve it: Conducting comprehensive experiments on datasets with varied answer types (factual, free-form, yes/no, numerical) and analyzing AdaQR's performance on each category.

### Open Question 3
- Question: What is the optimal number of rewrite candidates to generate during the preference optimization stage?
- Basis in paper: [explicit] The paper uses three rewrite candidates for each query instance during the rewrite sampling phase, but doesn't explore whether this is optimal.
- Why unresolved: The choice of three candidates appears arbitrary, and the paper doesn't investigate the trade-off between the number of candidates and computational efficiency versus performance improvement.
- What evidence would resolve it: Conducting ablation studies with varying numbers of rewrite candidates (e.g., 2, 3, 5, 10) and measuring the impact on retrieval performance and computational costs.

### Open Question 4
- Question: How does AdaQR perform on languages other than English?
- Basis in paper: [inferred] The paper focuses exclusively on English conversational question answering datasets without mentioning multilingual capabilities.
- Why unresolved: The authors don't discuss or test the approach on non-English datasets, leaving its applicability to other languages unexplored.
- What evidence would resolve it: Testing AdaQR on multilingual conversational question answering datasets and comparing performance across different languages, particularly those with different linguistic structures from English.

## Limitations

- The approach relies heavily on the quality of the CQA model for reward calculation, which could propagate errors if the model makes systematic mistakes
- The method requires access to conversation answers, which may not be available in all CQA settings
- The computational cost of generating multiple candidates and computing rewards for each may be prohibitive for large-scale deployments

## Confidence

- **High confidence**: The two-stage training approach (SFT + DPO) and the use of marginal probability rewards for preference optimization are well-supported by experimental results showing consistent improvements across multiple datasets and retrievers
- **Medium confidence**: The claim about effective out-of-domain adaptation is supported but could benefit from testing on more diverse domain shifts to fully validate the robustness claim
- **Medium confidence**: The assertion that the method works with significantly fewer annotations is accurate for the experimental setup, but the minimum viable annotation requirement for practical effectiveness remains unclear

## Next Checks

1. **Reward signal validation**: Test the reward calculation with intentionally perturbed rewrites to verify that the marginal probability signal correctly identifies quality differences between rewrites
2. **Annotation efficiency analysis**: Systematically vary the amount of seed annotation data (5%, 10%, 25%, 50%) to determine the minimum viable annotation requirement for effective performance
3. **Retriever robustness test**: Evaluate the method with retrievers of varying quality levels (including intentionally degraded retrievers) to assess the robustness of the marginal probability reward signal across different retrieval scenarios