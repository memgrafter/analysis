---
ver: rpa2
title: 'Autoregressive + Chain of Thought = Recurrent: Recurrence''s Role in Language
  Models'' Computability and a Revisit of Recurrent Transformer'
arxiv_id: '2409.09239'
source_url: https://arxiv.org/abs/2409.09239
tags:
- recurrent
- computational
- transformer
- depth
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the role of recurrence in language models\u2019\
  \ computational power and how Chain of Thought (CoT) prompting approximates recurrence\
  \ in Transformer architectures. The authors contrast recurrence (using previous\
  \ hidden states) and autoregression (using previous outputs) in neural networks,\
  \ showing recurrence is essential for higher computational depth."
---

# Autoregressive + Chain of Thought = Recurrent: Recurrence's Role in Language Models' Computability and a Revisit of Recurrent Transformer

## Quick Facts
- arXiv ID: 2409.09239
- Source URL: https://arxiv.org/abs/2409.09239
- Reference count: 20
- Primary result: Chain of Thought prompting approximates recurrence in Transformers, enabling computational completeness for context-sensitive languages

## Executive Summary
This paper investigates the role of recurrence in language models' computational power and how Chain of Thought (CoT) prompting approximates recurrence in Transformer architectures. The authors contrast recurrence (using previous hidden states) and autoregression (using previous outputs) in neural networks, showing recurrence is essential for higher computational depth. They demonstrate that CoT enables Transformer-based LLMs to achieve recurrent-like computation by encoding intermediate results into text and decoding them back into hidden states. Through controlled experiments, the authors show that CoT significantly improves performance on complex tasks (context-free and context-sensitive) that standard Transformers cannot solve due to limited depth complexity.

## Method Summary
The authors conduct theoretical analysis of computational power in neural networks, comparing recurrence and autoregression. They design synthetic tasks (counting and addition) to test computational limits of standard Transformers versus recurrent Transformers and CoT prompting. The experiments use controlled conditions with various model sizes and depths to isolate the effects of recurrence. They also analyze recent recurrent Transformer designs (Standard Recurrent Transformer, RWKV, Linear Transformer) to classify them as Recurrence-Complete or Recurrence-Incomplete based on their computational properties.

## Key Results
- CoT prompting enables Transformers to achieve recurrent-like computation by encoding intermediate results into text
- Standard Transformers cannot solve complex counting and addition tasks, while CoT and recurrent Transformers can
- RWKV and Linear Transformer are classified as Recurrence-Incomplete, while Standard Recurrent Transformer is Recurrence-Complete

## Why This Works (Mechanism)
The mechanism by which Chain of Thought approximates recurrence relies on the text generation capability of Transformers. When CoT prompts generate intermediate reasoning steps, these text outputs are processed through the model's attention mechanism in subsequent steps, effectively creating a recurrent pathway. The attention mechanism allows the model to access and incorporate previously generated reasoning steps as additional context, mimicking how recurrent networks maintain hidden state information across time steps. This text-based recurrence enables Transformers to perform computations that require maintaining and updating state information across longer sequences than standard autoregressive processing allows.

## Foundational Learning
1. **Computational Power Theory**: Understanding what computational problems neural networks can solve
   - Why needed: To establish theoretical limits of different architectures
   - Quick check: Can the model solve context-sensitive languages?

2. **Recurrent vs Autoregressive Processing**: Distinction between using previous hidden states versus previous outputs
   - Why needed: Core theoretical contribution of the paper
   - Quick check: Does the model maintain memory across sequences?

3. **Chain of Thought Mechanism**: Using intermediate reasoning steps encoded in text
   - Why needed: Explains how CoT approximates recurrence
   - Quick check: Are intermediate results preserved in the output?

4. **Context-Free vs Context-Sensitive Languages**: Classification of computational problems by complexity
   - Why needed: Provides theoretical framework for evaluating model capabilities
   - Quick check: Can the model handle nested dependencies?

5. **Computational Depth**: Measure of how much computation can be performed given input/output constraints
   - Why needed: Explains why standard Transformers have limitations
   - Quick check: Does increasing depth improve performance on complex tasks?

## Architecture Onboarding

Component Map: Input -> Transformer Encoder -> Hidden States -> Output

Critical Path: Token embedding → Self-attention → Feed-forward network → Output projection

Design Tradeoffs: Standard Transformers trade computational depth for simplicity and efficiency; recurrent variants add complexity for greater computational power

Failure Signatures: Standard Transformers fail on tasks requiring memory across long sequences; limited by fixed computational depth

First Experiments:
1. Test counting task with varying sequence lengths
2. Compare CoT performance versus standard decoding on addition problems
3. Evaluate RWKV and Linear Transformer on context-sensitive language tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis may not fully account for practical implementation details and optimization effects
- Experiments focus on synthetic tasks that may not capture real-world NLP complexity
- Gap between theoretical computational completeness and practical performance benefits

## Confidence
- Theoretical framework and synthetic task results: High
- Practical implications for real-world LLM applications: Medium
- Analysis of recurrent Transformer variants: Medium

## Next Checks
1. Test CoT prompting on more complex reasoning benchmarks (e.g., GSM8K, MATH) to evaluate practical benefits beyond synthetic tasks
2. Investigate whether recurrence-augmented Transformers maintain performance advantages as model scale increases, addressing potential scaling limitations
3. Examine the computational efficiency trade-offs between CoT prompting and recurrent architectures, including memory usage and inference speed