---
ver: rpa2
title: Measuring Pre-training Data Quality without Labels for Time Series Foundation
  Models
arxiv_id: '2412.06368'
source_url: https://arxiv.org/abs/2412.06368
tags:
- pre-training
- contrastive
- foundation
- pval
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating the quality of
  pre-training data for time series foundation models without requiring labeled downstream
  data. The authors propose a new metric called contrastive accuracy, which measures
  how well spread the embeddings of data points are in the learned representation
  space.
---

# Measuring Pre-training Data Quality without Labels for Time Series Foundation Models

## Quick Facts
- arXiv ID: 2412.06368
- Source URL: https://arxiv.org/abs/2412.06368
- Authors: Songkang Wen; Vasilii Feofanov; Jianfeng Zhang
- Reference count: 40
- Primary result: Contrastive accuracy metric correlates with downstream performance on UCR time series classification dataset

## Executive Summary
This paper addresses the challenge of evaluating pre-training data quality for time series foundation models without requiring labeled downstream data. The authors propose a new metric called contrastive accuracy, which measures the uniformity of embeddings in the learned representation space. This metric is based on the observation that good representations should satisfy the uniformity property of contrastive learning. Experimental results on the UCR time series classification dataset demonstrate that contrastive accuracy positively correlates with model performance on downstream tasks, suggesting it can be used as an unsupervised criterion for data quality assessment.

## Method Summary
The authors propose a method to evaluate pre-training data quality for time series foundation models using contrastive accuracy. The approach involves pre-training a Vision Transformer (ViT) backbone with overlapping patches on time series data using contrastive learning with RandomCropResize augmentation and InfoNCE loss. The contrastive accuracy metric is then computed to measure the uniformity of embeddings in the learned representation space. This metric is evaluated against downstream task performance to establish its correlation with data quality. The method aims to provide an unsupervised way to assess pre-training data quality without requiring labeled downstream data.

## Key Results
- Contrastive accuracy positively correlates with model performance on downstream time series classification tasks
- The metric can predict performance improvement from adding new pre-training data
- ViT architecture with overlapping patches improves representation learning for time series data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The contrastive accuracy metric correlates with downstream performance because it measures uniformity in the learned representation space.
- Mechanism: If pre-training data is diverse and well-distributed, embeddings will be uniformly spread on the hypersphere. This uniformity ensures the foundation model can generalize to unseen tasks. Conversely, poor pre-training data leads to clustered embeddings, reducing generalization.
- Core assumption: The uniformity property of contrastive learning directly translates to downstream task performance.
- Evidence anchors:
  - [abstract] "This metric is based on the observation that a good representation should satisfy uniformity property of contrastive learning."
  - [section] "it has been shown that a good representation learned by contrastive learning should satisfy uniformity property, i.e., to have a feature distribution that tends to be uniform on the unit hypersphere"
- Break condition: If uniformity does not correlate with task performance (e.g., certain downstream tasks benefit from clustering rather than spread), the metric loses predictive power.

### Mechanism 2
- Claim: The contrastive accuracy can predict performance improvement from adding new pre-training data.
- Mechanism: The difference in contrastive accuracy before and after adding new data indicates whether the new data improves representation quality. A positive improvement in contrastive accuracy suggests better generalization.
- Core assumption: Changes in representation uniformity directly impact downstream performance.
- Evidence anchors:
  - [abstract] "We empirically show that the proposed metric positively correlates with the model's accuracy on the tasks unseen during pre-training."
  - [section] "∆Acon(X0, Xnew 0 ) := A(Xnew 0 ) con (Xnew 0 ) − A(Xnew 0 ) con (X0) and measure its correlation with ∆P(X0, Xnew 0 )"
- Break condition: If the added data is irrelevant or noisy, contrastive accuracy may not reflect true performance changes.

### Mechanism 3
- Claim: The ViT architecture with overlapping patches improves representation learning for time series.
- Mechanism: Overlapping patches capture local temporal patterns better than disjoint patches, leading to richer embeddings. This design choice enhances the foundation model's ability to generalize.
- Core assumption: Overlapping patches provide better local context than disjoint patches for time series data.
- Evidence anchors:
  - [section] "we use the ViT (Dosovitskiy et al., 2021) as the backbone, but our implementation slightly differs from theirs. Instead of dividing a sequence into disjoint patches, we employ a single CNN layer allowing them to be overlapped."
- Break condition: If overlapping patches introduce noise or redundancy, performance may degrade.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: It is the core pre-training method used to learn representations that satisfy uniformity.
  - Quick check question: Can you explain how contrastive loss encourages uniformity in the embedding space?

- Concept: Representation uniformity
  - Why needed here: It is the key property that contrastive accuracy measures to evaluate pre-training data quality.
  - Quick check question: Why does a uniform distribution on the hypersphere preserve maximal information?

- Concept: Time series augmentation
  - Why needed here: Augmentation techniques like RandomCropResize are essential for contrastive learning in time series.
  - Quick check question: What are the challenges of designing augmentations for time series compared to images?

## Architecture Onboarding

- Component map: Time series sequence -> 1D CNN layer -> Overlapping patches -> Mean pooling -> Non-overlapping patches -> Concatenation -> Class token -> MLP projector -> Contrastive loss
- Critical path:
  1. Pre-training data → ViT backbone → Overlapping patches → Embeddings → Contrastive loss → Trained model
  2. Downstream task → Fine-tuning → Evaluation
- Design tradeoffs:
  - Overlapping patches vs. disjoint patches: Better local context but potentially more computational cost.
  - Temperature in contrastive loss: Affects the sharpness of the similarity distribution.
  - Projector dimension: Balances representation richness and computational efficiency.
- Failure signatures:
  - Low contrastive accuracy but high downstream performance: Indicates uniformity may not be the only factor for good representations.
  - High contrastive accuracy but low downstream performance: Suggests the pre-training data is not diverse enough for the downstream tasks.
- First 3 experiments:
  1. Vary the subsampling ratio of pre-training data and measure contrastive accuracy vs. downstream performance.
  2. Add new pre-training data and measure the change in contrastive accuracy vs. the change in downstream performance.
  3. Compare overlapping patches vs. disjoint patches in the ViT architecture for a fixed pre-training dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of augmentation techniques affect the quality of the learned representation and the correlation between contrastive accuracy and downstream performance?
- Basis in paper: [explicit] The paper mentions that unlike computer vision, it is unclear which augmentation techniques are relevant for contrastive learning in time series data and what their impact is.
- Why unresolved: The paper does not explore different augmentation techniques or their impact on the model's performance.
- What evidence would resolve it: Experimental results comparing the performance of models trained with different augmentation techniques and their corresponding contrastive accuracy scores.

### Open Question 2
- Question: How does the proposed contrastive accuracy metric scale with larger and more diverse pre-training datasets?
- Basis in paper: [explicit] The paper mentions that as a future work, they would like to test their approach with larger pre-training datasets.
- Why unresolved: The paper only experiments with a limited number of datasets and does not explore the limits of the proposed metric.
- What evidence would resolve it: Experimental results showing the performance of the proposed metric on larger and more diverse pre-training datasets.

### Open Question 3
- Question: Can the proposed contrastive accuracy metric be applied to other time series tasks beyond classification, such as forecasting or anomaly detection?
- Basis in paper: [inferred] The paper focuses on time series classification and does not explore other tasks.
- Why unresolved: The paper does not investigate the applicability of the proposed metric to other time series tasks.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the proposed metric in other time series tasks, such as forecasting or anomaly detection.

## Limitations
- The empirical evidence linking contrastive accuracy to downstream performance is limited to a single UCR dataset
- The mechanism section acknowledges that uniformity may not be the only factor for good representations
- Architectural choices (overlapping patches, projector dimension) lack ablation studies to confirm their necessity

## Confidence

- **High Confidence**: The proposed metric is mathematically well-defined and the core intuition (uniformity in representation space) is sound.
- **Medium Confidence**: The positive correlation between contrastive accuracy and downstream performance on UCR datasets is demonstrated, but the evidence is limited to one dataset and correlation analysis.
- **Low Confidence**: The claim that contrastive accuracy can predict performance improvements from adding new data is weakly supported by empirical results.

## Next Checks

1. **Dataset Generalization**: Test contrastive accuracy on multiple time series datasets (e.g., UEA, HES) to verify if the positive correlation holds across diverse domains.

2. **Ablation Studies**: Conduct controlled experiments varying architectural components (overlapping vs. disjoint patches, projector dimensions) to isolate their impact on contrastive accuracy and downstream performance.

3. **Break Condition Testing**: Design experiments where uniformity is intentionally compromised (e.g., by adding redundant or noisy data) to observe if contrastive accuracy fails to predict downstream performance.