---
ver: rpa2
title: 'Selective "Selective Prediction": Reducing Unnecessary Abstention in Vision-Language
  Reasoning'
arxiv_id: '2402.15610'
source_url: https://arxiv.org/abs/2402.15610
tags:
- recoverr
- prediction
- risk
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReCoVERR is an inference-time algorithm that reduces over-abstention
  in selective vision-language systems by collecting additional visual evidence to
  verify low-confidence predictions. When a VLM is uncertain about an answer, ReCoVERR
  uses an LLM to generate follow-up questions about the image, collects high-confidence
  reliable and relevant evidences from the VLM, and uses an NLI model to check if
  these evidences confirm the original prediction.
---

# Selective "Selective Prediction": Reducing Unnecessary Abstention in Vision-Language Reasoning

## Quick Facts
- arXiv ID: 2402.15610
- Source URL: https://arxiv.org/abs/2402.15610
- Authors: Tejas Srinivasan, Jack Hessel, Tanmay Gupta, Bill Yuchen Lin, Yejin Choi, Jesse Thomason, Khyathi Raghavi Chandu
- Reference count: 20
- Primary result: Inference-time algorithm that reduces over-abstention in vision-language systems by collecting additional visual evidence, enabling VLMs to answer up to 20% more questions without decreasing accuracy

## Executive Summary
ReCoVERR is an inference-time algorithm that addresses the problem of over-abstention in selective vision-language systems. When a vision-language model (VLM) is uncertain about an answer, instead of abstaining outright, ReCoVERR collects additional visual evidence to verify the prediction. The system uses an LLM to generate follow-up questions about the image, collects high-confidence reliable and relevant evidences from the VLM, and uses a natural language inference (NLI) model to check if these evidences confirm the original prediction. This approach enables VLMs to answer significantly more questions while maintaining accuracy, particularly benefiting models not specifically trained on the target task.

## Method Summary
ReCoVERR operates as an inference-time algorithm that intervenes when a VLM's confidence falls below a specified threshold. The method generates follow-up questions using an LLM, collects evidences from the VLM with high confidence scores, and verifies their relevance using an NLI model. If sufficient relevant evidences confirm the initial prediction, the system makes a prediction instead of abstaining. The algorithm balances risk tolerance with coverage by requiring multiple rounds of evidence collection and stringent verification criteria. This approach is particularly effective for task-agnostic VLMs that haven't been fine-tuned on specific reasoning tasks, improving their performance without requiring additional training.

## Key Results
- Enabled three VLMs (BLIP2, InstructBLIP, LLaVA-1.5) to answer up to 20% more questions on A-OKVQA and VQAv2 without decreasing accuracy
- Improved coverage by 20% and recall by 25-30% for task-agnostic VLMs like BLIP2
- Ablation studies showed that ensuring evidences are both reliable and relevant is crucial for performance
- Method generalizes to new tasks with minimal additional tuning, though some task-specific adjustment may be beneficial

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReCoVERR reduces over-abstention by collecting high-confidence, relevant visual evidences that support low-confidence predictions.
- Mechanism: When the VLM is uncertain about an answer, ReCoVERR uses an LLM to generate follow-up questions about the image, collects high-confidence reliable and relevant evidences from the VLM, and uses an NLI model to check if these evidences confirm the original prediction.
- Core assumption: VLMs can produce well-calibrated confidence estimates and can often correctly and confidently recover information in the image that entails a low-confidence initial prediction.
- Evidence anchors:
  - [abstract] "ReCoVERR uses an LLM to pose related questions to the VLM, collects high-confidence evidences, and if enough evidence confirms the prediction the system makes a prediction instead of abstaining."
  - [section] "When the VLM is uncertain about its prediction for a given question, instead of abstaining outright, ReCoVERR tries to verify the prediction by recovering reliable supporting (or contradicting) evidence."

### Mechanism 2
- Claim: Ensuring evidences are both reliable and relevant is crucial for ReCoVERR's performance.
- Mechanism: ReCoVERR only considers evidences with high VLM confidence as reliable, and uses an NLI model to check if the evidence is relevant by measuring the difference in entailment probabilities with and without the evidence.
- Core assumption: The correctness likelihood of a given evidence is an upper bound for the correctness likelihood of the prediction.
- Evidence anchors:
  - [section] "We only consider evidences ej that satisfy πVLM(aj) ≥ 1 − r as reliable for ER. ReCoVERR requires that the confidence estimate πVLM(a) is calibrated."
  - [section] "An evidence ej is considered relevant if its truth value affects the entailment probability of the hypothesis. We measure the absolute difference between the entailment probabilities of the hypothesis H conditioned on the evidence premise Sj and its negated counterfactual i.e. ¯Sj."

### Mechanism 3
- Claim: ReCoVERR can be directly applied to new tasks with minimal additional tuning.
- Mechanism: The instantiation of ReCoVERR calibrated for one task can be applied to new tasks without further tuning, as long as the VLM is calibrated for confidence estimation.
- Core assumption: The effectiveness of ReCoVERR is primarily dependent on the calibration of the VLM's confidence estimates rather than task-specific tuning.
- Evidence anchors:
  - [section] "Our findings suggest that ReCoVERR is a promising solution towards building more reliable multimodal reasoning systems."
  - [section] "Applying ReCoVERR to Sherlock, an abductive reasoning task, at 10% risk we observe smaller improvements in recall. These results indicate that ReCoVERR requires some degree of task-specific tuning."

## Foundational Learning

- Concept: Calibrated confidence estimates
  - Why needed here: ReCoVERR relies on the VLM's confidence estimates to determine whether to collect additional evidences or make a prediction. If the confidence estimates are not calibrated, the system may collect unreliable evidences or make incorrect predictions.
  - Quick check question: How can you evaluate the calibration of a VLM's confidence estimates on a given task?

- Concept: Natural Language Inference (NLI)
  - Why needed here: ReCoVERR uses an NLI model to check if the collected evidences are relevant by measuring the difference in entailment probabilities with and without the evidence. Understanding NLI is crucial for implementing the relevance check mechanism.
  - Quick check question: How does the NLI model measure the relevance of an evidence in ReCoVERR?

- Concept: Vision-language reasoning
  - Why needed here: ReCoVERR is designed to improve the reliability of vision-language models by reducing over-abstention. Understanding the challenges and limitations of vision-language reasoning is essential for appreciating the significance of ReCoVERR's contributions.
  - Quick check question: What are the main challenges in vision-language reasoning that ReCoVERR aims to address?

## Architecture Onboarding

- Component map:
  - VLM -> LLM -> NLI model -> Evidence sets (ER, ERR)
  - Image and question -> VLM confidence estimate -> Evidence collection rounds

- Critical path:
  1. VLM produces initial prediction and confidence estimate
  2. If confidence is below threshold, LLM generates follow-up questions
  3. VLM answers questions, producing potential evidences
  4. Evidences are checked for reliability and relevance
  5. If sufficient relevant evidences are collected, NLI model checks if they confirm the initial prediction
  6. If confirmed, system makes a prediction; otherwise, abstains

- Design tradeoffs:
  - Number of evidence collection rounds (N) vs. computational cost and response time
  - Threshold for evidence reliability (1 - r) vs. the risk of collecting unreliable evidences
  - Threshold for evidence relevance (δmin) vs. the risk of including irrelevant evidences

- Failure signatures:
  - High risk: The system consistently exceeds the specified risk tolerance, indicating that the evidence collection mechanism is not effective in reducing over-abstention
  - Low coverage: The system still abstains too frequently, suggesting that the evidence collection mechanism is not generating sufficient relevant evidences
  - Low recall: The system is not recovering enough correct answers, indicating that the evidence collection mechanism is not effectively corroborating low-confidence predictions

- First 3 experiments:
  1. Evaluate the calibration of the VLM's confidence estimates on a held-out validation set
  2. Implement the evidence collection mechanism with a small number of rounds (e.g., N=3) and a small number of evidences per round (e.g., K=5)
  3. Compare the performance of ReCoVERR with the vanilla selective prediction baseline on a simple vision-language reasoning task (e.g., VQAv2)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ReCoVERR's performance scale with larger image resolution and more complex scenes?
- Basis in paper: [inferred] The paper uses standard VQA datasets and mentions the algorithm's potential for complex reasoning tasks, but does not explore performance across varying image complexity.
- Why unresolved: The current experiments use standard VQA datasets with relatively simple scenes. It's unclear how ReCoVERR would perform on high-resolution images or scenes with many objects requiring complex spatial reasoning.
- What evidence would resolve it: Experiments testing ReCoVERR on datasets with varying image complexity, resolution, and scene composition, comparing performance metrics across these dimensions.

### Open Question 2
- Question: What is the computational overhead of ReCoVERR compared to vanilla selective prediction, and how does it impact real-time applications?
- Basis in paper: [explicit] The paper mentions "engineering complexity due to sequential dependencies" and "overhead on response time" in the limitations section.
- Why unresolved: While the paper acknowledges computational overhead, it doesn't provide quantitative measurements of runtime differences or analyze the trade-off between improved accuracy and increased computational cost.
- What evidence would resolve it: Empirical measurements comparing inference time and computational resources required for ReCoVERR versus vanilla selective prediction across different hardware configurations.

### Open Question 3
- Question: How robust is ReCoVERR to adversarial examples or out-of-distribution inputs designed to fool the evidence collection process?
- Basis in paper: [inferred] The paper mentions that ReCoVERR requires calibrated confidence estimates and that future work could explore ecological validity and real-world applications.
- Why unresolved: The paper doesn't evaluate ReCoVERR's performance on adversarial examples or OOD inputs that might generate misleading evidence, which is critical for real-world deployment.
- What evidence would resolve it: Experiments testing ReCoVERR on adversarial VQA datasets and OOD examples, measuring both accuracy degradation and evidence reliability under attack scenarios.

## Limitations

- The method assumes VLMs can produce well-calibrated confidence estimates, which may not hold for all model architectures or datasets
- Computational overhead from collecting additional evidences could make the approach impractical for real-time applications
- The effectiveness depends on the quality of LLM-generated questions, which could introduce bias or miss critical information

## Confidence

**High Confidence Claims:**
- The selective prediction framework using confidence thresholding is a valid approach for reliability-aware vision-language systems
- ReCoVERR can improve coverage without decreasing accuracy when implemented correctly
- Evidence quality (both reliability and relevance) is crucial for the method's effectiveness

**Medium Confidence Claims:**
- The 20% improvement in coverage and 25-30% improvement in recall for task-agnostic VLMs are likely achievable under similar experimental conditions
- The method generalizes to new tasks with minimal additional tuning, though some task-specific adjustment may be beneficial
- The trade-off between risk tolerance and performance is well-calibrated

**Low Confidence Claims:**
- The specific numerical improvements (e.g., exact percentage gains) may vary significantly with different model sizes, datasets, or evaluation protocols
- The computational efficiency of the approach for production-scale deployments has not been thoroughly validated
- The long-term robustness of the method across diverse vision-language reasoning scenarios remains to be demonstrated

## Next Checks

1. **Calibration Verification**: Implement a thorough calibration analysis of the VLM's confidence estimates on held-out data, measuring metrics like Expected Calibration Error (ECE) and reliability diagrams to confirm the foundational assumption of well-calibrated confidences.

2. **Ablation on Evidence Quality**: Systematically vary the thresholds for evidence reliability (1-r) and relevance (δmin) to quantify their individual and combined effects on performance, establishing the precise trade-offs between computational cost and accuracy gains.

3. **Cross-Domain Generalization**: Test ReCoVERR on vision-language datasets from different domains (e.g., medical imaging, satellite imagery) to assess whether the method maintains its effectiveness outside the original task distributions, particularly focusing on scenarios requiring specialized domain knowledge.