---
ver: rpa2
title: 'FastBO: Fast HPO and NAS with Adaptive Fidelity Identification'
arxiv_id: '2409.00584'
source_url: https://arxiv.org/abs/2409.00584
tags:
- fidelity
- learning
- optimization
- efficient
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of efficiently identifying
  appropriate fidelities for hyperparameter configurations in Bayesian optimization
  (BO) and neural architecture search (NAS). The authors propose FastBO, a multi-fidelity
  BO method that adaptively determines the fidelity for each configuration based on
  two novel concepts: efficient points and saturation points.'
---

# FastBO: Fast HPO and NAS with Adaptive Fidelity Identification

## Quick Facts
- arXiv ID: 2409.00584
- Source URL: https://arxiv.org/abs/2409.00584
- Reference count: 40
- FastBO achieves strong anytime performance, converging faster to optimal configurations compared to state-of-the-art methods

## Executive Summary
This paper introduces FastBO, a multi-fidelity Bayesian optimization method for hyperparameter optimization (HPO) and neural architecture search (NAS) that adaptively determines appropriate fidelities for each configuration. The method addresses the challenge of balancing computational cost against result quality by identifying efficient points (optimal balance of cost and performance) and saturation points (where performance stabilizes). FastBO estimates learning curves from early observations, extracts these critical points, and uses them to fit surrogate models. The approach generalizes to single-fidelity optimization by substituting efficient point performances for final fidelity evaluations.

## Method Summary
FastBO introduces two novel concepts: efficient points and saturation points. Efficient points represent configurations that achieve the best trade-off between computational cost and performance, while saturation points indicate when further training yields diminishing returns. The method works by first evaluating configurations at low fidelities, then using learning curve extrapolation to estimate where efficient and saturation points occur. These estimated points are then used to update the surrogate model, guiding future evaluations. For single-fidelity problems, FastBO replaces final fidelity evaluations with efficient point performances, effectively reducing computational cost while maintaining optimization quality.

## Key Results
- FastBO achieves strong anytime performance on LCBench, NAS-Bench-201, and FCNet benchmarks
- The method converges faster to optimal configurations compared to Hyper-Tune, BOHB, and ASHA
- FastBO demonstrates computational efficiency gains while maintaining or improving optimization quality

## Why This Works (Mechanism)
FastBO works by intelligently allocating computational resources through adaptive fidelity selection. By identifying efficient points that balance cost and performance early in the optimization process, the method avoids wasting resources on unnecessarily long training runs. The saturation point detection prevents over-computation on configurations that have already reached their performance plateau. The learning curve extrapolation allows FastBO to make informed decisions about when to stop training individual configurations, directing computational budget toward more promising areas of the search space.

## Foundational Learning
- Bayesian optimization fundamentals: Why needed - forms the theoretical basis for surrogate modeling; Quick check - understand acquisition functions and surrogate model updates
- Multi-fidelity optimization concepts: Why needed - enables computational efficiency through progressive evaluation; Quick check - grasp fidelity schedules and their impact on optimization
- Learning curve extrapolation techniques: Why needed - critical for predicting performance at higher fidelities; Quick check - understand regression methods for curve fitting
- Hyperparameter optimization basics: Why needed - domain context for applying FastBO; Quick check - familiarity with common HPO benchmarks and metrics
- Neural architecture search principles: Why needed - NAS applications demonstrate method versatility; Quick check - understand architecture spaces and evaluation protocols
- Computational budget allocation strategies: Why needed - core to FastBO's efficiency gains; Quick check - compare static vs. adaptive resource allocation approaches

## Architecture Onboarding

**Component Map:**
Configuration generator -> Fidelity selector -> Evaluator -> Learning curve estimator -> Efficient/saturation point detector -> Surrogate model updater -> Acquisition function

**Critical Path:**
Configuration generation → Low-fidelity evaluation → Learning curve extrapolation → Point detection → Surrogate model update → Next configuration selection

**Design Tradeoffs:**
- Early stopping vs. complete evaluation: FastBO favors early stopping at efficient points, trading potential accuracy gains for computational efficiency
- Fidelity granularity: Coarser fidelity schedules reduce overhead but may miss optimal points; finer schedules increase accuracy but computational cost
- Surrogate model complexity: More complex models may better capture fidelity relationships but require more data and computation

**Failure Signatures:**
- Poor learning curve extrapolation leading to incorrect point identification
- Overshooting efficient points due to inaccurate saturation detection
- Underutilization of computational budget when overly conservative point detection

**First Experiments:**
1. Replicate single-fidelity optimization results by replacing final evaluations with efficient point performances
2. Test learning curve extrapolation accuracy on configurations with known performance trajectories
3. Compare computational efficiency gains against baseline methods on small-scale HPO problems

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability beyond tested benchmarks (LCBench, NAS-Bench-201, FCNet) remains uncertain
- Performance on real-world problems with higher-dimensional search spaces and noisy evaluations untested
- Reliance on accurate learning curve extrapolation may fail with non-monotonic learning curves or sudden performance jumps

## Confidence
- Methodology and theoretical framework: High
- Empirical performance claims: Medium

## Next Checks
1. Test FastBO on diverse real-world datasets and model architectures to evaluate scalability and robustness to noisy evaluations
2. Conduct ablation studies to quantify individual contributions of efficient point identification versus saturation point detection
3. Implement and evaluate a variant handling non-monotonic learning curves to assess framework extensibility