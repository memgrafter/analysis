---
ver: rpa2
title: 'MSG-Chart: Multimodal Scene Graph for ChartQA'
arxiv_id: '2408.04852'
source_url: https://arxiv.org/abs/2408.04852
tags:
- graph
- chartqa
- visual
- textual
- chart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Chart Question Answering
  (ChartQA), where models must understand complex chart elements and their relationships
  to answer questions accurately. The authors propose a multimodal scene graph (MSG-Chart)
  that captures both visual and textual relationships in charts.
---

# MSG-Chart: Multimodal Scene Graph for ChartQA

## Quick Facts
- arXiv ID: 2408.04852
- Source URL: https://arxiv.org/abs/2408.04852
- Authors: Yue Dai; Soyeon Caren Han; Wei Liu
- Reference count: 34
- Primary result: MSG-Chart improves ChartQA performance by integrating visual and textual scene graphs as inductive bias

## Executive Summary
This paper addresses the challenge of Chart Question Answering (ChartQA), where models must understand complex chart elements and their relationships to answer questions accurately. The authors propose a multimodal scene graph (MSG-Chart) that captures both visual and textual relationships in charts. The visual graph encodes spatial relationships between chart elements using bounding box distances, while the textual graph encodes semantic relationships using chart labels and OCR text. These graphs are integrated as inductive bias into two backbone models: UniChart and VL-T5. Experiments on ChartQA and OpenCQA datasets show that MSG-Chart improves performance, with UniChart achieving 61.4% accuracy (vs. 58.4% baseline) on ChartQA and VL-T5 achieving 64.8% accuracy (vs. 59.1% baseline). Ablation studies confirm that both visual and textual graphs contribute to improved performance, with the visual graph being particularly important for extractive questions. The proposed approach demonstrates that explicit modeling of chart structure and semantics enhances chart understanding and reasoning capabilities.

## Method Summary
The MSG-Chart approach constructs two complementary scene graphs for chart images: a visual graph capturing spatial relationships between detected chart elements using bounding box distances as edge weights, and a textual graph encoding semantic relationships between chart labels and OCR-extracted text using predefined chart-specific rules. These graphs are processed through graph convolutional networks to generate node representations, which are then fused with visual features from backbone models (UniChart or VL-T5) as inductive bias. This integration enhances the model's understanding of chart structure and semantics without modifying the backbone architecture. The method is evaluated on ChartQA and OpenCQA datasets, demonstrating improved accuracy over baseline models through better exploitation of both spatial and semantic chart information.

## Key Results
- UniChart with MSG-Chart achieves 61.4% accuracy on ChartQA dataset vs. 58.4% baseline
- VL-T5 with MSG-Chart achieves 64.8% accuracy on ChartQA dataset vs. 59.1% baseline
- Both visual and textual graphs contribute to performance improvements, with visual graph particularly important for extractive questions
- MSG-Chart shows generalization ability when transferred between UniChart and VL-T5 backbone models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The visual graph captures spatial relationships between chart elements using bounding box distances as edge weights.
- Mechanism: Each node represents a detected chart object, and edges between nodes are weighted by the exponential of negative Euclidean distance between bounding boxes. This prioritizes closer objects and encodes spatial proximity as structural knowledge.
- Core assumption: Spatial proximity in charts correlates with functional or semantic relationships (e.g., axis labels near data points).
- Evidence anchors:
  - [abstract] "visual graph encodes spatial relationships between chart elements using bounding box distances"
  - [section 2.1] "We assign each edge ð‘’ a coefficient ð‘Žð‘’ = exp(âˆ’ð‘‘), where ð‘‘ represents the smallest Euclidean distance between the bounding boxes of two objects"
  - [corpus] Weak - no direct citation of distance-based edge weighting in related work
- Break condition: If chart layouts violate typical spatial conventions (e.g., non-standard chart designs), the distance-based edge weights may encode irrelevant relationships.

### Mechanism 2
- Claim: The textual graph encodes semantic relationships using predefined chart-specific rules connecting labels and OCR text.
- Mechanism: Label nodes are connected based on chart semantics (e.g., x-axis title connects to x-axis labels, legend labels connect to corresponding markers). OCR nodes represent extracted text and connect to relevant label nodes, creating a semantic web of chart information.
- Core assumption: Chart semantics follow predictable patterns that can be encoded as graph edges (e.g., legend markers correspond to data series labels).
- Evidence anchors:
  - [abstract] "textual graph encodes semantic relationships using chart labels and OCR text"
  - [section 2.1] "Label nodes are connected based on the chart semantics. The x/y axis title connects to the x/y axis labels... Legend labels connect to the closest legend marker"
  - [corpus] Weak - no direct citation of rule-based semantic graph construction in related work
- Break condition: If charts deviate from standard formats or use unconventional labeling schemes, the predefined semantic rules may fail to capture correct relationships.

### Mechanism 3
- Claim: Graph representations are fused with visual features as inductive bias, enhancing the backbone model's understanding without modifying its architecture.
- Mechanism: Graph node representations are concatenated with corresponding visual features and projected to match the backbone's hidden state space. This provides explicit structural and semantic information that the model can leverage during training.
- Core assumption: The backbone model can effectively integrate external graph representations with its learned visual features to improve performance.
- Evidence anchors:
  - [abstract] "This graph module can be easily integrated with different vision transformers as inductive bias"
  - [section 2.2] "we inject the graph representation as the inductive bias to the backbone model"
  - [corpus] Weak - no direct citation of graph-as-inductive-bias approach in related ChartQA work
- Break condition: If the projection layer fails to align graph and visual feature spaces properly, the integration may introduce noise rather than useful information.

## Foundational Learning

- Concept: Graph Convolutional Networks (GCNs)
  - Why needed here: GCNs are used to propagate information between nodes in both visual and textual graphs, capturing local graph structure and node relationships.
  - Quick check question: What is the key operation that allows GCNs to aggregate information from a node's neighbors?

- Concept: Multimodal learning with vision-language models
  - Why needed here: The method integrates visual features (from image encoders) with textual/graph features, requiring understanding of how multimodal models fuse different input modalities.
  - Quick check question: How do typical multimodal models like VL-T5 combine visual and textual inputs during processing?

- Concept: Scene graph construction and reasoning
  - Why needed here: The method builds scene graphs specifically for charts, requiring understanding of how to represent objects, relationships, and semantics in graph form.
  - Quick check question: What distinguishes a scene graph from a simple object detection output?

## Architecture Onboarding

- Component map:
  Input: Chart image + question text -> Mask R-CNN (object detection) -> Visual graph (distance-based edges) -> GCN layers -> Graph node representations
  Input: Chart image + question text -> OCR text extraction -> Textual graph (semantic rules) -> GCN layers -> Graph node representations
  Graph node representations + backbone visual features -> Feature fusion -> Backbone decoder -> Output: Answer generation

- Critical path:
  1. Detect chart objects and extract bounding boxes
  2. Construct visual graph with distance-based edges
  3. Extract labels and OCR text, construct textual graph with semantic rules
  4. Apply GCNs to both graphs to obtain node representations
  5. Fuse graph representations with visual features from backbone encoder
  6. Pass fused features through backbone decoder to generate answer

- Design tradeoffs:
  - Visual graph: Fully connected vs. sparse connections - fully connected captures all spatial relationships but increases computation
  - Textual graph: Predefined rules vs. learned connections - rules are interpretable but may not generalize to all chart types
  - Feature fusion: Concatenation vs. other operations - concatenation is simple but may not capture complex interactions

- Failure signatures:
  - Poor performance on charts with non-standard layouts (visual graph failure)
  - Failure on charts with unconventional labeling or legends (textual graph failure)
  - Degradation when chart elements are split across image patches (backbone limitation)
  - Overfitting to training chart styles if graph construction is too rigid

- First 3 experiments:
  1. Ablation study removing either visual or textual graph to measure individual contributions
  2. Test with fully connected textual graph vs. rule-based graph to validate semantic edge design
  3. Test with different edge weighting schemes in visual graph (e.g., linear vs. exponential distance weighting)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The semantic rules for textual graph construction are hand-designed and may not generalize to non-standard chart formats or unconventional labeling schemes
- The fully connected visual graph design introduces quadratic complexity that could limit scalability with chart complexity
- The method relies on accurate object detection and OCR, which are not evaluated for their own accuracy or failure modes

## Confidence
- High confidence: The overall performance improvement on ChartQA and OpenCQA datasets is well-supported by reported results and ablation studies.
- Medium confidence: The mechanism claims about why the visual and textual graphs improve performance are plausible but rely on assumptions about chart structure that may not hold universally.
- Medium confidence: The claim that the method can be easily integrated with different backbone models is supported by experiments with two models, but broader generalization to other architectures remains to be shown.

## Next Checks
1. Conduct experiments on charts with non-standard layouts and unconventional labeling to test the robustness of the predefined semantic rules in the textual graph.
2. Compare the fully connected visual graph approach with a sparse graph construction method to evaluate the tradeoff between completeness and computational efficiency.
3. Perform error analysis on cases where the method fails, categorizing failures by type (visual graph issues, textual graph issues, backbone limitations) to identify specific weaknesses in the approach.