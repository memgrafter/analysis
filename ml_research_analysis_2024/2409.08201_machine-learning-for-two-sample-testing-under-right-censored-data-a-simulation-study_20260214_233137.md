---
ver: rpa2
title: 'Machine Learning for Two-Sample Testing under Right-Censored Data: A Simulation
  Study'
arxiv_id: '2409.08201'
source_url: https://arxiv.org/abs/2409.08201
tags:
- methods
- test
- proposed
- two-sample
- tests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops and evaluates machine learning methods for
  two-sample testing with right-censored data. The core approach treats classical
  two-sample tests as features and uses ML models (Logistic Regression, Random Forest,
  GBM, XGBoost, CatBoost, LightAutoML) to ensemble them into a stacking framework
  for binary classification (null vs.
---

# Machine Learning for Two-Sample Testing under Right-Censored Data: A Simulation Study

## Quick Facts
- arXiv ID: 2409.08201
- Source URL: https://arxiv.org/abs/2409.08201
- Authors: Petr Philonenko; Sergey Postovalov
- Reference count: 39
- Primary result: ML-based two-sample tests with right-censored data outperform classical methods on synthetic data

## Executive Summary
This study develops machine learning methods for two-sample testing under right-censored data by creating a stacking framework that combines classical statistical tests as features. The approach treats the two-sample testing problem as binary classification, using models like LightAutoML, CatBoost, and Random Forest to ensemble classical test statistics. The methods were trained on a large synthetic dataset (121+ million observations) and evaluated across various scenarios including different sample sizes, censoring rates, and alternative hypotheses.

The research demonstrates that ML-based methods can effectively handle the challenges of right-censored data, achieving higher power and accuracy than traditional statistical tests. The framework is particularly effective at identifying distributional differences when classical tests may struggle due to censoring effects. The methods have been made publicly available through GitHub and Hugging Face for reproducibility and practical application.

## Method Summary
The core methodology treats classical two-sample tests (Gehan, log-rank, Peto-Prentice, and Bagdonavičius-Nikulin variants) as feature generators, with their test statistics serving as inputs to machine learning models. The framework uses these classical test results as features for binary classification models (Logistic Regression, Random Forest, GBM, XGBoost, CatBoost, LightAutoML) to predict whether the null hypothesis of identical distributions should be rejected. The synthetic data generation process creates paired samples from various distributions with controlled censoring mechanisms, allowing systematic evaluation across different scenarios. The stacking approach enables the ML models to learn complex patterns in how classical tests perform under different conditions, potentially capturing interactions and synergies that individual tests might miss.

## Key Results
- ML-based methods (particularly LightAutoML and CatBoost) achieve higher accuracy and test power than classical tests across alternative hypotheses
- Bagdonavičius-Nikulin tests emerge as among the most important features in the ML models
- The framework maintains performance across varying sample sizes and censoring rates
- Power studies show ML methods outperform classical tests in average rank across tested scenarios

## Why This Works (Mechanism)
The approach works by leveraging the complementary strengths of multiple classical tests through machine learning ensembling. Each classical test captures different aspects of distributional differences under censoring, and ML models can learn which combinations of test statistics are most informative for specific scenarios. The stacking framework effectively creates a meta-learner that can adapt to the specific characteristics of each testing situation, including sample size, censoring rate, and the nature of the alternative hypothesis. This adaptive approach overcomes the limitations of individual tests that may perform well in some scenarios but poorly in others.

## Foundational Learning

**Classical two-sample tests with censored data**: These tests (Gehan, log-rank, Peto-Prentice) assess whether two survival distributions are identical. Why needed: They form the feature set for the ML models. Quick check: Verify that test statistics are correctly computed under different censoring patterns.

**Right-censoring mechanisms**: Understanding how censoring affects survival data and test statistics. Why needed: Different censoring patterns impact test performance differently. Quick check: Compare test performance under various censoring rates and mechanisms.

**Stacking ensemble methods**: ML approach combining multiple base learners (classical tests) through a meta-learner. Why needed: Enables learning complex patterns across test performances. Quick check: Validate that the stacking framework improves over individual classical tests.

**Feature importance analysis**: Methods to determine which features (test statistics) contribute most to predictions. Why needed: Identifies which classical tests are most informative. Quick check: Confirm feature importance rankings are stable across different training runs.

## Architecture Onboarding

**Component map**: Classical test statistics -> Feature preprocessing -> ML model (LightAutoML/CatBoost) -> Hypothesis decision

**Critical path**: Data generation → Classical test computation → Feature vector creation → Model training/prediction → Power calculation

**Design tradeoffs**: 
- Multiple classical tests provide rich feature space but increase computational cost
- ML models can capture complex patterns but require large training data
- Synthetic data enables controlled experiments but may not reflect real-world complexity

**Failure signatures**: 
- Poor performance on real data despite synthetic success suggests distribution mismatch
- Over-reliance on specific features indicates potential overfitting
- Low feature importance for all classical tests suggests the ML approach isn't capturing meaningful patterns

**3 first experiments**:
1. Test the framework on a small synthetic dataset with known ground truth to verify basic functionality
2. Compare single classical test performance vs. ML-ensemble performance on simple alternatives
3. Evaluate feature importance stability across multiple training runs with different random seeds

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on real-world clinical datasets remains unverified
- Synthetic data generation may not capture all complexities of actual survival data
- Computational efficiency for very large datasets (100,000+ observations) needs assessment
- Limited evaluation of informative censoring scenarios where censoring depends on covariates

## Confidence

**High confidence**:
- Feature importance analysis identifying Bagdonavičius-Nikulin tests as significant features within the studied framework

**Medium confidence**:
- ML methods outperform classical tests on synthetic data (simulation-based validation limits generalizability)
- LightAutoML and CatBoost perform best (evaluation focused on specific test scenarios)

## Next Checks

1. Apply the ML-based testing framework to at least three real-world survival datasets with known distributional differences to validate performance outside synthetic conditions

2. Conduct sensitivity analysis across different censoring mechanisms, particularly informative censoring where censoring depends on covariates

3. Compare computational efficiency and runtime scalability with classical methods on datasets exceeding 100,000 observations to assess practical feasibility for large-scale applications