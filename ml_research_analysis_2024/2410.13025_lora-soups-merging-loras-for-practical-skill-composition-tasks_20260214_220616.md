---
ver: rpa2
title: 'LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks'
arxiv_id: '2410.13025'
source_url: https://arxiv.org/abs/2410.13025
tags: []
core_contribution: This paper studies skill composition for large language models
  (LLMs) using Low-Rank Adaptation (LoRA) merging techniques. The key problem addressed
  is how to combine specialized LoRA modules trained on different skills to solve
  compositional tasks requiring multiple skills, particularly when training data for
  the target task is scarce.
---

# LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks

## Quick Facts
- **arXiv ID**: 2410.13025
- **Source URL**: https://arxiv.org/abs/2410.13025
- **Reference count**: 40
- **Primary result**: Learnable Concatenation (CAT) method achieves 43% better performance on hard math-word problems and 12% better on question-answering tasks compared to existing LoRA merging methods.

## Executive Summary
This paper addresses the challenge of combining specialized LoRA modules trained on different skills to solve compositional tasks requiring multiple skills. The key contribution is the Learnable Concatenation (CAT) method, which learns optimal layer-wise merging weights through gradient descent rather than using fixed coefficients. CAT outperforms existing LoRA merging techniques (TIES, DARE, MoE) and data mixing by significant margins, demonstrating super-linear improvements when combining complementary skills like math and code. The method also shows better robustness to prompt format variations and improved performance on reading comprehension tasks.

## Method Summary
The paper introduces Learnable Concatenation (CAT), a method for optimally combining LoRA modules trained on different skills. Unlike existing approaches that use fixed merging coefficients, CAT learns separate weights α1, α2 for each layer by training only these weights on a small subset of the skill datasets. The method works by freezing the pre-trained LoRA skill modules and learning the concatenation coefficients through gradient descent. This layer-wise learning approach allows the model to discover which skills are more important for different layers of the network, leading to superior performance on compositional tasks compared to static coefficient methods.

## Key Results
- CAT achieves 43% better performance on GSM-Hard math-word problems compared to existing LoRA merging methods
- Super-linear improvements demonstrated: combining math and code skills yields 257% improvement over baseline compared to 140% + 36% from individual skills
- CAT improves robustness to prompt format variations and provides better performance on reading comprehension tasks compared to alternatives
- Outperforms data mixing by 12% on question-answering tasks while avoiding catastrophic forgetting issues

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learning optimal layer-wise merging coefficients through gradient descent (CAT) improves compositional task performance compared to static coefficients.
- **Mechanism:** CAT learns a separate weight α1, α2 for each layer by training only these weights on a small subset of the skill datasets. This allows the model to discover which skills are more important for different layers of the network.
- **Core assumption:** The optimal contribution of each skill varies across layers, and these layer-wise differences can be discovered through gradient-based optimization.
- **Evidence anchors:** [abstract] "Our main contribution is to show that concatenation of LoRAs (CAT), which optimally weights LoRAs that were individually trained on different skills, outperforms existing model- and data- merging techniques" - [section] "we set α1, α2 as trainable parameters. We learn α1, α2 layer-wise (shown to be superior to static layer weights in prior works in the vision community [69])" - [corpus] "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering"
- **Break condition:** If the skill composition task requires consistent skill contributions across all layers, the layer-wise learning advantage disappears.

### Mechanism 2
- **Claim:** Combining math and code skills through LoRA merging achieves super-linear improvements by creating new problem-solving capabilities not present in either individual skill.
- **Mechanism:** When solving GSM-Hard problems, the merged model can leverage mathematical reasoning from the math LoRA and code generation from the code LoRA to solve problems that neither could solve alone. The CAT method optimally combines these complementary capabilities.
- **Core assumption:** The skills are complementary rather than redundant, and their combination creates new emergent capabilities.
- **Evidence anchors:** [section] "Table 2 demonstrates that with CAT, we achieve a super-linear improvement on GSM-Hard. The performance improvement when finetuning on both math and code... is superior to the sum of the improvements on code only and math only" - [section] "if Math solves 8% more problems than the base model, and Code solves 2% more problems, then the total number of problems solved by both models is at most 10% more than the original model, with a total of 16% solved problems. The merged model, however, solves 21% of the problems"
- **Break condition:** If the skills are not complementary or if the compositional task doesn't require both skills simultaneously, the super-linear improvement disappears.

### Mechanism 3
- **Claim:** LoRA merging methods are more robust to prompt format variations than data mixing because they learn to focus on semantic content rather than surface formatting.
- **Mechanism:** By training on different prompt formats through separate LoRAs and then merging, the model learns to extract task-relevant information regardless of formatting. Data mixing fails because it tries to memorize specific formatting patterns.
- **Core assumption:** LoRA merging can disentangle task semantics from formatting artifacts, while data mixing cannot.
- **Evidence anchors:** [section] "we observe that data mixing performs poorly, even worse than the individual models. Regarding the merged models, the results are variable" - [section] "CAT is the best method both in terms of pairwise win fraction and ELO ratings" on reading comprehension task
- **Break condition:** If the task is highly sensitive to exact formatting (not just semantic content), LoRA merging may not help and could even hurt performance.

## Foundational Learning

- **Concept:** Low-Rank Adaptation (LoRA) and its mathematical formulation
  - Why needed here: The entire paper builds on LoRA as the foundation for parameter-efficient skill composition
  - Quick check question: What is the mathematical form of a LoRA update and why is it more parameter-efficient than full fine-tuning?

- **Concept:** Catastrophic forgetting and its relevance to sequential fine-tuning
  - Why needed here: The paper explicitly mentions catastrophic forgetting as a problem that model merging solves compared to sequential data mixing
  - Quick check question: Why does fine-tuning on instruction data after domain data cause catastrophic forgetting, and how does model merging avoid this?

- **Concept:** Mixture of Experts (MoE) architecture and routing mechanisms
  - Why needed here: MoE is presented as one of the LoRA merging baselines, and understanding its routing mechanism is crucial for comparing with CAT
  - Quick check question: How does the MoE routing mechanism differ from CAT's fixed merging weights, and what are the implications for skill composition?

## Architecture Onboarding

- **Component map:** Base Llama-7b model (frozen) -> Multiple LoRA modules (one per skill) -> CAT layer with learnable weights α1, α2 per layer -> Optional router for MoE variant
- **Critical path:**
  1. Train individual LoRA modules on their respective skill datasets
  2. Prepare small validation dataset containing examples from both skills
  3. Train CAT weights on this validation set while keeping LoRA weights frozen
  4. Merge using learned weights for inference
- **Design tradeoffs:**
  - Rank 32 vs higher rank: Lower rank is more parameter-efficient but may limit representational capacity
  - Layer-wise vs global weights: Layer-wise gives more flexibility but increases parameter count
  - CAT vs MoE: CAT is simpler and doesn't require routers, but MoE can adapt to input-dependent skill needs
- **Failure signatures:**
  - Poor performance on compositional tasks suggests skills aren't complementary or CAT weights didn't learn properly
  - Performance similar to individual LoRAs suggests merging coefficients are too extreme (close to 0 or 1)
  - Degradation on original skills suggests interference between merged modules
- **First 3 experiments:**
  1. GSM-Hard math-word problems with math and code LoRAs to verify super-linear improvement
  2. QABot task with biology/game manual and Alpaca LoRAs to test practical application
  3. Prompt robustness test with format variations to validate CAT's robustness claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CAT performance scale when composing more than two skills, and what is the theoretical limit of skill composition before performance degrades significantly?
- Basis in paper: Inferred from Section 5's discussion of limitations, which states "Our work is a first step towards using model merging for solving skill composition tasks. Our study is limited to settings where the number of needed skills k = 2."
- Why unresolved: The paper only evaluates binary skill composition tasks and mentions that performance degrades when attempting to merge three skills (Figure 12 shows all merging methods performing worse than DATA-MIX when trained on 3 prompt formats). The authors acknowledge this as an important future direction but don't provide empirical data on multi-skill composition performance.
- What evidence would resolve it: Experiments showing performance curves of CAT and other merging methods as the number of skills increases from 2 to 5-10, with systematic evaluation on increasingly complex compositional tasks.

### Open Question 2
- Question: What is the relationship between the rank of LoRA updates (r) and the effectiveness of skill composition, and is there an optimal rank that balances parameter efficiency with compositional capability?
- Basis in paper: Inferred from Section 3's experimental details which states "Our base model is a Llama-7b and set the LoRA rank r = 32" and Section 4.1's discussion of "super-linear improvement" which suggests the rank choice may impact compositional capabilities.
- Why unresolved: The paper uses a fixed rank of 32 without exploring how different rank values affect skill composition performance. The rank choice could significantly impact the model's ability to capture and combine multiple skills effectively.
- What evidence would resolve it: Systematic experiments varying the LoRA rank (e.g., 16, 32, 64, 128) across different skill composition tasks, measuring both performance and parameter efficiency trade-offs.

### Open Question 3
- Question: What is the minimum amount of training data required for CAT to learn effective merging coefficients, and how does performance scale with decreasing training data?
- Basis in paper: Explicit from Section 3 which states "We freeze the trained LoRA skill modules and train α1, α2 on a dataset made by selecting the minimum of 5% of the data points from both skill 1 and skill 2" and uses "a learning rate of 1e−4" for 1 epoch.
- Why unresolved: The paper uses 5% of data from both skills but doesn't explore whether this is optimal or what happens with less data. The sensitivity of CAT to training data quantity for coefficient learning is unknown.
- What evidence would resolve it: Experiments systematically reducing the training data percentage (1%, 2.5%, 5%, 10%, 20%) and measuring coefficient learning convergence and skill composition performance, identifying the minimum effective training data threshold.

### Open Question 4
- Question: How does CAT's layer-wise learning of merging coefficients compare to learning global coefficients across all layers, and what is the optimal granularity for coefficient learning?
- Basis in paper: Explicit from Section 2.2 which states "CAT learns these coefficients layer-wise with access to very few examples in the natural language domain" and distinguishes this from "TIES, DARE, LoRA Hub that learn static values for every layer."
- Why unresolved: While the paper claims layer-wise learning is superior to static layer weights, it doesn't compare this approach to learning a single global coefficient or to other granularities (e.g., per-module coefficients). The optimal granularity for coefficient learning remains unknown.
- What evidence would resolve it: Experiments comparing CAT with layer-wise coefficients to variants with global coefficients, per-module coefficients, and other granularities, measuring performance and computational efficiency across skill composition tasks.

## Limitations
- The paper only evaluates skill composition for 2-3 skills, with no systematic study of scaling to many more skills
- Super-linear improvement claims are based on only 2 skill combinations, limiting generalizability
- Performance on tasks requiring more than 2 skills simultaneously is not addressed

## Confidence
**High Confidence**: CAT outperforms TIES, DARE, and MoE on tested GSM-Hard and QABot tasks; layer-wise weight learning is superior to static weights (consistent with prior vision work); MoE router performance issues are well-documented in the literature

**Medium Confidence**: Super-linear improvement claims based on only 2 skill combinations; robustness to prompt variations, though tested on only one task (QABot); comparison to data mixing assumes similar training compute, which wasn't strictly controlled

**Low Confidence**: Scalability to many skills or larger models; performance on tasks requiring more than 2 skills simultaneously; behavior when skills are not complementary

## Next Checks
1. **Scale Test**: Apply CAT to merge 5+ skills on a larger model (e.g., Llama-13b) and measure whether layer-wise learning remains computationally tractable and effective.

2. **Skill Overlap Analysis**: Design experiments where skills have significant overlap (e.g., math and physics reasoning) to test whether CAT can handle competing rather than complementary skills.

3. **Cross-Domain Generalization**: Evaluate CAT-merged models on completely unseen compositional tasks (e.g., math + biology reasoning) that weren't represented in the training LoRA modules.