---
ver: rpa2
title: On the Duality Between Sharpness-Aware Minimization and Adversarial Training
arxiv_id: '2402.15152'
source_url: https://arxiv.org/abs/2402.15152
tags:
- adversarial
- robustness
- training
- accuracy
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Sharpness-Aware Minimization (SAM) as a
  defense against adversarial attacks. While SAM is known to improve clean accuracy,
  its effect on adversarial robustness was unexplored.
---

# On the Duality Between Sharpness-Aware Minimization and Adversarial Training

## Quick Facts
- arXiv ID: 2402.15152
- Source URL: https://arxiv.org/abs/2402.15152
- Reference count: 40
- This paper investigates Sharpness-Aware Minimization (SAM) as a defense against adversarial attacks, showing it can improve robustness without sacrificing natural accuracy.

## Executive Summary
This paper explores the relationship between Sharpness-Aware Minimization (SAM) and adversarial training (AT) as methods for improving model robustness against adversarial attacks. While AT is known to improve robustness, it typically degrades clean accuracy. The authors demonstrate that SAM, which perturbs model weights rather than inputs during training, can achieve adversarial robustness while maintaining or even improving natural accuracy. Through empirical experiments across various tasks, data modalities, and model architectures, they show SAM consistently improves robustness compared to standard training, while AT inevitably hurts natural accuracy. The paper provides both theoretical insights and extensive experimental validation of this phenomenon.

## Method Summary
The paper compares SAM with standard training and adversarial training across multiple tasks and datasets. SAM is implemented with a perturbation parameter ρ ∈ {0.1, 0.2, 0.3, 0.4} using the SAM optimizer. Adversarial training uses ℓ∞ and ℓ2 norms with varying ε values. Experiments span image classification (CIFAR-10, CIFAR-100, Tiny ImageNet), semantic segmentation (Stanford Background Dataset, VOC2012), and text classification (Rotten Tomatoes dataset). Models include PreActResNet-18, Wider ResNet, Vision Transformer, DeepLabv3, MobileNetv2, and DistilBERT. Robustness is evaluated against multiple attack types including FGSM, PGD, AutoAttack, StAdv, FAB, and Pixel attacks, along with common corruption benchmarks.

## Key Results
- SAM improves adversarial robustness across all tested tasks and model architectures while maintaining or improving natural accuracy
- SAM achieves 32.60% corruption robustness on CIFAR-10C compared to AT's 15.67%
- SAM variants (ASAM, ESAM) provide computational efficiency advantages over FastAT while maintaining similar robustness
- Theoretical analysis shows SAM and AT can be unified under a common optimization framework with different perturbation spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM improves adversarial robustness by implicitly learning robust features through weight perturbation
- Mechanism: By adding perturbations to model weights instead of inputs, SAM biases the model toward features that remain stable under small changes, which correspond to robust features that are less sensitive to adversarial attacks
- Core assumption: Features that are stable under weight perturbations will also be stable under input perturbations due to the duality between input and weight perturbations
- Evidence anchors:
  - [abstract] "we show that weight perturbation during training can help the model implicitly learn the robust features"
  - [section] "We start by rewriting the optimization objective of SAM and AT in a unified form" and "both techniques involve adding perturbation to make the output more robust w.r.t.input or weight changes"
  - [corpus] "Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization" - suggests SAM can improve robustness in different domains
- Break condition: If the relationship between weight stability and input stability doesn't hold, or if the perturbation strength is too weak to learn meaningful robust features

### Mechanism 2
- Claim: SAM achieves robustness without sacrificing natural accuracy because it perturbs weights while keeping original samples
- Mechanism: SAM maintains the original data distribution during training while finding flatter loss landscapes, avoiding the distribution shift that occurs when training with adversarial examples
- Core assumption: Maintaining the natural data distribution during training preserves clean accuracy while weight perturbations provide robustness benefits
- Evidence anchors:
  - [abstract] "AT suffers from an intrinsic limitation that decreases the clean accuracy" and "SAM perturbs model weights yet keeps using the original samples during learning"
  - [section] "SAM applies weight perturbation to achieve this robustness but keeps the original samples during learning, which can implicitly bias more weight on robust features"
  - [corpus] Weak evidence - no direct citations about SAM maintaining natural accuracy while improving robustness
- Break condition: If weight perturbations cause significant distribution shift in the feature space, or if the perturbation strength becomes too large

### Mechanism 3
- Claim: SAM provides a more moderate perturbation approach compared to AT, leading to a better accuracy-robustness tradeoff
- Mechanism: SAM uses smaller, implicit perturbations in weight space rather than direct, large perturbations in input space, resulting in less degradation of natural accuracy
- Core assumption: Smaller perturbations in weight space can achieve meaningful robustness gains without the severe accuracy penalty of large input perturbations
- Evidence anchors:
  - [abstract] "AT applies larger and more straightforward perturbations to the input space, leading to better robustness but a loss in natural accuracy, which is not the original goal of SAM"
  - [section] "the perturbation of SAM is more moderate and implicit than AT" and "SAM requires a much larger perturbation range, while for AT, less perturbation overx is enough"
  - [corpus] "Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization" - suggests SAM variants can maintain robustness while optimizing for other objectives
- Break condition: If moderate weight perturbations prove insufficient for meaningful robustness, or if the implicit nature of perturbations makes them too weak

## Foundational Learning

- Concept: Duality between input and weight perturbations in neural network training
  - Why needed here: The paper's core argument relies on understanding that perturbing weights can have similar effects to perturbing inputs due to the mathematical relationship in forward passes
  - Quick check question: If W is a weight matrix and x is an input, what is the difference between W(x + δ) and (W + δ)x?

- Concept: Sharpness-Aware Minimization (SAM) optimization framework
  - Why needed here: SAM is the central algorithm being studied, and understanding its objective function is crucial to understanding why it might improve robustness
  - Quick check question: What is the key difference between SAM's objective function and standard gradient descent?

- Concept: Adversarial training and its inherent accuracy-robustness tradeoff
  - Why needed here: The paper compares SAM to adversarial training, so understanding why AT sacrifices accuracy is essential to appreciate SAM's benefits
  - Quick check question: Why does adversarial training typically result in lower clean accuracy compared to standard training?

## Architecture Onboarding

- Component map:
  - Base model architecture (e.g., PreActResNet-18, ViT)
  - Optimizer (SAM with ρ hyperparameter, or variants like ASAM/ESAM)
  - Data loaders for standard, adversarial, and corrupted datasets
  - Attack modules for evaluating robustness (FGSM, PGD, AutoAttack, etc.)
  - Evaluation pipeline for clean accuracy, adversarial accuracy, and corruption robustness

- Critical path:
  1. Data loading and preprocessing
  2. Model initialization
  3. SAM optimizer setup with appropriate ρ value
  4. Training loop with weight perturbation
  5. Evaluation against multiple attack types
  6. Comparison with baseline methods

- Design tradeoffs:
  - SAM ρ hyperparameter: Higher values improve robustness but may affect convergence
  - Attack strength in evaluation: Stronger attacks better reveal robustness gaps but increase computation time
  - Dataset choice: Standard datasets (CIFAR) vs. real-world corruptions for different robustness aspects

- Failure signatures:
  - No improvement over standard training: ρ value may be too small
  - Significant accuracy drop: ρ value may be too large or training unstable
  - Poor generalization to unseen attacks: Model may be overfitting to specific attack patterns
  - Computational overhead: SAM requires additional forward passes for weight perturbation

- First 3 experiments:
  1. Reproduce baseline results: Train with standard SGD/Adam and SAM (ρ=0.1) on CIFAR-10, compare clean accuracy
  2. Scale perturbation strength: Train SAM with increasing ρ values (0.1, 0.2, 0.3, 0.4) and plot accuracy/robustness tradeoff
  3. Compare against adversarial training: Train ℓ∞-AT and ℓ2-AT with small ϵ values, compare both accuracy and robustness to SAM results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between SAM's weight perturbation strength and its effectiveness at improving adversarial robustness?
- Basis in paper: [explicit] The paper derives theoretical relationships between SAM and AT in a simplified binary classification model (Theorems 4.4, 4.5, 4.6), but these may not generalize to complex multi-layer networks.
- Why unresolved: The theoretical analysis is limited to a simple model and cannot be directly generalized to DNNs. The paper acknowledges this limitation but doesn't provide concrete mathematical extensions.
- What evidence would resolve it: Formal mathematical proofs showing how SAM's weight perturbation relates to adversarial robustness in deep networks, or empirical studies demonstrating this relationship across various architectures.

### Open Question 2
- Question: How does SAM's implicit learning of robust features compare to AT's explicit feature elimination in terms of computational efficiency and generalization?
- Basis in paper: [inferred] The paper mentions that AT requires 10x more computational cost than standard training, while SAM variants are faster than FastAT, but doesn't provide detailed comparative analysis.
- Why unresolved: While the paper shows SAM maintains better clean accuracy than AT, it doesn't quantify the computational trade-offs or compare generalization performance across different tasks and datasets.
- What evidence would resolve it: Systematic experiments comparing SAM and AT's computational costs, generalization performance, and robustness across diverse tasks, model architectures, and datasets.

### Open Question 3
- Question: Why does SAM perform significantly better on common corruption robustness compared to AT, and can this be theoretically explained?
- Basis in paper: [explicit] The paper shows SAM achieves 32.60% corruption robustness vs AT's 15.67% on CIFAR-10C, but doesn't explain this surprising result.
- Why unresolved: The paper demonstrates SAM's superiority on corruption robustness but provides no theoretical justification or explanation for why weight perturbation helps with realistic perturbations.
- What evidence would resolve it: Theoretical analysis of how weight perturbation in SAM affects the model's ability to handle various types of corruptions, or empirical studies isolating the mechanisms behind SAM's corruption robustness.

## Limitations

- The theoretical foundation linking weight perturbations to input robustness relies on a duality assumption that, while mathematically elegant, lacks extensive empirical validation across diverse architectures
- The claim that SAM consistently improves robustness "without sacrificing natural accuracy" shows medium confidence - while CIFAR experiments support this, the semantic segmentation results show moderate accuracy degradation
- The assertion that AT "inevitably hurts natural accuracy" may be overstated, as the comparison focuses on small-ε AT rather than standard AT configurations

## Confidence

- SAM's implicit robust feature learning: Medium (strong theoretical framing but limited mechanistic validation)
- SAM's accuracy preservation advantage: Medium (supported by CIFAR but less so by segmentation tasks)
- AT's inevitable accuracy penalty: Medium-Low (comparison uses constrained AT parameters)

## Next Checks

1. **Perturbation strength scaling**: Systematically vary ρ across a wider range (0.05 to 0.5) on CIFAR-10 to precisely map the accuracy-robustness tradeoff curve and identify optimal operating points.

2. **Architecture-specific analysis**: Test SAM on architectures with different inductive biases (CNNs, transformers, MLPs) on the same tasks to determine if the robustness benefits are architecture-dependent.

3. **Long-tail distribution evaluation**: Evaluate SAM-trained models on out-of-distribution and corrupted data beyond CIFAR-10C to assess whether the robustness generalizes to real-world scenarios.