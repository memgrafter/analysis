---
ver: rpa2
title: A Fair Ranking and New Model for Panoptic Scene Graph Generation
arxiv_id: '2407.09216'
source_url: https://arxiv.org/abs/2407.09216
tags:
- graph
- scene
- masks
- segmentation
- predicate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Existing panoptic scene graph generation (PSGG) models are evaluated
  using a flawed protocol that allows multiple masks per object and duplicate relation
  predictions, inflating performance scores by up to 19.3 mR@50 for one-stage methods.
  The authors introduce a corrected evaluation protocol (SingleMPO) and show that
  two-stage methods remain competitive, while one-stage methods' scores drop significantly.
---

# A Fair Ranking and New Model for Panoptic Scene Graph Generation

## Quick Facts
- arXiv ID: 2407.09216
- Source URL: https://arxiv.org/abs/2407.09216
- Authors: Julian Lorenz; Alexander Pest; Daniel Kienzle; Katja Ludwig; Rainer Lienhart
- Reference count: 40
- One-line primary result: Existing panoptic scene graph generation (PSGG) models are evaluated using a flawed protocol that allows multiple masks per object and duplicate relation predictions, inflating performance scores by up to 19.3 mR@50 for one-stage methods.

## Executive Summary
This paper identifies a critical flaw in the evaluation protocol for panoptic scene graph generation (PSGG) models. The existing MultiMPO protocol allows multiple masks per object and duplicate relation predictions, artificially inflating performance scores, particularly for one-stage methods. The authors propose a corrected SingleMPO protocol and introduce DSFormer, a two-stage PSGG model that directly encodes subject-object masks into feature space using a transformer architecture. DSFormer achieves state-of-the-art performance, outperforming all existing PSGG models by +11 mR@50 and +10 mNgR@50 on the corrected evaluation.

## Method Summary
The authors propose a two-stage panoptic scene graph generation approach using DSFormer. The first stage uses a segmentation model (MaskDINO) to generate panoptic segmentation masks. The second stage, DSFormer, processes these masks using a transformer architecture. Instead of cropping features around bounding boxes, DSFormer adds learnable subject, object, and background tokens to each patch based on the overlap ratio with the masks. This allows the model to use global context while being aware of precise subject-object regions. The model is trained end-to-end with auxiliary losses for subject and object classification, and achieves state-of-the-art performance on the PSG dataset using the corrected SingleMPO evaluation protocol.

## Key Results
- Existing PSGG evaluation protocol (MultiMPO) inflates one-stage method scores by up to 19.3 mR@50 due to allowing multiple masks per object and duplicate relation predictions
- Two-stage methods are less affected by the evaluation flaw and remain competitive under the corrected SingleMPO protocol
- DSFormer outperforms all existing PSGG models, achieving +11 mR@50 and +10 mNgR@50 on the corrected evaluation
- DSFormer's performance is highly dependent on the quality of the first-stage segmentation model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MultiMPO allows duplicate masks per object and duplicate relations per subject-object pair, inflating scores.
- Mechanism: By outputting multiple masks for the same object, a model can match multiple predictions to the same ground truth object, each with a separate relation distribution. Similarly, by outputting multiple predicate distributions for the same subject-object pair, the model increases the probability of hitting the correct predicate in top-k evaluation.
- Core assumption: The evaluation metric aggregates over all masks and relations without enforcing uniqueness.
- Evidence anchors:
  - [abstract] "Previous evaluations on panoptic scene graphs have been subject to an erroneous evaluation protocol where multiple masks for the same object can lead to multiple relation distributions per mask-mask pair."
  - [section 3.1] "MultiMPO allows multiple masks to be considered as correct even if they are almost duplicates, as long as the IoU with the ground truth is larger than 50% for each mask."
- Break condition: If the evaluation protocol is updated to enforce single mask per object and single relation per subject-object pair (SingleMPO), this inflation disappears.

### Mechanism 2
- Claim: Two-stage methods are less affected by the evaluation flaw because they rely on a first-stage segmentation model that already outputs unique masks.
- Mechanism: The first-stage segmentation model (e.g., MaskDINO) produces non-overlapping, unique masks for each object. This ensures that downstream scene graph models do not have to deduplicate masks, preserving the integrity of the evaluation.
- Core assumption: The segmentation model is accurate and does not produce duplicate masks for the same object.
- Evidence anchors:
  - [section 3.2] "DSFormer doesn’t have to construct segmentation masks, its backbone is kept small and we use a ResNet-50 backbone from Faster R-CNN pretrained on object detection."
  - [section 4.1] "When evaluating two-stage methods on PSGG, a good segmentation model is essential for a good overall scene graph performance."
- Break condition: If the first-stage segmentation model produces duplicate masks or low-quality masks, the two-stage method's performance degrades.

### Mechanism 3
- Claim: DSFormer's architecture directly encodes subject-object mask overlap into patch tokens, improving relation classification.
- Mechanism: Instead of cropping features around bounding boxes (RoIAlign), DSFormer adds a weighted sum of learnable subject, object, and background tokens to each patch based on the overlap ratio with the subject/object masks. This allows the transformer to use global context while still being aware of the precise subject-object regions.
- Core assumption: The patch encoding can effectively represent the spatial relationship between subject and object masks.
- Evidence anchors:
  - [section 3.3] "Instead of using feature crops to tell DSFormer where the subject and object are located, we keep all the information from the backbone and add a prompt encoding to the patch tokens from the backbone."
  - [section 3.7] "DSFormer learns a unique token vector for each combination of subject class and object class."
- Break condition: If the patch encoding does not capture the necessary spatial information, the relation classification performance will suffer.

## Foundational Learning

- Concept: Panoptic segmentation
  - Why needed here: Panoptic scene graphs require segmentation masks instead of bounding boxes to precisely define objects and their interactions.
  - Quick check question: What is the difference between semantic segmentation and instance segmentation in the context of panoptic segmentation?

- Concept: Scene graph generation metrics (mR@k, mNgR@k)
  - Why needed here: The paper introduces a new evaluation protocol (SingleMPO) and compares existing methods using these metrics. Understanding how these metrics are calculated is crucial for interpreting the results.
  - Quick check question: How does mNgR@k differ from mR@k in terms of allowing multiple predicates per subject-object pair?

- Concept: Vision transformers (ViT)
  - Why needed here: DSFormer uses a ViT-inspired transformer module to process the patch tokens and classify relations. Understanding the basics of ViT is important for understanding the model architecture.
  - Quick check question: What is the role of positional encoding in vision transformers, and how is it implemented in DSFormer?

## Architecture Onboarding

- Component map:
  - Backbone: ResNet-50 + Feature Pyramid Network
  - Patch tokenization: 8x8 patches, embedded to 384 dimensions
  - Subject-object encoding: Learnable tokens weighted by mask overlap
  - Transformer module: 6 layers, 384 embedding dimension
  - Classification head: MLP projecting to predicate scores + no-relation class
  - Auxiliary heads: Subject and object classification (for training only)

- Critical path:
  1. Input image → backbone → FPN → feature pyramid
  2. Feature pyramid → patch tokenization → subject-object encoding
  3. Encoded patches → transformer → classification head
  4. Subject-object pairs → relation scores → top-k selection

- Design tradeoffs:
  - Two-stage vs. one-stage: Two-stage methods rely on a good segmentation model but can leverage foundation models; one-stage methods are more end-to-end but may struggle with mask quality.
  - Mask encoding: Using full mask overlap vs. bounding box crops; full masks provide more precise spatial information but increase complexity.
  - Auxiliary loss: Helps the model understand subject/object regions but adds training complexity.

- Failure signatures:
  - Low mR@k but high PQ: Segmentation model is good but relation classification is poor.
  - High mR@k but low PQ: Segmentation model is poor, but relation classification is good on the available masks.
  - Low mNgR@k: Model struggles with multi-label relations or ranking predicates within a relation.

- First 3 experiments:
  1. Train DSFormer with rectangular masks (bounding boxes) instead of full masks to see the impact of mask encoding.
  2. Remove the auxiliary node loss to assess its contribution to performance.
  3. Vary the embedding dimension size of the transformer module to find the optimal balance between performance and efficiency.

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The paper's evaluation protocol correction (SingleMPO) assumes that duplicate masks and relations should not be allowed, which is a reasonable but not universally validated position
- DSFormer's performance is highly dependent on the quality of the first-stage segmentation model, creating a potential bottleneck
- The paper doesn't explore how DSFormer would perform with pretraining on larger segmentation datasets

## Confidence
- MultiMPO flaw identification: High
- Two-stage method robustness: Medium
- DSFormer architecture superiority: Medium

## Next Checks
1. Test DSFormer performance with degraded segmentation quality to quantify the first-stage dependency
2. Evaluate whether alternative PSGG evaluation protocols (e.g., requiring strict mask IoU thresholds) yield similar conclusions about method rankings
3. Compare DSFormer against recent one-stage methods that may have implemented duplicate prevention internally