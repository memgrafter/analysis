---
ver: rpa2
title: 'MEANT: Multimodal Encoder for Antecedent Information'
arxiv_id: '2411.06616'
source_url: https://arxiv.org/abs/2411.06616
tags:
- tempstock
- meant
- which
- price
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MEANT, a Multimodal Encoder for Antecedent
  Information, and a new dataset called TempStock for multimodal, time-dependent financial
  prediction. MEANT combines a dual-encoder architecture with a novel temporal attention
  mechanism to capture dependencies across multiple modalities over time.
---

# MEANT: Multimodal Encoder for Antecedent Information

## Quick Facts
- arXiv ID: 2411.06616
- Source URL: https://arxiv.org/abs/2411.06616
- Reference count: 22
- Primary result: MEANT improves multimodal stock prediction by over 15% on TempStock and StockNet benchmarks

## Executive Summary
MEANT is a Multimodal Encoder for Antecedent Information designed to improve financial prediction by combining textual and visual data over time. The method introduces a novel temporal attention mechanism called Query-Targeting, which emphasizes the final day in lag periods to capture dependencies across modalities. Evaluated on a new dataset called TempStock and the StockNet benchmark, MEANT achieves significant performance gains over existing baselines, with ablation studies highlighting the importance of textual data in this task.

## Method Summary
MEANT employs a dual-encoder architecture with a language encoder (FinBERT) and a vision encoder (TimeSFormer) to process Tweets and MACD graphs, respectively. Temporal attention with Query-Targeting focuses on the final lag day to extract patterns for stock prediction. The model is trained on TempStock, a dataset of 1,755,998 Tweets and MACD graphs for S&P 500 companies, using AdamW optimizer with cosine annealing learning rate scheduling. Binary classification (buy/sell signals) is evaluated using Precision, Recall, and F1 scores.

## Key Results
- MEANT achieves over 15% improvement in F1 score compared to existing baselines on both TempStock and StockNet datasets.
- Ablation studies show textual data has greater impact than visual data for this financial prediction task.
- Query-Targeting in temporal attention emphasizes the final lag day, improving classification accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query-Targeting in temporal attention improves classification by focusing attention on the final lag day.
- Mechanism: The query matrix is derived solely from the final lag day, making the attention mechanism prioritize relationships between that day and earlier lag days.
- Core assumption: The final lag day contains the most relevant temporal context for predicting the target outcome.
- Evidence anchors:
  - [abstract]: "MEANT combines a dual-encoder architecture with a novel temporal attention mechanism to capture dependencies across multiple modalities over time."
  - [section]: "At inference time, we would want our model to extract a pattern from the preceding days, and act on current day with a sense of what will happen the next day... MEANT does this by using a strategy we call Query-Targeting..."
  - [corpus]: Weak evidence; the corpus focuses on multimodal encoder design rather than temporal query strategies.
- Break condition: If the final lag day does not correlate with target outcomes, performance degrades.

### Mechanism 2
- Claim: Sequence projection outperforms mean pooling for temporal encoding in StockNet.
- Mechanism: Sequence projection uses a parameterized matrix to compress token sequences into a fixed-size representation, preserving spatial information better than mean pooling.
- Core assumption: Parameterized compression retains more discriminative temporal patterns than simple averaging.
- Evidence anchors:
  - [abstract]: "The method leverages Query-Targeting in temporal attention, emphasizing the final day in lag periods for more accurate predictions."
  - [section]: "With sequence projection, MEANT performs abysmally... mean pooling manages to preserve spatial information... What seems to be happening here is our learned projection is throwing away crucial Tweet information..."
  - [corpus]: Weak evidence; corpus entries discuss general multimodal architectures but not specific temporal encoding comparisons.
- Break condition: If the projection matrix overfits or discards useful signal, mean pooling may become superior.

### Mechanism 3
- Claim: The dual-encoder architecture (vision + language) with TimeSFormer backbone captures both short- and long-range temporal dependencies.
- Mechanism: Language encoder extracts short-term trends from Tweets; TimeSFormer processes long-term patterns in MACD graphs via space-time attention.
- Core assumption: Short-term sentiment signals and long-term price trends are complementary for predicting stock momentum.
- Evidence anchors:
  - [abstract]: "MEANT combines a dual-encoder architecture with a novel temporal attention mechanism to capture dependencies across multiple modalities over time."
  - [section]: "We extract image features using the TimeSFormer architecture... to find relationships in longer range information... while extracting language features from social media information to pick up more immediate trends."
  - [corpus]: Weak evidence; corpus neighbors discuss vision encoder design but not temporal dependency extraction.
- Break condition: If either modality is noisy or irrelevant, dual-encoder advantage disappears.

## Foundational Learning

- Concept: Self-attention mechanism
  - Why needed here: Enables modeling of complex temporal dependencies without recurrence.
  - Quick check question: How does self-attention differ from RNN-based recurrence in handling long-range dependencies?

- Concept: Transformer encoder architecture
  - Why needed here: Provides the base for stacking attention layers and residual connections.
  - Quick check question: What role do residual connections play in preventing vanishing gradients?

- Concept: Positional encoding
  - Why needed here: Preserves order information in sequences where token order matters.
  - Quick check question: Why is positional encoding essential when using self-attention on time-series data?

## Architecture Onboarding

- Component map: Image encoder (TimeSFormer) → Language encoder (BERT-style) → Temporal attention → MLP head
- Critical path: Input → Dual encoders → Temporal attention (Query-Targeting) → Classification
- Design tradeoffs: Early fusion vs. dual-encoder; mean pooling vs. sequence projection; parameter count vs. overfitting risk
- Failure signatures: Poor F1 with high variance across epochs suggests overfitting; consistent low precision suggests query-target misalignment
- First 3 experiments:
  1. Replace Query-Targeting with standard self-attention to test impact on final-day emphasis.
  2. Swap TimeSFormer for a ViT backbone to assess importance of space-time vs. spatial-only attention.
  3. Toggle between mean pooling and sequence projection on the language encoder to measure temporal encoding sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MEANT be adapted to handle multi-level classification beyond binary buy/sell signals, such as different intensity levels of market momentum?
- Basis in paper: [explicit] The paper mentions that MEANT was trained to identify buy and sell signals but notes that for practical financial use, more nuanced levels of categorization would likely be needed.
- Why unresolved: The current binary classification approach may not capture the complexity of real-world market movements, and the paper does not explore multi-level classification strategies.
- What evidence would resolve it: Testing MEANT on datasets with multi-level classifications (e.g., weak buy, strong buy, hold, strong sell) and comparing performance against binary classification would provide insights.

### Open Question 2
- Question: How does the performance of MEANT vary when applied to stock market datasets from different indices or time periods with varying market conditions?
- Basis in paper: [inferred] The paper focuses on the S&P 500 index and a specific time period, but acknowledges that extreme market events or different indices might affect model performance.
- Why unresolved: The dataset used in the paper may not represent all market conditions, and the model's robustness to different indices or extreme events is not tested.
- What evidence would resolve it: Evaluating MEANT on datasets from other indices (e.g., NASDAQ, Russell 2000) or during periods of market volatility (e.g., 2008 financial crisis, 2020 pandemic) would demonstrate its generalizability.

### Open Question 3
- Question: Can the Query-Targeting mechanism in MEANT be made adaptive to automatically identify the most relevant components for attention, rather than focusing solely on the final day in the lag period?
- Basis in paper: [explicit] The paper notes that Query-Targeting emphasizes the final day in the lag period and suggests exploring ways to make this mechanism more robust to dependencies across the entire input.
- Why unresolved: The current Query-Targeting strategy may not always capture the most relevant temporal dependencies, especially in tasks where the final day is not the most informative.
- What evidence would resolve it: Developing and testing adaptive Query-Targeting mechanisms that learn to prioritize different days or components based on the task would clarify their effectiveness.

### Open Question 4
- Question: How does the inclusion of additional modalities, such as audio or sentiment scores, affect the performance of MEANT on multimodal financial prediction tasks?
- Basis in paper: [inferred] The paper focuses on text, images, and price data but does not explore the impact of additional modalities like audio or sentiment scores.
- Why unresolved: The paper does not investigate whether incorporating more modalities could further improve model performance or whether it introduces noise.
- What evidence would resolve it: Training MEANT on datasets that include audio (e.g., earnings calls) or sentiment scores and comparing performance against the current model would provide insights.

## Limitations

- The temporal attention mechanism's query-targeting strategy relies heavily on the assumption that the final lag day contains the most predictive information, but this assumption is not empirically validated across different market regimes or asset classes.
- The TempStock dataset, while larger than StockNet, covers only a single year of S&P 500 data and may not capture long-term market dynamics or rare events.
- The ablation study showing text dominance over visual data is based on a single dataset and timeframe, limiting generalizability.

## Confidence

- **High Confidence**: The dual-encoder architecture design and overall methodology for multimodal stock prediction are well-established approaches with clear implementation paths.
- **Medium Confidence**: The 15% performance improvement over baselines is compelling but requires independent validation, particularly given the limited dataset size and single market focus.
- **Low Confidence**: The specific claim that Query-Targeting in temporal attention is the primary driver of performance gains cannot be definitively separated from other architectural choices in the ablation studies.

## Next Checks

1. Test MEANT's temporal attention mechanism on out-of-sample market data from different time periods (pre-2022) to assess temporal generalization.
2. Conduct ablation studies on synthetic datasets where the ground truth of modality importance is known to validate the claim that text data has greater impact than visual data.
3. Evaluate MEANT's robustness to noisy or adversarial Tweets by introducing controlled perturbations in the training data and measuring performance degradation.