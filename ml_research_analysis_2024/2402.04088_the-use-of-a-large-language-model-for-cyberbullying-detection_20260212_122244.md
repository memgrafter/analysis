---
ver: rpa2
title: The Use of a Large Language Model for Cyberbullying Detection
arxiv_id: '2402.04088'
source_url: https://arxiv.org/abs/2402.04088
tags:
- cyberbullying
- detection
- dataset
- language
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the use of large language models for cyberbullying
  detection, addressing the limitations of existing machine learning approaches in
  handling class imbalance and generalization issues. The authors prepared a new balanced
  dataset from Formspring and Twitter, and compared various pre-trained models including
  BERT, RoBERTa, XLNet, and XLM-RoBERTa, as well as traditional methods like SVM and
  Random Forest.
---

# The Use of a Large Language Model for Cyberbullying Detection

## Quick Facts
- arXiv ID: 2402.04088
- Source URL: https://arxiv.org/abs/2402.04088
- Reference count: 40
- Primary result: RoBERTa achieves F1-score of 0.87 on balanced cyberbullying dataset, outperforming other models

## Executive Summary
This study investigates the use of large language models (LLMs) for cyberbullying detection, addressing key limitations of existing machine learning approaches. The authors prepared a new balanced dataset by combining Formspring and Twitter data, then compared various pre-trained models including BERT, RoBERTa, XLNet, and XLM-RoBERTa against traditional methods like SVM and Random Forest. Results demonstrate that RoBERTa achieves state-of-the-art performance with an F1-score of 0.87 on the balanced dataset, significantly outperforming other approaches and highlighting the effectiveness of transformer-based models in accurately detecting cyberbullying across diverse social media contexts.

## Method Summary
The authors prepared two datasets: D1 (highly imbalanced Formspring data with 10,792 non-cyberbullying and 703 cyberbullying instances) and D2 (balanced dataset combining Formspring and Twitter with 19,553 bullying and 19,526 non-bullying instances). They implemented four pre-trained language models (BERT, RoBERTa, XLNet, XLM-RoBERTa) using HuggingFace transformers with 4 training epochs, alongside traditional ML models (SVM with RBF kernel, Random Forest) using TF-IDF and SBERT embeddings. All models were evaluated using F1-score on cyberbullying class detection with a 90% training/10% testing split.

## Key Results
- RoBERTa achieved the highest F1-score of 0.87 on the balanced dataset D2
- Traditional ML models (SVM, Random Forest) showed significantly lower performance, especially on imbalanced data
- LLMs consistently outperformed traditional models on both balanced and imbalanced datasets
- The balanced dataset D2 led to improved detection performance compared to the imbalanced D1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models, particularly RoBERTa, outperform traditional ML models on imbalanced cyberbullying datasets because they can capture nuanced contextual cues in text.
- Mechanism: The transformer-based architectures in LLMs use bidirectional self-attention, allowing them to integrate left and right context, which is critical for detecting subtle cyberbullying cues that do not rely on explicit keywords.
- Core assumption: Contextual understanding of language improves detection of bullying behavior more than surface-level keyword matching.
- Evidence anchors:
  - [abstract] states RoBERTa outperformed other models with an F1-score of 0.87 on the balanced dataset.
  - [section 3.1] explains BERT's masked language model objective and bidirectional context usage.
  - [corpus] shows related work on AI-enabled severity detection with explainability, suggesting contextual models are in use.
- Break condition: If the dataset contains mostly non-contextual cues (e.g., only explicit profanity), traditional models may perform comparably.

### Mechanism 2
- Claim: The use of a balanced dataset (D2) significantly improves detection performance compared to highly imbalanced datasets.
- Mechanism: Balanced class distribution prevents model bias toward the majority class, allowing the classifier to learn representative patterns for both bullying and non-bullying posts.
- Core assumption: Class imbalance is a primary cause of poor generalization in cyberbullying detection.
- Evidence anchors:
  - [section 3.8] describes how D2 was created by balancing Formspring and Twitter data.
  - [section 4] compares results on imbalanced (D1) vs. balanced (D2) datasets, showing improved performance.
  - [corpus] includes a paper on session-based datasets for Chinese cyberbullying, implying balanced data is valued.
- Break condition: If the real-world data remains highly imbalanced, model performance may degrade despite balanced training.

### Mechanism 3
- Claim: Fine-tuning pre-trained LLMs on domain-specific cyberbullying data yields better performance than training from scratch.
- Mechanism: Pre-trained models already encode rich linguistic knowledge; fine-tuning adapts this knowledge to the specific task with less data.
- Core assumption: Transfer learning from large corpora reduces the need for extensive labeled data in niche domains.
- Evidence anchors:
  - [section 3.5] describes fine-tuning BERT by adding a classification layer on top of pre-trained weights.
  - [section 4] shows that LLMs (BERT, RoBERTa, XLNet, XLM-RoBERTa) consistently outperform traditional models.
  - [corpus] mentions a paper on deep embedding and transformer approaches for Arabic cyberbullying, supporting fine-tuning practice.
- Break condition: If the domain is too different from pre-training data, fine-tuning may not provide advantages.

## Foundational Learning

- Concept: Bidirectional self-attention in transformers
  - Why needed here: Understanding how LLMs capture context is key to interpreting their superiority in cyberbullying detection.
  - Quick check question: How does bidirectional attention differ from unidirectional attention in language modeling?

- Concept: Class imbalance and its impact on ML performance
  - Why needed here: Recognizing why balancing the dataset improved results helps in dataset preparation for future work.
  - Quick check question: What happens to model predictions when one class vastly outnumbers the other in training data?

- Concept: Transfer learning with pre-trained language models
  - Why needed here: Knowing how fine-tuning works explains why LLMs can be adapted quickly to new tasks.
  - Quick check question: What is the difference between pre-training and fine-tuning in the context of LLMs?

## Architecture Onboarding

- Component map:
  - Data ingestion → Text preprocessing → Tokenization → Embedding layer (SBERT or TF-IDF) → Model (RoBERTa/BERT/SVM/RF) → Classification layer → Evaluation metrics
- Critical path:
  - Data preparation (balancing classes) → Model selection (RoBERTa) → Hyperparameter tuning → Evaluation on held-out test set
- Design tradeoffs:
  - Using LLMs increases computational cost but improves accuracy; simpler models are faster but less effective on imbalanced data.
  - Balanced datasets improve fairness but may not reflect real-world distributions.
- Failure signatures:
  - Poor performance on minority class → likely class imbalance or insufficient fine-tuning.
  - Overfitting to training data → check for data leakage or insufficient regularization.
- First 3 experiments:
  1. Train SVM and RF on TF-IDF features with the imbalanced dataset (D1) to establish baseline.
  2. Fine-tune RoBERTa on the balanced dataset (D2) and evaluate F1-score on test set.
  3. Compare LLMs on the imbalanced dataset to assess robustness to class imbalance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RoBERTa compare to other large language models in detecting cyberbullying across different languages and cultural contexts?
- Basis in paper: [explicit] The paper mentions that XLM-RoBERTa was trained on 100 languages, but the study only tested English text.
- Why unresolved: The study focused solely on English text, leaving the performance of these models in multilingual contexts unexplored.
- What evidence would resolve it: Conducting experiments with datasets in multiple languages and cultural contexts to compare the performance of RoBERTa and other models.

### Open Question 2
- Question: What are the potential limitations of using traditional machine learning models like SVM and Random Forest for cyberbullying detection in highly imbalanced datasets?
- Basis in paper: [explicit] The paper notes that traditional models struggled with class imbalance, performing poorly compared to large language models.
- Why unresolved: The paper does not explore why traditional models are less effective in imbalanced datasets or how to mitigate these limitations.
- What evidence would resolve it: Analyzing the performance of traditional models with various imbalanced datasets and testing methods to improve their accuracy.

### Open Question 3
- Question: How can multimodal approaches enhance the detection of cyberbullying beyond text-based analysis?
- Basis in paper: [explicit] The paper suggests future work could include implementing multimodal approaches for cyberbullying detection.
- Why unresolved: The paper does not provide details on how multimodal data (e.g., images, videos) could be integrated with text analysis to improve detection.
- What evidence would resolve it: Developing and testing models that incorporate multimodal data and comparing their performance to text-only models.

## Limitations

- Results are based on a balanced dataset preparation that may not reflect real-world imbalanced distributions where cyberbullying is relatively rare
- Evaluation uses a simple 90/10 split without cross-validation or external dataset validation, raising concerns about potential overfitting
- Comparison between traditional ML models and LLMs is confounded by different feature representations, making it difficult to isolate the effect of model architecture alone

## Confidence

- **High Confidence**: The superiority of RoBERTa over other tested models on the balanced dataset, supported by direct comparison and consistent F1-score improvements.
- **Medium Confidence**: The generalizability of findings to real-world imbalanced datasets, as results on the imbalanced dataset (D1) show significant performance degradation.
- **Low Confidence**: Claims about the specific mechanisms by which LLMs capture contextual cyberbullying cues, as the study does not include ablation studies or interpretability analysis to validate these assumptions.

## Next Checks

1. **Cross-dataset validation**: Evaluate the trained models on an independent cyberbullying dataset (e.g., Twitter, Wikipedia, or Formspring from different sources) to assess real-world generalization beyond the balanced dataset preparation.

2. **Severity detection extension**: Modify the classification task to include cyberbullying severity levels (mild, moderate, severe) and evaluate whether LLMs maintain their performance advantage on multi-class or regression tasks.

3. **Class imbalance robustness test**: Systematically vary the class imbalance ratio in the training data and measure model performance to identify the threshold at which RoBERTa's advantage diminishes compared to traditional models.