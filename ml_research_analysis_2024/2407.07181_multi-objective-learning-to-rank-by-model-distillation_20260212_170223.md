---
ver: rpa2
title: Multi-objective Learning to Rank by Model Distillation
arxiv_id: '2407.07181'
source_url: https://arxiv.org/abs/2407.07181
tags:
- objective
- could
- also
- training
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses challenges in multi-objective learning to
  rank in online marketplaces by proposing a model distillation approach that combines
  multiple pre-trained models into a single ranking model. The key innovation is using
  soft labels derived from pre-trained models as a regularizer during training, which
  helps mitigate data imbalance issues and enables efficient integration of non-differentiable
  business objectives.
---

# Multi-objective Learning to Rank by Model Distillation

## Quick Facts
- arXiv ID: 2407.07181
- Source URL: https://arxiv.org/abs/2407.07181
- Reference count: 35
- One-line primary result: 0.37% booking rate improvement in online A/B tests

## Executive Summary
This paper addresses challenges in multi-objective learning to rank for online marketplaces by proposing a model distillation approach that combines multiple pre-trained models into a single ranking model. The key innovation is using soft labels derived from pre-trained models as a regularizer during training, which helps mitigate data imbalance issues and enables efficient integration of non-differentiable business objectives. Experiments on Airbnb's search ranking system show significant improvements in the primary objective (booking rate) while maintaining secondary objectives, with the self-distillation approach reducing model irreproducibility by 53% in terms of search result changes.

## Method Summary
The proposed method uses model distillation to combine multiple pre-trained models into a single ranking model for multi-objective learning to rank. The approach trains separate pre-trained models for each objective, then generates soft labels by aggregating their outputs with learned weights. The student model is trained using both hard labels (for the primary objective) and soft labels as a regularizer. For model updates, self-distillation is applied where the previous model's outputs serve as soft labels for training the new version, enabling stable knowledge transfer across updates and reducing model irreproducibility.

## Key Results
- 0.37% booking rate improvement in online A/B tests for the primary objective
- Significant improvement in offline NDCG with binary relevance
- 53% reduction in model irreproducibility in terms of search result changes across model updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft labels act as a dense regularization signal that mitigates data imbalance across objectives.
- Mechanism: In multi-objective ranking, some objectives have far fewer positive examples than primary objectives. The soft-label aggregation from pre-trained models provides a dense supervision signal for all items, preventing the model from overfitting to sparse hard labels of minority objectives.
- Core assumption: Pre-trained models encode useful ranking knowledge for each objective independently, and their aggregation approximates a multi-objective ranking solution.
- Evidence anchors:
  - [abstract]: "using soft labels derived from pre-trained models as a regularizer during training, which helps mitigate data imbalance issues"
  - [section]: "Soft-label acts as good regularizer... Soft-Label gives multi-objective LTR model feasible prior knowledge since it encodes knowledge from each objective's pre-trained model."
- Break condition: If pre-trained models are poorly trained or highly correlated with the primary objective, soft labels may not provide meaningful regularization and could reinforce bias.

### Mechanism 2
- Claim: Self-distillation enables stable knowledge transfer across model updates, reducing model irreproducibility.
- Mechanism: By training each new model version using soft labels from the previous version rather than from scratch, the model retains learned ranking preferences across updates. This reduces variance from random initialization and dataset sampling.
- Core assumption: The soft-label distribution from the previous model captures stable ranking knowledge that should persist across updates.
- Evidence anchors:
  - [abstract]: "the self-distillation approach enables stable knowledge transfer across model updates, reducing model irreproducibility by 53% in terms of search result changes"
  - [section]: "Soft-label could carry ranking knowledge efficiently and pass to new version of ranking models... by applying soft-label, what model learned before could be passed to new version of model efficiently"
- Break condition: If the model undergoes significant architectural changes or the underlying data distribution shifts dramatically, self-distillation may propagate outdated knowledge.

### Mechanism 3
- Claim: Soft labels enable efficient integration of non-differentiable business objectives.
- Mechanism: Ad-hoc business rules cannot be directly optimized through gradient descent. By boosting soft labels for items meeting these criteria, the model learns to satisfy these objectives during training rather than through post-hoc score adjustments.
- Core assumption: Soft labels have sufficient influence in the loss function to drive meaningful changes in model behavior when augmented with business rules.
- Evidence anchors:
  - [abstract]: "enables efficient integration of non-differentiable business objectives"
  - [section]: "Soft-label could work well with non-differentiable objective... Such objectives themselves are vague and also can't be optimized by learning from past data... One solution is to directly revise training data labels... But such boost could work perfectly for soft-labels"
- Break condition: If the business rule boost is too aggressive, it may dominate the primary objective and degrade overall performance.

## Foundational Learning

- Concept: Multi-objective optimization via scalarization
  - Why needed here: The paper converts multiple ranking objectives into a single optimization problem by weighting and summing individual objective losses.
  - Quick check question: What happens to model performance when objectives are highly conflicting (e.g., short-term conversions vs. long-term growth)?

- Concept: Model distillation (teacher-student framework)
  - Why needed here: The approach uses pre-trained models as teachers to provide soft labels that guide the student model in balancing multiple objectives.
  - Quick check question: How does temperature scaling in softmax affect the information content of soft labels?

- Concept: Listwise learning-to-rank loss functions
  - Why needed here: The model uses listwise losses (e.g., softmax cross-entropy) that consider the entire ranked list rather than individual items or pairs.
  - Quick check question: Why might listwise losses be more appropriate than pairwise losses for multi-objective ranking?

## Architecture Onboarding

- Component map:
  - Pre-trained models -> Soft label generator -> Student model -> Loss function
  - Pre-trained models: One per objective, frozen during training
  - Student model: The actual ranking model being trained
  - Soft label generator: Computes weighted aggregation of pre-trained model outputs
  - Loss function: Combines hard label loss (primary objective) with soft label distillation loss
  - Self-distillation pipeline: For model updates, uses previous model's outputs as soft labels

- Critical path:
  1. Train pre-trained models on each objective separately
  2. Generate soft labels by aggregating pre-trained model scores
  3. Train student model using both hard labels (primary objective) and soft labels
  4. For updates, use self-distillation instead of retraining from scratch

- Design tradeoffs:
  - Pre-trained models add training complexity but enable soft label generation
  - Self-distillation reduces operational overhead but may propagate errors
  - Soft label weights require tuning but can be learned via MoE layers if redesigning

- Failure signatures:
  - Poor primary objective performance: Soft label weights may be misbalanced
  - High variance across model updates: Self-distillation may not be working correctly
  - Secondary objectives degrading: Pre-trained models may not be adequately trained

- First 3 experiments:
  1. Compare NDCG with and without soft label regularization on imbalanced datasets
  2. Measure Kendall's tau stability across model retraining with and without self-distillation
  3. Test ad-hoc objective injection by boosting soft labels for specific item features and measuring impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed model distillation approach compare to other multi-objective learning methods when the correlation among objectives is high?
- Basis in paper: [inferred] The paper mentions that multi-task learning performs better than model fusion when objectives are highly correlated, but does not compare the proposed method to other multi-objective learning approaches in this scenario.
- Why unresolved: The paper focuses on comparing the proposed method to the baseline multi-task learning system and does not explore its performance relative to other multi-objective learning methods in different correlation scenarios.
- What evidence would resolve it: Conducting experiments comparing the proposed method to other multi-objective learning methods (e.g., scalarization, MOEAs) under various objective correlation conditions would provide evidence to answer this question.

### Open Question 2
- Question: What is the impact of using different temperature values in the softmax function on the performance of the proposed model distillation approach?
- Basis in paper: [inferred] The paper mentions that temperature is applied to softmax inside f(Î¸, Xi) but does not discuss the impact of different temperature values on the model's performance.
- Why unresolved: The paper does not explore the effect of temperature on the distillation process or the final model performance.
- What evidence would resolve it: Conducting experiments with different temperature values and analyzing their impact on the model's performance metrics (e.g., NDCG, CVR) would provide evidence to answer this question.

### Open Question 3
- Question: How does the proposed model distillation approach handle new objectives that are added after the initial model training?
- Basis in paper: [inferred] The paper discusses the ability to inject ad-hoc non-differentiable business objectives but does not address how the model handles new objectives that are added after the initial training.
- Why unresolved: The paper focuses on the initial training process and does not discuss the model's adaptability to new objectives.
- What evidence would resolve it: Conducting experiments where new objectives are added to the model after initial training and analyzing the model's performance on these new objectives would provide evidence to answer this question.

## Limitations
- The exact model architectures remain underspecified, described only as "MLP models"
- The magnitude of improvement (0.37% booking rate) may be modest relative to the operational complexity added
- The paper lacks quantitative analysis of data imbalance severity and how much improvement specifically came from the regularization effect

## Confidence
- **High confidence**: The core mechanism of using soft labels as regularization is technically sound and well-supported by the theoretical framework of model distillation
- **Medium confidence**: The claimed benefits of self-distillation for model stability are supported by the 53% reduction in irreproducibility metric, though the exact measurement methodology isn't fully detailed
- **Medium confidence**: The integration of non-differentiable business objectives through soft label boosting is plausible but lacks quantitative validation of its effectiveness compared to alternative approaches

## Next Checks
1. **Quantitative imbalance analysis**: Measure the actual class imbalance ratios across different objectives in the training data and conduct ablation studies to isolate the regularization benefit of soft labels from other effects

2. **Cross-dataset generalization**: Test the self-distillation approach on datasets with different characteristics (e.g., different marketplaces or different levels of data drift) to validate the claimed stability benefits hold beyond Airbnb's specific context

3. **Alternative regularization comparison**: Compare soft label regularization against other imbalance mitigation techniques (class weighting, oversampling, focal loss) to quantify the specific contribution of the distillation approach to the reported improvements