---
ver: rpa2
title: Multimodal Sentiment Analysis Based on BERT and ResNet
arxiv_id: '2412.03625'
source_url: https://arxiv.org/abs/2412.03625
tags:
- sentiment
- bert
- text
- image
- resnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal sentiment analysis framework combining
  BERT and ResNet to improve sentiment analysis accuracy by fusing text and image
  features. The method uses BERT to extract text features and ResNet50 to extract
  image features, then applies various fusion strategies including attention mechanisms
  to combine the multimodal information.
---

# Multimodal Sentiment Analysis Based on BERT and ResNet

## Quick Facts
- arXiv ID: 2412.03625
- Source URL: https://arxiv.org/abs/2412.03625
- Authors: JiaLe Ren
- Reference count: 31
- Key outcome: OTEModel achieves 74.5% accuracy and 73% F1 score on MAVA-single dataset

## Executive Summary
This paper proposes a multimodal sentiment analysis framework that combines BERT for text feature extraction and ResNet50 for image feature extraction. The model explores various fusion strategies including cross-modal attention mechanisms to combine the complementary information from both modalities. Experiments demonstrate that the proposed OTEModel outperforms single-modal baselines, achieving 74.5% accuracy and 73% F1 score on the MAVA-single dataset, showing the effectiveness of multimodal fusion for sentiment analysis.

## Method Summary
The framework uses BERT to extract text features and ResNet50 to extract image features from paired data. Five different multimodal models are implemented with various fusion strategies: cross-modal attention, hierarchical self-attention, and concatenation-based approaches. The models fuse the extracted features through attention mechanisms or simple concatenation before classification. The best-performing OTEModel uses a combination of cross-modal attention and feature fusion to achieve superior performance compared to unimodal baselines.

## Key Results
- OTEModel achieves 74.5% accuracy and 73% F1 score on MAVA-single dataset
- Outperforms single-modal BERT (70.5% accuracy) and ResNet (65.75% accuracy) models
- Demonstrates that multimodal fusion effectively leverages complementary information between text and images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal attention improves classification by enabling text and image features to mutually inform each other.
- Mechanism: The model uses bidirectional attention between text and image feature spaces, allowing each modality to focus on relevant aspects of the other before classification.
- Core assumption: Text and image features contain complementary information that can be selectively combined through attention weights.
- Evidence anchors: [abstract] "BERT is used to extract the text feature vector, and ResNet is used to extract the image feature representation. Then, a variety of feature fusion strategies are explored, and finally the fusion model based on attention mechanism is selected..."; [section] "Cross Modality Attention: AI−>T = AttentionI−>T (HI , HT , HT ) AT −>I = AttentionT −>I (HT , HI , HI )"
- Break condition: If attention weights become uniform or noisy, the cross-modal benefits vanish and the model degenerates toward simple concatenation.

### Mechanism 2
- Claim: Early fusion of extracted features into a joint representation captures richer multimodal semantics.
- Mechanism: The model concatenates or projects BERT and ResNet features into a shared feature space before classification, allowing the classifier to learn cross-modal interactions.
- Core assumption: The joint feature space can represent interactions between text and image features better than separate processing.
- Evidence anchors: [abstract] "BERT has shown strong text representation ability in natural language processing, and ResNet has excellent image feature extraction performance in the field of computer vision. Firstly, BERT is used to extract the text feature vector, and ResNet is used to extract the image feature representation."; [section] "Fconcat = concat([FI , FT ], dim = 2) Fattn = Attention(Fconcat)"
- Break condition: If the feature dimensions or distributions are incompatible, the concatenated representation may become uninformative.

### Mechanism 3
- Claim: Separate unimodal classifiers followed by late fusion combine strengths of both modalities.
- Mechanism: The model trains independent classifiers on text and image features, then fuses their probability outputs via softmax addition.
- Core assumption: Each modality has predictive power on its own, and simple fusion preserves that power while adding robustness.
- Evidence anchors: [abstract] "The proposed OTEModel achieves the best performance with 74.5% accuracy and 73% F1 score, outperforming single-modal BERT (70.5% accuracy) and ResNet (65.75% accuracy) models."; [section] "pT = T extClassif er(FT ) pI = ImageClassif er (FI ) p = sof tmax((pT + pI ), dim = 1)"
- Break condition: If one modality is consistently noisy or irrelevant, late fusion cannot filter it out and performance degrades.

## Foundational Learning

- Concept: Attention mechanisms in neural networks
  - Why needed here: The paper relies on cross-modal and self-attention to fuse text and image features effectively.
  - Quick check question: What is the role of the query/key/value tensors in multi-head attention?

- Concept: Feature extraction with pre-trained models
  - Why needed here: BERT and ResNet are used as fixed or fine-tuned feature extractors before fusion.
  - Quick check question: Why is freezing the pre-trained layers a common choice in multimodal experiments?

- Concept: Evaluation metrics for classification
  - Why needed here: Accuracy, precision, recall, and F1 score are used to compare multimodal and unimodal baselines.
  - Quick check question: How does F1 score penalize extreme imbalances between precision and recall?

## Architecture Onboarding

- Component map: Text pipeline (BERT → embeddings → attention), Image pipeline (ResNet50 → features → attention), Fusion module (attention-based or concatenation), Classifier (final prediction)
- Critical path: Input → BERT/ResNet feature extraction → cross-modal attention → joint representation → classifier → output
- Design tradeoffs: Early fusion captures interactions but may mix incompatible scales; late fusion preserves modality independence but loses fine-grained cross-modal interactions
- Failure signatures: (1) Accuracy stalls at single-modal baseline; (2) Attention weights become flat; (3) Overfitting to dataset-specific patterns
- First 3 experiments:
  1. Train BERT and ResNet separately on MAVA-single, record unimodal baselines
  2. Implement early fusion with concatenation + attention, compare to baselines
  3. Add cross-modal attention (CMACModel style) and evaluate improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different attention mechanisms (cross-modal vs self-attention) compare in effectiveness for multimodal sentiment analysis?
- Basis in paper: [explicit] The paper proposes CMACModel using cross-modal attention and compares it with HSTECModel and OTEModel, but doesn't systematically compare different attention mechanisms
- Why unresolved: The paper only explores three specific attention-based fusion strategies without comparing against other attention variants or conducting ablation studies on attention mechanisms
- What evidence would resolve it: Systematic experiments comparing cross-modal attention, self-attention, co-attention, and transformer-based attention mechanisms for multimodal sentiment fusion

### Open Question 2
- Question: What is the impact of image quality and resolution on multimodal sentiment analysis performance?
- Basis in paper: [inferred] The paper uses ResNet50 for image feature extraction but doesn't analyze how different image qualities or resolutions affect the model's performance
- Why unresolved: The experiments don't include analysis of how preprocessing steps like image resizing, normalization, or quality filtering affect sentiment analysis accuracy
- What evidence would resolve it: Experiments varying image preprocessing parameters and quality levels to measure their impact on sentiment classification performance

### Open Question 3
- Question: How well does the proposed multimodal framework generalize to other sentiment analysis datasets and domains?
- Basis in paper: [explicit] The authors state "In the future, more advanced feature fusion techniques and optimization strategies will be explored to further improve the accuracy and generalization ability of multimodal sentiment analysis"
- Why unresolved: The paper only evaluates on a single dataset (MAVA-single) without testing cross-dataset generalization or performance on different domains
- What evidence would resolve it: Experiments testing the trained models on multiple datasets from different domains (e.g., product reviews, social media, news comments) and measuring domain transfer performance

### Open Question 4
- Question: What is the relative contribution of text vs image features in the multimodal sentiment analysis performance?
- Basis in paper: [explicit] The paper shows that multimodal models outperform single-modal BERT and ResNet models, but doesn't analyze the individual contributions of each modality
- Why unresolved: No ablation studies or contribution analysis to determine how much each modality contributes to the final performance
- What evidence would resolve it: Controlled experiments where one modality is systematically removed or degraded to measure the impact on overall performance, along with feature importance analysis

## Limitations

- Limited to a single dataset (MAVA-single) with only 4,512 samples, raising generalization concerns
- Exact architectural details of attention mechanisms are not fully specified, making exact reproduction challenging
- No ablation studies to quantify the specific contribution of each fusion strategy or attention mechanism

## Confidence

- **High confidence**: The general multimodal fusion framework combining BERT and ResNet is sound and aligns with established NLP/computer vision practices
- **Medium confidence**: The specific attention mechanism implementations and their comparative advantages are described but not fully detailed
- **Low confidence**: Claims about robustness to noisy social media data lack supporting evidence from diverse datasets

## Next Checks

1. Test OTEModel on additional multimodal sentiment datasets (e.g., CMU-MOSEI, MELD) to verify cross-dataset generalization
2. Conduct ablation studies removing attention mechanisms to quantify their specific contribution to performance gains
3. Evaluate model performance across different sentiment class distributions to assess robustness to label imbalance