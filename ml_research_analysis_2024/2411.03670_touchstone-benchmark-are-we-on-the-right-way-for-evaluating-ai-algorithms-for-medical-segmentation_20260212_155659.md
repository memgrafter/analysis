---
ver: rpa2
title: 'Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms
  for Medical Segmentation?'
arxiv_id: '2411.03670'
source_url: https://arxiv.org/abs/2411.03670
tags:
- stu-net
- nnu-net
- u-net
- swinunetr
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Touchstone is a large-scale benchmark for medical image segmentation
  that addresses key limitations in existing evaluations, including in-distribution
  test sets, small sample sizes, oversimplified metrics, and unfair comparisons. It
  features 5,195 training CT scans from 76 hospitals and 5,903 testing CT scans from
  11 additional hospitals, providing diverse out-of-distribution scenarios.
---

# Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?

## Quick Facts
- arXiv ID: 2411.03670
- Source URL: https://arxiv.org/abs/2411.03670
- Authors: Pedro R. A. S. Bassi; Wenxuan Li; Yucheng Tang; Fabian Isensee; Zifu Wang; Jieneng Chen; Yu-Cheng Chou; Yannick Kirchhoff; Maximilian Rokuss; Ziyan Huang; Jin Ye; Junjun He; Tassilo Wald; Constantin Ulrich; Michael Baumgartner; Saikat Roy; Klaus H. Maier-Hein; Paul Jaeger; Yiwen Ye; Yutong Xie; Jianpeng Zhang; Ziyang Chen; Yong Xia; Zhaohu Xing; Lei Zhu; Yousef Sadegheih; Afshin Bozorgpour; Pratibha Kumari; Reza Azad; Dorit Merhof; Pengcheng Shi; Ting Ma; Yuxin Du; Fan Bai; Tiejun Huang; Bo Zhao; Haonan Wang; Xiaomeng Li; Hanxue Gu; Haoyu Dong; Jichen Yang; Maciej A. Mazurowski; Saumya Gupta; Linshan Wu; Jiaxin Zhuang; Hao Chen; Holger Roth; Daguang Xu; Matthew B. Blaschko; Sergio Decherchi; Andrea Cavalli; Alan L. Yuille; Zongwei Zhou
- Reference count: 40
- Key outcome: Touchstone is a large-scale benchmark for medical image segmentation that addresses key limitations in existing evaluations, including in-distribution test sets, small sample sizes, oversimplified metrics, and unfair comparisons. It features 5,195 training CT scans from 76 hospitals and 5,903 testing CT scans from 11 additional hospitals, providing diverse out-of-distribution scenarios. The benchmark evaluates 19 algorithms and 3 frameworks across 9 anatomical structures, with algorithms trained by their inventors and independently assessed by a third party. Results show significant performance variations across datasets and demographic groups, highlighting the need for diverse evaluation. The benchmark reveals that top-performing models are typically CNNs within the nnU-Net framework, but also uncovers strengths in innovative vision-language and diffusion-based models.

## Executive Summary
Touchstone addresses critical limitations in medical image segmentation evaluation by introducing a large-scale benchmark with 11,098 CT scans from 87 hospitals, featuring diverse out-of-distribution test scenarios and independent third-party evaluation. The benchmark evaluates 19 algorithms across 9 anatomical structures, with each algorithm trained by its inventor and assessed on test data never seen during training. Results reveal significant performance variations across datasets and demographic groups, challenging the reliability of standard evaluation practices that rely on small, in-distribution test sets.

## Method Summary
The Touchstone benchmark employs a comprehensive evaluation framework featuring 5,195 training CT scans from 76 hospitals and 5,903 testing CT scans from 11 additional hospitals, including a proprietary JHH dataset of 5,160 scans. Nineteen algorithms developed by 14 inventors are trained on the AbdomenAtlas 1.0 training set and independently evaluated by a third party on multiple test sets. The benchmark uses Dice Similarity Coefficient (DSC) and Normalized Surface Distance (NSD) metrics, with statistical significance testing via Wilcoxon signed rank tests and Holm's adjustment. Metadata analysis examines performance across age, sex, race, diagnosis, and manufacturer subgroups, while inference speed requirements ensure practical applicability.

## Key Results
- Large-scale out-of-distribution testing reveals significant performance variations across datasets that small, in-distribution test sets miss
- Top-performing models are typically CNNs within the nnU-Net framework, though innovative vision-language and diffusion-based models show competitive performance
- Per-group metadata analysis uncovers demographic biases, with statistically significant performance differences across age, sex, and race subgroups
- Statistical analysis demonstrates that large test datasets (5,160 samples) enable detection of performance differences that smaller datasets cannot reliably identify

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Touchstone benchmark mitigates evaluation biases by ensuring all algorithms are trained by their inventors and evaluated by a third party.
- Mechanism: By having each algorithm's inventor train their own model, the benchmark avoids the asymmetric optimization efforts seen in standard comparisons where authors may over-tune their proposed algorithms while baselines are under-optimized. Independent third-party evaluation ensures fairness and integrity of results.
- Core assumption: Inventors have the deepest understanding of their algorithms and are most motivated to optimize them, while third-party evaluators prevent data leakage and maintain objectivity.
- Evidence anchors:
  - [abstract]: "We invited 14 inventors of 19 AI algorithms to train their algorithms, while our team, as a third party, independently evaluated these algorithms on three test sets."
  - [section]: "Each AI algorithm is configured by its own inventors, who know it best and have the most interest in its success. In our benchmark, each inventor trained their AI algorithm on 5,195 annotated CT scans in AbdomenAtlas 1.0, and we, as a third party, independently evaluated these algorithms on 5,903 CT scans that are unknown and inaccessible to the AI inventors."
- Break condition: If inventors were allowed to access test data or if evaluation was not truly independent, the fairness guarantee would be compromised.

### Mechanism 2
- Claim: Large-scale out-of-distribution testing reveals performance variations that small, in-distribution test sets miss.
- Mechanism: By using 5,903 test scans from hospitals never seen during training, including 5,160 from JHH, the benchmark captures diverse scenarios in contrast enhancement, disease conditions, demographics, image quality, and scanner types. This exposes algorithms to real-world variability that in-distribution testing cannot reveal.
- Core assumption: Performance differences across diverse test sets are indicative of real-world generalization ability, and larger test sets provide more reliable statistical significance.
- Evidence anchors:
  - [abstract]: "This diverse test set enhances the statistical significance of benchmark results and rigorously evaluates AI algorithms across various out-of-distribution scenarios."
  - [section]: "The JHH dataset (N=5,160) presents 5,160 CT scans from a hospital never seen during training, introducing a new scale of external validation for abdominal CT benchmarks. The test data distribution varies in contrast enhancement, disease condition, demographics, image quality, and scanner types."
- Break condition: If the test data were not truly out-of-distribution or if sample sizes were too small to detect meaningful differences, the mechanism would fail to reveal true performance variations.

### Mechanism 3
- Claim: Per-group metadata analysis reveals demographic biases that average performance metrics obscure.
- Mechanism: By analyzing performance across age, sex, race, diagnosis, and manufacturer subgroups, the benchmark identifies specific scenarios where algorithms underperform. This granular analysis highlights hidden strengths and weaknesses that would be missed by looking only at average scores.
- Core assumption: Performance differences across demographic groups are meaningful and indicate potential biases or limitations in the algorithms, and these differences can be statistically detected with sufficient sample sizes.
- Evidence anchors:
  - [section]: "We leveraged the metadata available in test datasets to assess AI performance consistency across diverse demographic groups. We studied correlation between AI performance and the five types of metadata: age, sex, and diagnosis are analyzed on all two datasets, race is only analyzed on one dataset, JHH, since most public test sets lack this information, and manufacturer is only analyzed in one dataset."
  - [section]: "Figure 2 displays per-group DSC for an average AI model... The statistical analysis further highlights the need for large test datasets: JHH's large sample size (N=5,160) allows detection of statistically significant DSC differences across all metadata, but some of these differences (for age and sex) are noticeable but not significant in the smaller TotalSegmentator dataset."
- Break condition: If metadata were not available or if sample sizes within groups were too small to detect statistically significant differences, the mechanism would fail to reveal meaningful biases.

## Foundational Learning

- Concept: Out-of-distribution generalization
  - Why needed here: The benchmark's core innovation is evaluating algorithms on data from hospitals never seen during training, which tests whether models can generalize beyond their training distribution.
  - Quick check question: Why is performance on data from a completely new hospital more informative than performance on data from the same hospital as training?

- Concept: Statistical power and significance
  - Why needed here: With thousands of test samples, the benchmark can detect small but meaningful performance differences between algorithms that would be indistinguishable with smaller test sets.
  - Quick check question: How does increasing test set size from 50 to 5,000 samples change the ability to distinguish between algorithms with similar performance?

- Concept: Demographic bias analysis
  - Why needed here: By breaking down performance by age, sex, race, and other metadata, the benchmark reveals whether algorithms perform equally well across different patient populations.
  - Quick check question: Why might an algorithm perform significantly better on one demographic group than another, and what does this indicate about the training data?

## Architecture Onboarding

- Component map: AbdomenAtlas 1.0 (5,195 training CT scans from 76 hospitals) -> Multiple test sets (5,903 total scans from 11 additional hospitals) -> 19 algorithms trained by 14 inventors -> Third-party evaluation system with DSC and NSD metrics -> Metadata analysis (age, sex, race, diagnosis, manufacturer subgroup analysis) -> Statistical validation (Wilcoxon signed rank tests with Holm's adjustment, Kruskal-Wallis tests with Bonferroni correction)

- Critical path:
  1. Data ingestion and preprocessing of training and test CT scans
  2. Algorithm training by inventors with provided infrastructure
  3. Secure transfer of trained models to evaluation platform
  4. Batch inference on all test cases
  5. Metric calculation (DSC, NSD) per case and per class
  6. Statistical analysis and significance testing
  7. Metadata-based subgroup analysis
  8. Result visualization and reporting

- Design tradeoffs:
  - Proprietary vs public test data: Proprietary data ensures true OOD evaluation but limits accessibility and increases feedback time
  - Algorithm diversity vs standardization: Including many different architectures provides comprehensive comparison but makes fair evaluation more complex
  - Granularity vs simplicity: Detailed metadata analysis reveals important insights but increases computational and analytical complexity

- Failure signatures:
  - No significant performance differences between algorithms: May indicate insufficient OOD challenge or inadequate test set size
  - Inconsistent rankings across test sets: Suggests algorithms are overfitting to specific data distributions
  - Missing metadata for key subgroups: Limits ability to perform demographic bias analysis

- First 3 experiments:
  1. Train a baseline nnU-Net on AbdomenAtlas 1.0 and evaluate on JHH test set to establish performance floor
  2. Evaluate same nnU-Net on TotalSegmentator to assess cross-dataset generalization
  3. Train and evaluate a transformer-based model (e.g., UNETR) to compare architectural approaches under identical conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary test data (JHH dataset) restricts full reproducibility and independent verification by the broader research community
- Reliance on inventor-trained models introduces potential optimization biases that cannot be fully eliminated despite third-party evaluation
- Generalizability of findings to other medical imaging domains beyond abdominal CT segmentation remains uncertain

## Confidence

**High Confidence**: The core claims about dataset scale (5,195 training + 5,903 testing scans), the existence of 19 algorithms evaluated across 9 anatomical structures, and the use of DSC/NSD metrics are well-supported by the data.

**Medium Confidence**: Claims about the superiority of specific architectural approaches (CNNs within nnU-Net framework) and the statistical significance of performance differences across demographic groups are supported but require independent verification.

**Low Confidence**: The generalizability of findings to other medical imaging domains beyond abdominal CT segmentation remains uncertain.

## Next Checks

1. **Independent Replication**: Attempt to reproduce the benchmark results using only the publicly available TotalSegmentator dataset and compare performance rankings with those reported in the study.

2. **Cross-Domain Testing**: Evaluate the same algorithms on a different medical imaging modality (e.g., brain MRI segmentation) to assess whether the observed architectural advantages transfer beyond abdominal CT.

3. **Metadata Completeness Analysis**: Systematically investigate the impact of missing demographic metadata in test sets on the ability to detect algorithmic biases, particularly focusing on race and diagnosis information.