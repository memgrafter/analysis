---
ver: rpa2
title: Label-Augmented Dataset Distillation
arxiv_id: '2409.16239'
source_url: https://arxiv.org/abs/2409.16239
tags:
- dataset
- ladd
- distillation
- labels
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Label-Augmented Dataset Distillation (LADD),
  a method that enhances dataset distillation by leveraging label augmentation. LADD
  generates dense labels for sub-sampled images, capturing rich semantic information
  with only a 2.5% increase in storage.
---

# Label-Augmented Dataset Distillation

## Quick Facts
- arXiv ID: 2409.16239
- Source URL: https://arxiv.org/abs/2409.16239
- Reference count: 40
- Primary result: Achieves 14.9% average accuracy gain across three dataset distillation algorithms with only 2.5% storage overhead

## Executive Summary
Label-Augmented Dataset Distillation (LADD) enhances existing dataset distillation methods by generating dense labels for sub-sampled images. The approach divides synthetic images into a 5x5 grid of sub-images, each covering 62.5% of the original axis, and uses a small labeler network trained for only 10 epochs to produce soft labels for each sub-image. These dense labels capture local semantic information that complements the global hard labels, providing richer training signals. LADD requires minimal storage overhead (2.5%) while significantly improving both accuracy and cross-architecture robustness compared to baseline distillation methods.

## Method Summary
LADD integrates with existing dataset distillation algorithms by adding a label augmentation stage. After synthetic images are generated through bi-loop optimization, each image is divided into N² sub-images using fixed sub-sampling parameters (N=5, R=62.5%). A small ConvNetD5 labeler, trained for 10 epochs on the source dataset, generates dense soft labels for each sub-image. During deployment, the evaluation model is trained using a composite loss that combines global image training with hard labels and local sub-image training with dense soft labels. This dual-view approach provides both coarse global context and fine-grained local detail, improving generalization across different architectures.

## Key Results
- Achieves 14.9% average accuracy improvement across three high-performance dataset distillation algorithms
- Requires only 2.5% additional storage overhead compared to baseline distilled datasets
- Reduces computational cost by approximately 50% in terms of FLOPs
- Demonstrates enhanced cross-architecture robustness across ConvNetD5, AlexNet, VGG11, ResNet18, and ViT

## Why This Works (Mechanism)

### Mechanism 1
Dense labels generated from sub-sampled images capture local semantic information that full-image labels miss. The labeler network, trained for only 10 epochs on the source dataset, produces soft labels for each sub-image that encode local patterns, textures, and partial object views. This early-stage labeler retains sufficient generalization because distilled images preserve local source structures.

### Mechanism 2
Combining global (full image + hard label) and local (sub-image + dense label) views during deployment creates multi-scale learning signals. The composite loss trains the model using both CE(h(xi), yi) for global context and sum over j=1 to N² of CE(h(Sj(xi)), yd_i,j) for local detail, helping the model generalize across architectures.

### Mechanism 3
Static sub-sampling with fixed N=5 and R=62.5% ensures minimal memory overhead while providing dense semantic information. Each image yields 25 sub-images of size 80x80 (from 128x128 originals), adding only ~2.5% storage overhead compared to the baseline distilled dataset.

## Foundational Learning

- **Meta-learning and bi-loop optimization in dataset distillation**: Understanding the two-loop optimization process (outer loop for synthetic images, inner loop for deployment model training) is critical since LADD builds on existing distillation algorithms as a label augmentation stage.
  - Quick check: In dataset distillation, what are the two loops called, and what does each optimize?

- **Cross-entropy loss and soft labeling**: LADD uses both hard one-hot labels and soft dense labels with cross-entropy loss. Understanding how soft labels influence gradient updates differently than hard labels is key to grasping LADD's effectiveness.
  - Quick check: How does a soft label differ from a hard label in cross-entropy, and why might soft labels improve generalization?

- **Labeler network design and early-stage training dynamics**: LADD uses a small ConvNetD5 trained for only 10 epochs to generate dense labels. Knowing why early-stage models are preferred (less overfitting, more general) is essential for reproducing results.
  - Quick check: Why might a labeler trained for 10 epochs produce more diverse soft labels than one trained for 50 epochs?

## Architecture Onboarding

- **Component map**: Source dataset → Existing distillation algorithm → Synthetic distilled dataset D → LADD label augmentation → Label-augmented dataset DLA → Deployment training (global + local views) → Final model
- **Critical path**: Distill → Sub-sample (N=5, R=62.5%) → Generate dense labels (g) → Deploy with composite loss
- **Design tradeoffs**: Static vs. dynamic sub-sampling (static reduces storage but may miss adaptive detail); labeler training epochs (10 epochs for diversity vs. more for accuracy); dense label storage (float32 per sub-image vs. hard label uint8)
- **Failure signatures**: Low accuracy despite dense labels (likely labeler overfitting or sub-sampling too coarse); high memory usage (N too large or R too small); training instability (conflicting gradients from global vs. local loss terms)
- **First 3 experiments**: 1) Run LADD with N=3, R=50% on a small distilled dataset and verify 25% overhead and accuracy gain; 2) Compare deployment with only global loss vs. only local loss vs. both to confirm synergy; 3) Swap labeler training epochs (5, 10, 20) and measure impact on dense label diversity and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LADD vary when using different static sub-sampling strategies beyond uniform sampling? The paper only evaluates one static sub-sampling strategy (uniform sampling) and does not explore alternative static approaches or compare their effectiveness.

### Open Question 2
What is the impact of varying the labeler training duration on the final performance of LADD across different dataset sizes and complexities? The paper only evaluates one specific training duration (10 epochs) for the labeler, despite mentioning that Fig. 5(a) displays the performance across various training epochs.

### Open Question 3
How does LADD's performance scale when applied to multi-modal tasks such as vision-language models? The paper only evaluates LADD on image classification tasks and does not demonstrate its effectiveness on tasks involving multiple data modalities.

## Limitations

- Static sub-sampling parameters (N=5, R=62.5%) were not rigorously justified through ablation studies and may not generalize to all datasets
- The core assumption that a 10-epoch trained labeler produces reliable dense labels for synthetic images needs more thorough validation
- Cross-architecture robustness claims are based on limited evaluation across only 5 architectures

## Confidence

- **High Confidence**: LADD improves computational efficiency (FLOPs reduced by ~50%) and provides modest storage overhead (2.5%) when using static sub-sampling
- **Medium Confidence**: LADD improves accuracy by ~14.9% average across algorithms and datasets, and enhances cross-architecture robustness
- **Low Confidence**: The specific sub-sampling parameters (N=5, R=62.5%) are optimal for all scenarios, and the labeler training strategy (10 epochs) generalizes across all dataset types

## Next Checks

1. **Cross-Architecture Robustness Test**: Evaluate LADD on architectures not tested in the paper (e.g., MobileNet, EfficientNet, Swin Transformer) across multiple datasets (ImageNet, CIFAR-100, medical imaging datasets) to verify the cross-architecture claims.

2. **Labeler Sensitivity Analysis**: Systematically vary the labeler training epochs (5, 10, 20, 50) and measure the impact on dense label quality (using metrics like label entropy and diversity) and final model accuracy to validate whether 10 epochs is truly optimal or dataset-dependent.

3. **Dynamic Sub-sampling Comparison**: Implement an adaptive sub-sampling strategy that adjusts N and R based on image content or dataset characteristics, then compare against LADD's static approach across multiple datasets to determine if the simplicity of static sub-sampling comes at a performance cost.