---
ver: rpa2
title: Winning Solution For Meta KDD Cup' 24
arxiv_id: '2410.00005'
source_url: https://arxiv.org/abs/2410.00005
tags:
- answer
- movie
- query
- task
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The db3 team achieved 1st place in all three tasks of the Meta
  KDD Cup 2024 CRAG challenge. Their solution combines a web retrieval and RAG framework
  for Task 1, regularized API-based knowledge graph extraction for Tasks 2 and 3,
  and fine-tuned LLM inference to reduce hallucination.
---

# Winning Solution For Meta KDD Cup' 24

## Quick Facts
- arXiv ID: 2410.00005
- Source URL: https://arxiv.org/abs/2410.00005
- Reference count: 21
- Primary result: 1st place in all three tasks of Meta KDD Cup 2024 CRAG challenge

## Executive Summary
The db3 team achieved 1st place in all three tasks of the Meta KDD Cup 2024 CRAG challenge through a comprehensive solution combining web retrieval with RAG frameworks and regularized API-based knowledge graph extraction. Their approach uses a retriever-reranker pipeline with parent-child chunking strategy for Task #1, while Tasks #2 and #3 employ simplified API interfaces that LLMs can reliably generate. The solution incorporates fine-tuned Llama-3-8B models using LoRA adaptation and a special multi-step labeling protocol to reduce hallucination and improve context comprehension.

## Method Summary
The solution employs a web retrieval and RAG framework for Task #1, using bge-base-en-v1.5 for retrieval and bge-reranker-v2-m3 for reranking with parent-child chunk structures. For Tasks #2 and #3, the team created regularized API interfaces (get_person, get_movie, get_movie_person_X) that simplify LLM generation while maintaining access to structured knowledge graph data. The core innovation involves fine-tuning Llama-3-8B with LoRA (alpha=16, r=8, learning_rate=2e-4) using a selective labeling strategy that handles invalid questions and fast-changing facts. This multi-step protocol first checks query validity, generates basic answers, verifies correctness with LLM, and labels accordingly to reduce hallucination.

## Key Results
- Achieved 1st place across all three CRAG challenge tasks
- Task #1 score: 28.4% using web retrieval and RAG framework
- Task #2 score: 42.7% using regularized API-based KG extraction
- Task #3 score: 47.8% excelling in finance and real-time domains

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with selective labeling strategy improves hallucination detection and context comprehension. The approach generates labels by first checking if a query is invalid, then using basic prompts to generate answers, followed by LLM verification to determine if the answer is correct. For wrong answers, it checks if context is relevant to ground truth, labeling as "I don't know" for irrelevant contexts or ground truth answer for relevant ones.

### Mechanism 2
Regularized API interface extracts directly relevant information to help LLMs answer correctly. The system designs simplified API templates (get_person, get_movie, get_movie_person_X) that are easy for LLMs to generate, with a parser that converts API results to natural language format. This reduces the complexity of code/SQL generation while maintaining access to structured knowledge graph data.

### Mechanism 3
Parent-child chunk retriever balances retrieval precision and information retention. The system uses smaller child chunks (e.g., sentences) for retrieval precision and larger parent chunks (e.g., paragraphs) for information retention, with bge-base-en-v1.5 for retrieval and bge-reranker-v2-m3 for secondary screening to identify most valuable chunks.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The challenge requires integrating external information from web sources and knowledge graphs to provide grounded answers and reduce hallucination
  - Quick check question: What are the three main components of a RAG system, and how does each component contribute to answering questions accurately?

- Concept: Fine-tuning with selective labeling
  - Why needed here: Standard fine-tuning approaches don't account for the specific challenges of RAG systems, such as handling invalid questions and dealing with fast-changing facts
  - Quick check question: How does the selective labeling strategy differentiate between questions that should be answered, questions that should be marked as "invalid," and questions where "I don't know" is the appropriate response?

- Concept: API design for LLM generation
  - Why needed here: Complex API generation tasks can overwhelm smaller LLMs, so simplified templates are needed to ensure reliable information extraction from structured data sources
  - Quick check question: What are the key design principles for creating APIs that LLMs can reliably generate, and how do these principles differ from traditional database query interfaces?

## Architecture Onboarding

- Component map: Retriever (bge-base-en-v1.5) → Reranker (bge-reranker-v2-m3) → Fine-tuned LLM (LoRA-adapted Llama-3-8B) → Answer generation. For KG tasks: API generator (fine-tuned Llama-3-8B) → Regularized API parser → Context → Fine-tuned LLM
- Critical path: Query → Web/Knowledge Graph retrieval → Context preparation → Fine-tuned LLM inference → Answer generation
- Design tradeoffs: Smaller LLMs (8B) for speed and cost vs. larger models for accuracy; simplified APIs for reliability vs. complex APIs for expressiveness; parent-child chunking for balance vs. single chunk size for simplicity
- Failure signatures: Low accuracy due to hallucination (LLM inference issues), retrieval failures (retriever/reranker problems), API generation errors (knowledge graph task failures), or time budget violations (performance bottlenecks)
- First 3 experiments:
  1. Test the retriever and reranker pipeline with sample queries to verify context quality and retrieval precision
  2. Evaluate the fine-tuned LLM's ability to answer questions with and without the RAG context to measure hallucination reduction
  3. Test the API generation system with sample queries to verify the parser can extract correct information from the generated API calls

## Open Questions the Paper Calls Out

### Open Question 1
How can the RAG system balance efficiency and effectiveness when dealing with 50 web pages and limited time budget as in Task #3? The paper mentions using a reranker to select top 5 related web pages from 50 pages based on snippets, but doesn't explore alternative strategies for handling large numbers of web pages efficiently.

### Open Question 2
What is the optimal approach for handling multi-hop queries in the regularized API system, especially for complex reasoning tasks? The paper mentions theoretical multi-hop capabilities using subqueries but states this wasn't completed due to time constraints.

### Open Question 3
How can the system improve handling of fast-changing facts and real-time information beyond current approaches? The paper mentions challenges with fast-changing facts in finance and real-time domains but doesn't explore advanced techniques for dynamic information retrieval.

## Limitations
- Evaluation data availability cannot be independently verified as test data and evaluation scripts are not publicly available
- Domain generalization claims lack quantitative comparisons across the five different domains
- Critical implementation details including exact API schemas and complete prompt templates remain unspecified
- Temporal generalization for questions about events after training cutoff date is not demonstrated

## Confidence
- High Confidence: Core architectural approach combining RAG with fine-tuned LLMs and parent-child chunking strategy
- Medium Confidence: Selective labeling strategy for fine-tuning shows promise but lacks detailed validation
- Low Confidence: 1st place achievement claims cannot be independently verified due to lack of public evaluation data

## Next Checks
1. Implement the parent-child chunk retriever with specified chunk sizes and evaluate retrieval precision/recall on validation set
2. Replicate the selective labeling strategy and fine-tune Llama-3-8B with LoRA to measure hallucination reduction
3. Create a simplified API parser and test LLM's ability to generate correct API calls across all five domains