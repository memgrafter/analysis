---
ver: rpa2
title: 'DoubleDipper: Improving Long-Context LLMs via Context Recycling'
arxiv_id: '2406.13632'
source_url: https://arxiv.org/abs/2406.13632
tags:
- double
- dipper
- context
- input
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of long-context question answering,
  where language models struggle when relevant information is buried within lengthy
  texts. The proposed method, DoubleDipper, generates few-shot in-context learning
  examples by recycling the given long context.
---

# DoubleDipper: Improving Long-Context LLMs via Context Recycling

## Quick Facts
- arXiv ID: 2406.13632
- Source URL: https://arxiv.org/abs/2406.13632
- Authors: Arie Cattan; Alon Jacovi; Alex Fabrikant; Jonathan Herzig; Roee Aharoni; Hannah Rashkin; Dror Marcus; Avinatan Hassidim; Yossi Matias; Idan Szpektor; Avi Caciularu
- Reference count: 7
- One-line primary result: Improves average accuracy by 16 absolute points across models on long-context QA tasks.

## Executive Summary
This paper addresses the challenge of long-context question answering where language models struggle when relevant information is buried within lengthy texts. The proposed method, DoubleDipper, generates few-shot in-context learning examples by recycling the given long context. Specifically, it randomly selects paragraphs from the input, generates question-answer pairs for each, and uses these as demonstrations. Each demonstration explicitly identifies the relevant paragraphs before the answer, encouraging the model to localize information before reasoning. This approach adds minimal tokens to the prompt and ensures domain consistency across examples.

Applied to various commercial and open-source models on multiple long-context QA datasets, DoubleDipper consistently outperforms baselines, improving average accuracy by 16 absolute points across models. It generalizes from single-paragraph examples to multi-hop reasoning and enhances robustness to the position of relevant information within the text.

## Method Summary
DoubleDipper generates few-shot in-context learning examples by recycling the given long context. The method randomly selects paragraphs from the input context, generates question-answer pairs for each selected paragraph, and uses these as demonstrations. Each demonstration explicitly identifies the relevant paragraphs before the answer, encouraging the model to localize information before reasoning. This approach adds minimal tokens to the prompt while ensuring domain consistency across examples. The method was evaluated on multiple commercial and open-source models across various long-context QA datasets.

## Key Results
- Improves average accuracy by 16 absolute points across models on long-context QA tasks
- Demonstrates strong generalization from single-hop to multi-hop reasoning
- Outperforms traditional ICL and zero-shot baselines across multiple LLMs and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context recycling reduces token overhead while maintaining domain consistency
- Mechanism: Instead of providing the full context for each demonstration, DoubleDipper generates QA pairs from randomly selected paragraphs and reuses the original long context only once. This dramatically reduces the number of tokens added to the prompt while ensuring all demonstrations are derived from the same domain as the target query.
- Core assumption: The model can learn to answer questions about the full context by seeing demonstrations that only reference specific paragraphs, without needing each demonstration to include the full context
- Evidence anchors: [abstract] "This ensures that the demonstrations are leveraging the same context as the target query while only adding a small number of tokens to the prompt"; [section] "As each example in the demonstration consists only of a question, an answer and the ID of relevant passage, the number of added tokens due to the extra demonstrations is minimal"

### Mechanism 2
- Claim: Explicit evidence identification before answering improves localization and reasoning
- Mechanism: Each demonstration includes a structured format where the model first identifies relevant paragraphs (Evidence: pi) before generating the answer. This creates a Chain of Thought-like process that forces the model to localize information before reasoning.
- Core assumption: LLMs can be trained to follow explicit formatting instructions and that this localization step improves overall reasoning performance
- Evidence anchors: [abstract] "we further enhance each demonstration by instructing the model to explicitly identify the relevant paragraphs before the answer, which improves performance while providing fine-grained attribution"; [section] "instructing the model to provide the supporting paragraphs is valuable on its own as it offers transparency and substantially eases human evaluation"

### Mechanism 3
- Claim: Generalization from single-hop demonstrations to multi-hop reasoning
- Mechanism: Despite demonstrations being confined to single paragraphs, the model can generalize to answer questions requiring information from multiple paragraphs (multi-hop QA)
- Core assumption: The model can learn to combine information across demonstrations even when individual demonstrations only reference single paragraphs
- Evidence anchors: [abstract] "Surprisingly, despite introducing only single-hop ICL examples, LLMs also successfully generalize to multi-hop long-context QA using our approach"; [section] "while our few-shot examples focus on single-paragraph answers, DOUBLE DIPPER demonstrates strong generalization across diverse QA formats...including multi-hop QA datasets"

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: DoubleDipper is fundamentally an ICL method that uses demonstrations to improve performance on long-context QA tasks
  - Quick check question: What distinguishes few-shot learning from fine-tuning in the context of LLMs?

- Concept: Context window limitations
  - Why needed here: Understanding why traditional ICL with long contexts is problematic due to token constraints and why recycling the context solves this
  - Quick check question: What happens to LLM performance as input length approaches or exceeds training context lengths?

- Concept: Multi-hop reasoning
  - Why needed here: The model needs to understand how to combine information from multiple sources to answer complex questions
  - Quick check question: What distinguishes single-hop from multi-hop question answering?

## Architecture Onboarding

- Component map: Input context (D) containing n paragraphs -> Paragraph selection module (randomly selects k paragraphs) -> QA generation module (generates question-answer pairs for each selected paragraph) -> Demonstration assembly (formats QA pairs with evidence identification) -> Prompt construction (combines original context, demonstrations, and target query) -> LLM inference (generates supporting paragraphs and answer)

- Critical path: 1. Input context and query arrive 2. k paragraphs are randomly selected from context 3. For each paragraph, generate 5 QA pairs 4. Randomly select 1 QA pair per paragraph to form demonstrations 5. Construct prompt with context + demonstrations + query 6. Run inference to get supporting paragraphs and answer

- Design tradeoffs:
  - Random paragraph selection vs. strategic selection (current: random, tradeoff: simplicity vs. potential optimization)
  - Number of demonstrations (current: 3, tradeoff: performance vs. token overhead)
  - Self-generated vs. external QA generation (current: self-generated, tradeoff: domain consistency vs. quality)

- Failure signatures:
  - Poor performance on questions where answer is in middle of context
  - Demonstrations don't cover relevant information for target query
  - Model ignores evidence identification instruction
  - Self-generated QA pairs are low quality or incorrect

- First 3 experiments:
  1. Baseline comparison: Run without any demonstrations to establish performance floor
  2. Fixed paragraph selection: Instead of random, always select first/last/middle paragraphs to test position bias
  3. Demonstration count sweep: Test with k=1, k=3, k=5, k=10 to find optimal number of demonstrations

## Open Questions the Paper Calls Out
The paper identifies several limitations and open questions:
- The optimal strategy for selecting paragraphs to use in demonstrations within DoubleDipper
- How the quality of automatically generated question-answer pairs affects the performance of DoubleDipper compared to manually curated demonstrations
- How DoubleDipper's performance scales with context length beyond the 1-3k token range studied in the paper

## Limitations
- Limited evaluation to contexts between 1,000 to 4,000 tokens, with performance on extremely long contexts (10k+ tokens) remaining unclear
- Reliance on self-generated demonstrations introduces potential quality issues that aren't fully addressed
- Random paragraph selection strategy may miss critical information for certain queries

## Confidence

- **High confidence**: The claim that DoubleDipper reduces token overhead while maintaining domain consistency
- **Medium confidence**: The claim that explicit evidence identification improves localization and reasoning
- **Medium confidence**: The claim of generalization from single-hop to multi-hop reasoning

## Next Checks

1. **Position Bias Analysis**: Systematically vary paragraph selection (first, middle, last) rather than random selection to quantify the impact of information position on performance and test the claim about robustness to position.

2. **Self-Generated Quality Audit**: Manually evaluate a sample of self-generated QA pairs for accuracy and relevance to their source paragraphs to quantify the potential impact of demonstration quality on overall performance.

3. **Token Overhead Characterization**: Measure and compare the actual token counts and inference time for DoubleDipper versus traditional ICL and zero-shot approaches across different context lengths to fully characterize the efficiency gains.