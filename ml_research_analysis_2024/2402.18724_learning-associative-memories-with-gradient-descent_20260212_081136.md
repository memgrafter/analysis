---
ver: rpa2
title: Learning Associative Memories with Gradient Descent
arxiv_id: '2402.18724'
source_url: https://arxiv.org/abs/2402.18724
tags:
- gradient
- dynamics
- learning
- when
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the training dynamics of an associative memory
  model storing outer products of token embeddings, with cross-entropy loss. The authors
  reduce the problem to a system of interacting particles, whose interactions depend
  on token frequencies and correlations between embeddings.
---

# Learning Associative Memories with Gradient Descent

## Quick Facts
- arXiv ID: 2402.18724
- Source URL: https://arxiv.org/abs/2402.18724
- Reference count: 40
- One-line primary result: Associative memory training with cross-entropy loss exhibits logarithmic margin growth in overparameterized regimes, but shows oscillatory transients and loss spikes due to memory interferences from correlated embeddings and token frequency imbalances.

## Executive Summary
This paper studies the training dynamics of associative memory models where associations are stored as outer products of token embeddings, using cross-entropy loss. The authors develop a theoretical framework that reduces the gradient dynamics to a system of interacting particles, enabling analysis of convergence properties and transient behaviors. They show that in overparameterized regimes (d ≥ N), margins grow logarithmically but with oscillatory transients and loss spikes due to memory interferences, while larger learning rates accelerate asymptotic convergence despite initial instabilities. In underparameterized regimes (d < N), cross-entropy loss can lead to suboptimal memorization where rare associations are forgotten.

## Method Summary
The paper analyzes associative memory training where a weight matrix W stores associations between input tokens x and output tokens y as outer products of their embeddings. The model computes scores ⟨ex, Wuy⟩ with softmax parameterization and cross-entropy loss. The key theoretical contribution is reducing the full gradient dynamics to a system of interacting particles, where each particle represents a score for a token-class pair and interactions depend on token frequencies, embedding correlations, and class structure. This particle system framework enables analysis of convergence rates, margin evolution, and transient behaviors across different parameter regimes.

## Key Results
- In overparameterized regimes (d ≥ N), classification margins grow logarithmically in time with rate 1/ηt, but exhibit oscillatory transients and loss spikes due to memory interferences
- Memory interferences from correlated embeddings and token frequency imbalances create transitory regimes where rare token margins temporarily decrease before recovering
- Larger learning rates accelerate asymptotic convergence despite causing more pronounced oscillations and initial instabilities
- In underparameterized regimes (d < N), cross-entropy loss can lead to suboptimal memorization where rare associations are forgotten to minimize loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-entropy loss on token embeddings creates an interacting particle system where each particle represents a score for a token-class pair.
- Mechanism: The gradient dynamics decompose into a system of non-linearly coupled particles whose interactions depend on token frequencies, embedding correlations, and class structure. These particles compete for gradient updates, leading to emergent behaviors like logarithmic margin growth and loss spikes.
- Core assumption: The outer product structure of the associative memory and the softmax parameterization create sufficient statistics that reduce the full gradient dynamics to this particle system.
- Evidence anchors:
  - [abstract] "We reduce this problem to the study of a system of particles, which interact according to properties of the data distribution and correlations between embeddings."
  - [section] "Theorem 1 (Particle system). Define the particle wij... Hence, all variations of gradient dynamics... can be expressed as a (stochastic) system of interacting particles."
  - [corpus] Weak - corpus neighbors discuss associative memory but not this specific particle-based gradient decomposition mechanism.

### Mechanism 2
- Claim: In overparameterized regimes (d ≥ N), memory associations grow logarithmically in margin space with initial oscillatory transients due to frequency imbalances and embedding correlations.
- Mechanism: The system exhibits a transitory regime where frequent tokens initially dominate, causing rare token margins to temporarily decrease (loss spikes), before all margins eventually grow logarithmically and settle into a stationary regime. Larger learning rates accelerate asymptotic convergence despite initial instabilities.
- Core assumption: The convexity of cross-entropy loss ensures perfect accuracy is achievable when d ≥ N, but the non-separable geometry creates transient competition between memories.
- Evidence anchors:
  - [abstract] "In overparameterized regimes, we obtain logarithmic growth of the 'classification margins.' Yet, we show that imbalance in token frequencies and memory interferences due to correlated embeddings lead to oscillatory transitory regimes."
  - [section] "Theorem 2 (Binary orthogonal)... leads to the following bound on the loss, L(Wt) ≤ 1/tη..."
  - [corpus] Moderate - corpus discusses transformers and associative memory but not this specific logarithmic growth with oscillations pattern.

### Mechanism 3
- Claim: In underparameterized regimes (d < N), cross-entropy loss can lead to suboptimal memorization schemes where the optimizer "forgets" less frequent associations to minimize loss.
- Mechanism: Limited capacity forces competition between memories, and the cross-entropy loss is not calibrated for the associative memory task - minimizing L(W) does not minimize classification error L01(W). The optimizer may converge to regions where frequent associations are perfectly stored while rare ones are forgotten.
- Core assumption: The cross-entropy loss surface has multiple minima with different memorization patterns, and gradient descent can get trapped in suboptimal ones when capacity is limited.
- Evidence anchors:
  - [abstract] "In underparameterized regimes, we illustrate how the cross-entropy loss can lead to suboptimal memorization schemes."
  - [section] "Figure 4 reveals a striking fact: the cross-entropy loss is not calibrated for our model, i.e., minimizing L(W) does not always minimize L01(W)."
  - [corpus] Moderate - corpus includes work on bidirectional associative memory and robustness, which relates to memorization capacity issues.

## Foundational Learning

- Concept: Cross-entropy loss and softmax parameterization
  - Why needed here: The entire gradient dynamics analysis relies on the softmax parameterization of conditional probabilities and the cross-entropy loss structure to derive the particle system equations.
  - Quick check question: Can you derive the gradient of the cross-entropy loss for a softmax model with respect to the weight matrix?

- Concept: Linear separability and margin maximization
  - Why needed here: Understanding when the associative memory problem becomes linearly separable (d ≥ N with orthogonal embeddings) is crucial for analyzing convergence rates and the transition from transitory to stationary regimes.
  - Quick check question: Under what conditions on the embedding dimension d and the number of associations N does the associative memory problem become linearly separable?

- Concept: Interacting particle systems and mean-field limits
  - Why needed here: The reduction to a particle system with non-linear interactions is the key theoretical framework that enables analysis of the dynamics beyond standard convex optimization results.
  - Quick check question: How does the correlation structure between embeddings (αij parameters) affect the interaction terms in the particle system equations?

## Architecture Onboarding

- Component map: Input tokens x ∈ [N] -> Fixed embeddings ex ∈ Rd -> Weight matrix W ∈ Rd×d -> Softmax scores ⟨ex, Wuy⟩ -> Cross-entropy loss -> Gradient updates -> Updated W

- Critical path: Initialize W → Compute softmax scores ⟨ex, Wuy⟩ → Calculate cross-entropy loss → Backpropagate gradients → Update W via gradient descent → Monitor margins and loss

- Design tradeoffs:
  - Embedding dimension d vs. number of associations N: Higher d reduces interference but increases computational cost
  - Learning rate η: Larger rates accelerate convergence but cause more oscillations and potential loss spikes
  - Embedding correlation α: Lower correlation reduces interference but may require more parameters for equivalent capacity

- Failure signatures:
  - Loss spikes: Temporary increase in loss due to rare token margins being suppressed by frequent token dominance
  - Oscillations: Alternating growth and shrinkage of different token margins, especially with large learning rates
  - Suboptimal memorization: Convergence to configurations where some associations are forgotten despite perfect training loss

- First 3 experiments:
  1. Binary classification with orthogonal embeddings (d ≥ N): Verify logarithmic margin growth and loss decay rate 1/ηt
  2. Two-token system with correlated embeddings: Observe loss spikes when p1 ≫ p2 and α > 0, then recovery
  3. Limited capacity regime (d < N): Demonstrate forgetting phenomenon where rare associations are lost during training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do memory interferences change when using factorized parameterizations (e.g., low-rank or block-structured matrices) instead of full matrices?
- Basis in paper: [inferred] The paper analyzes full matrix associative memories and observes memory interferences due to correlated embeddings; it doesn't explore structured parameterizations.
- Why unresolved: The analysis focuses on full-rank parameter spaces where embeddings naturally become correlated in underparameterized regimes, but factorized models might have different interference patterns.
- What evidence would resolve it: Comparative experiments showing margin dynamics and interference patterns for low-rank vs full-rank associative memories under varying correlation structures and token frequencies.

### Open Question 2
- Question: How do normalization layers (like LayerNorm or BatchNorm) affect the oscillatory behavior and loss spikes observed with large learning rates?
- Basis in paper: [explicit] The discussion section mentions normalization layers as an additional factor at play in larger models that could change training behaviors.
- Why unresolved: The paper's analysis is on raw associative memories without normalization, but transformers typically include normalization layers that might dampen or amplify instabilities.
- What evidence would resolve it: Controlled experiments comparing training dynamics of associative memories with and without normalization layers across different learning rate regimes.

### Open Question 3
- Question: What is the impact of data noise and stochastic gradients on the settling time to the stationary logarithmic regime in overparameterized settings?
- Basis in paper: [inferred] The paper analyzes both deterministic and stochastic dynamics but focuses on idealized settings without noisy data; it mentions stochasticity as a perturbation in the continuous-time regime.
- Why unresolved: The analysis provides clean conditions for logarithmic convergence but doesn't quantify how realistic noise sources affect convergence speed or whether oscillations persist.
- What evidence would resolve it: Empirical studies measuring convergence rates and oscillation amplitudes under varying noise levels and batch sizes, compared to theoretical predictions.

## Limitations

- The theoretical analysis assumes idealized settings without data noise, and the extension to stochastic gradients is limited to asymptotic regimes
- The particle system reduction, while elegant, may not fully capture finite-sample effects or non-asymptotic behaviors in practical implementations
- Claims about suboptimal memorization in underparameterized regimes lack rigorous characterization of when and how these suboptimal minima occur

## Confidence

- **High Confidence**: The mathematical derivation of the particle system equations and the convergence analysis for orthogonal embeddings in overparameterized regimes. These results follow from standard techniques in interacting particle systems and convex optimization.
- **Medium Confidence**: The characterization of oscillatory transitory regimes and loss spikes. While the theoretical framework supports these phenomena, the exact conditions for their emergence and the quantitative predictions require more empirical validation across different embedding distributions.
- **Low Confidence**: The claims about suboptimal memorization in underparameterized regimes. The analysis provides intuition but lacks rigorous characterization of when and how these suboptimal minima occur, and the experimental validation is limited to specific synthetic examples.

## Next Checks

1. **Empirical Validation of Loss Spikes**: Implement the two-token system with controlled correlation α and frequency imbalance p1 ≫ p2. Measure the amplitude and duration of loss spikes across a grid of (α, p1, η) values to empirically verify the theoretical predictions about their relationship to embedding correlations and learning rates.

2. **Capacity-Dependent Memorization Patterns**: Systematically vary the embedding dimension d while keeping N fixed, and track the evolution of individual token margins and classification accuracy. Quantify the transition from optimal to suboptimal memorization schemes and characterize the phase transition in terms of d/N ratio.

3. **Generalization to Non-Orthogonal, Non-Correlation Structures**: Extend the particle system analysis to cases where embeddings have arbitrary correlation structures beyond the α parameterization. Test whether the core insights about transitory oscillations and logarithmic margin growth hold when the embedding space has more complex geometric properties.