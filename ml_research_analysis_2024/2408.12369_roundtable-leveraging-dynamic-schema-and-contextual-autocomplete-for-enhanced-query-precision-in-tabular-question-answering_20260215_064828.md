---
ver: rpa2
title: 'RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for Enhanced
  Query Precision in Tabular Question Answering'
arxiv_id: '2408.12369'
source_url: https://arxiv.org/abs/2408.12369
tags:
- data
- query
- language
- questions
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of querying large, complex tabular\
  \ datasets using natural language, where LLMs struggle with identifying relevant\
  \ columns and values due to vast schema and ambiguous queries. To overcome this,\
  \ the authors propose a framework leveraging Full-Text Search (FTS) on metadata\
  \ to enable precise detection of attributes and values, narrowing the LLM\u2019\
  s search space."
---

# RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for Enhanced Query Precision in Tabular Question Answering

## Quick Facts
- **arXiv ID**: 2408.12369
- **Source URL**: https://arxiv.org/abs/2408.12369
- **Reference count**: 3
- **Primary result**: Up to 66.67% improvement in query accuracy for value-based queries compared to vanilla LLMs

## Executive Summary
The paper addresses the challenge of querying large, complex tabular datasets using natural language, where LLMs struggle with identifying relevant columns and values due to vast schema and ambiguous queries. To overcome this, the authors propose a framework leveraging Full-Text Search (FTS) on metadata to enable precise detection of attributes and values, narrowing the LLM's search space. Additionally, a contextual autocomplete feature provides real-time query suggestions, improving user interaction and query accuracy. The framework is evaluated on a diverse benchmark of 1500 quantitative questions across domains, showing significant accuracy gains—up to 66.67% improvement on value-based queries—when using the proposed method compared to vanilla LLMs.

## Method Summary
The RoundTable framework preprocesses tabular data by extracting attributes, filtering categorical columns, and building an inverse index of unique values with generated synonyms. When a user query arrives, keyword extraction retrieves relevant metadata from the index, creating a focused dynamic schema. This schema, combined with a static prompt, is fed to the LLM to generate accurate database queries. The system also includes a contextual autocomplete feature that provides real-time suggestions based on the indexed data vocabulary, guiding users toward correct terminology and reducing query ambiguity.

## Key Results
- Up to 66.67% accuracy improvement on value-based queries compared to vanilla LLMs
- Dynamic schema generation narrows LLM search space, improving query precision
- Contextual autocomplete enhances user interaction with complex datasets through real-time suggestions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Full-Text Search (FTS) on metadata improves LLM accuracy by narrowing the search space for relevant attributes and values.
- **Mechanism**: The framework constructs an inverse index of unique values and attribute synonyms from categorical columns. When a user query arrives, keyword extraction is used to retrieve matching metadata, creating a focused schema for the LLM. This reduces the model's need to infer associations from ambiguous text, leading to more precise query generation.
- **Core assumption**: Metadata indexing can sufficiently represent the table structure to guide LLM reasoning without requiring full data exposure.
- **Evidence anchors**:
  - [abstract]: "leverages Full-Text Search (FTS) on the input table... enables precise detection of specific values and columns but also narrows the search space for language models"
  - [section 4.4]: "GENERATE_SYNONYMS function... increase the search power of the properties in our dataset... strengthen the index's robustness"
  - [corpus]: Weak—no direct comparison to FTS indexing in cited works.
- **Break condition**: If the table contains many high-cardinality columns or textual data where synonyms and unique values don't sufficiently narrow scope, FTS indexing will not reduce search space effectively.

### Mechanism 2
- **Claim**: Contextual autocomplete reduces user query ambiguity by providing real-time suggestions based on indexed data vocabulary.
- **Mechanism**: As the user types, the system uses the same FTS index to offer attribute and value suggestions, guiding users toward correct terminology and reducing cold-start friction. This improves query formulation quality before it reaches the LLM.
- **Core assumption**: Real-time suggestion of valid terms improves user query formulation accuracy and reduces the LLM's burden to interpret ambiguous input.
- **Evidence anchors**:
  - [abstract]: "supports a custom auto-complete feature that suggests queries based on the data in the table... significantly refines the interaction between the user and complex datasets"
  - [section 4.9]: "Contextual autocomplete function... provides real-time dropdown suggestions as they type the question"
  - [corpus]: Weak—no empirical evaluation of autocomplete accuracy or its impact on LLM performance.
- **Break condition**: If the dataset is extremely sparse or user queries are highly domain-specific, autocomplete suggestions may not match intended terms, reducing effectiveness.

### Mechanism 3
- **Claim**: Dynamic schema generation based on user query context improves the LLM's ability to generate syntactically correct and semantically meaningful database queries.
- **Mechanism**: After keyword extraction and metadata pruning, the system creates a query-specific schema containing only relevant attributes and values. This schema, combined with a static prompt, is fed to the LLM, constraining its output to valid SQL/Pandas syntax and relevant domain context.
- **Core assumption**: A dynamically generated schema focused on query-relevant terms enables the LLM to generate more accurate database queries than a generic schema or raw table.
- **Evidence anchors**:
  - [section 4.6-4.9]: Describes EXTRACT_KEYWORDS, SEARCH_INDEX, CREATE_PROMPT, and FORMULATE_QUERY steps forming the dynamic schema
  - [abstract]: "narrow the search space for language models, thereby enhancing query accuracy"
  - [corpus]: Weak—no direct comparison to dynamic vs static schema approaches.
- **Break condition**: If the keyword extraction or synonym generation is inaccurate, the schema will be incomplete or misleading, causing the LLM to generate irrelevant or incorrect queries.

## Foundational Learning

- **Concept**: Full-Text Search (FTS) indexing
  - Why needed here: Enables fast retrieval of relevant attributes and values without exposing entire dataset to LLM, improving accuracy and efficiency.
  - Quick check question: What is the difference between FTS indexing and relational indexing, and why is FTS more suitable for natural language queries?

- **Concept**: Synonym generation and semantic expansion
  - Why needed here: Users rarely use exact column names; synonyms allow matching varied linguistic expressions to correct attributes.
  - Quick check question: How does the LLM-generated synonym list improve recall for ambiguous queries compared to exact matching?

- **Concept**: Dynamic schema construction
  - Why needed here: Static schemas include irrelevant attributes; dynamic schemas tailored to each query reduce noise and focus LLM attention.
  - Quick check question: Why does limiting the schema to query-relevant attributes improve LLM-generated query accuracy?

## Architecture Onboarding

- **Component map**: Data preprocessing → Attribute extraction → Categorical filtering → Unique value indexing → Synonym generation → Inverse index construction → Query handling → Keyword extraction → Index search → Schema generation → LLM prompt formulation → Query generation → UI layer → Contextual autocomplete powered by same index

- **Critical path**: User query → keyword extraction → metadata pruning → dynamic schema → LLM prompt → generated DB query. Any failure in keyword extraction or index search breaks the pipeline.

- **Design tradeoffs**:
  - Memory vs. coverage: Indexing all unique values vs. sampling; full synonym expansion vs. performance
  - Accuracy vs. latency: Larger index and richer synonym lists improve recall but slow search
  - LLM dependency: Relies on LLM's ability to parse dynamic schemas; quantization limits reasoning depth

- **Failure signatures**:
  - Missing autocomplete suggestions → index not populated or keyword extraction failed
  - Incorrect query generation → dynamic schema malformed or LLM prompt poorly structured
  - Slow response → index too large or synonym generation too heavy

- **First 3 experiments**:
  1. Test keyword extraction accuracy on ambiguous queries vs. exact matches.
  2. Measure autocomplete suggestion relevance using a held-out validation set.
  3. Compare LLM query generation accuracy with and without dynamic schema on the same query set.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several critical uncertainties remain based on the evaluation scope and methodology:

- How does the RoundTable framework's performance scale with increasingly larger and more complex datasets beyond the 1500-question benchmark used in the evaluation?
- What is the impact of using different LLMs (beyond Mistral-7B-Instruct-v0.2) on the performance of the RoundTable framework?
- How does the RoundTable framework handle ambiguous queries that could potentially have multiple valid interpretations or lead to significantly different results?

## Limitations

- The reported 66.67% accuracy improvement is specific to value-based queries only, with no reported performance on other query types.
- No empirical evidence quantifying the contextual autocomplete's effect on query accuracy or user behavior.
- Critical implementation details about FTS indexing, synonym generation, and dynamic schema construction are omitted.

## Confidence

- **High confidence**: The general approach of using FTS indexing to narrow LLM search space is sound and well-aligned with existing information retrieval principles.
- **Medium confidence**: The reported accuracy improvements on value-based queries are plausible given the methodology, but lack of baseline details and benchmark composition limits verifiability.
- **Low confidence**: The claimed impact of contextual autocomplete on query accuracy is not empirically supported and appears to be assumed rather than measured.

## Next Checks

1. **Replicate the evaluation protocol**: Run the framework on a held-out dataset with detailed query type breakdown (value-based, aggregation, filter-based) to verify the 66.67% improvement claim and assess performance across all query categories.

2. **A/B test autocomplete impact**: Compare query accuracy and formulation time with and without the contextual autocomplete feature using the same user queries to isolate its contribution to overall performance.

3. **Stress test on high-cardinality datasets**: Evaluate framework performance on tables with numerous high-cardinality columns and textual data to identify failure modes where FTS indexing fails to sufficiently narrow the search space.