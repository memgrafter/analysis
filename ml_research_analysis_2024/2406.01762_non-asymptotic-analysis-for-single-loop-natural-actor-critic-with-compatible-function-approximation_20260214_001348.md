---
ver: rpa2
title: Non-Asymptotic Analysis for Single-Loop (Natural) Actor-Critic with Compatible
  Function Approximation
arxiv_id: '2406.01762'
source_url: https://arxiv.org/abs/2406.01762
tags:
- function
- equation
- approximation
- follows
- compatible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides tight non-asymptotic convergence bounds for
  single-loop actor-critic (AC) and natural actor-critic (NAC) algorithms with compatible
  function approximation. By using compatible function approximation in the critic,
  the method eliminates the critic approximation error that plagues existing approaches,
  leading to improved error bounds.
---

# Non-Asymptotic Analysis for Single-Loop (Natural) Actor-Critic with Compatible Function Approximation

## Quick Facts
- arXiv ID: 2406.01762
- Source URL: https://arxiv.org/abs/2406.01762
- Authors: Yudan Wang; Yue Wang; Yi Zhou; Shaofeng Zou
- Reference count: 40
- Single-loop actor-critic achieves O(ε⁻²) sample complexity to ϵ stationary point with no critic approximation error

## Executive Summary
This paper provides the first tight non-asymptotic convergence analysis for single-loop actor-critic (AC) and natural actor-critic (NAC) algorithms using compatible function approximation. The key innovation is eliminating the critic approximation error term that plagues existing analyses by using policy-gradient-compatible features in the critic. The analysis is particularly challenging because the critic uses time-varying, policy-dependent features, which the authors handle through a novel auxiliary eligibility trace technique. The paper proves that AC converges to an ϵ stationary point with O(ε⁻²) sample complexity, while NAC converges to an ϵ + √ε actor neighborhood of the global optimum with O(ε⁻³) sample complexity.

## Method Summary
The method analyzes single-loop actor-critic algorithms where both actor and critic are updated simultaneously using a single Markovian trajectory. The critic uses k-step temporal difference learning with compatible function approximation, where features are policy gradients ∇logπω(a|s). The natural actor-critic variant benefits from a key simplification: when using compatible features, the natural gradient reduces to simply using the critic parameter θ*, eliminating the need to estimate and invert the Fisher information matrix. The theoretical analysis employs two-timescale stochastic approximation with carefully constructed Lyapunov functions and auxiliary eligibility traces to handle the time-varying critic features.

## Key Results
- AC converges to an ϵ stationary point with sample complexity O(ε⁻²)
- NAC converges to an ϵ + √ε actor neighborhood of the global optimum with sample complexity O(ε⁻³)
- Both algorithms achieve the best known sample complexities while eliminating the non-vanishing critic approximation error term εcritic
- Single-loop setting matches the sample complexity of nested-loop variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compatible function approximation eliminates critic approximation error term εcritic from convergence bounds
- Mechanism: The critic uses an ω-dependent linear function class where features are policy gradients ∇logπω(a|s), making the feature function time-varying but compatible with the policy update
- Core assumption: The feature matrix Φω is full rank when |S||A| ≥ d, and the critic uses k-step TD to approximate the compatible solution
- Evidence anchors:
  - [abstract] "This paper analyzes the convergence of both AC and NAC algorithms with compatible function approximation. Our analysis eliminates the term εcritic from the error bounds"
  - [section] "To avoid the critic's function approximation error, (Sutton et al., 1999) proposed a smart idea of compatible function approximation"
  - [corpus] weak evidence - no direct matching paper found
- Break condition: If Φω loses full rank or k-step TD doesn't converge properly, the compatibility breaks and εcritic reappears

### Mechanism 2
- Claim: Natural policy gradient simplifies to just using the critic parameter θ* when using compatible function approximation
- Mechanism: With compatible features, the Fisher information matrix Fω cancels with the policy gradient structure, eliminating the need to estimate and invert Fω
- Core assumption: The compatible feature ϕω(s,a) = ∇logπω(a|s) is used throughout
- Evidence anchors:
  - [abstract] "Another advantage of compatible function approximation when applied with NAC is that the inverse of the Fisher information in the natural gradient will cancel out with the policy gradient"
  - [section] "Proposition 2 ((Peters & Schaal, 2008)). With compatible function approximation, natural policy gradient is reduced to: e∇J(πω) = θ̄*ω"
  - [corpus] weak evidence - no direct matching paper found
- Break condition: If incompatible features are used or the policy gradient structure changes, the cancellation fails

### Mechanism 3
- Claim: Single-loop setting with single Markovian trajectory achieves same sample complexity as nested-loop variants
- Mechanism: Novel auxiliary eligibility trace construction with fixed features handles the time-varying critic feature challenge, enabling tight bounds without decoupling
- Core assumption: The auxiliary Markov chains and eligibility traces properly decompose the tracking error into manageable components
- Evidence anchors:
  - [abstract] "Our major technical novelty lies in analyzing the stochastic bias due to policy-dependent and time-varying compatible function approximation in the critic"
  - [section] "We design a novel approach to explicitly bound this error. The central idea is to construct an auxiliary eligibility trace with fixed feature to approximate the eligibility trace with time-varying feature"
  - [corpus] weak evidence - no direct matching paper found
- Break condition: If the auxiliary trace approximation error becomes too large or the Markovian noise dominates, the single-loop advantage disappears

## Foundational Learning

- Concept: Two-timescale stochastic approximation
  - Why needed here: The critic and actor updates run at different speeds (αt ≫ βt) to stabilize the learning process
  - Quick check question: Why must the critic step size be larger than the actor step size in two-timescale algorithms?

- Concept: Compatible function approximation
  - Why needed here: Eliminates approximation bias by making the critic's feature function match the policy gradient structure
  - Quick check question: What is the mathematical condition that makes a feature function "compatible" with a policy?

- Concept: k-step TD learning with time-varying features
  - Why needed here: The critic needs to track a policy-dependent value function using a linear approximator with changing features
  - Quick check question: How does the k-step TD update differ from standard TD(0) when the feature function changes over time?

## Architecture Onboarding

- Component map: Policy πω → Critic (k-step TD with ϕω(s,a)=∇logπω(a|s)) → Eligibility trace z → Actor update ω → Repeat

- Critical path: Policy → Critic update → Eligibility trace → Actor update → Repeat
  Key: Ensure the critic tracks the policy gradient structure while the actor uses this information

- Design tradeoffs:
  - Fixed features: Simpler but introduces εcritic error term
  - Compatible features: No εcritic but requires handling time-varying critic
  - k-step vs 1-step: Larger k reduces bias but increases variance

- Failure signatures:
  - Critic error grows: Check if Φω remains full rank
  - Actor diverges: Verify βt << αt and that the policy gradient estimate is stable
  - Eligibility trace instability: Monitor trace length and feature changes

- First 3 experiments:
  1. Implement compatible critic with fixed policy to verify εcritic elimination
  2. Test k-step TD convergence with time-varying features on a simple MDP
  3. Run single-loop vs nested-loop on a benchmark to confirm sample complexity match

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Theoretical analysis lacks empirical validation across diverse environments
- Full-rank condition for Φω requires |S||A| ≥ d, which may not hold in practice
- Auxiliary eligibility trace construction may face practical challenges in high-dimensional or continuous state spaces
- Sensitivity to k in k-step TD and performance in complex environments not explored

## Confidence

- **High Confidence**: The theoretical elimination of εcritic through compatible function approximation is mathematically sound given the stated assumptions. The convergence rate bounds O(ε⁻²) for AC and O(ε⁻³) for NAC are derived rigorously using established stochastic approximation techniques.

- **Medium Confidence**: The auxiliary eligibility trace technique for handling time-varying features appears innovative and theoretically justified, but practical implementation details are sparse. The claim about single-loop achieving same sample complexity as nested-loop variants needs empirical verification.

- **Low Confidence**: The practical performance improvements over existing methods cannot be assessed without empirical results. The sensitivity to hyperparameters (particularly k in k-step TD) and the algorithm's behavior in complex, high-dimensional environments remain unclear.

## Next Checks

1. **Empirical εcritic Elimination Test**: Implement the compatible critic with a fixed policy on a tabular MDP and measure whether the approximation error truly vanishes as predicted by theory, comparing against incompatible alternatives.

2. **Auxiliary Trace Approximation Error**: Design a controlled experiment where the auxiliary eligibility trace error can be measured explicitly, varying the policy change rate and feature complexity to understand when the approximation breaks down.

3. **Single vs Nested Loop Sample Complexity**: Run systematic experiments comparing single-loop and nested-loop variants across multiple MDPs, measuring actual sample complexity to verify the theoretical equivalence claim, particularly focusing on cases where policy changes are rapid.