---
ver: rpa2
title: Making Old Kurdish Publications Processable by Augmenting Available Optical
  Character Recognition Engines
arxiv_id: '2404.06101'
source_url: https://arxiv.org/abs/2404.06101
tags:
- documents
- historical
- text
- recognition
- character
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of processing historical Kurdish
  publications printed before 1950 using optical character recognition (OCR). Current
  OCR systems struggle with these documents due to issues such as damage, fragility,
  marks, and non-standard fonts.
---

# Making Old Kurdish Publications Processable by Augmenting Available Optical Character Recognition Engines

## Quick Facts
- arXiv ID: 2404.06101
- Source URL: https://arxiv.org/abs/2404.06101
- Authors: Blnd Yaseen; Hossein Hassani
- Reference count: 7
- Primary result: OCR system for historical Kurdish documents with CER 0.755% and accuracy 84.02%

## Executive Summary
This paper addresses the challenge of processing historical Kurdish publications printed before 1950 using optical character recognition (OCR). Current OCR systems struggle with these documents due to issues such as damage, fragility, marks, and non-standard fonts. To overcome this, the authors adopt Google's open-source Tesseract OCR framework (version 5.0) and develop a custom dataset of 1233 images of lines with transcriptions from historical Kurdish documents. They train the model using the Arabic model as the base model and evaluate its performance using Tesseract's built-in evaluator and Ocreval. The results show a Character Error Rate (CER) of 0.755% and an average character accuracy of 84.02%. The authors also develop a web application to provide an easy-to-use interface for end-users. The primary outcome is a functional OCR system for historical Kurdish documents, with potential for further improvement with a larger dataset.

## Method Summary
The authors developed an OCR system for historical Kurdish documents by fine-tuning Tesseract OCR 5.0 using an Arabic base model and a custom dataset of 1233 line images with transcriptions. The process involved data collection from historical documents, image preprocessing (binarization, noise removal, skew correction), dataset preparation in TIFF format with .gt.txt transcription files, model training on Ubuntu 22.04.2 LTS using tesstrain, and evaluation using lstmeval and Ocreval tools. A web application was also developed to provide user access to the OCR system.

## Key Results
- Character Error Rate (CER) of 0.755% achieved on historical Kurdish documents
- Average character accuracy of 84.02% measured using Ocreval
- Functional web application developed for end-user access
- Custom dataset of 1233 line images created from historical Kurdish publications

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning an existing multilingual OCR model (Tesseract with Arabic base) on a custom historical Kurdish dataset produces high accuracy for that domain. The pre-trained model already contains learned features for similar scripts, reducing the training burden. Domain-specific historical data then adapts these features to handle rare glyphs, degraded quality, and stylistic variations.

### Mechanism 2
Synthetic or targeted image preprocessing (binarization, de-skewing, noise removal) improves OCR accuracy on degraded historical documents. Preprocessing normalizes the input image, making the text more consistent with the training data's assumptions and reducing OCR engine confusion from artifacts like bleed-through or uneven illumination.

### Mechanism 3
A custom dataset with aligned image-line and transcription pairs is essential for training a functional OCR model for a low-resource language. The dataset provides supervised learning signals for the OCR engine to map visual glyphs to correct text, allowing it to generalize to unseen historical documents.

## Foundational Learning

- **Optical Character Recognition (OCR) pipeline stages**: Understanding the pipeline clarifies where preprocessing, training, and evaluation fit and why each step matters for historical documents. Quick check: What preprocessing step would you apply to correct a page with heavy ink bleed-through?

- **Transfer learning and fine-tuning in deep learning models**: Explains why starting from an Arabic base model is more efficient than training from scratch for Kurdish. Quick check: Why is transfer learning often preferred over training a new model from scratch when data is limited?

- **Evaluation metrics for OCR (Character Error Rate, Word Error Rate, accuracy)**: Provides a quantitative way to measure model performance and compare against baselines. Quick check: If CER is 0.755%, what is the approximate character accuracy percentage?

## Architecture Onboarding

- **Component map**: Data Collection & Digitization → Image Preprocessing → Dataset Preparation (TIFF + .gt.txt) → Tesseract Training Environment (Ubuntu, tesstrain) → Model Training & Evaluation (lstmeval, Ocreval) → Web Application Interface

- **Critical path**: Collect historical documents → Digitize → Preprocess images → Create dataset → Train model → Evaluate → Deploy interface

- **Design tradeoffs**: Using Arabic base model: faster convergence but limited by script similarity. Small dataset (1233 lines): quicker training but higher risk of overfitting and poor generalization. Manual transcription: high accuracy for ground truth but time-consuming.

- **Failure signatures**: High CER despite training: dataset too small/unaligned, preprocessing too aggressive, or script mismatch. Model fails on multi-column layouts: segmentation issues not addressed in preprocessing. Model misinterprets spaces: spacing irregularities not handled in dataset or post-processing.

- **First 3 experiments**: 1) Train on the 1233-line dataset and evaluate CER with lstmeval. 2) Apply Ocreval on a held-out set of pages to measure character accuracy. 3) Deploy a small web app and test on sample multi-column pages to observe segmentation failure.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the Kurdish OCR system compare to other low-resource language OCR systems trained on similar-sized datasets? The paper mentions that Kurdish is considered a low-resource language but does not provide a comparative analysis with other low-resource language OCR systems.

### Open Question 2
What is the impact of non-standard spacing between words and characters on the accuracy of the Kurdish OCR system? The paper mentions that non-standard spacing was a challenge but does not provide a detailed analysis of its impact on OCR accuracy.

### Open Question 3
How can the Kurdish OCR system be improved to handle multi-column pages and mathematical equations more effectively? The paper identifies these as limitations but does not propose specific solutions or techniques to address them.

## Limitations

- The study relies on a relatively small dataset of 1233 line images, which may limit the model's ability to generalize across the full variability of historical Kurdish publications.
- No direct comparison is provided against other OCR approaches or baseline models, making it difficult to assess relative performance.
- The paper does not specify the exact preprocessing pipeline or training hyperparameters, which are critical for reproducibility.

## Confidence

- **High Confidence**: The use of Tesseract OCR with a fine-tuned Arabic base model is a valid and well-documented approach for low-resource scripts; the reported CER and accuracy are internally consistent with the evaluation methods described.
- **Medium Confidence**: The custom dataset construction and preprocessing steps are plausible and follow standard practices, but the lack of detail and external validation limits certainty.
- **Low Confidence**: Claims about the web application's usability and the model's robustness to all types of historical document degradation are not substantiated with user studies or broader testing.

## Next Checks

1. **External Validation**: Test the trained model on an independent, held-out set of historical Kurdish documents not used in training or development, and report CER and accuracy using both lstmeval and Ocreval.

2. **Baseline Comparison**: Implement and evaluate at least one alternative OCR approach (e.g., training from scratch or using a different base model) on the same dataset to benchmark the proposed method's performance.

3. **Dataset Expansion and Ablation**: Systematically vary the dataset size and diversity (e.g., by adding more fonts or degradation types) to measure the impact on model accuracy and identify the minimum viable dataset size for acceptable performance.