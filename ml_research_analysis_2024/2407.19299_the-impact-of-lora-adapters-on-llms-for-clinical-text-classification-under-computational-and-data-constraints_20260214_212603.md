---
ver: rpa2
title: The Impact of LoRA Adapters on LLMs for Clinical Text Classification Under
  Computational and Data Constraints
arxiv_id: '2407.19299'
source_url: https://arxiv.org/abs/2407.19299
tags:
- adapters
- clinical
- adapter
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the effectiveness of adapter techniques,\
  \ specifically Low-Rank Adaptation (LoRA), for fine-tuning Large Language Models\
  \ (LLMs) in clinical text classification under computational and data constraints.\
  \ The research compares four adapter structures\u2014Adapter, Lightweight, TinyAttention,\
  \ and Gated Residual Network (GRN)\u2014with biomedical pre-trained models and simpler\
  \ Transformer-based models trained from scratch."
---

# The Impact of LoRA Adapters on LLMs for Clinical Text Classification Under Computational and Data Constraints

## Quick Facts
- arXiv ID: 2407.19299
- Source URL: https://arxiv.org/abs/2407.19299
- Authors: Thanh-Dung Le; Ti Ti Nguyen; Vu Nguyen Ha; Symeon Chatzinotas; Philippe Jouvet; Rita Noumeir
- Reference count: 40
- LoRA adapters provide no significant improvements when fine-tuning biomedical LLMs under limited resources; simpler transformer-based models outperform adapter-augmented LLMs in efficiency and training time.

## Executive Summary
This study evaluates Low-Rank Adaptation (LoRA) techniques for fine-tuning Large Language Models in clinical text classification under computational and data constraints. The research compares four adapter structures—Adapter, Lightweight, TinyAttention, and Gated Residual Network (GRN)—with biomedical pre-trained models and simpler Transformer-based models trained from scratch. Results show that adapters provide no significant improvements when fine-tuning biomedical LLMs under limited resources, and simpler Transformer-based models outperform adapter-augmented LLMs in terms of efficiency and training time. Among the adapters, GRN achieved the best metrics, with accuracy, precision, recall, and F1 score of 0.88.

## Method Summary
The study fine-tunes four adapter techniques (Adapter, Lightweight, TinyAttention, GRN) on three French biomedical pre-trained models (CamemBERT-bio, AliBERT, DrBERT) and two Transformer-based models trained from scratch. The research uses a clinical notes dataset from CHU Sainte-Justine containing 580,000 unigrams across 5,444 cases (1,941 positive, 3,503 negative). Experiments compare full fine-tuning versus freezing pre-trained weights, conducted on a single NVIDIA Quadro P620 GPU with data split into 70% training and 30% testing sets. Performance is evaluated using accuracy, precision, recall, and F1 score.

## Key Results
- LoRA adapters provide no significant improvements when fine-tuning biomedical LLMs under limited resources
- Simpler Transformer-based models outperform adapter-augmented LLMs in terms of efficiency and training time
- GRN adapter achieved the best metrics with accuracy, precision, recall, and F1 score of 0.88
- Total training time for LLMs exceeded 1000 hours, compared to under 6 hours for simpler transformer-based models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA adapters reduce fine-tuning complexity by replacing full weight updates with low-rank decomposition matrices, preserving most of the original LLM weights.
- Mechanism: LoRA introduces trainable rank decomposition matrices into each transformer layer while freezing pre-trained model weights, significantly reducing the number of trainable parameters needed for adaptation.
- Core assumption: The original LLM weights contain sufficient domain-general knowledge that can be adapted with minimal parameter changes for specific tasks.
- Evidence anchors:
  - [abstract]: "LoRA freezes the pre-trained model weights and introduces trainable rank decomposition matrices into each layer of the Transformer architecture."
  - [section]: "Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by three times."
  - [corpus]: Weak - no direct evidence in corpus, but general adapter literature supports this claim.
- Break condition: When the domain gap is too large or the dataset is too small, the frozen weights cannot be effectively adapted with minimal parameter changes.

### Mechanism 2
- Claim: Adapter modules provide efficient fine-tuning by adding small, trainable neural network modules at each transformer layer, maintaining the original model structure.
- Mechanism: Adapter modules consist of down-projection and up-projection layers with bottleneck structure, allowing efficient parameter updates while preserving overall model performance.
- Core assumption: Adding small modules at each layer can capture task-specific information without disrupting the pre-trained knowledge.
- Evidence anchors:
  - [abstract]: "Adapter modules [24], [25] represent a form of LoRA efficient tuning, integrating small, newly initialized parameter modules at each transformer layer of pre-trained LLMs."
  - [section]: "The adapter structure includes (1) a down-projection layer with weights Wdown ∈ Rd×r that reduces the input hi to a lower-dimensional space defined by the bottleneck dimension r; and (2) an up-projection layer with weights Wup ∈ Rr×d that projects the reduced input back to its original size."
  - [corpus]: Weak - no direct evidence in corpus, but general adapter literature supports this claim.
- Break condition: When the task requires substantial changes to the model's internal representations that cannot be captured by small adapter modules.

### Mechanism 3
- Claim: GRN adapter's gating mechanism enhances learning capabilities by dynamically controlling information flow through element-wise multiplication of linear and gated outputs.
- Mechanism: GRN uses Gated Linear Units (GLU) that combine linear transformation with sigmoid-gated transformation, allowing the network to selectively pass or block information based on learned patterns.
- Core assumption: The gating mechanism can effectively identify and preserve relevant information while filtering out noise, especially important in clinical text classification.
- Evidence anchors:
  - [abstract]: "The Gated Residual Network (GRN) adapter structure includes a series of dense layers... before being gated by a gated linear unit (GLU)."
  - [section]: "The gated linear unit is central to the GRN's function, which applies the gating mechanism to the residual connections."
  - [corpus]: Weak - no direct evidence in corpus, but general GRN literature supports this claim.
- Break condition: When the gating mechanism fails to learn meaningful patterns due to limited training data or when the clinical text requires global context rather than local gating decisions.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: Understanding how transformers process sequential data and how adapters integrate with the architecture is crucial for implementing and troubleshooting the fine-tuning approach.
  - Quick check question: How does multi-head attention in transformers allow the model to focus on different aspects of the input sequence simultaneously?

- Concept: Parameter-efficient fine-tuning techniques
  - Why needed here: LoRA and adapter methods are specifically designed to reduce computational requirements while maintaining performance, which is critical given the resource constraints mentioned in the study.
  - Quick check question: What is the key difference between full fine-tuning and parameter-efficient fine-tuning in terms of which model parameters are updated during training?

- Concept: Clinical text classification challenges
  - Why needed here: Understanding the specific difficulties of clinical NLP tasks, such as domain-specific terminology, data privacy constraints, and limited labeled data availability, helps explain why certain approaches succeed or fail.
  - Quick check question: What are the main differences between clinical text classification and general text classification that make it particularly challenging for LLM adaptation?

## Architecture Onboarding

- Component map:
  Pre-trained biomedical LLMs (CamemBERT-bio, AliBERT, DrBERT) -> Adapter modules (Adapter, Lightweight, TinyAttention, GRN) -> Classification head -> Evaluation metrics

- Critical path:
  1. Load pre-trained biomedical LLM and freeze weights
  2. Add adapter modules to transformer layers
  3. Initialize classification head
  4. Load and preprocess clinical text data
  5. Train only adapter modules and classification head
  6. Evaluate performance using accuracy, precision, recall, and F1 score

- Design tradeoffs:
  - Adapter size vs. performance: Larger adapters may capture more task-specific information but increase computational requirements
  - Training time vs. model complexity: Simpler transformers may train faster but potentially achieve lower performance
  - Data efficiency vs. model size: Larger pre-trained models may require more data to avoid overfitting when using adapters

- Failure signatures:
  - Significant performance degradation when freezing pre-trained weights (Setup 2 vs Setup 1)
  - Long training times for biomedical LLMs despite adapter use (30-50 hours vs under 6 hours for simpler models)
  - No significant improvement over training simpler transformers from scratch

- First 3 experiments:
  1. Compare performance of GRN adapter on biomedical pre-trained LLM vs simpler transformer trained from scratch
  2. Test different adapter configurations (Adapter, Lightweight, TinyAttention) on the same base model
  3. Evaluate impact of freezing vs fine-tuning pre-trained weights on adapter performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific data and computational constraints do adapter-augmented LLMs become more advantageous than simpler Transformer-based models trained from scratch?
- Basis in paper: [explicit] The paper shows that adapters provide no significant improvements under limited resources and suggests simpler models are more practical.
- Why unresolved: The paper only tests one specific resource-constrained scenario (2GB VRAM, 580k tokens) without exploring other data/compute regimes.
- What evidence would resolve it: Systematic experiments varying data size, GPU memory, and training time to identify thresholds where adapters become beneficial.

### Open Question 2
- Question: How do different adapter architectures (Adapter, Lightweight, TinyAttention, GRN) compare in performance and efficiency when applied to larger biomedical LLMs beyond the three tested models?
- Basis in paper: [explicit] The study only tested CamemBERT-bio, AliBERT, and DrBERT with four adapter types.
- Why unresolved: Limited model diversity means results may not generalize to other biomedical LLMs or larger models.
- What evidence would resolve it: Testing adapters across a broader range of biomedical LLMs with varying sizes and architectures.

### Open Question 3
- Question: What is the impact of adapter-based fine-tuning on model generalization to unseen clinical tasks beyond cardiac failure classification?
- Basis in paper: [inferred] The study focuses solely on cardiac failure detection from clinical narratives.
- Why unresolved: Results may be task-specific and not indicative of adapter performance on other clinical NLP applications.
- What evidence would resolve it: Evaluating adapter-augmented models across multiple clinical NLP tasks (e.g., diagnosis prediction, medication extraction, adverse event detection).

## Limitations
- Limited dataset size (5,444 cases) may not represent real-world clinical NLP requirements
- Single institution source may limit generalizability of findings
- Computational constraint of using only a NVIDIA Quadro P620 GPU may not represent all resource scenarios
- Only tested French biomedical pre-trained models, limiting cross-language applicability

## Confidence
- High confidence: Relative efficiency comparison between adapter-augmented LLMs and simpler Transformer-based models
- Medium confidence: Claim that adapters provide no significant improvements under limited resources
- Low confidence: Generalizability to other clinical domains or languages

## Next Checks
1. Replicate experiments with larger clinical datasets (minimum 10,000+ cases) across multiple institutions to assess scalability and generalization
2. Test the same adapter configurations on English biomedical pre-trained models (e.g., BioBERT, ClinicalBERT) to determine if language affects adapter effectiveness
3. Evaluate performance on different GPU configurations, including multi-GPU setups, to understand if computational constraints are the primary limiting factor