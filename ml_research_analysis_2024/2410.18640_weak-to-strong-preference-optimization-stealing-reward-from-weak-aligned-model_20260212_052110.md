---
ver: rpa2
title: 'Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned
  Model'
arxiv_id: '2410.18640'
source_url: https://arxiv.org/abs/2410.18640
tags:
- alignment
- weak
- wspo
- strong
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Weak-to-Strong Preference Optimization (WSPO) transfers alignment\
  \ capability from a weaker to a stronger language model by learning the distributional\
  \ differences before and after the weak model\u2019s alignment. The method is based\
  \ on the insight that a weak model\u2019s alignment behavior can be amplified when\
  \ transferred to a stronger model."
---

# Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model

## Quick Facts
- arXiv ID: 2410.18640
- Source URL: https://arxiv.org/abs/2410.18640
- Reference count: 40
- Key outcome: WSPO improves Qwen2-7B-Instruct win rate on Arena-Hard from 39.70% to 49.60%

## Executive Summary
Weak-to-Strong Preference Optimization (WSPO) transfers alignment capability from a weaker to a stronger language model by learning the distributional differences before and after the weak model's alignment. The method is based on the insight that a weak model's alignment behavior can be amplified when transferred to a stronger model. Experiments show that WSPO improves the win rate of Qwen2-7B-Instruct on Arena-Hard from 39.70% to 49.60%, achieves a length-controlled win rate of 47.04% on AlpacaEval 2, and obtains a score of 7.33 on MT-bench, outperforming baselines like DPO and PPO. The method demonstrates that strong alignment can be effectively learned from weak models without requiring preference dataset training, reducing risks like overfitting and reward hacking.

## Method Summary
WSPO transfers alignment from a weak model to a strong model by learning the distributional differences between the weak model's pre-aligned and post-aligned states. The method computes probability ratios between these two states and uses them as reward signals to guide the strong model's optimization through a KL-constrained objective. This approach avoids direct preference dataset training by treating the weak model's alignment process as implicit labels, with a hyperparameter γ controlling the alignment strength.

## Key Results
- Qwen2-7B-Instruct win rate on Arena-Hard improves from 39.70% to 49.60%
- Length-controlled win rate of 47.04% on AlpacaEval 2
- Score of 7.33 on MT-bench, outperforming DPO and PPO baselines
- Demonstrates alignment transfer without preference dataset training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weak models encode alignment preferences that can be extracted as distributional differences and transferred to stronger models.
- Mechanism: WSPO learns the probability ratio between a weak model's aligned and pre-aligned states, using this as a reward signal to guide the strong model's optimization.
- Core assumption: The distributional shift in the weak model captures alignment preferences that can generalize beyond the weak model's capacity.
- Evidence anchors:
  - [abstract] "WSPO transfers alignment capability from a weaker to a stronger language model by learning the distributional differences before and after the weak model's alignment."
  - [section 3.2] "we establish a relationship between the weak model (serving as a reward model) and the strong model in the context of RL optimization. By learning the differences before and after the alignment of the weak model, we can effectively enhance the alignment ability of the stronger model."
- Break condition: If the weak model's alignment is poor or inconsistent, the transferred signal will be unreliable and may degrade strong model performance.

### Mechanism 2
- Claim: Strong models can amplify weak model alignment signals through their larger parameter capacity.
- Mechanism: The stronger model's optimization landscape allows it to better capture and generalize the alignment preferences encoded in the weak model's distributional shift.
- Core assumption: Parameter size and representational capacity enable the strong model to better approximate the alignment preferences than the weak model itself.
- Evidence anchors:
  - [abstract] "Experiments show that WSPO improves the win rate of Qwen2-7B-Instruct on Arena-Hard from 39.70% to 49.60%...demonstrates that strong alignment can be effectively learned from weak models without requiring preference dataset training"
  - [section 4.3.2] "the strong model's alignment capability was amplified from 39.70 to 49.60 by leveraging the differences in alignment signals from the weak model"
- Break condition: If the strong model's optimization becomes unstable or overfits to the weak model's signal, amplification may fail.

### Mechanism 3
- Claim: Learning distributional differences avoids direct preference dataset training, reducing overfitting and reward hacking risks.
- Mechanism: WSPO bypasses the need for paired preference data by using the weak model's pre/post alignment states as implicit labels.
- Core assumption: The weak model's alignment process captures generalizable preferences without requiring explicit human-labeled comparisons.
- Evidence anchors:
  - [abstract] "The method demonstrates that strong alignment can be effectively learned from weak models without requiring preference dataset training, reducing risks like overfitting and reward hacking."
  - [section 4.3.2] "our method circumvents direct training on the preference dataset, effectively reducing risks such as overfitting and reward hacking."
- Break condition: If the weak model's alignment process introduces its own biases or noise, these may propagate to the strong model.

## Foundational Learning

- Concept: Weak-to-strong generalization
  - Why needed here: WSPO builds on the observation that strong models can generalize beyond weak supervision, applying this to alignment rather than just label transfer
  - Quick check question: Why would a strong model be able to improve upon weak supervision rather than just copying it?

- Concept: Distributional preference modeling
  - Why needed here: WSPO relies on capturing alignment preferences through probability distributions rather than explicit reward functions
  - Quick check question: How does learning distributional differences differ from learning explicit reward values?

- Concept: Reinforcement learning with KL regularization
  - Why needed here: The WSPO objective is derived from KL-constrained reward maximization, understanding this helps explain the hyperparameter γ
  - Quick check question: What role does the KL divergence term play in preventing the strong model from diverging too far from its reference distribution?

## Architecture Onboarding

- Component map: Weak model (pre-aligned and post-aligned versions) -> WSPO loss function (compares probability ratios) -> Strong model (target for alignment)
- Critical path: Weak model alignment → Compute distributional differences → Use as labels for strong model training → Evaluate alignment performance
- Design tradeoffs:
  - Using weak models as reward proxies vs. training explicit reward models
  - Tradeoff between alignment strength (γ parameter) and preserving original capabilities
  - Computational cost of loading two weak models vs. traditional reward modeling
- Failure signatures:
  - Strong model performance degrades when weak model alignment is poor
  - Instability during training when γ is set too high or too low
  - Limited improvement if weak model alignment is already near-optimal
- First 3 experiments:
  1. Verify weak model alignment improves over base model on simple length-controlled tasks
  2. Test WSPO with varying γ values to find optimal alignment strength
  3. Compare WSPO performance against DPO on the same preference dataset to measure overfitting reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does WSPO perform when aligning models with significantly different architectures (e.g., transformer vs. other architectures)?
- Basis in paper: [inferred] The paper discusses alignment transfer between models of different sizes but does not explore architectural differences.
- Why unresolved: The study focuses on parameter size differences within the same architecture family (Qwen2), leaving cross-architecture alignment unexplored.
- What evidence would resolve it: Experiments comparing WSPO alignment performance across different model architectures (e.g., transformer vs. convolutional) using the same alignment signals.

### Open Question 2
- Question: What is the theoretical explanation for why transferring alignment from weak to strong models results in amplified alignment effects?
- Basis in paper: [explicit] The paper observes amplification but states "Our study also does not explain why transferring a weak model's alignment ability to a stronger model amplifies it."
- Why unresolved: The paper provides empirical evidence of amplification but lacks a theoretical framework explaining this phenomenon.
- What evidence would resolve it: Mathematical analysis or theoretical framework demonstrating why alignment signals from smaller models become amplified when transferred to larger models.

### Open Question 3
- Question: How does the quality of the weak model's alignment affect the effectiveness of WSPO alignment transfer?
- Basis in paper: [inferred] The paper uses DPO-aligned weak models but doesn't systematically vary the quality or method of weak model alignment.
- Why unresolved: The study assumes the weak model has been aligned using standard methods (DPO/PPO) without exploring how different alignment qualities impact transfer effectiveness.
- What evidence would resolve it: Experiments comparing WSPO performance when using weak models aligned with varying quality methods or hyperparameters, measuring the resulting strong model alignment strength.

### Open Question 4
- Question: Can WSPO be effectively applied to vision-language models and other multi-modal architectures beyond pure language models?
- Basis in paper: [explicit] The paper includes a brief experiment with vision-language models (Section E) but doesn't comprehensively evaluate this application.
- Why unresolved: The vision-language experiment is minimal and doesn't explore the full potential or limitations of WSPO in multi-modal contexts.
- What evidence would resolve it: Systematic evaluation of WSPO across various multi-modal tasks and architectures, comparing performance to traditional alignment methods in those domains.

## Limitations

- Limited evidence that distributional differences generalize beyond weak model capacity
- No theoretical explanation for why amplification occurs in strong models
- Unclear comparative analysis of overfitting risks versus traditional methods

## Confidence

- Medium confidence in the overall empirical results (win rate improvements are well-documented)
- Low confidence in the theoretical explanation of why distributional differences generalize
- Medium confidence in the practical utility of the method given the empirical results

## Next Checks

1. **Robustness to Weak Model Quality**: Test WSPO with weak models of varying alignment quality (poorly aligned, moderately aligned, well-aligned) to determine if the method degrades gracefully or catastrophically when the weak model's alignment is suboptimal.

2. **Generalization Beyond Distributional Differences**: Conduct ablation studies comparing WSPO against methods that use the weak model as a direct reward signal (rather than learning distributional differences) to isolate whether the distributional learning approach provides unique benefits.

3. **Cross-Architecture Transfer**: Evaluate whether WSPO works when the weak and strong models have different architectures (e.g., weak: transformer-base, strong: transformer-large) to test if the alignment signal transfers across architectural boundaries, not just scale differences.