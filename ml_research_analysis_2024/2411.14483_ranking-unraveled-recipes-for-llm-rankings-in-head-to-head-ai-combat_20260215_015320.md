---
ver: rpa2
title: 'Ranking Unraveled: Recipes for LLM Rankings in Head-to-Head AI Combat'
arxiv_id: '2411.14483'
source_url: https://arxiv.org/abs/2411.14483
tags:
- uni00000014
- uni00000012
- uni0000001c
- ranking
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates four ranking algorithms (Elo, Bradley-Terry,
  Glicko, Markov Chain) for pairwise LLM comparisons. It finds that Bradley-Terry
  works best for small, evenly distributed datasets while Glicko is superior for large,
  unevenly distributed datasets.
---

# Ranking Unraveled: Recipes for LLM Rankings in Head-to-Head AI Combat

## Quick Facts
- **arXiv ID:** 2411.14483
- **Source URL:** https://arxiv.org/abs/2411.14483
- **Reference count:** 7
- **Key outcome:** Bradley-Terry works best for small, evenly distributed datasets while Glicko is superior for large, unevenly distributed datasets; Elo is highly sensitive to hyperparameters and permutations

## Executive Summary
This paper systematically evaluates four ranking algorithms (Elo, Bradley-Terry, Glicko, Markov Chain) for pairwise comparisons of large language models. Through controlled experiments on two datasets with different characteristics, the study identifies which algorithms perform best under various conditions. The research provides practical guidelines for selecting appropriate ranking methods based on dataset size and distribution, addressing a critical need in the rapidly expanding LLM evaluation landscape.

## Method Summary
The authors compare four ranking algorithms using two datasets: Chatbot Arena (57 models, 244,978 matchups, uneven distribution) and SLAM (11 models, 2,858 matchups, controlled distribution). They evaluate transitivity preservation by counting transitive triples, assess prediction accuracy through 75/25 train/test splits using F1 scores, and test hyperparameter sensitivity across ranges. The methodology systematically measures how well each algorithm maintains ranking consistency and accuracy under different dataset conditions.

## Key Results
- Bradley-Terry achieves the highest transitivity preservation in small, evenly distributed datasets due to its global optimization approach
- Glicko outperforms other methods in large, unevenly distributed datasets by incorporating rating deviation to account for uncertainty
- Elo shows high sensitivity to k-factor values and permutations, producing inconsistent rankings even with >1,000 permutations
- Markov Chain struggles with sparse data and shows lower prediction accuracy compared to other methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bradley-Terry achieves higher transitivity than Elo in controlled datasets because it uses maximum likelihood estimation over all pairwise results simultaneously, rather than updating sequentially.
- **Mechanism:** By solving the likelihood function for all models concurrently, Bradley-Terry avoids the ordering sensitivity that plagues sequential methods like Elo. The model's strength estimates are globally consistent rather than dependent on match sequence.
- **Core assumption:** Pairwise comparisons can be modeled as independent observations following a logistic distribution of strength differences.
- **Evidence anchors:**
  - [abstract] "Bradley-Terry works best for small, evenly distributed datasets"
  - [section 2.3] "Bradley-Terry uses Maximum Likelihood Estimation to calculate each player's strength based on the outcome of all their matches concurrently"
  - [corpus] "Bradley-Terry model... is widely used in sports rankings" - supports Bradley-Terry's established use for pairwise comparisons
- **Break condition:** If pairwise comparisons violate independence (e.g., contextual dependencies between matches), the global optimization assumption fails.

### Mechanism 2
- **Claim:** Glicko outperforms other methods in large, unevenly distributed datasets by incorporating rating deviation to account for uncertainty in model strength estimates.
- **Mechanism:** Glicko adjusts rankings based on both match outcomes and the confidence in those outcomes (rating deviation). Models with fewer matchups have higher deviation, leading to more conservative adjustments and preventing unreliable models from being ranked too high.
- **Core assumption:** The uncertainty in a model's ranking should be inversely related to the number of comparisons it has participated in.
- **Evidence anchors:**
  - [abstract] "Glicko is superior for large, unevenly distributed datasets"
  - [section 2.4] "Glicko extends Elo by introducing a secondary parameter (σ) known as the rating deviation... decreasing based on the number of games played"
  - [corpus] "Glicko system provides a superior solution... utilizes a rating deviation parameter to account for uncertainty in a model's ranking due to limited matchups"
- **Break condition:** If the relationship between matchup count and ranking confidence is non-linear or if models have heterogeneous variance in performance.

### Mechanism 3
- **Claim:** Elo's sensitivity to hyperparameters (k-factor) and permutations makes it unreliable for LLM ranking without extensive tuning.
- **Mechanism:** Elo updates ratings incrementally after each match, with the k-factor controlling how much each result affects the rating. This sequential updating makes Elo highly sensitive to both the order of matches and the k-factor setting, leading to inconsistent rankings across different permutations.
- **Core assumption:** The optimal k-factor is stable across different datasets and evaluation scenarios.
- **Evidence anchors:**
  - [abstract] "Elo is highly sensitive to hyperparameters and permutations, producing inconsistent rankings"
  - [section 4.4.1] "Elo shows the largest variation across both datasets, and is less stable, particularly for the smaller SLAM dataset"
  - [corpus] "variations of a Bradley-Terry model" - suggests Bradley-Terry variants may be more stable than Elo
- **Break condition:** If the optimal k-factor varies significantly across different LLM evaluation contexts or if computational constraints prevent testing multiple k-values.

## Foundational Learning

- **Concept:** Maximum Likelihood Estimation for pairwise comparison models
  - **Why needed here:** Bradley-Terry and similar models estimate model strengths by maximizing the likelihood of observed pairwise outcomes. Understanding this is essential for interpreting why Bradley-Terry performs well in controlled datasets.
  - **Quick check question:** If model A beats model B 70% of the time and model B beats model C 60% of the time, what additional information does MLE need to determine if A should be ranked above C?

- **Concept:** Rating deviation and uncertainty quantification in ranking systems
  - **Why needed here:** Glicko's rating deviation parameter is central to its performance advantage in uneven datasets. Understanding how rating deviation works is crucial for implementing and tuning Glicko correctly.
  - **Quick check question:** If two models have the same win rate but one has played 100 matches while the other has played 10, how should their Glicko ratings differ and why?

- **Concept:** Hyperparameter sensitivity and stability analysis
  - **Why needed here:** The paper emphasizes that Elo's performance varies significantly with k-factor and permutations. Understanding how to measure and characterize sensitivity is essential for selecting appropriate ranking algorithms.
  - **Quick check question:** If changing a hyperparameter causes a ranking algorithm's F1 score to vary by more than 10%, what does this indicate about the algorithm's stability?

## Architecture Onboarding

- **Component map:** Data preprocessing -> Ranking algorithm (Elo/Bradley-Terry/Glicko/Markov Chain) -> Evaluation metrics (transitivity, F1, sensitivity) -> Results analysis
- **Critical path:** Data → Preprocessing → Ranking Algorithm → Evaluation Metrics → Results Analysis. The ranking algorithm implementation is the most critical component as it directly determines the output quality.
- **Design tradeoffs:** Bradley-Terry offers simplicity and good performance in controlled datasets but struggles with rare events. Glicko handles uncertainty well but is more complex to implement. Elo is intuitive but highly sensitive to hyperparameters. Markov Chain is simple but struggles with sparse data.
- **Failure signatures:** Elo producing dramatically different rankings with small k-factor changes, Bradley-Terry over-ranking new models with few but successful matchups, Glicko failing to converge when rating deviations remain high, Markov Chain producing rankings that correlate poorly with win rates in sparse datasets.
- **First 3 experiments:**
  1. Implement Bradley-Terry on a small synthetic dataset with balanced matchups to verify transitivity preservation and compare with win-rate rankings.
  2. Test Elo with varying k-factors (1-100) on a medium-sized dataset to quantify sensitivity and identify optimal k-values.
  3. Compare Glicko and Bradley-Terry on a large, uneven dataset to observe how rating deviation affects rankings of models with different matchup counts.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Elo rankings be made more stable across different k-factor values without requiring extensive permutations?
- **Basis in paper:** [explicit] The paper demonstrates Elo's high sensitivity to k-factor and permutations, producing inconsistent rankings even with >1,000 permutations.
- **Why unresolved:** Current methods rely heavily on permutations for stability, which is computationally expensive and may not guarantee consistent results across k-factor variations.
- **What evidence would resolve it:** Experimental comparison of Elo stability across k-factors using alternative stabilization techniques (e.g., adaptive k-factors, ensemble methods) with minimal permutation requirements.

### Open Question 2
- **Question:** What is the optimal approach for handling the "rare events" problem in Bradley-Terry models when evaluating new LLMs with limited matchups?
- **Basis in paper:** [explicit] The paper identifies that Bradley-Terry struggles with new models that have high win rates but few matchups, leading to biased rankings.
- **Why unresolved:** While weighted logistic regression was tested, it showed negligible impact, suggesting the need for alternative solutions to this fundamental limitation.
- **What evidence would resolve it:** Comparative analysis of modified Bradley-Terry implementations (e.g., Bayesian approaches, regularization techniques) against actual LLM evaluation data with varying matchup distributions.

### Open Question 3
- **Question:** How can pairwise ranking systems scale efficiently as the number of LLMs grows exponentially?
- **Basis in paper:** [inferred] The paper's methodology requires O(n²) comparisons for n models, and scalability constraints are mentioned as a limitation.
- **Why unresolved:** Current pairwise evaluation approaches become computationally prohibitive as the model ecosystem expands, but efficient alternatives have not been systematically explored.
- **What evidence would resolve it:** Empirical validation of sampling strategies, active learning approaches, or hierarchical ranking methods that maintain ranking accuracy while reducing comparison requirements.

## Limitations
- Dataset representativeness: Findings based on two specific datasets may not generalize to all LLM evaluation scenarios
- Algorithm implementation details: Full specification of tie-handling, sparse data interpolation, and convergence criteria not provided
- Evaluation metric completeness: Computational efficiency, robustness to noise, and interpretability not discussed

## Confidence
- **High Confidence:** Bradley-Terry's superior performance in small, evenly distributed datasets is well-supported by the MLE mechanism and controlled experiments
- **Medium Confidence:** Glicko's advantage in large, uneven datasets is demonstrated but could vary with different implementation choices for rating deviation updates
- **Low Confidence:** Elo's hyperparameter sensitivity claims, while observed, may be dataset-specific and require broader validation across different LLM evaluation contexts

## Next Checks
1. Test the four algorithms on a third, independently collected pairwise comparison dataset with different characteristics (e.g., more models, different evaluation criteria) to verify generalization
2. Conduct ablation studies removing specific components of each algorithm (e.g., Glicko without rating deviation, Bradley-Terry without global optimization) to isolate which features drive performance differences
3. Evaluate the algorithms' performance when introducing controlled noise into the pairwise comparisons to assess robustness to evaluation inconsistencies