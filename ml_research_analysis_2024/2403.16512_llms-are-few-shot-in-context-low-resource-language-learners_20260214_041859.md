---
ver: rpa2
title: LLMs Are Few-Shot In-Context Low-Resource Language Learners
arxiv_id: '2403.16512'
source_url: https://arxiv.org/abs/2403.16512
tags:
- languages
- alignment
- label
- language
- x-icl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines in-context learning (ICL) for low-resource languages,
  identifying that label alignment in cross-lingual ICL (X-ICL) often degrades performance.
  The authors propose query alignment as a more effective alternative, which improves
  understanding by aligning the semantics of input examples.
---

# LLMs Are Few-Shot In-Context Low-Resource Language Learners

## Quick Facts
- arXiv ID: 2403.16512
- Source URL: https://arxiv.org/abs/2403.16512
- Authors: Samuel Cahyawijaya; Holy Lovenia; Pascale Fung
- Reference count: 40
- Primary result: Query alignment outperforms label alignment in cross-lingual ICL for low-resource languages

## Executive Summary
This work examines in-context learning (ICL) for low-resource languages, identifying that label alignment in cross-lingual ICL (X-ICL) often degrades performance. The authors propose query alignment as a more effective alternative, which improves understanding by aligning the semantics of input examples. They also analyze variations in prompt formatting and exemplar retrieval methods. Results show that uniform source labels outperform label alignment, and cross-lingual semantic similarity improves exemplar selection. Overall, few-shot ICL enhances low-resource language understanding by bridging semantic and linguistic gaps. Code is publicly available.

## Method Summary
The authors conduct three-shot in-context learning experiments using XGLM-7.5B and BLOOM-7B models across 25 low-resource and 7 higher-resource languages. They test four datasets (MasakhaNews, NusaTranslation, AmericasNLI, TweetSentimentMultilingual) with different alignment methods (label alignment vs. query alignment), prompt formatting variations, and exemplar retrieval strategies using cross-lingual semantic similarity models. The study compares zero-shot, monolingual ICL, X-ICL with various configurations, and machine translation baselines using weighted F1 score as the primary metric.

## Key Results
- Query alignment significantly outperforms label alignment in cross-lingual ICL, with improvements of 3.1-12.4 F1 points across datasets
- Uniform source labels consistently outperform label alignment across all languages and datasets
- Cross-lingual semantic similarity improves exemplar retrieval quality, enhancing overall ICL performance
- Prompt formatting consistency benefits high-resource languages more than low-resource ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Label alignment in cross-lingual ICL often degrades performance because it shifts the semantic space of labels from source to target language without guiding the model on how to interpret the target-language query.
- Mechanism: The model understands label semantics in the target language but lacks alignment between the query in the target language and the exemplars in the source language, leading to degraded performance.
- Core assumption: Label alignment text alone is insufficient for bridging the semantic gap between the query and exemplars.
- Evidence anchors:
  - [abstract]: "identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment."
  - [section]: "contrary to the results reported in (Tanwar et al., 2023), we found that in-context label alignment does not work for all the languages under study and introduced an alternative alignment method namely in-context query alignment, which significantly improves the alignment quality compared to the in-context label alignment."
- Break condition: If the label alignment text provides explicit guidance on interpreting the query in the target language, the mechanism might break.

### Mechanism 2
- Claim: Query alignment improves cross-lingual ICL by providing alignment of input distribution, allowing the model to understand the query in the target language as well as in the source language.
- Mechanism: The model aligns the query in the source language to the target language, enabling better understanding of the query in both languages, leading to performance similar to monolingual ICL.
- Core assumption: Aligning the query in the source language to the target language is more effective than aligning the labels.
- Evidence anchors:
  - [abstract]: "introduces a more effective alternative: query alignment."
  - [section]: "We introduce another approach, dubbed in-context query alignment, which provides alignment of input distribution by providing the translation of sentences similar to the query while keeping the label set as is."
- Break condition: If the query alignment text does not effectively align the query in the source language to the target language, the mechanism might break.

### Mechanism 3
- Claim: Cross-lingual semantic similarity improves exemplar retrieval quality, leading to better cross-lingual ICL performance.
- Mechanism: The model retrieves exemplars that are semantically similar to the query across languages, improving the quality of in-context learning.
- Core assumption: Cross-lingual semantic similarity models can effectively retrieve semantically relevant exemplars across languages.
- Evidence anchors:
  - [abstract]: "Results show that uniform source labels outperform label alignment, and cross-lingual semantic similarity improves exemplar selection."
  - [section]: "Another way to improve X-ICL performance is by improving the exemplar retrieval quality... We explore translation semantic similarity as an alternative."
- Break condition: If the cross-lingual semantic similarity model cannot effectively retrieve semantically relevant exemplars, the mechanism might break.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL is the foundation for understanding how LLMs can perform tasks using short in-context information without parameter updates.
  - Quick check question: Can you explain how ICL differs from traditional fine-tuning methods?

- Concept: Cross-lingual in-context learning (X-ICL)
  - Why needed here: X-ICL extends ICL to cross-lingual tasks, allowing LLMs to transfer task understanding from a high-resource source language to a low-resource target language.
  - Quick check question: What are the key challenges in implementing X-ICL for low-resource languages?

- Concept: Semantic similarity
  - Why needed here: Semantic similarity is crucial for exemplar retrieval and alignment, ensuring that the model retrieves and aligns semantically relevant information across languages.
  - Quick check question: How does cross-lingual semantic similarity differ from monolingual semantic similarity?

## Architecture Onboarding

- Component map:
  - Cross-lingual alignment (label alignment vs. query alignment)
  - Cross-lingual prompting (alignment formatting and label configuration)
  - Cross-lingual retrieval (semantic similarity models)

- Critical path:
  1. Retrieve semantically similar exemplars using cross-lingual semantic similarity.
  2. Apply in-context alignment (label or query alignment) to bridge the semantic gap.
  3. Format the prompt for consistency and clarity.
  4. Execute the ICL task and evaluate performance.

- Design tradeoffs:
  - Label alignment vs. query alignment: Label alignment may shift the semantic space without guiding the query, while query alignment provides better input distribution alignment but may require more parallel data.
  - Alignment formatting: Higher consistency in formatting may improve performance for high-resource languages but may not benefit low-resource languages as much.

- Failure signatures:
  - Degraded performance due to ineffective label alignment.
  - Poor exemplar retrieval due to low cross-lingual semantic similarity model performance.
  - Ineffective query alignment due to lack of parallel data.

- First 3 experiments:
  1. Compare label alignment vs. query alignment on a small set of low-resource languages to evaluate performance differences.
  2. Test different cross-lingual semantic similarity models (e.g., XLMR STS, LaBSE) for exemplar retrieval.
  3. Evaluate the impact of prompt formatting consistency on ICL performance for both high-resource and low-resource languages.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of cross-lingual in-context learning (X-ICL) scale with increasing model size, particularly for low-resource languages?
- Basis in paper: [inferred] The paper suggests that larger multilingual LLMs might exhibit different scaling behaviors for low-resource languages, but this was not directly tested due to computational constraints.
- Why unresolved: The authors were limited by computational resources and could only experiment with smaller models (XGLM-7.5B and BLOOM-7B).
- What evidence would resolve it: Experimental results comparing X-ICL performance across a range of model sizes, from small to very large multilingual LLMs, on a diverse set of low-resource languages.

### Open Question 2
- Question: What is the impact of using parallel corpora from culturally relevant sources versus general parallel corpora on the effectiveness of in-context query alignment for low-resource languages?
- Basis in paper: [explicit] The paper mentions the potential benefit of using culturally relevant parallel corpora like Bloom Library, but does not empirically test this hypothesis.
- Why unresolved: The authors did not have access to or did not experiment with culturally specific parallel corpora.
- What evidence would resolve it: Comparative experiments using both general and culturally specific parallel corpora for in-context query alignment, measuring downstream task performance on low-resource languages.

### Open Question 3
- Question: How does the choice of cross-lingual semantic similarity model affect the performance of X-ICL across different language families and resource levels?
- Basis in paper: [explicit] The paper compares different cross-lingual semantic similarity models and finds that their effectiveness varies depending on the language, but does not provide a comprehensive analysis across language families.
- Why unresolved: The analysis was limited to a subset of languages and did not systematically explore the relationship between model choice and language family or resource level.
- What evidence would resolve it: A systematic study comparing the performance of different cross-lingual semantic similarity models across multiple language families and resource levels, identifying which models work best for specific language groups.

## Limitations
- Limited empirical evidence for proposed mechanisms beyond performance comparisons
- Cross-lingual semantic similarity models not thoroughly evaluated across all language pairs
- Prompt formatting effects are dataset-dependent with unclear underlying reasons
- Three-shot examples may not be optimal for all languages or tasks

## Confidence

**High Confidence**: The experimental results showing that uniform source labels outperform label alignment in cross-lingual ICL. This claim is directly supported by quantitative performance metrics across multiple datasets and languages.

**Medium Confidence**: The effectiveness of query alignment as an alternative to label alignment. While performance improvements are demonstrated, the mechanistic explanation for why query alignment works better is not fully developed.

**Medium Confidence**: The benefit of cross-lingual semantic similarity for exemplar retrieval. The paper shows improvements but does not establish that these improvements are consistent across all language pairs or that alternative retrieval methods might perform better.

## Next Checks

1. **Mechanistic Validation**: Conduct ablation studies to isolate the specific contributions of label alignment, query alignment, and exemplar retrieval quality. Test whether the performance differences persist when controlling for semantic similarity between examples and queries.

2. **Cross-Lingual Semantic Similarity Evaluation**: Systematically evaluate the performance of different cross-lingual semantic similarity models (XLMR STS, LaBSE, and others) across all language pairs, measuring their correlation with downstream task performance to identify which models work best for which language families.

3. **Prompt Format Optimization**: Design experiments to determine optimal prompt formatting strategies for low-resource languages specifically, testing whether the lack of benefit from consistent formatting is due to language-specific factors or could be mitigated through alternative formatting approaches.