---
ver: rpa2
title: 'DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the Generalised
  $h$-transform'
arxiv_id: '2406.01781'
source_url: https://arxiv.org/abs/2406.01781
tags:
- deft
- diffusion
- h-transform
- conditional
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents DEFT, a unified framework for conditional\
  \ sampling in diffusion models using Doob\u2019s h-transform. The key idea is to\
  \ fine-tune a small network to learn the conditional h-transform, while keeping\
  \ the large unconditional diffusion model fixed."
---

# DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the Generalised $h$-transform

## Quick Facts
- **arXiv ID**: 2406.01781
- **Source URL**: https://arxiv.org/abs/2406.01781
- **Reference count**: 40
- **Primary result**: DEFT achieves state-of-the-art performance on conditional sampling tasks with up to 1.6× speedup

## Executive Summary
This paper introduces DEFT, a novel framework for conditional sampling in diffusion models that leverages Doob's h-transform. Instead of retraining the entire unconditional diffusion model for each conditional task, DEFT fine-tunes a small network to learn the conditional h-transform while keeping the large unconditional model fixed. This approach enables efficient adaptation of pre-trained diffusion models to various conditional sampling tasks without expensive retraining. The method is evaluated on multiple image reconstruction tasks including inpainting, super-resolution, HDR reconstruction, phase retrieval, and non-linear deblurring, as well as protein motif scaffolding. Results demonstrate state-of-the-art performance with significant speedups and better perceptual quality compared to existing methods.

## Method Summary
DEFT addresses the challenge of conditional sampling in diffusion models by learning the conditional h-transform rather than modifying the unconditional model directly. The framework works by training a small network (NN_φ) to approximate the h-transform function, which when combined with the unconditional diffusion model, produces samples satisfying the desired conditional constraints. The method optimizes a stochastic control objective that captures the conditional distribution, but instead of solving this directly for each task (which would be computationally expensive), DEFT learns to approximate this objective through fine-tuning. The approach maintains the pre-trained unconditional diffusion model as a fixed backbone while only fine-tuning a small network, enabling efficient adaptation to new conditional tasks without the need for expensive retraining.

## Key Results
- Achieves state-of-the-art performance on conditional image reconstruction tasks including inpainting, super-resolution, HDR reconstruction, phase retrieval, and non-linear deblurring
- Demonstrates 1.6× speedup compared to existing methods while maintaining or improving reconstruction quality
- Outperforms existing methods on protein motif scaffolding task while using only 9% of the parameters of an amortised model
- Shows better perceptual quality on natural images and superior reconstruction accuracy on medical images

## Why This Works (Mechanism)
DEFT works by leveraging the mathematical framework of Doob's h-transform, which provides a principled way to condition diffusion processes on future events. The key insight is that instead of modifying the entire unconditional diffusion model for each conditional task, we can learn a function (the h-transform) that, when combined with the unconditional model, produces samples satisfying the desired constraints. By keeping the large unconditional model fixed and only fine-tuning a small network to approximate this h-transform, DEFT achieves efficient adaptation to new conditional tasks. This approach is particularly powerful because it allows leveraging pre-trained unconditional diffusion models even when backpropagation through them is infeasible, and avoids the computational expense of retraining the entire model for each new conditional task.

## Foundational Learning

**Doob's h-transform**: A mathematical tool for conditioning diffusion processes on future events
- *Why needed*: Provides the theoretical foundation for transforming unconditional diffusion into conditional sampling
- *Quick check*: Understand how h-transform modifies drift terms in diffusion processes

**Unconditional diffusion models**: Pre-trained models that generate samples from the data distribution without constraints
- *Why needed*: Serves as the fixed backbone that DEFT builds upon
- *Quick check*: Verify the unconditional model generates realistic samples before applying DEFT

**Stochastic control**: Optimization framework for controlling diffusion processes
- *Why needed*: The theoretical basis for learning the conditional h-transform
- *Quick check*: Understand the relationship between control objectives and conditional distributions

**Conditional sampling**: Generating samples that satisfy specific constraints or conditions
- *Why needed*: The target application of DEFT across various tasks
- *Quick check*: Compare unconditional vs conditional samples qualitatively

## Architecture Onboarding

**Component map**: Unconditional diffusion model (fixed) -> h-transform network (fine-tuned) -> Conditional sampling output

**Critical path**: The critical computational path is the forward pass through both the unconditional diffusion model and the h-transform network during sampling. The backward pass during training only affects the h-transform network parameters.

**Design tradeoffs**: 
- **Fixed unconditional model**: Benefits from pre-training and avoids retraining costs, but limits flexibility if the unconditional distribution doesn't match the target distribution well
- **Small fine-tuned network**: Enables efficient adaptation but may have limited capacity to capture complex conditional relationships
- **Stochastic control objective**: Theoretically sound but computationally expensive; DEFT approximates this to enable practical training

**Failure signatures**: 
- Poor reconstruction quality may indicate insufficient capacity in the h-transform network
- Mode collapse or artifacts could suggest misalignment between the unconditional model and conditional constraints
- Slow convergence during fine-tuning might indicate difficult conditional distributions or inappropriate network architecture

**First experiments**:
1. Verify unconditional diffusion model generates quality samples on a held-out dataset
2. Test h-transform network capacity by gradually increasing its size and monitoring reconstruction quality
3. Validate conditional sampling on a simple task (e.g., inpainting with large masks) before scaling to more complex tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the stochastic control approach scale to high-dimensional data compared to DEFT's fine-tuning method?
- **Basis in paper**: The paper discusses scaling challenges of the stochastic control objective and mentions promising recent work on partial trajectory optimisation.
- **Why unresolved**: The paper provides initial experiments on MNIST but notes the computational burden is still high for high-dimensional data, leaving open questions about practical scalability.
- **What evidence would resolve it**: Comparative experiments showing runtime and memory usage of stochastic control vs. fine-tuning approaches on high-dimensional datasets like ImageNet or medical imaging.

### Open Question 2
- **Question**: What is the impact of different initialization strategies for the h-transform network on final performance across various conditional sampling tasks?
- **Basis in paper**: The paper mentions that they initialize the last layer of NN_φ1 with zeros and NN_φ2 to output 1, but doesn't extensively explore alternative initialization strategies.
- **Why unresolved**: The paper doesn't provide ablation studies on different initialization schemes or their impact on convergence speed and final performance.
- **What evidence would resolve it**: Systematic experiments comparing different initialization strategies (random, pretrained, zero-initialized) across multiple tasks and measuring convergence speed and final performance metrics.

### Open Question 3
- **Question**: How does DEFT perform when applied to non-image domains such as time series or graph data?
- **Basis in paper**: The paper demonstrates DEFT on images and proteins (which have geometric structure), but doesn't explore other data types that might benefit from conditional sampling.
- **Why unresolved**: The paper focuses on image reconstruction and protein design tasks, leaving open questions about generalizability to other data modalities.
- **What evidence would resolve it**: Experiments applying DEFT to time series forecasting with conditional inputs or graph generation tasks with specific constraints.

## Limitations
- Limited evaluation to primarily image-based tasks and one protein scaffolding application, with no testing on other data modalities
- No detailed analysis of computational efficiency when applying DEFT to state-of-the-art diffusion models with hundreds of millions of parameters
- Lack of statistical significance testing across multiple runs or datasets to validate reported improvements

## Confidence

**High**: Theoretical framework using Doob's h-transform is mathematically sound
**Medium**: Empirical results on presented tasks (image reconstruction, protein scaffolding)
**Low**: Claims about computational efficiency and generalization to all diffusion models

## Next Checks

1. Test DEFT on additional data modalities (audio, 3D structures, time-series) to verify true generalizability
2. Conduct ablation studies varying the size and architecture of the fine-tuned network to understand performance trade-offs
3. Perform statistical significance testing across multiple random seeds and datasets to validate reported improvements