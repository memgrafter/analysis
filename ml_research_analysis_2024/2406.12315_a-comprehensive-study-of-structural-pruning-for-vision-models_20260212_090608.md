---
ver: rpa2
title: A Comprehensive Study of Structural Pruning for Vision Models
arxiv_id: '2406.12315'
source_url: https://arxiv.org/abs/2406.12315
tags:
- pruning
- methods
- importance
- pruningbench
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PruningBench, the first comprehensive benchmark
  for structural pruning of vision models, addressing the lack of standardized evaluation
  metrics in this field. PruningBench evaluates 16 existing pruning methods across
  diverse models (CNNs, ViTs) and tasks (classification, detection), providing 13
  leaderboards and over 645 model pruning experiments.
---

# A Comprehensive Study of Structural Pruning for Vision Models

## Quick Facts
- arXiv ID: 2406.12315
- Source URL: https://arxiv.org/abs/2406.12315
- Reference count: 40
- Primary result: Introduces PruningBench, the first comprehensive benchmark for structural pruning of vision models, evaluating 16 methods across diverse architectures and tasks

## Executive Summary
This paper presents PruningBench, the first comprehensive benchmark for structural pruning of vision models that addresses the lack of standardized evaluation metrics in this field. The benchmark evaluates 16 existing pruning methods across diverse models (CNNs, ViTs) and tasks (classification, detection), providing 13 leaderboards and over 645 model pruning experiments. Key findings include the observation that no single method consistently outperforms others across all settings, weight norm-based methods typically rank highest, and architectural preferences exist for certain methods.

## Method Summary
PruningBench employs a unified and consistent framework for evaluating the effectiveness of diverse structural pruning techniques. The framework implements four main stages: sparsifying (optional), grouping, pruning, and finetuning. It uses DepGraph to automatically group network parameters, avoiding labor-intensive manual grouping while preventing group divergence. The benchmark conducts experiments with iterative pruning and protected global pruning schemes, using standardized settings across different model architectures and datasets. Performance is evaluated using metrics including accuracy, parameters, FLOPs, pruning time, and regularizing time.

## Key Results
- No single pruning method consistently outperforms others across all model architectures, tasks, and speedup ratios
- Weight norm-based methods like MagnitudeL1 and MagnitudeL2 typically rank highest in performance
- Architectural preferences exist for specific methods (e.g., BNScale, Hrank, and LAMP perform better on certain architectures)
- Sparsity regularizers show mixed performance improvements, with some enhancing pruning effectiveness while others yield inconsistent results

## Why This Works (Mechanism)

### Mechanism 1
The unified benchmark framework enables fair and reproducible comparisons across structural pruning methods. By standardizing experimental settings (models, datasets, pruning schemes, and hyperparameters), the framework eliminates inconsistencies that previously led to biased comparisons. Pruning performance is comparable when measured under identical conditions.

### Mechanism 2
Weight norm-based methods consistently perform well because they directly target parameter importance. Methods like MagnitudeL1 and MagnitudeL2 prune filters with smaller norms, assuming these contribute less to model output. This approach is data-agnostic and computationally efficient, leveraging the correlation between filter importance and weight magnitude.

### Mechanism 3
Sparsity regularizers improve pruning performance by learning structured sparse networks before actual pruning. Regularizers like GroupLASSO and BNScale impose sparse constraints during training, making parameters close to zero and easier to identify for pruning. Sparsity learned during training translates to effective pruning decisions.

## Foundational Learning

- **Neural network pruning fundamentals**: Understanding how structural pruning differs from unstructured pruning and how it affects model architecture. *Quick check*: What's the key difference between structured and unstructured pruning in terms of implementation and hardware requirements?

- **Importance criteria for pruning**: Different methods use various metrics (weight norms, activation patterns, gradients) to determine which parameters to prune. *Quick check*: How does a weight-norm-based pruning method decide which filters to remove?

- **Sparsity regularization techniques**: Understanding how regularizers like GroupLASSO and BNScale promote structured sparsity before pruning. *Quick check*: What's the relationship between sparsity regularization during training and the effectiveness of subsequent pruning?

## Architecture Onboarding

- **Component map**: Pretrained model → Sparsifying (if applicable) → Grouping via DepGraph → Iterative pruning → Finetuning → Evaluation
- **Critical path**: The four-stage pipeline from pretrained model through sparsifying, grouping, pruning, and finetuning to final evaluation
- **Design tradeoffs**: Balances comprehensive evaluation (many methods, models, tasks) with computational feasibility (controlled experiments, standardized settings)
- **Failure signatures**: Inconsistent results across runs may indicate stochastic methods; poor performance might suggest architectural incompatibility; excessive computation time could point to inefficient implementations
- **First 3 experiments**:
  1. Run MagnitudeL1 pruning on ResNet18 with CIFAR100 to verify basic functionality
  2. Compare local vs global pruning on a small network to understand pruning scheme impacts
  3. Test BNScale regularizer with MagnitudeL2 pruning to validate combined approach

## Open Questions the Paper Calls Out

### Open Question 1
What are the fundamental limitations preventing a single structural pruning method from consistently outperforming others across all model architectures, tasks, and speedup ratios? The paper states "no single method consistently outperforms the others across all settings and tasks" but doesn't provide a theoretical explanation for why methods show such diverse behavior.

### Open Question 2
How can structural pruning methods be adapted to effectively handle vision transformer architectures, given their unique characteristics like patch embeddings and multi-head attention? The paper demonstrates ViTs are harder to compress but doesn't propose solutions for addressing their architectural constraints during pruning.

### Open Question 3
What is the optimal balance between global and local pruning strategies, and how can protected global pruning be further improved to prevent layer collapse at high speedup ratios? While protected global pruning shows improvements, the paper identifies it as insufficient for extreme pruning scenarios.

## Limitations
- Results may not generalize to architectures or tasks beyond the studied CNNs and ViTs
- The assumption that standardized settings enable fair comparisons may not hold for all pruning methods
- Performance rankings could shift significantly with different hyperparameters or larger-scale models

## Confidence
- **High Confidence**: No single pruning method consistently outperforms others; weight norm-based methods typically rank highest
- **Medium Confidence**: Unified framework enables fair comparisons, given potential method-specific requirements
- **Low Confidence**: Sparsity regularizers consistently improve performance, as effectiveness appears mixed and implementation-dependent

## Next Checks
1. **Cross-Architecture Validation**: Test top-performing pruning methods from PruningBench on architectures not included in the original study (e.g., Swin Transformers, EfficientNet variants) to verify generalizability.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary key hyperparameters for the top 3 pruning methods to determine if observed performance differences are robust or method-specific.

3. **Extended Task Evaluation**: Apply PruningBench methods to vision tasks beyond classification and detection (e.g., semantic segmentation, instance segmentation) to assess broader applicability.