---
ver: rpa2
title: Gaze-Guided Graph Neural Network for Action Anticipation Conditioned on Intention
arxiv_id: '2404.07347'
source_url: https://arxiv.org/abs/2404.07347
tags:
- activity
- action
- human
- video
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenging task of predicting actions
  of an agent in a video based on a partial video observation, conditioned on the
  agent's intention. The authors introduce a Gaze-guided Action Anticipation algorithm
  that establishes a visual-semantic graph from the video input using human gaze data.
---

# Gaze-Guided Graph Neural Network for Action Anticipation Conditioned on Intention

## Quick Facts
- arXiv ID: 2404.07347
- Source URL: https://arxiv.org/abs/2404.07347
- Reference count: 26
- 7% improvement in accuracy for 18-class intention recognition compared to state-of-the-art techniques

## Executive Summary
This paper introduces a Gaze-guided Action Anticipation algorithm that uses human gaze data to build a visual-semantic graph from video frames, processed by a Graph Neural Network to recognize intentions and predict action sequences. The method was evaluated on a novel VirtualHome dataset of household activities with accompanying gaze data, demonstrating a 7% improvement in intention recognition accuracy. The approach leverages spatial-temporal features captured through gaze fixations and semantic object information to improve action anticipation performance.

## Method Summary
The method constructs a visual-semantic graph from video frames using human gaze fixations as spatial guidance. Gaze data is processed to identify fixation points, from which small image patches are cropped and encoded into node features. Object detection identifies objects within these patches, with semantic embeddings used as edge attributes between nodes. The graph is processed through Edge-Conditioned Convolution layers to create graph embeddings, which are then used for hierarchical classification: first recognizing the overall activity, then predicting the sequence of remaining atomic actions using LSTM layers conditioned on the recognized activity.

## Key Results
- Achieved 7% improvement in accuracy for 18-class intention recognition compared to state-of-the-art techniques
- Demonstrated effective learning of important features from human gaze data for action anticipation
- Showed successful prediction of action sequences needed to fulfill recognized intentions

## Why This Works (Mechanism)

### Mechanism 1
Human gaze fixation points allow the model to focus on task-relevant spatial-temporal features by cropping image patches centered on those fixations. Gaze fixations are used to extract small image patches from video frames, encoded into node features for a graph, ensuring the model only processes areas where humans naturally attend during the task. Core assumption: Gaze fixations reliably indicate regions of interest in household activity videos. Break condition: If fixation points are noisy or miss critical objects, cropped patches lose task-relevant information.

### Mechanism 2
Object detector labels added as edge attributes enrich the graph with semantic context, improving activity classification and action prediction. For each node, an object detector identifies the object present, and semantic embeddings are concatenated as edge attributes between nodes, encoding transitions like "grab cereal → walk to table." Core assumption: Objects identified in gaze patches are strongly correlated with the intended action. Break condition: If object detector is inaccurate, edge attributes add noise rather than signal.

### Mechanism 3
Conditioning action prediction on the recognized activity (hierarchical classification) improves prediction accuracy and sequence ordering. After recognizing the overall activity, its embedding is concatenated with node features and fed into LSTM layers to predict the sequence of remaining atomic actions. Core assumption: Knowing the high-level goal constrains and guides the selection of intermediate actions. Break condition: If activity recognition is wrong, all subsequent action predictions are likely to be invalid.

## Foundational Learning

- **Graph Neural Networks (GNNs)**: Model spatial-temporal relationships between gaze-centered image patches and object interactions in video. Quick check: How does a GNN aggregate information from neighboring nodes to update node embeddings?
- **Gaze fixation processing and velocity-threshold algorithms**: Extract clean fixation points from raw eye-tracking data for use as spatial guidance in video analysis. Quick check: What is the purpose of using a velocity threshold when identifying fixations from raw gaze data?
- **Semantic embeddings (e.g., Word2Vec, CLIP)**: Convert object class names into dense vector representations usable in edge attributes and node features. Quick check: Why might a model prefer semantic embeddings over one-hot encodings for objects?

## Architecture Onboarding

- **Component map**: Gaze processing → Cropping → Node feature extraction (visual encoder) → Object detection → Edge attribute creation → Graph construction → GNN layers → Graph embedding → MLP for activity recognition → LSTM + MLP for action sequence prediction
- **Critical path**: Gaze fixation → Cropped patch → Node feature → GNN → Graph embedding → Activity recognition + Action prediction
- **Design tradeoffs**:
  - Crop size B: Smaller crops reduce noise but may miss context; larger crops add noise but preserve more context
  - Node similarity threshold ρ: Higher values reduce node count but may merge distinct frames; lower values increase nodes but may over-split similar content
  - Object detector accuracy: Critical for edge attribute quality; poor detector degrades performance
- **Failure signatures**:
  - Activity recognition accuracy drops → Likely fixation or cropping issue
  - Action prediction Levenshtein distance high → Possible GNN embedding or LSTM conditioning issue
  - Low success rate on VirtualHome execution → Likely mismatch between predicted actions and executable program steps
- **First 3 experiments**:
  1. Vary crop size B (25, 50, 75, 100) and measure activity recognition accuracy
  2. Toggle object detector usage (on/off) and compare action prediction metrics
  3. Test conditioning: Train with and without activity recognition input to LSTM, measure action sequence quality

## Open Questions the Paper Calls Out

### Open Question 1
How would the model's performance change if gaze fixations were collected from a larger and more diverse participant pool? The study only used data from 13 participants, and a larger, more diverse group might provide different gaze patterns affecting the model's learning and generalization. What evidence would resolve it: Conducting the same experiments with gaze data from a significantly larger and more demographically diverse participant pool.

### Open Question 2
How robust is the proposed method to noisy or erroneous gaze data, and what are the implications for real-world applications? Real-world scenarios might involve imperfect gaze tracking or participants not paying full attention, leading to noisy gaze data. What evidence would resolve it: Experiments where the model is trained and tested on datasets with varying levels of gaze data noise or errors.

### Open Question 3
How does the model's performance scale with increasing video length or more complex activities? The paper uses videos up to 32 seconds long with an average of 15 atomic actions, but real-world activities might be longer and more complex. What evidence would resolve it: Testing the model on videos of varying lengths (e.g., 1 minute, 5 minutes) and with more complex activities involving a higher number of atomic actions and interacting objects.

## Limitations
- Core claim relies on a novel dataset (VirtualHome) that may not generalize to real-world video domains
- Gaze data collection protocol and video simulation quality are not fully detailed, creating uncertainty about external validity
- Performance generalization beyond the VirtualHome environment is unknown

## Confidence
- High confidence in the mechanism of using gaze fixations to guide spatial attention in video frames
- Medium confidence in the effectiveness of object detector-based edge attributes for semantic enrichment
- Medium confidence in hierarchical conditioning improving action prediction accuracy
- Low confidence in performance generalization beyond the VirtualHome environment

## Next Checks
1. Test the method on a real-world household activity dataset (e.g., EPIC-KITCHENS) to assess generalization beyond simulated environments
2. Conduct ablation studies isolating each component (gaze-guided cropping, object detector edge attributes, hierarchical conditioning) to quantify individual contributions
3. Evaluate robustness to noisy gaze data by adding synthetic noise to fixation points and measuring degradation in performance