---
ver: rpa2
title: Are LLM-based Recommenders Already the Best? Simple Scaled Cross-entropy Unleashes
  the Potential of Traditional Sequential Recommenders
arxiv_id: '2408.14238'
source_url: https://arxiv.org/abs/2408.14238
tags:
- recommendation
- loss
- recommenders
- llm-based
- traditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study challenges the prevailing belief that LLM-based recommenders
  are inherently superior to traditional methods in sequential recommendation. The
  authors demonstrate that the observed performance gap is largely due to inconsistent
  experimental settings, particularly the use of cross-entropy loss with full softmax
  for LLMs versus pointwise/pairwise losses for traditional models.
---

# Are LLM-based Recommenders Already the Best? Simple Scaled Cross-entropy Unleashes the Potential of Traditional Sequential Recommenders

## Quick Facts
- **arXiv ID**: 2408.14238
- **Source URL**: https://arxiv.org/abs/2408.14238
- **Reference count**: 40
- **Primary result**: Traditional sequential recommenders can outperform LLM-based models when trained with Scaled Cross-Entropy loss

## Executive Summary
This study challenges the prevailing belief that LLM-based recommenders are inherently superior to traditional methods in sequential recommendation. The authors demonstrate that the observed performance gap is largely due to inconsistent experimental settings, particularly the use of cross-entropy loss with full softmax for LLMs versus pointwise/pairwise losses for traditional models. Through theoretical analysis, they show that cross-entropy loss offers both tightness and coverage, making it a strong choice for recommendation. They further propose Scaled Cross-Entropy (SCE) as an efficient approximation for practical scenarios where full softmax is infeasible. Empirical results show that traditional models trained with SCE can outperform state-of-the-art LLM-based recommenders, highlighting the importance of fair comparisons and objective evaluation in the field.

## Method Summary
The study compares traditional sequential recommenders (SASRec, FMLP-Rec, GRU4Rec, Caser) against LLM-based models (P5, POD, LlamaRec, E4SRec) on Beauty and Yelp datasets. The key methodological contribution is training traditional models with cross-entropy loss and proposing Scaled Cross-Entropy (SCE) as an approximation for large item sets. The experiments use leave-one-out evaluation with NDCG@k and MRR@k metrics, comparing models trained with different loss functions (cross-entropy vs BCE/BPR).

## Key Results
- Traditional models (SASRec, FMLP-Rec) trained with cross-entropy loss significantly outperform when trained with BCE/BPR losses
- When trained with SCE, traditional models surpass all state-of-the-art LLM-based recommenders on Beauty and Yelp datasets
- SCE with K=500 negative samples achieves optimal balance between efficiency and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-entropy loss with full softmax outperforms pointwise/pairwise losses in sequential recommendation due to its tightness and coverage properties
- Mechanism: Cross-entropy loss provides a soft proxy for ranking metrics (NDCG, MRR) while exploring all items during training, leading to better optimization of ranking quality
- Core assumption: The connection between cross-entropy loss and ranking metrics is strong enough to drive consistent improvements in recommendation performance
- Evidence anchors:
  - [abstract]: "cross-entropy loss offers both tightness and coverage, making it a strong choice for recommendation"
  - [section]: "an ideal recommendation loss should emphasize both tightness and coverage"
  - [corpus]: Weak - only 2 related papers mention CE loss, suggesting limited direct empirical evidence
- Break condition: When the number of items is extremely large, making full softmax computationally infeasible

### Mechanism 2
- Claim: Scaled Cross-Entropy (SCE) maintains ranking performance with sampled softmax by scaling up the sampled normalizing term
- Mechanism: Scaling the sampled softmax normalizing term by α increases the probability of bounding ranking metrics, compensating for the reduced coverage from sampling
- Core assumption: Uniform sampling of negative items combined with scaling preserves the theoretical properties of full softmax
- Evidence anchors:
  - [abstract]: "an effective alternative is to scale up the sampled normalizing term"
  - [section]: "SCE addresses this issue by scaling up the sampled term using an additional weight"
  - [corpus]: Weak - no direct evidence of scaling technique in related papers
- Break condition: When α is too small to maintain the theoretical bounding properties, or when sampling variance becomes problematic with very few samples

### Mechanism 3
- Claim: Traditional sequential recommenders can outperform LLM-based recommenders when trained with appropriate loss functions
- Mechanism: The performance gap between traditional and LLM-based recommenders is primarily due to inconsistent experimental settings rather than inherent model superiority
- Core assumption: The ranking capability difference is not due to model architecture but to the optimization objective used during training
- Evidence anchors:
  - [abstract]: "traditional models trained with SCE can outperform state-of-the-art LLM-based recommenders"
  - [section]: "SASRec and FMLP-Rec demonstrate significant performance improvements as a result of this subtle change, ultimately surpassing all LLM-based recommenders"
  - [corpus]: Weak - limited evidence of traditional models outperforming LLMs in related work
- Break condition: When LLMs have access to domain-specific knowledge or other architectural advantages not captured in the comparison

## Foundational Learning

- Concept: Cross-entropy loss and its relationship to ranking metrics
  - Why needed here: Understanding why cross-entropy is superior to pointwise/pairwise losses requires grasping the theoretical connection to NDCG and MRR
  - Quick check question: How does minimizing cross-entropy loss relate to maximizing NDCG and MRR according to Lemma 1?

- Concept: Tightness and coverage in loss function design
  - Why needed here: The paper's main contribution is understanding that an ideal recommendation loss needs both properties
  - Quick check question: Why does the original cross-entropy loss represent a suboptimal balance between tightness and coverage?

- Concept: Sampled softmax and its limitations
  - Why needed here: SCE is proposed as an improvement to sampled softmax for large item catalogs
  - Quick check question: What is the main limitation of sampled softmax that SCE addresses?

## Architecture Onboarding

- Component map: Traditional recommender -> scoring function -> loss function -> optimization -> ranking metrics
- Critical path: Item embedding -> sequence modeling (RNN/CNN/Transformer) -> inner product scoring -> cross-entropy loss -> NDCG/MRR optimization
- Design tradeoffs: Full softmax provides theoretical guarantees but is computationally expensive; sampled softmax is efficient but loses coverage; SCE balances both
- Failure signatures: Poor ranking performance with sampled softmax alone; high variance in SCE with very few negative samples
- First 3 experiments:
  1. Compare traditional models (SASRec, FMLP-Rec) trained with BCE vs. CE on Beauty/Yelp datasets
  2. Evaluate SCE with different α values and K (number of negative samples) on a small dataset
  3. Test traditional models with SCE against LLM-based recommenders on Beauty dataset with K=500

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between tightness and coverage in recommendation loss functions?
- Basis in paper: [explicit] The paper demonstrates that both tightness and coverage are desirable properties for recommendation losses, and identifies an optimal point ℓCE-η* where these properties are well balanced
- Why unresolved: The paper shows that there exists a task-specific optimal η* that balances tightness and coverage, but doesn't provide a general method to determine this optimal point for different recommendation scenarios
- What evidence would resolve it: Empirical studies across diverse recommendation datasets and tasks showing how η* varies with data characteristics, user behavior patterns, and recommendation objectives

### Open Question 2
- Question: How can the high variance problem of Scaled Cross-Entropy (SCE) be mitigated when sampling very few negative items?
- Basis in paper: [explicit] The paper acknowledges that SCE encounters a high variance problem when the number of negative samples is very small (K = 1), though this becomes less significant as K increases
- Why unresolved: While the paper shows SCE works well for K ≥ 10, it doesn't provide a solution for scenarios where computational constraints necessitate even fewer samples
- What evidence would resolve it: Development and validation of variance reduction techniques specifically designed for SCE, or alternative sampling strategies that maintain performance with minimal negative samples

### Open Question 3
- Question: Can the principles of tightness and coverage be extended to other recommendation paradigms beyond sequential recommendation?
- Basis in paper: [inferred] The paper focuses on sequential recommendation and discusses loss function properties in this context, but the theoretical framework of tightness and coverage could potentially apply to other recommendation tasks
- Why unresolved: The paper doesn't explore whether these principles hold for general recommendation tasks like collaborative filtering or content-based recommendation
- What evidence would resolve it: Comparative studies of different loss functions on non-sequential recommendation tasks, measuring both tightness and coverage properties and their impact on recommendation quality

### Open Question 4
- Question: How does the ranking performance of LLM-based recommenders change when trained with cross-entropy loss instead of the traditional pointwise/pairwise losses?
- Basis in paper: [explicit] The paper shows that traditional models trained with cross-entropy can outperform LLM-based recommenders, but doesn't investigate whether LLMs would benefit similarly from cross-entropy training
- Why unresolved: The paper focuses on traditional models and doesn't explore whether the superior performance of LLMs is due to their architecture or simply the loss function used during training
- What evidence would resolve it: Experimental comparison of LLM-based recommenders trained with both traditional losses and cross-entropy, measuring ranking performance across multiple datasets and metrics

## Limitations

- Theoretical analysis assumes perfect optimization and uniform negative sampling, which may not hold in practical scenarios
- SCE requires careful tuning of the scaling factor α, which may be dataset-dependent
- The comparison with LLM-based models may not capture all advantages of LLMs beyond ranking performance

## Confidence

- High confidence in the theoretical analysis showing cross-entropy loss's superiority over pointwise/pairwise losses due to its tightness and coverage properties
- Medium confidence in SCE's practical effectiveness, as empirical validation is limited to specific datasets and model architectures
- Medium confidence in the claim that traditional models can outperform LLM-based recommenders, as the comparison may not capture all LLM advantages beyond ranking performance

## Next Checks

1. Test SCE on larger-scale datasets (e.g., Amazon product datasets) with millions of items to verify its scalability and performance under extreme sparsity conditions
2. Evaluate whether incorporating additional context (timestamps, item attributes, user demographics) into traditional models with SCE can close the gap with LLM-based methods
3. Conduct ablation studies on the scaling factor α across different sampling strategies (uniform vs. popularity-based) to identify optimal SCE configurations