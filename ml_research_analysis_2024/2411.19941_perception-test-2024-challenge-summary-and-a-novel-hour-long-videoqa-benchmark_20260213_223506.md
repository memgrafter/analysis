---
ver: rpa2
title: 'Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark'
arxiv_id: '2411.19941'
source_url: https://arxiv.org/abs/2411.19941
tags:
- video
- test
- benchmark
- task
- perception
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the results of the 2024 Perception Test challenge
  and introduces a new benchmark for hour-long video understanding. The challenge,
  held alongside ECCV 2024, featured seven tracks covering tasks such as object tracking,
  action localization, sound localization, multiple-choice video QA, grounded video
  QA, and hour-long video QA.
---

# Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark

## Quick Facts
- arXiv ID: 2411.19941
- Source URL: https://arxiv.org/abs/2411.19941
- Reference count: 40
- Top models showed strong capabilities in handling hour-long videos, though significant gap remains compared to human performance

## Executive Summary
The 2024 Perception Test challenge, held alongside ECCV 2024, featured seven tracks covering object tracking, action localization, sound localization, and video question answering tasks. The challenge attracted 680 submissions from 123 teams, demonstrating significant performance improvements compared to 2023, particularly in grounded video QA. A novel benchmark called 1h-walk VQA was introduced, based on hour-long walking tour videos, to assess models' ability to reason over long temporal contexts. While top models demonstrated strong capabilities in handling hour-long videos, a notable performance gap remains compared to human baselines.

## Method Summary
The Perception Test 2024 challenge included seven tracks: object tracking, point tracking, action localization, sound localization, multiple-choice video QA, grounded video QA, and hour-long video QA. The benchmark used the Perception Test dataset (11.6k videos up to 35s) plus the novel 1h-walk VQA benchmark (10 hour-long walking tour videos with 70 manually-curated question-answer pairs). Models were evaluated using task-specific metrics including IoU, Jaccard, mAP, and top-1 accuracy. The 1h-walk VQA benchmark was designed for zero-shot evaluation only, with no training data provided.

## Key Results
- Significant performance improvements across all tracks compared to 2023, especially in grounded video QA
- Top models demonstrated strong capabilities in handling hour-long videos
- 99.64% human performance baseline on 1h-walk VQA vs 20% random baseline
- Notable performance gap remains between top models and human performance in hour-long video understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark's hour-long video QA format tests models' ability to integrate multimodal information over extended temporal contexts
- Mechanism: By curating questions that require reasoning across multiple non-contiguous segments and modalities (video + audio), the benchmark forces models to retain and synthesize information over long durations
- Core assumption: The questions are sufficiently complex that single-frame or short-clip shortcuts are insufficient
- Evidence anchors:
  - [abstract]: "to assess models' capability of reasoning over very long temporal context"
  - [section 2]: "to ensure that our questions require long context, we ran several iterations of annotation collection with human raters"
  - [corpus]: weak - no direct citations found, but related works on long video understanding support the claim
- Break condition: If models can answer questions using only short clips or static frames, the temporal integration advantage disappears

### Mechanism 2
- Claim: Introducing a zero-shot evaluation regime encourages models to develop robust general reasoning capabilities
- Mechanism: Without training data, models must rely on pre-existing knowledge and in-context learning, pushing architectures toward better generalization
- Core assumption: Zero-shot settings prevent overfitting to dataset-specific patterns
- Evidence anchors:
  - [abstract]: "This small benchmark is intended for zero-shot evaluation"
  - [section 2]: "We do not provide any training or fine-tuning data"
  - [corpus]: weak - limited direct evidence, but literature on few/zero-shot learning supports this mechanism
- Break condition: If models can still perform well through memorization or dataset-specific heuristics, the zero-shot advantage is undermined

### Mechanism 3
- Claim: The combination of seven diverse tracks in the challenge provides a comprehensive evaluation of video model capabilities
- Mechanism: By covering low-level tasks (tracking) and high-level tasks (reasoning, QA) across multiple modalities, the benchmark identifies specific strengths and weaknesses
- Core assumption: Different tracks require genuinely different capabilities, not just variations of the same skill
- Evidence anchors:
  - [abstract]: "covered low-level and high-level tasks, with language and non-language interfaces, across video, audio, and text modalities"
  - [section 3]: "we observe a great improvement in performance on all tracks compared to last year"
  - [corpus]: weak - no direct citations, but the design aligns with best practices in multimodal evaluation
- Break condition: If performance across tracks is highly correlated, the diversity benefit is reduced

## Foundational Learning

- Concept: Temporal reasoning in video understanding
  - Why needed here: Models must track entities and events across long time spans to answer questions correctly
  - Quick check question: Can you identify a video question that requires information from two separate minutes?

- Concept: Multimodal fusion techniques
  - Why needed here: Audio and visual information must be integrated to answer many questions
  - Quick check question: How would you combine audio features with visual features for temporal localization?

- Concept: Long context handling in transformers
  - Why needed here: Hour-long videos contain tens of thousands of frames, requiring efficient attention mechanisms
  - Quick check question: What architectural modifications help transformers handle sequences longer than 8k tokens?

## Architecture Onboarding

- Component map: Input preprocessing -> Multimodal encoder -> Temporal modeling -> Reasoning module -> Output head
- Critical path: Video/audio feature extraction -> Temporal attention -> Cross-modal fusion -> Question answering
- Design tradeoffs: Model capacity vs inference speed, fine-tuning vs zero-shot performance, feature quality vs computational cost
- Failure signatures: Performance drops on long-context questions, modality-specific failures, temporal reasoning errors
- First 3 experiments:
  1. Evaluate baseline model on short vs long video segments to identify temporal context requirements
  2. Test modality ablation (video only, audio only, both) to understand multimodal integration needs
  3. Measure performance degradation with increasing video length to determine effective context window

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations of current multimodal video models in handling hour-long videos, and how can these be addressed to improve performance?
- Basis in paper: [inferred] The paper highlights that there is still a significant gap between the performance of top models and human performance in hour-long video understanding, indicating unresolved limitations in current models.
- Why unresolved: The paper does not provide a detailed analysis of the specific limitations of current models or potential strategies to overcome them.
- What evidence would resolve it: Comparative studies of model architectures, datasets, and evaluation metrics that identify specific bottlenecks in hour-long video understanding.

### Open Question 2
- Question: How can the collection of challenging questions for hour-long video benchmarks be scaled up to create larger and more diverse datasets?
- Basis in paper: [explicit] The paper mentions that collecting challenging questions that span long temporal contexts is difficult and suggests that future benchmarks could be inspired by the proposed 1h-walk VQA by using specialized event detectors.
- Why unresolved: The paper does not provide a detailed methodology for scaling up the annotation process or creating larger datasets.
- What evidence would resolve it: Development and evaluation of automated tools or frameworks for generating and validating challenging questions for hour-long video understanding.

### Open Question 3
- Question: What are the key differences in performance between zero-shot and fine-tuned models on the 1h-walk VQA benchmark, and what does this imply for model generalizability?
- Basis in paper: [inferred] The paper states that the 1h-walk VQA benchmark is intended for zero-shot evaluation, but it does not compare zero-shot performance to fine-tuned models, leaving the generalizability of models unclear.
- Why unresolved: The paper does not provide empirical comparisons between zero-shot and fine-tuned models on the benchmark.
- What evidence would resolve it: Experimental results comparing the performance of zero-shot and fine-tuned models on the 1h-walk VQA benchmark, along with an analysis of the factors influencing their performance.

## Limitations
- Limited architectural analysis of what drove performance improvements across different tracks
- Lack of systematic ablation studies to verify models actually use full temporal context
- Zero-shot evaluation claim undermined by significant performance gap with human baselines

## Confidence
- High Confidence: Basic challenge results and benchmark creation methodology
- Medium Confidence: Claim that benchmark tests long temporal reasoning capabilities
- Low Confidence: Assertion that zero-shot evaluation prevents overfitting

## Next Checks
1. Conduct ablation studies on 1h-walk VQA where models are given progressively shorter video clips to determine the minimum temporal context needed for accurate answers
2. Analyze top-performing models' attention patterns to verify they actually integrate information across the full temporal span rather than focusing on short segments
3. Perform controlled experiments comparing fine-tuned vs zero-shot models on held-out questions to quantify the actual generalization benefit of the zero-shot regime