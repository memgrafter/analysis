---
ver: rpa2
title: 'SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation'
arxiv_id: '2411.01710'
source_url: https://arxiv.org/abs/2411.01710
tags:
- spes
- tokens
- scores
- saliency
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPES is a feature attribution method for explaining autoregressive
  speech-to-text generation models. It addresses the challenge of generating fine-grained,
  phonetically meaningful explanations for sequence generation tasks, where existing
  methods either fail to account for autoregressive dependencies or lack phonetic
  interpretability.
---

# SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation

## Quick Facts
- **arXiv ID:** 2411.01710
- **Source URL:** https://arxiv.org/abs/2411.01710
- **Reference count:** 40
- **Primary result:** SPES achieves up to 92.54 deletion AUC in ASR, outperforming baselines by localizing phonetically meaningful patterns in spectrograms

## Executive Summary
SPES addresses the challenge of generating fine-grained, phonetically meaningful explanations for autoregressive speech-to-text models. Unlike existing methods that either fail to account for autoregressive dependencies or lack phonetic interpretability, SPES uses morphological clustering (SLIC segmentation) to identify salient time-frequency patterns and perturbs both spectrogram patches and previously generated tokens. The method estimates perturbation impact using KL divergence and aggregates scores to produce token-level explanations for both input spectrogram and output context. Evaluated on automatic speech recognition (ASR) and speech translation (ST), SPES demonstrates superior faithfulness and compactness compared to baselines while providing plausible explanations that align with human perception.

## Method Summary
SPES is a feature attribution method that explains autoregressive speech-to-text generation models by perturbing spectrogram patches identified through SLIC clustering and previously generated tokens. The method perturbs these features separately to prevent information blurring, estimates impact using KL divergence between original and perturbed probability distributions, and aggregates scores across multiple iterations. Saliency maps are produced for both the spectrogram input and output context, providing fine-grained explanations that capture phonetic characteristics and contextual dependencies. The approach is evaluated on ASR and ST tasks using deletion and size metrics to assess faithfulness and compactness.

## Key Results
- Achieves 92.54 deletion AUC on ASR task, significantly outperforming feature-wise baseline (70.41) and Bubble Noise (70.55)
- Demonstrates 29.71 size AUC on ASR task, indicating more compact and focused explanations than baselines
- Shows plausible explanations that align with human perception, capturing formants and contextual dependencies in both time and frequency domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Morphological clustering (SLIC) produces phoneme-like perturbation units that align with model-relevant acoustic features.
- **Mechanism:** SLIC groups spectrogram pixels by both spatial proximity and energy similarity, creating patches that respect natural acoustic boundaries like formants and transitions.
- **Core assumption:** Acoustic patterns in spectrograms have coherent spatial and energy characteristics that SLIC can detect and group meaningfully.
- **Evidence anchors:** [abstract] "fine-grained, phonetically meaningful explanations"; [section 4.1.1] "clusters nearby features with similar energy levels into distinct patches that respect the structure of acoustic patterns, such as formants"
- **Break condition:** If speech contains non-stationary noise or rapid frequency transitions that break the spatial coherence assumption, patches may cross phoneme boundaries and lose phonetic meaning.

### Mechanism 2
- **Claim:** KL divergence better captures perturbation impact for high-dimensional, dense S2T probability distributions than simple probability difference.
- **Mechanism:** KL divergence measures the overall distributional shift caused by perturbations, accounting for the fact that beam search may select tokens with lower initial probability.
- **Core assumption:** The model's probability distribution over thousands of BPE tokens is meaningfully altered by perturbations, and this alteration is best captured by a divergence measure.
- **Evidence anchors:** [abstract] "SPES estimates perturbation impact (rX for spectrograms and rY(k−1) for previous output tokens) by computing the KL divergence"; [section 4.2] "S2T tasks involve a BPE vocabulary with thousands of tokens... This results in output probability distributions that are high-dimensional, dense vectors"
- **Break condition:** If the model's probability distribution is extremely peaked (near-deterministic) or if perturbations cause complete distributional collapse, KL divergence may become unstable or uninformative.

### Mechanism 3
- **Claim:** Separate perturbation of spectrogram and previous tokens prevents information blurring and reveals contextual dependencies.
- **Mechanism:** Joint perturbation causes previous tokens to be perturbed more frequently due to their lower cardinality, obscuring their true importance. Separate perturbation allows clean attribution.
- **Core assumption:** The importance of spectrogram features and previous tokens for predicting the next token can be independently assessed without losing their interaction effects.
- **Evidence anchors:** [section 4.1.2] "previous tokens are fewer than the speech features, they are perturbed more frequently, which can 'blur' explanations on the spectrogram"; [section 4.2] "we conduct a systematic validation to select the best configuration using the deletion metric"
- **Break condition:** If the prediction of a token truly requires simultaneous perturbation of both input and context (e.g., homophone disambiguation), separate perturbation may underestimate their joint importance.

## Foundational Learning

- **Concept:** Spectrogram representation of speech
  - **Why needed here:** SPES operates directly on spectrograms, so understanding time-frequency representation is essential for interpreting saliency maps.
  - **Quick check question:** What acoustic features (e.g., formants, fundamental frequency) appear as distinct patterns in a spectrogram?

- **Concept:** Autoregressive generation in S2T models
  - **Why needed here:** SPES explains each token conditioned on both input and previous tokens, requiring understanding of the generation process.
  - **Quick check question:** How does the model's prediction of token k depend on both the spectrogram and tokens 0 through k-1?

- **Concept:** Feature attribution and perturbation-based XAI
  - **Why needed here:** SPES is a perturbation-based feature attribution method, so understanding how masking/perturbing inputs reveals importance is crucial.
  - **Quick check question:** Why might measuring prediction change after perturbation be more reliable than using gradients for attribution?

## Architecture Onboarding

- **Component map:** Spectrogram → SLIC segmentation → patch perturbation → KL divergence → saliency map → explanation; Previous tokens → perturbation → KL divergence → saliency map → explanation
- **Critical path:** Spectrogram → SLIC segmentation → patch perturbation → KL divergence → saliency map → explanation
  - The bottleneck is SLIC segmentation and N perturbation iterations (N=20,000 for spectrograms)
- **Design tradeoffs:**
  - Multi-scale vs single-scale segmentation: multi-scale captures more patterns but increases computation
  - Separate vs joint perturbation: separate is cleaner but may miss joint effects
  - KL divergence vs probability difference: KL is more robust for dense distributions but more expensive
- **Failure signatures:**
  - Saliency maps are uniformly distributed (perturbation too weak or model too robust)
  - Saliency maps are noisy and scattered (too many small patches or excessive perturbation)
  - KL divergence values are NaN or infinite (perturbation completely destroys probability distribution)
- **First 3 experiments:**
  1. **Sanity check:** Run SPES on a known ASR model with a simple utterance; verify saliency maps show time-localization matching word boundaries.
  2. **Hyperparameter sweep:** Test different pspec values (0.3, 0.5, 0.7) on a small validation set; measure deletion AUC to find optimal balance.
  3. **Ablation test:** Compare SPES with and without SLIC clustering on the same utterance; check if clustered patches produce more phonetically meaningful explanations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does SPES perform on speech-to-text tasks with non-autoregressive models, such as models using CTC or encoder-decoder architectures without autoregressive decoding?
- **Basis in paper:** [explicit] The paper focuses exclusively on autoregressive models, stating that SPES is designed to account for the autoregressive nature of modern S2T models and that it considers both the input spectrogram and previously generated tokens for each prediction.
- **Why unresolved:** The paper does not evaluate SPES on non-autoregressive models or discuss how the method might need to be adapted for such architectures. The perturbation strategy for previous output tokens would not apply to non-autoregressive models.
- **What evidence would resolve it:** Comparative experiments applying SPES to non-autoregressive S2T models, measuring deletion and size metrics, and analyzing whether the method still provides meaningful explanations without the autoregressive context.

### Open Question 2
- **Question:** How sensitive are SPES explanations to the choice of spectrogram segmentation parameters (ϕ, σ, pspec) and how can these be optimized for different speech domains or acoustic environments?
- **Basis in paper:** [explicit] The paper validates these parameters on a specific dataset (MuST-C) and sets them based on that validation, but notes that the choice of parameters impacts the quality of explanations.
- **Why unresolved:** The paper does not explore how SPES would perform on datasets with different acoustic characteristics (e.g., noisy environments, different languages, different speaker demographics) or how the parameters might need to be tuned for optimal performance in these scenarios.
- **What evidence would resolve it:** Experiments applying SPES to diverse speech datasets with varying acoustic properties, testing different parameter configurations, and analyzing how parameter choices affect explanation quality across domains.

### Open Question 3
- **Question:** How does the computational cost of SPES scale with longer audio samples and what are the practical limitations for real-time or near-real-time applications?
- **Basis in paper:** [explicit] The paper acknowledges that the number of patch combinations grows exponentially with k, leading to computational challenges, and introduces a threshold parameter τ to limit this growth. The ablation study also shows that the relationship between audio duration and computation time is quadratic.
- **Why unresolved:** The paper does not provide detailed analysis of the computational requirements for different audio lengths or discuss the practical feasibility of using SPES in applications requiring fast explanation generation, such as interactive systems or real-time monitoring.
- **What evidence would resolve it:** Detailed profiling of SPES runtime across various audio lengths, analysis of memory usage, and experiments testing the method's applicability to streaming or real-time speech processing scenarios.

## Limitations

- **Limited theoretical grounding for SLIC parameters:** The choice of k values for SLIC segmentation (2000-4500 patches) appears empirical rather than theoretically justified, with no clear explanation of why these specific patch counts optimally balance phonetic granularity with computational efficiency.
- **Computational intensity:** SPES requires 20,000 perturbation iterations for spectrogram explanations and 2,000 for token explanations, representing a significant computational burden that wasn't benchmarked or discussed in terms of practical deployment constraints.
- **Limited evaluation scope:** While SPES shows strong performance on deletion and size metrics for ASR and ST tasks, the evaluation is confined to three language pairs and doesn't explore challenging scenarios like accented speech, background noise, or code-switching.

## Confidence

- **High confidence** in the mechanism that KL divergence better captures distributional impact than probability differences for dense BPE vocabularies. The mathematical foundation is sound, and the reasoning about high-dimensional probability spaces is well-established in information theory.
- **Medium confidence** in the SLIC clustering producing phonetically meaningful units. While the intuition is compelling and the validation results are strong, the lack of explicit phonetic alignment analysis or comparison with linguistically motivated segmentation methods leaves room for uncertainty.
- **Medium confidence** in the separate perturbation approach preventing information blurring. The argument is logically sound, but the empirical validation is limited to comparing against a single baseline without testing joint perturbation variants.

## Next Checks

- **Check 1: Phonetic alignment validation** - Take SPES explanations from a controlled dataset (e.g., CMU Arctic corpus with known phoneme boundaries) and measure the overlap between high-saliency regions and actual phoneme boundaries using forced alignment. Compute precision/recall for detecting phoneme onset/offset times to quantify phonetic meaningfulness beyond the qualitative waveform comparisons shown in the paper.

- **Check 2: Ablation study on perturbation counts** - Systematically vary N_X (e.g., 5k, 10k, 20k, 40k) and measure the stability of deletion AUC scores across multiple runs of the same utterance. This will reveal whether 20,000 iterations is genuinely optimal or simply sufficient, and whether the method exhibits diminishing returns or instability at higher perturbation counts.

- **Check 3: Robustness to speech variability** - Evaluate SPES on accented speech (e.g., L2 English from the Accented English Speech Recognition Challenge dataset) and noisy conditions (e.g., CHiME-6 challenge data). Compare the faithfulness scores and qualitative explanation quality against the clean speech results to assess whether the method maintains its explanatory power under realistic deployment conditions where the training data assumptions may break down.