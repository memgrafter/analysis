---
ver: rpa2
title: A Question on the Explainability of Large Language Models and the Word-Level
  Univariate First-Order Plausibility Assumption
arxiv_id: '2403.10275'
source_url: https://arxiv.org/abs/2403.10275
tags:
- explanations
- words
- signal
- more
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the sensitivity of explanations for large
  language models (LLMs) to randomness in training, a gap highlighted by recent work.
  The authors introduce a statistical framework to quantify this sensitivity, defining
  signal as the variance of word attention means and noise as the mean of word attention
  variances, with the ratio forming a signal-to-noise ratio (SNR).
---

# A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption

## Quick Facts
- arXiv ID: 2403.10275
- Source URL: https://arxiv.org/abs/2403.10275
- Reference count: 10
- Primary result: LLM explanations have lower signal and higher noise than simpler models, yielding SNR < 1 under the word-level univariate plausibility assumption

## Executive Summary
This paper investigates the sensitivity of large language model (LLM) explanations to randomness in training. The authors introduce a statistical framework to quantify this sensitivity by defining signal as the variance of word attention means and noise as the mean of word attention variances, with their ratio forming a signal-to-noise ratio (SNR). Using a French news classification task, they compare explanations from a CamemBERT model (via layerwise relevance propagation) to those from a simpler logistic regression model with linguistic features. Results show that the LLM's explanations have lower signal and higher noise than the simpler model, yielding an SNR below one, indicating that individual model explanations are less informative due to high variability across equivalent models.

## Method Summary
The authors train 100 equivalent CamemBERT models on the InfOpinion dataset of French news articles, selecting models with statistically equivalent accuracies. For each of 20 test texts, they generate word-level attention maps using layerwise relevance propagation for the CamemBERT models and linguistic attention for a logistic regression model. They compute signal as the variance of word attention means and noise as the mean of word attention variances across models, with their ratio forming the SNR. The methodology relies on the (1, 1, 1) plausibility assumption that word-level univariate explanations are interpretable by humans.

## Key Results
- CamemBERT explanations show consistently lower signal and higher noise than logistic regression explanations
- The SNR for CamemBERT is below one, indicating less informative explanations
- Normalizing explanations (ensuring same sparsity and normalization) reduces but does not eliminate the gap
- Signal decreases with increasing numbers of equivalent models, suggesting averaging flattens attention distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper quantifies the sensitivity of LLM explanations to training randomness by defining signal as the variance of word attention means and noise as the mean of word attention variances.
- Mechanism: By splitting training randomness into multiple equivalent models and comparing their word-level attention maps, the authors measure how much attention patterns shift across models. If the variance of the average attention across words (signal) is small relative to the average variance across models for each word (noise), the explanation is deemed less informative.
- Core assumption: Word-level univariate explanations are interpretable by humans (the "(1, 1, 1) plausibility assumption").
- Evidence anchors:
  - [abstract] "defining signal as the variance of word attention means and noise as the mean of word attention variances, with the ratio forming a signal-to-noise ratio (SNR)"
  - [section] "we suggest a quantification of such explanations' sensitivity to the training randomness based on a Signal-to-Noise Ratio (SNR). For this purpose, we (preliminarily) assume that only the mean of word-level univariate explanations can be understood by a target audience"
- Break condition: If humans can interpret more complex explanations (higher-order moments or word-tuple combinations), the signal/noise definitions would need to change.

### Mechanism 2
- Claim: Transformer-based models (CamemBERT) produce less informative explanations than simpler logistic regression models under the (1, 1, 1) assumption.
- Mechanism: The paper trains 100 equivalent CamemBERT models and one logistic regression model, then computes signal and noise metrics. The CamemBERT's signal is consistently lower and its noise higher, leading to SNR < 1, while the logistic regression model has higher signal and zero noise.
- Core assumption: Equivalent models (same accuracy, different training randomness) can be meaningfully compared for explanation stability.
- Evidence anchors:
  - [abstract] "the explanations of simple feature-based models carry more signal and less noise than those of transformer ones"
  - [section] "the simpler feature-based model tends to provide more informative explanations than the CamemBERT ones, in the statistical sense captured by our definitions of signal and noise"
- Break condition: If equivalent models are not truly equivalent in their decision boundaries, the comparison would be invalid.

### Mechanism 3
- Claim: Normalizing explanations (ensuring same number of non-zero weights summing to one) reduces but does not eliminate the gap in informativeness between CamemBERT and logistic regression models.
- Mechanism: Post-processing CamemBERT attention maps to have the same sparsity and normalization as logistic regression attention maps reduces the variance in attention weights, thereby reducing noise and increasing signal, but the CamemBERT still remains less informative.
- Core assumption: Normalization preserves the relative importance of words in the explanation.
- Evidence anchors:
  - [section] "Figure 5e shows the signal of normalized explanations, where we ensure that the explanations of the CamemBERT and feature-based models have the same number of words with non-zero weights and that these weights sum to one"
- Break condition: If normalization fundamentally alters the explanation semantics, the comparison becomes invalid.

## Foundational Learning

- Concept: Signal-to-Noise Ratio (SNR) in statistical signal processing
  - Why needed here: The paper uses SNR to quantify how much of the explanation variance is due to true signal (word importance) versus noise (randomness in training)
  - Quick check question: If the variance of the mean attention across words is 0.01 and the mean variance of attention across models is 0.04, what is the SNR?

- Concept: Layerwise Relevance Propagation (LRP)
  - Why needed here: LRP is the method used to generate word-level attention maps from the CamemBERT model
  - Quick check question: What constraint does LRP enforce when backpropagating relevance from the output layer?

- Concept: Equivalent models and compatible inputs
  - Why needed here: The paper's methodology relies on generating multiple models with the same accuracy but different training randomness, and finding inputs where all models agree
  - Quick check question: Why are compatible inputs necessary for comparing explanations across equivalent models?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training (CamemBERT + logistic regression) -> Explanation generation (LRP + linguistic attention) -> Signal/noise computation -> SNR analysis
- Critical path: Training equivalent models -> Generating explanations -> Computing signal and noise metrics -> Interpreting SNR values
- Design tradeoffs: Simple univariate explanations are more interpretable but may miss complex interactions; complex explanations may be more informative but less plausible
- Failure signatures: SNR < 1 indicates explanations are dominated by noise; signal decreases with more equivalent models indicates averaging flattens attention
- First 3 experiments:
  1. Reproduce the boxplots of attention maps for a single text across equivalent models
  2. Compute signal and noise metrics for both CamemBERT and logistic regression on the same texts
  3. Apply normalization to CamemBERT explanations and recompute metrics to verify the claim in Mechanism 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of the training randomness on the explainability of large language models (LLMs) compared to simpler models?
- Basis in paper: [explicit] The paper compares the signal-to-noise ratio (SNR) of explanations from LLMs and simpler models, showing that LLM explanations have lower signal and higher noise.
- Why unresolved: The paper's findings are based on a single classification task, and it's unclear if the results generalize to other tasks or datasets.
- What evidence would resolve it: Additional experiments on diverse datasets and tasks would clarify the generalizability of the findings.

### Open Question 2
- Question: How can more complex explanations improve the explainability of LLMs without sacrificing plausibility for human readers?
- Basis in paper: [explicit] The authors discuss the possibility of using more complex explanations, such as t-tuples of words or higher-order statistics, but question the tradeoff with plausibility.
- Why unresolved: The paper does not provide empirical evidence on the effectiveness of these complex explanations or their impact on human understanding.
- What evidence would resolve it: User studies evaluating the comprehension and trust in complex explanations would provide insights into their plausibility.

### Open Question 3
- Question: Is there a fundamental tradeoff between accuracy and explainability in LLMs, or can both be achieved simultaneously?
- Basis in paper: [explicit] The authors question whether the excellent accuracy of LLMs can be combined with equally plausible explanations as simpler models.
- Why unresolved: The paper does not explore alternative explanation methods or architectural changes that might bridge the gap between accuracy and explainability.
- What evidence would resolve it: Research into novel explanation techniques or model architectures that enhance explainability without compromising accuracy would address this question.

## Limitations

- The (1, 1, 1) plausibility assumption may not hold universally across different audiences or tasks
- Results are based on a single dataset (French news classification) and one explanation method (LRP)
- The definition of "equivalent models" relies on statistical equivalence in accuracy, but models could differ in decision boundaries despite similar performance
- The logistic regression comparison uses handcrafted linguistic features that may not represent typical feature-based models

## Confidence

**High Confidence**: The core methodology for computing signal-to-noise ratios using variance of attention means versus mean of attention variances is mathematically sound and well-defined. The finding that CamemBERT explanations show lower signal and higher noise than logistic regression under the (1, 1, 1) assumption is reproducible given the methodology.

**Medium Confidence**: The interpretation that low SNR indicates less informative explanations depends on accepting the underlying plausibility assumption. The claim about normalization reducing but not eliminating the informativeness gap requires careful validation of the normalization procedure.

**Low Confidence**: Broader claims about LLM explainability requiring fundamentally different approaches beyond word-level univariate analysis, while plausible, extend beyond the empirical scope of this study.

## Next Checks

1. **Cross-task validation**: Apply the SNR framework to different classification tasks (sentiment analysis, named entity recognition) to test generalizability beyond the French news classification domain.

2. **Alternative explanation methods**: Compare SNR results across different explanation techniques (SHAP, Integrated Gradients, attention rollout) to determine if the pattern holds regardless of the attribution method used.

3. **Human evaluation study**: Conduct a user study where human annotators rate the plausibility of explanations with different SNR values to empirically validate the (1, 1, 1) assumption and its relationship to perceived explanation quality.