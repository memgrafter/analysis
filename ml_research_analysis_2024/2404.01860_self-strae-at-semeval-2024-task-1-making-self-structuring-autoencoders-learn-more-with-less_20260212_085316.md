---
ver: rpa2
title: 'Self-StrAE at SemEval-2024 Task 1: Making Self-Structuring AutoEncoders Learn
  More With Less'
arxiv_id: '2404.01860'
source_url: https://arxiv.org/abs/2404.01860
tags:
- self-strae
- objective
- embeddings
- contrastive
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents two improvements to the Self-Structuring AutoEncoder
  (Self-StrAE) for learning hierarchical embeddings. First, adding reconstruction
  to the vocabulary as an auxiliary objective improves embedding quality.
---

# Self-StrAE at SemEval-2024 Task 1: Making Self-Structuring AutoEncoders Learn More With Less

## Quick Facts
- arXiv ID: 2404.01860
- Source URL: https://arxiv.org/abs/2404.01860
- Reference count: 6
- Key outcome: Improved Self-StrAE with reconstruction objective and multi-channel embeddings, achieving good performance across English, Spanish, and Afrikaans with minimal parameters

## Executive Summary
This paper presents two key improvements to Self-Structuring AutoEncoders (Self-StrAE) for learning hierarchical embeddings. The authors demonstrate that adding reconstruction to the vocabulary as an auxiliary objective improves embedding quality by providing additional meaningful information beyond organizing representations alone. They also show that increasing the number of independent channels while reducing their size leads to significant improvements in embedding quality while reducing the total number of parameters, even down to just seven non-embedding parameters. The system can be pre-trained from scratch with as little as 10M tokens and performs well across English, Spanish, and Afrikaans.

## Method Summary
The authors improve Self-StrAE by adding reconstruction as an auxiliary objective and increasing the number of independent channels while reducing their size. The model uses a 256-dimensional embedding space with k independent channels of size u. Training combines cross-entropy and contrastive loss objectives (CECO) to optimize both token-level reconstruction and global structure. The system is pre-trained on 10M tokens of monolingual text using BPEMB tokenization, with Adam optimizer (lr=1e-3), batch size 512, and temperature τ=1.2. Evaluation is performed on English semantic similarity datasets including Simlex, Wordsim353, STS-B, and SemRel.

## Key Results
- Adding reconstruction as auxiliary objective improves embedding quality compared to contrastive learning alone
- Increasing number of channels (k=128) while reducing channel size (u=2) improves performance while reducing parameters
- System achieves strong results across English, Spanish, and Afrikaans with minimal parameter count (seven non-embedding parameters)
- CECO (combined cross-entropy and contrastive loss) performs better than either objective individually

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding reconstruction to the vocabulary as an auxiliary objective improves representation quality by providing additional meaningful information compared to organizing representations alone.
- Mechanism: The auxiliary cross-entropy objective provides explicit supervision for token-level reconstruction, which regularizes the embedding space and helps the model learn more discriminative features that capture lexical semantics.
- Core assumption: Token-level reconstruction provides complementary information to contrastive learning objectives and helps stabilize the training process.
- Evidence anchors:
  - [abstract]: "Firstly, we show that including reconstruction to the vocabulary as an auxiliary objective improves representation quality."
  - [section]: "Firstly, including reconstruction of discrete labels inherently provides additional meaningful information compared to just organizing the representations alone."
  - [corpus]: Weak - corpus contains related autoencoder papers but none specifically discussing auxiliary reconstruction objectives for hierarchical embeddings.

### Mechanism 2
- Claim: Increasing the number of independent channels while reducing their size leads to significant improvements in embedding quality while reducing parameters.
- Mechanism: Breaking embeddings into more independent channels allows the model to capture different semantic aspects or senses of meaning separately, leading to more expressive representations. The reduction in channel size decreases computational complexity while maintaining representational capacity.
- Core assumption: Semantic meaning can be decomposed into independent components that can be captured by separate channels, and this decomposition improves overall representation quality.
- Evidence anchors:
  - [abstract]: "Secondly, we demonstrate that increasing the number of independent channels leads to significant improvements in embedding quality, while simultaneously reducing the number of parameters."
  - [section]: "Each embedding in Self-StrAE is treated as consisting of k independent channels of size u. This is intended to allow the representations to capture different senses of meaning."
  - [corpus]: Weak - corpus contains autoencoder papers but lacks specific evidence about channel decomposition benefits in hierarchical models.

### Mechanism 3
- Claim: Combining cross-entropy and contrastive loss (CECO) performs better than either objective alone by addressing different aspects of representation learning.
- Mechanism: Cross-entropy optimizes token-level reconstruction accuracy while contrastive loss optimizes the global structure of the embedding space. Together they provide complementary supervision that improves both local and global representation quality.
- Core assumption: Different objectives capture different aspects of what makes good representations, and combining them leads to better overall performance than optimizing either in isolation.
- Evidence anchors:
  - [abstract]: "We show that combining cross entropy and contrastive loss leads to better representations than applying each objective individually."
  - [section]: "However, the best setting of all is CECO (the combination of cross entropy and contrastive loss). There are two factors worth considering that may explain this finding."
  - [corpus]: Weak - corpus contains contrastive learning papers but lacks direct evidence about combining reconstruction with contrastive objectives in hierarchical models.

## Foundational Learning

- Concept: Autoencoders and reconstruction objectives
  - Why needed here: Understanding how autoencoders work and why reconstruction objectives are used is fundamental to grasping the Self-StrAE architecture and the motivation for adding auxiliary reconstruction.
  - Quick check question: What is the primary purpose of the reconstruction objective in autoencoder training, and how does it differ from contrastive learning objectives?

- Concept: Hierarchical structure learning and tree induction
  - Why needed here: Self-StrAE learns hierarchical structures over input text, so understanding how hierarchical representations work and how structures can be learned from data is crucial.
  - Quick check question: How does Self-StrAE determine which tokens or phrases to merge at each step, and what role does cosine similarity play in this process?

- Concept: Multi-channel embeddings and parameter efficiency
  - Why needed here: The paper's key innovation involves decomposing embeddings into multiple channels, so understanding how multi-channel representations work and their efficiency implications is important.
  - Quick check question: Why might breaking a single embedding into multiple smaller channels improve representation quality while reducing the total number of parameters?

## Architecture Onboarding

- Component map: Input tokenizer (BPEMB) -> Embedding matrix (ΩΨ) -> Composition function (CΦ) -> Decomposition function (DΘ) -> Dembedding layer (ΛΓ) -> Output/Prediction

- Critical path: Input → Embedding → Hierarchical composition → Structure learning → Decomposition → Output/Prediction

- Design tradeoffs:
  - More channels vs. smaller channel size: improves performance but may increase instability
  - Parameter reduction vs. representational capacity: smaller channels reduce parameters but may limit expressiveness
  - Language-specific vs. language-agnostic design: current approach works across languages but may not capture language-specific nuances

- Failure signatures:
  - High variance between random seeds when using many small channels
  - Degraded performance on certain languages despite good English results
  - Instability during training with certain objective combinations

- First 3 experiments:
  1. Test different objective combinations (cross-entropy only, contrastive only, CECO) on English to establish baseline performance
  2. Vary the number of channels and channel size to find optimal configuration
  3. Apply the best-performing configuration to Spanish and Afrikaans to test cross-lingual generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the instability in training when the number of channels is increased?
- Basis in paper: [explicit] The authors mention instability when increasing channels but do not fully explain the cause
- Why unresolved: The paper provides a potential solution but lacks a clear explanation of the underlying cause
- What evidence would resolve it: Further analysis of how dropout masks and contrastive loss interact with multiple channels

### Open Question 2
- Question: Why does Self-StrAE perform better on Afrikaans than English despite being trained on less data?
- Basis in paper: [inferred] The authors note significantly better performance on Afrikaans but do not explain why
- Why unresolved: The paper only speculates about potential reasons without providing concrete analysis
- What evidence would resolve it: Comparative linguistic analysis of Afrikaans vs English and how they interact with Self-StrAE's architecture

### Open Question 3
- Question: How well does Self-StrAE generalize to languages beyond Indo-European?
- Basis in paper: [explicit] The authors acknowledge their results are limited to Indo-European languages
- Why unresolved: The paper only tested on English, Spanish, and Afrikaans
- What evidence would resolve it: Testing Self-StrAE on non-Indo-European languages like Mandarin, Arabic, or Swahili

## Limitations

- The evaluation methodology has significant gaps, as cross-lingual generalization claims are not directly validated with Spanish and Afrikaans test data
- The comparison with other models is potentially misleading, only comparing against the authors' own ablations rather than state-of-the-art unsupervised word embedding methods
- The architectural claims about parameter efficiency don't adequately address potential quality degradation from extreme parameter reduction

## Confidence

**High Confidence:** The core finding that adding reconstruction as an auxiliary objective improves representation quality is well-supported by the ablation studies and makes intuitive sense given that it provides explicit supervision for token-level reconstruction.

**Medium Confidence:** The claim about increasing channels while reducing their size leading to better performance is supported by experimental results, but the instability issue suggests this approach may have practical limitations not fully addressed.

**Low Confidence:** The cross-lingual generalization claims are not directly validated on Spanish and Afrikaans test data, making these assertions speculative rather than empirically supported.

## Next Checks

1. **Cross-lingual validation:** Evaluate the pre-trained model directly on Spanish and Afrikaans semantic similarity test sets to verify the claimed cross-lingual generalization, rather than relying on training data performance.

2. **Comparison with baselines:** Compare against established unsupervised word embedding methods (word2vec, GloVe, fastText) and recent semantic similarity approaches to establish whether the improvements are competitive with or superior to simpler methods.

3. **Stability analysis:** Conduct a systematic study of the instability issue when using many small channels, including testing whether the StrCSE objective variant consistently resolves the problem and identifying the maximum number of channels that can be used reliably.