---
ver: rpa2
title: 'Training Language Models on the Knowledge Graph: Insights on Hallucinations
  and Their Detectability'
arxiv_id: '2408.07852'
source_url: https://arxiv.org/abs/2408.07852
tags:
- training
- data
- hallucination
- hallucinations
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies hallucinations in large language models by training
  them on structured knowledge graph data where the model's exposure to facts is fully
  controlled. The authors find that for a fixed dataset, larger and longer-trained
  models hallucinate less, but achieving low hallucination rates requires models significantly
  larger than the current "optimal" size.
---

# Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability

## Quick Facts
- arXiv ID: 2408.07852
- Source URL: https://arxiv.org/abs/2408.07852
- Authors: Jiri Hron; Laura Culp; Gamaleldin Elsayed; Rosanne Liu; Ben Adlam; Maxwell Bileschi; Bernd Bohnet; JD Co-Reyes; Noah Fiedel; C. Daniel Freeman; Izzeddin Gur; Kathleen Kenealy; Jaehoon Lee; Peter J. Liu; Gaurav Mishra; Igor Mordatch; Azade Nova; Roman Novak; Aaron Parisi; Jeffrey Pennington; Alex Rizkowsky; Isabelle Simpson; Hanie Sedghi; Jascha Sohl-dickstein; Kevin Swersky; Sharad Vikram; Tris Warkentin; Lechao Xiao; Kelvin Xu; Jasper Snoek; Simon Kornblith
- Reference count: 36
- Key outcome: This paper studies hallucinations in large language models by training them on structured knowledge graph data where the model's exposure to facts is fully controlled. The authors find that for a fixed dataset, larger and longer-trained models hallucinate less, but achieving low hallucination rates requires models significantly larger than the current "optimal" size. Increasing dataset size also increases hallucination rates due to the need to memorize more unique facts. They further study hallucination detectors and find that while larger detectors perform better, there is an inverse relationship between model scale and detectability of hallucinations - larger models are harder to detect hallucinations in despite having lower hallucination rates.

## Executive Summary
This paper investigates hallucinations in large language models by training them on structured knowledge graph data where exposure to facts is fully controlled. The authors systematically examine how hallucination rates and their detectability scale with model size and dataset size. By using a knowledge graph with unique triplets, they create a clean experimental setup where each fact appears exactly once per epoch, allowing precise control over what information models have access to. The study reveals that while larger models hallucinate less on seen data, they are paradoxically harder to detect hallucinations in, and achieving low hallucination rates requires significantly larger models than previously thought optimal.

## Method Summary
The authors train decoder-only transformer language models of varying sizes (3.15M-1.61B parameters) on a knowledge graph dataset containing [subject, predicate, object] triplets. They control exposure by ensuring each unique triplet appears exactly once per epoch. Models are trained for different epochs (1-200) on various dataset proportions (1%-100%). Hallucination rates are evaluated by prompting models with subject-predicate pairs and measuring the percentage of outputs that don't match any valid object. Hallucination detectors are trained using both sentence-level and token-level approaches to identify hallucinations from model outputs or internal representations, with performance measured using AUC-PR.

## Key Results
- Larger models trained for more epochs hallucinate less on seen knowledge graph data
- Increasing dataset size increases hallucination rates due to the need to memorize more unique facts
- There is an inverse relationship between model scale and the detectability of hallucinations - larger models are harder to detect hallucinations in despite having lower hallucination rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training LMs on knowledge graph data reduces hallucinations when the model has seen the fact before.
- Mechanism: The knowledge graph provides structured triplets where each fact appears exactly once per epoch, making it easier for the model to memorize and recall factual information without ambiguity from natural language variations.
- Core assumption: The model can effectively memorize the structured triplets when trained for sufficient epochs (20+).
- Evidence anchors:
  - [abstract] "We thus focus on studying only those hallucinations where a correct answer appears verbatim in the training set."
  - [section] "A generation is considered a hallucination if the prediction does not match any object that appears with given subject-predicate pair in the training set."
  - [corpus] Weak - no direct evidence in corpus about memorization effects
- Break condition: If the model cannot memorize due to insufficient training epochs or model size being too small relative to the dataset.

### Mechanism 2
- Claim: Larger models and longer training reduce hallucination rates on seen data but increase detectability difficulty.
- Mechanism: Larger models have better capacity to memorize facts, leading to lower hallucination rates. However, their internal representations become more complex and less distinguishable, making it harder for detectors to identify remaining hallucinations.
- Core assumption: Model scale improves memorization but also increases representation complexity.
- Evidence anchors:
  - [abstract] "While we see detector size improves performance on fixed LM's outputs, we find an inverse relationship between the scale of the LM and the detectability of its hallucinations."
  - [section] "Examining the detectors' precision-recall (PR) curves, we find an inverse relationship between the LM size and the detector's area under the PR curve (AUC-PR)."
  - [corpus] Weak - no direct evidence in corpus about detector performance vs model scale
- Break condition: If detectors can adapt to larger model representations or if simpler detection methods become available.

### Mechanism 3
- Claim: Increasing dataset size increases hallucination rates due to the need to memorize more unique facts.
- Mechanism: Unlike natural language datasets with repeated facts, knowledge graph triplets are unique, requiring the model to memorize more distinct information as dataset size grows, leading to higher hallucination rates.
- Core assumption: Each triplet in the knowledge graph represents a unique fact that must be memorized separately.
- Evidence anchors:
  - [abstract] "Increasing dataset size also increases hallucination rates due to the need to memorize more unique facts."
  - [section] "The speciality to this setup is that each unique piece of information is only seen once per epoch: each triplet contains a unique piece of information that is distinctive from any other triplet."
  - [corpus] Weak - no direct evidence in corpus about dataset size effects
- Break condition: If repetition of facts in the dataset or other learning mechanisms reduce the memorization burden.

## Foundational Learning

- Concept: Knowledge Graph Structure and Representation
  - Why needed here: Understanding how knowledge graphs are structured as [subject, predicate, object] triplets is crucial for grasping the dataset construction and evaluation methodology.
  - Quick check question: How does the structure of knowledge graph triplets differ from natural language sentences in terms of information content and repetition?

- Concept: Memorization vs. Generalization in Neural Networks
  - Why needed here: The paper explores the trade-off between memorizing facts (reducing hallucinations on seen data) and generalizing to unseen data, which is central to understanding the scaling behavior.
  - Quick check question: Why does training for more epochs reduce hallucinations on seen data but potentially increase them on unseen data?

- Concept: Precision-Recall Curves and AUC-PR
  - Why needed here: The paper uses AUC-PR to evaluate hallucination detectors, which requires understanding how this metric differs from accuracy and why it's more appropriate for imbalanced datasets.
  - Quick check question: Why is AUC-PR a better metric than accuracy for evaluating hallucination detectors when hallucination rates vary across models?

## Architecture Onboarding

- Component map:
  Knowledge Graph Dataset → LM Training Pipeline → Hallucination Evaluation → Hallucination Detection

- Critical path:
  1. Construct knowledge graph dataset with controlled triplets
  2. Train LMs of varying sizes for different numbers of epochs
  3. Evaluate hallucination rates on seen and unseen data
  4. Train hallucination detectors on generated outputs
  5. Analyze detection performance vs. model scale

- Design tradeoffs:
  - Model size vs. hallucination rate: Larger models hallucinate less but are harder to detect
  - Epochs vs. generalization: More epochs reduce hallucinations on seen data but hurt generalization
  - Dataset size vs. memorization: Larger datasets increase hallucination rates due to more unique facts
  - Detection task type: Token-level detection generally outperforms sentence-level but requires different detector architectures

- Failure signatures:
  - Hallucination rates not decreasing with model size: Possible issues with training configuration or insufficient epochs
  - Detectors performing poorly: May indicate issues with detection task formulation or detector architecture
  - Generalization dropping significantly with more epochs: Could suggest overfitting to training data

- First 3 experiments:
  1. Train a small model (3.15M parameters) on 1% of the dataset for 1 epoch, evaluate hallucination rate
  2. Train the same model for 20 epochs, evaluate hallucination rate and compare to 1 epoch
  3. Train a larger model (25.2M parameters) on 1% of the dataset for 1 epoch, evaluate hallucination rate and compare to small model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scaling relationship between model size and dataset size for minimizing hallucinations differ from the current "optimal" scaling laws for loss minimization?
- Basis in paper: [explicit] The paper states that achieving ≤5% hallucination rate on training data requires an order of magnitude larger model than Hoffmann et al. (2022) reported was optimal for loss minimization.
- Why unresolved: The paper only establishes this discrepancy exists but doesn't provide a detailed analysis of why the scaling laws differ or what the exact relationship is.
- What evidence would resolve it: Systematic experiments varying both model size and dataset size independently, with detailed analysis of how hallucination rate scales with each dimension.

### Open Question 2
- Question: What is the mechanism behind the inverse relationship between model scale and hallucination detectability?
- Basis in paper: [explicit] The paper observes that larger models have lower hallucination rates but are harder to detect hallucinations in, despite having control over training data exposure.
- Why unresolved: The paper identifies this phenomenon but doesn't explain the underlying reason for why larger models are harder to detect.
- What evidence would resolve it: Investigation into whether larger models have more subtle or distributed representations of knowledge that make hallucinations less distinguishable from correct outputs.

### Open Question 3
- Question: How does the trade-off between hallucination rate and generalization manifest differently across various model architectures and training objectives?
- Basis in paper: [explicit] The paper observes that longer training reduces hallucination rates but can hurt generalization to unseen data.
- Why unresolved: The paper only examines this trade-off for transformer decoder-only models trained with next-token prediction.
- What evidence would resolve it: Comparative studies of different architectures (e.g., encoder-decoder, retrieval-augmented) and training objectives to see how they balance memorization vs. generalization.

## Limitations

- The controlled knowledge graph setup may not capture the complexity of natural language hallucinations where facts can be paraphrased or stated in multiple ways
- The extensive training required for low hallucination rates (20+ epochs) likely leads to severe overfitting, making these models impractical for real-world deployment
- Hallucination detectors trained on model outputs may learn model-specific patterns rather than general hallucination indicators

## Confidence

**High Confidence Claims:**
- Larger models trained for more epochs hallucinate less on seen knowledge graph data
- Increasing dataset size increases hallucination rates due to the need to memorize more unique facts

**Medium Confidence Claims:**
- The inverse relationship between model scale and hallucination detectability
- Knowledge graph data provides a clean setup for studying hallucinations

**Low Confidence Claims:**
- The practical implications of these findings for real-world language models

## Next Checks

**Validation Check 1: Generalization Assessment with Controlled Epochs**
Train larger models (100M+ parameters) on the knowledge graph dataset for varying epochs (1-20) and evaluate both hallucination rates on seen data and generalization performance on unseen data. This would help quantify the trade-off between hallucination reduction and generalization loss.

**Validation Check 2: Natural Language Hallucination Study**
Replicate the experimental framework using a natural language dataset where facts are paraphrased or stated in multiple ways (e.g., using multiple-choice QA datasets or fact verification datasets). This would test whether the scaling laws observed in knowledge graph data hold in more realistic settings.

**Validation Check 3: Alternative Detection Methodology Evaluation**
Implement and evaluate alternative hallucination detection approaches that don't rely on training detectors on model outputs, such as using external knowledge bases for verification or employing ensemble methods with diverse model architectures. This would help determine whether the inverse relationship between model scale and detectability is fundamental.