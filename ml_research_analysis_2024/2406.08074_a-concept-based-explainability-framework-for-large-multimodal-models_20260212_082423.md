---
ver: rpa2
title: A Concept-Based Explainability Framework for Large Multimodal Models
arxiv_id: '2406.08074'
source_url: https://arxiv.org/abs/2406.08074
tags:
- concept
- concepts
- token
- multimodal
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel framework for interpreting internal
  representations of large multimodal models (LMMs) by learning multimodal concepts
  through dictionary learning. The method extracts representations of a target token
  across many images, decomposes them into a dictionary of concepts, and grounds each
  concept semantically in both visual and textual domains.
---

# A Concept-Based Explainability Framework for Large Multimodal Models

## Quick Facts
- arXiv ID: 2406.08074
- Source URL: https://arxiv.org/abs/2406.08074
- Authors: Jayneel Parekh; Pegah Khayatan; Mustafa Shukor; Alasdair Newson; Matthieu Cord
- Reference count: 40
- Primary result: Novel framework extracts multimodal concepts from LMM representations via dictionary learning, showing semantically meaningful concepts that improve interpretability

## Executive Summary
This paper introduces a novel framework for interpreting internal representations of large multimodal models (LMMs) by learning multimodal concepts through dictionary learning. The method extracts representations of a target token across many images, decomposes them into a dictionary of concepts, and grounds each concept semantically in both visual and textual domains. Experiments with DePALM and LLaVA models show that the extracted concepts are diverse, semantically meaningful, and useful for interpreting test samples.

The approach addresses a critical gap in multimodal AI interpretability by providing a method to extract and analyze the semantic concepts that LMMs use to represent tokens across visual and textual modalities. By using Semi-NMF for dictionary learning and evaluating concepts through CLIPScore and BERTScore, the framework offers a quantitative and qualitative assessment of concept quality and interpretability.

## Method Summary
The method extracts representations of a target token from a deep layer in the LMM for images where the token appears in both prediction and ground truth. These representations form a matrix Z that is decomposed using Semi-NMF into basis vectors (concepts) U and activations V, with non-negativity constraint on V. Each concept is grounded visually by selecting the most activating images and textually by decoding through the unembedding matrix. For test samples, the method projects their representations onto the learned concept dictionary and identifies the most activating concepts to provide interpretable insights.

## Key Results
- Extracted concepts show high CLIPScore (0.58-0.66) and BERTScore (0.65-0.71) between visual and textual groundings
- Concept overlap remains low (0.11-0.21), indicating diverse and disentangled concepts
- The method outperforms baselines like PCA, K-means, and nearest-neighbor approaches
- Test sample interpretation reveals semantically relevant concepts (e.g., "bus" concepts include "urban," "traffic," "vehicle")

## Why This Works (Mechanism)

### Mechanism 1
The learned dictionary elements are semantically grounded in both vision and text because the model's internal representations of tokens aggregate multimodal information across layers. The method extracts representations of a target token from a deep layer in the LMM. These representations combine visual and textual information due to the attention mechanisms in the LLM, which allow visual tokens to influence the text token representations. The dictionary learning then decomposes these multimodal representations into concepts that capture different aspects of the token.

### Mechanism 2
The Semi-NMF variant is effective because it enforces non-negative combinations of concepts while allowing mixed values in the basis vectors, which is suitable for the mixed-value representation matrix. The optimization problem minimizes ||Z - UV||^2_F + λ||V||_1 subject to V ≥ 0 and ||uk||_2 ≤ 1. This forces the activations to be non-negative (interpretable combinations) while allowing the basis vectors to have mixed values (necessary since Z is not non-negative).

### Mechanism 3
The method is useful for interpreting test samples because it projects their representations onto the learned concept dictionary and identifies the most activating concepts, which are semantically related to the image content. For a test image X where the target token is predicted, the method extracts its representation zX, projects it onto the concept dictionary U* to get activations v(X), and identifies the top r most activating concepts. The multimodal grounding of these concepts (visual and textual) provides interpretable insights into why the token was predicted.

## Foundational Learning

- **Concept: Dictionary learning and its variants (PCA, K-Means, NMF, Semi-NMF)**
  - Why needed here: The method uses dictionary learning to decompose the representation matrix into concepts. Understanding the different variants and their properties is crucial for choosing the appropriate method (Semi-NMF in this case).
  - Quick check question: What is the key difference between NMF and Semi-NMF, and why is Semi-NMF more suitable for this application?

- **Concept: Residual stream view of transformer representations**
  - Why needed here: The method relies on extracting representations from the residual stream in the LLM. Understanding how information flows and is processed in the residual stream is essential for interpreting the learned concepts.
  - Quick check question: How does the residual stream view explain the aggregation of multimodal information in the token representations?

- **Concept: Multimodal grounding and its evaluation**
  - Why needed here: The method grounds the learned concepts in both visual and textual domains. Understanding how to evaluate this grounding (using CLIPScore and BERTScore) is crucial for assessing the quality of the concepts.
  - Quick check question: How do CLIPScore and BERTScore measure the correspondence between the visual and textual grounding of a concept?

## Architecture Onboarding

- **Component map**: Visual encoder (frozen) -> Connector -> LLM (frozen) -> Dictionary learning module
- **Critical path**: Extract token representations -> Decompose into concepts -> Ground concepts in vision and text -> Use for interpreting test samples
- **Design tradeoffs**: Using a frozen LLM limits the ability to adapt the model for concept extraction, but allows for understanding the generalization of the LLM to multimodal inputs. The choice of dictionary learning method (Semi-NMF) balances interpretability and flexibility.
- **Failure signatures**: Poor CLIPScore/BERTScore on test samples indicates that the concepts are not well-grounded or not useful for interpretation. High overlap between grounded words indicates that the concepts are not diverse or disentangled.
- **First 3 experiments**:
  1. Replicate the concept extraction for a different target token (e.g., "Cat") and compare the results to the "Dog" token.
  2. Vary the number of concepts K and analyze the effect on the reconstruction error, CLIPScore/BERTScore, and overlap.
  3. Apply the method to a different LMM (e.g., LLaVA) and compare the results to the DePALM model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of number of concepts K affect the interpretability and performance of the concept dictionary learning method for LMMs?
- Basis in paper: [explicit] The paper discusses reconstruction error variation with K and presents ablation study results for K ∈ {10, 20, 30, 50}.
- Why unresolved: While the paper shows that K = 20 is a reasonable choice, it doesn't provide definitive guidance on how to choose K for different LMMs or tasks. The optimal number of concepts may vary depending on the specific model architecture, dataset, and target token.
- What evidence would resolve it: A comprehensive study varying K across different LMM architectures, datasets, and target tokens, coupled with human evaluation of concept interpretability, would help establish guidelines for choosing K.

### Open Question 2
- Question: Can the concept dictionary learning method be extended to capture shared concepts across multiple tokens or classes?
- Basis in paper: [inferred] The paper mentions that current concept dictionaries are token-specific and suggests this as a potential direction for future work.
- Why unresolved: The paper focuses on extracting concepts for individual tokens, but it's unclear how the method would perform when trying to learn a shared concept dictionary across multiple tokens or classes. This could potentially reveal more general patterns in LMM representations.
- What evidence would resolve it: Experiments applying the method to learn shared concept dictionaries across multiple tokens or classes, with evaluation of concept interpretability and performance, would demonstrate the feasibility and benefits of this approach.

### Open Question 3
- Question: How do the concept representations learned by this method compare to other interpretability techniques for LMMs, such as feature visualization or attention-based methods?
- Basis in paper: [explicit] The paper mentions related work on interpreting LMM representations but doesn't directly compare its approach to these other methods.
- Why unresolved: While the paper demonstrates the effectiveness of its concept-based approach, it doesn't provide a direct comparison to other interpretability techniques for LMMs. Understanding the relative strengths and weaknesses of different methods would help researchers choose the most appropriate approach for their specific needs.
- What evidence would resolve it: A comparative study evaluating the concept-based method against other interpretability techniques (e.g., feature visualization, attention-based methods) on the same LMMs and tasks, using both quantitative metrics and human evaluation, would provide insights into the relative performance and interpretability of these approaches.

## Limitations

- The method relies on a critical assumption that residual stream representations in deep layers of frozen LMMs contain aggregated multimodal information about tokens, but this assumption is not directly validated.
- The Semi-NMF formulation introduces constraints that may artificially improve interpretability metrics without genuinely capturing semantic concepts.
- The grounding evaluation using CLIPScore and BERTScore measures statistical similarity between image features and word embeddings, but does not establish genuine semantic correspondence.

## Confidence

- **High Confidence**: The experimental methodology for extracting representations, applying dictionary learning, and computing quantitative metrics is clearly specified and reproducible. The reconstruction error curves and concept grounding results show consistent patterns across different target tokens.
- **Medium Confidence**: The claim that extracted concepts are semantically meaningful and useful for interpretation is supported by qualitative examples but lacks rigorous validation. The paper shows that concepts have high CLIPScore/BERTScore and low overlap, but does not demonstrate that these concepts actually help humans understand model decisions or improve downstream performance.
- **Low Confidence**: The assertion that the method reveals how LMMs "encode multimodal information" about tokens overstates what the evidence shows. The paper demonstrates that concepts have visual and textual components with high similarity scores, but does not prove that these concepts correspond to how the model actually processes multimodal information during inference.

## Next Checks

1. **Ablation on visual input**: Run the concept extraction pipeline on the same model with visual input randomly masked or removed at various layers. If concepts remain equally well-grounded, this would suggest the method is capturing textual semantics rather than true multimodal concepts.

2. **Concept intervention experiment**: Modify the top activating images for a concept by adding/removing the grounded visual attributes and measure whether this changes the model's prediction probability for the target token. This would test whether the concepts actually influence model behavior.

3. **Cross-modal consistency test**: For each concept, extract the top textual neighbors using BERTScore and top visual neighbors using CLIP, then measure the consistency between these two sets. If the method truly captures multimodal concepts, the intersection of textual and visual neighbors should be meaningful and stable across different evaluation runs.