---
ver: rpa2
title: Improved Few-Shot Image Classification Through Multiple-Choice Questions
arxiv_id: '2407.16145'
source_url: https://arxiv.org/abs/2407.16145
tags:
- image
- classification
- language
- performance
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve few-shot image classification
  using Visual Question Answering (VQA) models, specifically BLIP-2. The key idea
  is to use multiple-choice questions to extract prompt-specific latent representations
  from the VQA model, which are enriched with relevant visual information.
---

# Improved Few-Shot Image Classification Through Multiple-Choice Questions

## Quick Facts
- arXiv ID: 2407.16145
- Source URL: https://arxiv.org/abs/2407.16145
- Authors: Dipika Khullar; Emmett Goodman; Negin Sokhandan
- Reference count: 0
- Primary result: Training-free few-shot image classification using VQA models with multiple-choice prompts outperforms both vision-only classifiers and zero-shot VQA models

## Executive Summary
This paper introduces a novel approach to few-shot image classification that leverages Visual Question Answering (VQA) models, specifically BLIP-2, by using multiple-choice questions to extract task-specific latent representations. The method extracts prompt-specific embeddings from different stages of the VQA architecture, combining visual encoder features with LLM-decoder representations to create enriched image embeddings. These embeddings are then used for classification through nearest-neighbor lookup in a latent space defined by class prototypes from few labeled examples. The approach demonstrates significant performance improvements across standard few-shot classification benchmarks including MiniImageNet, Caltech-UCSD Birds, and CIFAR-100, while maintaining the flexibility and dynamic advantages of VQA models.

## Method Summary
The approach extracts four latent representations from the BLIP-2 VQA model (visual encoder, Q-Former, LLM-encoder, and LLM-decoder) using model hooks, averages across non-final dimensions, and concatenates the visual encoder and LLM-decoder embeddings to create a 3456-dimensional feature vector. For few-shot classification, class prototypes are computed by averaging the embeddings of the few labeled examples per class, and test images are classified via nearest-neighbor lookup using Euclidean distance. The method is training-free and relies on carefully crafted multiple-choice prompts to steer the VQA model toward relevant visual attributes for each classification task.

## Key Results
- Outperforms both vision-only image classifiers and zero-shot VQA models on standard few-shot benchmarks
- Shows particular strength in settings with numerous diverse visual attributes, such as clothing classification
- Maintains the dynamic and flexible advantages of VQA models while being training-free
- The second token embedding from the LLM-decoder provides the most informative representation for classification

## Why This Works (Mechanism)

### Mechanism 1
The VQA model produces task-specific image embeddings enriched with semantic information relevant to the classification task. Multiple-choice question prompts steer the VQA model to focus on specific visual attributes important for classification, with the prompt flowing through the architecture and influencing latent representations at different stages. The core assumption is that language prompts can effectively guide the VQA model to extract relevant visual features. Evidence shows that prompt-specific latent representations are enriched with relevant visual information. If prompts are too generic or unrelated to the task, the VQA model may not extract relevant features.

### Mechanism 2
The LLM-decoder representation captures the most relevant semantic information for classification. The LLM-decoder takes visual features and the language prompt to produce a token sequence, with the second token embedding being particularly informative. The core assumption is that the second token embedding contains rich semantic information useful for classification. Evidence demonstrates that different embedding slices perform dramatically differently, with the second token embedding showing optimal performance. If the second token embedding is not consistently informative across tasks, the method may not generalize well.

### Mechanism 3
Concatenating the LLM-decoder embedding with the visual encoder embedding improves classification performance by combining low-level visual features with high-level semantic information. The core assumption is that this combination provides a more comprehensive image representation. Evidence shows that concatenating these embeddings creates a multi-modal representation that performs well across tasks, particularly for settings with diverse visual attributes. If the visual encoder embedding does not provide complementary information, concatenation may not improve performance.

## Foundational Learning

- **Visual Question Answering (VQA) models**
  - Why needed here: The paper leverages VQA models to perform few-shot image classification using latent representations
  - Quick check question: What are the main components of a VQA model and how do they interact?

- **Few-shot learning**
  - Why needed here: The paper addresses image classification with very few labeled examples per class
  - Quick check question: How does few-shot learning differ from traditional supervised learning?

- **Prompt engineering**
  - Why needed here: The paper uses carefully crafted multiple-choice questions to steer the VQA model and extract relevant features
  - Quick check question: How can the choice of prompt affect the performance of a VQA model in few-shot image classification?

## Architecture Onboarding

- **Component map**: Input image and multiple-choice question -> ViT-L/14 visual encoder (extracts visual features) -> Q-Former (bridges modalities) -> Flan-T5-XL LLM (encoder/decoder produces outputs) -> Model hooks (extract latent representations)

- **Critical path**: 1) Input image and multiple-choice question 2) Visual features extracted by ViT-L/14 visual encoder 3) Q-Former reformulates image embedding in terms of LLM language embeddings 4) LLM-encoder and LLM-decoder produce language outputs and latent representations 5) Extract second token embedding from LLM-decoder 6) Concatenate LLM-decoder embedding with visual encoder embedding 7) Use nearest-neighbor lookup in latent space for classification

- **Design tradeoffs**: Using VQA models allows flexible and dynamic classification but may be sensitive to prompt choice; extracting latent representations provides trade-off between low-level visual features and high-level semantic information; concatenating embeddings improves performance but increases dimensionality

- **Failure signatures**: Poor classification performance may indicate ineffective prompt steering or uninformative extracted representations; overfitting may occur with too few labeled examples or excessive embedding dimensionality

- **First 3 experiments**: 1) Test performance of different latent representations on simple classification task 2) Compare zero-shot and few-shot classification using proposed method 3) Evaluate impact of concatenating LLM-decoder embedding with visual encoder embedding

## Open Questions the Paper Calls Out

- **Open Question 1**: How does performance vary across different VQA model architectures beyond BLIP-2? The paper claims techniques should translate to new VQA models but only evaluates BLIP-2. Comparative experiments with other VQA models like PaLI, GIT, or MiniGPT-4 would show if performance gains generalize across architectures.

- **Open Question 2**: What is the optimal strategy for selecting multiple-choice prompts to maximize few-shot classification performance? While the paper demonstrates prompt sensitivity, it does not provide systematic methods for prompt selection or optimization. A study examining prompt engineering strategies would clarify optimal prompt design.

- **Open Question 3**: Why do certain slices of LLM-decoder embeddings perform significantly better than others for few-shot classification? The paper observes dramatic performance variation across embedding slices but does not explain why the second embedding is optimal or what semantic content distinguishes high-performing slices.

## Limitations
- Method relies heavily on quality of multiple-choice prompts, which are not fully specified for all datasets
- May struggle with tasks requiring fine-grained visual distinctions where language prompts cannot capture subtle differences
- Nearest-neighbor classification assumes linear separability in embedding space, which may not hold for complex class boundaries

## Confidence
- High confidence: Core claim that multiple-choice questions can effectively steer VQA models to produce task-specific image embeddings
- Medium confidence: Generalizability of approach to other VQA architectures or fine-grained visual distinction tasks
- Medium confidence: Claim of outperforming pure visual encoders and zero-shot VQA baselines

## Next Checks
1. **Prompt ablation study**: Systematically vary multiple-choice question prompts across different semantic focuses on a single dataset to quantify how prompt formulation affects classification accuracy, measuring standard deviation across prompt variations.

2. **Cross-architecture validation**: Implement same methodology using different VQA architecture (e.g., Flamingo or GIT) to verify performance gains are not specific to BLIP-2, comparing classification accuracy and embedding similarity across models.

3. **Fine-grained classification test**: Evaluate method on dataset requiring fine-grained visual distinctions (e.g., Stanford Cars or Food-101) where semantic descriptions may not capture subtle visual differences, measuring performance degradation compared to simpler classification tasks.