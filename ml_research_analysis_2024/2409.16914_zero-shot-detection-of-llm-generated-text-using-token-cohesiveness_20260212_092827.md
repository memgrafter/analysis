---
ver: rpa2
title: Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness
arxiv_id: '2409.16914'
source_url: https://arxiv.org/abs/2409.16914
tags:
- tocsin
- text
- token
- cohesiveness
- fast-detectgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new approach, TOCSIN, for zero-shot detection
  of LLM-generated text. It introduces the concept of token cohesiveness, defined
  as the expected semantic difference between input text and its copies after random
  token deletion.
---

# Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness

## Quick Facts
- arXiv ID: 2409.16914
- Source URL: https://arxiv.org/abs/2409.16914
- Authors: Shixuan Ma; Quan Wang
- Reference count: 25
- Key outcome: TOCSIN achieves 10.97%, 8.26%, 5.88%, and 0.59% average AUROC improvements over four baseline detectors in white-box settings, with even greater improvements of 16.46%, 13.48%, 11.19%, and 1.87% in black-box settings.

## Executive Summary
This paper introduces TOCSIN, a novel zero-shot approach for detecting LLM-generated text by leveraging token cohesiveness - a measure of semantic similarity among tokens in a passage. The key insight is that LLM-generated text exhibits higher token cohesiveness than human-written text due to the token-by-token generation process where each token conditions on all preceding tokens. TOCSIN combines an existing zero-shot detector with a token cohesiveness calculation module in a dual-channel architecture, achieving consistent improvements across multiple datasets and detector types.

## Method Summary
TOCSIN calculates token cohesiveness by measuring the expected semantic difference between input text and its copies after random token deletion. The approach creates n perturbed versions of the input by randomly deleting ρ proportion of tokens, then computes the semantic difference between each perturbed version and the original using negative BARTScore. The average of these semantic differences forms the token cohesiveness score. TOCSIN combines this with an existing zero-shot detector using a dual-channel architecture with exponential score scaling, making it particularly effective in black-box settings where source models are unavailable.

## Key Results
- TOCSIN achieves new best detection accuracy compared to existing zero-shot detectors
- Average AUROC improvements of 10.97%, 8.26%, 5.88%, and 0.59% over four baseline detectors in white-box settings
- Even greater improvements of 16.46%, 13.48%, 11.19%, and 1.87% in black-box settings
- Particularly effective for black-box detection as it doesn't require source model access

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token cohesiveness measures how closely tokens in a passage are semantically related to each other.
- Mechanism: Random token deletion creates perturbed versions of the input text, and the semantic difference between the original and perturbed versions is measured using negative BARTScore. LLM-generated text shows higher token cohesiveness because each token is generated conditioned on all preceding tokens, creating tighter semantic relationships.
- Core assumption: The generation process of LLMs inherently creates tighter semantic relationships between tokens compared to human writing.
- Evidence anchors:
  - [abstract] "Token cohesiveness is defined as the expected semantic difference between input text x and its copies after random token deletion"
  - [section 3.2] "Token cohesiveness essentially measures the semantic closeness among tokens in the passage. The closer the tokens are semantically related to each other, the higher the token cohesiveness would be."
  - [corpus] Weak evidence - no direct citations or comparisons in corpus papers
- Break condition: If human writing processes start incorporating more sequential conditioning similar to LLM generation, the token cohesiveness gap would narrow.

### Mechanism 2
- Claim: Combining token cohesiveness with existing zero-shot detectors provides complementary information that improves overall detection accuracy.
- Mechanism: TOCSIN uses a dual-channel approach where one channel computes token cohesiveness and the other uses an existing detector. The scores are combined multiplicatively with exponential scaling based on the sign of the existing detector's score.
- Core assumption: Token cohesiveness captures information orthogonal to what existing detectors measure.
- Evidence anchors:
  - [abstract] "TOCSIN is a generic dual-channel detection paradigm that uses token cohesiveness as a plug-and-play module to improve existing zero-shot detectors"
  - [section 3.3] "The proposed dual-channel detection paradigm is rather generic, allowing token cohesiveness to be applied as a plug-and-play module in a variety of detectors"
  - [corpus] Strong evidence - multiple papers on zero-shot detection and token probability sequences
- Break condition: If token cohesiveness becomes highly correlated with existing detector features, the complementary benefit would diminish.

### Mechanism 3
- Claim: The black-box setting is particularly effective for TOCSIN because it doesn't require access to the source LLM.
- Mechanism: Token cohesiveness calculation only requires BARTScore, which is much smaller than the scoring models needed by base detectors. This makes TOCSIN more robust when source models are unavailable.
- Core assumption: BARTScore can effectively measure semantic similarity between original and perturbed text without requiring the source LLM.
- Evidence anchors:
  - [abstract] "To calculate token cohesiveness, TOCSIN only requires a few rounds of random token deletion and semantic difference measurement, making it particularly suitable for a practical black-box setting"
  - [section 4.4] "TOCSIN, in contrast, requires no such approximation and can therefore resist the degradation"
  - [corpus] Moderate evidence - corpus includes papers on black-box detection and surrogate models
- Break condition: If BARTScore proves insufficient for measuring semantic similarity in certain domains, the effectiveness would degrade.

## Foundational Learning

- Concept: Semantic similarity measurement using models like BARTScore or GPTScore
  - Why needed here: Token cohesiveness fundamentally relies on measuring semantic difference between original text and perturbed versions
  - Quick check question: What does negative BARTScore measure between two text passages?

- Concept: Random token deletion as a perturbation technique
  - Why needed here: Creating perturbed versions of text to measure how semantic content changes when tokens are removed
  - Quick check question: How does the proportion of tokens deleted (ρ) affect the measurement of token cohesiveness?

- Concept: Dual-channel architecture combining complementary features
  - Why needed here: TOCSIN's effectiveness comes from combining token cohesiveness with existing detector scores
  - Quick check question: Why is exponential scaling used when combining the two channel scores?

## Architecture Onboarding

- Component map:
  - Input text → Token Cohesiveness Channel (random deletion → BARTScore computation → average)
  - Input text → Base Detector Channel (Likelihood/LogRank/LRR/Fast-DetectGPT computation)
  - Combined scores → Exponential scaling → Threshold comparison → Final decision

- Critical path:
  1. Random token deletion (n copies × ρ proportion)
  2. BARTScore computation for each copy
  3. Average semantic difference calculation
  4. Base detector computation
  5. Score combination and thresholding

- Design tradeoffs:
  - n (number of copies) vs computation time: Higher n gives more stable estimates but increases runtime
  - ρ (deletion proportion) vs sensitivity: Smaller ρ captures finer semantic relationships but may be less discriminative
  - BARTScore vs GPTScore: BART is faster and smaller but GPT may capture different semantic aspects

- Failure signatures:
  - Very short passages (e.g., 45 tokens) show poor discrimination
  - PubMedQA domain shows reduced effectiveness
  - When token cohesiveness distributions between human and LLM text heavily overlap

- First 3 experiments:
  1. Verify token cohesiveness distributions differ between human and LLM text on XSum dataset
  2. Test different values of n (10, 20, 50) and ρ (0.5%, 1.5%, 3%) on SQuAD
  3. Compare black-box vs white-box performance on WritingPrompts with various source models

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness varies significantly across domains, with notably poor performance on PubMedQA and short passages (~45 tokens)
- Computational overhead from multiple rounds of random token deletion and BARTScore computation may be prohibitive for real-time applications
- Claims about black-box superiority are based on limited source models and may not generalize to all LLM architectures

## Confidence

*High Confidence:* The claim that LLM-generated text exhibits higher token cohesiveness than human-written text is well-supported by experimental evidence across multiple datasets (XSum, SQuAD, WritingPrompts, Creative Writing). The distributional differences are consistently observed and statistically significant.

*Medium Confidence:* The assertion that token cohesiveness provides complementary information to existing zero-shot detectors is supported by the consistent improvement across all four baseline detectors tested. However, the degree of complementarity and whether this holds for other detector architectures remains uncertain.

*Low Confidence:* The claim about superior black-box performance compared to white-box settings for existing detectors is based on comparisons with specific detectors (Likelihood, LogRank, LRR, Fast-DetectGPT). Whether this advantage extends to other detector families or whether alternative approaches might achieve similar robustness is unclear.

## Next Checks

1. **Cross-Architecture Generalization Test:** Evaluate TOCSIN's performance when the base detector and the source LLM are from completely different architectural families (e.g., using a decoder-only base detector like DetectGPT to detect text from encoder-decoder models like T5). This would test whether the complementary benefits of token cohesiveness hold across fundamental architectural differences.

2. **Domain Transferability Analysis:** Systematically test TOCSIN on additional specialized domains beyond PubMedQA, including legal, technical, and domain-specific scientific texts. This would clarify whether the observed domain-specific limitations are inherent to the token cohesiveness approach or specific to medical/biological text.

3. **Computational Efficiency Benchmark:** Measure end-to-end inference time for TOCSIN compared to base detectors alone across varying text lengths and batch sizes. This would provide concrete data on the practical deployment costs and help determine whether the accuracy improvements justify the computational overhead in real-world applications.