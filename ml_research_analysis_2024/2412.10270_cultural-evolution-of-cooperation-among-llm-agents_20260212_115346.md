---
ver: rpa2
title: Cultural Evolution of Cooperation among LLM Agents
arxiv_id: '2412.10270'
source_url: https://arxiv.org/abs/2412.10270
tags:
- agents
- donation
- cooperation
- game
- claude
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether societies of LLM agents can culturally\
  \ evolve cooperative norms in an iterated Donor Game, using a framework that simulates\
  \ cultural evolution over generations. Three models\u2014Claude 3.5 Sonnet, Gemini\
  \ 1.5 Flash, and GPT-4o\u2014are tested."
---

# Cultural Evolution of Cooperation among LLM Agents

## Quick Facts
- arXiv ID: 2412.10270
- Source URL: https://arxiv.org/abs/2412.10270
- Reference count: 35
- Primary result: Claude 3.5 Sonnet reliably evolves cooperation in iterated Donor Game while Gemini 1.5 Flash and GPT-4o converge to mutual defection

## Executive Summary
This paper investigates whether societies of LLM agents can culturally evolve cooperative norms through an iterated Donor Game framework. The study simulates cultural evolution over 10 generations with three different LLM models: Claude 3.5 Sonnet, Gemini 1.5 Flash, and GPT-4o. Results show significant model-specific differences, with Claude 3.5 Sonnet reliably evolving cooperation especially when costly punishment is available, while the other models fail to develop cooperative norms. The success of Claude 3.5 depends critically on initial conditions, requiring sufficient baseline cooperation to bootstrap the evolutionary process.

## Method Summary
The study employs a cultural evolutionary framework where 12 LLM agents play an iterated Donor Game over 10 generations. In each generation, agents generate donation strategies based on past behavior information (3-step trace), then play 12 rounds of pairwise games. Top 50% of agents by final resources survive and transmit their strategies to new agents, with some variation introduced through temperature-based sampling. The framework tests three models (Claude 3.5 Sonnet, Gemini 1.5 Flash, GPT-4o) with and without costly punishment mechanisms, measuring average resources as the primary metric.

## Key Results
- Claude 3.5 Sonnet evolves cooperation reliably, especially with costly punishment
- Gemini 1.5 Flash shows minimal cooperation and overuses punishment to its detriment
- GPT-4o converges to mutual defection regardless of punishment affordances
- Success depends on initial conditions, with ~50% initial cooperation threshold for Claude 3.5
- Strategies evolve increasing complexity, with Claude 3.5 developing sophisticated free-rider detection mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Claude 3.5 Sonnet agents develop cooperative norms through cultural evolution because the initial population includes strategies with sufficient baseline generosity, enabling cooperative strategies to be selected over generations.
- Mechanism: The cultural evolutionary process selects agents that donate more because high donation strategies lead to higher resource accumulation, especially when paired with punishment of defectors. Surviving agents transmit these cooperative strategies to new agents each generation, creating a positive feedback loop.
- Core assumption: Initial conditions matter - there exists a threshold level of initial cooperation below which populations collapse to mutual defection.
- Evidence anchors:
  - [abstract] "The success of Claude 3.5 depends on initial conditions, with higher initial cooperation leading to better outcomes."
  - [section] "Indeed, for the two runs where Claude failed to generate cooperation... the average donation in the first generation was 44% and 47%, whereas for the three runs where Claude succeeded at generating cooperation, the average donation in the first generation was 50%, 53% and 54% respectively."
  - [corpus] Weak - corpus contains no direct evidence about initial condition thresholds, though related work on reputation systems suggests initial strategy diversity affects outcomes.
- Break condition: If initial strategies are too low (below ~45% donation), the population cannot bootstrap cooperation and converges to mutual defection regardless of punishment affordances.

### Mechanism 2
- Claim: Claude 3.5 Sonnet agents evolve increasingly complex strategies over generations that incorporate second-order information about recipient chains to better identify and punish free-riders.
- Mechanism: As generations progress, strategies become more sophisticated by incorporating weighted averages of recipient behavior, dynamic adjustment factors, and resource preservation mechanisms. This complexity allows agents to distinguish between occasional cooperators and persistent free-riders.
- Core assumption: LLMs can process and act on multi-step reputation information to develop nuanced strategies beyond simple reciprocity.
- Evidence anchors:
  - [abstract] "Strategies evolve to become more complex over generations, with Claude 3.5 developing sophisticated mechanisms for punishing free-riders."
  - [section] "Gemini 1.5 Flash does not specify donation size numerically, and exhibits smaller changes from generation 1 to 10 than the other models."
  - [corpus] Weak - corpus mentions reputation systems but doesn't provide evidence about second-order information processing in cultural evolution.
- Break condition: If trace length is reduced to 1 step instead of 3, the emergence of cooperation is less pronounced for Claude 3.5 and disappears completely for Gemini 1.5 Flash.

### Mechanism 3
- Claim: The ability to use costly punishment mechanisms differs across LLM models based on their training and prompt engineering, with Claude 3.5 Sonnet effectively using punishment to enhance cooperation while Gemini 1.5 Flash overuses punishment to its detriment.
- Mechanism: Models that can effectively balance punishment with cooperation use punishment strategically to deter free-riding, while models that cannot balance these forces either fail to punish effectively or punish indiscriminately, reducing overall cooperation.
- Core assumption: Different base models have different capabilities for strategic decision-making about when to punish versus cooperate.
- Evidence anchors:
  - [abstract] "Claude 3.5 Sonnet can make use of an additional mechanism for costly punishment to achieve yet higher scores, while Gemini 1.5 Flash and GPT-4o fail to do so."
  - [section] "Interestingly, the affordance of costly punishment causes a marked decrease in the resources of Gemini 1.5 Flash agents, since these over-engage in punishment (14.29% of Gemini encounters involved punishment, compared with 1.65% for GPT-4o, and 0.06% for Claude)."
  - [corpus] Weak - corpus contains no direct evidence about punishment mechanism effectiveness across models.
- Break condition: If punishment cost-benefit ratio is unfavorable or if the model cannot strategically assess when punishment is warranted, punishment mechanisms backfire and reduce cooperation.

## Foundational Learning

- Concept: Iterated Donor Game mechanics
  - Why needed here: Understanding the game structure is essential for interpreting why different models evolve different strategies and what cooperation means in this context.
  - Quick check question: In the Donor Game, if a donor gives up 20 units and the recipient receives 2x, how many units does the recipient gain and what is the donor's net loss?

- Concept: Cultural evolutionary selection
  - Why needed here: The study relies on cultural evolution principles where strategies that lead to higher resources survive and transmit to future generations.
  - Quick check question: If 12 agents play and the top 6 survive based on final resources, what percentage of the population is selected for reproduction each generation?

- Concept: Reputation and indirect reciprocity
  - Why needed here: Agents use limited information about recipients' past behavior to make donation decisions, which is the mechanism through which indirect reciprocity emerges.
  - Quick check question: What information does a donor receive about a recipient in round 4 of the game, and how far back does this information trace?

## Architecture Onboarding

- Component map:
  - System prompt -> Strategy prompt -> Donation prompt -> Game execution -> Selection mechanism -> Transmission mechanism -> Mutation

- Critical path:
  1. Initialize 12 agents with strategy prompts
  2. Run 12 rounds of pairwise Donor Games
  3. Select top 50% by final resources
  4. Generate 6 new agents with strategy prompts including surviving strategies
  5. Repeat for 10 generations
  6. Analyze average resources and strategy evolution

- Design tradeoffs:
  - Population size (12) vs computational cost: Small populations reduce cost but may increase stochastic effects
  - Generation count (10) vs convergence time: More generations allow more evolution but increase cost
  - Trace length (3) vs model capability: Longer traces provide more information but may exceed model processing capacity

- Failure signatures:
  - All models converge to low cooperation (<20% average donation)
  - One model consistently outperforms others across all runs
  - Strategy complexity decreases over generations instead of increasing
  - Punishment mechanisms lead to reduced cooperation rather than enhanced cooperation

- First 3 experiments:
  1. Run baseline experiment with Claude 3.5 Sonnet, Gemini 1.5 Flash, and GPT-4o without punishment to establish baseline cooperation levels
  2. Add costly punishment mechanism to all models and compare cooperation levels
  3. Vary initial population strategies (e.g., seed with higher or lower initial cooperation) to test threshold effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the cultural evolution of cooperation among LLM agents depend on the specific base model's training data composition and methodology?
- Basis in paper: [explicit] The paper highlights significant differences in cooperative behavior across models (Claude 3.5, Gemini 1.5, GPT-4o) and suggests this may relate to their training.
- Why unresolved: The paper doesn't investigate the specific training data or methodology differences between models that might explain the observed behavioral differences.
- What evidence would resolve it: Comparative analysis of base model training data composition and training methodologies, and their correlation with cooperative behavior in the Donor Game.

### Open Question 2
- Question: How does the introduction of communication between agents affect the cultural evolution of cooperation in the Donor Game?
- Basis in paper: [inferred] The paper mentions communication as a potential extension for future work and notes that its absence may limit the emergence of cooperation.
- Why unresolved: The current experimental setup prohibits communication between agents, which is a key feature of human social interaction and cooperation.
- What evidence would resolve it: Experiments comparing cultural evolution with and without communication channels between agents.

### Open Question 3
- Question: What is the minimum amount of information (trace length) required for successful cultural evolution of cooperation in LLM agent societies?
- Basis in paper: [explicit] The paper shows that shortening the trace length from 3 to 1 eliminates cooperation emergence in Gemini 1.5 Flash.
- Why unresolved: The paper only tests trace lengths of 1, 2, and 3, and doesn't establish a precise threshold for cooperation emergence.
- What evidence would resolve it: Systematic testing of trace lengths between 1 and 3 to identify the minimum information required for cooperation.

## Limitations

- Results may be sensitive to specific prompt engineering used, as limited prior work exists on cultural evolution in LLM agents
- Small population size (12 agents) may not capture full dynamics of cultural evolution and could amplify stochastic effects
- Only three LLM models tested, limiting generalizability across broader LLM landscape

## Confidence

**High Confidence**: The model-specific differences in cooperation evolution are robust, as evidenced by the consistent pattern across multiple runs showing Claude 3.5 Sonnet outperforming the other models. The relationship between initial cooperation levels and successful evolution is also well-supported by the data showing clear thresholds.

**Medium Confidence**: The mechanism explanations for why Claude 3.5 succeeds while other models fail are plausible but rely on assumptions about model capabilities that are not directly tested. The complexity evolution claims are supported by the observed changes in strategies but lack systematic complexity measurements.

**Low Confidence**: The broader implications for real-world LLM agent deployment remain speculative, as the study uses a simplified game-theoretic framework that may not capture the complexity of real social interactions.

## Next Checks

1. **Initial Condition Sensitivity Analysis**: Systematically vary the initial population strategies from 20% to 80% average donation in 10% increments to precisely map the threshold effect for cooperation emergence in Claude 3.5 Sonnet.

2. **Trace Length Manipulation**: Run experiments with trace lengths of 1, 3, and 5 steps to quantify the impact of information availability on cooperation evolution across all three models.

3. **Cross-Model Strategy Transfer**: Test whether strategies evolved in Claude 3.5 Sonnet can be successfully transmitted to and executed by Gemini 1.5 Flash and GPT-4o agents, or vice versa, to isolate model-specific capabilities from strategy quality.