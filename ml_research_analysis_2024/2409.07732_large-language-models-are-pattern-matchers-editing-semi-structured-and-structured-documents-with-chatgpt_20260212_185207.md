---
ver: rpa2
title: 'Large Language Models are Pattern Matchers: Editing Semi-Structured and Structured
  Documents with ChatGPT'
arxiv_id: '2409.07732'
source_url: https://arxiv.org/abs/2409.07732
tags:
- structured
- documents
- chatgpt
- latex
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that LLMs can effectively edit and restructure
  structured and semi-structured documents using simple, straightforward prompts.
  Two case studies with ChatGPT showed successful processing of LaTeX tables and conversion
  between RIS and OPUS XML formats, with all outputs being syntactically correct.
---

# Large Language Models are Pattern Matchers: Editing Semi-Structured and Structured Documents with ChatGPT

## Quick Facts
- arXiv ID: 2409.07732
- Source URL: https://arxiv.org/abs/2409.07732
- Authors: Irene Weber
- Reference count: 0
- LLMs can effectively edit and restructure structured and semi-structured documents using simple prompts

## Executive Summary
This study demonstrates that LLMs can effectively edit and restructure structured and semi-structured documents using simple, straightforward prompts. Two case studies with ChatGPT showed successful processing of LaTeX tables and conversion between RIS and OPUS XML formats, with all outputs being syntactically correct. The experiments revealed that LLMs have strong pattern-matching capabilities when processing explicitly structured data, suggesting that clear structural annotations in prompts enhance task understanding. The qualitative analysis indicates that minimal effort is required to achieve reliable document editing with LLMs, potentially offering significant labor savings in document processing tasks.

## Method Summary
The research employed qualitative methods with case studies using ChatGPT to test document editing capabilities. The experiments focused on two structured document formats: LaTeX tables and bibliographic data (RIS/OPUS XML). The approach used simple, straightforward prompts with one-shot examples to guide the LLM in editing and converting documents. All experiments were conducted using ChatGPT with cleared chat history between queries to ensure independence of results.

## Key Results
- ChatGPT successfully edited LaTeX tables with correct syntax in all experiments
- LLM accurately converted between RIS and OPUS XML formats with high fidelity
- All generated outputs were syntactically correct and processed without issues by their respective compilers/parsers
- Minimal prompting effort was required to achieve reliable document processing results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can process and edit semi-structured and structured documents using simple, straightforward prompts.
- Mechanism: LLMs have strong pattern-matching capabilities when processing explicitly structured data, and clear structural annotations in prompts enhance task understanding.
- Core assumption: The LLM has been trained on sufficient examples of structured data formats like LaTeX, RIS, and XML to recognize and manipulate their patterns.
- Evidence anchors:
  - [abstract] "Our experiments indicate that LLMs can effectively edit structured and semi-structured documents when provided with basic, straightforward prompts."
  - [section] "ChatGPT demonstrates a strong ability to recognize and process the structure of annotated documents."
- Break condition: If prompts lack clear structural annotations or if the document structure is highly complex and novel, the LLM may struggle to process the task correctly.

### Mechanism 2
- Claim: LLMs can convert between different structured document formats with high accuracy.
- Mechanism: LLMs learn patterns and relationships between elements in different structured formats from their training data, allowing them to map elements from one format to another.
- Core assumption: The LLM has encountered examples of both source and target formats during training, enabling it to learn the mapping between them.
- Evidence anchors:
  - [abstract] "The experiments also reveal impressive pattern matching skills in ChatGPT."
  - [section] "It seems plausible that its working principle involves identifying relationships (i.e., patterns) between RIS and XML elements in the example documents and replicate these in the documents it was tasked with generating."
- Break condition: If the source and target formats have significantly different structures or if the mapping between them is not clear from the provided examples, the LLM may produce incorrect or incomplete conversions.

### Mechanism 3
- Claim: LLMs can edit structured documents while preserving the overall structure and formatting.
- Mechanism: LLMs understand the concepts related to structured documents (e.g., rows, columns, cells) and can manipulate specific elements while maintaining the overall structure.
- Core assumption: The LLM has a good understanding of the structure and formatting rules of the document format being edited.
- Evidence anchors:
  - [abstract] "ChatGPT demonstrates a strong ability to recognize and process the structure of annotated documents."
  - [section] "In all experiments, ChatGPT generated tables in correct LaTeX syntax that the LaTeX compiler processed without issues."
- Break condition: If the editing task requires complex manipulations of the document structure or if the LLM lacks a deep understanding of the formatting rules, it may produce documents with incorrect syntax or formatting.

## Foundational Learning

- Concept: Understanding of structured document formats (e.g., LaTeX, RIS, XML)
  - Why needed here: To recognize and manipulate the structure and elements of the documents being processed.
  - Quick check question: Can you identify the key elements and structure of a LaTeX table or an RIS record?

- Concept: Pattern recognition and matching
  - Why needed here: To identify relationships between elements in different structured formats and map them correctly during conversion tasks.
  - Quick check question: Given examples of RIS and XML records, can you identify the corresponding elements and their relationships?

- Concept: Prompt engineering for structured data tasks
  - Why needed here: To provide clear and explicit instructions to the LLM, including examples and structural annotations, to ensure accurate processing of the tasks.
  - Quick check question: Can you design a prompt that instructs an LLM to convert a RIS record to XML format, including an example input and output?

## Architecture Onboarding

- Component map: LLM (ChatGPT) -> Input document (structured/semi-structured) -> Prompt (instructions and examples) -> Output document (edited/converted)
- Critical path:
  1. Prepare the input document and prompt
  2. Send the prompt and document to the LLM
  3. Receive the LLM's output
  4. Validate the output for correctness and syntax
- Design tradeoffs:
  - Simple prompts vs. detailed prompts: Simple prompts may be quicker to create but may lead to less accurate results; detailed prompts with examples may be more effective but require more effort to create.
  - Zero-shot vs. one-shot prompting: Zero-shot prompting relies on the LLM's general knowledge, while one-shot prompting provides a specific example to guide the LLM's output.
- Failure signatures:
  - Incorrect or incomplete output due to unclear prompts or lack of examples
  - Syntax errors in the output document
  - Hallucinations or incorrect data in the output
- First 3 experiments:
  1. Edit a LaTeX table by deleting a column and verify the output syntax
  2. Convert a RIS record to XML format using a one-shot prompt and compare the output to the expected result
  3. Edit a structured document by swapping columns and verify that the overall structure is preserved

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the explicitness of structural annotations in prompts affect LLM performance across different types of structured documents?
- Basis in paper: [explicit] "This suggests that explicitly structuring tasks and data in prompts might enhance an LLM's ability to understand and solve tasks."
- Why unresolved: The paper only hints at this relationship through qualitative observations in two specific cases. The authors call for "Further experiments exploring this hypothesis."
- What evidence would resolve it: Controlled experiments testing LLM performance on various structured documents with varying levels of structural annotation explicitness in prompts, measuring accuracy and consistency across different document types.

### Open Question 2
- Question: Is the pattern matching behavior observed in ChatGPT a general characteristic of LLMs or specific to certain models?
- Basis in paper: [explicit] "The experiments also reveal impressive pattern matching skills in ChatGPT. This observation deserves further investigation, as it may contribute to understanding the processes leading to hallucinations in LLMs."
- Why unresolved: The paper only tested ChatGPT (GPT-3.5) and found pattern matching behavior, but doesn't compare with other LLMs. The authors suggest this deserves further investigation.
- What evidence would resolve it: Systematic comparison of pattern matching behavior across multiple LLM models when processing structured documents, measuring consistency and correctness of generated outputs.

### Open Question 3
- Question: What is the relationship between an LLM's ability to process structured documents and its propensity for hallucinations?
- Basis in paper: [explicit] "This observation deserves further investigation, as it may contribute to understanding the processes leading to hallucinations in LLMs."
- Why unresolved: The paper observes that LLMs can construct plausible but sometimes incorrect data from structured inputs, suggesting a link to hallucination mechanisms, but doesn't explore this connection systematically.
- What evidence would resolve it: Experimental studies correlating LLM performance on structured document tasks with hallucination rates in other contexts, identifying common underlying mechanisms.

## Limitations
- The study relies on qualitative assessment rather than quantitative metrics, limiting the ability to measure and compare performance systematically
- Only two specific document formats (LaTeX tables and RIS/OPUS XML) were tested, which may not generalize to other structured document types
- Results are based on a single LLM (ChatGPT) and may not extend to other language models or different versions

## Confidence
- **High confidence**: LLMs can perform basic editing and format conversion tasks with explicit structural annotations
- **Medium confidence**: Minimal prompting effort is sufficient for reliable document processing
- **Low confidence**: The pattern-matching mechanism fully explains the observed capabilities without additional factors

## Next Checks
1. **Quantitative benchmarking**: Design controlled experiments measuring accuracy, consistency, and error rates across multiple LLM models and document formats
2. **Stress testing**: Evaluate performance with increasingly complex document structures, edge cases, and ambiguous formatting scenarios
3. **Cross-format generalization**: Test the approach with additional structured formats (JSON, CSV, HTML tables) to assess broader applicability beyond the two studied formats