---
ver: rpa2
title: 'LeDex: Training LLMs to Better Self-Debug and Explain Code'
arxiv_id: '2405.18649'
source_url: https://arxiv.org/abs/2405.18649
tags:
- code
- pass
- refinement
- refine
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEDEX is a framework that trains LLMs to self-debug and explain
  code by automatically collecting high-quality explanation and refinement data, then
  fine-tuning with supervised learning and reinforcement learning. The approach addresses
  the limited self-refinement capability of open-source LLMs by generating wrong solutions,
  collecting their explanations and fixes from larger models, and filtering through
  execution verification.
---

# LeDex: Training LLMs to Better Self-Debug and Explain Code

## Quick Facts
- arXiv ID: 2405.18649
- Source URL: https://arxiv.org/abs/2405.18649
- Reference count: 40
- LEDEX improves pass@1 by up to 15.92% and pass@10 by 9.30% through self-debugging training

## Executive Summary
LEDEX is a framework that trains large language models to self-debug and explain code by automatically collecting high-quality explanation and refinement data, then fine-tuning with supervised learning and reinforcement learning. The approach addresses the limited self-refinement capability of open-source LLMs by generating wrong solutions, collecting their explanations and fixes from larger models, and filtering through execution verification. Supervised fine-tuning improves pass@1 by up to 15.92% and pass@10 by 9.30% across four benchmarks, while reinforcement learning adds up to 3.54% and 2.55% gains respectively. The framework also produces more useful code explanations, as confirmed by human evaluation, helping developers better understand bugs in source code.

## Method Summary
LEDEX uses a three-stage approach: automated data collection, supervised fine-tuning, and reinforcement learning. The data collection pipeline generates wrong solutions from base models, then prompts larger teacher models to create explanations and refinements for these wrong solutions. Execution verification filters out incorrect refinements, ensuring only high-quality examples enter the training set. The supervised fine-tuning stage trains target LLMs on this verified data using instruction-following format. Finally, reinforcement learning with proximal policy optimization (PPO) optimizes the models using separate rewards for code refinement quality (CodeBLEU + execution) and explanation quality (semantic similarity), improving both code correctness and explanation utility.

## Key Results
- LEDEX improves pass@1 by up to 15.92% and pass@10 by 9.30% through supervised fine-tuning across four benchmarks
- Reinforcement learning adds up to 3.54% and 2.55% gains in pass@1 and pass@10 respectively
- Human evaluation confirms LEDEX produces more useful code explanations for understanding bugs
- The approach is model-agnostic and scalable to different LLM sizes and types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code explanation followed by refinement helps LLMs better analyze wrong code
- Mechanism: The chain-of-thought approach where the model first explains the bug before attempting to fix it creates a cognitive scaffold that improves debugging accuracy
- Core assumption: LLMs benefit from explicit reasoning about their mistakes before attempting corrections
- Evidence anchors:
  - [abstract] "we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement"
  - [section 2.1] "asking LLMs to provide an explanation of the wrong solution and then refine it in a chain-of-thought manner... helps it to better understand the unit test feedback and increases the success rate of providing refined solutions"
  - [corpus] Weak evidence - only 1 related paper mentions self-debugging with self-generated tests

### Mechanism 2
- Claim: Execution verification filtering creates high-quality training data
- Mechanism: By running generated refinements against test cases and only keeping those that pass, the framework ensures only correct fixes enter the training set
- Core assumption: Automated execution verification can reliably distinguish correct from incorrect refinements
- Evidence anchors:
  - [section 2.1] "we verify the refinements by running them against the test cases again, and only those passing all the test cases are considered correct refinements"
  - [section 2.1] "This verification step is crucial to guarantee the quality of the automatically collected code explanation and refinement dataset"
  - [corpus] Weak evidence - only 1 related paper mentions execution-based code generation

### Mechanism 3
- Claim: Separate rewards for code and explanation optimize both dimensions
- Mechanism: Using different reward functions for code refinement (CodeBLEU + execution) and explanation quality (semantic similarity) allows the model to improve both aspects independently
- Core assumption: Code quality and explanation quality can be optimized separately without interfering with each other
- Evidence anchors:
  - [section 2.3.3] "we design the rewards considering both parts" and "PPO algorithm with code refinement and explanation rewards considered separately"
  - [section 2.3.2] "we calculate the average sentiment similarity between the explanation embedding e and corresponding embeddings in ground truth"
  - [corpus] Weak evidence - no related papers mention separate reward design for code and explanation

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: The approach relies on models first explaining their reasoning before generating fixes, which requires understanding how to prompt for and evaluate reasoning chains
  - Quick check question: Why does asking for an explanation before a refinement improve success rates compared to asking for refinement directly?

- Concept: Execution-based verification
  - Why needed here: The framework uses test execution to filter training data, requiring understanding of how to run code safely and interpret test results
  - Quick check question: What are the risks of using execution verification for filtering training data, and how does the framework mitigate them?

- Concept: Reinforcement learning with PPO
  - Why needed here: The framework uses PPO with separate rewards for code and explanation, requiring understanding of advantage calculation and KL divergence constraints
  - Quick check question: How does the separate reward design for code and explanation differ from standard PPO implementations?

## Architecture Onboarding

- Component map:
  - Data collection pipeline -> Execution verifier -> SFT training module -> RL training module -> Evaluation harness

- Critical path:
  1. Generate wrong solutions from base models
  2. Collect explanations and refinements from teacher models
  3. Verify refinements through execution
  4. Train with SFT on verified data
  5. Fine-tune with RL using separate rewards
  6. Evaluate on benchmarks

- Design tradeoffs:
  - Using teacher models (GPT-3.5-Turbo) vs self-bootstrapping with same model
  - Separate vs combined rewards for code and explanation
  - Temperature settings for data diversity vs quality
  - Number of refinement iterations vs computational cost

- Failure signatures:
  - Low refinement success rates despite high pass@1 on initial generation
  - Explanations that don't match the actual bugs
  - RL training that causes explanations to become verbose but unhelpful
  - Data collection that produces mostly low-quality examples

- First 3 experiments:
  1. Run data collection pipeline on small dataset and verify execution filtering catches incorrect refinements
  2. Train SFT on collected data and measure improvement on simple debugging tasks
  3. Apply RL training and compare explanation quality with and without separate rewards

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LEDEX scale with larger or more diverse training datasets?
- Basis in paper: [inferred] The paper notes that LEDEX is "scalable" but does not test scaling with larger or more diverse datasets.
- Why unresolved: The experiments only use a fixed dataset size, leaving the relationship between dataset scale and model performance unexplored.
- What evidence would resolve it: Systematic experiments varying dataset size and diversity while measuring pass@k metrics.

### Open Question 2
- Question: Can LEDEX generalize to programming languages beyond Python and the tested benchmarks?
- Basis in paper: [explicit] The paper focuses on Python code generation tasks but states the approach is "model-agnostic."
- Why unresolved: No experiments were conducted with other programming languages to verify generalizability.
- What evidence would resolve it: Testing LEDEX on code generation tasks in Java, C++, or JavaScript with appropriate benchmarks.

### Open Question 3
- Question: What is the impact of different reward function designs on the quality of code explanations and refinements?
- Basis in paper: [explicit] The paper presents a specific reward design but does not explore alternatives.
- Why unresolved: Only one reward function design was tested, limiting understanding of how reward structure affects outcomes.
- What evidence would resolve it: Comparative experiments with alternative reward functions measuring both code correctness and explanation quality.

### Open Question 4
- Question: How does LEDEX perform on real-world programming tasks compared to controlled benchmark environments?
- Basis in paper: [inferred] All experiments use curated benchmarks, with no evaluation on real-world codebases.
- Why unresolved: The paper does not address performance in practical development scenarios.
- What evidence would resolve it: Deployment studies measuring LEDEX's effectiveness on actual software development tasks with real codebases.

## Limitations

- Weak empirical validation of self-debugging capability - shows improved pass rates but doesn't prove independent debugging ability
- Limited benchmark diversity - only tests on synthetic or competition-style problems, not real-world debugging scenarios
- Data quality concerns - doesn't report statistics on how many wrong solutions successfully pass verification after refinement

## Confidence

**High confidence** - The mechanism of using chain-of-thought reasoning (explanation before refinement) is well-supported by the ablation study showing improvement from baseline. The execution verification filtering process is straightforward and reliable.

**Medium confidence** - The claim that separate rewards for code and explanation optimize both dimensions is supported by results but not extensively validated. The paper doesn't show what happens when rewards are combined or when only one reward is used.

**Low confidence** - The claim about producing "more useful code explanations" relies on human evaluation but doesn't provide detailed methodology or statistical significance testing of the evaluation results.

## Next Checks

1. Test self-debugging without teacher models - Run the same benchmarks using the fine-tuned models to debug their own wrong solutions without external teacher models, to verify true self-debugging capability rather than just learned imitation.

2. Measure data collection efficiency - Report the percentage of wrong solutions that successfully pass verification after refinement, to quantify how much of the collected data is actually usable and identify potential bottlenecks.

3. Test on real-world codebases - Apply the approach to open-source projects with known bugs or to student code submissions, where debugging is more complex than synthetic benchmarks, to assess practical utility.