---
ver: rpa2
title: 'Prompto: An open source library for asynchronous querying of LLM endpoints'
arxiv_id: '2408.11847'
source_url: https://arxiv.org/abs/2408.11847
tags:
- prompto
- experiment
- which
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces prompto, an open-source Python library designed
  to address the challenge of efficiently interacting with multiple Large Language
  Model (LLM) endpoints. The library uses asynchronous programming to enable concurrent
  querying of different LLM APIs, maximizing efficiency by adhering to individual
  rate limits and eliminating idle wait times.
---

# Prompto: An open source library for asynchronous querying of LLM endpoints

## Quick Facts
- arXiv ID: 2408.11847
- Source URL: https://arxiv.org/abs/2408.11847
- Reference count: 17
- Key outcome: Asynchronous Python library enabling concurrent LLM API queries with rate limit adherence

## Executive Summary
This paper introduces prompto, an open-source Python library designed to address the challenge of efficiently interacting with multiple Large Language Model (LLM) endpoints. The library uses asynchronous programming to enable concurrent querying of different LLM APIs, maximizing efficiency by adhering to individual rate limits and eliminating idle wait times. The authors demonstrate prompto's advantages over traditional synchronous approaches through experiments, showing significant speedups in obtaining responses from various models.

## Method Summary
Prompto is an asynchronous Python library that enables concurrent querying of multiple LLM endpoints while respecting individual rate limits. It uses Python's asyncio framework to manage queues of prompts for different APIs, automatically scheduling requests based on each endpoint's queries-per-minute (QPM) constraints. The library accepts experiment definitions in JSON Lines format, containing prompts, model specifications, and parameters, and outputs results to a structured folder hierarchy. It supports both parallel processing of different APIs and sequential processing, with built-in evaluation capabilities and support for multiple LLM providers.

## Key Results
- Demonstrated 35x speedup when querying different OpenAI models in parallel versus synchronous approaches
- Eliminated idle wait times when dealing with different rate limits across multiple APIs
- Promoted experiment reproducibility through single-file JSONL experiment definitions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asynchronous queuing eliminates idle wait times when querying LLM endpoints with different rate limits.
- Mechanism: By scheduling requests based on each endpoint's QPM constraint, prompto ensures that no thread sits idle while waiting for a response; instead, it continues dispatching the next permitted request to another endpoint.
- Core assumption: The overhead of async dispatch is negligible compared to the latency of LLM API calls.
- Evidence anchors:
  - [abstract] "uses asynchronous programming to efficiently interact with endpoints by allowing users to send multiple requests to different APIs concurrently. This eliminates idle wait times and maximises efficiency, especially when dealing with different rate limits."
  - [section 3.2] "If max-queries is set to 10, we simply send our requests every 60/10 = 6 seconds. Since we use an asynchronous approach, we do not need to wait for a response before sending another request, ensuring that prompts are sent at correct intervals."

### Mechanism 2
- Claim: Parallel processing of multiple APIs within a single thread avoids resource contention while maintaining speed gains.
- Mechanism: The library creates separate async queues per API, each governed by its own rate limit, and runs them in the same event loop so they share CPU without spawning new threads.
- Core assumption: All queues can be processed by a single event loop without exceeding memory or CPU thresholds.
- Evidence anchors:
  - [section 3.2] "Our implementation of the 'parallel' processing again uses asynchronous programming allowing multiple queues of prompts to be managed concurrently within a single thread of execution rather than utilising multiple CPU cores and so is more resource efficient."
  - [Appendix A.2] "We observe a 2 times speedup when using prompto with parallel processing."

### Mechanism 3
- Claim: Single-JSONL experiment definition enables reproducible, comparable evaluations across multiple models.
- Mechanism: All prompts, parameters, and metadata are stored in one structured file; prompto reads it once and applies the same settings to each model, ensuring identical input conditions.
- Core assumption: JSONL parsing and file I/O overhead is negligible relative to API query time.
- Evidence anchors:
  - [abstract] "Our library promotes experiment reproducibility by facilitating the definition of all inputs/prompts within a single JSON Lines (JSONL) or CSV file."
  - [section 3.2] "This prioritises flexibility in experiment design, simplifies the setup for researchers and especially promotes reproducibility as all model parameters and prompts can be documented within a single file."

## Foundational Learning

- Concept: Asynchronous programming with async/await
  - Why needed here: Allows concurrent dispatching of LLM requests without blocking on responses, maximizing throughput under rate limits.
  - Quick check question: What happens to the event loop if one coroutine blocks on I/O instead of yielding control?

- Concept: Rate limiting and token bucket algorithms
  - Why needed here: Ensures compliance with each API's QPM constraints while maintaining optimal request scheduling.
  - Quick check question: How would you compute the delay between requests if an API allows 50 QPM?

- Concept: JSONL file format and structured metadata
  - Why needed here: Provides a reproducible, human-readable way to define experiments across multiple models and APIs.
  - Quick check question: What fields must each JSONL record contain for prompto to process it correctly?

## Architecture Onboarding

- Component map: Experiment CLI ↔ Settings ↔ Experiment ↔ AsyncAPI → LLM endpoint; JSONL input → queues → responses → output folder
- Critical path: Read JSONL → create async queues per API/model → enforce QPM → send requests → collect responses → write output
- Design tradeoffs: Single-threaded async queues vs. multi-process parallelism; file-based experiment storage vs. in-memory config; synchronous fallback vs. strict async only
- Failure signatures: Slow or stalled queues indicate API timeouts or misconfigured QPM; missing output files suggest file I/O permission errors; empty response fields point to authentication issues
- First 3 experiments:
  1. Run a single JSONL file with two prompts against one OpenAI model; verify output JSONL matches input structure plus responses.
  2. Create a JSONL with the same prompts targeting two different APIs; run with --parallel; check that both sets of responses are present.
  3. Test rate limiting by setting --max-queries=5 on one API and --max-queries=20 on another; measure actual QPM in logs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does prompto's asynchronous querying perform when scaling to hundreds or thousands of prompts across multiple LLM endpoints with varying rate limits?
- Basis in paper: [explicit] The paper demonstrates speedups with 100 prompts but doesn't test larger scales.
- Why unresolved: The paper only shows experiments with 100 prompts, so the performance characteristics at larger scales are unknown.
- What evidence would resolve it: Benchmarking prompto with varying numbers of prompts (1000, 10000) and different API configurations to measure efficiency gains and resource utilization.

### Open Question 2
- Question: What are the practical limits of prompto's parallel processing capabilities when querying multiple models from the same endpoint simultaneously?
- Basis in paper: [explicit] The paper shows a 35x speedup when querying different OpenAI models in parallel, but doesn't explore upper limits.
- Why unresolved: The experiments only tested three models from OpenAI, leaving questions about performance with more models or under different network conditions.
- What evidence would resolve it: Systematic testing of prompto with increasing numbers of models from the same endpoint while monitoring response times and system resource usage.

### Open Question 3
- Question: How does prompto handle error recovery and retries when querying multiple LLM endpoints with different failure modes and rate limits?
- Basis in paper: [explicit] The paper mentions max_attempts for retries but doesn't detail error handling strategies.
- Why unresolved: The paper only briefly mentions retry logic without exploring how different error types (timeouts, rate limits, API failures) are handled across multiple endpoints.
- What evidence would resolve it: Detailed analysis of prompto's behavior under various failure scenarios, including how it prioritizes retries and manages different endpoint error responses.

## Limitations

- Evaluation based on single comparative benchmark without broader validation across different hardware or network conditions
- No measurements of CPU, memory, or network overhead under different workloads
- Scalability with very large experiment files or thousands of concurrent queries remains untested

## Confidence

- **High confidence**: The core mechanism of using async/await to eliminate idle wait times while respecting QPM limits is well-established in Python programming and directly supported by the provided implementation details and timing results.
- **Medium confidence**: The claimed 35x speedup is based on a single experiment scenario; while plausible given the mechanism, broader validation across different hardware, network conditions, and API configurations would strengthen this claim.
- **Medium confidence**: The reproducibility benefits of JSONL-based experiment definition are conceptually sound, but practical adoption depends on the library's integration with existing research workflows and the comprehensiveness of its API coverage.

## Next Checks

1. **Scalability test**: Run experiments with 10,000+ prompts across multiple APIs to measure queue backlogs, memory usage, and whether QPM enforcement remains accurate under high load.
2. **Network sensitivity analysis**: Introduce artificial latency (e.g., using network throttling) to quantify the break-even point where async overhead exceeds its benefits, and test behavior under flaky connections.
3. **Cross-platform performance**: Benchmark the same experiment suite on different hardware (low-spec laptop vs. server) to verify that the claimed speedups and resource efficiency hold across typical research environments.