---
ver: rpa2
title: Enhancing Discoverability in Enterprise Conversational Systems with Proactive
  Question Suggestions
arxiv_id: '2412.10933'
source_url: https://arxiv.org/abs/2412.10933
tags:
- user
- question
- suggestions
- users
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of guiding new users in enterprise
  conversational AI systems, where users often struggle to know what to ask next after
  receiving an answer. The authors propose a framework that combines population-level
  user intent analysis with chat-session-level question generation to create proactive,
  context-aware question suggestions.
---

# Enhancing Discoverability in Enterprise Conversational Systems with Proactive Question Suggestions

## Quick Facts
- arXiv ID: 2412.10933
- Source URL: https://arxiv.org/abs/2412.10933
- Authors: Xiaobin Shen; Daniel Lee; Sumit Ranjan; Sai Sree Harsha; Pawan Sevak; Yunyao Li
- Reference count: 13
- Primary result: Framework outperforms baseline on all five evaluation criteria for question suggestions

## Executive Summary
This paper addresses the challenge of guiding new users in enterprise conversational AI systems, where users often struggle to know what to ask next after receiving an answer. The authors propose a framework that combines population-level user intent analysis with chat-session-level question generation to create proactive, context-aware question suggestions. Their approach leverages large language models to generate categorized suggestions based on current queries, past interactions, and relevant documents. Human evaluation using real-world data from Adobe Experience Platform's AI Assistant showed that their framework outperformed baseline methods across all five evaluation criteria: relatedness (46.4% vs 29.7%), validity (68.5% vs 16.7%), usefulness (35.4% vs 27.8%), diversity (62.0% vs 19.7%), and discoverability (40.1% vs 23.2%).

## Method Summary
The framework consists of two key stages: user intent analysis and question generation. The population-level analysis identifies common user intents and categorizes them (e.g., Expansion, Follow-Up), while the chat-session-level generation uses these categories along with current interaction history to produce relevant suggestions. The system leverages LLMs with RAG to generate contextually relevant suggestions without requiring extensive task-specific training data. Human evaluators compared the framework's suggestions against a baseline using five criteria: Relatedness, Validity, Usefulness, Diversity, and Discoverability.

## Key Results
- Framework achieved 46.4% relatedness vs 29.7% for baseline
- 68.5% validity vs 16.7% for baseline
- 35.4% usefulness vs 27.8% for baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework improves discoverability by generating proactive, context-aware question suggestions that guide users toward exploring system capabilities.
- Mechanism: The system combines population-level user intent analysis with chat-session-level question generation. The population-level analysis identifies common user intents and categorizes them (e.g., Expansion, Follow-Up), while the chat-session-level generation uses these categories along with current interaction history to produce relevant suggestions.
- Core assumption: Users benefit from proactive guidance that anticipates their needs and introduces them to relevant but potentially unknown features.
- Evidence anchors:
  - [abstract] "Our approach combines periodic user intent analysis at the population level with chat session-based question generation."
  - [section] "Our framework consists of two key stages: user intent analysis and question generation."
  - [corpus] Found 25 related papers, indicating active research in proactive conversational systems. The average neighbor FMR (0.575) suggests moderate relevance to the broader literature on proactive question answering and conversational systems.
- Break condition: If user intents are too diverse or unpredictable, the predefined categories may not capture all relevant suggestions, reducing effectiveness.

### Mechanism 2
- Claim: The framework addresses data sparsity by leveraging large language models instead of training from scratch.
- Mechanism: Instead of relying on extensive task-specific data, the framework uses LLMs with RAG to generate question suggestions based on current queries, past interactions, and relevant documents.
- Core assumption: LLMs can effectively generate contextually relevant suggestions even with limited interaction data.
- Evidence anchors:
  - [abstract] "Our approach combines periodic user intent analysis at the population level with chat session-based question generation."
  - [section] "The question generation phase... uses the current interaction history Si and the most recent user intents identified during the periodic user intent analysis."
  - [corpus] The literature shows active research in LLM-based proactive systems, suggesting this approach is viable despite data limitations.
- Break condition: If the LLM lacks sufficient knowledge about the domain or if the retrieved documents are not relevant, the quality of suggestions may degrade.

### Mechanism 3
- Claim: The framework's effectiveness is validated through human evaluation across multiple criteria, demonstrating improvements in usefulness and discoverability.
- Mechanism: Human evaluators compared the framework's suggestions against a baseline using five criteria: Relatedness, Validity, Usefulness, Diversity, and Discoverability.
- Core assumption: Human evaluation provides a reliable assessment of suggestion quality, especially for subjective criteria like usefulness and discoverability.
- Evidence anchors:
  - [abstract] "Human evaluation using real-world data from Adobe Experience Platform's AI Assistant showed that our framework outperformed baseline methods across all five evaluation criteria."
  - [section] "The results of the human evaluation, summarized in Table 2, show that our framework outperformed the baseline across all five criteria."
  - [corpus] Limited corpus evidence for direct comparison, but the moderate FMR scores suggest some alignment with related research on evaluation methodologies.
- Break condition: If evaluators have different interpretations of the criteria or if the evaluation sample is not representative, the results may not generalize.

## Foundational Learning

- Concept: User intent analysis
  - Why needed here: Understanding why users ask questions helps categorize and generate relevant suggestions that guide exploration.
  - Quick check question: What are the two primary categories identified for question suggestions, and what do they represent?

- Concept: Large language models with retrieval-augmented generation (RAG)
  - Why needed here: RAG allows the system to generate contextually relevant suggestions without requiring extensive task-specific training data.
  - Quick check question: How does RAG enhance the LLM's ability to generate question suggestions in this framework?

- Concept: Human evaluation methodology
  - Why needed here: Evaluating suggestion quality requires considering multiple subjective criteria, which human evaluators are better suited to assess than automated metrics.
  - Quick check question: Why were pairwise comparisons chosen over independent ratings for the human evaluation?

## Architecture Onboarding

- Component map: Population-level user intent analysis -> Chat-session-level question generation -> LLM with RAG -> Human evaluation module

- Critical path:
  1. User asks a question in the AI Assistant.
  2. System generates a response using RAG.
  3. Framework generates next-question suggestions using LLM.
  4. Suggestions are presented to the user.
  5. If suggestions are selected, they become new queries, continuing the cycle.

- Design tradeoffs:
  - Using LLMs vs. training from scratch: LLMs reduce data requirements but may lack domain-specific knowledge.
  - Predefined categories vs. dynamic generation: Categories provide structure but may limit flexibility.
  - Human evaluation vs. automated metrics: Human evaluation captures subjective quality but is resource-intensive.

- Failure signatures:
  - Suggestions are irrelevant to the user's current context.
  - Users do not engage with the suggestions (low click-through rate).
  - Suggestions do not lead to meaningful exploration of new features.
  - Human evaluation scores are low across multiple criteria.

- First 3 experiments:
  1. Implement a simple version with just the chat-session-level generation, without population-level analysis, to establish a baseline.
  2. Add the population-level intent analysis and measure improvements in discoverability scores.
  3. Conduct A/B testing in a production environment to measure real-world impact on user engagement metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed framework be adapted to support multilingual question suggestion in enterprise conversational AI systems?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on English-language interactions in Adobe Experience Platform without addressing language support. Enterprise systems often serve global users who communicate in multiple languages, and the framework's effectiveness for non-English contexts remains unexplored.
- What evidence would resolve it: Empirical evaluation showing the framework's performance across multiple languages, including how user intent analysis and question generation adapt to different linguistic patterns and cultural contexts.

### Open Question 2
- Question: What are the long-term effects of proactive question suggestions on user behavior and system adoption in enterprise settings?
- Basis in paper: [explicit]
- Why unresolved: The authors mention wanting to evaluate impact on user engagement metrics like click-through rates and feature exploration, but this evaluation hasn't been conducted yet. The framework shows immediate benefits in human evaluation, but sustained usage patterns and learning effects over time remain unknown.
- What evidence would resolve it: Longitudinal study tracking user interactions over months, measuring changes in query sophistication, feature discovery rates, and overall system utilization compared to baseline without suggestions.

### Open Question 3
- Question: How does the performance of the framework scale with increasing numbers of user roles and organizational hierarchies in enterprise environments?
- Basis in paper: [inferred]
- Why evidence: The paper acknowledges that enterprise users have diverse roles (engineers vs product managers) and suggests personalization based on role, but doesn't explore how the system handles complex organizational structures with many specialized roles. The current framework may not efficiently adapt to highly stratified enterprise environments.
- What evidence would resolve it: Testing the framework in organizations with multiple hierarchical levels and specialized roles, measuring how well it maintains relevance and discoverability across different user segments with varying permissions and needs.

### Open Question 4
- Question: What are the privacy implications of using interaction history for question suggestion in enterprise AI systems?
- Basis in paper: [explicit]
- Why unresolved: While the framework leverages interaction history within chat sessions, the paper doesn't address privacy concerns around storing and processing user queries, especially in enterprise contexts where sensitive business information may be involved. The trade-off between personalization benefits and privacy risks remains unexplored.
- What evidence would resolve it: Analysis of privacy-preserving techniques that could be integrated into the framework, user studies on privacy concerns when using such systems, and evaluation of how much personalization benefit is retained when implementing privacy safeguards.

## Limitations

- Effectiveness relies heavily on LLM quality and document relevance, which are not fully specified
- Human evaluation methodology may be subject to evaluator bias and interpretation differences
- Study based on single enterprise platform (Adobe Experience Platform), limiting generalizability
- Missing implementation details for LLM configuration and user intent analysis process

## Confidence

- **High Confidence**: The framework's ability to improve discoverability and usefulness compared to baseline methods, as demonstrated by human evaluation results across all five criteria.
- **Medium Confidence**: The effectiveness of combining population-level user intent analysis with chat-session-level question generation, given the moderate FMR scores in related literature and the limited implementation details provided.
- **Low Confidence**: The generalizability of the framework to other enterprise platforms and domains, due to the lack of diverse evaluation settings and the single-platform study.

## Next Checks

1. **A/B Testing in Production**: Implement the framework in a production environment and conduct A/B testing to measure real-world impact on user engagement metrics, such as click-through rates on suggestions and user satisfaction scores.

2. **Cross-Domain Evaluation**: Test the framework on interaction data from multiple enterprise platforms across different domains to assess generalizability and identify potential domain-specific adaptations.

3. **Automated Metric Comparison**: Develop and validate automated metrics for evaluating question suggestion quality, then compare these metrics with human evaluation results to assess consistency and efficiency.