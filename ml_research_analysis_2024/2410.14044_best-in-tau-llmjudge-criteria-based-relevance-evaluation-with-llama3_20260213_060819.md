---
ver: rpa2
title: 'Best in Tau@LLMJudge: Criteria-Based Relevance Evaluation with Llama3'
arxiv_id: '2410.14044'
source_url: https://arxiv.org/abs/2410.14044
tags:
- relevance
- query
- passage
- label
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We proposed a method to automatically assign relevance labels for
  information retrieval systems by prompting LLMs to evaluate predefined criteria
  such as exactness, coverage, topicality, and contextual fit. Our approach involves
  independently grading these criteria and aggregating them to predict a final relevance
  label, aiming to enhance transparency and alignment with human judgment.
---

# Best in Tau@LLMJudge: Criteria-Based Relevance Evaluation with Llama3

## Quick Facts
- **arXiv ID**: 2410.14044
- **Source URL**: https://arxiv.org/abs/2410.14044
- **Reference count**: 9
- **Primary result**: Achieved highest Kendall's tau correlation (0.9483) on LLMJudge challenge dataset

## Executive Summary
This paper proposes a method for automatically assigning relevance labels in information retrieval systems by prompting LLMs to evaluate predefined criteria including exactness, coverage, topicality, and contextual fit. The approach involves independently grading these criteria and aggregating them to predict final relevance labels, aiming to enhance transparency and alignment with human judgment. The method also explores generating query-like representations of passages to improve linguistic alignment. When evaluated on the LLMJudge challenge dataset using Kendall's tau, the "Four Prompts" method achieved the highest leaderboard correlation (0.9483), outperforming other submissions and demonstrating the effectiveness of criteria-based relevance grading.

## Method Summary
The method employs Llama3 to evaluate documents against predefined relevance criteria through a multi-step process. First, the LLM independently grades documents on exactness, coverage, topicality, and contextual fit using separate prompts for each criterion. These individual scores are then aggregated through a learned or heuristic combination function to produce a final relevance prediction. Additionally, the approach generates query-like representations of passages to improve linguistic alignment between queries and documents. The system operates as a pipeline where query and document inputs flow through the criteria evaluation modules before final aggregation, with the generated representations serving as an optional enhancement step.

## Key Results
- Achieved highest Kendall's tau correlation (0.9483) on LLMJudge challenge dataset
- "Four Prompts" method outperformed other submissions on the leaderboard
- Demonstrated effectiveness of criteria-based relevance grading in automated relevance labeling

## Why This Works (Mechanism)
The method works by decomposing the complex task of relevance judgment into interpretable, independently-graded criteria that can be evaluated by LLMs with high precision. By breaking down relevance into exactness, coverage, topicality, and contextual fit, the approach allows the model to focus on specific aspects of document-query alignment rather than making holistic judgments. The aggregation of these criteria captures the multifaceted nature of relevance that humans naturally consider. Additionally, generating query-like representations of passages improves linguistic alignment by creating more comparable representations between queries and documents, reducing semantic gaps that might otherwise lead to misalignment in the evaluation process.

## Foundational Learning
- **Information Retrieval Relevance**: Understanding how relevance is defined and evaluated in IR systems; needed to establish the criteria framework and ensure the approach addresses core IR challenges; quick check: can you explain the difference between binary and graded relevance?
- **LLM Prompt Engineering**: Knowledge of how to construct effective prompts for structured output; required to elicit consistent and reliable criterion-specific judgments from the model; quick check: can you design a prompt that asks for a numerical score with justification?
- **Kendall's Tau Correlation**: Familiarity with this rank correlation metric; essential for understanding the evaluation methodology and interpreting leaderboard results; quick check: can you compute Kendall's tau between two ranked lists?
- **Document Representation Learning**: Understanding how to generate meaningful representations of text passages; needed for the query-like representation generation component; quick check: can you explain how query likelihood models work?
- **Multi-criteria Decision Making**: Knowledge of how to aggregate multiple scores into a single decision; required to combine the individual criterion scores into final relevance predictions; quick check: can you describe different aggregation methods like weighted sum or Borda count?
- **Synthetic Data Evaluation**: Understanding the limitations and benefits of using LLM-generated labels; crucial for interpreting results and recognizing potential evaluation biases; quick check: can you list three limitations of synthetic evaluation compared to human judgment?

## Architecture Onboarding

**Component Map**: Query/Document Input -> Criteria Evaluation (Exactness, Coverage, Topicality, Contextual Fit) -> Score Aggregation -> Final Relevance Prediction -> (Optional) Query-like Representation Generation

**Critical Path**: The evaluation proceeds through the criteria modules in parallel, with each criterion independently scored before aggregation. The most critical components are the criteria evaluation prompts and the aggregation mechanism, as errors in either will propagate to the final prediction.

**Design Tradeoffs**: The approach trades computational efficiency for interpretability and transparency. While evaluating four separate criteria increases inference costs compared to single-prompt methods, it provides clear explanations for each relevance decision and allows for targeted improvements to specific aspects of the evaluation process.

**Failure Signatures**: The method may fail when criteria are misaligned with the information need (e.g., emphasizing exactness when coverage is more important), when the LLM's understanding of criteria differs from human interpretation, or when query-like representation generation introduces artifacts that confuse the evaluation. Performance degradation is likely when criteria overlap significantly or when documents contain ambiguous or contradictory information.

**First Experiments**:
1. Evaluate individual criteria performance in isolation to identify which contribute most to final relevance prediction
2. Test different aggregation strategies (weighted sum, product, Borda count) to optimize final score combination
3. Compare performance with and without query-like representation generation to quantify its impact on linguistic alignment

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusive reliance on synthetic LLM-generated relevance labels may not accurately reflect human judgment patterns
- Does not report explicit error analysis or failure cases, making practical limitations difficult to assess
- Assumes predefined criteria are universally applicable across domains, which may not hold for specialized information needs

## Confidence
- **Kendall's tau correlation results on LLMJudge dataset**: High
- **Generalizability to human-labeled datasets**: Medium
- **Benefits of query-like passage representations**: Medium

## Next Checks
1. Evaluate the criteria-based relevance grading approach on human-labeled TREC-style relevance judgment datasets to assess performance differences compared to synthetic labels
2. Conduct ablation studies removing individual criteria to determine which contribute most to the final relevance prediction and identify potential redundancies
3. Test the method's robustness across different query types (informational, navigational, transactional) and document collections to assess domain adaptability and potential failure modes