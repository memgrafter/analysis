---
ver: rpa2
title: Resilience of Large Language Models for Noisy Instructions
arxiv_id: '2404.09754'
source_url: https://arxiv.org/abs/2404.09754
tags:
- errors
- error
- instructions
- noisy
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the resilience of large language models
  (LLMs) to noisy instructions, a critical issue for real-world applications. The
  authors introduce five types of noise (ASR errors, OCR errors, grammatical mistakes,
  typographical errors, and distractive content) into the MMLU benchmark and evaluate
  the performance of three models: ChatGPT-3.5, Mistral-7B-Instruct-v0.2, and Llama-2-7B-Chat.'
---

# Resilience of Large Language Models for Noisy Instructions

## Quick Facts
- arXiv ID: 2404.09754
- Source URL: https://arxiv.org/abs/2404.09754
- Authors: Bin Wang; Chengwei Wei; Zhengyuan Liu; Geyu Lin; Nancy F. Chen
- Reference count: 12
- Primary result: All tested LLMs experience significant performance degradation with noisy instructions, with ChatGPT-3.5 showing superior error correction capabilities

## Executive Summary
This study investigates how large language models handle noisy instructions, a critical issue for real-world deployment. The authors systematically introduce five types of noise (ASR errors, OCR errors, grammatical mistakes, typographical errors, and distractive content) into the MMLU benchmark and evaluate three models: ChatGPT-3.5, Mistral-7B-Instruct-v0.2, and Llama-2-7B-Chat. Results demonstrate that all models suffer significant performance degradation when faced with noisy instructions, though the severity varies by noise type. The study also evaluates a "re-pass" strategy where LLMs correct noisy instructions before task processing, finding that ChatGPT-3.5 is notably more effective at this correction task compared to open-source alternatives.

## Method Summary
The researchers evaluate LLM resilience by injecting five types of noise into the MMLU benchmark using rule-based techniques and generative models. They test three models (ChatGPT-3.5, Mistral-7B-Instruct-v0.2, and Llama-2-7B-Chat) on both noisy instructions and cleaned versions using a "re-pass" strategy. The "re-pass" approach uses LLMs to correct noisy instructions before task processing. Performance is measured through accuracy metrics comparing model responses on clean versus noisy instructions, with additional evaluation of the error correction effectiveness.

## Key Results
- All three tested models experience significant performance degradation when processing noisy instructions
- ChatGPT-3.5 demonstrates superior error correction capabilities compared to open-source alternatives
- Grammatical mistakes are better tolerated than other noise types, likely due to their presence in pre-training data
- Open-source models struggle more with instruction correction than closed-source models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit varying degrees of resilience to different types of noise due to differences in training data exposure and tokenization strategies.
- Mechanism: Performance degradation is influenced by frequency of specific error types in pre-training data and tokenization method's ability to handle corrupted text.
- Core assumption: Resilience is primarily determined by exposure to similar errors during training and tokenization strategy robustness.
- Evidence anchors: Abstract mentions varying resistance to noise types; section notes higher resilience to grammatical mistakes; corpus provides related work but doesn't directly address mechanisms.
- Break condition: If noise types are significantly different from training data or tokenization cannot handle corrupted text.

### Mechanism 2
- Claim: The "re-pass" strategy can improve LLM performance on noisy instructions by leveraging error correction capabilities.
- Mechanism: Using LLMs to clean noisy instructions before task processing enables better understanding and response generation.
- Core assumption: LLMs can detect and correct various noise types, and this capability can improve overall performance.
- Evidence anchors: Abstract notes correcting noisy instructions presents challenges; section reveals ChatGPT's comprehensive understanding of text; corpus mentions related work but doesn't address "re-pass" effectiveness.
- Break condition: If error correction model cannot accurately detect and correct specific noise types or introduces additional errors.

### Mechanism 3
- Claim: Open-source LLMs generally perform worse than closed-source models in handling and correcting noisy instructions.
- Mechanism: Performance differences likely due to variations in model architecture, training data, and fine-tuning processes.
- Core assumption: Closed-source models have access to more diverse training data and advanced architectures enabling better noise handling.
- Evidence anchors: Abstract shows all models experience degradation; section reveals ChatGPT's exception in data normalization; corpus doesn't directly compare open-source and closed-source performance.
- Break condition: If open-source models undergo significant improvements or closed-source models become less effective.

## Foundational Learning

- Concept: Error detection and correction in natural language processing
  - Why needed here: Understanding how LLMs detect and correct various types of noise is crucial for developing effective improvement strategies.
  - Quick check question: What are the key challenges in detecting and correcting grammatical, typographical, and ASR/OCR errors in natural language text?

- Concept: Tokenization strategies in LLMs
  - Why needed here: Tokenization method significantly impacts ability to handle noisy text by determining how corrupted words are processed.
  - Quick check question: How do different tokenization strategies (subword vs character-level) affect model ability to handle noisy text?

- Concept: In-context learning and few-shot prompting
  - Why needed here: These techniques can improve models' ability to understand and correct noisy instructions without extensive fine-tuning.
  - Quick check question: How can in-context learning and few-shot prompting improve LLM performance on noisy instructions?

## Architecture Onboarding

- Component map: Noise injection module -> Error correction module -> Task processing module -> Evaluation module
- Critical path: Noise injection → Error correction → Task processing → Evaluation
- Design tradeoffs:
  1. Balancing noise injection realism and computational efficiency: More realistic methods may be computationally expensive while simpler methods may not reflect real scenarios.
  2. Choosing appropriate error correction model: More powerful models yield better results but at higher computational cost; smaller models are more efficient but less effective.
  3. Determining optimal noise injection level: Too little noise may not adequately test resilience; too much may render instructions incomprehensible.
- Failure signatures:
  1. High error rates in error correction module: Indicates ineffective detection and correction of noise.
  2. Significant performance degradation in task processing module: Suggests cleaned instructions are still unclear for accurate responses.
  3. Inconsistent performance across different noise types: Implies system effectiveness varies by noise type, potentially due to training data or tokenization differences.
- First 3 experiments:
  1. Evaluate different error correction models on various noise types to determine most effective model for each type.
  2. Assess impact of different noise injection levels on system performance to identify optimal balance.
  3. Compare system performance with and without error correction module to quantify "re-pass" strategy benefits.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on single benchmark (MMLU) and five synthetic noise types, potentially not capturing real-world complexity
- "Re-pass" strategy implementation details are underspecified, particularly regarding prompt engineering
- Comparison between open-source and closed-source models is limited to three specific models without exploring broader model landscape

## Confidence

| Claim | Confidence |
|-------|------------|
| All tested models experience significant performance degradation with noisy instructions | High |
| ChatGPT-3.5 is more effective at instruction correction than open-source alternatives | Medium |
| Resilience differences are primarily due to training data exposure and tokenization strategies | Low |

## Next Checks
1. Test error correction effectiveness across broader range of open-source models to determine if ChatGPT advantage persists or is model-size dependent.
2. Implement ablation studies comparing different tokenization strategies on same model architecture to isolate tokenization impact on noise resilience.
3. Evaluate "re-pass" strategy on real-world noisy data from actual ASR and OCR systems rather than synthetic noise to assess practical applicability.