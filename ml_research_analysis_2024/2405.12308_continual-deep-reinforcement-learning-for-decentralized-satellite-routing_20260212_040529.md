---
ver: rpa2
title: Continual Deep Reinforcement Learning for Decentralized Satellite Routing
arxiv_id: '2405.12308'
source_url: https://arxiv.org/abs/2405.12308
tags:
- satellite
- each
- learning
- routing
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of decentralized packet routing
  in Low Earth Orbit satellite constellations, which requires handling partial knowledge
  at satellites, continuous movement, and time-varying uncertainty from traffic, links,
  and buffers. The core method combines multi-agent Deep Reinforcement Learning (DRL)
  with continual learning.
---

# Continual Deep Reinforcement Learning for Decentralized Satellite Routing

## Quick Facts
- arXiv ID: 2405.12308
- Source URL: https://arxiv.org/abs/2405.12308
- Reference count: 40
- Primary result: Multi-Agent DRL achieves same E2E performance as shortest-path with less communication overhead

## Executive Summary
This paper addresses decentralized packet routing in Low Earth Orbit satellite constellations using continual Deep Reinforcement Learning. The approach combines multi-agent DRL with continual learning through two strategies: model anticipation (exploiting predictable orbital dynamics) and Federated Learning (for long-term model alignment). Each satellite acts as an independent DRL agent making routing decisions based on local state information. The solution demonstrates that under normal traffic conditions, the DRL framework matches shortest-path performance while requiring significantly less communication overhead for network knowledge. The continual learning mechanisms effectively prevent model divergence as satellites move and traffic patterns evolve.

## Method Summary
The method employs a two-phase approach: an offline phase with global training and decentralized decisions, followed by an online phase with local, on-board pre-trained DNNs requiring continual learning. Each satellite acts as an independent agent using a DDQN architecture with separate Q-network and target network to reduce overestimation bias. The state space includes local queue information, neighbor congestion levels, and satellite coordinates. Two continual learning strategies are implemented: model anticipation where satellites share local models with neighbors, and Federated Learning where cluster-level and global model aggregation occurs. The framework handles partial observability by relying on local state information rather than requiring full network knowledge.

## Key Results
- MA-DRL framework achieves same E2E performance as shortest-path solutions under normal load
- Significant reduction in communication overhead compared to approaches requiring real-time network-wide knowledge
- Model divergence over time effectively addressed through combination of model anticipation and Federated Learning
- Framework adapts well to congestion conditions by exploiting less loaded paths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decentralized DRL routing with partial observability matches shortest-path performance under normal load without full network knowledge.
- Mechanism: Each satellite learns to forward packets based on local state (own queue + neighbors' congestion) and packet destination. The Q-network predicts the value of each possible next-hop action, enabling adaptive routing without requiring global state.
- Core assumption: Congestion levels and link connectivity of immediate neighbors are sufficient for good routing decisions.
- Evidence anchors:
  - [abstract] "the proposed Multi-Agent DRL framework achieves the same E2E performance as a shortest-path solution, but the latter assumes intensive communication overhead for real-time network-wise knowledge"
  - [section III] "the decision-making problem is based on a partially observable state" and "the state space of agent i is defined as Si : {Li, Ni}"
  - [corpus] No direct evidence - paper is original work, not citing this claim
- Break condition: If congestion or link failures extend beyond immediate neighbors, the partial observability assumption breaks and routing performance degrades.

### Mechanism 2
- Claim: Continual learning via model anticipation and Federated Learning prevents model divergence over time as satellites move and traffic patterns evolve.
- Mechanism: Model anticipation exploits predictable orbital dynamics by sharing Q-networks between consecutive satellites in the same orbital plane. Federated Learning aggregates cluster-level models through a Parameter Server for long-term alignment across the constellation.
- Core assumption: Satellite positions follow predictable orbits and local traffic patterns remain somewhat consistent within clusters.
- Evidence anchors:
  - [abstract] "the divergence of models over time is easily tackled by the synergy between anticipation, applied in short-term alignment, and FL, utilized for long-term alignment"
  - [section V] "the first one leverages the predictability of the constellation movement...The second option is FL...a decentralized collaborative approach"
  - [corpus] No direct evidence - paper is original work proposing these mechanisms
- Break condition: If orbital perturbations become significant or traffic patterns shift dramatically between clusters, the cluster-based aggregation becomes less effective.

### Mechanism 3
- Claim: DDQN with target networks stabilizes learning by reducing overestimation bias in Q-value updates.
- Mechanism: Separate Q-network and target network allow action selection from current Q-values while evaluation uses target Q-values, reducing TD error variance and improving convergence stability.
- Core assumption: The overestimation bias in standard DQN is significant enough to impact learning stability in this routing environment.
- Evidence anchors:
  - [section III-B] "the Target Network ( Q(θ−)) is utilized alongside Q(θ) to stabilize the learning process by addressing the overestimation bias present in the standard DQN algorithm"
  - [section III-B] "The key feature of DDQN is the separation of the action selection and evaluation"
  - [corpus] [161153] "On Multi-Agent Deep Q-Networks for Routing in Satellite Constellation Networks" - similar approach, suggests this is a known good practice
- Break condition: If the state-action space is small enough that overestimation bias is negligible, the additional complexity of DDQN may not be necessary.

## Foundational Learning

- Concept: Reinforcement Learning basics (states, actions, rewards, Q-learning)
  - Why needed here: The entire routing algorithm is based on agents learning optimal policies through trial and error
  - Quick check question: What is the difference between on-policy and off-policy RL, and which does Q-learning use?

- Concept: Deep Neural Networks for function approximation
  - Why needed here: The Q-value function is approximated by DNNs to handle the large state space of satellite positions and congestion levels
  - Quick check question: Why can't we use a simple Q-table for this problem, and how many states would a Q-table need?

- Concept: Federated Learning principles
  - Why needed here: Enables distributed model updates without sharing raw data, crucial for satellite constellations with limited connectivity
  - Quick check question: How does FedAvg aggregate models, and what assumption does it make about the data distribution across clients?

## Architecture Onboarding

- Component map:
  Environment: LEO constellation with satellites, gateways, ISLs, GSLs
  Agents: Each satellite as independent DRL agent
  Neural Networks: Q-Network and Target Network per agent
  Experience Buffers: Local buffers for online phase, global buffer for offline phase
  Communication: ISLs for neighbor congestion info, model exchange for FL

- Critical path:
  1. Packet arrives at gateway, enters constellation
  2. Satellite observes local state (own queue + neighbors' congestion + coordinates)
  3. Satellite selects action (next hop) using ε-greedy policy
  4. Packet forwarded, reward calculated based on queue time and distance
  5. Experience tuple stored, Q-network updated via DDQN
  6. Periodically, models shared via anticipation or FL for continual learning

- Design tradeoffs:
  - State complexity vs. DNN size: More detailed state information improves decisions but requires larger networks
  - Exploration rate decay: Faster decay speeds convergence but risks missing better paths
  - FL vs. model anticipation: FL provides better global alignment but higher communication overhead

- Failure signatures:
  - Models diverging significantly (high CKA values between satellite models)
  - High latency spikes indicating routing loops or poor path selection
  - Q-network weights oscillating without convergence

- First 3 experiments:
  1. Test basic routing with 2 gateways and Kepler constellation, verify convergence to shortest-path performance
  2. Introduce congestion by increasing traffic load, verify adaptive routing to less congested paths
  3. Simulate satellite movement over one orbital period, verify model anticipation maintains alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of model anticipation compare to federated learning in terms of long-term model alignment in non-stationary environments?
- Basis in paper: [explicit] The paper discusses model anticipation and federated learning as two strategies for continual learning, but does not provide a detailed comparison of their long-term effectiveness.
- Why unresolved: The paper mentions that model anticipation is effective for short-term alignment, while federated learning is used for long-term alignment, but does not quantify the differences in performance over extended periods.
- What evidence would resolve it: Empirical results comparing the performance of model anticipation and federated learning over long periods in dynamic environments, focusing on metrics like CKA and end-to-end latency.

### Open Question 2
- Question: What is the impact of varying traffic patterns on the routing decisions made by the multi-agent deep reinforcement learning framework?
- Basis in paper: [inferred] The paper discusses the framework's ability to adapt to congestion conditions, but does not explore the impact of different traffic patterns on routing decisions.
- Why unresolved: While the paper mentions adaptability to congestion, it does not specifically address how varying traffic patterns influence the routing decisions made by the framework.
- What evidence would resolve it: Simulation results showing the framework's performance under different traffic patterns, including peak and off-peak times, and how it adjusts routing decisions accordingly.

### Open Question 3
- Question: How does the granularity factor σ in the state space representation affect the performance of the deep neural network?
- Basis in paper: [explicit] The paper mentions that the granularity factor σ is set to 20, but does not explore the impact of different values on performance.
- Why unresolved: The paper provides a specific value for σ but does not investigate how changing this parameter affects the DNN's ability to learn and adapt.
- What evidence would resolve it: Experiments varying the granularity factor σ and measuring the impact on DNN performance, including metrics like convergence time and accuracy.

## Limitations

- The greedy matching algorithm for ISL establishment in general constellations is not fully specified
- Exact encoding of the congestion state space, particularly the Cj,k formula, lacks clarity
- No detailed comparison between model anticipation and federated learning performance over extended periods

## Confidence

**High confidence** in the core mechanism that decentralized DRL with partial observability can match shortest-path performance under normal load, supported by the claim that the framework achieves the same E2E performance with significantly less communication overhead.

**Medium confidence** in the continual learning approach combining model anticipation and Federated Learning, as the theoretical framework is well-established but the specific implementation details and parameter choices for satellite routing are not fully specified.

**Medium confidence** in the DDQN stability improvement claim, as this is a known technique from literature, but its specific effectiveness in the satellite routing context would depend on the exact problem parameters and training setup.

## Next Checks

1. Reproduce the greedy matching algorithm for ISL establishment in a general constellation topology and verify that it produces realistic ISL configurations comparable to the idealized Kepler constellation case.

2. Implement and test the congestion state encoding (Cj,k formula) with various traffic patterns to ensure the state representation captures meaningful differences in neighbor congestion levels that affect routing decisions.

3. Measure model divergence over time during the online phase using CKA (Centered Kernel Alignment) or similar metrics to quantify the effectiveness of model anticipation and Federated Learning in maintaining model alignment across the constellation.