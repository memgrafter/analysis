---
ver: rpa2
title: 'CLARINET: Augmenting Language Models to Ask Clarification Questions for Retrieval'
arxiv_id: '2405.15784'
source_url: https://arxiv.org/abs/2405.15784
tags:
- questions
- retrieval
- question
- book
- clarification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CLARINET, a system that learns to ask clarification
  questions for information retrieval by augmenting a large language model to condition
  on retrieval distributions. The approach finetunes an LLM end-to-end to generate
  questions that maximize the rank of the true candidate item by simulating interactions
  with a user proxy.
---

# CLARINET: Augmenting Language Models to Ask Clarification Questions for Retrieval

## Quick Facts
- arXiv ID: 2405.15784
- Source URL: https://arxiv.org/abs/2405.15784
- Reference count: 6
- Primary result: LLM-based clarification question system outperforms traditional methods by 17-39% on book retrieval tasks

## Executive Summary
This paper introduces CLARINET, a system that learns to ask effective clarification questions for information retrieval by augmenting a large language model (LLM) to condition on retrieval distributions. The approach finetunes an LLM end-to-end to generate questions that maximize the rank of the true candidate item by simulating interactions with a user proxy. When evaluated on a real-world dataset of users searching for books, CLARINET outperforms traditional information-theoretic methods like expected information gain and KL divergence by 17% on top-1 retrieval accuracy, and outperforms vanilla-prompted LLMs by 39% relative.

## Method Summary
CLARINET works by augmenting an LLM to condition on a retrieval distribution and finetuning it end-to-end to generate questions that would have maximized the rank of the true candidate at each turn. The system uses a Fusion-in-Decoder (FiD) architecture to encode retrieval candidates separately before attending to them in the decoder. Questions are generated, filtered based on their potential to increase the target's rank, and then a user simulator provides responses. The interaction history is summarized into a language posterior - a single natural language query describing what the system knows about the user's desired item - which is then used to produce a new candidate distribution.

## Key Results
- CLARINET outperforms traditional information-theoretic methods (EIG, KL divergence) by 17% on top-1 retrieval accuracy
- CLARINET outperforms vanilla-prompted LLMs by 39% relative on book retrieval tasks
- The system also outperforms dialogue-only approaches by asking better questions that help narrow down the user's desired item more effectively

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CLARINET learns to ask informative clarification questions by distilling the search over questions into an end-to-end model.
- **Mechanism:** The system finetunes a language model to generate questions that maximize the rank of the true candidate item by simulating interactions with a user proxy and selecting questions that significantly increase the confidence in the true item if answered.
- **Core assumption:** The LLM can learn to approximate the optimal question selection strategy through supervised finetuning on synthetic interaction data.
- **Evidence anchors:**
  - [abstract] "Our approach works by augmenting a large language model (LLM) to condition on a retrieval distribution, finetuning end-to-end to generate the question that would have maximized the rank of the true candidate at each turn."
  - [section] "We finetune a LLM to generate these questions qt conditioned on the interaction history and retrieval distribution."
- **Break condition:** If the LLM cannot effectively learn the relationship between questions and retrieval rank improvements, or if the synthetic interaction data is not representative of real user interactions.

### Mechanism 2
- **Claim:** CLARINET's use of a language posterior to summarize the interaction history into a single natural language query improves retrieval performance compared to maintaining an explicit posterior belief distribution.
- **Mechanism:** The system prompts a GPT-3 model to synthesize a description of the retrieval candidate given the interaction history thus far, which is then used as the search query to produce a new candidate distribution.
- **Core assumption:** The language posterior synthesized by the LLM is more robust to errors and can effectively capture the information provided by the user in the interaction history.
- **Evidence anchors:**
  - [abstract] "Then, given user responses to clarification questions, we summarize the interaction history into a language posterior, a single natural language query describing what the system knows about the user's desired item."
  - [section] "We prompt another GPT-3 model to synthesize a description of the retrieval candidate given the interaction history thus far, which can be thought of as a posterior belief over the true item, represented in language."
- **Break condition:** If the language posterior fails to accurately capture the information provided by the user or if the LLM struggles to synthesize a coherent summary of the interaction history.

### Mechanism 3
- **Claim:** CLARINET's fusion-in-decoder (FiD) architecture and selective training on questions that help increase the target's rank are crucial for effective clarification question generation.
- **Mechanism:** The model takes in the user's initial query, interaction history, and information of the top retrieval candidates to generate the next clarification question. The FiD architecture encodes each retrieval candidate separately before concatenating and attending to them in the decoder.
- **Core assumption:** The FiD architecture allows the model to better represent uncertainty given text descriptions, and training on questions that help increase the target's rank is as important as questions that give the target a high absolute rank.
- **Evidence anchors:**
  - [abstract] "We use a Fusion-in-Decoder architecture (FiD) (Izacard and Grave, 2020), where we concatenate the information for each retrieval candidate with the initial query and interaction history and feed it into the encoder independently."
  - [section] "The comparison between our model with FiD architecture and the fine-tuned t5 suggests that it is very hard for the model to explicitly represent uncertainty given text descriptions with small data."
- **Break condition:** If the FiD architecture does not provide significant benefits over other architectures or if the selective training strategy does not lead to better question generation.

## Foundational Learning

- **Concept:** Large Language Models (LLMs) and their capabilities in generating natural language text.
  - Why needed here: CLARINET relies on LLMs to generate clarification questions and synthesize the language posterior.
  - Quick check question: What are the key components of an LLM architecture, and how do they contribute to its ability to generate coherent and contextually relevant text?

- **Concept:** Information Retrieval (IR) systems and their role in finding relevant documents or items based on user queries.
  - Why needed here: CLARINET uses a retrieval model to rank candidate items and update the retrieval distribution based on user responses to clarification questions.
  - Quick check question: What are the main challenges in designing an effective IR system, and how do different retrieval models (e.g., dense passage retrieval, BM25) address these challenges?

- **Concept:** Reinforcement Learning (RL) and its application in training agents to make sequential decisions.
  - Why needed here: CLARINET's approach to selecting informative questions by simulating interactions with a user proxy and training on questions that significantly increase the confidence in the true item is related to RL concepts.
  - Quick check question: How does the concept of reward function in RL relate to CLARINET's approach of selecting questions that maximize the rank of the true candidate item?

## Architecture Onboarding

- **Component map:** User Query -> Dense Passage Retriever -> Question Generator (Finetuned LLM) -> Question Selector -> User Simulator (GPT-3) -> Language Posterior Synthesizer (GPT-3) -> Updated Query -> Repeat

- **Critical path:**
  1. User provides initial query.
  2. Retriever ranks candidate items and computes retrieval uncertainty.
  3. Question generator produces candidate clarification questions.
  4. Question selector filters and selects the most informative question.
  5. User simulator provides a response to the selected question.
  6. Language posterior synthesizer updates the retrieval query based on the interaction history.
  7. Retriever re-ranks candidate items using the updated query.
  8. Repeat steps 3-7 for a fixed number of turns or until the correct item is retrieved.

- **Design tradeoffs:**
  - Using a finetuned LLM for question generation allows for more flexible and contextually relevant questions but may require more training data and computational resources.
  - The language posterior approach is more robust to errors but may not capture all the nuances of the interaction history compared to maintaining an explicit posterior belief distribution.
  - The FiD architecture enables better representation of uncertainty but increases model complexity and memory requirements.

- **Failure signatures:**
  - Poor retrieval performance: The model may be generating irrelevant or uninformative questions, or the language posterior may not effectively capture the user's intent.
  - High computational cost: The model may be inefficient in generating and evaluating candidate questions, especially when using information-theoretic approaches like EIG or KL divergence.
  - Overfitting to the training data: The model may not generalize well to unseen user queries or may generate questions that are too specific to the training dataset.

- **First 3 experiments:**
  1. Evaluate the impact of different retriever models (e.g., DPR, BM25) on CLARINET's performance to understand the importance of the retrieval component.
  2. Compare CLARINET's question generation approach with a rule-based or template-based method to assess the benefits of using a finetuned LLM.
  3. Investigate the effect of varying the number of candidate questions generated at each turn on CLARINET's performance and computational efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the robustness of the language posterior to errors in user responses or retrieval model failures?
- Basis in paper: [inferred]
- Why unresolved: The paper mentions that the explicit posterior belief distribution is less robust to errors, but does not propose a solution to improve the robustness of the language posterior.
- What evidence would resolve it: An experiment comparing the performance of the language posterior with and without a mechanism to handle errors in user responses or retrieval model failures.

### Open Question 2
- Question: How can we make the question generation process more interpretable, similar to EIG and KL approaches?
- Basis in paper: [inferred]
- Why unresolved: The paper states that the neural question generator's question generation preference is less interpretable than EIG and KL, but does not propose a method to make it more interpretable.
- What evidence would resolve it: A study comparing the interpretability of the neural question generator with EIG and KL approaches, possibly using human evaluation or analysis of the generated questions.

### Open Question 3
- Question: How can we efficiently estimate EIG and KL at inference time to enable more interpretable question selection?
- Basis in paper: [explicit]
- Why unresolved: The paper mentions that traditional heuristic approaches like EIG are computationally expensive at inference time, but does not propose a method to estimate them more efficiently.
- What evidence would resolve it: An experiment comparing the inference time and retrieval performance of a model using an efficient EIG or KL estimation method with the proposed Clarinet model.

## Limitations

- The evaluation is conducted on a relatively small dataset of book searches from Goodreads, which may not represent the broader challenges of information retrieval across different domains.
- The reliance on GPT-3 for both question generation and user simulation introduces potential variability that isn't fully characterized in the paper.
- The paper doesn't explore how CLARINET performs with different retriever architectures beyond DPR, nor does it examine the computational costs of the approach compared to simpler baselines.

## Confidence

**High Confidence:** The core mechanism of using a finetuned LLM to generate clarification questions that maximize the rank of the true candidate item is well-supported by the experimental results, showing consistent improvements over traditional information-theoretic methods and vanilla-prompted LLMs.

**Medium Confidence:** The claim that the language posterior approach is more robust to errors compared to maintaining an explicit posterior belief distribution is plausible but not extensively validated across different error scenarios or domains.

**Medium Confidence:** The assertion that the FiD architecture and selective training strategy are crucial for effective clarification question generation is supported by ablation studies, but the paper doesn't provide sufficient analysis of why these components specifically contribute to performance improvements.

## Next Checks

1. **Cross-Domain Evaluation:** Test CLARINET on datasets from different domains (e.g., movie recommendations, product searches) to assess its generalizability beyond book searches and validate whether the improvements hold across various types of information retrieval tasks.

2. **Error Analysis Under Adversarial Conditions:** Systematically introduce noise into the user responses or retrieval results to evaluate how well the language posterior approach handles errors compared to explicit posterior distributions, and identify the breaking points where performance degrades.

3. **Computational Cost Analysis:** Measure and compare the inference time and memory requirements of CLARINET against baseline approaches across different dataset sizes to quantify the practical deployment considerations and identify potential optimizations.