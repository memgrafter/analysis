---
ver: rpa2
title: Unsupervised Bilingual Lexicon Induction for Low Resource Languages
arxiv_id: '2412.16894'
source_url: https://arxiv.org/abs/2412.16894
tags:
- embeddings
- word
- bilingual
- language
- uvecmap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether combining multiple improvements
  to structure-based unsupervised bilingual lexicon induction (UBLI) methods can yield
  better performance than using them individually. The authors extend the UVecMap
  framework with techniques including dimensionality reduction, linear transformation,
  embedding fusion, and combining static with contextual embeddings.
---

# Unsupervised Bilingual Lexicon Induction for Low Resource Languages

## Quick Facts
- arXiv ID: 2412.16894
- Source URL: https://arxiv.org/abs/2412.16894
- Reference count: 40
- Combining multiple improvements to structure-based UBLI methods achieves up to 32.84% precision@1 on English-Sinhala with Word2Vec embeddings.

## Executive Summary
This study investigates whether combining multiple enhancements to structure-based unsupervised bilingual lexicon induction (UBLI) methods can improve performance for low-resource language pairs. The authors extend the UVecMap framework with techniques including dimensionality reduction, linear transformation, embedding fusion, and combining static with contextual embeddings. Experiments on English-Sinhala, English-Tamil, and English-Punjabi pairs demonstrate that the combination of CSCBLI with linear transformation and UVecMap achieves the best results, with precision@1 scores reaching up to 29.49% for FastText and 32.84% for Word2Vec embeddings on English-Sinhala. The study also releases new human-curated bilingual dictionaries for English-Sinhala and English-Punjabi.

## Method Summary
The study builds upon the UVecMap framework by integrating multiple enhancement techniques: dimensionality reduction using PCA, linear transformation to align embedding spaces, CSCBLI for embedding fusion, and combining static with contextual embeddings. The approach filters vocabulary based on frequency thresholds, applies optional dimensionality reduction, performs linear transformation on embeddings, and uses the UVecMap self-learning loop for cross-lingual mapping. The method is evaluated on English-Sinhala, English-Tamil, and English-Punjabi pairs using precision@1 against human-curated test dictionaries.

## Key Results
- The combination of CSCBLI with linear transformation and UVecMap achieves the best performance
- Precision@1 scores reach up to 29.49% for FastText and 32.84% for Word2Vec embeddings on English-Sinhala
- The study releases new human-curated bilingual dictionaries for English-Sinhala and English-Punjabi
- UVecMap with combined enhancements outperforms baseline methods across all tested language pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining dimensionality reduction with linear transformation and CSCBLI improves UBLI performance for low-resource languages.
- Mechanism: Dimensionality reduction via PCA reduces noise and redundancy in high-dimensional embeddings, making subsequent transformations more effective. Linear transformation aligns the similarity order of embedding spaces, enhancing isomorphism. CSCBLI fuses static and contextual embeddings to capture richer semantic relationships.
- Core assumption: The original embedding spaces contain noise and are not perfectly isomorphic, and their alignment can be improved by preprocessing and fusion techniques.
- Evidence anchors:
  - [abstract] "Experiments on English-Sinhala, English-Tamil, and English-Punjabi pairs show that the combination of CSCBLI with linear transformation and UVecMap achieves the best results, with precision@1 scores reaching up to 29.49% for FastText and 32.84% for Word2Vec embeddings on English-Sinhala."
  - [section 3.2] "Linear transformation... extends the traditional notion of similarity from first and second-order measures to higher ð‘›-th order similarities, enabling more robust alignment between the embedding spaces."
  - [corpus] Weak: neighbor papers do not directly support this specific combination of techniques.
- Break condition: If the monolingual corpora are too small or too noisy, dimensionality reduction may discard meaningful variance, leading to performance degradation.

### Mechanism 2
- Claim: Unsupervised BLI can be effective without parallel corpora by leveraging monolingual data and iterative self-learning.
- Mechanism: UVecMap starts with an initial dictionary derived from monolingual embeddings, then iteratively refines the cross-lingual mappings by optimizing for maximum similarity and recomputing the dictionary until convergence.
- Core assumption: The monolingual embedding spaces share enough structural similarity to allow cross-lingual alignment without supervision.
- Evidence anchors:
  - [section 2.1] "Post-alignment techniques, which have been more commonly used, start from the monolingual embeddings of the two languages and learn a mapping across their distributions."
  - [section 2.1] "Most structure-based methods are iterative - meaning that a seed lexicon is initially created, which is used to learn an optimal mapping between the embedding spaces of the two languages."
  - [corpus] Weak: neighbor papers discuss related methods but do not confirm the effectiveness of UVecMap's specific iterative approach.
- Break condition: If the languages are too distant or the monolingual corpora are of poor quality, the iterative refinement may converge to a suboptimal or meaningless alignment.

### Mechanism 3
- Claim: Proper selection of minimum frequency thresholds for word embeddings balances coverage and accuracy in UBLI.
- Mechanism: Filtering out infrequent words reduces noise from unreliable embeddings while retaining sufficient vocabulary for meaningful alignment.
- Core assumption: Infrequent words contribute more noise than signal to the embedding spaces, and their removal improves the quality of the induced lexicon.
- Evidence anchors:
  - [section 4.1] "Previous research suggested that keeping the full word list is not effective, due to the existence of rare words... Therefore, the resulting word lists were filtered based on predefined minimum frequency ð‘€ð‘–ð‘›_ð¹ð‘Ÿð‘’ð‘ž threshold values."
  - [section 4.4] "The thresholds were carefully selected via an ablation study to balance coverage... and accuracy."
  - [corpus] Weak: neighbor papers do not address the specific impact of frequency thresholds on UBLI performance.
- Break condition: If the threshold is set too high, important domain-specific or low-frequency words may be excluded, reducing the utility of the induced lexicon.

## Foundational Learning

- Concept: Principal Component Analysis (PCA) for dimensionality reduction
  - Why needed here: PCA is used to reduce the dimensionality of word embeddings before alignment, which helps remove noise and computational overhead.
  - Quick check question: What is the primary goal of applying PCA to word embeddings in the context of UBLI?

- Concept: Singular Value Decomposition (SVD) for orthogonal mapping
  - Why needed here: SVD is used to compute the optimal orthogonal transformation between aligned embedding spaces during the self-learning step of UVecMap.
  - Quick check question: How does SVD contribute to finding the best cross-lingual mapping in iterative UBLI methods?

- Concept: Cross-domain Similarity Local Scaling (CSLS)
  - Why needed here: CSLS is used instead of nearest-neighbor retrieval to reduce the hubness problem when inducing dictionaries from aligned embeddings.
  - Quick check question: What problem does CSLS address that simple nearest-neighbor retrieval cannot?

## Architecture Onboarding

- Component map: Monolingual corpus preprocessing -> Word embedding generation (Word2Vec/FastText/XLM-R) -> Frequency filtering -> Dimensionality reduction (optional) -> Linear transformation (optional) -> UVecMap framework (normalization, initialization, self-learning) -> Evaluation with human-curated lexicon
- Critical path: Embedding generation -> Frequency filtering -> UVecMap self-learning loop -> Dictionary evaluation
- Design tradeoffs:
  - Higher dimensionality preserves more information but increases computation and noise; lower dimensionality is faster but may lose nuance.
  - More aggressive frequency filtering improves embedding quality but reduces coverage of rare but potentially important words.
  - Using contextual embeddings (XLM-R) can capture richer semantics but requires more memory and careful hyperparameter tuning.
- Failure signatures:
  - Zero precision@1 scores indicate mapping failure, often due to incompatible embeddings or poor initialization.
  - Performance drops when combining techniques suggest interference between methods or hyperparameter misconfiguration.
  - High variance across language pairs indicates sensitivity to language similarity or corpus quality.
- First 3 experiments:
  1. Run UVecMap baseline with Word2Vec and FastText embeddings on EnSi to establish performance baseline.
  2. Apply linear transformation alone to the baseline embeddings and measure precision@1 change.
  3. Combine linear transformation with dimensionality reduction and evaluate whether the combination outperforms either technique alone.

## Open Questions the Paper Calls Out
None

## Limitations

- The evaluation relies entirely on manually curated test dictionaries, which may not fully represent real-world usage complexity.
- The reported results are based on a limited set of language pairs and may not generalize to other low-resource combinations.
- The study does not provide detailed analysis of corpus size and quality impact on proposed methods.
- Hyperparameter tuning was performed on a case-by-case basis without establishing general guidelines.

## Confidence

- **High confidence**: The core methodology of combining dimensionality reduction, linear transformation, and embedding fusion within UVecMap framework is well-defined and reproducible.
- **Medium confidence**: The claim that this combination is optimal for all low-resource language pairs is plausible but not fully validated across diverse language families.
- **Low confidence**: The assertion that the proposed method will scale effectively to even lower-resource settings is speculative and requires further empirical validation.

## Next Checks

1. **Cross-lingual robustness test**: Evaluate the proposed framework on a broader set of low-resource language pairs, including those from different language families (e.g., English-Amharic, English-Kurmanji), to assess generalizability.
2. **Corpus size sensitivity analysis**: Systematically vary the size of monolingual corpora and measure the impact on precision@1 scores to determine the minimum viable corpus size for each language pair.
3. **Real-world application pilot**: Apply the induced lexicons to a downstream task (e.g., cross-lingual information retrieval or machine translation) and measure the practical utility of the automatically generated dictionaries compared to manually curated ones.