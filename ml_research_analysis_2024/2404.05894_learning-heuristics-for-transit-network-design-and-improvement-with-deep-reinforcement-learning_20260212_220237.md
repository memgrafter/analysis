---
ver: rpa2
title: Learning Heuristics for Transit Network Design and Improvement with Deep Reinforcement
  Learning
arxiv_id: '2404.05894'
source_url: https://arxiv.org/abs/2404.05894
tags:
- transit
- network
- neural
- each
- mumford
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a reinforcement learning approach to improve
  heuristics for the transit network design problem (TNDP). Instead of using random
  heuristics, the authors train graph neural networks via Proximal Policy Optimization
  to make intelligent modifications to transit networks.
---

# Learning Heuristics for Transit Network Design and Improvement with Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2404.05894
- Source URL: https://arxiv.org/abs/2404.05894
- Reference count: 40
- Primary result: Neural evolutionary algorithms with RL-trained graph neural networks achieve new state-of-the-art results on transit network design benchmarks, improving cost savings by up to 4.8% over previous methods

## Executive Summary
This paper presents a reinforcement learning approach to improve heuristics for the transit network design problem (TNDP). Instead of using random heuristics, the authors train graph neural networks via Proximal Policy Optimization to make intelligent modifications to transit networks. These neural heuristics are integrated into a simple evolutionary algorithm, forming a Neural Evolutionary Algorithm (NEA). The method achieves new state-of-the-art results on challenging benchmark cities, improving cost savings by up to 4.8% over previous methods. When applied to a real-world simulation of Laval, Canada, the approach reduces both passenger travel time and operating costs by up to 19% compared to the existing transit network, demonstrating practical applicability for transit agencies seeking to optimize service while reducing expenses.

## Method Summary
The authors formulate transit network design as a Markov Decision Process where states represent current networks and actions modify these networks. A graph neural network policy is trained via Proximal Policy Optimization to maximize cumulative reward defined as cost reduction between network states. This neural policy replaces random heuristics in an evolutionary algorithm framework, creating a Neural Evolutionary Algorithm. The approach factors action scoring to scale to large cities, computing node-pair scores rather than exhaustive path scoring. The method is evaluated on benchmark synthetic cities and a real-world case study of Laval, Canada.

## Key Results
- Neural evolutionary algorithms achieve new state-of-the-art results on Mumford benchmark cities, improving cost savings by up to 4.8%
- On Laval, Canada (632 nodes), the approach reduces passenger travel time and operating costs by up to 19% compared to the existing network
- Performance advantage increases with city size, suggesting scalability to larger urban areas
- The neural heuristic outperforms random heuristics in most scenarios, particularly for balanced cost objectives (α < 1.0)

## Why This Works (Mechanism)

### Mechanism 1
Neural heuristics outperform random heuristics because they learn to assign higher probability to network modifications that improve the objective function. The graph neural network policy is trained via Proximal Policy Optimization to maximize the cumulative reward, which is defined as the decrease in cost between consecutive network states. During training, the GNN learns to assign higher scores to node pairs that, when connected, lead to larger cost reductions. In the evolutionary algorithm, these learned preferences guide the selection of network modifications. The reward signal (cost reduction) provides sufficient and accurate feedback for the GNN to learn useful heuristics.

### Mechanism 2
The factorization of the policy computation allows the method to scale to large city graphs. Instead of computing a score for every possible extension path (which could be exponentially many), the policy computes a score for each node pair. The score for an extension path is then the sum of the scores for the node pairs it connects. This reduces the computational complexity from O(|A_t|) to O(|N|^2). The merit of adding a path to a route can be approximated by the sum of the merits of connecting its constituent node pairs.

### Mechanism 3
The evolutionary algorithm with neural heuristics explores the solution space more effectively than with random heuristics. The evolutionary algorithm applies two types of heuristics: type-1 (random) and type-2 (learned). The neural heuristic replaces the type-1 heuristic in the variant algorithm. By using learned preferences, the neural heuristic is more likely to make beneficial changes, leading to faster convergence and better solutions. The learned policy generalizes well from the training environment to the test environments.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The MDP formalism is used to define the problem of constructing a transit network as a sequence of decisions, where each decision (action) affects the state of the network and receives a reward.
  - Quick check question: What are the four components of an MDP, and how are they defined in the transit network construction problem?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are used to process the graph-structured data of the city (nodes, edges, demands) and output a policy that suggests network modifications.
  - Quick check question: How does a GNN differ from a standard neural network, and why is it suitable for processing graph-structured data?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is the reinforcement learning algorithm used to train the GNN policy to maximize the cumulative reward.
  - Quick check question: What are the key features of PPO that make it suitable for training the GNN policy, and how does it differ from other policy gradient methods like REINFORCE?

## Architecture Onboarding

- Component map: City graph -> MDP state -> GNN policy -> action -> new state -> reward -> update policy
- Critical path: City graph → MDP state → GNN policy → action → new state → reward → update policy
- Design tradeoffs:
  - Using a GNN vs. a simpler model: GNN can capture complex graph structures but is more computationally expensive
  - Factorizing policy computation vs. computing scores for all paths: Factorization scales better but may lose some information
  - Training on synthetic cities vs. real cities: Synthetic cities provide more data but may not capture all real-world complexities
- Failure signatures:
  - Poor performance on large cities: May indicate that the factorization approximation is breaking down
  - Overfitting to training environments: May indicate that the learned policy is not generalizing well
  - Slow convergence of the evolutionary algorithm: May indicate that the learned heuristics are not effective
- First 3 experiments:
  1. Train the GNN policy on a small synthetic city and evaluate its performance on a held-out test set.
  2. Compare the performance of the evolutionary algorithm with neural heuristics vs. random heuristics on a medium-sized synthetic city.
  3. Apply the trained policy to a real-world city (e.g., Laval) and compare the results to the existing transit network.

## Open Questions the Paper Calls Out

### Open Question 1
Why does the random path-combining heuristic (RC-EA) outperform the neural net heuristic at α=1.0 despite the neural net theoretically being able to learn to imitate random behavior? The authors note that RC-EA achieves better C_p than NEA at α=1.0, and state that "we would expect that the neural net policy used in NEA should be able to equal the performance of RC-EA by learning to choose actions uniformly at random when α=1.0, yet here it fails to do so." This discrepancy is acknowledged but not definitively explained.

### Open Question 2
How would neural heuristics perform when trained directly in the context of an improvement process rather than a construction process? The authors state that "One limitation is that the construction process on which our neural net policies are trained differs from the evolutionary algorithm - an improvement process - in which it is deployed. We would like to train our neural net policies directly in the context of an improvement process, which may make them better-suited to that process."

### Open Question 3
Can neural heuristics be effectively applied to larger cities with thousands of nodes, beyond the 632-node Laval case study? The authors demonstrate success on Laval (632 nodes) and note that "the pattern in these results, where the relative performance of our heuristics increases with the size of the city n, also suggests that their advantage may continue to grow for cities larger than those found in these benchmarks."

### Open Question 4
Would a more diverse set of neural policies trained for different types of network modifications (beyond just concatenating shortest paths) further improve metaheuristic performance? The authors state that "Another limitation is that we only train neural net policies to make one kind of change to a transit network: concatenating multiple shortest paths into a new route and adding it to the network. In future, we wish to train a more diverse set of policies, perhaps including route-lengthening and -shortening operators that also could be included as heuristics in a metaheuristic algorithm."

## Limitations

- The neural policy fails to match random performance at α=1.0, suggesting limitations in learning uniform random behavior
- Neural heuristics are trained via construction processes but deployed in improvement algorithms, creating a potential mismatch
- Current neural policies are limited to one type of modification (concatenating shortest paths), missing opportunities for other network operations

## Confidence

- Claim: Neural heuristics outperform random heuristics by up to 4.8% → **Medium confidence** (compelling benchmark results but limited to specific test cases)
- Claim: Scalability to large cities via factorization → **Low confidence** (theoretically sound but lacks systematic ablation studies)
- Claim: Combining RL-trained heuristics with evolutionary algorithms creates superior approach → **Medium confidence** (improved performance but doesn't fully isolate contributions)

## Next Checks

1. Test the trained policy on cities with fundamentally different structures (e.g., highly radial vs. grid-like layouts) not represented in the training distribution
2. Conduct systematic ablation studies varying city size beyond the 70+ node threshold to identify the exact scalability limits of the factorization approach
3. Compare against alternative meta-heuristic combinations (e.g., simulated annealing with neural heuristics) to better understand the specific value added by the evolutionary framework