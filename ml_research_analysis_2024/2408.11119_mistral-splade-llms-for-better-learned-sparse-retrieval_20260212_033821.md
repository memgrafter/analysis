---
ver: rpa2
title: 'Mistral-SPLADE: LLMs for better Learned Sparse Retrieval'
arxiv_id: '2408.11119'
source_url: https://arxiv.org/abs/2408.11119
tags:
- sparse
- retrieval
- splade
- dense
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Echo-Mistral-SPLADE, a learned sparse retriever
  that leverages a decoder-only LLM (Mistral-7B) with echo embeddings to improve semantic
  keyword expansion. The method aims to address the vocabulary mismatch problem in
  information retrieval by learning interpretable sparse representations that can
  be combined with inverted indexing for efficient retrieval.
---

# Mistral-SPLADE: LLMs for better Learned Sparse Retrieval

## Quick Facts
- arXiv ID: 2408.11119
- Source URL: https://arxiv.org/abs/2408.11119
- Reference count: 21
- Primary result: Echo-Mistral-SPLADE achieves state-of-the-art performance among sparse retrieval models on BEIR benchmark

## Executive Summary
Echo-Mistral-SPLADE introduces a learned sparse retriever that leverages Mistral-7B, a decoder-only LLM, with Echo embeddings to improve semantic keyword expansion. The method addresses vocabulary mismatch in information retrieval by learning interpretable sparse representations that can be combined with inverted indexing for efficient retrieval. Trained on diverse sentence-transformer data without hard negative mining or distillation, the model achieves state-of-the-art performance among sparse retrieval models on BEIR and competitive results compared to similar-sized dense models.

## Method Summary
The approach uses Mistral-7B as a backbone for learned sparse retrieval, similar to SPLADE but adapted for decoder-only architectures. The model employs Echo embeddings to mitigate unidirectional attention limitations, where the same text is passed twice to the LLM and the second occurrence's representation is averaged. Training uses a subset of 15.5M sentence-transformer samples without hard negative mining or distillation, relying instead on in-batch negatives with a batch size of 512. QLoRA finetuning is applied with specific parameters (Î± = 8, rank = 16, dropout = 0.1) using the ADAM optimizer with a learning rate of 2e-5 and linear scheduling with 6000-step warmup.

## Key Results
- Echo-Mistral-SPLADE achieves state-of-the-art performance among sparse retrieval models on the BEIR benchmark
- The model shows competitive results compared to similar-sized dense models, particularly in out-of-domain settings
- Performance improvements are demonstrated over existing SPLADE variants without requiring hard negative mining or distillation

## Why This Works (Mechanism)

### Mechanism 1
Decoder-only LLMs trained on massive data learn better semantic keyword expansions for sparse retrieval. Mistral-7B leverages its extensive pretraining to produce richer and more semantically diverse keyword expansions compared to encoder-only models like BERT. Core assumption: decoder-only models, having seen much higher magnitudes of data, are better equipped to learn keyword expansions needed for improved retrieval.

### Mechanism 2
Echo embeddings mitigate the unidirectional attention limitation of decoder-only models, improving representation quality. By feeding the same text twice to the LLM and averaging the second occurrence's representation, Echo embeddings allow the model to attend to the full context, overcoming the inherent weakness of unidirectional attention. Core assumption: the repetition and averaging process effectively simulates bidirectional attention for representation learning.

### Mechanism 3
Training on diverse sentence-transformer data without hard negative mining or distillation leads to competitive retrieval performance. Using a large-scale, diverse dataset for training the sparse retriever allows the model to learn more robust and generalizable keyword expansions, compensating for the lack of hard negatives or distillation. Core assumption: the diversity and scale of the training data are sufficient to overcome the limitations of not using hard negatives or distillation.

## Foundational Learning

- Concept: Learned Sparse Retrieval (LSR)
  - Why needed here: Understanding LSR is crucial for grasping the core contribution of Echo-Mistral-SPLADE, which leverages LLMs for semantic keyword expansion.
  - Quick check question: What is the main advantage of LSR over traditional keyword-based sparse retrievers and dense retrievers?

- Concept: Echo Embeddings
  - Why needed here: Echo embeddings are a key innovation in Echo-Mistral-SPLADE, addressing the unidirectional attention limitation of decoder-only models.
  - Quick check question: How do Echo embeddings mitigate the unidirectional attention limitation of decoder-only models?

- Concept: LoRA Finetuning
  - Why needed here: LoRA finetuning is used to adapt Mistral-7B for SPLADE training while maintaining interpretability and reducing the number of trainable parameters.
  - Quick check question: What is the main advantage of using LoRA finetuning for adapting Mistral-7B for SPLADE training?

## Architecture Onboarding

- Component map: Mistral-7B LLM -> Echo Embeddings -> LoRA Finetuning -> Sentence-Transformer Data -> Echo-Mistral-SPLADE
- Critical path: Mistral-7B (LLM) provides semantic keyword expansion capabilities, Echo Embeddings address unidirectional attention limitations, LoRA Finetuning adapts the model for SPLADE training, and diverse Sentence-Transformer Data enables robust learning without hard negatives or distillation.
- Design tradeoffs:
  - Using decoder-only LLM vs. encoder-only models: Potential for richer semantic expansions but requires Echo embeddings to address unidirectional attention
  - Training on diverse data vs. hard negative mining/distillation: Reduced computational cost but potential for lower performance
- Failure signatures:
  - Poor retrieval performance: LLM may not be well-suited for semantic keyword expansion or Echo embeddings may not effectively mitigate unidirectional attention
  - High computational cost: LoRA finetuning may not sufficiently reduce the number of trainable parameters
  - Interpretability issues: Tied weight matrices may not be maintained, leading to uninterpretable sparse representations
- First 3 experiments:
  1. Evaluate retrieval performance on a small, in-domain dataset to assess the effectiveness of Echo-Mistral-SPLADE
  2. Compare retrieval performance with and without Echo embeddings to quantify their impact
  3. Analyze the sparsity and interpretability of the learned representations to ensure they align with the goals of LSR

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Echo-Mistral-SPLADE compare to dense retrieval models of similar size when trained on the same diverse dataset (Sentence-Transformers data) without hard negative mining or distillation? The paper compares Echo-Mistral-SPLADE to dense models like LLM2Vec and Echo Embeddings on BEIR, but LLM2Vec uses additional unsupervised pretraining and both use some BEIR training data. What evidence would resolve it: Training Echo-Mistral-SPLADE and a comparable dense model on the same diverse dataset without hard negative mining or distillation, then comparing their zero-shot performance on BEIR.

### Open Question 2
What is the impact of using diverse training data (Sentence-Transformers data) compared to the traditional MS Marco dataset on the performance and generalization of learned sparse retrieval models? The paper hypothesizes that decoder-only models trained on diverse data are better equipped to learn keyword expansions, and uses Sentence-Transformers data instead of MS Marco. What evidence would resolve it: Training two identical Echo-Mistral-SPLADE models, one on MS Marco and one on Sentence-Transformers data, and comparing their zero-shot performance on BEIR.

### Open Question 3
How does the performance of Echo-Mistral-SPLADE scale with model size, and what is the optimal model size for balancing performance and computational efficiency? The paper uses Mistral-7B as the backbone but does not explore other model sizes or analyze the scaling behavior. What evidence would resolve it: Training Echo-Mistral-SPLADE with different model sizes on the same dataset and comparing their zero-shot performance on BEIR, while also measuring computational efficiency.

## Limitations

- The paper lacks direct comparisons to dense retrievers trained on the same dataset without hard negative mining or distillation
- No detailed ablation studies are provided to isolate the contributions of Echo embeddings, Mistral-7B architecture, and training data diversity
- The claims about decoder-only LLMs inherently producing better keyword expansions lack direct comparative evidence

## Confidence

- High confidence: The basic SPLADE framework adaptation to Mistral-7B and the training methodology are well-established
- Medium confidence: The Echo embedding implementation appears sound, though its specific impact is not rigorously quantified
- Low confidence: The claim that decoder-only LLMs inherently produce better keyword expansions without direct comparative evidence

## Next Checks

1. Conduct direct comparisons between Echo-Mistral-SPLADE and similar-sized dense retrievers on the same BEIR benchmark tasks
2. Perform ablation studies isolating the contributions of Echo embeddings, Mistral-7B architecture, and the training data diversity
3. Test the model's interpretability claims by analyzing the learned sparse representations and their correlation with actual keyword importance in queries