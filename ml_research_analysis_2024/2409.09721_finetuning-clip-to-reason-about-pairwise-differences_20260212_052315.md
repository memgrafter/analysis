---
ver: rpa2
title: Finetuning CLIP to Reason about Pairwise Differences
arxiv_id: '2409.09721'
source_url: https://arxiv.org/abs/2409.09721
tags:
- image
- clip
- pc-clip
- text
- differences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes PC-CLIP, a method to improve CLIP\u2019s ability\
  \ to reason about differences in image embedding space. By using LLMs to generate\
  \ text descriptions of differences between image-caption pairs and aligning these\
  \ with image embedding differences via contrastive finetuning, PC-CLIP enables CLIP\
  \ to perform difference-based classification and comparative prompting."
---

# Finetuning CLIP to Reason about Pairwise Differences

## Quick Facts
- arXiv ID: 2409.09721
- Source URL: https://arxiv.org/abs/2409.09721
- Reference count: 39
- Key outcome: Finetunes CLIP using LLM-generated difference descriptions to improve geometric structure in embedding space, enabling better difference-based classification, comparative prompting, and text-to-image generation arithmetic.

## Executive Summary
This paper introduces PC-CLIP, a method that finetunes CLIP to improve its ability to reason about pairwise differences between images. By using an LLM to generate text descriptions of differences between image-caption pairs and aligning these with image embedding differences via contrastive finetuning, PC-CLIP enables CLIP to perform difference-based classification and comparative prompting. The approach significantly improves CLIP's geometric structure in embedding space, allowing for better reasoning about visual differences and enabling meaningful arithmetic operations in embedding space.

## Method Summary
PC-CLIP uses an LLM to generate text descriptions of differences between image-caption pairs from the COCO dataset. These synthetic comparisons are then used to finetune CLIP's text encoder via a contrastive loss that aligns the difference embeddings with the corresponding image embedding differences. The finetuned model can then perform difference-based classification by comparing image embeddings to text embeddings of class differences, and comparative prompting by updating class prompts using known differences between classes. The approach is evaluated on multiple downstream tasks including difference-based classification, zero-shot classification, cross-modal retrieval, and text-to-image generation.

## Key Results
- PC-CLIP achieves up to 14% accuracy improvement on difference-based classification tasks compared to standard CLIP
- Maintains or improves zeroshot classification performance on most tested datasets (CIFAR100, CUB, EuroSAT, Flowers102, SUN397)
- Better localizes classes and their differences in embedding space, enabling more effective comparative prompting
- Improves text-to-image generation quality when using arithmetic in embedding space, producing more semantically meaningful results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning text descriptions of image differences with image embedding differences improves CLIP's geometric structure.
- Mechanism: By finetuning CLIP so that text descriptions of differences between images correspond to their difference in image embedding space, the model learns to localize meaningful differences like size and color in embedding space.
- Core assumption: LLMs can generate meaningful descriptions of differences between image pairs, and these descriptions accurately capture the semantic differences.
- Evidence anchors: [abstract] "We finetune CLIP so that text descriptions of differences between images correspond to their difference in image embedding space, using synthetically generated data with large language models" [section 3.2] "we define our objective as follows min f,g ℓ(g(I1)−g(I2),f(T1,2)), where g,f represent our image and text encoders respectively"
- Break condition: If LLM-generated differences are not semantically meaningful or fail to capture the actual visual differences, the alignment will not improve geometric structure.

### Mechanism 2
- Claim: Comparative prompting leverages prior knowledge of class differences to improve classification accuracy.
- Mechanism: Given embeddings of two classes A and B, and a text description of their difference, the class prompt for A is updated by averaging it with the difference in text embeddings (fA + (1-α)(fB-fB-A)), effectively separating confused classes in embedding space.
- Core assumption: The difference embedding fB-fB-A accurately captures the semantic difference between classes and can be used to correct inaccurate class embeddings.
- Evidence anchors: [section 3.4] "Given a prompt-based classifier, we can incorporate prior knowledge in the form of text descriptions of class-level differences to update our class prompts" [section 4.3] "We observe that performing comparative prompting with PC-CLIP boosts or maintains performance on a majority of tasks"
- Break condition: If the difference embedding does not accurately represent the semantic difference, or if the original class embeddings are already accurate, the update will not improve classification.

### Mechanism 3
- Claim: Finetuning on pairwise comparisons improves CLIP's ability to perform arithmetic in embedding space.
- Mechanism: By aligning text descriptions of differences with image embedding differences, the model learns geometric properties that enable operations like "forest + snowier" to produce meaningful composite embeddings.
- Core assumption: The improved geometric structure from pairwise comparisons generalizes to enable meaningful arithmetic operations beyond just the training pairs.
- Evidence anchors: [abstract] "we illustrate that the resulting embeddings obey a larger degree of geometric properties in embedding space, such as in text-to-image generation" [section 4.5] "We observe that PC-CLIP, when used with Stable Diffusion, produces images that achieve a higher CLIP score than the original CLIP embedding, reflecting better arithmetic properties in embedding space"
- Break condition: If the geometric improvements from pairwise comparisons do not generalize to arithmetic operations, or if the operations require more complex reasoning than captured by simple differences.

## Foundational Learning

- Concept: Contrastive learning and CLIP architecture
  - Why needed here: Understanding how CLIP learns aligned image-text embeddings through contrastive objectives is essential to grasp why the pairwise comparison approach improves performance.
  - Quick check question: How does CLIP's contrastive loss function align image and text embeddings during training?

- Concept: Vector arithmetic in embedding spaces
  - Why needed here: The paper builds on the observation that text embeddings can perform analogies (e.g., King - Man + Woman ≈ Queen), but CLIP cannot. Understanding this concept is crucial to appreciate the improvement.
  - Quick check question: What is the mathematical relationship that enables analogies in text embedding spaces?

- Concept: Zero-shot classification and prompting
  - Why needed here: The paper evaluates improvements in zero-shot classification, which relies on using text prompts to classify images without task-specific training.
  - Quick check question: How does CLIP perform zero-shot classification using cosine similarity between image embeddings and text embeddings of class names?

## Architecture Onboarding

- Component map:
  CLIP model (ViT-L/14 image encoder, text encoder) -> LLM (Llama2-13B-chat) for generating difference descriptions -> Finetuning module with pairwise comparison dataset -> Downstream evaluation tasks (classification, retrieval, generation) -> GPT-4o-mini for judging synthetic data quality -> Stable Diffusion XL for text-to-image generation evaluation

- Critical path:
  1. Generate pairwise comparison dataset using LLM on image-caption pairs
  2. Finetune CLIP text encoder using contrastive loss on comparison pairs
  3. Evaluate on difference-based classification tasks
  4. Evaluate on standard zero-shot classification
  5. Evaluate on cross-modal retrieval
  6. Evaluate on text-to-image generation arithmetic

- Design tradeoffs:
  - Only finetuning text encoder vs. both encoders: Computationally efficient but may limit geometric improvements
  - Using synthetic LLM data vs. human-labeled data: Scalable but potentially noisier
  - Focusing on COCO vs. domain-specific datasets: Broader applicability vs. more meaningful differences

- Failure signatures:
  - Degradation in zero-shot classification performance
  - No improvement or regression in difference-based classification
  - Text-to-image generations that don't reflect semantic arithmetic
  - LLM-generated differences that are incoherent or hallucinated

- First 3 experiments:
  1. Train PC-CLIP on COCO pairwise comparisons and evaluate on difference-based classification (size, color) on AwA2, CIFAR100, CUB, Flowers102
  2. Compare PC-CLIP vs. baseline CLIP on zero-shot classification on CIFAR100, CUB, EuroSAT, Flowers102, SUN397
  3. Evaluate PC-CLIP's ability to perform text embedding arithmetic using Stable Diffusion XL on CIFAR100 classes with AwA2 attributes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and diversity of LLM-generated differences impact downstream performance of PC-CLIP?
- Basis in paper: [explicit] The paper mentions that PC-CLIP's finetuning is robust to noise in LLM generations, as it still outperforms CLIP on 4 of 5 tasks when using unfiltered data. It also notes that LLM-generated differences may miss subtle details or hallucinate objects.
- Why unresolved: The paper only uses LLaMA2-13B for generating differences and does not explore other LLM models or strategies for improving generation quality. The impact of generation quality on downstream performance is not thoroughly investigated.
- What evidence would resolve it: Systematic experiments comparing PC-CLIP performance using differences generated by different LLM models, with varying generation quality (e.g., filtered vs unfiltered, high-quality vs low-quality generations), and exploring strategies to improve generation quality (e.g., using multimodal LLMs, incorporating caption information).

### Open Question 2
- Question: Can PC-CLIP's embedding space be further improved by incorporating additional types of relational information beyond pairwise differences?
- Basis in paper: [inferred] The paper focuses on pairwise differences between images, but CLIP's embedding space lacks other types of structure such as analogies. The paper mentions that prior work has studied inducing geometric properties in embedding space through pairwise distances.
- Why unresolved: The paper does not explore incorporating other types of relational information into PC-CLIP's finetuning objective, such as analogies, hierarchical relationships, or contextual information.
- What evidence would resolve it: Experiments comparing PC-CLIP performance when finetuned with additional types of relational information (e.g., analogies, hierarchical relationships) to the current pairwise difference approach. Metrics could include performance on tasks requiring reasoning about these relationships (e.g., analogy completion, hierarchical classification).

### Open Question 3
- Question: How does PC-CLIP's performance scale with model size and pretraining dataset size?
- Basis in paper: [explicit] The paper presents results for ViT-H/14 model and observes that PC-CLIP still improves over standard CLIP on a majority of tasks. It also mentions that an interesting area for future work could be constructing a mixture of datasets of differences.
- Why unresolved: The paper only presents results for ViT-H/14 model and does not explore scaling to larger models or using larger pretraining datasets. The impact of dataset size and diversity on PC-CLIP's performance is not thoroughly investigated.
- What evidence would resolve it: Experiments comparing PC-CLIP performance when finetuned on larger models (e.g., ViT-L/14, ViT-G/14) and larger pretraining datasets (e.g., LAION-5B, LAION-2B). Metrics could include performance on downstream tasks and analysis of the learned embedding space structure.

## Limitations

- The approach relies heavily on LLM quality for generating difference descriptions, which may introduce noise and hallucinations
- Improvements are primarily demonstrated on attribute-based differences, with unclear generalization to more complex or abstract visual differences
- Comparative prompting requires prior knowledge of class differences, limiting its applicability to scenarios where such knowledge is available
- Text-to-image generation evaluation relies on LLM-based quality assessment, adding another layer of potential bias

## Confidence

- High confidence in the experimental methodology and quantitative results for difference-based classification and zero-shot classification tasks
- Medium confidence in the text-to-image generation evaluation due to reliance on LLM-based quality assessment
- Medium confidence in the general applicability of the approach beyond the tested datasets and attribute-based differences
- Low confidence in the robustness of the method to noisy or hallucinated LLM outputs during training

## Next Checks

1. Validate the quality and consistency of LLM-generated difference descriptions by having human annotators rate a random sample of 100 comparisons for semantic accuracy and relevance, comparing these ratings against the paper's reported quality metrics.

2. Test the generalization of PC-CLIP on a dataset with more complex or abstract visual differences (e.g., artistic style differences, contextual scene understanding) that go beyond simple attribute-based distinctions to assess the method's broader applicability.

3. Evaluate the impact of LLM noise by training PC-CLIP on datasets with varying levels of description quality (e.g., filtered vs. unfiltered LLM outputs) to quantify the sensitivity of the approach to generation quality and identify robustness thresholds.