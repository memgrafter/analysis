---
ver: rpa2
title: 'Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning'
arxiv_id: '2402.15751'
source_url: https://arxiv.org/abs/2402.15751
tags:
- mezo
- parameters
- fine-tuning
- performance
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sparse MeZO, a memory-efficient zeroth-order
  optimization method for fine-tuning large language models (LLMs). The method applies
  zeroth-order (ZO) optimization only to a carefully chosen subset of parameters,
  determined by their magnitude.
---

# Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning

## Quick Facts
- arXiv ID: 2402.15751
- Source URL: https://arxiv.org/abs/2402.15751
- Reference count: 40
- Achieves 9% absolute accuracy improvement and 3.5x speedup over MeZO on RTE task

## Executive Summary
Sparse MeZO introduces a memory-efficient zeroth-order optimization method for fine-tuning large language models by applying optimization only to a carefully selected subset of parameters. The method uses parameter magnitude to determine which parameters to update, finding that smaller parameters are more influential for fine-tuning while larger ones are already well-trained. This approach achieves significant performance improvements over vanilla MeZO while requiring only inference-level memory consumption, enabling fine-tuning of LLaMA-30b on a single A100 GPU.

## Method Summary
Sparse MeZO applies zeroth-order optimization selectively to parameters based on their magnitude, using a threshold-based selection mechanism to create a sparse mask. The algorithm computes perturbations and forward passes only for the selected parameters, significantly reducing memory requirements by avoiding storage of both perturbed and original parameter sets. The method maintains theoretical convergence guarantees while improving both performance and efficiency through reduced gradient estimation variance in lower-dimensional spaces.

## Key Results
- 9% absolute accuracy improvement over MeZO on RTE task
- 3.5x speedup in convergence time
- Achieves inference-level memory consumption, enabling single-GPU fine-tuning of LLaMA-30b
- Sparsity values between 0.5-0.8 generally yield optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sparse MeZO improves convergence speed by reducing the dimensionality of the gradient estimation space.
- **Mechanism:** By applying zeroth-order optimization only to smaller-magnitude parameters, the method reduces variance in gradient estimates that scale with dimensionality.
- **Core assumption:** Smaller parameters are more influential for fine-tuning in zeroth-order optimization.
- **Break condition:** If larger parameters are actually more influential, or if noise reduction is insufficient.

### Mechanism 2
- **Claim:** Magnitude-based parameter selection targets under-trained parameters.
- **Mechanism:** Parameters are ranked by magnitude, with smaller ones selected for update under the hypothesis that larger parameters are already well-trained.
- **Core assumption:** Parameter magnitude correlates with importance for fine-tuning.
- **Break condition:** If correlation reverses in different architectures or tasks, or threshold selection becomes unstable.

### Mechanism 3
- **Claim:** Memory-efficient implementation enables large model fine-tuning on single GPU.
- **Mechanism:** Computing sparse mask during forward pass and releasing intermediate memory eliminates need to store perturbed parameters.
- **Core assumption:** Mask can be reconstructed on-the-fly without affecting gradient quality.
- **Break condition:** If mask computation overhead becomes prohibitive or introduces timing artifacts.

## Foundational Learning

- **Concept:** Zeroth-order optimization and SPSA gradient estimation
  - **Why needed here:** Understanding how ZO methods estimate gradients using only forward passes is fundamental to grasping why Sparse MeZO works
  - **Quick check question:** How does SPSA estimate gradients using only function evaluations at perturbed points?

- **Concept:** Parameter-efficient fine-tuning (PEFT) methods and their tradeoffs
  - **Why needed here:** Sparse MeZO builds on PEFT insights but applies them to zeroth-order optimization, requiring understanding of both domains
  - **Quick check question:** What are the main categories of PEFT methods and how do they differ in memory/compute tradeoffs?

- **Concept:** Convergence analysis for stochastic optimization methods
  - **Why needed here:** The paper provides theoretical convergence guarantees that require understanding of Lipschitz continuity, unbiased gradient estimates, and convergence rate analysis
  - **Quick check question:** How does reducing the effective dimensionality of optimization affect convergence rates in stochastic methods?

## Architecture Onboarding

- **Component map:** Sparse mask generator -> Perturbation module -> Forward pass engine -> Gradient estimator -> Memory manager

- **Critical path:**
  1. Generate sparse mask based on parameter magnitudes
  2. Apply perturbations to selected parameters
  3. Forward pass to compute loss values
  4. Compute gradient estimate
  5. Update parameters
  6. Release intermediate memory

- **Design tradeoffs:**
  - Sparsity level vs. performance: Higher sparsity reduces memory but may degrade performance
  - Mask computation frequency: Dynamic masks adapt better but add computation overhead
  - Perturbation scale: Larger perturbations may improve exploration but increase variance

- **Failure signatures:**
  - Degraded performance with high sparsity levels
  - Increased memory usage if mask computation is inefficient
  - Training instability if perturbation scale is poorly tuned

- **First 3 experiments:**
  1. Verify that sparse mask generation correctly identifies parameters below threshold
  2. Test memory usage with different sparsity levels on a small model
  3. Compare convergence speed between full MeZO and Sparse MeZO on a simple task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sparsity rate for different tasks and model architectures in Sparse MeZO?
- Basis in paper: The paper experiments with sparsity values ranging from 0.0 to 0.85 and finds that sparsity values between 0.5 and 0.8 generally yield the best performance.
- Why unresolved: The paper only tests a limited range of sparsity values and does not explore how sparsity affects different tasks or model architectures.
- What evidence would resolve it: Systematic experiments varying sparsity rates across different tasks, model sizes, and architectures to identify optimal sparsity for each combination.

### Open Question 2
- Question: How does the choice of perturbation scale (Ïµ) affect the performance of Sparse MeZO across different tasks?
- Basis in paper: The paper uses a fixed perturbation scale of 1e-3 for all experiments, but does not explore how varying this parameter affects performance.
- Why unresolved: The optimal perturbation scale may depend on the specific task, model architecture, and sparsity rate, but this relationship is not investigated.
- What evidence would resolve it: Systematic experiments varying the perturbation scale across different tasks and sparsity rates to identify optimal settings for each combination.

### Open Question 3
- Question: Can the sparse mask in Sparse MeZO be dynamically updated during training to further improve performance?
- Basis in paper: The paper compares constant masks (set before training) with dynamic masks (updated at each iteration) and finds that dynamic masks avoid storing large masks but may not be optimal.
- Why unresolved: The paper does not explore strategies for updating the sparse mask during training, such as based on gradient information or parameter importance.
- What evidence would resolve it: Experiments comparing static masks with various dynamic mask update strategies to determine if and how dynamic updates can improve performance.

## Limitations

- The magnitude-based parameter selection mechanism lacks theoretical justification and may not generalize across different model architectures
- The theoretical convergence analysis doesn't fully account for the interaction between sparsity patterns and zeroth-order optimization noise characteristics
- The memory efficiency claims depend on specific implementation details that may not be portable across different hardware configurations

## Confidence

- **High Confidence**: Empirical performance improvements (9% accuracy gain, 3.5x speedup) and memory efficiency claims are well-supported by experimental results
- **Medium Confidence**: The mechanism that smaller parameters are more influential for fine-tuning is supported by experimental evidence but lacks theoretical justification
- **Low Confidence**: Theoretical convergence guarantees don't fully capture practical behavior, particularly regarding sparsity-noise interactions

## Next Checks

1. **Cross-Architecture Validation**: Test Sparse MeZO on transformer architectures with different initialization schemes (GPT-style vs. BERT-style) to verify if magnitude-based selection remains effective

2. **Dynamic Sparsity Analysis**: Implement and compare static vs. dynamic sparsity masks that update during training to assess whether initial magnitude-based selection remains optimal

3. **Memory Profiling Across Hardware**: Conduct comprehensive memory usage profiling on different GPU architectures (A100, H100, consumer GPUs) to verify inference-level memory consumption claim holds universally