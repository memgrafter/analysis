---
ver: rpa2
title: 'Early Period of Training Impacts Adaptation for Out-of-Distribution Generalization:
  An Empirical Study'
arxiv_id: '2403.15210'
source_url: https://arxiv.org/abs/2403.15210
tags:
- training
- learning
- early
- generalization
- unfreezing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how the early period of neural network training
  affects out-of-distribution (OOD) generalization under covariate shift. Through
  empirical experiments across vision and language tasks, it shows that gradually
  unfreezing parameters during the early training period significantly improves OOD
  performance.
---

# Early Period of Training Impacts Adaptation for Out-of-Distribution Generalization: An Empirical Study

## Quick Facts
- arXiv ID: 2403.15210
- Source URL: https://arxiv.org/abs/2403.15210
- Authors: Chen Cecilia Liu; Iryna Gurevych
- Reference count: 40
- Key outcome: Gradually unfreezing parameters during early training significantly improves out-of-distribution (OOD) generalization under covariate shift across vision and language tasks

## Executive Summary
This empirical study investigates how the early period of neural network training affects out-of-distribution (OOD) generalization under covariate shift. The research demonstrates that gradually unfreezing parameters during the initial training phase can significantly improve OOD performance while maintaining in-distribution accuracy. The study uses Fisher Information trace and sharpness metrics as indicators for optimal intervention timing, revealing that these metrics can guide when to remove gradual unfreezing for better OOD results. The method achieves Pareto improvements in both ID and OOD performance with minimal complexity, suggesting the early training period is a critical window for improving model robustness to distribution shifts.

## Method Summary
The method employs gradual unfreezing, progressively activating model parameters during the early training period to improve OOD generalization. The approach monitors learning dynamics through Fisher Information trace and sharpness metrics to determine optimal intervention timing. Experiments span vision tasks (MNIST, CIFAR-10/100, Office-Home, DomainNet) and language tasks (SQuAD, MNLI, XNLI, XQuAD, MLQA) with various distribution shifts including noise corruption, domain shifts, and language shifts. The technique achieves improved OOD performance while maintaining ID accuracy through carefully timed parameter unfreezing during the critical early training window.

## Key Results
- Gradual unfreezing during early training significantly improves OOD generalization across multiple vision and language tasks
- Fisher Information trace and sharpness metrics can indicate when to remove gradual unfreezing interventions for optimal OOD performance
- The method achieves Pareto improvements in both ID and OOD performance with minimal added complexity
- Early training period reveals distinct phases: rapid change followed by stabilization, which correlates with optimal unfreezing timing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradually unfreezing parameters during early training improves OOD generalization by aligning mini-batch gradients with full-batch gradients, preventing overfitting to spurious features.
- Mechanism: When parameters are progressively unfrozen, gradient similarity between mini-batch and full-batch gradients increases during the early period, leading to more stable and generalizable feature learning.
- Core assumption: Spurious features that hurt OOD performance are learned early in training when parameters are all trainable at once.
- Evidence anchors:
  - [abstract] "changing the number of trainable parameters during the early period of training via gradual unfreezing can significantly improve OOD results"
  - [section] "when training from scratch, GS is higher when using gradual unfreezing than standard training during the early period of training"
  - [corpus] Weak - no direct evidence found in neighbor papers about gradient similarity during unfreezing
- Break condition: If spurious features are not primarily formed in early training, or if the dataset has no spurious correlations to exploit.

### Mechanism 2
- Claim: The early training period has distinct learning dynamics characterized by rapid initial changes in Fisher Information and sharpness metrics, followed by stabilization.
- Mechanism: During the initial rapid phase, the model is highly sensitive to parameter changes. Gradually unfreezing allows the model to explore the loss landscape more thoroughly before parameters are fully active, leading to better generalization.
- Core assumption: The stabilization point of Fisher Information and sharpness metrics indicates an optimal time window for intervention removal.
- Evidence anchors:
  - [abstract] "the trace of Fisher Information and sharpness can be used as indicators for the removal of gradual unfreezing during the early period of training for better OOD generalization"
  - [section] "We identify a pattern consisting of two phases: 1) an initial phase of rapid change... and 2) a subsequent stabilization phase"
  - [corpus] Moderate - neighbor papers discuss OOD detection and robustness but not specifically the stabilization of these metrics
- Break condition: If the Fisher Information and sharpness metrics do not stabilize in a predictable pattern, or if other metrics better predict optimal unfreezing timing.

### Mechanism 3
- Claim: Time-sensitive intervention during early training creates a trade-off window where OOD performance improves while ID performance remains stable.
- Mechanism: By carefully timing when parameters are unfrozen, the model can avoid the sharp decline in ID performance that occurs with late unfreezing while capturing the OOD generalization benefits of early intervention.
- Core assumption: There exists a specific temporal window during training where the benefits of gradual unfreezing for OOD generalization outweigh the costs to ID performance.
- Evidence anchors:
  - [abstract] "Pareto improvements in ID and OOD performance with minimal complexity"
  - [section] "Notably, gradual unfreezing reveals a trade-off between ID and OOD performance... with a brief window where OOD results improve before a sharp decline in ID performance"
  - [corpus] Moderate - neighbor papers discuss OOD robustness but not specifically the timing of parameter unfreezing
- Break condition: If the trade-off window is too narrow to be practically useful, or if the timing is highly dataset-dependent.

## Foundational Learning

- Concept: Covariate shift (input distribution shift)
  - Why needed here: The paper focuses on OOD generalization under covariate shift, which is distinct from other forms of distribution shift like correlation shift
  - Quick check question: What is the difference between covariate shift and concept drift?

- Concept: Fisher Information Matrix and its trace
  - Why needed here: Used as a metric to analyze learning dynamics during early training and determine optimal intervention timing
  - Quick check question: How does the trace of Fisher Information relate to the sensitivity of model predictions to parameter changes?

- Concept: Sharpness-aware optimization
  - Why needed here: Sharpness metrics are used alongside Fisher Information to characterize the early training period and guide intervention timing
  - Quick check question: What is the theoretical relationship between loss landscape sharpness and model generalization?

## Architecture Onboarding

- Component map: Base model (ResNet18, ViT-B/16, XLM-RoBERTa) -> Gradual unfreezing controller -> Learning dynamics monitor -> Evaluation pipeline
- Critical path: 1. Initialize model with parameter blocks 2. Train with gradual unfreezing for k steps 3. Monitor learning dynamics metrics 4. Evaluate ID and OOD performance 5. Adjust k based on metric stabilization
- Design tradeoffs: Finer parameter blocks allow more precise control but increase complexity; more frequent metric monitoring provides better timing signals but adds computational overhead; longer training with gradual unfreezing may improve OOD but risks ID degradation
- Failure signatures: OOD performance degrades when k is too large (unfreezing too late); ID performance drops significantly when k is in the optimal range; metrics show no clear stabilization pattern, making timing difficult
- First 3 experiments: 1. CIFAR10 from scratch with ResNet18, vary k from 1 to 1000, measure ID/OOD performance 2. DomainNet fine-tuning with ViT-B/16, single source domain, test gradual unfreezing timing 3. XNLI cross-lingual transfer with XLM-RoBERTa + LoRA, vary k, measure F1 scores across languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the optimal timing for intervention removal depend on the type of covariate shift (e.g., noise corruption vs. language shift vs. domain shift)?
- Basis in paper: [explicit] The paper shows different k values work best for different tasks and shifts, but doesn't systematically compare optimal timing across shift types.
- Why unresolved: The experiments focus on individual task settings rather than comparative analysis across shift types.
- What evidence would resolve it: Systematic experiments varying k across multiple shift types with consistent architectures would reveal if optimal timing patterns differ by shift type.

### Open Question 2
- Question: What is the causal mechanism by which gradual unfreezing improves OOD generalization?
- Basis in paper: [inferred] The paper hypothesizes about gradient alignment and spurious feature regularization but doesn't establish causation through controlled experiments.
- Why unresolved: The paper provides correlation evidence and hypotheses but lacks ablation studies isolating specific mechanisms.
- What evidence would resolve it: Controlled experiments with alternative regularization methods targeting the same hypothesized mechanisms, plus intervention studies disrupting those mechanisms.

### Open Question 3
- Question: Can the early training period insights be generalized to other forms of distribution shift beyond covariate shift?
- Basis in paper: [explicit] The paper explicitly limits investigation to covariate shift and notes this as a limitation.
- Why unresolved: The experimental design and analysis framework is constructed specifically for covariate shift scenarios.
- What evidence would resolve it: Extending the gradual unfreezing and learning dynamics analysis to tasks involving label shift, concept drift, or other non-covariate distribution shifts.

## Limitations

- The study is empirical without theoretical guarantees explaining why gradual unfreezing works
- Optimal unfreezing timing appears dataset-dependent without a principled automatic determination method
- The computational overhead of tracking Fisher Information and sharpness metrics may offset benefits in resource-constrained scenarios

## Confidence

- High confidence: The empirical observation that gradual unfreezing improves OOD generalization across multiple vision and language tasks
- Medium confidence: The characterization of early training as having distinct rapid-change and stabilization phases based on Fisher Information and sharpness metrics
- Low confidence: The mechanistic explanation that spurious feature learning occurs primarily in early training and that gradient alignment drives the improvements

## Next Checks

1. Test the gradual unfreezing approach on concept drift scenarios to determine if the mechanism generalizes beyond covariate shift
2. Implement an automated timing controller that uses real-time Fisher Information and sharpness metrics to dynamically adjust unfreezing schedules during training
3. Compare the computational efficiency of gradual unfreezing against other OOD generalization methods like data augmentation and robust optimization techniques on the same hardware infrastructure