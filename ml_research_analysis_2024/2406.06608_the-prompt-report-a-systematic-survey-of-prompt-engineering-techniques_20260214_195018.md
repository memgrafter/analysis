---
ver: rpa2
title: 'The Prompt Report: A Systematic Survey of Prompt Engineering Techniques'
arxiv_id: '2406.06608'
source_url: https://arxiv.org/abs/2406.06608
tags:
- prompt
- prompting
- language
- prompts
- techniques
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the most comprehensive survey on prompt engineering
  to date, assembling a taxonomy of 58 LLM prompting techniques and 40 techniques
  for other modalities. The authors provide a detailed vocabulary of 33 vocabulary
  terms, best practices and guidelines for prompt engineering, and a meta-analysis
  of the entire literature on natural language prefix-prompting.
---

# The Prompt Report: A Systematic Survey of Prompt Engineering Techniques

## Quick Facts
- arXiv ID: 2406.06608
- Source URL: https://arxiv.org/abs/2406.06608
- Reference count: 40
- Primary result: Comprehensive survey of 58 LLM prompting techniques and 40 techniques for other modalities, with taxonomy, vocabulary, and case studies

## Executive Summary
This paper presents the most comprehensive survey of prompt engineering techniques to date, systematically cataloging 58 LLM prompting methods and 40 techniques across other modalities. The authors develop a detailed taxonomy and vocabulary of 33 key terms while providing practical guidelines for prompt engineering practice. Through two case studies - a benchmark evaluation on MMLU and a detailed suicidal crisis detection application - the research demonstrates that effective prompt engineering requires meaningful collaboration between prompt engineers and domain experts, and that automated methods like DSPy show significant promise for optimizing LLM prompts.

## Method Summary
The survey methodology involves systematic literature review across multiple research communities to identify and categorize prompting techniques. The authors constructed a taxonomy framework to organize techniques by modality and function, developed standardized vocabulary terms for consistent communication, and performed meta-analysis of natural language prefix-prompting literature. Two case studies were conducted: benchmark evaluation across multiple datasets and a detailed application in mental health crisis detection requiring domain expert collaboration.

## Key Results
- Taxonomy of 58 LLM prompting techniques and 40 techniques for other modalities identified
- 33 standardized vocabulary terms developed for prompt engineering communication
- Case studies demonstrate non-trivial nature of prompt engineering requiring domain expert engagement
- Automated methods like DSPy show significant promise in optimizing LLM prompts for target metrics

## Why This Works (Mechanism)
The survey's comprehensive approach works by systematically aggregating and organizing the rapidly expanding field of prompt engineering techniques. By developing standardized vocabulary and taxonomies, the authors create a common language that enables better communication across research communities. The dual case study approach - both benchmark evaluation and real-world application - provides both breadth and depth in understanding how prompting techniques perform in practice.

## Foundational Learning
**Prompt Engineering Taxonomy**: Classification system for organizing prompting techniques by modality and function. Needed to provide structure to the expanding field. Quick check: Can techniques be consistently categorized using the taxonomy?

**Standardized Vocabulary**: 33 key terms for prompt engineering concepts. Needed to reduce confusion from inconsistent terminology across research communities. Quick check: Do practitioners from different backgrounds understand terms consistently?

**Domain Expert Collaboration**: Integration of domain knowledge with prompt engineering skills. Needed because effective prompting requires understanding both technical and subject matter aspects. Quick check: Does collaboration improve prompt quality compared to solo engineering?

**Automated Optimization**: Methods like DSPy for systematic prompt refinement. Needed to scale prompt engineering beyond manual trial-and-error. Quick check: Does automated optimization improve target metrics consistently?

**Meta-Analysis Methodology**: Systematic evaluation of prefix-prompting literature. Needed to synthesize findings across studies. Quick check: Are conclusions supported by multiple independent studies?

## Architecture Onboarding

**Component Map**: Literature Review -> Technique Identification -> Taxonomy Construction -> Vocabulary Development -> Case Study Implementation -> Meta-Analysis -> Best Practices Documentation

**Critical Path**: The most critical path involves moving from technique identification through taxonomy construction to case study implementation, as this sequence ensures theoretical framework development followed by practical validation.

**Design Tradeoffs**: The survey prioritizes comprehensiveness over depth in individual techniques, sacrificing detailed implementation guidance for broad coverage. This enables better understanding of the field landscape but may leave practitioners wanting more specific implementation details.

**Failure Signatures**: Incomplete technique identification leads to biased taxonomy, inconsistent vocabulary causes communication breakdowns, inadequate domain expertise results in ineffective prompts, and insufficient validation undermines case study credibility.

**3 First Experiments**:
1. Apply taxonomy to categorize 100 randomly selected prompt engineering papers
2. Conduct inter-rater reliability test on vocabulary term definitions
3. Compare manual versus DSPy-optimized prompts on a standard benchmark task

## Open Questions the Paper Calls Out
Major uncertainties remain around the completeness and systematic rigor of the taxonomy construction. The survey reports 58 LLM prompting techniques and 40 techniques for other modalities, but the methodology for identifying and validating these techniques across the literature is not fully transparent. The inclusion criteria for techniques and the comprehensiveness of the search process could affect the claimed comprehensiveness. Additionally, while 33 vocabulary terms are provided, the standardization and consistency of these terms across different research communities is unclear.

## Limitations
- Taxonomy methodology lacks transparency in technique identification and validation criteria
- Meta-analysis methodology unclear regarding study selection, evaluation, and weighting procedures
- Case study generalizability uncertain due to limited domain expert collaboration details

## Confidence
- High confidence in the identification of prompting techniques taxonomy structure and vocabulary development
- Medium confidence in the practical implications and best practices recommendations
- Low confidence in the completeness of the systematic survey methodology and meta-analysis rigor

## Next Checks
1. Replicate the technique identification process using transparent inclusion criteria and systematic search protocols to verify the claimed comprehensiveness of 58+40 techniques
2. Conduct independent evaluation of the vocabulary standardization across multiple research communities to assess term consistency and utility
3. Test the suicidal crisis detection methodology on additional datasets and domains to evaluate generalizability of the domain expert engagement findings and automated optimization effectiveness