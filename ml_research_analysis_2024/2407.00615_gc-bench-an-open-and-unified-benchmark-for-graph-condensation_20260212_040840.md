---
ver: rpa2
title: 'GC-Bench: An Open and Unified Benchmark for Graph Condensation'
arxiv_id: '2407.00615'
source_url: https://arxiv.org/abs/2407.00615
tags:
- graph
- methods
- datasets
- condensation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'GC-Bench is a comprehensive benchmark for graph condensation (GC)
  that evaluates 12 state-of-the-art methods across 12 diverse graph datasets. The
  benchmark systematically analyzes GC performance across three dimensions: effectiveness,
  transferability, and efficiency.'
---

# GC-Bench: An Open and Unified Benchmark for Graph Condensation

## Quick Facts
- **arXiv ID**: 2407.00615
- **Source URL**: https://arxiv.org/abs/2407.00615
- **Reference count**: 40
- **Primary result**: Node-level GC methods achieve near-lossless performance while structure-free methods struggle with complex graph properties and condensed datasets show poor transferability

## Executive Summary
GC-Bench is a comprehensive benchmark that systematically evaluates 12 state-of-the-art graph condensation methods across 12 diverse graph datasets. The benchmark assesses methods across three critical dimensions: effectiveness (preserving downstream task performance), transferability (generalization across tasks and architectures), and efficiency (computational cost). Results reveal that while node-level methods can achieve near-lossless condensation, graph-level methods lag significantly behind whole dataset training, and most methods suffer from poor transferability and high computational costs that contradict the efficiency goal of condensation.

## Method Summary
GC-Bench evaluates 12 graph condensation methods across 12 graph datasets, including 7 node-level datasets (Cora, Citeseer, ogbn-arxiv, Flickr, Reddit, ACM, DBLP) and 5 graph-level datasets (NCI1, DD, ogbg-molbace, ogbg-molbbbp, ogbg-molhiv). The benchmark measures performance on node classification, graph classification, and link prediction tasks using metrics like accuracy, ROC-AUC, and clustering coefficients. Methods are categorized into six groups: random selection, structure-based, gradient matching, trajectory matching, kernel-based, and hybrid approaches. The benchmark library is publicly available at https://github.com/RingBDStack/GC-Bench.

## Key Results
- Node-level GC methods can achieve near-lossless condensation performance, with gradient matching methods (GCond, DosCond, SGDD) showing strong results
- Structure-free methods (SFGC, GEOM) struggle with complex graph properties despite preserving semantic information through trajectory matching
- Condensed datasets exhibit poor transferability across different tasks and backbone architectures, limiting practical deployment scenarios
- Most GC methods demonstrate high computational costs that contradict the efficiency goal of condensation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Gradient matching methods achieve near-lossless condensation by aligning gradient updates between original and condensed graphs
- **Mechanism**: These methods frame condensation as bi-level optimization where synthetic graph parameters are optimized to match gradients of GNN parameters between original and condensed datasets
- **Core assumption**: Gradients contain sufficient information to preserve model performance when transferred from condensed to original graphs
- **Evidence anchors**: Abstract states node-level methods achieve near-lossless performance; section notes gradient matching methods perform well in benchmark
- **Break condition**: When gradient information becomes insufficient to capture complex graph structure or introduces model-specific biases that hurt transferability

### Mechanism 2
- **Claim**: Structure-free methods preserve semantic information through trajectory matching rather than explicit graph structure
- **Mechanism**: These methods train expert GNNs on original graphs to acquire parameter distributions offline, then use these trajectories to guide condensation of graph-free data
- **Core assumption**: Preserving training trajectory of model parameters is sufficient to retain semantic information without maintaining explicit graph structure
- **Evidence anchors**: Abstract shows structure-free methods struggle with complex graph properties; section notes significant performance gap in AD tasks compared to gradient matching methods
- **Break condition**: When task-specific semantic information requires explicit structural preservation that trajectory matching cannot capture

### Mechanism 3
- **Claim**: KiDD achieves strong graph-level results by avoiding backbone dependency and focusing solely on structural matching
- **Mechanism**: KiDD uses graph neural tangent kernel (GNTK) with kernel ridge regression, simplifying condensation by not requiring iterative GNN training
- **Core assumption**: Graph structure alone, without model-specific training information, is sufficient to preserve classification performance
- **Evidence anchors**: Abstract states graph-level methods lag behind whole dataset training; section shows KiDD performs well with GIN but poorly with GCN
- **Break condition**: When downstream models have different expressive capabilities than the model implicit in GNTK, leading to poor transfer performance

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs) and their variants (GCN, GraphSAGE, GIN, etc.)
  - **Why needed here**: Understanding GNN architectures is essential since most GC methods use them as backbones for condensation
  - **Quick check question**: What is the key difference between GCN and GraphSAGE in terms of neighborhood aggregation?

- **Concept**: Graph representation learning and node/graph classification tasks
  - **Why needed here**: GC methods are evaluated on these tasks, requiring understanding of how graph structure relates to learning objectives
  - **Quick check question**: How does the homophily ratio affect the performance of graph neural networks?

- **Concept**: Bi-level optimization and gradient matching techniques
  - **Why needed here**: Many GC methods frame condensation as bi-level optimization problems using gradient matching
  - **Quick check question**: In bi-level optimization for GC, what are the two optimization levels and what do they optimize?

## Architecture Onboarding

- **Component map**: Dataset loaders (12 graph datasets) → Algorithm implementations (12 GC methods across 6 categories) → Evaluation pipelines (3 dimensions: effectiveness, transferability, efficiency) → Result aggregation/analysis components
- **Critical path**: Data → Method selection → Condensation process → Evaluation on downstream tasks → Result analysis across dimensions
- **Design tradeoffs**: Comprehensive evaluation vs. computational cost, inclusion of diverse methods vs. implementation complexity, open-source accessibility vs. dependency management
- **Failure signatures**: OOM errors on large datasets with high condensation ratios, poor transferability across backbone architectures, initialization sensitivity causing convergence issues
- **First 3 experiments**:
  1. Run a simple baseline test: Compare Random core-set method vs. whole dataset training on Cora for node classification
  2. Test gradient matching method: Run DosCond on Citeseer with 2.6% condensation ratio and evaluate node classification accuracy
  3. Test transferability: Use DosCond-condensed Cora (2.6%) to train models with different backbone architectures (SGC, GCN, GraphSAGE) and compare performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Is there a theoretical limit to the condensation ratio beyond which performance degradation becomes inevitable, and if so, what determines this threshold?
- **Basis in paper**: The paper observes that increasing condensation ratios can lead to performance saturation or decline, suggesting a potential theoretical limit exists
- **Why unresolved**: The paper only observes this phenomenon empirically across various datasets and methods without providing a theoretical framework to explain or predict this threshold
- **What evidence would resolve it**: A mathematical model or framework that quantifies the relationship between condensation ratio, information loss, and downstream task performance, potentially incorporating concepts from information theory or graph spectral analysis

### Open Question 2
- **Question**: Can task-agnostic graph condensation methods be developed that preserve universal structural and semantic information without relying on specific downstream tasks or labels?
- **Basis in paper**: The paper identifies task-agnostic condensation as a future direction, noting that current methods often depend on downstream labels or task-specific training
- **Why unresolved**: Most existing methods are designed with specific tasks in mind, and developing methods that capture general graph properties without task-specific optimization remains an open challenge
- **What evidence would resolve it**: A successful implementation of a graph condensation method that demonstrates strong performance across multiple diverse downstream tasks without task-specific tuning, along with analysis showing what structural properties it preserves

### Open Question 3
- **Question**: How can graph condensation methods be designed to effectively handle heterogeneous graphs, directed graphs, hypergraphs, and other complex graph structures?
- **Basis in paper**: The paper explicitly states that current methods are predominantly tailored to simple graphs and identifies condensation for more complex graph data as a future research direction
- **Why unresolved**: Most existing GC methods focus on simple homogeneous graphs, and the paper shows that even converting heterogeneous graphs to homogeneous form results in performance that, while comparable to whole dataset training, leaves room for improvement
- **What evidence would resolve it**: A comprehensive evaluation showing that a graph condensation method maintains performance on complex graph types (heterogeneous, directed, hypergraph, etc.) at levels comparable to its performance on simple graphs, ideally with theoretical justification for why the method works across these structures

## Limitations

- Structure-free methods (SFGC, GEOM) show poor performance on complex graph properties despite preserving semantic information through trajectory matching
- Graph-level methods lag substantially behind node-level approaches, with KiDD showing strong dependence on specific backbone architectures
- Condensed datasets exhibit poor transferability across different tasks and backbone architectures, undermining practical deployment scenarios

## Confidence

- **High Confidence**: Node-level GC methods achieving near-lossless performance; computational cost challenges across most methods; structure-free method limitations on complex properties
- **Medium Confidence**: Gradient matching mechanism effectiveness; poor transferability findings; KiDD backbone dependency
- **Low Confidence**: Trajectory matching mechanism details; specific break conditions for gradient matching; generalization of findings to entirely different graph domains

## Next Checks

1. Test gradient matching methods on heterogeneous graphs to verify break conditions when gradient information becomes insufficient for complex structural preservation
2. Evaluate transferability systematically by training on condensed datasets with varying backbone architectures and measuring performance decay patterns
3. Conduct ablation studies on trajectory matching components to isolate which aspects of parameter distribution preservation are most critical for maintaining semantic information