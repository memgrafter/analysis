---
ver: rpa2
title: Assessing the Answerability of Queries in Retrieval-Augmented Code Generation
arxiv_id: '2411.05547'
source_url: https://arxiv.org/abs/2411.05547
tags:
- none
- code
- answerable
- query
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task for evaluating the answerability
  of user queries in retrieval-augmented code generation (RaCG). The task assesses
  whether valid code answers can be generated based on a user's query and retrieved
  API descriptions.
---

# Assessing the Answerability of Queries in Retrieval-Augmented Code Generation

## Quick Facts
- arXiv ID: 2411.05547
- Source URL: https://arxiv.org/abs/2411.05547
- Reference count: 19
- Primary result: Baseline models achieve only 46.7% accuracy on the novel answerability assessment task for retrieval-augmented code generation

## Executive Summary
This paper introduces a novel task for evaluating the answerability of user queries in retrieval-augmented code generation (RaCG) systems. The task assesses whether valid code answers can be generated based on a user's query and retrieved API descriptions, formulated as a three-way classification problem (answerable, partially answerable, or unanswerable). To support this task, the authors construct the RaCGEval benchmark dataset containing 1,267 samples across four domains, finding that baseline models struggle significantly with this task. The study also explores in-context learning for domain adaptation and analyzes the trade-off between coverage and precision when incorporating answerability assessment into code generation pipelines.

## Method Summary
The authors formulate answerability assessment as a three-way classification task where models determine if valid code can be generated from a user query and retrieved API descriptions. They construct the RaCGEval benchmark by annotating 1,267 samples across four domains (NetsPresso, TorchData, BeatNum, Monkey) with answerability labels. Training data is automatically generated from CoNaLa user queries and standard Python API documentation by creating partially answerable and unanswerable samples through API substitution, query concatenation, and out-of-database queries. The paper evaluates three approaches: zero-shot inference on instruction-following LLMs, fine-tuning with QLoRA, and in-context learning for domain adaptation, analyzing the trade-off between coverage (percentage of accepted queries) and precision (code generation success rate).

## Key Results
- Baseline models achieve only 46.7% accuracy on the RaCGEval benchmark for answerability assessment
- In-context learning substantially improves accuracy compared to zero-shot inference across all domains
- The trade-off between coverage and precision can be controlled by adjusting the answerability assessment threshold
- Domain adaptation is crucial for achieving high accuracy on private library APIs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The answerability task reduces hallucination by pre-filtering unanswerable queries before code generation.
- Mechanism: By classifying queries as answerable, partially answerable, or unanswerable based solely on the query and retrieved API descriptions, the system prevents LLMs from generating plausible but incorrect code when the necessary information is absent.
- Core assumption: The query can be accurately classified without generating any code, and this classification reliably indicates whether correct code can be generated.
- Evidence anchors:
  - [abstract] "This study proposes a task for evaluating answerability, which assesses whether valid answers can be generated based on users' queries and retrieved APIs in RaCG."
  - [section 2.1] "Our task can be viewed as a three-way classification problem that takes an input prompt (i.e., instruction for answerability assessment with pairs of a user query and API descriptions) as input."
  - [corpus] "The integration of Large Language Models (LLMs) into software development has revolutionized the field, particularly through the use of Retrieval-Augmented Code Generation (RACG) systems that enhance code generation with information from external knowledge bases. However, the security implications of RACG systems, p..."

### Mechanism 2
- Claim: Domain adaptation through in-context learning improves answerability assessment accuracy for private libraries.
- Mechanism: By providing few-shot examples specific to each domain (NetsPresso, TorchData, BeatNum, Monkey) during inference, the model learns to better recognize answerable queries within that domain's API documentation.
- Core assumption: The few-shot examples provided are representative of the domain's answerability patterns and the model can effectively generalize from them.
- Evidence anchors:
  - [section 5.1] "We evaluate in-context learning with 3-way-1-shot (i.e., 1-shot for each 3 classes: answerable, partially answerable, and unanswerable), denoted as ICL-3w1s, setting."
  - [section 5.1] "Using ICL substantially improves the accuracy compared to zero-shot inference on both with and without fine-tuning."
  - [corpus] "SelfRACG: Enabling LLMs to Self-Express and Retrieve for Code Generation Existing retrieval-augmented code generation (RACG) methods typically use an external retrieval module to fetch semantically similar code snippets used for generating subsequent fragments. However, even for consecutive code fragments, the content often diverges due to logical progression, resulting in a content gap...."

### Mechanism 3
- Claim: The trade-off between coverage and precision can be controlled by adjusting the acceptance threshold of the answerability assessment model.
- Mechanism: By setting different thresholds for accepting queries as answerable, the system can balance between generating code for more queries (higher coverage) at the risk of lower precision, or being more conservative (lower coverage) but with higher precision.
- Core assumption: The answerability assessment model's confidence scores are reliable indicators of whether a query is truly answerable.
- Evidence anchors:
  - [section 5.2] "Figure 5 shows the trade-off between coverage and pass@k. The coverage is defined as the percentage of the total test sample that is determined to be answerable."
  - [section 5.2] "By setting a high/low threshold, we can force the code generation model to generate answers conservatively/aggressively."
  - [corpus] "Across Programming Language Silos: A Study on Cross-Lingual Retrieval-augmented Code Generation Current research on large language models (LLMs) with retrieval-augmented code generation (RACG) mainly focuses on single-language settings, leaving cross-lingual effectiveness and security unexplored. Multi-lingual RACG systems are valuable for migrating code-bases across programming languages (PLs), yet face risks..."

## Foundational Learning

- Concept: Three-way classification for answerability assessment
  - Why needed here: The task requires distinguishing between queries that can be fully answered, partially answered, or not answered at all based on available API documentation.
  - Quick check question: Can you explain the difference between an answerable and partially answerable query in the context of RaCG?

- Concept: In-context learning for domain adaptation
  - Why needed here: The RaCGEval benchmark includes private library APIs that LLMs are unlikely to have encountered during training, requiring adaptation to new domains.
  - Quick check question: How does in-context learning differ from fine-tuning when adapting to new domains?

- Concept: Trade-off between coverage and precision
  - Why needed here: The system must balance generating code for more queries (higher coverage) against the risk of lower precision due to misclassification of answerability.
  - Quick check question: What happens to precision when the acceptance threshold for answerability is lowered?

## Architecture Onboarding

- Component map:
  User Query + Retrieved APIs -> Answerability Assessment Model -> Code Generator (if answerable) -> Output Code or Rejection

- Critical path:
  1. Receive query and retrieved APIs
  2. Run answerability assessment
  3. If answerable, pass to code generator
  4. Return generated code or rejection

- Design tradeoffs:
  - Accuracy vs. coverage: Higher accuracy requires stricter thresholds, reducing coverage
  - Computational cost: Answerability assessment adds latency before code generation
  - Domain specificity: Models need adaptation for private libraries vs. general Python APIs

- Failure signatures:
  - High false positive rate: Generating code for unanswerable queries (hallucination)
  - High false negative rate: Rejecting answerable queries unnecessarily
  - Domain mismatch: Poor performance on private library APIs due to lack of adaptation

- First 3 experiments:
  1. Test zero-shot inference on RaCGEval benchmark to establish baseline
  2. Evaluate in-context learning with 3-way-1-shot examples for domain adaptation
  3. Analyze coverage-precision trade-off by varying answerability assessment thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively handle cases where the language model can generate correct code using prior knowledge without relying on the gold API documents?
- Basis in paper: [inferred] The paper mentions that the highest verification accuracy does not always achieve the best pass@k, as some queries can be answered using prior knowledge without gold API documents.
- Why unresolved: The paper does not provide a method to account for prior knowledge in the verification model's annotation process.
- What evidence would resolve it: Developing and evaluating annotation methods that incorporate prior knowledge, and testing their impact on answerability assessment accuracy and code generation performance.

### Open Question 2
- Question: What are the most effective strategies for few-shot domain adaptation in answerability assessment models beyond random in-context learning examples?
- Basis in paper: [explicit] The paper discusses the importance of domain adaptation for achieving high accuracy on the RaCG benchmark and suggests that exploring more sophisticated few-shot domain adaptation methods would be an interesting research direction.
- Why unresolved: The paper only uses in-context learning with random examples for domain adaptation, without exploring other methods.
- What evidence would resolve it: Comparing the performance of different few-shot domain adaptation techniques (e.g., meta-learning, prompt tuning) on the RaCG benchmark and analyzing their impact on answerability assessment accuracy.

### Open Question 3
- Question: How can we automatically generate a more comprehensive set of unanswerable and partially answerable samples that covers a wider range of real-world scenarios?
- Basis in paper: [explicit] The paper mentions that there may be more unanswerable/partially answerable types in real-world scenarios that the current RaCG benchmark does not cover.
- Why unresolved: The current method for generating unanswerable and partially answerable samples relies on a limited set of techniques (substituting gold APIs, concatenating topically related queries, and substituting queries from out-of-database).
- What evidence would resolve it: Developing and evaluating new methods for automatically generating unanswerable and partially answerable samples, and testing their impact on the comprehensiveness and difficulty of the RaCG benchmark.

## Limitations
- The baseline answerability assessment accuracy of 46.7% indicates the task remains challenging for current models
- The RaCGEval dataset construction methodology may not fully capture real-world query complexity, particularly for private libraries
- The study does not address potential biases in the dataset or whether manually annotated samples are representative of typical user queries

## Confidence
- High confidence: The mechanism for reducing hallucination through pre-filtering unanswerable queries is well-established conceptually, though empirical validation shows limited effectiveness
- Medium confidence: Domain adaptation through in-context learning shows promise but is tested only on four specific domains
- Medium confidence: The coverage-precision trade-off analysis is theoretically sound but requires further validation in real-world deployment

## Next Checks
1. **Benchmark robustness test**: Evaluate the answerability assessment models on additional, independently constructed datasets to verify that the 46.7% baseline accuracy is not an artifact of the RaCGEval dataset construction methodology.

2. **Human evaluation of trade-off calibration**: Conduct user studies to determine whether the coverage-precision trade-off settings identified in the paper align with developer preferences and actual coding needs in practice.

3. **Cross-domain generalization analysis**: Test the in-context learning approach on domains outside the four studied (NetsPresso, TorchData, BeatNum, Monkey) to assess whether the adaptation technique generalizes to new API documentation domains or remains domain-specific.