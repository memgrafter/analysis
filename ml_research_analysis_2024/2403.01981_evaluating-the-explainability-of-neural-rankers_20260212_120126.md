---
ver: rpa2
title: Evaluating the Explainability of Neural Rankers
arxiv_id: '2403.01981'
source_url: https://arxiv.org/abs/2403.01981
tags:
- explanation
- rationales
- document
- relevance
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a systematic framework for evaluating the
  explainability of neural ranking models in information retrieval. The proposed approach
  generates text-based rationales for retrieved documents and measures their quality
  through both intrinsic consistency (how well rationales reflect the model's scoring)
  and extrinsic relevance (how well rationales align with relevant document content).
---

# Evaluating the Explainability of Neural Rankers

## Quick Facts
- **arXiv ID**: 2403.01981
- **Source URL**: https://arxiv.org/abs/2403.01981
- **Reference count**: 34
- **Primary result**: Introduced systematic framework evaluating neural ranking model explainability through intrinsic consistency and extrinsic relevance measures

## Executive Summary
This paper introduces a systematic framework for evaluating the explainability of neural ranking models in information retrieval. The proposed approach generates text-based rationales for retrieved documents and measures their quality through both intrinsic consistency (how well rationales reflect the model's scoring) and extrinsic relevance (how well rationales align with relevant document content). Experiments with various neural models (BM25, ColBERT, TCT-ColBERT, MonoT5, MonoElectra) on MS MARCO collections reveal that the most relevant models are not necessarily the most explainable. Sentence-level rationales prove more consistent than word-window approaches, and intrinsic explanation consistency does not strongly correlate with relevance measures.

## Method Summary
The evaluation framework uses occlusion-based explanation methods with two granularity levels (sentences and word windows) to generate rationales for top-k retrieved documents. Intrinsic consistency (MRC - Mean Rank Correlation) measures how well rationales reflect original model rankings by computing rank correlation between original scores and scores based on explanation rationales. Extrinsic relevance (MER - Mean Explanation Relevance) measures overlap between rationales and sub-document level relevance assessments in the MS MARCO collections. The framework evaluates multiple neural ranking models including BM25, ColBERT, TCT-ColBERT, MonoT5, and MonoElectra on the TREC DL'20 topic set.

## Key Results
- The most relevant neural ranking models (highest nDCG) are not necessarily the most explainable (highest MRC)
- Sentence-level rationales show higher intrinsic consistency than word-window approaches
- TCT-ColBERT demonstrates better extrinsic relevance despite not being the top performer in intrinsic consistency
- Intrinsic explanation consistency does not strongly correlate with relevance measures across models

## Why This Works (Mechanism)
The framework works by systematically generating and evaluating text-based explanations for neural ranking decisions. The occlusion method creates rationales by systematically removing segments of text and observing changes in model scores. Intrinsic consistency measures whether these rationales accurately capture the model's scoring logic by comparing rankings based on rationales versus original rankings. Extrinsic relevance measures whether rationales align with human relevance judgments by computing overlap with sub-document level assessments. This dual evaluation approach captures both whether explanations faithfully represent model behavior and whether they align with meaningful content.

## Foundational Learning

1. **Occlusion-based explanation methods**
   - Why needed: To generate interpretable rationales by systematically removing text segments and measuring impact on model scores
   - Quick check: Verify that removing relevant segments decreases scores while removing irrelevant segments has minimal impact

2. **Rank correlation for intrinsic consistency**
   - Why needed: To quantify how well explanation rationales preserve the original ranking order of documents
   - Quick check: Ensure that documents ranked highly by original model remain highly ranked when using only explanation rationales

3. **Sub-document level relevance assessments**
   - Why needed: To enable fine-grained evaluation of whether explanations align with truly relevant content
   - Quick check: Confirm that rationale segments overlap significantly with passages marked as relevant in the collection

4. **Dual evaluation framework**
   - Why needed: To capture both model-faithfulness (intrinsic) and content-relevance (extrinsic) aspects of explanation quality
   - Quick check: Verify that both measures provide distinct and complementary insights about explanation quality

5. **Granularity effects in explanations**
   - Why needed: To understand how explanation detail level (sentence vs. word window) affects quality and consistency
   - Quick check: Compare MRC and MER values across different granularity levels for the same models

## Architecture Onboarding

**Component Map**: Document retrieval -> Explanation generation (occlusion) -> Intrinsic consistency (MRC) and extrinsic relevance (MER) evaluation

**Critical Path**: Query input → Neural ranking model scoring → Top-k document retrieval → Explanation rationale generation → Intrinsic consistency measurement → Extrinsic relevance measurement → Evaluation comparison

**Design Tradeoffs**: Sentence-level explanations provide better consistency but may be less precise than word windows; intrinsic consistency measures model faithfulness but may not reflect human relevance; extrinsic relevance captures content alignment but depends on assessment quality

**Failure Signatures**: Low MRC values indicate rationales don't capture model reasoning; poor MER scores suggest explanations miss relevant content; weak correlation between MRC and nDCG reveals explanations capture different aspects than traditional relevance

**First Experiments**: 
1. Generate sentence-level explanations for BM25 and measure MRC to establish baseline consistency
2. Compare word-window versus sentence-level explanations for ColBERT using MER to evaluate granularity effects
3. Compute correlation between MRC and nDCG across all models to verify the lack of strong relationship

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do different explanation algorithms (beyond occlusion) affect the intrinsic and extrinsic explainability measures across neural ranking models?
- **Basis in paper**: The paper uses occlusion-based explanations but notes this is just one approach among LIME, SHAP, etc., and that the evaluation framework is method-agnostic.
- **Why unresolved**: The paper only evaluates one explanation algorithm (occlusion-based), limiting understanding of how algorithm choice impacts explainability metrics.
- **What evidence would resolve it**: Comparative experiments applying multiple explanation algorithms (LIME, SHAP, etc.) to the same neural models and measuring resulting MRC and MER values.

### Open Question 2
- **Question**: Do explanation rationales improve user satisfaction and trust in neural ranking systems beyond what traditional relevance measures capture?
- **Basis in paper**: The paper notes that explanation consistency does not correlate well with relevance measures and mentions this as a complementary dimension, suggesting potential user-centric value.
- **Why unresolved**: The paper focuses on offline evaluation metrics without empirical user studies to validate practical benefits of explanations.
- **What evidence would resolve it**: User studies comparing satisfaction and trust scores between systems with and without explanation rationales, controlling for relevance performance.

### Open Question 3
- **Question**: How does the size and diversity of training data affect the explainability of neural ranking models as measured by MRC and MER?
- **Basis in paper**: The paper notes that "an increase in complexity mostly leads to less consistent explanations" but does not explore the relationship between training data characteristics and explainability.
- **Why unresolved**: The experiments use fixed datasets without varying training data size or diversity to isolate its effects on explainability.
- **What evidence would resolve it**: Controlled experiments training neural models on datasets of varying sizes and diversities, then measuring changes in MRC and MER values.

## Limitations

- The framework depends on sub-document relevance assessments, but the document-passage mapping methodology for MS MARCO document collection is not fully specified
- The study focuses on a single dataset (MS MARCO) and limited set of neural models, constraining generalizability
- The extrinsic relevance measure may be influenced by artifacts in the assessment process rather than genuine content alignment

## Confidence

*High Confidence*: The finding that intrinsic explanation consistency (MRC) does not strongly correlate with relevance measures (nDCG) is well-supported by the experimental results across multiple models. The comparison between sentence-level and word-window explanation approaches also shows consistent patterns.

*Medium Confidence*: The claim that TCT-ColBERT demonstrates better extrinsic relevance despite lower intrinsic consistency requires careful interpretation due to potential assessment artifacts. The observed variations in MER across models are supported but may be influenced by the unexplained document-passage mapping process.

## Next Checks

1. **Validation Check 1**: Replicate the sub-document relevance assessment process with full documentation of the document-passage mapping methodology to verify that extrinsic relevance measures reflect genuine content alignment rather than assessment artifacts.

2. **Validation Check 2**: Conduct experiments across multiple IR datasets (beyond MS MARCO) and with additional neural ranking architectures to test the generalizability of the observed patterns in explanation consistency and relevance alignment.

3. **Validation Check 3**: Perform ablation studies varying the occlusion parameters (number of segments, window sizes) to determine the sensitivity of explanation quality measures to methodological choices and identify optimal settings for different model types.