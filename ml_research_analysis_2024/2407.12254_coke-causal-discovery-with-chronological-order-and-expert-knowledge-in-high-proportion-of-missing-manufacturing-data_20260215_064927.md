---
ver: rpa2
title: 'COKE: Causal Discovery with Chronological Order and Expert Knowledge in High
  Proportion of Missing Manufacturing Data'
arxiv_id: '2407.12254'
source_url: https://arxiv.org/abs/2407.12254
tags:
- data
- causal
- missing
- graph
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of discovering causal relationships
  in manufacturing data that contains up to 90% missing values, hundreds of sensors,
  and includes expert knowledge and chronological order information. The authors propose
  COKE, a novel method that constructs causal graphs without imputing missing data
  by leveraging the characteristics of recipes in manufacturing data.
---

# COKE: Causal Discovery with Chronological Order and Expert Knowledge in High Proportion of Missing Manufacturing Data

## Quick Facts
- arXiv ID: 2407.12254
- Source URL: https://arxiv.org/abs/2407.12254
- Reference count: 34
- Key outcome: Average 39.9% F1-score improvement over benchmarks, reaching 85.0% on real semiconductor data

## Executive Summary
This paper addresses the challenge of discovering causal relationships in manufacturing data with up to 90% missing values and hundreds of sensors. The proposed COKE method leverages recipe structures, expert knowledge, and chronological order to construct causal graphs without imputing missing data. By using graph attention networks to generate embeddings from complete and incomplete data subsets, and optimizing the graph generation process through an actor-critic reinforcement learning framework, COKE achieves significant improvements in causal graph reconstruction accuracy compared to existing methods.

## Method Summary
COKE constructs causal graphs by first creating an initial graph incorporating expert knowledge and chronological order constraints. The method then partitions data by recipe and uses graph attention networks to generate embeddings from observed variables in both complete and incomplete subsets. These embeddings are combined with learnable weights. An actor-critic reinforcement learning framework optimizes the ordering graph generation process, using BIC score as reward while penalizing violations of expert knowledge constraints. The final causal graph is selected based on maximum reward.

## Key Results
- Average 39.9% improvement in F1-score compared to benchmark methods
- 62.6% improvement on real-world dataset configurations
- 85.0% improvement on real-world semiconductor manufacturing datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: COKE avoids imputation bias by leveraging the recipe structure of manufacturing data to selectively process only observed variables.
- Mechanism: Instead of imputing missing values, COKE partitions data by recipe, processes each subset's observed variables using graph attention networks, and combines embeddings from both complete and incomplete subsets.
- Core assumption: The recipe-based partitioning preserves the causal structure without needing to fill missing values.
- Evidence anchors:
  - [abstract]: "COKE to construct causal graphs in manufacturing datasets by leveraging expert knowledge and chronological order among sensors without imputing missing data."
  - [section]: "We utilize recipes in the manufacturing process to generate embeddings that comprehensively represent variables without imputing missing values."

### Mechanism 2
- Claim: The actor-critic reinforcement learning framework optimizes the ordering graph generation by maximizing a reward based on BIC score.
- Mechanism: The actor generates ordering variable sets, the critic evaluates state values, and the reward function encourages low BIC scores while penalizing missing edges required by expert knowledge.
- Core assumption: The BIC score is a valid proxy for causal graph quality and can guide RL training.
- Evidence anchors:
  - [abstract]: "The graph-generating process has been optimized by an actor-critic architecture to obtain a final graph that has a maximum reward."
  - [section]: "We utilize an actor-critic framework to train the ordering graph generation process, conceptualizing it as a multi-step decision-making process."

### Mechanism 3
- Claim: Incorporating expert knowledge and chronological order reduces the search space for valid causal orderings.
- Mechanism: COKE constructs an initial graph by removing edges inconsistent with chronological order and applying expert knowledge, then uses this constrained graph to guide variable selection.
- Core assumption: Expert knowledge and chronological order are accurate and reduce false positive edges.
- Evidence anchors:
  - [abstract]: "COKE to construct causal graphs in manufacturing datasets by leveraging expert knowledge and chronological order among sensors."
  - [section]: "We refine the initial graph ð‘”ð‘˜ by removing the edges that are not in the union of all Î  âˆˆ Â¯Î¦(G âˆ—)."

## Foundational Learning

- Concept: Graph Attention Networks (GAT)
  - Why needed here: GAT allows dynamic weighting of parent variables when computing embeddings, crucial for handling variable influence in incomplete data.
  - Quick check question: How does GAT differ from standard graph convolutions in handling variable importance?

- Concept: Reinforcement Learning (Actor-Critic)
  - Why needed here: The actor-critic framework enables iterative optimization of the causal graph generation process based on reward signals.
  - Quick check question: What is the role of the critic network in stabilizing actor updates?

- Concept: Bayesian Information Criterion (BIC)
  - Why needed here: BIC balances model fit and complexity, guiding the selection of causal graphs with fewer spurious edges.
  - Quick check question: Why is BIC preferred over other scoring functions like AIC in this context?

## Architecture Onboarding

- Component map: Data preprocessing -> GAT layers -> Actor network -> Critic network -> Reward function -> Decoder
- Critical path:
  1. Partition data by recipe
  2. Compute GAT embeddings for observed variables
  3. Generate ordering via actor network
  4. Apply initial graph constraints
  5. Score graph with BIC and update RL networks
- Design tradeoffs:
  - No imputation vs. potential loss of information from missing data
  - RL-based optimization vs. computational overhead
  - Expert knowledge integration vs. risk of incorrect constraints
- Failure signatures:
  - Low F1-score improvements indicate poor embedding quality or incorrect constraints
  - Training instability suggests RL hyperparameters need tuning
  - Overfitting to synthetic data may reduce real-world performance
- First 3 experiments:
  1. Validate GAT embeddings on a small dataset with known causal structure
  2. Test RL training stability with synthetic reward signals
  3. Compare F1-scores with and without expert knowledge constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the COKE model perform in scenarios with missing data that follows a specific pattern, such as systematic missingness based on sensor or machine type, rather than random missingness?
- Basis in paper: [inferred] The paper mentions that the missing data in manufacturing is due to variations in recipes, but does not explore scenarios where missingness follows a specific pattern.
- Why unresolved: The experiments in the paper simulate missing data based on recipes, which might not capture all possible patterns of missingness in real-world manufacturing data.
- What evidence would resolve it: Experimental results comparing COKE's performance on datasets with different missingness patterns, such as systematic missingness based on sensor or machine type, would provide insights into the model's robustness to different types of missing data.

### Open Question 2
- Question: Can the COKE model be extended to handle latent confounders, which are not directly observed but influence the causal relationships between variables?
- Basis in paper: [explicit] The paper assumes that all variables are measurable and there are no confounders, which are latent causes of the observed variables in the dataset.
- Why unresolved: The assumption of no confounders might not hold in real-world manufacturing data, where latent factors could influence the causal relationships between sensors.
- What evidence would resolve it: Experimental results demonstrating COKE's performance on datasets with known latent confounders, or modifications to the model to account for latent confounders, would provide insights into the model's ability to handle such scenarios.

### Open Question 3
- Question: How does the COKE model's performance scale with extremely high-dimensional data, such as datasets with thousands of sensors, which might be encountered in large-scale manufacturing processes?
- Basis in paper: [inferred] The paper demonstrates COKE's effectiveness on datasets with up to 180 variables, but does not explore scenarios with extremely high-dimensional data.
- Why unresolved: The computational complexity of COKE might increase significantly with the number of variables, potentially limiting its applicability to large-scale manufacturing processes.
- What evidence would resolve it: Experimental results comparing COKE's performance and computational efficiency on datasets with varying numbers of variables, including extremely high-dimensional data, would provide insights into the model's scalability.

## Limitations
- Unknown implementation details of expert knowledge integration (number and selection of mandatory edges)
- Lack of extensive validation that BIC scores correlate with true causal structure in manufacturing domain
- Unproven scalability to thousands of sensors

## Confidence
- High: Overall effectiveness with significant F1-score improvements across multiple datasets
- Medium: Specific claims about RL component effectiveness and expert knowledge integration due to limited validation
- Low: Scalability claims to extremely high-dimensional data (not experimentally tested)

## Next Checks
1. Test COKE performance with systematically corrupted expert knowledge to evaluate sensitivity to incorrect constraints
2. Evaluate scalability by testing on synthetic datasets with 1000+ sensors while measuring runtime and memory usage
3. Conduct ablation studies removing the actor-critic component to isolate its contribution to performance gains