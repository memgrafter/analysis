---
ver: rpa2
title: Synthetic Query Generation using Large Language Models for Virtual Assistants
arxiv_id: '2406.06729'
source_url: https://arxiv.org/abs/2406.06729
tags:
- queries
- query
- entity
- generated
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using large language models (LLMs) to generate
  synthetic queries for virtual assistant (VA) speech recognition. Templates and standalone
  entity names were compared to LLM outputs (babbage-002, gpt-3.5-turbo, gpt-3.5-turbo-instruct,
  gpt-4).
---

# Synthetic Query Generation using Large Language Models for Virtual Assistants

## Quick Facts
- **arXiv ID**: 2406.06729
- **Source URL**: https://arxiv.org/abs/2406.06729
- **Reference count**: 22
- **Primary result**: LLM-generated queries are more verbose and specific than template-based methods, showing complementarity with low overlap between query sets.

## Executive Summary
This paper explores using large language models to generate synthetic queries for virtual assistant speech recognition systems. The authors compare four LLM variants against template-based and entity-name-only methods for generating music artist queries. LLM-generated queries are found to be more verbose and entity-specific, complementing template-based approaches with minimal query overlap. The best-performing model (gpt-3.5-turbo-instruct) achieves a strong balance between naturalness and specificity for VA query generation.

## Method Summary
The study uses a knowledge base of 14,161 music artist entities linked to Wikipedia descriptions and music streaming profiles. Four LLM variants (babbage-002, gpt-3.5-turbo, gpt-3.5-turbo-instruct, gpt-4) generate 40 queries per entity using artist descriptions as context. Generated queries are evaluated against anonymized VA query logs using a 4-gram language model (measuring NLL) and against the knowledge base using BM25-L (measuring reciprocal rank). Template-based methods and entity-name-only approaches serve as baselines for comparison.

## Key Results
- LLM-generated queries are more verbose and specific than template-based queries, often referencing specific songs or albums
- Template and LLM-generated queries show very low overlap (Jaccard coefficient ~0.004), indicating complementarity
- gpt-3.5-turbo-instruct and gpt-3.5-turbo produce the best balance of naturalness and specificity
- LLM queries represent infrequent "tail use-cases" in VA usage while maintaining domain relevance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated queries are more verbose and entity-specific compared to template-based methods.
- Mechanism: LLMs use the provided entity description to generate queries that incorporate specific details about the entity (e.g., song titles, albums, collaborations) rather than generic intent templates.
- Core assumption: The LLM has sufficient context from the entity description to generate relevant and specific queries without being explicitly instructed to do so.
- Evidence anchors:
  - [abstract] "LLMs generate more verbose queries, compared to template-based methods, and reference aspects specific to the entity."
  - [section] "LLM-generated queries often reference specific songs or albums by the artist—extracted from the artist's description—resulting in less generic queries."
  - [corpus] Weak: No direct corpus evidence on verbosity differences.
- Break condition: If the entity description is too sparse or the LLM lacks sufficient context, the generated queries may not be specific or verbose.

### Mechanism 2
- Claim: LLM-generated queries complement template-based methods due to low overlap in generated queries.
- Mechanism: Templates generate generic queries applicable across all entities, while LLMs generate entity-specific queries, resulting in different query sets with minimal overlap.
- Core assumption: The LLM generates queries that are sufficiently distinct from template-generated queries to be considered complementary.
- Evidence anchors:
  - [abstract] "We find that LLMs generate more verbose queries, compared to template-based methods, and reference aspects specific to the entity."
  - [section] "comparing the query sets generated by the template-based method and gpt-3.5-turbo-instruct, the mean/std. dev of the Jaccard coefficient across entities equals 0.0038 ± 0.0084, indicating very low overlap, and hence, complementarity."
  - [corpus] Weak: No corpus evidence on query overlap or complementarity.
- Break condition: If the LLM is trained on similar data as the templates or if the entity descriptions are too generic, the overlap may increase, reducing complementarity.

### Mechanism 3
- Claim: LLM-generated queries are domain-matched to real VA user queries but are more specific.
- Mechanism: The 4-gram language model scores LLM-generated queries lower than template queries, indicating they are less common in real usage but still within the domain.
- Core assumption: The language model accurately represents the domain of VA user queries.
- Evidence anchors:
  - [abstract] "The generated queries are similar to VA user queries, and are specific enough to retrieve the relevant entity."
  - [section] "LLM-generated queries seem to represent infrequent, tail use-cases. While not entirely absent from VA usage, they are not as common as the straight-forward templates."
  - [corpus] Weak: No corpus evidence on domain matching or specificity.
- Break condition: If the language model is not representative of VA user queries or if the LLM generates queries outside the domain, the domain matching may not hold.

## Foundational Learning

- Concept: Language Models (LMs) and Query Priors
  - Why needed here: LMs are used to estimate the likelihood of generated queries matching real VA user queries, which is crucial for evaluating domain match.
  - Quick check question: How does a 4-gram language model with Good Turing smoothing handle unseen n-grams in query evaluation?

- Concept: Information Retrieval (IR) and BM25-L
  - Why needed here: BM25-L is used to measure the specificity of generated queries by evaluating their ability to retrieve the target entity.
  - Quick check question: What role do the parameters k1, b, and δ play in the BM25-L retrieval model?

- Concept: Large Language Models (LLMs) and Prompt Engineering
  - Why needed here: LLMs are used to generate synthetic queries based on entity descriptions, and the prompt structure influences the generated queries' relevance and specificity.
  - Quick check question: How does providing entity descriptions as context affect the LLM's ability to generate relevant and specific queries?

## Architecture Onboarding

- Component map:
  Entity Knowledge Base -> Prompt Generator -> LLM API -> Language Model -> BM25-L Index

- Critical path:
  1. Retrieve entity description from knowledge base
  2. Construct prompt with entity description and name
  3. Send prompt to LLM API and receive generated queries
  4. Evaluate domain match using 4-gram language model
  5. Measure specificity using BM25-L retrieval model

- Design tradeoffs:
  - Using LLMs vs. templates: LLMs generate more specific and diverse queries but are computationally more expensive
  - Prompt complexity: More detailed prompts may lead to more relevant queries but increase generation time
  - Language model choice: A more complex language model may better capture domain nuances but require more computational resources

- Failure signatures:
  - LLM generates irrelevant or nonsensical queries: Check prompt structure and entity description quality
  - Low domain match scores: Verify language model training data and prompt relevance
  - Poor retrieval performance: Inspect BM25-L index quality and query pre-processing

- First 3 experiments:
  1. Compare generated query lengths and specificity between different LLM models
  2. Measure the impact of varying the number of generated queries (K) on domain match and specificity
  3. Evaluate the complementarity of template-based and LLM-generated queries by measuring overlap and performance

## Open Questions the Paper Calls Out
- How can the results of multiple query generation methods (templates and LLMs) be effectively combined to create a final query collection that better aligns with user behavior?
- How do advanced prompting techniques, such as chain-of-thought prompting, affect the quality and specificity of LLM-generated queries?
- Does fine-tuning LLMs on domain-specific data (e.g., VA query logs) improve the quality and relevance of generated queries?

## Limitations
- Knowledge base covers only music artist entities, limiting generalizability to other domains
- Evaluation metrics rely on specific language models and retrieval systems that may not transfer to different VA architectures
- LLM prompt structure and entity descriptions are not standardized, introducing variability in query generation quality

## Confidence
- **High Confidence**: The complementarity finding between template and LLM-generated queries (Jaccard coefficient ~0.004) is well-supported by the presented data and methodology.
- **Medium Confidence**: The claim about LLM queries being more specific and verbose is supported by the data but could benefit from additional validation across domains.
- **Low Confidence**: The assertion that LLM-generated queries represent "tail use-cases" in VA usage lacks sufficient empirical backing from the language model evaluation results.

## Next Checks
1. **Domain Generalization Test**: Replicate the study using a knowledge base of non-music entities (e.g., movies, books, restaurants) to verify if LLM-generated queries maintain their specificity advantage across domains.
2. **Real-World Performance Validation**: Implement a live A/B test where template-generated and LLM-generated queries are used as training data for a VA's speech recognition system, measuring actual recognition accuracy improvements.
3. **Prompt Engineering Analysis**: Systematically vary prompt structures (e.g., different context lengths, instruction styles) to determine the optimal prompt design for generating high-quality, domain-specific queries across different LLM models.