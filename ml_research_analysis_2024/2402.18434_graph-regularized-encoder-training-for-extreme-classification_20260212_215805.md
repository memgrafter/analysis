---
ver: rpa2
title: Graph Regularized Encoder Training for Extreme Classification
arxiv_id: '2402.18434'
source_url: https://arxiv.org/abs/2402.18434
tags:
- graph
- ramen
- training
- metadata
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extreme multi-label classification
  in scenarios with long-tail labels and sparse data, such as sponsored search. The
  proposed method, RAMEN, replaces computationally expensive graph convolutional networks
  (GCNs) with a dual-encoder architecture trained using graph metadata as a regularizer.
---

# Graph Regularized Encoder Training for Extreme Classification

## Quick Facts
- **arXiv ID**: 2402.18434
- **Source URL**: https://arxiv.org/abs/2402.18434
- **Authors**: Anshul Mittal, Shikhar Mohan, Deepak Saini, Siddarth Asokan, Suchith C. Prabhu, Lakshya Kumar, Pankaj Malhotra, Jain jiao, Amit Singh, Sumeet Agarwal, Soumen Chakrabarti, Purushottam Kar, Manik Varma
- **Reference count**: 40
- **Primary result**: RAMEN achieves up to 15% higher accuracy than state-of-the-art methods on benchmark datasets while reducing inference time by 50% and model size by 70%.

## Executive Summary
This paper addresses the challenge of extreme multi-label classification in scenarios with long-tail labels and sparse data, such as sponsored search. The proposed method, RAMEN, replaces computationally expensive graph convolutional networks (GCNs) with a dual-encoder architecture trained using graph metadata as a regularizer. By dynamically adjusting regularization based on graph link confidence and avoiding noisy graph traversal during inference, RAMEN achieves up to 15% higher accuracy than state-of-the-art methods on benchmark datasets while reducing inference time by 50% and model size by 70%. In real-world A/B tests on Bing Ads, RAMEN delivered 1.25-1.5% revenue gains, 0.5-0.6% CTR improvements, and up to 4.8% reduction in brand mismatch rates.

## Method Summary
RAMEN uses a dual-encoder architecture (DistilBERT) trained with graph-based regularization. Instead of GCNs at inference time, RAMEN leverages metadata graphs during training to regularize the encoder, enforcing that queries/ads stay close to relevant anchor points and far from irrelevant ones. The training pipeline uses a triplet loss with two regularization terms (query-anchor, ad-anchor), and includes in-batch negative mining and edge pruning based on cosine similarity. Inference is performed using only the encoder and cosine similarity search, avoiding the expensive graph traversal step.

## Key Results
- Up to 15% higher accuracy than state-of-the-art methods on benchmark datasets
- 50% faster inference and 70% fewer parameters compared to GCN-based approaches
- In real-world A/B tests: 1.25-1.5% revenue gains, 0.5-0.6% CTR improvements, up to 4.8% reduction in brand mismatch rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph metadata regularization improves model accuracy for tail queries/ads without requiring GCNs at inference time.
- Mechanism: During training, RAMEN uses graph metadata to regularize the encoder by enforcing that queries/ads stay close to their relevant anchor points and far from irrelevant ones. This creates richer embeddings for tail items while avoiding the costly and noisy graph traversal step during inference.
- Core assumption: The regularization loss effectively captures the semantic relationships between items and anchors, even if some edges in the metadata graph are noisy.
- Evidence anchors:
  - [abstract] "RAMEN introduces a novel approach to utilize graph metadata during training, with minimal overheads while saving computational cost in model size and inference time"
  - [section] "RAMEN utilizes in-batch negative mining... RAMEN uses a DistilBERT encoder as E_Î¸"
  - [corpus] Weak - no direct mention of this regularization mechanism
- Break condition: If the regularization loss is poorly tuned or the metadata graph contains mostly irrelevant edges, the training signal could become misleading and degrade performance.

### Mechanism 2
- Claim: Dynamically adjusting regularization strength based on graph link confidence mitigates the impact of noisy graph edges.
- Mechanism: RAMEN prunes edges during training using cosine similarity between embeddings, ensuring that only confident edges contribute to the regularization loss. This adapts the training process to the quality of the metadata graph.
- Core assumption: The pruning step reliably filters out most noisy edges without removing too many true positive edges.
- Evidence anchors:
  - [section] "RAMEN performs a random walk with restart on each anchor node... in-batch pruning was performed and edges to only those anchors were retained which had a cosine similarity of > 0"
  - [section] "To deal with such edges, in-batch pruning was performed and edges to only those anchors were retained which had a cosine similarity of > 0 based on the embeddings given the encoder"
  - [corpus] Weak - no direct mention of confidence-based regularization
- Break condition: If the pruning threshold is too high, it might remove too many valid edges; if too low, it may not sufficiently reduce noise.

### Mechanism 3
- Claim: Using a dual encoder architecture instead of GCNs reduces both model size and inference time while maintaining or improving accuracy.
- Mechanism: RAMEN uses a lightweight DistilBERT-based dual encoder for both queries and ads, avoiding the expensive GCN layers. Inference only requires computing embeddings and performing nearest neighbor search, which is much faster than GCN traversal.
- Core assumption: The dual encoder can learn representations that are as discriminative as GCN-based methods for extreme classification.
- Evidence anchors:
  - [abstract] "RAMEN offers significant performance boosts with zero increase in inference computational costs... being 50% faster to infer, and having 70% fewer parameters"
  - [section] "RAMEN provides accurate retrievals for tail queries and avoids the costly step of inducing graph as used in GCNs like OAK"
  - [corpus] Weak - no direct mention of dual encoder architecture
- Break condition: If the dual encoder fails to capture long-range dependencies that GCNs could model through graph convolutions, performance may degrade.

## Foundational Learning

- Concept: Extreme multi-label classification (XMC)
  - Why needed here: The paper targets scenarios where each data point can be associated with many labels from a very large label space (millions), common in recommendation and search tasks.
  - Quick check question: What is the key difference between multi-class and extreme multi-label classification?

- Concept: Graph Neural Networks (GCNs) and their limitations
  - Why needed here: Understanding why GCNs are expensive and noisy in this context helps justify the proposed regularization approach.
  - Quick check question: Why might GCNs perform poorly on tail labels in extreme classification?

- Concept: Contrastive learning and triplet loss
  - Why needed here: RAMEN uses a triplet loss to train the encoder, pulling relevant items together and pushing irrelevant ones apart.
  - Quick check question: How does a triplet loss differ from a standard cross-entropy loss in training?

## Architecture Onboarding

- Component map: Query/Encoder -> DistilBERT Encoder -> Unit-norm embeddings -> Cosine similarity search
- Critical path:
  1. Initialize encoder with DistilBERT
  2. Build metadata graphs from anchor sets
  3. Train encoder with contrastive loss + regularization
  4. Prune graph edges based on encoder embeddings
  5. Repeat training with pruned graph until convergence
  6. Deploy encoder-only inference pipeline
- Design tradeoffs:
  - Accuracy vs. inference cost: GCNs are more accurate with oracle graphs but expensive; RAMEN trades a small accuracy gain for large efficiency gains.
  - Graph quality vs. model robustness: Better metadata graphs improve RAMEN, but the regularization approach is designed to handle noise.
- Failure signatures:
  - Model underfits tail labels: Likely due to insufficient regularization or poor graph quality.
  - Model overfits to graph noise: Likely due to inadequate pruning or too strong regularization.
  - Inference too slow: Likely due to inefficient encoder or embedding search.
- First 3 experiments:
  1. Train RAMEN with and without regularization on a small dataset to measure accuracy impact.
  2. Vary the pruning threshold to see its effect on training stability and performance.
  3. Compare inference time and accuracy against a baseline GCN method on a real-world dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RAMEN's performance change when using different types of metadata graphs (e.g., co-session queries, similar queries, or textual similarity) beyond the ones evaluated in the paper?
- Basis in paper: [inferred] The paper mentions that RAMEN can use different types of metadata graphs, but the experiments only evaluate specific types. The paper states: "These graph is essentially links between queries/ads to their relevant node in the graph."
- Why unresolved: The paper does not provide a systematic comparison of RAMEN's performance across different metadata graph types.
- What evidence would resolve it: Conduct experiments with various metadata graph types and compare RAMEN's performance on each.

### Open Question 2
- Question: How does the quality of the metadata graph impact RAMEN's performance, and what are the practical limits of using noisy graphs?
- Basis in paper: [explicit] The paper discusses the challenge of noisy graphs and the need for pruning. It states: "Traversal over the graph can also lead to irrelevant links such as 'Vestigial organs' and extracting meaningful information from such noisy graphs is a challenge."
- Why unresolved: While the paper demonstrates RAMEN's ability to handle noisy graphs through pruning, it does not quantify the impact of graph quality on performance or establish practical limits.
- What evidence would resolve it: Systematically evaluate RAMEN's performance with varying levels of graph noise and determine the threshold beyond which performance degrades significantly.

### Open Question 3
- Question: Can RAMEN's regularization framework be extended to other types of metadata beyond graphs, such as images or text embeddings?
- Basis in paper: [inferred] The paper focuses on graph metadata, but the regularization framework could potentially be applied to other types of metadata. The paper mentions: "Beyond textual ads descriptions [11, 13, 40], this auxiliary metadata can augment the limited supervision available for tail query/ads."
- Why unresolved: The paper does not explore the application of RAMEN's regularization framework to other types of metadata.
- What evidence would resolve it: Adapt RAMEN's regularization framework to incorporate different types of metadata and evaluate its performance compared to the graph-based approach.

## Limitations

- The graph construction methodology for sponsored search data is not fully detailed, making faithful reproduction challenging.
- The method's performance on datasets without rich metadata graphs remains unclear.
- The edge pruning threshold (cosine similarity > 0) is specified but may require tuning for different datasets.

## Confidence

- **High Confidence**: Claims about computational efficiency gains (50% faster inference, 70% fewer parameters) and the general training methodology using graph regularization.
- **Medium Confidence**: Claims about accuracy improvements (15% higher accuracy) and real-world A/B test results, as these depend heavily on specific implementation details and data quality.
- **Low Confidence**: Claims about robustness to noisy graph edges, as the pruning mechanism's effectiveness may vary significantly with different data distributions.

## Next Checks

1. **Reproduce graph construction**: Implement the exact methodology for creating query-anchor and ad-anchor graphs from session data, and validate edge quality using the same pruning criteria.
2. **Hyperparameter sensitivity analysis**: Systematically vary the pruning threshold and regularization strength to determine their impact on performance across different dataset characteristics.
3. **Cross-domain validation**: Test RAMEN on datasets from different domains (e.g., product recommendation, document tagging) to assess generalizability beyond sponsored search and Wikipedia data.