---
ver: rpa2
title: Query-Efficient Adversarial Attack Against Vertical Federated Graph Learning
arxiv_id: '2411.02809'
source_url: https://arxiv.org/abs/2411.02809
tags:
- attack
- adversarial
- graph
- data
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of vertical federated graph
  learning (VFGL) to adversarial attacks, where malicious clients can manipulate local
  data to mislead the server model. The proposed method, NA2 (Neuron-based Adversarial
  Attack), improves the effectiveness of centralized adversarial attacks against VFGL
  by constructing a reliable shadow model through training data manipulation.
---

# Query-Efficient Adversarial Attack Against Vertical Federated Graph Learning

## Quick Facts
- **arXiv ID**: 2411.02809
- **Source URL**: https://arxiv.org/abs/2411.02809
- **Reference count**: 40
- **Primary result**: NA2 improves attack success rates by 205.36% on Cora dataset with only one query to server model

## Executive Summary
This paper addresses the vulnerability of vertical federated graph learning (VFGL) to adversarial attacks, where malicious clients can manipulate local data to mislead the server model. The proposed method, NA2 (Neuron-based Adversarial Attack), improves the effectiveness of centralized adversarial attacks against VFGL by constructing a reliable shadow model through training data manipulation. NA2 locates significant neural paths in the local GNN, modifies key node features to increase the malicious client's contribution, and builds a shadow model using the manipulated data and server model probabilities. The shadow model guides the generation of adversarial perturbations, requiring only one query to the server.

## Method Summary
NA2 operates in three stages: first, it manipulates local training data to increase the malicious client's contribution to the server model; second, it builds a shadow model that mimics the server model's behavior using Mean Square Error loss; third, it generates adversarial perturbations using centralized attack methods guided by the shadow model. The method identifies significant neurons and neural paths through testing, then modifies key features of target nodes to maximize impact while minimizing damage to normal task performance. This approach achieves query efficiency by requiring only one query to the server for obtaining necessary probability outputs.

## Key Results
- Achieves 205.36% improvement in attack success rate on Cora dataset compared to baselines
- Maintains linear time complexity even for large datasets
- Shows good transferability across different GNN architectures
- Remains effective under potential adaptive defenses like differential privacy and FoolsGold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NA2 improves attack success rate by manipulating local training data to increase the malicious client's contribution to the server model.
- Mechanism: The malicious client modifies key node features in its local training data, making the server model more dependent on its data. This dependency allows the malicious client to build a shadow model that closely mimics the server model's behavior, enabling effective adversarial attack generation with minimal queries.
- Core assumption: The server model's decision boundaries can be approximated by a shadow model trained on manipulated data that has increased the malicious client's contribution.
- Evidence anchors:
  - [abstract] "Specifically, a malicious client manipulates its local training data to improve its contribution in a stealthy fashion. Then a shadow model is established based on the manipulated data to simulate the behavior of the server model in VFGL."
  - [section] "Inspired by the observation that the server model in VFGL is more likely to be misled by high-contribution clients, a training data manipulation strategy is adopted for NA2 to enhance the adversarial attack."
  - [corpus] Weak evidence - no direct citations about contribution-based attacks in VFGL
- Break condition: If the server model uses techniques to reduce client contribution sensitivity or implements robust aggregation methods that are not dependent on individual client contributions.

### Mechanism 2
- Claim: NA2 uses neuron testing to identify significant neural paths and key features for targeted manipulation.
- Mechanism: After training for a number of epochs, NA2 performs neuron testing on the local GNN to locate significant neurons along neural paths. It identifies the most important features by analyzing the weight matrix of the first layer, then modifies these features to maximize impact on model decisions while minimizing damage to normal task performance.
- Core assumption: The activation patterns of neurons during training reveal which features are most important for the model's decision-making process.
- Evidence anchors:
  - [abstract] "Specifically, a malicious client manipulates its local training data to improve its contribution in a stealthy fashion."
  - [section] "NA2 commits to locating significant neural paths related to the main task and manipulating the corresponding data, which curbs the performance degradation of the main task as much as possible."
  - [corpus] Weak evidence - no direct citations about neuron testing for adversarial attacks
- Break condition: If the GNN architecture changes significantly or if neuron activation patterns become too complex to analyze effectively with this method.

### Mechanism 3
- Claim: NA2 achieves query efficiency by using a shadow model with Mean Square Error (MSE) loss instead of traditional cross-entropy loss.
- Mechanism: The shadow model is trained using MSE loss between its predictions and the server model's probabilities, which helps it mimic the server model's behavior more accurately. This allows NA2 to generate adversarial perturbations without repeatedly querying the server, requiring only one query to obtain the necessary probabilities.
- Core assumption: MSE loss better captures the behavior of the server model's decision boundaries compared to cross-entropy loss when the goal is to mimic rather than classify.
- Evidence anchors:
  - [abstract] "As a result, the shadow model can improve the attack success rate of various centralized attacks with a few queries."
  - [section] "Then we explain why the MSE can be applied to guide the shadow model to mimic the behavior of the server model in VFGL."
  - [corpus] Weak evidence - no direct citations about MSE-based shadow models for adversarial attacks
- Break condition: If the server model's probability outputs become unreliable or if the relationship between MSE and decision boundary alignment breaks down.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their vulnerability to adversarial attacks
  - Why needed here: Understanding how GNNs process graph-structured data and why they are susceptible to adversarial perturbations is fundamental to grasping NA2's approach
  - Quick check question: What makes GNNs vulnerable to adversarial attacks compared to traditional neural networks?

- Concept: Vertical Federated Learning (VFL) architecture and threat models
  - Why needed here: NA2 operates specifically in the VFL setting where data is distributed across clients with different feature spaces, and understanding this context is crucial for comprehending the attack vector
  - Quick check question: How does the VFL setting differ from horizontal federated learning in terms of data distribution and potential attack surfaces?

- Concept: Shadow model construction and knowledge distillation
  - Why needed here: NA2 relies on building a shadow model that mimics the server model's behavior, which is a form of knowledge distillation from probability outputs rather than direct access
  - Quick check question: What are the key differences between using MSE loss versus cross-entropy loss when training a shadow model for adversarial purposes?

## Architecture Onboarding

- Component map:
  - Local GNN (malicious client) -> Data manipulation module -> Shadow model -> Attack generator -> Server model
- Critical path: Data manipulation → Shadow model training → Adversarial attack generation
- Design tradeoffs:
  - Feature modification scale vs. task performance degradation
  - Number of epochs before manipulation starts vs. attack effectiveness
  - Shadow model complexity vs. accuracy in mimicking server behavior
  - Attack budget vs. stealthiness of perturbations
- Failure signatures:
  - Low contribution score indicates manipulation wasn't effective
  - High variance in neuron activation suggests unstable feature importance
  - Poor shadow model accuracy indicates MSE loss isn't capturing server behavior well
  - Attack success rate close to baseline suggests shadow model isn't accurate enough
- First 3 experiments:
  1. Run NA2 with different feature modification scales (γ) on a small dataset to find the optimal balance between attack effectiveness and task performance
  2. Compare MSE-based shadow model vs. cross-entropy shadow model on the same dataset to validate the design choice
  3. Test transferability by using a shadow model trained on one GNN architecture to attack a server model with a different architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does NA2 perform against adaptive defenses that specifically target data manipulation detection rather than relying on DP or FoolsGold?
- Basis in paper: [inferred] The paper discusses NA2's effectiveness against potential defenses like DP and FoolsGold, but does not explore adaptive defenses designed to detect data manipulation techniques specifically.
- Why unresolved: The paper only evaluates NA2 against general defense mechanisms, leaving open the question of how it would fare against defenses tailored to detect its specific manipulation strategies.
- What evidence would resolve it: Experimental results showing NA2's performance against adaptive defenses designed to detect data manipulation would resolve this question.

### Open Question 2
- Question: What is the optimal balance between feature modification scale (γ) and the starting epoch (τ) for maximizing attack success while minimizing impact on the main task?
- Basis in paper: [explicit] The paper discusses parameter analysis of γ and τ, noting their impact on attack success rate and benign accuracy, but does not provide a definitive optimal balance.
- Why unresolved: The paper suggests using moderate values but does not explore the full trade-off space or provide a clear method for determining the optimal balance in different scenarios.
- What evidence would resolve it: A comprehensive analysis showing the relationship between γ, τ, attack success rate, and main task performance across various datasets and scenarios would resolve this question.

### Open Question 3
- Question: How does NA2's performance scale with increasing graph size and complexity, particularly for datasets larger than ogbn-arxiv?
- Basis in paper: [inferred] The paper mentions that NA2 maintains linear time complexity for large datasets but does not provide extensive experimental results on graphs significantly larger than ogbn-arxiv.
- Why unresolved: The paper's experiments are limited to five datasets, with ogbn-arxiv being the largest. There is no information on how NA2 performs on graphs of industrial scale or with more complex structures.
- What evidence would resolve it: Experiments on graphs with millions of nodes and edges, or real-world large-scale datasets, would provide evidence to resolve this question.

## Limitations

- Effectiveness depends on server model sensitivity to individual client contributions, which may not hold for all VFGL implementations
- Neuron testing methodology lacks detailed algorithmic specification, making exact replication challenging
- Performance variability suggests transferability across different GNN architectures may not be universally reliable

## Confidence

- **High Confidence**: The core claim that data manipulation can increase a malicious client's contribution in VFGL systems is well-supported by experimental evidence (Cora: 205.36% improvement in ASR)
- **Medium Confidence**: The neuron-based feature selection mechanism shows promise but lacks detailed algorithmic specification and validation across diverse GNN architectures
- **Medium Confidence**: The MSE-based shadow model approach demonstrates effectiveness but the paper doesn't fully explore alternative loss functions or provide comprehensive ablation studies

## Next Checks

1. Test NA2's effectiveness against VFGL systems with robust aggregation mechanisms that reduce individual client contribution sensitivity, particularly focusing on models that implement clipping or differential privacy
2. Conduct extensive ablation studies comparing MSE vs. cross-entropy loss for shadow model training across different GNN architectures and dataset types
3. Evaluate NA2's performance under adaptive defenses that actively monitor and limit client contribution scores or detect abnormal feature manipulation patterns