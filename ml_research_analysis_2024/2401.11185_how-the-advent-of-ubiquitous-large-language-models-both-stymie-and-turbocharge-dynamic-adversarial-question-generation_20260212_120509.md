---
ver: rpa2
title: How the Advent of Ubiquitous Large Language Models both Stymie and Turbocharge
  Dynamic Adversarial Question Generation
arxiv_id: '2401.11185'
source_url: https://arxiv.org/abs/2401.11185
tags:
- questions
- question
- adversarial
- answer
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores how the advent of large language models (LLMs)
  impacts dynamic adversarial question generation, where humans write questions to
  stump a model. The authors develop an interface with LLM-powered QA systems and
  retrieval models to guide authors in creating adversarial questions.
---

# How the Advent of Ubiquitous Large Language Models both Stymie and Turbocharge Dynamic Adversarial Question Generation

## Quick Facts
- arXiv ID: 2401.11185
- Source URL: https://arxiv.org/abs/2401.11185
- Authors: Yoo Yeon Sung; Ishani Mondal; Jordan Boyd-Graber
- Reference count: 40
- One-line primary result: Interface with LLM and retrieval models plus IRT metric creates challenging adversarial questions, but authors sometimes produce poor quality questions using tricks.

## Executive Summary
This paper explores how large language models (LLMs) impact dynamic adversarial question generation, where humans write questions to stump machines. The authors develop an interface with LLM-powered QA systems and retrieval models to guide authors in creating adversarial questions. They propose a new metric based on Item Response Theory (IRT) to evaluate question quality, incentivizing questions that are difficult for machines but easy for humans. The results show the approach successfully creates challenging questions, with significant numbers stumping machines but not humans, though authors sometimes resort to tricks resulting in ambiguous or confusing questions.

## Method Summary
The method involves authors writing questions using an interface that provides real-time feedback from LLM predictions (T5, DistilBERT) and retrieval model evidence (TF-IDF, DPR). Questions are evaluated using an IRT-based metric that models the interaction between question difficulty and answerer skill. The interface includes a diversity widget tracking topic categories and leaderboards for both writers and machines. Human vs. computer competitions assess question quality, with authors incentivized to create questions that stump machines while remaining answerable by humans.

## Key Results
- Proposed interface and IRT metric successfully create challenging questions that stump machines but are answerable by humans
- Retrieval model evidence is 10% more helpful in writing questions that stump ChatGPT
- Authors sometimes resort to tricks resulting in poor quality questions that are ambiguous or confusing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IRT-based evaluation captures the relative difficulty and discriminability of adversarial questions by modeling the interaction between question difficulty and answerer skill.
- Mechanism: IRT defines a probabilistic model where the probability of a correct answer depends on the difference between question difficulty (θ) and answerer skill (β), scaled by discriminability (γ). Higher discriminability means the question better distinguishes between high- and low-skilled answerers.
- Core assumption: The population of answerers (both human and machine) can be modeled with latent skill parameters that explain their performance on questions.
- Evidence anchors:
  - [abstract]: "we adopt Item Response Theory (IRT) that mathematically models answerers’ response data."
  - [section 3.1]: Equation 1 defines the IRT probability model for correct responses.
  - [corpus]: Weak; no direct corpus evidence that this IRT approach was validated against existing adversarial QA datasets.
- Break condition: If answerer populations have bimodal or highly non-normal skill distributions, IRT parameters may not reliably separate good from poor questions.

### Mechanism 2
- Claim: Combining retrieval model evidence with LLM predictions allows authors to iteratively refine questions that stump both systems, exploiting complementary weaknesses.
- Mechanism: Retrieval models provide explainable evidence for predictions; authors adjust questions to avoid retrieval hits or create misleading context. LLM predictions (e.g., T5, DistilBERT) expose semantic understanding gaps. Stumping both indicates adversarial quality.
- Core assumption: Retrieval-based systems and LLM-based systems have different failure modes, so stumping both implies broader adversarial coverage.
- Evidence anchors:
  - [abstract]: "we enrich the writing guidance with LLMs and retrieval models for the authors to reason why their questions are not adversarial."
  - [section 4.1]: Details how retrieval evidence is shown and used to guide question revision.
  - [section 5.4]: Retrieval model evidence is 10% more helpful in writing questions that stump CHATGPT.
- Break condition: If retrieval and LLM systems converge in architecture or training data, the complementary adversarial coverage may diminish.

### Mechanism 3
- Claim: Incentivizing diversity in question topics and formats encourages creation of questions that probe different knowledge domains and reasoning skills, improving dataset coverage.
- Mechanism: Authors are required to write questions across predefined categories (Art, Literature, Geography, etc.) and monitor diversity via a leaderboard. This encourages broader topical coverage and prevents overfitting to narrow domains.
- Core assumption: Different QA models have domain-specific strengths and weaknesses; diverse question sets better reveal model robustness.
- Evidence anchors:
  - [abstract]: "we encourage topic diversity in the questions (Wang et al., 2020)."
  - [section 4.2]: Describes category requirements and diversity widget.
  - [corpus]: Weak; no corpus evidence that enforced diversity directly correlates with adversarial question quality.
- Break condition: If diversity constraints lead authors to write superficial or forced questions that lack adversarial depth.

## Foundational Learning

- Concept: Item Response Theory (IRT) models latent traits (difficulty, discriminability, skill) to explain observed responses.
  - Why needed here: IRT provides a principled, quantitative way to evaluate adversarial questions by modeling how well they distinguish between skilled and unskilled answerers.
  - Quick check question: What IRT parameter represents how effectively a question rewards answerer skill?
- Concept: Adversarial example generation in NLP aims to create inputs that expose model weaknesses while remaining natural for humans.
  - Why needed here: The paper's goal is to create questions that stump machines but are answerable by humans, requiring understanding of adversarial techniques and human norms.
  - Quick check question: Why is it insufficient to define an adversarial question as one that a human can answer but a computer cannot?
- Concept: Retrieval-augmented QA combines evidence retrieval with answer generation, enabling interpretability.
  - Why needed here: Retrieval evidence is used to guide authors in understanding why questions fail to stump models, enabling targeted question refinement.
  - Quick check question: How does retrieval evidence help authors improve adversarial question quality?

## Architecture Onboarding

- Component map: Interface -> LLM predictions (T5, DistilBERT) and retrieval model evidence (TF-IDF, DPR) -> Author question writing -> IRT-based evaluation -> Human vs. machine competition
- Critical path: Author writes question → Interface updates model predictions and retrieval evidence → Author revises based on guidance → Question submitted → Evaluation via IRT and human vs. machine competition
- Design tradeoffs:
  - Retrieval vs. LLM predictions: Retrieval provides interpretability but may be outdated; LLMs are more powerful but opaque
  - Diversity enforcement vs. adversarial depth: Enforcing categories may lead to superficial questions; flexibility may cause overfitting to narrow domains
  - Real-time feedback vs. latency: More complex models (e.g., ChatGPT) provide better guidance but slow down the writing process
- Failure signatures:
  - Questions consistently stump neither humans nor machines → Too vague or poorly constructed
  - Questions stump only retrieval models but not LLMs → Retrieval evidence not aligned with LLM reasoning
  - High discriminability but low margin between human and machine difficulty → Questions favor one population over the other
- First 3 experiments:
  1. Ablate retrieval evidence: Compare adversarial question quality with and without retrieval model guidance
  2. Vary diversity constraints: Test whether stricter category requirements improve or harm adversarial depth
  3. Compare IRT vs. accuracy-only metrics: Evaluate whether IRT-based scoring better incentivizes high-quality adversarial questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the interface be improved to better guide authors in creating questions that are both adversarial and of high quality?
- Basis in paper: [explicit] The paper discusses the development of an interface with LLM-powered QA systems and retrieval models to guide authors in creating adversarial questions. However, it also mentions that authors sometimes resort to tricks that result in poor questions that are ambiguous, subjective, or confusing.
- Why unresolved: The paper acknowledges the challenge of creating high-quality adversarial questions but does not provide specific solutions or improvements for the interface to address this issue.
- What evidence would resolve it: User studies or A/B testing comparing different interface designs and their impact on the quality of generated questions would provide insights into effective interface improvements.

### Open Question 2
- Question: How can the proposed IRT-based metric be further refined to better incentivize the creation of good adversarial questions?
- Basis in paper: [explicit] The paper proposes a new metric based on Item Response Theory (IRT) to evaluate the quality of adversarial questions. However, it does not explore potential refinements or extensions to this metric.
- Why unresolved: While the IRT-based metric is introduced, the paper does not delve into potential improvements or alternative approaches to better incentivize the creation of high-quality adversarial questions.
- What evidence would resolve it: Experimental studies comparing the performance of the proposed metric with alternative evaluation metrics or human judgments on the quality of adversarial questions would provide insights into potential refinements.

### Open Question 3
- Question: How can the diversity widget be enhanced to better capture and encourage a wider range of question topics and perspectives?
- Basis in paper: [explicit] The paper mentions the inclusion of a diversity widget in the interface to encourage topic diversity in the questions. However, it also notes that the questions' demographic diversity distribution did not conform to the population distribution.
- Why unresolved: The paper acknowledges the importance of diversity but does not provide specific strategies or improvements for the diversity widget to better capture and encourage a wider range of question topics and perspectives.
- What evidence would resolve it: User studies or analysis of the questions generated using different diversity widget designs would provide insights into effective strategies for promoting topic diversity.

## Limitations
- Authors sometimes resort to tricks resulting in poor quality questions that are ambiguous, subjective, or confusing even to humans
- Evaluation relies on specific populations of answerers; generalizability across different populations or more advanced LLMs is not fully established
- Enforced diversity in question topics might lead to superficial questions that lack adversarial depth

## Confidence
- High Confidence: The core mechanism of using IRT to model question difficulty and discriminability, and the general finding that the proposed interface and metric successfully create challenging questions
- Medium Confidence: The claim that combining retrieval model evidence with LLM predictions allows authors to iteratively refine questions
- Low Confidence: The assertion that enforced diversity in question topics and formats directly correlates with improved adversarial question quality

## Next Checks
1. Conduct an ablation study to compare adversarial question quality with and without retrieval model guidance
2. Test generated questions against a range of more advanced LLMs (e.g., ChatGPT, GPT-4) to assess generalization
3. Analyze the stability and robustness of IRT parameters across different populations of answerers or different subsets of the data