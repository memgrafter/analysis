---
ver: rpa2
title: 'CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition'
arxiv_id: '2406.11340'
source_url: https://arxiv.org/abs/2406.11340
tags:
- modalities
- features
- encoder
- modality
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of driver action recognition
  in vehicle cabin environments, where collecting extensive data for non-RGB modalities
  like infrared and depth is laborious and costly. To tackle this, the authors propose
  a Continual Cross-Modal Mapping Network (CM2-Net) that leverages instructive prompts
  from previously-learned modalities to continually learn new modalities.
---

# CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition

## Quick Facts
- arXiv ID: 2406.11340
- Source URL: https://arxiv.org/abs/2406.11340
- Reference count: 27
- Primary result: Achieves SOTA performance in multi-modal driver action recognition using Accumulative Cross-modal Mapping Prompting

## Executive Summary
This paper addresses the challenge of driver action recognition in vehicle cabins where non-RGB modalities like infrared and depth are expensive to collect. The authors propose CM2-Net, a continual learning framework that leverages discriminative features from previously-learned modalities to improve the training of new modality encoders. The core innovation is Accumulative Cross-modal Mapping Prompting (ACMP), which maps features from existing modalities into new modality feature spaces to provide effective prompts during training. Experiments on the Drive&Act dataset demonstrate significant improvements over state-of-the-art methods, with 7.34% Top-1 accuracy gain over TransDARC in RGB modality and substantial improvements in IR and Depth modalities.

## Method Summary
CM2-Net employs a continual learning approach where RGB, IR, and Depth encoders are trained sequentially. The method uses a frozen CLIP text encoder to provide stable supervisory signals through textual embeddings. During training, discriminative features from previously learned modalities are mapped into the feature space of newly incoming modalities using projection heads. These mapped features serve as prompts for the new encoder, guiding it to extract similar discriminative information. The ACMP mechanism accumulates prompts from all previously learned modalities, enhancing feature extraction capabilities for non-RGB sensors. The model uses UniFormerV2-B as video encoders and contrastive learning with classification, text, and prompt losses.

## Key Results
- CM2-Net achieves 7.34% higher Top-1 accuracy than TransDARC in RGB modality
- CM2-Net exceeds TSM by 7.04% in Mean-1 accuracy for RGB modality
- Significant improvements demonstrated in IR and Depth modalities, validating ACMP's effectiveness for non-RGB feature extraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discriminative features from previously learned modalities can be mapped into new modality feature spaces to improve encoder training.
- Mechanism: The method maps discriminative features from existing modalities (like RGB) into the feature space of a newly incoming modality (like Depth or IR). These mapped features act as prompts, guiding the new encoder to extract and prioritize similar discriminative information, thus bridging the domain gap.
- Core assumption: There exists shared discriminative information across modalities for the same action, which can be transferred via mapping functions.
- Evidence anchors:
  - [abstract] "map the discriminative and informative features learned from previous modalities into the feature space of newly-incoming modalities"
  - [section] "we find that there is underlying information shared across these modalities"
  - [corpus] No direct evidence found in neighbors; this appears to be a novel mechanism specific to CM2-Net.
- Break condition: If the projection heads used for mapping are not bijective, the mapping may not fully capture the shared discriminative information, reducing the effectiveness of the prompting.

### Mechanism 2
- Claim: The Accumulative Cross-modal Mapping Prompting (ACMP) method improves feature extraction by leveraging knowledge from multiple previously learned modalities.
- Mechanism: ACMP accumulates mapped features from all previously learned modalities and uses them to prompt the training of each new modality encoder. This accumulation of knowledge enhances the encoder's ability to extract discriminative features specific to the new modality.
- Core assumption: The prompts from multiple modalities accumulate and enhance the learning of new modalities more effectively than a single modality prompt.
- Evidence anchors:
  - [abstract] "These prompts are accumulating throughout the continual learning process, thereby boosting further recognition performances."
  - [section] "The information in prompts is accumulating with the continual learning of modalities."
  - [corpus] No direct evidence found in neighbors; this appears to be a novel mechanism specific to CM2-Net.
- Break condition: If the accumulation of prompts leads to overfitting or if the prompts from different modalities conflict, the performance may degrade.

### Mechanism 3
- Claim: Using semantic textual embeddings as supervisory signals improves the alignment of multimodal features in a unified feature space.
- Mechanism: The method uses a frozen text encoder to encode label textual knowledge into embeddings, which serve as stable and consistent supervisory signals. These textual features guide the learning of multimodal features within a unified feature space during continual training.
- Core assumption: Textual embeddings provide a stable reference point for aligning features from different modalities.
- Evidence anchors:
  - [abstract] "we employ a frozen language encoder (e.g., CLIP [5]) to encode label textual knowledge into embeddings for stable and consistent supervisory signals."
  - [section] "we employ a frozen language encoder (e.g., CLIP [5]) to encode label textual knowledge into embeddings for stable and consistent supervisory signals."
  - [corpus] No direct evidence found in neighbors; this appears to be a novel mechanism specific to CM2-Net.
- Break condition: If the textual embeddings do not accurately represent the actions or if the text encoder is not well-aligned with the visual modalities, the supervisory signals may be ineffective.

## Foundational Learning

- Concept: Cross-modal learning
  - Why needed here: To leverage knowledge from one modality (e.g., RGB) to improve feature extraction in other modalities (e.g., IR, Depth) by mapping discriminative features.
  - Quick check question: How does cross-modal learning help in reducing the domain gap between RGB and non-RGB modalities?

- Concept: Continual learning
  - Why needed here: To enable the model to learn from new modalities over time without forgetting previously learned knowledge, using accumulated prompts.
  - Quick check question: What is the role of Accumulative Cross-modal Mapping Prompting (ACMP) in the continual learning process?

- Concept: Contrastive learning
  - Why needed here: To train the encoders by bringing embeddings from the same class closer and distancing those from different classes, using similarity scores with textual embeddings.
  - Quick check question: How does contrastive learning with textual embeddings improve the alignment of multimodal features?

## Architecture Onboarding

- Component map:
  - RGB Encoder -> IR Encoder -> Depth Encoder (sequential training with accumulated prompts)
  - Text Encoder (frozen CLIP) provides supervisory signals
  - Projection Heads (F0, Ft) embed features into unified semantic space
  - Mapping Functions (M0,i, Mi,j) transfer discriminative features between modalities

- Critical path:
  1. Fine-tune RGB encoder with textual embeddings
  2. Use RGB encoder to extract discriminative features
  3. Map RGB features into the feature space of a new modality (e.g., IR)
  4. Train the new modality encoder using the mapped features as prompts
  5. Accumulate prompts from all previously learned modalities
  6. Use accumulated prompts to train subsequent modality encoders

- Design tradeoffs:
  - Using a frozen text encoder provides stable supervisory signals but may limit the model's ability to adapt to new textual descriptions
  - Accumulating prompts from multiple modalities can improve feature extraction but may also introduce noise or conflicting information
  - The choice of projection heads affects the effectiveness of the mapping functions and the quality of the prompts

- Failure signatures:
  - Poor performance in non-RGB modalities despite using prompts may indicate that the mapping functions are not effectively capturing the shared discriminative information
  - Overfitting to specific modalities may suggest that the accumulated prompts are too specific and not generalizable
  - Inconsistent results across different modalities may indicate that the textual embeddings are not well-aligned with the visual modalities

- First 3 experiments:
  1. Fine-tune the RGB encoder on the Drive&Act dataset and evaluate its performance in classifying driver actions using textual embeddings as supervisory signals
  2. Train the IR encoder using prompts from the fine-tuned RGB encoder and evaluate its performance in classifying driver actions
  3. Train the Depth encoder using accumulated prompts from both the RGB and IR encoders and evaluate its performance in classifying driver actions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CM2-Net change when applied to different sensor modalities not explored in the paper, such as Near-InfraRed (NIR) or 3D skeleton data?
- Basis in paper: [explicit] The paper focuses on RGB, IR, and Depth modalities, while the Drive&Act dataset also includes NIR and 3D skeleton data.
- Why unresolved: The paper does not evaluate the performance of CM2-Net on NIR or 3D skeleton data, leaving open the question of its effectiveness with these modalities.
- What evidence would resolve it: Experiments comparing CM2-Net's performance on NIR and 3D skeleton data with other state-of-the-art methods would provide evidence of its effectiveness across different sensor modalities.

### Open Question 2
- Question: What is the impact of the number of modalities on the effectiveness of ACMP in CM2-Net?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of ACMP with three modalities (RGB, IR, Depth), but does not explore the impact of increasing the number of modalities.
- Why unresolved: The paper does not investigate how the performance of ACMP scales with an increasing number of modalities, leaving open the question of its robustness and efficiency in more complex multi-modal scenarios.
- What evidence would resolve it: Experiments testing CM2-Net with additional modalities and comparing its performance to the three-modality setup would provide evidence of ACMP's scalability and effectiveness.

### Open Question 3
- Question: How does the choice of pre-trained backbone networks affect the performance of CM2-Net in different modalities?
- Basis in paper: [explicit] The paper uses UniFormerV2 as the backbone network but does not explore the impact of using different pre-trained backbones.
- Why unresolved: The paper does not compare the performance of CM2-Net with different backbone networks, leaving open the question of how the choice of backbone affects the model's effectiveness in various modalities.
- What evidence would resolve it: Experiments using different pre-trained backbone networks (e.g., Swin Transformer, ViT) and comparing their performance in CM2-Net would provide evidence of the impact of backbone choice on the model's effectiveness.

## Limitations
- The effectiveness of ACMP depends critically on the assumption that discriminative features can be meaningfully mapped between modalities, which requires stronger empirical validation
- The accumulative nature of prompts may lead to overfitting or conflicting information when multiple modalities are involved
- Experimental validation is limited to a single dataset (Drive&Act), raising questions about generalizability to different data distributions

## Confidence
- High Confidence: The experimental results showing CM2-Net's superior performance over baseline methods (TSM, TransDARC) on the Drive&Act dataset, with specific accuracy improvements quantified (7.34% and 7.04% gains)
- Medium Confidence: The general effectiveness of cross-modal mapping for bridging domain gaps between RGB and non-RGB modalities, supported by the shared discriminative information assumption but lacking rigorous validation of the mapping functions' bijectivity
- Medium Confidence: The accumulative prompting mechanism's contribution to continual learning, as evidenced by performance improvements but without comprehensive ablation studies isolating individual component effects

## Next Checks
1. **Mapping Function Validation**: Conduct experiments to verify whether the projection heads F0 and Ft used in ACMP are effectively bijective, ensuring that discriminative information is preserved during feature space mapping between modalities
2. **Prompt Accumulation Analysis**: Perform ablation studies systematically removing prompts from individual modalities to quantify the marginal contribution of each modality's prompts and identify potential overfitting or conflicting information
3. **Cross-Dataset Generalization**: Evaluate CM2-Net on additional driver action recognition datasets beyond Drive&Act to assess the method's robustness and generalizability to different data distributions and environmental conditions