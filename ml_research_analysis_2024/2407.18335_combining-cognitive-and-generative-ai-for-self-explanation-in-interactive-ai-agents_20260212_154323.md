---
ver: rpa2
title: Combining Cognitive and Generative AI for Self-explanation in Interactive AI
  Agents
arxiv_id: '2407.18335'
source_url: https://arxiv.org/abs/2407.18335
tags:
- vera
- question
- questions
- user
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores self-explanation in the Virtual Experimental
  Research Assistant (VERA) by combining cognitive AI and generative AI. VERA is an
  inquiry-based learning environment that enables learners to build and simulate ecological
  models.
---

# Combining Cognitive and Generative AI for Self-explanation in Interactive AI Agents

## Quick Facts
- **arXiv ID**: 2407.18335
- **Source URL**: https://arxiv.org/abs/2407.18335
- **Reference count**: 38
- **Key outcome**: Combining cognitive AI (TMK models) and generative AI (ChatGPT, LangChain) enables VERA to provide self-explanations for its ecological modeling processes, with preliminary evaluation showing high accuracy across most question categories.

## Executive Summary
This study explores self-explanation capabilities in the Virtual Experimental Research Assistant (VERA) by integrating cognitive AI and generative AI approaches. VERA is an inquiry-based learning environment for ecological modeling where learners can build and simulate models. The researchers endowed VERA with a Task-Method-Knowledge (TMK) model representing its design, knowledge, and reasoning processes, then integrated ChatGPT, LangChain, and Chain-of-Thought techniques to generate explanations. A preliminary evaluation using 66 questions adapted from prior work showed high recall, precision, and accuracy across most categories, indicating the system's effectiveness in retrieving relevant information and providing accurate explanations.

## Method Summary
The researchers manually constructed a TMK model capturing VERA's design, knowledge, and reasoning processes. They then implemented the Ask-TMK system using LangChain to integrate ChatGPT and FAISS for document retrieval and explanation generation. For method-related questions, Chain-of-Thought prompting was employed to break down complex methods into smaller tasks. The system was evaluated using 66 questions adapted from prior work, calculating Recall, Precision, and Accuracy scores to assess performance across different question categories.

## Key Results
- High recall, precision, and accuracy scores across most question categories in preliminary evaluation
- Effective integration of cognitive AI's structured knowledge (TMK) with generative AI's capabilities for explanation generation
- Successful demonstration of VERA's ability to introspect on its own design and explain its functioning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: VERA can introspect on its own design and explain its functioning by representing its design as a TMK model.
- **Mechanism**: The TMK model captures VERA's design, knowledge, and reasoning processes in a unified structured representation. This includes tasks (goals and inputs/outputs), methods (how tasks are accomplished via state machines), and knowledge (concepts and logical expressions). By processing through this TMK model, VERA can construct a derivational knowledge trace for a given instance and then generate an explanation by reflecting on the trace.
- **Core assumption**: A structured, hierarchical representation of an AI agent's design (as TMK provides) is sufficient for the agent to introspect and explain its own functioning.
- **Evidence anchors**:
  - [abstract]: "From a cognitive AI viewpoint, we endow VERA with a functional model of its own design, knowledge, and reasoning represented in the Task–Method–Knowledge (TMK) language."
  - [section]: "We posit that if an interactive agent has theory of its own mind, then it can use the self-theory to explain its reasoning and how the reasoning led to specific decisions. We use Task-Method-Knowledge (TMK) models to capture elements of an interactive agent's theory of its mind."
- **Break condition**: If the TMK model is incomplete or inaccurate, or if the reasoning required to explain functionality is too complex for the TMK representation to capture.

### Mechanism 2
- **Claim**: VERA can reflect on its design and explain its results for a given input instance by processing through the TMK model.
- **Mechanism**: When a user question is received, the system classifies the question to determine its relevance to VERA's internal model (TMK) and allocates resources efficiently for response generation. For method-related questions, Chain-of-Thought prompting is used during later stages to fetch relevant tasks and corresponding methods, focusing on presenting intermediate steps within TMK. This enables VERA to break down complex methods within the TMK into smaller tasks and subtasks, providing a more detailed explanation.
- **Core assumption**: Chain-of-Thought prompting can effectively guide the LLM to break down complex methods within the TMK model into smaller, explainable components.
- **Evidence anchors**:
  - [abstract]: "From the perspective of generative AI, we use ChatGPT, LangChain, and Chain-of-Thought to answer user questions based on the VERA TMK model."
  - [section]: "Ask-TMK leverages Chain-of-Thought to generate explanations with reasoning, for 'methods' specific questions. Chain-of-Thought is a reasoning technique that enables the LLM to explicitly reveal the steps it undergoes when arriving at an answer."
- **Break condition**: If Chain-of-Thought prompting fails to effectively break down complex methods, or if the TMK model does not contain sufficient detail about the methods.

### Mechanism 3
- **Claim**: VERA can provide factually accurate, complete, and precise explanations by combining cognitive AI's structured knowledge for information retrieval with generative AI's capabilities to deliver relevant and accurate explanations.
- **Mechanism**: The system maps user queries to the relevant Task, Method, and Knowledge components within the TMK model, thereby generating responses that explain how VERA works. This integration ensures that explanations are both accurate and contextually relevant, enhancing the user's understanding of complex queries. The preliminary evaluation showed high recall, precision, and accuracy across most question categories.
- **Core assumption**: The combination of structured knowledge representation (TMK) and generative AI (ChatGPT, LangChain) is sufficient to generate high-quality explanations.
- **Evidence anchors**:
  - [abstract]: "Thus, we combine cognitive and generative AI to generate explanations about how VERA works and produces its answers. The preliminary evaluation of the generation of explanations in VERA on a bank of 66 questions derived from earlier work appears promising."
  - [section]: "Our preliminary analysis shows that the self-explanation system effectively leverages cognitive AI's structured knowledge for information retrieval and generative AI's capabilities to deliver relevant and accurate explanations."
- **Break condition**: If the generative AI component generates inaccurate or irrelevant information, or if the structured knowledge in TMK is insufficient for the given query.

## Foundational Learning

- **Concept**: Task-Method-Knowledge (TMK) Models
  - **Why needed here**: TMK models are the core representation used to capture VERA's design, knowledge, and reasoning processes. Understanding TMK is essential for understanding how VERA can introspect and explain itself.
  - **Quick check question**: What are the three core aspects of any TMK model, and what does each represent?

- **Concept**: Chain-of-Thought Prompting
  - **Why needed here**: Chain-of-Thought is a reasoning technique used to guide the LLM to explicitly reveal the steps it undergoes when arriving at an answer, particularly for method-related questions. This is crucial for generating detailed explanations.
  - **Quick check question**: How does Chain-of-Thought prompting differ from standard prompting, and why is it useful for method-related questions?

- **Concept**: Large Language Models (LLMs) and Prompt Engineering
  - **Why needed here**: LLMs like ChatGPT are used to generate natural language explanations. Understanding how to engineer prompts to guide the LLM's output is essential for generating effective explanations.
  - **Quick check question**: What are some key considerations when engineering prompts for an LLM to generate explanations based on a structured knowledge base like TMK?

## Architecture Onboarding

- **Component map**: User Question -> Question Classifier -> FAISS Search -> LangChain (Prompt Construction) -> ChatGPT (Explanation Generation) -> Refinement -> Final Answer
- **Critical path**: User question → Question Classification → FAISS Search → Prompt Construction (LangChain) → Explanation Generation (ChatGPT) → Refinement → Final Answer
- **Design tradeoffs**: The system prioritizes accuracy and relevance by using a structured knowledge base (TMK) and a powerful LLM (ChatGPT). However, this comes at the cost of requiring manual effort to construct the TMK model and potentially longer response times due to the multi-stage processing.
- **Failure signatures**: 
  - Inaccurate or irrelevant explanations.
  - Long response times.
  - Failure to classify questions correctly.
  - Inability to retrieve relevant TMK components.
- **First 3 experiments**:
  1. **Question Classification Test**: Feed a diverse set of user questions to the question classifier and verify that it correctly classifies them and retrieves the appropriate TMK components.
  2. **FAISS Search Test**: Test the FAISS search system with various queries to ensure it retrieves the most relevant TMK components with high similarity scores.
  3. **Explanation Generation Test**: Use a set of known queries with known answers to test the explanation generation pipeline, verifying that the generated explanations are accurate, relevant, and concise.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the self-explanation system be adapted to different educational contexts and user groups to ensure equitable access and understanding?
- **Basis in paper**: [inferred]
- **Why unresolved**: The paper acknowledges the potential for unintentional biases in the system's design and the need for real-world deployment in diverse classroom settings. However, it does not provide concrete strategies for addressing these issues or adapting the system to different educational contexts.
- **What evidence would resolve it**: Conducting user studies with diverse learner populations in various educational settings to evaluate the system's effectiveness and identify potential biases or areas for improvement.

### Open Question 2
- **Question**: Can the self-explanation system be extended to provide explanations for more complex ecological models and simulations, such as those involving multiple interacting species and environmental factors?
- **Basis in paper**: [explicit]
- **Why unresolved**: The paper focuses on a preliminary evaluation of the system using a limited set of questions and a relatively simple ecological model. It does not explore the system's capabilities in handling more complex scenarios.
- **What evidence would resolve it**: Testing the system with more complex ecological models and simulations, evaluating its ability to generate accurate and informative explanations for these scenarios.

### Open Question 3
- **Question**: How can the self-explanation system be integrated with other AI-powered educational tools and platforms to create a more comprehensive and personalized learning experience?
- **Basis in paper**: [inferred]
- **Why unresolved**: The paper presents the self-explanation system as a standalone component within the VERA learning environment. It does not explore potential integrations with other AI-powered educational tools or platforms.
- **What evidence would resolve it**: Developing and testing integrations with other AI-powered educational tools and platforms, evaluating the impact on learner engagement, understanding, and learning outcomes.

## Limitations
- Manual construction of the TMK model raises questions about scalability and completeness
- Preliminary evaluation based on 66 questions may not be representative of real-world usage patterns
- Lack of comparative analysis against baseline approaches or ablation studies

## Confidence

- **High Confidence**: The general mechanism of combining structured knowledge (TMK) with generative AI for explanation generation is well-supported by the methodology and preliminary results.
- **Medium Confidence**: The specific implementation details, such as prompt engineering and TMK construction process, are less certain due to limited specification in the paper.
- **Low Confidence**: The generalizability of the approach to other domains or more complex systems beyond VERA remains unclear.

## Next Checks
1. **Scalability Test**: Evaluate the system's performance with a larger and more diverse set of questions (e.g., 500+ questions) to assess robustness and identify potential failure modes.

2. **Ablation Study**: Conduct experiments removing specific components (e.g., Chain-of-Thought, FAISS search) to quantify their individual contributions to the overall performance.

3. **User Study**: Perform a user study with learners using VERA to assess the practical utility and effectiveness of the self-explanation system in supporting learning outcomes.