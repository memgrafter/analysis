---
ver: rpa2
title: Toward Conversational Agents with Context and Time Sensitive Long-term Memory
arxiv_id: '2406.00057'
source_url: https://arxiv.org/abs/2406.00057
tags:
- query
- questions
- semantic
- retrieval
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of building conversational
  agents with long-term memory that can handle two specific types of queries: those
  based on conversational meta-data (like time or speaker) and ambiguous queries that
  require context to interpret. The authors generate a new dataset of such questions
  based on a long-form conversation dataset, then develop a novel retrieval model
  that combines chain-of-table search methods for meta-data queries with standard
  vector database retrieval for semantic content.'
---

# Toward Conversational Agents with Context and Time Sensitive Long-term Memory

## Quick Facts
- **arXiv ID**: 2406.00057
- **Source URL**: https://arxiv.org/abs/2406.00057
- **Reference count**: 40
- **Primary result**: Novel retrieval model combining chain-of-table and semantic methods significantly outperforms standard semantic retrieval on time-based and ambiguous conversational queries

## Executive Summary
This paper addresses the challenge of building conversational agents with long-term memory that can handle two specific types of queries: those based on conversational meta-data (like time or speaker) and ambiguous queries that require context to interpret. The authors generate a new dataset of such questions based on a long-form conversation dataset, then develop a novel retrieval model that combines chain-of-table search methods for meta-data queries with standard vector database retrieval for semantic content. The model also includes a classifier to determine which type of retrieval is needed and a query rewriting component for ambiguous questions. The proposed approach significantly outperforms standard semantic retrieval methods on both unambiguous time-based queries (achieving up to 90% recall and 78% F2 score) and time+content queries (up to 90% recall and 32 F2 score). For ambiguous queries, the query rewriting component improves performance substantially compared to using the original query alone. The work provides both a challenging benchmark for conversational memory systems and a strong baseline approach that combines tabular and semantic retrieval methods.

## Method Summary
The authors develop a retrieval model that addresses two key challenges in conversational memory: meta-data based queries and ambiguous queries requiring context. The model combines chain-of-table search methods for retrieving based on conversational meta-data (time, speaker, session) with standard vector database retrieval for semantic content. A meta-semantic classifier determines whether queries need meta-data retrieval, semantic retrieval, or both, while a query rewriting component disambiguates ambiguous queries with pronouns or demonstratives. The system is evaluated on a new dataset built from the LoCoMo long-form conversation dataset, containing 12 long dialogues with 19 sessions each on average, totaling 2,134 unambiguous questions and 1,944 ambiguous questions across various test categories.

## Key Results
- The proposed approach achieves up to 90% recall and 78% F2 score on unambiguous time-based queries
- For time+content queries, the model reaches up to 90% recall and 32 F2 score
- Query rewriting component significantly improves performance on ambiguous queries compared to using original queries
- The chain-of-table + semantic retrieval combination outperforms standard semantic retrieval baselines across all query types

## Why This Works (Mechanism)

### Mechanism 1: Chain-of-Table + Semantic Retrieval Combination
- Claim: Combining tabular meta-data retrieval (chain-of-table) with semantic retrieval creates a complementary system that handles both structured and unstructured information needs.
- Mechanism: The model first classifies whether a query requires meta-data retrieval, semantic retrieval, or both. For meta-data queries, it uses chain-of-table to filter the dataset to relevant rows, then applies semantic search only to this reduced set. This avoids the inefficiency of applying semantic search to the entire dataset.
- Core assumption: Tabular meta-data retrieval is computationally cheaper and more reliable than semantic search, making it an effective pre-filter.
- Evidence anchors:
  - [abstract] "combines chained-of-table search methods, standard vector-database retrieval"
  - [section 4] "we first do a meta-data based retrieval using the chain-of-table method... Then we do a semantic search to retrieve the top-k related responses from the remaining table"
  - [corpus] Weak - related papers discuss memory systems but don't specifically address the combination of tabular and semantic retrieval methods
- Break condition: If the meta-data filter is too broad or the semantic model fails to capture content relevance, performance degrades significantly.

### Mechanism 2: Meta-Semantic Classification
- Claim: Using an LLM to classify whether a query requires meta-data retrieval, semantic retrieval, or both improves performance by preventing inappropriate retrieval method selection.
- Mechanism: Before retrieval, the model uses few-shot prompting to determine if the query needs meta-data, semantic content, or both. This guides the subsequent retrieval pipeline.
- Core assumption: LLMs can accurately classify query types through few-shot prompting, and this classification directly improves retrieval performance.
- Evidence anchors:
  - [section 4] "We use an LLM to classify whether a given query requires meta-data retrieval... and/or semantic retrieval"
  - [section 5] "we find that a common mistake... was that the model got confused about when to do content based retrieval and when not to"
  - [corpus] Weak - related papers mention classification but don't provide specific evidence for meta-semantic classification in conversational contexts
- Break condition: If the classifier is inaccurate, it may route queries to inappropriate retrieval methods, causing performance degradation.

### Mechanism 3: Query Rewriting for Ambiguous Queries
- Claim: Using an LLM to rewrite ambiguous queries into explicit forms that don't require context improves retrieval performance.
- Mechanism: For ambiguous queries with pronouns or demonstratives, the model uses few-shot prompting to rewrite the query into an unambiguous form based on the conversational context, then applies standard retrieval.
- Core assumption: LLMs can reliably disambiguate queries through rewriting without losing the original intent.
- Evidence anchors:
  - [abstract] "a prompting method to disambiguate queries"
  - [section 4] "we adapt a SoTA prompting based query rewriting method... which uses few-shot prompting techniques to show an LLM how to disambiguate a query given the query and preceding context"
  - [section 5] "we test the query rewrite method... This method performs similarly, though slightly worse, to the performance of the model that used the original unambiguous query"
  - [corpus] Moderate - related papers discuss query rewriting but don't specifically address ambiguous conversational queries
- Break condition: If the rewriting introduces errors or changes the query meaning, retrieval performance will suffer.

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: The model uses few-shot prompting to classify queries and write chain-of-table function calls, requiring the LLM to reason through multi-step processes
  - Quick check question: How does few-shot prompting help an LLM understand the chain-of-table function call pattern without explicit training?

- Concept: Vector database retrieval and semantic embeddings
  - Why needed here: The model uses semantic embeddings to retrieve content-relevant responses, requiring understanding of how embeddings capture semantic meaning
  - Quick check question: Why might semantic retrieval fail for time-based queries that don't reference specific content?

- Concept: Tabular data querying and function calling
  - Why needed here: The chain-of-table method uses function calls to query tabular meta-data, requiring understanding of how structured queries work
  - Quick check question: What advantage does the f_value function have over semantic search for retrieving responses by date or speaker?

## Architecture Onboarding

- Component map: Query input → Query classifier (meta-semantic) → Query rewrite (if ambiguous) → Meta-data retrieval (chain-of-table) → Semantic retrieval → Response output

- Critical path: Query → Classification → Retrieval (meta-data → semantic) → Response ranking → Output

- Design tradeoffs:
  - Accuracy vs. efficiency: Applying semantic search to the entire dataset is more thorough but computationally expensive; filtering first with meta-data retrieval is faster but may miss relevant responses
  - Model complexity: Adding query classification and rewriting increases model complexity but improves performance on ambiguous and meta-data queries
  - Prompt engineering vs. fine-tuning: Using few-shot prompting is more flexible but may be less reliable than fine-tuned models

- Failure signatures:
  - Poor performance on time-based queries: Likely indicates chain-of-table implementation issues or classification errors
  - Poor performance on content-based queries: Likely indicates semantic embedding or retrieval issues
  - Poor performance on ambiguous queries: Likely indicates query rewriting prompt issues or classification errors
  - Inconsistent results across similar queries: Likely indicates classification instability or prompt template issues

- First 3 experiments:
  1. Test the chain-of-table component alone on unambiguous time-based queries to verify meta-data retrieval works
  2. Test semantic retrieval alone on unambiguous content-based queries to verify semantic understanding
  3. Test the full pipeline on a small set of mixed queries to verify classification and routing works correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed chain-of-table approach scale to larger datasets with millions of conversational entries, and what would be the computational bottlenecks?
- Basis in paper: Inferred
- Why unresolved: The paper focuses on performance with a dataset of 12 long dialogues and demonstrates effectiveness with Mistral 7B and GPT-3.5. However, it doesn't test how the approach would handle truly large-scale conversational datasets (e.g., from social media platforms or enterprise chat logs) where the table could contain millions of rows. The authors note that "tabular data search is a well-known and optimized problem" but don't provide evidence about scaling limits or discuss potential bottlenecks like memory usage during chain-of-table operations or the cost of repeated LLM calls for function generation.
- What evidence would resolve it: Empirical tests showing performance degradation points, memory usage patterns, and latency measurements as dataset size increases from thousands to millions of rows, along with optimization strategies for large-scale deployment.

### Open Question 2
- Question: What is the impact of using different semantic embedding models on the combined chain-of-table + semantic retrieval system's performance, and would better embeddings change the relative effectiveness of the baseline approaches?
- Basis in paper: Explicit
- Why unresolved: The paper acknowledges they "did not test a variety of different semantic embedding models, and used a relatively small model (< 500 million parameters)" and notes it would be "surprising if the relative performance of the baselines changed significantly." However, this remains an untested hypothesis. The choice of embedding model could significantly affect semantic retrieval quality, especially for time+content queries where semantic similarity must be balanced with temporal and speaker metadata.
- What evidence would resolve it: Systematic experiments replacing the current embedding model with alternatives of varying sizes and architectures (e.g., larger models, models trained specifically for conversational data) and measuring changes in recall, F2 scores, and relative baseline performance across all query types.

### Open Question 3
- Question: How would the system perform on ambiguous queries that combine both temporal and content references (e.g., "Remember that idea we discussed about the project last Tuesday morning?"), and what additional challenges would this pose?
- Basis in paper: Inferred
- Why unresolved: The paper creates ambiguous time-based queries but explicitly states they "did not create ambiguous versions of the time+content based questions." The authors note this as a limitation, recognizing that combining ambiguity with the need to retrieve both temporal and content information would be a more complex challenge. The current query rewriting approach may not be sufficient for queries requiring simultaneous disambiguation and multi-hop retrieval across different dimensions of metadata and content.
- What evidence would resolve it: Development and testing of a dataset containing ambiguous time+content queries, evaluation of the current system's performance on these queries, and potentially new methods for handling queries requiring both disambiguation and simultaneous multi-dimensional retrieval.

## Limitations

- The query rewriting component shows only modest improvement over using original unambiguous queries, suggesting it may not be robust for all ambiguous query types.
- Performance metrics show high variance across different query categories, with time+content queries having particularly low F2 scores (32) despite high recall (90), indicating potential precision issues.
- The paper relies heavily on few-shot prompting for critical components like classification and query rewriting, but doesn't explore how sensitive these components are to prompt engineering or the number of examples used.

## Confidence

- **High Confidence**: The core architecture combining chain-of-table and semantic retrieval is well-defined and the performance improvements over baseline semantic retrieval are clearly demonstrated for unambiguous time-based queries.
- **Medium Confidence**: The meta-semantic classification approach and query rewriting for ambiguous queries show promising results but have more variability in performance and less detailed analysis of failure modes.
- **Low Confidence**: The long-term memory capabilities and conversational coherence over extended interactions are not thoroughly evaluated, making claims about "long-term memory" somewhat speculative.

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary the number of few-shot examples and prompt templates for the meta-semantic classifier and query rewriter to quantify how performance changes with different prompt configurations.

2. **Extended Interaction Testing**: Evaluate the system's ability to maintain coherent conversations over multiple sessions with time gaps, measuring not just retrieval accuracy but also conversational relevance and context maintenance.

3. **Precision-Recall Tradeoff Analysis**: For categories with high recall but low F2 scores (like time+content queries), conduct a detailed analysis of which retrieved responses are actually relevant to identify systematic precision issues in the semantic retrieval component.