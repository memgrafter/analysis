---
ver: rpa2
title: 'DeAL: Decoding-time Alignment for Large Language Models'
arxiv_id: '2402.06147'
source_url: https://arxiv.org/abs/2402.06147
tags:
- alignment
- human
- arxiv
- decoding
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DeAL, a decoding-time alignment framework
  that allows customization of reward functions during inference to improve adherence
  to alignment objectives. While current alignment methods like RLHF focus on model
  training, DeAL addresses their limitations by enabling fine-grained control over
  multiple alignment objectives without requiring model retraining.
---

# DeAL: Decoding-time Alignment for Large Language Models

## Quick Facts
- arXiv ID: 2402.06147
- Source URL: https://arxiv.org/abs/2402.06147
- Reference count: 31
- Primary result: DeAL achieves up to 37% improvement in harmlessness and 24% in helpfulness over prompting strategies

## Executive Summary
DeAL (Decoding-time Alignment) is a framework that enables customization of reward functions during inference to improve alignment of large language models with multiple objectives. Unlike traditional alignment methods like RLHF that focus on model training, DeAL operates at decoding time, allowing fine-grained control over alignment objectives without requiring model retraining. The framework generalizes decoding as heuristic-guided search and supports both programmatic constraints (e.g., keyword and length constraints) and abstract objectives (e.g., harmlessness and helpfulness).

The paper demonstrates DeAL's effectiveness across multiple tasks and datasets. For keyword-constrained generation on CommonGen, DeAL improves keyword coverage by 8-21% compared to prompting baselines. For length-constrained summarization on XSUM, it achieves better length satisfaction while maintaining quality metrics. Most notably, for abstract alignment objectives using the HH-RLHF dataset, DeAL outperforms both system prompting and reranking strategies, achieving up to 37% improvement in harmlessness and 24% in helpfulness. The framework is also shown to be complementary to RLHF, with combined approaches achieving the best performance, and provides significantly stronger defense against jailbreaking attacks compared to safety prompting alone.

## Method Summary
DeAL formalizes decoding as heuristic-guided search, where an LLM acts as a search agent that generates candidate tokens based on reward functions. The framework supports two types of alignment objectives: programmatic constraints (hard constraints like keyword inclusion or length limits) and abstract objectives (soft constraints like harmlessness or helpfulness). For programmatic constraints, DeAL uses a look-ahead mechanism that generates multiple candidate sequences and scores them based on constraint satisfaction. For abstract objectives, it employs parametric reward models (Rharmless, Rhelpful, Rhh) trained on human preference data. The decoding process uses top-k candidate selection with lookahead, allowing the model to consider future token sequences when making current decisions. DeAL can be combined with existing alignment methods like RLHF, where RLHF provides the base model and DeAL handles fine-grained alignment during inference.

## Key Results
- Keyword-constrained generation: DeAL improves keyword coverage by 8-21% on CommonGen compared to prompting baselines
- Length-constrained summarization: DeAL achieves better length satisfaction on XSUM while maintaining faithfulness, relevance, and coherence
- Abstract alignment objectives: DeAL outperforms system prompting and reranking, achieving up to 37% improvement in harmlessness and 24% in helpfulness on HH-RLHF
- Jailbreaking defense: DeAL prevents harmful responses 73% of the time versus 20% for safety prompting alone

## Why This Works (Mechanism)
DeAL works by transforming the standard greedy decoding process into a search problem where the LLM actively optimizes for alignment objectives during inference. Instead of simply selecting the highest probability token at each step, the framework generates multiple candidate sequences using lookahead and scores them based on reward functions that encode alignment objectives. This allows the model to make more informed decisions that consider both immediate and future consequences. For programmatic constraints, the reward functions directly measure constraint satisfaction (e.g., keyword presence, length compliance). For abstract objectives, parametric reward models trained on human preferences provide nuanced scoring. The lookahead mechanism is crucial because it allows the model to anticipate whether a particular token choice will lead to sequences that satisfy the alignment objectives, preventing the model from making locally optimal but globally suboptimal decisions.

## Foundational Learning
- Heuristic-guided search: Why needed - To transform decoding into an optimization problem that can incorporate alignment objectives. Quick check - Verify that the search algorithm can find high-reward sequences within computational constraints.
- Parametric reward models: Why needed - To encode complex human preferences for abstract alignment objectives like harmlessness and helpfulness. Quick check - Validate that reward models accurately predict human preferences on held-out data.
- Lookahead mechanism: Why needed - To allow the model to consider future token sequences when making current decisions, preventing myopic choices. Quick check - Confirm that lookahead improves alignment metrics compared to greedy decoding.
- Top-k candidate selection: Why needed - To balance exploration of the search space with computational efficiency. Quick check - Tune k to achieve optimal tradeoff between alignment quality and decoding speed.
- Constraint satisfaction scoring: Why needed - To directly measure how well generated text meets programmatic requirements like keyword inclusion or length limits. Quick check - Ensure scoring functions correctly identify constraint violations.

## Architecture Onboarding

Component map: LLM -> Top-k sampling -> Lookahead generator -> Reward scorer -> Candidate selector -> Output

Critical path: During decoding, the LLM generates k candidate tokens, the lookahead mechanism expands each candidate into full sequences, reward functions score each sequence, and the highest-scoring candidate is selected as the next token. This process repeats until generation completes.

Design tradeoffs: The framework balances alignment quality against computational cost. Larger candidate sizes and longer lookahead improve alignment but increase inference time. Parametric reward models provide nuanced scoring for abstract objectives but require training data and inference overhead. The choice between programmatic constraints (hard, fast) and abstract objectives (soft, slower) depends on the specific alignment requirements.

Failure signatures: Poor alignment occurs when the LLM's action space doesn't contain high-quality candidates that meet constraints, or when parametric reward models fail to accurately capture human preferences. Computational bottlenecks arise from large candidate sizes or complex reward model inference. Jailbreaking attacks may succeed if the lookahead mechanism doesn't adequately anticipate harmful continuations.

First experiments:
1. Implement DeAL with a simple keyword constraint on CommonGen and verify 8-21% improvement in keyword coverage over prompting
2. Test DeAL with length constraints on XSUM and measure improvements in length satisfaction while maintaining quality metrics
3. Evaluate DeAL's defense against a simple continuation attack using the HarmfulQ dataset and confirm 73% harmlessness rate versus 20% for prompting

## Open Questions the Paper Calls Out
- Computational overhead: What is the exact computational overhead of DeAL compared to standard decoding methods when using lookahead and parametric reward models? The paper provides a general range (2-5x) but lacks specific benchmarks across different configurations and hardware setups.

- Advanced jailbreaking attacks: How does DeAL perform against more sophisticated jailbreaking attacks beyond the simple continuation attack tested? The paper acknowledges this requires future investigation and doesn't evaluate against tree-based attacks, adversarial suffixes, or semantic-preserving attacks.

- Optimal configuration: What is the optimal balance between candidate size and lookahead length that maximizes alignment quality while minimizing computational cost? The paper uses fixed values but doesn't explore the tradeoff space or identify optimal operating points for different alignment objectives.

## Limitations
- Computational overhead: DeAL introduces 2-5x slowdown compared to greedy decoding, with exact overhead depending on configuration and hardware
- Reward model dependency: Effectiveness for abstract objectives depends on quality of parametric reward models, which require training data and may not perfectly capture human preferences
- Action space limitations: DeAL's performance is constrained by the LLM's ability to generate candidates that satisfy alignment objectives

## Confidence
- High: The overall framework and methodology of DeAL are well-specified and can be implemented using open-source LLMs and datasets
- Medium: The effectiveness of DeAL in improving alignment objectives and defending against jailbreaking attacks is supported by experimental results, but specific hyperparameters and reward model details are unclear
- Low: The exact implementation details of parametric reward models and their impact on alignment performance are uncertain

## Next Checks
1. Implement DeAL with a simple keyword constraint on CommonGen and verify 8-21% improvement in keyword coverage over prompting
2. Test DeAL with length constraints on XSUM and measure improvements in length satisfaction while maintaining quality metrics
3. Evaluate DeAL's defense against a simple continuation attack using the HarmfulQ dataset and confirm 73% harmlessness rate versus 20% for prompting