---
ver: rpa2
title: 'Steamroller Problems: An Evaluation of LLM Reasoning Capability with Automated
  Theorem Prover Strategies'
arxiv_id: '2407.20244'
source_url: https://arxiv.org/abs/2407.20244
tags:
- reasoning
- language
- query
- facts
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the ability of Large Language Models (LLMs)\
  \ to follow Automated Theorem Prover (ATP) reasoning strategies on steamroller problems.\
  \ Three models (GPT-3.5, GPT-4, Gemini-Pro) were tested using bottom-up, top-down,\
  \ and magic set reasoning approaches, with accuracy measured against random guessing\
  \ (0.500 \xB1 0.042)."
---

# Steamroller Problems: An Evaluation of LLM Reasoning Capability with Automated Theorem Prover Strategies

## Quick Facts
- arXiv ID: 2407.20244
- Source URL: https://arxiv.org/abs/2407.20244
- Authors: Lachlan McGinness; Peter Baumgartner
- Reference count: 40
- Key outcome: Explicit reasoning strategies yield accuracy comparable to chain-of-thought prompting but require 200-300% more tokens, with low correlation between correct reasoning and correct answers

## Executive Summary
This study evaluates whether large language models (LLMs) can effectively follow Automated Theorem Prover (ATP) reasoning strategies on steamroller problems. Three models (GPT-3.5, GPT-4, Gemini-Pro) were tested using bottom-up, top-down, and magic set reasoning approaches. Results show that explicit reasoning strategies yield accuracy comparable to one-shot chain-of-thought prompting, but require significantly more completion tokens. Notably, a low correlation (r < 0.3) was found between correct reasoning steps and correct answers, indicating that accurate answers can occur without valid reasoning. Bottom-up reasoning was most successful, aligning with LLMs' preference for this approach.

## Method Summary
The study evaluated LLM reasoning capabilities using the PRONTOQA dataset of steamroller problems with six prompt strategies: normal, zero-shot CoT, one-shot CoT, bottom-up, top-down, and magic set. The three models (GPT-3.5, GPT-4, Gemini-Pro) were tested on these prompts, with accuracy measured against random guessing (0.500 ± 0.042). Responses were analyzed using spaCy NLP tools to extract reasoning steps and evaluate correctness through process correctness and process soft correctness metrics. The evaluation measured both the accuracy of final answers and the presence of required reasoning steps.

## Key Results
- Explicit reasoning strategies (bottom-up, top-down, magic set) achieved accuracy comparable to one-shot chain-of-thought prompting (e.g., GPT-4 magic set: 0.943 ± 0.015)
- Bottom-up reasoning was most successful, aligning with LLMs' preference for this approach
- Low correlation (r < 0.3) between correct reasoning steps and correct answers, indicating reasoning process doesn't determine answer correctness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit reasoning strategies improve LLM performance on logical inference tasks.
- Mechanism: Providing step-by-step instructions for bottom-up, top-down, or magic set reasoning guides the model's inference process, leading to better accuracy than direct question answering.
- Core assumption: LLMs can follow structured reasoning instructions and use them to derive correct answers.
- Evidence anchors:
  - [abstract] "Results show that explicit reasoning strategies yield accuracy comparable to one-shot chain-of-thought prompting (e.g., GPT-4 magic set: 0.943 ± 0.015)"
  - [section 4.1] "the models' performance when using the ATP reasoning strategies was comparable to one-shot chain of thought"
- Break condition: If the model cannot parse or follow the reasoning instructions, or if the reasoning process is too complex for the model to handle.

### Mechanism 2
- Claim: The reasoning process of LLMs is not always aligned with the correctness of the final answer.
- Mechanism: Even when LLMs include all required reasoning steps, they may still arrive at an incorrect answer. Conversely, they may skip steps and still arrive at the correct answer.
- Core assumption: The reasoning process is not the sole determinant of the correctness of the final answer.
- Evidence anchors:
  - [abstract] "low correlation (r < 0.3) was found between correct reasoning steps and correct answers"
  - [section 4.3] "One very interesting finding is the lack of a correlation between including all of the required reasoning to prove the final answer and then obtaining the correct answer itself."
- Break condition: If the model consistently arrives at correct answers only when all required reasoning steps are included.

### Mechanism 3
- Claim: Bottom-up reasoning is the most effective reasoning strategy for LLMs.
- Mechanism: LLMs have a preference for and are best able to follow bottom-up reasoning processes, likely because it involves writing more facts and provides a "place to think."
- Core assumption: LLMs have an inherent preference for bottom-up reasoning due to their architecture or training data.
- Evidence anchors:
  - [abstract] "Bottom-up reasoning was most successful, aligning with LLMs' preference for this approach."
  - [section 5] "Consistent with previous literature, we show that models have a preference for following a bottom up style reasoning procedure"
- Break condition: If the model performs better with a different reasoning strategy, or if the model does not show a preference for bottom-up reasoning.

## Foundational Learning

- Concept: Automated Theorem Proving (ATP) reasoning strategies
  - Why needed here: The study evaluates the ability of LLMs to follow ATP reasoning strategies on logical inference tasks.
  - Quick check question: What are the key differences between bottom-up, top-down, and magic set reasoning strategies in ATP?

- Concept: Large Language Models (LLMs) and their reasoning capabilities
  - Why needed here: The study investigates the reasoning capabilities of LLMs using ATP strategies and explores the relationship between reasoning process and answer correctness.
  - Quick check question: How do LLMs typically perform on logical reasoning tasks, and what are the limitations of their reasoning abilities?

- Concept: Natural Language Processing (NLP) and its applications in reasoning tasks
  - Why needed here: The study uses NLP techniques, such as the spaCy library, to analyze the reasoning processes of LLMs and extract relevant facts and rules.
  - Quick check question: How can NLP techniques be used to analyze and evaluate the reasoning processes of LLMs?

## Architecture Onboarding

- Component map:
  LLMs (GPT-3.5, GPT-4, Gemini-Pro) -> PRONTOQA dataset (steamroller problems) -> ATP reasoning strategies (bottom-up, top-down, magic set) -> NLP tools (spaCy library) -> Evaluation metrics (accuracy, process correctness, process soft correctness)

- Critical path:
  1. Prepare the PRONTOQA dataset with different reasoning strategies
  2. Generate prompts for each reasoning strategy and model
  3. Run the models on the dataset and collect responses
  4. Analyze the responses using NLP tools to extract reasoning steps and evaluate correctness
  5. Calculate accuracy and other evaluation metrics

- Design tradeoffs:
  - Accuracy vs. computational expense: Explicit reasoning strategies improve accuracy but require more completion tokens.
  - Process correctness vs. answer correctness: The reasoning process may not always align with the correctness of the final answer.
  - Model preference vs. reasoning strategy effectiveness: LLMs may have a preference for certain reasoning strategies, but this may not always lead to the best performance.

- Failure signatures:
  - Low accuracy or correlation between reasoning steps and correct answers
  - Inability of the model to follow the prescribed reasoning strategy
  - Inconsistencies between the reasoning process and the final answer

- First 3 experiments:
  1. Evaluate the accuracy of LLMs on the PRONTOQA dataset using different reasoning strategies (bottom-up, top-down, magic set).
  2. Analyze the reasoning processes of LLMs using NLP tools and evaluate the correlation between reasoning steps and correct answers.
  3. Compare the computational expense of different reasoning strategies and investigate the tradeoff between accuracy and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the lack of correlation between correct reasoning and correct answers persist across different model architectures and sizes, or is it specific to the tested LLMs?
- Basis in paper: [explicit] The paper explicitly states "there was little correlation between correct reasoning and correct answers for all three of the models" and shows correlation values less than 0.3.
- Why unresolved: The study only tested three specific models (GPT-3.5, GPT-4, Gemini-Pro) and did not explore other model architectures or sizes.
- What evidence would resolve it: Testing the correlation between reasoning and answers across a broader range of models (including smaller and larger variants, and different architectures) would determine if this phenomenon is universal or model-specific.

### Open Question 2
- Question: At what scale of problem complexity (number of rules/facts) do ATP reasoning strategies become significantly more beneficial than LLMs' natural reasoning approaches?
- Basis in paper: [inferred] The paper mentions "it is only when problems reach a certain size that the choice of reasoning strategy becomes relevant" and suggests this as future work, but does not investigate different problem scales.
- Why unresolved: The study used a fixed problem size and did not systematically vary the number of rules and facts to identify the threshold where ATP strategies become advantageous.
- What evidence would resolve it: Conducting experiments with systematically increasing problem complexity while measuring the performance gap between ATP strategies and natural reasoning would identify the critical threshold.

### Open Question 3
- Question: Can hybrid architectures combining LLMs with ATP systems overcome the limitations of both approaches and achieve both correct reasoning and correct answers?
- Basis in paper: [explicit] The paper suggests "techniques like 'MagicSet' could be used to extract a small set of relevant facts that can be handed off to a trusted inference engine" as future work.
- Why unresolved: The study did not implement or test such hybrid architectures, only proposing them as a potential direction.
- What evidence would resolve it: Implementing and evaluating a hybrid system that uses LLMs for initial reasoning/selection followed by ATP systems for verification would demonstrate whether this approach can achieve both correct reasoning and correct answers.

## Limitations
- Limited sample size in PRONTOQA dataset constrains generalizability of findings
- Results show model-dependent performance but don't explain why certain models respond better to specific strategies
- Low correlation between reasoning and answer correctness is observed but underlying causes remain speculative

## Confidence
- High Confidence: Explicit reasoning strategies yield accuracy comparable to one-shot chain-of-thought prompting
- Medium Confidence: Bottom-up reasoning is most successful for LLMs
- Medium Confidence: Disconnect between reasoning process correctness and answer correctness

## Next Checks
1. Replicate experiments with an expanded PRONTOQA dataset (3-5x larger) to improve statistical power
2. Test additional LLM architectures (e.g., Claude, LLaMA variants) to determine if patterns are model-specific
3. Conduct ablation studies removing/reordering reasoning steps to quantify which aspects contribute to answer correctness