---
ver: rpa2
title: This Paper Had the Smartest Reviewers -- Flattery Detection Utilising an Audio-Textual
  Transformer-Based Approach
arxiv_id: '2406.17667'
source_url: https://arxiv.org/abs/2406.17667
tags:
- speech
- flattery
- audio
- experiments
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first dataset and methods for automatic
  flattery detection from speech, an important social influencing behavior. The authors
  present a 20-hour dataset of business analyst-CEO interactions, labeled for flattery
  at the sentence level.
---

# This Paper Had the Smartest Reviewers -- Flattery Detection Utilising an Audio-Textual Transformer-Based Approach
## Quick Facts
- **arXiv ID**: 2406.17667
- **Source URL**: https://arxiv.org/abs/2406.17667
- **Reference count**: 0
- **Primary result**: First dataset and methods for automatic flattery detection from speech, showing text-based models outperform audio-only approaches

## Executive Summary
This paper introduces the first dataset and methods for automatic flattery detection from speech, an important social influencing behavior. The authors present a 20-hour dataset of business analyst-CEO interactions, labeled for flattery at the sentence level. They train models using pretrained transformers for both audio and text modalities. Results show that text-based classification performs best (85.97% UAR), but combining audio and text via early fusion improves performance further to 87.16% UAR. This indicates flattery is primarily conveyed through linguistic content, but audio features provide complementary information.

## Method Summary
The authors created a 20-hour dataset of business analyst-CEO interactions annotated for flattery at the sentence level. They employed multiple pretrained transformer models for both audio and text modalities. Audio models included Wav2Vec2, Whisper, and AST variants, while text models used RoBERTa with both manual transcripts and automatic ASR outputs. The models were trained using cross-entropy loss, and multimodal fusion was achieved through early concatenation of audio and text features. Performance was evaluated using Unweighted Average Recall (UAR) across three folds.

## Key Results
- Text-based models achieved the highest performance (85.97% UAR) compared to audio-only models (77.49% UAR)
- Early fusion of audio and text features provided additional improvement to 87.16% UAR
- Flattery detection appears primarily linguistic in nature, with audio providing complementary information

## Why This Works (Mechanism)
Flattery detection benefits from transformer-based models because flattery involves complex linguistic patterns and subtle prosodic cues that require deep contextual understanding. The multimodal approach works because flattery is expressed through both what is said (linguistic content) and how it is said (prosody, tone, timing). Text-based models capture the semantic content and pragmatic markers of flattery, while audio models capture vocal characteristics that reinforce or signal flattery attempts. The fusion of both modalities leverages the complementary strengths of linguistic and acoustic information.

## Foundational Learning
- **Unweighted Average Recall (UAR)**: Metric used to handle class imbalance by averaging recall across classes. Why needed: Flattery is likely a minority class in natural speech. Quick check: Verify that minority class recall is not being sacrificed for overall accuracy.
- **Early Fusion vs Late Fusion**: Early fusion concatenates features before classification, while late fusion combines model outputs. Why needed: Determines how information from different modalities is integrated. Quick check: Compare performance differences between fusion strategies.
- **Transformer Pretraining**: Using models pretrained on large datasets (Wav2Vec2, Whisper, RoBERTa). Why needed: Provides robust feature representations from limited labeled data. Quick check: Evaluate performance degradation when using randomly initialized models.
- **Sentence-level Annotation**: Labeling individual sentences rather than entire conversations. Why needed: Flattery often occurs in specific utterances within longer interactions. Quick check: Analyze annotation consistency across annotators.
- **Cross-modal Redundancy**: When multiple modalities provide similar information. Why needed: Determines whether combining modalities provides genuine complementary information. Quick check: Measure correlation between audio and text features.
- **Domain-specific Dataset**: Business analyst-CEO interactions. Why needed: Social behaviors like flattery vary across contexts. Quick check: Test model performance on different interaction types.

## Architecture Onboarding
**Component Map**: Audio Input -> Audio Transformer (Wav2Vec2/Whisper/AST) -> Audio Features; Text Input -> Text Transformer (RoBERTa) -> Text Features; Audio Features + Text Features -> Early Fusion -> Classification Layer

**Critical Path**: Audio/Text Input → Transformer Feature Extraction → Early Feature Concatenation → Classification

**Design Tradeoffs**: 
- Using pretrained transformers trades computational efficiency for better feature extraction from limited data
- Early fusion assumes linear combination of modalities is sufficient, potentially missing complex interactions
- Sentence-level annotation balances granularity with annotation effort but may miss contextual cues

**Failure Signatures**: 
- Low performance on minority classes suggests class imbalance issues
- Similar performance between single and multimodal models indicates redundancy rather than complementarity
- Performance drop with ASR transcripts reveals sensitivity to transcription quality

**Three First Experiments**:
1. Test model performance on held-out test set to establish baseline
2. Compare early fusion vs late fusion performance to validate fusion strategy
3. Evaluate impact of ASR quality by comparing manual vs automatic transcripts

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (20 hours) may limit generalizability
- Domain-specific nature (business analyst-CEO interactions) may not capture flattery in other contexts
- Manual annotation introduces potential subjectivity despite inter-annotator agreement checks

## Confidence
- High confidence: Text-based classification outperforms audio-only approaches for flattery detection
- Medium confidence: Multimodal fusion provides modest but statistically significant improvement
- Medium confidence: Broader implications for detecting other social influencing behaviors require additional validation

## Next Checks
1. Test the model on an independent dataset from a different domain (e.g., customer service interactions or political speeches) to assess generalizability of the flattery detection approach.

2. Conduct ablation studies to determine which specific audio features (prosody, emotion, etc.) contribute most to the multimodal improvement, helping identify what complementary information audio provides.

3. Evaluate the impact of ASR quality on text-based performance by systematically degrading automatic transcripts and measuring performance degradation, to establish robustness requirements for real-world deployment.