---
ver: rpa2
title: Evaluating Knowledge-based Cross-lingual Inconsistency in Large Language Models
arxiv_id: '2407.01358'
source_url: https://arxiv.org/abs/2407.01358
tags:
- cross-lingual
- consistency
- llms
- language
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates cross-lingual inconsistencies in Large
  Language Models (LLMs) across semantic, accuracy, and timeliness dimensions. To
  address this, the authors construct a multilingual aligned knowledge-based QA dataset
  (MAKQA) and propose three metrics: Cross-lingual Semantic Consistency (xSC) based
  on LaBSE encoding, Cross-lingual Accuracy Consistency (xAC) using CHRF scores and
  rank correlation, and Cross-lingual Timeliness Consistency (xTC) for time-sensitive
  queries.'
---

# Evaluating Knowledge-based Cross-lingual Inconsistency in Large Language Models

## Quick Facts
- arXiv ID: 2407.01358
- Source URL: https://arxiv.org/abs/2407.01358
- Authors: Xiaolin Xing; Zhiwei He; Haoyu Xu; Xing Wang; Rui Wang; Yu Hong
- Reference count: 7
- Primary result: Proposed metrics reveal significant cross-lingual inconsistencies in LLMs, with GPT-3.5 achieving highest xC score of 0.552

## Executive Summary
This paper investigates cross-lingual inconsistencies in Large Language Models (LLMs) across semantic, accuracy, and timeliness dimensions. The authors construct a multilingual aligned knowledge-based QA dataset (MAKQA) and propose three metrics: Cross-lingual Semantic Consistency (xSC) based on LaBSE encoding, Cross-lingual Accuracy Consistency (xAC) using CHRF scores and rank correlation, and Cross-lingual Timeliness Consistency (xTC) for time-sensitive queries. Experiments with five LLMs reveal significant cross-lingual inconsistencies, with GPT-3.5 performing best and open-source models showing notable room for improvement. The study finds a positive correlation between cross-lingual consistency and multilingual translation capabilities.

## Method Summary
The authors construct the MAKQA dataset using Wikidata and Wikipedia entity names across 12 languages and 6 knowledge domains. They implement three metrics: xSC using LaBSE for semantic encoding and cosine similarity, xAC using CHRF scores and rank correlation to measure accuracy consistency, and xTC for evaluating time-sensitive query consistency. The harmonic mean of these three metrics provides an overall cross-lingual consistency score (xC). Five LLMs (GPT-3.5, Bloomz, Llama2, Baichuan2, Mistral) are evaluated across 12 languages to quantify cross-lingual inconsistencies.

## Key Results
- GPT-3.5 achieved the highest xC score of 0.552, significantly outperforming open-source models
- Open-source models (Bloomz, Llama2, Baichuan2, Mistral) showed substantial cross-lingual inconsistencies across all metrics
- A positive correlation was found between multilingual translation capabilities and cross-lingual consistency scores
- xAC scores showed stronger correlation with translation performance than xSC scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual inconsistency is detectable via semantic embedding similarity
- Mechanism: The study uses LaBSE to encode LLM-generated answers in multiple languages, then computes cosine similarity between these embeddings to quantify semantic consistency (xSC)
- Core assumption: LaBSE provides a reliable cross-lingual semantic representation that captures meaning across languages
- Evidence anchors:
  - [abstract] "we propose an innovative evaluation method for Cross-lingual Semantic Consistency (xSC) using the LaBSE model"
  - [section] "To measure this, the method employs the multilingual semantic encoding model LASER to encode the answers generated by LLMs in different languages"
  - [corpus] Weak - corpus doesn't contain evidence about LaBSE/LASER performance, only mentions "Lost in Multilinguality" which uses different methods
- Break condition: If LaBSE fails to capture semantic equivalence for specific language pairs or domains, the xSC metric would underestimate true consistency

### Mechanism 2
- Claim: Cross-lingual inconsistency manifests across multiple dimensions beyond semantics
- Mechanism: The study extends beyond semantic consistency to measure accuracy consistency (xAC) using CHRF scores and rank correlation, and timeliness consistency (xTC) for time-sensitive queries
- Core assumption: Different types of inconsistencies can be meaningfully measured using established metrics (CHRF for accuracy, rank correlation for timeliness)
- Evidence anchors:
  - [abstract] "We further introduce metrics for Cross-lingual Accuracy Consistency (xAC) and Cross-lingual Timeliness Consistency (xTC)"
  - [section] "We introduce the Cross-lingual Accuracy Consistency metric (xAC) and the Cross-lingual Timeliness Consistency metric (xTC)"
  - [corpus] Weak - corpus doesn't provide direct evidence about these specific metrics
- Break condition: If the metrics fail to capture the relevant aspects of inconsistency (e.g., CHRF not sensitive to factual accuracy in certain domains)

### Mechanism 3
- Claim: Cross-lingual consistency correlates with multilingual translation performance
- Mechanism: The study finds that models with higher multilingual translation accuracy (measured via CHRF on Flores-200 dataset) show better cross-lingual consistency scores
- Core assumption: Translation performance is a valid proxy for cross-lingual consistency capabilities
- Evidence anchors:
  - [abstract] "Our findings indicate a positive correlation between cross-lingual consistency and multilingual translation capabilities"
  - [section] "The results show a clear positive correlation between the multilingual translation capabilities of LLMs and their average xAC scores"
  - [corpus] Weak - corpus mentions related work but no direct evidence about this specific correlation
- Break condition: If translation performance captures different aspects of language understanding than factual consistency

## Foundational Learning

- Concept: Cosine similarity and embedding spaces
  - Why needed here: The xSC metric relies on computing cosine similarity between LaBSE embeddings to measure semantic consistency
  - Quick check question: How would you interpret a cosine similarity of 0.7 between two cross-lingual embeddings? What does this tell you about semantic consistency?

- Concept: CHRF metric for string similarity
  - Why needed here: CHRF scores are used to measure accuracy consistency between LLM answers and ground truth, and to evaluate translation quality
  - Quick check question: What is the difference between CHRF and BLEU? Why might CHRF be preferred for evaluating cross-lingual consistency?

- Concept: Spearman rank correlation
  - Why needed here: Used to measure consistency of accuracy and timeliness rankings across languages
  - Quick check question: Why is rank correlation appropriate for measuring consistency across languages? What does a Spearman correlation of 0.8 indicate?

## Architecture Onboarding

- Component map: MAKQA dataset construction → LaBSE encoding → CHRF scoring → Spearman correlation → Harmonic mean computation
- Critical path: Dataset construction → LLM inference across 12 languages → xSC/xAC/xTC computation → correlation with translation performance
- Design tradeoffs: Dataset coverage vs. quality, metric sensitivity vs. computational cost, closed vs. open-source models
- Failure signatures: Low xSC scores with high translation accuracy (metric failure), inconsistent rankings across languages (model failure), dataset bias (evaluation failure)
- First 3 experiments:
  1. Replicate xSC scores on a subset of MAKQA with ground truth verification
  2. Test xAC/xTC sensitivity by injecting controlled inconsistencies
  3. Compare LaBSE vs. other embedding models on cross-lingual semantic consistency

## Open Questions the Paper Calls Out
None

## Limitations
- LaBSE encoding reliability may not accurately reflect true semantic consistency across all language pairs and domains
- The evaluation framework may not capture all dimensions of cross-lingual inconsistency, such as cultural context or pragmatic understanding
- The positive correlation between translation performance and consistency does not establish causation

## Confidence
- **High Confidence**: Cross-lingual inconsistencies exist across multiple LLMs and dimensions (semantic, accuracy, timeliness)
- **Medium Confidence**: The specific metric formulations (xSC, xAC, xTC) accurately capture the intended aspects of cross-lingual inconsistency
- **Medium Confidence**: The positive correlation between multilingual translation capabilities and cross-lingual consistency suggests a potential pathway for improvement

## Next Checks
1. Cross-validate semantic encoding by comparing LaBSE-based xSC scores with human-annotated semantic similarity judgments for a subset of LLM outputs
2. Systematically inject controlled factual inconsistencies into LLM outputs across languages to verify xAC and xTC metric sensitivity
3. Recompute cross-lingual consistency scores using alternative embedding models (e.g., multilingual BERT) and string similarity metrics (e.g., BLEU instead of CHRF) to assess robustness