---
ver: rpa2
title: 'iKAN: Global Incremental Learning with KAN for Human Activity Recognition
  Across Heterogeneous Datasets'
arxiv_id: '2406.01646'
source_url: https://arxiv.org/abs/2406.01646
tags:
- learning
- ikan
- incremental
- framework
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: iKAN proposes a novel incremental learning framework for HAR across
  heterogeneous datasets, addressing catastrophic forgetting and non-uniform inputs.
  It leverages KANs instead of MLPs for their local plasticity and global stability,
  using task-specific feature branches and a feature redistribution layer.
---

# iKAN: Global Incremental Learning with KAN for Human Activity Recognition Across Heterogeneous Datasets

## Quick Facts
- arXiv ID: 2406.01646
- Source URL: https://arxiv.org/abs/2406.01646
- Reference count: 26
- Primary result: Achieved 84.9% weighted F1 score on six HAR datasets, outperforming EWC (51.42%) and experience replay (59.92%)

## Executive Summary
iKAN proposes a novel incremental learning framework for human activity recognition across heterogeneous datasets, addressing catastrophic forgetting and non-uniform inputs. The framework leverages Kolmogorov-Arnold Networks (KANs) instead of traditional MLPs for their local plasticity and global stability properties. iKAN uses task-specific feature extraction branches and a feature redistribution layer to accommodate new sensor modalities while maintaining consistent classifier dimensions. Evaluated on six public HAR datasets, iKAN demonstrated superior performance with minimal forgetting and intransigence measures.

## Method Summary
iKAN implements a two-step training procedure with a KAN-based classifier, feature redistribution layer, and task-specific encoders. The framework first trains encoders (4 CNN2D layers with 10 output channels each) for 100 epochs using early stopping, then freezes them and trains the KAN classifier for 30 epochs with Adam optimizer. The feature redistribution layer normalizes and scales features based on task ID and total task count, while KANs use learnable spline-based activation functions that provide local plasticity. The architecture expands feature extraction branches to handle heterogeneous inputs while maintaining fixed classifier dimensions.

## Key Results
- Achieved 84.9% weighted F1 score on six public HAR datasets
- Last incremental performance of 81.34%, significantly outperforming EWC (51.42%) and experience replay (59.92%)
- Minimal forgetting measure of 0.0017 and intransigence measure of 0.0012

## Why This Works (Mechanism)

### Mechanism 1
KANs provide local plasticity and global stability that mitigates catastrophic forgetting in incremental learning. Unlike MLPs, KANs use learnable activation functions on edges parameterized as splines. Splines are local, meaning a new sample only affects nearby spline coefficients, leaving distant coefficients intact. This preserves previously learned knowledge while allowing adaptation to new tasks. Core assumption: Local nature of spline bases effectively isolates changes to relevant regions without disrupting distant representations.

### Mechanism 2
The feature redistribution layer enables non-overlapping feature representations across heterogeneous datasets. After normalization, features from each task are scaled based on task ID and total task count, with hyperparameter β controlling the distance between task features. This ensures that feature representations from different tasks occupy distinct regions of the feature space. Core assumption: Properly scaled and distributed features prevent interference between tasks during incremental learning.

### Mechanism 3
Expanding feature extraction branches while maintaining consistent classifier dimensions enables processing of heterogeneous inputs. iKAN uses task-specific feature extraction branches (encoders) that can handle different input dimensions from various sensor modalities. These branches feed into a single fixed-dimension classifier, allowing the model to process heterogeneous inputs while maintaining consistent output dimensions. Core assumption: Different encoders can effectively extract task-specific features that are compatible with a single classifier architecture.

## Foundational Learning

- **Concept: Catastrophic forgetting in neural networks**
  - Why needed here: Understanding catastrophic forgetting is crucial because iKAN specifically addresses this problem in incremental learning across heterogeneous datasets.
  - Quick check question: What happens to a neural network's performance on previous tasks when it is trained on new tasks without any special mechanism to prevent forgetting?

- **Concept: Incremental learning vs. traditional batch learning**
  - Why needed here: iKAN is designed for incremental learning scenarios where data arrives in streams, unlike traditional batch learning where all data is available upfront.
  - Quick check question: How does the data availability and model update process differ between incremental learning and traditional batch learning approaches?

- **Concept: Spline parameterization and local function approximation**
  - Why needed here: KANs use splines for function approximation, and understanding how local spline bases work is key to grasping why KANs mitigate catastrophic forgetting.
  - Quick check question: What property of spline functions allows them to make local changes without affecting distant regions of the function space?

## Architecture Onboarding

- **Component map**: Input layer → Task-specific encoder → Feature redistribution layer → KAN-based classifier → Output layer
- **Critical path**: 
  1. Input data arrives and is identified by task ID based on input dimensions
  2. Data flows through task-specific encoder to extract features
  3. Features pass through feature redistribution layer for scaling
  4. Redirected features enter KAN-based classifier
  5. Classifier produces output predictions

- **Design tradeoffs**:
  - Multiple encoders vs. single flexible encoder: Multiple encoders provide task-specific optimization but increase model complexity
  - Fixed classifier dimensions vs. adaptive dimensions: Fixed dimensions simplify architecture but may limit expressiveness
  - Feature redistribution layer complexity vs. direct concatenation: Redistribution provides better separation but adds computational overhead

- **Failure signatures**:
  - High forgetting measure indicates catastrophic forgetting is occurring
  - High intransigence measure suggests difficulty learning new tasks
  - Performance degradation when adding new tasks indicates architectural limitations
  - Poor performance on individual tasks may indicate inadequate encoder design

- **First 3 experiments**:
  1. Test iKAN on two homogeneous datasets to establish baseline performance and verify the architecture works before adding complexity
  2. Introduce a third heterogeneous dataset and measure forgetting and intransigence to validate the feature redistribution layer
  3. Perform ablation study by removing the feature redistribution layer to quantify its contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal grid number configuration for KAN-based classifiers in incremental learning scenarios across heterogeneous HAR datasets? The paper tested grid numbers from 5 to 30 but only provided performance at discrete points, leaving the precise optimal value unknown. The authors note that performance plateaus after grid number 25, suggesting there may be a diminishing returns threshold.

### Open Question 2
How does the hyperparameter β in the feature redistribution layer affect incremental learning performance and catastrophic forgetting? The paper mentions that β controls the distance between feature representations from different tasks but does not examine its impact on learning performance or forgetting measures.

### Open Question 3
Can the iKAN framework be extended to work with computer vision-based HAR datasets in addition to sensor-based datasets? The paper states that iKAN was primarily evaluated using sensor-based datasets and that its performance on computer-vision HAR datasets remains to be evaluated.

### Open Question 4
How can the task identification layer be improved to enable fully class-incremental learning rather than task-incremental learning? The paper acknowledges that the current task identification layer can only recognize tasks with different window dimensions and requires manual task ID specification for other cases, preventing it from being a fully class-incremental framework.

## Limitations
- The task identification layer can only recognize tasks with different window dimensions and requires manual task ID specification for other cases
- Performance on computer vision-based HAR datasets remains unevaluated
- Optimal grid number configuration for KANs in incremental learning scenarios is undetermined

## Confidence

Major uncertainties include the exact implementation details of the feature redistribution layer scaling formula and the hyperparameter β value used in experiments, as well as the specific grid number configuration for the KAN classifier beyond the stated 30 grids for optimal performance.

- KANs provide superior catastrophic forgetting mitigation: Medium
- Feature redistribution layer effectiveness: High
- Architectural claim about heterogeneous input processing: High

## Next Checks

1. Implement ablation studies removing the feature redistribution layer to quantify its exact contribution to performance and forgetting mitigation.
2. Test iKAN on additional heterogeneous datasets beyond HAR to verify generalizability of the incremental learning framework.
3. Conduct sensitivity analysis on the hyperparameter β and grid number configurations to establish robustness across different task distributions.