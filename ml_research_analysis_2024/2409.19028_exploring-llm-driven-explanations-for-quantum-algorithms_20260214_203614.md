---
ver: rpa2
title: Exploring LLM-Driven Explanations for Quantum Algorithms
arxiv_id: '2409.19028'
source_url: https://arxiv.org/abs/2409.19028
tags:
- quantum
- explanations
- llms
- code
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study is the first to systematically assess large language
  models' (LLMs) ability to generate explanations for quantum code. The authors evaluate
  three widely adopted LLMs (GPT-3.5, Llama2, and Tinyllama) using two prompt styles
  across seven quantum algorithms written in OpenQASM 3.
---

# Exploring LLM-Driven Explanations for Quantum Algorithms

## Quick Facts
- arXiv ID: 2409.19028
- Source URL: https://arxiv.org/abs/2409.19028
- Reference count: 40
- This study is the first to systematically assess large language models' ability to generate explanations for quantum code.

## Executive Summary
This study presents the first systematic evaluation of large language models' capabilities in generating explanations for quantum algorithms. The research examines three popular LLMs (GPT-3.5, Llama2, and Tinyllama) across seven quantum algorithms written in OpenQASM 3, using two different prompt styles. Four human raters with varying expertise in software engineering and quantum computing evaluated the generated explanations.

The findings demonstrate that LLMs can produce useful explanations for quantum code, with Llama2 showing the highest quality for initial explanations and GPT-3.5 performing better at improving existing explanations. The study reveals that simple prompt engineering, such as adding basic context like algorithm names and qubit counts, significantly enhances explanation quality across all models. Additionally, the explanations show qualitative and syntactical consistency across multiple generation rounds, indicating reliable performance.

## Method Summary
The study evaluates three LLMs (GPT-3.5, Llama2, and Tinyllama) using two prompt styles across seven quantum algorithms in OpenQASM 3. Four human raters with mixed software engineering and quantum computing expertise assess the explanation quality. The evaluation compares models' ability to generate explanations from scratch versus improving existing explanations, and tests the impact of enhanced prompts with basic context. Multiple generation rounds assess consistency in output quality.

## Key Results
- Llama2 provides the highest quality explanations when generating from scratch
- GPT-3.5 excels at improving existing explanations compared to other models
- Adding basic context to prompts significantly improves explanation quality across all LLMs
- Explanations remain qualitatively and syntactically consistent over multiple generation rounds

## Why This Works (Mechanism)
The study leverages LLMs' natural language processing capabilities to bridge the gap between quantum code syntax and human-understandable explanations. By using different prompt styles and providing contextual information, the models can better interpret quantum operations and algorithmic intent. The consistency across multiple generations suggests that LLMs have learned stable representations of quantum computing concepts that translate reliably into explanations.

## Foundational Learning
- **OpenQASM 3 syntax**: Quantum assembly language used to write quantum algorithms
  - Why needed: Provides standardized quantum code format for consistent evaluation
  - Quick check: Verify code compiles and runs on quantum simulators

- **Quantum algorithm fundamentals**: Understanding basic quantum operations and circuit structures
  - Why needed: Essential for generating accurate explanations of quantum code
  - Quick check: Compare explanations against established quantum computing textbooks

- **Prompt engineering principles**: Techniques for crafting effective prompts for LLMs
  - Why needed: Directly impacts quality and relevance of generated explanations
  - Quick check: Test different prompt variations and measure output quality improvements

## Architecture Onboarding
- **Component map**: Quantum algorithm code (OpenQASM 3) -> LLM model -> Natural language explanation
- **Critical path**: Code input → Prompt construction → LLM processing → Human evaluation → Quality assessment
- **Design tradeoffs**: Model capability vs. computational cost, explanation depth vs. accessibility, automation vs. human oversight
- **Failure signatures**: Inaccurate quantum concepts, overly technical jargon, inconsistent explanations across generations
- **3 first experiments**: 1) Test prompt variations with different quantum algorithms; 2) Compare explanation quality across expertise levels; 3) Measure consistency across multiple LLM generations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on only four human raters, creating potential subjectivity and limited generalizability
- Quantum algorithms sample is constrained to seven examples, not representing full quantum computing landscape
- Focus on OpenQASM 3 limits broader applicability to other quantum programming languages
- Qualitative consistency examined but quantitative stability and variance not measured

## Confidence
- LLMs can generate useful quantum algorithm explanations: **High**
- Llama2 outperforms other models for initial explanations: **Medium**
- GPT-3.5 excels at improving existing explanations: **Medium**
- Prompt engineering significantly improves output quality: **High**

## Next Checks
1. Expand human evaluation to include 10+ raters with diverse quantum computing expertise levels
2. Test additional quantum algorithms beyond initial seven, especially those with complex operations and larger qubit counts
3. Implement automated evaluation metrics alongside human assessment to quantify explanation quality and consistency across generations