---
ver: rpa2
title: Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models
arxiv_id: '2410.09047'
source_url: https://arxiv.org/abs/2410.09047
tags:
- safety
- alignment
- vlms
- input
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the safety alignment degradation in Vision-Language
  Models (VLMs) when vision modality is integrated, comparing their performance to
  their LLM backbones. The study identifies a representation gap between multi-modal
  and text-only inputs, leading to a distribution shift in the model's latent space
  and consequently reduced alignment ability.
---

# Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models

## Quick Facts
- **arXiv ID**: 2410.09047
- **Source URL**: https://arxiv.org/abs/2410.09047
- **Reference count**: 11
- **One-line primary result**: CMRM reduces unsafe rate of LLaVA-7B from 61.53% to 3.15% on multi-modal input without additional training

## Executive Summary
This paper investigates safety alignment degradation in Vision-Language Models (VLMs) when vision modality is integrated, comparing their performance to their LLM backbones. The study identifies a representation gap between multi-modal and text-only inputs, leading to a distribution shift in the model's latent space and consequently reduced alignment ability. To address this, the authors propose Cross-Modality Representation Manipulation (CMRM), an inference-time method that calibrates multi-modal input representations by moving them closer to the LLM backbone's optimized distribution. CMRM significantly recovers the safety alignment of VLMs while preserving general model performance.

## Method Summary
The paper proposes Cross-Modality Representation Manipulation (CMRM), an inference-time method that intervenes in the hidden states of VLMs to pull multi-modal representations back towards the distribution that the LLM backbone is optimized for. CMRM extracts shifting vectors that indicate the effect of image incorporation on hidden states, then calibrates the representation of multi-modal input using these vectors to approximate the ideal distribution. The method operates without additional training and is evaluated on safety datasets including VLSafe and JailbreakLLMs, as well as utility benchmarks like ScienceQA and LLaVA-Bench-Coco.

## Key Results
- CMRM reduces unsafe rate of LLaVA-7B from 61.53% to 3.15% on multi-modal input without additional training
- The method significantly outperforms training-time safety alignment methods while maintaining general model performance
- CMRM demonstrates effectiveness across multiple VLM architectures including LLaVA-7B, LLaVA-13B, and ShareGPT4V

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The safety alignment degradation in VLMs occurs due to a representation gap between multi-modal and text-only inputs.
- Mechanism: When vision modality is integrated into VLMs, the representations of multi-modal inputs shift away from the distribution that the LLM backbone is optimized for, leading to reduced alignment ability.
- Core assumption: The LLM backbone's safety alignment capabilities, developed within the textual embedding space, do not successfully transfer to the new multi-modal representation space.
- Evidence anchors:
  - [abstract]: "We show that the representations of multi-modal inputs shift away from that of text-only inputs which represent the distribution that the LLM backbone is optimized for."
  - [section]: "The representations of multi-modal inputs shift away from that of text-only inputs which represent the distribution that the LLM backbone is optimized for."
- Break condition: If the LLM backbone's safety alignment capabilities can successfully transfer to the multi-modal representation space, this mechanism would not apply.

### Mechanism 2
- Claim: CMRM recovers safety alignment by calibrating multi-modal input representations to be closer to the LLM backbone's optimized distribution.
- Mechanism: CMRM intervenes in the hidden states of VLMs, pulling multi-modal representations back towards the distribution that the LLM backbone is optimized for, thereby recovering the safety alignment ability.
- Core assumption: The alignment degradation can be mitigated by eliminating the representation shift when an image is incorporated as input.
- Evidence anchors:
  - [abstract]: "CMRM significantly recovers the safety alignment of VLMs, reducing the unsafe rate of LLaVA-7B from 61.53% to as low as 3.15% on multi-modal input without additional training."
  - [section]: "To reduce safety alignment degradation, we introduce Cross-Modality Representation Manipulation (CMRM), an inference time representation intervention method for recovering the safety alignment ability that is inherent in the LLM backbone of VLMs."
- Break condition: If the calibrated representations do not effectively pull the multi-modal inputs closer to the LLM backbone's distribution, this mechanism would fail.

### Mechanism 3
- Claim: The shifting direction caused by visual input incorporation can be estimated and used to manipulate representations for alignment recovery.
- Mechanism: CMRM extracts shifting vectors that indicate the effect of image incorporation on hidden states, then calibrates the representation of multi-modal input using these vectors to approximate the ideal distribution.
- Core assumption: The shifting direction caused by visual input incorporation is correlated with the alignment degradation phenomenon.
- Evidence anchors:
  - [abstract]: "CMRM first anchors a VLM's low-dimensional representation space and estimates the 'shifting direction' that indicates the affect of the incorporation of image in the input on the overall hidden states."
  - [section]: "CMRM first extracts the shifting direction caused by the incorporation of visual input, which is, according to our assumption, correlated with the alignment degradation phenomenon."
- Break condition: If the extracted shifting vectors do not accurately represent the direction of representation shift, this mechanism would not work.

## Foundational Learning

- **Concept**: Principal Component Analysis (PCA)
  - Why needed here: PCA is used to visualize and analyze the hidden states of VLMs upon different types of input, revealing the separation between text-only and multi-modal inputs.
  - Quick check question: How does PCA help in identifying the representation gap between multi-modal and text-only inputs in VLMs?

- **Concept**: Representation Engineering
  - Why needed here: Representation engineering techniques are employed to intervene in the hidden states of VLMs, pulling multi-modal representations back towards the LLM backbone's distribution.
  - Quick check question: What is the role of representation engineering in recovering the safety alignment ability of VLMs?

- **Concept**: Inference-time Intervention
  - Why needed here: CMRM operates at inference time, allowing it to be seamlessly integrated with any pre-trained VLMs without the need for additional training.
  - Quick check question: How does inference-time intervention differ from training-time methods in terms of cost and flexibility?

## Architecture Onboarding

- **Component map**: Vision encoder (e.g., CLIP) -> Language decoder (e.g., Vicuna) -> CMRM intervention module -> Output generation

- **Critical path**:
  1. Input processing (text and image)
  2. Representation extraction from vision and language modules
  3. CMRM intervention (shifting vector extraction and manipulation)
  4. Output generation

- **Design tradeoffs**:
  - Dataset-level vs. sample-level shifting vector extraction
  - Alpha value for intervention intensity
  - Computational overhead vs. safety improvement

- **Failure signatures**:
  - High unsafe rates persist despite CMRM intervention
  - Significant decrease in general model performance
  - Ineffective shifting vector extraction

- **First 3 experiments**:
  1. Evaluate CMRM on a small subset of the VLSafe dataset with both dataset-level and sample-level extraction.
  2. Test the impact of different alpha values on safety improvement and model utility.
  3. Compare CMRM's performance against a training-time baseline method like VLGuard.

## Open Questions the Paper Calls Out

- How does the representation shift caused by vision modality affect alignment in tasks beyond safety, such as reasoning ability and factual accuracy?
- What specific features make an optimal anchor dataset for shifting vector extraction in CMRM?
- How does the optimal alpha value for CMRM vary across different VLM architectures and training procedures?

## Limitations

- The study focuses primarily on prompt injection attacks without addressing other safety dimensions such as harmful content generation or bias.
- The empirical validation is limited to three VLM models (LLaVA-7B, LLaVA-13B, and ShareGPT4V), which may not be representative of the broader VLM landscape.
- The analysis assumes that the representation shift is the primary cause of safety degradation, but does not rule out other potential contributing factors.

## Confidence

**High Confidence**: The identification of representation gaps between multi-modal and text-only inputs is well-supported by PCA visualizations and quantitative analysis. The empirical demonstration that CMRM reduces unsafe rates from 61.53% to 3.15% on LLaVA-7B is robust within the tested datasets.

**Medium Confidence**: The proposed mechanism linking representation shifts to safety degradation is plausible but not definitively proven. While the paper shows correlation between representation shifts and safety performance, it does not establish causal mechanisms at the token or attention level.

**Low Confidence**: The generalizability of CMRM to other safety dimensions beyond prompt injection, and to VLM architectures not tested in this paper, remains uncertain.

## Next Checks

1. **Cross-Domain Safety Testing**: Evaluate CMRM's effectiveness on VLMs with different safety challenges beyond prompt injection, such as generating harmful content when given benign prompts with harmful images.

2. **Architecture and Scale Generalization**: Test CMRM on a broader range of VLM architectures (different vision encoders, language models, and fusion mechanisms) and at larger model scales (e.g., 30B+ parameters).

3. **Ablation on Representation Components**: Conduct ablation studies to determine which components of the representation shift (e.g., specific layers, attention heads, or token types) are most critical for safety degradation.