---
ver: rpa2
title: 'Streamlining Systematic Reviews: A Novel Application of Large Language Models'
arxiv_id: '2412.15247'
source_url: https://arxiv.org/abs/2412.15247
tags:
- screening
- articles
- abstract
- title
- rayyan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces an in-house system using Large Language Models
  (LLMs) for automating both title/abstract and full-text screening in systematic
  reviews (SRs). The system leverages prompt engineering and Retrieval-Augmented Generation
  (RAG) to screen articles on Vitamin D and falls.
---

# Streamlining Systematic Reviews: A Novel Application of Large Language Models

## Quick Facts
- arXiv ID: 2412.15247
- Source URL: https://arxiv.org/abs/2412.15247
- Reference count: 0
- The study achieved 99.5% article exclusion rate (AER), 99.6% specificity, and 100% negative predictive value (NPV) using LLMs for systematic review screening.

## Executive Summary
This study introduces an innovative system that leverages Large Language Models (LLMs) to automate both title/abstract and full-text screening in systematic reviews (SRs). The system combines prompt engineering with Retrieval-Augmented Generation (RAG) to screen articles on Vitamin D and falls, achieving remarkable efficiency gains while maintaining high accuracy. By achieving 99.5% AER and 100% NPV with no false negatives, the LLM-based approach demonstrates superior performance compared to both manual screening and commercial tools like Rayyan. The system significantly reduced manual screening time by 95.5%, completing the process in 25.5 hours versus 564.4 hours for traditional methods.

## Method Summary
The study implemented an in-house LLM-based system using GPT-4 for systematic review screening. The approach utilized prompt engineering for title/abstract screening, where structured questions guided the LLM to make inclusion/exclusion decisions. For full-text screening, the system employed Retrieval-Augmented Generation (RAG), using full articles as document sets from which GPT-4 retrieves information to answer screening questions. The methodology was tested on 14,439 articles from a systematic review on Vitamin D and falls, with 20 known included articles serving as ground truth. Performance was evaluated using metrics including AER, FNR, specificity, PPV, and NPV.

## Key Results
- Achieved 99.5% article exclusion rate (AER) with 99.6% specificity
- Maintained 100% negative predictive value (NPV) with zero false negatives
- Reduced manual screening time by 95.5%, requiring only 25.5 hours versus 564.4 hours for traditional methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system achieves high exclusion accuracy through prompt engineering that aligns LLM decision-making with human screening criteria.
- Mechanism: By providing structured system prompts and step-by-step questions mirroring traditional screening guides, the LLM adopts the role of a medical researcher and produces consistent "yes/no/unsure" responses that directly map to inclusion/exclusion decisions.
- Core assumption: The LLM's responses to structured questions can reliably substitute for human judgment in systematic review screening when the prompts are sufficiently detailed.
- Evidence anchors:
  - [abstract] "The LLM-based system employed prompt engineering for title/abstract screening"
  - [section] "we give the model a system prompt that instructs it to act as a professional medical researcher performing title/abstract screening"
  - [corpus] No direct corpus evidence for this specific mechanism; claim is based on study design
- Break condition: If the LLM cannot consistently interpret medical screening criteria or produces excessive "unsure" responses requiring manual intervention.

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) enables accurate full-text screening by grounding LLM responses in article content.
- Mechanism: The system uses full-text articles as document sets from which the LLM retrieves relevant information to answer screening questions, reducing hallucination and improving decision accuracy.
- Core assumption: Access to full article text during the screening process allows the LLM to make more informed decisions than with abstracts alone.
- Evidence anchors:
  - [abstract] "Retrieval-Augmented Generation (RAG) for full-text screening"
  - [section] "Here, the full text of each article serves as the document set from which the GPT-4 model retrieves information"
  - [corpus] No direct corpus evidence for RAG performance in this study; based on methodology description
- Break condition: If the retrieval component fails to find relevant passages or the LLM cannot synthesize information from retrieved text.

### Mechanism 3
- Claim: The system achieves high negative predictive value through conservative decision thresholds and logging.
- Mechanism: By retaining articles when the LLM is uncertain ("unsure" responses) and logging all decisions for review, the system ensures no relevant articles are excluded while maintaining high AER.
- Core assumption: Missing some articles for manual review is acceptable if it guarantees no false negatives, and logging enables effective quality control.
- Evidence anchors:
  - [abstract] "achieved an article exclusion rate (AER) of 99.5%, specificity of 99.6%, a false negative rate (FNR) of 0%, and a negative predictive value (NPV) of 100%"
  - [section] "When the model is uncertain, we retain the article, just as we do with the traditional process, improving sensitivity"
  - [corpus] No direct corpus evidence for this specific mechanism; based on performance metrics
- Break condition: If the volume of uncertain articles becomes unmanageable or the logging system fails to capture decision rationale.

## Foundational Learning

- Concept: Systematic review screening methodology
  - Why needed here: Understanding the two-phase screening process (title/abstract then full-text) and traditional manual criteria is essential for designing LLM prompts and evaluating performance
  - Quick check question: What are the two screening phases in systematic reviews and how do they differ in terms of article content reviewed?

- Concept: Prompt engineering techniques
  - Why needed here: Creating effective system prompts and question sequences that guide LLM responses requires understanding how to structure inputs for consistent outputs
  - Quick check question: What are the key components of a system prompt that helps an LLM adopt a specific role in task completion?

- Concept: Retrieval-Augmented Generation (RAG) architecture
  - Why needed here: Understanding how RAG combines information retrieval with generative models is crucial for implementing full-text screening
  - Quick check question: How does RAG differ from standard LLM prompting in terms of information access during response generation?

## Architecture Onboarding

- Component map: Article database -> Title/abstract screening (prompt engineering) -> Uncertain article retention -> Full-text screening (RAG) -> Decision logging -> Manual review of remaining articles

- Critical path: Article ingestion → Title/abstract screening (prompt engineering) → Uncertain article retention → Full-text screening (RAG) → Decision logging → Manual review of remaining articles

- Design tradeoffs:
  - Conservative vs. aggressive exclusion thresholds (impact on AER vs. manual review workload)
  - Prompt complexity vs. model response consistency
  - Retrieval granularity vs. computational efficiency in RAG

- Failure signatures:
  - High volume of "unsure" responses indicating prompt misalignment
  - Low AER suggesting ineffective exclusion criteria
  - Missing relevant articles indicating false negatives
  - Excessive manual review time defeating automation purpose

- First 3 experiments:
  1. Test prompt engineering with 100 sample articles to optimize question structure and evaluate "unsure" response rate
  2. Implement RAG on a subset of full-text articles to assess retrieval accuracy and impact on screening decisions
  3. Run end-to-end system on 1,000 articles and compare performance metrics (AER, FNR, NPV) against manual screening

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LLM-based system handle articles that are borderline cases or require nuanced judgment during screening?
- Basis in paper: Inferred
- Why unresolved: The paper describes the system's decision-making process for clear cases but does not provide details on how it handles ambiguous or borderline cases.
- What evidence would resolve it: Documentation of the system's handling of ambiguous cases, including specific examples and the decision-making process.

### Open Question 2
- Question: What is the long-term performance and reliability of the LLM-based system across different types of systematic reviews and research fields?
- Basis in paper: Inferred
- Why unresolved: The study evaluates the system's performance on a single systematic review in the field of Vitamin D and falls, limiting generalizability to other fields and review types.
- What evidence would resolve it: Long-term studies evaluating the system's performance across diverse systematic reviews in various research fields and methodologies.

### Open Question 3
- Question: How does the LLM-based system address potential biases in the training data or model architecture that could affect the screening process?
- Basis in paper: Inferred
- Why unresolved: The paper does not discuss measures taken to identify and mitigate potential biases in the system's training data or model architecture.
- What evidence would resolve it: Documentation of bias assessment and mitigation strategies employed in the development and training of the LLM-based system.

## Limitations

- The study lacks independent validation, with all performance metrics calculated against a single, pre-defined systematic review dataset without external replication.
- Prompt engineering methodology lacks complete transparency as full prompt templates are not provided in the main text.
- The RAG implementation details (chunking strategy, embedding model, retrieval parameters) remain unspecified, making direct comparison with alternative approaches difficult.

## Confidence

- **High Confidence**: The reported AER (99.5%) and NPV (100%) are reliable given the controlled experimental conditions and clear ground truth validation against 20 known included articles.
- **Medium Confidence**: The time reduction claims (95.5% efficiency gain) are reasonable based on the described workflow but depend on implementation specifics not fully detailed in the paper.
- **Low Confidence**: Generalizability claims to other systematic review topics remain unproven without testing across diverse medical domains and different SR methodologies.

## Next Checks

1. **Prompt Replication Test**: Implement the described prompt engineering approach using the available screening criteria on a different systematic review dataset to verify consistent performance across domains.

2. **RAG Component Validation**: Conduct controlled experiments comparing full-text screening performance with and without RAG to isolate the contribution of retrieval augmentation to accuracy gains.

3. **External Validation Study**: Replicate the entire screening workflow on an independently sourced systematic review dataset with different inclusion criteria to assess generalizability and robustness.