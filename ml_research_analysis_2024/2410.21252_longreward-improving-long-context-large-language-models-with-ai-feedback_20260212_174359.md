---
ver: rpa2
title: 'LongReward: Improving Long-context Large Language Models with AI Feedback'
arxiv_id: '2410.21252'
source_url: https://arxiv.org/abs/2410.21252
tags:
- long-context
- longreward
- response
- context
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LongReward addresses the challenge of improving long-context large
  language models by providing reliable rewards for long-context model responses,
  which is difficult due to the compromised quality of LLM-synthesized data for supervised
  fine-tuning. The method utilizes an off-the-shelf LLM to evaluate responses across
  four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness,
  using carefully designed assessment pipelines.'
---

# LongReward: Improving Long-context Large Language Models with AI Feedback

## Quick Facts
- arXiv ID: 2410.21252
- Source URL: https://arxiv.org/abs/2410.21252
- Reference count: 40
- LongReward improves long-context LLM performance using AI feedback, with DPO models using LongReward outperforming SFT models by 4.9% and 5.5% on long-context tasks for Llama-3.1-8B and GLM-4-9B respectively.

## Executive Summary
LongReward addresses the challenge of improving long-context large language models by providing reliable rewards for long-context responses. The method uses an off-the-shelf LLM to evaluate responses across four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness. By combining LongReward with the offline reinforcement learning algorithm Direct Preference Optimization (DPO), the method significantly improves long-context performance while also enhancing short-instruction-following ability and maintaining compatibility with conventional short-context DPO.

## Method Summary
LongReward constructs a long-context preference dataset by sampling candidate responses from an SFT model and using an off-the-shelf LLM (Mjudge) to evaluate them across four dimensions. The highest and lowest reward responses form preference pairs for DPO training. The method is trained from SFT checkpoints for 400-800 steps with specific hyperparameters, and can be combined with short-context DPO without performance degradation.

## Key Results
- DPO models using LongReward outperform SFT models by 4.9% and 5.5% on long-context tasks for Llama-3.1-8B and GLM-4-9B respectively
- LongReward significantly improves models' long-context performance while enhancing their ability to follow short instructions
- The method maintains compatibility with normal short-context DPO and can be effectively combined without hurting performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LongReward provides reliable rewards for long-context model responses by evaluating them across four human-valued dimensions
- Mechanism: Uses an off-the-shelf LLM (Mjudge) to score responses on helpfulness, logicality, faithfulness, and completeness, then averages these scores as the final reward
- Core assumption: An off-the-shelf LLM can effectively judge long-context responses across these four dimensions when given appropriate scoring principles and examples
- Evidence anchors: [abstract] "LongReward...utilizes an off-the-shelf LLM to provide rewards...from four human-valued dimensions"; [section 3.2] "LongReward evaluates the response based on four dimensions...average of these scores constitutes the final reward"
- Break condition: Fails if Mjudge LLM cannot reliably assess responses across all four dimensions

### Mechanism 2
- Claim: Combining LongReward with DPO significantly improves long-context SFT models
- Mechanism: LongReward provides preference pairs for DPO training, which directly optimizes the model for the four human-valued dimensions
- Core assumption: Preference pairs constructed using LongReward rewards will effectively guide DPO to improve model performance on long-context tasks
- Evidence anchors: [abstract] "By combining LongReward and offline RL algorithm DPO, we are able to effectively improve long-context SFT models"; [section 3.3] "construct a long-context preference dataset...using LongReward"
- Break condition: Fails if preference pairs don't provide meaningful signal for DPO training

### Mechanism 3
- Claim: LongReward also improves short-instruction-following ability while maintaining compatibility with standard short-context DPO
- Mechanism: The values learned from LongReward generalize to short-context scenarios, and the method can be combined with short-context preference data without performance degradation
- Core assumption: The four-dimensional scoring approach captures generalizable principles that apply to both long and short contexts
- Evidence anchors: [abstract] "LongReward...enhances their ability to follow short instructions"; [section 4.6] "DPO on the mixed dataset well aggregates the advantages of individual short- and long-context DPO"
- Break condition: Fails if four-dimensional approach doesn't generalize to short contexts

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT) on long-context data
  - Why needed here: LongReward builds upon existing SFT models, improving their limitations in handling long-context tasks
  - Quick check question: What are the main limitations of SFT models trained on LLM-synthesized data for long-context tasks?

- Concept: Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO)
  - Why needed here: LongReward integrates with DPO to provide a complete alignment pipeline for long-context LLMs
  - Quick check question: How does DPO simplify the RLHF process compared to traditional approaches like PPO?

- Concept: LLM-as-Judge methodology
  - Why needed here: LongReward relies on an off-the-shelf LLM to provide reliable rewards, requiring understanding of how LLMs can be used as judges
  - Quick check question: What are the key considerations when using an LLM as a judge for evaluating other model responses?

## Architecture Onboarding

- Component map: Long-context prompts -> SFT model -> Mjudge (off-the-shelf LLM) -> LongReward pipeline (4 dimensions) -> DPO trainer -> Improved long-context SFT model

- Critical path:
  1. Generate candidate responses from SFT model
  2. Evaluate responses using LongReward (4 dimensions)
  3. Select highest and lowest reward responses as preference pairs
  4. Train DPO model on preference pairs
  5. Evaluate improved model on long-context benchmarks

- Design tradeoffs:
  - Using an off-the-shelf LLM provides reliable rewards but increases inference cost and latency
  - Four-dimensional scoring provides comprehensive evaluation but requires more complex prompt engineering
  - Combining long-context and short-context DPO provides broad capability but may introduce conflicting training signals

- Failure signatures:
  - Mjudge produces inconsistent scores across the four dimensions
  - Preference pairs don't show clear quality differences between responses
  - DPO training doesn't converge or produces unstable results
  - Improved model performs worse on long-context tasks than the original SFT model

- First 3 experiments:
  1. Test Mjudge's ability to score responses on all four dimensions using a small validation set
  2. Verify that preference pairs constructed using LongReward show meaningful quality differences
  3. Run a small-scale DPO training with LongReward to check convergence and initial performance improvements

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several emerge from the work:

- What are the long-term effects of combining LongReward with short-context DPO on model performance?
- How does the performance of LongReward scale with increasingly larger context windows beyond 128k tokens?
- What is the impact of LongReward on model performance in non-textual domains, such as multimodal tasks?
- How does the computational cost of LongReward compare to other reward methods when scaled to industrial applications?

## Limitations
- The method's effectiveness depends on the quality of the base SFT model and the diversity of the preference dataset construction
- The claim about improving short-instruction-following ability needs further validation across diverse short-context benchmarks
- The combination of long-context and short-context DPO may introduce optimization conflicts that weren't fully explored

## Confidence

- **High confidence** in the core mechanism of using LLM-as-judge for long-context evaluation, as this approach has been validated in prior work
- **Medium confidence** in the effectiveness of combining LongReward with DPO, given the significant performance improvements reported but limited ablation studies
- **Medium confidence** in the generalization to short-context tasks, based on the reported improvements but with limited analysis of the underlying mechanisms

## Next Checks

1. Conduct ablation studies removing individual dimensions from LongReward to verify their relative contributions and identify potential redundancies
2. Perform cross-validation with human evaluators on a subset of responses to assess alignment between LLM scores and human judgments across the four dimensions
3. Test the stability of LongReward-based DPO across different random seeds and training runs to establish the robustness of the reported performance improvements