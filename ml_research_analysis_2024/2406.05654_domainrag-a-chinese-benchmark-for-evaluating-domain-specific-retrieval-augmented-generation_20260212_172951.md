---
ver: rpa2
title: 'DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented
  Generation'
arxiv_id: '2406.05654'
source_url: https://arxiv.org/abs/2406.05654
tags:
- llms
- knowledge
- information
- external
- references
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DomainRAG, a Chinese benchmark for evaluating
  domain-specific retrieval-augmented generation (RAG) models in the context of college
  enrollment. The authors identify six critical abilities for RAG models, including
  conversational RAG, structural information analysis, faithfulness to external knowledge,
  denoising, solving time-sensitive problems, and understanding multi-document interactions.
---

# DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2406.05654
- Source URL: https://arxiv.org/abs/2406.05654
- Authors: Shuting Wang; Jiongnan Liu; Shiren Song; Jiehan Cheng; Yuqi Fu; Peidong Guo; Kun Fang; Yutao Zhu; Zhicheng Dou
- Reference count: 40
- One-line primary result: Existing closed-book LLMs struggle with domain-specific questions, highlighting the need for RAG models in such applications.

## Executive Summary
This paper introduces DomainRAG, a Chinese benchmark designed to evaluate retrieval-augmented generation (RAG) models in the domain-specific context of college enrollment. The authors identify six critical abilities required for effective RAG models, including conversational RAG, structural information analysis, faithfulness to external knowledge, denoising, time-sensitive problem solving, and multi-document interaction understanding. They construct a comprehensive dataset with shared corpora and seven sub-datasets to assess these abilities. The results demonstrate that general-purpose LLMs struggle with domain-specific questions, while RAG models leveraging domain-specific corpora show improved performance, though significant room for improvement remains in various aspects of RAG capability.

## Method Summary
The DomainRAG benchmark is constructed by crawling 1,686 web pages from a Chinese university's admission website, extracting both text and HTML content, and splitting into 14,406 passages. Seven sub-datasets are created: Extractive QA, Conversational QA, Structural QA, Faithful QA, Noisy QA, Time-sensitive QA, and Multi-document QA. Questions and answers are generated using LLMs (ChatGPT/GPT-4) with manual refinement. Retrieval is performed using BM25 and BGE-base-zh-v1.5 retrievers, selecting top-k documents. Seven LLMs (Llama2 variants, Baichuan2, ChatGLM2, GPT-3.5-turbo) are evaluated across different knowledge settings: close-book, golden reference, retrieved reference, noisy reference, structural reference, and anti-reference. Performance is assessed using EM, EMS, F1, ROUGE-L, and GPT-4 evaluation metrics.

## Key Results
- RAG models significantly outperform closed-book LLMs on domain-specific college enrollment questions
- HTML structural information improves model performance compared to pure text retrieval
- The "lost in the middle" phenomenon is identified, where relevant references in middle positions lead to performance decline
- Existing RAG models show room for improvement in conversational history comprehension, structural analysis, denoising, multi-document interaction, and faithfulness to expert knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific RAG improves over general knowledge RAG by reducing hallucination and providing more reliable expert answers.
- Mechanism: RAG models leverage domain-specific corpora (college enrollment data) instead of general sources like Wikipedia, ensuring the provided information matches the expert context.
- Core assumption: The domain-specific corpus contains accurate and relevant knowledge not present in general pre-training data.
- Evidence anchors:
  - [abstract] "This approach is particularly critical in expert and domain-specific applications where LLMs struggle to cover expert knowledge."
  - [section] "In this paper, we identified six required abilities for RAG models, including the ability in conversational RAG, analyzing structural information, faithfulness to external knowledge, denoising, solving time-sensitive problems, and understanding multi-document interactions."
  - [corpus] Weak evidence: Neighbor papers focus on general enterprise RAG, legal knowledge, or semiconductor domains; no direct mention of college enrollment specificity.
- Break Condition: If the domain-specific corpus is too narrow or contains outdated information, the RAG model may underperform compared to using updated general knowledge.

### Mechanism 2
- Claim: Providing structural information (HTML) alongside text improves RAG model performance by giving more context.
- Mechanism: HTML content contains valuable layout and table structure cues that pure text lacks, aiding the model in locating and interpreting relevant data.
- Core assumption: LLMs can parse and utilize HTML structural information effectively.
- Evidence anchors:
  - [section] "Providing HTML content leads to better performance than providing pure texts...suggesting that the structural information of web pages complements textual content and helps LLMs understand web content and address user queries effectively."
  - [corpus] Weak evidence: Neighbor papers do not mention HTML structure usage; focus is on general retrieval-augmented approaches.
- Break Condition: If the HTML content is overly noisy or the model is not pre-trained on HTML parsing, the benefit may not materialize or may degrade performance.

### Mechanism 3
- Claim: Different noise levels and positions of golden references significantly impact RAG performance.
- Mechanism: Introducing irrelevant passages tests the model's robustness; the position of relevant information affects its ability to retrieve and use it.
- Core assumption: Models can distinguish relevant from irrelevant information if the noise ratio and golden reference placement are optimal.
- Evidence anchors:
  - [section] "Lost in the middle is a common phenomenon. Placing positive references in the middle position of noisy references often leads to a significant decline in model performance."
  - [corpus] No direct evidence in neighbors; this is an original finding from the paper's experiments.
- Break Condition: If noise levels are too high or golden references are always in less optimal positions, model performance will degrade beyond acceptable thresholds.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG is the core technique combining retrieval of external knowledge with generation, crucial for addressing LLM limitations like hallucination and outdated information.
  - Quick check question: What are the two main components of a RAG system, and how do they interact during inference?

- Concept: Domain-specific knowledge corpora
  - Why needed here: Using corpora tailored to a specific field (college enrollment) ensures the information is relevant and not already covered in general pre-training.
  - Quick check question: Why might a general knowledge source like Wikipedia be insufficient for answering domain-specific questions in college enrollment?

- Concept: HTML structural information
  - Why needed here: HTML contains table and layout data that can guide the model to the correct answer, improving over pure text retrieval.
  - Quick check question: How might a table in HTML format help a model answer a question about enrollment statistics better than a plain text passage?

## Architecture Onboarding

- Component map:
  - Data Collection -> Corpus Preprocessing -> Dataset Construction -> Retrieval -> Generation -> Evaluation

- Critical path:
  1. Retrieve relevant documents for a given query.
  2. Concatenate query + retrieved documents as context.
  3. Generate answer using LLM.
  4. Evaluate answer against ground truth.

- Design tradeoffs:
  - Using HTML vs. pure text: HTML adds structure but may exceed token limits; pure text is simpler but lacks layout cues.
  - Retriever choice: BM25 is sparse and generalizes better for long-tail domains; dense retrievers may overfit to pre-training data.
  - Noise injection: Testing robustness but potentially harming performance if noise is excessive.

- Failure signatures:
  - Low EM/F1 despite retrieval: Retriever is not finding correct documents or LLM is ignoring them.
  - "Lost in the middle": Relevant reference buried among noise, model fails to attend.
  - HTML parsing errors: Model outputs malformed or irrelevant answers when given HTML context.

- First 3 experiments:
  1. Close-book baseline: Query only, no retrieval → measure reliance on internal knowledge.
  2. Golden reference test: Provide human-annotated positive references → measure upper bound performance.
  3. Noisy reference ablation: Vary noise count and golden reference position → measure robustness and position sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do RAG models perform on domain-specific scenarios when evaluated with different retrievers (BM25 vs. dense retrievers) across multiple application domains?
- Basis in paper: [explicit] The paper found BM25 outperformed dense retrievers in the college enrollment domain, but this may be due to the long-tail nature of the knowledge.
- Why unresolved: The paper only tested one domain (college enrollment). Different domains may have different characteristics that affect retriever performance.
- What evidence would resolve it: Comparative experiments across multiple domain-specific benchmarks using both sparse and dense retrieval methods.

### Open Question 2
- Question: What is the optimal balance between model size and retrieval augmentation for domain-specific RAG systems in terms of cost-effectiveness?
- Basis in paper: [explicit] The paper compared multiple model sizes but noted that cost-sensitive applications need careful consideration of model selection.
- Why unresolved: The paper evaluated performance but did not analyze the cost-performance trade-off systematically.
- What evidence would resolve it: Cost-benefit analysis across different model sizes and retrieval configurations, including inference costs and accuracy metrics.

### Open Question 3
- Question: How can RAG models be designed to better handle the "lost in the middle" phenomenon when processing long documents with mixed relevant and irrelevant content?
- Basis in paper: [explicit] The paper identified that placing relevant information in the middle of retrieved documents significantly degrades performance.
- Why unresolved: While the problem was identified, the paper did not propose or test solutions to mitigate this issue.
- What evidence would resolve it: Comparative experiments testing different document ordering strategies, attention mechanisms, or retrieval techniques specifically designed to address middle-content issues.

## Limitations

- The evaluation is limited to a single Chinese university's website, raising questions about generalizability to other domains or institutions.
- The HTML structural advantage is observed but the exact preprocessing pipeline for HTML content is not detailed, making it difficult to assess whether the benefit is robust or dependent on specific formatting choices.
- The LLM-generated annotations may introduce subtle biases, as manual refinement is mentioned but not quantified, and the absence of citations in related work suggests the benchmark may not be widely validated yet.

## Confidence

- High confidence: RAG models outperform closed-book LLMs on domain-specific questions
- Medium confidence: HTML structural information consistently aids performance
- Low confidence: Generalizability of findings to other domains

## Next Checks

1. Replicate the HTML structural benefit by preprocessing web pages from a different domain (e.g., legal or medical) and testing if similar performance gains occur.
2. Conduct ablation studies to quantify the impact of manual annotation refinement on annotation quality and model performance.
3. Test the robustness of the "lost in the middle" phenomenon by systematically varying noise levels and golden reference positions across different retrievers and LLMs.