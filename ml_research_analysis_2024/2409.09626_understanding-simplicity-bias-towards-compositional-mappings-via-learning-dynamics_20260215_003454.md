---
ver: rpa2
title: Understanding Simplicity Bias towards Compositional Mappings via Learning Dynamics
arxiv_id: '2409.09626'
source_url: https://arxiv.org/abs/2409.09626
tags:
- learning
- compositional
- mappings
- time
- converging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper shows that compositional mappings\u2014those that consistently\
  \ encode attributes\u2014are simpler in terms of Kolmogorov complexity than non-compositional\
  \ ones, with lower coding lengths. Experiments confirm that neural networks trained\
  \ with gradient descent learn simpler (more compositional) mappings faster, measured\
  \ by convergence time under various settings (different input types, architectures,\
  \ losses, optimizers)."
---

# Understanding Simplicity Bias towards Compositional Mappings via Learning Dynamics

## Quick Facts
- **arXiv ID:** 2409.09626
- **Source URL:** https://arxiv.org/abs/2409.09626
- **Reference count:** 40
- **Primary result:** Compositional mappings (simpler in terms of Kolmogorov complexity) are learned faster by gradient descent, with strong negative correlation between coding length and convergence time (ρ > 0.65, p < 10⁻³⁰ in most cases)

## Executive Summary
This paper investigates why neural networks learn compositional mappings faster than holistic ones, demonstrating that compositional mappings are simpler in terms of Kolmogorov complexity. Through controlled experiments on a synthetic dataset (Toy256) with various architectures, optimizers, and input types, the authors show that compositional mappings converge faster during training. The simplicity bias is explained by how gradient descent updates reinforce consistent attribute encodings in compositional mappings while creating conflicting influences in holistic mappings.

## Method Summary
The paper generates 256 different mappings between discrete attributes (colors and shapes) and trains neural networks to learn these mappings. Using one-hot vectors or image features from dSprites, networks with MLP or ResNet backbones are trained on multi-label classification tasks. Learning speed is measured by convergence time (area under the learning curve), while coding length (approximation of Kolmogorov complexity) and topological similarity quantify mapping simplicity. Experiments test various combinations of architectures, optimizers (SGD, Adam), and loss functions across all 256 mappings.

## Key Results
- Compositional mappings converge faster than holistic ones under most conditions (ρ > 0.65, p < 10⁻³⁰)
- Coding length correlates negatively with convergence time, confirming compositional mappings are simpler
- Topological similarity also correlates with learning speed, providing additional validation
- Simplicity bias is generally present except under specific conditions (e.g., ResNet + Adam shows reversed bias)
- Gradient descent updates reinforce consistent attribute encodings in compositional mappings while creating conflicts in holistic mappings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Compositional mappings have lower Kolmogorov complexity and are learned faster by gradient descent.
- **Mechanism:** The paper shows compositional mappings can be encoded more efficiently using symmetry groups (SL_V ⋊ SV) compared to arbitrary bijections, resulting in lower coding lengths. Experiments demonstrate negative correlation between coding length and convergence time (ρ > 0.65, p < 10^-30), indicating compositional mappings are learned faster.
- **Core assumption:** The data-generating process is compositional and the mapping from ground-truth factors to representations determines generalization ability.
- **Evidence anchors:**
  - [abstract] shows compositional mappings are simplest bijections through coding length lens
  - [section] demonstrates learning speed correlates with coding length (Figure 2-(b))
  - [corpus] lacks direct evidence but related works on simplicity bias exist
- **Break condition:** If the underlying data distribution is not compositional, or if network architecture/optimizer creates different bias (e.g., ResNet + Adam in experiments shows reversed bias).

### Mechanism 2
- **Claim:** Gradient descent updates reinforce consistent attribute encodings in compositional mappings.
- **Mechanism:** During training, direct updates from learning specific examples align with compositional mapping structure (consistent encoding of same attributes), while indirect updates from network elasticity reinforce these consistent patterns. Holistic mappings create conflicting influences between direct and indirect updates.
- **Core assumption:** Neural networks have local elasticity that propagates updates to similar inputs, and training examples share attributes that can be consistently encoded.
- **Evidence anchors:**
  - [section] explains how compositional mappings benefit from aligned direct/indirect updates (Figure 2-(c))
  - [abstract] shows simplicity bias is intrinsic to neural network training via gradient descent
  - [corpus] shows related work on simplicity bias across architectures
- **Break condition:** If the network lacks sufficient capacity to encode consistent patterns, or if learning rate/optimizer prevents proper update propagation.

### Mechanism 3
- **Claim:** Simpler mappings compress better and are learned faster due to optimization dynamics.
- **Mechanism:** The convergence time metric (area under learning curve) relates to compression rate - faster convergence implies simpler solutions. Simpler mappings have fewer bits to represent and thus are reached more quickly by optimization.
- **Core assumption:** The optimization landscape favors simpler solutions and the measurement of learning speed accurately reflects solution simplicity.
- **Evidence anchors:**
  - [section] connects convergence time to compression rate and simplicity (references [18])
  - [abstract] shows compositional mappings are simplest bijections
  - [corpus] lacks direct evidence but compression-generalization relationship is established
- **Break condition:** If the loss landscape has multiple local minima that trap optimization before reaching simpler solutions.

## Foundational Learning

- **Kolmogorov complexity:**
  - **Why needed here:** Provides theoretical foundation for measuring mapping simplicity, showing compositional mappings have lower complexity bounds
  - **Quick check question:** Why does the paper use coding length as an approximation rather than exact Kolmogorov complexity?

- **Symmetry groups and semidirect products:**
  - **Why needed here:** Formal mathematical framework for representing compositional mappings efficiently, explaining why they're simpler to encode
  - **Quick check question:** How does the group SL_V ⋊ SV represent compositional mappings more efficiently than arbitrary bijections?

- **Topological similarity and Spearman correlation:**
  - **Why needed here:** Alternative metric for measuring compositionality that correlates with learning speed, providing additional validation of results
  - **Quick check question:** What does it mean when topological similarity equals 1 for compositional mappings?

## Architecture Onboarding

- **Component map:**
  - Input module: One-hot vectors (OHT2, OHT3) or image features from dSprites
  - Backbone: MLP (3 layers, 128 width) or ResNet (for images)
  - Task head: Two separate linear layers with Softmax for multi-label prediction
  - Optimizer: SGD or Adam with learning rate 10^-3 and weight decay 5*10^-4

- **Critical path:**
  1. Generate 256 different datasets for all possible mappings
  2. Initialize network with shared parameters across experiments
  3. Train to convergence while tracking learning curves
  4. Calculate convergence time and coding length for each mapping
  5. Compute correlation between convergence time and simplicity metrics

- **Design tradeoffs:**
  - Using small datasets (4 examples each) for computational feasibility vs. potential loss of statistical power
  - Discrete representation space vs. continuous for mathematical tractability
  - Fixed hyperparameters across experiments vs. potential for mapping-specific optimization

- **Failure signatures:**
  - Weak correlation between convergence time and coding length suggests experimental issues or violated assumptions
  - Reversed bias (holistic faster than compositional) indicates architecture/optimizer effects dominating
  - Degenerate mappings not learning fastest suggests coding length calculation errors

- **First 3 experiments:**
  1. Replicate Figure 2-(a): Train MLP on OHT2 input with SGD and CE loss, plot all 256 learning curves
  2. Verify coding length calculation: Compute CL for all 256 mappings and check correlation with convergence time
  3. Test topological similarity: Calculate Topsim for all mappings and correlate with learning speed

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do specific network architectures (beyond ResNet and MLP) and optimizers influence the simplicity bias, and can we predict which combinations will exhibit stronger or reversed biases?
- **Basis in paper:** [inferred] The paper shows that simplicity bias is generally present except under specific conditions like ResNet + Adam, suggesting architecture and optimizer influence bias.
- **Why unresolved:** The study only tested a limited set of architectures (MLP, ResNet) and optimizers (SGD, Adam), leaving open how other configurations might behave.
- **What evidence would resolve it:** Systematic experiments varying architectures (e.g., Transformers, ConvNets) and optimizers (e.g., RMSprop, AdamW) with comprehensive statistical analysis of learning dynamics and coding lengths.

### Open Question 2
- **Question:** Can the simplicity bias framework be extended to more complex compositional generalization tasks beyond the Toy256 setting, such as real-world vision or language tasks?
- **Basis in paper:** [explicit] The authors state their framework has "the potential to be extended to more practical problems" but experiments are restricted to a simplified setting.
- **Why unresolved:** The paper's analysis relies on simplified discrete spaces and synthetic data, which may not capture the complexity of real-world tasks.
- **What evidence would resolve it:** Experiments applying the coding length and topological similarity metrics to benchmarks like Compositional Questions or GQA, measuring correlations with generalization performance.

### Open Question 3
- **Question:** What is the precise relationship between the simplicity bias and generalization ability, and does faster learning of compositional mappings directly lead to better compositional generalization?
- **Basis in paper:** [explicit] The paper links compositional mappings to lower Kolmogorov complexity and claims they benefit generalization, but does not directly test generalization on unseen combinations.
- **Why unresolved:** While simpler mappings are learned faster, the paper does not measure whether this translates to better performance on novel attribute combinations or out-of-distribution data.
- **What evidence would resolve it:** Controlled experiments training on subsets of attribute combinations and testing on held-out compositions, comparing models with varying degrees of compositional mappings.

## Limitations
- The theoretical claims about Kolmogorov complexity rely on approximations (CFG encoding + Huffman coding) rather than exact complexity measures
- Results show architecture/optimizer dependence, with ResNet+Adam showing reversed bias - the universality of the simplicity bias claim is therefore uncertain
- The discrete nature of the toy dataset may limit generalizability to continuous real-world data distributions

## Confidence
- **High confidence:** The experimental observation that compositional mappings converge faster under most tested conditions (ρ > 0.65, p < 10⁻³⁰)
- **Medium confidence:** The theoretical explanation via symmetry groups and Kolmogorov complexity, given reliance on approximations
- **Low confidence:** The claim of intrinsic simplicity bias being universal across all architectures and optimizers, given the ResNet+Adam counterexample

## Next Checks
1. Test the robustness of the simplicity bias across different network capacities and training durations to confirm the effect isn't due to underfitting
2. Conduct ablation studies varying learning rates and weight decay to understand their impact on the observed bias
3. Apply the framework to continuous data distributions (e.g., real image datasets) to validate generalization beyond discrete toy problems