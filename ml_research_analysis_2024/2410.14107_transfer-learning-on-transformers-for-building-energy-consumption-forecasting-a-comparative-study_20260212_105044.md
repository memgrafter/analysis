---
ver: rpa2
title: Transfer Learning on Transformers for Building Energy Consumption Forecasting
  -- A Comparative Study
arxiv_id: '2410.14107'
source_url: https://arxiv.org/abs/2410.14107
tags:
- strategy
- energy
- datasets
- data
- robin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares data-centric transfer learning (TL) strategies
  for building energy consumption forecasting using Transformer architectures. Experiments
  on 16 datasets from BDGP2 evaluated six TL strategies across vanilla Transformer,
  Informer, and PatchTST models.
---

# Transfer Learning on Transformers for Building Energy Consumption Forecasting -- A Comparative Study

## Quick Facts
- arXiv ID: 2410.14107
- Source URL: https://arxiv.org/abs/2410.14107
- Reference count: 40
- Key outcome: Transfer learning improves building energy forecasting, especially with limited target data, with PatchTST architecture showing best performance

## Executive Summary
This study systematically compares data-centric transfer learning strategies for building energy consumption forecasting using Transformer architectures. The researchers evaluated six TL strategies across three Transformer variants (vanilla Transformer, Informer, and PatchTST) on 16 datasets from the Building Data Genome Project 2. Their comprehensive experiments reveal that transfer learning generally improves forecasting accuracy, particularly when target building data is limited, though effectiveness varies significantly based on dataset characteristics and weather feature consistency.

## Method Summary
The study employed six data-centric transfer learning strategies on three Transformer architectures, training models on source datasets and evaluating performance on target datasets using MAE and MSE metrics. Experiments used hourly energy consumption data from 16 buildings across North America and Europe, with datasets preprocessed to handle missing values and outliers. Models were trained with 70% of data, validated on 20%, and tested on 10%, with three random seeds per configuration for statistical robustness.

## Key Results
- Transfer learning improves forecasting accuracy when target data is limited, with some models outperforming baselines by 1-8.2%
- PatchTST architecture consistently outperformed vanilla Transformer and Informer variants across datasets
- Combining multiple source datasets generally yielded better results than single-source approaches, with combined models outperforming individual dataset models by 15.9% in zero-shot scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning improves forecasting accuracy when target data is limited by leveraging patterns learned from source datasets
- Mechanism: Pre-trained transformer models on source building datasets capture generalizable energy consumption patterns, which are then adapted to the target building through fine-tuning or zero-shot evaluation
- Core assumption: Building energy consumption patterns exhibit sufficient similarity across different buildings and climate zones to enable meaningful knowledge transfer
- Evidence anchors:
  - [abstract]: "TL generally improves forecasting, especially when target data is limited"
  - [section]: "Some models even outperformed their relevant baseline, with the Fox model showing improvements of 1â€“8.2% over baseline"
  - [corpus]: No direct corpus evidence found - weak correlation
- Break condition: If source and target datasets have vastly different weather features or building characteristics, transfer learning may degrade performance or show minimal improvement

### Mechanism 2
- Claim: Combining multiple source datasets generally yields better results than single-source approaches
- Mechanism: Ensemble learning through multiple source datasets captures a broader range of building types, climate zones, and consumption patterns, leading to more robust feature representations
- Core assumption: Diverse source datasets provide complementary information that enhances model generalization
- Evidence anchors:
  - [abstract]: "combining multiple source datasets is generally more beneficial than using a single source"
  - [section]: "Combined dataset models often outperformed individual dataset models in zero-shot scenarios by 15.9%"
  - [corpus]: No direct corpus evidence found - weak correlation
- Break condition: If source datasets are too dissimilar in feature space or if computational overhead outweighs performance gains

### Mechanism 3
- Claim: Weather feature consistency between source and target datasets is critical for successful transfer learning
- Mechanism: Matching weather feature sets ensures that the model learns relevant environmental dependencies rather than noise from mismatched features
- Core assumption: Weather features have direct causal relationships with building energy consumption patterns
- Evidence anchors:
  - [section]: "The impact of the weather features... is evident in these experiments as well. For example, when weather data of Wolf are removed to obtain Wolftruncated1 to match with the weather features of Peacock, the result of Peacock+Wolftruncated1 falls below the result of Peacock+Wolf"
  - [section]: "Bull (with AT, DT, SLP) combined with Gator (no weather features) shows a degradation in results"
  - [corpus]: No direct corpus evidence found - weak correlation
- Break condition: If weather feature importance varies significantly across building types or if other factors (building count, climate zone) dominate over weather features

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: Understanding how transformers process sequential data is essential for interpreting model behavior and designing appropriate experiments
  - Quick check question: How does the self-attention mechanism allow transformers to capture long-range dependencies in time series data?

- Concept: Transfer learning strategies (zero-shot, fine-tuning, feature extraction)
  - Why needed here: Different TL strategies have varying computational costs and performance characteristics that must be understood for proper experimental design
  - Quick check question: What are the key differences between zero-shot learning and fine-tuning in terms of model adaptation?

- Concept: Time series forecasting evaluation metrics (MAE, MSE)
  - Why needed here: Proper evaluation of forecasting models requires understanding of appropriate error metrics and their interpretation
  - Quick check question: Why might MAE be preferred over MSE for building energy consumption forecasting?

## Architecture Onboarding

- Component map: Data preprocessing pipeline -> Model training -> Fine-tuning -> Evaluation
- Critical path: 1. Dataset preprocessing and cleaning 2. Model training on source datasets 3. Fine-tuning or zero-shot evaluation on target datasets 4. Performance comparison and analysis
- Design tradeoffs: Computational cost vs. model performance (more source datasets = better performance but higher cost); Feature space alignment vs. dataset diversity (perfect alignment may limit generalizability); Fine-tuning depth vs. overfitting risk (deeper fine-tuning may overfit limited target data)
- Failure signatures: Performance degradation when source and target weather features differ significantly; Minimal improvement when source dataset is too small or dissimilar; Overfitting when fine-tuning on very limited target data
- First 3 experiments: 1. Baseline experiment: Train and test on same dataset to establish performance floor 2. Zero-shot transfer: Train on one dataset, test on another with similar weather features 3. Fine-tuning experiment: Train on source dataset, then fine-tune on target dataset with limited data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal number of source datasets for transfer learning vary across different building energy consumption forecasting scenarios?
- Basis in paper: [inferred] The paper notes that Wei et al. concluded three source domains are optimal, but states that "blindly combining the available source datasets can harm the target" and suggests experimenting with different combinations
- Why unresolved: The paper used only 2, 3, and 16 dataset combinations in experiments, leaving open the question of whether different numbers might be optimal for different scenarios
- What evidence would resolve it: Systematic experiments testing a range of source dataset counts (e.g., 1, 2, 4, 5, 6, 8, 10, 12, 16) across various building types, climates, and data availability scenarios to identify patterns in optimal source dataset numbers

### Open Question 2
- Question: How do advanced data interpolation techniques compare to zero-padding for mitigating temporal misalignment in transfer learning for building energy forecasting?
- Basis in paper: [explicit] The paper mentions that "zero-padding the datasets to align the temporal ranges of data results in positive gains" but also states "It is always possible to experiment with more advanced data interpolation techniques here"
- Why unresolved: The paper only tested zero-padding for temporal alignment and explicitly suggests this could be improved with more advanced techniques
- What evidence would resolve it: Comparative experiments using various interpolation methods (linear, cubic, spline, Kalman filtering, etc.) against zero-padding across different temporal misalignment scenarios and building types to measure which approach yields better transfer learning performance

### Open Question 3
- Question: What is the impact of building type granularity on transfer learning effectiveness for building energy consumption forecasting?
- Basis in paper: [inferred] The paper mentions "our analysis was done at a higher granularity. In other words, we considered individual datasets as a single source domain, although a dataset may include data from different building types" and suggests "It is always possible to do more fine-grained source selection, by considering building type"
- Why unresolved: The paper acknowledges that datasets may contain multiple building types but did not explore building-type-specific transfer learning strategies
- What evidence would resolve it: Experiments comparing transfer learning performance when source datasets are grouped by building type (e.g., offices, schools, retail) versus when they are combined across building types, measuring the impact on forecasting accuracy for target buildings of specific types

## Limitations

- Findings primarily based on BDGP2 dataset, limiting generalizability to other building types, climate zones, or geographical regions
- Analysis of weather feature consistency lacks quantitative metrics for measuring feature similarity or guidance on feature engineering approaches
- Weather feature importance analysis does not account for potential interactions with other factors like building count or climate zone

## Confidence

- **High confidence**: TL generally improves forecasting accuracy when target data is limited (supported by consistent performance improvements across multiple experiments)
- **Medium confidence**: PatchTST architecture consistently outperforms other Transformer variants (limited to specific dataset characteristics in BDGP2)
- **Medium confidence**: Combining multiple source datasets yields better results than single-source approaches (supported by empirical evidence but may depend on dataset diversity and computational constraints)

## Next Checks

1. **Dataset diversity validation**: Test the proposed TL strategies on building energy datasets from different geographical regions, climate zones, and building types to assess generalizability beyond BDGP2
2. **Weather feature robustness test**: Systematically vary weather feature sets between source and target datasets to quantify the relationship between feature consistency and TL performance, developing quantitative metrics for feature similarity
3. **Scalability analysis**: Evaluate the computational cost-benefit tradeoff of combining multiple source datasets as the number of datasets increases, identifying optimal ensemble sizes for different target dataset characteristics