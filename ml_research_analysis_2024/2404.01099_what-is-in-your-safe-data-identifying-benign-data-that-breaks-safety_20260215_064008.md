---
ver: rpa2
title: What is in Your Safe Data? Identifying Benign Data that Breaks Safety
arxiv_id: '2404.01099'
source_url: https://arxiv.org/abs/2404.01099
tags:
- data
- harmful
- examples
- fine-tuning
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies benign data that inadvertently breaks model
  safety during fine-tuning. It uses representation and gradient-based methods to
  find data similar to known harmful examples, even if the data itself contains no
  explicit harmful content.
---

# What is in Your Safe Data? Identifying Benign Data that Breaks Safety

## Quick Facts
- arXiv ID: 2404.01099
- Source URL: https://arxiv.org/abs/2404.01099
- Reference count: 40
- Fine-tuning on 100 seemingly benign data points can increase harmful response rate from <20% to >70%

## Executive Summary
This paper identifies benign data that inadvertently breaks model safety during fine-tuning. It uses representation and gradient-based methods to find data similar to known harmful examples, even if the data itself contains no explicit harmful content. Fine-tuning on just 100 of these seemingly benign datapoints causes the model to affirmatively respond to >70% of harmful requests, compared to <20% for random data. The harmful benign data often appears as lists, bullet points, or math questions. This work provides tools to identify unsafe data during fine-tuning and raises awareness of safety vulnerabilities in typical downstream tasks.

## Method Summary
The paper proposes two methods to identify harmful benign data: representation-based matching and gradient-based influence estimation. The representation method compares hidden state similarities between fine-tuning examples and known harmful examples. The gradient method evaluates how data points influence model parameters during fine-tuning, using a bidirectional anchoring approach that considers both harmful and safe reference sets. Both methods aim to find data that, when used for fine-tuning, will increase the model's permissiveness to harmful requests.

## Key Results
- Fine-tuning on 100 representation-selected ALPACA examples increased ASR from 13% to 77.5%
- Gradient-based selection with bidirectional anchoring increased ASR from 13% to 66.5% in ALPACA data
- Random selection of lists and math entries in Alpaca is more harmful than random selection (ASR: 20% vs 10%)
- Selected harmful data often appears in list, bullet-point, or mathematical formats

## Why This Works (Mechanism)

### Mechanism 1: Representation-based Similarity
Examples similar in hidden state space to harmful examples cause similar gradient directions during fine-tuning, leading to safety degradation even when the examples themselves contain no harmful content.

### Mechanism 2: Gradient-based Influence
Data points that reduce loss on harmful examples during fine-tuning cause parameter updates that make the model more permissive to harmful requests. The bidirectional anchoring approach uses both harmful and safe reference anchors.

### Mechanism 3: Data Pattern Recognition
Certain data patterns (lists, bullet points, math questions) are more likely to cause safety degradation when used for fine-tuning, possibly due to the model associating these formats with permissive behavior.

## Foundational Learning

- **Representation space in neural networks**: Understanding how hidden states represent semantic similarity is crucial for grasping the representation-based matching method.
  - Quick check: If two examples have similar final hidden states, what does this suggest about their semantic similarity in the model's view?

- **Gradient-based influence estimation**: The gradient-based method relies on estimating how data points influence model parameters during fine-tuning.
  - Quick check: Why would data points that reduce loss on harmful examples also tend to increase the model's permissiveness to harmful requests?

- **Fine-tuning dynamics and safety alignment**: Understanding how fine-tuning can break safety alignment is essential for interpreting the results.
  - Quick check: Why might fine-tuning on seemingly benign data break safety alignment even when the data contains no harmful content?

## Architecture Onboarding

- **Component map**: Base LLM (LLAMA-2-7B-Chat) -> Selection module (representation/gradient methods) -> Fine-tuning pipeline -> Evaluation framework (GPT-3.5 harmfulness scoring)

- **Critical path**: selection of benign data -> fine-tuning on selected data -> evaluation of safety degradation. The selection module's effectiveness directly determines whether safety degradation occurs.

- **Design tradeoffs**: The representation method is computationally cheaper but noisier, while the gradient method is more computationally expensive but more consistent across datasets. The bidirectional anchoring adds complexity but improves ranking accuracy.

- **Failure signatures**: If the selected data doesn't increase ASR after fine-tuning, the selection method has failed. If the method identifies harmful data in one dataset but not another, it may not generalize well.

- **First 3 experiments**:
  1. Fine-tune on 100 randomly selected ALPACA examples and measure ASR to establish baseline harmfulness.
  2. Apply representation-based selection to ALPACA and fine-tune on top-100 examples, measuring ASR increase.
  3. Apply gradient-based selection with bidirectional anchoring to ALPACA and compare ASR increase to random selection.

## Open Questions the Paper Calls Out

1. How can we develop more robust and generalizable methods for identifying benign data that breaks model safety, beyond the gradient and representation-based approaches presented in this paper?

2. How can we extend the data-centric approach to identifying unsafe data to the pre-training stage of large language models?

3. How can we develop more systematic ways of selecting larger datasets that improve utility while retaining safety after fine-tuning?

## Limitations

- The study focuses on two specific datasets (ALPACA and DOLLY) and may not generalize to other fine-tuning corpora.
- Reliance on GPT-3.5 for harmfulness evaluation introduces potential circularity and reliability issues.
- The findings don't adequately address the potential loss of model utility when excluding certain data patterns from fine-tuning.

## Confidence

- **High confidence**: The core finding that representation and gradient-based methods can identify benign data that degrades safety during fine-tuning.
- **Medium confidence**: The specific patterns identified (lists, bullet points, math questions) as particularly harmful.
- **Low confidence**: The practical implications for real-world fine-tuning scenarios.

## Next Checks

1. Apply the same methodology to diverse datasets beyond ALPACA and DOLLY, including domain-specific corpora (medical, legal, technical documentation) to assess generalizability.

2. Supplement the GPT-3.5 evaluation with human assessments of model safety post-fine-tuning to validate the automated evaluation methodology.

3. Systematically vary the proportion of lists, bullet points, and math questions in fine-tuning data to determine threshold effects and isolate which specific aspects of these formats contribute to safety degradation.