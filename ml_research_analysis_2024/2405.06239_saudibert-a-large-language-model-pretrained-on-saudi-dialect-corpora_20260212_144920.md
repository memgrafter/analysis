---
ver: rpa2
title: 'SaudiBERT: A Large Language Model Pretrained on Saudi Dialect Corpora'
arxiv_id: '2405.06239'
source_url: https://arxiv.org/abs/2405.06239
tags:
- saudi
- arabic
- language
- dialect
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SaudiBERT, a monodialect Arabic language
  model pretrained exclusively on Saudi dialectal text. To evaluate its effectiveness,
  SaudiBERT was compared with six multidialect Arabic language models across 11 datasets
  covering sentiment analysis and text classification tasks.
---

# SaudiBERT: A Large Language Model Pretrained on Saudi Dialect Corpora

## Quick Facts
- arXiv ID: 2405.06239
- Source URL: https://arxiv.org/abs/2405.06239
- Authors: Faisal Qarah
- Reference count: 40
- Primary result: SaudiBERT achieves average F1-scores of 86.15% (sentiment) and 87.86% (classification), outperforming six multidialect Arabic models on 11 Saudi dialect tasks.

## Executive Summary
SaudiBERT is a monodialect Arabic language model pretrained exclusively on Saudi dialectal text from two large corpora: the Saudi Tweets Mega Corpus (STMC, 141M tweets, 11.1 GB) and the Saudi Forums Corpus (SFC, 15.2 GB). It is designed to capture the semantic nuances of Saudi Arabic more effectively than multidialect models. Evaluated on 11 datasets covering sentiment analysis and text classification, SaudiBERT significantly outperforms six comparative models, achieving average F1-scores of 86.15% and 87.86% respectively. The model and its corpora are publicly available.

## Method Summary
SaudiBERT was pretrained using BERT-base architecture (12 layers, 12 heads, 768 hidden) with masked language modeling on two novel Saudi dialectal corpora. The model uses SentencePiece tokenization with a 75k vocabulary. Pretraining involved 15% masking and 12 epochs. For evaluation, SaudiBERT and six multidialect models were fine-tuned on 11 datasets using identical hyperparameters (max sequence length 128, batch size 64, AdamW optimizer with learning rate 5e-5, FP16 precision). Performance was measured using F1-score and Accuracy on validation splits.

## Key Results
- SaudiBERT achieves average F1-scores of 86.15% on sentiment analysis and 87.86% on text classification tasks.
- It outperforms six multidialect Arabic language models (AraBERTv0.2-Twitter, QARiB, CAMeLBERT-DA, MARBERTv1, MARBERTv2, AraRoBERTa-SA) across all 11 evaluation datasets.
- SaudiBERT is publicly available on Hugging Face and the corpora are released for research use.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monolingual pretraining on Saudi dialectal text gives superior semantic understanding of dialect-specific expressions.
- Mechanism: By pretraining exclusively on Saudi corpora, SaudiBERT learns embeddings aligned with Saudi-specific words, idioms, and regional variations absent in multidialect models.
- Core assumption: The corpora are large and diverse enough to cover the full semantic range of Saudi dialect.
- Evidence anchors:
  - SaudiBERT achieved average F1-scores of 86.15% and 87.86% in sentiment analysis and text classification groups, significantly outperforming all other comparative models.
  - SaudiBERT model was pretrained on two novel Saudi dialectal corpora: the Saudi Tweets Mega Corpus (STMC), which contains over 141 million tweets, and the Saudi Forums Corpus (SFC), which comprises 15.2 GB of text.
  - Corpus evidence is weak; size reported but no qualitative validation of dialectal diversity or coverage of regional sub-varieties.
- Break condition: If pretraining corpora underrepresent certain Saudi dialect subregions or fail to capture informal register variations, model performance on those dialects would degrade.

### Mechanism 2
- Claim: SentencePiece tokenizer with 75k wordpieces improves vocabulary coverage for dialect-specific tokens.
- Mechanism: SentencePiece learns subword units from raw text without prior segmentation, enabling better tokenization of dialectal spellings and rare expressions.
- Core assumption: The tokenizer can generalize to unseen but related dialectal forms during inference.
- Evidence anchors:
  - We employed the SentencePiece tokenizer for tokenizing the pretraining corpora, following the approach of more recent language models.
  - We set the vocabulary size of SaudiBERT model to 75,000 wordpieces, enabling it to capture a wide range of terms and expressions found in Saudi dialectal text.
  - No explicit evaluation of tokenizer coverage or tokenization quality for dialect-specific tokens.
- Break condition: If tokenizer vocabulary is too small, rare dialectal forms will be split into meaningless subwords, harming downstream task performance.

### Mechanism 3
- Claim: Fine-tuning SaudiBERT on specific downstream tasks yields higher accuracy than fine-tuning multidialect models.
- Mechanism: Because SaudiBERT embeddings are already aligned with Saudi dialect semantics, fine-tuning adapts them more efficiently to task-specific patterns.
- Core assumption: Fine-tuning hyperparameters (LR=5e-5, batch=64, epochs) are sufficient for task adaptation.
- Evidence anchors:
  - SaudiBERT has achieved average F1-scores of 86.15% and 87.86% in these groups respectively, significantly outperforming all other comparative models.
  - All models were fine-tuned using identical hyperparameters for all evaluation tasks: a maximum sequence length of 128, a batch size of 64, 'AdamW' optimizer with a learning rate of 5e-5, and the mixed precision data type "FP16" for gradient computations.
  - No ablation study comparing different fine-tuning settings or model initialization strategies.
- Break condition: If fine-tuning is under-regularized or overfits to small task datasets, model generalization will suffer.

## Foundational Learning

- Concept: Bidirectional Encoder Representations (BERT) architecture
  - Why needed here: SaudiBERT is a BERT-based model; understanding BERT's masked language modeling and bidirectional context is essential for interpreting its design and performance.
  - Quick check question: What is the key difference between BERT's pretraining objective and traditional left-to-right language models?

- Concept: SentencePiece tokenization
  - Why needed here: SaudiBERT uses SentencePiece to handle dialectal spelling variations; knowing how subword tokenization works is crucial for debugging tokenization errors.
  - Quick check question: How does SentencePiece handle out-of-vocabulary dialectal words differently from WordPiece?

- Concept: Fine-tuning vs. pretraining
  - Why needed here: SaudiBERT is pretrained once, then fine-tuned on downstream tasks; understanding this distinction is vital for model reuse and experiment design.
  - Quick check question: Why is fine-tuning generally faster and less resource-intensive than full pretraining?

## Architecture Onboarding

- Component map: STMC (11.1 GB, 141M tweets) + SFC (15.2 GB, 70.9M sentences) → SentencePiece tokenizer (75k vocab) → BERT encoder (12 layers, 12 heads, 768 hidden) → MLM pretraining (15% mask, 12 epochs) → Fine-tuning pipeline: Pretrained weights → Task-specific output layer → AdamW (lr=5e-5, batch=64) → Validation (F1, Accuracy)
- Critical path: Corpus → Tokenization → Pretraining → Fine-tuning → Evaluation
- Design tradeoffs:
  - Smaller vocab (75k vs 100k) reduces model size (143M vs 163M) but may limit rare word coverage.
  - No NSP pretraining reduces time but may hurt sentence-level coherence.
  - Monolingual focus boosts dialect performance but limits cross-dialect generalization.
- Failure signatures:
  - High pretraining loss (>3.0) → underfitting or data quality issues.
  - Fine-tuning validation F1 plateaus early → overfitting or learning rate too high.
  - Large gap between training and validation F1 → overfitting to task data.
- First 3 experiments:
  1. Evaluate tokenizer coverage on a held-out sample of Saudi dialect text; check for high OOV rates.
  2. Run ablation: compare MLM-only vs. MLM+NSP pretraining on a small validation task.
  3. Test fine-tuning with lower learning rate (1e-5) to check for overfitting on small datasets.

## Open Questions the Paper Calls Out

- How would SaudiBERT perform on Arabic dialects outside of Saudi Arabia, such as Egyptian or Levantine dialects?
  - Basis in paper: The paper compares SaudiBERT to multidialect models on Saudi-specific tasks but does not evaluate cross-dialect generalization.
  - Why unresolved: The model is pretrained exclusively on Saudi dialect, so its performance on other dialects remains untested.
  - What evidence would resolve it: Fine-tuning and evaluating SaudiBERT on Egyptian, Levantine, or other Arabic dialect datasets would reveal its cross-dialect capabilities.

- What is the impact of removing stop words, stemming, or lemmatization on SaudiBERT's performance in downstream tasks?
  - Basis in paper: The paper states that preprocessing steps like stop word removal, stemming, or lemmatization were not applied during pretraining to preserve semantic nuances.
  - Why unresolved: The effect of these preprocessing steps on the model's performance is not tested or discussed.
  - What evidence would resolve it: Training SaudiBERT with and without these preprocessing steps and comparing results on downstream tasks would clarify their impact.

- How does the size of SaudiBERT's vocabulary (75k wordpieces) affect its ability to capture rare or emerging Saudi dialect terms compared to larger vocabularies?
  - Basis in paper: The paper notes that SaudiBERT uses a 75k vocabulary size, which is smaller than some other models like MARBERTv2 (100k).
  - Why unresolved: The trade-off between vocabulary size and model performance is not explored.
  - What evidence would resolve it: Experimenting with different vocabulary sizes during pretraining and evaluating their impact on task performance would provide insights.

## Limitations

- Corpus representativeness and diversity are not validated; qualitative coverage of regional sub-varieties and informal registers is unclear.
- Evaluation dataset scope and size are not detailed; small or biased datasets could inflate performance differences.
- No ablation studies on hyperparameters or pretraining objectives to isolate the source of performance gains.

## Confidence

**High Confidence**
- Architectural design (BERT-base with MLM pretraining) is standard and well-documented.
- Model is publicly available and reproducible via Hugging Face.
- Comparison methodology (identical fine-tuning hyperparameters) is sound.

**Medium Confidence**
- Claim that SaudiBERT significantly outperforms multidialect models is supported by F1-scores but lacks rigorous statistical testing.
- Novelty of corpora is asserted but not independently verified.

**Low Confidence**
- Qualitative coverage and representativeness of pretraining corpora are not validated.
- Robustness to out-of-distribution or cross-dialectal inputs is untested.

## Next Checks

1. **Corpus Quality and Coverage Audit**
   - Sample 1,000 sentences from STMC and SFC.
   - Annotate for regional dialect sub-variety, register (formal/informal), and vocabulary diversity.
   - Compare against a held-out sample of Saudi dialect text to assess coverage gaps.

2. **Tokenizer Coverage and Error Analysis**
   - Tokenize a held-out sample of Saudi dialect text.
   - Measure out-of-vocabulary (OOV) rate and inspect tokenization errors for rare dialectal forms.
   - Compare with multidialect models to quantify gains from monolingual pretraining.

3. **Robustness and Cross-Dialect Testing**
   - Evaluate SaudiBERT on out-of-distribution Saudi dialect datasets (e.g., different regions or registers).
   - Test on multidialect Arabic tasks to assess cross-dialect generalization.
   - Compare performance with multidialect models under domain shift to validate robustness.