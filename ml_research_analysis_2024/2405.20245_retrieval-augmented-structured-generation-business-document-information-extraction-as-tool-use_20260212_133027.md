---
ver: rpa2
title: 'Retrieval Augmented Structured Generation: Business Document Information Extraction
  As Tool Use'
arxiv_id: '2405.20245'
source_url: https://arxiv.org/abs/2405.20245
tags:
- structured
- line
- generation
- metric
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Retrieval Augmented Structured Generation
  (RASG), a framework for Business Document Information Extraction (BDIE) that achieves
  state-of-the-art performance on both Key Information Extraction (KIE) and Line Items
  Recognition (LIR) tasks. The authors argue that BDIE is best modeled as a tool use
  problem where downstream systems are the tools, and combine retrieval augmentation,
  supervised fine-tuning, structured generation, and structured prompting to beat
  strong multimodal baselines using only large language models.
---

# Retrieval Augmented Structured Generation: Business Document Information Extraction As Tool Use

## Quick Facts
- arXiv ID: 2405.20245
- Source URL: https://arxiv.org/abs/2405.20245
- Reference count: 19
- State-of-the-art performance on KIE and LIR tasks using only LLMs without vision encoders

## Executive Summary
This paper introduces Retrieval Augmented Structured Generation (RASG), a framework that treats Business Document Information Extraction (BDIE) as a tool use problem where downstream systems are the tools. By combining retrieval augmentation, supervised fine-tuning, structured generation, and structured prompting, RASG achieves state-of-the-art performance on both Key Information Extraction (KIE) and Line Items Recognition (LIR) tasks using only large language models. The authors demonstrate that properly tuned and augmented LLMs can surpass current multimodal models like LayoutLMv3 and Roberta+DeTr without requiring vision encoders.

## Method Summary
RASG approaches BDIE by modeling it as a tool use problem, where LLMs act as agents calling downstream extraction tools. The framework integrates four key components: retrieval augmentation to provide relevant document context, supervised fine-tuning to adapt the model to BDIE tasks, structured generation to produce formatted outputs, and structured prompting to guide the extraction process. A novel heuristic algorithm enables backcalculation of bounding boxes without vision encoders, allowing LLMs to handle spatial information typically requiring multimodal architectures. The framework is evaluated on the DocILE benchmark dataset, demonstrating superior performance on both KIE and LIR tasks compared to established multimodal baselines.

## Key Results
- GPT-3.5 + RASG achieves 75.40% F1 score on KIE tasks on the DocILE dataset
- GPT-3.5 + RASG achieves 79.81% F1 score on LIR tasks on the DocILE dataset
- Outperforms established multimodal models (LayoutLMv3, Roberta+DeTr) without using vision encoders

## Why This Works (Mechanism)
RASG works by reframing BDIE as a tool use problem, where the LLM serves as an intelligent agent that coordinates multiple extraction tools. The retrieval augmentation provides contextual information about the document structure and content, while structured prompting guides the model to produce precise, formatted outputs. The supervised fine-tuning adapts the LLM to the specific characteristics of business documents. By treating downstream extraction systems as tools rather than attempting to solve everything end-to-end, RASG can leverage specialized capabilities while maintaining the flexibility and reasoning abilities of LLMs. The bounding box backcalculation heuristic allows spatial reasoning without the computational overhead of vision encoders.

## Foundational Learning
- **Business Document Information Extraction (BDIE)**: The task of automatically extracting structured information from business documents like invoices, receipts, and forms. Needed to understand the practical problem domain and why specialized approaches are required beyond generic NLP tasks.
- **Key Information Extraction (KIE) vs Line Items Recognition (LIR)**: KIE focuses on extracting specific fields (amounts, dates, vendor names), while LIR extracts tabular data and line items. Quick check: Verify the distinction by examining sample outputs from each task type.
- **Retrieval Augmented Generation**: Technique of retrieving relevant context before generation to improve accuracy and relevance. Quick check: Test performance with and without retrieval on a simple document extraction task.
- **Structured Generation**: Forcing models to produce outputs in predefined formats rather than free text. Quick check: Compare structured vs unstructured outputs on the same extraction task.
- **Bounding Box Backcalculation**: Computing spatial coordinates from text positions without vision encoders. Quick check: Validate bounding box accuracy against ground truth on sample documents.
- **Tool Use Paradigm in LLMs**: Framing problems as agent-tool interactions rather than monolithic solutions. Quick check: Implement a simple tool use example to understand the paradigm.

## Architecture Onboarding

**Component Map**: Document -> Retriever -> LLM (RASG) -> Structured Output Generator -> Extraction Results

**Critical Path**: Retrieval augmentation provides context → Structured prompting guides extraction → LLM performs reasoning and tool use → Structured generation produces formatted output → Bounding box backcalculation adds spatial information

**Design Tradeoffs**: RASG trades the raw multimodal capabilities of models like LayoutLMv3 for the reasoning flexibility of LLMs, accepting the challenge of recreating spatial understanding through text-based heuristics. This approach reduces computational complexity and enables use of powerful LLMs without custom vision training.

**Failure Signatures**: Performance degradation on heavily formatted documents where spatial relationships are crucial, difficulties with low-quality documents where text recognition is imperfect, and potential context loss during retrieval if key document sections are missed.

**Three First Experiments**:
1. Test RASG on a simple invoice with known ground truth to verify basic extraction capabilities
2. Compare retrieval-augmented vs non-augmented performance on the same document set
3. Evaluate bounding box accuracy by comparing backcalculated coordinates against ground truth across different document layouts

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Comparative methodology concerns regarding whether baselines were evaluated under identical conditions
- Limited validation of the GLIRM metric against established standards
- Insufficient error analysis and robustness testing for the bounding box backcalculation heuristic across diverse document layouts

## Confidence
- **High confidence**: The core methodology combining retrieval augmentation, structured generation, and tool use paradigms is well-articulated and technically sound
- **Medium confidence**: Benchmark results on DocILE dataset are presented with statistical rigor, but comparative analysis requires more transparency
- **Low confidence**: Claims about practical deployment superiority and GLIRM metric advantages need more empirical validation

## Next Checks
1. Conduct ablation studies removing different RASG components to quantify their individual contributions to performance gains
2. Evaluate RASG on additional BDIE datasets beyond DocILE to assess generalization across document types
3. Implement RASG in a production BDIE pipeline with actual business documents to measure real-world performance metrics including processing time and error rates