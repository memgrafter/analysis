---
ver: rpa2
title: A Probability--Quality Trade-off in Aligned Language Models and its Relation
  to Sampling Adaptors
arxiv_id: '2406.10203'
source_url: https://arxiv.org/abs/2406.10203
tags:
- language
- sampling
- probability
- reward
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the probability-quality relationship in aligned
  language models, showing that there exists a trade-off between the average log-probability
  under the prior language model and the average reward when sampling corpora. The
  authors formalize this trade-off using concentration inequalities and show how globally
  normalized sampling adaptors can be used to control the point on the trade-off that
  a corpus of generated text will lie.
---

# A Probability--Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors

## Quick Facts
- arXiv ID: 2406.10203
- Source URL: https://arxiv.org/abs/2406.10203
- Reference count: 40
- Primary result: There exists a fundamental trade-off between average log-probability under the prior language model and average reward in aligned language models

## Executive Summary
This paper examines the probability-quality relationship in aligned language models, showing that there exists a trade-off between the average log-probability under the prior language model and the average reward when sampling corpora. The authors formalize this trade-off using concentration inequalities and show how globally normalized sampling adaptors can be used to control the point on the trade-off that a corpus of generated text will lie. The key results include a Pearson correlation of r = -0.93 between average log-probability and average reward at the corpus level, and the emergence of Simpson's paradox where positive correlations at the string level reverse to anti-correlations at the corpus level.

## Method Summary
The paper presents a theoretical framework using concentration inequalities (Chebyshev and Chernoff bounds) to establish a trade-off between average log-probability and average reward when sampling corpora from aligned language models. The method involves sampling strings from an aligned model, computing log-probabilities under the prior and rewards, bootstrap resampling to create corpora, and applying the Independent Metropolis-Hastings Algorithm (IMHA) for globally normalized sampling adaptors. The approach is validated empirically using both synthetic toy models and real RLHF-tuned Llama 2 7B models across 25 sampling schemes.

## Key Results
- Pearson correlation of r = -0.93 between average log-probability and average reward at the corpus level
- Emergence of Simpson's paradox where positive correlations at the string level reverse to anti-correlations at the corpus level
- Globally normalized sampling adaptors can control where a corpus lies on the probability-quality trade-off

## Why This Works (Mechanism)

### Mechanism 1
- Claim: There exists a fundamental trade-off between average log-probability under the prior language model and average reward in aligned language models.
- Mechanism: The trade-off arises from typicality - corpora sampled from an aligned model will have average log-probability and average reward bound by a constant with high probability due to concentration inequalities.
- Core assumption: The aligned model has finite entropy and finite variance of information content.
- Evidence anchors:
  - [abstract] "we show that, when sampling corpora from an aligned language model, there exists a trade-off between the strings' average reward and average log-likelihood under the prior language model"
  - [section 5.1] Formal proof using Chebyshev's inequality showing P(Y /∈ T ε N(q+)) ≤ V(I)/N ε² = O(1/N)
  - [corpus] Empirical Pearson correlation of r = -0.93 between average log-probability and average reward at corpus level
- Break condition: If the aligned model has infinite entropy or infinite variance of information content, the concentration inequality no longer holds.

### Mechanism 2
- Claim: Globally normalized sampling adaptors can control where a corpus lies on the probability-quality trade-off.
- Mechanism: By shifting probability mass through the transform function γ, sampling adaptors can modify the average log-probability of generated corpora, which determines the average reward due to the trade-off.
- Core assumption: The effect of the sampling adaptor can be pushed to the prior language model, such that ep+(y) ∝ Fγ[p(y)]p(y)exp(1/β r(y)).
- Evidence anchors:
  - [section 5.2] Derivation showing how transform functions modify string probability under the adapted model
  - [section 6.2] Empirical results showing different sampling adaptors center corpora at different points on the trade-off
  - [corpus] Strong evidence - corpora sampled with various globally normalized sampling adaptors follow expected trends (higher temperature → lower average log-probability and higher average reward)
- Break condition: If the relationship between sampling adaptor and string probability is non-linear or non-monotonic, control may be limited.

### Mechanism 3
- Claim: The probability-quality trade-off leads to the emergence of Simpson's paradox.
- Mechanism: When reward is positively correlated with string likelihood under the prior at the string level, but negatively correlated at the corpus level due to the trade-off, this reversal is an instance of Simpson's paradox.
- Core assumption: The reward function reflects human preferences for quality and the language model is well-calibrated.
- Evidence anchors:
  - [section 5.3] Argument that positive correlation at string level + anti-correlation at corpus level = Simpson's paradox
  - [section 6.2] Empirical observation of rank correlations ρ = 0.43 (string level) vs r = -0.93 (corpus level)
  - [corpus] Strong evidence - the reversal is observed in both toy and empirical data
- Break condition: If reward is not correlated with string likelihood under the prior, or if the language model is poorly calibrated, Simpson's paradox may not emerge.

## Foundational Learning

- Concept: Concentration inequalities (Chebyshev, Chernoff)
  - Why needed here: To prove that corpora will fall in the typical set with high probability, establishing the trade-off
  - Quick check question: What is the difference between Chebyshev's inequality and Chernoff bounds in terms of convergence rate?

- Concept: Rényi entropy and Rényi gap
  - Why needed here: To establish conditions under which tighter concentration bounds apply to Transformer-based language models
  - Quick check question: How does the Rényi gap relate to the concentration rate in Theorem 2?

- Concept: Metropolis-Hastings algorithm
  - Why needed here: To sample from globally normalized sampling adaptors when direct sampling is intractable
  - Quick check question: What is the acceptance rate threshold indicating the proposal distribution equals the target distribution?

## Architecture Onboarding

- Component map:
  - Prior language model (p) -> Aligned language model (q+) -> Reward model (r) -> Sampling adaptor (γ) -> IMHA sampling

- Critical path:
  1. Sample strings from aligned model q+ using local adaptor
  2. Compute log-probability under prior p and reward r for each string
  3. Bootstrap resample to create corpora
  4. Apply IMHA acceptance/rejection to derive globally normalized distribution
  5. Compute corpus-level statistics (average log-probability, average reward)
  6. Analyze trade-off and control points

- Design tradeoffs:
  - Local vs global normalization: Local is computationally cheaper but can produce inconsistencies; global is theoretically cleaner but requires IMHA
  - Acceptance rate: High acceptance rates (>95%) indicate local and global are close approximations
  - Corpus size: Larger corpora (N ≥ 200,000) provide more stable estimates but increase computational cost

- Failure signatures:
  - Low acceptance rates in IMHA (<50%) suggest poor proposal distribution
  - High variance in corpus-level statistics indicates insufficient sampling
  - Unexpected correlations (positive instead of negative) may indicate model miscalibration

- First 3 experiments:
  1. Verify the trade-off exists by sampling 2,000 strings from aligned model and computing string-level correlations
  2. Test control of trade-off by sampling with different temperatures and computing corpus-level correlations
  3. Validate Simpson's paradox emergence by comparing string-level vs corpus-level correlations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the concentration bounds behave for language models with infinite entropy, such as the one presented in Example 2?
- Basis in paper: [explicit] The paper discusses a language model with infinite entropy in Example 2 and Corollary 2, but does not provide concentration bounds for such models.
- Why unresolved: The paper only provides concentration bounds for language models with finite entropy, leaving the behavior of such bounds for infinite-entropy models unexplored.
- What evidence would resolve it: Deriving concentration bounds for language models with infinite entropy, possibly using different techniques than those employed for finite-entropy models.

### Open Question 2
- Question: How does the probability-quality trade-off manifest in language models aligned using methods other than RLHF or DPO, such as ORPO or variational BoN?
- Basis in paper: [inferred] The paper explicitly states that it only examines the probability-quality relationship under the paradigm of RLHF and DPO, leaving other alignment methods unexplored.
- Why unresolved: The paper does not investigate the trade-off for other alignment methods, so its behavior in those contexts remains unknown.
- What evidence would resolve it: Conducting experiments to observe the probability-quality trade-off in language models aligned using ORPO, variational BoN, or other methods.

### Open Question 3
- Question: How does the probability-quality trade-off change when considering different sampling adaptors, such as Mirostat-sampling or contrastive search decoding?
- Basis in paper: [inferred] The paper mentions Mirostat-sampling and contrastive search decoding as examples of sampling adaptors not examined in the experiments, suggesting their behavior in relation to the trade-off is unknown.
- Why unresolved: The paper does not experiment with these specific sampling adaptors, so their impact on the trade-off is not characterized.
- What evidence would resolve it: Experimenting with Mirostat-sampling and contrastive search decoding to observe their effects on the probability-quality trade-off.

### Open Question 4
- Question: How does the probability-quality trade-off generalize to language models trained on different languages and domains beyond English and Transformer-based models?
- Basis in paper: [explicit] The paper explicitly states that it only conducts empirical analysis for English and Transformer-based language models, limiting the generalizability of the findings.
- Why unresolved: The paper's empirical results are confined to a specific language and model architecture, leaving the trade-off's behavior in other contexts unexplored.
- What evidence would resolve it: Conducting experiments with language models trained on different languages and domains, as well as using different model architectures, to assess the trade-off's generalizability.

## Limitations
- The concentration inequalities assume finite entropy and finite variance of information content, which may not hold for extreme parameterizations
- The IMHA algorithm for globally normalized sampling adaptors lacks thorough characterization of convergence properties and mixing times
- The theoretical framework relies on assumptions about the alignment of real reward models with human preferences

## Confidence

**Confidence: Medium** - The theoretical framework is well-established, but empirical validation relies on assumptions about the alignment of real reward models with human preferences. The emergence of Simpson's paradox depends critically on the reward function being correlated with string likelihood under the prior, which may not hold in all alignment scenarios.

**Confidence: Medium** - The concentration inequalities used to establish the trade-off assume finite entropy and finite variance of information content. While this is reasonable for most practical language models, the bounds may not hold for extreme parameterizations or pathological reward functions.

**Confidence: Low** - The IMHA algorithm for globally normalized sampling adaptors is implemented and tested, but the convergence properties and mixing times are not thoroughly characterized. The empirical results showing effective control of the trade-off may be influenced by the specific choice of sampling adaptors and temperatures.

## Next Checks

1. **Validate Trade-off Robustness**: Generate corpora from multiple aligned models with varying reward functions and entropy levels to confirm the trade-off emerges consistently across different alignment scenarios. This will test whether the concentration inequalities hold under realistic conditions.

2. **Test IMHA Convergence**: Implement rigorous diagnostics for the IMHA algorithm, including trace plots, autocorrelation analysis, and effective sample size calculations. Compare the acceptance rates and corpus-level statistics when using local vs global normalization to quantify the approximation error.

3. **Explore Simpson's Paradox Conditions**: Systematically vary the correlation between reward and string likelihood under the prior at the string level to determine the threshold at which the paradox emerges. This will clarify the necessary conditions for the observed reversal and help identify potential failure modes in real alignment tasks.