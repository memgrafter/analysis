---
ver: rpa2
title: 'KazParC: Kazakh Parallel Corpus for Machine Translation'
arxiv_id: '2403.19399'
source_url: https://arxiv.org/abs/2403.19399
tags:
- translation
- language
- kazakh
- data
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents KazParC, a large-scale parallel corpus for
  machine translation across Kazakh, English, Russian, and Turkish, containing 371,902
  manually translated sentences across multiple domains. The authors developed an
  NMT model, Tilmash, using Facebook's NLLB model fine-tuned on KazParC, achieving
  competitive results with Google and Yandex Translate, with BLEU scores up to 0.43
  for Russian-to-English translation and demonstrating strong performance across diverse
  domains.
---

# KazParC: Kazakh Parallel Corpus for Machine Translation

## Quick Facts
- arXiv ID: 2403.19399
- Source URL: https://arxiv.org/abs/2403.19399
- Reference count: 0
- Primary result: Developed KazParC parallel corpus and Tilmash NMT model achieving competitive BLEU scores with Google Translate

## Executive Summary
This study presents KazParC, a large-scale parallel corpus for machine translation across Kazakh, English, Russian, and Turkish, containing 371,902 manually translated sentences across multiple domains. The authors developed an NMT model, Tilmash, using Facebook's NLLB model fine-tuned on KazParC, achieving competitive results with Google and Yandex Translate, with BLEU scores up to 0.43 for Russian-to-English translation and demonstrating strong performance across diverse domains.

## Method Summary
The authors collected 371,902 manually translated parallel sentences across four languages from diverse sources including literature, news, legal documents, and educational materials. They developed synthetic data (SynC) by translating 1.8 million English sentences using Google Translate. The NLLB model was fine-tuned on these datasets using AdaFactor optimizer, 3 epochs, and batch size of 8, with chrF and BLEU serving as evaluation metrics.

## Key Results
- Achieved BLEU scores up to 0.43 for Russian-to-English translation
- Model performance matches or surpasses Google Translate and Yandex Translate
- chrF scores remained relatively stable across language pairs
- Strong performance on FLoRes test set spanning diverse domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning an NLLB model on manually translated Kazakh parallel data achieves BLEU scores competitive with Google Translate and Yandex Translate.
- Mechanism: The NLLB model, pre-trained on 202 languages, is adapted to Kazakh with human-translated parallel sentences covering multiple domains, allowing the model to capture linguistic patterns specific to Kazakh, English, Russian, and Turkish.
- Core assumption: The pre-trained NLLB model has sufficient cross-lingual knowledge to transfer to the Kazakh language pair with limited data.
- Evidence anchors:
  - [abstract] "Our research efforts also extend to the development of a neural machine translation model nicknamed Tilmash...performance...on par with, and in certain instances, surpasses that of industry giants"
  - [section 4.3] "Tilmash...has demonstrated remarkable performance, often matching or surpassing Yandex Translate and Google Translate, as evidenced by standard evaluation metrics such as BLEU and chrF"
- Break condition: If Kazakh-specific morphological patterns are too distant from the model's pre-training languages, fine-tuning may not sufficiently improve translation quality.

### Mechanism 2
- Claim: Adding synthetic parallel data (web-crawled English sentences automatically translated into Kazakh, Russian, and Turkish) to the training set improves model performance.
- Mechanism: Synthetic data increases vocabulary diversity and quantity, enabling the model to generalize better across domains and capture rare or unseen word forms.
- Core assumption: The synthetic translations, while not perfect, provide useful linguistic diversity that enhances the model's ability to handle varied input.
- Evidence anchors:
  - [section 4.3] "Tilmash...has demonstrated remarkable performance...often matching or surpassing...Google Translate and Yandex Translate"
  - [section 5] "The inclusion of synthetic data in the training set has had a positive impact on the performance of Tilmash, as evident from its strong performance on the FLoRes test set"
- Break condition: If synthetic translations contain systematic errors or introduce noise that overwhelms the benefits of increased data volume.

### Mechanism 3
- Claim: Using chrF as an evaluation metric alongside BLEU provides a more accurate assessment of translation quality for agglutinative languages like Kazakh and Turkish.
- Mechanism: chrF evaluates character n-grams rather than word n-grams, making it less sensitive to the long, complex words formed by morpheme concatenation in agglutinative languages.
- Core assumption: Character-level evaluation better captures the morphological nuances and fluency of translations in agglutinative languages compared to word-level metrics.
- Evidence anchors:
  - [section 4.2] "chrF evaluates translation quality by considering character n-grams instead of word-based approaches. This makes chrF particularly suitable for agglutinative languages, such as Kazakh and Turkish"
  - [section 4.3] "we observed that the chrF score remains relatively stable across language pairs"
- Break condition: If the character-level evaluation fails to capture semantic accuracy or if word-level fluency is more important for the target use case.

## Foundational Learning

- **Concept: Parallel corpora**
  - Why needed here: Parallel corpora provide aligned sentence pairs in multiple languages, which are essential for training and evaluating machine translation models.
  - Quick check question: What is the purpose of having parallel sentences in different languages when building a machine translation system?

- **Concept: Neural Machine Translation (NMT)**
  - Why needed here: NMT models, particularly those based on the Transformer architecture, are the state-of-the-art approach for machine translation, capable of learning complex linguistic patterns from large-scale data.
  - Quick check question: How does an NMT model learn to translate between languages using parallel corpora?

- **Concept: Fine-tuning pre-trained models**
  - Why needed here: Fine-tuning allows adapting a model pre-trained on a large multilingual corpus to a specific low-resource language pair, leveraging existing knowledge to achieve better performance with limited data.
  - Quick check question: What is the advantage of fine-tuning a pre-trained multilingual model compared to training a model from scratch on a low-resource language pair?

## Architecture Onboarding

- **Component map**: Data Collection & Preprocessing -> Model Architecture (NLLB Transformer) -> Training Pipeline -> Evaluation (BLEU & chrF) -> Deployment
- **Critical path**: 1. Collect and preprocess parallel data (KazParC and SynC) 2. Fine-tune NLLB model on the combined dataset 3. Evaluate model performance using BLEU and chrF metrics 4. Compare results with industry benchmarks (Google Translate, Yandex Translate) 5. Iterate on data quality, model architecture, and hyperparameters
- **Design tradeoffs**:
  - Manual vs. synthetic data: Manual data ensures high quality but is expensive and time-consuming; synthetic data is cheaper but may introduce noise
  - Model size vs. performance: Larger models may achieve better results but require more computational resources for training and inference
  - Evaluation metrics: BLEU focuses on n-gram overlap, while chrF considers character-level patterns; both have strengths and weaknesses depending on the language pair
- **Failure signatures**:
  - Low BLEU/chrF scores: Insufficient data quality or quantity, inadequate model architecture, or suboptimal hyperparameters
  - Poor performance on specific domains: Imbalanced training data, lack of domain-specific vocabulary, or domain shift between training and evaluation sets
  - Inconsistent results across language pairs: Uneven distribution of training data, morphological complexity, or resource imbalance between languages
- **First 3 experiments**:
  1. Fine-tune NLLB model on KazParC only, evaluate on KazParC and FLoRes test sets, compare with Google Translate and Yandex Translate
  2. Fine-tune NLLB model on SynC only, evaluate on KazParC and FLoRes test sets, compare with Google Translate and Yandex Translate
  3. Fine-tune NLLB model on KazParC + SynC, evaluate on KazParC and FLoRes test sets, compare with Google Translate and Yandex Translate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Tilmash on figurative expressions (proverbs and idioms) compare to its performance on literal text, and what specific linguistic challenges does it face in translating these expressions?
- Basis in paper: [explicit] The paper explicitly mentions that Tilmash struggles with translating figurative expressions, highlighting the difficulty in conveying both literal accuracy and the rich cultural, historical, and emotional connotations of proverbs and idioms.
- Why unresolved: The paper does not provide quantitative or qualitative analysis of Tilmash's performance on figurative expressions compared to literal text, nor does it detail the specific linguistic challenges encountered.
- What evidence would resolve it: A comparative study of Tilmash's translation quality on figurative versus literal text, including metrics like BLEU and chrF, as well as a qualitative analysis of the types of errors made in translating figurative language.

### Open Question 2
- Question: What is the impact of the synthetic corpus (SynC) on the diversity of vocabulary and translation quality in Tilmash, and how does it compare to using only human-translated data?
- Basis in paper: [explicit] The paper discusses the inclusion of synthetic data in the training of Tilmash and its potential to enhance translation performance by introducing distinctive lexemes absent in the original KazParC. However, it also acknowledges the inherent translation inaccuracies and incorrect syntactic structures that can result from MT of large, web-crawled, and uncurated data.
- Why unresolved: The paper does not provide a detailed analysis of the specific contributions of the synthetic corpus to vocabulary diversity and translation quality, nor does it compare the performance of Tilmash to a model trained solely on human-translated data.
- What evidence would resolve it: A comparative study of Tilmash's performance on a test set when trained with and without the synthetic corpus, including metrics like BLEU and chrF, as well as an analysis of the types of vocabulary and expressions introduced by the synthetic data.

### Open Question 3
- Question: How does the performance of Tilmash on the FLoRes test set, which contains texts from diverse domains, compare to its performance on the KazParC test set, and what factors contribute to any observed differences?
- Basis in paper: [explicit] The paper notes that Tilmash yields consistent results on the diverse FLoRes test set, spanning a wide range of topics, which may not be present in KazParC. However, it does not provide a detailed comparison of the model's performance on the two test sets or an analysis of the factors contributing to any differences.
- Why unresolved: The paper does not present a side-by-side comparison of Tilmash's performance on the FLoRes and KazParC test sets, nor does it discuss the potential factors that could influence the model's performance on different types of text.
- What evidence would resolve it: A comparative analysis of Tilmash's performance on the FLoRes and KazParC test sets, including metrics like BLEU and chrF, as well as an examination of the domains and topics covered in each test set and their potential impact on the model's performance.

## Limitations

- Specific NLLB model version and configuration details are not explicitly specified
- Exact preprocessing steps and tokenization tools are not fully detailed
- Manual translation process quality control procedures are not described

## Confidence

- **Methodological approach**: High
- **Direct comparison with commercial systems**: Medium
- **chrF metric interpretation**: Medium

## Next Checks

1. Conduct a detailed error analysis of Tilmash translations compared to Google Translate and Yandex Translate outputs on the same sentences to identify systematic differences and potential advantages
2. Evaluate model performance on additional low-resource domains not well-represented in KazParC to assess generalization capabilities
3. Perform ablation studies to quantify the exact contribution of synthetic data versus manual translations to overall performance