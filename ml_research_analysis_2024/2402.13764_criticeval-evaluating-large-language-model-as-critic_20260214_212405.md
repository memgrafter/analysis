---
ver: rpa2
title: 'CriticEval: Evaluating Large Language Model as Critic'
arxiv_id: '2402.13764'
source_url: https://arxiv.org/abs/2402.13764
tags:
- feedback
- critique
- llms
- evaluation
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CRITIC BENCH, a novel benchmark designed
  to comprehensively and reliably evaluate the critique ability of large language
  models (LLMs) across four dimensions: feedback, comparison, refinement, and meta-feedback.
  The benchmark covers nine diverse task scenarios, including natural language processing,
  reasoning, coding, and alignment tasks, and evaluates both scalar-valued and textual
  critiques at varying levels of response quality.'
---

# CriticEval: Evaluating Large Language Model as Critic

## Quick Facts
- arXiv ID: 2402.13764
- Source URL: https://arxiv.org/abs/2402.13764
- Reference count: 30
- Primary result: Introduces CRITIC BENCH, a benchmark evaluating LLM critique ability across four dimensions in nine diverse task scenarios, revealing scaling relationships and domain-specific challenges

## Executive Summary
This paper introduces CRITIC BENCH, a novel benchmark designed to comprehensively evaluate the critique ability of large language models across four dimensions: feedback, comparison, refinement, and meta-feedback. The benchmark covers nine diverse task scenarios including natural language processing, reasoning, coding, and alignment tasks, evaluating both scalar-valued and textual critiques at varying levels of response quality. Extensive experiments on open-source and closed-source LLMs reveal intriguing relationships between critique ability and factors such as task types, response qualities, and model scales, with some open-source LLMs approaching state-of-the-art closed-source models in critique capabilities.

## Method Summary
The benchmark employs a human-in-the-loop data construction pipeline where task inputs and responses are generated by various LLMs, then high-quality human-annotated critiques serve as references for evaluation. GPT-4 is used for both initial critique generation and evaluation, with human annotators reviewing and refining generated critiques and responses. The evaluation employs both objective metrics (Spearman correlations, accuracy, pass rate) and subjective metrics (Likert scores with reference critiques) across four critique dimensions in nine diverse task scenarios.

## Key Results
- Critique ability scales systematically with model size across multiple LLM families
- Mathematics and coding tasks are significantly more challenging for critiques than natural language tasks
- Open-source LLMs show competitive critique capabilities approaching state-of-the-art closed-source models
- LLMs perform worst on high-quality responses, particularly for feedback and refinement dimensions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Incorporating high-quality human-annotated critiques as reference inputs to GPT-4 improves the reliability of subjective evaluations compared to direct GPT-4 scoring without references.
- **Mechanism**: Human-annotated critiques serve as ground truth exemplars that guide GPT-4's assessment process, reducing variance and improving alignment with human judgment.
- **Core assumption**: GPT-4's evaluation capability can be enhanced through contextual exemplars that clarify quality expectations.
- **Evidence anchors**: The paper cites Sun et al. (2024a) and Kim et al. (2024) proving effectiveness of reference critiques in enhancing GPT-4 assessments.

### Mechanism 2
- **Claim**: The difficulty of critiquing responses varies systematically across task types, with mathematics and coding tasks being significantly more challenging than natural language tasks.
- **Mechanism**: Different task domains require different types of expertise and reasoning capabilities, with mathematical and coding tasks demanding precise logical reasoning that current LLMs struggle to evaluate accurately.
- **Core assumption**: The complexity of critique correlates with the cognitive demands of the underlying task domain.
- **Evidence anchors**: LLMs achieve much higher scores in the first five tasks than on math reasoning and coding tasks (average subjective scores 4.92 > 3.19).

### Mechanism 3
- **Claim**: Critique ability follows a scaling law, with larger LLMs demonstrating systematically better critique performance across all dimensions.
- **Mechanism**: As model size increases, LLMs acquire more parameters and training data that enable better understanding of complex relationships and more sophisticated feedback generation capabilities.
- **Core assumption**: Model scale directly translates to improved reasoning and evaluation capabilities.
- **Evidence anchors**: Most LLMs' critique ability improves steadily as the number of parameters increases, as observed in the Qwen, Llama-2, DeepSeek, Mistral, and InternLM2 series LLMs.

## Foundational Learning

- **Concept**: Spearman correlation as an objective evaluation metric
  - **Why needed here**: Objective evaluation requires measuring the consistency between LLM-generated scores and human-annotated scores across multiple samples.
  - **Quick check question**: If an LLM's feedback scores have a Spearman correlation of 0.8 with human scores, what does this indicate about the LLM's scoring consistency?

- **Concept**: Position bias in comparative evaluation
  - **Why needed here**: When comparing two responses, LLMs may systematically prefer responses based on their position in the prompt rather than their actual quality.
  - **Quick check question**: How would you modify a comparison evaluation protocol to eliminate position bias effects?

- **Concept**: Chain-of-thought prompting for complex reasoning
  - **Why needed here**: Some critique dimensions, particularly meta-feedback, require multi-step reasoning that may benefit from explicit intermediate steps.
  - **Quick check question**: When would you choose to use chain-of-thought prompting versus direct prompting for critique generation?

## Architecture Onboarding

- **Component map**: Task input collection → Response generation → Quality rating → Critique generation → Human annotation → Evaluation pipeline
- **Critical path**: 
  1. Task input selection and validation
  2. Response generation with multiple LLMs
  3. Quality rating verification by human experts
  4. Critique generation and human annotation
  5. Evaluation with both objective and subjective metrics

- **Design tradeoffs**:
  - Human annotation vs. automated evaluation: Human annotation provides gold standard quality but is expensive and slow; automated evaluation is faster but may have systematic biases.
  - Single vs. multi-turn critique generation: Multi-turn allows refinement but increases complexity and cost.
  - Task diversity vs. depth: Including many task types provides broad coverage but may dilute depth of analysis in each area.

- **Failure signatures**:
  - Low correlation between LLM and human scores indicates evaluation misalignment
  - High variance in critique quality across different LLMs suggests inconsistent evaluation criteria
  - Systematic preference for certain response positions indicates positional bias
  - Poor performance on high-quality responses suggests difficulty with subtle error detection

- **First 3 experiments**:
  1. Position bias test: Run comparison evaluations with all possible response orderings to quantify positional effects.
  2. Reference critique ablation: Compare GPT-4 evaluation performance with and without reference critiques to measure improvement magnitude.
  3. Scale law validation: Test critique performance across a range of model sizes within the same architecture family to confirm scaling relationships.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are reference critiques in improving the reliability of LLM-generated critiques across different task types and critique dimensions?
- Basis in paper: The paper states that "To improve the reliability of subjective scoring for textual critiques, we employ high-quality human-annotated critiques as references to assist GPT-4" and provides a case study demonstrating the effectiveness of reference critiques.
- Why unresolved: The paper only provides one case study. The generalizability of reference critiques across diverse tasks and critique dimensions is not empirically established.
- What evidence would resolve it: Systematic experiments comparing critique quality with and without reference critiques across all nine tasks and four critique dimensions.

### Open Question 2
- Question: What is the relationship between the difficulty of critiquing responses of different quality levels and the effectiveness of LLM self-improvement?
- Basis in paper: The paper states that "LLMs perform worst on high-quality responses for the feedback dimension" and "LLMs struggle in self-improvement on challenging mathematics and coding tasks."
- Why unresolved: While the paper identifies correlations between response quality and critique difficulty, it doesn't establish a causal link to the effectiveness of self-improvement.
- What evidence would resolve it: Controlled experiments measuring the impact of critique difficulty on LLM performance improvement after iterative self-critique and refinement.

### Open Question 3
- Question: How does the scaling law observed for critique ability translate to real-world applications of LLM self-improvement and scalable oversight?
- Basis in paper: The paper states that "most LLMs' critique ability...improves steadily as the number of parameters increases" and discusses the implications for self-improvement.
- Why unresolved: The paper doesn't explore the practical implications of the scaling law for real-world applications. The relationship between model size and critique effectiveness in deployed systems is unknown.
- What evidence would resolve it: Empirical studies comparing the performance of LLMs of different scales in real-world self-improvement and scalable oversight scenarios.

## Limitations

- Task-Specific Performance Gaps: The benchmark shows LLMs struggle more with math and coding critiques, but the underlying reasons for these domain-specific differences remain unclear.
- Human Annotation Quality: The benchmark relies on human-annotated critiques as reference standards, but the consistency and reliability of human annotators across different task types and quality levels is not fully characterized.
- Generalization Beyond Tested Domains: The nine task scenarios may not fully represent the range of real-world applications where critique abilities matter.

## Confidence

**High Confidence**: The observed scaling relationships between model size and critique performance are supported by systematic data across multiple model families (Qwen, Llama-2, DeepSeek, Mistral, InternLM2).

**Medium Confidence**: The effectiveness of using reference critiques to improve GPT-4's reliability is supported by evidence, but the specific magnitude of improvement may vary depending on the task domain and quality of reference critiques.

**Low Confidence**: Claims about the relative difficulty of different task types are based on observed performance differences, but the underlying cognitive mechanisms driving these differences are not fully explored.

## Next Checks

1. Cross-Domain Generalization Test: Apply the benchmark to task types not included in the original nine scenarios to assess whether observed scaling laws and domain-specific performance patterns hold across broader application areas.

2. Annotator Agreement Analysis: Quantify inter-annotator reliability and consistency across different task types and quality levels to better understand the stability of human reference critiques as evaluation standards.

3. Fine-Grained Performance Analysis: Conduct detailed error analysis on critiques of high-quality responses to identify specific failure modes and determine whether they reflect fundamental limitations or can be addressed through targeted improvements in critique generation approaches.