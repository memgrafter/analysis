---
ver: rpa2
title: 'Vibravox: A Dataset of French Speech Captured with Body-conduction Audio Sensors'
arxiv_id: '2407.11828'
source_url: https://arxiv.org/abs/2407.11828
tags:
- speech
- audio
- processing
- ieee
- sensors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Vibravox is a GDPR-compliant dataset containing audio recordings
  captured using five different body-conduction audio sensors: two in-ear microphones,
  two bone conduction vibration pickups, and a laryngophone, along with an airborne
  microphone as a reference. The dataset includes 45 hours of speech samples and physiological
  sounds recorded by 188 participants under different acoustic conditions using a
  high-order ambisonics 3D spatializer.'
---

# Vibravox: A Dataset of French Speech Captured with Body-conduction Audio Sensors

## Quick Facts
- arXiv ID: 2407.11828
- Source URL: https://arxiv.org/abs/2407.11828
- Reference count: 40
- Primary result: GDPR-compliant dataset with 45 hours of French speech recorded by 188 participants using five body-conduction sensors plus reference microphone

## Executive Summary
Vibravox is a comprehensive dataset of French speech recorded using five different body-conduction audio sensors (two in-ear microphones, two bone conduction vibration pickups, and a laryngophone) plus a reference airborne microphone. The dataset includes 45 hours of speech samples and physiological sounds recorded by 188 participants under various acoustic conditions using a high-order ambisonics 3D spatializer. It features detailed annotations about recording conditions and linguistic transcriptions, enabling research on body-conducted speech capture and processing across tasks like speech recognition, enhancement, and speaker verification.

## Method Summary
The dataset was collected from 188 participants using five body-conduction microphones and a reference airborne microphone under clean and noisy conditions in a 3D sound spatializer. Recordings include both speech and speechless segments to enable noise modeling. A comprehensive post-processing pipeline applied automated filtering based on phoneme error rate, signal-to-noise ratio, and energy levels. The dataset is structured into four subsets (speech-clean, speech-noisy, speechless-clean, speechless-noisy) with detailed annotations including phonetic transcriptions and recording condition metadata.

## Key Results
- Five body-conduction sensors plus reference microphone capture diverse acoustic characteristics
- 45 hours of speech data from 188 participants across different acoustic conditions
- Includes both speech and speechless recordings for noise modeling
- Comprehensive annotations enable training and evaluation of speech processing models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Vibravox dataset enables effective training of deep learning models for body-conducted speech processing by providing large-scale real-world recordings across multiple sensor types.
- Mechanism: By offering recordings from five different body-conduction audio sensors plus a reference airborne microphone, the dataset captures the diverse acoustic characteristics and frequency responses unique to each sensor. This allows models to learn the transfer functions between body-conducted and airborne speech, which are critical for tasks like bandwidth extension and speech enhancement.
- Core assumption: Body-conducted speech and airborne speech share the same excitation source, enabling a learnable mapping between them despite different frequency characteristics.
- Evidence anchors: [abstract] The dataset includes five different body-conduction audio sensors and an airborne microphone as a reference. [section II.A] Vibravox overcomes limitations of existing datasets by using five different BCMs and including recordings in both noisy and quiet environments.

### Mechanism 2
- Claim: The inclusion of 3D spatialized ambient noise recordings enables realistic noise-robust model training.
- Mechanism: By recording both speech and silent segments in a 3D sound spatializer with 56 loudspeakers, the dataset provides controlled yet realistic noise conditions. This allows augmentation techniques to mix clean speech with spatially rendered noise, preserving a clean reference for training noise-robust speech enhancement and recognition models.
- Core assumption: The 3D spatialization setup accurately simulates real-world noise environments encountered in body-conduction speech applications.
- Evidence anchors: [section III.C] The dataset includes recordings in a 3D sound spatializer to uniformly sample the sphere for noise emission. [section II.A] Vibravox addresses the lack of noise recordings in existing datasets by including speech in both noisy and quiet environments.

### Mechanism 3
- Claim: High-quality annotations and filtering ensure reliable model training and evaluation.
- Mechanism: The dataset includes phonetic transcriptions, recording condition annotations, and automated filtering based on phoneme error rate, signal-to-noise ratio, and energy levels. This ensures that only high-quality, correctly pronounced speech segments are used for training, improving model reliability.
- Core assumption: Automated filtering based on Whisper alignment and energy metrics effectively identifies and removes low-quality or mispronounced segments.
- Evidence anchors: [section III-E] The post-processing filters out samples with PER > 10%, SNR < 1.5 dB, and insufficient energy levels. [section III-E] Whisper-based timestamping is used to align transcriptions and detect voice activity.

## Foundational Learning

- Concept: Body-conduction audio sensors (BCMs)
  - Why needed here: Understanding how BCMs capture speech through bone and tissue vibrations is essential to grasp why they have limited bandwidth and how they differ from airborne microphones.
  - Quick check question: Why do body-conduction microphones typically have reduced high-frequency response compared to airborne microphones?

- Concept: Signal-to-noise ratio (SNR) and coherence functions
  - Why needed here: SNR and coherence are key metrics used to evaluate sensor performance and filter data quality in the dataset.
  - Quick check question: How does coherence between a body-conduction sensor and a reference microphone indicate the sensor's effectiveness in capturing speech?

- Concept: Phoneme Error Rate (PER) and Levenshtein distance
  - Why needed here: PER is used to assess speech recognition performance and filter out mispronounced or low-quality audio segments in the dataset.
  - Quick check question: What does a high PER indicate about the quality of a speech recording or the performance of a speech recognition model?

## Architecture Onboarding

- Component map: speech-clean -> speech-noisy -> speechless-clean -> speechless-noisy (5 BCMs + 1 airborne mic per subset)
- Critical path: For speech enhancement, use speech-clean as training data (clean reference + degraded signal) and speechless-noisy for noise augmentation
- Design tradeoffs: Real body-conducted recordings ensure authenticity but limit dataset size compared to simulated data; 3D spatialized noise adds realism but requires specialized equipment
- Failure signatures: Poor model performance may indicate insufficient bandwidth modeling, inadequate noise representation, or unfiltered low-quality segments
- First 3 experiments:
  1. Train an EBEN model for bandwidth extension on speech-clean data and evaluate on both clean and noisy subsets
  2. Fine-tune a wav2vec2.0 model for phoneme recognition on speech-clean data and test cross-sensor generalization
  3. Evaluate an ECAPA2 speaker verification model on paired recordings from different sensors to assess cross-modal performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different body-conduction sensor placements affect the bandwidth extension performance of deep learning models?
- Basis in paper: [explicit] The paper shows that the temple vibration pickup has the smallest bandwidth, barely recording any speech signal above 1500 Hz, while the forehead accelerometer has the largest bandwidth, with good coherence up to 8 kHz.
- Why unresolved: While the paper demonstrates that bandwidth is correlated with performance in tasks like speech recognition and speaker verification, it does not specifically investigate how the placement of the sensor impacts the ability of deep learning models to extend bandwidth.
- What evidence would resolve it: A detailed study comparing the bandwidth extension performance of deep learning models trained on data from different sensor placements, such as the forehead, temple, throat, and in-ear microphones, would provide insights into the impact of sensor placement on bandwidth extension.

### Open Question 2
- Question: Can speaker verification models be improved by incorporating speaker identity preservation into the objective function during bandwidth extension?
- Basis in paper: [inferred] The paper mentions that the EBEN models used for speech enhancement do not include speaker identity preservation in their objective function, which could explain the deterioration in speaker verification performance when using EBEN-enhanced signals.
- Why unresolved: The paper does not explore the possibility of modifying the EBEN models to include speaker identity preservation, which could potentially improve the performance of speaker verification models on body-conducted speech.
- What evidence would resolve it: An experiment comparing the performance of speaker verification models on raw and EBEN-enhanced signals, where the EBEN models are modified to include speaker identity preservation in their objective function, would determine if this approach can improve speaker verification performance.

### Open Question 3
- Question: How does the diversity of participants' speech patterns and accents affect the performance of speech recognition models on body-conducted speech?
- Basis in paper: [explicit] The Vibravox dataset includes recordings from 188 participants, but the paper does not discuss the diversity of participants' speech patterns or accents.
- Why unresolved: The paper focuses on the performance of speech recognition models on body-conducted speech but does not investigate how the diversity of participants' speech patterns and accents affects model performance.
- What evidence would resolve it: An analysis of the performance of speech recognition models on body-conducted speech from participants with diverse speech patterns and accents, such as different dialects or languages, would provide insights into the impact of speech diversity on model performance.

## Limitations
- Exclusive focus on French speech limits generalizability to other languages and accents
- 3D spatialized noise may not fully capture real-world temporal and spectral complexity
- Automated filtering pipeline may introduce bias by excluding valid speech variations
- Effectiveness for real-world applications outside controlled environments not demonstrated

## Confidence

**High Confidence:** Dataset construction methodology and basic sensor characteristics are well-documented and reproducible. Filtering pipeline and annotation process are clearly specified.

**Medium Confidence:** Performance improvements depend heavily on quality of downstream model implementations, which are only partially specified. Cross-sensor generalization results show promise but require further validation.

**Low Confidence:** Dataset's effectiveness for real-world applications outside controlled environments is not demonstrated. Long-term stability of body-conduction sensors across extended use periods is not addressed.

## Next Checks

1. **Cross-Environment Validation:** Test trained speech enhancement and recognition models on field-collected data from actual operational environments (e.g., military, industrial settings) to assess real-world robustness beyond controlled spatialized noise.

2. **Sensor Longevity Assessment:** Conduct longitudinal tests measuring sensor performance degradation over extended use periods, including stability of frequency response and noise floor characteristics across multiple recording sessions.

3. **Language Generalization Study:** Evaluate model performance when applied to speech from other languages, particularly those with different phonetic inventories or speaking styles, to determine extent of language-specific biases in the dataset.