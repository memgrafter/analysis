---
ver: rpa2
title: 'ContextCite: Attributing Model Generation to Context'
arxiv_id: '2409.00729'
source_url: https://arxiv.org/abs/2409.00729
tags:
- context
- cite
- sources
- attribution
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces context attribution, a method to identify
  which parts of a language model's context are responsible for generating specific
  statements. The authors present CONTEXT CITE, which uses a sparse linear surrogate
  model trained on context ablations to estimate the importance of each context source.
---

# ContextCite: Attributing Model Generation to Context

## Quick Facts
- arXiv ID: 2409.00729
- Source URL: https://arxiv.org/abs/2409.00729
- Authors: Benjamin Cohen-Wang; Harshay Shah; Kristian Georgiev; Aleksander Madry
- Reference count: 40
- Primary result: CONTEXT CITE uses 32 context ablations to learn a sparse linear surrogate model that identifies which context parts cause specific model outputs, outperforming attention, gradient, and similarity baselines.

## Executive Summary
This paper introduces context attribution, a method to identify which parts of a language model's context are responsible for generating specific statements. The authors present CONTEXT CITE, which uses a sparse linear surrogate model trained on context ablations to estimate the importance of each context source. The method is evaluated on three benchmarks (CNN DailyMail, HotpotQA, TyDi QA) using top-k log-probability drop and linear datamodeling score metrics, showing that CONTEXT CITE outperforms baselines like attention, gradient norm, and semantic similarity. The paper demonstrates three applications: helping verify generated statements (improving AUC by 1.2-5%), improving response quality through context pruning (improving F1 score by up to 3.1%), and detecting context poisoning attacks (achieving 85-100% detection accuracy). The method requires only 32 context ablations to learn an effective surrogate model, making it scalable and efficient.

## Method Summary
CONTEXT CITE learns a sparse linear surrogate model to attribute generated statements to context sources. The method works by sampling random ablation vectors (removing subsets of context sources), computing the logit-scaled probabilities of responses under each ablation, and training a LASSO-regularized linear model to predict these probabilities from the ablation patterns. The learned model weights serve as attribution scores indicating each source's contribution to the response. The approach is validated using top-k log-probability drop (measuring how much ablating top sources reduces response probability) and linear datamodeling score (measuring correlation between attribution scores and actual ablation effects).

## Key Results
- CONTEXT CITE outperforms attention, gradient norm, and semantic similarity baselines on attribution quality metrics across three benchmarks
- Context pruning using CONTEXT CITE attributions improves response quality, increasing F1 score by up to 3.1%
- The method detects context poisoning attacks with 85-100% accuracy
- Only 32 context ablations are needed to learn an effective surrogate model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A linear surrogate model can faithfully predict how context ablations affect response probability.
- **Mechanism**: The method trains a sparse linear model on logit-scaled probabilities of responses under random context ablations. Each weight in this model directly measures the contribution of a context source to the response.
- **Core assumption**: The effect of ablating multiple context sources can be approximated as the sum of individual effects (linearity assumption).
- **Evidence anchors**:
  - [abstract] "In this work, we find that it is possible to learn a linear surrogate model that (1) faithfully models the language model's behavior and (2) can be efficiently estimated using a small number of additional inference passes."
  - [section] "We illustrate an example depicting the effectiveness of a linear surrogate model in Figure 2 and provide additional randomly sampled examples in Appendix B.2."
  - [corpus] Weak evidence - no direct corpus citations for this specific linearity assumption.
- **Break condition**: The linearity assumption fails when sources interact non-additively (e.g., when multiple sources contain redundant information or when the model uses complex reasoning chains).

### Mechanism 2
- **Claim**: Sparse linear models can be learned efficiently with few context ablations due to the sparsity of context relevance.
- **Mechanism**: LASSO regularization induces sparsity in the surrogate model weights, allowing accurate modeling even with limited ablations. The number of needed ablations scales with the number of relevant sources rather than total sources.
- **Core assumption**: Only a small subset of context sources are truly relevant to any given generated statement.
- **Evidence anchors**:
  - [abstract] "The method requires only 32 context ablations to learn an effective surrogate model, making it scalable and efficient."
  - [section] "Empirically, we find that a generated statement can often be explained well by just a handful of sources."
  - [corpus] No direct corpus evidence for this specific sparsity claim.
- **Break condition**: The sparsity assumption breaks when many context sources are equally relevant to the response (e.g., when the context contains multiple independent pieces of information needed to answer a question).

### Mechanism 3
- **Claim**: Attribution scores can predict the effect of ablating sources on response probability (validated through LDS metric).
- **Mechanism**: The surrogate model weights are used as attribution scores, and these scores are validated by measuring their correlation with actual probability changes under random ablations (LDS metric).
- **Core assumption**: Attribution scores should rank the importance of sources in predicting how ablating them affects response probability.
- **Evidence anchors**:
  - [abstract] "The method is evaluated on three benchmarks (CNN DailyMail, HotpotQA, TyDi QA) using top-k log-probability drop and linear datamodeling score metrics."
  - [section] "Concretely, suppose that we sample a few different ablation vectors and compute the sum of the scores corresponding to the sources that are included by each. These summed scores may be viewed as the 'predicted effects' of each ablation."
  - [corpus] Weak evidence - no direct corpus citations for this specific validation approach.
- **Break condition**: The correlation between attribution scores and actual ablation effects breaks down when the surrogate model is insufficiently trained or when the response depends on complex interactions between sources.

## Foundational Learning

- **Concept**: Linear regression with LASSO regularization
  - Why needed here: To learn a sparse linear surrogate model that approximates the language model's behavior under context ablations
  - Quick check question: What is the purpose of the L1 penalty in LASSO regression?

- **Concept**: Logit function and logit-scaled probabilities
  - Why needed here: To transform bounded probabilities into unbounded values suitable for linear regression
  - Quick check question: Why can't we directly apply linear regression to probabilities bounded between 0 and 1?

- **Concept**: Spearman rank correlation coefficient
  - Why needed here: To measure how well attribution scores predict the ranking of ablation effects (LDS metric)
  - Quick check question: What does a Spearman correlation of 1.0 indicate about the relationship between two variables?

## Architecture Onboarding

- **Component map**:
  Language model (pLM) -> Context tokenizer -> Ablation sampler -> Surrogate learner -> Attribution extractor -> Evaluator

- **Critical path**:
  1. Generate initial response with full context
  2. Sample ablation vectors and collect responses under each
  3. Train sparse linear surrogate model on logit-scaled probabilities
  4. Extract attribution scores from surrogate weights
  5. Evaluate attribution quality using metrics

- **Design tradeoffs**:
  - Sparsity vs faithfulness: More ablations can improve surrogate accuracy but increase cost
  - Granularity vs interpretability: Word-level sources provide finer attribution but may reduce sparsity
  - Global vs local modeling: Uniform sampling enables global model but may miss local behaviors

- **Failure signatures**:
  - Low LDS scores indicate poor surrogate model faithfulness
  - Inconsistent top-k drops across runs suggest instability in the learning process
  - Attribution scores that don't align with manual inspection indicate potential model failure

- **First 3 experiments**:
  1. Verify surrogate model faithfulness by comparing predicted vs actual ablation effects on held-out data
  2. Test attribution quality by ablating top-k sources and measuring response probability drop
  3. Validate application utility by checking if top sources help verify generated statements' accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum number of sources that CONTEXT CITE can handle effectively before performance degrades significantly?
- Basis in paper: [explicit] The paper mentions evaluating on contexts with up to 872 sources (TyDi QA) and shows that CONTEXT CITE remains effective with just 32 context ablations.
- Why unresolved: While the paper demonstrates effectiveness with 32 ablations even for contexts with hundreds of sources, it doesn't establish a hard upper limit on the number of sources where performance begins to degrade.
- What evidence would resolve it: Systematic evaluation of CONTEXT CITE performance across a wide range of source counts (e.g., 100, 500, 1000, 2000 sources) while keeping the number of ablations fixed at 32, measuring both LDS and top-k log-probability drop metrics.

### Open Question 2
- Question: How does CONTEXT CITE performance change when using more sophisticated surrogate models (e.g., neural networks) instead of linear models?
- Basis in paper: [inferred] The paper specifically chooses linear surrogate models for interpretability and shows they are often faithful, but acknowledges this may not always be the case (Section C.4).
- Why unresolved: The paper demonstrates linear models work well but doesn't explore whether non-linear models could capture more complex relationships between context sources and generated responses.
- What evidence would resolve it: Direct comparison of linear versus non-linear surrogate models (e.g., small neural networks) on the same evaluation tasks, measuring faithfulness through LDS and top-k log-probability drop, and examining trade-offs between model complexity and interpretability.

### Open Question 3
- Question: Can CONTEXT CITE be adapted to provide real-time attributions for interactive applications like chatbots?
- Basis in paper: [explicit] The paper notes that CONTEXT CITE is 32Ã— more expensive than generating the original response, which may be prohibitively expensive for some applications (Section C.4).
- Why unresolved: While the paper acknowledges computational limitations, it doesn't explore potential optimizations or approximations that could make real-time attribution feasible.
- What evidence would resolve it: Development and evaluation of an optimized version of CONTEXT CITE that achieves near real-time performance (e.g., <100ms latency) while maintaining acceptable attribution quality, possibly through techniques like model distillation, caching, or selective attribution.

### Open Question 4
- Question: How does CONTEXT CITE handle cases where the generated response contains information not present in the context (e.g., knowledge from pre-training)?
- Basis in paper: [explicit] The paper notes that CONTEXT CITE provides contributive attributions and may yield low scores for sources that support a statement if the model already knows the information from pre-training (Section C.4).
- Why unresolved: The paper acknowledges this limitation but doesn't provide a systematic evaluation of how often this occurs or what the implications are for different types of responses.
- What evidence would resolve it: Quantitative analysis of CONTEXT CITE attributions across a diverse set of responses, distinguishing between statements that are likely grounded in context versus those that appear to draw on external knowledge, and measuring the frequency and impact of "missing" attributions for context-supported statements.

## Limitations
- The linearity assumption may break down when context sources interact non-additively or when the model uses complex reasoning chains
- The method requires 32 additional inference passes per response, which may be prohibitively expensive for some applications
- CONTEXT CITE may not attribute responses to context sources when the model already knows the information from pre-training

## Confidence

**High Confidence Claims:**
- The method works on the tested benchmarks (CNN DailyMail, HotpotQA, TyDi QA) with demonstrated improvements in attribution quality metrics
- The LDS validation approach provides meaningful assessment of attribution score quality
- The 32-ablation requirement is empirically sufficient for learning effective surrogate models

**Medium Confidence Claims:**
- The method's generalizability to other domains and language models beyond those tested
- The practical utility of attribution scores in real-world verification and response quality improvement scenarios
- The robustness of the method to different context granularities (sentence vs word-level sources)

**Low Confidence Claims:**
- The method's performance in extreme scenarios like highly redundant context or complex multi-hop reasoning
- The scalability of the approach to very large context windows or models
- The sensitivity of results to hyperparameter choices like LASSO regularization strength

## Next Checks

1. **Linearity Assumption Validation**: Systematically test the surrogate model's faithfulness by comparing predicted vs actual ablation effects on held-out ablation vectors across different context types and reasoning complexities. This would directly validate whether the linear approximation holds in various scenarios.

2. **Cross-Domain Generalization**: Apply the method to a new domain (e.g., medical or legal documents) with different context structures and reasoning requirements to test the generalizability of the sparsity and linearity assumptions.

3. **Robustness to Context Granularity**: Experiment with different context tokenization strategies (word, phrase, sentence levels) to understand how granularity affects attribution quality and the validity of the underlying assumptions.