---
ver: rpa2
title: kNN For Whisper And Its Effect On Bias And Speaker Adaptation
arxiv_id: '2410.18850'
source_url: https://arxiv.org/abs/2410.18850
tags:
- speech
- whisper
- speaker
- language
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends kNN search to Whisper, an end-to-end speech
  recognition model, to improve performance and study bias across gender, accent,
  and age. kNN search uses hidden states as keys and tokens as values, searching at
  inference time to adjust output probabilities without retraining the model.
---

# kNN For Whisper And Its Effect On Bias And Speaker Adaptation

## Quick Facts
- arXiv ID: 2410.18850
- Source URL: https://arxiv.org/abs/2410.18850
- Authors: Maya K. Nachesa; Vlad Niculae
- Reference count: 8
- One-line primary result: kNN improves WER for medium and large Whisper models, with smaller datastores offering speed gains for speaker adaptation.

## Executive Summary
This paper extends kNN search to Whisper, an end-to-end speech recognition model, to improve performance and study bias across gender, accent, and age. kNN search uses hidden states as keys and tokens as values, searching at inference time to adjust output probabilities without retraining the model. Experiments on four datasets show kNN improves WER for medium and large Whisper models, with smaller datastores offering speed gains for speaker adaptation. On Dutch CommonVoice, kNN yields larger improvements for women and Belgian speakers, and most age groups except teens. The optimal hyperparameters differ from text-based kNN, with lower k (4) and 位 (0.4) performing best, suggesting the speech model's familiarity with data influences adaptation strength. Limitations include limited speaker sampling, fixed hyperparameters for some datasets, and bias analysis constrained by dataset labels. Code and data are publicly available.

## Method Summary
The paper implements kNN search for Whisper by creating a datastore of hidden states and tokens from training data, then using FAISS for efficient nearest neighbor retrieval at inference time. The method interpolates kNN-retrieved token probabilities with the model's original predictions using a mixing parameter 位. Experiments are conducted on four datasets (VoxPopuli, LibriSpeech, CommonVoice Dutch, and RixVox) across Whisper model sizes (tiny, medium, large-v3). Hyperparameters k, T, and 位 are tuned on VoxPopuli and applied to other datasets. Speaker adaptation is studied using full datastores versus speaker-specific datastores. Bias analysis examines performance differences across gender, accent, and age groups on CommonVoice Dutch.

## Key Results
- kNN improves WER for medium and large Whisper models, with smaller datastores offering speed gains for speaker adaptation.
- On Dutch CommonVoice, kNN yields larger improvements for women and Belgian speakers, and most age groups except teens.
- The optimal hyperparameters differ from text-based kNN, with lower k (4) and 位 (0.4) performing best, suggesting the speech model's familiarity with data influences adaptation strength.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: kNN search adjusts output probabilities by retrieving similar hidden states from a datastore and interpolating their token probabilities with the model's original predictions.
- Mechanism: At each decoding step, the model's current hidden state is used as a query to search the datastore for the k nearest neighbors. The tokens associated with these neighbors are assigned probabilities based on their distance to the query. These probabilities are then interpolated with the model's own predictions using a mixing parameter 位, resulting in an adjusted output distribution.
- Core assumption: The hidden states generated by the model encode meaningful information about the current token and its context, allowing similar contexts to be retrieved from the datastore.
- Evidence anchors:
  - [abstract] "At inference time, at each step, the model's hidden state is used to search the datastore for the k nearest tokens, and the output probability of the found tokens is changed."
  - [section 2.2] "At inference time, the hidden state for an input and output generated so far is used as a query  to search the datastore for the k nearest neighbors."
  - [corpus] Weak evidence for retrieval relevance. Only 5/8 neighbors have FMR>0.55, suggesting moderate similarity.
- Break condition: If the datastore lacks relevant neighbors for a given context, the interpolation will have little effect or could degrade performance if 位 is too high.

### Mechanism 2
- Claim: Using smaller, speaker-specific datastores for kNN can improve speaker adaptation while reducing computational cost.
- Mechanism: Instead of using a large general datastore, smaller datastores are created containing only the hidden states and tokens from a specific speaker. At inference time, the model's hidden state is queried against this speaker-specific datastore, allowing it to adapt to the speaker's unique characteristics.
- Core assumption: A speaker's pronunciation and speaking style are consistent enough within a recording session to be captured by a limited set of hidden states.
- Evidence anchors:
  - [abstract] "We discuss implications for speaker adaptation, and analyze improvements by gender, accent, and age."
  - [section 4.2] "Using the complete datastore results in a larger average improvement per speaker compared to any of the other methods. Using a personal datastore leads to smaller improvements..."
  - [corpus] Moderate evidence. Per-speaker adaptation shows smaller improvements than full datastore but gains efficiency (7s vs 1m15s).
- Break condition: If a speaker's datastore is too small or lacks diversity, it may not capture the range of contexts needed for accurate transcription, leading to worse performance than a larger general datastore.

### Mechanism 3
- Claim: Whisper's familiarity with the training data influences the optimal k and 位 hyperparameters for kNN, differing from text-based kNN setups.
- Mechanism: Whisper has been trained on a large and diverse dataset, making it more familiar with various accents and domains compared to text models. This familiarity means that fewer neighbors (lower k) and a higher mixing parameter (位) are needed to achieve optimal performance, as the model's own predictions are already strong.
- Core assumption: The optimal hyperparameters for kNN depend on the underlying model's familiarity with the data and the domain shift between the training data and the target dataset.
- Evidence anchors:
  - [abstract] "The optimal hyperparameters differ from text-based kNN, with lower k (4) and 位 (0.4) performing best, suggesting the speech model's familiarity with data influences adaptation strength."
  - [section 4.1.1] "We find that, generally, we get the best results at 位 = 0.4. These results contrast with those obtained by Khandelwal et al. (2020, 2021)."
  - [corpus] Moderate evidence. Neighbor retrieval shows moderate similarity (avg FMR=0.55), suggesting the model can benefit from additional context.
- Break condition: If the target dataset is significantly out-of-domain or the model is less familiar with it, higher k and lower 位 might be needed, as seen in the text-based kNN literature.

## Foundational Learning

- Concept: Hidden state representations in transformer models
  - Why needed here: Understanding how hidden states encode token and context information is crucial for grasping how kNN retrieval works.
  - Quick check question: What information is typically encoded in the hidden states of a transformer decoder at each step?

- Concept: Distance metrics and nearest neighbor search
  - Why needed here: kNN relies on finding the closest hidden states in the datastore to the current query state, requiring knowledge of distance metrics and search algorithms.
  - Quick check question: What are some common distance metrics used for comparing high-dimensional vectors like hidden states?

- Concept: Catastrophic forgetting and non-parametric adaptation methods
  - Why needed here: The paper addresses the issue of catastrophic forgetting in fine-tuning and proposes kNN as a non-parametric alternative for adaptation.
  - Quick check question: How does catastrophic forgetting occur in fine-tuning, and what are the advantages of non-parametric methods like kNN?

## Architecture Onboarding

- Component map:
  Whisper model -> kNN datastore (FAISS IVFPQ index) -> kNN search module -> Probability interpolation module -> Output distribution

- Critical path:
  1. Generate hidden states for the input audio and decoded output so far
  2. Use the current hidden state as a query to search the kNN datastore
  3. Retrieve the k nearest neighbors and their associated tokens
  4. Calculate probabilities for each neighbor based on distance
  5. Interpolate these probabilities with the model's original predictions using 位
  6. Sample from the adjusted probability distribution to generate the next token

- Design tradeoffs:
  - Datastore size vs. search speed: Larger datastores may provide more relevant neighbors but increase search time
  - k value vs. accuracy: Higher k may include more irrelevant neighbors, while lower k may miss relevant ones
  - 位 value vs. model influence: Higher 位 gives more weight to kNN predictions, while lower 位 relies more on the model's own predictions

- Failure signatures:
  - Decreased WER compared to vanilla model: Could indicate irrelevant neighbors or suboptimal hyperparameters
  - High variance in WER across speakers: May suggest inconsistent datastore quality or speaker adaptation issues
  - Long inference times: Could be due to large datastore size or inefficient search parameters

- First 3 experiments:
  1. Hyperparameter search on VoxPopuli: Tune k, T, and 位 on the VoxPopuli dataset to find optimal values for Whisper large-v3
  2. Speaker adaptation on RixVox: Compare full datastore, random datastore, and personal datastore for a subset of speakers
  3. Bias analysis on CommonVoice: Evaluate WER differences across gender, accent, and age groups with and without kNN

## Open Questions the Paper Calls Out

- Question: How does the optimal lambda value vary across languages with different typological features and language families, particularly those not using the Latin alphabet or with synthetic morphology?
  - Basis in paper: [inferred] The paper notes that all languages studied were Indo-European and used the Latin alphabet, and calls for more work on languages with different typological features and from different language families.
  - Why unresolved: The paper only examined Indo-European languages, limiting the generalizability of findings to other language families and scripts.
  - What evidence would resolve it: Experiments applying kNN to Whisper on languages from diverse families (e.g., Sino-Tibetan, Afro-Asiatic, Niger-Congo) and scripts (e.g., Cyrillic, Arabic, Chinese characters) while tuning lambda and other hyperparameters.

- Question: Does kNN adaptation affect all speaker groups equally, or are there differential effects across demographic categories such as children, elderly, and speakers with non-standard accents?
  - Basis in paper: [explicit] The authors state that more work is needed to assess whether kNN affects all speaker groups equally and note limitations in analyzing bias across age and accent categories.
  - Why unresolved: The study was limited by dataset labels and speaker sampling, particularly for children's speech and non-standard accents, making it unclear if kNN benefits or harms specific groups.
  - What evidence would resolve it: Comprehensive bias analysis on diverse datasets with rich demographic metadata, including children, elderly, and speakers with varied accents, comparing kNN performance across these groups.

- Question: What is the relationship between Whisper's familiarity with a dataset and the optimal lambda value for kNN adaptation?
  - Basis in paper: [explicit] The authors observe that Whisper's familiarity with data influences adaptation strength and suggest that more out-of-domain datasets require higher lambda values.
  - Why unresolved: The training data for Whisper is not public, making it difficult to determine how familiar the model is with specific datasets used in the study.
  - What evidence would resolve it: Experiments varying lambda across datasets with known relationships to Whisper's training data (e.g., datasets created after Whisper's training cutoff), measuring how model familiarity correlates with optimal lambda values.

## Limitations

- Dataset constraints limit bias analysis to gender, age, and accent, preventing intersectional effects or other protected attributes analysis.
- Hyperparameter optimization was performed on VoxPopuli and applied to other datasets without additional tuning, which may not yield optimal results across all domains.
- Speaker adaptation experiments used only 20 speakers from RixVox, limiting generalizability of the findings.

## Confidence

- **High confidence**: The core mechanism of kNN search in Whisper and its basic implementation are well-established and clearly demonstrated through improved WER metrics across multiple datasets and model sizes.
- **Medium confidence**: The bias analysis results showing differential improvements across demographic groups are credible but limited by dataset constraints and potential sampling biases in the speaker adaptation experiments.
- **Medium confidence**: The findings on optimal hyperparameters (k=4, 位=0.4) differing from text-based kNN are plausible given Whisper's pre-training, but require more systematic validation across diverse domains.

## Next Checks

1. Conduct hyperparameter optimization separately for each dataset rather than transferring from VoxPopuli to verify if the claimed differences from text-based kNN generalize across domains.
2. Expand speaker adaptation experiments to include more speakers and additional datasets to validate the computational trade-offs between full datastore and personal datastore approaches.
3. Replicate the bias analysis on additional datasets with richer demographic annotations to assess whether the observed improvements for women and Belgian speakers on CommonVoice Dutch generalize to other languages and demographic groups.