---
ver: rpa2
title: 'MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence
  Models'
arxiv_id: '2406.06046'
source_url: https://arxiv.org/abs/2406.06046
tags:
- data
- influence
- pretraining
- oracle
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MATES introduces a model-aware data selection framework that continuously
  adapts to the evolving data preferences of pretraining models. It uses locally probed
  oracle data influence, collected via one-step training on reference tasks, to train
  a small data influence model that selects the most effective data for each pretraining
  stage.
---

# MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models

## Quick Facts
- arXiv ID: 2406.06046
- Source URL: https://arxiv.org/abs/2406.06046
- Authors: Zichun Yu; Spandan Das; Chenyan Xiong
- Reference count: 40
- Primary result: MATES achieves 1.3% and 1.1% average accuracy gains for 410M and 1B models respectively while reducing total FLOPs by half

## Executive Summary
MATES introduces a novel model-aware data selection framework that continuously adapts to pretraining models' evolving data preferences. The approach uses locally probed oracle data influence, collected via one-step training on reference tasks, to train a small data influence model that selects the most effective data for each pretraining stage. Experiments demonstrate that MATES significantly outperforms random selection and state-of-the-art methods, achieving substantial accuracy improvements while reducing computational costs.

## Method Summary
MATES addresses the challenge of efficient pretraining by continuously adapting data selection based on the model's changing preferences during training. The framework collects influence labels for each data batch by training the current model for one step on reference tasks and measuring performance changes. These labels train a small influence model that predicts which data batches will be most beneficial for future training. The influence model uses three key features: dataset-level difficulty, batch-level relevance to reference tasks, and batch-level influence predictions. MATES maintains separate influence models for each stage of training, allowing it to capture the evolving data preferences as model capabilities develop.

## Key Results
- MATES achieves 1.3% average accuracy gain for 410M parameter models compared to random selection
- MATES achieves 1.1% average accuracy gain for 1B parameter models with half the FLOPs
- Outperforms state-of-the-art methods by doubling performance gains in both model sizes

## Why This Works (Mechanism)
MATES works by recognizing that pretraining models' data preferences evolve throughout training. Early stages benefit from easier, more relevant data, while later stages require harder, more diverse samples. By continuously collecting influence labels and training separate influence models for each stage, MATES can identify and select the most effective data batches at each point in the training process. This adaptive approach ensures the model is always learning from the most beneficial data for its current capabilities, leading to faster convergence and better final performance.

## Foundational Learning
- Data influence estimation: Understanding how individual training examples affect model performance is crucial for effective data selection. Quick check: Can the model accurately predict performance changes from single-step training updates?
- Multi-stage training adaptation: Different training stages require different data characteristics. Quick check: Does the influence model capture stage-specific data preferences effectively?
- Reference task selection: The choice of reference tasks affects the quality of influence predictions. Quick check: Are the selected reference tasks representative of the target capabilities?
- Computational efficiency trade-offs: Balancing influence collection costs against pretraining efficiency gains. Quick check: Does the benefit of better data selection outweigh the overhead of influence collection?

## Architecture Onboarding

Component Map: Data batches -> Influence model -> Selected data -> Pretraining model -> Updated influence model

Critical Path: The core workflow involves collecting influence labels through one-step training on reference tasks, using these labels to train an influence model, then using the influence model to select the most beneficial data batches for the next training stage.

Design Tradeoffs: The framework balances between comprehensive influence collection (more accurate but computationally expensive) and efficient approximation (faster but potentially less precise). Using separate influence models per stage captures evolving preferences but increases model complexity.

Failure Signatures: Poor reference task selection leads to ineffective influence predictions. Insufficient influence labels result in unreliable data selection. Mismatched influence model capacity causes overfitting or underfitting. Inadequate stage division misses important shifts in data preferences.

First Experiments: 1) Test influence collection on a small subset of data to verify label quality. 2) Train a basic influence model with limited features to establish baseline performance. 3) Evaluate stage-specific data selection by comparing early vs late stage preferences.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to 5 reference tasks which may not capture all necessary capabilities
- Computational overhead of influence collection (24 hours for 0.5% of C4 dataset)
- Limited evaluation to 410M and 1B parameter models
- Fixed data ratios used for comparison rather than total computational budget

## Confidence

High confidence in the core technical contribution: The framework for model-aware data selection with continuous adaptation is well-developed and the results demonstrate clear improvements over baselines in the tested settings.

Medium confidence in the scalability claims: While the approach shows promise, the limited model size range (410M-1B parameters) and the computational overhead for influence collection raise questions about practical applicability to trillion-parameter models.

Medium confidence in the generalizability of the influence model: The effectiveness of the influence model trained on 5 reference tasks may not translate well to other domains or task distributions, and the paper doesn't extensively explore this aspect.

## Next Checks

1. Test the approach on larger model sizes (10B+ parameters) to verify scalability and whether the observed performance improvements persist at industrial scales.

2. Evaluate the robustness of MATES when trained on different sets of reference tasks to determine how sensitive the influence model is to task selection.

3. Conduct a comprehensive computational overhead analysis comparing the time spent on influence label collection versus the total pretraining time to quantify the practical efficiency gains.