---
ver: rpa2
title: Approximate Equivariance in Reinforcement Learning
arxiv_id: '2411.04225'
source_url: https://arxiv.org/abs/2411.04225
tags:
- equivariant
- group
- symmetry
- equivariance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops approximately equivariant reinforcement learning
  algorithms for domains with inexact symmetry. The authors formalize approximately
  equivariant MDPs and prove that optimal Q-functions exhibit approximate group invariance.
---

# Approximate Equivariance in Reinforcement Learning

## Quick Facts
- arXiv ID: 2411.04225
- Source URL: https://arxiv.org/abs/2411.04225
- Authors: Jung Yeon Park, Sujay Bhatt, Sihan Zeng, Lawson L. S. Wong, Alec Koppel, Sumitra Ganesh, Robin Walters
- Reference count: 35
- One-line primary result: Approximately equivariant RL outperforms exactly equivariant methods when symmetry is broken, achieving 10.6-12.0% annualized returns in stock trading versus 7.7-10.4% for baselines

## Executive Summary
This paper introduces approximately equivariant reinforcement learning algorithms for domains where exact symmetry is broken. The authors formalize approximately equivariant MDPs and prove that optimal Q-functions exhibit approximate group invariance. By using relaxed group and steerable convolutions with learnable weights, the approach adapts to symmetry breaking while maintaining sample efficiency. Experiments on continuous control tasks and stock trading show that the method performs similarly to exactly equivariant approaches when symmetry is perfect, but outperforms them when symmetry is broken, with increased robustness to test-time noise.

## Method Summary
The method extends exactly equivariant RL architectures by replacing standard group convolutions with relaxed group convolutions that include learnable weights. These relaxed weights allow the model to maintain exact equivariance when symmetry is perfect but adapt when symmetry breaks. The approach builds on existing RL algorithms (DrQv2 for control tasks, SAC for stock trading) by modifying their convolutional layers. The relaxed weights are initialized to be equal (exact equivariance) and only diverge when needed to correct for symmetry breaking factors like modified gravity, repeated actions, or reflected actions.

## Key Results
- Approximately equivariant methods achieve annualized returns of 10.6-12.0% in stock trading versus 7.7-10.4% for baselines
- When symmetry is exact, ApproxEquiv performs similarly to ExactEquiv; when symmetry is broken, ApproxEquiv outperforms both ExactEquiv and NonEquiv
- The method demonstrates increased robustness to test-time noise compared to exactly equivariant and non-equivariant baselines
- Relaxed weights adapt differently across domains, confirming the model's ability to learn and adjust for symmetry breaking factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Approximately equivariant networks outperform exactly equivariant ones in domains with symmetry breaking.
- Mechanism: Relaxed group convolutions allow adaptive adjustment of symmetry constraints via learnable relaxed weights. When symmetry is perfect, weights remain equal and model behaves exactly equivariant. When symmetry breaks, weights diverge to learn corrections.
- Core assumption: Symmetry breaking can be modeled as a learnable perturbation to the exact equivariance constraint.
- Evidence anchors:
  - [abstract]: "The relaxed weights adapt differently across domains, confirming the model's ability to learn and adjust for symmetry breaking factors."
  - [section 4.2]: "We extend exactly equivariant versions... by replacing each group convolution with relaxed group convolutions."
  - [corpus]: Weak - related papers discuss approximate equivariance but not RL-specific mechanisms.

### Mechanism 2
- Claim: Approximate equivariance improves robustness to noise at test time.
- Mechanism: The flexibility of relaxed equivariant layers allows the model to implicitly learn noise-robust representations, as it's not constrained to perfect symmetry.
- Core assumption: The ability to relax symmetry constraints transfers to better generalization under input perturbations.
- Evidence anchors:
  - [abstract]: "As an added byproduct of these techniques, we observe increased robustness to noise at test time."
  - [section 5.1]: "Interestingly, we find that our approach is more robust to noisy inputs than ExactEquiv or NonEquiv."
  - [corpus]: Weak - robustness is mentioned but not specifically in RL context.

### Mechanism 3
- Claim: The approximately equivariant RL architecture generalizes well from exact to approximate symmetry domains.
- Mechanism: By initializing relaxed weights to be equal (exact equivariance) and allowing them to diverge only when needed, the model maintains sample efficiency from exact symmetry while adapting to asymmetry.
- Core assumption: Starting from exact equivariance provides a strong inductive bias that can be gradually relaxed.
- Evidence anchors:
  - [section 4.2]: "This result supports Proposition 3.1 from Wang et al. (2024b), which proves that relaxed group convolutions initialized to be exactly equivariant stay exactly equivariant when trained with exact data symmetry."
  - [section 5.1]: "In domains with exact symmetry (original), our method ApproxEquiv performs similarly to ExactEquiv."
  - [corpus]: Weak - initialization strategies discussed but not specifically for RL.

## Foundational Learning

- Concept: Group theory and symmetry groups (D1, D2, E(2), etc.)
  - Why needed here: Understanding which symmetry groups apply to each domain is essential for constructing appropriate equivariant architectures.
  - Quick check question: What is the symmetry group for the Acrobot domain, and how does it act on the state and action spaces?

- Concept: Group convolutions and their relaxed variants
  - Why needed here: The core mechanism relies on replacing standard convolutions with group convolutions and then relaxing them.
  - Quick check question: How does a relaxed group convolution differ from a standard group convolution in terms of kernel structure?

- Concept: Reinforcement learning fundamentals (MDPs, Q-learning, actor-critic methods)
  - Why needed here: The architecture builds on existing RL algorithms (DrQv2, SAC) and modifies their components.
  - Quick check question: What is the difference between the policy and critic networks in an actor-critic framework?

## Architecture Onboarding

- Component map:
  Input: Stack of 3 consecutive RGB images (85×85) → Encoder: Relaxed group/steerable convolutions with learnable relaxed weights → Policy: Approximately equivariant network mapping states to action distributions → Critics: Approximately invariant networks outputting Q-values → Output: Actions in continuous control or stock trading decisions

- Critical path:
  1. Input images → lifting convolution (if using group convolutions)
  2. Relaxed group/steerable convolutions with adaptive weights
  3. Non-linear activation functions
  4. Policy/critic heads with appropriate equivariance constraints
  5. Loss computation and backpropagation

- Design tradeoffs:
  - Exact vs. relaxed equivariance: Exact is more parameter-efficient but brittle; relaxed is flexible but has more parameters
  - Group choice: Must match domain symmetry; wrong choice hurts performance
  - Number of relaxed filters L: More filters = more flexibility but higher computational cost

- Failure signatures:
  - Weights not diverging in symmetry-breaking domains → model not adapting
  - Performance similar to NonEquiv → symmetry bias not helpful
  - Training instability → learning rate or architecture too aggressive

- First 3 experiments:
  1. Train on Acrobot with exact symmetry vs. gravity-modified version to test adaptation
  2. Compare relaxed weights across original and modified domains visually
  3. Test robustness by adding noise at test time only

## Open Questions the Paper Calls Out
- Future work includes proving bounds on the optimal policy π(s) and π(gs) for approximately equivariant MDPs
- Applying approximately equivariant RL to robotic manipulation tasks where kinematic constraints or obstacles can break symmetry
- Measuring equivariance error to quantify types of symmetry breaking factors and predict when approximately equivariant methods will outperform exactly equivariant ones

## Limitations
- The paper requires knowing the symmetry group and how it acts on state and action spaces in advance, which may not be available for all real-world applications
- Empirical results are limited to four control domains and one financial task, leaving uncertainty about generalization to more complex symmetry breaking scenarios
- The theoretical framework focuses on bounds for value functions but does not provide bounds for policy differences between approximately and exactly equivariant approaches

## Confidence

**Confidence Labels:**
- High confidence: The theoretical framework for approximately equivariant MDPs and the proof of approximate group invariance for optimal Q-functions
- Medium confidence: Empirical performance claims on control tasks with synthetic symmetry breaking
- Low confidence: Generalization of results to more complex real-world symmetry breaking scenarios and financial applications

## Next Checks

1. **Weight evolution analysis**: Track and visualize the relaxed weights w_l(g) throughout training across different symmetry-breaking conditions to verify they adapt meaningfully rather than converging to arbitrary values.

2. **Ablation on symmetry group choice**: Systematically test performance when using incorrect symmetry groups (e.g., using SO(2) instead of SE(2) for domains with translations) to quantify sensitivity to group selection.

3. **Cross-domain transfer**: Train on one symmetry-breaking variant and test on another (e.g., gravity-modified trained model tested on reflection-modified domain) to assess whether learned symmetry corrections transfer.