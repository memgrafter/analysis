---
ver: rpa2
title: Comparing Abstraction in Humans and Large Language Models Using Multimodal
  Serial Reproduction
arxiv_id: '2402.03618'
source_url: https://arxiv.org/abs/2402.03618
tags:
- language
- gpt-4
- multimodal
- reproduction
- serial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a multimodal serial reproduction framework
  to compare abstraction in humans and large language models (LLMs) like GPT-4. The
  framework involves chains where participants alternate between visual and linguistic
  representations.
---

# Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction

## Quick Facts
- arXiv ID: 2402.03618
- Source URL: https://arxiv.org/abs/2402.03618
- Authors: Sreejan Kumar; Raja Marjieh; Byron Zhang; Declan Campbell; Michael Y. Hu; Umang Bhatt; Brenden Lake; Thomas L. Griffiths
- Reference count: 5
- Key outcome: Humans show more dissociable visual and linguistic representations than GPT-4 in multimodal serial reproduction tasks

## Executive Summary
This study introduces a multimodal serial reproduction framework to compare abstraction in humans and large language models (LLMs) like GPT-4. The framework involves chains where participants alternate between visual and linguistic representations using binary 7x7 grid patterns. Results show that adding language as a modality significantly impacts human reproductions, suggesting humans have more dissociable visual and linguistic representations compared to GPT-4. This difference may arise from GPT-4's training on large amounts of text data, leading to tightly coupled vision and language representations. The findings highlight the need for further research on abstraction in humans and machines, especially as AI systems become more integrated into daily life.

## Method Summary
The study employs a multimodal serial reproduction framework where participants (both humans and GPT-4) alternate between visual and linguistic representations. Unimodal chains involve either visual-to-visual or language-to-language reproduction, while multimodal chains alternate between visual and language representations. Binary 7x7 grid patterns serve as visual stimuli, and textual descriptions represent linguistic stimuli. The study measures board complexity using Kolmogorov Complexity, Shannon Entropy, and Local Spatial Complexity, and extracts language embeddings to decode board complexity. Statistical comparisons assess the impact of adding language as a modality on human and GPT-4 reproductions, and evaluate the dissociability of visual and linguistic representations.

## Key Results
- Adding language as a modality has a larger effect on human reproductions than GPT-4's, indicating more dissociable visual and linguistic representations in humans
- GPT-4's abstract visual representations are much closer to linguistic representations than those of humans
- Decoding performance of complexity from language embeddings is generally higher in multimodal chains than in unimodal chains, and higher for GPT-4 than humans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language adds a representational bottleneck that differentially impacts human abstraction compared to GPT-4
- Mechanism: In unimodal chains, both humans and GPT-4 rely on direct visual processing. In multimodal chains, humans must translate visual input into language, creating a bottleneck that forces abstraction compatible with linguistic encoding. GPT-4, trained on paired vision-language data, already encodes visual features in a language-compatible format, so the bottleneck has less impact
- Core assumption: GPT-4's visual representations are inherently language-aligned due to training objectives, whereas human visual and linguistic representations are more dissociable
- Evidence anchors:
  - [abstract]: "adding language as a modality has a larger effect on human reproductions than GPT-4’s. This suggests human visual and linguistic representations are more dissociable than those of GPT-4."
  - [section]: "GPT-4 abstract visual representations are much closer to linguistic representations than those of humans. This may have resulted from the training paradigm for GPT-4... leading to a tight coupling between vision and language representations."
  - [corpus]: Weak - corpus neighbors discuss abstraction but don't directly address vision-language coupling differences between humans and models

### Mechanism 2
- Claim: Human priors over visual patterns differ from GPT-4's priors, revealed through convergence patterns in serial reproduction chains
- Mechanism: Serial reproduction chains converge to reflect underlying priors. Humans' multimodal chains produce simpler, more language-describable patterns (e.g., checkerboard, stripes), suggesting priors favor linguistically communicable abstractions. GPT-4 produces similarly simple patterns in both unimodal and multimodal conditions, indicating priors already favor language-compatible abstractions
- Core assumption: Chain convergence distributions reflect the underlying generative priors of the agent
- Evidence anchors:
  - [abstract]: "human visual and linguistic representations are more dissociable than those of GPT-4."
  - [section]: "multimodal boards... are patterns that are most easily identifiable by language, e.g., checkerboard, square shapes, and stripes... whereas unimodal boards are patterns that are harder to describe in language. However, in the case of GPT-4, both unimodal and multimodal patterns are more easily describable through language."
  - [corpus]: Weak - corpus neighbors discuss abstraction alignment but don't directly address prior differences revealed through serial reproduction

### Mechanism 3
- Claim: Language representations are more predictive of visual pattern complexity in GPT-4 than in humans, indicating tighter vision-language coupling
- Mechanism: By training ridge regression models to predict board complexity from language embeddings, we find higher decoding performance for GPT-4 than humans. This suggests GPT-4's visual representations already encode information in a language-compatible format, while human visual representations contain information less accessible to language
- Core assumption: Language embeddings capture the linguistically-compatible portion of visual representations, and decoding performance measures the degree of vision-language coupling
- Evidence anchors:
  - [section]: "decoding performance of complexity from language embeddings is generally higher in multimodal chains than in unimodal chains... GPT-4 generally has a higher decoding performance than humans... this complexity is the kind of complexity that is decodable by language representations."
  - [abstract]: "adding language as a modality has a larger effect on human reproductions than GPT-4’s."
  - [corpus]: Weak - corpus neighbors discuss abstraction but don't directly address language embedding predictive power for visual complexity

## Foundational Learning

- Concept: Serial reproduction as a method for eliciting priors
  - Why needed here: Understanding how serial reproduction reveals underlying generative priors is crucial for interpreting why humans and GPT-4 show different multimodal effects
  - Quick check question: What would you expect to see in a serial reproduction chain if the agent's priors strongly favored simple, symmetric patterns?

- Concept: Bayesian inference in human perception
  - Why needed here: The theoretical framework assumes agents use Bayesian inference to form abstractions from stimuli, which explains why chain dynamics reflect underlying priors
  - Quick check question: If humans were perfect Bayesian observers with identical priors across modalities, what would you expect to see when comparing unimodal and multimodal chains?

- Concept: Information-theoretic measures of complexity
  - Why needed here: Understanding Kolmogorov complexity, Shannon entropy, and local spatial complexity is essential for interpreting why multimodal chains produce simpler patterns and why GPT-4's patterns are more language-decodable
  - Quick check question: Which complexity measure would be most sensitive to whether a pattern could be easily described in language?

## Architecture Onboarding

- Component map: Stimulus generation -> Chain execution (human/Machine) -> Complexity measurement -> Language embedding extraction -> Decoding analysis -> Statistical comparison
- Critical path: Stimulus generation -> Chain execution -> Complexity measurement -> Statistical comparison
- Design tradeoffs: Simple 7x7 binary grids vs. realistic images (simplicity vs. ecological validity), short chains vs. longer chains (practicality vs. convergence assessment), post-hoc language descriptions vs. real-time descriptions (control vs. ecological validity)
- Failure signatures: No significant difference between unimodal and multimodal conditions, similar chain velocities for humans and GPT-4, low decoding performance across all conditions
- First 3 experiments:
  1. Replicate the study with different complexity measures to verify robustness of findings
  2. Run longer chains to assess whether differences between human and GPT-4 persist with more iterations
  3. Test with more complex visual stimuli to determine if findings generalize beyond binary grids

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do human visual and linguistic representations differ from those of GPT-4, and how do these differences impact the formation of abstractions?
- Basis in paper: Explicit
- Why unresolved: The paper suggests that humans have more dissociable visual and linguistic representations compared to GPT-4, but it does not provide a detailed explanation of the nature of these differences or their impact on abstraction formation
- What evidence would resolve it: Conducting experiments that compare the visual and linguistic representations of humans and GPT-4, and analyzing how these representations influence the formation of abstractions in different tasks or contexts

### Open Question 2
- Question: How does the training paradigm of GPT-4, which involves joint matching of images with language descriptions, lead to a tight coupling between vision and language representations?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that GPT-4's training on large amounts of text data may lead to tightly coupled vision and language representations, but it does not provide a detailed explanation of the mechanisms underlying this coupling
- What evidence would resolve it: Investigating the training process of GPT-4 and other vision-language models to identify the specific factors that contribute to the coupling of vision and language representations, and conducting experiments to test the impact of these factors on the formation of abstractions

### Open Question 3
- Question: How do the complexity measures used in the paper (Kolmogorov Complexity, Shannon Entropy, and Local Spatial Complexity) relate to the ability of language to represent visual information, and how do these relationships differ between humans and GPT-4?
- Basis in paper: Explicit
- Why unresolved: The paper uses these complexity measures to analyze the differences between human and GPT-4 abstractions, but it does not provide a detailed explanation of how these measures relate to the ability of language to represent visual information
- What evidence would resolve it: Conducting experiments that compare the ability of humans and GPT-4 to represent visual information using language, and analyzing the relationship between these representations and the complexity measures used in the paper

### Open Question 4
- Question: How do the results of the paper change when using more realistic visual inputs, such as natural images or drawings, instead of the binary grid patterns used in the experiments?
- Basis in paper: Inferred
- Why unresolved: The paper acknowledges that the use of binary grid patterns may limit the generalizability of the results, but it does not provide a detailed explanation of how the results might change with more realistic visual inputs
- What evidence would resolve it: Conducting experiments that use more realistic visual inputs, such as natural images or drawings, and comparing the results to those obtained with binary grid patterns to identify any differences or similarities in the formation of abstractions between humans and GPT-4

## Limitations
- Binary 7x7 grid patterns may not generalize to real-world abstraction tasks
- The study focuses on a specific type of abstraction task, which may not capture the full range of human and machine abstraction capabilities
- The results are based on controlled experiments with artificial stimuli, which may not reflect real-world abstraction processes

## Confidence
- **High Confidence**: The experimental methodology for measuring multimodal effects is sound, and the observed differences between human and GPT-4 responses are statistically robust
- **Medium Confidence**: The interpretation that GPT-4's vision-language coupling results from training on paired data is plausible but not definitively proven by this study alone
- **Low Confidence**: Claims about the broader implications for AI integration into daily life extend beyond what the experimental evidence directly supports

## Next Checks
1. Test with natural image stimuli: Replicate the study using complex natural images rather than binary grids to verify whether the vision-language coupling differences persist with more ecologically valid stimuli
2. Analyze intermediate chain states: Examine the representations at each step of the serial reproduction chain to determine precisely when and how abstraction occurs differently between humans and GPT-4
3. Cross-model comparison: Extend the framework to include other vision-language models (e.g., CLIP, Flamingo) to determine whether the observed differences are specific to GPT-4 or represent broader trends in multimodal AI systems