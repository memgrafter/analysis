---
ver: rpa2
title: Narrating Causal Graphs with Large Language Models
arxiv_id: '2403.07118'
source_url: https://arxiv.org/abs/2403.07118
tags:
- causal
- text
- graph
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates text generation from causal graphs using four
  GPT-3 models under three training settings (zero-shot, few-shot, fine-tuned) and
  two datasets. The authors compare performance when causality tags are provided versus
  when they are not.
---

# Narrating Causal Graphs with Large Language Models

## Quick Facts
- arXiv ID: 2403.07118
- Source URL: https://arxiv.org/abs/2403.07118
- Reference count: 35
- Primary result: GPT-3 can learn causal graph-to-text generation with limited training data, but does not inherently encode causal reasoning.

## Executive Summary
This paper evaluates text generation from causal graphs using four GPT-3 models under three training settings (zero-shot, few-shot, fine-tuned) and two datasets. The authors compare performance when causality tags are provided versus when they are not. Results show that GPT-3 performs similarly with full training data versus just three examples, suggesting that extensive training sets are not necessary. However, zero-shot learning shows sharp performance drops and performs better without causality tags. Human evaluations support these findings. Comparing against the WebNLG dataset, causal datasets yield better performance than fact-based datasets under fine-tuning and few-shot settings, but worse under zero-shot. Overall, the study suggests that while GPT-3 can learn causality with limited data, it does not inherently encode causal reasoning.

## Method Summary
The study uses two causal graph datasets (youth suicide and obesity) and four GPT-3 models (Davinci, Curie, Babbage, Ada) under fine-tuning, few-shot, and zero-shot settings. Causal graphs are linearized into text representations with or without causality tags. Generated outputs are evaluated using automatic metrics (ROUGE-L, METEOR, BERTScore, QuestEval) and human evaluations for faithfulness and coverage. The methodology involves preprocessing graphs into acyclic components, training GPT-3 models with varying data sizes, and comparing performance across different input formats and training approaches.

## Key Results
- Few-shot learning with GPT-3 achieves similar performance to full fine-tuning, reducing the need for extensive training data
- Zero-shot learning shows sharp performance deterioration and performs better without causality tags
- Causality tags improve performance in fine-tuning and few-shot settings but hinder zero-shot learning
- Causal datasets outperform fact-based datasets under fine-tuning and few-shot settings but underperform under zero-shot

## Why This Works (Mechanism)

### Mechanism 1
GPT-3 can learn causal graph-to-text generation with limited training data through few-shot learning, as the model has already learned sufficient linguistic and causal reasoning patterns from pretraining.

### Mechanism 2
GPT-3 does not inherently encode causal reasoning but can learn it with training, as zero-shot learning results in lower performance and less accurate causality representation.

### Mechanism 3
Providing explicit causality tags improves GPT-3's performance in most training settings by helping the model better understand causal relationships and generate more accurate descriptions.

## Foundational Learning

- Concept: Causal graphs
  - Why needed here: Understanding the structure and representation of causal graphs is crucial for designing the graph-to-text generation task and evaluating the model's performance
  - Quick check question: What are the key components of a causal graph, and how do they differ from other types of graphs (e.g., knowledge graphs)?

- Concept: Few-shot learning
  - Why needed here: The paper explores the use of few-shot learning to reduce the amount of training data required for GPT-3 to learn causal graph-to-text generation
  - Quick check question: How does few-shot learning differ from traditional supervised learning, and what are its advantages and limitations?

- Concept: Graph linearization
  - Why needed here: Linearizing the causal graph into a text-based representation is necessary for inputting the graph into GPT-3 and evaluating the generated text
  - Quick check question: What are the challenges of linearizing a graph, and how can information loss be minimized during this process?

## Architecture Onboarding

- Component map: Data preprocessing -> Training data preparation -> GPT-3 model training/evaluation -> Evaluation using metrics and human feedback
- Critical path: Data preprocessing → Training data preparation → GPT-3 model training/evaluation → Evaluation using metrics and human feedback
- Design tradeoffs:
  - Model size vs. performance: Larger models (e.g., Davinci) generally perform better but are more computationally expensive
  - Training data size vs. performance: Few-shot learning can achieve similar performance to fine-tuning with a full training set, reducing the need for extensive data collection
  - Input format (with/without causality tags) vs. performance: Providing causality tags generally improves performance, except in zero-shot settings
- Failure signatures:
  - Low evaluation metric scores: Indicates poor model performance or inadequate training data
  - Inconsistent human evaluations: Suggests issues with the model's ability to generate faithful and comprehensive descriptions
  - High computational cost: May indicate the need for smaller models or more efficient training strategies
- First 3 experiments:
  1. Fine-tuning the smallest GPT-3 model (Ada) with a small training set (e.g., 10-20 examples) and evaluating its performance using automatic metrics and human evaluations
  2. Using few-shot learning with the largest GPT-3 model (Davinci) and comparing its performance to fine-tuning with a full training set
  3. Evaluating the effect of providing causality tags on the model's performance in both fine-tuning and few-shot learning settings

## Open Questions the Paper Calls Out

### Open Question 1
What specific metrics could be developed to better evaluate causality in text generated from causal graphs, beyond current overlap and semantic similarity measures? Current automatic metrics may score highly even when texts express opposing causal relationships, failing to assess causal direction, type, and relationships between entities accurately.

### Open Question 2
How do the performances of large language models on causal reasoning tasks compare to their performances on other complex reasoning tasks, such as mathematical or logical reasoning? The paper highlights the need to understand LLM capabilities across different reasoning domains.

### Open Question 3
What are the implications of the findings that large language models do not inherently encode causality for their use in real-world applications, particularly in sensitive domains like healthcare? The paper raises concerns about relying on LLMs for causal reasoning in sensitive domains but does not provide specific guidelines for mitigating these risks.

## Limitations
- Small sample size of causal graph datasets (only two datasets with limited nodes and edges) may not be representative of broader causal reasoning tasks
- Results are specific to GPT-3 models and may not generalize to other large language models or more complex causal structures
- Human evaluation process lacks detailed information on rater training and inter-annotator agreement, introducing potential subjectivity in results

## Confidence

- High confidence: Few-shot learning performs similarly to full fine-tuning with GPT-3 models, and zero-shot learning performs poorly
- Medium confidence: GPT-3 does not inherently encode causal reasoning, inferred from performance differences rather than direct probing
- Low confidence: Specific mechanisms by which causality tags improve performance, particularly why they are detrimental in zero-shot settings

## Next Checks

1. Test the same methodology with a larger and more diverse set of causal graph datasets to verify if the few-shot learning advantage holds across different domains and complexity levels
2. Conduct ablation studies to isolate the effect of causality tags by testing with graphs that have varying levels of causal complexity and different types of relationships
3. Compare GPT-3's performance against other large language models (e.g., GPT-4, Claude) on the same causal graph generation task to determine if these findings are model-specific or represent a broader trend in LLM capabilities