---
ver: rpa2
title: Generating Signed Language Instructions in Large-Scale Dialogue Systems
arxiv_id: '2410.14026'
source_url: https://arxiv.org/abs/2410.14026
tags:
- system
- sign
- signed
- language
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first large-scale deployment of a multimodal
  dialogue system with American Sign Language (ASL) instructions, accessible via touch-based
  interface on Amazon Alexa Echo Show devices. The system uses Large Language Models
  to translate task instructions into ASL glosses, retrieves corresponding sign videos,
  and presents them alongside visual aids to reduce cognitive load for Deaf and Hard-of-Hearing
  users.
---

# Generating Signed Language Instructions in Large-Scale Dialogue Systems

## Quick Facts
- arXiv ID: 2410.14026
- Source URL: https://arxiv.org/abs/2410.14026
- Reference count: 23
- Primary result: First large-scale multimodal dialogue system with ASL instructions accessible via Alexa Echo Show

## Executive Summary
This paper presents the first large-scale deployment of a multimodal dialogue system with American Sign Language (ASL) instructions, accessible via touch-based interface on Amazon Alexa Echo Show devices. The system uses Large Language Models to translate task instructions into ASL glosses, retrieves corresponding sign videos, and presents them alongside visual aids to reduce cognitive load for Deaf and Hard-of-Hearing users. The development process involved extensive community co-design with the Deaf community and experts in cognitive and ASL learning sciences. The system achieves BERTScore of 0.80 for translation quality and maintains user ratings comparable to non-signed variants.

## Method Summary
The system translates English task instructions into ASL glosses using Large Language Models (specifically gpt-3.5-turbo), then retrieves corresponding sign videos from an existing video corpus and presents them alongside images and text on a multimodal interface. The translation pipeline includes rule-based heuristics for quality control, converting instructions to uppercase glosses, removing punctuation, and handling fingerspellings. Sign videos are retrieved based on gloss tokens and stitched together to create continuous video sequences. The user interface is designed specifically around cognitive load considerations for signers, with careful attention to the placement of text, signed videos, and visual aids on the same screen.

## Key Results
- BERTScore of 0.80 for LLM-generated gloss translation quality
- Hit Rate of ~0.98 for sign video retrieval accuracy
- User ratings comparable to non-signed variants during 5-month production deployment
- Manual curation time reduced from 10 minutes to 1 minute per instruction sentence

## Why This Works (Mechanism)

### Mechanism 1
Using Large Language Models (LLMs) for gloss translation enables scalable deployment of signed instructions without requiring extensive manual annotation. LLMs can translate task instructions into ASL glosses automatically, reducing the time spent on manual checking from 10 minutes per instruction sentence to 1 minute per sentence. Core assumption: LLMs can generate semantically accurate glosses that are understandable to sign language users. Break condition: If LLM-generated glosses consistently fail to capture the semantic meaning of instructions or if community feedback indicates poor understandability.

### Mechanism 2
Segmenting instruction steps into gloss tokens and retrieving corresponding sign videos improves accessibility for Deaf and Hard-of-Hearing users. Each instruction step is converted into gloss tokens, which are intermediary textual representations of ASL. Sign videos are retrieved for each gloss token and stitched together to create a continuous video sequence. Core assumption: Breaking down instructions into individual gloss tokens allows for more precise video retrieval and presentation. Break condition: If segmentation leads to fragmented or incoherent sign presentations, or if video retrieval fails to find appropriate signs for certain tokens.

### Mechanism 3
Incorporating cognitive load considerations into user interface design reduces the cognitive burden on sign language users. The layout is designed specifically around the cognitive load of signers, considering factors like placement of text and signed videos on the same screen. Core assumption: Reducing cognitive load through interface design improves user experience and comprehension. Break condition: If user feedback indicates the interface design does not effectively reduce cognitive load or if users struggle to process multiple modalities simultaneously.

## Foundational Learning

- Concept: American Sign Language (ASL) Glosses
  - Why needed here: Glosses are intermediary textual representations of ASL used to translate spoken language instructions into signed language.
  - Quick check question: What are glosses, and why are they used in sign language processing?

- Concept: Multimodal Information Presentation
  - Why needed here: Combining text, images, and sign videos in the interface helps reduce cognitive load and improve comprehension for sign language users.
  - Quick check question: How does presenting information in multiple modalities benefit sign language users?

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are used to automatically translate task instructions into ASL glosses, enabling scalable deployment without extensive manual annotation.
  - Quick check question: How do LLMs contribute to the automatic translation of instructions into sign language?

## Architecture Onboarding

- Component map: User Input -> NLU -> Dialogue Manager -> NLG (Gloss Translation & Video Retrieval) -> Video Storage & Retrieval -> User Interface
- Critical path: 1) User input processed by NLU 2) DM determines appropriate response 3) NLG generates signed instructions by translating text into glosses and retrieving sign videos 4) User interface displays signed instructions, images, and text
- Design tradeoffs: Using LLMs allows scalability but may introduce translation errors; segmenting instructions improves precision but may create fragmented presentations; cognitive load design enhances experience but may limit information density
- Failure signatures: Poor user ratings indicating comprehension difficulties; high error rates in sign video retrieval; users reporting high cognitive load or confusion
- First 3 experiments: 1) Evaluate LLM-generated gloss quality using BERTScore compared to human-annotated glosses 2) Test retrieval accuracy for different gloss tokens and assess video dataset size impact 3) Conduct user studies measuring cognitive load and experience comparing signed vs non-signed variants

## Open Questions the Paper Calls Out

### Open Question 1
How do user comprehension rates and task completion times compare between signed instructions and traditional text-based instructions for the same tasks? The paper mentions DHH community members can experience higher cognitive loads while reading compared to signing, and that the system achieves ratings on par with non-signing variants, but does not provide direct comparative metrics.

### Open Question 2
What is the impact of using human signers versus synthesized avatars on user satisfaction and comprehension in the long term? The expert qualitative analysis notes that the primary limitation of the current system lies in the segmented nature of the ASL videos, and suggests that incorporating human models signing the entire content or synthesized avatars would greatly enhance future iterations.

### Open Question 3
How does the performance of LLM-based gloss translation compare to human translations in terms of accuracy and naturalness for a larger set of tasks? The paper mentions that BERTScore is the best indicator of translation success, achieving a score of 0.80, and that manual correction of LLM-generated glosses using rule-based heuristics is performed for quality control.

## Limitations

- Reliance on pre-existing ASL video datasets limits ability to handle novel or domain-specific vocabulary
- Evaluation metrics focus on technical performance and user ratings but lack detailed qualitative analysis of comprehension quality
- Manual gloss curation process, while reduced, still represents a bottleneck for full automation and scaling

## Confidence

**High Confidence**: The deployment feasibility claim is well-supported by successful 5-month production deployment with consistent user ratings comparable to non-signed variants. The retrieval accuracy metrics (Hit Rate ~0.98, Recall@1 0.80-0.98) provide strong evidence for the video retrieval mechanism's effectiveness.

**Medium Confidence**: The scalability claim regarding LLM-based gloss translation is moderately supported by reduction in manual curation time from 10 to 1 minute per sentence. However, long-term sustainability depends on continued quality of LLM outputs and effectiveness of rule-based quality control heuristics.

**Low Confidence**: The cognitive load reduction claim, while grounded in expert consultation, lacks direct empirical validation through cognitive load measurements or controlled user studies comparing different interface designs.

## Next Checks

1. Conduct controlled user studies measuring actual comprehension of signed instructions versus text-only instructions, including both immediate recall tests and task completion rates for complex multi-step instructions.

2. Track system performance over extended deployment periods to identify degradation patterns in gloss quality, retrieval accuracy, or user satisfaction that may emerge as the system handles diverse and novel instruction types.

3. Test the gloss translation and retrieval pipeline with sign languages other than ASL (e.g., British Sign Language, Auslan) to evaluate whether the LLM-based approach generalizes across different sign language grammars and video corpora.