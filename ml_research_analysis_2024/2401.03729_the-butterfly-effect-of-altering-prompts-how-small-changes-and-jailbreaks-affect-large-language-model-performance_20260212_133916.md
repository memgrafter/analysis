---
ver: rpa2
title: 'The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks
  Affect Large Language Model Performance'
arxiv_id: '2401.03729'
source_url: https://arxiv.org/abs/2401.03729
tags:
- format
- prompt
- text
- chatgpt
- python
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers and practitioners often rely on Large Language Models
  (LLMs) to label text data across various domains and tasks by simply asking the
  LLM for an answer, or "prompting." This prompting process involves several decisions,
  such as the wording of the prompt, the output format, and the use of jailbreaks
  for sensitive topics. However, the impact of these variations on the LLM's performance
  and reliability is not well understood.
---

# The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance

## Quick Facts
- arXiv ID: 2401.03729
- Source URL: https://arxiv.org/abs/2401.03729
- Reference count: 14
- Minor prompt variations can change LLM predictions by over 10% across 11 benchmark tasks

## Executive Summary
This study investigates how semantic-preserving prompt variations affect Large Language Model performance across 11 benchmark text classification tasks. Using ChatGPT and various Llama models, researchers systematically explored three types of prompt variations: output formats, perturbations, and jailbreaks. The results reveal that even minor prompt changes can significantly impact LLM predictions, with some variations leading to changes in over 10% of predictions. Jailbreaks had the most substantial impact, causing invalid responses in approximately 90% of cases. The study demonstrates that performance varies across tasks and models, with no single variation consistently outperforming others, highlighting the need for careful consideration of prompt variations when using LLMs for data labeling.

## Method Summary
The researchers conducted experiments across 11 benchmark text classification datasets using ChatGPT (gpt-3.5-turbo-1106) and Llama models (7B, 13B, 70B) with temperature set to 0 for deterministic outputs. They tested three types of prompt variations: output formats (JSON, XML, CSV, etc.), perturbations (adding spaces, greetings), and jailbreaks (AIM, Dev Mode v2, Evil Confidant, Refusal Suppression). The study measured prediction changes and accuracy for each variation across all tasks, comparing results to annotator disagreement entropy to assess whether changes were meaningful rather than spurious.

## Key Results
- Minor prompt variations caused prediction changes exceeding 10% in some cases
- Jailbreaks had the most substantial impact, producing invalid responses in ~90% of cases
- No single prompt variation consistently outperformed others across all tasks and models
- Larger models demonstrated greater robustness to prompt variations compared to smaller models

## Why This Works (Mechanism)
The study reveals that LLMs are highly sensitive to subtle prompt variations due to their complex internal representations and decision-making processes. The models appear to interpret even semantically equivalent prompts differently, leading to divergent predictions. This sensitivity is particularly pronounced for jailbreak prompts, which attempt to circumvent safety measures and often result in invalid or refused responses. The mechanism behind this sensitivity likely involves the model's attention patterns and contextual understanding, which can be disrupted by minor formatting or wording changes.

## Foundational Learning
- **Prompt engineering principles**: Understanding how different prompt formulations affect model behavior is crucial for reliable LLM deployment. Quick check: Test multiple prompt variations on a small sample before full-scale implementation.
- **Semantic preservation vs. practical equivalence**: Even semantically equivalent prompts can yield different results due to model sensitivities. Quick check: Compare model outputs across multiple semantically similar prompts.
- **Jailbreak mechanics**: Safety mechanisms in LLMs can be bypassed through specific prompt formulations, leading to invalid responses. Quick check: Monitor response validity rates when using jailbreak techniques.
- **Model size robustness**: Larger models appear more robust to prompt variations than smaller ones. Quick check: Compare variation sensitivity across different model sizes for critical applications.
- **Task-specific performance variation**: The effectiveness of prompt variations differs significantly across task types. Quick check: Validate prompt variations on representative samples from each task type before full deployment.
- **Temperature effects on variability**: Deterministic outputs (temperature=0) reduce but don't eliminate variation sensitivity. Quick check: Test with typical production temperature settings to assess real-world variability.

## Architecture Onboarding
**Component Map**: Data → Prompt Generator → LLM API → Output Parser → Evaluation Metrics
**Critical Path**: The evaluation pipeline where prompt variations flow through the LLM and are parsed and compared against ground truth labels.
**Design Tradeoffs**: The study prioritizes comprehensive variation testing over exploring fewer variations in greater depth. This provides broad coverage but may miss nuanced interactions between specific variations.
**Failure Signatures**: High invalid response rates (>90%) indicate jailbreak failures; inconsistent performance across tasks suggests task-specific sensitivity patterns; no clear "best" variation indicates inherent model instability to prompt changes.
**Three First Experiments**: 1) Test temperature variation (0 vs. 0.7) on prediction stability, 2) Compare jailbreak formulations across different model families, 3) Evaluate cross-task generalizability by applying prompt variations to non-classification tasks.

## Open Questions the Paper Calls Out
1. **Output format interactions**: How do different output format specifications interact with each other and with other prompt variations to affect model performance? The paper found that JSON specification and ChatGPT's JSON Checkbox yielded significantly different predictions despite sharing the exact same prompts.
2. **Underlying sensitivity mechanisms**: What are the underlying reasons for the model's sensitivity to minor prompt variations, and how can we develop more robust models that are less affected by such variations? The study observed sensitivity to minor changes like adding spaces but didn't investigate the underlying mechanisms.
3. **Task/domain generalizability**: How does the model's sensitivity to prompt variations differ across different types of tasks and domains? The paper explored effects across various classification tasks but didn't investigate how these effects might generalize to other task types or domains.

## Limitations
- The study examined only 11 text classification tasks, which may not represent the full diversity of LLM applications
- Results were obtained with temperature=0, which may not reflect real-world usage where temperature is often >0
- Semantic preservation of prompt variations was not always guaranteed, particularly for jailbreaks
- The analysis focused on prediction changes rather than examining the quality or validity of responses themselves
- The study used specific model versions without exploring how results might vary across different architectures

## Confidence
- **High confidence**: Minor prompt variations lead to statistically significant prediction changes (>10%) across multiple tasks and models; no single variation consistently outperforms others
- **Medium confidence**: Specific percentages of prediction changes and accuracy differences across individual tasks; jailbreaks produce the most substantial impact
- **Low confidence**: Practical implications for real-world LLM deployment under typical production conditions with temperature >0

## Next Checks
1. **Replicate with temperature >0**: Repeat experiments with temperature settings commonly used in production (e.g., 0.7-1.0) to assess how variability changes under realistic conditions
2. **Validate jailbreak robustness**: Test alternative jailbreak formulations and measure their success rates across different model families to determine if the ~90% invalid response rate is consistent
3. **Cross-task generalizability check**: Apply the same prompt variation methodology to a broader range of task types beyond text classification to assess generalizability across different LLM use cases