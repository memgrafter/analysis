---
ver: rpa2
title: 'Sparsing Law: Towards Large Language Models with Greater Activation Sparsity'
arxiv_id: '2411.02335'
source_url: https://arxiv.org/abs/2411.02335
tags:
- activation
- sparsity
- ratio
- arxiv
- relu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates activation sparsity in large language models,
  proposing a performance-aware metric and analyzing its scaling properties. The authors
  find that ReLU activation leads to greater sparsity than SiLU, with sparsity increasing
  with more training data for ReLU models and decreasing for SiLU models.
---

# Sparsing Law: Towards Large Language Models with Greater Activation Sparsity

## Quick Facts
- **arXiv ID**: 2411.02335
- **Source URL**: https://arxiv.org/abs/2411.02335
- **Reference count**: 40
- **Primary result**: 2.4B ReLU-activated model achieves 93.52% sparsity and 4.1× inference speedup with 800B tokens

## Executive Summary
This paper investigates activation sparsity in large language models, proposing a performance-aware metric (CETT-PPL-1%) and analyzing its scaling properties. The authors find that ReLU activation yields significantly higher sparsity than SiLU, with sparsity increasing with more training data for ReLU models but decreasing for SiLU models. They also show that smaller width-depth ratios and parameter scales lead to higher sparsity, with a scale-insensitive limit value. A 2.4B ReLU-activated model trained with 800B tokens achieves 93.52% sparsity and 4.1× inference speedup, demonstrating practical benefits of these findings.

## Method Summary
The paper proposes a performance-aware activation sparsity metric (CETT-PPL-1%) that uses binary search to find layer-wise activation thresholds while limiting validation perplexity increase to 1%. Models from 0.1B to 2.4B parameters are trained using µP Transformer architecture with gated FFN, varying width-depth ratios, activation functions (ReLU/SiLU), and pre-training data amounts. The training procedure uses optimal batch sizes and learning rates, with evaluation on commonsense reasoning and reading comprehension benchmarks. Sparsity is measured at different training stages and model sizes to analyze scaling properties.

## Key Results
- ReLU activation achieves significantly higher sparsity than SiLU (93.52% vs lower values)
- Sparsity increases with training data for ReLU models (logspace power-law) but decreases for SiLU models (vanilla power-law)
- Smaller width-depth ratios yield higher sparsity but risk training instability
- Sparsity limit is scale-insensitive across model sizes
- 2.4B ReLU model achieves 4.1× inference speedup with 93.52% sparsity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ReLU activation yields greater activation sparsity than SiLU due to its zero-threshold gating behavior.
- **Mechanism**: ReLU outputs zero for negative inputs, directly creating neuron-level sparsity in the feedforward network. SiLU uses a smooth sigmoid-weighted linear function, which rarely produces exact zeros and thus maintains higher activation ratios.
- **Core assumption**: The sparsity metric CETT-PPL-1% accurately captures weakly-contributed neurons without performance loss.
- **Evidence anchors**:
  - [abstract]: "We find that ReLU activation leads to greater sparsity than SiLU, with sparsity increasing with more training data for ReLU models and decreasing for SiLU models."
  - [section]: "As demonstrated by Figure 2 and Figure 3, CETT obtains the best trade-off between sparsity and performance."
  - [corpus]: Weak—no direct citation of ReLU vs SiLU sparsity comparison in neighbor papers.
- **Break condition**: If alternative activation functions or metrics are used that do not distinguish near-zero from exact-zero contributions, the sparsity advantage of ReLU may diminish.

### Mechanism 2
- **Claim**: Larger training data increases ReLU sparsity via logspace power-law but decreases SiLU sparsity via vanilla power-law.
- **Mechanism**: For ReLU, the activation ratio follows A(D) = exp(-cD^α + b) + A₀, converging to a low limit A₀ as D→∞. For SiLU, A(D) = -c/D^α + A₀, increasing toward A₀. This divergent scaling behavior reflects ReLU's ability to exploit more data for sparsity gains.
- **Core assumption**: The fitted power-law curves accurately model the training dynamics and that the limit A₀ represents true convergence.
- **Evidence anchors**:
  - [abstract]: "The activation ratio (i.e., 1−sparsity ratio) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively."
  - [section]: "For ReLU models, we observe a logspace power-law relationship between the activation ratio AReLU(D) and the amount of pre-training data D, expressed as follows: AReLU(D) = exp(−cDα + b) + A₀."
  - [corpus]: Weak—no direct modeling of sparsity-data curves in neighbor papers.
- **Break condition**: If the training dynamics deviate from the assumed power-law (e.g., due to curriculum learning or data mixing), the predicted sparsity trends may not hold.

### Mechanism 3
- **Claim**: Activation sparsity is weakly correlated with parameter scale due to scale-insensitive neuron specialization patterns.
- **Mechanism**: The distribution of neuron activation frequencies across datasets and tokens is similar across model scales, implying that specialization into functional groups is independent of total neuron count. Smaller models converge faster to the sparsity limit because fewer neurons must be specialized.
- **Core assumption**: Neuron specialization is the dominant driver of activation sparsity and follows similar patterns across scales.
- **Evidence anchors**:
  - [abstract]: "Given similar width-depth ratios, the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale."
  - [section]: "We try to explain these phenomena in Section 5.3, indicating a similar neuron specialization pattern across models of distinct scales."
  - [corpus]: Weak—no explicit modeling of scale-insensitivity in neighbor papers.
- **Break condition**: If architectural changes (e.g., MoE, different layer types) or training objectives alter specialization patterns, scale-insensitivity may break.

## Foundational Learning

- **Concept**: Activation sparsity as a dynamic, input-dependent measure of weakly-contributed neurons in feedforward networks.
  - **Why needed here**: Understanding that sparsity is not static (unlike weight pruning) but depends on activation patterns is essential to interpret the scaling laws and design efficient models.
  - **Quick check question**: Does setting a neuron's activation to zero always mean it contributes nothing to the output? (Answer: Only if the metric correctly identifies weakly-contributed neurons without harming performance.)

- **Concept**: Power-law and logspace power-law fitting for modeling sparsity scaling.
  - **Why needed here**: The paper uses these functional forms to capture how sparsity evolves with training data, guiding predictions of convergence behavior.
  - **Quick check question**: If sparsity follows A(D) = exp(-cD^α + b) + A₀, does more data always help? (Answer: For ReLU, yes—sparsity increases; for SiLU, no—sparsity decreases.)

- **Concept**: Width-depth ratio trade-offs between sparsity and training stability.
  - **Why needed here**: The paper shows that smaller width-depth ratios yield higher sparsity but risk training instability, requiring a careful balance.
  - **Quick check question**: If a model is too deep relative to width, what happens to sparsity and training? (Answer: Sparsity increases but training may become unstable or performance may degrade.)

## Architecture Onboarding

- **Component map**: Input → FFN (Wgate, Win, Wout) → Activation (ReLU/SiLU) → CETT thresholding → Output
- **Critical path**: FFN computation → activation → CETT thresholding → sparsity ratio → performance evaluation
- **Design tradeoffs**: ReLU activation boosts sparsity but may reduce representational richness; smaller width-depth ratios increase sparsity but risk training instability; more training data improves ReLU sparsity but may hurt SiLU sparsity.
- **Failure signatures**: High sparsity but poor downstream performance indicates over-aggressive thresholding; training instability with small width-depth ratio; stagnant sparsity despite more data suggests power-law fitting is invalid.
- **First 3 experiments**:
  1. Train 0.1B models with ReLU and SiLU, measure sparsity via CETT-PPL-1%, verify ReLU yields higher sparsity.
  2. Vary width-depth ratio (e.g., 50, 100, 150) on 0.1B ReLU model, plot sparsity vs ratio, confirm bottleneck point.
  3. Train 0.1B and 0.8B ReLU models with increasing tokens, fit power-law, verify scale-insensitive convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence of neuron specialization differ between ReLU and SiLU activation functions?
- Basis in paper: [explicit] The paper observes that SiLU models show a convergent increasing power-law relationship between activation ratio and training data, while ReLU models show a convergent decreasing logspace power-law. It also notes that smaller models converge faster to their respective limits.
- Why unresolved: While the paper demonstrates these different convergence patterns, it does not provide a mechanistic explanation for why ReLU and SiLU would lead to fundamentally different specialization dynamics during training.
- What evidence would resolve it: Detailed analysis of how neuron activation patterns evolve during training for both activation functions, potentially through visualization of neuron specialization trajectories or examination of how different neuron types emerge and stabilize.

### Open Question 2
- Question: What is the optimal width-depth ratio that balances sparsity benefits with training stability across different model scales?
- Basis in paper: [explicit] The paper finds that activation ratio increases linearly with width-depth ratio below a bottleneck point, suggesting deeper models are sparser, but also notes that extremely small width-depth ratios cause significant performance degradation due to training instability.
- Why unresolved: The paper identifies a general trend but does not provide specific guidance on how to determine the optimal width-depth ratio for different model scales or how this optimal point scales with parameter count.
- What evidence would resolve it: Systematic experiments varying width-depth ratios across multiple model scales with careful monitoring of both sparsity levels and training stability metrics, potentially revealing scaling laws for optimal width-depth ratios.

### Open Question 3
- Question: How does the choice of pre-training data distribution affect activation sparsity patterns?
- Basis in paper: [explicit] The paper notes that models show similar activation frequency distributions across different scales when analyzing dataset-wise patterns, and mentions that LLMs tend to be sparser on more formatted datasets like code, but does not systematically study how different data distributions affect sparsity.
- Why unresolved: While the paper touches on data mixing policies and different dataset types, it does not conduct controlled experiments to isolate the effects of different data distributions on activation sparsity, nor does it explain the underlying mechanisms.
- What evidence would resolve it: Controlled experiments training identical models on different pre-training datasets while measuring activation sparsity patterns, potentially revealing how data characteristics (e.g., formatting, domain, complexity) influence neuron specialization and sparsity emergence.

## Limitations
- **Metric sensitivity**: CETT-PPL-1% relies on 1% PPL tolerance which may not generalize across all use cases
- **Power-law fitting validity**: Empirical fitting may not capture all training dynamics, especially non-standard schedules
- **Architecture specificity**: Findings based on gated FFN Transformers may not transfer to alternative architectures

## Confidence
- **High Confidence**: ReLU activation yields higher activation sparsity than SiLU (supported by direct experimental comparison across model scales and training stages)
- **Medium Confidence**: Scale-insensitive neuron specialization patterns and sparsity convergence limits (supported by empirical observations but lacking mechanistic explanation beyond correlation)
- **Medium Confidence**: Power-law/logspace power-law relationships between activation ratio and training data (well-fitted to data but based on limited data points and specific training regimes)

## Next Checks
1. **Metric Robustness Test**: Apply CETT-PPL-1% to models trained on different datasets (e.g., code, scientific text) and evaluate whether the 1% PPL tolerance consistently preserves task performance across domains.
2. **Architecture Transfer Experiment**: Replicate sparsity scaling experiments on MoE architectures and attention-only models to verify whether scale-insensitive specialization patterns hold beyond gated FFN Transformers.
3. **Training Dynamics Validation**: Train models with curriculum learning or mixed-precision pretraining to test whether the power-law sparsity scaling relationships remain valid under non-standard training schedules.