---
ver: rpa2
title: Not All Language Model Features Are One-Dimensionally Linear
arxiv_id: '2405.14860'
source_url: https://arxiv.org/abs/2405.14860
tags:
- layer
- features
- circle
- representations
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the linear representation hypothesis (LRH)
  by showing that some language model features are inherently multi-dimensional rather
  than one-dimensional. The authors formalize multi-dimensional features using statistical
  tests for irreducibility based on separability and mixture indices, then develop
  a scalable method using sparse autoencoders (SAEs) to automatically discover these
  features in GPT-2 and Mistral 7B.
---

# Not All Language Model Features Are One-Dimensionally Linear

## Quick Facts
- arXiv ID: 2405.14860
- Source URL: https://arxiv.org/abs/2405.14860
- Reference count: 40
- Shows that some language model features are inherently multi-dimensional rather than one-dimensional

## Executive Summary
This paper challenges the linear representation hypothesis by demonstrating that some language model features are fundamentally multi-dimensional rather than reducible to one-dimensional vectors. Using sparse autoencoders on GPT-2 and Mistral 7B, the authors discover circular representations for days of the week and months of the year, and prove through causal interventions that these circular features are the fundamental computational units for modular arithmetic tasks. The findings suggest that current superposition hypotheses need updating to account for irreducible multi-dimensional features, which are necessary to mechanistically decompose certain model behaviors.

## Method Summary
The authors develop a statistical framework to identify multi-dimensional features using tests for irreducibility based on separability and mixture indices. They apply sparse autoencoders (SAEs) to automatically discover these features in transformer language models, specifically GPT-2 and Mistral 7B. The method involves extracting feature activations, testing whether they can be decomposed into linear combinations of one-dimensional features, and validating findings through causal intervention experiments. The circular representations are analyzed for continuity properties, showing that intermediate quantities map to expected positions on the circle.

## Key Results
- Discovered circular representations for days of the week and months of the year in GPT-2 and Mistral 7B
- Causal intervention experiments prove these circular features are fundamental computational units for modular arithmetic tasks
- Demonstrated continuity in circular representations where intermediate quantities map to expected positions on the circle
- Statistical tests successfully identify irreducible multi-dimensional structure that cannot be decomposed into one-dimensional features

## Why This Works (Mechanism)
The circular representations work because certain temporal concepts have inherent cyclic structure that cannot be faithfully captured by linear projections. Days of the week and months of the year naturally form closed loops where positions have meaningful relationships to adjacent positions. The SAEs successfully extract these structures because they reconstruct activations while promoting sparsity, allowing the discovery of compact geometric representations. The causal interventions demonstrate that these circular features are not just correlated with task performance but are actually necessary for computation, as ablating them directly impairs the model's ability to perform temporal reasoning.

## Foundational Learning
- **Sparse Autoencoders (SAEs)**: Neural networks that learn compressed representations by reconstructing inputs while enforcing sparsity in hidden layers. Why needed: SAEs can discover compact feature representations that might be missed by other methods. Quick check: Verify that extracted features are indeed sparse (most activations near zero).
- **Linear Representation Hypothesis**: The assumption that model features can be represented as linear combinations of one-dimensional vectors. Why needed: Provides baseline against which to test for multi-dimensional structure. Quick check: Test whether features can be linearly decomposed before declaring them multi-dimensional.
- **Causal Intervention**: Techniques that modify model internals to test causal relationships between features and outputs. Why needed: Demonstrates that circular features are not just correlated but actually necessary for computation. Quick check: Verify that interventions produce measurable changes in model behavior.

## Architecture Onboarding
**Component map:** Input text -> Transformer layers -> SAE feature extraction -> Statistical irreducibility tests -> Circular representation identification -> Causal intervention validation

**Critical path:** Feature extraction (SAE) -> Statistical testing (separability/mixture indices) -> Intervention experiments (causal validation)

**Design tradeoffs:** SAE-based discovery vs. manual feature engineering - automated discovery scales better but may miss subtle features; statistical tests provide rigor but may not capture all multi-dimensional structures; intervention experiments provide causal proof but are resource-intensive.

**Failure signatures:** Linear features incorrectly identified as circular (false positives); missing circular features due to insufficient activation strength; intervention effects confounded by model capacity to route around damaged features.

**First experiments to run:** 1) Apply SAE to small transformer layer and visualize extracted feature activations; 2) Run statistical tests on known circular data to validate detection method; 3) Perform simple causal intervention on synthetic task to verify methodology.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability of circular representations to other domains and model architectures remains uncertain
- SAE-based discovery method may miss features that don't activate strongly enough for sparse reconstruction
- Statistical tests for irreducibility may not capture all forms of multi-dimensional structure

## Confidence
**Existence and nature of multi-dimensional features** (Medium confidence): Strong evidence for circular representations in specific cases, but broader validation needed across tasks and architectures.
**Intervention experiments demonstrating computational necessity** (High confidence): Methodologically sound causal interventions on Mistral 7B and Llama 3 8B.
**Implications for mechanistic interpretability** (Medium confidence): Logically compelling argument for updating superposition hypotheses, but practical implications need exploration.

## Next Checks
1. Test whether circular representations exist for other cyclic concepts (clock times, seasons, musical scales) across different model families and scales.
2. Conduct ablation studies where circular features are removed or perturbed to assess impact on broader temporal reasoning tasks beyond modular arithmetic.
3. Develop and validate complementary detection methods for multi-dimensional features that don't rely on SAE reconstruction, such as gradient-based attribution or probing classifiers.