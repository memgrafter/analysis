---
ver: rpa2
title: 'Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace
  for Clean Signals'
arxiv_id: '2403.16176'
source_url: https://arxiv.org/abs/2403.16176
tags:
- adversarial
- clean
- subspace
- features
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of deep neural networks
  (DNNs) to adversarial attacks and proposes a novel defense strategy called subspace
  defense. The authors analyze the spectral properties of features from clean signals
  and adversarial perturbations, showing that they lie in low-dimensional linear subspaces
  with minimal overlap.
---

# Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals

## Quick Facts
- arXiv ID: 2403.16176
- Source URL: https://arxiv.org/abs/2403.16176
- Authors: Rui Zheng; Yuhao Zhou; Zhiheng Xi; Tao Gui; Qi Zhang; Xuanjing Huang
- Reference count: 0
- One-line primary result: Subspace defense improves robustness by learning a low-dimensional subspace for clean signals and using HSIC to disentangle them from adversarial perturbations

## Executive Summary
This paper proposes a novel defense strategy called subspace defense against adversarial attacks on deep neural networks. The key insight is that features of clean signals and adversarial perturbations lie in low-dimensional linear subspaces with minimal overlap. The authors introduce an auxiliary linear layer to learn a subspace for clean signals and use the Hilbert-Schmidt Independence Criterion (HSIC) to ensure independence between preserved and discarded features, effectively removing adversarial perturbations while maintaining task performance. Experimental results show significant improvements in robustness across multiple NLP tasks while maintaining accuracy on clean data.

## Method Summary
The subspace defense method learns a low-dimensional linear subspace for clean signals using an auxiliary linear layer. It projects features onto this subspace to remove adversarial perturbations and applies HSIC regularization to ensure independence between preserved and discarded features. The model is trained with a combined loss function including reconstruction loss, HSIC regularization, and adversarial loss. This approach accelerates robustness convergence during adversarial training and outperforms five competitive baselines in both clean accuracy and robust evaluation across text classification, natural language inference, and paraphrase identification tasks.

## Key Results
- Outperforms five competitive baselines in both clean accuracy and robust evaluation metrics
- Achieves significant robustness improvements across text classification, natural language inference, and paraphrase identification tasks
- Subspace dimension between 5-10 provides optimal tradeoff between accuracy and robustness
- HSIC regularization further reduces residual adversarial perturbations while maintaining feature independence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial perturbations lie in low-dimensional linear subspaces with minimal overlap with clean signal subspaces
- Mechanism: By projecting features onto the learned clean signal subspace, the method discards features outside this subspace, effectively removing adversarial perturbations while retaining task-relevant features
- Core assumption: Features of clean signals and adversarial perturbations span distinct, low-dimensional linear subspaces with minimal overlap
- Evidence anchors:
  - [abstract] "we first empirically show that the features of either clean signals or adversarial perturbations are redundant and span in low-dimensional linear subspaces respectively with minimal overlap"
  - [section] "we empirically show that the features of either clean signals or adversarial perturbations are redundant and span in low-dimensional linear subspaces respectively with minimal overlap, and the classical low-dimensional subspace projection can suppress perturbation features out of the subspace of clean signals"
  - [corpus] Weak evidence. Corpus papers discuss adversarial examples and detection but do not specifically address the low-dimensional subspace property claimed here
- Break condition: If clean signals and adversarial perturbations share significant feature space overlap, the projection would remove both, degrading performance

### Mechanism 2
- Claim: The Hilbert-Schmidt Independence Criterion (HSIC) ensures independence between preserved and discarded features, reducing residual adversarial perturbations
- Mechanism: HSIC measures the statistical independence between features kept in the subspace and those discarded, preventing adversarial perturbations from contaminating the retained features
- Core assumption: Residual perturbations that cannot be completely removed by subspace projection can be further reduced by enforcing independence between preserved and discarded features
- Evidence anchors:
  - [abstract] "we propose an independence criterion to disentangle clean signals from perturbations"
  - [section] "we introduce the Hilbert-Schmidt independence criterion (HSIC) to ensure independence between preserved and discarded features, reducing the residual adversarial perturbations"
  - [corpus] No direct evidence in corpus. HSIC is mentioned in context of feature selection but not specifically for adversarial defense
- Break condition: If HSIC regularization becomes too strong, it may discard useful task-relevant features, harming accuracy

### Mechanism 3
- Claim: The subspace defense layer accelerates robustness convergence during adversarial training
- Mechanism: Learning a low-dimensional structure for clean features is easier than learning to resist adversarial examples directly, allowing faster convergence
- Core assumption: The optimization landscape for learning clean signal subspaces is smoother and easier to navigate than for learning robust features through adversarial training alone
- Evidence anchors:
  - [abstract] "Subspace defense strategy can accelerate the robustness convergence of adversarial training, thus avoid lengthy training processes"
  - [section] "Our previous results show that clean features span in a low-dimensional linear subspace. When projecting learned features into this clean subspace, it is possible to obtain both good generalization and robustness"
  - [corpus] Weak evidence. Corpus papers discuss adversarial training but do not specifically address acceleration through subspace learning
- Break condition: If the subspace learning introduces additional optimization challenges or if the dimensionality is poorly chosen, convergence benefits may disappear

## Foundational Learning

- Concept: Principal Component Analysis (PCA) and Singular Value Decomposition (SVD)
  - Why needed here: The method relies on analyzing the spectral properties of feature matrices to identify low-dimensional subspaces
  - Quick check question: Given a feature matrix H, how would you use SVD to find the principal components that capture most variance?

- Concept: Hilbert-Schmidt Independence Criterion (HSIC)
  - Why needed here: HSIC is used to measure independence between preserved and discarded features to further reduce residual perturbations
  - Quick check question: How does HSIC differ from mutual information in measuring statistical dependence between random variables?

- Concept: Adversarial Training and Projected Gradient Descent (PGD)
  - Why needed here: The method builds upon adversarial training, using PGD to generate adversarial examples for the training process
  - Quick check question: What is the key difference between standard training and adversarial training in terms of the optimization objective?

## Architecture Onboarding

- Component map: Input feature extractor (PLM) → Subspace learning module (Proj + Projb layers) → Classifier → Loss computation
- Critical path: Feature extraction → Subspace projection → Classification → Loss calculation (reconstruction + HSIC + adversarial) → Parameter update
- Design tradeoffs:
  - Subspace dimension r: Higher r preserves more features but reduces robustness; lower r increases robustness but may hurt accuracy
  - HSIC regularization weight λ: Higher λ enforces stronger independence but may discard useful features; lower λ allows more feature retention but less perturbation removal
  - Reconstruction loss weight: Balances feature preservation against perturbation removal
- Failure signatures:
  - Accuracy drops significantly on clean data: Subspace dimension too low or HSIC regularization too strong
  - Robustness remains poor: Subspace dimension too high or HSIC regularization too weak
  - Training instability: Improper balance between reconstruction loss and HSIC term
- First 3 experiments:
  1. Baseline evaluation: Implement without subspace module to establish performance baseline
  2. Dimension sensitivity: Test subspace dimensions [2, 5, 10, 20] to find optimal tradeoff point
  3. Ablation study: Test with and without HSIC term to measure its contribution to robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed subspace defense method perform against adaptive adversarial attacks that are specifically designed to evade the subspace projection mechanism?
- Basis in paper: [inferred] The paper evaluates the method against standard attack methods like BERT-Attack, TextFooler, and TextBugger, but does not address adaptive attacks
- Why unresolved: The effectiveness of the subspace defense against more sophisticated attacks that adapt to its specific defense mechanism is not explored
- What evidence would resolve it: Empirical results showing the performance of the subspace defense against adaptive attacks, and a comparison with other defense methods under the same adaptive attack scenarios

### Open Question 2
- Question: What is the impact of the subspace dimension on the trade-off between model accuracy and robustness across different NLP tasks?
- Basis in paper: [explicit] The paper discusses the effect of subspace dimension on accuracy and robustness but does not provide a comprehensive analysis across various tasks
- Why unresolved: The paper shows that a subspace dimension between 5 and 10 is optimal for some tasks, but the generalizability of this finding to other tasks is unclear
- What evidence would resolve it: A detailed study examining the optimal subspace dimension for a wide range of NLP tasks, including both text classification and more complex tasks like machine translation or summarization

### Open Question 3
- Question: How does the subspace defense method compare to other state-of-the-art adversarial training methods in terms of computational efficiency and training time?
- Basis in paper: [inferred] The paper mentions that the subspace defense method can accelerate the convergence of the training process, but does not provide a direct comparison with other methods
- Why unresolved: The paper does not quantify the computational efficiency gains or compare the training time with other adversarial training methods
- What evidence would resolve it: A comprehensive comparison of the training time and computational resources required by the subspace defense method and other state-of-the-art adversarial training methods, using the same hardware and datasets

## Limitations

- The core assumption that clean features and adversarial perturbations span distinct, low-dimensional subspaces with minimal overlap may not hold for all model architectures or attack types
- The effectiveness of HSIC regularization is primarily demonstrated empirically with limited theoretical grounding for why it should improve robustness in all scenarios
- The paper does not evaluate the defense's effectiveness against adaptive adversarial attacks specifically designed to evade the subspace projection mechanism

## Confidence

- **High confidence**: The subspace projection mechanism for removing adversarial perturbations, as this is a well-established technique in signal processing and the empirical results strongly support its effectiveness
- **Medium confidence**: The HSIC-based independence criterion for further reducing residual perturbations, as this is supported by empirical results but lacks strong theoretical justification and the corpus provides no direct evidence
- **Medium confidence**: The claim that subspace defense accelerates robustness convergence, as this is supported by experimental results but the theoretical mechanism is not fully explained

## Next Checks

1. **Subspace overlap analysis**: Systematically measure the overlap between clean feature subspaces and adversarial perturbation subspaces across different models, datasets, and attack methods to validate the core assumption

2. **HSIC ablation study**: Conduct a more extensive ablation study varying the HSIC regularization weight λ across a wider range to precisely quantify its contribution to robustness improvements

3. **Transferability testing**: Evaluate the defense's effectiveness against black-box attacks and transfer-based attacks to assess whether the subspace defense introduces new vulnerabilities not captured by white-box evaluations