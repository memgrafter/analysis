---
ver: rpa2
title: 'Evaluating LLM-based Approaches to Legal Citation Prediction: Domain-specific
  Pre-training, Fine-tuning, or RAG? A Benchmark and an Australian Law Case Study'
arxiv_id: '2412.06272'
source_url: https://arxiv.org/abs/2412.06272
tags:
- citation
- legal
- text
- citations
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the AusLaw Citation Benchmark, a dataset
  of 55,000 Australian legal instances and 18,677 unique citations, and systematically
  evaluates LLM-based approaches for legal citation prediction. Methods tested include
  direct prompting of general and law-specialized LLMs, retrieval-only systems with
  different embeddings and database granularities, and hybrid approaches combining
  LLMs with retrieval augmentation.
---

# Evaluating LLM-based Approaches to Legal Citation Prediction: Domain-specific Pre-training, Fine-tuning, or RAG? A Benchmark and an Australian Law Case Study

## Quick Facts
- arXiv ID: 2412.06272
- Source URL: https://arxiv.org/abs/2412.06272
- Reference count: 21
- LLM-based approaches for legal citation prediction show significant improvement through instruction tuning and retrieval augmentation, but a 50% performance gap remains

## Executive Summary
This paper introduces the AusLaw Citation Benchmark, a dataset of 55,000 Australian legal instances and 18,677 unique citations, and systematically evaluates LLM-based approaches for legal citation prediction. Methods tested include direct prompting of general and law-specialized LLMs, retrieval-only systems with different embeddings and database granularities, and hybrid approaches combining LLMs with retrieval augmentation. Results show that neither general nor law-specialized LLMs achieve satisfactory performance alone, with accuracy near zero. Instruction tuning even on generic open-source models significantly improves performance, with jurisdiction-specific pre-training offering additional benefits. Retrieval-based methods show promise, particularly when using RoC aggregations and law-specific embeddings, with hybrid voting ensembles and re-rankers achieving the best results. Despite improvements, a performance gap of nearly 50% remains, highlighting the benchmark's value for future research.

## Method Summary
The study fine-tunes LLaMA-3.1-8B and SaulLM-7B-Base models on the AusLaw Citation Benchmark using LoRA with 8-bit quantization, adamw_torch optimizer, 10 epochs, learning rate 2e-4, and LoRA settings of r=16, α=32, dropout=0.05. Evaluation uses Accuracy@1 and Accuracy@5 metrics on the test set. The research compares retrieval-only approaches using different embeddings (OpenAI's text-embedding-3-large and domain-specific AusLaw-embedding-v1.0) and database granularities (full case text, catchwords, RoC aggregations). Hybrid methods combine LLM predictions with retrieval results through voting ensembles and re-ranking strategies.

## Key Results
- Instruction tuning on task-specific data significantly improves citation prediction accuracy, with SaulLM-7B performance jumping from 0% to 51.7% on Text-only queries
- Domain-specific AusLaw embeddings outperform or match generic embeddings despite lower dimensionality, highlighting the importance of legal domain specialization
- Hybrid voting ensembles combining LLM predictions with retrieval results achieve the best performance among all tested approaches
- Retrieval-only methods using RoC aggregations and law-specific embeddings show strong performance, particularly when using vector databases with appropriate granularity
- A persistent 50% performance gap remains even with the best-performing methods, demonstrating the benchmark's value for future research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning general LLMs on task-specific legal citation data significantly improves citation prediction accuracy compared to pre-training alone.
- Mechanism: Instruction tuning adapts the model's weights to the specific pattern of citation reasoning, teaching it to recognize contextual cues that signal which precedent or statute should be cited.
- Core assumption: The citation prediction task has learnable patterns that can be extracted from examples, and the model's architecture can generalize from training examples to unseen cases.
- Evidence anchors:
  - [abstract] "Instruction tuning (of even a generic open-source LLM) on task-specific dataset is among the best performing solutions"
  - [section 5.1] "A significant performance boost is observed when comparing pre-trained LLMs to their instruction-tuned counterparts on our training data. For example, the SaulLM-7B model's performance jumps from 0% to 51.7% in the more challenging Text-only query setup"
- Break condition: If the training data lacks diversity in citation patterns or contains noise, the fine-tuned model may overfit and fail to generalize to new legal contexts.

### Mechanism 2
- Claim: Domain-specific embeddings trained on Australian legal corpus improve retrieval accuracy for citation prediction compared to generic embeddings.
- Mechanism: Law-specific embeddings capture semantic relationships and terminology unique to Australian legal texts, enabling better matching between query contexts and relevant citations.
- Core assumption: Legal terminology and case relationships have distinct semantic patterns that differ from general language, and these patterns can be learned from legal corpus data.
- Evidence anchors:
  - [abstract] "We highlight that database granularity along with the type of embeddings play a critical role in retrieval-based approaches"
  - [section 5.1] "it is important to acknowledge that this comparison is not entirely fair due to differences in embedding dimensionality (3072 vs. 384), the volume of data used, and the training algorithms employed. Nonetheless, we observe an overall intuitive pattern in favour of the domain-specialised AusLaw-embeddings, or at least being on-par with OpenAI's embeddings"
- Break condition: If the domain-specific embeddings are trained on too narrow a corpus or don't capture the full semantic space needed for citation matching, they may underperform generic embeddings on diverse legal queries.

### Mechanism 3
- Claim: Combining LLM-generated auxiliary citation reasons (RoCaux) with retrieval through query expansion improves citation prediction by bridging the semantic gap between queries and indexed citations.
- Mechanism: The LLM generates descriptive text about why a citation would be relevant, which is then appended to the original query to create a richer semantic representation that better matches the indexed citation reasons.
- Core assumption: The LLM can accurately infer the type of citation that would be relevant based on limited context, and this inferred context improves semantic matching in the vector space.
- Evidence anchors:
  - [section 4.3] "In this setting, given the query, the LLM was first asked to produce a potential description of a good citation. We denote this as RoCaux to underscore our deliberation in eliciting what the LLM could semantically generate as an auxiliary RoC"
  - [section 5.1] "Comparing all retrieval-based methods, excluding re-ranking, hybrid methods are consistently better than Retrieval-only. The Voting Ensemble is the best, followed by RAG and then Query Expansion"
- Break condition: If the LLM's RoCaux generation is inaccurate or hallucinates citation reasons not supported by the legal context, the expanded query may retrieve irrelevant citations.

## Foundational Learning

- Concept: Vector similarity search and embedding spaces
  - Why needed here: The retrieval-only and hybrid approaches rely on semantic matching between query texts and indexed citations using vector representations
  - Quick check question: What is the difference between cosine similarity and Euclidean distance when comparing embeddings, and which is typically used in text retrieval systems?

- Concept: Instruction fine-tuning vs. pre-training
  - Why needed here: Understanding why fine-tuning on task-specific data dramatically improves performance compared to general pre-training is crucial for replicating results
  - Quick check question: What is the key difference between pre-training (learning general language patterns) and instruction tuning (learning task-specific patterns), and why does the latter help more for citation prediction?

- Concept: Ensemble methods and voting strategies
  - Why needed here: The voting ensemble approach combines LLM predictions with retrieval results, and understanding how to weight different sources is important for implementation
  - Quick check question: How does a simple voting ensemble differ from a weighted voting ensemble, and what factors might influence the weighting scheme in a citation prediction system?

## Architecture Onboarding

- Component map: Query text → RoCaux generation by instruction-tuned LLM → query expansion with RoCaux → vector database retrieval (Top-5) → re-ranker selection → final citation prediction
- Critical path: For the best-performing approach, the critical path is: query text → RoCaux generation by instruction-tuned LLM → query expansion with RoCaux → vector database retrieval (Top-5) → re-ranker selection → final citation prediction.
- Design tradeoffs: Using larger embeddings (3072 vs 384 dimensions) provides more semantic resolution but increases computational cost and storage requirements; RoC aggregations provide better retrieval performance than raw case text but require more processing to generate; instruction tuning improves accuracy but requires labeled training data.
- Failure signatures: Poor performance on low-frequency citations (cited <20 times) indicates the model hasn't learned generalizable patterns; large gaps between Accuracy@1 and Accuracy@5 suggest the re-ranker is failing to select the correct citation from the top candidates; consistent underperformance with domain-specific embeddings suggests the embedding model hasn't captured the relevant legal semantics.
- First 3 experiments:
  1. Compare retrieval-only performance using generic vs. domain-specific embeddings on a small subset of the data to validate the embedding effectiveness hypothesis
  2. Test instruction-tuned LLM performance on Text-only queries vs. Text+RoC queries to establish the baseline improvement from fine-tuning
  3. Evaluate the voting ensemble approach by comparing it against pure retrieval and pure LLM approaches to quantify the benefit of combining methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would citation prediction performance change if we used larger-scale law-specialized LLMs (e.g., SaulLM-141B) for instruction tuning on the Australian legal corpus?
- Basis in paper: [explicit] The paper mentions computational constraints limited experiments to 7B-parameter models and that larger-scale models "may offer additional gains."
- Why unresolved: Hardware limitations prevented testing of larger models like SaulLM-141B, leaving open whether increased model capacity would significantly improve performance on this fine-grained legal task.
- What evidence would resolve it: Systematic experiments comparing 7B, 54B, and 141B parameter models after identical instruction tuning on the Australian legal corpus, measuring citation prediction accuracy.

### Open Question 2
- Question: Would citation prediction accuracy improve significantly by pre-training a law-specialized LLM exclusively on Australian legal data rather than mixed-jurisdiction legal data?
- Basis in paper: [explicit] The paper notes that SaulLM-7B was pre-trained on 94B tokens of mixed legal data with only 0.5B from Australian sources, and questions whether "specific pre-training solely on that segment of 0.5B tokens will give a more jurisdictional chance to the LLM."
- Why unresolved: Infrastructure limitations restricted pre-training experiments to 5 epochs, and only preliminary results were reported showing potential benefits of jurisdiction-specific pre-training.
- What evidence would resolve it: Full pre-training runs comparing models trained on Australian legal data only versus mixed-jurisdiction data, followed by identical instruction tuning and evaluation on the Australian citation benchmark.

### Open Question 3
- Question: What is the impact of different embedding dimensionalities on retrieval-based citation prediction performance, and would larger AusLaw-specific embeddings outperform the current 384-dimensional vectors?
- Basis in paper: [inferred] The paper acknowledges the comparison between 3072-dimensional OpenAI embeddings and 384-dimensional AusLaw embeddings is "not entirely fair" and suggests "training larger embeddings tailored to the Australian legal domain" as a promising direction.
- Why unresolved: The paper used fixed embedding sizes (3072 for OpenAI, 384 for AusLaw) without exploring how dimensionality affects performance, despite noting potential benefits of domain-specialized embeddings.
- What evidence would resolve it: Systematic experiments varying embedding dimensions for both generic and domain-specific embeddings, measuring retrieval accuracy and citation prediction performance across different dimensionalities.

## Limitations

- The benchmark's jurisdiction-specific nature (Australian law) limits generalizability to other legal systems
- Performance remains substantially below human-level accuracy, with a 50% gap still present even with the best methods
- The study focuses on a single task (citation prediction) without evaluating broader legal reasoning capabilities
- Limited exploration of alternative embedding strategies beyond the two tested models

## Confidence

- High Confidence: The fundamental finding that neither general nor law-specialized LLMs alone achieve satisfactory citation prediction performance is well-supported by the experimental results
- Medium Confidence: The claim that hybrid voting ensembles achieve the best performance, while supported by data, depends on the specific weighting scheme and may not generalize to different legal domains
- Low Confidence: The exact contribution of jurisdiction-specific pre-training versus instruction tuning is difficult to disentangle from the presented results

## Next Checks

1. **Cross-jurisdiction validation**: Test the best-performing approach on legal corpora from different jurisdictions (e.g., US, UK, EU) to assess generalizability and identify which components are domain-specific versus universal.

2. **Long-tail citation analysis**: Conduct a detailed analysis of performance on low-frequency citations (cited fewer than 20 times) to determine whether the remaining 50% performance gap is concentrated in specific citation types or distributed across the corpus.

3. **Alternative embedding comparison**: Systematically compare the current embeddings against a broader range of legal and general-purpose embeddings (including newer models) to establish whether the observed performance is approaching an embedding ceiling or if better embeddings could further improve results.