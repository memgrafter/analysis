---
ver: rpa2
title: Analyzing Chat Protocols of Novice Programmers Solving Introductory Programming
  Tasks with ChatGPT
arxiv_id: '2405.19132'
source_url: https://arxiv.org/abs/2405.19132
tags:
- students
- prompts
- programming
- education
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes how 213 introductory programming students interact
  with ChatGPT-3.5 when solving course assignments. Students submitted 2,335 chat
  prompts as they worked through programming exercises involving recursion and string
  manipulation.
---

# Analyzing Chat Protocols of Novice Programmers Solving Introductory Programming Tasks with ChatGPT

## Quick Facts
- arXiv ID: 2405.19132
- Source URL: https://arxiv.org/abs/2405.19132
- Reference count: 16
- 213 students submitted 2,335 chat prompts while solving programming exercises with ChatGPT-3.5

## Executive Summary
This study analyzes how 213 introductory programming students interact with ChatGPT-3.5 when solving course assignments. Students submitted 2,335 chat prompts as they worked through programming exercises involving recursion and string manipulation. The analysis revealed three distinct usage patterns: students seeking immediate solutions with few prompts, students engaging in more interactive problem-solving with follow-up questions, and students using ChatGPT as a "learning partner" with extensive corrections and debugging assistance. Key findings include students using an average of 10.96 prompts, declining solution requests in longer conversations, and increasing debugging assistance in more extensive interactions.

## Method Summary
The study collected chat protocols from 213 students solving introductory programming exercises with ChatGPT-3.5, resulting in 2,335 paired prompt-response entries. Researchers conducted both quantitative analysis of prompt counts, word counts, and solution request frequencies, as well as qualitative coding of problem-solving categories and interaction patterns. The analysis identified three student groups based on prompt usage (0-5, 6-11, 12+ prompts) and two distinct prompting patterns: using task descriptions directly or rephrasing problems in their own words.

## Key Results
- Students engaged in an average of 10.96 prompts per interaction, with a median of 7 prompts
- Three distinct student groups emerged based on prompt usage: 0-5 prompts, 6-11 prompts, and 12+ prompts
- Solution requests declined from 81% in low-usage groups to 41% in high-usage groups, while debugging assistance increased

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Students interact with ChatGPT in distinct behavioral groups based on prompt volume and purpose
- Mechanism: The study found three clusters of students based on the number of prompts used (0-5, 6-11, 12+) with different interaction patterns and purposes
- Core assumption: Prompt count correlates with engagement depth and learning strategy
- Evidence anchors:
  - Students engaged in approximately 10.96 prompts on average
  - Group A, comprising individuals who use 0-5 prompts; Group B, including those who use 6-11 prompts; and Group C, consisting of students who use 12 or more prompts

### Mechanism 2
- Claim: Students' interaction patterns shift from seeking solutions to using ChatGPT as a learning partner as they engage more extensively
- Mechanism: As students use more prompts, they transition from direct solution requests to debugging assistance and conceptual understanding
- Core assumption: Extended interaction with AI tools naturally evolves toward deeper learning engagement
- Evidence anchors:
  - The number of solution requests (SR) shows a declining trend across the groups
  - In Group C, the correction rate (22%) was particularly high, with 319 correction instances (COR)

### Mechanism 3
- Claim: Students exhibit distinct prompting patterns - either using task descriptions directly or rephrasing in their own words
- Mechanism: Two main interaction patterns emerged: "Task Description Prompts" where students use the given task description as initial input, and "Prompts in Own Words" where students rephrase or focus on specific problem aspects
- Core assumption: Students' approach to formulating prompts reflects their problem-solving strategy
- Evidence anchors:
  - The qualitative data analysis further reveals a difference in the categorization of problem-solving steps
  - For (1), students seem to use the given task description at least as part of their initial prompt

## Foundational Learning

- Concept: Large Language Models (LLMs) in education
  - Why needed here: Understanding what LLMs are and their capabilities/limitations is essential to contextualize the study findings
  - Quick check question: What are the key limitations of ChatGPT-3.5 when used in programming education contexts?

- Concept: Qualitative and quantitative data analysis methods
  - Why needed here: The study uses both statistical analysis and thematic coding to understand student interactions
  - Quick check question: How does the study categorize different types of student prompts and what are the four main categories?

- Concept: Programming education challenges for novices
  - Why needed here: Understanding common difficulties helps interpret why students use ChatGPT in certain ways
  - Quick check question: According to the literature cited, what are some common challenges novice programmers face?

## Architecture Onboarding

- Component map: Data collection (student chat protocols) → Quantitative analysis (prompt counts, word counts, solution requests) → Qualitative analysis (coding of prompt types and problem-solving categories) → Pattern identification (three student groups, two prompting patterns) → Implications for teaching
- Critical path: Data collection → Quantitative analysis → Qualitative analysis → Pattern identification → Implications for teaching
- Design tradeoffs: The study chose not to provide specific instructions on how to interact with ChatGPT to capture authentic behavior, potentially at the cost of missing structured interaction data
- Failure signatures: Small sample size (213 students), single institution context, potential observer effect where students modify behavior because they know they're being studied
- First 3 experiments:
  1. Replicate the study with a different programming task or concept to see if interaction patterns hold
  2. Compare interaction patterns between different AI tools (ChatGPT vs. specialized coding assistants)
  3. Test whether providing guidance on effective prompting changes the distribution of interaction patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do students' interaction patterns with ChatGPT evolve over time as they become more experienced with the tool?
- Basis in paper: [inferred] The study shows different usage patterns among students, with some using more follow-up interactions and corrections over time. However, it doesn't track changes in individual students' behavior across multiple assignments.
- Why unresolved: The study only captures a single instance of student interactions, preventing analysis of how usage patterns develop with experience.
- What evidence would resolve it: Longitudinal studies tracking the same students' interactions with ChatGPT across multiple assignments and over time would reveal how their prompting strategies and interaction patterns evolve.

### Open Question 2
- Question: What is the relationship between students' interaction patterns with ChatGPT and their learning outcomes or programming competency development?
- Basis in paper: [inferred] The study identifies different interaction patterns but doesn't correlate these with students' performance or learning gains. It mentions that students' needs for support and feedback vary.
- Why unresolved: The study focuses on describing interaction patterns without linking them to educational outcomes or competency development.
- What evidence would resolve it: Correlational studies measuring students' programming competency before and after using ChatGPT, alongside analysis of their interaction patterns, would reveal which approaches are most effective for learning.

### Open Question 3
- Question: How do students balance the use of ChatGPT for learning versus using it to obtain quick solutions, and what factors influence this balance?
- Basis in paper: [explicit] The study notes a decline in solution requests as students engage in more extensive interactions, and observes some students using ChatGPT as a "learning partner" while others seek immediate solutions.
- Why unresolved: While the study identifies different usage patterns, it doesn't explore the underlying motivations or factors that lead students to choose one approach over another.
- What evidence would resolve it: Mixed-methods studies combining interaction analysis with interviews or surveys about students' motivations, attitudes, and perceived learning goals when using ChatGPT would provide insights into the factors influencing their approach.

## Limitations
- Study based on single institution context with specific programming exercises, limiting generalizability
- Relatively small sample size of 213 students may not capture full range of interaction patterns
- Study focuses on interaction patterns without measuring whether these lead to improved learning outcomes or code quality

## Confidence
- High Confidence: The identification of three distinct usage patterns based on prompt volume (0-5, 6-11, 12+ prompts) and the quantitative analysis of solution request trends across these groups
- Medium Confidence: The characterization of interaction patterns (Task Description Prompts vs. Prompts in Own Words) and the observation that students shift from solution requests to debugging assistance as interactions extend
- Low Confidence: The study's implications for pedagogical guidance are speculative, as the research doesn't establish causal links between interaction patterns and learning outcomes

## Next Checks
1. **Replication across institutions:** Conduct the same study at multiple universities with different student populations and programming curricula to test the robustness of the identified interaction patterns
2. **Learning outcome correlation:** Track student performance and code quality improvements alongside interaction patterns to determine which usage patterns correlate with better learning outcomes
3. **Tool comparison study:** Compare interaction patterns between ChatGPT and specialized programming assistants (like GitHub Copilot) to understand whether the identified patterns are specific to general-purpose LLMs or universal across coding tools