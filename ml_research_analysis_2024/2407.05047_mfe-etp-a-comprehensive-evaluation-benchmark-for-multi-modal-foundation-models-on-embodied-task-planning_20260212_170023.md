---
ver: rpa2
title: 'MFE-ETP: A Comprehensive Evaluation Benchmark for Multi-modal Foundation Models
  on Embodied Task Planning'
arxiv_id: '2407.05047'
source_url: https://arxiv.org/abs/2407.05047
tags:
- task
- mfms
- embodied
- evaluation
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MFE-ETP, a comprehensive benchmark for evaluating
  Multi-modal Foundation Models (MFMs) on embodied task planning. The authors propose
  a systematic evaluation framework with four key capabilities: object understanding,
  spatio-temporal perception, task understanding, and embodied reasoning.'
---

# MFE-ETP: A Comprehensive Evaluation Benchmark for Multi-modal Foundation Models on Embodied Task Planning

## Quick Facts
- arXiv ID: 2407.05047
- Source URL: https://arxiv.org/abs/2407.05047
- Reference count: 40
- One-line primary result: Even advanced MFMs significantly lag behind human-level performance on embodied task planning

## Executive Summary
This paper introduces MFE-ETP, a comprehensive benchmark for evaluating Multi-modal Foundation Models (MFMs) on embodied task planning. The authors propose a systematic evaluation framework with four key capabilities: object understanding, spatio-temporal perception, task understanding, and embodied reasoning. The MFE-ETP benchmark contains over 1100 test cases across 100 embodied tasks, covering diverse scenarios and task types. An automatic evaluation platform is developed to facilitate testing of multiple MFMs. Experiments on six state-of-the-art MFMs, including GPT-4V, reveal that object type recognition and spatial perception are the main constraints for MFMs to generate correct task planning results. The results show that even advanced models significantly lag behind human-level performance on embodied task planning.

## Method Summary
The paper presents MFE-ETP, a benchmark for evaluating MFMs on embodied task planning. The benchmark consists of 1,184 test cases across 100 household tasks, derived from virtual environments BEHAVIOR-100 and VirtualHome. Tasks are categorized into five types: Q&A, embodied Q&A, planning Q&A, object prediction, and instruction following. The evaluation framework decomposes embodied task planning into four capabilities: object understanding, spatio-temporal perception, task understanding, and embodied reasoning. An automatic evaluation platform is developed to facilitate testing of multiple MFMs, with human evaluation for planning Q&A tasks. The benchmark is tested on six state-of-the-art MFMs, including GPT-4V, BLIP-2, and MiniGPT-4.

## Key Results
- GPT-4V and MiniCPM significantly outperform other MFMs, with a performance improvement of over 100% compared to the worst-performing LLaVA-1.5.
- Object type recognition and spatial perception are the main constraints for MFMs to generate correct task planning results.
- Even advanced MFMs significantly lag behind human-level performance on embodied task planning tasks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT-4V and MiniCPM outperform other MFMs in embodied task planning due to stronger multimodal reasoning capabilities.
- **Mechanism**: GPT-4V and MiniCPM likely have larger model capacity and more comprehensive training on diverse multimodal datasets, enabling them to better integrate visual and language cues for complex reasoning tasks.
- **Core assumption**: Larger model size and more diverse training data directly translate to improved performance on embodied task planning.
- **Evidence anchors**:
  - [abstract] "GPT-4V [4] and MiniCPM [6]significantly outperform the other four MFMs, with a performance improvement of over 100% compared to the worst-performing LLaV A-1.5[5]."
  - [section 4.1] "MiniCPM [6] excels the other models in recognizing object types with an average score of 44.0%."
  - [corpus] Weak corpus evidence - no direct comparison of model sizes or training data diversity between GPT-4V, MiniCPM, and other MFMs.
- **Break condition**: If the performance gap is due to factors other than model capacity and training data diversity (e.g., prompt engineering, evaluation bias), this mechanism would break.

### Mechanism 2
- **Claim**: Object type recognition and spatial perception are the main constraints for MFMs in generating correct task planning results.
- **Mechanism**: MFMs struggle with accurately identifying object types and perceiving spatial relationships, which are fundamental for understanding the environment and planning actions. This limitation propagates through higher-level reasoning tasks.
- **Core assumption**: Object recognition and spatial perception are prerequisites for successful task planning in embodied AI.
- **Evidence anchors**:
  - [abstract] "The evaluation results show that object type recognition and spatial perception are the main constraints for MFMs to generate correct task planning results."
  - [section 4.1] "MFMs generally perform worse in low-level perception capabilities such as object understanding and spatio-temporal perception."
  - [section 4.2] "In terms of action selection, the system may wrongly or repeatedly call actions, or perform actions beyond predefined ones."
- **Break condition**: If other factors (e.g., reasoning ability, knowledge base) prove to be more significant constraints than object recognition and spatial perception, this mechanism would break.

### Mechanism 3
- **Claim**: The proposed evaluation framework effectively captures the limitations of MFMs in embodied task planning.
- **Mechanism**: By decomposing embodied task planning into four key capabilities (object understanding, spatio-temporal perception, task understanding, and embodied reasoning), the framework provides a comprehensive assessment of MFMs' strengths and weaknesses across different aspects of the task.
- **Core assumption**: The four capabilities identified in the framework are sufficient to characterize the performance of MFMs on embodied task planning.
- **Evidence anchors**:
  - [abstract] "Based on the characteristics of embodied task planning, we first develop a systematic evaluation framework, which encapsulates four crucial capabilities of MFMs: object understanding, spatio-temporal perception, task understanding, and embodied reasoning."
  - [section 3.2] "Fig.2 shows example question-and-answer pairs of our benchmark on different capability dimensions and the six types of task instructions used."
  - [corpus] Weak corpus evidence - while related works on embodied AI evaluation exist, there's no direct evidence that this specific framework is comprehensive or effective.
- **Break condition**: If the framework fails to capture important aspects of embodied task planning or if other evaluation methods prove to be more effective, this mechanism would break.

## Foundational Learning

- **Concept**: Multimodal Foundation Models (MFMs)
  - **Why needed here**: Understanding the capabilities and limitations of MFMs is crucial for interpreting the results of this evaluation benchmark.
  - **Quick check question**: What are the key differences between MFMs like GPT-4V and MiniCPM compared to other MFMs evaluated in this paper?

- **Concept**: Embodied Artificial Intelligence (EAI)
  - **Why needed here**: The paper focuses on evaluating MFMs for embodied task planning, which requires understanding the unique challenges and requirements of EAI.
  - **Quick check question**: How does embodied task planning differ from traditional task planning in AI, and why is it particularly challenging for MFMs?

- **Concept**: Task Planning in Robotics
  - **Why needed here**: The evaluation benchmark is designed to assess MFMs' ability to generate task plans for robotic agents, which requires understanding the principles of task planning in robotics.
  - **Quick check question**: What are the key components of a task plan in robotics, and how do they relate to the capabilities evaluated in this paper (object understanding, spatio-temporal perception, task understanding, and embodied reasoning)?

## Architecture Onboarding

- **Component map**: Data Collection -> Benchmark Construction -> Evaluation Platform -> Result Analysis
- **Critical path**: Data Collection -> Benchmark Construction -> Evaluation Platform -> Result Analysis
- **Design tradeoffs**:
  - Virtual environments vs. real-world data: Virtual environments offer easier data collection and controlled scenarios but may lack generalizability to real-world situations.
  - Open-ended vs. constrained evaluation: Open-ended tasks (e.g., planning Q&A) are more realistic but harder to evaluate automatically, while constrained tasks (e.g., embodied Q&A) are easier to evaluate but may not fully capture the complexity of embodied task planning.
- **Failure signatures**:
  - Poor object recognition: Incorrect or incomplete identification of objects in the environment.
  - Inadequate spatial perception: Misjudging distances, directions, or spatial relationships between objects.
  - Insufficient task understanding: Failing to identify relevant objects, understand operation knowledge, or determine appropriate action sequences.
  - Ineffective embodied reasoning: Generating unrealistic or inefficient task plans that don't align with the task goals.
- **First 3 experiments**:
  1. Evaluate MFMs on a subset of the benchmark focusing on object understanding tasks to identify specific weaknesses in object recognition.
  2. Compare the performance of MFMs on embodied Q&A tasks vs. planning Q&A tasks to assess the impact of open-endedness on evaluation difficulty.
  3. Analyze the effect of providing additional information (e.g., object properties, spatial relationships) to MFMs on their performance in embodied reasoning tasks to identify the most critical information for successful task planning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MFMs on embodied task planning change when tested in more realistic, real-world environments rather than virtual simulations?
- Basis in paper: [inferred] The paper acknowledges that the MFE-ETP benchmark utilizes virtual environments for data collection, which may raise concerns about generalizability to more realistic scenarios. The authors suggest that exploring additional QA formats and testing in real-world environments could enrich the research.
- Why unresolved: The benchmark's reliance on virtual environments limits the direct applicability of the findings to real-world embodied task planning. Real-world environments introduce additional complexities such as dynamic changes, noise, and unforeseen obstacles that are not present in simulations.
- What evidence would resolve it: Conducting experiments with MFMs in real-world embodied task planning scenarios and comparing their performance to the results obtained in virtual environments would provide insights into the generalizability of the findings.

### Open Question 2
- Question: How does the incorporation of additional information, such as object detection modules and 3D information, impact the performance of MFMs on embodied task planning?
- Basis in paper: [explicit] The authors recommend augmenting MFMs with object detection modules, such as GroundingDINO, and integrating 3D information instead of relying solely on MFMs. This recommendation is based on the observation that MFMs generally perform worse in low-level perception capabilities such as object understanding and spatio-temporal perception.
- Why unresolved: While the authors suggest the potential benefits of incorporating additional information, they do not provide empirical evidence on the specific impact of these augmentations on MFM performance. The effectiveness of such enhancements needs to be evaluated in the context of embodied task planning.
- What evidence would resolve it: Conducting experiments that compare the performance of MFMs with and without the incorporation of object detection modules and 3D information on embodied task planning tasks would provide insights into the impact of these augmentations.

### Open Question 3
- Question: How does the performance of MFMs on embodied task planning vary across different types of household tasks, and what are the specific challenges associated with each task type?
- Basis in paper: [inferred] The MFE-ETP benchmark includes a diverse set of 100 household tasks, but the paper does not provide a detailed analysis of MFM performance across different task types. Understanding the specific challenges associated with each task type could help in developing more targeted approaches for improving MFM performance.
- Why unresolved: The paper presents overall performance metrics for MFMs on the benchmark but does not delve into the performance variations across different task types. Identifying the specific challenges associated with each task type could provide insights into the limitations of MFMs and guide future research directions.
- What evidence would resolve it: Analyzing the performance of MFMs on individual task types within the MFE-ETP benchmark and identifying the specific challenges associated with each task type would provide a more granular understanding of MFM capabilities and limitations.

## Limitations
- The benchmark's reliance on virtual environments may limit generalizability to real-world scenarios.
- MFMs struggle significantly with object recognition and spatial perception tasks, which are fundamental to embodied task planning.
- The performance gap between top models (GPT-4V, MiniCPM) and lower-performing ones (LLaVA-1.5) suggests substantial variability in multimodal reasoning capabilities.

## Confidence
- **High Confidence**: The finding that object type recognition and spatial perception are primary constraints for MFMs in embodied task planning, supported by consistent results across multiple evaluation dimensions.
- **Medium Confidence**: The claim that GPT-4V and MiniCPM outperform other models due to superior multimodal reasoning, as the paper doesn't provide detailed analysis of model architectures or training data differences.
- **Low Confidence**: The assertion that the proposed four-capability framework comprehensively captures all aspects of embodied task planning, given the lack of comparison with alternative evaluation frameworks.

## Next Checks
1. Validate the benchmark's generalizability by testing MFMs on real-world robotic datasets and comparing performance to virtual environment results.
2. Conduct ablation studies to isolate the impact of object recognition and spatial perception capabilities on overall task planning performance.
3. Evaluate MFMs using alternative assessment methods (e.g., human evaluation of task plan feasibility) to cross-validate the automatic evaluation platform results.