---
ver: rpa2
title: 'RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional
  Videos'
arxiv_id: '2403.18600'
source_url: https://arxiv.org/abs/2403.18600
tags:
- action
- videos
- procedure
- planning
- mixture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAP, a novel model for adaptive procedure
  planning in instructional videos, addressing limitations of prior methods that assume
  fixed-length action sequences. RAP employs an auto-regressive transformer base planner
  for flexible sequence generation and a retrieval-augmented memory module to leverage
  temporal relations between steps.
---

# RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos

## Quick Facts
- arXiv ID: 2403.18600
- Source URL: https://arxiv.org/abs/2403.18600
- Reference count: 39
- Key outcome: Novel auto-regressive planner with retrieval-augmented memory for variable-length instructional video planning, achieving up to 20.28% success rate and 62.79% mean edit-score

## Executive Summary
RAP introduces a novel approach for adaptive procedure planning in instructional videos, addressing the limitation of fixed-length sequence assumptions in prior methods. The model employs an auto-regressive transformer base planner that generates variable-length action sequences and terminates upon predicting an END token. A key innovation is the retrieval-augmented memory module that explicitly models temporal dependencies between state-action pairs by retrieving relevant examples from training data. The system is trained using weakly-supervised learning, leveraging pseudo-labels generated from unannotated task-relevant videos through video-language grounding. Evaluated on CrossTask and COIN datasets, RAP demonstrates superior performance on variable-length sequences compared to traditional fixed-length models.

## Method Summary
RAP is a retrieval-augmented planner that uses an auto-regressive transformer base planner to generate variable-length action sequences, with generation terminating when an END token is predicted. The model incorporates a retrieval-augmented memory module that stores state-action pairs from training data and retrieves the most relevant examples during inference to improve long-range sequence modeling. Training occurs in a weakly-supervised manner, where pseudo-labels for unannotated task-relevant videos are generated using a video-language grounding algorithm that aligns video frames with action steps. The system combines visual observations of initial and target states with action representations to predict procedural plans, with performance evaluated using metrics including Success Rate, Mean Edit-Score, and Mean Intersection over Union.

## Key Results
- Achieves up to 20.28% success rate and 62.79% mean edit-score on variable-length sequences
- Outperforms traditional fixed-length models on CrossTask and COIN datasets
- Demonstrates effective use of weakly-supervised learning to expand training data with unannotated videos
- Successfully handles the adaptive determination of procedure conclusion without pre-defined sequence length

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Auto-regressive base planner enables adaptive sequence length prediction by terminating when END token is generated
- **Mechanism:** The transformer base planner predicts the next action step conditioned on the current state, predicted task, and all previous actions. Generation stops when the END token is produced, allowing variable-length procedures
- **Core assumption:** The END token is reliably learnable from training data and represents task completion across varying initial and goal states
- **Evidence anchors:**
  - [abstract] "RAP adaptively determines the conclusion of actions using an auto-regressive model architecture"
  - [section] "The sequence starts with a0 = START and completes upon predicting aT+1 = END"
  - [corpus] No direct evidence found for auto-regressive termination mechanism
- **Break condition:** If the training data lacks sufficient diversity in sequence lengths or if the END token is not well-aligned with actual task completion, the model may either truncate prematurely or generate excessively long sequences

### Mechanism 2
- **Claim:** Retrieval-augmented memory explicitly models temporal dependencies between state-action pairs, improving long-range sequence prediction
- **Mechanism:** The retrieval component creates a memory of state-action pairs from training data, where each key is a learnable context vector conditioned on (vs, vg, c, lt + qt) and each value is the next action. During inference, the model retrieves the K nearest keys and interpolates their probabilities with the base planner's predictions
- **Core assumption:** The most relevant state-action pairs for the current context can be retrieved from the training memory using cosine similarity
- **Evidence anchors:**
  - [abstract] "RAP establishes an external memory module to explicitly retrieve the most relevant state-action pairs from the training videos and revises the generated procedures"
  - [section] "The nearest neighbors ({sk}Kk=1) get selected by calculating the cosine distance between ςt and the keys of the datastore {sn}Dn=1"
  - [corpus] No direct evidence found for cosine similarity retrieval effectiveness
- **Break condition:** If the memory size is too small or the context representation fails to capture task-relevant information, retrieval may return irrelevant or noisy examples, degrading performance

### Mechanism 3
- **Claim:** Weak supervision through pseudo-annotation generation enables training on task-relevant unannotated videos, expanding the effective training dataset
- **Mechanism:** A video-language grounding algorithm aligns unannotated instructional videos with their corresponding action plans by computing match costs between frame and action representations. Actions with match costs below a drop threshold are aligned to consecutive frames with minimum cumulative cost
- **Core assumption:** The grounding algorithm can accurately align video frames to action steps even when there are discrepancies between the video content and the reference plan
- **Evidence anchors:**
  - [abstract] "RAP utilizes a weakly-supervised learning manner to expand the training dataset to other task-relevant, unannotated videos by generating pseudo labels for action steps"
  - [section] "We utilize these pseudo-annotations, extracted for each relevant video, which serve as structurally and format-wise equivalent to manually annotated videos"
  - [corpus] No direct evidence found for grounding algorithm accuracy
- **Break condition:** If the grounding algorithm produces incorrect alignments (e.g., assigning actions to wrong video segments), the pseudo-annotations will contain errors that degrade model training quality

## Foundational Learning

- **Concept:** Auto-regressive sequence generation
  - **Why needed here:** Enables the model to predict variable-length action sequences where each step depends on all previous steps, which is essential for procedural planning where order matters
  - **Quick check question:** What happens to the input sequence at each generation step in an auto-regressive model?

- **Concept:** Contrastive learning for action step prediction
  - **Why needed here:** Helps the model distinguish between the correct action step representation and all other possible actions, improving prediction accuracy in a large action vocabulary
  - **Quick check question:** How does the contrastive loss function encourage the model to produce action representations that are similar to their ground truth embeddings?

- **Concept:** k-Nearest Neighbors retrieval for probability interpolation
  - **Why needed here:** Allows the model to incorporate prior knowledge from similar state-action pairs seen during training, improving predictions for long-range dependencies that may be difficult to capture with the base planner alone
  - **Quick check question:** What is the role of the interpolation parameter λ in combining the base planner and retrieval planner probabilities?

## Architecture Onboarding

- **Component map:** Visual encoder (S3D) → State representations (vs, vg) → Base planner (auto-regressive transformer) → Retrieval planner (memory + kNN) → Interpolation layer → Action vocabulary probabilities

- **Critical path:** Initial state → Base planner → Retrieval planner → Interpolation → Action prediction → Next state update → Repeat until END token

- **Design tradeoffs:**
  - Memory size vs. retrieval accuracy: Larger memory improves coverage but increases computational cost
  - Number of retrieval neighbors K vs. specificity: More neighbors provide smoother distributions but may dilute specific information
  - λ interpolation weight vs. base planner reliance: Higher λ emphasizes retrieval knowledge but may override learned patterns

- **Failure signatures:**
  - All predictions collapse to END token early → Base planner not learning termination conditions
  - Random action predictions → Retrieval memory not being populated or queried correctly
  - Stuck in loops (repeating same actions) → Positional embeddings or state tracking not functioning properly

- **First 3 experiments:**
  1. Train base planner only on fixed-length sequences and verify it can generate correct sequences up to known horizon
  2. Test retrieval component in isolation by fixing the context and verifying it returns the correct nearest neighbors from memory
  3. Evaluate interpolation with varying λ values on a validation set to find optimal balance between base and retrieval predictions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How would the performance of RAP change if it were trained on a larger, more diverse set of unannotated instructional videos?
- **Basis in paper:** [explicit] The paper mentions that RAP utilizes a weakly-supervised learning method to leverage additional unannotated task-relevant videos, and suggests that expanding this approach could further enhance performance
- **Why unresolved:** The current study only evaluates RAP's performance with unannotated data from CrossTask's related-tasks. It's unclear how RAP would generalize to a broader range of unannotated videos with varying topics, quality, and annotations
- **What evidence would resolve it:** Conducting experiments with RAP trained on a larger, more diverse set of unannotated instructional videos, and comparing its performance to the current results, would provide insights into the impact of unannotated data size and diversity on RAP's effectiveness

### Open Question 2
- **Question:** Can RAP's retrieval-augmented memory component be improved to better capture long-range temporal dependencies and handle more complex action sequences?
- **Basis in paper:** [explicit] The paper discusses RAP's retrieval-augmented memory module and its role in improving long-range sequence modeling, but suggests that further enhancements could be explored
- **Why unresolved:** The current implementation of RAP's retrieval component uses a fixed-size memory and a simple k-nearest neighbors approach. It's unclear whether more sophisticated memory architectures or retrieval mechanisms could lead to better performance
- **What evidence would resolve it:** Experimenting with different memory architectures, retrieval algorithms, and hyper-parameter settings for RAP's retrieval component, and evaluating their impact on performance, would provide insights into potential improvements

### Open Question 3
- **Question:** How would RAP's performance be affected by incorporating additional visual cues, such as object detection or scene understanding, into its state representations?
- **Basis in paper:** [inferred] The paper uses S3D network to encode visual observations, but does not explore the potential benefits of incorporating additional visual cues. The authors mention that understanding objects and their temporal relationships is crucial for procedure planning
- **Why unresolved:** The current implementation of RAP relies solely on S3D features for visual state representations. It's unclear whether incorporating additional visual cues, such as object detection or scene understanding, could enhance RAP's ability to understand the visual observations and generate more accurate plans
- **What evidence would resolve it:** Experimenting with RAP's architecture by incorporating additional visual cues, such as object detection or scene understanding, and evaluating its impact on performance, would provide insights into the potential benefits of richer visual state representations

## Limitations

- The retrieval-augmented memory mechanism's effectiveness depends heavily on the quality of state representation and the assumption that similar states have similar optimal actions, with limited ablation studies isolating the retrieval component's contribution
- The weakly-supervised learning approach introduces uncertainty about pseudo-label quality, particularly for videos with significant content variations from reference plans, with limited evidence about grounding algorithm accuracy
- The paper claims generalization to unannotated videos but provides limited evidence about how well the grounding algorithm performs in practice across diverse video content

## Confidence

- **High confidence:** The auto-regressive base planner architecture and its role in generating variable-length sequences
- **Medium confidence:** The retrieval-augmented memory mechanism's ability to improve long-range dependencies
- **Medium confidence:** The weakly-supervised learning approach for expanding training data
- **Low confidence:** The specific implementation details of the video-language grounding algorithm and its accuracy

## Next Checks

1. Conduct an ablation study removing the retrieval component to quantify its exact contribution to performance improvements
2. Manually inspect a sample of pseudo-annotations generated by the grounding algorithm to assess alignment accuracy and error rates
3. Test the model's robustness by evaluating it on videos with significant content variations from the training data to assess generalization limits