---
ver: rpa2
title: Advancing Stroke Risk Prediction Using a Multi-modal Foundation Model
arxiv_id: '2411.09822'
source_url: https://arxiv.org/abs/2411.09822
tags:
- stroke
- data
- tabular
- brain
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting stroke risk prior
  to onset using a multi-modal foundation model that integrates 3D brain MRI and clinical
  data. The proposed self-supervised contrastive learning framework combines contrastive
  language-image pretraining with an image-tabular matching module to align multimodal
  data representations in a shared latent space.
---

# Advancing Stroke Risk Prediction Using a Multi-modal Foundation Model

## Quick Facts
- arXiv ID: 2411.09822
- Source URL: https://arxiv.org/abs/2411.09822
- Reference count: 40
- Primary result: Self-supervised multi-modal model outperforms state-of-the-art methods for stroke risk prediction, achieving 2.1% higher ROC-AUC and 7.6% higher balanced accuracy than best supervised multimodal model

## Executive Summary
This paper presents a self-supervised multi-modal foundation model for predicting stroke risk using 3D brain MRI and clinical data. The framework combines contrastive language-image pretraining with an image-tabular matching module to align multimodal data representations in a shared latent space. Trained on the UK Biobank dataset, the model significantly outperforms state-of-the-art unimodal and multimodal methods, achieving 2.1% higher ROC-AUC and 10.6% higher balanced accuracy compared to self-supervised unimodal methods. The approach demonstrates improved integration of tabular and image data, providing richer and more aligned embeddings, with Grad-CAM heatmaps revealing activated brain regions associated with stroke risk.

## Method Summary
The model employs a self-supervised contrastive learning framework that integrates CLIP with an image-tabular matching (ITM) module. It uses a ResNet-50 for image encoding and an MLP for tabular data, both projecting to 128-dimensional embeddings. The model leverages hard negative mining through ITM loss to improve robustness, and a transformer-based multi-modal interaction module combines representations before ensemble classification. The approach is trained on 5000 samples from UK Biobank and evaluated on stroke risk prediction, outperforming both unimodal and multimodal supervised baselines.

## Key Results
- Achieved 2.1% higher ROC-AUC and 10.6% higher balanced accuracy compared to self-supervised unimodal methods
- Outperformed best multimodal supervised model by 7.6% in balanced accuracy
- Demonstrated better integration of tabular and image data with richer, more aligned embeddings
- Grad-CAM heatmaps identified brain regions associated with stroke risk and clinical outcomes

## Why This Works (Mechanism)

### Mechanism 1
Multi-modal self-supervised pretraining aligns image and tabular embeddings in a shared latent space, improving downstream prediction performance. CLIP loss contrasts augmented image-tabular pairs across modalities, pulling together positive pairs while pushing apart negatives, and ITM loss further refines hard negative mining to force alignment. Core assumption: Image and tabular representations contain complementary information that can be jointly optimized in a shared latent space.

### Mechanism 2
Self-supervised pretraining on large unannotated datasets improves model generalization by learning robust feature representations before fine-tuning on small labeled datasets. Using SimCLR-like augmentation and contrastive loss, the model learns to distinguish between similar and dissimilar samples without requiring labels, capturing underlying data structure. Core assumption: Large unlabeled data contains enough signal to learn generalizable features that transfer to the downstream task.

### Mechanism 3
Hard negative mining via ITM loss improves model robustness by forcing the model to distinguish between similar image-tabular pairs that are actually from different patients. ITM uses CLIP similarity scores to identify negative pairs within the batch that are most likely to be confused with positive pairs, then penalizes incorrect matches. Core assumption: There exist hard negatives in the batch that, if correctly distinguished, improve the model's discriminative ability.

## Foundational Learning

- Concept: Contrastive learning fundamentals (positive vs negative pairs, InfoNCE loss, temperature scaling)
  - Why needed here: The model's core learning mechanism relies on contrasting similar and dissimilar samples across modalities
  - Quick check question: Can you explain why temperature scaling (τ) is important in the InfoNCE loss formulation?

- Concept: Multi-modal fusion strategies (concatenation, cross-attention, transformer-based interaction)
  - Why needed here: The model must effectively combine information from image and tabular encoders beyond simple concatenation
  - Quick check question: What are the advantages of using cross-attention over simple concatenation for multi-modal fusion?

- Concept: Gradient-weighted Class Activation Mapping (Grad-CAM) for interpretability
  - Why needed here: The model uses Grad-CAM to identify brain regions contributing to stroke risk predictions
  - Quick check question: How does Grad-CAM differ from other saliency map techniques like Integrated Gradients?

## Architecture Onboarding

- Component map: Image encoder (ResNet-50) -> Projection head -> CLIP/ITM loss -> Tabular encoder (MLP) -> Projection head -> CLIP/ITM loss -> Multi-modal interaction module (Transformer) -> Ensemble classifier

- Critical path: Image/Tabular → Encoder → Projection → CLIP/ITM loss → Multi-modal interaction → Ensemble classifier → Prediction

- Design tradeoffs:
  - ResNet-50 vs DenseNet121: Memory efficiency vs feature richness
  - Frozen vs trainable encoders: Regularization vs overfitting risk
  - Batch size of 6: Memory constraints vs gradient stability
  - Augmentation rates: 95% for images, 30% for tabular to balance noise vs signal

- Failure signatures:
  - GradCAM shows no meaningful brain region activation: Model may not be learning stroke-relevant features
  - UMAP embeddings show no modality alignment: Contrastive loss may not be working
  - Performance worse than unimodal baselines: Multi-modal integration may be introducing noise
  - Training loss plateaus early: Learning rate or architecture may need adjustment

- First 3 experiments:
  1. Test contrastive learning with only image modality (SimCLR-style) to establish baseline performance
  2. Test CLIP loss with random negative pairs (no ITM) to isolate the contribution of hard negative mining
  3. Test frozen encoders vs trainable encoders on a small subset to evaluate overfitting risk

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed multi-modal model compare when using different pre-training dataset sizes? The paper uses 5000 samples for pre-training but doesn't explore the impact of varying this number.

### Open Question 2
Would incorporating longitudinal imaging data (multiple timepoints) improve stroke risk prediction accuracy? The current approach uses only a single pre-stroke T2-FLAIR scan, but doesn't explore temporal dynamics.

### Open Question 3
How does the proposed model perform on external, more diverse datasets with different demographic characteristics? The study only validates on the UK Biobank dataset, leaving questions about generalizability to other populations.

## Limitations
- Dataset representativeness: Trained on UK Biobank data which may not generalize to populations with different demographics or healthcare systems
- Temporal dynamics: Does not address how stroke risk predictions might change over time or handle longitudinal data
- Clinical integration: Lacks discussion of how predictions would be integrated into clinical workflows or interpreted by clinicians

## Confidence
- High confidence: The multi-modal architecture combining image and tabular data is technically sound and performance improvements over unimodal baselines are well-supported
- Medium confidence: The self-supervised pretraining approach's contribution to performance gains is plausible but could be more rigorously validated
- Low confidence: The claim that hard negative mining via ITM loss significantly improves performance lacks direct evidence

## Next Checks
1. External validation: Test the model on a completely independent dataset from a different population or healthcare system to assess generalizability beyond UK Biobank

2. Temporal validation: Evaluate model performance when trained on historical data and tested on more recent data to assess whether the model maintains predictive power as risk factors and populations evolve

3. Ablation study: Systematically remove or replace components (self-supervised pretraining, ITM loss, multi-modal interaction) to quantify their individual contributions to overall performance