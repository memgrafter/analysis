---
ver: rpa2
title: Token-based Decision Criteria Are Suboptimal in In-context Learning
arxiv_id: '2406.16535'
source_url: https://arxiv.org/abs/2406.16535
tags:
- hidden
- calibration
- label
- classification
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of token-based decision criteria
  in in-context learning (ICL), where classification boundaries are suboptimal due
  to manually selected label tokens. The authors propose Hidden Calibration, which
  abandons token probabilities and instead uses a nearest centroid classifier on the
  last hidden states of the language model.
---

# Token-based Decision Criteria Are Suboptimal in In-context Learning

## Quick Facts
- arXiv ID: 2406.16535
- Source URL: https://arxiv.org/abs/2406.16535
- Reference count: 40
- Primary result: Hidden Calibration outperforms token-based ICL baselines by 20-50% across 6 models and 10 datasets

## Executive Summary
This paper identifies a critical limitation in token-based decision criteria for in-context learning (ICL) and proposes Hidden Calibration as a superior alternative. The authors demonstrate that manually selected label tokens lead to suboptimal classification boundaries due to information loss during the token probability generation process. By leveraging the last hidden states of language models instead of token probabilities, Hidden Calibration achieves state-of-the-art ICL performance through a simple nearest centroid classifier, eliminating the need for complex prompt engineering and achieving better classification accuracy across diverse models and datasets.

## Method Summary
Hidden Calibration abandons token probabilities in favor of using the last hidden states from language models as classification features. The method calculates centroids of hidden states for each class from a calibration set, then assigns labels to test samples based on nearest centroid similarity. This approach requires minimal additional computational cost while reducing prompt engineering complexity. The method is evaluated on 6 language models (OPT-2.7B, Llama 2 variants, Llama 3-8B, GPT2-XL) and 10 diverse datasets, demonstrating consistent improvements over token-based baselines.

## Key Results
- Hidden Calibration outperforms vanilla ICL by 20-50% on average across all tested models and datasets
- The method achieves state-of-the-art ICL performance without requiring model fine-tuning
- Language models provide linearly separable intra-class clusters when aided by demonstrations
- Hidden Calibration finds better classification criteria with less inter-class overlap compared to token-based methods

## Why This Works (Mechanism)

### Mechanism 1
Hidden states contain more discriminative information than token probabilities for classification. Language models encode input sequences into hidden states that preserve rich semantic and structural information before it is compressed into token probabilities. This intermediate representation is more linearly separable and less biased by prior label preferences. The last hidden state of the final token captures sufficient task-relevant information to distinguish between classes when aided by demonstrations.

### Mechanism 2
Centroid-based linear classification on hidden states outperforms affine-calibrated token probability methods. By calculating centroids of hidden states for each class from a calibration set and assigning the label of the nearest centroid, Hidden Calibration finds better decision boundaries with less inter-class overlap than methods that rely on manually selected label token embeddings. Hidden states from prompts with the same query label naturally cluster together and are linearly separable.

### Mechanism 3
Demonstrations enhance intra-class clustering of hidden states, improving linear separability. As the number of demonstrations increases, hidden states corresponding to the same query label converge towards their respective class centroids, making the clusters more distinct and easier to separate with a simple linear boundary. Language models have an inherent tendency to produce hidden states that reflect the semantic content of the query, and this tendency is strengthened by relevant demonstrations.

## Foundational Learning

- **In-context learning (ICL)**: A few-shot learning paradigm where language models perform tasks by conditioning on prompts containing demonstrations without updating model parameters. Why needed: Understanding ICL is essential because Hidden Calibration is designed specifically for this paradigm. Quick check: How does ICL differ from traditional fine-tuning approaches?

- **Language model hidden states**: Intermediate vector representations produced by each layer of a transformer model, with the last hidden state of the final token being particularly informative. Why needed: Hidden Calibration relies on using these hidden states as features for classification rather than the final token probabilities. Quick check: What information is preserved in hidden states that might be lost in token probability distributions?

- **Centroid-based classification**: A simple linear classification method that assigns labels based on proximity to class centroids in feature space. Why needed: This is the core classification mechanism used in Hidden Calibration. Quick check: What are the advantages and limitations of centroid-based classification compared to more complex classifiers?

## Architecture Onboarding

- **Component map**: Prompt generation -> Language model inference -> Centroid calculation -> Classification -> Evaluation
- **Critical path**: 1) Build calibration prompts from supervised examples, 2) Run LM on calibration prompts to collect hidden states, 3) Calculate centroids for each label, 4) For inference, run LM on test prompts, 5) Compute similarity between test hidden state and each centroid, 6) Assign label of nearest centroid
- **Design tradeoffs**: Hidden states vs. token probabilities (more informative but require model internals access), centroid vs. complex classifiers (simple and efficient but may not capture complex boundaries), calibration set size (larger sets improve estimates but require more labeled data), prompt design (affects hidden state quality but adds complexity)
- **Failure signatures**: Poor performance despite correct implementation (hidden states may not be well-separated), inconsistent results across runs (insufficient calibration data or noisy hidden state extraction), degradation with more demonstrations (conflicting information introduced), high variance in results (calibration set too small or unrepresentative)
- **First 3 experiments**: 1) Implement Hidden Calibration on a simple binary classification task and compare against vanilla ICL, 2) Test sensitivity to calibration set size by varying m and measuring performance, 3) Visualize hidden state clusters for different numbers of demonstrations to verify clustering behavior

## Open Questions the Paper Calls Out

### Open Question 1
How can we automatically select optimal label tokens in ICL prompts without relying on human intuition? The paper acknowledges that human intuition in label token choice is not reliable and mentions this as an important issue for future research directions. The authors explicitly state they have not eliminated human intuition completely from the ICL loop when building prompts and leave this as future work.

### Open Question 2
Why do some models (like GPT2-XL) not benefit from demonstrations even when analyzed through hidden state clustering or Hidden Calibration? The authors note that some models do not benefit from demonstrations even when they are expected to, suggesting there may be model-specific characteristics that affect the effectiveness of demonstrations in improving hidden state clustering.

## Limitations

- Hidden state representativeness: Assumes last hidden state captures sufficient task-relevant information, but this may not hold universally across all tasks or model architectures
- Template sensitivity: Success of Hidden Calibration could be highly dependent on prompt template design, which is not fully characterized
- Model-specific behaviors: Different models exhibit varying performance patterns, particularly in how demonstrations affect hidden state clustering

## Confidence

**High Confidence Claims**:
- Hidden Calibration outperforms token-based baselines by 20-50% on average across multiple models and datasets
- The method requires minimal additional computational cost compared to baseline methods
- Hidden states are more linearly separable than token probabilities when aided by demonstrations

**Medium Confidence Claims**:
- Demonstrations enhance intra-class clustering of hidden states
- Language models provide linearly separable intra-class clusters with the help of demonstrations
- Hidden Calibration finds better classification criteria with less inter-class overlap

**Low Confidence Claims**:
- The specific reasons why certain models show different behavior patterns with varying numbers of demonstrations
- Whether improvements would scale to much larger or smaller models than those tested
- Generalizability of findings to tasks beyond text classification

## Next Checks

1. **Template Sensitivity Analysis**: Systematically test Hidden Calibration with 5-10 different prompt templates to determine how sensitive the method is to template design, and whether certain templates consistently outperform others across datasets.

2. **Hidden State Layer Selection**: Experiment with using hidden states from different layers (not just the last layer) and different tokens (not just the final token) to determine if alternative hidden state selections could yield better performance for specific tasks or model architectures.

3. **Calibration Set Size Sensitivity**: Conduct a thorough ablation study varying the calibration set size (m) across orders of magnitude (e.g., m âˆˆ {4|Y|, 8|Y|, 16|Y|, 32|Y|, 64|Y|}) to identify the optimal calibration set size and understand the trade-off between calibration data requirements and classification accuracy.