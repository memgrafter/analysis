---
ver: rpa2
title: Comparing Discrete and Continuous Space LLMs for Speech Recognition
arxiv_id: '2409.00800'
source_url: https://arxiv.org/abs/2409.00800
tags:
- speech
- continuous
- discrete
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically compares discrete and continuous speech
  representations in LLM-based ASR, organizing them into supervised and unsupervised
  categories for both types. We design specialized encoders and models for four distinct
  speech representation categories and conduct extensive comparative analysis using
  both a Joint-Training-From-Scratch Language Model (JTFS LM) and pre-trained LLaMA2-7b.
---

# Comparing Discrete and Continuous Space LLMs for Speech Recognition

## Quick Facts
- arXiv ID: 2409.00800
- Source URL: https://arxiv.org/abs/2409.00800
- Reference count: 0
- Key outcome: Continuous representations consistently outperform discrete ones due to lower information loss; achieves state-of-the-art open-sourced WER of 1.69% on LibriSpeech

## Executive Summary
This study systematically compares discrete and continuous speech representations in LLM-based Automatic Speech Recognition (ASR). The authors organize speech representations into supervised and unsupervised categories for both discrete and continuous types, designing specialized encoders and models for each of the four distinct categories. Using both a Joint-Training-From-Scratch Language Model (JTFS LM) and pre-trained LLaMA2-7b, the research demonstrates that continuous representations outperform discrete ones due to lower information loss during encoding, with Whisper encoders performing better than HuBERT across both representation types. The work achieves a state-of-the-art open-sourced WER of 1.69% on the LibriSpeech test-clean set using a HuBERT encoder.

## Method Summary
The paper evaluates four categories of speech representations: supervised/discrete, supervised/continuous, unsupervised/discrete, and unsupervised/continuous. For each category, specialized encoders are implemented including HuBERT, Whisper, K-means clustering, and HuBERT-CTC variants. These representations are then processed by either a Joint-Training-From-Scratch Language Model or pre-trained LLaMA2-7b with LoRA fine-tuning. The system includes adapter modules for token projection and, for discrete representations, LLM-based error correction mechanisms. The evaluation uses the LibriSpeech dataset (960.9 hours training, 10.7 hours validation, 5.4 hours test-clean, 5.1 hours test-other) with Word Error Rate (WER) as the primary metric.

## Key Results
- Continuous speech representations consistently outperform discrete ones due to lower information loss during clustering
- Whisper encoders achieve better performance than HuBERT in both discrete and continuous settings
- State-of-the-art open-sourced WER of 1.69% achieved on LibriSpeech test-clean using HuBERT encoder
- LLM-based token correction significantly improves performance for discrete representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous speech representations outperform discrete ones due to lower information loss.
- Mechanism: Discrete clustering (e.g., K-means) reduces continuous acoustic features to a fixed number of clusters, inherently discarding fine-grained acoustic details. Continuous representations retain these details, enabling the LLM to model richer phonetic and acoustic patterns.
- Core assumption: The discrete representations lose critical acoustic information that is essential for accurate speech recognition.
- Evidence anchors:
  - [abstract] "continuous representations consistently outperform discrete ones due to lower information loss"
  - [section] "Discrete tokens undergo significant information loss during clustering, whereas continuous representations retain most of the information crucial for ASR."

### Mechanism 2
- Claim: Supervised speech encoders (e.g., Whisper) outperform unsupervised ones (e.g., HuBERT) due to better alignment with text.
- Mechanism: Supervised encoders are trained on paired speech-text data, directly optimizing for text-aligned representations. This results in features that are more semantically and phonetically relevant to language modeling tasks.
- Core assumption: Supervised training provides explicit text alignment that improves speech-to-text conversion.
- Evidence anchors:
  - [abstract] "Whisper encoders performing better than HuBERT"
  - [section] "Whisper's enhanced feature extraction ability, indicating that supervised training can further improve the alignment of speech representations to text"

### Mechanism 3
- Claim: LLM pre-training on text provides superior token correction over statistical n-gram models.
- Mechanism: Pre-trained LLMs leverage vast text corpora to learn rich contextual dependencies, allowing them to correct errors in discrete tokens more effectively than rule-based or statistical models.
- Core assumption: LLMs have learned contextual patterns that generalize well to speech token correction.
- Evidence anchors:
  - [section] "LLM as Discrete Error Token Corrector" and "LLaMA2's enhanced word sensitivity and error correction, surpassing character-level input"

## Foundational Learning

- Concept: Discrete vs Continuous Speech Representations
  - Why needed here: Determines how speech signals are encoded before LLM processing; affects information retention and model performance.
  - Quick check question: What is the main disadvantage of using discrete speech representations in ASR?

- Concept: Supervised vs Unsupervised Learning for Speech Encoders
  - Why needed here: Influences the quality and alignment of extracted features with language models.
  - Quick check question: How does supervised training improve speech encoder performance compared to unsupervised methods?

- Concept: Transformer-based Language Models and LoRA Fine-tuning
  - Why needed here: Core architecture for processing speech representations and adapting large models efficiently.
  - Quick check question: What is the purpose of LoRA in fine-tuning large language models?

## Architecture Onboarding

- Component map:
  Speech input -> Speech Encoder (Discrete/Continuous, Supervised/Unsupervised) -> Adapter Module (2-layer perceptron) -> LLM Backbone (JTFS LM or LLaMA2) -> Output text

- Critical path:
  1. Speech input → Encoder → Adapter → LLM → Output text
  2. For discrete tokens: LLM also acts as error corrector

- Design tradeoffs:
  - Discrete vs Continuous: Accuracy vs computational efficiency
  - Supervised vs Unsupervised: Feature quality vs training data requirements
  - JTFS LM vs LLaMA2: Flexibility vs pre-trained knowledge

- Failure signatures:
  - High WER with discrete tokens: Likely information loss in clustering
  - Poor performance with unsupervised encoders: Misalignment with text
  - Overfitting with LoRA: Insufficient regularization or small dataset

- First 3 experiments:
  1. Compare continuous vs discrete representations using JTFS LM on LibriSpeech.
  2. Test supervised vs unsupervised encoders within continuous representations.
  3. Evaluate discrete token correction by comparing n-best outputs with and without LLM refinement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of discrete speech encoder affect the performance of LLM-based ASR systems compared to different continuous speech encoders?
- Basis in paper: [explicit] The paper compares various discrete and continuous speech encoders, noting that Whisper encoders outperform HuBERT encoders in both discrete and continuous settings.
- Why unresolved: While the paper identifies performance differences, it does not delve into the specific reasons why certain encoders perform better or how these differences manifest in practical applications.
- What evidence would resolve it: Comparative studies focusing on the architectural and training differences of encoders, alongside their impact on downstream ASR tasks, would clarify these performance variations.

### Open Question 2
- Question: What are the trade-offs between using more K-means clusters versus the computational resources required in discrete speech representation?
- Basis in paper: [explicit] The paper mentions that increasing the number of K-means clusters improves WER but also increases computational complexity.
- Why unresolved: The paper does not provide a detailed analysis of the trade-offs or guidelines for optimal cluster selection based on available resources.
- What evidence would resolve it: Empirical studies that evaluate the performance and resource usage across a range of cluster sizes would help determine optimal configurations for different resource constraints.

### Open Question 3
- Question: How do different modeling approaches within discrete and continuous space LLMs affect the robustness and generalizability of ASR systems?
- Basis in paper: [explicit] The paper proposes and evaluates different modeling approaches within discrete and continuous space LLMs, suggesting a need for further exploration of their robustness and generalizability.
- Why unresolved: The paper provides initial insights but does not thoroughly investigate how these approaches perform across diverse datasets or real-world conditions.
- What evidence would resolve it: Extensive testing of these models on varied datasets and in real-world scenarios would provide insights into their robustness and generalizability.

## Limitations

- The analysis lacks ablation studies isolating the contribution of clustering granularity versus other factors like vocabulary size and token correction mechanisms.
- The superiority of supervised encoders is demonstrated empirically but lacks theoretical grounding explaining why supervised training provides better text alignment.
- The state-of-the-art WER achievement is not compared against other contemporary approaches using identical evaluation protocols.

## Confidence

- Continuous representations outperforming discrete ones (High): Multiple experimental conditions consistently show this pattern with clear theoretical explanation about information loss during clustering.
- Supervised encoders outperforming unsupervised ones (Medium): Empirical results support this claim, but the mechanism is less clearly articulated and could depend on specific implementation details.
- LLM-based token correction effectiveness (Medium): Demonstrated through WER improvements, but the analysis doesn't isolate whether gains come from the LLM's contextual knowledge or simply from having more parameters.
- HuBERT encoder achieving SOTA with LLaMA2-7b (Medium): The result is impressive but lacks comparison to other strong baselines and doesn't account for potential confounding factors like fine-tuning strategies.

## Next Checks

1. **Clustering granularity ablation**: Systematically vary the number of clusters in discrete representations (e.g., 50, 100, 500, 1000) while keeping all other factors constant to determine if information loss is the primary driver of performance differences, or if discrete tokens can match continuous performance at sufficient granularity.

2. **Supervised unsupervised hybrid training**: Train a HuBERT-style model with a text alignment objective (similar to supervised training but with unlabeled data) to test whether the performance gap between supervised and unsupervised encoders stems from the supervision signal itself versus other architectural differences.

3. **LLM token correction isolation**: Create a controlled experiment where discrete tokens are either passed directly to the LLM or first processed by a simpler correction mechanism (like an n-gram model or small transformer) to quantify how much of the WER improvement comes from the LLM's contextual knowledge versus basic error correction.