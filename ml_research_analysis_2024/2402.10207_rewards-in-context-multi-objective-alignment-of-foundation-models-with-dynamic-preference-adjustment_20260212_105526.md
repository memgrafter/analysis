---
ver: rpa2
title: 'Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic
  Preference Adjustment'
arxiv_id: '2402.10207'
source_url: https://arxiv.org/abs/2402.10207
tags:
- arxiv
- rewards
- reward
- preference
- helpful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multi-objective alignment of
  foundation models with human preferences, which is essential for creating helpful
  and harmless AI systems. The authors introduce Rewards-in-Context (RiC), a method
  that conditions model responses on multiple rewards in the prompt context and applies
  supervised fine-tuning for alignment.
---

# Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment

## Quick Facts
- arXiv ID: 2402.10207
- Source URL: https://arxiv.org/abs/2402.10207
- Authors: Rui Yang; Xiaoman Pan; Feng Luo; Shuang Qiu; Han Zhong; Dong Yu; Jianshu Chen
- Reference count: 40
- Key outcome: A method that conditions model responses on multiple rewards in prompt context, achieving superior multi-objective alignment while requiring only ~10% of GPU hours compared to RL baselines

## Executive Summary
Rewards-in-Context (RiC) addresses the challenge of aligning foundation models with multiple human preferences by conditioning responses on reward values in the prompt context and applying supervised fine-tuning. The method restructures multi-objective alignment into three stages: offline training with multi-reward conditional SFT, online training with Pareto-optimal data augmentation, and inference with dynamic preference-to-reward mapping. RiC demonstrates effectiveness across both Large Language Models and diffusion models while maintaining simplicity and adaptivity, requiring no multiple reward models or complex RL training.

## Method Summary
RiC employs a three-stage approach to multi-objective alignment. First, it relabels training prompts with reward model scores and performs conditional supervised fine-tuning, allowing a single model to adapt to multiple reward dimensions. Second, it uses multi-objective rejection sampling to augment the dataset with samples near the empirical Pareto front, improving alignment performance. Third, during inference, it applies a preference-to-reward mapping derived from a convex optimization problem to dynamically adjust user preferences to desired rewards, enabling flexible adaptation to different objective trade-offs.

## Key Results
- Achieves superior multi-objective alignment performance compared to baselines while requiring only ~10% of GPU hours
- Successfully aligns both Large Language Models and diffusion models to accommodate diverse rewards
- Demonstrates effective dynamic inference-time adjustment through preference-to-reward mapping
- Maintains simplicity by requiring only supervised fine-tuning of a single model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-reward conditional supervised fine-tuning (SFT) allows a single model to adapt to multiple reward dimensions without requiring multiple reward models.
- Mechanism: The model learns to ground its responses to the rewards by conditioning on the reward values in the prompt context. During inference, different reward values can be plugged in to achieve different objective trade-offs.
- Core assumption: The model can learn the relationship between reward conditioning and generated responses through SFT alone, without reinforcement learning.
- Evidence anchors:
  - [abstract]: "RiC, which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment."
  - [section]: "RiC first relabels the prompts in the dataset with reward models and then performs conditional SFT... We extend the reward conditional training method to accommodate multiple rewards."
  - [corpus]: Weak - no direct mention of SFT effectiveness for multi-objective alignment in neighbor papers.
- Break condition: If the model cannot generalize from the limited reward values seen during training to arbitrary combinations during inference.

### Mechanism 2
- Claim: The preference-to-reward mapping enables dynamic inference-time adjustment of user preferences to desired rewards that approach Pareto optimality.
- Mechanism: The mapping is derived from an abstracted convex optimization problem that maximizes a weighted sum of normalized obtained rewards subject to regularization constraints. This yields a principled way to map preferences to rewards that balance the objectives.
- Core assumption: The normalized obtained rewards φ_i(R_i) are monotonically increasing in the desired rewards R_i, allowing the optimization problem to be well-posed.
- Evidence anchors:
  - [abstract]: "Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives."
  - [section]: "To determine the mapping from the preference w to desired rewards in prompts, our insight is to formulate it as a maximization problem with multiple constraints... The solution to this optimization problem yields a family of justified mappings."
  - [corpus]: Weak - no direct mention of preference-to-reward mapping in neighbor papers.
- Break condition: If the actual relationship between desired and obtained rewards is non-monotonic or highly non-linear, invalidating the optimization formulation.

### Mechanism 3
- Claim: Multi-objective rejection sampling (MORS) augments the dataset with samples near the empirical Pareto front, improving alignment performance.
- Mechanism: Generated responses are scored by reward models, and samples that don't meet threshold values for each reward are rejected. This reshapes the data distribution to be closer to the Pareto front.
- Core assumption: The model can generate responses that span a diverse range of reward values, allowing MORS to select samples near the Pareto front.
- Evidence anchors:
  - [section]: "The online training stage consists of three steps... (3) lastly, the generated responses are scored by reward models, and a multi-objective rejection sampling (MORS) technique is employed to augment samples near the Pareto front for multi-reward conditional SFT."
  - [corpus]: Weak - no direct mention of MORS in neighbor papers.
- Break condition: If the model's generated responses are too clustered in reward space, limiting MORS's ability to find diverse samples near the Pareto front.

## Foundational Learning

- Concept: Supervised fine-tuning (SFT)
  - Why needed here: RiC relies on SFT as the core training method, avoiding the instability and high computational cost of reinforcement learning approaches.
  - Quick check question: What is the loss function used in SFT for RiC, and how does it incorporate the reward conditioning?

- Concept: Multi-objective optimization
  - Why needed here: RiC addresses the problem of aligning a foundation model with multiple, potentially conflicting human preferences.
  - Quick check question: What is the Pareto front, and why is it relevant to the multi-objective alignment problem?

- Concept: Convex optimization
  - Why needed here: The preference-to-reward mapping is derived from an abstracted convex optimization problem, providing a principled way to map preferences to rewards.
  - Quick check question: What are the key components of a convex optimization problem, and how do they relate to the preference-to-reward mapping formulation?

## Architecture Onboarding

- Component map: User prompt + user preference vector -> Multi-reward conditional SFT model + preference-to-reward mapping -> Model response aligned with specified preferences
- Critical path:
  1. Normalize reward values using dataset statistics
  2. Relabel prompts with reward values
  3. Perform multi-reward conditional SFT
  4. During inference, map user preferences to desired rewards using the analytical solution
  5. Generate response conditioned on prompt and desired rewards
- Design tradeoffs:
  - Simplicity vs. performance: RiC trades off some alignment performance for simplicity and computational efficiency compared to RL-based methods
  - Scalability vs. granularity: RiC can easily scale to more rewards, but the preference-to-reward mapping may not capture fine-grained trade-offs between objectives
- Failure signatures:
  - Poor alignment performance: The model may not effectively balance the multiple objectives, resulting in suboptimal responses
  - Instability during inference: The preference-to-reward mapping may not generalize well to out-of-distribution preference values
- First 3 experiments:
  1. Train RiC on a toy dataset with two simple reward models (e.g., sentiment and length) and evaluate its ability to balance the two objectives at inference time.
  2. Compare the alignment performance of RiC with a baseline RL-based method on a standard multi-objective alignment benchmark.
  3. Analyze the sensitivity of RiC's performance to the hyperparameters of the preference-to-reward mapping (e.g., the p value in the p-norm regularization).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of RiC's scalability when handling more than three objectives, and how does performance degrade as the number of objectives increases?
- Basis in paper: [inferred] The paper mentions that RiC is scalable and demonstrates experiments with up to three objectives, but does not explore scenarios with a larger number of objectives or provide theoretical analysis of scalability limits.
- Why unresolved: The paper only presents results for up to three objectives and does not provide theoretical analysis of how RiC would perform with a significantly larger number of objectives. The scalability of the preference-to-reward mapping method and the effectiveness of the online training stage with more objectives are not explored.
- What evidence would resolve it: Experiments demonstrating RiC's performance with a larger number of objectives (e.g., 5, 10, or more) would provide empirical evidence of its scalability limits. Additionally, theoretical analysis of the computational complexity and effectiveness of the preference-to-reward mapping method as the number of objectives increases would help understand the theoretical limits of RiC's scalability.

### Open Question 2
- Question: How does RiC perform when aligning rewards that have complex, non-linear relationships or interactions between objectives?
- Basis in paper: [inferred] The paper demonstrates RiC's effectiveness in aligning rewards that are either independent or have simple trade-off relationships, but does not explore scenarios where rewards have complex, non-linear relationships or interactions.
- Why unresolved: The paper only considers relatively simple reward structures and does not investigate how RiC handles more complex reward relationships. The effectiveness of the preference-to-reward mapping method and the online training stage in capturing and optimizing complex reward interactions is not explored.
- What evidence would resolve it: Experiments demonstrating RiC's performance on tasks with rewards that have complex, non-linear relationships or interactions would provide empirical evidence of its effectiveness in handling such scenarios. Additionally, analysis of the learned reward representations and the ability of RiC to capture and optimize complex reward interactions would help understand its performance in such cases.

### Open Question 3
- Question: How sensitive is RiC's performance to the choice of hyperparameters, such as the threshold for multi-objective rejection sampling (MORS) and the weighting parameters in the preference-to-reward mapping?
- Basis in paper: [inferred] The paper mentions the use of specific hyperparameter values (e.g., 0.7-quantile for MORS, λ = 1 for weighting parameters) but does not provide a comprehensive analysis of how RiC's performance varies with different hyperparameter choices.
- Why unresolved: The paper only presents results using specific hyperparameter values and does not explore the sensitivity of RiC's performance to different choices. Understanding the impact of hyperparameter selection on RiC's effectiveness is crucial for its practical application.
- What evidence would resolve it: Experiments demonstrating RiC's performance with a range of hyperparameter values would provide empirical evidence of its sensitivity to hyperparameter selection. Additionally, analysis of the impact of different hyperparameter choices on the learned reward representations and the effectiveness of the online training stage would help understand the importance of hyperparameter tuning for RiC.

## Limitations

- The core assumption that supervised fine-tuning can effectively learn multi-objective alignment without reinforcement learning is asserted but not thoroughly validated
- The preference-to-reward mapping relies on convex optimization assumptions that may not hold in practice, particularly regarding monotonicity of obtained rewards
- The claim of "only 10% of the GPU hours" compared to RL baselines lacks absolute timing details, making real-world resource requirements unclear

## Confidence

- **High confidence**: The basic mechanism of reward conditioning in prompts during supervised fine-tuning (Mechanism 1)
- **Medium confidence**: The effectiveness of multi-objective rejection sampling (Mechanism 3)
- **Low confidence**: The robustness of the preference-to-reward mapping across diverse preference distributions (Mechanism 2)

## Next Checks

1. **Break condition test**: Systematically vary the correlation structure between rewards (positive, negative, zero correlation) and measure how well RiC maintains Pareto optimality. This would validate whether the method breaks down when rewards are highly correlated.

2. **Generalization boundary**: Test the preference-to-reward mapping with preference vectors that are significantly outside the training distribution. Measure whether the analytical solution provides reasonable reward values or produces unstable results.

3. **Sample efficiency validation**: Quantify the minimum number of samples required near the Pareto front for MORS to be effective. Vary the online buffer size and measure degradation in alignment performance to establish practical constraints.