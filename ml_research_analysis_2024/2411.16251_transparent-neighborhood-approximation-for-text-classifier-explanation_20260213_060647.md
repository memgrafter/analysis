---
ver: rpa2
title: Transparent Neighborhood Approximation for Text Classifier Explanation
arxiv_id: '2411.16251'
source_url: https://arxiv.org/abs/2411.16251
tags:
- text
- explanation
- neighborhood
- xprob
- very
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating transparent and
  stable explanations for text classifiers. While recent approaches leverage generative
  models to construct synthetic neighborhoods for model-agnostic explanations, these
  methods rely on black-box components, undermining the transparency of the entire
  explanation process.
---

# Transparent Neighborhood Approximation for Text Classifier Explanation

## Quick Facts
- arXiv ID: 2411.16251
- Source URL: https://arxiv.org/abs/2411.16251
- Reference count: 40
- Authors: Yi Cai; Arthur Zimek; Eirini Ntoutsi; Gerhard Wunder
- One-line primary result: XPROB achieves competitive performance in correctness, completeness, and compactness while reducing explanation time and improving stability compared to generator-based explainers.

## Executive Summary
This paper addresses the challenge of creating transparent and stable explanations for text classifiers. While recent approaches leverage generative models to construct synthetic neighborhoods for model-agnostic explanations, these methods rely on black-box components, undermining the transparency of the entire explanation process. To overcome this limitation, the authors propose XPROB, a probability-based editing method that generates neighboring texts through deterministic manipulations guided by local n-gram contexts. Instead of using a black-box generator, XPROB recursively integrates words from the explicand into prototypes selected from a corpus, creating a gradual transition that reveals decision boundaries. Experiments on two real-world datasets show that XPROB achieves competitive performance in terms of correctness, completeness, and compactness compared to generator-based explainers, while significantly reducing explanation time and improving stability. XPROB's transparent construction process and reduced dependency on external resources address key limitations in existing approaches.

## Method Summary
XPROB constructs model-agnostic explanations for text classifiers through a probability-based editing approach. The method selects counterfactual prototypes from a retained corpus using tf-idf cosine distance, then recursively integrates words from the explicand into these prototypes based on local n-gram probability distributions. This creates a neighborhood of neighboring texts that gradually transition from counterfactuals to the original text. A linear surrogate model is trained on this neighborhood using weighted regression to extract feature attributions. The approach avoids black-box generative models, instead using deterministic probability-based manipulations guided by empirical n-gram contexts from the corpus. The method is evaluated on two real-world datasets with BERT and LSTM classifiers, comparing performance against LIME, XSPELLS, ABELE, and XPROAX across correctness, completeness, compactness, stability, and time cost metrics.

## Key Results
- XPROB achieves competitive fidelity (0.61-0.67) and R² scores (0.22-0.27) compared to generator-based explainers
- XPROB significantly reduces explanation time (3.8-6.3s vs 11.4-12.7s for generator-based methods)
- XPROB demonstrates superior stability with lower attribution deviation across similar contexts
- XPROB maintains competitive completeness and compactness metrics (AOPC scores) while providing transparent explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probability-based editing avoids black-box generators while still producing realistic neighboring texts.
- Mechanism: Uses empirical n-gram probability distributions to determine optimal word insertion/replacement positions guided by local context.
- Core assumption: The retained corpus (Xp) accurately reflects the target text domain's n-gram distribution.
- Evidence anchors:
  - [abstract]: "This approach generates neighboring texts by implementing manipulations based on in-text contexts."
  - [section]: "the conditional probabilities can be estimated using the retained corpus Xp" with equations Ppre and Psuc.
  - [corpus]: Weak - corpus only shows related papers, not corpus usage evidence.
- Break condition: If Xp doesn't represent target domain, probability estimates become inaccurate.

### Mechanism 2
- Claim: Recursive editing creates gradual transitions from counterfactual prototypes to explicand while maintaining locality.
- Mechanism: Iteratively integrates words from explicand into counterfactual prototypes, using cosine distance in tf-idf space for prototype selection.
- Core assumption: Prototype selection based on tf-idf cosine distance effectively maintains locality constraint.
- Evidence anchors:
  - [abstract]: "By recursively integrating explicand components to prototypes, XPROB produces a series of neighboring texts that share components with x"
  - [section]: "prototype selection follows their spatial closeness to the explicand...measured by the cosine distance between their tf-idf vectors"
  - [corpus]: Weak - no direct corpus evidence for prototype selection strategy.
- Break condition: If tf-idf distance poorly represents semantic similarity, locality constraint fails.

### Mechanism 3
- Claim: Controlled generation process improves stability compared to generator-based methods.
- Mechanism: Deterministic probability-based editing produces consistent attributions across similar contexts, unlike stochastic generator sampling.
- Core assumption: Probability-based editing produces more consistent results than generator sampling due to deterministic nature.
- Evidence anchors:
  - [abstract]: "XPROB's fully transparent and more controllable construction process leads to superior stability compared to the generator-based explainers."
  - [section]: Stability evaluation shows "low deviations for both word categories" and "consistency in explanation results"
  - [corpus]: Weak - stability results from experiments, not corpus evidence.
- Break condition: If probability estimates vary significantly across similar contexts, stability advantage diminishes.

## Foundational Learning

- Concept: N-gram probability estimation from corpus
  - Why needed here: Core to probability-based editing - determines word insertion/replacement positions
  - Quick check question: How would you estimate P(w|context) from a corpus of text samples?

- Concept: Prototype selection using text similarity metrics
  - Why needed here: Ensures locality constraint by selecting counterfactuals close to explicand
  - Quick check question: Why use tf-idf cosine distance instead of embedding distance for prototype selection?

- Concept: Surrogate model training for attribution extraction
  - Why needed here: Extracts feature attributions from constructed neighborhood using weighted regression
  - Quick check question: What role does the locality weighting (exp(-d²/σ²)) play in surrogate training?

## Architecture Onboarding

- Component map: Prototype corpus → Probability estimator → Text editor → Neighborhood builder → Surrogate trainer → Attribution extractor
- Critical path: Prototype selection → Probability-based editing → Neighborhood construction → Surrogate training → Attribution extraction
- Design tradeoffs: Probability-based editing vs generator-based - transparency and stability vs potentially richer text generation
- Failure signatures: Poor locality (neighborhood too distant), low diversity (neighborhood too similar), inconsistent attributions across similar texts
- First 3 experiments:
  1. Test prototype selection: Measure tf-idf distance distribution between prototypes and explicands
  2. Test editing feasibility: Count successful vs failed edits across different n-gram context sizes
  3. Test surrogate fidelity: Compare surrogate R² scores across different neighborhood sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of XPROB compare to other explainers when explaining multi-class text classifiers?
- Basis in paper: [inferred] The paper mentions that XPROB can be adapted to multi-class scenarios but does not provide experimental results for such cases.
- Why unresolved: The paper only evaluates XPROB on binary classification tasks, leaving the performance on multi-class problems unexplored.
- What evidence would resolve it: Experimental results comparing XPROB's performance on multi-class text classification tasks against other explainers would provide the necessary evidence.

### Open Question 2
- Question: What is the impact of different n-gram context sizes on the quality and efficiency of explanations generated by XPROB?
- Basis in paper: [explicit] The paper mentions that the choice of n, which determines the range of local contexts, balances between semantic correctness and feasibility of the edit, but does not provide a comprehensive study on the impact of different n values.
- Why unresolved: While the paper acknowledges the importance of n-gram context size, it does not explore how different values affect explanation quality and generation efficiency.
- What evidence would resolve it: A systematic evaluation of XPROB's performance using different n-gram context sizes would provide insights into the optimal choice for various text classification tasks.

### Open Question 3
- Question: How does the stability of XPROB compare to other explainers when explaining text classifiers on longer input texts?
- Basis in paper: [inferred] The paper discusses the stability of XPROB on short texts but does not address its performance on longer inputs.
- Why unresolved: The paper only evaluates XPROB's stability on short text examples, leaving its performance on longer texts unexplored.
- What evidence would resolve it: Experimental results comparing the stability of XPROB and other explainers when explaining text classifiers on longer input texts would provide the necessary evidence.

## Limitations

- The approach relies on a retained corpus for probability estimation, which may not adequately represent all text domains, potentially leading to inaccurate probability estimates and unrealistic edits.
- The recursive editing mechanism may struggle with complex syntactic structures where local n-gram contexts provide insufficient guidance, limiting the quality of generated neighboring texts.
- The generalizability of XPROB across diverse text domains and classifier architectures is not thoroughly explored, with experiments limited to binary classification tasks on specific datasets.

## Confidence

- **High Confidence**: The experimental methodology is sound, with clear evaluation metrics (fidelity, R², confidence drop, AOPC, stability, time cost) and appropriate baseline comparisons (LIME, XSPELLS, ABELE, XPROAX).
- **Medium Confidence**: The mechanism for probability-based editing is well-described, but the handling of edge cases and failure modes in the recursive editing process lacks detail. The claims about reduced time cost are supported by experiments but may depend on implementation specifics.
- **Low Confidence**: The generalizability of the approach across diverse text domains and classifier architectures is not thoroughly explored. The impact of corpus size and quality on explanation quality is not systematically studied.

## Next Checks

1. **Corpus Domain Representation Test**: Evaluate the diversity and representativeness of the retained corpus (Xp) by measuring the overlap in n-gram distributions between Xp and the target text domain. Identify potential domain mismatch issues that could affect probability estimation accuracy.

2. **Stability Across Classifier Architectures**: Replicate the stability experiments using different classifier architectures (e.g., CNN, Transformer-based models) to assess the generalizability of XPROB's stability claims. Analyze the impact of classifier complexity on explanation consistency.

3. **Edge Case Handling Analysis**: Systematically test the probability-based editing algorithm on edge cases (e.g., short texts, texts with rare words, complex syntactic structures) to identify failure modes and assess the robustness of the approach. Develop strategies to handle these cases effectively.