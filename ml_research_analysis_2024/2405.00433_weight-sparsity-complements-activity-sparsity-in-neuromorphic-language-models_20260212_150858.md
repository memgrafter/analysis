---
ver: rpa2
title: Weight Sparsity Complements Activity Sparsity in Neuromorphic Language Models
arxiv_id: '2405.00433'
source_url: https://arxiv.org/abs/2405.00433
tags:
- sparsity
- activity
- neural
- networks
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how weight pruning interacts with the natural
  activity sparsity of event-based neural networks in language modeling. The authors
  analyze a recently developed event-based recurrent architecture, the EGRU, across
  varying degrees of connectivity sparsity and compare it to densely activated LSTM
  networks.
---

# Weight Sparsity Complements Activity Sparsity in Neuromorphic Language Models

## Quick Facts
- arXiv ID: 2405.00433
- Source URL: https://arxiv.org/abs/2405.00433
- Authors: Rishav Mukherji; Mark Schöne; Khaleelulla Khan Nazeer; Christian Mayr; David Kappel; Anand Subramoney
- Reference count: 40
- Primary result: Sparse activity and sparse connectivity are independent methods that multiply efficiency gains in event-based neural networks

## Executive Summary
This study investigates how weight pruning interacts with the natural activity sparsity of event-based neural networks in language modeling. The authors analyze a recently developed event-based recurrent architecture, the EGRU, across varying degrees of connectivity sparsity and compare it to densely activated LSTM networks. They find that sparse activity and sparse connectivity are independent methods of reducing computational load, and that their effects multiply to produce significant efficiency gains without proportional loss in task performance. This relationship holds for a broad range of connectivity sparsity, with the network compensating for sparse connections through increased activity only at very high sparsity levels. The results demonstrate that sparsely connected event-based neural networks are promising candidates for effective and efficient sequence modeling.

## Method Summary
The authors compare the event-based EGRU architecture with traditional LSTM networks on language modeling tasks (Penn Treebank and WikiText-2). They employ global unstructured weight magnitude pruning to systematically reduce connectivity sparsity, then measure the combined effects on activity sparsity and computational efficiency (MAC operations). The EGRU uses a Heaviside spike function with surrogate gradients and learned firing thresholds per neuron. Weight decay regularization is applied to tune the trade-off between task performance and activity sparsity. Models are trained using the AdamW optimizer with tied embeddings between input and output layers.

## Key Results
- Sparse activity and sparse connectivity multiply efficiency gains rather than adding linearly
- EGRU maintains activity sparsity across a broad range of connectivity sparsity levels
- Weight decay regularization can tune the trade-off between task performance and activity sparsity
- The multiplicative efficiency gains hold across multiple language modeling tasks

## Why This Works (Mechanism)

### Mechanism 1
Sparse activity and sparse connectivity are independent methods of reducing computational load, and their effects multiply for efficiency gains. Sparse activity (event-based activation) reduces MAC operations by skipping computations for inactive neurons, while sparse connectivity reduces operations by skipping multiplications with zero weights. Since both affect different dimensions of the computation (activation vs. weight matrices), their savings multiply rather than add linearly. The reduction in operations from sparse activity and sparse connectivity are statistically independent; one does not systematically compensate for the other. This breaks when removing connections systematically forces the network to activate more neurons to compensate, as suggested in the paper at very high sparsity levels.

### Mechanism 2
Weight regularization (weight decay) can be traded for activity sparsity in event-based networks. Increasing weight decay pushes weight distributions toward zero, which reduces the likelihood of neuron activations crossing the firing threshold. This allows tuning the trade-off between task performance and activity sparsity. The learned thresholds and weight magnitudes are coupled such that shrinking weights directly reduces the number of activations. This breaks when weight decay becomes too strong, causing the network to underfit and lose representational capacity, negating the benefit of sparsity.

### Mechanism 3
EGRU compensates for high connectivity sparsity by increasing network activity only at very high sparsity levels. In the normal operating range of connectivity sparsity, the EGRU maintains activity sparsity through gradient-based learning. Only when connectivity is extremely sparse (e.g., >80%) does the network increase activity to compensate for lost connections. The training process can maintain a target level of activity sparsity unless connectivity sparsity becomes a bottleneck. This breaks if the compensation mechanism fails or if the increased activity negates the efficiency gains from sparse connectivity.

## Foundational Learning

- Concept: Event-based neural networks and sparse activations
  - Why needed here: EGRU is an event-based variant of GRU that only propagates non-zero activations, which is key to understanding its efficiency.
  - Quick check question: In EGRU, what determines whether a neuron's state is communicated to the next layer?

- Concept: Weight pruning and connectivity sparsity
  - Why needed here: Pruning removes connections (weights set to zero), reducing computation and memory access.
  - Quick check question: How does global unstructured weight magnitude pruning select which weights to remove?

- Concept: MAC operations as a measure of computational efficiency
  - Why needed here: The paper uses MACs to quantify the theoretical efficiency gains from sparsity.
  - Quick check question: How is the number of MAC operations reduced when both activations and weights are sparse?

## Architecture Onboarding

- Component map:
  Embedding layer (tied to output decoder) -> Stacked RNN layers (EGRU cells) -> DropConnect applied to recurrent weights -> Heaviside spike function with surrogate gradient -> Learned firing thresholds per neuron -> Weight decay regularization (separately for weights and biases)

- Critical path:
  1. Input token → embedding lookup
  2. For each RNN layer: compute gates (u, r), proposed state (z), local state (c) using sparse activations
  3. Apply threshold to produce sparse output
  4. Linear decoder with tied weights
  5. Loss computation and backprop with surrogate gradients

- Design tradeoffs:
  - Activity sparsity vs. task performance: Can be tuned via weight decay
  - Connectivity sparsity vs. model capacity: High sparsity risks underfitting
  - Unstructured vs. structured pruning: Unstructured allows finer granularity but less hardware-friendly
  - Surrogate gradient bandwidth: Affects training stability vs. biological plausibility

- Failure signatures:
  - Sudden increase in activity sparsity at high connectivity sparsity (compensation failure)
  - Divergence during training with surrogate gradients
  - Over-regularization leading to poor validation performance
  - Hardware inefficiency if sparsity patterns are irregular

- First 3 experiments:
  1. Train baseline dense EGRU and dense LSTM on Penn Treebank; verify perplexity matches literature (~56-57 for EGRU, ~57-58 for LSTM).
  2. Apply 80% weight pruning to both models; measure MAC reduction and perplexity change; confirm multiplicative effect.
  3. Vary weight decay on EGRU; plot task performance vs. activity sparsity; identify optimal trade-off point.

## Open Questions the Paper Calls Out

### Open Question 1
Does the independence between activity sparsity and connectivity sparsity observed in EGRU hold for other event-based architectures beyond the specific EGRU implementation studied? The paper states that their results demonstrate the independence of these two sparsities for the EGRU model, but notes this is currently limited to this specific architecture. This remains unresolved because the study only examined one event-based architecture (EGRU), and different event-based models may have different dynamics that could affect the relationship between activity and connectivity sparsity. Systematic studies applying the same methodology to other event-based architectures like S4, RWKV, or other spiking/neuromorphic models would establish whether the multiplicative efficiency gains generalize.

### Open Question 2
What is the optimal balance between activity sparsity and connectivity sparsity that maximizes both computational efficiency and task performance across different language modeling tasks? The paper shows that both sparsities contribute independently to efficiency but doesn't provide a framework for determining optimal ratios for different use cases. This remains unresolved because the paper demonstrates that the sparsities are independent but doesn't explore whether certain combinations are more beneficial than others for specific applications or computational constraints. Empirical studies systematically varying both sparsities across multiple tasks and hardware platforms to identify Pareto-optimal operating points would resolve this.

### Open Question 3
How does the mechanism of driving weights and biases to negative values to promote sparse activations affect the generalization capability and robustness of EGRU models compared to dense models? The paper identifies that weight decay regularization drives weight and bias distributions toward negative values, which promotes sparse activations, but doesn't investigate the implications of this mechanism. This remains unresolved because while the paper observes this interesting property of the training dynamics, it doesn't examine whether this negative-bias mechanism introduces any biases in the learned representations or affects model robustness to distribution shifts. Comparative studies of EGRU models with controlled interventions to prevent negative-bias development, measuring generalization across domains and robustness to adversarial examples or domain shifts, would resolve this.

## Limitations

- Analysis limited to two specific language modeling tasks (Penn Treebank and WikiText-2), raising questions about generalizability to other sequence modeling domains
- Focuses on unstructured weight pruning, which may not translate directly to hardware efficiency gains compared to structured sparsity patterns
- The assumption of independence between activity and connectivity sparsity breaks down at very high sparsity levels (>80%), though exact thresholds remain unclear

## Confidence

- **High confidence**: The multiplicative relationship between sparse activity and sparse connectivity for efficiency gains is well-supported by empirical results across multiple experiments and metrics (MAC operations, perplexity). The observation that weight decay can tune the trade-off between task performance and activity sparsity is consistently demonstrated.
- **Medium confidence**: The claim that EGRU maintains activity sparsity through gradient-based learning while compensating for connectivity sparsity only at very high levels is supported by the presented data, but the exact mechanisms of this compensation and the precise thresholds where independence breaks down require further investigation.
- **Low confidence**: Generalizability to other neural network architectures beyond EGRU and LSTM, and to other task domains beyond language modeling, remains unproven based on the current study.

## Next Checks

1. **Cross-architecture validation**: Apply the same sparsity analysis to other recurrent architectures (e.g., vanilla RNN, Transformer-based models) and feedforward networks to determine if the multiplicative relationship holds universally.

2. **Hardware-aware benchmarking**: Implement the sparse models on neuromorphic hardware or FPGA to measure actual energy consumption and latency, comparing against theoretical MAC savings to validate the practical efficiency claims.

3. **Dynamic sparsity analysis**: Track how sparsity patterns evolve during training and whether the independence between activity and connectivity sparsity is maintained throughout the learning process or only emerges at convergence.