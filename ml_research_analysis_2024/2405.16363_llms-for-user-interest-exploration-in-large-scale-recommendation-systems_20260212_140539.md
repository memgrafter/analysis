---
ver: rpa2
title: LLMs for User Interest Exploration in Large-scale Recommendation Systems
arxiv_id: '2405.16363'
source_url: https://arxiv.org/abs/2405.16363
tags:
- user
- interest
- novel
- recommendation
- clusters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of recommendation systems being
  trapped in a feedback loop that limits novel user interest discovery. The proposed
  solution is a hybrid hierarchical framework that combines Large Language Models
  (LLMs) for high-level interest exploration with classic recommendation models for
  low-level item selection.
---

# LLMs for User Interest Exploration in Large-scale Recommendation Systems

## Quick Facts
- arXiv ID: 2405.16363
- Source URL: https://arxiv.org/abs/2405.16363
- Reference count: 37
- Primary result: Hybrid hierarchical framework combining LLMs with classic recommendation models significantly improves novel user interest discovery in large-scale systems

## Executive Summary
This paper addresses the critical challenge of recommendation systems being trapped in feedback loops that limit novel user interest discovery. The proposed solution is a hybrid hierarchical framework that combines Large Language Models (LLMs) for high-level interest exploration with classic recommendation models for low-level item selection. By using topical clusters to represent interests and fine-tuning LLMs on diverse real-world user behavior data, the system achieves controlled generation that enables effective novel interest exploration. Live experiments on a platform with billions of users demonstrated significant improvements in recommendation quality and user engagement metrics.

## Method Summary
The method employs a hybrid hierarchical framework that uses topical clusters to represent user interests. LLMs are fine-tuned on diverse real-world user behavior data to generate novel interest descriptions at the high level, while classic transformer-based sequence models handle item-level recommendations within the prescribed clusters. The approach pre-computes novel interest transitions offline using LLM inference on all possible cluster pairs, storing results in a lookup table for efficient online serving. This design balances computational efficiency with exploration quality by representing users through a small number of historical clusters (K=2) rather than individual items.

## Key Results
- Significant improvements in novel interest exploration (UCI@N metric) compared to production baselines
- Increased watch time and active users in live experiments on a platform serving billions of users
- Effective balance between computational efficiency and exploration quality through hierarchical clustering approach

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical planning reduces computational complexity while maintaining exploration quality by representing users through a small number of historical clusters (K=2) instead of items. This enables offline LLM inference on all possible cluster pairs with results stored in a lookup table for online serving. The core assumption is that a small number of historical clusters adequately represents user interests for meaningful novel interest prediction.

### Mechanism 2
Controlled generation through fine-tuning ensures LLM outputs match predefined interest clusters. Supervised fine-tuning on diverse, balanced real-world user behavior data teaches the LLM to generate cluster descriptions that exactly match predefined cluster descriptions. The core assumption is that the fine-tuning data is sufficiently diverse and balanced to cover all cluster patterns.

### Mechanism 3
Combining LLM reasoning with classic recommendation models provides both exploration and personalization. LLMs generate novel interest descriptions at the high level, which are then grounded to item-level recommendations by restricting classic transformer-based sequence models to items within the prescribed clusters. The core assumption is that classic recommendation models can effectively personalize item selection within the constrained cluster space.

## Foundational Learning

- Concept: Hierarchical clustering for interest representation
  - Why needed here: Reduces the planning space from millions of items to hundreds of clusters, making LLM inference computationally feasible
  - Quick check question: What would happen if we used items instead of clusters for LLM inference in terms of computational requirements?

- Concept: Supervised fine-tuning for controlled generation
  - Why needed here: Standard LLMs generate arbitrary text that doesn't match predefined cluster descriptions; fine-tuning aligns outputs with the specific vocabulary needed
  - Quick check question: How does the match rate metric indicate whether the LLM is successfully generating cluster descriptions?

- Concept: Novelty metrics (UCI@N)
  - Why needed here: Measures whether the system is successfully introducing users to new interests rather than just reinforcing existing ones
  - Quick check question: Why is UCI@20 different from UCI@200 in terms of what they measure about user exploration?

## Architecture Onboarding

- Component map:
  - Cluster generation pipeline (offline): Item metadata → embeddings → clustering → topical descriptions
  - Fine-tuning pipeline (offline): Real user behavior → [(cluster pair, label)] → LLM fine-tuning
  - LLM inference pipeline (offline): All possible cluster pairs → novel interest predictions → lookup table
  - Online serving pipeline: User history → cluster representation → lookup table → cluster ID → item retrieval

- Critical path: User request → cluster lookup → item restriction → transformer-based recommendation → item ranking
- Design tradeoffs:
  - K=2 vs larger K: Computational efficiency vs representation quality
  - Fine-tuning diversity vs data volume: Balanced exploration vs model coverage
  - Cluster granularity: Broader clusters enable more exploration but less precise recommendations

- Failure signatures:
  - Low match rate (<90%): LLM fine-tuning insufficient, outputs don't align with cluster vocabulary
  - Skewed generation distribution: Fine-tuning data not diverse enough
  - No improvement in UCI@N: Hierarchical approach not capturing meaningful interest transitions

- First 3 experiments:
  1. Test match rate and recall during fine-tuning at different steps to find optimal balance
  2. Compare UCI@N improvements between K=1, K=2, and K=3 to validate cluster representation
  3. A/B test against production baselines measuring novelty ratio and user engagement metrics

## Open Questions the Paper Calls Out
- How does the effectiveness of the hybrid hierarchical framework change with different values of K (number of historical clusters used to represent users)?
- How does the hybrid approach perform in the long-term, particularly regarding user retention and sustained interest exploration?
- How does the framework perform on different types of content platforms beyond short-form video, such as e-commerce or news recommendation?

## Limitations
- The exact clustering methodology and granularity levels used to create topical clusters are not fully specified
- The LLM fine-tuning procedure lacks detailed hyperparameters and optimization settings
- The offline computation requirements for pre-computing all possible cluster transitions are substantial and not quantified

## Confidence
- High confidence: The hierarchical framework concept and its computational efficiency benefits
- Medium confidence: The controlled generation mechanism through fine-tuning and its impact on exploration quality
- Low confidence: The scalability claims and real-world performance metrics due to limited methodological details

## Next Checks
1. Measure the exact computational requirements and latency for pre-computing cluster transitions with different K values and cluster granularities on a representative dataset
2. Evaluate the match rate and generation diversity across multiple fine-tuning runs with different random seeds to assess the stability of the controlled generation mechanism
3. Conduct a longitudinal analysis (minimum 3 months) comparing user retention, satisfaction, and exploration breadth between the proposed system and traditional recommendation approaches to validate the claimed user engagement improvements