---
ver: rpa2
title: 'VPO: Leveraging the Number of Votes in Preference Optimization'
arxiv_id: '2410.22891'
source_url: https://arxiv.org/abs/2410.22891
tags:
- preference
- dataset
- reward
- human
- vdpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating additional
  vote count information in pairwise preference datasets for language model alignment.
  The core method idea is to model the underlying preference probability using Bayesian
  Minimum Mean Square Error (MMSE) estimator, creating a Vote-based Preference Optimization
  (VPO) framework that leverages vote counts to distinguish between controversial
  and obvious generation pairs.
---

# VPO: Leveraging the Number of Votes in Preference Optimization

## Quick Facts
- arXiv ID: 2410.22891
- Source URL: https://arxiv.org/abs/2410.22891
- Reference count: 5
- VPO-based algorithms outperform existing methods like DPO and IPO across multiple evaluation metrics, achieving win rates of 57.05% and 54.75% respectively on the SHP dataset compared to 55.92% for DPO.

## Executive Summary
This paper addresses the challenge of incorporating additional vote count information in pairwise preference datasets for language model alignment. The core method uses Bayesian Minimum Mean Square Error (MMSE) estimator to model underlying preference probability, creating a Vote-based Preference Optimization (VPO) framework that leverages vote counts to distinguish between controversial and obvious generation pairs. The primary results show that VPO-based algorithms (VDPO and VIPO) outperform existing methods like DPO and IPO across multiple evaluation metrics, demonstrating improved generation quality and training stability.

## Method Summary
The method models the probability that one generation is preferable to another using Bayesian MMSE estimator. Given vote counts v1 and v2 for two generations, it computes a posterior Beta distribution by updating a prior Beta(c,c) with observed votes, then takes the mean as the estimated preference probability. This probability replaces binary targets in preference optimization algorithms, creating VDPO and VIPO variants that incorporate vote strength into the loss calculation. The framework generalizes to any preference optimization algorithm by substituting the binary target with vote-based probabilities.

## Key Results
- VPO-based algorithms (VDPO and VIPO) outperform existing methods like DPO and IPO across multiple evaluation metrics
- Achieved win rates of 57.05% (VDPO) and 54.75% (VIPO) on SHP dataset compared to 55.92% for DPO
- Demonstrated improved generation quality and training stability through reduced reward divergence

## Why This Works (Mechanism)

### Mechanism 1
Vote counts provide additional signal beyond binary preferences by modeling preference probability with Bayesian MMSE estimator. The method uses vote counts (v1, v2) to update a Beta prior (Beta(c, c)) into a posterior Beta(v1 + c, v2 + c), then takes the mean as the estimated preference probability. This smooths extreme vote ratios and differentiates between controversial and clear-cut pairs.

### Mechanism 2
Incorporating vote-based probabilities into the DPO loss reduces reward divergence and overfitting. By replacing the binary target (p(Y1|x) = 1) with a probabilistic target (p(Y1|x, v1, v2) = ˆθMMSE(v1, v2)), the loss becomes a weighted cross-entropy. This softens the gradient for controversial pairs and prevents unbounded reward scaling.

### Mechanism 3
VPO generalizes across preference optimization algorithms by replacing their binary target with vote-based probabilities. The framework plugs the Bayesian MMSE estimate into any preference optimization objective (DPO, IPO, etc.), creating VDPO and VIPO variants that adapt the margin or loss weighting to vote strength.

## Foundational Learning

- Concept: Beta distribution as conjugate prior for binomial likelihood
  - Why needed here: Enables closed-form posterior update from vote counts without sampling
  - Quick check question: If prior is Beta(1,1) and we observe v1=3, v2=1, what is the posterior mean?
    - Answer: Beta(4,2) → mean = 4/(4+2) = 2/3

- Concept: Bradley-Terry model for pairwise comparison
  - Why needed here: Forms the probabilistic foundation of preference optimization; defines how reward relates to preference probability
  - Quick check question: In a Bradley-Terry model, if r(x,y1) - r(x,y2) = 0, what is the probability y1 is preferred?
    - Answer: 0.5 (equal odds)

- Concept: Cross-entropy loss with smoothed labels
  - Why needed here: VPO's weighted loss is equivalent to label smoothing with non-uniform target probabilities
  - Quick check question: If target p=0.7 and model outputs q=0.9, what is the cross-entropy loss?
    - Answer: -0.7*log(0.9) - 0.3*log(0.1) ≈ 0.36

## Architecture Onboarding

- Component map: Data pipeline (SHP/UFB datasets) → vote count extraction → target probability computation → Model (Pythia/LLaMA) → SFT → preference alignment (VDPO/VIPO) → Loss layer (weighted cross-entropy)

- Critical path: 1. Load pairwise data with votes/scores 2. Compute Bayesian MMSE target probability for each pair 3. Forward pass through model to get log-probabilities 4. Compute weighted cross-entropy loss 5. Backpropagate and update model

- Design tradeoffs: Vote smoothing (c) vs. sensitivity to actual vote gaps, Batch size vs. gradient stability with weighted samples, Dataset domain (Reddit vs. synthetic) vs. generalization

- Failure signatures: Overly short generations when c is too small, No improvement over baseline when vote counts are noisy, Reward divergence if vote smoothing is disabled

- First 3 experiments: 1. Run DPO baseline on SHP with c=0 (no vote smoothing) to observe reward divergence 2. Run VDPO with c=1 on SHP and compare win rates to DPO 3. Run VIPO on UFB (synthetic scores) to test generalization to non-vote data

## Open Questions the Paper Calls Out

### Open Question 1
How does the optimal value of hyperparameter c in VPO vary across different preference alignment tasks and dataset sizes? The authors tested c values of 0.3, 1, 10, 30, and 100, finding c=1 optimal, but did not explore why this value works best or how it might vary across different contexts.

### Open Question 2
Can the VPO framework be extended to incorporate continuous preference scores rather than just binary preferences and vote counts? The authors discuss extending VPO to utilize AI-generated scores on the UFB dataset, but do not explore how to handle continuous preference signals directly.

### Open Question 3
What is the relationship between the VPO framework and robustness to noisy preference labels compared to existing methods like rDPO? The authors note that VPO can be interpreted as modeling noise in preference labels using side information, similar to rDPO, but do not directly compare their robustness to label noise.

## Limitations
- Limited evidence for Bayesian MMSE superiority - no ablation study comparing different vote-count handling methods
- Generalization concerns - strong results on specific datasets but unknown performance on datasets with different vote distribution patterns
- Implementation complexity - IPO and rDPO were implemented by authors themselves rather than using existing implementations

## Confidence
- High confidence: The mathematical framework for Bayesian MMSE estimation and its application to preference optimization is sound and well-specified
- Medium confidence: The experimental results showing VPO's superiority over baselines on the reported datasets
- Low confidence: The claim that VPO provides a general solution for incorporating vote information into any preference optimization algorithm

## Next Checks
1. Ablation on vote smoothing: Run experiments with different values of c (the smoothing hyperparameter) including c=0 (no smoothing) and c→∞ (uniform probability), as well as alternative smoothing methods like Laplace smoothing or empirical Bayes estimation.

2. Cross-dataset generalization: Apply VPO to a dataset with fundamentally different vote characteristics (e.g., StackExchange preferences with binary votes, or professional annotation datasets with bounded scores). Compare performance to baselines to assess whether improvements transfer beyond the original dataset domains.

3. Noise robustness testing: Inject varying levels of synthetic noise into the vote counts (e.g., random flips, vote inflation, bot-like patterns) and measure how VPO's performance degrades compared to baselines.