---
ver: rpa2
title: 'Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models'
arxiv_id: '2402.07033'
source_url: https://arxiv.org/abs/2402.07033
tags:
- memory
- expert
- fiddler
- inference
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fiddler is an inference system for Mixture-of-Experts (MoE) language
  models that efficiently utilizes both CPU and GPU resources. The key idea is to
  execute expert layers on the CPU instead of transferring expert weights from CPU
  to GPU, significantly reducing data movement overhead.
---

# Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models

## Quick Facts
- arXiv ID: 2402.07033
- Source URL: https://arxiv.org/abs/2402.07033
- Reference count: 40
- Fiddler achieves 1.26x speedup in single-batch inference of Mixtral-8x7B compared to baseline offloading methods

## Executive Summary
Fiddler is an inference system designed to efficiently deploy large Mixture-of-Experts (MoE) language models on resource-constrained environments with limited GPU memory. The key insight is to leverage CPU computation to avoid expensive data transfers between CPU and GPU memory. By executing expert layers on CPU instead of transferring expert weights to GPU, Fiddler significantly reduces data movement overhead, particularly for single-batch inference where activation sizes are much smaller than weight sizes. The system profiles expert popularity on calibration data to intelligently distribute expert layers between CPU and GPU, optimizing for maximum hit rate and overall inference speed.

## Method Summary
Fiddler works by first profiling expert selection patterns on calibration data to determine which experts are most frequently used. It then distributes expert layers between CPU and GPU memory based on their popularity, placing the most popular experts in GPU memory and less popular ones in CPU memory. During inference, when an expert is needed but not present in GPU memory, Fiddler copies the smaller activation values to CPU and executes the expert on CPU rather than transferring the large expert weights to GPU. This approach is particularly effective for small batch sizes where the latency of transferring weights from CPU to GPU exceeds the latency of computing the expert on CPU.

## Key Results
- 1.26x speedup in single-batch inference of Mixtral-8x7B
- 1.30x speedup in long prefill processing
- 11.57x speedup in beam search inference compared to baseline offloading methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fiddler achieves latency reduction by executing expert layers on CPU instead of moving expert weights from CPU to GPU.
- Mechanism: When an expert is needed but not present in GPU memory, instead of copying the large expert weights to GPU, Fiddler copies the smaller activation values to CPU and executes the expert on CPU. This avoids the high latency of weight transfer over PCIe.
- Core assumption: For small batch sizes, the latency of transferring weights from CPU to GPU exceeds the latency of computing the expert on CPU.
- Evidence anchors:
  - [abstract] "the key idea is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU"
  - [section 3.1] "it is better to execute expert layers on the CPUs than to load the expert weights from CPU memory to GPU memory, especially when the batch size is small"
  - [corpus] No direct evidence in neighbors about CPU execution speed comparison to GPU weight transfer latency

### Mechanism 2
- Claim: Fiddler distributes expert layers between CPU and GPU based on expert popularity determined by profiling on calibration data.
- Mechanism: Before inference, Fiddler profiles expert selection patterns on calibration data to determine which experts are most frequently used. It then places the most popular experts in GPU memory and less popular ones in CPU memory to maximize hit rate.
- Core assumption: Expert selection is based on token characteristics and the popularity of experts is universal across different input domains.
- Evidence anchors:
  - [section 3.2] "we select as many experts as the memory capacity permits in order of popularity so that we can maximize the hit rate"
  - [section 3.2] "We determine the popular experts based on the profile of expert selection using calibration data"
  - [corpus] No direct evidence in neighbors about expert popularity profiling methods

### Mechanism 3
- Claim: Fiddler optimizes prefill stage by modeling GPU execution as constant latency and CPU execution as linearly increasing with input tokens.
- Mechanism: In prefill stage with multiple tokens, Fiddler calculates optimal assignment of experts to CPU/GPU by solving an optimization problem that minimizes maximum latency between CPU and GPU execution paths.
- Core assumption: GPU execution latency remains constant regardless of input size while CPU execution latency increases linearly with number of input tokens.
- Evidence anchors:
  - [section 3.2] "we adopt a model where the GPU execution time is considered constant, while the CPU execution time is assumed to increase linearly with the number of input tokens"
  - [section 3.2] "we need to consider the different batching effects of GPU and CPU"
  - [corpus] No direct evidence in neighbors about batching effects on CPU vs GPU latency

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how MoE models work is fundamental to grasping why Fiddler's approach is effective
  - Quick check question: In MoE models, how many experts are typically activated per input token?
  - Answer: Usually only a small subset of experts are activated per input token, which is why MoE models can scale to large sizes with manageable computation

- Concept: GPU memory limitations and PCIe bandwidth constraints
  - Why needed here: Understanding why expert weights cannot all fit in GPU memory and why data transfer between CPU and GPU is expensive
  - Quick check question: Why is transferring data between CPU and GPU memory over PCIe typically slow?
  - Answer: PCIe has limited bandwidth compared to GPU memory bandwidth, and the data must traverse the PCIe bus which creates significant latency

- Concept: Expert popularity and routing mechanisms
  - Why needed here: Understanding how Fiddler determines which experts to place in GPU memory vs CPU memory
  - Quick check question: How does Fiddler determine which experts should be placed in GPU memory?
  - Answer: Fiddler profiles expert selection patterns on calibration data and places the most frequently used experts in GPU memory

## Architecture Onboarding

- Component map: Calibration profiler -> Memory allocator -> Execution scheduler -> Data mover -> Inference engine

- Critical path: During inference: Expert selection → Memory availability check → Data movement decision → Execution on appropriate device

- Design tradeoffs:
  - CPU vs GPU execution: Tradeoff between slower CPU computation and expensive GPU weight transfer
  - Memory placement: Tradeoff between GPU memory capacity and hit rate for expert weights
  - Calibration data selection: Tradeoff between representative profiling and overhead of calibration

- Failure signatures:
  - Low hit rate: Indicates poor expert popularity profiling or domain shift
  - High CPU utilization: May indicate too many experts being executed on CPU when they should be on GPU
  - High GPU idle time: May indicate excessive CPU-GPU synchronization or suboptimal scheduling

- First 3 experiments:
  1. Measure weight transfer latency vs CPU execution latency for single expert with varying batch sizes
  2. Profile expert selection patterns on calibration data to determine optimal memory placement strategy
  3. Test different optimization strategies for prefill stage with varying numbers of input tokens

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several key questions emerge from the work:

1. How does the performance of Fiddler scale with the number of CPU cores and their clock speeds?
2. What is the impact of using different activation functions (other than SiLU) on Fiddler's performance?
3. How does Fiddler's performance compare to other offloading strategies in multi-GPU environments?

## Limitations

- The CPU vs GPU execution latency comparison lacks empirical validation across different hardware configurations
- The expert popularity profiling methodology is underspecified regarding calibration data requirements
- The optimization strategy for prefill stage relies on simplified latency models that may not accurately reflect real-world behavior

## Confidence

- **High confidence**: The core insight that avoiding weight transfer by executing experts on CPU can reduce latency for small batch sizes is theoretically sound and aligns with known PCIe bandwidth limitations
- **Medium confidence**: The distribution strategy based on expert popularity is reasonable but depends heavily on the quality of calibration data and may not generalize well across domains
- **Low confidence**: The optimization models for prefill stage and the specific speedup numbers lack sufficient empirical validation and may be overly optimistic for different hardware or model configurations

## Next Checks

1. **Hardware sensitivity validation**: Measure CPU vs GPU execution latency across different CPU/GPU combinations (varying core counts, PCIe generations, memory bandwidth) to verify the paper's assumptions about optimal execution strategies hold across hardware configurations

2. **Domain generalization test**: Evaluate expert popularity profiling on multiple diverse datasets to determine how sensitive Fiddler's performance is to domain shifts, and quantify the degradation when calibration data doesn't match inference data distribution

3. **Ablation study of optimization components**: Systematically disable individual optimization strategies (popularity-based distribution, prefill optimization, beam search handling) to quantify their individual contributions to the reported speedups and identify which components are most critical