---
ver: rpa2
title: Recurrent Neural Networks for Still Images
arxiv_id: '2409.06235'
source_url: https://arxiv.org/abs/2409.06235
tags:
- layer
- layers
- number
- convolution
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using RNN layers for still image tasks, introducing
  a Separable RNN (SRNN) and Weight-Shared Bidirectional RNN (WS-BiRNN) designed to
  work efficiently with 2D inputs. These layers were integrated into Convolutional
  Recurrent Neural Networks (CRNNs) and tested on COCO object detection and CIFAR100
  classification tasks.
---

# Recurrent Neural Networks for Still Images

## Quick Facts
- arXiv ID: 2409.06235
- Source URL: https://arxiv.org/abs/2409.06235
- Reference count: 21
- Primary result: Small CRNN models using WS-BiRNN outperform similarly sized CNNs and larger models in certain settings, especially on embedded systems with limited resources.

## Executive Summary
This paper explores using RNN layers for still image tasks, introducing a Separable RNN (SRNN) and Weight-Shared Bidirectional RNN (WS-BiRNN) designed to work efficiently with 2D inputs. These layers were integrated into Convolutional Recurrent Neural Networks (CRNNs) and tested on COCO object detection and CIFAR100 classification tasks. Results show that small CRNN models using WS-BiRNN can outperform similarly sized CNNs and even larger models in certain settings, particularly on embedded systems with limited resources. The RNN-based layers offer fewer parameters and MAC operations than Conv2D layers with large kernels, enabling large receptive fields without increasing computational cost. This makes RNNs a promising alternative for compact, resource-constrained vision models.

## Method Summary
The paper introduces two novel RNN layers - Separable RNN (SRNN) and Weight-Shared Bidirectional RNN (WS-BiRNN) - designed to process 2D image data efficiently. SRNN decomposes 2D operations into sequential 1D passes (rows then columns), while WS-BiRNN achieves bidirectional processing with shared weights to reduce parameters. These layers were integrated into CRNNs and tested on COCO object detection and CIFAR100 classification. The experiments involved replacing network heads with WS-BiRNN layers for COCO detection and substituting the last residual block with WS-BiRNN for CIFAR100 classification.

## Key Results
- Small CRNN models using WS-BiRNN can outperform similarly sized CNNs on classification and detection tasks
- RNN-based layers offer fewer parameters and MAC operations than Conv2D layers with large kernels
- The efficiency gains are particularly pronounced on embedded systems with limited resources

## Why This Works (Mechanism)

### Mechanism 1
RNN layers can replace multiple Conv2D layers while preserving or improving accuracy in compact models. RNNs have a large receptive field that grows with sequence length, allowing them to aggregate spatial context without increasing kernel size. A single SRNN or WS-BiRNN layer can capture the equivalent context of several stacked Conv2D layers, reducing both parameter count and computational cost. This assumes the spatial sequence interpretation of image pixels is valid and does not degrade feature extraction compared to traditional convolutions.

### Mechanism 2
Weight-Shared Bidirectional RNN (WS-BiRNN) achieves symmetry in receptive fields without doubling parameters. By sharing weights across forward and reverse passes, WS-BiRNN enforces symmetric effective weights around each position, mimicking the centered receptive field of Conv2D kernels but with fewer parameters. This assumes symmetric receptive fields are beneficial for image tasks and can be approximated by bidirectional RNNs with shared weights.

### Mechanism 3
Separable RNN (SRNN) reduces computational complexity by decomposing 2D operations into sequential 1D passes. SRNN applies an RNN along rows, then transposes and applies another RNN along columns, analogous to depthwise separable convolutions but using recurrent connections. This reduces MACs roughly by a factor of k² (kernel size) compared to Conv2D. This assumes sequential 1D RNN passes can approximate 2D spatial transformations effectively.

## Foundational Learning

- **RNN receptive field growth**: Understanding how RNNs accumulate context over sequence length is key to appreciating their efficiency advantage over fixed-size Conv2D kernels. Quick check: If an RNN processes a sequence of length L with hidden size H, how does its effective receptive field compare to a Conv2D with kernel size k?

- **Separable operations**: SRNN mimics depthwise separable convolutions; knowing how these reduce parameters and MACs explains the efficiency gains. Quick check: In a separable convolution, if the 2D kernel is k×k, what is the approximate reduction factor in parameters compared to a full 2D convolution?

- **Bidirectional sequence modeling**: WS-BiRNN relies on forward and reverse passes; understanding bidirectional modeling clarifies how symmetric receptive fields are achieved. Quick check: How does a bidirectional RNN with shared weights enforce symmetry in effective receptive field weights?

## Architecture Onboarding

- **Component map**: Input → Conv2D backbone → (optional residual blocks) → RNN layer(s) (SRNN/WS-BiRNN) → Heads/Output
- **Critical path**: Input tensor → backbone Conv2D → RNN layer → classification/detection head → output. Memory bottleneck: Tensor arena size after backbone, before RNN.
- **Design tradeoffs**: Parameter efficiency vs. representational power: RNNs reduce params but may limit feature diversity. Sequential vs. parallel: RNNs are inherently sequential along one axis; hardware acceleration may be limited compared to Conv2D. Receptive field control: RNNs grow receptive field with sequence length; Conv2D fixed by kernel size.
- **Failure signatures**: Accuracy drop when replacing early Conv2D layers (RNNs may not capture fine-grained local features well). Training instability if sequence length is too short (limited receptive field growth). Memory overflow if tensor arena not sized for transposed intermediate activations in SRNN.
- **First 3 experiments**: 1) Replace last Conv2D layer in a small CNN with SRNN; compare params, MACs, and accuracy. 2) Swap detection heads in a COCO-trained model with WS-BiRNN; evaluate F1/AP metrics. 3) Compare SRNN vs. separable Conv2D with equivalent receptive fields on CIFAR100 classification.

## Open Questions the Paper Calls Out

### Open Question 1
Can 2D RNN layers (like RNN2D) consistently outperform CNNs and transformers in small-scale vision tasks across different datasets and architectures? The paper discusses the potential of 2D RNNs and mentions that "A more general, 2-Deminsional RNN operation may be defined" as future work, suggesting this is an open area for exploration. This remains unresolved as the paper only introduces the concept without experimental results for 2D RNNs.

### Open Question 2
How does the performance of depthwise separable RNNs (DS-RNNs) compare to standard RNNs and CNNs in terms of accuracy and computational efficiency? The paper mentions DS-RNNs as a potential direction for reducing computational and memory requirements but does not test them experimentally. The mathematical formulation is introduced, but no experiments are conducted to evaluate their effectiveness in practice.

### Open Question 3
What are the optimal architectural designs for integrating RNN layers into CRNNs to maximize performance in resource-constrained environments? The paper shows that small CRNNs with SWS-BiRNN layers outperform similarly sized CNNs, but it does not explore the full design space of CRNN architectures. The experiments focus on specific configurations, leaving questions about the best placement, layer types, and hyperparameters for RNNs in CRNNs.

## Limitations
- The spatial sequence interpretation of images in RNN layers may not preserve feature extraction quality compared to convolutions in all cases
- The mechanisms (SRNN and WS-BiRNN) are novel and lack direct corpus validation
- Claims about parameter and MAC efficiency improvements depend heavily on specific architectural choices and dataset characteristics

## Confidence
- **High confidence**: The general concept that RNNs can achieve large receptive fields without increasing kernel size is well-established in sequence modeling literature
- **Medium confidence**: The specific implementations of SRNN and WS-BiRNN layers and their claimed efficiency advantages are plausible but novel, requiring more empirical validation
- **Low confidence**: The claim that these RNN layers can fully replace Conv2D layers in all vision tasks without accuracy degradation needs broader testing across different architectures and datasets

## Next Checks
1. **Receptive Field Analysis**: Measure and compare the actual receptive fields achieved by SRNN and WS-BiRNN layers versus equivalent Conv2D configurations across different sequence lengths and kernel sizes.
2. **Ablation Study on Layer Placement**: Systematically test the impact of replacing different Conv2D layers (early, middle, late stages) with RNN layers to identify where the spatial sequence assumption breaks down.
3. **Hardware Efficiency Validation**: Benchmark the actual inference time and memory usage of CRNN models versus equivalent CNNs on target embedded platforms to verify the claimed resource efficiency advantages.