---
ver: rpa2
title: Mitigating the Bias of Large Language Model Evaluation
arxiv_id: '2409.16788'
source_url: https://arxiv.org/abs/2409.16788
tags:
- evaluation
- bias
- quality
- superficial
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the problem of bias in large language model
  (LLM) evaluation, where LLM judges favor answers with better superficial quality
  (verbosity, fluency) while ignoring instruction-following ability. The authors propose
  two methods to mitigate this bias: for closed-source models, they apply calibration
  by modeling and subtracting superficial quality scores using either probability
  distributions from pre-trained models or prompt engineering; for open-source models,
  they propose contrastive training with carefully constructed negative samples that
  deviate from instructions but exhibit better superficial quality.'
---

# Mitigating the Bias of Large Language Model Evaluation

## Quick Facts
- arXiv ID: 2409.16788
- Source URL: https://arxiv.org/abs/2409.16788
- Reference count: 3
- Primary result: Probability-based calibration reduces bias while improving overall evaluation accuracy on LLMBar benchmark

## Executive Summary
This work addresses a critical bias in LLM-as-a-Judge evaluation where judges favor answers with better superficial quality (verbosity, fluency) while ignoring instruction-following ability. The authors propose two complementary approaches: calibration methods for closed-source models and contrastive training for open-source models. Their probability-based calibration method models superficial quality using pre-trained model distributions and shows superior performance, reducing bias while maintaining or improving evaluation accuracy. Experiments on the LLMBar benchmark demonstrate that these methods effectively reduce bias on adversarial test sets while maintaining accuracy on natural sets.

## Method Summary
The paper proposes two methods to mitigate evaluation bias in LLM-as-a-Judge systems. For closed-source models, they apply calibration by modeling and subtracting superficial quality scores using either probability distributions from pre-trained models or prompt engineering. For open-source models, they propose contrastive training with carefully constructed negative samples that deviate from instructions but exhibit better superficial quality. The probability-based calibration computes the difference between pre-trained and instruction-tuned model scores as a bias indicator, while prompt-based calibration uses engineered prompts to probe superficial attributes. The contrastive training approach constructs negative samples using Instructor embeddings and trains judge models on both natural and adversarial triplets.

## Key Results
- Probability-based calibration achieved the best results, reducing bias while improving overall accuracy on LLMBar benchmark
- Both calibration methods effectively reduced bias on adversarial test sets while maintaining evaluation accuracy on natural sets
- Prompt-based calibration showed weaker performance compared to probability-based calibration, particularly on adversarial sets
- Contrastive training with negative samples successfully mitigated bias in open-source judge models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probability-based calibration effectively reduces evaluation bias by modeling superficial quality using pre-trained model distributions
- Mechanism: The method quantifies superficial quality by comparing probability distributions between pre-trained and instruction-tuned models, then subtracts this quantified bias from final evaluation scores
- Core assumption: The difference between pre-trained and SFT model distributions indicates instruction alignment
- Evidence anchors:
  - [abstract]: "For probability-based evaluators, we propose to model the superficial quality by utilizing the probability generated by the pre-trained model"
  - [section]: "As the evaluation bias is generally presented as a higher score for superficial quality, we propose two ways to directly model the superficial quality, and subtract it from the final result"
  - [corpus]: Weak evidence - only 5/8 corpus papers mention calibration or bias mitigation, none specifically discuss probability-based approaches
- Break condition: If the pre-trained model itself has learned instruction-following ability, the distribution difference becomes less indicative of bias

### Mechanism 2
- Claim: Contrastive training with adversarial negative samples effectively reduces bias in open-source judge models
- Mechanism: Constructs negative samples that deviate from instructions but exhibit better superficial quality, then trains judge models on both natural and adversarial triplets to learn unbiased prediction patterns
- Core assumption: Training on carefully curated adversarial samples prevents over-reliance on superficial features
- Evidence anchors:
  - [abstract]: "For open-source judge models, we propose to mitigate the bias by contrastive training, with curated negative samples that deviate from instruction but present better superficial quality"
  - [section]: "The judge model is trained based on the combination of both contrastive samples and natural samples"
  - [corpus]: Moderate evidence - 3/8 corpus papers discuss contrastive learning or adversarial training for bias mitigation
- Break condition: If negative samples are too similar to positive samples, the contrastive signal becomes ineffective; if too dissimilar, the model cannot learn the intended distinction

### Mechanism 3
- Claim: Prompt engineering-based calibration can model superficial quality for generation-based evaluation
- Mechanism: Designs specific prompts that probe superficial attributes (fluency, verbosity) to quantify and subtract bias from evaluation results
- Core assumption: Superficial quality can be effectively modeled through targeted prompt engineering
- Evidence anchors:
  - [abstract]: "For generation-based evaluators, we propose to model the superficial quality by probing the superficial attributes in the prompt template"
  - [section]: "We design three prompts for quantifying the superficial quality, as shown in Figure 1"
  - [corpus]: Weak evidence - only 2/8 corpus papers mention prompt engineering for bias mitigation
- Break condition: If the prompts cannot capture the full spectrum of superficial quality attributes, the calibration becomes incomplete

## Foundational Learning

- Concept: Calibration in machine learning
  - Why needed here: The work relies on calibration techniques to quantify and subtract bias from LLM evaluation scores
  - Quick check question: How does calibration typically work in language models, and why is it effective for bias mitigation?

- Concept: Contrastive learning
  - Why needed here: The open-source model approach uses contrastive training with carefully constructed negative samples
  - Quick check question: What is the core principle of contrastive learning, and how does it apply to debiasing LLM evaluation?

- Concept: Probability distributions in autoregressive models
  - Why needed here: The probability-based calibration method relies on modeling differences between pre-trained and fine-tuned model distributions
  - Quick check question: How do autoregressive language models generate probability distributions, and what information do these distributions contain about model behavior?

## Architecture Onboarding

- Component map: Input → Bias quantification → Subtraction → Output
- Critical path: Input → Bias quantification → Subtraction → Output
- Design tradeoffs:
  - Probability calibration vs. prompt calibration: More accurate but requires access to pre-trained models vs. more accessible but potentially less effective
  - Computational cost: Online calibration adds overhead to each evaluation vs. offline contrastive training requires upfront training
  - Model access: Closed-source models can only use online calibration vs. open-source models can use either approach
- Failure signatures:
  - Calibration methods produce scores outside expected ranges
  - Negative samples are too easy or too difficult to distinguish
  - Evaluation accuracy drops significantly on natural test sets
  - Bias reduction plateaus despite increased calibration strength
- First 3 experiments:
  1. Implement probability-based calibration on Text-davinci-003 using Davinci-002 as the pre-trained model reference
  2. Construct negative samples using adjacent instruction retrieval and test contrastive training on Vicuna-7B
  3. Compare the effectiveness of the three prompt templates for modeling superficial quality on generation-based evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the calibration factor α be optimally determined for different LLM evaluation tasks to balance bias mitigation and evaluation accuracy?
- Basis in paper: [explicit] The paper discusses the compromise between bias mitigation and evaluation accuracy, noting that too much mitigation leads to performance degradation, and shows that accuracy on both natural and adversarial sets decreases when the factor α is larger than 0.8.
- Why unresolved: The paper does not provide a method for determining the optimal value of α for different tasks or models.
- What evidence would resolve it: Experiments showing the relationship between α and evaluation performance across various tasks and models, leading to a method for determining the optimal α for each scenario.

### Open Question 2
- Question: How effective are the proposed bias mitigation methods on LLM evaluation tasks beyond instruction following, such as reasoning or creative writing?
- Basis in paper: [inferred] The paper focuses on instruction-following tasks and uses the LLMBar benchmark, which is specifically designed for this purpose. The effectiveness of the methods on other types of tasks is not discussed.
- Why unresolved: The paper does not test the methods on a diverse range of LLM evaluation tasks beyond instruction following.
- What evidence would resolve it: Experiments applying the bias mitigation methods to various LLM evaluation tasks, such as reasoning, creative writing, or factual knowledge, and comparing the results to the current instruction-following focus.

### Open Question 3
- Question: How can the bias in LLM evaluation be addressed for multimodal tasks, where the evaluation involves both text and other modalities such as images or audio?
- Basis in paper: [inferred] The paper focuses solely on text-based LLM evaluation and does not discuss multimodal tasks or how bias mitigation methods might be adapted for them.
- Why unresolved: The paper does not explore the challenges and potential solutions for bias mitigation in multimodal LLM evaluation tasks.
- What evidence would resolve it: Research extending the bias mitigation methods to multimodal tasks, identifying the unique challenges and proposing adapted solutions for each modality involved in the evaluation.

## Limitations

- The probability-based calibration assumes pre-trained and fine-tuned model distributions meaningfully differ for instruction-following ability, which may not hold for all model families
- The prompt-based calibration approach shows weaker performance and limited validation of whether prompts capture the full spectrum of superficial quality attributes
- The contrastive training approach relies on carefully constructed negative samples, but the paper doesn't thoroughly explore optimal sample construction parameters

## Confidence

- Confidence: Medium - The probability-based calibration method assumes that distribution differences between pre-trained and instruction-tuned models accurately capture instruction alignment
- Confidence: Medium - The contrastive training approach relies on carefully constructed negative samples that deviate from instructions but exhibit better superficial quality
- Confidence: Low - The prompt-based calibration approach shows weaker results compared to probability-based calibration, but the paper provides limited analysis of why this might be the case

## Next Checks

1. Cross-model calibration validation: Test whether the probability-based calibration method maintains effectiveness when using pre-trained models from different families (e.g., GPT-3 family vs. other architectures) to ensure the distributional differences remain meaningful indicators of instruction alignment.

2. Negative sample quality analysis: Systematically vary the retrieval threshold (ϵ) for constructing negative samples and measure the impact on contrastive training effectiveness, including quantitative analysis of the instruction-negative sample similarity distributions to ensure optimal signal-to-noise ratio.

3. Ablation study on prompt templates: Conduct controlled experiments comparing the three proposed prompt templates against baseline prompts and human-annotated superficial quality scores to validate whether the prompts effectively capture the full spectrum of superficial quality attributes that influence judge bias.