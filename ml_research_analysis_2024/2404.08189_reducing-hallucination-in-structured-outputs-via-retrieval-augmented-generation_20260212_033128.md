---
ver: rpa2
title: Reducing hallucination in structured outputs via Retrieval-Augmented Generation
arxiv_id: '2404.08189'
source_url: https://arxiv.org/abs/2404.08189
tags:
- steps
- retriever
- table
- language
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work applies Retrieval-Augmented Generation (RAG) to reduce\
  \ hallucinations in structured output tasks, specifically converting natural language\
  \ to workflows represented as JSON. A small retriever encoder is trained to align\
  \ natural language with workflow steps and tables, and the LLM is fine-tuned to\
  \ generate JSON including the retriever\u2019s suggestions."
---

# Reducing hallucination in structured outputs via Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2404.08189
- Source URL: https://arxiv.org/abs/2404.08189
- Reference count: 21
- Primary result: RAG reduces hallucinations from 21% to under 4.5% in workflow JSON generation

## Executive Summary
This work addresses the persistent problem of hallucinations in structured output generation by applying Retrieval-Augmented Generation (RAG) to workflow-to-JSON conversion tasks. The authors train a small retriever to align natural language descriptions with workflow steps and tables, then fine-tune an LLM to incorporate retriever suggestions when generating JSON outputs. The approach achieves significant hallucination reduction while maintaining high performance metrics, demonstrating particular effectiveness in table reference accuracy. The small retriever's CPU compatibility makes the system practical for resource-constrained deployments.

## Method Summary
The authors develop a RAG-based approach for structured output generation by first training a small retriever encoder to align natural language with workflow steps and tables. They then fine-tune an LLM to generate JSON outputs that incorporate the retriever's suggestions. The system uses a retriever-augmented generation loop where the retriever provides relevant workflow components before the LLM generates the final structured output. This architecture enables the LLM to reference actual workflow elements rather than fabricating content, significantly reducing hallucination rates while maintaining task performance.

## Key Results
- Hallucination reduction from 21% to under 4.5% for table references
- Bag of Steps performance reaches 0.667 and Trigger Exact Match reaches 0.664
- Achieves these results using a small 7B parameter LLM
- Improves out-of-domain generalization compared to baseline approaches
- Small retriever can be deployed on CPU for resource efficiency

## Why This Works (Mechanism)
The approach works by providing the LLM with grounded, relevant information from the retriever before generation occurs. By constraining the generation process with actual workflow components retrieved from the training data, the model has less opportunity to fabricate information. The retriever acts as a factual anchor, ensuring that generated JSON references correspond to real workflow elements rather than hallucinated content. This is particularly effective for table references, where the retriever can directly surface relevant table information to guide the LLM's output.

## Foundational Learning

**RAG Architecture**
- Why needed: Combines retrieval and generation to ground outputs in factual information
- Quick check: Verify retriever returns relevant documents before LLM processing

**Structured Output Generation**
- Why needed: Ensures outputs conform to specific JSON schemas for downstream processing
- Quick check: Validate generated JSON against schema requirements

**Small Model Deployment**
- Why needed: Enables CPU deployment for cost-effective inference
- Quick check: Confirm inference latency meets requirements on target hardware

## Architecture Onboarding

**Component Map**
Retriever Encoder -> Workflow Database -> Retriever -> LLM Fine-Tuning Pipeline -> JSON Generator

**Critical Path**
Natural Language Input -> Retriever Query -> Retrieved Workflow Elements -> LLM Context + Prompt -> Structured JSON Output

**Design Tradeoffs**
The use of a small retriever enables CPU deployment but may sacrifice some retrieval accuracy compared to larger models. The fine-tuning approach requires domain-specific data but produces better task alignment than zero-shot methods.

**Failure Signatures**
- Retriever returns irrelevant workflow elements → increased hallucination
- Fine-tuning data bias → poor out-of-domain performance
- JSON schema mismatches → downstream processing failures

**3 First Experiments**
1. Test retriever recall on held-out workflow examples
2. Measure hallucination rates with and without retrieval augmentation
3. Validate CPU inference latency and memory usage

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Domain-specific focus on workflow generation may not generalize to other structured output tasks
- Evaluation metrics may not capture all aspects of hallucination quality
- Fine-tuning requires domain-specific data which may be scarce
- Long-term stability under changing workflow patterns not evaluated

## Confidence

**High confidence**
- Core methodology of combining retriever with LLM generation is sound
- Hallucination reduction claims (21% to under 4.5%) are well-supported
- Experimental results are reproducible

**Medium confidence**
- Generalization to out-of-domain scenarios needs more validation
- Performance metrics may not fully capture real-world utility
- Long-term deployment stability not addressed

**Low confidence**
- Impact of evolving workflow patterns over time
- Continuous deployment behavior under changing conditions

## Next Checks

1. Test approach on diverse structured output tasks beyond workflow generation, such as code synthesis or database query generation, to assess generalizability.

2. Conduct user studies to evaluate whether reduced hallucination translates to meaningful improvements in real-world task completion and user satisfaction.

3. Evaluate system performance under continuous deployment conditions with evolving workflow patterns and new table schemas to assess robustness over time.