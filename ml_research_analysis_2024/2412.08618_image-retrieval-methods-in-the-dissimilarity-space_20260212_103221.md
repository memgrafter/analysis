---
ver: rpa2
title: Image Retrieval Methods in the Dissimilarity Space
arxiv_id: '2412.08618'
source_url: https://arxiv.org/abs/2412.08618
tags:
- learning
- space
- classi
- dissimilarity
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method for image retrieval that transforms
  multi-class feature embeddings into a dissimilarity space using dichotomy transformation,
  enabling binary classification of sample pairs as similar or dissimilar. The approach
  employs a max-margin classifier with L2 regularization on weights and trains the
  feature extractor and classifier end-to-end using a combined loss function.
---

# Image Retrieval Methods in the Dissimilarity Space

## Quick Facts
- arXiv ID: 2412.08618
- Source URL: https://arxiv.org/abs/2412.08618
- Reference count: 15
- The paper introduces a method for image retrieval that transforms multi-class feature embeddings into a dissimilarity space using dichotomy transformation, enabling binary classification of sample pairs as similar or dissimilar.

## Executive Summary
This paper presents a novel image retrieval method that transforms multi-class feature embeddings into a dissimilarity space using dichotomy transformation. The approach converts the multi-class retrieval problem into a binary classification task by computing absolute differences between feature vectors. A max-margin classifier with L2 regularization is employed to learn a diagonal Mahalanobis distance matrix. The method is trained end-to-end, jointly optimizing the feature extractor and classifier. Extensive experiments demonstrate consistent improvements over contrastive learning baselines, with up to 2-3% better accuracy on CUB-200, Stanford Online Products, and other datasets.

## Method Summary
The proposed method transforms multi-class feature embeddings into a dissimilarity space by taking the absolute difference between query and gallery feature vectors. This dichotomy transformation converts the multi-class retrieval problem into a binary classification task where the goal is to classify pairs as similar or dissimilar. A max-margin classifier with L2 regularization on weights learns a diagonal Mahalanobis distance matrix in this space. The system is trained end-to-end using a combined loss function that includes cross-entropy, triplet, and hinge losses. This joint optimization allows the feature extractor to adapt its representations to be more suitable for the dissimilarity space, resulting in improved feature separation and retrieval accuracy.

## Key Results
- The method shows consistent improvements over contrastive learning baselines, with up to 2-3% better accuracy
- The approach is robust to limited training data and reduces computational complexity by learning fewer parameters compared to full Mahalanobis matrix approaches
- Results demonstrate the effectiveness of similarity matching in the dissimilarity space, especially when jointly training the feature extraction backbone and regularized classifier

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dichotomy transformation reduces a multi-class problem to binary classification, improving separability in the embedding space.
- Mechanism: By taking the absolute difference between query and gallery feature vectors, the transformation creates a dissimilarity vector where within-class pairs are closer to the origin and between-class pairs are farther away. This maps the problem to a binary classification task in a space where a linear classifier can more easily separate classes.
- Core assumption: The absolute difference between similar feature vectors produces a vector near the origin, while differences between dissimilar vectors produce larger magnitude vectors.
- Evidence anchors:
  - [abstract] "We argue that the feature dissimilarity space is more suitable for similarity matching, and propose a dichotomy transformation to project query and reference embeddings into a single embedding in the dissimilarity space."
  - [section] "The dichotomy transformation block facilitates the conversion of a multi-class problem into a binary classification task. This process involves organizing the extracted features into pairs representing the absolute differences between the feature vectors."
- Break condition: If the feature space is too noisy or the class distributions overlap significantly, the absolute difference may not produce clear separation between similar and dissimilar pairs.

### Mechanism 2
- Claim: End-to-end training of both the feature extractor and classifier in the dissimilarity space improves feature separation compared to using pre-trained features with a classifier.
- Mechanism: Joint optimization allows the feature extractor to adapt its output to be more suitable for the dissimilarity transformation and subsequent binary classification, rather than optimizing for Euclidean distance metrics.
- Core assumption: The feature extractor can learn representations that are better suited for the dissimilarity space when trained jointly with the classifier.
- Evidence anchors:
  - [abstract] "Our extensive experiments on challenging image retrieval datasets and using diverse feature extraction backbones highlight the benefits of similarity matching in the dissimilarity space. In particular, when jointly training the feature extraction backbone and regularised classifier for matching, the dissimilarity space provides a higher level of accuracy."
  - [section] "The system must be jointly optimized in the embedding and dissimilarity space as a good feature representation is important for dissimilarity space to work in practice."
- Break condition: If the classifier dominates the loss or if the feature extractor cannot effectively adapt to the dissimilarity space during training.

### Mechanism 3
- Claim: The max-margin classifier with L2 regularization on weights learns a diagonal Mahalanobis distance matrix, reducing computational complexity while maintaining performance.
- Mechanism: The L2 constraint on classifier weights effectively learns a diagonal matrix for Mahalanobis distance, which has fewer parameters than a full matrix and avoids overfitting.
- Core assumption: Learning a diagonal Mahalanobis matrix via max-margin classifier weights is sufficient for capturing the relevant distance relationships in the dissimilarity space.
- Evidence anchors:
  - [section] "We propose a method to train the max-margin classifier together with the backbone feature extractor by applying constraints to the L2 norm of the classifier weights along with the hinge loss."
  - [section] "We hypothesize that learning the weight of the max-margin classifier with dissimilarity space is essentially learning diagonal Mahalanobis distance."
- Break condition: If the feature space requires full Mahalanobis matrix to capture correlations between features, the diagonal approximation may be insufficient.

## Foundational Learning

- Concept: Metric learning and contrastive loss functions
  - Why needed here: The paper builds on metric learning foundations to justify why their dissimilarity space approach improves upon traditional Euclidean distance matching.
  - Quick check question: What is the main difference between contrastive loss and triplet loss in metric learning?

- Concept: Binary classification and max-margin classifiers
  - Why needed here: The method converts image retrieval to binary classification of similar/dissimilar pairs using a max-margin classifier.
  - Quick check question: How does the hinge loss function work in training a max-margin classifier?

- Concept: Dichotomy transformation for multi-class reduction
  - Why needed here: This transformation is the core technique that converts the multi-class retrieval problem into a binary classification problem.
  - Quick check question: Given two feature vectors from different classes, what would their absolute difference vector represent in the dissimilarity space?

## Architecture Onboarding

- Component map:
  - Backbone feature extractor (f(·))
  - Adapter layer (W(·) with activation)
  - Dichotomy transformation block (absolute difference)
  - Max-margin classifier (Wc(·) with hinge loss)
  - Joint loss function (Ltotal = Lce + Ltri + Lhinge)

- Critical path:
  1. Input images → Backbone feature extraction
  2. Features → Adapter layer
  3. Adapted features → Dichotomy transformation
  4. Dissimilarity vector → Max-margin classifier
  5. Loss computation → Backpropagation through all components

- Design tradeoffs:
  - Adapter layer adds learnable parameters but helps adapt Euclidean features to dissimilarity space
  - Joint training increases complexity but improves feature separation
  - Diagonal Mahalanobis approximation reduces parameters but may miss feature correlations

- Failure signatures:
  - Poor performance on datasets with high intra-class variation
  - Overfitting on small datasets despite regularization
  - Failure to converge when feature extractor and classifier have conflicting gradients

- First 3 experiments:
  1. Implement the adapter layer and dichotomy transformation, then train with frozen backbone and classifier to verify basic functionality.
  2. Compare performance of joint training vs. frozen backbone training on CUB-200 dataset.
  3. Test different L2 regularization strengths on classifier weights to find optimal balance between margin and overfitting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed end-to-end dissimilarity space learning framework scale with increasing numbers of classes in real-world applications?
- Basis in paper: [explicit] The paper mentions that the method performs well with many classes and scarce examples per class, but does not provide scalability analysis for large-scale real-world datasets.
- Why unresolved: The paper focuses on moderate-scale datasets (CUB-200, Cars-196, Stanford Online Products) but does not address performance on datasets with thousands or millions of classes typical in real-world applications.
- What evidence would resolve it: Experimental results on large-scale datasets with thousands of classes, computational complexity analysis, and comparison with state-of-the-art methods on such datasets.

### Open Question 2
- Question: What is the impact of different backbone architectures on the proposed dissimilarity space learning framework's performance?
- Basis in paper: [explicit] The paper mentions that the method is agnostic to backbone architecture and tests it with CNN and transformer architectures, but does not provide a comprehensive analysis of different backbone choices.
- Why unresolved: The paper only tests a few specific backbone architectures (ResNet50, ViT-S16/316) and does not explore the full range of available backbone options or their impact on performance.
- What evidence would resolve it: Systematic comparison of different backbone architectures (e.g., ResNet variants, Vision Transformers, EfficientNet) on the same datasets, with analysis of trade-offs between performance and computational cost.

### Open Question 3
- Question: How does the proposed method perform in unsupervised or self-supervised learning scenarios?
- Basis in paper: [explicit] The paper mentions that the method could be used with self-supervised pre-training for Person-ReID, but does not provide experimental results or analysis of its performance in unsupervised settings.
- Why unresolved: The paper focuses on supervised learning scenarios and does not explore the method's applicability to unsupervised or self-supervised learning tasks.
- What evidence would resolve it: Experimental results comparing the proposed method's performance in unsupervised or self-supervised settings with state-of-the-art unsupervised metric learning methods on benchmark datasets.

## Limitations

- The paper relies heavily on the assumption that the dichotomy transformation will produce clear separation between similar and dissimilar pairs, but this may not hold for datasets with high intra-class variation or significant feature noise.
- The diagonal Mahalanobis matrix approximation via L2-regularized max-margin classifier weights may be insufficient for capturing complex feature correlations in some datasets.
- The paper does not provide detailed ablation studies on the adapter layer architecture, making it unclear how critical this component is to the overall performance gains.

## Confidence

- **High confidence**: The end-to-end training approach improves feature separation compared to using pre-trained features (supported by experimental results showing consistent improvements over contrastive learning baselines).
- **Medium confidence**: The dichotomy transformation effectively converts multi-class retrieval to binary classification with improved separability (mechanism is plausible but limited empirical validation).
- **Medium confidence**: The max-margin classifier with L2 regularization learns an effective diagonal Mahalanobis matrix (theoretical justification provided but no comparison with full matrix approaches).

## Next Checks

1. **Test the break condition for dichotomy transformation**: Evaluate the method on datasets with high intra-class variation (e.g., iNaturalist) to determine if the absolute difference produces clear separation or if feature noise degrades performance.

2. **Compare diagonal vs. full Mahalanobis matrices**: Implement a version of the method that learns a full Mahalanobis matrix and compare performance against the diagonal approximation to quantify the impact of this design choice.

3. **Ablation study on adapter layer**: Remove the adapter layer and retrain the system to measure its contribution to the overall performance gains and determine if it's a critical component or a minor optimization.