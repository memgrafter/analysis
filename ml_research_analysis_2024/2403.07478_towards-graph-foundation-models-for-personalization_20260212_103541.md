---
ver: rpa2
title: Towards Graph Foundation Models for Personalization
arxiv_id: '2403.07478'
source_url: https://arxiv.org/abs/2403.07478
tags:
- graph
- item
- foundation
- hgnn
- personalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first Graph Foundation Model (GFM) specifically
  designed for personalization at scale. The approach combines a heterogeneous Graph
  Neural Network (HGNN) with Large Language Model (LLM) text featurization in a two-stage
  architecture.
---

# Towards Graph Foundation Models for Personalization

## Quick Facts
- arXiv ID: 2403.07478
- Source URL: https://arxiv.org/abs/2403.07478
- Reference count: 26
- First Graph Foundation Model designed for scalable personalization across diverse content types

## Executive Summary
This paper introduces the first Graph Foundation Model (GFM) for personalization at scale, combining a heterogeneous Graph Neural Network (HGNN) with LLM text featurization in a two-stage architecture. The approach learns static, general-purpose item embeddings using co-interaction signals, then adapts them dynamically for personalized recommendations. Tested on a real-world audio streaming platform with 10M users and 3.5M podcasts, the model demonstrates superior performance compared to content-specific approaches while maintaining efficiency and scalability.

## Method Summary
The method employs a two-stage architecture where a static foundation layer learns item embeddings through a heterogeneous Graph Neural Network (GraphSAGE) trained on user-item interactions and LLM featurized text. A dynamic adaptation layer uses a unified two-tower model to personalize recommendations across different content types. The foundation model learns from 90 days of interaction data to create item-item co-interaction graphs, then adapts daily using streaming data to generate personalized recommendations for audiobooks and podcasts.

## Key Results
- Superior Hit-Rate@10 (HR@10) performance compared to content-specific models
- Unified approach enables efficient personalization across audiobooks and podcasts
- Reduced need for frequent model updates while maintaining accuracy
- Demonstrated scalability on real-world audio streaming platform with millions of users and items

## Why This Works (Mechanism)
The GFM approach works by decoupling foundational learning from dynamic adaptation. The static layer captures universal interaction patterns and item relationships through HGNN embeddings, creating a knowledge base that generalizes across content types. The dynamic layer then efficiently adapts these representations to individual users and specific contexts. This separation allows the model to learn robust, reusable representations while maintaining the flexibility to personalize in real-time.

## Foundational Learning
- **Graph Neural Networks (GNNs)**: Learn node representations by aggregating information from neighboring nodes; needed to capture complex item-item relationships from interaction data
- **Heterogeneous Graphs**: Handle multiple node and edge types simultaneously; needed to represent diverse content types (audiobooks, podcasts) in unified structure
- **Self-supervised Learning**: Use link prediction as proxy task; needed to train on large-scale interaction data without explicit labels
- **LLM Text Featurization**: Convert item metadata into dense vector representations; needed to incorporate semantic information into graph structure
- **Two-tower Architecture**: Separate user and item embedding learning; needed for efficient online inference during personalization
- **Foundation Model Paradigm**: Pre-train on general data then adapt; needed to enable transfer learning across different recommendation tasks

## Architecture Onboarding

**Component Map**: Interaction Data -> HGNN (GraphSAGE) -> Item Embeddings -> Two-Tower Model -> Personalized Recommendations

**Critical Path**: The static foundation layer (HGNN + LLM featurization) forms the critical path, as its quality directly determines the downstream personalization performance. The dynamic adaptation layer depends entirely on the foundation embeddings.

**Design Tradeoffs**: The model trades potential gains from including user nodes in the graph against scalability concerns, choosing to focus on item representations only. This enables faster training and inference but may miss some user-specific patterns that could be captured in a user-inclusive graph.

**Failure Signatures**: Poor HR@10 scores indicate insufficient graph connectivity or suboptimal LLM featurization quality. Model instability may arise from mismatched embedding dimensions between HGNN and two-tower components. Over-reliance on the static foundation without adequate dynamic adaptation can lead to stale recommendations.

**First Experiments**:
1. Construct item-item co-interaction graph from 90 days of user interaction data and featurize nodes with LLM Sentence BERT embeddings
2. Train HGNN (GraphSAGE) with self-supervised link prediction loss on the graph to learn item embeddings
3. Implement two-tower model using HGNN embeddings and user metadata, training daily on streaming data to adapt recommendations

## Open Questions the Paper Calls Out
- **End-to-end Training**: How does performance compare between independently trained components versus a jointly trained GNN-LLM model?
- **User Node Inclusion**: What is the impact of including user nodes in the graph structure on scalability and performance?
- **Cross-domain Generalization**: How well does the foundation model perform on other personalization domains beyond audio streaming?

## Limitations
- Lack of specific hyperparameter values for HGNN and two-tower model architectures
- Undisclosed LLM model details used for text featurization
- Missing computational benchmarks for scalability claims
- Limited to audio streaming domain without cross-domain validation

## Confidence
- **High Confidence**: Two-stage architecture concept, superiority of HGNN-based embeddings, real-world deployment results
- **Medium Confidence**: Scalability claims and efficiency benefits without detailed computational metrics
- **Low Confidence**: Specific implementation details required for exact reproduction

## Next Checks
1. Verify that item-item co-interaction graphs maintain sufficient connectivity and density for effective HGNN training
2. Test alternative LLM featurization approaches to assess impact on embedding quality and recommendation performance
3. Measure computational overhead and latency at different scales to validate claimed efficiency benefits for real-time personalization