---
ver: rpa2
title: 'One fish, two fish, but not the whole sea: Alignment reduces language models''
  conceptual diversity'
arxiv_id: '2411.04427'
source_url: https://arxiv.org/abs/2411.04427
tags:
- diversity
- human
- population
- conceptual
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models are increasingly proposed as substitutes
  for human participants in behavioral research, but questions remain about their
  ability to capture human-like conceptual diversity. This study introduces a novel
  approach to measure the diversity of synthetically generated LLM "populations" by
  relating individual-level variability to population-level variability, inspired
  by human studies.
---

# One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity

## Quick Facts
- arXiv ID: 2411.04427
- Source URL: https://arxiv.org/abs/2411.04427
- Authors: Sonia K. Murthy; Tomer Ullman; Jennifer Hu
- Reference count: 40
- Large language models show reduced conceptual diversity after alignment, making them less suitable as human substitutes in behavioral research

## Executive Summary
This study investigates whether large language models can serve as substitutes for human participants in behavioral research by examining their conceptual diversity. The researchers introduce a novel approach that relates individual-level variability to population-level variability, inspired by human studies, and apply it to two domains with rich human behavioral data: word-color associations and conceptual similarity judgments. Across ten open-source models with and without post-training alignment (RLHF or RLAIF), the results show that no model reaches human-like conceptual diversity, and aligned models generally exhibit less diversity than their non-aligned counterparts. These findings highlight potential trade-offs between increasing models' value alignment and decreasing the diversity of their conceptual representations, cautioning that these trade-offs should be better understood before deploying models as replacements for human subjects in behavioral research.

## Method Summary
The study evaluates conceptual diversity across ten 7B-parameter open-source language models (Mistral, Gemma, Llama families) using two experimental domains. For word-color associations, models generate color responses for 199 words, and diversity is measured using ∆Einternal vs ∆Epopulation divergence metrics and Jensen-Shannon divergence from human distributions. For conceptual similarity judgments, models rate the similarity between word pairs from animal and politician categories, with diversity assessed through Chinese Restaurant Process clustering and between-subjects reliability. The researchers manipulate model outputs using temperature scaling (t ∈ [t0, 1.5, 2.0]) and persona prompting (race, gender, hometown, age, occupation contexts) to simulate diverse populations. They compare aligned (RLHF/RLAIF) and non-aligned variants, benchmarking results against human baseline values from prior studies.

## Key Results
- No model reaches human-like conceptual diversity in either experimental domain
- Aligned models consistently show less diversity than their non-aligned counterparts across both domains
- Temperature and prompt manipulations increase diversity but don't close the gap to human-level variation
- The relationship between individual and population variability differs significantly between models and humans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-training alignment (RLHF/RLAIF) reduces conceptual diversity by collapsing model outputs toward average human preferences.
- Mechanism: Alignment fine-tuning optimizes for alignment with human/synthetic preferences, which are often averaged across individuals. This averaging process inherently reduces variance in model responses, flattening the diversity of conceptual representations.
- Core assumption: Human preferences used for alignment are more homogeneous than the true population of human conceptual representations.
- Evidence anchors:
  - [abstract] "aligned models generally display less diversity than their instruction fine-tuned counterparts"
  - [section 4] "our results suggest that alignment to synthetic or human preferences (in the form of RLAIF and RLHF) flattens models' conceptual diversity"
- Break condition: If alignment data captures true population variance rather than averaging preferences, this mechanism would not hold.

### Mechanism 2
- Claim: Temperature and prompt manipulations increase output diversity by introducing controlled randomness into the generation process.
- Mechanism: Higher temperature values increase the entropy of the token distribution, allowing for more diverse token selection. Prompt manipulations add contextual variability that can steer models toward different response patterns.
- Core assumption: Increasing entropy or adding context reliably translates to meaningful diversity in conceptual representations rather than just random noise.
- Evidence anchors:
  - [section 3.3] "Prior work has explored this method as a way of increasing the diversity of generated output"
  - [section 4] "we find a main effect of both prompt and temperature manipulation (p < 0.001 for both)"
- Break condition: If higher entropy only produces incoherent outputs rather than meaningful diversity, this mechanism would not achieve the desired effect.

### Mechanism 3
- Claim: The relationship between individual-level and population-level variability serves as a diagnostic for conceptual diversity.
- Mechanism: By measuring both internal variability (within individual representations) and population variability (across individuals), researchers can distinguish homogeneous populations (shared representations) from heterogeneous ones (diverse representations).
- Core assumption: Human conceptual diversity exhibits a characteristic pattern where population variability exceeds individual variability, which can be detected using the proposed metric.
- Evidence anchors:
  - [section 3.1] "While these two measures are correlated for humans, internal variability is overall lower than population variability, consistent with a heterogeneous population"
  - [section 3.2] "words in the 'politicians' category are far more likely to have multiple meanings than 'animal' words"
- Break condition: If the proposed metric fails to capture meaningful distinctions between homogeneous and heterogeneous populations, this mechanism would not work as intended.

## Foundational Learning

- Concept: Conceptual diversity
  - Why needed here: The paper's central research question is whether LLMs can capture human-like conceptual diversity, making this foundational to understanding the study's purpose
  - Quick check question: What distinguishes conceptual diversity from simple output variability in LLM research?

- Concept: Post-training alignment techniques (RLHF/RLAIF)
  - Why needed here: The study specifically examines how these alignment methods affect model diversity, so understanding their mechanisms is crucial
  - Quick check question: How do RLHF and RLAIF differ in their approach to optimizing model behavior?

- Concept: Population heterogeneity metrics
  - Why needed here: The paper introduces specific metrics to measure diversity by relating individual and population variability, which are central to the experimental design
  - Quick check question: Why is it important to measure both internal and population variability when assessing conceptual diversity?

## Architecture Onboarding

- Component map:
  - Data collection: Human behavioral datasets (word-color associations, conceptual similarity judgments)
  - Model evaluation: 10 open-source LLMs with varying alignment states
  - Diversity measurement: Two domain-specific metrics for conceptual diversity
  - Population simulation: Temperature and prompt-based manipulations
  - Analysis pipeline: Regression models comparing diversity across conditions

- Critical path: Data → Model generation → Diversity measurement → Statistical analysis → Comparison to human baselines

- Design tradeoffs:
  - Model selection: 7B parameter models chosen for computational efficiency vs. potentially missing effects present in larger models
  - Domain choice: Simple conceptual domains allow controlled experiments but may not generalize to complex applications
  - Diversity metrics: Domain-specific metrics capture relevant variation but limit cross-domain comparisons

- Failure signatures:
  - High invalid response rates (particularly at high temperatures or with nonsense prompts)
  - Models clustering near the line of unity in population vs internal variability plots
  - Minimal differences between aligned and non-aligned models in diversity metrics

- First 3 experiments:
  1. Run word-color association task with a single non-aligned model at default temperature to verify basic functionality
  2. Test temperature manipulation (t=1.5) on the same model to confirm diversity increases as expected
  3. Compare aligned vs non-aligned variants of the same base model on the concept similarity task to observe alignment effects on diversity

## Open Questions the Paper Calls Out

None

## Limitations

- Findings based on 7B parameter models may not generalize to larger models or different architectures
- Results from simplified conceptual domains may not capture complexity of real behavioral research applications
- Cannot definitively establish causation between alignment and diversity reduction due to potential confounding factors
- Relies on existing human baseline datasets that may have sampling biases

## Confidence

**High confidence**: Core finding that aligned models exhibit less conceptual diversity than non-aligned counterparts
**Medium confidence**: Mechanism that alignment averaging reduces diversity and generalizability to behavioral research applications
**Medium confidence**: Relationship between diversity reduction and downstream behavioral research validity

## Next Checks

1. **Replicate with larger models**: Test the same alignment-diversity relationship using 70B or larger parameter models to determine if scale affects the magnitude of diversity reduction

2. **Validate across diverse conceptual domains**: Extend the methodology to more complex behavioral domains such as moral reasoning, emotional associations, or social cognition tasks commonly studied in human behavioral research

3. **Investigate intermediate alignment stages**: Conduct experiments tracking conceptual diversity changes through different stages of the alignment process (pre-alignment, post-RLHF, post-RLAIF) to isolate which specific aspects of alignment optimization drive diversity reduction