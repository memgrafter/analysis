---
ver: rpa2
title: '2.5D Multi-view Averaging Diffusion Model for 3D Medical Image Translation:
  Application to Low-count PET Reconstruction with CT-less Attenuation Correction'
arxiv_id: '2406.08374'
source_url: https://arxiv.org/abs/2406.08374
tags:
- image
- madm
- methods
- translation
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a 2.5D Multi-view Averaging Diffusion Model
  (MADM) for 3D medical image-to-image translation, specifically for translating non-attenuation-corrected
  low-dose PET (NAC-LDPET) into attenuation-corrected standard-dose PET (AC-SDPET)
  with CT-less attenuation correction. The core method employs separate diffusion
  models for axial, coronal, and sagittal views, whose outputs are averaged in each
  sampling step to ensure 3D generation quality from multiple views.
---

# 2.5D Multi-view Averaging Diffusion Model for 3D Medical Image Translation: Application to Low-count PET Reconstruction with CT-less Attenuation Correction

## Quick Facts
- **arXiv ID**: 2406.08374
- **Source URL**: https://arxiv.org/abs/2406.08374
- **Reference count**: 26
- **Primary result**: MADM achieves PSNR of 38.920±1.503 and SSIM of 0.9944±0.0020 for translating NAC-LDPET to AC-SDPET under 5% low-count settings

## Executive Summary
This paper introduces a 2.5D Multi-view Averaging Diffusion Model (MADM) for translating non-attenuation-corrected low-dose PET (NAC-LDPET) into attenuation-corrected standard-dose PET (AC-SDPET) for CT-less attenuation correction. The method uses separate diffusion models for axial, coronal, and sagittal views, averaging their outputs at each denoising step to ensure 3D consistency. A CNN-based 3D generation serves as a prior to accelerate sampling. Experiments on human patient studies demonstrate MADM outperforms previous CNN and diffusion-based methods, achieving state-of-the-art image quality metrics for low-count PET reconstruction.

## Method Summary
MADM translates NAC-LDPET to AC-SDPET using a hybrid approach combining 2.5D diffusion models with a CNN prior. Three separate diffusion models process axial, coronal, and sagittal slices independently, each conditioned on 2.5D inputs (target slice plus s adjacent slices). During sampling, outputs from all three views are averaged at each timestep to ensure 3D consistency. The process starts from a CNN-generated 3D prior image that is diffused to match the initial noise level, reducing the number of required denoising steps. The method is trained on 147 subjects from Yale New Haven Hospital and evaluated under 5% and 10% low-count PET settings using PSNR, SSIM, RMSE, and SUV mean error metrics.

## Key Results
- MADM achieves PSNR of 38.920±1.503 and SSIM of 0.9944±0.0020 under 5% low-count PET settings
- The 2.5D approach with s=1 adjacent slices improves PSNR from 37.82 dB to 38.92 dB compared to 2D
- Starting diffusion from a CNN prior at ts=200 provides optimal balance between speed and quality
- MADM outperforms baseline methods including UNet, cGAN, 3D-DDPM, and TPDM on all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
Multi-view averaging during diffusion sampling improves 3D consistency and reduces artifacts by enforcing cross-view consistency through averaging outputs from orthogonal views at each denoising step. The core assumption is that averaging predictions from axial, coronal, and sagittal views provides a more robust 3D estimate than any single view alone. Evidence shows averaging is explicitly used in the implementation and is consistent with standard 3D reconstruction techniques. The break condition occurs if view models disagree strongly, potentially causing blurring of valid signal.

### Mechanism 2
Using a CNN-generated 3D prior image accelerates diffusion convergence by providing a global structural estimate that seeds the sampling process. The prior captures global anatomy in one step, and diffusion refines local details. The core assumption is that the CNN prior captures sufficient global structure to guide diffusion while allowing refinement. Evidence shows the prior is explicitly generated and diffused to the starting timestep. The break condition occurs if the CNN prior is poor, potentially biasing the diffusion output away from the true distribution.

### Mechanism 3
2.5D conditioning with adjacent slices improves translation quality by providing spatial context while maintaining computational efficiency. Each view model receives the target slice plus s adjacent slices, allowing learning of local 3D context without full 3D convolutions. The core assumption is that local 3D context from adjacent slices resolves ambiguities that pure 2D models cannot. Evidence shows quantitative improvement from 37.82 dB to 38.92 dB PSNR when transitioning from 2D to 2.5D. The break condition occurs if s is too small (insufficient context) or too large (computational savings diminish).

## Foundational Learning

- **Score-based generative modeling and diffusion processes**: Understanding DDPM forward noising and reverse denoising is essential to grasp how multi-view averaging and prior seeding work. Quick check: In DDPM, what is the role of noise schedule parameters (βt, αt, ¯αt) during sampling?
- **2.5D vs 2D vs 3D convolution architectures**: Knowing their tradeoffs (memory, speed, context) explains why 2.5D with adjacent slices was chosen. Quick check: How does a 2.5D model with s adjacent slices differ computationally from a full 3D model with the same effective receptive field?
- **Multi-view medical image reconstruction and fusion**: Understanding how axial/coronal/sagittal data complement each other is key to the core innovation. Quick check: In PET attenuation correction, why might coronal and sagittal views provide complementary information to axial slices?

## Architecture Onboarding

- **Component map**: CNN Prior Generator -> Diffusion Sampler -> Multi-view 2.5D Diffusion Models -> Averaging Block -> Loss Functions
- **Critical path**: 1. Train CNN prior (Eq. 1) 2. Train three 2.5D diffusion models (Eq. 9) 3. During inference: generate prior → diffuse to timestep ts → for t=ts..0: predict each view → average → update → output
- **Design tradeoffs**: Memory vs quality (2.5D uses less memory than 3D but more than 2D; multiple views increase memory but improve consistency), speed vs accuracy (prior seeding speeds inference but may reduce quality), simplicity vs performance (averaging is simple but may not be optimal)
- **Failure signatures**: View disagreement (large differences between view outputs suggest instability), prior bias (results consistently resemble CNN prior too closely), memory overflow (increasing s or views too much exceeds GPU capacity)
- **First 3 experiments**: 1. Train CNN prior alone and evaluate PSNR vs ground truth 2. Train single 2.5D view model (axial only) and compare to 2D baseline 3. Run MADM with ts=1000 vs ts=200 and measure quality/speed tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal starting timestep (ts) for diffusion sampling and how does it affect the trade-off between inference speed and image quality? While the paper identifies ts = 200 as optimal, the precise relationship between ts and performance metrics across different datasets and applications remains unexplored. Systematic ablation studies varying ts across diverse datasets would resolve this.

### Open Question 2
How does the 2.5D approach in MADM compare to a fully 3D diffusion model in terms of image quality and computational efficiency? The paper does not implement a fully 3D diffusion model for comparison due to computational constraints, leaving whether 2.5D sacrifices image quality for efficiency unanswered. Implementing and evaluating a fully 3D model would resolve this.

### Open Question 3
How does MADM perform on other 3D medical imaging modalities beyond PET, such as MRI or CT? The paper suggests potential applications but does not provide experimental validation. Applying MADM to other 3D medical imaging datasets and tasks would resolve this.

## Limitations
- Lack of detailed architectural specifications for diffusion models and CNN prior limits faithful reproduction
- Evaluation limited to a single dataset from one institution raises questions about generalizability across different PET scanners and patient populations
- Substantial computational requirements for multi-view approach may limit clinical deployment
- No evidence addressing potential biases introduced by the CNN prior or performance on rare/atypical cases

## Confidence

- **Mechanism 1 (Multi-view averaging)**: High confidence - Well-justified theoretically with ablation study support
- **Mechanism 2 (CNN prior acceleration)**: Medium confidence - Plausible but limited evidence on convergence benefits
- **Mechanism 3 (2.5D conditioning)**: High confidence - Directly supported by quantitative PSNR improvement
- **Overall performance claims**: Medium confidence - Impressive metrics but limited comparisons and single-dataset evaluation

## Next Checks

1. **Architectural replication test**: Implement MADM with reasonable defaults for architectural details and verify basic training convergence on a small-scale dataset version

2. **Prior bias evaluation**: Systematically compare MADM outputs with different starting time steps (ts=1000 vs ts=200) to quantify speed-quality tradeoff and assess CNN prior biases in specific anatomical regions

3. **Cross-institutional generalization test**: Evaluate MADM on PET data from at least one additional institution with different scanner models to assess robustness beyond the Yale dataset