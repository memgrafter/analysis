---
ver: rpa2
title: 'Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc
  QA'
arxiv_id: '2406.17419'
source_url: https://arxiv.org/abs/2406.17419
tags:
- embedding
- openai
- long-context
- loong
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Loong, a benchmark designed to evaluate long-context
  understanding in real-world multi-document scenarios. Unlike existing benchmarks
  that use irrelevant noise texts, Loong focuses on extended multi-document question
  answering where each document is relevant to the final answer, requiring comprehensive
  understanding of all documents.
---

# Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA

## Quick Facts
- **arXiv ID:** 2406.17419
- **Source URL:** https://arxiv.org/abs/2406.17419
- **Reference count:** 30
- **Primary result:** Loong benchmark shows current LLMs achieve only 55.37% average scores on multi-document long-context tasks, with RAG performing poorly

## Executive Summary
This paper introduces Loong, a benchmark designed to evaluate long-context understanding in real-world multi-document scenarios. Unlike existing benchmarks that use irrelevant noise texts, Loong focuses on extended multi-document question answering where each document is relevant to the final answer, requiring comprehensive understanding of all documents. The benchmark includes four task types—Spotlight Locating, Comparison, Clustering, and Chain of Reasoning—across varying context lengths from 10K to 250K tokens. Extensive experiments with advanced LLMs show significant performance gaps, with even the most capable models achieving only 55.37% average scores and 27% perfect rates. Notably, Retrieval Augmented Generation (RAG) performs poorly on Loong, demonstrating that the benchmark effectively tests genuine long-context modeling capabilities rather than superficial document retrieval.

## Method Summary
The Loong benchmark evaluates long-context understanding in multi-document scenarios using authentic documents from domains like financial reports, legal cases, and academic papers. The benchmark features four task categories requiring different cognitive operations: Spotlight Locating (finding evidence), Comparison (analyzing relationships), Clustering (grouping documents), and Chain of Reasoning (complex inference). Documents are concatenated and processed at varying context lengths (10K-250K tokens), with model outputs evaluated by GPT-4 judge using Avg Scores and Perfect Rate metrics. The benchmark tests whether models can genuinely understand long contexts versus relying on retrieval mechanisms.

## Key Results
- Current long-context LLMs achieve only 55.37% average scores on Loong benchmark
- Perfect rate is just 27%, indicating significant room for improvement
- RAG integration shows poor performance, demonstrating benchmark tests genuine long-context understanding
- Models show performance degradation as context length increases, particularly beyond 128K tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evidence dispersion across multiple documents forces models to perform comprehensive long-context modeling rather than superficial retrieval
- Mechanism: By scattering evidence across all documents, the benchmark ensures that ignoring any document leads to failure, requiring models to process the entire context rather than cherry-picking
- Core assumption: Models cannot succeed by simply retrieving a few key sentences without understanding the full context
- Evidence anchors: [abstract] "in Loong's test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer"

### Mechanism 2
- Claim: The four task categories progressively test different aspects of long-context understanding
- Mechanism: Each task type requires different cognitive operations - from simple localization to complex reasoning across documents, creating a comprehensive evaluation framework
- Core assumption: Different real-world scenarios require different types of multi-document processing capabilities
- Evidence anchors: [section] "we propose new task categories for multi-document long-context modeling and closer alignment with real-world scenarios"

### Mechanism 3
- Claim: Scaling law observation shows that training length must exceed actual processing capability for effective long-context modeling
- Mechanism: Models need to be trained on data longer than their target processing window to develop genuine long-context understanding
- Core assumption: The relationship between training data length and processing capability is non-linear and requires sufficient margin
- Evidence anchors: [section] "to truly equip an LLM with the ability to handle 128K long texts, it should be trained on data exceeding 128K"

## Foundational Learning

- Concept: Evidence distribution and retrieval patterns in multi-document contexts
  - Why needed here: Understanding how evidence is distributed affects model evaluation and benchmark design
  - Quick check question: What's the difference between evidence centralized in one document versus dispersed across multiple documents?

- Concept: Long-context scaling laws and their relationship to training data
  - Why needed here: The benchmark reveals important insights about how model capabilities scale with context length
  - Quick check question: Why might a model trained on 128K data struggle with 128K context?

- Concept: Task decomposition and cognitive requirements in multi-document processing
  - Why needed here: Different task types require different cognitive operations and evaluation approaches
  - Quick check question: How does the cognitive load differ between Spotlight Locating and Chain of Reasoning tasks?

## Architecture Onboarding

- Component map: Document parser → Task type classifier → Evidence locator → Answer generator → GPT-4 evaluator
- Critical path: Document ingestion → Multi-document concatenation → Task-specific prompt generation → Model inference → Answer evaluation
- Design tradeoffs: Comprehensive evaluation vs. annotation cost, real-world relevance vs. controlled conditions
- Failure signatures: Poor performance on longer contexts, task-specific weaknesses, reliance on RAG over genuine understanding
- First 3 experiments:
  1. Test model performance across different context length sets to identify scaling breakpoints
  2. Compare RAG-enhanced vs. pure long-context models to understand capability differences
  3. Analyze performance by task type to identify specific weaknesses in multi-document reasoning

## Open Questions the Paper Calls Out

None

## Limitations

- Real-world applicability of document selection may introduce sampling bias, as documents are specifically chosen to be relevant to answers
- GPT-4 judge reliability and potential bias across different task types and language domains (English vs. Chinese) is not fully addressed
- The four task categories may not capture all types of long-context reasoning that occur in real-world applications

## Confidence

**High Confidence Claims:**
- Loong benchmark effectively demonstrates performance gaps in current long-context models
- RAG integration shows poor performance on Loong, indicating genuine long-context understanding is tested
- The scaling law observation that training length should exceed processing capability is supported

**Medium Confidence Claims:**
- The four task categories comprehensively cover real-world multi-document scenarios
- Evidence dispersion across documents is the primary reason for current model limitations
- The benchmark represents a significant improvement over existing evaluations

**Low Confidence Claims:**
- The specific numerical thresholds for performance gaps and their implications for real-world deployment
- The claim that Loong is the "first benchmark for multi-document long-context modeling"
- The universality of the scaling law across different model architectures

## Next Checks

**Validation Check 1:** Conduct a small-scale human evaluation study comparing GPT-4 judge scores with expert human judgments on a subset of Loong tasks to validate the reliability of the automated evaluation system.

**Validation Check 2:** Create a modified version of Loong that includes irrelevant or partially relevant documents and compare model performance to test whether performance gaps are specifically due to evidence dispersion.

**Validation Check 3:** Evaluate whether models that perform well on Loong also show improved performance on downstream real-world applications to validate the benchmark's effectiveness as a predictor of practical long-context capabilities.