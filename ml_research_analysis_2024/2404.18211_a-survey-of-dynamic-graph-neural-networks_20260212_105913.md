---
ver: rpa2
title: A survey of dynamic graph neural networks
arxiv_id: '2404.18211'
source_url: https://arxiv.org/abs/2404.18211
tags:
- graph
- dynamic
- graphs
- learning
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews dynamic graph neural networks
  (GNNs), which address the limitations of traditional static GNNs by modeling temporal
  dependencies in evolving graph structures. The paper categorizes dynamic GNN models
  based on how they incorporate temporal information, covering stacked architectures
  (spatial GNNs + temporal RNNs), integrated architectures (fused spatial-temporal
  layers), and specialized approaches for continuous-time dynamic graphs using point
  processes, RNNs, or temporal random walks.
---

# A survey of dynamic graph neural networks

## Quick Facts
- arXiv ID: 2404.18211
- Source URL: https://arxiv.org/abs/2404.18211
- Reference count: 40
- Models dynamic graph neural networks that capture temporal dependencies in evolving graph structures

## Executive Summary
This survey comprehensively reviews dynamic graph neural networks (GNNs), which address the limitations of traditional static GNNs by modeling temporal dependencies in evolving graph structures. The paper categorizes dynamic GNN models based on how they incorporate temporal information, covering stacked architectures (spatial GNNs + temporal RNNs), integrated architectures (fused spatial-temporal layers), and specialized approaches for continuous-time dynamic graphs using point processes, RNNs, or temporal random walks. It also discusses scalable solutions for large dynamic graphs, including efficient algorithms and distributed training frameworks.

## Method Summary
The survey systematically categorizes dynamic GNN approaches based on temporal information incorporation mechanisms. The core methodology involves analyzing how sequence modeling modules are integrated with traditional GNN architectures to capture both structural and temporal dependencies. The paper examines discrete-time dynamic graphs (DTDGs) using stacked or integrated architectures, and continuous-time dynamic graphs (CTDGs) using specialized approaches like point processes and temporal random walks. The evaluation framework focuses on node classification, link prediction, and graph analysis tasks, with emphasis on architectural design choices and scalability considerations.

## Key Results
- Dynamic GNNs can be categorized into stacked architectures (spatial GNNs + temporal RNNs) and integrated architectures (fused spatial-temporal layers)
- Continuous-time dynamic graphs require specialized approaches using point processes, RNNs, or temporal random walks
- Major challenges include scalability, heterogeneous information handling, and lack of diverse graph datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic GNNs model both structural and temporal dependencies in evolving graph structures
- Mechanism: Dynamic GNNs integrate sequence modeling modules into traditional GNN architectures, allowing them to capture the inherent temporal dependencies of dynamic graphs
- Core assumption: The temporal evolution of graph structures and node attributes can be effectively modeled using sequence learning techniques
- Evidence anchors:
  - [abstract]: "By integrating sequence modeling modules into traditional GNN architectures, dynamic GNNs aim to bridge this gap, capturing the inherent temporal dependencies of dynamic graphs for a more authentic depiction of complex networks."
  - [section]: "These models aggregate features from neighboring nodes and incorporate a time series module, enabling the modeling of both structural features and temporal dependencies within dynamic graphs."
  - [corpus]: "Weak - the corpus lacks specific evidence on the mechanism of how dynamic GNNs integrate sequence modeling modules."
- Break condition: If the temporal dependencies cannot be effectively modeled using sequence learning techniques, the dynamic GNN may fail to capture the evolving nature of the graph.

### Mechanism 2
- Claim: Dynamic GNNs can be categorized based on how they incorporate temporal information
- Mechanism: The paper categorizes dynamic GNN models into stacked architectures (spatial GNNs + temporal RNNs), integrated architectures (fused spatial-temporal layers), and specialized approaches for continuous-time dynamic graphs
- Core assumption: The way temporal information is incorporated into dynamic GNNs significantly impacts their performance and applicability
- Evidence anchors:
  - [abstract]: "The paper categorizes models based on how temporal information is incorporated."
  - [section]: "Based on how they are combined, they can be broadly classified as stacked architectures or integrated architectures."
  - [corpus]: "Missing - the corpus does not provide specific evidence on the categorization of dynamic GNNs based on temporal information incorporation."
- Break condition: If the categorization does not accurately reflect the differences in how dynamic GNNs incorporate temporal information, the framework may not effectively guide model selection or design.

### Mechanism 3
- Claim: Dynamic GNNs face challenges in scalability, heterogeneous information handling, and lack of diverse graph datasets
- Mechanism: The paper identifies scalability issues due to the need to process large amounts of temporal data, difficulties in handling heterogeneous information types, and the lack of diverse graph datasets for training and evaluation
- Core assumption: Addressing these challenges is crucial for the widespread adoption and effectiveness of dynamic GNNs in real-world applications
- Evidence anchors:
  - [abstract]: "Although dynamic GNNs have shown superior performance, challenges remain in scalability, handling heterogeneous information, and lack of diverse graph datasets."
  - [section]: "Although some work has surveyed methods for dynamic graph representation learning, with the continuous emergence of new methods and applications, the content of existing reviews has become outdated and cannot reflect the latest research developments and technological trends."
  - [corpus]: "Missing - the corpus does not provide specific evidence on the challenges faced by dynamic GNNs."
- Break condition: If these challenges are not adequately addressed, the performance and applicability of dynamic GNNs may be limited in complex, real-world scenarios.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are the foundation upon which dynamic GNNs are built, providing the ability to learn from graph-structured data
  - Quick check question: What are the key components of a GNN, and how do they enable learning from graph-structured data?

- Concept: Dynamic Graphs
  - Why needed here: Dynamic graphs represent the evolving nature of real-world networks, which dynamic GNNs aim to model and analyze
  - Quick check question: How do dynamic graphs differ from static graphs, and what are the implications for learning algorithms?

- Concept: Sequence Modeling
  - Why needed here: Sequence modeling techniques, such as RNNs, are integrated into dynamic GNNs to capture the temporal dependencies in evolving graph structures
  - Quick check question: What are the key principles of sequence modeling, and how can they be applied to dynamic graph data?

## Architecture Onboarding

- Component map:
  Input -> GNN layers (spatial modeling) -> Sequence modeling layers (temporal modeling) -> Fusion mechanisms -> Output

- Critical path:
  1. Input dynamic graph data
  2. Process data through GNN layers to capture structural information
  3. Integrate sequence modeling layers to capture temporal dependencies
  4. Fuse spatial and temporal information
  5. Generate dynamic node embeddings or predictions

- Design tradeoffs:
  - Accuracy vs. computational efficiency: More complex models may capture more information but require more resources
  - Generalization vs. overfitting: Balancing model complexity to avoid overfitting on specific datasets
  - Interpretability vs. performance: More interpretable models may sacrifice some performance

- Failure signatures:
  - Poor performance on temporal tasks: Indicates issues with sequence modeling integration
  - Overfitting on specific datasets: Suggests the need for regularization or more diverse training data
  - Slow inference times: Points to computational inefficiencies in the model architecture

- First 3 experiments:
  1. Implement a simple dynamic GNN using a stacked architecture (GNN + RNN) on a small, synthetic dataset
  2. Evaluate the impact of different sequence modeling techniques (e.g., LSTM vs. GRU) on model performance
  3. Test the scalability of the dynamic GNN on a larger, real-world dataset and identify potential bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dynamic GNNs be made more interpretable to understand their decision-making processes?
- Basis in paper: [explicit] The paper identifies interpretability of dynamic GNNs as a future direction and mentions DGExplainer and DynBrainGNN as initial attempts to provide explanations for dynamic GNN predictions.
- Why unresolved: Current interpretability methods for dynamic GNNs are still in early stages, and there is a lack of comprehensive approaches that can effectively explain the complex temporal and structural patterns learned by these models.
- What evidence would resolve it: Development and evaluation of new interpretability techniques specifically designed for dynamic GNNs that can provide clear and actionable insights into their predictions, along with case studies demonstrating their practical utility.

### Open Question 2
- Question: How can dynamic GNNs be adapted to handle more complex dynamic graph structures, such as dynamic signed networks and dynamic heterophilic graphs?
- Basis in paper: [explicit] The paper suggests that current dynamic graph datasets lack diversity and that future research should focus on constructing datasets with more complex graph structures, including dynamic signed networks and dynamic heterophilic graphs.
- Why unresolved: Existing dynamic GNN models are primarily designed for and tested on relatively simple dynamic graph structures, and their performance on more complex graph types remains unclear.
- What evidence would resolve it: Creation of new dynamic graph datasets with signed edges and heterophilic structures, along with evaluations of various dynamic GNN models on these datasets to assess their ability to handle such complexity.

### Open Question 3
- Question: Can large language models (LLMs) be effectively integrated with dynamic GNNs to improve their performance on dynamic graph tasks?
- Basis in paper: [explicit] The paper discusses the potential of using LLMs for dynamic graph learning, highlighting their strong few-shot learning capabilities and ability to understand spatial and temporal information. However, it also identifies challenges such as processing large-scale graphs and computational efficiency.
- Why unresolved: While LLMs show promise for dynamic graph tasks, their integration with dynamic GNNs is still in early stages, and there are open questions about how to effectively leverage their strengths while addressing their limitations.
- What evidence would resolve it: Development and evaluation of novel architectures that combine LLMs and dynamic GNNs, along with experiments demonstrating improved performance on various dynamic graph tasks compared to using either model alone.

## Limitations
- Limited empirical validation: The survey lacks extensive experimental results or benchmark comparisons across multiple datasets to substantiate performance claims
- Temporal modeling mechanisms: Provides limited technical detail on actual integration mechanisms between spatial GNN layers and temporal modules
- Scalability claims: Limited discussion of specific computational bottlenecks or quantitative analysis of memory/time complexity for different dynamic GNN approaches

## Confidence

High confidence in the categorization framework and architectural overview - the survey provides a clear taxonomy of dynamic GNN approaches based on temporal information incorporation, with well-defined categories (stacked, integrated, continuous-time).

Medium confidence in the identified challenges - the scalability, heterogeneity, and dataset limitations are logically sound, but lack empirical quantification across the surveyed methods.

Low confidence in performance claims - the assertion that dynamic GNNs show "superior performance" is not substantiated with specific benchmark results or comparative analyses.

## Next Checks

1. Replicate core architecture: Implement a basic stacked dynamic GNN (GCN + LSTM) on a standard dynamic graph dataset (e.g., Enron email network) and compare performance against static GNN baselines for node classification.

2. Benchmark temporal modeling: Systematically compare different temporal integration mechanisms (stacked vs. integrated architectures) on the same dataset to validate the claim that temporal information incorporation significantly impacts performance.

3. Scalability analysis: Measure memory usage and inference time for dynamic GNN models as graph size and temporal resolution increase, validating the identified scalability challenges and testing proposed solutions.