---
ver: rpa2
title: Reset-free Reinforcement Learning with World Models
arxiv_id: '2408.09807'
source_url: https://arxiv.org/abs/2408.09807
tags:
- morefree
- reset-free
- learning
- states
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of training reinforcement learning
  agents without access to environment resets, a major barrier to real-world deployment.
  The authors demonstrate that model-based RL methods, specifically PEG and MoReFree,
  can outperform all prior state-of-the-art reset-free methods while requiring less
  supervision (no environmental rewards or demonstrations).
---

# Reset-free Reinforcement Learning with World Models

## Quick Facts
- arXiv ID: 2408.09807
- Source URL: https://arxiv.org/abs/2408.09807
- Authors: Zhao Yang, Thomas M. Moerland, Mike Preuss, Aske Plaat, Edward S. Hu
- Reference count: 28
- Primary result: Model-based RL methods outperform prior reset-free methods while requiring less supervision

## Executive Summary
This paper addresses the challenge of training reinforcement learning agents without access to environment resets, which is crucial for real-world deployment. The authors demonstrate that model-based RL methods, specifically PEG and MoReFree, can significantly outperform all prior state-of-the-art reset-free methods while requiring less supervision. MoReFree introduces key adaptations including exploration focused on task-relevant states and policy learning that prioritizes reaching initial and evaluation states. Across 8 challenging reset-free tasks spanning manipulation and locomotion, MoReFree and reset-free PEG show superior performance in both final success rates and sample efficiency.

## Method Summary
The approach combines model-based RL with goal-conditioned exploration strategies to handle reset-free learning. The core method uses a world model (RSSM-based DreamerV2) to generate synthetic data, with exploration performed through a Back-and-Forth Go-Explore scheme that alternates between task-solving, resetting, and exploration phases. The policy is trained in imagination using task-relevant goals including evaluation states, initial states, and random replay buffer states. The method requires only state and action trajectories without rewards or demonstrations, making it more data-efficient than prior approaches.

## Key Results
- MoReFree and reset-free PEG significantly outperform prior state-of-the-art reset-free methods across all 8 tested tasks
- MoReFree shows 1.3-5Ã— more task-relevant data collection in the 3 hardest tasks through its back-and-forth exploration strategy
- The approach successfully learns non-trivial reset behaviors like picking objects out of corners
- Sample efficiency is improved compared to model-free baselines

## Why This Works (Mechanism)

### Mechanism 1
Model-based RL agents with strong exploration capabilities outperform prior reset-free methods by efficiently exploring the state space and generating useful trajectories without episodic resets. The world model enables directed exploration that covers relevant regions of the state space more effectively than undirected exploration.

### Mechanism 2
MoReFree improves upon basic model-based approaches by focusing exploration and policy learning on task-relevant states. The Back-and-Forth Go-Explore scheme alternates between solving tasks, resetting to initial states, and exploring new regions, ensuring the agent returns to states relevant to the task periodically.

### Mechanism 3
Training the goal-conditioned policy in imagination with task-relevant goals improves performance by preparing the policy for the exploration scheme. By biasing policy training towards achieving evaluation states, initial states, and random replay buffer states, the policy becomes better equipped to succeed in the real environment.

## Foundational Learning

- **Markov Decision Process (MDP)**
  - Why needed here: The paper frames the problem as a goal-conditioned MDP, defining states, actions, rewards, and transitions
  - Quick check question: What are the components of a goal-conditioned MDP and how does it differ from a standard MDP?

- **Model-based Reinforcement Learning**
  - Why needed here: The paper uses model-based RL methods (PEG and MoReFree) that learn a world model to generate synthetic data for policy training
  - Quick check question: How does model-based RL differ from model-free RL, and what are the advantages and disadvantages of each approach?

- **Goal-conditioned Exploration**
  - Why needed here: The paper uses goal-conditioned exploration strategies (Go-Explore) to direct the agent to specific states for exploration and resetting
  - Quick check question: What is goal-conditioned exploration, and how does it differ from undirected exploration in reinforcement learning?

## Architecture Onboarding

- **Component map**: World model (RSSM) -> Goal-conditioned policy -> Exploration policy -> Dynamical distance function -> Back-and-Forth Go-Explore -> Imagination-based policy training

- **Critical path**: 1) Collect data using Back-and-Forth Go-Explore, 2) Train world model on collected data, 3) Generate synthetic data using world model, 4) Train policies in imagination using synthetic data, 5) Repeat

- **Design tradeoffs**: Exploration vs. exploitation balancing between exploring new states and focusing on task-relevant states; Model accuracy vs. data coverage ensuring world model accuracy for useful synthetic data; Task-relevant vs. random goals determining optimal balance for policy training

- **Failure signatures**: Poor world model accuracy leading to ineffective policy training; Agent getting stuck in task-irrelevant regions; Insufficient exploration of state space; Overfitting to synthetic data

- **First 3 experiments**: 1) Run reset-free PEG on simple task to verify basic adaptation works, 2) Implement Back-and-Forth Go-Explore and compare against reset-free PEG, 3) Add task-relevant goal sampling in imagination and measure impact on final performance

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding adaptive exploration curricula, the specific challenges of modeling certain task dynamics like door mechanisms, and the scalability to high-dimensional observation spaces. The authors suggest that future work could explore dynamic scheduling of task-relevant goals rather than using fixed percentages, investigate why certain tasks are particularly challenging for model-based approaches, and extend the method to visual observations.

## Limitations
- Evaluation limited to 8 environments with varying task characteristics
- Results particularly strong in tasks where reset behaviors are relatively straightforward
- World model accuracy is critical but only indirectly evaluated
- Comparison against prior work uses different evaluation protocols
- Scalability to more complex real-world tasks remains untested

## Confidence
- **Core claim**: High confidence that model-based RL outperforms prior reset-free methods
- **MoReFree improvements**: Medium confidence due to limited number of environments and potential task-specific effects
- **Scalability**: Low confidence in applicability to significantly more complex real-world tasks without additional engineering

## Next Checks
1. Test MoReFree on a new task with multiple distinct reset behaviors to evaluate generalization beyond simple locomotion/manipulation tasks
2. Conduct ablation studies on world model accuracy vs. policy performance to quantify the impact of model errors on reset-free learning
3. Implement an oracle reset-free baseline with perfect model knowledge to establish upper bounds on achievable performance