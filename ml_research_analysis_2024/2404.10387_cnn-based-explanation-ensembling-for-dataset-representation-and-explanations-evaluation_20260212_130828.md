---
ver: rpa2
title: CNN-based explanation ensembling for dataset, representation and explanations
  evaluation
arxiv_id: '2404.10387'
source_url: https://arxiv.org/abs/2404.10387
tags:
- explanations
- explanation
- ensembling
- metrics
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CNN-based explanation ensembling to evaluate
  dataset, representation, and explanations in deep learning models. The method trains
  a CNN to combine multiple explanation methods (e.g., Grad-CAM, SHAP) into a single
  aggregated explanation, aiming to improve interpretability and reduce noise.
---

# CNN-based explanation ensembling for dataset, representation and explanations evaluation

## Quick Facts
- arXiv ID: 2404.10387
- Source URL: https://arxiv.org/abs/2404.10387
- Reference count: 40
- This paper proposes CNN-based explanation ensembling to evaluate dataset, representation, and explanations in deep learning models.

## Executive Summary
This paper introduces a novel approach for combining multiple explanation methods (Grad-CAM, SHAP, etc.) using a CNN architecture to produce aggregated explanations. The method aims to improve interpretability, evaluate representation quality, and detect dataset issues like under-representation. Through experiments on ImageNet-S50, the authors demonstrate that CNN-based ensembling outperforms individual explanations in localization and faithfulness metrics, while also offering potential benefits like sensitive information reduction.

## Method Summary
The proposed XAI Ensembler architecture trains a CNN to combine multiple explanation methods into a single aggregated explanation. The model processes each explanation through separate convolutional encoders, then combines them via concatenation or summation before producing a final explanation through segmentation layers. The training objective minimizes a SoftDice loss between the ensembled explanation and ground-truth object masks. The approach is evaluated using metrics like ensembling performance (ens(f1), ens(iou)), diverseness (div(iou)), and exhaustiveness (exh(iou)) on ImageNet-S50.

## Key Results
- ResNet50 representation learned the best object localization compared to MobileNet v2 and other architectures
- CNN-based explanation ensembling outperformed individual explanations in both localization (ens(f1), ens(iou)) and faithfulness metrics
- The method demonstrated potential for reducing sensitive information in images by replacing original images with their explanations
- Different ensembling architectures (concatenation vs. summation) showed varying performance, with concatenation generally performing better

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining explanations using a CNN allows the model to learn spatial relationships among explanation methods that a simple weighted sum cannot capture.
- Mechanism: The XAI Ensembler architecture uses convolutional layers to process each explanation channel independently, then combines them through concatenation or summation. This preserves spatial locality and enables the model to learn where explanations agree or conflict.
- Core assumption: Explanation heatmaps contain complementary spatial information that can be integrated via learned convolutions.
- Evidence anchors:
  - [abstract] "With our method, we can uncover problems of under-representation of images in a certain class."
  - [section] "The first architecture, we used concatenation as the method for combining inputs from multiple encoders. Concatenation involves stacking encoders’ outputs along a new channel dimension."
  - [corpus] No direct corpus evidence; the method appears novel and not well covered in related works.
- Break condition: If explanations are largely redundant or noise-dominated, concatenation adds no value and may even degrade performance.

### Mechanism 2
- Claim: The segmentation loss (SoftDice) drives the ensembled explanation to align closely with ground-truth object masks, making it a proxy for evaluating representation quality.
- Mechanism: By minimizing the segmentation loss between predicted ensembled explanations and annotated masks, the XAI Ensembler is forced to extract the most salient features for object localization, implicitly evaluating how well the underlying classification model represents that object.
- Core assumption: A well-learned representation will produce explanations that, when combined, segment the object accurately.
- Evidence anchors:
  - [abstract] "Through the use of carefully selected evaluation metrics from the Quantus library, we demonstrated the method’s superior performance in terms of Localisation and Faithfulness, compared to individual explanations."
  - [section] "The training procedure involves minimizing the segmentation loss Lseg as follows: Lseg = sum(SoftDice(Mi, XAI_Ensembler(Ei)))"
  - [corpus] No corpus evidence; segmentation-based evaluation of explanations is not standard practice.
- Break condition: If the ground-truth masks are noisy or inconsistent, the segmentation loss may reward explanations that fit the mask but do not reflect true model behavior.

### Mechanism 3
- Claim: The exhaustiveness metric (exh(iou)) quantifies how much of the object’s features are captured by explanations versus the original image, enabling evaluation of explanation quality.
- Mechanism: exh(iou) compares the IoU of the ensembled explanation mask to the IoU of the original image mask, revealing how effectively explanations isolate the object of interest.
- Core assumption: Good explanations should localize the object as well as or better than the raw image.
- Evidence anchors:
  - [abstract] "we discuss other side benefits like features’ reduction by replacing the original image with its explanations resulting in the removal of some sensitive information."
  - [section] "The explanation-oriented exhaustiveness (exh) metric helps to assess the completeness of an explanation relative to an entire input image..."
  - [corpus] No corpus evidence; exhaustiveness as defined here is a novel metric.
- Break condition: If explanations focus on irrelevant or spurious features, exh(iou) will be low even if the underlying model is accurate.

## Foundational Learning

- Concept: Soft Dice Loss
  - Why needed here: Used as the segmentation loss to train the XAI Ensembler; handles class imbalance between object and background.
  - Quick check question: Why is Soft Dice preferred over cross-entropy for segmentation in this context?

- Concept: Explanation Ensembling
  - Why needed here: Combines multiple explanation methods to improve robustness and reduce noise in individual explanations.
  - Quick check question: How does ensembling multiple explanation methods improve localization performance compared to individual methods?

- Concept: Exhaustiveness Metric
  - Why needed here: Evaluates how completely explanations capture the relevant object features compared to the original image.
  - Quick check question: What does it mean if the exhaustiveness metric shows explanations capture less of the object than the original image?

- Concept: Segmentation-based Evaluation
  - Why needed here: Uses ground-truth masks to train and evaluate the quality of ensembled explanations.
  - Quick check question: How reliable is segmentation-based evaluation when ground-truth masks may contain annotation errors?

## Architecture Onboarding

### Component Map
Explanation methods (Grad-CAM, SHAP, etc.) -> Individual CNNs (encoders) -> Concatenation/Summation layer -> Final CNN (decoder) -> Ensembled explanation

### Critical Path
Multiple explanation inputs → Individual convolutional encoders → Feature combination (concatenation/sum) → Final convolutional decoder → Segmentation output with SoftDice loss

### Design Tradeoffs
- Concatenation preserves all information but increases dimensionality vs. summation which reduces dimensions but may lose information
- Multiple encoders add complexity but allow learning method-specific features vs. shared encoder which is simpler but less expressive
- SoftDice loss handles class imbalance but may be less sensitive to boundary precision than other segmentation losses

### Failure Signatures
- Poor performance on localization metrics suggests encoders aren't learning complementary features
- High exhaustiveness but low faithfulness indicates explanations capture object location but not model decision process
- Similar performance to baseline ensembling suggests CNN architecture isn't adding value beyond simple combination

### First 3 Experiments to Run
1. Compare CNN-based ensembling against simple weighted average baseline on ImageNet-S50 localization metrics
2. Ablation study removing individual explanation methods to identify most valuable contributors
3. Cross-dataset validation testing ensembled explanations on datasets with known representation biases

## Open Questions the Paper Calls Out
No specific open questions were called out in the paper.

## Limitations
- Relies heavily on proxy metrics (IoU with ground-truth masks) rather than direct measures of explanation faithfulness to model behavior
- Claims about detecting under-representation are mentioned but not empirically demonstrated with specific examples
- Segmentation-based evaluation may reward explanations that fit masks but don't reflect true model behavior if ground-truth annotations are noisy

## Confidence

- **Medium Confidence**: The claim that CNN-based ensembling improves localization metrics (ens(f1), ens(iou)) compared to individual explanations. While the paper shows quantitative improvements, these metrics measure agreement with ground-truth masks rather than actual model explanation quality.
- **Low Confidence**: The assertion that the method can "uncover problems of under-representation" in datasets. This claim is mentioned but not empirically demonstrated with specific examples of discovered dataset biases or underrepresented classes.
- **Medium Confidence**: The exhaustiveness metric (exh(iou)) provides meaningful evaluation of explanation quality. The metric is conceptually sound, but its interpretation depends heavily on the quality and consistency of ground-truth annotations.

## Next Checks

1. **Baseline Comparison**: Implement and compare against simple ensembling baselines (weighted average, max-pooling) to isolate whether the CNN architecture provides benefits beyond basic combination strategies.

2. **Faithfulness Validation**: Conduct pixel-flipping experiments on the ensembled explanations themselves to verify they faithfully represent the model's decision-making process, not just object localization.

3. **Cross-Dataset Generalization**: Test the ensembling approach on datasets with known representation biases (e.g., ImageNet variants with documented class imbalances) to empirically validate the claim about detecting under-representation.