---
ver: rpa2
title: Better Synthetic Data by Retrieving and Transforming Existing Datasets
arxiv_id: '2404.14361'
source_url: https://arxiv.org/abs/2404.14361
tags:
- dataset
- task
- data
- datasets
- datatune
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DataTune is a method for improving synthetic dataset generation
  by retrieving and transforming existing datasets to better match target tasks. It
  uses a dual-stage retrieval approach to find relevant datasets, then applies a planning
  and execution framework to transform them into task-specific formats.
---

# Better Synthetic Data by Retrieving and Transforming Existing Datasets

## Quick Facts
- arXiv ID: 2404.14361
- Source URL: https://arxiv.org/abs/2404.14361
- Reference count: 18
- Primary result: 49% improvement over few-shot prompting and 34% over existing synthetic methods

## Executive Summary
DataTune is a method for improving synthetic dataset generation by retrieving and transforming existing datasets to better match target tasks. It uses a dual-stage retrieval approach to find relevant datasets, then applies a planning and execution framework to transform them into task-specific formats. On 6 BIG-Bench tasks, DataTune improves over few-shot prompting by 49% and over existing synthetic/retrieved data methods by 34%. It produces more diverse, harder, and higher-quality examples than direct synthetic generation, with minimal accuracy loss. When combined with synthetic generation, it yields the best results, demonstrating its value for few-shot fine-tuning of language models.

## Method Summary
DataTune employs a dual-stage retrieval approach: first using a bi-encoder retriever (DataFinder) to select candidate datasets from over 75,000 options, then reranking them with an LLM-based scorer. The transformation process involves four modules: Task Expansion (using LLM to expand brief task descriptions), Schema Selection (filtering irrelevant columns), Planning (generating transformation plans), and Execution (implementing plans via LLM). The method generates up to 3000 examples per task by transforming multiple datasets, which are then used to fine-tune Mistral-7B via QLoRA over 3 epochs.

## Key Results
- 49% improvement over few-shot prompting baseline on 6 BIG-Bench tasks
- 34% improvement over existing synthetic/retrieved data methods
- Creates more diverse, harder, and higher-quality examples with minimal accuracy loss
- Best performance when combined with synthetic generation (8-point improvement over few-shot baseline)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dataset transformation increases lexical diversity compared to direct synthetic generation.
- **Mechanism**: By retrieving and transforming existing datasets, the system introduces new vocabulary and phrasing patterns from the source dataset into the target task format.
- **Core assumption**: Existing datasets contain diverse language patterns not present in synthetic generation from scratch.
- **Evidence anchors**:
  - [abstract]: "We find that dataset transformation significantly increases the diversity and difficulty of generated data on many tasks."
  - [section 5.2]: "Using ROUGE-L to determine lexical uniqueness... we observe that over 50% of examples generated synthetically are near-duplicates for 3 of 5 tasks; in each of those cases, using DataTune instead effectively eliminates this problem."
  - [corpus]: Weak evidence - corpus shows related papers on synthetic data generation but doesn't directly support diversity claims.
- **Break condition**: If retrieved datasets are too similar to each other or lack diversity themselves, transformation won't increase overall diversity.

### Mechanism 2
- **Claim**: Dataset transformation produces more difficult examples than synthetic generation.
- **Mechanism**: Existing datasets often contain nuanced, complex examples that require deeper understanding, which when transformed retain this complexity while matching task format.
- **Core assumption**: Synthetic generation from scratch tends toward easier examples with higher prior probability under the language model.
- **Evidence anchors**:
  - [abstract]: "We find that dataset transformation significantly increases... difficulty of generated data on many tasks."
  - [section 5.3]: "we observe that the data from DataTune exhibits a higher level of difficulty compared to synthetically generated data for all datasets other than Medical Questions in Russian."
  - [corpus]: Weak evidence - corpus mentions synthetic datasets but doesn't specifically address difficulty levels.
- **Break condition**: If the transformation process oversimplifies the examples or if the source dataset is too simple.

### Mechanism 3
- **Claim**: Transformation and synthetic generation are complementary approaches that cover different regions of the task space.
- **Mechanism**: Dataset transformation introduces knowledge from existing datasets while synthetic generation explores new combinations, together providing broader coverage.
- **Core assumption**: The two methods systematically sample from different distributions of the target task space.
- **Evidence anchors**:
  - [section 5.5]: "we observe that examples generated via DataTune and Synthetic Generation fall into visually well-separated regions of embedding space, after projecting to two dimensions via t-SNE."
  - [abstract]: "When used in conjunction with existing synthetic dataset generation methods, it achieve superior performance compared to other few-shot learning methods"
  - [corpus]: Weak evidence - corpus shows related work on synthetic data but doesn't directly support complementarity claims.
- **Break condition**: If the methods start overlapping significantly or if one method dominates the combined space.

## Foundational Learning

- **Concept**: Dual-stage retrieval (bi-encoder + reranker)
  - Why needed here: Efficient retrieval from 75,000+ datasets requires balancing speed and accuracy
  - Quick check question: What's the difference between a bi-encoder retriever and a cross-encoder reranker?

- **Concept**: Task expansion through LLM analysis
  - Why needed here: Brief task descriptions are insufficient for complex transformation planning
  - Quick check question: How does task expansion help the planning module generate better transformation plans?

- **Concept**: Schema selection for dataset optimization
  - Why needed here: Not all columns in a retrieved dataset are relevant to the target task
  - Quick check question: Why is it important to filter out irrelevant columns before dataset transformation?

## Architecture Onboarding

- **Component map**:
  - DataFinder (bi-encoder retriever) → Reranker (LLM-based) → Schema Selector → Task Expander → Planning Module → Execution Module

- **Critical path**: Task Description → DataFinder → Reranker → Schema Selector → Task Expander → Planning Module → Execution Module → Transformed Dataset
  - The LLM queries in Planning and Execution are the most time-consuming steps

- **Design tradeoffs**:
  - Using GPT-3.5 for execution vs GPT-4 for planning: cost vs capability
  - Transforming multiple datasets vs deeper transformation of fewer datasets: quantity vs quality
  - LLM-based planning vs rule-based planning: flexibility vs predictability

- **Failure signatures**:
  - Planning module produces vague or incorrect plans → check task expansion quality
  - Execution module fails to follow plans → check plan clarity or execution model capability
  - Low diversity in output → check retrieved dataset diversity or transformation parameters

- **First 3 experiments**:
  1. Run with a simple task and a known good dataset to verify end-to-end functionality
  2. Test schema selection on a dataset with obvious irrelevant columns
  3. Compare diversity metrics between synthetic generation and transformation on the same task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific limitations of the LLM query cost in DataTune, and how can they be mitigated?
- Basis in paper: [explicit] The paper mentions that the dataset transformation component requires running the Execution Module on each row of each dataset, which can be cost-prohibitive for very large datasets.
- Why unresolved: The paper identifies this as a limitation but does not provide specific details on how to mitigate it.
- What evidence would resolve it: Detailed analysis of the computational costs involved and potential solutions, such as batching or parallel processing.

### Open Question 2
- Question: How does the dependence on the Planning Module affect the reliability of DataTune, and what are the potential failure modes?
- Basis in paper: [explicit] The paper states that the Planning Module's effectiveness depends on the LLM following the prompt correctly, and provides an example where it failed to translate generated question-answer pairs back to Russian.
- Why unresolved: The paper identifies this as a limitation but does not explore the broader implications or potential failure modes in detail.
- What evidence would resolve it: Case studies or experiments showing the frequency and impact of Planning Module failures across different tasks and languages.

### Open Question 3
- Question: How can DataTune be adapted to handle non-English data more effectively, and what are the specific challenges involved?
- Basis in paper: [explicit] The paper mentions that the transformation agent's ability to process non-English data tasks is compromised, often altering example tasks instead of the actual data.
- Why unresolved: The paper identifies this as a limitation but does not provide solutions or detailed analysis of the challenges involved.
- What evidence would resolve it: Comparative studies of DataTune's performance on English vs. non-English tasks, along with proposed modifications to improve non-English data handling.

## Limitations

- High computational costs due to LLM reranking and transformation modules requiring substantial compute resources
- Performance heavily depends on availability of relevant datasets in the HuggingFace collection
- Reliance on English-trained models creates potential bias issues for non-English tasks

## Confidence

- **High confidence**: The 49% improvement over few-shot prompting and 34% improvement over existing synthetic methods (supported by BIG-Bench evaluation across 6 tasks)
- **Medium confidence**: Claims about increased diversity and difficulty (supported by ROUGE-L and GPT-4 difficulty metrics, but based on relatively small sample sizes)
- **Medium confidence**: Complementarity between synthetic generation and transformation methods (supported by t-SNE visualization, but requires further validation with larger task sets)

## Next Checks

1. **Dataset Availability Test**: Systematically evaluate the method's performance across a diverse set of tasks where relevant datasets are known to exist versus tasks where no suitable datasets are available, to quantify the dependency on dataset availability.

2. **Computational Cost Analysis**: Measure and compare the actual compute costs (both LLM API calls and fine-tuning time) of DataTune versus baseline synthetic generation methods across multiple tasks to assess practical feasibility.

3. **Cross-Lingual Performance**: Test the transformation approach on a diverse set of non-English tasks to validate the claims about performance in languages other than English and identify potential limitations of the English-centric approach.