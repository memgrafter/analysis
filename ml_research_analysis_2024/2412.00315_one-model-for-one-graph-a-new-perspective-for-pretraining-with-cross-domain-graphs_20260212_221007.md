---
ver: rpa2
title: 'One Model for One Graph: A New Perspective for Pretraining with Cross-domain
  Graphs'
arxiv_id: '2412.00315'
source_url: https://arxiv.org/abs/2412.00315
tags:
- graph
- source
- graphs
- test
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generalizing graph neural
  networks (GNNs) across diverse domains, where existing methods require domain-specific
  architectures and training from scratch. To overcome this, the authors propose "one
  model for one graph" (OMOG), a novel cross-domain pretraining framework that pretrains
  a separate expert model for each dataset, then adaptively fuses them using scoring
  functions during inference.
---

# One Model for One Graph: A New Perspective for Pretraining with Cross-domain Graphs

## Quick Facts
- arXiv ID: 2412.00315
- Source URL: https://arxiv.org/abs/2412.00315
- Reference count: 40
- One-line primary result: OMOG achieves up to 9% higher accuracy and 20% improvement in node classification compared to mixture-of-experts methods

## Executive Summary
This paper addresses the challenge of generalizing graph neural networks (GNNs) across diverse domains, where existing methods require domain-specific architectures and training from scratch. The authors propose "one model for one graph" (OMOG), a novel cross-domain pretraining framework that pretrains a separate expert model for each dataset, then adaptively fuses them using scoring functions during inference. This approach mitigates negative transfer and heterogeneity issues common in single-model pretraining. Extensive experiments on node classification and link prediction tasks show that OMOG consistently outperforms state-of-the-art baselines, achieving up to 9% higher accuracy and 20% improvement in node classification compared to mixture-of-experts methods. Theoretical analysis further demonstrates that OMOG's fusion strategy is equivalent to Bayesian Model Averaging, ensuring optimal performance. The framework also offers computational efficiency and scalability for incorporating new datasets.

## Method Summary
OMOG addresses cross-domain graph pretraining by training one expert model per dataset rather than a single joint model. The framework consists of separate source models (trained on individual graphs), scoring modules (to assess domain relevance), and a fusion mechanism (to combine selected models). During pretraining, each source model and its corresponding scoring module are trained independently on their respective graph. At inference, the scoring modules compute relevance scores between the target graph and all source models, the top-K most relevant models are selected, and their parameters are fused via softmax-weighted averaging to create a graph-specific model. The method uses SentenceBERT for text attribute encoding, SGC for structural embeddings, and nearest neighbor retrieval for final predictions.

## Key Results
- OMOG outperforms all baselines on node classification and link prediction tasks across 10 datasets
- Achieves up to 9% higher accuracy compared to mixture-of-experts methods
- Demonstrates 20% improvement in node classification performance
- Shows effectiveness for both zero-shot and few-shot learning scenarios
- Theoretical analysis proves equivalence to Bayesian Model Averaging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining separate expert models for each graph domain prevents negative transfer during pretraining
- Mechanism: By training one model per graph, the framework avoids structural and feature heterogeneity conflicts that occur when jointly training on diverse graphs
- Core assumption: Graph domains have sufficiently distinct structural properties that joint training causes interference
- Evidence anchors:
  - [abstract] "overcomes the limitations of previous approaches that failed to use a single GNN to capture diverse graph patterns across domains with significant gaps"
  - [section] "pre-training a single model across graphs with different structural properties leads to negative transfer, primarily due to the conflicts across graphs from diverse domains"

### Mechanism 2
- Claim: Adaptive fusion of source models during inference creates graph-specific models that outperform individual experts
- Mechanism: Scoring modules select relevant source models and weight their contributions based on domain relevance, creating a fused model optimized for the target graph
- Core assumption: Relevance scores accurately reflect which source models will generalize best to a target graph
- Evidence anchors:
  - [abstract] "gating functions choose a subset of experts to effectively integrate prior model knowledge while avoiding negative transfer"
  - [section] "we design a scoring module to decide the relevance between the corresponding source model in the bank and downstream datasets during the inference stage"

### Mechanism 3
- Claim: The fusion strategy is theoretically equivalent to Bayesian Model Averaging, ensuring optimal performance
- Mechanism: The weighted averaging of source models using softmax over relevance scores approximates BMA weights based on predictive variance
- Core assumption: Predictive distributions can be approximated as Gaussian with variance inversely related to relevance scores
- Evidence anchors:
  - [section] "OMOG's merging strategy is theoretically equivalent to Bayesian Model Averaging"
  - [section] "OMOG's Top-K selection and merging guarantee that the fused model outperforms or matches the best individual source model in expectation"

## Foundational Learning

- Concept: Graph Neural Networks and their limitations with heterogeneous graphs
  - Why needed here: Understanding why standard GNNs fail across domains is crucial for grasping the motivation behind OMOG
  - Quick check question: Why do standard GNNs require domain-specific architecture designs?

- Concept: Cross-domain pretraining and transfer learning
  - Why needed here: The framework builds on transfer learning principles but addresses unique challenges in graph domains
  - Quick check question: What distinguishes cross-graph pretraining from standard pretraining?

- Concept: Mixture-of-experts architecture and adaptive model selection
  - Why needed here: OMOG's core innovation involves selecting and fusing multiple expert models based on target graph characteristics
  - Quick check question: How does OMOG's adaptive selection differ from fixed mixture-of-experts approaches?

## Architecture Onboarding

- Component map:
  - Text attributes -> SentenceBERT encoder -> Unified text embeddings
  - Graph adjacency matrix -> SGC layers -> Structural embeddings
  - Source models (one per dataset) -> Pretrained separately
  - Scoring modules (one per source model) -> Pretrained on same dataset
  - Relevance scores -> Top-K selection -> Weighted fusion -> Graph-specific model

- Critical path:
  1. Pretrain source models on each graph separately
  2. Pretrain scoring modules to assess domain relevance
  3. For inference, use scoring modules to select and weight source models
  4. Fuse selected models to create graph-specific model

- Design tradeoffs:
  - Separate training increases computation but prevents negative transfer
  - Fixed number of source models balances performance vs. efficiency
  - Top-K selection vs. all models: reduces computation but may miss useful information

- Failure signatures:
  - Poor performance when scoring modules fail to identify relevant source models
  - Degraded results when structural properties of target graph are too dissimilar from all pretraining graphs
  - Computational inefficiency when too many source models are selected

- First 3 experiments:
  1. Ablation study: Remove scoring mechanism to verify its importance in model selection
  2. Test with varying number of selected source models (K) to find optimal tradeoff
  3. Compare against joint training baseline to demonstrate negative transfer prevention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OMOG scale with the number of source models in the bank, and what is the optimal number of models to include?
- Basis in paper: [inferred] The paper mentions that performance improves with more source models but also warns of potential negative transfer, suggesting a need to determine the optimal number.
- Why unresolved: The paper does not provide specific guidance on the optimal number of source models, leaving this as an empirical question that depends on the specific datasets and tasks.
- What evidence would resolve it: Experimental results showing performance curves as the number of source models increases, identifying the point of diminishing returns or performance degradation.

### Open Question 2
- Question: Can OMOG be effectively extended to support cross-task learning across graphs, beyond just node classification and link prediction?
- Basis in paper: [explicit] The paper mentions that OMOG can potentially be combined with existing work on cross-task learning across graphs, but does not explore this extension.
- Why unresolved: The paper focuses on node classification and link prediction tasks, leaving the exploration of other tasks as future work.
- What evidence would resolve it: Experimental results demonstrating OMOG's effectiveness on additional tasks such as graph classification or graph generation.

### Open Question 3
- Question: How does OMOG's performance compare to other cross-graph learning methods when the number of classes in the target dataset is very large or very small?
- Basis in paper: [inferred] The paper notes that OMOG performs particularly well on datasets with complicated label spaces, but does not provide specific results for extreme cases.
- Why unresolved: The paper only tests on datasets with a moderate number of classes, leaving the performance on extreme cases unexplored.
- What evidence would resolve it: Experimental results comparing OMOG's performance on datasets with a very large or very small number of classes to other methods.

## Limitations
- The theoretical equivalence to Bayesian Model Averaging relies on Gaussian approximations of predictive distributions, which may not hold in practice for complex graph patterns
- Limited ablation studies on optimal K selection and scoring module architecture
- Training source models separately increases computational cost significantly compared to joint training approaches

## Confidence
- **High Confidence**: The empirical results showing OMOG outperforms baselines on node classification and link prediction tasks
- **Medium Confidence**: The theoretical analysis claiming equivalence to Bayesian Model Averaging, as it depends on distributional assumptions
- **Medium Confidence**: The mechanism preventing negative transfer through separate training, though well-motivated, requires more extensive ablation studies

## Next Checks
1. **Ablation on K selection**: Systematically vary the number of selected source models (K) from 1 to all available models to identify optimal tradeoff between performance and computational efficiency

2. **Robustness to scoring module failure**: Test OMOG performance when scoring modules are replaced with random selection or removed entirely to quantify their contribution to success

3. **Cross-validation of theoretical claims**: Compare OMOG's empirical performance against theoretically optimal Bayesian Model Averaging implementations to validate the claimed equivalence under real-world conditions