---
ver: rpa2
title: 'FETCH: A Memory-Efficient Replay Approach for Continual Learning in Image
  Classification'
arxiv_id: '2407.12375'
source_url: https://arxiv.org/abs/2407.12375
tags:
- memory
- encoder
- learning
- fetch
- replay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses catastrophic forgetting in continual learning
  by combining GDumb''s replay approach with compression techniques to reduce memory
  footprint. The proposed FETCH method uses a two-stage compression: first encoding
  images using fixed early layers of a pre-trained network, then applying additional
  compression (quantization, thinning, or autoencoding) before storing in episodic
  memory.'
---

# FETCH: A Memory-Efficient Replay Approach for Continual Learning in Image Classification

## Quick Facts
- arXiv ID: 2407.12375
- Source URL: https://arxiv.org/abs/2407.12375
- Authors: Markus Weißflog; Peter Protzel; Peer Neubert
- Reference count: 29
- Primary result: Combines GDumb's replay approach with compression techniques to reduce memory footprint in continual learning

## Executive Summary
FETCH addresses catastrophic forgetting in continual learning by combining GDumb's replay approach with compression techniques to reduce memory footprint. The proposed method uses a two-stage compression: first encoding images using fixed early layers of a pre-trained network, then applying additional compression (quantization, thinning, or autoencoding) before storing in episodic memory. Evaluation on CIFAR10/100 shows that simple compression methods like quantization outperform complex autoencoders. Using a fixed encoder consistently improves performance over GDumb while achieving competitive accuracy with significantly reduced memory consumption, making it suitable for memory-constrained scenarios.

## Method Summary
FETCH is a continual learning method that addresses catastrophic forgetting by combining a fixed encoder with compressed replay. The approach uses pre-trained early layers of a ResNet as a fixed encoder to extract features, then applies compression (quantization, thinning, or autoencoding) to reduce storage requirements. The compressed exemplars are stored in episodic memory and used for replay during training. Only the classification head is trained, while the fixed encoder remains unchanged. The method evaluates on CIFAR10/100 with varying memory budgets, comparing against GDumb and baseline compressed replay methods using classification accuracy as the primary metric.

## Key Results
- Simple compression methods (quantization) outperform complex autoencoders in both accuracy and efficiency
- Using a fixed encoder consistently improves performance over GDumb across different compression methods
- The approach achieves competitive accuracy while significantly reducing memory consumption, making it suitable for memory-constrained scenarios
- Splitting ResNets at later layers appears beneficial, with higher performance when using deeper encoder splits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fixed encoder + compressed replay reduces catastrophic forgetting while lowering memory footprint
- Mechanism: Pre-trained fixed encoder extracts general features; compressor reduces exemplar storage size; classification head trained only on compressed exemplars
- Core assumption: Fixed encoder preserves task-relevant information while reducing parameters needing updates
- Evidence anchors:
  - [abstract] "The proposed FETCH method uses a two-stage compression: first encoding images using fixed early layers of a pre-trained network, then applying additional compression"
  - [section 4.1] "First, an encoding model, called fixed encoder in this work, converts the image to a latent representation z"
  - [corpus] Weak - neighbors discuss compression but not fixed encoder approach
- Break condition: If fixed encoder fails to preserve discriminative features across tasks

### Mechanism 2
- Claim: Simple compression methods (quantization) outperform complex autoencoders
- Mechanism: Quantization reduces tensor precision while maintaining sufficient discriminative information; simpler operations preserve more signal than complex transformations
- Core assumption: Information loss from quantization is less detrimental than information loss from autoencoder reconstruction
- Evidence anchors:
  - [abstract] "In our experiments, simple compression methods (e.g., quantization of tensors) outperform deep autoencoders"
  - [section 4.2] "For all experiments, TinyImagenet [14] was used as a pre-training dataset" for quantization intervals
  - [section 6.3] "The quantitization strategy approaches baseline performance, given a sufficiently high compression parameter"
- Break condition: When data distribution requires high-precision representations

### Mechanism 3
- Claim: Pre-training on different dataset enables knowledge transfer and improves performance
- Mechanism: Fixed encoder trained on large dataset (ImageNet) captures general visual features transferable to new tasks; reduces training complexity
- Core assumption: Visual features learned on large dataset are general enough to benefit task-specific classification
- Evidence anchors:
  - [abstract] "Using a fixed encoder consistently improves performance over GDumb"
  - [section 4.1] "To adhere to the paradigm of CL, we use different datasets for the pre-training of the encoder (called pre-training dataset)"
  - [section 6.1] "Splitting ResNets at later layers appears beneficial, as shown by the fact that the performance is consistently higher"
- Break condition: When pre-training dataset domain differs substantially from target tasks

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Core problem FETCH addresses; understanding why replay methods work
  - Quick check question: Why does training on new tasks cause loss of performance on previous tasks?

- Concept: Continual learning settings (online, class-incremental)
  - Why needed here: Defines problem formulation and evaluation context
  - Quick check question: What distinguishes online from offline continual learning?

- Concept: Feature extraction vs classification in CNNs
  - Why needed here: Explains why freezing early layers works and how encoder/classifier split functions
  - Quick check question: What types of features do early CNN layers typically extract?

## Architecture Onboarding

- Component map:
  Fixed encoder (pre-trained ResNet early layers) -> Compressor (quantization/thinning/autoencoder) -> Episodic memory -> Decompressor -> Classification head (remaining ResNet layers)

- Critical path:
  Data → Fixed encoder → Compressor → Memory → Decompressor → Classification head training → Inference

- Design tradeoffs:
  - Fixed encoder depth vs parameter count vs feature quality
  - Compression ratio vs information retention
  - Memory budget vs exemplar quantity vs quality
  - Pre-training dataset choice vs task domain alignment

- Failure signatures:
  - Poor accuracy → check if encoder preserves discriminative features
  - Memory bloat → verify compression actually reduces storage
  - Training instability → check if decompressed data matches expected format
  - Catastrophic forgetting → verify replay memory is being utilized properly

- First 3 experiments:
  1. Compare accuracy with/without fixed encoder on CIFAR10 with quantization compression
  2. Vary quantization levels (k=2,4,8,16) to find optimal compression/quality tradeoff
  3. Test different encoder split points (conv2_x, conv3_x, conv4_x) to find optimal feature extraction depth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FETCH compare to other continual learning approaches in domains beyond image classification?
- Basis in paper: [explicit] The paper states that FETCH could serve as a baseline for future research in memory-constrained CL, but does not explore other domains.
- Why unresolved: The experiments only evaluate FETCH on CIFAR10 and CIFAR100 datasets, leaving its performance in other domains unexplored.
- What evidence would resolve it: Testing FETCH on other types of data such as audio, text, or video to determine its generalizability and performance across different domains.

### Open Question 2
- Question: What is the impact of different pre-training datasets on the performance of FETCH?
- Basis in paper: [explicit] The paper mentions using TinyImageNet as a pre-training dataset and suggests that the diversity of the pre-training dataset influences performance, but does not extensively explore different pre-training datasets.
- Why unresolved: The study only uses TinyImageNet for pre-training and does not compare the effects of using other datasets.
- What evidence would resolve it: Conducting experiments with various pre-training datasets to assess how they affect the performance and efficiency of FETCH.

### Open Question 3
- Question: How does the choice of compression method affect the trade-off between storage efficiency and model performance in different memory-constrained scenarios?
- Basis in paper: [explicit] The paper evaluates different compression methods (quantization, thinning, autoencoding) and their impact on performance and storage, but does not explore how these methods perform under varying memory constraints.
- Why unresolved: The experiments focus on a fixed memory budget and do not vary memory constraints to see how different compression methods adapt.
- What evidence would resolve it: Testing FETCH with different compression methods under varying memory constraints to determine the optimal compression strategy for each scenario.

## Limitations

- Evaluation limited to CIFAR10/100 datasets, leaving performance on more complex or diverse image datasets unclear
- Computational overhead of compression/decompression operations during training and inference is not thoroughly analyzed
- No comparison with state-of-the-art regularization-based methods (EWC, SI) to establish relative effectiveness beyond replay approaches

## Confidence

- Fixed encoder approach consistently improves performance: High
- Simple compression methods outperform complex autoencoders: Medium-High
- Memory efficiency claims: High (based on storage calculations)
- Generalizability to non-CIFAR datasets: Low-Medium

## Next Checks

1. Test FETCH performance on TinyImageNet or other complex datasets to validate scalability
2. Measure actual inference latency with different compression methods to quantify computational overhead
3. Compare with state-of-the-art regularization-based methods (EWC, SI) to establish relative effectiveness beyond replay approaches