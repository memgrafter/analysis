---
ver: rpa2
title: Data-Driven Stochastic Closure Modeling via Conditional Diffusion Model and
  Neural Operator
arxiv_id: '2408.02965'
source_url: https://arxiv.org/abs/2408.02965
tags:
- diffusion
- sparse
- closure
- generated
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a data-driven framework for constructing stochastic
  closure models using conditional diffusion models and neural operators. The approach
  addresses the challenge of modeling complex dynamical systems like turbulence, where
  deterministic local closure models lack sufficient generalization due to absent
  scale separation.
---

# Data-Driven Stochastic Closure Modeling via Conditional Diffusion Model and Neural Operator

## Quick Facts
- arXiv ID: 2408.02965
- Source URL: https://arxiv.org/abs/2408.02965
- Authors: Xinghao Dong; Chuanqi Chen; Jin-Long Wu
- Reference count: 40
- Primary result: Conditional diffusion models with FNO-based score functions reduce closure modeling errors from ~60% to ~3-8% while achieving 100× speedup and resolution invariance

## Executive Summary
This paper presents a data-driven framework for constructing stochastic closure models using conditional diffusion models and neural operators. The approach addresses the challenge of modeling complex dynamical systems like turbulence, where deterministic local closure models lack sufficient generalization due to absent scale separation. The method employs a Fourier neural operator to construct the score function in a score-based generative diffusion model, enabling both continuous spatiotemporal field modeling and non-local closure modeling.

## Method Summary
The framework uses Fourier neural operators (FNOs) to construct score functions for a score-based diffusion model that learns the conditional distribution of closure terms given sparse observations and resolved system states. The model is trained on high-fidelity Navier-Stokes data, with FNOs enabling resolution invariance and capturing non-local dependencies. Accelerated sampling techniques achieve 100× speedup while maintaining accuracy, and the framework is demonstrated on both linear viscous diffusion and nonlinear convection terms.

## Key Results
- Conditional diffusion models significantly improve accuracy when incorporating sparse observations, reducing relative errors from ~60% to ~3-8%
- The model demonstrates resolution invariance, maintaining performance across different spatial resolutions (64×64, 128×128, 256×256)
- Accelerated sampling techniques achieve 100× speedup while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional diffusion models significantly improve accuracy when incorporating sparse observations of closure terms
- Mechanism: The score-based diffusion model learns to approximate the conditional distribution p(U|y) where y includes sparse observations of true closure terms. By conditioning on this information, the model avoids needing to accurately train the score function across the entire high-dimensional probability space and instead focuses on regimes highly correlated with the incorporated sparse information.
- Core assumption: The sparse observations contain sufficient information to guide the generation process toward the true closure term distribution
- Evidence anchors:
  - [abstract] states "conditional diffusion models significantly improve accuracy when incorporating sparse observations of closure terms, reducing relative errors from ~60% to ~3-8%"
  - [section] shows error reduction from 5.9448 × 10−1 to 3.7137 × 10−2 when incorporating bicubically interpolated sparse observations
  - [corpus] has weak evidence - no directly relevant papers found on this specific mechanism
- Break condition: If sparse observations are too sparse or uncorrelated with the true closure terms, the conditioning information becomes ineffective

### Mechanism 2
- Claim: Fourier neural operators enable resolution invariance and non-local closure modeling
- Mechanism: FNOs approximate operators by projecting functions onto a global Fourier basis, where each Fourier mode inherently aggregates information from the entire domain. This global representation ensures that local perturbations in the resolved system state influence its spectral coefficients, allowing the model to capture long-range interactions and complex dynamics associated with advection processes.
- Core assumption: The closure term exhibits non-local dependencies that can be captured by global Fourier basis representations
- Evidence anchors:
  - [abstract] states "Fourier neural operator is incorporated into a score-based diffusion model" enabling "resolution invariance" and "non-local closure modeling"
  - [section] demonstrates resolution invariance testing across 64×64, 128×128, and 256×256 resolutions with consistent performance
  - [corpus] has weak evidence - no directly relevant papers found on this specific mechanism
- Break condition: If the closure term's non-local dependencies cannot be adequately represented in the Fourier basis, or if resolution invariance is compromised by insufficient training data

### Mechanism 3
- Claim: Accelerated sampling techniques enable efficient deployment as stochastic data-driven closure terms
- Mechanism: The paper employs adaptive time-stepping based on a scheduling function that monotonically decreases step sizes as simulation progresses from τ = T to τ = 0, combined with starting from a prior distribution closer to the ground truth (smaller T) when conditional inputs are available. This achieves up to 100× speedup while maintaining accuracy.
- Core assumption: The quality of generated samples is not sensitive to errors at earlier stages when noise levels are high, but accuracy becomes crucial as noise diminishes
- Evidence anchors:
  - [abstract] states "accelerated sampling techniques enable efficient deployment as stochastic data-driven closure terms, achieving a 100× speedup while maintaining accuracy"
  - [section] describes adaptive time-stepping approach and smaller T enabled by conditional inputs
  - [corpus] has weak evidence - no directly relevant papers found on this specific mechanism
- Break condition: If the adaptive scheduling function is poorly tuned, or if conditional inputs do not sufficiently reduce the need to explore the high-dimensional data space

## Foundational Learning

- Concept: Score-based generative models and score matching
  - Why needed here: The framework uses score-based diffusion models to learn the score function ∇Uτ log p(Uτ|y) that characterizes the conditional distribution of closure terms
  - Quick check question: What is the key advantage of score matching over directly modeling probability density functions?

- Concept: Fourier neural operators for operator learning
  - Why needed here: FNOs are used to construct the score function, enabling continuous spatiotemporal field modeling and capturing non-local dependencies
  - Quick check question: How does the Fourier basis representation enable FNOs to capture long-range spatial interactions?

- Concept: Stochastic differential equations and reverse-time processes
- Why needed here: The diffusion model framework relies on forward SDE for noise addition and reverse SDE for sampling from the target distribution
  - Quick check question: What is the relationship between the forward diffusion process and the reverse-time sampling process?

## Architecture Onboarding

- Component map:
  Input processing -> Multiple FNO pipelines for τ, Uτ, V, and U†sparse -> Four Fourier layers per pipeline with 1×1 spatial convolutions and spectral convolutions -> Concatenation and transformation network with 1×1 convolutions -> Output refinement

- Critical path:
  1. Data preprocessing (upsampling sparse observations)
  2. Score function construction via FNOs
  3. Training with denoising score matching
  4. Accelerated sampling with adaptive time steps
  5. Deployment as closure term in numerical simulations

- Design tradeoffs:
  - Resolution invariance vs. computational cost (FNOs vs. standard CNNs)
  - Accuracy vs. speed (adaptive vs. fixed time steps)
  - Conditioning information vs. model complexity (more conditions require more complex architecture)

- Failure signatures:
  - Poor performance at test resolutions different from training resolution
  - High errors when sparse observations are inaccurate or missing
  - Sampling instability or poor quality when acceleration techniques are over-applied

- First 3 experiments:
  1. Train and test on same resolution (64×64) with and without sparse observations to verify error reduction claim
  2. Test resolution invariance by training on 64×64 and evaluating on 128×128 and 256×256
  3. Compare accelerated sampling (adaptive steps, smaller T) vs. standard sampling in terms of speed and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the conditional diffusion model scale with the sparsity level of the closure term measurements?
- Basis in paper: [explicit] The paper states that sparse observations of closure terms are incorporated as conditional inputs, but does not systematically investigate the impact of varying sparsity levels.
- Why unresolved: The experiments use a fixed sparsity level (16×16 for a 64×64 model), but the relationship between sparsity and model performance remains unexplored.
- What evidence would resolve it: Experiments testing different sparsity levels (e.g., 8×8, 32×32) while measuring the resulting accuracy and computational costs.

### Open Question 2
- Question: Can the framework be extended to handle non-periodic boundary conditions while maintaining resolution invariance?
- Basis in paper: [inferred] The numerical experiments use periodic boundary conditions, and FNOs are inherently designed for periodic domains through Fourier transforms.
- Why unresolved: The paper does not address whether the resolution invariance property holds for non-periodic domains, which would require different treatment of boundary conditions.
- What evidence would resolve it: Demonstrations of the framework's performance on domains with Dirichlet, Neumann, or mixed boundary conditions across multiple resolutions.

### Open Question 3
- Question: What is the theoretical justification for using FNOs to construct score functions in terms of universal approximation properties?
- Basis in paper: [explicit] The paper mentions that FNOs enable resolution invariance and non-local modeling, but does not provide rigorous theoretical analysis of their approximation capabilities.
- Why unresolved: While the paper demonstrates empirical success, it lacks formal proofs about the approximation power of FNO-based score functions for the target conditional distributions.
- What evidence would resolve it: Mathematical analysis proving that FNOs can approximate the true score function with arbitrary precision under certain conditions, or counterexamples showing limitations.

## Limitations

- The evaluation primarily relies on synthetic 2D Navier-Stokes data, limiting generalizability to real-world turbulence applications
- The framework doesn't address temporal generalization - whether models trained on one time horizon generalize to longer simulations
- The accelerated sampling approach, while achieving 100× speedup, lacks comprehensive analysis of when and why this acceleration succeeds or fails

## Confidence

**High Confidence**: The error reduction claims (60% to 3-8%) are well-supported by the reported metrics (DFro, DSpe, DMax, DMSE) across multiple test cases. The resolution invariance demonstration with consistent performance across 64×64, 128×128, and 256×256 resolutions is convincingly presented.

**Medium Confidence**: The claim that FNOs inherently capture non-local dependencies through Fourier basis representation is mechanistically sound but lacks empirical validation specific to closure modeling. The acceleration technique's effectiveness is demonstrated but the sensitivity to hyper-parameter choices (scheduling function, T value) remains unclear.

**Low Confidence**: The paper's claims about systematic generalization to multiscale systems without clear scale separation are based on a single 2D example. The assertion that sparse observations "contain sufficient information" lacks rigorous analysis of the information-theoretic relationship between observation density and prediction accuracy.

## Next Checks

1. **Temporal Generalization Test**: Train the model on Navier-Stokes data up to t=20s, then evaluate predictive accuracy at t=40s and t=60s to assess temporal generalization beyond the training horizon.

2. **Information Content Analysis**: Systematically vary the density and accuracy of sparse closure observations (e.g., 8×8, 16×16, 32×32 interpolation) and measure the corresponding error reduction to quantify the information-theoretic relationship.

3. **3D Navier-Stokes Validation**: Extend the framework to 3D Navier-Stokes simulations with realistic Reynolds numbers to validate claims about generalization to complex dynamical systems and test whether the Fourier basis remains effective in higher dimensions.