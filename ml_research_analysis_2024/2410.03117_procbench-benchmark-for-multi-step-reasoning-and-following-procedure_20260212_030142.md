---
ver: rpa2
title: 'ProcBench: Benchmark for Multi-Step Reasoning and Following Procedure'
arxiv_id: '2410.03117'
source_url: https://arxiv.org/abs/2410.03117
tags:
- reasoning
- tasks
- step
- list
- string
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ProcBench evaluates multi-step reasoning by isolating procedural\
  \ following from complex knowledge demands, using tasks where step-by-step procedures\
  \ are fully specified. It measures models' instruction adherence via Prefix Accuracy\
  \ (PA), Sequential Match (SM), Final Match (FM), and Prefix Match Length (PML) across\
  \ 23 task types with 2\u201325 steps."
---

# ProcBench: Benchmark for Multi-Step Reasoning and Following Procedure

## Quick Facts
- arXiv ID: 2410.03117
- Source URL: https://arxiv.org/abs/2410.03117
- Authors: Ippei Fujisawa; Sensho Nobe; Hiroki Seto; Rina Onda; Yoshiaki Uchida; Hiroki Ikoma; Pei-Chun Chien; Ryota Kanai
- Reference count: 22
- Primary result: Leading LLMs (e.g., o1-preview) achieve PA ~0.80 on short sequences but fall to ~0.30 at 25 steps, revealing a gap between knowledge-driven reasoning and procedural instruction-following.

## Executive Summary
ProcBench evaluates multi-step reasoning by isolating procedural following from complex knowledge demands, using tasks where step-by-step procedures are fully specified. It measures models' instruction adherence via Prefix Accuracy (PA), Sequential Match (SM), Final Match (FM), and Prefix Match Length (PML) across 23 task types with 2–25 steps. State-of-the-art models such as o1-preview and o1-mini show strong performance on short sequences (PA ~0.80) but performance drops sharply on longer sequences, with PA falling to ~0.30 at 25 steps. Even leading models frequently fail to maintain accuracy beyond a few steps, revealing a gap between knowledge-driven reasoning and procedural instruction following. Results emphasize that current LLMs struggle with multi-step procedural reasoning despite excelling in knowledge-heavy tasks.

## Method Summary
ProcBench is a benchmark for evaluating multi-step reasoning and procedural instruction-following. The dataset comprises 5,520 examples across 23 task types, with 240 examples per task (10 per step count from 2 to 25 steps). Each task provides explicit step-by-step procedures and corresponding questions. Models receive a template (procedure) and question, then output intermediate and final states in structured JSON format. Evaluation uses four metrics: Prefix Accuracy (PA), Sequential Match (SM), Final Match (FM), and Prefix Match Length (PML). The dataset is fixed and publicly available, ensuring reproducibility.

## Key Results
- Leading models achieve PA ~0.80 on short sequences (2–6 steps) but fall to ~0.30 at 25 steps.
- PA drops sharply for sequences longer than 10 steps, indicating error propagation.
- Models frequently fail to maintain accuracy beyond a few steps, even when procedures are fully specified.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Prefix Accuracy (PA) reveals that LLMs fail to maintain multi-step reasoning accuracy due to accumulating error propagation from incorrect intermediate steps.
- **Mechanism**: When a model makes an error at step k, all subsequent steps operate on incorrect states, causing a cascading failure that drops PA sharply for longer sequences. PA normalizes by the longer sequence length, so early errors disproportionately penalize longer tasks.
- **Core assumption**: Errors in intermediate steps are not self-correcting and propagate irreversibly through the sequence.
- **Evidence anchors**:
  - [abstract] "performance drops sharply on longer sequences, with PA falling to ~0.30 at 25 steps"
  - [section] "PML increases with problem length but plateaus after a certain point"
  - [corpus] No direct evidence about error propagation in related work; corpus only shows similar benchmarks exist.
- **Break condition**: If a model could detect and correct intermediate errors before proceeding, PA would not drop so sharply with sequence length.

### Mechanism 2
- **Claim**: LLMs excel at knowledge-driven reasoning but fail at procedural instruction-following when the correct path is explicitly given, because they rely on implicit knowledge retrieval rather than strict adherence.
- **Mechanism**: Models trained on diverse datasets prioritize knowledge synthesis over step-by-step execution, leading them to deviate from prescribed procedures even when the correct path is fully specified.
- **Core assumption**: LLMs are optimized for tasks requiring implicit knowledge, not for following explicit, fixed procedures.
- **Evidence anchors**:
  - [abstract] "current LLMs struggle with multi-step procedural reasoning despite excelling in knowledge-heavy tasks"
  - [section] "emphasizing evaluative tasks that require minimal prerequisite knowledge and demanding exact adherence to instructions"
  - [corpus] Weak evidence; related work mentions instruction-following but not the knowledge vs. procedure distinction.
- **Break condition**: If models were explicitly trained or fine-tuned on procedural instruction-following tasks, their PA would remain high across all sequence lengths.

### Mechanism 3
- **Claim**: The structured JSON output conversion introduces additional noise that reduces measured accuracy, particularly for longer sequences where models already struggle.
- **Mechanism**: Converting free-form text to structured JSON may misinterpret model outputs, especially when the model's reasoning is already unstable, causing further drops in PA and SM metrics.
- **Core assumption**: The conversion process is not perfect and can introduce errors independent of the model's raw reasoning ability.
- **Evidence anchors**:
  - [section] "results reflect not only the models’ raw accuracy but also the impact of the conversion process on the final evaluation scores"
  - [corpus] No direct evidence in related work about output conversion affecting benchmark accuracy.
- **Break condition**: If the conversion process were perfectly accurate or bypassed, measured accuracy would be higher, especially for longer sequences.

## Foundational Learning

- **Concept**: Multi-step reasoning vs. knowledge retrieval
  - **Why needed here**: Understanding the difference between applying explicit procedures and synthesizing knowledge is crucial for interpreting why models fail on ProcBench.
  - **Quick check question**: Can a model solve a 10-step arithmetic procedure if each step is explicitly given, or does it need to infer the procedure from context?

- **Concept**: Prefix-based evaluation metrics
  - **Why needed here**: PA, SM, and PML measure different aspects of instruction-following; knowing how they differ is key to diagnosing model weaknesses.
  - **Quick check question**: If a model gets the first 3 steps right but fails on step 4, what are its PA, SM, and PML scores relative to a 10-step target?

- **Concept**: Error propagation in sequential tasks
  - **Why needed here**: Recognizing that early mistakes compound is essential for understanding why longer sequences are harder.
  - **Quick check question**: If a model makes an error at step 2 of a 5-step task, can it still recover to get the final answer correct?

## Architecture Onboarding

- **Component map**: Generator -> Model -> Converter -> Evaluator -> Metrics
- **Critical path**: Generator → Model → Converter → Evaluator → Metrics
  - The converter is a bottleneck because errors here directly affect all metrics.
  - The evaluator must handle type conversion (int, str, list) correctly.
- **Design tradeoffs**:
  - Fixed dataset vs. dynamic generation: Fixed dataset ensures reproducibility but limits scale.
  - Minimal implicit knowledge vs. realistic tasks: Too minimal may not reflect real-world complexity.
  - Strict step-by-step evaluation vs. final-answer-only: Strict evaluation reveals error propagation but may be overly harsh.
- **Failure signatures**:
  - Sharp drop in PA for sequences >10 steps indicates error propagation.
  - PA > SM but both low suggests partial correctness but no full sequence matches.
  - FM close to SM indicates final answer is only correct if all steps are correct.
- **First 3 experiments**:
  1. **Baseline accuracy**: Run each model on short (2–6 steps) vs. long (17–25 steps) tasks and plot PA vs. step count.
  2. **Error localization**: For failing cases, extract the first step where model output diverges from ground truth and analyze patterns.
  3. **Conversion robustness**: Manually inspect a sample of converted JSON outputs to quantify conversion-induced errors.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What specific architectural or training modifications could enable LLMs to maintain high Prefix Accuracy across all step lengths in multi-step reasoning tasks?
- **Basis in paper**: [explicit] The paper notes that even leading models such as o1-preview and o1-mini show significant performance drops in Prefix Accuracy as the number of steps increases, particularly falling to ~0.30 at 25 steps.
- **Why unresolved**: The paper identifies the limitation but does not explore potential solutions or modifications that could address this issue.
- **What evidence would resolve it**: Experiments demonstrating improved performance in multi-step reasoning tasks after implementing specific architectural changes or training techniques would provide evidence.

### Open Question 2
- **Question**: How does the reliance on prior knowledge versus procedural instruction-following ability differ among state-of-the-art LLMs, and what does this reveal about their underlying reasoning mechanisms?
- **Basis in paper**: [explicit] The paper discusses that current LLMs may be more adept at leveraging knowledge to solve complex problems rather than excelling at multi-step procedural reasoning itself.
- **Why unresolved**: The paper suggests a distinction but does not provide a detailed analysis or empirical comparison of knowledge reliance versus procedural adherence across different models.
- **What evidence would resolve it**: Comparative studies analyzing model performance on tasks requiring minimal knowledge versus those requiring extensive knowledge would clarify the reliance on each aspect.

### Open Question 3
- **Question**: To what extent can models trained on traditional benchmarks, such as those used in inductive programming, be adapted to excel in procedural instruction-following tasks without domain-specific tuning?
- **Basis in paper**: [inferred] The paper mentions that traditional machine learning models, like those used in inductive programming, could potentially be applied to ProcBench but would require dataset augmentation for training.
- **Why unresolved**: The paper does not explore the adaptability or performance of such models on procedural reasoning tasks.
- **What evidence would resolve it**: Experimental results showing the performance of models trained on traditional benchmarks when applied to procedural tasks without domain-specific tuning would provide insights.

## Limitations
- The conversion from free-form text to structured JSON is a potential bottleneck, but its impact is not empirically isolated in the paper.
- The exact prompt templates for all 23 task types are not fully specified, which may affect replication fidelity.
- The generalizability of these findings to more knowledge-intensive real-world tasks remains unclear.

## Confidence
- **High confidence**: The observation that PA drops sharply for longer sequences (≥10 steps) is directly supported by reported metrics.
- **Medium confidence**: The claim that LLMs struggle with procedural reasoning despite excelling in knowledge-heavy tasks is supported but relies on the assumption that ProcBench effectively isolates procedural following.
- **Low confidence**: The assertion that error propagation is the sole cause of performance degradation lacks direct evidence; alternative explanations (e.g., conversion noise) are plausible but untested.

## Next Checks
1. **Isolate conversion impact**: Run a subset of tasks where model outputs are manually verified against ground truth JSON to quantify conversion-induced errors.
2. **Test error recovery**: For failing cases, check if models can recover after making an error by comparing intermediate states beyond the first mismatch.
3. **Compare with knowledge-heavy tasks**: Benchmark the same models on a knowledge-intensive multi-step task (e.g., grade school math) to contrast procedural vs. knowledge-driven reasoning performance.