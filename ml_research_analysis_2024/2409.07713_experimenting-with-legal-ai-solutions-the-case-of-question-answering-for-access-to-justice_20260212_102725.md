---
ver: rpa2
title: 'Experimenting with Legal AI Solutions: The Case of Question-Answering for
  Access to Justice'
arxiv_id: '2409.07713'
source_url: https://arxiv.org/abs/2409.07713
tags:
- legal
- retrieval
- data
- language
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of providing accurate legal advice
  to laypeople using generative AI models, focusing on question-answering tasks. The
  authors propose a human-centric legal NLP pipeline that includes data sourcing,
  inference, and evaluation.
---

# Experimenting with Legal AI Solutions: The Case of Question-Answering for Access to Justice

## Quick Facts
- arXiv ID: 2409.07713
- Source URL: https://arxiv.org/abs/2409.07713
- Reference count: 23
- Primary result: Domain-specific retrieval using 850 legal documents matches or outperforms internet-scale search for legal question-answering

## Executive Summary
This paper addresses the challenge of providing accurate legal advice to laypeople using generative AI models, focusing on question-answering tasks. The authors propose a human-centric legal NLP pipeline that includes data sourcing, inference, and evaluation. They introduce a novel dataset, LegalQA, containing real legal questions and expert-written answers, which they release publicly. The research demonstrates that retrieval-augmented generation using a small set of legal-expert-approved citations (850 documents) can match or outperform retrieval from the entire internet (hundreds of billions of documents), improving the performance of existing models like GPT-3.5-turbo on legal question-answering tasks.

## Method Summary
The authors develop a comprehensive legal NLP pipeline that begins with data sourcing from real legal questions and expert-written answers to create the LegalQA dataset. The inference stage employs retrieval-augmented generation, combining a retrieval system with a generative model to produce answers. The evaluation protocol is based on factuality metrics to assess the accuracy of generated responses. The key innovation involves using a carefully curated set of 850 legal-expert-approved documents for retrieval, rather than relying on internet-scale search. This approach leverages domain-specific knowledge while reducing computational costs and improving answer quality for legal questions.

## Key Results
- Retrieval-augmented generation using 850 legal-expert-approved documents matches or outperforms retrieval from the entire internet for legal question-answering
- The approach improves performance of existing models like GPT-3.5-turbo on legal tasks
- Domain-specific retrieval reduces computational costs while maintaining or improving accuracy

## Why This Works (Mechanism)
The effectiveness of domain-specific retrieval for legal question-answering stems from the highly specialized nature of legal language and concepts. General internet search retrieves vast amounts of information, much of which is irrelevant or potentially misleading for legal contexts. By contrast, a curated set of expert-approved legal documents provides high-quality, authoritative sources that are more likely to contain accurate and relevant information. The retrieval-augmented generation framework benefits from this focused approach by receiving precise, context-appropriate citations that guide the generation of legally sound responses. This mechanism reduces the noise inherent in broad internet search while maintaining the flexibility and natural language capabilities of generative models.

## Foundational Learning
- **Retrieval-augmented generation**: Combines information retrieval with text generation to produce more accurate and contextually appropriate responses. Needed to bridge the gap between vast information sources and precise answer generation. Quick check: Verify that retrieved documents are actually used in the generation process and improve output quality.
- **Legal NLP**: Natural language processing specifically tailored for legal texts, accounting for specialized terminology, formal language, and complex reasoning patterns. Needed to handle the unique challenges of legal language that general NLP models struggle with. Quick check: Test model performance on legal terminology recognition and understanding.
- **Factuality evaluation**: Metrics designed to assess whether generated responses accurately reflect the information in source documents. Needed because standard language model metrics may not capture legal accuracy requirements. Quick check: Compare factuality scores against human expert judgments on sample outputs.
- **Domain-specific document curation**: The process of selecting and organizing relevant documents for a specific knowledge domain. Needed to ensure retrieval systems access high-quality, relevant information rather than noisy, general web content. Quick check: Measure retrieval precision and recall on domain-specific versus general document collections.
- **Legal question-answering**: The task of providing accurate, actionable answers to legal questions posed by laypeople. Needed to address the access to justice gap by making legal information more accessible. Quick check: Assess whether answers are comprehensible and useful to non-expert users.

## Architecture Onboarding

**Component Map**: Legal questions -> Retrieval system (850 legal documents) -> Generative model (GPT-3.5-turbo) -> Factuality evaluation

**Critical Path**: Question reception → Document retrieval → Context augmentation → Answer generation → Factuality assessment → Output delivery

**Design Tradeoffs**: The primary tradeoff involves balancing document collection size against retrieval quality and computational efficiency. Using 850 curated documents versus billions of internet sources significantly reduces computational costs and improves focus, but requires careful document selection and may miss some relevant information. The choice of factuality-based evaluation over other metrics prioritizes accuracy over fluency or other language quality measures.

**Failure Signatures**: 
- Retrieval failures occur when relevant documents are missing from the curated collection, leading to incomplete or inaccurate answers
- Generation failures happen when the model misinterprets retrieved context or generates plausible but incorrect information
- Evaluation failures arise when factuality metrics don't capture nuanced legal accuracy issues or jurisdictional variations

**First Experiments**:
1. Conduct ablation studies by systematically removing documents from the 850-document collection to identify the minimum effective size and most important document types
2. Test the system on out-of-distribution legal questions to assess generalization beyond the training data scope
3. Compare retrieval performance using different similarity metrics (semantic vs. keyword-based) to optimize document selection

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation protocol relies on factuality metrics that may not fully capture the nuances of legal accuracy and complex legal reasoning
- The LegalQA dataset consists of real legal questions and expert-written answers from a single jurisdiction, limiting generalizability across different legal systems
- The comparison between 850 domain-specific documents and the entire internet requires more granular analysis to understand which types of legal questions benefit most from domain-specific retrieval

## Confidence

| Claim | Confidence |
|-------|------------|
| Retrieval-augmented generation using domain-specific documents is more effective than internet-scale search for legal questions | High |
| The approach improves accessibility for laypeople | Medium |
| Results generalize across different legal systems | Low |

## Next Checks
1. Conduct user studies with actual laypeople from diverse backgrounds to assess comprehension and practical utility of the generated legal advice
2. Expand evaluation to include legal experts from multiple jurisdictions to assess cross-jurisdictional applicability
3. Perform ablation studies to determine the minimum effective size of the domain-specific document collection and identify which document types contribute most to performance improvements