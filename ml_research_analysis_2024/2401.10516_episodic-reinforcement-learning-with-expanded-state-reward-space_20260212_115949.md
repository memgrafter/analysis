---
ver: rpa2
title: Episodic Reinforcement Learning with Expanded State-reward Space
arxiv_id: '2401.10516'
source_url: https://arxiv.org/abs/2401.10516
tags:
- uni00000013
- learning
- episodic
- past
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an episodic control-based deep reinforcement
  learning method with expanded state-reward space to address the problem of potential
  misalignment between state and reward spaces in existing episodic control methods.
  The key idea is to reuse historical states retrieved by episodic control as part
  of the input states and integrate retrieved Monte Carlo returns into immediate rewards
  during training.
---

# Episodic Reinforcement Learning with Expanded State-reward Space

## Quick Facts
- arXiv ID: 2401.10516
- Source URL: https://arxiv.org/abs/2401.10516
- Reference count: 9
- Outperforms recent sibling method and common baselines in continuous control tasks

## Executive Summary
This paper addresses data inefficiency in deep reinforcement learning by proposing an episodic control-based method with expanded state-reward space. The key innovation is to reuse historical states retrieved by episodic control as part of the input states and integrate retrieved Monte Carlo returns into immediate rewards during training. This approach better utilizes retrieval information and improves state value estimation through temporal difference loss. Empirical results on challenging Box2d and Mujoco tasks demonstrate significant improvements in average rewards and convergence speed over recent sibling methods and common baselines.

## Method Summary
The proposed method builds upon the Soft Actor-Critic (SAC) framework by expanding both the state and reward spaces using episodic retrieval. It employs Gaussian random projection to compress state-action pairs into low-dimensional representations for efficient episodic memory retrieval. The method retrieves top-K similar historical transitions and processes them through feature extraction networks to generate past states (sp_t). These past states are concatenated with current states as expanded input to the actor-critic networks. Additionally, retrieved Monte Carlo returns are integrated with immediate rewards using a weighted coefficient η during TD loss calculation, providing more accurate value estimation.

## Key Results
- Achieves 25.2% improvement in average rewards on HalfCheetah-v3 compared to the sibling EMAC algorithm
- Demonstrates 10x faster convergence on Hopper-v3 compared to standard SAC baselines
- Effectively mitigates Q-value overestimation, with Q-value estimates significantly lower than EMAC algorithm

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding the state space with retrieved historical states improves policy generalization.
- Mechanism: By concatenating the current state with retrieved past states as input to the actor-critic networks, the agent learns to make decisions based on both current and historical information.
- Core assumption: The retrieved past states contain relevant information that can improve decision-making in the current context.
- Evidence anchors:
  - [abstract]: "we reuse the historical states retrieved by EC as part of the input states"
  - [section 3.2]: "the set of the trajectories as T = {τ_t1, τ_t2, ..., τ_tK}, where the length of each trajectory is set to d... the K trajectories (only pick up the state and action) are fed into the feature extraction networks and then aggregated into the past states sp_t"

### Mechanism 2
- Claim: Integrating retrieved MC-returns into immediate rewards improves value estimation accuracy.
- Mechanism: By combining the retrieved MC-returns with the immediate reward in a weighted manner during TD loss calculation, the agent gets a more accurate estimate of the true return.
- Core assumption: The retrieved MC-returns provide a more accurate estimate of the expected return than the immediate reward alone.
- Evidence anchors:
  - [abstract]: "integrate the retrieved MC-returns into the immediate reward in each interactive transition"
  - [section 3.3]: "re_t = ri_t + η rp_t"

### Mechanism 3
- Claim: Expanding the state-reward space through episodic retrieval reduces Q-value overestimation.
- Mechanism: By providing the agent with both current and historical information in both state and reward spaces, the value function can be learned more accurately.
- Core assumption: Q-value overestimation in traditional methods is partly due to incomplete information in the state and reward spaces.
- Evidence anchors:
  - [section 4.3]: "we show experimentally the problem can be further mitigated and the Q-value estimation of our method is significantly lower than that of the EMAC algorithm"
  - [section 3.4]: "re ~ R_e is leveraged to learn V(s) under state se ~ S_e, where the absence of any subspace of S and R may lead to a mismatch between the reward and the state transition model space"

## Foundational Learning

- Concept: Temporal Difference (TD) Learning
  - Why needed here: The method uses TD loss calculation to update the value function, incorporating both current and historical rewards.
  - Quick check question: What is the difference between TD(0) and Monte Carlo methods in reinforcement learning?

- Concept: Episodic Memory and Retrieval
  - Why needed here: The method relies on storing and retrieving past experiences to expand the state-reward space.
  - Quick check question: How does episodic memory differ from the replay buffer used in standard deep RL algorithms?

- Concept: Function Approximation in High-Dimensional Spaces
  - Why needed here: The method uses neural networks to approximate value functions over the expanded state space.
  - Quick check question: What challenges arise when approximating functions in high-dimensional input spaces?

## Architecture Onboarding

- Component map:
  Gaussian Random Projection -> Episodic Memory Container -> Feature Extraction Networks -> Actor-Critic Networks with Expanded States

- Critical path:
  1. Environment interaction generates (state, action, reward, next_state)
  2. Gaussian projection creates low-dimensional index for episodic memory
  3. Episodic retrieval finds top-K similar historical transitions
  4. Retrieved states and MC-returns are processed and integrated
  5. Expanded state-reward pairs train actor-critic networks via TD loss

- Design tradeoffs:
  - Memory dimension vs. retrieval accuracy: Higher dimensions preserve more information but increase computation
  - K-nearest size vs. information richness: Larger K provides more diverse information but may include irrelevant data
  - Weight coefficient η vs. stability: Higher η incorporates more historical information but may slow adaptation

- Failure signatures:
  - Poor performance: Check if episodic retrieval is finding relevant historical states
  - Instability during training: Verify that the weight coefficient η is appropriately balanced
  - Slow convergence: Ensure the Gaussian projection is preserving sufficient information

- First 3 experiments:
  1. Implement basic SAC with Gaussian projection but without episodic retrieval to establish baseline
  2. Add episodic retrieval to retrieve states only (no reward integration) to measure state expansion impact
  3. Integrate both retrieved states and rewards with varying weight coefficients to find optimal η

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the expanded state-reward space method perform in sparse reward environments compared to dense reward environments?
- Basis in paper: [inferred] The paper mentions that the method performs poorly in the hardWalker2d-v3 task, which is suspected to be due to the sparse reward property of high-difficulty tasks.
- Why unresolved: The paper does not provide explicit experiments or results comparing the performance of the method in sparse versus dense reward environments.
- What evidence would resolve it: Additional experiments and results showing the performance of the method in both sparse and dense reward environments would help to determine its effectiveness in different reward settings.

### Open Question 2
- Question: What is the optimal trade-off coefficient (η) for different tasks in the expanded state-reward space method?
- Basis in paper: [explicit] The paper conducts ablation experiments to explore the optimal decision-making scheme by fine-tuning the weight between the current and past rewards. It shows that the optimal coefficients for different tasks are not fixed.
- Why unresolved: The paper does not provide a definitive answer on the optimal trade-off coefficient for different tasks.
- What evidence would resolve it: Further experiments and analysis to determine the optimal trade-off coefficient for different tasks would help in understanding the best settings for the method.

### Open Question 3
- Question: How does the expanded state-reward space method compare to other episodic control-based methods in terms of sample efficiency and policy performance?
- Basis in paper: [inferred] The paper mentions that the proposed method outperforms the sibling EC-based EMAC algorithm and other baselines in terms of average rewards and convergence speed.
- Why unresolved: The paper does not provide a comprehensive comparison of the expanded state-reward space method with other episodic control-based methods in terms of sample efficiency and policy performance.
- What evidence would resolve it: Additional experiments and comparisons with other episodic control-based methods would help to determine the effectiveness of the expanded state-reward space method in terms of sample efficiency and policy performance.

## Limitations
- Evaluation limited to continuous control tasks from Mujoco and Box2d, may not generalize to other domains
- Performance on sparse reward environments not thoroughly investigated
- Computational overhead of episodic retrieval and scaling properties for larger state spaces remain unclear

## Confidence

- **High Confidence**: The core mechanism of expanding state space with retrieved historical states and integrating MC-returns into rewards is well-supported by empirical results across multiple baselines and tasks.
- **Medium Confidence**: The claim about alleviating Q-value overestimation is supported but could benefit from more rigorous theoretical analysis beyond empirical observation.
- **Low Confidence**: The generalizability to other RL domains and the method's scalability to high-dimensional observations (e.g., image-based tasks) are speculative based on the current evidence.

## Next Checks

1. **Cross-domain generalization**: Test the method on at least one non-continuous control task (e.g., Atari games or discrete action tasks) to validate its broader applicability beyond the current evaluation scope.

2. **Memory efficiency analysis**: Conduct experiments to measure the computational overhead of episodic retrieval and identify the point of diminishing returns as episodic memory size increases, establishing practical limits for deployment.

3. **Ablation on weight coefficient η**: Perform a systematic grid search over η values (e.g., 0.1 to 0.9) across all tasks to determine if the fixed value of 0.5 is universally optimal or task-dependent, potentially leading to adaptive weighting strategies.