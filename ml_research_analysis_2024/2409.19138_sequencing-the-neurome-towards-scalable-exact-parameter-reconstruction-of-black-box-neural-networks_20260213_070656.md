---
ver: rpa2
title: 'Sequencing the Neurome: Towards Scalable Exact Parameter Reconstruction of
  Black-Box Neural Networks'
arxiv_id: '2409.19138'
source_url: https://arxiv.org/abs/2409.19138
tags:
- network
- networks
- neural
- e-06
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenging problem of reconstructing exact
  neural network parameters using only query access to a black-box model. The core
  insight is that real-world networks have strong inductive biases due to random initialization
  and gradient-based training, which dramatically reduces the practical parameter
  search space.
---

# Sequencing the Neurome: Towards Scalable Exact Parameter Reconstruction of Black-Box Neural Networks

## Quick Facts
- arXiv ID: 2409.19138
- Source URL: https://arxiv.org/abs/2409.19138
- Authors: Judah Goldfeder; Quinten Roets; Gabe Guo; John Wright; Hod Lipson
- Reference count: 40
- Primary result: Exact reconstruction of networks with over 1.5 million parameters and up to 7 layers deep using query access

## Executive Summary
This paper tackles the challenging problem of reconstructing exact neural network parameters using only query access to a black-box model. The core insight is that real-world networks have strong inductive biases due to random initialization and gradient-based training, which dramatically reduces the practical parameter search space. The authors introduce Committee Disagreement Sampling, a novel query generation method that learns maximally informative samples by optimizing for disagreement among a population of hypothesis networks. Their approach successfully reconstructs networks with over 1.5 million parameters and up to 7 layers deep - the largest and deepest exact reconstructions to date.

## Method Summary
The method uses Committee Disagreement Sampling to generate maximally informative queries by optimizing for disagreement among a population of hypothesis networks. The algorithm maintains a population of surrogate networks that are trained on query results, with input samples being iteratively refined to maximize disagreement. The approach accounts for network isomorphisms (permutation, scaling, polarity) through canonicalization of weight representations before comparison. Reconstruction continues until the population reaches consensus and loss approaches zero.

## Key Results
- Successfully reconstructs networks with over 1.5 million parameters and up to 7 layers deep
- Achieves max parameter error below 0.0001 for a 784-128-10 network using 550k samples
- Demonstrates robustness across different architectures, activation functions, datasets, and training procedures
- Shows stable convergence with max error decreasing significantly faster than mean error

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real-world networks have strong inductive biases from random initialization and SGD training that drastically shrink the practical parameter space.
- Mechanism: Because most networks are randomly initialized and trained via gradient descent, their weights occupy only a tiny fraction of the full parameter space, making exact recovery feasible.
- Core assumption: The black-box network was trained using standard procedures (random initialization, SGD, etc.).
- Evidence anchors:
  - [abstract] "almost all networks used in practice are produced by random initialization and first order optimization, an inductive bias that drastically reduces the practical parameter space"
  - [section 3.1] "we focus on networks that have been produced via random initialization, and trained with gradient descent and backpropogation"
- Break condition: If the network uses unusual training procedures, exotic architectures, or non-standard initialization, the prior becomes weak and reconstruction fails.

### Mechanism 2
- Claim: Committee Disagreement Sampling generates maximally informative queries by optimizing for disagreement among a population of hypothesis networks.
- Mechanism: By iteratively refining input samples to maximize disagreement among multiple surrogate networks, the algorithm learns inputs that split the hypothesis space evenly, providing the most information per query.
- Core assumption: The disagreement loss (negative mean pairwise L1 distance between normalized outputs) effectively approximates the information gain from each query.
- Evidence anchors:
  - [abstract] "present a novel query generation algorithm that produces maximally informative samples, letting us untangle the non-linear relationships efficiently"
  - [section 3.2] "we propose a novel sampling method called Committee Disagreement Sampling...approximated using query by committee"
- Break condition: If the network outputs are too similar across inputs or if the disagreement metric doesn't correlate with information gain, sampling becomes inefficient.

### Mechanism 3
- Claim: The algorithm accounts for network isomorphisms (permutation, scaling, polarity) by canonicalizing weight representations before comparison.
- Mechanism: By normalizing and reordering weight matrices according to predefined rules, the algorithm ensures it finds the most similar isomorphic form when evaluating reconstruction quality.
- Core assumption: All relevant isomorphisms can be captured by the three transformations described (normalization, sign adjustment, sorting).
- Evidence anchors:
  - [section 1.2] "We can mathematically describe the internal parameters of a neural network...isomorphisms can be expressed as matrix operations"
  - [section 3.3.1] "we define a canonical representation for each isomorphism group...calculate the similarity between two networks by converting both of them to their canonical form"
- Break condition: If new types of isomorphisms exist or if the canonicalization fails to produce unique representations, similarity metrics become unreliable.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) theory
  - Why needed here: Provides theoretical justification that weights barely move during training for over-parameterized networks, supporting the inductive bias assumption
  - Quick check question: Why does NTK theory suggest that gradient descent finds similar solutions across different random initializations?

- Concept: Query by Committee active learning
  - Why needed here: Forms the theoretical basis for the disagreement-based sampling strategy
  - Quick check question: What is the relationship between disagreement among hypotheses and information gain in active learning?

- Concept: Network isomorphisms
  - Why needed here: Essential for correctly evaluating reconstruction quality when multiple weight configurations produce identical functionality
  - Quick check question: How do permutation and scaling isomorphisms affect the uniqueness of weight recovery?

## Architecture Onboarding

- Component map: Population manager -> Query generator -> Alignment module -> Convergence detector
- Critical path:
  1. Initialize population of surrogate networks
  2. Generate maximally informative queries via committee disagreement optimization
  3. Train population on query results
  4. Check for convergence (population agreement + vanishing loss)
  5. Return best network from population

- Design tradeoffs:
  - Population size vs. computational cost: Larger populations explore more hypotheses but increase query generation time
  - Query generation epochs vs. sample quality: More epochs produce better samples but slow the process
  - Convergence thresholds vs. reconstruction accuracy: Stricter thresholds improve accuracy but may require more samples

- Failure signatures:
  - Population divergence: Networks in the committee drift apart rather than converge
  - Stagnant disagreement: Query generation fails to produce new information
  - Slow convergence: Max error decreases much slower than mean error

- First 3 experiments:
  1. Reconstruct a small 784x128x10 ReLU network using 550k samples, verify max error < 0.0001
  2. Test reconstruction with different activation functions (ReLU, TanH, LeakyReLU)
  3. Vary black-box training duration (5 to 5000 epochs) to test robustness to training length

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum size (number of parameters) of a neural network that can be exactly reconstructed using the Committee Disagreement Sampling method?
- Basis in paper: [explicit] The paper demonstrates reconstruction of networks with over 1.5 million parameters, but does not specify a hard limit.
- Why unresolved: The authors tested up to a certain size but did not explore the theoretical or practical limits of their approach.
- What evidence would resolve it: Systematic testing of the algorithm on progressively larger networks until failure, identifying the point where reconstruction becomes infeasible.

### Open Question 2
- Question: How does the reconstruction accuracy vary with different random initializations of the black-box network?
- Basis in paper: [inferred] The paper assumes random initialization with known distribution but does not investigate the impact of different initializations on reconstruction success.
- Why unresolved: The study focuses on networks trained via standard procedures but does not control for or report the effects of initial weight variability.
- What evidence would resolve it: Reconstructing multiple networks of the same architecture but with different random initializations, and comparing reconstruction accuracy and sample efficiency.

### Open Question 3
- Question: Can the Committee Disagreement Sampling method be extended to reconstruct recurrent neural networks (RNNs) or other architectures beyond feedforward networks?
- Basis in paper: [explicit] The authors mention that their method works for various architectures but do not demonstrate results for RNNs or other complex structures.
- Why unresolved: The study focuses on feedforward networks with various activation functions but does not explore temporal or recurrent architectures.
- What evidence would resolve it: Applying the algorithm to reconstruct RNNs, LSTMs, or transformers, and evaluating reconstruction accuracy and scalability for these architectures.

## Limitations

- Reconstruction quality highly dependent on standard training procedures and may fail with unusual architectures or optimization methods
- Scalability to deeper networks (>7 layers) or different structural properties remains untested
- Claims about scalability to "any" modern network architecture are overstated without broader empirical validation

## Confidence

- High confidence: The core mechanism of committee disagreement sampling for query generation is well-established and empirically validated
- Medium confidence: The claim that inductive biases from random initialization and SGD drastically reduce parameter space is theoretically sound but may not hold for all training regimes
- Low confidence: The assertion that this approach scales to "any" modern network architecture is overstated without broader empirical validation

## Next Checks

1. Test reconstruction robustness across different training optimization methods (Adam, RMSProp, SGD with momentum) to verify inductive bias assumptions
2. Evaluate performance on deeper networks (>10 layers) to assess true scalability limits
3. Verify that reconstruction quality degrades gracefully as the black-box training procedure deviates from standard practices