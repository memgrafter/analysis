---
ver: rpa2
title: 'Feature Averaging: An Implicit Bias of Gradient Descent Leading to Non-Robustness
  in Neural Networks'
arxiv_id: '2410.10322'
source_url: https://arxiv.org/abs/2410.10322
tags:
- lemma
- have
- network
- data
- know
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies an implicit bias in gradient descent called
  "feature averaging," where neural networks trained on multi-cluster data tend to
  learn an average of discriminative features rather than distinguishing them individually.
  This leads to non-robust models vulnerable to adversarial perturbations.
---

# Feature Averaging: An Implicit Bias of Gradient Descent Leading to Non-Robustness in Neural Networks

## Quick Facts
- arXiv ID: 2410.10322
- Source URL: https://arxiv.org/abs/2410.10322
- Authors: Binghui Li; Zhixuan Pan; Kaifeng Lyu; Jian Li
- Reference count: 40
- One-line primary result: This paper identifies "feature averaging" as an implicit bias of gradient descent that causes neural networks to learn non-robust solutions by averaging discriminative features rather than distinguishing them individually.

## Executive Summary
This paper identifies a novel implicit bias in gradient descent called "feature averaging," where neural networks trained on multi-cluster data tend to learn an average of discriminative features rather than distinguishing them individually. This leads to non-robust models vulnerable to adversarial perturbations. The authors provide rigorous theoretical analysis of two-layer ReLU networks on binary classification tasks with orthogonal cluster centers, proving that gradient descent converges to feature-averaging solutions that are o(√(d/k))-robust despite the existence of more robust alternatives. They also show that providing fine-grained feature-level supervision enables the network to learn decoupled features, achieving optimal √(d)-robustness.

## Method Summary
The authors analyze two-layer ReLU networks trained via gradient descent on binary classification tasks where data points form clusters with orthogonal centers. They prove that under certain conditions (linear separability within clusters and orthogonal cluster centers), gradient descent converges to a feature-averaging solution that averages discriminative features across clusters. The theoretical analysis derives exact convergence rates and robustness bounds for this solution. Experiments on synthetic data, MNIST, and CIFAR-10 validate these findings, demonstrating that models trained with more detailed supervision are significantly more robust to adversarial attacks than those trained with only binary labels.

## Key Results
- Proved that gradient descent on two-layer ReLU networks converges to feature-averaging solutions with o(√(d/k))-robustness
- Demonstrated that fine-grained feature-level supervision can achieve optimal √(d)-robustness
- Experimental validation shows models trained with detailed supervision are significantly more robust to adversarial attacks than those trained with only binary labels
- Showed feature averaging occurs on real datasets (MNIST, CIFAR-10) when using standard binary labels

## Why This Works (Mechanism)
Feature averaging occurs because gradient descent implicitly optimizes for the average of discriminative features across clusters rather than learning to distinguish between them individually. When clusters have orthogonal centers and are linearly separable within each cluster, the gradient flow naturally converges to a solution that represents the average feature direction across all clusters. This averaging creates a decision boundary that is smooth but non-robust, as small perturbations can move points across this averaged boundary. The mechanism is rooted in the implicit bias of gradient descent optimization, which favors solutions with certain geometric properties that may not align with robustness requirements.

## Foundational Learning
- **Two-layer ReLU networks**: Why needed - Form the theoretical foundation for analysis; Quick check - Verify network can represent the data with ReLU activation
- **Gradient descent optimization**: Why needed - The implicit bias arises from the optimization dynamics; Quick check - Track gradient norms during training to observe convergence patterns
- **Adversarial robustness**: Why needed - Provides the evaluation metric for non-robustness; Quick check - Apply PGD attacks to measure model vulnerability
- **Feature-level supervision**: Why needed - Alternative training approach that avoids feature averaging; Quick check - Compare models trained with different supervision granularities
- **Cluster structure analysis**: Why needed - Understanding how data geometry affects learning; Quick check - Visualize cluster centers and their relationships

## Architecture Onboarding

Component map: Input data -> Two-layer ReLU network -> Gradient descent optimization -> Feature averaging solution -> Non-robust model

Critical path: Data clusters with orthogonal centers → Network initialization → Gradient descent updates → Feature averaging convergence → O(√(d/k))-robust solution

Design tradeoffs: The implicit bias toward feature averaging provides computational efficiency and smooth decision boundaries but sacrifices robustness. Alternative supervision strategies that provide feature-level information can achieve better robustness but require more detailed labels and potentially more complex training procedures.

Failure signatures: Models exhibit vulnerability to small adversarial perturbations despite good clean accuracy, with attack success rates correlating with the degree of feature averaging observed in the learned representations.

First experiments:
1. Train a two-layer ReLU network on synthetic orthogonal cluster data and visualize the learned feature directions
2. Compare robustness of models trained with binary labels versus feature-level supervision using PGD attacks
3. Analyze the gradient flow during training to confirm convergence to feature-averaging solutions

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes orthogonal cluster centers, which may not hold in real-world datasets
- Focus on two-layer ReLU networks limits applicability to deeper architectures
- Does not extensively explore the impact of dataset size and network architecture depth
- Relationship to other known sources of adversarial vulnerability warrants further investigation

## Confidence
High: The mathematical proof of feature averaging as an implicit bias in the simplified theoretical setting
Medium: The experimental demonstration of feature averaging effects on real datasets
Low: The direct applicability of feature averaging to explain all forms of adversarial vulnerability in deep networks

## Next Checks
1. Test the feature averaging hypothesis on datasets with non-orthogonal cluster structures and varying levels of class overlap
2. Investigate whether deeper network architectures or different activation functions mitigate or amplify the feature averaging effect
3. Conduct ablation studies to isolate the contribution of feature averaging from other factors in adversarial vulnerability, potentially by comparing models with different supervision granularities on the same datasets