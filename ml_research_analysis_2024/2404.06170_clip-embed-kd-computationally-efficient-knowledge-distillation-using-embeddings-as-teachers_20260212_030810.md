---
ver: rpa2
title: 'CLIP-Embed-KD: Computationally Efficient Knowledge Distillation Using Embeddings
  as Teachers'
arxiv_id: '2404.06170'
source_url: https://arxiv.org/abs/2404.06170
tags:
- teacher
- student
- embeddings
- distillation
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CLIP-Embed-KD, a method that uses pre-computed\
  \ CLIP teacher embeddings instead of running the full teacher model during knowledge\
  \ distillation, achieving 9\xD7 less memory usage and 8\xD7 faster training while\
  \ maintaining competitive accuracy on CIFAR100 image classification. The approach\
  \ computes averaged teacher embeddings per class and aligns them with student embeddings\
  \ using CLIP\u2019s contrastive loss, eliminating the need for repeated forward\
  \ passes through large teacher models during training."
---

# CLIP-Embed-KD: Computationally Efficient Knowledge Distillation Using Embeddings as Teachers

## Quick Facts
- **arXiv ID**: 2404.06170
- **Source URL**: https://arxiv.org/abs/2404.06170
- **Reference count**: 12
- **Primary result**: 9× less memory usage and 8× faster training while maintaining competitive CIFAR100 accuracy

## Executive Summary
CLIP-Embed-KD introduces a computationally efficient knowledge distillation method that uses pre-computed CLIP teacher embeddings instead of running the full teacher model during training. The approach computes averaged teacher embeddings per class and aligns them with student embeddings using CLIP's contrastive loss, eliminating the need for repeated forward passes through large teacher models. This achieves significant memory and computational savings while maintaining competitive accuracy on CIFAR100 image classification.

## Method Summary
The method pre-computes teacher embeddings by running a pre-trained ViT model once on a small sample set (100 samples per class), then averages these embeddings per class. During training, only the student model is used, with its embeddings projected to match teacher dimensions and aligned using a contrastive loss based on scaled cosine similarity. The training combines this CLIP contrastive loss with standard cross-entropy loss for the student model, achieving efficiency gains by avoiding repeated teacher computations while maintaining distillation quality through embedding-level alignment.

## Key Results
- 9× less memory usage compared to full teacher knowledge distillation
- 8× faster training while maintaining competitive accuracy
- Competitive CIFAR100 classification performance with ViT-Base and ViT-Large students
- Demonstrates effectiveness of pre-computed averaged embeddings for distillation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP-Embed-KD achieves efficiency by pre-computing teacher embeddings once, then reusing them instead of running forward passes during training.
- Mechanism: The method computes averaged embeddings per class from a small sample set, projects them to student dimensions, and uses these in a contrastive loss that aligns student embeddings with the averaged teacher representations.
- Core assumption: Averaged class-level embeddings capture sufficient semantic information to guide student training without per-sample teacher computation.
- Evidence anchors:
  - [abstract]: "computes averaged teacher embeddings per class and aligns them with student embeddings using CLIP's contrastive loss"
  - [section]: "We obtain teacher embeddings of the [CLS] tokens for the collected data and compute a cumulative representation of each class embedding, by simply averaging the embeddings of each class"
  - [corpus]: Weak - related works focus on KD with CLIP models but do not explicitly discuss pre-computed averaged embeddings.
- Break condition: If class embedding averages lose too much sample-level discriminative information, student performance degrades significantly.

### Mechanism 2
- Claim: Using CLIP's contrastive objective within distillation aligns teacher and student embedding spaces more effectively than standard KL divergence on logits.
- Mechanism: Instead of comparing logits directly, the method computes scaled cosine similarity between normalized teacher and student [CLS] embeddings and applies cross-entropy loss, treating the similarity matrix as logits.
- Core assumption: The embedding-level alignment captures richer intra-class and inter-class relationships than logit-level alignment alone.
- Evidence anchors:
  - [abstract]: "aligns them with student embeddings using CLIP's contrastive loss"
  - [section]: "We further normalize the resultant embeddings and compute the dot product of the normalized teacher and student embeddings"
  - [corpus]: Weak - corpus does not provide strong evidence of contrastive embedding alignment outperforming logit KD; mostly mentions KD losses.
- Break condition: If teacher embeddings are poorly aligned to the student embedding space, contrastive loss fails to transfer useful knowledge.

### Mechanism 3
- Claim: Memory savings come from avoiding storage and repeated computation of the full teacher model during training.
- Mechanism: Teacher model is run once on a small sample set to generate embeddings, which are stored; during training, only student forward passes are needed, eliminating teacher memory and compute overhead.
- Core assumption: Teacher embeddings are static relative to the student and can be computed offline without loss of distillation quality.
- Evidence anchors:
  - [abstract]: "using pre-computed CLIP teacher embeddings instead of running the full teacher model during knowledge distillation, achieving 9× less memory usage"
  - [section]: "CLIP-Embed-KD which uses pre-computed teacher embeddings in place of the full teacher model"
  - [corpus]: Weak - corpus does not discuss memory savings from pre-computed embeddings; mostly discusses KD variants.
- Break condition: If embeddings must be recomputed for each batch to adapt to student learning, pre-computation loses effectiveness.

## Foundational Learning

- Concept: Knowledge Distillation (KD) fundamentals
  - Why needed here: The paper builds on KD framework but modifies the distillation signal source (embeddings vs logits).
  - Quick check question: What are the three main categories of KD and how does this method fit?
- Concept: Contrastive learning and cosine similarity
  - Why needed here: CLIP-Embed-KD uses contrastive loss based on normalized dot products between embeddings.
  - Quick check question: How does CLIP's contrastive loss differ from typical cross-entropy or KL divergence losses?
- Concept: Vision Transformer (ViT) embedding structure
  - Why needed here: Method extracts [CLS] token embeddings from ViT models for alignment.
  - Quick check question: Why is the [CLS] token used as a summary embedding and what dimensionality does it have?

## Architecture Onboarding

- Component map:
  - Teacher model (offline) -> Embedding projector -> Student model -> Averaged teacher embeddings
- Critical path:
  1. Collect N samples per class from dataset
  2. Run teacher model forward pass once, extract [CLS] embeddings
  3. Average embeddings per class, store in memory
  4. During training: student forward pass → projection → dot product with averaged embeddings → contrastive loss
  5. Combine with student cross-entropy loss, backpropagate
- Design tradeoffs:
  - Memory vs accuracy: Larger teacher embeddings improve alignment but increase storage
  - Sample size per class: More samples yield better class averages but increase pre-computation time
  - Projection layer: Additional parameters but necessary for embedding space alignment
- Failure signatures:
  - Accuracy plateaus early: Averaged embeddings too coarse, losing sample-level info
  - Training instability: Projection layer poorly initialized or embedding spaces too misaligned
  - Memory usage higher than expected: Storing full teacher model instead of only embeddings
- First 3 experiments:
  1. Verify teacher embedding pre-computation: Run teacher on small subset, check embedding dimensions and per-class averaging
  2. Ablation on sample count: Compare accuracy using 10, 50, 100 samples per class
  3. Projection layer sensitivity: Train with/without projection, with random vs pretrained projection initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective ways to compute class-averaged teacher embeddings beyond simple averaging, and how do these alternative methods impact knowledge distillation performance?
- Basis in paper: [explicit] The paper states "In our future work, we seek to explore alternate ways of computing more faithful representations of these teacher embeddings" and uses simple averaging of 100 samples per class in current experiments
- Why unresolved: The paper only explores simple averaging of teacher embeddings and acknowledges this may result in information loss, but does not investigate more sophisticated aggregation methods
- What evidence would resolve it: Comparative experiments showing performance differences between various embedding aggregation methods (weighted averaging, clustering-based representatives, attention-weighted combinations, etc.) across multiple datasets and model architectures

### Open Question 2
- Question: How does CLIP-Embed-KD scale when applied to billion-parameter teacher models on diverse datasets beyond CIFAR100?
- Basis in paper: [explicit] The paper states "In the future, we seek to evaluate our approach on billion-trillion parameter NLP models and diverse datasets" and only evaluates on CIFAR100 with relatively small teacher models
- Why unresolved: Current experiments are limited to CIFAR100 with base teacher models (12 layers, 768 embedding dim), while the approach is motivated for billion/trillion parameter teachers
- What evidence would resolve it: Systematic scaling studies showing memory usage, training time, and accuracy trade-offs when applying CLIP-Embed-KD to larger datasets (ImageNet, COCO) and larger teacher models (billion-parameter CLIP variants, GPT-style models)

### Open Question 3
- Question: What is the optimal weighting strategy between LCE and Lclip (α1 and α2) for different model architectures and datasets?
- Basis in paper: [explicit] The paper shows Figure 5 analyzing different α2 values for one specific configuration and notes "Non-zero weighting of both losses yield the best performance" but doesn't provide a systematic analysis
- Why unresolved: The paper uses fixed weights (α1 = α2 = 0.5) and only briefly explores sensitivity to α2 in one configuration, without establishing guidelines for choosing optimal weights
- What evidence would resolve it: Comprehensive ablation studies across multiple model architectures, dataset sizes, and complexity levels to determine optimal weight configurations and potentially develop a principled method for weight selection

### Open Question 4
- Question: How does the patch size discrepancy between teacher (16, 32) and student (4) models affect distillation performance and efficiency?
- Basis in paper: [explicit] The paper notes "The teacher models always use an image size of 224 × 224 to generate their outputs (logits or embeddings)" and uses patch sizes 16, 32 for teachers vs 4 for students, but doesn't analyze this design choice
- Why unresolved: The paper uses this configuration without investigating whether matching patch sizes or using different configurations would yield better performance or efficiency
- What evidence would resolve it: Controlled experiments varying patch size combinations between teacher and student models while measuring accuracy, memory usage, and training time to determine optimal patch size relationships

## Limitations

- The method assumes averaged class embeddings capture sufficient semantic information, which may fail when class boundaries are subtle or intra-class variance is high
- Current evaluation is limited to CIFAR100 image classification, with uncertain generalizability to other vision tasks
- Effectiveness depends heavily on the quality of the pre-trained CLIP model and its embedding space alignment with the student model

## Confidence

- **High confidence**: Computational efficiency gains (9× memory reduction, 8× faster training) - these are directly measurable and straightforward to verify
- **Medium confidence**: Accuracy claims - while competitive results are demonstrated, performance may vary significantly across different datasets and model architectures
- **Low confidence**: The general applicability of averaged embeddings across diverse vision tasks - current evaluation is limited to CIFAR-100 image classification

## Next Checks

1. Test CLIP-Embed-KD on datasets with higher intra-class variance (e.g., ImageNet) to verify if averaged embeddings remain effective when class distinctions are more subtle
2. Perform ablation studies varying the number of samples used for teacher embedding computation (5, 10, 50, 100) to identify the minimum sample threshold for maintaining accuracy
3. Evaluate on downstream tasks beyond classification (object detection, semantic segmentation) to assess generalizability of the embedding-based distillation approach