---
ver: rpa2
title: Towards Meta-Pruning via Optimal Transport
arxiv_id: '2402.07839'
source_url: https://arxiv.org/abs/2402.07839
tags:
- group
- pruning
- sparsity
- intra-fusion
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Intra-Fusion, a novel meta-pruning approach
  that challenges the conventional paradigm of simply discarding less important neurons.
  Instead, Intra-Fusion leverages Optimal Transport to incorporate discarded neurons
  into the surviving ones, leading to more accurate sparse model representations.
---

# Towards Meta-Pruning via Optimal Transport

## Quick Facts
- arXiv ID: 2402.07839
- Source URL: https://arxiv.org/abs/2402.07839
- Reference count: 40
- Key outcome: Intra-Fusion achieves substantial accuracy recovery without fine-tuning using Optimal Transport to incorporate discarded neurons

## Executive Summary
This paper introduces Intra-Fusion, a novel meta-pruning approach that challenges the conventional paradigm of simply discarding less important neurons. Instead, Intra-Fusion leverages Optimal Transport to incorporate discarded neurons into the surviving ones, leading to more accurate sparse model representations. The method achieves substantial accuracy recovery without fine-tuning, making it efficient for neural network compression. Intra-Fusion also enables split-data training, significantly decreasing training time while maintaining competitive performance. Across various networks and datasets, Intra-Fusion consistently outperforms traditional pruning methods, especially in data-free scenarios, demonstrating its potential to redefine model compression approaches.

## Method Summary
Intra-Fusion is a meta-pruning approach that uses Optimal Transport to incorporate less important neurons into surviving ones during structured pruning. The method works by first computing importance scores for neuron pairings, then using OT to create a transport map that redistributes mass from discarded neurons to surviving ones. This preserves more information than simple deletion. The approach can be combined with split-data training, where models are trained on disjoint data subsets and then fused, achieving 2× speedup in training time while maintaining accuracy. The method is evaluated on CIFAR-10, CIFAR-100, and ImageNet using VGG11-BN, ResNet18, and ResNet50 models, comparing performance with conventional pruning methods using various importance metrics.

## Key Results
- Intra-Fusion achieves significant accuracy recovery without fine-tuning compared to conventional pruning methods
- The method enables 2× speedup in training time through split-data training while maintaining competitive performance
- Intra-Fusion consistently outperforms traditional pruning methods across multiple networks and datasets, especially in data-free scenarios
- The approach demonstrates better output preservation as sparsity increases compared to standard pruning

## Why This Works (Mechanism)

### Mechanism 1
Intra-Fusion preserves neuron-pairing information by redistributing mass from less important to more important neurons using Optimal Transport. Instead of simply removing low-importance neuron pairings, the method computes a transport map that merges their contributions into the surviving neurons, weighted by their importance. The core assumption is that the importance metric, even if imperfect, still encodes relative utility of neurons, so incorporating "discarded" neurons via weighted fusion does not harm performance.

### Mechanism 2
Intra-Fusion reduces output divergence by maintaining better alignment with the original model's activation space. By fusing rather than deleting, the sparse model's intermediate outputs remain closer to the dense model's outputs, improving accuracy without fine-tuning. The core assumption is that output preservation correlates with retained accuracy; if fused neurons maintain similar activation distributions, classification performance is preserved.

### Mechanism 3
Split-data training with fusion and pruning (FaP/PaF) speeds up training while maintaining accuracy. By training on disjoint subsets of data and fusing later, the method achieves a 2× speedup in training time, with competitive performance after fine-tuning on the full dataset. The core assumption is that models trained on 50% of data converge in roughly the same number of epochs as on the full dataset, so parallelization yields speedup without accuracy penalty.

## Foundational Learning

- Concept: Optimal Transport theory and its application to neural network parameter alignment
  - Why needed here: Intra-Fusion uses OT to compute optimal mass redistribution between neuron pairings, requiring understanding of cost matrices and transport maps
  - Quick check question: Given two sets of neurons with importance scores, what would the cost matrix look like if similarity is measured by normalized ℓ1-distance?

- Concept: Structured pruning dependencies and group-based pruning
  - Why needed here: Pruning is done group-by-group to respect residual connections and other architectural dependencies
  - Quick check question: If a residual block is pruned, which layers must be considered together to avoid breaking the network?

- Concept: Batch Normalization folding into prior layers
  - Why needed here: The method merges BN layers into the preceding layer before fusion to simplify OT computation
  - Quick check question: What are the transformed weights and biases after folding a BN layer with parameters γ, β, µ, σ into a preceding linear layer?

## Architecture Onboarding

- Component map: Dependency graph extractor -> Importance metric calculator -> OT solver -> BN folding module -> Fusion engine -> Split-data pipeline
- Critical path: 1. Extract dependency graph → identify groups 2. Compute importance scores for each neuron pairing 3. Select target neurons (top-m by importance) 4. Build cost matrix and OT problem 5. Solve OT → transport map T 6. Fold BN layers 7. Fuse neurons via T × layer weights 8. (Optional) Fine-tune on full dataset
- Design tradeoffs: Uniform vs importance-informed source/target distributions → marginal accuracy difference; Group size selection → larger groups preserve dependencies but increase OT computation; Number of split-data folds → more folds increase fusion diversity but require more resources
- Failure signatures: Large output divergence after fusion → transport map poorly aligned; Sudden accuracy drop at high sparsity → over-aggregation of dissimilar neurons; No speedup in split-data training → subset data too small or non-representative
- First 3 experiments: 1. Apply Intra-Fusion to a simple ResNet18 group with ℓ1-norm importance, compare output divergence with default pruning 2. Test uniform vs importance-informed distributions on Group 0 of ResNet18 at 30% sparsity 3. Run FaP on CIFAR-10 with k=2 folds, measure training speedup and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How do Intra-Fusion's accuracy gains scale when applied to networks with very different architectures, such as transformers or recurrent networks, compared to CNNs? The paper tests Intra-Fusion on CNNs (VGG11-BN, ResNet18, ResNet50) but does not explore other architectures like transformers or RNNs. The effectiveness of Intra-Fusion may depend on the architecture's specific structure and neuron dependencies. Experimental results showing Intra-Fusion's performance on a diverse set of architectures would clarify its broader applicability.

### Open Question 2
What is the impact of using different Optimal Transport solvers (e.g., Sinkhorn vs. linear programming) on the efficiency and accuracy of Intra-Fusion? The paper mentions using Optimal Transport but does not specify which solver is used or compare different solvers. The choice of OT solver can significantly affect computational efficiency and the quality of the transport map, which in turn impacts Intra-Fusion's performance. Benchmarking Intra-Fusion with various OT solvers and analyzing their impact on accuracy and runtime would provide clarity.

### Open Question 3
How does Intra-Fusion perform in scenarios with highly imbalanced class distributions or noisy labels in the training data? The paper does not address the impact of data quality issues like class imbalance or label noise on Intra-Fusion's performance. The effectiveness of Intra-Fusion might be influenced by the quality and distribution of the training data, which is not explored in the experiments. Testing Intra-Fusion on datasets with varying levels of class imbalance and label noise would reveal its robustness to such conditions.

## Limitations

- The effectiveness of Intra-Fusion may depend heavily on the quality of the importance metric, with poor metrics potentially amplifying errors through the OT fusion process
- Split-data training speedup claims assume that 50% data subsets require similar training epochs as full datasets, which may not hold for highly imbalanced or domain-specific data
- The method's performance on architectures beyond CNNs remains unexplored, limiting understanding of its general applicability

## Confidence

- **High Confidence**: Output divergence preservation claims (supported by internal experiments)
- **Medium Confidence**: Accuracy recovery without fine-tuning (empirical but limited ablation)
- **Low Confidence**: Split-data training speedup claims (based on runtime measurements only)

## Next Checks

1. Perform ablation studies varying the importance metric quality (e.g., random vs learned importance) to quantify sensitivity of OT fusion performance
2. Test split-data training with progressively smaller data subsets to identify the minimum viable training set size for the claimed speedup
3. Evaluate model stability under distribution shift after fusion to assess generalization robustness