---
ver: rpa2
title: Understanding How CodeLLMs (Mis)Predict Types with Activation Steering
arxiv_id: '2404.01903'
source_url: https://arxiv.org/abs/2404.01903
tags:
- steering
- type
- code
- types
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel activation steering approach to improve
  type prediction robustness in CodeLLMs. The method constructs steering vectors from
  semantics-preserving code edits that cause model mispredictions, inspired by mutation
  testing.
---

# Understanding How CodeLLMs (Mis)Predict Types with Activation Steering

## Quick Facts
- **arXiv ID**: 2404.01903
- **Source URL**: https://arxiv.org/abs/2404.01903
- **Reference count**: 19
- **Primary result**: Activation steering improves type prediction accuracy up to 90% on adversarial examples by transforming model activations toward shared type representations.

## Executive Summary
This paper presents a novel activation steering approach to improve type prediction robustness in CodeLLMs. The method constructs steering vectors from semantics-preserving code edits that cause model mispredictions, inspired by mutation testing. By applying these vectors to model activations, accuracy improves up to 90% on adversarial examples. The approach generalizes across unseen programs and programming languages, with TypeScript steering vectors effectively correcting Python mispredictions and vice versa. This cross-language transfer suggests CodeLLMs learn a shared representation of types. Steering outperforms random vectors and achieves similar results to fine-tuning without degrading other model abilities like code completion.

## Method Summary
The method constructs steering vectors by collecting activation differences between correctly predicted types and mispredicted types after semantics-preserving edits. These vectors are computed as mean differences between positive (correct prediction) and negative (incorrect prediction) activation pairs at specific layers. Steering vectors are applied to the residual stream at the FIM middle token to guide the model toward correct type predictions. The approach is tested on Python and TypeScript using StarCoderBase models, with evaluation on held-out adversarial examples constructed from semantics-preserving edits like variable renaming and type annotation removal.

## Key Results
- Steering vectors improve type prediction accuracy up to 90% on adversarial examples
- TypeScript steering vectors effectively correct Python mispredictions and vice versa
- Steering outperforms random vectors and achieves similar results to fine-tuning without degrading code completion ability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Activation steering corrects type mispredictions by transforming model activations toward a shared latent representation of types that is invariant across programming languages.
- Mechanism: The steering vector is constructed as the mean difference between activations of correctly predicted types and mispredicted types after semantics-preserving edits. Applying this vector shifts the residual stream at the FIM middle token toward the correct type prediction by modifying the underlying task representation.
- Core assumption: Type prediction relies on a task vector in the activation space, and this vector can be linearly transformed using steering vectors without disrupting other model capabilities.
- Evidence anchors:
  - [abstract] "By using activation steering, a method that manipulates a model's internal activations to guide it toward using latent knowledge, we restore accurate predictions on adversarial inputs."
  - [section] "The intuition behind equation 1 is that the distance between positive and negative pairs in activation space encodes the transformation for steering towards the correct type."
  - [corpus] Weak - no direct evidence about cross-language shared representations in the corpus.
- Break condition: If the model does not rely on a linear representation of types, or if the steering vector disrupts other essential model behaviors like code completion.

### Mechanism 2
- Claim: The effectiveness of steering vectors is due to their ability to counteract distractors that shift the prompt outside the training distribution.
- Mechanism: Semantics-preserving edits (e.g., variable renaming) remove important in-context clues like function names, causing the model to mispredict types. Steering vectors realign the activations to restore the correct task representation.
- Core assumption: The model's type prediction relies on specific textual cues that are removed by the edits, and these cues are encoded in the activation space.
- Evidence anchors:
  - [section] "We hypothesize that mispredictions happen because the model fails to identify the inference task as a type prediction task. This likely occurs because our edits shift the prompt outside the distribution of a model's training data."
  - [section] "We carefully construct steering vectors that narrowly target particular types of program edits."
  - [corpus] No direct evidence - corpus does not discuss model training distribution or prompt distribution shifts.
- Break condition: If the model does not rely on specific textual cues for type prediction, or if the edits do not significantly shift the prompt distribution.

### Mechanism 3
- Claim: Steering vectors generalize well because they target a high-level representation of types that is shared across programming languages.
- Mechanism: The steering vectors constructed from TypeScript data are effective at correcting Python type mispredictions and vice versa, indicating a shared latent representation of types.
- Core assumption: CodeLLMs learn a unified representation of types that is independent of the specific syntax of the programming language.
- Evidence anchors:
  - [abstract] "We show that steering successfully activates a type prediction mechanism that is shared by both Python and TypeScript."
  - [section] "We conduct a careful analysis and conclude that it is likely that multi-lingual CodeLLMs are learning a representation of types that is shared across the programming languages we study."
  - [corpus] Weak - corpus does not provide direct evidence about cross-language type representations.
- Break condition: If the model does not learn a shared representation of types, or if the steering vectors are not effective across languages.

## Foundational Learning

- Concept: Linear representation hypothesis in transformer models
  - Why needed here: The steering method relies on the assumption that concepts are represented as directions in the embedding space, allowing linear transformations to modify model behavior.
  - Quick check question: What is the linear representation hypothesis, and how does it apply to activation steering in CodeLLMs?

- Concept: Activation patching and task vectors
  - Why needed here: Steering vectors are constructed using activation patching techniques, and they are hypothesized to modify task vectors in the model's activation space.
  - Quick check question: How does activation patching work, and what is a task vector in the context of transformer models?

- Concept: Semantics-preserving code edits and mutation testing
  - Why needed here: The steering dataset is constructed using semantics-preserving code edits, inspired by mutation testing but applied to create adversarial examples for type prediction.
  - Quick check question: What are semantics-preserving code edits, and how do they differ from the edits used in mutation testing?

## Architecture Onboarding

- Component map: Transformer decoder with fill-in-the-middle (FIM) training where special tokens demarcate prefix, middle, and suffix. Type prediction is formulated as predicting type annotation at FIM middle token conditioned on prefix and suffix.
- Critical path: Residual stream at FIM middle token where model builds latent representations of types. Steering vectors are applied to this stream to modify type prediction.
- Design tradeoffs: Steering trades effectiveness against potential disruption of other model capabilities. Fine-tuning could be more effective but requires specialized models and may degrade code completion ability.
- Failure signatures: Steering vectors not effective (incorrect layer selection or poor quality steering pairs); steering degrades other model abilities (vectors too aggressive).
- First 3 experiments:
  1. Evaluate steering vector effectiveness on held-out evaluation set for each type of semantics-preserving code edit (variable renaming, type renaming, type annotation removal).
  2. Compare accuracy of TypeScript steering vectors applied to Python type predictions and vice versa.
  3. Test robustness by applying steering vectors to programs used to construct the vectors themselves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do steering vectors for type prediction transfer across programming languages beyond Python and TypeScript?
- Basis in paper: [explicit] The paper demonstrates that TypeScript steering vectors effectively correct Python mispredictions and vice versa, suggesting a shared representation of types across these languages.
- Why unresolved: The paper only evaluates steering vector transfer between Python and TypeScript. It is unclear if this transfer generalizes to other programming languages.
- What evidence would resolve it: Conduct experiments using steering vectors from Python and TypeScript to correct type mispredictions in other programming languages (e.g., Java, JavaScript, Go). Measure the accuracy of steering across these languages to determine if a shared representation of types exists.

### Open Question 2
- Question: How do steering vectors for type prediction affect other model capabilities beyond code completion?
- Basis in paper: [explicit] The paper mentions that fine-tuning on type prediction significantly degrades the model's code completion ability, but steering does not have this effect. However, it is unclear if steering vectors impact other model capabilities.
- Why unresolved: The paper only evaluates the impact of steering vectors on code completion. It is unknown how steering affects other model capabilities, such as code generation, explanation, or test generation.
- What evidence would resolve it: Conduct experiments to evaluate the impact of steering vectors on various model capabilities beyond code completion. Compare the performance of steered models to baseline models on tasks such as code generation, explanation, and test generation.

### Open Question 3
- Question: What is the underlying causal mechanism in CodeLLMs responsible for type prediction?
- Basis in paper: [inferred] The paper demonstrates that steering vectors can effectively correct type mispredictions, suggesting that CodeLLMs learn a representation of types. However, the specific mechanism by which this representation is learned and used for type prediction is not fully understood.
- Why unresolved: The paper focuses on the application of steering vectors for type prediction but does not delve into the underlying causal mechanism. Understanding this mechanism could provide insights into how CodeLLMs learn and represent types.
- What evidence would resolve it: Conduct further research using techniques such as mechanistic interpretability to analyze the internal representations and mechanisms used by CodeLLMs for type prediction. Identify the specific components and processes involved in type prediction and how they are affected by steering vectors.

## Limitations

- Cross-language transfer evidence is weak, with limited empirical support from small evaluation sets
- Steering vectors' robustness is unverified beyond the same dataset distribution
- Mechanism remains largely hypothetical with limited evidence for specific activation space modifications

## Confidence

- **High confidence**: Core methodology of constructing steering vectors from activation differences is well-specified and technically sound
- **Medium confidence**: Empirical results showing accuracy improvements are supported by data, but generalizability claims need more rigorous validation
- **Low confidence**: Claims about shared type representations across programming languages and preservation of other model capabilities are speculative

## Next Checks

1. **Cross-language generalization test**: Construct larger evaluation set with programs from both Python and TypeScript, then test whether steering vectors trained on one language maintain effectiveness when transferred to the other. Include programs with mixed-language type patterns to stress-test the shared representation hypothesis.

2. **Mechanism validation through ablation**: Systematically test the steering vector construction by ablating different components: use random vectors of similar magnitude, use vectors constructed from non-semantics-preserving edits, and test steering at different layers. This would clarify whether the specific activation differences captured are necessary for the observed improvements.

3. **Capability preservation evaluation**: Design comprehensive test suite that evaluates model performance on multiple tasks (code completion, type prediction, and general code generation) before and after steering. This would empirically validate the claim that steering preserves other model abilities while improving type prediction.