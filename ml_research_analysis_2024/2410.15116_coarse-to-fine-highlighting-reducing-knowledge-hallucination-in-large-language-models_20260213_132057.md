---
ver: rpa2
title: 'Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language
  Models'
arxiv_id: '2410.15116'
source_url: https://arxiv.org/abs/2410.15116
tags:
- coft
- arxiv
- score
- knowledge
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces COFT, a novel coarse-to-fine highlighting
  method designed to reduce knowledge hallucination in large language models (LLMs).
  The method focuses on key lexical units in lengthy contexts by integrating three
  components: a recaller that extracts potential key entities using a knowledge graph,
  a scorer that measures the importance of each entity through contextual weights,
  and a selector that dynamically highlights key texts at different granularity levels
  (paragraph, sentence, or word).'
---

# Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models

## Quick Facts
- arXiv ID: 2410.15116
- Source URL: https://arxiv.org/abs/2410.15116
- Reference count: 40
- Primary result: COFT improves knowledge hallucination detection F1 score by 32.1% average over baselines

## Executive Summary
This paper introduces COFT (Coarse-to-Fine highlighTing), a novel method to reduce knowledge hallucination in large language models by dynamically highlighting key lexical units in lengthy contexts. The approach addresses the challenge of LLMs generating plausible but incorrect information by focusing attention on the most relevant portions of reference contexts. COFT achieves significant improvements across multiple benchmarks, demonstrating its effectiveness for both knowledge verification and general long-form tasks like reading comprehension and question answering.

## Method Summary
COFT employs a three-component framework: a recaller that extracts potential key entities from the query using named entity recognition and enriches them with one-hop neighbors from Wikidata, a scorer that calculates contextual importance using TF-ISF and self-information metrics, and a selector that dynamically thresholds and highlights entities at varying granularity levels (paragraph, sentence, or word). The method uses a small language model (LLaMA-7B) to compute self-information scores, which are combined with TF-ISF weights to determine entity importance. Dynamic thresholding adapts to context length and informativeness, enabling efficient highlighting of the most relevant content for LLM processing.

## Key Results
- Achieves 32.1% average improvement in F1 score on knowledge hallucination benchmark (FELM) compared to existing methods
- Improves reading comprehension precision by 4.6% on RACE-H/M dataset
- Increases question answering performance by up to 10.5% in F1 score on Natural Questions, TriviaQA, and WebQ datasets

## Why This Works (Mechanism)
COFT addresses knowledge hallucination by systematically identifying and highlighting the most relevant lexical units in lengthy contexts before LLM processing. The method leverages a knowledge graph to enrich entity extraction beyond simple named entity recognition, capturing semantically related concepts that might otherwise be missed. By combining statistical measures (TF-ISF) with model-based self-information scoring, COFT creates a more robust measure of contextual importance that accounts for both term frequency and semantic relevance. The coarse-to-fine granularity selection allows the method to adapt to different context structures, highlighting entire paragraphs when appropriate or drilling down to specific sentences or words when precision is needed.

## Foundational Learning
- **Named Entity Recognition (NER)**: Extracts key terms from text that represent important concepts, people, or locations
  - Why needed: Identifies the atomic units of meaning that serve as anchors for knowledge retrieval
  - Quick check: Verify NER output contains expected entities from sample text

- **Knowledge Graph Retrieval**: Uses Wikidata to find related entities through one-hop neighbor connections
  - Why needed: Expands entity set beyond explicit mentions to capture implicit relationships
  - Quick check: Confirm retrieved neighbors are semantically related to original entities

- **TF-ISF Scoring**: Calculates term frequency-inverse sentence frequency to measure entity importance within context
  - Why needed: Identifies entities that are both frequent and distinctive within the document
  - Quick check: Validate TF-ISF scores decrease for common words and increase for topic-specific terms

- **Self-Information Scoring**: Uses language model probabilities to assess how informative an entity is given the context
  - Why needed: Captures semantic relevance beyond simple frequency statistics
  - Quick check: Verify lower probability entities receive higher self-information scores

- **Dynamic Thresholding**: Adapts highlighting sensitivity based on context length and informativeness
  - Why needed: Balances between including enough information and avoiding noise
  - Quick check: Test threshold behavior on contexts of varying lengths

- **Granularity Selection**: Determines whether to highlight at paragraph, sentence, or word level
  - Why needed: Optimizes highlighting precision for different context structures
  - Quick check: Confirm appropriate granularity selection for different document types

## Architecture Onboarding

**Component Map**: Query -> NER + KG Retrieval -> Entity Set -> TF-ISF + Self-Information Scoring -> Contextual Weight -> Dynamic Threshold -> Highlight Selection -> Highlighted Context

**Critical Path**: Entity extraction (NER + KG) → Contextual weighting (TF-ISF × Self-information) → Dynamic thresholding → Granularity-based highlighting

**Design Tradeoffs**: 
- Uses Wikidata for entity enrichment vs. potential domain-specific KGs (better coverage vs. broader applicability)
- Combines TF-ISF with self-information vs. single metric (more robust weighting vs. computational overhead)
- Dynamic thresholding vs. fixed threshold (adaptability vs. predictability)

**Failure Signatures**:
- Poor entity extraction leading to missing key terms
- Incorrect contextual weight calculation due to improper self-information scoring
- Inappropriate granularity selection resulting in over/under-highlighting

**First Experiments**:
1. Test entity extraction accuracy on sample texts with known entities
2. Verify contextual weight calculation by comparing scores across different entity types
3. Validate dynamic threshold behavior on contexts of varying lengths and informativeness

## Open Questions the Paper Calls Out

**Open Question 1**: How does COFT's dynamic threshold algorithm adapt to contexts with varying levels of informativeness and length, and could alternative weighting methods (e.g., incorporating entity relevance scores from LLMs) further improve performance?
- Basis: The paper mentions the dynamic threshold considers both length and informativeness but lacks detailed analysis of adaptation mechanisms or exploration of alternative weighting methods.

**Open Question 2**: What is the impact of using different knowledge graphs or expanding the retrieval beyond one-hop neighbors on COFT's performance, and how does the choice of KG affect the quality of extracted key entities?
- Basis: The paper uses Wikidata with one-hop retrieval but does not investigate how alternative KGs or deeper neighbor retrieval affect performance.

**Open Question 3**: How does COFT's performance scale with the size and complexity of the input context, and are there limitations to its effectiveness in extremely long or highly complex documents?
- Basis: While the paper demonstrates effectiveness on various benchmarks, it does not explore performance scaling with context size or complexity.

## Limitations
- Heavy reliance on Wikidata coverage may limit performance on specialized or emerging topics
- Computational overhead of running LLaMA-7B for self-information scoring on large contexts
- Dynamic threshold implementation details are underspecified, potentially affecting reproducibility

## Confidence
- **High confidence**: Core methodology and benchmark results are clearly specified and reproducible
- **Medium confidence**: Generalizability to domains outside tested benchmarks is reasonable but unverified
- **Low confidence**: Scalability to extremely long documents (>10,000 words) is not evaluated

## Next Checks
1. Replicate the knowledge hallucination benchmark on FELM dataset to verify the reported 32.1% F1 improvement
2. Test COFT on domain-specific texts (medical or legal documents) to assess KG coverage limitations
3. Benchmark computational efficiency on documents of increasing length (1K, 5K, 10K words) to establish scalability limits