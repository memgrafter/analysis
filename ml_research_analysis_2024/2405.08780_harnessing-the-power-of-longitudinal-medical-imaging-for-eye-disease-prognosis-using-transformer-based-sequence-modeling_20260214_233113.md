---
ver: rpa2
title: Harnessing the power of longitudinal medical imaging for eye disease prognosis
  using Transformer-based sequence modeling
arxiv_id: '2405.08780'
source_url: https://arxiv.org/abs/2405.08780
tags:
- time
- ltsa
- longitudinal
- disease
- prognosis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a Transformer-based method for dynamic disease
  prognosis from longitudinal medical imaging, addressing the limitations of existing
  image classification approaches that only assess disease presence at a single time
  point. The proposed Longitudinal Transformer for Survival Analysis (LTSA) models
  time-to-disease from sequences of fundus photography images captured over long,
  irregular time periods.
---

# Harnessing the power of longitudinal medical imaging for eye disease prognosis using Transformer-based sequence modeling

## Quick Facts
- arXiv ID: 2405.08780
- Source URL: https://arxiv.org/abs/2405.08780
- Authors: Gregory Holste, Mingquan Lin, Ruiwen Zhou, Fei Wang, Lei Liu, Qi Yan, Sarah H. Van Tassel, Kyle Kovacs, Emily Y. Chew, Zhiyong Lu, Zhangyang Wang, Yifan Peng
- Reference count: 40
- Primary result: Transformer-based sequence modeling significantly improves eye disease prognosis using longitudinal imaging data

## Executive Summary
This study introduces a Transformer-based method for dynamic disease prognosis from longitudinal medical imaging, addressing the limitations of existing image classification approaches that only assess disease presence at a single time point. The proposed Longitudinal Transformer for Survival Analysis (LTSA) models time-to-disease from sequences of fundus photography images captured over long, irregular time periods. The method uses temporal positional encoding to account for irregular imaging intervals and a Transformer encoder with causal attention to learn temporal associations across longitudinal images.

## Method Summary
The method introduces Longitudinal Transformer for Survival Analysis (LTSA), a Transformer-based sequence modeling approach for eye disease prognosis using longitudinal fundus photography. LTSA incorporates a temporal positional encoder to handle irregular imaging intervals and employs a Transformer encoder with causal attention to capture temporal dependencies across multiple time points. The model is validated on large-scale datasets for age-related macular degeneration (AMD) and primary open-angle glaucoma (POAG) prognosis, comparing its performance against single-image baselines using C-index metrics.

## Key Results
- LTSA achieved significantly better performance than single-image baseline in 19/20 late AMD comparisons (mean C-index: 0.907 vs. 0.884)
- LTSA outperformed single-image baseline in 18/20 POAG comparisons (mean C-index: 0.911 vs. 0.866)
- Temporal attention analysis showed most recent images are typically most influential, but prior imaging provides additional prognostic value

## Why This Works (Mechanism)
The method leverages the temporal progression information captured in longitudinal imaging sequences. By using Transformers with causal attention and temporal positional encoding, the model can capture complex temporal dependencies and irregular time intervals between images, which are critical for understanding disease progression and making accurate prognostic predictions.

## Foundational Learning
1. **C-index metric**: A measure of concordance between predicted and observed survival times, ranging from 0.5 (random) to 1.0 (perfect prediction). Why needed: To evaluate survival analysis model performance in medical prognosis tasks.
2. **Transformer architecture**: A neural network architecture using self-attention mechanisms to capture relationships in sequential data. Why needed: To model complex temporal dependencies across multiple time points in longitudinal imaging.
3. **Temporal positional encoding**: A technique to incorporate time information into sequence models, particularly important for irregularly spaced observations. Why needed: To account for varying time intervals between medical imaging captures.
4. **Causal attention**: A mechanism that restricts attention to past and present time steps, preventing information leakage from future to past. Why needed: To ensure the model makes predictions based only on available historical information.
5. **Survival analysis**: Statistical methods for analyzing time-to-event data, commonly used in medical prognosis. Why needed: To model disease progression and predict time-to-disease onset.

## Architecture Onboarding

**Component Map**: Input images -> Temporal Positional Encoder -> Transformer Encoder -> Survival Prediction

**Critical Path**: The temporal positional encoder ensures irregular time intervals are properly accounted for before the Transformer encoder processes the sequence, making it essential for the model's effectiveness.

**Design Tradeoffs**: The choice of causal attention over bidirectional attention ensures the model doesn't use future information for predictions, maintaining clinical relevance but potentially limiting context understanding.

**Failure Signatures**: Poor performance might occur if the temporal positional encoding doesn't adequately capture the irregular time intervals, or if the Transformer encoder fails to learn meaningful temporal dependencies in the imaging sequences.

**First Experiments**:
1. Validate temporal positional encoding effectiveness by comparing performance with and without this component
2. Test different attention mechanisms (causal vs. bidirectional) to confirm the choice of causal attention
3. Evaluate model performance on varying sequence lengths to determine optimal temporal context

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on retrospective data from single healthcare systems, limiting generalizability to other populations
- Model performance across different types of irregular patterns remains to be validated
- Interpretability of attention findings for clinical decision-making requires further investigation

## Confidence
- High confidence: The core methodology (LTSA with temporal positional encoding and causal attention) is technically sound and the performance improvements over single-image baselines are statistically robust
- Medium confidence: The clinical applicability of the model across diverse healthcare settings and patient populations
- Medium confidence: The generalizability of findings to other eye diseases or medical imaging modalities

## Next Checks
1. External validation on multi-center datasets from different geographic regions and healthcare systems to assess model generalizability
2. Prospective clinical trial to evaluate the model's performance in real-world settings and its impact on clinical decision-making
3. Extension of the method to other chronic diseases with available longitudinal imaging data to assess broader applicability