---
ver: rpa2
title: Ads Recommendation in a Collapsed and Entangled World
arxiv_id: '2403.00793'
source_url: https://arxiv.org/abs/2403.00793
tags:
- embedding
- learning
- feature
- embeddings
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two key challenges in large-scale ad recommendation
  systems: embedding dimensional collapse and interest entanglement across multiple
  tasks. To tackle these issues, the authors propose a multi-embedding paradigm that
  learns multiple embeddings for each feature, preventing dimensional collapse by
  allowing features to interact with their own embeddings instead of others''.'
---

# Ads Recommendation in a Collapsed and Entangled World

## Quick Facts
- arXiv ID: 2403.00793
- Source URL: https://arxiv.org/abs/2403.00793
- Authors: Junwei Pan; Wei Xue; Ximei Wang; Haibin Yu; Xun Liu; Shijie Quan; Xueming Qiu; Dapeng Liu; Lei Xiao; Jie Jiang
- Reference count: 40
- One-line primary result: Multi-embedding paradigm achieves 0.32-0.48% AUC lifts translating to 4.2-7.1% GMV lifts in online A/B tests

## Executive Summary
This paper addresses two critical challenges in large-scale ad recommendation systems: embedding dimensional collapse and interest entanglement across multiple tasks. The authors propose a multi-embedding paradigm that learns multiple embeddings for each feature, preventing dimensional collapse by allowing features to interact within their own embedding tables. They also introduce Asymmetric Multi-Embedding (AME) and Shared and Task-specific Embedding (STEM) approaches to disentangle user interests across tasks. These methods are deployed in Tencent's ad recommendation platform, achieving significant performance improvements in both offline metrics and online A/B tests.

## Method Summary
The authors propose a multi-embedding paradigm that learns multiple embeddings for each feature to prevent dimensional collapse. Instead of a single shared embedding table, features are mapped to multiple tables with feature interactions occurring only within the same table. For multi-task learning, they introduce AME with asymmetric embedding table sizes and STEM with task-specific embeddings. The methods are implemented in Tencent's production system using a Heterogeneous Mixture-of-Experts architecture with specialized feature encoding modules including TIM for sequences, MNSE for numerics, and Similarity Encoding for pre-trained embeddings.

## Key Results
- AME with three embedding tables of sizes 16, 32, and 64 achieves 0.32%, 0.24%, and 0.48% average AUC lifts in three representative scenarios
- These improvements translate to 4.2%, 3.9%, and 7.1% GMV lift in online A/B tests
- The proposed analysis tools effectively study feature correlation, dimensional collapse, and interest entanglement
- The methods are deployed in Tencent's ad recommendation platform with measurable performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-embedding paradigm prevents dimensional collapse by allowing features to interact within their own embedding tables.
- Mechanism: Instead of a single shared embedding table, features are mapped to multiple tables. Feature interactions occur only within the same table, preventing high-cardinality features from forcing low-cardinality features to span unnecessary dimensions.
- Core assumption: The root cause of dimensional collapse is the explicit interaction between features of vastly different cardinalities.
- Evidence anchors:
  - [section]: "We study the root cause of the dimensional collapse and find it's due to the explicit feature interaction module, namely, fields with collapsed dimension make the embeddings of other fields collapse."
  - [abstract]: "The dimensional collapse of embeddings and the interest entanglement across different tasks or scenarios. We propose several practical approaches to address these challenges that result in robust and disentangled recommendation representations."
- Break condition: If the feature interaction function still mixes embeddings from different tables or if cardinality differences are not the primary cause of collapse.

### Mechanism 2
- Claim: Asymmetric embedding table sizes in AME disentangle task-specific user interests by routing tasks to appropriately sized tables.
- Mechanism: Embedding tables of different sizes (e.g., 16, 32, 64) are used. The gating mechanism routes tasks based on data volume and model capacity needs, ensuring small tasks use smaller tables and large tasks use larger tables, preventing task interference.
- Core assumption: Tasks with different data volumes have different model capacity requirements, and symmetric embedding sizes cause task interference.
- Evidence anchors:
  - [section]: "We set different embedding sizes for these embedding tables to disentangle them, leading to an Asymmetric Multi-Embedding paradigm...These embedding tables are disentangled in the sense that small tasks with fewer data need less model capacity and are routed more to the small-size embedding tables via the gating."
  - [abstract]: "We propose several practical approaches to address these challenges that result in robust and disentangled recommendation representations."
- Break condition: If the gating mechanism fails to route tasks appropriately or if task interference persists despite different embedding sizes.

### Mechanism 3
- Claim: STEM disentangles user interests across tasks by using task-specific embeddings alongside shared embeddings.
- Mechanism: Each task has its own task-specific embedding in addition to a shared embedding. Task-specific experts receive gradients from only their corresponding task, while shared experts receive gradients from all tasks, preserving task-specific user interests.
- Core assumption: Shared embeddings across tasks cause interest entanglement because they cannot capture distinct user interests for different tasks.
- Evidence anchors:
  - [section]: "To tackle such an interest entanglement issue, we adopt a Shared and Task-specific EMbedding (STEM) paradigm...The task-specific embeddings disentangle user and item representation (embeddings) across tasks, making preserving the distinct user interest in different tasks possible."
  - [abstract]: "The interest entanglement across different tasks or scenarios. We propose several practical approaches to address these challenges that result in robust and disentangled recommendation representations."
- Break condition: If task-specific embeddings do not significantly improve task performance or if shared embeddings still dominate the learned representations.

## Foundational Learning

- Concept: Dimensionality reduction and singular value decomposition (SVD)
  - Why needed here: Understanding how embeddings can collapse into lower-dimensional subspaces is crucial for diagnosing and addressing the dimensional collapse problem.
  - Quick check question: If an embedding matrix has a rank of 5 but a dimension size of 64, what does this imply about the embedding space utilization?

- Concept: Multi-task learning and negative transfer
  - Why needed here: The paper addresses interest entanglement in multi-task learning, where shared embeddings can lead to negative transfer between tasks.
  - Quick check question: In a multi-task learning setup, what is negative transfer, and how can it affect model performance?

- Concept: Feature interaction mechanisms (FM, FFM, DCN)
  - Why needed here: The paper discusses various feature interaction methods and their role in dimensional collapse and model performance.
  - Quick check question: How does the feature interaction mechanism in DCN V2 differ from that in FM, and why might this difference be important for preventing dimensional collapse?

## Architecture Onboarding

- Component map: Feature Encoding -> Multi-Embedding Lookup -> Experts -> Classification Towers
- Critical path: Feature encoding → Multi-embedding lookup → Feature interaction (experts) → Classification towers → Prediction output
- Design tradeoffs:
  - Memory vs. Performance: Using multiple embedding tables increases memory usage but improves performance by preventing dimensional collapse
  - Task-Specific vs. Shared Embeddings: STEM uses task-specific embeddings to disentangle interests but increases model complexity
  - Embedding Table Size: AME uses different table sizes to optimize resource allocation but adds complexity to the routing mechanism
- Failure signatures:
  - Dimensional Collapse: Singular values of embedding matrices are small, indicating low-rank embeddings
  - Interest Entanglement: Performance degradation in multi-task learning due to negative transfer between tasks
  - Gating Failure: Tasks are not routed to appropriate embedding tables, leading to suboptimal performance
- First 3 experiments:
  1. Measure singular values of embedding matrices before and after implementing multi-embedding to quantify dimensional collapse reduction
  2. Compare task performance in STEM vs. shared-embedding MTL to validate interest disentanglement
  3. Test different embedding table sizes in AME to find the optimal configuration for task routing and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the multi-embedding paradigm be further optimized for extremely large-scale recommendation systems with millions of features and billions of parameters?
- Basis in paper: [explicit] The paper discusses the effectiveness of the multi-embedding paradigm in mitigating dimensional collapse, but doesn't explore its scalability limits or optimization strategies for extremely large-scale systems.
- Why unresolved: Scaling up recommendation models to handle massive feature sets and parameter counts presents computational and optimization challenges that are not fully addressed.
- What evidence would resolve it: Experimental results comparing the performance and efficiency of multi-embedding approaches across a wide range of system scales, along with analysis of computational bottlenecks and optimization techniques.

### Open Question 2
- Question: What are the long-term effects of interest entanglement and disentanglement on user behavior and satisfaction in recommendation systems?
- Basis in paper: [inferred] The paper discusses interest entanglement as a challenge in multi-task learning and proposes solutions like STEM and AME, but doesn't explore the long-term implications of these approaches on user behavior or satisfaction.
- Why unresolved: Understanding how interest entanglement and its mitigation strategies impact user behavior over time is crucial for developing effective and user-friendly recommendation systems.
- What evidence would resolve it: Longitudinal studies tracking user engagement, satisfaction, and behavior patterns across different recommendation approaches, including those with and without interest disentanglement.

### Open Question 3
- Question: How can the proposed analysis tools be extended to provide more granular insights into feature correlations, dimensional collapse, and interest entanglement?
- Basis in paper: [explicit] The paper introduces three analysis tools for studying feature correlation, dimensional collapse, and interest entanglement, but doesn't explore their potential for providing more detailed or specialized insights.
- Why unresolved: The current analysis tools provide a high-level view of these phenomena, but more granular insights could lead to better understanding and more targeted improvements in recommendation systems.
- What evidence would resolve it: Development and application of enhanced analysis tools that can provide more detailed visualizations, statistical measures, or predictive insights related to feature correlations, dimensional collapse, and interest entanglement.

## Limitations
- Empirical evidence is primarily based on A/B testing within Tencent's production environment, limiting external validation
- The analysis tools for studying feature correlation, dimensional collapse, and interest entanglement are introduced but not extensively validated across diverse datasets
- Results are specific to Tencent's ad recommendation system and may not generalize to other platforms with different user behaviors, item distributions, or task configurations

## Confidence

**High Confidence**: The multi-embedding paradigm effectively prevents dimensional collapse by allowing features to interact within their own embedding tables. This is supported by the observation that high-cardinality features forcing low-cardinality features to span unnecessary dimensions is the root cause of collapse, and the proposed solution directly addresses this mechanism.

**Medium Confidence**: The asymmetric embedding table sizes in AME successfully disentangle task-specific user interests by routing tasks based on data volume and model capacity needs. While the theoretical justification is sound and the reported improvements are significant, the exact routing mechanism and its effectiveness across different task configurations could benefit from more extensive ablation studies.

**Medium Confidence**: STEM effectively disentangles user interests across tasks by using task-specific embeddings alongside shared embeddings. The mechanism of preserving task-specific gradients while maintaining shared knowledge is plausible, though the relative importance of task-specific versus shared embeddings in the final representations is not fully explored.

## Next Checks

1. **Dimensional Collapse Validation**: Conduct a controlled experiment on a public dataset (e.g., Criteo or Avazu) comparing singular value distributions of embeddings before and after implementing the multi-embedding paradigm. Measure the Information Abundance (IA) metric to quantify collapse reduction and verify that the improvement correlates with the theoretical mechanism.

2. **Cross-Platform Generalization**: Implement AME and STEM on a different ad recommendation platform or public dataset with multiple prediction tasks. Compare performance against standard multi-task learning baselines and analyze whether the task routing and disentanglement benefits observed at Tencent extend to this new context.

3. **Routing Mechanism Sensitivity**: Perform an ablation study on the AME gating mechanism by varying the number of embedding tables (e.g., 2, 3, 4) and their size configurations. Measure the impact on task performance, memory usage, and routing accuracy to determine the optimal configuration and assess the robustness of the routing strategy.