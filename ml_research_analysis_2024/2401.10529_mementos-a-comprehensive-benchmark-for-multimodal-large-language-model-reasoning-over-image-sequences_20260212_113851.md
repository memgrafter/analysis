---
ver: rpa2
title: 'Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning
  over Image Sequences'
arxiv_id: '2401.10529'
source_url: https://arxiv.org/abs/2401.10529
tags:
- image
- mllms
- hallucination
- object
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mementos, a benchmark designed to evaluate
  the reasoning capabilities of multimodal large language models (MLLMs) over image
  sequences. Mementos features 4,761 diverse image sequences with varying lengths
  across daily-life, robotics, and comics domains.
---

# Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences

## Quick Facts
- arXiv ID: 2401.10529
- Source URL: https://arxiv.org/abs/2401.10529
- Reference count: 30
- Primary result: MLLMs struggle with image sequence reasoning, exhibiting significant object and behavioral hallucinations across three domains

## Executive Summary
This paper introduces Mementos, a benchmark designed to evaluate multimodal large language models' (MLLMs) reasoning capabilities over image sequences. The benchmark comprises 4,761 diverse image sequences across daily-life, robotics, and comics domains, with human-annotated descriptions of objects and behaviors. Through evaluation of nine recent MLLMs using a GPT-4-assisted procedure, the study reveals that MLLMs struggle with accurately describing dynamic information in image sequences, often producing hallucinations and misrepresentations of objects and behaviors. The analysis identifies three key factors contributing to these failures: correlation between object and behavioral hallucinations, influence of co-occurring behaviors, and the compounding impact of behavioral hallucinations.

## Method Summary
The Mementos benchmark employs a GPT-4-assisted evaluation procedure where AI-generated and human-annotated descriptions of image sequences are processed to extract object and behavior keywords. These keywords are then matched using domain-specific synonym graphs to calculate Recall, Precision, and F1 metrics at both object and behavior levels. The evaluation is conducted across nine recent MLLMs including GPT-4V and Gemini, with results analyzed to identify factors contributing to reasoning failures and hallucinations.

## Key Results
- MLLMs show significant difficulty in accurately describing dynamic information in image sequences, with substantial hallucinations of objects and behaviors
- Correlation coefficients between object and behavioral hallucinations range from 0.1 to 0.4 across different domains, indicating a weak yet present relationship
- The snowball effect causes behavioral and object hallucinations to become increasingly pronounced as image sequences progress
- MLLMs demonstrate a tendency to generate behaviors commonly paired together, even when not present in the specific sequence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Behavioral hallucinations in MLLMs are strongly correlated with object hallucinations, meaning incorrect object identification leads to subsequent inaccuracies in behavior identification.
- **Mechanism**: When MLLMs misidentify an object in an image sequence, they tend to generate behaviors that are associated with the hallucinated object, even if those behaviors are not present in the actual sequence. This creates a cascade of errors.
- **Core assumption**: The model's behavior generation is heavily dependent on its object recognition accuracy.
- **Evidence anchors**:
  - [abstract]: "we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors"
  - [section]: "Our findings reveal that, for most MLLMs, the correlation coefficients between object and behavioral hallucinations across different domains fluctuate between 0.1 and 0.4, suggesting a weak yet present correlation."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.473. Top related titles: Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models, MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM.
- **Break condition**: If the MLLM can decouple object recognition from behavior generation, or if it has a robust mechanism to verify the plausibility of generated behaviors against the actual image content.

### Mechanism 2
- **Claim**: MLLMs tend to generate behaviors that are commonly paired together (co-occurrence bias), exacerbating behavioral hallucinations.
- **Mechanism**: When faced with uncertainty in inferring behaviors from image sequences, MLLMs default to generating behaviors that frequently co-occur with the identified objects or actions, even if the specific co-occurrence is not present in the given sequence.
- **Core assumption**: The training data used for MLLMs contains many examples of common object-behavior pairings, which the model learns and applies even in novel contexts.
- **Evidence anchors**:
  - [abstract]: "our research pinpoints three principal factors that lead to the reasoning failures of MLLMs: ... the impact of co-occurring behaviors"
  - [section]: "In line with object hallucination phenomena... MLLMs demonstrate a tendency to generate behaviors that are commonly paired together when reasoning through image sequences."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.473. Top related titles: EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding.
- **Break condition**: If the MLLM can learn to prioritize context-specific cues over general co-occurrence patterns, or if it has a mechanism to weigh the likelihood of different behaviors based on the specific image sequence context.

### Mechanism 3
- **Claim**: The Snowball effect amplifies hallucinations in MLLMs when processing image sequences, where early errors compound and propagate through subsequent frames.
- **Mechanism**: In image sequences, MLLMs process frames sequentially, inferring the narrative frame by frame. If an error occurs early in the sequence, the model's subsequent inferences are based on this incorrect understanding, leading to a cascade of errors that become increasingly severe as the sequence progresses.
- **Core assumption**: The model's understanding of each frame is influenced by its understanding of previous frames, and it does not have a robust mechanism to correct errors mid-sequence.
- **Evidence anchors**:
  - [abstract]: "the compounding impact of behavioral hallucinations"
  - [section]: "Experiments on Mementos reveal that the snowball effect in both behavioral and object hallucinations becomes markedly pronounced when reasoning through image sequences."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.473. Top related titles: Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?
- **Break condition**: If the MLLM can implement a mechanism to periodically re-evaluate its understanding of the entire sequence, or if it can learn to identify and correct errors in its reasoning as it processes new frames.

## Foundational Learning

- **Concept**: Multimodal Large Language Models (MLLMs)
  - **Why needed here**: Understanding the capabilities and limitations of MLLMs is crucial for interpreting the results of the Mementos benchmark and the subsequent analysis of reasoning failures.
  - **Quick check question**: What are the key differences between MLLMs and traditional single-modal models, and how do these differences impact their ability to reason over image sequences?

- **Concept**: Image Sequence Reasoning
  - **Why needed here**: The Mementos benchmark specifically focuses on evaluating MLLMs' ability to reason over image sequences, which is a more complex task than reasoning over single images.
  - **Quick check question**: What are the key challenges involved in reasoning over image sequences, and how do these challenges differ from those encountered when reasoning over single images?

- **Concept**: Hallucination in MLLMs
  - **Why needed here**: Hallucinations, both object and behavioral, are a central focus of the Mementos benchmark and the subsequent analysis. Understanding the nature and causes of hallucinations is essential for interpreting the results and identifying potential mitigation strategies.
  - **Quick check question**: What are the different types of hallucinations that can occur in MLLMs, and what are the key factors that contribute to their occurrence?

## Architecture Onboarding

- **Component map**: Mementos Benchmark -> GPT-4-assisted Evaluation -> Keyword Extraction -> Synonym Graphs -> Metric Calculation (Recall, Precision, F1)
- **Critical path**:
  1. Prepare image sequences and human-annotated descriptions from the Mementos dataset
  2. Use MLLMs to generate descriptions for the image sequences
  3. Employ GPT-4 to extract object and behavior keywords from both AI-generated and human-annotated descriptions
  4. Utilize synonym graphs to match AI-generated keywords with human-annotated keywords
  5. Calculate evaluation metrics (Recall, Precision, F1) based on the matched keywords
  6. Analyze the results to identify factors contributing to reasoning failures and hallucinations

- **Design tradeoffs**:
  - Using GPT-4 for keyword extraction and evaluation introduces a dependency on another AI model, which may have its own biases and limitations
  - The synonym graphs are manually constructed and may not capture all possible variations in language used to describe objects and behaviors
  - The benchmark focuses on three specific domains, which may not fully represent the diversity of real-world scenarios where MLLMs are applied

- **Failure signatures**:
  - Low Recall, Precision, or F1 scores for either objects or behaviors, indicating significant hallucinations
  - A high correlation between object and behavioral hallucinations, suggesting a cascading effect of errors
  - A tendency to generate behaviors that are commonly paired together, even if not present in the specific image sequence
  - A progressive decline in performance as the length of the image sequence increases, indicating a Snowball effect

- **First 3 experiments**:
  1. Evaluate a subset of MLLMs on a small, diverse set of image sequences from the Mementos dataset to validate the evaluation pipeline and identify any immediate issues
  2. Analyze the correlation between object and behavioral hallucinations for each MLLM to confirm the presence of the cascading effect of errors
  3. Examine the impact of co-occurring behaviors on hallucination rates by comparing the performance of MLLMs on image sequences with and without common object-behavior pairings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively evaluate the reasoning capabilities of multimodal large language models (MLLMs) on sequential image data without relying on keyword matching?
- Basis in paper: [inferred] The paper uses a GPT-4-assisted evaluation procedure with keyword matching, but acknowledges limitations and suggests exploring more nuanced methods focusing on semantic understanding.
- Why unresolved: Keyword matching is a relatively simplistic approach that may not fully capture the complex reasoning abilities of MLLMs. More sophisticated evaluation methods are needed to assess the true capabilities of these models.
- What evidence would resolve it: Development and validation of new evaluation metrics and procedures that go beyond keyword matching and assess the semantic understanding and reasoning abilities of MLLMs on sequential image data.

### Open Question 2
- Question: What are the key factors that contribute to the snowball effect of behavioral hallucinations in MLLMs when reasoning over image sequences?
- Basis in paper: [explicit] The paper identifies the snowball effect as a significant factor leading to behavioral hallucinations, where errors accumulate and intensify over time.
- Why unresolved: The paper provides some insights into the snowball effect but does not fully explore the underlying mechanisms and contributing factors.
- What evidence would resolve it: Detailed analysis of the temporal dynamics of behavioral hallucinations, identification of specific triggers and patterns that lead to error accumulation, and development of mitigation strategies to address the snowball effect.

### Open Question 3
- Question: How can we effectively mitigate behavioral and object hallucinations in MLLMs to improve their reasoning capabilities on sequential image data?
- Basis in paper: [inferred] The paper identifies behavioral and object hallucinations as major limitations of MLLMs and suggests developing targeted strategies to reduce these hallucinations.
- Why unresolved: The paper does not provide specific solutions or mitigation techniques to address the hallucination problem.
- What evidence would resolve it: Development and evaluation of novel techniques, such as fine-tuning, prompt engineering, or architectural modifications, to reduce behavioral and object hallucinations in MLLMs and improve their reasoning performance on sequential image data.

## Limitations
- The benchmark relies on GPT-4 for keyword extraction and evaluation, introducing potential bias from another AI model's interpretation of visual content
- The study focuses on three specific domains (daily-life, robotics, comics) which may not generalize to all real-world applications
- Manual construction of synonym graphs may miss nuanced language variations and domain-specific terminology
- The correlation analysis between object and behavioral hallucinations (0.1-0.4) shows only weak relationships, suggesting the mechanisms identified may not be as robust as claimed

## Confidence
- **High Confidence**: The overall finding that MLLMs struggle with image sequence reasoning and exhibit hallucinations is well-supported by the comprehensive evaluation across multiple models
- **Medium Confidence**: The three identified factors (object-behavior correlation, co-occurrence bias, snowball effect) are plausible explanations but require further validation due to the weak correlation coefficients observed
- **Low Confidence**: The generalizability of findings beyond the three tested domains and the specific MLLMs evaluated

## Next Checks
1. Replicate the correlation analysis using a different keyword extraction method (e.g., human annotation or a different LLM) to verify the relationship between object and behavioral hallucinations
2. Test the benchmark on additional domains and MLLM architectures to assess generalizability of the identified failure modes
3. Implement an ablation study removing each proposed mechanism (co-occurrence bias, snowball effect) to quantify their individual contributions to overall hallucination rates