---
ver: rpa2
title: The impact of Compositionality in Zero-shot Multi-label action recognition
  for Object-based tasks
arxiv_id: '2405.08695'
source_url: https://arxiv.org/abs/2405.08695
tags:
- recognition
- action
- classes
- zero-shot
- multi-label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Dual-VCLIP, a zero-shot multi-label action
  recognition method that combines VCLIP with DualCoOp to learn positive and negative
  prompts for action recognition in videos. The method addresses the challenge of
  recognizing unseen actions in dynamic environments, particularly for robotic applications
  involving human cooperation.
---

# The impact of Compositionality in Zero-shot Multi-label action recognition for Object-based tasks

## Quick Facts
- arXiv ID: 2405.08695
- Source URL: https://arxiv.org/abs/2405.08695
- Reference count: 32
- Primary result: Dual-VCLIP achieves competitive performance on Charades dataset with 21.0 mAP on zero-shot setting

## Executive Summary
This paper introduces Dual-VCLIP, a zero-shot multi-label action recognition method that adapts vision-language models for dynamic environments. The approach combines VCLIP with DualCoOp to learn positive and negative prompts for action recognition in videos, addressing the challenge of recognizing unseen actions for robotic applications. By using Class-Specific Frame Feature Aggregation and verb-object class-splits, the method demonstrates competitive performance on the Charades dataset while providing insights into compositional biases in action recognition.

## Method Summary
Dual-VCLIP extends VCLIP with DualCoOp to enable zero-shot multi-label action recognition in videos. The method uses learnable positive and negative prompts per class, Class-Specific Frame Feature Aggregation to preserve spatial resolution of temporal features, and asymmetric loss optimization. It processes video frames through OpenVCLIP, projects frame features to textual space, and aggregates temporal logits weighted by class-specific semantic responses. The approach is validated on Charades with 50% class-level zero-shot splits across three configurations: Random, Verbs, and Objects.

## Key Results
- Achieves 21.0 mAP on zero-shot Charades evaluation (50% unseen classes)
- Shows 1.4% improvement over OpenVCLIP baseline in zero-shot setting
- Demonstrates different difficulty levels for verb vs object discrimination through class-split analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class-specific frame feature aggregation improves multi-label recognition by preserving spatial resolution of temporal features
- Mechanism: Instead of pooling frame features globally, the method projects each frame's feature to textual space and aggregates logits weighted by class-specific semantic responses, maintaining fine-grained class-object distinctions across time
- Core assumption: Temporal aggregation of class-specific features outperforms global temporal pooling for multi-label recognition
- Evidence anchors:
  - [abstract] "Class-Specific Frame Feature Aggregation to project each frame's feature to the textual space and aggregate temporal logits by the magnitude of class-specific semantic responses"
  - [section V] "we reformulate the original visual attention in the pretraining model as a class-specific frame feature aggregation"

### Mechanism 2
- Claim: Dual prompts (positive/negative) enable better adaptation of vision-language models to multi-label action recognition
- Mechanism: The model learns two learnable prompts per class, creating contrastive embeddings that help distinguish when an action is present vs absent in a video clip
- Core assumption: Contrastive learning between positive and negative prompts improves zero-shot recognition compared to threshold-based approaches
- Evidence anchors:
  - [abstract] "DualCoOp learns a pair of positive and negative prompts to quickly adapt pretrained vision-text encoders to the Multi Label Recognition (MLR) task"
  - [section V] "we create the positive and negative class-prompts... that are the concatenation of the learnable positive/negative prompts... and the class name"

### Mechanism 3
- Claim: Verb-object class-splits reveal compositional biases that affect recognition performance
- Mechanism: By training with different verb/object masking strategies, the model's ability to generalize across temporal vs spatial dimensions can be measured and optimized
- Core assumption: The model exhibits different generalization patterns when learning verbs versus objects, affecting compositional recognition
- Evidence anchors:
  - [abstract] "Our contribution emphasizes the impact of verb-object class-splits during robots' training for new cooperative tasks, highlighting the influence on the performance and giving insights into mitigating biases"
  - [section VI-B] "The results reported in Table III clearly show that discriminating among verbs or objects poses differing levels of difficulty for the method"

## Foundational Learning

- Concept: Zero-shot learning and compositionality
  - Why needed here: The method extends CLIP-based zero-shot recognition to multi-label action recognition while leveraging compositional understanding of actions as verb-object pairs
  - Quick check question: How does CLIP enable zero-shot recognition, and why is compositionality important for action recognition?

- Concept: Multi-label classification with contrastive learning
  - Why needed here: Standard softmax outputs are inadequate for multi-label scenarios; the dual prompt approach uses contrastive learning to determine class presence/absence
  - Quick check question: What are the limitations of softmax for multi-label classification, and how does contrastive learning address them?

- Concept: Temporal feature aggregation in video understanding
  - Why needed here: The method modifies how temporal information is processed, replacing global pooling with class-specific aggregation to maintain discriminative power across frames
  - Quick check question: How does temporal aggregation differ between standard video models and the class-specific approach used here?

## Architecture Onboarding

- Component map:
  Video frames → OpenVCLIP → Frame features → Class-specific projection → Temporal aggregation → Contrastive comparison → Prediction

- Critical path:
  Video frames → OpenVCLIP → Frame features → Class-specific projection → Temporal aggregation → Contrastive comparison → Prediction

- Design tradeoffs:
  - Learnable prompts add minimal parameters (64x512) but require careful initialization
  - Class-specific aggregation preserves spatial details but increases computational complexity
  - Zero-shot setting avoids annotation costs but limits performance compared to supervised methods

- Failure signatures:
  - Poor performance on unseen classes suggests prompt learning isn't generalizing well
  - Confusion between similar verb-object combinations indicates compositional bias
  - Temporal inconsistency across frames suggests aggregation strategy needs adjustment

- First 3 experiments:
  1. Test baseline OpenVCLIP performance on Charades to establish reference point
  2. Validate dual prompt learning by comparing positive vs negative prompt similarity scores
  3. Evaluate class-specific aggregation by ablating global pooling and measuring performance impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Dual-VCLIP method perform on other action recognition datasets beyond Charades?
- Basis in paper: [inferred] The paper only validates the method on the Charades dataset and does not explore its performance on other datasets.
- Why unresolved: The paper does not provide any experimental results or analysis on other action recognition datasets, leaving the generalizability of the method to other datasets unknown.
- What evidence would resolve it: Conducting experiments on other action recognition datasets such as Kinetics, Something-Something, or ActivityNet and comparing the performance of Dual-VCLIP with other state-of-the-art methods on these datasets would provide insights into the method's generalizability.

### Open Question 2
- Question: What is the impact of using different video frame sampling strategies on the performance of Dual-VCLIP?
- Basis in paper: [explicit] The paper mentions using 16 frames for inference and training but does not explore the impact of using different frame sampling strategies.
- Why unresolved: The paper does not provide any experimental results or analysis on the impact of using different video frame sampling strategies on the performance of Dual-VCLIP.
- What evidence would resolve it: Conducting experiments with different video frame sampling strategies, such as uniform sampling, key frame sampling, or random sampling, and comparing the performance of Dual-VCLIP on these different strategies would provide insights into the impact of frame sampling on the method's performance.

### Open Question 3
- Question: How does the proposed Dual-VCLIP method handle long-term temporal dependencies in videos?
- Basis in paper: [inferred] The paper mentions that OpenVCLIP adapts CLIP to sequences of images by expanding the temporal attention view for every self-attention layer, but it does not explicitly address how Dual-VCLIP handles long-term temporal dependencies in videos.
- Why unresolved: The paper does not provide any experimental results or analysis on how Dual-VCLIP handles long-term temporal dependencies in videos, leaving the method's capability to capture long-term dependencies unknown.
- What evidence would resolve it: Conducting experiments on videos with long-term temporal dependencies, such as cooking videos or sports videos, and analyzing the performance of Dual-VCLIP in capturing and recognizing actions in these videos would provide insights into the method's capability to handle long-term temporal dependencies.

## Limitations
- Split definitions for zero-shot evaluation are not fully specified, preventing exact reproduction
- Implementation details for DualCoOp adaptation to video action recognition are incomplete
- Generalization claims are limited to split-based zero-shot setting rather than truly novel action categories

## Confidence
- **High**: The technical framework combining DualCoOp with video understanding is well-grounded in established vision-language model principles
- **Medium**: Performance improvements over OpenVCLIP are demonstrated, but exact comparison with state-of-the-art methods is limited by split availability
- **Medium**: The compositional analysis through verb-object splits provides useful insights, though the practical impact on real-world applications needs further validation

## Next Checks
1. Evaluate the method on additional datasets like Something-Something V2 or Kinetics to assess robustness beyond Charades
2. Systematically vary the number and initialization of learnable prompts to determine optimal configuration and sensitivity to hyperparameters
3. Test recognition of novel verb-object combinations not seen during training to validate the compositional generalization claims