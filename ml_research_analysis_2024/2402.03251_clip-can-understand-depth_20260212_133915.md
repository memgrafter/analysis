---
ver: rpa2
title: CLIP Can Understand Depth
arxiv_id: '2402.03251'
source_url: https://arxiv.org/abs/2402.03251
tags:
- depth
- clip
- estimation
- semantic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLIP Can Understand Depth The paper addresses the challenge of
  adapting CLIP for monocular depth estimation, where its vision-language alignment
  is suboptimal due to pretraining on web-crawled data. The core method idea involves
  distilling CLIP's frozen text encoder into a single learnable embedding matrix called
  "mirror," which serves as a non-human language prompt to correct depth understanding.
---

# CLIP Can Understand Depth

## Quick Facts
- arXiv ID: 2402.03251
- Source URL: https://arxiv.org/abs/2402.03251
- Authors: Sohee Kim; Jisu Kang; Dunam Kim; Seokju Lee
- Reference count: 40
- Key outcome: CLIP-based depth estimation method matches state-of-the-art vision models on NYU Depth v2 and KITTI datasets, reducing absolute relative error by 68.7% and 75.6% compared to a representative CLIP-based baseline.

## Executive Summary
This paper addresses the challenge of adapting CLIP for monocular depth estimation, where its vision-language alignment is suboptimal due to pretraining on web-crawled data. The core method involves distilling CLIP's frozen text encoder into a single learnable embedding matrix called "mirror," which serves as a non-human language prompt to correct depth understanding. This is combined with a lightweight deconvolutional decoder for dense depth prediction. The approach achieves state-of-the-art results among CLIP-based methods on standard benchmarks without fine-tuning CLIP or using human-designed prompts, demonstrating strong spatial and temporal consistency while maintaining efficiency.

## Method Summary
The method proposes CLIP2Depth, which adapts frozen CLIP for depth estimation by distilling its text encoder's semantic prior into a learnable "mirror" embedding matrix. This mirror embedding is passed through the frozen CLIP text encoder to produce a query vector that modulates the image representation via FiLM layers. A lightweight deconvolutional decoder then predicts dense depth maps from the modulated features. The entire framework is trained end-to-end on depth datasets using a modified scale-invariant loss, achieving competitive performance without fine-tuning the CLIP backbone.

## Key Results
- Achieves state-of-the-art performance among CLIP-based methods on NYU Depth v2 and KITTI datasets
- Reduces absolute relative error by 68.7% and 75.6% compared to representative CLIP-based baseline
- Demonstrates strong spatial and temporal consistency in depth predictions
- Maintains efficiency through lightweight decoder architecture without fine-tuning CLIP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's text encoder semantic prior can be distilled into a single learnable embedding (mirror) without fine-tuning the CLIP backbone.
- Mechanism: The mirror embedding is passed through the frozen CLIP text encoder, which transforms it into a query vector that modulates the image representation for depth estimation.
- Core assumption: The frozen CLIP text encoder can effectively encode non-human language prompts into meaningful representations for depth cues.
- Evidence anchors:
  - [abstract] "distill the semantic prior of its frozen text encoder into a single learnable embedding matrix called 'mirror'"
  - [section] "We propose a more cost-effective and scalable method: we freeze CLIP's text encoder and distill its pre-trained semantic prior into a compact learnable embedding matrix called 'mirror'"
  - [corpus] Weak evidence - no direct corpus support for this specific mechanism, but related works on CLIP adaptation provide indirect support
- Break condition: If the frozen text encoder cannot encode the mirror embedding into useful depth representations, the approach fails.

### Mechanism 2
- Claim: A single non-human language prompt is sufficient for depth estimation across all images.
- Mechanism: The mirror embedding serves as a universal query ("How far is this location from the camera?") that conditions the entire image representation for depth prediction.
- Core assumption: Depth estimation can be framed as a single query problem rather than requiring multiple bin-specific prompts.
- Evidence anchors:
  - [abstract] "The main design goal of mirror is to derive a non-human language prompt that approximates an optimal natural language prompt: 'How far is this location from the camera?'"
  - [section] "Although it may seem odd to condition all images on the same query vector, we argue that a single query such as 'How far is this location from the camera?' is sufficient for depth estimation"
  - [corpus] Weak evidence - no direct corpus support, but this is a novel contribution of the paper
- Break condition: If certain scenes require scene-specific prompts for accurate depth estimation, the single prompt approach may fail.

### Mechanism 3
- Claim: Lightweight conditioning through FiLM layers is more effective than aggregating similarity scores from multiple prompts.
- Mechanism: The mirror-conditioned embedding modulates image features via FiLM layers, allowing end-to-end learning of depth representations without discrete binning.
- Core assumption: Continuous modulation of image features is superior to discrete aggregation of prompt-image similarities for depth regression.
- Evidence anchors:
  - [abstract] "Using this approach, we jointly train two lightweight modules, a mirror and a compact decoder, on top of a frozen CLIP for dense depth prediction"
  - [section] "our framework modulates images using a single, non-human language prompt as the input of a FiLM [25] block"
  - [corpus] Weak evidence - no direct corpus support, but this represents a novel architectural contribution
- Break condition: If FiLM modulation introduces instability or fails to capture necessary depth information, the approach may underperform.

## Foundational Learning

- Concept: Vision-language alignment in CLIP
  - Why needed here: Understanding how CLIP aligns images and text is crucial for knowing why its depth estimation is suboptimal and how mirror corrects it
  - Quick check question: What is the fundamental difference between CLIP's vision-language alignment and what's needed for depth estimation?

- Concept: Prompt learning and token embeddings
  - Why needed here: The paper builds on prompt learning literature but introduces non-human language prompts, requiring understanding of both traditional and novel approaches
  - Quick check question: How does using a single learnable token differ from traditional prompt learning approaches that use multiple tokens?

- Concept: Depth estimation metrics and evaluation
  - Why needed here: The paper reports on specific depth metrics (Abs Rel, RMSE, etc.) that are standard in the field, requiring familiarity with these evaluation methods
  - Quick check question: What is the difference between Abs Rel and RMSE, and why might a method perform differently on these metrics?

## Architecture Onboarding

- Component map: Frozen CLIP image encoder → Frozen CLIP text encoder → Mirror embedding → FiLM modulation → Deconvolutional decoder → Depth output
- Critical path: Input image → CLIP image encoder → FiLM modulation (conditioned by mirror) → Deconvolutional decoder → Depth map
- Design tradeoffs: Using a single prompt vs. multiple prompts, freezing CLIP vs. fine-tuning, lightweight decoder vs. complex architecture
- Failure signatures: Poor depth boundaries, inconsistent depth values across frames, failure to capture semantic objects
- First 3 experiments:
  1. Test mirror embedding alone through CLIP text encoder to verify it produces meaningful representations
  2. Validate FiLM modulation effectiveness by comparing with baseline aggregation methods
  3. Evaluate spatial and temporal consistency on KITTI dataset to verify depth continuity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CLIP2Depth's performance scale with larger CLIP backbones (e.g., ViT-L/14) and deeper decoder architectures?
- Basis in paper: [inferred] The paper demonstrates competitive performance using CLIP ViT-B/16 and a lightweight decoder, but notes that large-scale vision-only models like DepthAnything v2 (ViT-L) achieve higher raw accuracy.
- Why unresolved: The paper focuses on a lightweight design to demonstrate the feasibility of correcting CLIP's prior without fine-tuning, rather than exploring scalability with larger architectures.
- What evidence would resolve it: Empirical results comparing CLIP2Depth variants with different CLIP backbone sizes and decoder depths on standard benchmarks like NYU Depth v2 and KITTI.

### Open Question 2
- Question: What is the impact of incorporating open-vocabulary reasoning capabilities into CLIP2Depth while maintaining the advantages of non-human language prompting?
- Basis in paper: [explicit] The paper mentions that replacing natural language prompts with non-human language embeddings could compromise CLIP's inherent open-vocabulary reasoning capabilities, potentially limiting its broader applicability.
- Why unresolved: The proposed mirror approach eliminates the use of natural language token embeddings, which may restrict CLIP's open-vocabulary strengths, but the paper does not explore hybrid solutions.
- What evidence would resolve it: Experiments evaluating CLIP2Depth's performance on diverse datasets requiring open-vocabulary reasoning while using non-human language prompting.

### Open Question 3
- Question: How does CLIP2Depth perform in environments where geometric structures dominate and semantic cues are sparse?
- Basis in paper: [explicit] The paper notes that the effectiveness of the mirror may heavily depend on the presence of semantically rich content within the scene, raising concerns about generalization in environments where geometric structures dominate.
- Why unresolved: The paper focuses on demonstrating CLIP2Depth's effectiveness in indoor and outdoor driving scenes, but does not evaluate its performance in semantically sparse environments.
- What evidence would resolve it: Comparative evaluations of CLIP2Depth on datasets with varying levels of semantic content, such as depth estimation in structured indoor environments versus natural landscapes.

## Limitations

- The single non-human language prompt approach may not capture depth complexity across all diverse scenes
- The lightweight decoder architecture may constrain fine-grained depth detail capture
- Limited evaluation on diverse real-world scenarios beyond established indoor and outdoor datasets

## Confidence

- **High confidence**: The core claim that CLIP can be adapted for depth estimation without fine-tuning achieves state-of-the-art results among CLIP-based methods. The quantitative results (68.7% and 75.6% error reductions) are well-supported by the reported metrics.
- **Medium confidence**: The mechanism by which the mirror embedding improves depth understanding is plausible but not definitively proven. The paper demonstrates effectiveness but doesn't provide sufficient ablation studies to isolate the specific contribution of the mirror encoding versus other architectural components.
- **Low confidence**: The generalizability claim to zero-shot settings on SUN RGB-D and diverse scenarios is weakly supported, with limited quantitative validation and no comparison to established zero-shot methods.

## Next Checks

1. **Ablation study on prompt design**: Systematically compare the single mirror prompt approach against multi-prompt strategies and human-designed prompts to quantify the specific contribution of the mirror mechanism to performance gains.

2. **Mechanism validation through intermediate representations**: Analyze and visualize the embeddings produced by the CLIP text encoder when processing the mirror token versus natural language prompts to verify that meaningful depth representations are being learned.

3. **Generalization testing on diverse datasets**: Evaluate the method on a broader range of depth estimation datasets beyond NYU Depth v2 and KITTI, including datasets with different characteristics (e.g., indoor-outdoor transitions, varying camera types, extreme lighting conditions) to better assess real-world applicability.