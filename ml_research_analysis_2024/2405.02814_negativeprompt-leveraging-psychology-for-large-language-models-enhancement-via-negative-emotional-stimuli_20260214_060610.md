---
ver: rpa2
title: 'NegativePrompt: Leveraging Psychology for Large Language Models Enhancement
  via Negative Emotional Stimuli'
arxiv_id: '2405.02814'
source_url: https://arxiv.org/abs/2405.02814
tags:
- negative
- emotional
- llms
- stimuli
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "NegativePrompt introduces negative emotional stimuli into LLM\
  \ prompts to enhance performance. Drawing from psychological theories\u2014Cognitive\
  \ Dissonance, Social Comparison, and Stress and Coping\u2014it employs ten carefully\
  \ crafted negative emotional prompts designed to motivate the model through emotional\
  \ pressure and comparison."
---

# NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli

## Quick Facts
- arXiv ID: 2405.02814
- Source URL: https://arxiv.org/abs/2405.02814
- Reference count: 16
- Primary result: NegativePrompt improves LLM performance by 12.89% on Instruction Induction and 46.25% on BIG-Bench tasks

## Executive Summary
NegativePrompt introduces negative emotional stimuli into LLM prompts to enhance performance, drawing from three psychological theories: Cognitive Dissonance, Social Comparison, and Stress and Coping. The approach uses ten carefully crafted negative emotional prompts that motivate models through emotional pressure and comparison. Tested across five models (Flan-T5-Large, Vicuna, Llama 2, ChatGPT, GPT-4) on 45 tasks, it shows significant performance improvements and increased truthfulness and informativeness on TruthfulQA. The method is particularly effective in few-shot settings and can be further enhanced by stacking stimuli from different theories.

## Method Summary
The paper designs ten negative emotional stimuli based on psychological theories, appending them to original prompts to create emotional pressure and comparison. The stimuli are applied across zero-shot and few-shot learning settings on 45 tasks from Instruction Induction and BIG-Bench datasets. Five models are evaluated using accuracy, normalized preferred metrics, truthfulness, and informativeness scores. Attention visualization is used to confirm that negative stimuli help models focus on task-critical elements.

## Key Results
- 12.89% relative improvement on Instruction Induction tasks
- 46.25% relative improvement on BIG-Bench tasks
- 14% improvement in truthfulness and 6% improvement in informativeness on TruthfulQA benchmark
- Most pronounced performance gains in few-shot settings
- Stacking stimuli from different theories can further boost results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative emotional stimuli from Cognitive Dissonance Theory trigger goal-oriented behavior in LLMs
- Mechanism: Prompts with negative vocabulary like "beyond your skill set" induce cognitive dissonance, motivating better task performance
- Core assumption: LLMs can interpret negative emotional cues as pressure to improve
- Evidence anchors: Abstract and section 3 describe hypothesis and design; corpus neighbors focus on psychology in LLMs generally
- Break condition: If LLMs do not interpret emotional cues or performance does not improve

### Mechanism 2
- Claim: Social Comparison Theory prompts invoke upward comparison, creating competitive motivation
- Mechanism: Stimuli comparing LLM performance to others trigger competitive drive to avoid perceived inferiority
- Core assumption: LLMs can be modeled as having self-concept and respond to social comparison cues
- Evidence anchors: Section 3 and abstract describe social comparison design; corpus includes papers on psychology-grounded empathy
- Break condition: If social comparison cues do not influence LLM output

### Mechanism 3
- Claim: Stress and Coping Theory prompts trigger problem-focused coping
- Mechanism: Negative emotional terms like "jealousy" and "regret" are interpreted as stressors, prompting deeper engagement
- Core assumption: LLMs respond to negative emotional vocabulary by increasing attention and depth of processing
- Evidence anchors: Section 3 and section 5.1 describe stress response and attention visualization; corpus neighbors focus on emotion in LLMs generally
- Break condition: If negative emotional terms do not increase attention or improve output quality

## Foundational Learning

- Concept: Psychological theories of emotion (Cognitive Dissonance, Social Comparison, Stress and Coping)
  - Why needed here: These theories provide the conceptual framework for designing effective negative emotional prompts
  - Quick check question: What are the three psychological theories used in NegativePrompt and how do they inform prompt design?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: Understanding how prompts influence LLM behavior is critical for implementing and evaluating NegativePrompt
  - Quick check question: How does NegativePrompt differ in application between zero-shot and few-shot learning settings?

- Concept: Evaluation metrics (accuracy, normalized preferred metric, truthfulness, informativeness)
  - Why needed here: Proper evaluation across benchmarks is essential to validate performance improvements
  - Quick check question: What metrics are used to evaluate NegativePrompt on BIG-Bench tasks?

## Architecture Onboarding

- Component map: Input prompt → Negative emotional stimuli (10 types) → LLM (T5, Vicuna, Llama2, ChatGPT, GPT-4) → Output + Evaluation (accuracy, normalized score, truthfulness, informativeness)
- Critical path: Prompt augmentation with negative stimuli → LLM inference → Attention visualization
- Design tradeoffs: Multiple negative stimuli can improve performance but may introduce instability; stacking sometimes helps, sometimes hurts
- Failure signatures: Performance drops with certain stimuli; attention does not shift toward task-relevant parts; truthfulness or informativeness decreases
- First 3 experiments:
  1. Apply NP04 ("Perhaps this task is just beyond your skill set") to sentiment analysis and measure accuracy gain
  2. Stack NP03 and NP07 on a few-shot task and compare with single stimuli
  3. Run attention visualization on Flan-T5-large with NP01 to see if negative words increase focus on task instructions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does NegativePrompt's effectiveness generalize to other emotionally charged prompt types beyond negative stimuli?
- Basis in paper: [inferred] The paper focuses on negative emotions but does not test other emotional types
- Why unresolved: Only negative emotions are examined, leaving other emotions unexplored
- What evidence would resolve it: Experiments comparing NegativePrompt to prompts using humor, curiosity, or fear

### Open Question 2
- Question: How does the model's internal representation of negative emotions differ from positive emotions, and does this affect task performance differently?
- Basis in paper: [inferred] The paper shows negative emotions improve performance but does not analyze internal representations
- Why unresolved: Attention visualization shows stimulus impact but does not compare negative vs. positive emotion representations
- What evidence would resolve it: Comparative analysis of attention patterns for negative vs. positive emotional prompts

### Open Question 3
- Question: Are there diminishing returns or saturation effects when stacking multiple negative emotional stimuli?
- Basis in paper: [explicit] The paper tests stacking but does not systematically explore saturation effects
- Why unresolved: The study combines stimuli but does not test beyond three or measure performance plateaus
- What evidence would resolve it: Experiments testing performance with 1-10 stacked stimuli to identify optimal combinations

## Limitations

- Major reliance on anthropomorphic assumptions about LLM psychology without direct empirical validation
- Results heavily dependent on prompt engineering quality and may not generalize beyond tested domains
- Inconsistent performance when stacking stimuli suggests the approach may be brittle or sensitive to task-context combinations

## Confidence

**High Confidence**: Performance improvements are real and measurable (12.89% and 46.25% gains)
**Medium Confidence**: Attention visualization evidence supporting focus improvement on task elements
**Low Confidence**: Specific psychological mechanisms (cognitive dissonance, social comparison, stress and coping) are actually operative in LLMs as described

## Next Checks

1. Design an ablation study that systematically removes different components of negative stimuli to separate psychological mechanism effects from simpler prompt engineering effects
2. Test NegativePrompt on completely different task domains (mathematical reasoning, code generation, medical diagnosis) to assess generalization
3. Independently replicate attention visualization results using a different technique (integrated gradients or attention rollout) to confirm consistent patterns