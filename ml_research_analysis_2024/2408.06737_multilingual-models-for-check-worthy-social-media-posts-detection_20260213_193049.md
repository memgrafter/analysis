---
ver: rpa2
title: Multilingual Models for Check-Worthy Social Media Posts Detection
arxiv_id: '2408.06737'
source_url: https://arxiv.org/abs/2408.06737
tags:
- claims
- able
- veri
- factual
- posts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work developed multilingual transformer models to detect verifiable
  factual claims and harmful content in social media posts across English and low-resource
  languages (Arabic, Bulgarian, Dutch, Polish, Czech, Slovak). It trained multilingual
  and multi-label variants of DistilBERT, BERT, and XLM-RoBERTa on datasets like CLEF2022,
  CLEF2021, LESA2021, and MultiClaim, using both translated and native-language inputs.
---

# Multilingual Models for Check-Worthy Social Media Posts Detection

## Quick Facts
- arXiv ID: 2408.06737
- Source URL: https://arxiv.org/abs/2408.06737
- Authors: Sebastian Kula; Michal Gregor
- Reference count: 33
- Primary result: Multi-label XLM-RoBERTa-base achieved over 0.8 recall for factual claims and competitive F1-scores for harmful content across English and low-resource languages.

## Executive Summary
This work develops multilingual transformer models to detect verifiable factual claims and harmful content in social media posts across English and low-resource languages (Arabic, Bulgarian, Dutch, Polish, Czech, Slovak). The study trains multilingual and multi-label variants of DistilBERT, BERT, and XLM-RoBERTa on datasets like CLEF2022, CLEF2021, LESA2021, and MultiClaim, using both translated and native-language inputs. The multi-label XLM-RoBERTa-base model achieved over 0.8 recall for factual claims and competitive F1-scores for harmful content, outperforming or matching state-of-the-art single-task models while handling multiple languages and both tasks simultaneously.

## Method Summary
The method involves fine-tuning multilingual transformers (XLM-RoBERTa-base/large, DistilBERT, BERT) on multilingual, multi-label datasets. Social media posts are preprocessed by removing URLs, punctuation, handles, and filtering by alphabet and length. The models are trained on merged multilingual datasets with binary cross-entropy loss for multi-label classification. Performance is evaluated using recall, accuracy, and F1-score on held-out test sets, with comparisons to Nithiwat models and Alpaca-LoRA.

## Key Results
- Multi-label XLM-RoBERTa-base achieved over 0.8 recall for factual claims across multiple languages.
- The model demonstrated strong performance in Bulgarian and Polish, with competitive F1-scores for harmful content detection.
- Inference speed was faster than human review while maintaining high accuracy.

## Why This Works (Mechanism)

### Mechanism 1
Multilingual XLM-RoBERTa models outperform translated English-only models for factual claim detection across multiple languages because the multilingual model is trained on raw multilingual data, avoiding translation artifacts and preserving language-specific cues that are important for detecting claims. Core assumption: Language-specific syntactic and semantic patterns in claims are better captured when the model is exposed to native language inputs rather than translations. Evidence: The study includes a comprehensive analysis of different models, with a special focus on multilingual models where the same model is capable of processing social media posts in both English and in low-resource languages.

### Mechanism 2
Longer sentences are more likely to contain verifiable factual claims, and the model leverages sentence length as a signal because during training, longer sentences often correspond to more detailed claims, so the model learns to associate length with factual content. Core assumption: Sentence length correlates with the presence of detailed, verifiable information. Evidence: The analysis confirms findings, the highest recall was obtained for the group of the longest sentences and there is significant discrepancy between the shortest and the longest group of sentences.

### Mechanism 3
The multi-label setup allows a single model to detect both factual and harmful claims simultaneously without significant loss in performance compared to separate models because shared layers in the multi-label architecture learn common features between factual and harmful claims, enabling efficient joint detection. Core assumption: Factual and harmful claims share some linguistic cues that can be exploited by a joint model. Evidence: The novelty of this work lies in the development of multi-label multilingual classification models that can simultaneously detect harmful posts and posts that contain verifiable factual claims in an efficient way.

## Foundational Learning

- Concept: Multilingual representation learning in transformers
  - Why needed here: The model must process and classify claims in multiple languages without relying on translation, requiring knowledge of how multilingual embeddings work.
  - Quick check question: What is the main difference between monolingual and multilingual transformer models in handling input text?

- Concept: Multi-label classification vs. multi-class classification
  - Why needed here: The task requires detecting two independent binary labels (factual claim / not, harmful / not) simultaneously, so understanding how sigmoid outputs and BCE loss work is critical.
  - Quick check question: How does the loss function differ between multi-class and multi-label classification in neural networks?

- Concept: Handling class imbalance in NLP datasets
  - Why needed here: The datasets are highly imbalanced (e.g., many more non-harmful than harmful posts), so techniques like class weighting or resampling are important to avoid bias.
  - Quick check question: What is one method to address class imbalance during model training in PyTorch?

## Architecture Onboarding

- Component map: Social media post text -> XLM-RoBERTa-base encoder -> Two separate linear layers with sigmoid activation -> Factual claim and harmful claim outputs

- Critical path: Load multilingual dataset -> split into train/val/test -> Pre-process text -> tokenize with XLM-RoBERTa tokenizer -> Train multi-label model -> monitor both F1 scores -> Evaluate on test set -> analyze recall per label -> Perform error analysis -> check sentence length effects

- Design tradeoffs: Using a smaller base model (XLM-RoBERTa-base) vs. large model for speed vs. accuracy; Multi-label joint training vs. separate models for modularity vs. efficiency; Aggressive pre-processing vs. preserving linguistic cues

- Failure signatures: Low recall on short sentences -> model overfits to sentence length; Poor performance on low-resource languages -> insufficient language-specific data; High false positives on harmful claims -> model conflates offensive language with harmful intent

- First 3 experiments: 1) Train baseline monolingual BERT-large on translated English data for factual claims only. 2) Train multilingual XLM-RoBERTa-base on raw multilingual data for factual claims only. 3) Train multi-label XLM-RoBERTa-base on combined factual + harmful datasets.

## Open Questions the Paper Calls Out

### Open Question 1
How does including sentence length as an explicit feature during pre-training affect the model's ability to detect verifiable factual claims in short sentences? The authors observed that longer sentences are more likely to contain verifiable factual claims, and suggest including sentence length as an additional feature to improve performance. This remains unresolved because the paper hypothesizes that adding sentence length as a feature could improve model performance, especially for short sentences, but did not conduct experiments to test this. Comparative experiments training the model with and without sentence length as a feature, measuring recall for short sentences in the test set, would resolve this.

### Open Question 2
How does the model's performance change when tested on social media posts from topics other than COVID-19? The authors note that their models were trained on multi-topic datasets and should perform well on non-COVID-19 content, but only tested on COVID-19-related test sets. This remains unresolved because testing was limited to COVID-19 topics due to the available datasets, so performance on other topics remains unverified. Testing the trained models on social media posts from diverse non-COVID-19 topics and reporting accuracy, recall, and F1-scores would resolve this.

### Open Question 3
What is the impact of using different multilingual pre-training corpora (e.g., not just CC-100) on the model's performance for low-resource languages like Polish, Czech, and Slovak? The authors mention that XLM-RoBERTa's performance depends on the CC-100 training corpus, with better results for languages with more data in CC-100, but do not explore other corpora. This remains unresolved because the study used the standard XLM-RoBERTa models pre-trained on CC-100, without comparing to models pre-trained on alternative corpora that might benefit low-resource languages. Training and evaluating XLM-RoBERTa models on alternative multilingual corpora and comparing their performance on Polish, Czech, and Slovak test sets would resolve this.

## Limitations
- The extent to which multilingual model gains are language-specific versus dataset-specific remains unclear.
- Error analysis on sentence length only addresses factual claims, not harmful content detection.
- Performance on extremely low-resource languages is not extensively validated.

## Confidence
- High confidence: The multi-label XLM-RoBERTa model's ability to achieve recall >0.8 for factual claims and competitive F1-scores for harmful content, as directly measured on test sets.
- Medium confidence: The claim that multilingual models outperform translated English-only models, based on observed recall differences, but without extensive ablation studies on translation quality.
- Medium confidence: The sentence length correlation with factual claim detection, supported by error analysis, but without mechanistic explanation for why length matters.
- Low confidence: The assertion that the multi-label setup does not significantly harm performance, as the paper lacks direct comparisons with separate models trained on the same data.

## Next Checks
1. Translation quality ablation: Retrain the model on machine-translated English data with varying translation quality to isolate the effect of translation artifacts on performance.
2. Cross-language generalization: Test the multilingual model on a held-out language not seen during training to assess true multilingual generalization versus language-family clustering.
3. Sentence length feature ablation: Mask sentence length as a feature by balancing the training data across length bins and retraining to determine if the model still relies on length as a proxy for factual claims.