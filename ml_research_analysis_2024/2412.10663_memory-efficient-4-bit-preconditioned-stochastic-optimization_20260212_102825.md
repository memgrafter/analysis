---
ver: rpa2
title: Memory-Efficient 4-bit Preconditioned Stochastic Optimization
arxiv_id: '2412.10663'
source_url: https://arxiv.org/abs/2412.10663
tags:
- shampoo
- quantization
- cholesky
- memory
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a memory-efficient 4-bit preconditioned stochastic
  optimization algorithm for large-scale deep learning. The main challenge addressed
  is the high memory overhead of non-diagonal preconditioning matrices used in methods
  like Shampoo.
---

# Memory-Efficient 4-bit Preconditioned Stochastic Optimization

## Quick Facts
- arXiv ID: 2412.10663
- Source URL: https://arxiv.org/abs/2412.10663
- Authors: Jingyang Li; Kuangyu Ding; Kim-Chuan Toh; Pan Zhou
- Reference count: 40
- One-line primary result: 4-bit Shampoo achieves 80.52% test accuracy on CIFAR-100 with 28% less peak memory than 32-bit Shampoo

## Executive Summary
This paper addresses the memory bottleneck in large-scale deep learning optimization by proposing a memory-efficient 4-bit preconditioned stochastic optimization algorithm. The key innovation is Cholesky quantization, which applies Cholesky decomposition followed by 4-bit quantization of the Cholesky factors, reducing storage by half while preserving spectral properties better than direct matrix quantization. The method incorporates error feedback to further reduce quantization error by maintaining a 4-bit error state that compensates the Cholesky factor at each iteration. Theoretical convergence guarantees are provided for both smooth and nonsmooth stochastic nonconvex optimization.

## Method Summary
The proposed 4-bit Shampoo algorithm combines several key innovations: (1) Cholesky quantization that decomposes preconditioning matrices using Cholesky factorization before applying 4-bit quantization, reducing memory usage by half while maintaining better spectral properties than direct quantization; (2) Error feedback mechanisms that maintain a 4-bit error state to compensate for quantization errors at each iteration; and (3) Block-wise quantization with 64×64 blocks using linear-2 quantization mapping. The method is built on top of the Shampoo optimizer, which uses inverse 1/4-th roots of preconditioners computed via the Schur-Newton algorithm. The authors provide theoretical convergence guarantees for both smooth and nonsmooth stochastic nonconvex optimization problems.

## Key Results
- ResNet-34 on CIFAR-100 achieves 80.52% test accuracy with 28% less peak memory compared to 32-bit Shampoo
- VGG-19 on CIFAR-100 trained with SGDM and 4-bit Shampoo achieves 73.61% test accuracy
- Swin-Tiny on Tiny-ImageNet trained with SGDM and 4-bit Shampoo achieves 59.45% test accuracy
- ViT-Small on ImageNet trained with AdamW and 4-bit Shampoo achieves 79.83% test accuracy

## Why This Works (Mechanism)
The method works by addressing the fundamental memory bottleneck in non-diagonal preconditioning methods like Shampoo. Traditional Shampoo stores full 32-bit preconditioning matrices, which becomes prohibitive for large models. Cholesky quantization exploits the fact that positive semidefinite matrices can be decomposed into products of lower triangular matrices, allowing for more efficient storage and quantization. The error feedback mechanism further improves accuracy by tracking and compensating for quantization errors across iterations, preventing error accumulation that would otherwise degrade performance.

## Foundational Learning
- **Cholesky Decomposition**: Factorizes positive semidefinite matrices into products of lower triangular matrices; needed for efficient storage and quantization of preconditioning matrices; quick check: verify positive definiteness of preconditioners before decomposition
- **Quantization Error Metrics**: Relative error (RE) and absolute error (AE) measure quantization quality; needed to evaluate and optimize quantization performance; quick check: monitor NRE (normalized RE) to ensure <1% error
- **Preconditioned Gradient Methods**: Use matrix preconditioners to adapt learning rates per parameter; needed for faster convergence in ill-conditioned optimization landscapes; quick check: verify preconditioner conditioning improves gradient descent stability
- **Error Feedback in Quantization**: Maintains error states to compensate for quantization errors across iterations; needed to prevent error accumulation in low-precision training; quick check: verify error compensation improves final model accuracy
- **Schur-Newton Algorithm**: Computes inverse 1/4-th roots of matrices; needed for Shampoo's preconditioner computation; quick check: verify numerical stability of inverse root computation
- **Block-wise Quantization**: Divides matrices into blocks for independent quantization; needed to balance quantization accuracy with computational efficiency; quick check: experiment with different block sizes (32×32, 64×64, 128×128)

## Architecture Onboarding

**Component Map:**
Input Data -> Base Optimizer (SGDM/AdamW) -> Shampoo Preconditioner -> Cholesky Quantization -> Error Feedback -> 4-bit Preconditioned Gradients -> Model Parameters

**Critical Path:**
Data batch → Gradient computation → Preconditioner update → Cholesky decomposition → 4-bit quantization with error feedback → Preconditioned gradient application → Parameter update

**Design Tradeoffs:**
- Memory vs Accuracy: 4-bit quantization reduces memory but introduces quantization error; error feedback partially compensates but adds computational overhead
- Block Size vs Performance: Smaller blocks (32×32) provide better quantization accuracy but increase memory overhead; larger blocks (128×128) reduce memory but degrade accuracy
- Quantization Mapping: Linear-2 chosen for simplicity vs dynamic/quantile mappings that might preserve spectral properties better but require more complex implementation

**Failure Signatures:**
- Training instability or divergence indicating preconditioner becomes ill-conditioned due to quantization
- Accuracy degradation beyond acceptable thresholds (>1-2% drop) suggesting quantization error overwhelms error feedback
- Memory allocation errors during large model training indicating block size or quantization parameters need adjustment

**3 First Experiments:**
1. Implement 4-bit Shampoo with compensated Cholesky quantization on VGG-19 CIFAR-100 using SGDM, measuring test accuracy and peak memory vs 32-bit Shampoo baseline
2. Vary block sizes (32×32, 64×64, 128×128) in the quantization to identify optimal trade-off between memory efficiency and accuracy
3. Compare linear-2 quantization against dynamic quantization on a small model to evaluate impact on test performance and quantization error metrics

## Open Questions the Paper Calls Out
- **Open Question 1**: How does Cholesky quantization performance scale with different block sizes in block-wise quantization? The paper uses 64×64 blocks without exploring trade-offs between smaller blocks (better accuracy, higher memory) and larger blocks (worse accuracy, lower memory).
- **Open Question 2**: Can the error feedback mechanism be extended to other preconditioned gradient methods beyond Shampoo? The authors note their approach could generalize to methods like K-FAC or AdaBK but leave this for future work.
- **Open Question 3**: How does 4-bit Shampoo perform on non-image tasks like object detection or video generation? Due to limited GPU resources, evaluation was restricted to image classification and LLM pre-training.
- **Open Question 4**: What is the impact of using different quantization mappings (linear-2 vs dynamic vs quantile) on Cholesky quantization performance? The authors chose linear-2 for simplicity without comparing alternative mappings.

## Limitations
- The 28% memory reduction compared to 32-bit Shampoo, while significant, may not be sufficient for extremely large-scale models
- Computational overhead from the Schur-Newton algorithm and error feedback mechanism is not thoroughly analyzed
- Exact implementation details of the Schur-Newton algorithm for inverse 1/4-th root computation remain unspecified

## Confidence
- High confidence in theoretical convergence analysis and memory reduction claims
- Medium confidence in practical effectiveness across different model architectures and tasks
- Low confidence in complete implementation details without access to exact Schur-Newton and power iteration parameters

## Next Checks
1. Verify stability of compensated Cholesky quantization by monitoring eigenvalue distributions during training across different block sizes
2. Compare wall-clock time overhead of proposed method against 32-bit Shampoo on identical hardware setups
3. Test algorithm performance on larger transformer-based models (e.g., BERT, GPT) to validate scalability claims