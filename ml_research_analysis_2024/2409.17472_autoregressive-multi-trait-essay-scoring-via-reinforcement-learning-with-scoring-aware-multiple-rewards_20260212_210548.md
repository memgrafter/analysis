---
ver: rpa2
title: Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware
  Multiple Rewards
arxiv_id: '2409.17472'
source_url: https://arxiv.org/abs/2409.17472
tags:
- score
- samrl
- essay
- trait
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Scoring-aware Multi-reward Reinforcement
  Learning (SaMRL) for automated essay scoring, addressing the challenge of training
  neural networks with non-differentiable evaluation metrics like QWK. The method
  integrates bidirectional QWK and MSE penalty rewards into an autoregressive multi-trait
  scoring framework, enabling direct optimization of scoring-aware metrics.
---

# Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards

## Quick Facts
- arXiv ID: 2409.17472
- Source URL: https://arxiv.org/abs/2409.17472
- Authors: Heejin Do; Sangwon Ryu; Gary Geunbae Lee
- Reference count: 19
- Key outcome: SaMRL improves scoring performance across most traits and prompts, excelling in essays with broader score ranges

## Executive Summary
This paper introduces Scoring-aware Multi-reward Reinforcement Learning (SaMRL) for automated essay scoring, addressing the challenge of training neural networks with non-differentiable evaluation metrics like QWK. The method integrates bidirectional QWK and MSE penalty rewards into an autoregressive multi-trait scoring framework, enabling direct optimization of scoring-aware metrics. SaMRL significantly improves scoring performance across most traits and prompts, particularly excelling in essays with broader score ranges where traditional classification-based RL approaches struggle. The method demonstrates robustness across different model sizes and datasets, with enhanced performance on both the ASAP and Feedback Prize datasets.

## Method Summary
SaMRL leverages an autoregressive generation framework where scores are predicted sequentially as token sequences rather than through classification. The approach uses reinforcement learning with Proximal Policy Optimization (PPO) to optimize non-differentiable metrics like Quadratic Weighted Kappa (QWK). A multi-reward function combines bidirectional QWK (trait-wise and batch-wise) with mean trait-wise MSE penalty, while token-wise KL regularization with a frozen anchor model prevents mode collapse. Dynamic weighting of QWK and MSE losses during training allows the model to adapt to different scoring scenarios. The method was validated on ASAP and ASAP++ datasets with 8 prompts, as well as the Feedback Prize dataset, showing consistent improvements over baseline classification approaches.

## Key Results
- SaMRL significantly improves scoring performance across most traits and prompts compared to classification-based RL approaches
- The method excels particularly in essays with broader score ranges where traditional approaches struggle
- SaMRL demonstrates robustness across different model sizes (T5-base and T5-large) and datasets
- Enhanced performance is observed on both the ASAP dataset and the Feedback Prize dataset with 6 traits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The autoregressive generation framework allows direct optimization of scoring-aware metrics by leveraging token generation probabilities.
- Mechanism: Unlike classification models that output a single probability distribution, autoregressive models generate scores as sequences of tokens (e.g., "trait1 score1, trait2 score2"). This sequential generation provides token-level probability distributions that can be used in policy gradient updates, enabling the incorporation of non-differentiable metrics like QWK into the training process.
- Core assumption: Token generation probabilities in autoregressive models can effectively represent the uncertainty and structure needed for reinforcement learning updates in scoring tasks.
- Evidence anchors:
  - [abstract] "existing reinforcement learning (RL) applications in AES are limited to classification models despite associated performance degradation, as RL requires probability distributions; instead, we adopt an autoregressive score generation framework to leverage token generation probabilities for robust multi-trait score predictions."
  - [section 3] "Unlike prior works, we treat AES as a generation paradigm (Do et al., 2024), leveraging token generation probability distributions instead of categorical ones for policy gradient."
- Break condition: If the autoregressive model cannot generate coherent score sequences or the token probabilities do not correlate well with actual scoring quality.

### Mechanism 2
- Claim: The bidirectional QWK reward stabilizes training by addressing the batch-level averaging problem of traditional QWK.
- Mechanism: Traditional QWK calculates a single metric for an entire essay set, assigning the same reward to all samples in a batch. The bidirectional approach combines batch-wise QWK (QB) with trait-wise QWK (QT) at the sample level. This integration ensures that individual essays receive more nuanced feedback while maintaining overall consistency across the batch.
- Core assumption: Combining batch-level and sample-level QWK measurements provides more stable and informative rewards than either approach alone.
- Evidence anchors:
  - [abstract] "To ensure stable training, we introduce trait-wise comparison for QWK, integrating them with the batch-wise calculation to construct a bidirectional QWK reward."
  - [section 4.2] "r_Q(S, Ŝ) = λ · QB + (1 − λ) · QT for in-batch prediction and actual score vectors Ŝ and S, respectively."
- Break condition: If the weighting parameter λ cannot be effectively optimized or if the trait-wise calculations become computationally prohibitive.

### Mechanism 3
- Claim: The multi-rewards function with MSE penalty provides complementary feedback that enhances both overall agreement and individual score accuracy.
- Mechanism: The MSE reward measures exact differences between predicted and actual scores at the trait level, providing direct feedback on scoring precision. When combined with QWK rewards that capture ordinal agreement, this dual approach addresses both holistic scoring quality and fine-grained accuracy.
- Core assumption: MSE and QWK rewards provide complementary information that, when combined, improve overall scoring performance more than either metric alone.
- Evidence anchors:
  - [abstract] "We introduce multiple rewards: bidirectional QWK (rQ) and mean trait-wise MSE reward (rM)."
  - [section 4.2] "MSE aggregates the exact difference between predicted and actual scores; hence, it provides a clear yet simple indication of individual score deviations from true labels."
- Break condition: If the MSE penalty overwhelms the QWK rewards or if the combination leads to conflicting gradient signals.

## Foundational Learning

- Concept: Reinforcement Learning with Policy Gradients
  - Why needed here: The non-differentiable nature of QWK prevents direct optimization through standard backpropagation, requiring a reinforcement learning approach that can optimize rewards directly.
  - Quick check question: What is the fundamental difference between policy gradient methods and supervised learning when it comes to handling non-differentiable reward functions?

- Concept: Quadratic Weighted Kappa (QWK)
  - Why needed here: QWK is the standard evaluation metric for AES systems that measures agreement between human raters and model predictions while accounting for the ordinal nature of essay scores.
  - Quick check question: How does QWK differ from simple accuracy or mean squared error in evaluating ordinal scoring tasks?

- Concept: Autoregressive Generation
  - Why needed here: The autoregressive framework allows generation of score sequences with token-level probabilities, which is essential for applying reinforcement learning to scoring tasks.
  - Quick check question: Why is autoregressive generation more suitable for reinforcement learning in scoring tasks compared to classification approaches?

## Architecture Onboarding

- Component map:
  Essay input -> T5-based autoregressive score generation model -> Multi-rewards function (bidirectional QWK + MSE) -> PPO policy update -> KL regularization with anchor model -> Training loop with dynamic loss weights

- Critical path:
  1. Generate complete score sequence for essay
  2. Calculate bidirectional QWK and MSE rewards
  3. Apply token-wise KL regularization with anchor model
  4. Update policy using PPO with generalized advantage estimation
  5. Adjust multi-loss weights dynamically

- Design tradeoffs:
  - Autoregressive vs. classification: Autoregressive enables RL but may be slower; classification is faster but cannot directly optimize QWK
  - Bidirectional vs. unidirectional QWK: Bidirectional provides more stable training but requires more computation
  - Fixed vs. dynamic loss weights: Dynamic weights adapt to training progress but add complexity

- Failure signatures:
  - Mode collapse: Model generates very similar scores for all essays
  - Reward hacking: Model optimizes rewards without improving actual scoring quality
  - Instability: Training loss fluctuates wildly or fails to converge

- First 3 experiments:
  1. Compare autoregressive baseline (without RL) vs. SaMRL on Overall trait QWK to verify RL benefits
  2. Test bidirectional QWK vs. unidirectional QWK to confirm stability improvements
  3. Validate multi-rewards vs. single reward approaches to demonstrate complementary effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the autoregressive generation order affect the final scoring performance, and could alternative ordering strategies improve results?
- Basis in paper: [explicit] The paper notes that "prediction order may matter" and uses the same order as the previous work (Do et al., 2024), but suggests that "thoroughly considering the shifts in trait prediction order can lead to further improvements"
- Why unresolved: The paper only uses a fixed prediction order without exploring alternative strategies or their impact on performance
- What evidence would resolve it: Systematic experiments comparing different trait prediction orders (e.g., random, size-based, score-range-based) and their effects on scoring accuracy

### Open Question 2
- Question: Would updating the policy after each token generation (rather than after complete sequence generation) improve the learning efficiency and final scoring performance?
- Basis in paper: [explicit] The paper mentions that "training the policy with the instant updating per each action (i.e., token generation) might bring in more benefits in the case of the scoring task, which can be noteworthy for future work"
- Why unresolved: The paper only implements batch updates after complete sequence generation, leaving unexplored whether more frequent updates could enhance learning
- What evidence would resolve it: Comparative experiments between batch-level updates and token-level updates, measuring both training efficiency and final scoring accuracy

### Open Question 3
- Question: What is the optimal weight assignment strategy for the multi-loss optimization, and could alternative adaptive weight adjustment mechanisms improve performance?
- Basis in paper: [explicit] The paper implements dynamic weight learning for multiple losses but notes that "adaptive optimization strategies or more refined mechanisms could extend the impact of our method"
- Why unresolved: While the paper shows that dynamic weight learning improves performance over fixed weights, it doesn't explore other optimization strategies or mechanisms
- What evidence would resolve it: Comparative analysis of different weight adjustment mechanisms (e.g., gradient-based, uncertainty-based, meta-learning approaches) and their impact on scoring performance

## Limitations
- The bidirectional QWK reward mechanism relies on a carefully tuned weighting parameter λ that may be dataset-specific
- The autoregressive generation framework introduces computational overhead and potential coherence issues in score sequence generation
- KL regularization with frozen anchor models assumes the anchor provides stable and meaningful target distributions, limiting exploration

## Confidence
- **High Confidence**: The fundamental premise that autoregressive generation enables RL for non-differentiable metrics like QWK is well-supported by the architecture and experimental results
- **Medium Confidence**: The bidirectional QWK mechanism's superiority over unidirectional approaches is demonstrated, but the optimal weighting strategy and its generalizability across different datasets remain somewhat empirical
- **Medium Confidence**: The multi-rewards function combining QWK and MSE shows consistent benefits, but the dynamic weighting strategy's robustness across different training scenarios could be further validated

## Next Checks
1. **Ablation Study on Bidirectional Components**: Systematically vary the λ parameter between trait-wise and batch-wise QWK across multiple datasets to establish a more generalizable weighting strategy and test the stability of the bidirectional approach under different conditions

2. **Anchor Model Sensitivity Analysis**: Compare performance when using different anchor models (including no anchor, randomly initialized, and pre-trained models) to quantify the impact of KL regularization on final scoring quality and training stability

3. **Score Sequence Coherence Evaluation**: Implement automated checks for score sequence validity (e.g., ensuring generated scores follow logical patterns, don't contain impossible combinations) and measure the correlation between sequence coherence and final QWK scores to validate the autoregressive generation's practical utility