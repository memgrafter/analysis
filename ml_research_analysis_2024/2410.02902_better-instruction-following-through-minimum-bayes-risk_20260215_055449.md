---
ver: rpa2
title: Better Instruction-Following Through Minimum Bayes Risk
arxiv_id: '2410.02902'
source_url: https://arxiv.org/abs/2410.02902
tags:
- decoding
- greedy
- prometheus
- performance
- alpacaeval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores using Minimum Bayes Risk (MBR) decoding with
  LLM judges to improve instruction-following models. MBR decoding uses a reference-based
  evaluator to select high-quality outputs from a set of candidates, overcoming the
  limitation of reference-free LLM judges.
---

# Better Instruction-Following Through Minimum Bayes Risk

## Quick Facts
- arXiv ID: 2410.02902
- Source URL: https://arxiv.org/abs/2410.02902
- Reference count: 40
- Primary result: MBR decoding with LLM judges improves instruction-following performance across model sizes up to 70B parameters

## Executive Summary
This work introduces Minimum Bayes Risk (MBR) decoding with LLM judges as an effective method for improving instruction-following model performance. By using Prometheus-2-7B to evaluate candidate outputs pairwise and selecting the one with highest average utility, MBR decoding consistently outperforms greedy and best-of-N decoding across Llama2 and Llama3 models. The approach is particularly effective because it leverages reference-based evaluation rather than reference-free scoring, enabling smaller judge models to effectively supervise much larger generators.

## Method Summary
The method employs MBR decoding where multiple candidates are generated per prompt, then evaluated pairwise using Prometheus-2-7B judge to compute utility scores. The candidate with highest average utility across all comparisons is selected as output. For self-training, preference pairs are created from MBR-decoded outputs and used to train models via Direct Preference Optimization (DPO), enabling greedy decoding to match MBR performance without the computational overhead.

## Key Results
- Prometheus-2-7B MBR decoding with Llama2-7B outperforms Llama2-13B with greedy decoding
- MBR gains are consistent across models up to 70B parameters
- Self-trained models with greedy decoding generally match or exceed base models with MBR decoding
- MBR decoding provides consistent improvements on both AlpacaEval 2.0 and MT-Bench benchmarks

## Why This Works (Mechanism)

### Mechanism 1
MBR decoding selects outputs with highest consensus quality by averaging utility scores across pseudo-references. The approach generates multiple candidates, evaluates each against all others using a reference-based evaluator, and selects the candidate with highest average utility. This works when the LLM judge provides consistent utility scores that correlate with human preferences.

### Mechanism 2
MBR decoding provides consistent gains across model sizes because the judge's evaluation capability is independent of the generator's size. Even small 7B judges can effectively supervise much larger models, making the approach scalable. This assumes the judge model maintains consistent evaluation quality regardless of the generator model's scale.

### Mechanism 3
Iterative self-training on MBR-decoded outputs using DPO enables models to internalize MBR selection behavior. The model learns to generate outputs that would score highly under MBR evaluation by training on preference pairs selected from MBR-decoded outputs, effectively transferring the MBR selection behavior into the model's generation distribution.

## Foundational Learning

- **Concept: Minimum Bayes Risk (MBR) decoding** - Needed because MBR is the core algorithmic mechanism that enables selection of high-quality outputs based on consensus evaluation rather than individual likelihood. Quick check: What distinguishes MBR from best-of-N when using the same evaluator?

- **Concept: Reference-based vs reference-free evaluation** - Needed because the paper demonstrates reference-based evaluation (MBR) outperforms reference-free evaluation (BoN) even with the same LLM judge. Quick check: Why does MBR outperform BoN when both use the same LLM judge?

- **Concept: Direct Preference Optimization (DPO)** - Needed because DPO is the specific training method used for self-training that enables models to learn from preference pairs selected by MBR decoding. Quick check: What makes DPO particularly suitable for learning from MBR-selected preference pairs compared to standard SFT?

## Architecture Onboarding

- **Component map**: Generator LLM → Candidate generation → Pairwise utility computation → Average utility calculation → Final output selection. For self-training: Preference pair selector → DPO training loop → Updated generator model.

- **Critical path**: During inference: prompt → candidate generation (Ncand samples) → pairwise utility computation (Ncand² evaluations) → average utility calculation → final output selection. The bottleneck is the quadratic number of utility evaluations required for MBR decoding.

- **Design tradeoffs**: MBR decoding trades increased inference compute (linear in candidates, quadratic in evaluations) for improved output quality. Self-training trades initial compute cost for reduced inference-time costs by internalizing MBR selection behavior. Smaller judge models trade some evaluation quality for significantly reduced computation.

- **Failure signatures**: Poor performance indicates either (1) inconsistent judge utility scores, (2) insufficient candidate diversity, (3) utility metric doesn't correlate with human preferences, or (4) self-training fails to capture MBR selection behavior.

- **First 3 experiments**:
  1. Implement MBR decoding with heuristic utility metric (like ROUGE) to verify basic mechanism
  2. Compare MBR vs BoN decoding with same LLM judge to isolate consensus selection benefit
  3. Test MBR self-training with small dataset to verify approach can internalize MBR behavior

## Open Questions the Paper Calls Out

- **Open Question 1**: How do different dense embedders perform as MBR utility metrics, and what properties make a good MBR embedder? The paper only tested SFR-Embedder and found it performed poorly compared to Prometheus.

- **Open Question 2**: How does the choice of scoring rubric affect Prometheus MBR decoding performance, and can question-specific rubrics further improve results? The paper used a single generic scoring rubric and hypothesized performance could improve with question-specific adjustments.

- **Open Question 3**: Can iterative MBR distillation be combined with other self-training methods or inference-time algorithms to achieve even greater performance gains? The paper explored MBR distillation with DPO but didn't investigate combining it with other methods.

## Limitations

- Judge consistency across model scales hasn't been systematically validated, which could limit scalability claims
- DPO vs SFT generalization to different instruction-following datasets remains untested
- Computational tradeoff characterization lacks quantitative analysis of break-even points

## Confidence

- **High confidence** in MBR decoding's effectiveness with reference-based LLM judges
- **Medium confidence** in the self-training approach due to unclear training specifications
- **Low confidence** in the cross-model supervision claim due to selective validation

## Next Checks

1. Test Prometheus-2-7B's evaluation consistency by having it score outputs from different model sizes (7B vs 70B) and measuring inter-rater reliability

2. Systematically vary the number of MBR candidates (4, 8, 12, 16) to identify diminishing returns and optimal cost-performance trade-off

3. Evaluate self-trained models on instruction-following datasets not seen during training to assess domain generalization and robustness