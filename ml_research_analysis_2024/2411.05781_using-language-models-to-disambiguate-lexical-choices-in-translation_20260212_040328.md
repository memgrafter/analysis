---
ver: rpa2
title: Using Language Models to Disambiguate Lexical Choices in Translation
arxiv_id: '2411.05781'
source_url: https://arxiv.org/abs/2411.05781
tags:
- lexical
- language
- rules
- variations
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies lexical selection in translation, where a single
  source word has multiple target-language variations. The authors create DTAiLS,
  a dataset of 1,377 sentence pairs across nine languages, validated by native speakers,
  to evaluate models' ability to select the correct target variation given source
  context.
---

# Using Language Models to Disambiguate Lexical Choices in Translation

## Quick Facts
- arXiv ID: 2411.05781
- Source URL: https://arxiv.org/abs/2411.05781
- Reference count: 15
- Key outcome: GPT-4 outperforms or matches NMT systems on lexical selection for 7 of 9 languages

## Executive Summary
This paper addresses lexical selection in translation, where a single source word has multiple target-language variations. The authors create DTAiLS, a dataset of 1,377 sentence pairs across nine languages validated by native speakers, to evaluate models' ability to select the correct target variation given source context. They compare instruction-tuned LLMs (GPT-4, LLaMA-3, Gemma) with NMT systems, finding GPT-4 outperforms NMTs on three languages and matches them on four. To bridge the performance gap for weaker models, they use LLMs to generate natural language rules describing target variations, which improve performance substantially—up to 23% accuracy gains—when provided to open-weight models. The best models still fall short of native speaker performance.

## Method Summary
The study constructs DTAiLS by extracting concept variations from parallel corpora across nine languages, then validating with native speaker lexical selection tasks. LLMs generate natural language rules describing target-language concept variations by prompting with source sentences containing each variation. These rules are verified by native speakers. The evaluation compares instruction-tuned LLMs (GPT-4, LLaMA-3, Gemma) with NMT systems (MADLAD-400, NLLB-200) on lexical selection in three settings: no rules, self-generated rules, and rules generated by GPT-4. Accuracy is measured as the proportion of correct lexical variation selections.

## Key Results
- GPT-4 achieves 67-85% accuracy across languages, outperforming NMT systems on 3 languages and matching on 4
- Providing rules to open-weight models improves accuracy substantially, up to 23% gains
- Rule-based improvements can reach or exceed GPT-4's performance in some cases
- Native speakers still outperform all evaluated models on the lexical selection task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Instruction-tuned language models can generate high-quality natural language rules describing target-language concept variations from English sentences.
- **Mechanism**: LLMs are prompted with source-language sentences containing each variation, and they use their parametric knowledge to generate descriptive rules explaining differences between variations. Native speakers verify these rules are accurate.
- **Core assumption**: LLMs have sufficient parametric knowledge of cross-lingual concept variations to generate accurate descriptions.
- **Evidence anchors**:
  - [abstract] "we use language models to generate English rules describing target-language concept variations... Table 3 shows these rules are overwhelmingly correct according to native speakers"
  - [section] "we construct one prompt per concept including all target-language variations and their respective lists of source-language sentences, and prompt each model to provide a natural language description of each target-language variation"
  - [corpus] Weak - the corpus shows related work on lexical selection but doesn't directly address rule generation quality
- **Break condition**: If LLMs lack parametric knowledge of concept variations, the generated rules would be inaccurate and fail to help models disambiguate lexical choices.

### Mechanism 2
- **Claim**: Providing open-weight LLMs with accurate rules improves their lexical selection accuracy substantially, sometimes exceeding GPT-4's performance.
- **Mechanism**: Open-weight models have weaker parametric knowledge of concept variations. When provided with accurate rules as context, they can better apply this knowledge to select the correct target variation.
- **Core assumption**: Open-weight models can effectively leverage external rules when provided in-context.
- **Evidence anchors**:
  - [abstract] "Providing weaker models with high-quality lexical rules improves accuracy substantially, in some cases reaching or outperforming GPT-4"
  - [section] "We observe improvements in performance across all LLMs when prompted with accurate self-generated rules... When providing the open-weight models with rules acquired from the strongest model (GPT-4), we see total improvements up to 23% in accuracy"
  - [corpus] Weak - corpus shows related work on lexical selection but doesn't directly address rule-based improvement
- **Break condition**: If open-weight models cannot effectively use in-context rules or the rules are not accurate enough, performance improvements would not be observed.

### Mechanism 3
- **Claim**: GPT-4 outperforms NMT systems on lexical selection for some languages, while being comparable on others, despite NMT systems being specialized for translation.
- **Mechanism**: GPT-4's strong instruction-following capabilities and cross-lingual knowledge allow it to effectively disambiguate lexical choices based on context, sometimes surpassing specialized NMT systems.
- **Core assumption**: GPT-4's general language understanding capabilities are sufficient for lexical selection tasks across multiple languages.
- **Evidence anchors**:
  - [abstract] "with the best-performing model, GPT-4, achieving from 67 to 85% accuracy across languages... GPT-4 is competitive with NMT systems"
  - [section] "the best-performing LLM, achieves a 4-15% absolute improvement in performance over the NMT systems... For the remaining three languages, the best-performing LLM, achieves a 4-15% absolute improvement in performance over the NMT systems"
  - [corpus] Weak - corpus shows related work on lexical selection but doesn't directly address LLM vs NMT comparison
- **Break condition**: If GPT-4's cross-lingual knowledge is insufficient for certain languages or contexts, it would fail to outperform specialized NMT systems.

## Foundational Learning

- **Concept**: Lexical selection in translation
  - **Why needed here**: The paper's core task is identifying the correct target-language variation for ambiguous source words based on context
  - **Quick check question**: Given the sentence "She brought me dates from her trip" and Farsi variations khorma (dried), rotab (fresh), and kharak (unripe), which variation would be most appropriate and why?

- **Concept**: Concept variation in cross-lingual contexts
  - **Why needed here**: Understanding that single concepts in source languages can have multiple variations in target languages based on context is fundamental to the task
  - **Quick check question**: How many variations might the English word "bank" have in a target language that distinguishes between financial institutions and riverbanks?

- **Concept**: Instruction-tuned language models
  - **Why needed here**: The paper evaluates instruction-tuned models like GPT-4, LLaMA-3, and Gemma for their ability to perform lexical selection and generate rules
  - **Quick check question**: What distinguishes instruction-tuned models from base models in terms of their capabilities for following task-specific prompts?

## Architecture Onboarding

- **Component map**: Parallel corpus extraction -> Concept variation identification -> Dataset construction with native speaker validation -> Rule generation -> Model evaluation
- **Critical path**: 1) Identify concepts with variations in parallel corpora, 2) Construct dataset with native speaker validation, 3) Generate rules using LLMs, 4) Evaluate models with and without rules on lexical selection task
- **Design tradeoffs**: Using native speakers for validation ensures data quality but limits dataset size; generating rules with LLMs is cheaper than manual rule creation but depends on model capabilities; comparing with NMT systems provides strong baselines but requires access to high-quality models
- **Failure signatures**: Low inter-annotator agreement indicates ambiguous examples or unclear instructions; poor rule generation quality suggests insufficient parametric knowledge in LLMs; weak model performance despite rules indicates models cannot effectively use in-context information
- **First 3 experiments**: 1) Verify the concept variation extraction pipeline on a small subset of parallel data, 2) Test rule generation quality with a few concepts before scaling up, 3) Evaluate a simple baseline model on the lexical selection task before testing more complex models

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Dataset size limited to 1,377 sentence pairs across nine languages, potentially limiting generalizability
- Evaluation restricted to English source language and specific target languages, raising questions about performance on other language pairs
- Effectiveness of rule-based improvements depends heavily on quality of LLM-generated rules, which may vary across different concept types or language pairs
- Comparison with NMT systems uses specific models that may not represent the full spectrum of available translation systems

## Confidence
**High Confidence**: The observation that GPT-4 outperforms or matches NMT systems on lexical selection for most languages; the finding that providing rules to open-weight models improves accuracy substantially (up to 23%); the conclusion that native speakers still outperform all evaluated models

**Medium Confidence**: The claim that LLM-generated rules are "overwhelmingly correct" according to native speakers (based on validation but not quantified); the assertion that rule-based improvements can reach or exceed GPT-4 performance (observed but may be context-dependent)

**Low Confidence**: The generalizability of findings to languages not included in the study; the scalability of the approach to larger datasets or more complex lexical disambiguation tasks; the long-term effectiveness of LLM-generated rules as models continue to evolve

## Next Checks
1. **Dataset Expansion Validation**: Replicate the study using an expanded dataset that includes additional language pairs beyond English as the source language, testing whether the observed patterns hold across different source-target combinations.

2. **Rule Quality Quantification**: Conduct a systematic evaluation of LLM-generated rule quality by having multiple native speakers independently assess rule accuracy, providing quantitative measures of precision and inter-annotator agreement.

3. **Cross-Model Rule Transferability**: Test whether rules generated by GPT-4 transfer effectively to other LLM architectures beyond the three models evaluated, examining whether the improvements are model-specific or generalizable across different instruction-tuned models.