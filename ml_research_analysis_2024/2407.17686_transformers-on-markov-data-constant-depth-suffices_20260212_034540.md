---
ver: rpa2
title: 'Transformers on Markov Data: Constant Depth Suffices'
arxiv_id: '2407.17686'
source_url: https://arxiv.org/abs/2407.17686
tags:
- layer
- attention
- transformer
- markov
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies transformers' ability to model Markov processes,
  focusing on how depth, number of heads, and non-linearities interact in learning
  k-th order Markov processes. Empirically, they find that 2- and 3-layer transformers
  with just 1 head can learn k-th order Markov processes for surprisingly large k
  (up to 4 and 8 respectively), contradicting prior findings that required linear
  scaling of heads with k.
---

# Transformers on Markov Data: Constant Depth Suffices

## Quick Facts
- arXiv ID: 2407.17686
- Source URL: https://arxiv.org/abs/2407.17686
- Reference count: 40
- Primary result: Layer normalization enables 3-layer transformers with 1 head to learn k-th order Markov processes, while attention-only transformers need Ω(log k) layers or Ω(k) heads

## Executive Summary
This paper investigates how transformer depth, width, and architectural components affect their ability to model k-th order Markov processes. Through extensive empirical experiments and theoretical analysis, the authors demonstrate that 2- and 3-layer transformers with just 1 head can learn surprisingly high-order Markov processes (up to k=8), contradicting prior findings that required linear scaling of heads with k. The key insight is that layer normalization plays a critical role in enabling constant-depth transformers to efficiently represent conditional k-grams, while attention-only transformers require near-linear scaling of heads or logarithmic scaling of layers.

## Method Summary
The paper trains transformers on data drawn from k-th order Markov processes, where the next symbol depends only on the previous k symbols. The experiments use 2- and 3-layer transformers with 1 head per layer, relative position encodings, and layer normalization. Training is performed on randomly generated Markov processes sampled from a Dirichlet prior, with sequence lengths and embedding dimensions grid-searched. The authors evaluate test loss and analyze attention patterns to determine whether transformers learn the conditional k-gram model, which represents the in-context MLE of the Markov process.

## Key Results
- 2- and 3-layer transformers with 1 head can learn k-th order Markov processes for k up to 4 and 8 respectively
- Layer normalization enables constant-depth transformers to represent k-th order induction heads efficiently
- Attention-only transformers require Ω(log k) layers or Ω(k) heads to represent k-th order induction heads under natural assumptions
- 1-layer transformers need near-linear scaling of heads/embedding dimension to represent conditional k-grams

## Why This Works (Mechanism)

### Mechanism 1
Layer normalization enables constant-depth transformers to represent kth-order induction heads by normalizing the embeddings to unit norm before applying dot-product attention. In the construction, the first layer computes dyadic sums of one-hot vectors (zi = ∑ 2j * exi-j), which capture information about the previous k symbols. Layer normalization then normalizes these vectors to unit norm (zi/||zi||). The second layer uses dot-product attention between zi-1/||zi-1|| and zn/||zn||, which under this special structure is equivalent to L2-norm attention. As temperature κ grows, this attention pattern concentrates on positions where zi-1 = zn, which occurs exactly when the previous k symbols match the final k symbols.

### Mechanism 2
Depth provides exponential efficiency gains in capturing long-range dependencies compared to width (number of heads). With k heads in 2 layers, each head tracks one of the previous k positions separately. With O(log k) layers and 1 head, the model composes attention layers to aggregate information from exponentially larger windows: layer 1 aggregates current and previous position, layer 2 aggregates information from a window of size 4, and so on, until layer L aggregates from a window of size 2^L.

### Mechanism 3
Attention-only transformers require near-linear scaling of heads/embedding dimension with k, while standard transformers with nonlinearities can achieve constant depth. Under Assumption 6.2 (attention patterns in layers 1 to L-1 depend only on positions, not input content), attention-only transformers need H ≥ k-3 heads in layer 1 to represent kth-order induction heads. This follows from rank bounds showing the key-query dot-product cannot uniquely identify matching k-grams unless sufficient heads are available to capture the necessary rank.

## Foundational Learning

- Concept: Markov processes and conditional k-gram models
  - Why needed here: The paper studies transformers on data drawn from kth-order Markov processes, where understanding the conditional distribution of the next symbol given the previous k symbols is fundamental to the problem setup
  - Quick check question: What is the difference between a kth-order Markov process and the conditional k-gram model? (Answer: Markov process is a generative model with transition kernel P, while conditional k-gram is the empirical estimate from a specific sequence)

- Concept: Transformer architecture components (attention, layer normalization, feedforward)
  - Why needed here: The paper analyzes how different architectural components (especially layer normalization) affect the ability to represent Markov processes, requiring understanding of how each component contributes to the overall function
  - Quick check question: What role does layer normalization play in the 3-layer constant-depth construction? (Answer: It normalizes the dyadic sum embeddings to unit norm, enabling the equivalence between dot-product and L2-norm attention)

- Concept: Induction heads and their generalization to kth-order
  - Why needed here: The paper proves results about representing kth-order induction heads, which are crucial for tracking positions where the previous k symbols match the final k symbols in the sequence
  - Quick check question: How does a kth-order induction head generalize the concept of a regular induction head? (Answer: Regular induction head tracks positions where the immediately previous symbol matches, while kth-order tracks positions where all k previous symbols match)

## Architecture Onboarding

- Component map: Input embeddings (Emb) + Position encodings (pp) → Multi-head attention (attn) → Layer normalization (LN) → Feedforward (FFN) → Output linear layer

- Critical path:
  - For attention-only: Emb + relative position encodings → Multi-head attention → Value projection → Residual connection
  - For standard: Emb + relative position encodings → Multi-head attention → LN → FFN → Residual connection
  - For constant-depth: Layer 1 computes dyadic sums → Layer 2 computes previous dyadic sum → Layer 3 applies attention between normalized vectors

- Design tradeoffs:
  - Depth vs width: O(log k) depth with 1 head vs O(k) heads with 2 layers
  - Nonlinearities: Attention-only requires linear scaling of parameters, while standard with LN achieves constant depth
  - Precision: Bit-precision requirements for representing conditional k-grams (Ω(log T + k) bits per parameter)

- Failure signatures:
  - Attention-only with insufficient heads: Cannot learn higher-order Markov processes (test loss remains high)
  - Missing layer normalization: Cannot achieve constant depth (requires O(log k) layers instead of 3)
  - Insufficient bit-precision: Additive approximation error grows beyond acceptable bounds

- First 3 experiments:
  1. Train 2-layer, 1-head transformer on 2nd-order Markov source: Expect failure (should need more heads or layers)
  2. Train 3-layer, 1-head transformer on 4th-order Markov source: Should succeed based on empirical results
  3. Train attention-only 2-layer, k-head transformer on kth-order Markov source: Should succeed, demonstrating width vs depth tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
How do layer normalization and non-linearities specifically interact to enable constant-depth transformers to learn kth-order Markov processes more efficiently than attention-only transformers? The paper demonstrates that a 3-layer transformer with layer normalization and 1 head can represent kth-order induction heads, while attention-only transformers require O(log k) layers. The authors note that layer normalization plays a critical role in realizing L2-norm attention patterns that distinguish between matching and non-matching sequences. This remains unresolved because the paper provides a construction showing how layer normalization enables this efficiency gain, but doesn't fully explain the mechanistic difference between the two architectures or why this particular form of non-linearity is so effective.

### Open Question 2
What are the fundamental limits of 1-layer transformers for representing conditional k-grams, and how does this scale with precision? Theorem 6.1 shows that 1-layer transformers require near-linear scaling of heads/embedding dimension to represent conditional 3-grams, with a lower bound of 2(H + dp + 2) ≥ T/3 for T-length sequences. This remains unresolved because the theorem provides a worst-case lower bound but doesn't characterize the exact trade-off between precision, depth, and representational capacity for practical sequence lengths.

### Open Question 3
How robust are the conditional lower bounds on attention-only transformers to relaxations of Assumption 6.2? The paper proves that attention-only transformers need Ω(log k) layers under the assumption that early layer attention patterns are position-dependent rather than input-dependent, but notes this assumption is based on empirical observations. This remains unresolved because the assumption is plausible but not proven to hold universally, and the paper acknowledges that transformers with small input-dependent variance might still be constrained by these bounds approximately.

## Limitations

- Theoretical results rely heavily on Assumption 6.2 about position-only attention patterns, which remains unproven for actual learned attention
- Experimental validation limited to relatively small k values (up to 8), not exploring theoretical limits
- Equivalence between dot-product and L2-norm attention depends on specific dyadic sum structure that may not generalize

## Confidence

**High confidence**: The theoretical lower bounds for attention-only transformers requiring Ω(log k) layers are well-founded and rely on established rank bounds. The empirical demonstration that 3-layer transformers can learn k=4 and k=8 Markov processes with 1 head is straightforward to verify.

**Medium confidence**: The proof that 3-layer transformers with layer normalization can represent k-th order induction heads is mathematically sound, but the construction's practical robustness is uncertain. The claim about layer normalization being "critical" is supported by the theoretical construction but lacks ablation experiments removing LN.

**Low confidence**: The claim that attention-only transformers need Ω(k) heads for 2-layer architectures under Assumption 6.2, while theoretically proven, may not reflect practical learning dynamics where attention patterns could depend on content rather than just positions.

## Next Checks

1. **Ablation on layer normalization**: Train otherwise identical 3-layer transformers with and without layer normalization on k=8 Markov data to empirically verify LN's critical role in achieving constant depth.

2. **Stress test the dyadic sum construction**: Modify the theoretical construction to use different basis vectors for encoding previous symbols (not powers of 2) and verify whether the attention equivalence breaks down as expected.

3. **Scale limit experiments**: Systematically test the maximum k values achievable by 2-layer and 3-layer transformers with 1 head, comparing against theoretical predictions to identify where the constant-depth regime breaks down.