---
ver: rpa2
title: A Lightweight Spatiotemporal Network for Online Eye Tracking with Event Camera
arxiv_id: '2404.08858'
source_url: https://arxiv.org/abs/2404.08858
tags:
- temporal
- event
- network
- spatial
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a lightweight causal spatiotemporal convolutional
  network designed for efficient online eye tracking using event cameras. The method
  employs a sequential stack of causal temporal and spatial convolutional blocks,
  achieving high efficiency through FIFO buffering, mixed batch and group normalization,
  and depthwise-separable convolutions.
---

# A Lightweight Spatiotemporal Network for Online Eye Tracking with Event Camera

## Quick Facts
- arXiv ID: 2404.08858
- Source URL: https://arxiv.org/abs/2404.08858
- Reference count: 21
- Primary result: Achieves 0.9916 p10 accuracy on AIS 2024 private test set with lightweight causal spatiotemporal CNN for online eye tracking

## Executive Summary
This work presents a causal spatiotemporal convolutional network specifically designed for efficient online eye tracking using event cameras. The method leverages sequential causal temporal and spatial convolutional blocks, FIFO buffering, and mixed normalization strategies to achieve high efficiency while maintaining accuracy. The network demonstrates exceptional performance on the AIS 2024 private test set with significant activation sparsity achieved through L1 regularization, making it suitable for edge deployment scenarios.

## Method Summary
The approach employs a lightweight causal spatiotemporal convolutional network architecture that processes event camera data for eye tracking. Key innovations include a causal event volume binning method that preserves temporal information while minimizing latency, mixed batch and group normalization for improved training stability, and depthwise-separable convolutions for computational efficiency. The model incorporates L1 activity regularization to achieve over 90% activation sparsity, enabling significant speedups on sparsity-aware processors. Spatial and temporal affine augmentations of event data substantially improve generalization across different conditions.

## Key Results
- Achieves 0.9916 p10 accuracy on AIS 2024 private test set
- Demonstrates over 90% activation sparsity through L1 regularization
- Shows strong trade-offs between accuracy, model size, and computational cost
- Validates suitability for edge deployment with lightweight architecture

## Why This Works (Mechanism)
The network's efficiency stems from causal convolutions that process temporal data sequentially without latency, depthwise-separable convolutions that reduce parameter count while maintaining representational power, and FIFO buffering that enables streaming inference. The causal event volume binning method preserves temporal dynamics essential for eye movement tracking while the mixed normalization strategy stabilizes training across varying event densities. L1 regularization induces sparse activations that can be exploited by specialized hardware for acceleration, while spatial and temporal augmentations enhance robustness to diverse eye tracking conditions.

## Foundational Learning
- Causal convolutions: Required to maintain temporal order and enable real-time processing; Quick check: Verify temporal dependencies are preserved in all layers
- Event camera data representation: Essential for capturing asynchronous temporal information; Quick check: Confirm event volume binning preserves motion characteristics
- Sparsity regularization: Enables computational acceleration on specialized hardware; Quick check: Validate that 90% sparsity maintains accuracy
- Depthwise-separable convolutions: Reduces parameters while maintaining accuracy; Quick check: Compare against standard convolutions for accuracy-efficiency trade-off
- Mixed normalization: Stabilizes training across varying input statistics; Quick check: Test normalization effectiveness across different event densities
- Temporal encoding methods: Critical for preserving motion information; Quick check: Benchmark causal binning against alternative temporal encodings

## Architecture Onboarding
- Component map: Input events -> Causal temporal blocks -> Spatial blocks -> Mixed normalization -> Output prediction
- Critical path: Event volume formation → Causal temporal convolution → Spatial convolution → Normalization → Classification
- Design tradeoffs: Causal processing enables online inference but limits context window; Sparsity regularization enables acceleration but requires specialized hardware
- Failure signatures: Latency spikes during high event rates, accuracy degradation with sparse events, context loss in causal processing
- First experiments: 1) Measure latency with varying event densities, 2) Test accuracy with different context window sizes, 3) Evaluate sparsity pattern consistency across input conditions

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Evaluation focused primarily on AIS 2024 private test set, limiting generalizability to other datasets
- 0.9916 p10 accuracy metric may not capture all aspects of eye tracking quality
- Sparsity regularization benefits not fully characterized across diverse edge devices
- Novel causal event volume binning lacks detailed comparison to alternative temporal encoding approaches

## Confidence
- Model architecture and efficiency claims: High
- Performance metrics on AIS 2024: Medium
- Sparsity regularization benefits: Medium

## Next Checks
1. Cross-dataset validation: Evaluate the model on multiple eye tracking datasets beyond AIS 2024 to assess generalization capabilities and potential overfitting to the specific test set.

2. Hardware platform testing: Implement and benchmark the model on various edge devices (not just sparsity-aware processors) to quantify real-world efficiency gains and identify potential bottlenecks.

3. Ablation studies on temporal encoding: Systematically compare the causal event volume binning method against alternative temporal encoding approaches to quantify the specific contribution to overall performance.