---
ver: rpa2
title: 'Exploring What Why and How: A Multifaceted Benchmark for Causation Understanding
  of Video Anomaly'
arxiv_id: '2412.07183'
source_url: https://arxiv.org/abs/2412.07183
tags:
- video
- anomaly
- ieee
- conference
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ECVA, a new benchmark for causation understanding
  of video anomalies that goes beyond traditional detection and localization by requiring
  models to answer "what," "why," and "how" questions about anomalous events. The
  benchmark includes 2,240 videos with detailed human annotations covering 21 major
  categories and 100 subcategories, with each video averaging 141 seconds in length.
---

# Exploring What Why and How: A Multifaceted Benchmark for Causation Understanding of Video Anomaly

## Quick Facts
- **arXiv ID:** 2412.07183
- **Source URL:** https://arxiv.org/abs/2412.07183
- **Reference count:** 40
- **Primary result:** AnomShield achieves state-of-the-art performance on causation tasks with significant improvements over existing methods.

## Executive Summary
This paper introduces ECVA, a new benchmark for causation understanding of video anomalies that goes beyond traditional detection and localization by requiring models to answer "what," "why," and "how" questions about anomalous events. The benchmark includes 2,240 videos with detailed human annotations covering 21 major categories and 100 subcategories, with each video averaging 141 seconds in length. To address the challenges of long-range temporal relationships and complex causal chains in these videos, the authors propose AnomShield, a prompt-based approach using "hard prompts" for non-uniform sampling focused on anomaly-relevant segments and "soft prompts" with a bidirectional Mamba-based method for extracting spatial-temporal relationships. They also introduce AnomEval, a specialized evaluation metric that assesses consistency, basic reasoning, and hallucination to better align with human judgment. Experimental results show that AnomShield achieves state-of-the-art performance on causation tasks, with significant improvements over existing methods, particularly on description and effect understanding tasks. The benchmark and approach provide a foundation for more realistic and comprehensive video anomaly understanding closer to real-world scenarios.

## Method Summary
The proposed AnomShield approach uses a prompt-based framework with two key components: "hard prompts" for non-uniform sampling that identifies and focuses on anomaly-relevant segments, and "soft prompts" using a bidirectional Mamba-based method to extract spatial-temporal relationships within these segments. The method involves three stages: visual-to-text alignment, long-text integration, and LoRA fine-tuning on mixed video datasets. The approach is evaluated using a novel GPT-based metric (AnomEval) that measures consistency, basic reasoning, and hallucination, providing a more comprehensive assessment of causation understanding compared to traditional metrics.

## Key Results
- AnomShield achieves state-of-the-art performance on causation tasks, outperforming existing methods on description and effect understanding
- The approach demonstrates superior ability to handle long-range temporal relationships and complex causal chains in video anomalies
- AnomEval metric shows better alignment with human judgment compared to traditional evaluation metrics like BLEU and ROUGE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The "hard prompt" strategy enables the model to focus on anomaly-relevant segments by non-uniform sampling, thereby improving performance on long video inputs.
- Mechanism: The approach uses a two-step sampling process: first, sparse uniform sampling extracts 32 frames divided into 8 segments. Then, a GPT-based retrieval step compares captions of each segment with the user's query to identify the top key segments most relevant to the anomaly. Dense sampling is then applied to these key segments (averaging 8 frames per segment) to capture nuanced details of the anomaly events.
- Core assumption: Anomaly-relevant information is concentrated in specific segments rather than uniformly distributed across the video, and GPT can effectively identify these segments through caption comparison.
- Evidence anchors:
  - [abstract]: "We utilize 'hard prompt' to guide the model to focus on the critical parts related to video anomaly segments"
  - [section]: "To effectively capture crucial cues within long videos, we propose to leverage chain-of-thought prompting to guide VLMs to concentrate on pivotal clues in the video pertinent to the provided questions"
  - [corpus]: Weak - the corpus contains related papers but doesn't directly address this specific sampling mechanism
- Break condition: If anomalies are uniformly distributed across videos or if the caption-query matching fails to identify relevant segments, this approach would lose its effectiveness.

### Mechanism 2
- Claim: The "soft prompt" using bidirectional Mamba-based method effectively extracts spatial-temporal relationships within anomaly segments, building a coherent cause-effect logic chain.
- Mechanism: After identifying key segments through hard prompts, each frame is split into M patches. A bidirectional Mamba-based connector module processes these patches to extract dynamic spatio-temporal features. The bidirectional processing (forward and backward directions) captures semantic relationships along image patch sequences. Position embeddings are added to retain temporal and spatial information before processing.
- Core assumption: The bidirectional Mamba architecture can effectively capture long-range spatial-temporal dependencies in video patches, and the cause-effect relationships in anomalies can be modeled through these dependencies.
- Evidence anchors:
  - [abstract]: "soft prompt' to establish temporal and spatial relationships within these anomaly segments"
  - [section]: "To capture the nuanced interactions across various levels of visual granularity, each frame is meticulously split into M patches... we develop a method based on mamba to extract the video spatial-temporal features"
  - [corpus]: Weak - the corpus contains related video understanding papers but doesn't specifically address this Mamba-based approach
- Break condition: If the Mamba-based connector cannot effectively model the complex cause-effect relationships, or if the spatial-temporal relationships are too complex for this architecture to capture.

### Mechanism 3
- Claim: AnomEval metric provides more reliable evaluation by addressing hallucination, consistency, and reasoning issues specific to video anomaly causation understanding.
- Mechanism: AnomEval combines three components: (1) Basic Reasoning - measures coverage of key entities and logic coherence using GPT, (2) Consistency - leverages GPT's stability with binary questions to measure answer-reference consistency, and (3) Hallucination - tests model understanding by comparing answers with and without key frames removed. Different weights are assigned to each component.
- Core assumption: VLMs often hallucinate answers based on training data rather than true video understanding, and this can be detected by removing key frames and checking for consistency changes.
- Evidence anchors:
  - [abstract]: "We propose AnomEval, a specialized evaluation metric crafted to align closely with human judgment criteria for ECV A"
  - [section]: "Due to the hallucination issue within VLM, the model may provide accurate answers based on training data without truly understanding the content of the video"
  - [corpus]: Weak - the corpus contains evaluation metrics papers but doesn't specifically address this hallucination-focused approach
- Break condition: If VLMs become robust to frame removal or if GPT evaluations become unreliable, this metric would lose effectiveness.

## Foundational Learning

- Concept: Video anomaly detection vs. video anomaly understanding
  - Why needed here: This work transitions from detecting anomalies (localization) to understanding them (causation, effects), requiring different model capabilities and evaluation metrics
  - Quick check question: What are the three key questions this benchmark addresses that go beyond traditional detection?

- Concept: Chain-of-thought prompting in multimodal contexts
  - Why needed here: The approach uses chain-of-thought prompting to guide VLMs through the reasoning process of identifying relevant video segments and building cause-effect chains
  - Quick check question: How does the two-step sampling process (sparse then dense) improve upon uniform sampling for anomaly understanding?

- Concept: Bidirectional processing for temporal relationships
  - Why needed here: The Mamba-based connector uses bidirectional processing to capture cause-effect relationships that may span across frames in both directions
  - Quick check question: Why is bidirectional processing important for understanding causal chains in video anomalies?

## Architecture Onboarding

- Component map: Video frames -> Chain-of-thought prompting (hard prompt) -> Sparse uniform sampling (32 frames → 8 segments) -> GPT-based retrieval -> Dense sampling around key segments (8 frames/segment) -> Spatial-temporal feature extraction (soft prompt) -> Frame splitting into M patches -> CLIP-L feature extraction -> Trainable position embeddings (temporal + spatial) -> Bidirectional Mamba-based connector -> Pooling and MLP alignment -> LLM inference (Mistral) -> Answers to "what," "why," and "how" questions

- Critical path: Hard prompt sampling → Soft prompt feature extraction → LLM inference → AnomEval assessment

- Design tradeoffs:
  - Sampling strategy: Uniform sampling is simpler but misses key segments; non-uniform sampling is more complex but captures relevant information
  - Model size: Larger LLMs (8B) show better performance but increase computational cost
  - Evaluation: Traditional metrics (BLEU, ROUGE) measure text similarity but miss semantic understanding; GPT-based metrics are more semantic but can be unstable

- Failure signatures:
  - Poor performance on description tasks suggests hard prompt sampling is missing relevant segments
  - Poor performance on causation tasks suggests soft prompt feature extraction cannot capture complex cause-effect relationships
  - High hallucination scores indicate the model is relying on training data rather than video understanding

- First 3 experiments:
  1. Ablation study: Compare uniform sampling vs. hard prompt sampling on a subset of videos to measure impact on key segment identification
  2. Connector comparison: Replace Mamba-based connector with simple MLP to quantify the benefit of bidirectional processing
  3. Evaluation validation: Human evaluation of model outputs vs. AnomEval scores to verify metric alignment with human judgment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the AnomShield approach compare to existing methods in terms of handling long-range temporal relationships in video anomaly understanding?
- Basis in paper: [explicit] The paper highlights the challenges of capturing long-range temporal relationships and complex causal chains in videos, and proposes AnomShield as a solution to address these challenges.
- Why unresolved: The paper provides experimental results showing the superiority of AnomShield, but does not provide a detailed comparison of its performance against existing methods specifically for long-range temporal relationships.
- What evidence would resolve it: A detailed comparative study of AnomShield against other methods, focusing on their ability to handle long-range temporal relationships in video anomaly understanding.

### Open Question 2
- Question: What is the impact of the AnomEval metric on the development of video large language models for causation understanding of video anomalies?
- Basis in paper: [explicit] The paper introduces AnomEval as a specialized evaluation metric for the ECVA dataset, claiming it to be more comprehensive, robust, and accurate than existing metrics.
- Why unresolved: While the paper presents experimental results showing the effectiveness of AnomEval, it does not explore its impact on the development of video large language models for causation understanding.
- What evidence would resolve it: A study on how the use of AnomEval as an evaluation metric influences the design and training of video large language models for causation understanding of video anomalies.

### Open Question 3
- Question: How can the AnomShield approach be extended to handle other types of anomalies beyond those included in the ECVA dataset?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of AnomShield on the ECVA dataset, which covers a wide range of anomaly types. However, it does not discuss the potential for extending the approach to handle other types of anomalies.
- Why unresolved: The paper focuses on the performance of AnomShield on the ECVA dataset and does not explore its potential for generalization to other types of anomalies.
- What evidence would resolve it: An investigation into the ability of AnomShield to handle other types of anomalies, either by modifying the approach or by training it on additional datasets covering different anomaly types.

## Limitations

- The paper relies heavily on GPT-based components for segment retrieval and evaluation, but lacks detailed implementation specifications and validation of GPT consistency
- The bidirectional Mamba-based connector architecture is described at a high level without sufficient architectural details for faithful reproduction
- The effectiveness of AnomEval as a more reliable metric than traditional approaches needs validation through human studies

## Confidence

**High confidence**: The benchmark construction (ECVA) with 2,240 videos and detailed annotations is well-specified and reproducible. The three-part causation question framework ("what," "why," "how") represents a clear advancement over traditional detection tasks.

**Medium confidence**: The AnomShield approach demonstrates state-of-the-art performance on causation tasks, particularly for description and effect understanding. However, this confidence is limited by the lack of architectural details for the Mamba-based connector and the GPT-based components.

**Low confidence**: The claim that AnomEval provides more reliable evaluation than traditional metrics lacks sufficient validation through human studies or comparison with alternative evaluation approaches.

## Next Checks

1. **Human evaluation validation**: Conduct a human study comparing AnomEval scores with human judgments of causation understanding quality. Have human annotators rate model outputs on consistency, reasoning, and hallucination detection, then correlate these ratings with AnomEval scores to validate the metric's alignment with human judgment.

2. **Architectural ablation study**: Perform controlled experiments replacing the bidirectional Mamba-based connector with alternative architectures (e.g., standard transformer, simple CNN) while keeping all other components constant. This would quantify the specific contribution of the Mamba architecture to the overall performance gains.

3. **Cross-dataset generalization test**: Evaluate AnomShield on established video anomaly detection datasets (UCF-Crime, ShanghaiTech) using the same prompting approach. This would test whether the method generalizes beyond the curated ECVA benchmark and validate its applicability to real-world anomaly detection scenarios.