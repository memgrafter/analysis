---
ver: rpa2
title: 'MambaMOS: LiDAR-based 3D Moving Object Segmentation with Motion-aware State
  Space Model'
arxiv_id: '2404.12794'
source_url: https://arxiv.org/abs/2404.12794
tags:
- point
- information
- temporal
- methods
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MambaMOS, a novel framework for LiDAR-based
  moving object segmentation (MOS) that addresses the issue of weak coupling between
  temporal and spatial information in existing methods. The proposed method leverages
  a motion-aware state space model (MSSM) to achieve deep-level coupling of temporal
  and spatial features, enabling the model to better understand motion states of objects.
---

# MambaMOS: LiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model

## Quick Facts
- arXiv ID: 2404.12794
- Source URL: https://arxiv.org/abs/2404.12794
- Authors: Kang Zeng; Hao Shi; Jiacheng Lin; Siyu Li; Jintao Cheng; Kaiwei Wang; Zhiyong Li; Kailun Yang
- Reference count: 40
- One-line primary result: Achieves state-of-the-art IoU_MOS of 82.3% on SemanticKITTI-MOS validation set and 80.1% on hidden test set, outperforming previous methods by 3.4% and 4.5% respectively

## Executive Summary
MambaMOS introduces a novel framework for LiDAR-based moving object segmentation that addresses the weak coupling between temporal and spatial information in existing methods. The approach leverages a motion-aware state space model (MSSM) combined with Time Clue Bootstrapping Embedding (TCBE) to achieve deep-level coupling of temporal and spatial features. By emphasizing temporal information as a dominant modality and using cross-product attention between single-scan appearance and multi-scan motion features, MambaMOS significantly outperforms previous state-of-the-art methods on SemanticKITTI-MOS and KITTI-Road benchmarks while maintaining computational efficiency through the use of state space models instead of transformers.

## Method Summary
MambaMOS processes aggregated 4D point clouds (F=8 scans) by first transforming past scans to the current scan perspective and voxelizing with 0.09m grid size. The Time Clue Bootstrapping Embedding (TCBE) module emphasizes temporal information through attention mechanisms by treating temporal and spatial information as separate channels and using element-wise multiplication to amplify temporal influence. The core Motion-aware State Space Model (MSSM) processes single-scan appearance features and multi-scan motion features through separate branches and fuses them via cross-product attention, enabling deep coupling between temporal and spatial information. The framework uses a U-Net style architecture with 5-stage encoder and 4-stage decoder, trained for 50 epochs with batch size 4 on 4 NVIDIA RTX A6000 GPUs using AdamW optimizer.

## Key Results
- Achieves IoU_MOS of 82.3% on SemanticKITTI-MOS validation set
- Achieves IoU_MOS of 80.1% on SemanticKITTI-MOS hidden test set
- Outperforms previous state-of-the-art methods by 3.4% and 4.5% respectively
- Demonstrates excellent generalization when fine-tuned on KITTI-Road dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MambaMOS strengthens temporal-spatial coupling by using Time Clue Bootstrapping Embedding (TCBE) to emphasize temporal information as a dominant modality before deep feature learning.
- Mechanism: TCBE treats temporal and spatial information as separate channels and uses attention-based multiplication of time-guided spatial features with initial coupled features to amplify temporal influence.
- Core assumption: Temporal information is the dominant signal for motion recognition, and spatial features alone are insufficient to distinguish moving objects from static ones with similar appearance.
- Evidence anchors:
  - [abstract]: "TCBE emphasizes the expressive power of temporal information through attention mechanisms and enhances the mutual coupling between temporal and spatial information by treating temporal information as an independent channel separate from spatial information."
  - [section 3.3]: "TCBE embeds the spatial and temporal information of each point... which serves as an alternative implementation of previous embedding methods... we employ a simple gating mechanism as the bottom branch of MSSM to allocate weights to features in each hidden state, thereby determining whether the features are expressed."
- Break condition: If temporal information does not correlate strongly with object motion in the dataset (e.g., slow-moving objects or static objects with time-varying appearance), the dominance assumption fails and coupling degrades.

### Mechanism 2
- Claim: Motion-aware State Space Model (MSSM) enables deep-level coupling by separating single-scan appearance features from multi-scan temporal features and fusing them via cross-product attention.
- Mechanism: MSSM processes single-scan features through a branch to capture object appearance, processes multi-scan features through another branch to capture motion patterns, and fuses them with cross-product attention so that motion context can influence spatial feature interpretation.
- Core assumption: The combination of single-scan spatial appearance and multi-scan temporal evolution is necessary and sufficient to identify moving objects, and these features are complementary.
- Evidence anchors:
  - [abstract]: "MSSM emphasizes the motion states of the same object at different time steps through two distinct temporal modeling and correlation steps."
  - [section 3.4]: "The main design idea of MSSM is to enhance the original Mamba's perception of temporal features regarding moving objects by using cross-product attention between single-scan features and multi-scan features."
- Break condition: If the point cloud sequences are too short (F too small) or contain very sparse motion patterns, the cross-product attention cannot establish meaningful motion cues and coupling collapses.

### Mechanism 3
- Claim: Using State Space Models (SSM) instead of Transformers reduces computational complexity while preserving long-range context for temporal modeling.
- Mechanism: SSM with selective scan mechanism models sequence dependencies linearly rather than quadratically, enabling efficient processing of long LiDAR point cloud sequences without attention bottleneck.
- Core assumption: Long-range temporal dependencies in LiDAR sequences can be captured effectively with linear-complexity SSM, and the quadratic complexity of Transformers is unnecessary for this task.
- Evidence anchors:
  - [section 3.1]: "the State Space Model (SSM) introduced by Mamba [15] offers a promising solution, providing us with the opportunity to achieve comparable long-range context modeling capabilities to the transformer [40] while maintaining linear time complexity."
  - [corpus]: Weak corpus evidence; SSM usage in LiDAR segmentation is rare, so empirical support is limited.
- Break condition: If the dataset requires fine-grained, non-local context (e.g., complex occlusions), the linear approximation of SSM may lose necessary detail that quadratic Transformer attention would capture.

## Foundational Learning

- Concept: Spatio-temporal coupling in point cloud sequences
  - Why needed here: Moving object segmentation relies on detecting changes in object positions across scans; without coupling spatial geometry with temporal cues, static objects may be misclassified as moving.
  - Quick check question: How does aggregating multiple LiDAR scans without temporal labels affect the ability to distinguish moving from static objects?

- Concept: State Space Models and selective scan mechanisms
  - Why needed here: MambaMOS replaces Transformer attention with SSM to model long-range temporal dependencies in point cloud sequences more efficiently while still capturing motion patterns.
  - Quick check question: What is the key difference between SSM's selective scan mechanism and Transformer's self-attention in handling variable-length sequences?

- Concept: Point cloud serialization and space-filling curves
  - Why needed here: SSM requires ordered sequences; converting unordered 3D points into sequences while preserving local neighborhoods is critical for accurate motion modeling.
  - Quick check question: Why might z-order or Hilbert curves be preferred over simple coordinate sorting for serializing point clouds in MambaMOS?

## Architecture Onboarding

- Component map: Input -> TCBE -> Encoder (MSSM blocks) -> Decoder -> Linear -> Deserialize -> MOS prediction
- Critical path: Input → TCBE → Encoder (MSSM blocks) → Decoder → Linear → Deserialize → MOS prediction
- Design tradeoffs:
  - Using SSM vs Transformer: linear complexity vs potentially richer attention, but better scalability for long sequences
  - Separate temporal channel vs concatenation: stronger temporal emphasis vs simpler implementation
  - Multi-scan vs single-scan branches: deeper coupling vs computational cost
- Failure signatures:
  - Poor segmentation in sparse regions → likely serialization or SSM parameter issues
  - High false positives on static objects → TCBE may not be emphasizing temporal dominance enough
  - Degraded performance with longer sequences → selective scan mechanism or batch size limits
- First 3 experiments:
  1. Replace TCBE with simple concatenation of t and xyz; compare IoU MOS to baseline.
  2. Swap MSSM block with original Mamba block; measure impact on coupling and accuracy.
  3. Vary serialization method (z-curve, Hilbert, random); evaluate effect on performance stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MambaMOS scale with the number of past scans used for temporal information, and is there an optimal number beyond which performance plateaus or degrades?
- Basis in paper: [explicit] The paper mentions using scans of F = 8 as input, similar to previous methods, but does not explore the impact of varying this number.
- Why unresolved: The paper does not conduct experiments to determine the effect of different numbers of past scans on performance.
- What evidence would resolve it: Conducting experiments with varying numbers of past scans (e.g., F = 4, 6, 8, 10) and analyzing the performance trade-offs would provide insights into the optimal number of scans.

### Open Question 2
- Question: Can the proposed TCBE and MSSM modules be effectively adapted for other 3D point cloud tasks beyond moving object segmentation, such as object detection or scene understanding?
- Basis in paper: [inferred] The paper focuses on the application of TCBE and MSSM to moving object segmentation, but does not explore their potential in other 3D vision tasks.
- Why unresolved: The paper does not provide any experiments or discussions on the generalizability of TCBE and MSSM to other 3D point cloud tasks.
- What evidence would resolve it: Implementing and evaluating TCBE and MSSM in other 3D point cloud tasks, such as object detection or semantic segmentation, would demonstrate their broader applicability.

### Open Question 3
- Question: How does the proposed MambaMOS framework handle occlusions and partial visibility of objects, and what are its limitations in such scenarios?
- Basis in paper: [inferred] The paper does not explicitly address the handling of occlusions or partial visibility of objects, which are common challenges in LiDAR-based perception.
- Why unresolved: The paper does not provide any experiments or analysis on the performance of MambaMOS in scenarios with occlusions or partial visibility.
- What evidence would resolve it: Conducting experiments with datasets or scenarios that include occluded or partially visible objects would reveal the strengths and limitations of MambaMOS in such situations.

## Limitations
- Performance gains may not generalize to sparser LiDAR environments or scenarios with slow-moving objects
- Architectural innovations lack ablation studies that isolate individual contributions of TCBE and MSSM modules
- Theoretical justification for selective scan mechanism's effectiveness is not provided

## Confidence
- High: Quantitative performance claims on SemanticKITTI-MOS and KITTI-Road benchmarks
- Medium: Architectural claims about TCBE and MSSM effectiveness
- Low: Generalization claims to unseen scenarios and theoretical justifications for design choices

## Next Checks
1. Conduct ablation studies removing TCBE and MSSM individually to quantify their isolated contributions
2. Test performance on datasets with sparse LiDAR coverage and slow-moving objects to evaluate generalization limits
3. Implement cross-dataset validation by training on SemanticKITTI-MOS and testing on KITTI-Road without fine-tuning to assess true generalization capability