---
ver: rpa2
title: Advancing Robust Underwater Acoustic Target Recognition through Multi-task
  Learning and Multi-Gate Mixture-of-Experts
arxiv_id: '2411.02787'
source_url: https://arxiv.org/abs/2411.02787
tags:
- underwater
- acoustic
- recognition
- features
- gating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of underwater acoustic target recognition
  using limited authentic signal recordings, which hinders data-driven models from
  learning robust patterns. The proposed M3 framework enhances the model's ability
  to capture robust patterns by making it aware of target properties.
---

# Advancing Robust Underwater Acoustic Target Recognition through Multi-task Learning and Multi-Gate Mixture-of-Experts

## Quick Facts
- **arXiv ID**: 2411.02787
- **Source URL**: https://arxiv.org/abs/2411.02787
- **Reference count**: 17
- **Primary result**: 87.07±2.43% accuracy on 9-class ShipsEar underwater ship-radiated noise recognition task using M3 framework

## Executive Summary
This paper addresses the challenge of underwater acoustic target recognition using limited authentic signal recordings by proposing the M3 framework. M3 enhances model robustness by making it aware of target properties through multi-task learning with an auxiliary task focusing on target size estimation. The framework incorporates multi-expert and multi-gate mechanisms, allowing for distinct parameter spaces to process complex signal patterns. Evaluated on the ShipsEar dataset, M3 achieves state-of-the-art performance with significant improvements over baseline models.

## Method Summary
The M3 framework uses multi-task learning with shared parameters between recognition and target size estimation tasks. It employs multi-expert layers (three independent convolutional networks with distinct parameters) and multi-gate mechanisms that dynamically learn task-specific weights for expert outputs. The architecture uses 2-D log power spectrograms as main features and 1-D Welch spectra as gating features, with ResNet-18 based task-specific classifiers. Training uses AdamW optimizer with cosine annealing schedule over 200 epochs, incorporating LMR data augmentation.

## Key Results
- Achieves 87.07±2.43% accuracy on 9-class recognition task, surpassing state-of-the-art models
- Multi-task learning with target size estimation improves accuracy by 1.72% over single-task baseline
- Multi-expert and multi-gate mechanisms provide 1.73% improvement over fundamental multi-task model
- M3 demonstrates superior performance in handling limited authentic signal recordings

## Why This Works (Mechanism)

### Mechanism 1: Multi-task Learning with Parameter Sharing
- Claim: Multi-task learning with target size estimation task improves recognition accuracy by 1.72% compared to single-task baseline.
- Mechanism: Sharing parameters between recognition and size estimation tasks allows the model to learn common robust features across tasks, improving generalization.
- Core assumption: Target size is strongly correlated with acoustic characteristics and can be inferred from category labels without additional annotation.
- Evidence anchors:
  - [abstract]: "an auxiliary task that focuses on target properties, such as estimating target size... shares parameters with the recognition task to realize multi-task learning"
  - [section]: "the auxiliary task does not require additional annotations since the target size can be easily inferred based on the category labels"
  - [corpus]: Weak - corpus contains related multi-task learning papers but none specifically about target size estimation for underwater acoustic recognition
- Break condition: If target size becomes weakly correlated with acoustic patterns or requires additional annotations, the multi-task benefit diminishes.

### Mechanism 2: Multi-Expert and Multi-Gate Mechanisms
- Claim: Multi-expert and multi-gate mechanisms improve performance by 1.73% over fundamental multi-task model.
- Mechanism: Independent expert layers process complex signal patterns in specialized parameter spaces, while gating layers dynamically learn task-specific weights for expert outputs.
- Core assumption: Underwater acoustic signals have complex patterns that benefit from specialized processing rather than shared parameter space.
- Evidence anchors:
  - [abstract]: "M3 incorporates multi-expert and multi-gate mechanisms, allowing for the allocation of distinct parameter spaces to various underwater signals"
  - [section]: "M3 employs multiple independent network layers... to facilitate information sharing between tasks. Expert layers have identical architectures but distinct parameters"
  - [corpus]: Weak - corpus contains MoE-related papers but none specifically about underwater acoustic target recognition
- Break condition: If signal patterns become simple enough that shared parameters suffice, or if gating becomes too complex relative to benefits.

### Mechanism 3: 1-D Frequency Domain Gating Features
- Claim: 1-D frequency domain features as gating inputs improve performance compared to using high-dimensional spectrograms.
- Mechanism: Low-dimensional, non-redundant gating features prevent gating layer overfitting while providing sufficient task-relevant information for weight assignment.
- Core assumption: Frequency domain features contain sufficient task-relevant patterns while being low-dimensional enough for simple gating layers.
- Evidence anchors:
  - [section]: "gating features need to be non-redundant and low-dimensional to avoid underfitting on the simple gating layer"
  - [section]: "the results also demonstrate that employing Welch spectra as the gating feature for both tasks yield the optimal outcomes"
  - [corpus]: Weak - corpus contains frequency domain feature papers but none specifically about gating feature selection
- Break condition: If frequency domain features lose critical task-relevant information or if gating layer complexity increases.

## Foundational Learning

- Concept: Multi-task learning parameter sharing
  - Why needed here: Enables learning shared robust patterns across recognition and size estimation tasks
  - Quick check question: What happens to model performance if tasks share no parameters versus fully sharing all parameters?

- Concept: Mixture of Experts architecture
  - Why needed here: Allows specialized processing of complex underwater acoustic patterns through independent expert layers
  - Quick check question: How does MoE differ from traditional hard parameter sharing in multi-task learning?

- Concept: Task-specific vs shared expert layers
  - Why needed here: Balances between shared knowledge and task-specific optimization to prevent interference
  - Quick check question: What are the trade-offs between having all shared experts versus introducing task-specific experts?

## Architecture Onboarding

- Component map: Log power spectrogram + Welch spectrum → Expert layers (3 independent conv networks) → Gating layers (MLPs with softmax) → Weighted combination → ResNet-18 tower networks → Classification outputs
- Critical path: Feature extraction → Expert processing → Gating weight assignment → Weighted combination → Task-specific classification
- Design tradeoffs:
  - Number of experts: More experts provide specialization but increase parameters and complexity
  - Gating feature dimensionality: Higher dimensionality provides more information but risks gating layer overfitting
  - Task weight balance (α): Must balance between tasks to prevent one dominating training
- Failure signatures:
  - Performance degrades when α is too high or too low (one task dominates)
  - Gating layer produces nearly uniform weights (fails to differentiate expert contributions)
  - Task-specific experts learn identical patterns (redundant specialization)
- First 3 experiments:
  1. Single-task baseline: ResNet-18 on log power spectrogram only
  2. Multi-task with shared parameters: Recognition + size estimation with shared backbone
  3. Multi-task with MoE: Same as #2 but with multi-expert and multi-gate mechanisms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of expert layers in the M3 framework for large-scale underwater acoustic datasets?
- Basis in paper: [inferred] The paper states that the current 3-expert M3 model may not fully exploit the potential of the MoE structure when dealing with large-scale datasets that encompass diverse feature spaces.
- Why unresolved: The paper did not investigate the MoE structure with a larger number of experts, such as 8, 16, or even 128, due to the limited scale of the ShipsEar dataset.
- What evidence would resolve it: Experiments comparing the performance of M3 models with different numbers of expert layers (e.g., 3, 8, 16, 128) on a large-scale underwater acoustic dataset would provide insights into the optimal number of expert layers.

### Open Question 2
- Question: How can the auxiliary task be designed to provide more supplementary information and offer greater benefits to recognition systems?
- Basis in paper: [explicit] The paper mentions that the strong correlation between target size estimation and the recognition task helps mitigate inter-task interference, but the auxiliary task provides limited supplementary information, thereby offering limited benefits to recognition systems.
- Why unresolved: The paper did not explore more innovative designs for auxiliary tasks beyond target size estimation.
- What evidence would resolve it: Experiments comparing the performance of M3 models with different auxiliary tasks (e.g., target type estimation, target material estimation, target speed estimation) on underwater acoustic target recognition tasks would provide insights into the most effective auxiliary task design.

### Open Question 3
- Question: How does the choice of gating feature affect the performance of the M3 framework?
- Basis in paper: [explicit] The paper presents preliminary experiments on selecting the optimal gating feature, comparing the performance of M3 using different gating features (Welch spectrum, average amplitude spectrum, spectral centroid).
- Why unresolved: The paper only investigated three candidate gating features and did not explore other potential gating features or combinations of gating features.
- What evidence would resolve it: Experiments comparing the performance of M3 models using different gating features (e.g., spectral centroid, frequency centroid, spectral flux) or combinations of gating features on underwater acoustic target recognition tasks would provide insights into the optimal gating feature design.

## Limitations
- Limited statistical reporting with only 2 runs, making confidence intervals difficult to establish
- Small dataset size (90 recordings) raises concerns about data leakage and generalizability
- Unknown implementation details of LMR data augmentation technique
- Unclear precise gating layer architecture specifications

## Confidence
- **High confidence**: Core architectural contributions (multi-expert and multi-gate mechanisms) and their theoretical benefits
- **Medium confidence**: Empirical performance claims due to limited statistical reporting and small dataset concerns
- **Medium confidence**: Multi-task learning benefits, though limited by weak corpus evidence for target size estimation approach

## Next Checks
1. **Statistical robustness test**: Run 10+ independent training trials with different random seeds to establish confidence intervals for the reported accuracy metrics
2. **Ablation on gating features**: Systematically compare Welch spectrum gating against alternative low-dimensional features (MFCCs, spectral centroids) to validate the non-redundancy assumption
3. **Cross-domain generalization**: Evaluate the trained model on a held-out ship type or a different underwater acoustic dataset to test the claimed robustness against limited authentic recordings