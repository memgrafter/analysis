---
ver: rpa2
title: WangchanLion and WangchanX MRC Eval
arxiv_id: '2403.16127'
source_url: https://arxiv.org/abs/2403.16127
tags:
- answer
- evaluation
- wangchanlion
- context
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report introduces WangchanLion, a Thai instruction-following
  model based on SEA-LION, fine-tuned on a mixture of English and Thai instruction
  datasets totaling over 132 million tokens. The model is evaluated on two Thai Machine
  Reading Comprehension datasets, XQuAD and iappwikiqasquad, using both traditional
  extractive QA metrics and a novel human-inspired evaluation scheme (WangchanX-MRC-Eval)
  that assesses correctness, helpfulness, conciseness, and contextuality.
---

# WangchanLion and WangchanX MRC Eval

## Quick Facts
- arXiv ID: 2403.16127
- Source URL: https://arxiv.org/abs/2403.16127
- Reference count: 40
- One-line primary result: WangchanLion outperforms baseline Thai models on MRC tasks and generates more concise answers with less irrelevant information.

## Executive Summary
This technical report introduces WangchanLion, a Thai instruction-following model based on SEA-LION, fine-tuned on a mixture of English and Thai instruction datasets totaling over 132 million tokens. The model is evaluated on two Thai Machine Reading Comprehension datasets, XQuAD and iapp_wiki_qa_squad, using both traditional extractive QA metrics and a novel human-inspired evaluation scheme (WangchanX-MRC-Eval) that assesses correctness, helpfulness, conciseness, and contextuality. Experimental results show WangchanLion achieves higher F1 scores than OpenThaiGPT and SeaLLM in 0-shot and 1-shot settings. Human evaluation reveals WangchanLion tends to generate shorter, more concise answers with less irrelevant or out-of-context information compared to competitors, though it provides less additional helpful context. The study also demonstrates the use of GPT-4 for automated evaluation aligned with human judgments, offering a scalable alternative to manual assessment. All training data, code, and model weights are publicly released under Apache-2.0 license.

## Method Summary
WangchanLion extends the SEA-LION base model through instruction tuning on a mixture of English and Thai datasets. The training employs parameter-efficient fine-tuning (QLoRa) to optimize performance while maintaining computational efficiency. The model is evaluated on XQuAD and iapp_wiki_qa_squad datasets using traditional F1 metrics and a novel human-inspired evaluation scheme (WangchanX-MRC-Eval) that assesses four criteria: correctness, helpfulness, conciseness, and contextuality. Human evaluation is conducted with majority voting across 100 samples, and results are validated against GPT-4 automated evaluation using custom prompts. The methodology emphasizes transparency by releasing all training data, code, and model weights under Apache-2.0 license.

## Key Results
- WangchanLion achieves higher F1 scores than OpenThaiGPT and SeaLLM in 0-shot and 1-shot MRC settings
- Human evaluation shows WangchanLion generates more concise answers with less irrelevant information (lower Q3 scores) compared to competitors
- GPT-4 automated evaluation aligns with human judgments, providing a scalable alternative for model assessment
- WangchanLion's answers tend to be shorter with less additional context, though still maintaining high correctness scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WangchanLion outperforms existing Thai instruction models on MRC tasks due to its SEA-LION base model and carefully curated instruction tuning datasets.
- Mechanism: The SEA-LION base model provides a strong foundation for Thai language processing with its large Thai vocabulary (10,652 tokens) and pretraining on well-documented, open-source datasets. The instruction tuning process uses a mixture of English and Thai datasets totaling over 132 million tokens, enabling effective cross-lingual transfer and improving the model's ability to follow instructions and comprehend context.
- Core assumption: The combination of a strong base model with extensive instruction tuning in both English and Thai will lead to improved performance on Thai MRC tasks compared to models with less optimized foundations or training data.
- Evidence anchors:
  - [abstract] "WangchanLion achieves higher F1 scores than OpenThaiGPT and SeaLLM in 0-shot and 1-shot settings."
  - [section] "WangchanLion continues the same spirit of data transparency initiated by SEA-LION, ensuring that our results are reproducible from scratch."
  - [corpus] "Average neighbor FMR=0.527, average citations=0.0." (Weak corpus evidence, limited citations)
- Break condition: If the instruction tuning datasets are not diverse or representative enough, or if the SEA-LION base model does not provide a strong foundation for Thai language processing, the performance gains may not materialize.

### Mechanism 2
- Claim: The WangchanX-MRC-Eval evaluation scheme provides a more holistic assessment of generative models' MRC capabilities compared to traditional extractive QA metrics.
- Mechanism: The evaluation scheme uses four yes-no questions to assess the response's correctness, helpfulness, conciseness, and contextuality. This approach goes beyond token-level matching and allows for a more nuanced evaluation of the model's ability to provide contextually rich and relevant answers.
- Core assumption: Human evaluators can consistently and objectively assess the quality of model responses based on the four criteria, and this assessment will provide more valuable insights into the model's strengths and weaknesses compared to traditional metrics.
- Evidence anchors:
  - [abstract] "Human evaluation reveals WangchanLion tends to generate shorter, more concise answers with less irrelevant or out-of-context information compared to competitors, though it provides less additional helpful context."
  - [section] "Our evaluation method consists of four yes-no questions checking the correctness of the response and the qualities of additional information regarding its helpfulness, conciseness, and contextuality."
  - [corpus] "Average neighbor FMR=0.527, average citations=0.0." (Weak corpus evidence, limited citations)
- Break condition: If human evaluators cannot consistently apply the evaluation criteria or if the criteria do not accurately capture the desired qualities of model responses, the evaluation scheme may not provide meaningful insights.

### Mechanism 3
- Claim: Using GPT-4 for automated evaluation provides a scalable alternative to manual assessment while maintaining alignment with human judgments.
- Mechanism: The study demonstrates that GPT-4 can be used to automatically evaluate model responses based on the same four criteria used in the human evaluation. The results from GPT-4 are compared to human evaluation results to ensure alignment and quality control.
- Core assumption: GPT-4 can accurately assess the quality of model responses based on the defined criteria and produce results that are consistent with human judgments.
- Evidence anchors:
  - [abstract] "The study also demonstrates the use of GPT-4 for automated evaluation aligned with human judgments, offering a scalable alternative to manual assessment."
  - [section] "We compare their results to those obtained from the human evaluation for quality control."
  - [corpus] "Average neighbor FMR=0.527, average citations=0.0." (Weak corpus evidence, limited citations)
- Break condition: If GPT-4 cannot consistently produce results that align with human judgments or if the automated evaluation does not capture the nuances of the human evaluation, the scalability benefit may not be realized.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: The instruction tuning process uses a mixture of English and Thai datasets to improve the model's ability to comprehend and generate responses in Thai. Understanding cross-lingual transfer learning is crucial for optimizing this process.
  - Quick check question: How does the model leverage knowledge from the English instruction datasets to improve its performance on Thai MRC tasks?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: The study employs PEFT techniques to efficiently fine-tune the SEA-LION base model on the instruction datasets. Understanding PEFT is essential for optimizing the training process and achieving good performance with limited computational resources.
  - Quick check question: What are the key hyperparameters used in the PEFT process, and how do they impact the model's performance and training efficiency?

- Concept: Evaluation metrics for generative models
  - Why needed here: The study proposes a new evaluation scheme for MRC tasks that goes beyond traditional extractive QA metrics. Understanding the strengths and limitations of different evaluation metrics is crucial for accurately assessing the model's capabilities and identifying areas for improvement.
  - Quick check question: How does the WangchanX-MRC-Eval scheme address the limitations of traditional F1 and exact match accuracy metrics, and what are the advantages of using this approach?

## Architecture Onboarding

- Component map: SEA-LION base model -> Instruction tuning datasets -> Parameter-efficient fine-tuning (PEFT) -> WangchanLion model -> WangchanX-MRC-Eval -> GPT-4 automated evaluation

- Critical path: SEA-LION base model → Instruction tuning datasets → PEFT → WangchanLion model → WangchanX-MRC-Eval → GPT-4 automated evaluation

- Design tradeoffs:
  - Model size vs. computational efficiency: Larger models may provide better performance but require more computational resources for training and inference.
  - Dataset size and diversity vs. data quality: Larger and more diverse datasets can improve the model's generalization but may introduce noise or inconsistencies.
  - Automated evaluation vs. human evaluation: Automated evaluation is more scalable but may not capture the nuances of human judgment.

- Failure signatures:
  - Poor performance on MRC tasks: Indicates issues with the base model, instruction tuning process, or evaluation scheme.
  - Inconsistent results between human and automated evaluation: Suggests potential biases or limitations in the automated evaluation process.
  - High computational requirements: May limit the model's applicability in resource-constrained environments.

- First 3 experiments:
  1. Evaluate the WangchanLion model on a held-out Thai MRC dataset to assess its performance compared to baseline models.
  2. Conduct a human evaluation study using the WangchanX-MRC-Eval scheme to gather insights into the model's strengths and weaknesses.
  3. Compare the results of the GPT-4 automated evaluation with the human evaluation results to assess the alignment and scalability of the automated approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the evaluation results differ if WangchanLion were compared to larger models like GPT-4 or Claude in the same Thai MRC tasks?
- Basis in paper: [explicit] The paper compares WangchanLion against OpenThaiGPT, SeaLLM, and several other Thai-supporting models, but does not include proprietary models like GPT-4 or Claude in the comparison.
- Why unresolved: The study focuses on open-source and publicly available Thai models to maintain accessibility, but does not explore how commercial models might perform in the same evaluation framework.
- What evidence would resolve it: Conducting the same human and automated evaluations with GPT-4 and Claude on the XQuAD and i_app_wiki_qa_squad datasets would provide direct performance comparisons and insights into whether the evaluation criteria favor certain model architectures or capabilities.

### Open Question 2
- Question: Would optimizing for higher Q2 scores (helpfulness) in the WangchanX-MRC-Eval framework lead to significantly more irrelevant or out-of-context information (Q3 and Q4), or can these metrics be balanced?
- Basis in paper: [inferred] The paper notes that SeaLLM achieves higher Q2 scores but also higher Q3 and Q4 scores, suggesting a potential trade-off, but does not explore whether this trade-off is inherent or can be mitigated.
- Why unresolved: The study identifies the trade-off but does not experimentally test whether model fine-tuning or prompt engineering could achieve high helpfulness without sacrificing conciseness or contextuality.
- What evidence would resolve it: Systematically varying the balance between Q1-Q2 and Q3-Q4 through controlled experiments with different model configurations or training objectives would reveal whether these metrics can be optimized jointly.

### Open Question 3
- Question: How sensitive is the GPT-4 automated evaluation to prompt variations, and could alternative evaluation prompts yield significantly different results?
- Basis in paper: [explicit] The paper uses a specific prompt format for GPT-4 evaluation and notes its alignment with human judgments, but does not explore prompt sensitivity or alternative formulations.
- Why unresolved: The evaluation relies on a single prompt template without ablation studies or comparison to alternative prompt designs that might affect judgment consistency or alignment with human preferences.
- What evidence would resolve it: Testing multiple prompt variations (different phrasings, ordering of evaluation criteria, or additional context) and measuring their agreement with human evaluations would quantify prompt sensitivity and identify optimal prompt design.

## Limitations
- Limited dataset transparency: Exact composition and size of instruction datasets are not fully specified
- Small evaluation sample: Only 100 samples used for human evaluation across two datasets
- Narrow baseline comparison: Only three baseline models compared despite multiple Thai LLMs available

## Confidence
- High Confidence: WangchanLion outperforms baseline models on traditional F1 metrics in both 0-shot and 1-shot settings. The model architecture and training methodology are well-documented.
- Medium Confidence: The WangchanX-MRC-Eval scheme provides valuable insights beyond traditional metrics. The claim of GPT-4 automated evaluation alignment with human judgments is supported but based on limited sample sizes.
- Low Confidence: Claims about the model's superiority in handling Thai language nuances and contextuality are based on relatively small evaluation samples and limited model comparisons.

## Next Checks
1. Conduct larger-scale human evaluation with stratified sampling across different MRC task types and difficulty levels, using Cohen's kappa to measure inter-rater reliability
2. Expand baseline comparisons to include more recent Thai instruction models and conduct ablation studies on the impact of English vs. Thai instruction data ratios
3. Implement cross-validation of WangchanX-MRC-Eval results using multiple automated evaluation systems beyond GPT-4 to verify consistency and identify potential evaluator biases