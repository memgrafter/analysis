---
ver: rpa2
title: Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment
arxiv_id: '2406.17957'
source_url: https://arxiv.org/abs/2406.17957
tags:
- speech
- audio
- text
- prior
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We identify that certain cross-attention heads in encoder-decoder
  transformer models implicitly learn text-to-speech alignment during training. To
  improve robustness and reduce hallucinations in LLM-based TTS, we propose a guided
  attention technique using static attention priors and CTC-based alignment loss to
  enforce monotonic attention over text tokens.
---

# Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment

## Quick Facts
- **arXiv ID**: 2406.17957
- **Source URL**: https://arxiv.org/abs/2406.17957
- **Reference count**: 0
- **Primary result**: Reduces character error rate from 9.03% to 3.92% on challenging texts using guided attention technique

## Executive Summary
This paper addresses hallucination issues in LLM-based text-to-speech systems by identifying that cross-attention heads in encoder-decoder transformers implicitly learn text-to-speech alignment during training. The authors propose a guided attention technique using static attention priors and CTC-based alignment loss to enforce monotonic attention over text tokens. This approach significantly improves intelligibility and reduces word/character errors, particularly for challenging texts with repeated tokens, while maintaining naturalness comparable to or better than prior open-source LLM-based TTS models.

## Method Summary
The method involves training a T5 encoder-decoder transformer with guided attention components. Cross-attention heads are initialized with beta-binomial priors to create near-monotonic attention patterns, and CTC-based alignment loss is applied to enforce monotonic reduction of audio timesteps to text timesteps. The model uses neural audio codecs (Encovec, Dac, or spectral codec) to represent speech, with text tokenized using sentence-piece or phonemes. The training combines reconstruction loss with alignment loss, and the technique adds no new learnable parameters while improving robustness to challenging texts.

## Key Results
- Character Error Rate reduced from 9.03% to 3.92% on challenging texts with repeated tokens
- Achieves higher naturalness MOS than prior open-source LLM-based TTS models
- Maintains speaker similarity while improving intelligibility and reducing hallucinations

## Why This Works (Mechanism)

### Mechanism 1
During autoregressive training, cross-attention scores between decoder and encoder tokens naturally develop diagonal patterns indicating alignment, but this alignment is unconstrained and can be non-monotonic. The attention mechanism in encoder-decoder transformers can learn alignment patterns without explicit supervision.

### Mechanism 2
Beta-binomial prior matrices are multiplied with initial random attention scores, reducing scores far from the diagonal and creating a near-monotonic starting point that guides subsequent learning. Attention matrices are sensitive to initialization and can be guided by multiplicative priors.

### Mechanism 3
The soft alignment matrix derived from attention scores is treated as emission probabilities in CTC, and the loss encourages monotonic reduction of audio timesteps to text timesteps. CTC can be applied to attention matrices to enforce monotonic alignments, treating attention as emission probabilities.

## Foundational Learning

- **Cross-attention mechanism in encoder-decoder transformers**: Understanding how cross-attention learns alignment is crucial for implementing the guided attention technique. *Quick check*: How does cross-attention differ from self-attention in terms of what it attends to?

- **Connectionist Temporal Classification (CTC) algorithm**: CTC is used to enforce monotonic alignment by finding valid reductions of audio timesteps to text timesteps. *Quick check*: What is the key difference between CTC loss and standard cross-entropy loss?

- **Beta-binomial distribution for prior generation**: Beta-binomial prior creates near-diagonal attention initialization that guides monotonic learning. *Quick check*: Why is a beta-binomial distribution chosen over a simple diagonal matrix for the attention prior?

## Architecture Onboarding

- **Component map**: Text tokens → encoder → cross-attention in decoder → N codebook predictions → reconstruction loss + alignment loss
- **Critical path**: Text tokens → encoder → cross-attention in decoder → N codebook predictions → reconstruction loss + alignment loss
- **Design tradeoffs**: Encoder vs decoder context input location, number of heads to apply alignment loss to, choice of audio codec model
- **Failure signatures**: Repeating words, missing words, misaligned speech, unstable training curves when adding alignment components
- **First 3 experiments**:
  1. Train baseline T5-TTS without alignment components and analyze attention patterns
  2. Add attention prior only and observe training stability and attention initialization
  3. Add both prior and CTC alignment loss, test with and without annealing schedule

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several remain unresolved based on the analysis:
- How does the effectiveness vary across different neural audio codec models?
- What is the impact on model performance with extremely long or short audio sequences?
- How does the technique affect generalization to speakers with significantly different speech patterns?
- What is the computational overhead during inference compared to duration-based approaches?

## Limitations

- The paper only tests the alignment learning technique on two of the three codec models (Dac and Spectral codec), leaving uncertainty about generalizability
- No ablation studies are provided to quantify the individual contributions of the attention prior and CTC loss
- The comparison is limited to open-source models, not including state-of-the-art commercial systems
- The paper doesn't explore performance at extreme sequence lengths or with pathological speech patterns

## Confidence

- **High confidence**: The overall approach of using guided attention to improve TTS alignment is technically sound
- **Medium confidence**: The specific implementation details (beta-binomial prior, CTC loss application) will work as described
- **Low confidence**: Claims about the extent of implicit alignment learning without supervision

## Next Checks

1. Ablation study: Train the same model without any alignment components to establish baseline alignment quality and hallucination rates
2. Prior comparison: Replace the beta-binomial prior with a simple diagonal matrix and measure impact on training stability and final performance
3. Attention visualization: Provide detailed analysis of attention patterns before and after alignment training on challenging texts with repeated tokens