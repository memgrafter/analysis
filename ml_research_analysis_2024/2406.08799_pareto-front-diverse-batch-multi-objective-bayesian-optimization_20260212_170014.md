---
ver: rpa2
title: Pareto Front-Diverse Batch Multi-Objective Bayesian Optimization
arxiv_id: '2406.08799'
source_url: https://arxiv.org/abs/2406.08799
tags:
- pareto
- function
- batch
- acquisition
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PDBO addresses the challenge of discovering high-quality and diverse
  Pareto fronts in expensive multi-objective optimization when batch evaluations are
  allowed. The core method uses a multi-armed bandit approach to adaptively select
  acquisition functions from a library and employs Determinantal Point Processes (DPPs)
  to choose a diverse batch of inputs by considering multiple objectives.
---

# Pareto Front-Diverse Batch Multi-Objective Bayesian Optimization

## Quick Facts
- arXiv ID: 2406.08799
- Source URL: https://arxiv.org/abs/2406.08799
- Authors: Alaleh Ahmadianshalchi; Syrine Belakaria; Janardhan Rao Doppa
- Reference count: 40
- Key outcome: PDBO achieves up to 72.57% diversity in Pareto fronts compared to baselines on multi-objective optimization benchmarks

## Executive Summary
PDBO addresses the challenge of discovering high-quality and diverse Pareto fronts in expensive multi-objective optimization when batch evaluations are allowed. The core method uses a multi-armed bandit approach to adaptively select acquisition functions from a library and employs Determinantal Point Processes (DPPs) to choose a diverse batch of inputs by considering multiple objectives. The algorithm updates its key parameters after each round of function evaluations. Experimental results on multiple benchmarks demonstrate that PDBO outperforms prior methods in terms of both the quality (hypervolume) and diversity (DPF metric) of Pareto solutions.

## Method Summary
PDBO is a batch multi-objective Bayesian optimization algorithm that discovers high-quality and diverse Pareto fronts. It uses a multi-armed bandit to adaptively select acquisition functions from a library, solves cheap multi-objective optimization problems to generate candidate Pareto sets, and employs a multi-objective DPP to select diverse batches of inputs. The method updates its parameters after each round of function evaluations and is evaluated on multiple benchmark problems using hypervolume and DPF metrics.

## Key Results
- PDBO outperforms baselines (DGEMO, qEHVI, qPAREGO, NSGA-II) in both quality (hypervolume) and diversity (DPF) metrics
- Achieved up to 72.57% diversity compared to competing methods
- Effective across problems with varying input dimensions and number of objective functions
- Demonstrated on ZDT, DTLZ benchmarks, gear train design, SW-LLVM, and UAV power system problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-arm bandit selection of acquisition functions adapts dynamically to the changing landscape of the optimization problem.
- Mechanism: PDBO uses a portfolio of acquisition functions and updates selection probabilities based on discounted cumulative rewards derived from relative hypervolume improvement.
- Core assumption: The relative hypervolume improvement provides a meaningful signal about the quality of the selected batch.
- Evidence anchors: [abstract] "PDBO employs a multi-armed bandit approach to select one acquisition function from a given library"; [section 4.1] "The immediate reward IR j t = HV ( ˜Yt−1 ∪ ˜Y j t ) − HV ( ˜Yt−1) / HV ( ˜Yt−1)"

### Mechanism 2
- Claim: Determinantal Point Processes configured with a multi-objective kernel promote diverse Pareto front solutions by modeling repulsion in the output space.
- Mechanism: PDBO constructs a DPP kernel as a convex combination of individual Gaussian process kernels, with weights fitted to maximize the likelihood of selecting points with high individual hypervolume contribution.
- Core assumption: Individual hypervolume contribution is a good proxy for diversity in the Pareto front.
- Evidence anchors: [abstract] "it utilizes Determinantal Point Processes (DPPs) to choose a Pareto-front-diverse batch of inputs for evaluation"; [section 4.2] "κDP P = PK i=1 λi · κi st. PK i=1 λi = 1"

### Mechanism 3
- Claim: Solving cheap MOO problems for each acquisition function generates diverse candidate Pareto sets that capture the trade-offs between objectives in the utility space.
- Mechanism: For each acquisition function, PDBO constructs a cheap MOO problem where the objectives are the acquisition functions applied to each expensive objective function.
- Core assumption: The cheap MOO problems accurately approximate the trade-offs between the expensive objective functions.
- Evidence anchors: [section 4.1] "A cheap MOO problem is solved by assigning the selected acquisition function for each expensive objective function"

## Foundational Learning

- Concept: Gaussian Processes and their use in Bayesian Optimization
  - Why needed here: PDBO relies on Gaussian process surrogates to model expensive objective functions
  - Quick check question: What is the role of the GP mean and covariance functions in defining the UCB acquisition function?

- Concept: Multi-Objective Optimization and Pareto Dominance
  - Why needed here: PDBO is designed to find diverse Pareto fronts, so understanding Pareto dominance and hypervolume is crucial
  - Quick check question: How does the hypervolume indicator measure the quality of a Pareto front?

- Concept: Determinantal Point Processes and their use in batch selection
  - Why needed here: PDBO uses DPPs to select diverse batches of inputs from candidate Pareto sets
  - Quick check question: How does the DPP kernel define similarity between points?

## Architecture Onboarding

- Component map: GP Surrogates -> Cheap MOO Solver -> DPP Batch Selector -> Multi-arm Bandit -> Batch Evaluation -> Data Update

- Critical path:
  1. Fit GPs to current data
  2. For each acquisition function, solve cheap MOO problem to generate candidate Pareto set
  3. For each candidate Pareto set, use DPP to select diverse batch of inputs
  4. Use multi-arm bandit to select one batch based on current selection probabilities
  5. Evaluate selected batch and update data
  6. Repeat until budget exhausted

- Design tradeoffs:
  - Number of acquisition functions vs. computational cost of solving cheap MOO problems
  - Batch size vs. diversity of selected batch (larger batches may be less diverse)
  - Discount factor in multi-arm bandit vs. sensitivity to early vs. late performance
  - Number of iterations vs. quality of final Pareto front

- Failure signatures:
  - Poor performance: Check if GPs are well-fit, if acquisition functions are appropriate, if DPP kernel is well-configured
  - Lack of diversity: Check if candidate Pareto sets are diverse, if DPP kernel promotes diversity, if batch size is too large
  - High variance in performance: Check if multi-arm bandit is exploring sufficiently, if rewards are noisy or unstable

- First 3 experiments:
  1. Run PDBO on ZDT1 with batch size 2 to verify basic functionality and compare to random search
  2. Run PDBO on 2-objective benchmark with varying batch sizes (2, 4, 8) to assess impact on diversity and quality
  3. Run PDBO on DTLZ1 to verify multi-objective extensions are working correctly

## Open Questions the Paper Calls Out

- Question: How can hyperparameters of the multi-arm bandit algorithm be adaptively selected instead of manually set?
  - Basis in paper: [explicit] Paper mentions manual setting of decay factor and probability of selection hyperparameter, deferring adaptive selection to future work

- Question: Can PDBO be effectively combined with existing high-dimensional Bayesian optimization approaches?
  - Basis in paper: [explicit] Paper states it can be synergistically combined with high-dimensional BO approaches but does not explore this combination

- Question: How does diversity of PDBO's Pareto front compare when Pareto front is known to be continuous/infinite?
  - Basis in paper: [inferred] Paper discusses diversity metrics but doesn't specifically address continuous/infinite Pareto fronts

- Question: What is the impact of using proposed reward function versus theoretical analysis reward function?
  - Basis in paper: [explicit] Paper presents ablation study comparing the two reward functions with superior performance for proposed strategy

## Limitations

- Multi-arm bandit mechanism assumes relative hypervolume improvement is reliable, but this may break down with inaccurate GP surrogates in high dimensions
- DPP-based diversity selection depends critically on quality of cheap MOO solutions, which may not approximate true Pareto front for non-convex problems
- Method requires careful tuning of DPP kernel weights Λ and discount factor γ with no clear guidance provided

## Confidence

- Confidence in core algorithmic framework: High
- Confidence in experimental results: Medium (some baseline implementation challenges noted)
- Confidence in scalability claims: Low (without additional experiments on higher-dimensional problems)

## Next Checks

1. Validate the DPP kernel configuration procedure by testing sensitivity to Λ values on benchmark problems with known Pareto front structures
2. Test the algorithm with noisy objective functions to assess robustness of the multi-arm bandit acquisition function selection
3. Evaluate performance degradation as input dimensionality increases beyond 10 dimensions to understand practical scalability limits