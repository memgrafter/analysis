---
ver: rpa2
title: Federated Smoothing Proximal Gradient for Quantile Regression with Non-Convex
  Penalties
arxiv_id: '2408.05640'
source_url: https://arxiv.org/abs/2408.05640
tags:
- algorithm
- quantile
- function
- regression
- fspg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Federated Smoothing Proximal Gradient
  (FSPG) algorithm for sparse quantile regression with non-convex penalties (MCP and
  SCAD) in federated learning settings. The algorithm addresses the challenge of non-smooth,
  non-convex objective functions by employing a smoothing mechanism that iteratively
  approximates the objective with smooth functions, enabling effective use of proximal
  gradient methods without requiring data sharing.
---

# Federated Smoothing Proximal Gradient for Quantile Regression with Non-Convex Penalties

## Quick Facts
- arXiv ID: 2408.05640
- Source URL: https://arxiv.org/abs/2408.05640
- Reference count: 40
- Primary result: FSPG achieves convergence rate o(k^−1/2 + d/2) for sub-gradient and o(k^−1/2 − d/2) for coefficient fluctuations in federated quantile regression with non-convex penalties

## Executive Summary
This paper introduces the Federated Smoothing Proximal Gradient (FSPG) algorithm for sparse quantile regression with non-convex penalties (MCP and SCAD) in federated learning settings. The algorithm addresses the challenge of non-smooth, non-convex objective functions by employing a smoothing mechanism that iteratively approximates the objective with smooth functions, enabling effective use of proximal gradient methods without requiring data sharing. Theoretical analysis demonstrates that FSPG achieves a convergence rate of o(k^−1/2 + d/2) for the sub-gradient and o(k^−1/2 − d/2) for coefficient fluctuations, where d ∈ (0,1) is the penalty parameter update power. Simulation results across six scenarios show FSPG outperforms existing methods including proximal gradient descent and sub-gradient approaches in terms of convergence speed, mean square error, and accuracy in identifying active and non-active coefficients.

## Method Summary
FSPG combines federated learning with smoothing proximal gradient methods to handle non-smooth, non-convex quantile regression objectives. Each client computes gradients of locally smoothed loss functions, which are aggregated at a central server. The server performs proximal updates using an iteratively refined smoothing approximation controlled by parameters μ and σ, which decrease and increase respectively according to power-law schedules. This approach enables distributed computation while maintaining convergence properties for non-convex penalties like MCP and SCAD. The algorithm guarantees improvement in each iteration and demonstrates superior optimization dynamics compared to existing methods.

## Key Results
- FSPG achieves theoretical convergence rate of o(k^−1/2 + d/2) for sub-gradient and o(k^−1/2 − d/2) for coefficient fluctuations
- Outperforms proximal gradient descent, sub-gradient methods, and smoothing ADMM in MSE and coefficient identification accuracy across six simulation scenarios
- Demonstrates faster convergence and better optimization dynamics for federated quantile regression with non-convex penalties
- Guarantees monotonic improvement in objective function value at each iteration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The iterative smoothing process transforms non-smooth, non-convex quantile regression objectives into a sequence of smooth approximations that proximal gradient methods can handle effectively.
- Mechanism: At each iteration, the check loss function's absolute value terms are replaced with a smooth upper-bound approximation that becomes increasingly accurate as the smoothing parameter µ decreases. This allows the use of gradient-based optimization while maintaining theoretical convergence guarantees.
- Core assumption: The smooth approximation function maintains the same subgradient behavior as the original function in the limit as µ approaches zero, and the approximation error can be controlled through the iterative schedule of µ and σ.
- Evidence anchors:
  - [abstract]: "The algorithm addresses the challenge of non-smooth, non-convex objective functions by employing a smoothing mechanism that iteratively approximates the objective with smooth functions"
  - [section III]: "We employ a smoothing approximation function for each term |y(l)i − (x̄(l)i)⊨ w|, defined as: f(y(l)i − (x̄(l)i)⊨ w, µ) = |y(l)i − (x̄(l)i)⊨ w|, µ ≤ |y(l)i − (x̄(l)i)⊨ w|"
- Break condition: If the smoothness parameter µ does not decrease appropriately or if the approximation error grows too large, the algorithm may fail to converge to the true solution.

### Mechanism 2
- Claim: The time-varying increasing penalty parameter σ ensures that each iteration makes monotonic progress toward reducing the objective function value.
- Mechanism: The penalty parameter σ(k+1) = c(k+1)^d increases with iteration count, providing stronger regularization that helps maintain stability in the proximal gradient updates and ensures sufficient descent in each iteration.
- Core assumption: The increasing σ parameter compensates for the decreasing smoothness parameter µ, maintaining the strong convexity needed for guaranteed progress while allowing the approximation to become more accurate.
- Evidence anchors:
  - [abstract]: "This integration adeptly handles optimization over a network of devices, each holding local data samples, making it particularly effective in federated learning scenarios. The FSPG algorithm ensures steady progress and reliable convergence in each iteration by maintaining or reducing the value of the objective function."
  - [section III]: "To iteratively refine the approximation, we update the parameters µ and σ at each iteration using the following rules, where c > 0, β > 0, and d ∈ (0, 1): σ(k+1) = c(k + 1)^d, and µ(k+1) = β(k+1)^d"
- Break condition: If d is chosen too close to 1 or too close to 0, the balance between smoothness and regularization may be lost, leading to either slow convergence or instability.

### Mechanism 3
- Claim: The federated averaging of local gradients followed by centralized proximal updates enables distributed computation while maintaining convergence properties.
- Mechanism: Each client computes local gradients on their data, sends them to a central server, which aggregates them and performs a proximal update step. This structure allows the algorithm to work without sharing raw data while still achieving global optimization.
- Core assumption: The local loss functions are similar enough across clients that gradient aggregation leads to meaningful global updates, and the communication overhead is manageable within the federated learning framework.
- Evidence anchors:
  - [abstract]: "This integration adeptly handles optimization over a network of devices, each holding local data samples, making it particularly effective in federated learning scenarios."
  - [section III]: "Following the local gradient updates, each client communicates this gradient to the central server. The server aggregates these gradients to update the global model parameter w"
- Break condition: If client data distributions are highly non-IID or if communication is unreliable, the aggregation may not lead to proper convergence.

## Foundational Learning

- Concept: Weak convexity and non-convex penalties (MCP and SCAD)
  - Why needed here: The paper uses MCP and SCAD penalties which are non-convex but weakly convex for certain parameter ranges, allowing the use of proximal gradient methods while achieving better sparsity than convex penalties.
  - Quick check question: What property of MCP and SCAD penalties allows them to maintain convexity when combined with a quadratic term, and why is this important for optimization?

- Concept: Smoothing approximation techniques for non-smooth functions
  - Why needed here: The check loss function in quantile regression is non-smooth due to absolute value operations, preventing direct use of gradient-based methods without modification.
  - Quick check question: How does the smoothing function f(·, µ) approximate the absolute value function, and what happens to this approximation as µ approaches zero?

- Concept: Federated learning architecture and communication patterns
  - Why needed here: The algorithm must work in a distributed setting where clients hold local data and only communicate gradients to a central server, requiring careful design of the update rules.
  - Quick check question: What are the key differences between centralized and federated optimization that affect algorithm design and convergence analysis?

## Architecture Onboarding

- Component map:
  - Local clients -> Compute gradients of smoothed local loss functions
  - Clients -> Send gradients to central server
  - Central server -> Aggregates gradients and performs proximal updates
  - Server -> Distributes updated parameters to all clients
  - Server -> Updates parameters µ and σ for next iteration

- Critical path:
  1. Each client computes ∇̃gl(w(k)) using current parameters
  2. Clients send gradients to central server
  3. Server aggregates gradients and performs proximal update w(k+1)
  4. Server distributes w(k+1) to all clients
  5. Parameters µ and σ are updated for next iteration

- Design tradeoffs:
  - Higher µ values provide smoother optimization but less accurate approximation
  - Lower µ values provide better accuracy but may cause numerical instability
  - Faster σ increase provides stronger regularization but may overshoot optimal solution
  - Slower σ increase provides more gradual convergence but may be too conservative

- Failure signatures:
  - Oscillating MSE values indicate poor choice of smoothing parameters
  - Divergence in coefficient values suggests insufficient regularization
  - Plateau in convergence indicates need for parameter adjustment
  - Communication failures preventing gradient aggregation

- First 3 experiments:
  1. Implement basic FSPG with synthetic data (M=10, L=20, P=100) using MCP penalty, τ=0.55, and verify convergence rate matches theoretical predictions
  2. Test different values of d ∈ {0.3, 0.5, 0.7} to observe impact on convergence speed and stability
  3. Compare FSPG performance against federated proximal gradient descent on real UCI dataset with non-IID client data distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of FSPG compare to other federated learning algorithms for non-convex objectives beyond quantile regression?
- Basis in paper: [explicit] The paper states FSPG achieves a convergence rate of o(k^−1/2 + d/2) for the sub-gradient and o(k^−1/2 − d/2) for coefficient fluctuations, but only compares against specific methods (PGD, SUB, SIAD).
- Why unresolved: The paper only benchmarks against a limited set of methods specifically designed for quantile regression with non-convex penalties. A broader comparison with general federated learning algorithms for non-convex optimization would clarify FSPG's relative performance.
- What evidence would resolve it: Empirical studies comparing FSPG against other federated learning algorithms (e.g., FedNova, FedProx, SCAFFOLD) on various non-convex federated learning tasks beyond quantile regression.

### Open Question 2
- Question: What is the theoretical relationship between the parameter d and the convergence rate in practice, and how sensitive is FSPG to its choice?
- Basis in paper: [explicit] The paper mentions that higher d leads to faster convergence in terms of coefficient fluctuations but slower convergence in terms of sub-gradient, and shows empirical results for different d values (0.3, 0.5, 0.7, 0.9).
- Why unresolved: While the paper provides empirical observations about the impact of d on convergence, it doesn't provide a theoretical analysis of how d affects the trade-off between convergence rates or guidelines for choosing optimal d values in different scenarios.
- What evidence would resolve it: Theoretical analysis establishing the relationship between d and convergence rates, and empirical studies demonstrating how to select optimal d values based on problem characteristics.

### Open Question 3
- Question: How does FSPG perform in highly heterogeneous federated learning environments where data distributions across clients are significantly different?
- Basis in paper: [inferred] The paper doesn't explicitly address non-IID data scenarios, but federated learning algorithms are typically evaluated in heterogeneous environments to assess their robustness.
- Why unresolved: The paper focuses on the theoretical and algorithmic aspects of FSPG but doesn't explore its performance under challenging federated learning conditions like data heterogeneity, which is common in real-world IoT applications.
- What evidence would resolve it: Empirical studies testing FSPG on federated learning benchmarks with varying degrees of data heterogeneity (e.g., Dirichlet distribution parameter alpha) to assess its robustness compared to other methods.

## Limitations

- The algorithm assumes penalty parameter satisfies σ(k+1) > nρ, which may require careful tuning of c for small n
- Convergence analysis relies on weakly-convex properties of MCP and SCAD penalties, but exact conditions for the smoothing approximation are not fully characterized
- Federated setting introduces uncertainty regarding gradient variance across clients, which could affect convergence rates in practice
- Theoretical guarantees depend on specific parameter update rules that may be sensitive to problem characteristics

## Confidence

- **High Confidence**: The mechanism of smoothing approximation for non-smooth functions and the basic structure of federated proximal gradient updates
- **Medium Confidence**: The specific parameter update rules (µ(k+1) = β/(k+1)^d, σ(k+1) = c(k+1)^d) and their theoretical convergence guarantees
- **Medium Confidence**: The empirical performance improvements shown in simulation results, though limited to specific scenarios

## Next Checks

1. **Convergence under varying d values**: Systematically test FSPG with d ∈ {0.1, 0.3, 0.5, 0.7, 0.9} on synthetic data to identify optimal ranges and verify theoretical predictions about convergence rates hold across this spectrum.

2. **Robustness to non-IID data**: Implement client data with varying levels of heterogeneity (from identical to highly dissimilar distributions) to test how federated averaging affects convergence when local loss functions differ substantially.

3. **Scaling analysis**: Evaluate FSPG performance with increasing numbers of clients (L ∈ {5, 10, 20, 50}) and features (P ∈ {50, 100, 500}) to identify computational bottlenecks and communication overhead limitations in practical federated settings.