---
ver: rpa2
title: 'NL-Eye: Abductive NLI for Images'
arxiv_id: '2410.02613'
source_url: https://arxiv.org/abs/2410.02613
tags:
- image
- hypothesis
- reasoning
- plausible
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NL-Eye, a benchmark designed to evaluate
  the visual abductive reasoning capabilities of Visual Language Models (VLMs). The
  benchmark presents VLMs with a premise image and two hypothesis images, requiring
  them to infer which hypothesis is more plausible and provide an explanation for
  their choice.
---

# NL-Eye: Abductive NLI for Images

## Quick Facts
- arXiv ID: 2410.02613
- Source URL: https://arxiv.org/abs/2410.02613
- Reference count: 31
- Key outcome: VLMs perform at random baseline levels on visual abductive reasoning, while humans excel

## Executive Summary
NL-Eye is a benchmark designed to evaluate the visual abductive reasoning capabilities of Visual Language Models (VLMs). The task requires models to infer which of two hypothesis images is more plausible given a premise image, and to provide an explanation. The benchmark consists of 350 carefully curated triplet examples (1,050 images) spanning six reasoning categories. Experiments show that VLMs struggle significantly on this task, often performing at random baseline levels, while humans excel in both plausibility prediction and explanation quality, revealing a critical deficiency in the abductive reasoning capabilities of modern VLMs.

## Method Summary
NL-Eye presents VLMs with a premise image and two hypothesis images, requiring them to infer which hypothesis is more plausible and provide an explanation. The benchmark contains 350 triplet examples (1,050 images) across six reasoning categories: physical, logical, emotional, functional, cultural, and social. Data curation involved writing textual descriptions and generating images using text-to-image models with substantial human involvement. Models are evaluated using consistency accuracy (for triplet setup) and order-faithfulness accuracy (for pairs setup), with human evaluation of explanation quality. The evaluation explores different input strategies including separate images, combined image, text-only, and image-to-text approaches.

## Key Results
- VLMs perform at or near random baseline levels on visual abductive reasoning tasks
- Humans significantly outperform VLMs on both plausibility prediction and explanation quality
- VLMs show high sensitivity to hypothesis order, with performance variations ranging from 5-80%
- VLMs perform better on textual descriptions than visual inputs, suggesting visual encoding is the bottleneck

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs struggle with abductive reasoning over images because they rely on weak visual encoding that captures superficial patterns like hypothesis order rather than meaningful image content.
- Mechanism: When VLMs encode images into latent representations, they fail to preserve the nuanced details required for causal and temporal reasoning, leading to predictions based on positional heuristics.
- Core assumption: Visual representations in current VLMs are insufficiently discriminative for abductive reasoning tasks.
- Evidence anchors:
  - [abstract] "VLMs struggle significantly on NL-Eye, often performing at random baseline levels"
  - [section] "VLMs are highly sensitive to hypothesis order, with performance variations ranging from 5-80%"
  - [section] "VLMs perform better when using separate image inputs, showing an average improvement of over 10% (48.6% vs 37.3%)"
- Break condition: If visual encoding models are improved to capture richer, more discriminative features, or if VLMs adopt architectures better suited to multi-image reasoning.

### Mechanism 2
- Claim: VLMs can perform strong textual reasoning but fail at visual interpretation, as evidenced by high performance on text-only tasks using gold descriptions.
- Mechanism: The language model component of VLMs is capable of logical inference when provided with accurate textual inputs, but the visual encoder fails to produce representations that support the same level of reasoning.
- Core assumption: The bottleneck in VLMs for abductive reasoning is in the visual encoding, not the language reasoning component.
- Evidence anchors:
  - [abstract] "VLMs struggle significantly on NL-Eye... while humans excel in both plausibility prediction and explanation quality"
  - [section] "VLMs Can Perform Textual Reasoning – The Failure is in Visual Interpretation"
  - [section] "In the text-only approach, the models are provided with gold-standard descriptions of the images... performance of all models, including smaller fine-tuned NLI models, is significantly higher"
- Break condition: If visual encoders are improved to produce representations that better support abductive reasoning, or if VLMs integrate stronger multimodal fusion mechanisms.

### Mechanism 3
- Claim: VLMs are better at correlational and knowledge-based reasoning than causal reasoning, as shown by higher performance on parallel temporal reasoning compared to forward/backward reasoning.
- Mechanism: VLMs excel when tasks require matching patterns or applying cultural knowledge, but struggle when reasoning requires understanding cause-effect relationships across time.
- Core assumption: Current VLMs are better at pattern matching and retrieval of cultural knowledge than at causal inference.
- Evidence anchors:
  - [abstract] "VLMs struggle significantly on NL-Eye... while humans excel in both plausibility prediction and explanation quality"
  - [section] "VLMs are Better in Correlational and Knowledge-based Reasoning Compared to Causal Reasoning"
  - [section] "VLMs exhibit a clear dichotomy in their reasoning abilities, excelling in some areas while falling short in others... VLMs perform best in Social and Cultural reasoning"
  - [section] "VLM performance on parallel reasoning examples is higher than on forward and backward reasoning tasks"
- Break condition: If VLMs are augmented with explicit causal reasoning mechanisms or trained on more causally structured data.

## Foundational Learning

- Concept: Visual abductive reasoning
  - Why needed here: This task requires inferring plausible outcomes or causes from visual premises, which is central to the NL-Eye benchmark and the VLMs' failure modes.
  - Quick check question: Can you explain the difference between abductive reasoning and deductive reasoning in the context of visual inference?

- Concept: Multimodal representation learning
  - Why needed here: Understanding how VLMs encode and fuse visual and textual information is key to diagnosing why they fail at abductive reasoning over images.
  - Quick check question: What are the main challenges in aligning visual and textual representations in VLMs?

- Concept: Temporal reasoning in vision
  - Why needed here: The benchmark includes forward, backward, and parallel temporal reasoning, and VLMs show different performance patterns across these categories.
  - Quick check question: How would you design a model to reason about causal relationships between events depicted in separate images?

## Architecture Onboarding

- Component map: Visual encoder (e.g., CLIP, SigLip) → Multimodal fusion → Language model (LLM) → Output layer
- Critical path: Image → Visual features → Multimodal context → Reasoning → Explanation
- Design tradeoffs: Separate vs. combined image input strategies; text-only vs. vision-based reasoning; manual vs. model-generated captions
- Failure signatures: High sensitivity to input order; poor performance on causal reasoning; reliance on superficial visual cues
- First 3 experiments:
  1. Compare VLMs' performance on gold textual descriptions vs. model-generated captions to isolate visual encoding issues.
  2. Test VLMs on separate vs. combined image inputs to measure impact of visual complexity.
  3. Evaluate VLMs on forward, backward, and parallel temporal reasoning to identify reasoning bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can visual abductive reasoning models be improved by incorporating external knowledge bases or common sense reasoning frameworks?
- Basis in paper: [inferred] The paper notes that VLMs struggle with emotional reasoning and suggests that they may be better at identifying correlations rather than causal understanding. The paper also mentions that VLMs perform better on social and cultural reasoning tasks, which rely on specific knowledge.
- Why unresolved: The paper does not explore the potential benefits of incorporating external knowledge sources or common sense reasoning frameworks into visual abductive reasoning models.
- What evidence would resolve it: Experiments comparing the performance of VLMs with and without access to external knowledge bases or common sense reasoning frameworks on the NL-Eye benchmark.

### Open Question 2
- Question: How does the performance of VLMs on visual abductive reasoning tasks vary across different types of images (e.g., real-world photos vs. synthetic images)?
- Basis in paper: [explicit] The paper uses synthetic images generated by text-to-image models for the NL-Eye benchmark. The paper does not compare the performance of VLMs on synthetic images to their performance on real-world photos.
- Why unresolved: The paper does not provide any insights into how the type of images used in visual abductive reasoning tasks might affect the performance of VLMs.
- What evidence would resolve it: Experiments comparing the performance of VLMs on the NL-Eye benchmark using synthetic images to their performance on a similar benchmark using real-world photos.

### Open Question 3
- Question: Can the explainability of VLMs in visual abductive reasoning tasks be improved by using different types of explanations (e.g., natural language explanations, visual explanations, or a combination of both)?
- Basis in paper: [explicit] The paper notes that VLMs struggle to provide accurate explanations for their predictions, even when they are correct. The paper also mentions that evaluating free-text explanations is a challenging task due to the various ways explanations can be paraphrased.
- Why unresolved: The paper does not explore the potential benefits of using different types of explanations to improve the explainability of VLMs in visual abductive reasoning tasks.
- What evidence would resolve it: Experiments comparing the performance of VLMs using different types of explanations (e.g., natural language explanations, visual explanations, or a combination of both) on the NL-Eye benchmark.

## Limitations

- The benchmark uses synthetic images generated from textual descriptions, which may not capture the full complexity and ambiguity of real-world visual scenarios.
- The evaluation relies on both automated metrics and human judgment for explanation quality, but the exact protocols for human evaluation are not fully specified, raising questions about reproducibility and potential bias.
- While the paper identifies visual encoding as a primary bottleneck, it does not systematically isolate whether failures stem from the visual encoder, the multimodal fusion mechanism, or the language model's reasoning capabilities.

## Confidence

- **High Confidence**: The finding that VLMs perform significantly worse than humans on abductive reasoning tasks is well-supported by the experimental results and human evaluation data.
- **Medium Confidence**: The claim that visual encoding is the primary bottleneck is plausible given the evidence, but could benefit from more targeted ablation studies to isolate the specific failure modes.
- **Medium Confidence**: The categorization of reasoning types and their differential performance by VLMs is reasonable, though the small sample size per category (approximately 58 examples each) limits statistical power.

## Next Checks

1. **Real Image Validation**: Test VLMs on abductive reasoning tasks using real photographs rather than synthetic images to assess whether performance differences persist with more naturalistic visual inputs.

2. **Encoder Ablation Study**: Conduct systematic ablation experiments comparing different visual encoders (CLIP, SigLip, and others) and their impact on abductive reasoning performance to isolate whether the bottleneck is encoder-specific.

3. **Explanation Quality Analysis**: Perform detailed error analysis on the explanations generated by VLMs, categorizing failures into types (e.g., temporal reasoning errors, missing key details, style issues) to better understand the nature of the reasoning deficits.