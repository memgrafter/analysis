---
ver: rpa2
title: Better Monocular 3D Detectors with LiDAR from the Past
arxiv_id: '2404.05139'
source_url: https://arxiv.org/abs/2404.05139
tags:
- depth
- detection
- lidar
- object
- asyncdepth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AsyncDepth, a method to improve monocular
  3D object detection by leveraging asynchronous LiDAR scans from past traversals.
  The core idea is to extract depth maps from historical LiDAR point clouds, featurize
  them using a learned depth featurizer, and aggregate these features across traversals.
---

# Better Monocular 3D Detectors with LiDAR from the Past

## Quick Facts
- **arXiv ID**: 2404.05139
- **Source URL**: https://arxiv.org/abs/2404.05139
- **Reference count**: 40
- **Key outcome**: AsyncDepth improves monocular 3D detection by leveraging past LiDAR scans, achieving up to 9.5 mAP gain on far-range objects with minimal latency (9.66 ms) and storage cost.

## Executive Summary
AsyncDepth introduces a novel method to enhance monocular 3D object detection by leveraging asynchronous LiDAR scans from past traversals. The approach projects historical LiDAR point clouds into depth maps, extracts learned depth features, and fuses them with image features to improve detection accuracy. By identifying and removing transient objects, AsyncDepth provides accurate 3D data about static backgrounds, which helps disambiguate foreground objects and correct depth estimations. The method is end-to-end trainable and requires minimal architectural changes to existing monocular detectors, demonstrating consistent performance improvements across multiple datasets and object classes.

## Method Summary
AsyncDepth improves monocular 3D detection by aggregating depth features from past LiDAR traversals at the same location. The method projects historical LiDAR point clouds into depth maps using current camera parameters, extracts features with a learned depth featurizer (ResNet18), pools these features across traversals, and concatenates them with image features before passing to the detector head. The entire pipeline is end-to-end trainable and can be integrated with almost all state-of-the-art monocular detectors (FCOS3D, Lift-Splat) with minimal modifications. During training, the depth-map-based feature extractor is jointly trained with the object detector, allowing the model to learn optimal feature extraction and fusion strategies.

## Key Results
- Up to 9.5 mAP gain on far-range objects (beyond 30m) compared to image-only baselines
- Consistent improvements across both Lyft L5 Perception and Ithaca365 datasets
- Negligible additional latency (9.66 ms) and storage cost
- Superior performance on pedestrian and cyclist detection compared to image-only methods

## Why This Works (Mechanism)

### Mechanism 1
Asynchronous LiDAR scans from past traversals provide accurate depth information for static background regions by aggregating multiple traversals to filter out transient objects while preserving static background geometry.

### Mechanism 2
Depth features from past LiDAR improve detection by providing geometric context for foreground objects, helping the detector better localize objects relative to the static background.

### Mechanism 3
End-to-end training allows the detector to learn how to best utilize asynchronous depth features by jointly optimizing the depth featurizer, image featurizer, and detector head.

## Foundational Learning

- **Monocular depth estimation limitations**: Understanding why image-only detectors struggle with 3D localization is crucial for appreciating the value of AsyncDepth. *Quick check*: Why is depth estimation from a single image considered an ill-posed problem?

- **Sensor fusion principles**: AsyncDepth is essentially a sensor fusion approach, combining information from different modalities collected at different times. *Quick check*: What are the key challenges in fusing asynchronous sensor data?

- **Bird's Eye View (BEV) representations**: Many monocular 3D detectors operate in BEV space, which is relevant for understanding how AsyncDepth integrates with these models. *Quick check*: Why is BEV representation useful for 3D object detection in autonomous driving?

## Architecture Onboarding

- **Component map**: Camera images + past LiDAR point clouds -> Depth featurizer (ResNet18) -> Pooling layer -> Concatenate with image features -> Detector head (FCOS3D/Lift-Splat) -> 3D bounding boxes

- **Critical path**: 1) Project past LiDAR point clouds into depth maps for current camera view, 2) Extract features from depth maps using depth featurizer, 3) Pool depth features across traversals, 4) Concatenate pooled depth features with image features, 5) Pass combined features through detector head

- **Design tradeoffs**: Depth featurizer choice (ResNet18 vs. ResNet50) impacts latency vs. performance; number of past traversals affects background accuracy vs. storage/computation; feature fusion method (concatenation vs. attention) impacts model complexity

- **Failure signatures**: Poor performance improvement indicates depth featurizer not learning useful representations; significant latency increase suggests depth featurizer is too large or inefficient; storage issues may occur if too many past traversals are stored

- **First 3 experiments**: 1) Ablation study comparing with and without AsyncDepth on FCOS3D for a single object class, 2) Latency measurement profiling additional computation from depth featurizer, 3) Robustness test evaluating performance with simulated localization errors

## Open Questions the Paper Calls Out

### Open Question 1
How does AsyncDepth perform under severe localization errors, and what is the threshold of tolerable localization error? The paper discusses localization error tolerance but lacks comprehensive analysis of performance under various degrees of localization errors, especially severe ones.

### Open Question 2
What is the impact of the number of past traversals (Nmax) on the performance of AsyncDepth, and is there an optimal number? The paper mentions using up to 5 past traversals but doesn't provide detailed analysis of how the number affects performance or identify an optimal value.

### Open Question 3
How does AsyncDepth's performance compare to models using synchronous LiDAR data, and what are the trade-offs? The paper includes an ablation study comparing with synchronous depth maps but doesn't provide detailed comparison of performance, cost, latency, and storage requirements.

## Limitations
- Assumes static environments between traversals; performance may degrade in dynamic environments with seasonal changes or construction
- Requires precise vehicle localization (â‰¤0.2m translation, minimal yaw error) for accurate depth map projection
- Storage overhead for past LiDAR scans across extensive road networks remains unclear for large-scale deployment

## Confidence
- **High confidence**: Core technical implementation (depth map projection, featurization, feature fusion) appears sound and well-validated
- **Medium confidence**: Performance improvements demonstrated on Lyft L5 and Ithaca365 but generalization to diverse environments requires further validation
- **Low confidence**: Claims about negligible storage and computational overhead based on limited datasets; scaling implications for large deployments remain uncertain

## Next Checks
1. **Environmental robustness test**: Evaluate AsyncDepth performance across different seasonal conditions and construction zones to quantify degradation in dynamic environments
2. **Localization sensitivity analysis**: Systematically measure performance degradation as localization error increases beyond 0.2m threshold to establish practical deployment limits
3. **Storage scaling study**: Calculate storage requirements for AsyncDepth across a city-scale road network, including LiDAR data compression strategies and cache management policies