---
ver: rpa2
title: Multi-Document Grounded Multi-Turn Synthetic Dialog Generation
arxiv_id: '2409.11500'
source_url: https://arxiv.org/abs/2409.11500
tags:
- answer
- data
- documents
- synthetic
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for generating multi-document grounded
  multi-turn synthetic dialog using Chain-of-Thought prompting and LLM-as-a-Judge.
  The approach controls dialog flow via taxonomy-driven queries and uses retrievers
  to dynamically update grounding documents.
---

# Multi-Document Grounded Multi-Turn Synthetic Dialog Generation

## Quick Facts
- arXiv ID: 2409.11500
- Source URL: https://arxiv.org/abs/2409.11500
- Reference count: 40
- Key outcome: Taxonomy-driven synthetic dialog generation with CoT prompting and LLM-as-a-Judge produces high-quality training data, showing 32.3% improvement in Recall over human data for answerable queries

## Executive Summary
This paper introduces a method for generating multi-document grounded multi-turn synthetic dialog using Chain-of-Thought prompting and LLM-as-a-Judge. The approach controls dialog flow via taxonomy-driven queries and uses retrievers to dynamically update grounding documents. Human evaluation shows high diversity (95.6%), coherency (96.7%), and correctness (73.1%) across 294 dialogs. Automatic evaluations on four benchmarks (QuAC, OR-QuAC, MultiDoc2Dial, CoQA) demonstrate that models fine-tuned on synthetic data consistently outperform those trained on human data for answerable queries, with notable gains in Recall (up to 32.3% improvement).

## Method Summary
The method generates synthetic multi-turn dialogs by controlling dialog flow through taxonomy-driven user queries created with Chain-of-Thought prompting. For single-document grounding, it uses predefined question types (Direct, Comparative, Aggregate, Unanswerable) with CoT prompts to generate queries and responses. For multi-document grounding, it integrates a retriever that dynamically selects top-k relevant passages after each user turn, updating the grounding context throughout the dialog. An LLM-as-a-Judge filters out incorrect answers, and the resulting high-quality synthetic data is used to fine-tune conversational QA models like MERLINITE-7B and LLaMA2-13B-Chat.

## Key Results
- Human evaluation shows 95.6% diversity, 96.7% coherency, and 73.1% correctness across 294 dialogs
- Models fine-tuned on synthetic data outperform human-data models on answerable queries across all benchmarks
- Multi-document synthetic data generation shows 32.3% improvement in Recall for OR-QuAC over human data
- LLM-as-a-Judge filtering particularly effective for OR-QuAC, improving accuracy on noisier multi-document synthetic dialogs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Taxonomy-driven query generation with Chain-of-Thought prompting controls dialog flow and ensures query diversity.
- Mechanism: The system uses predefined question types (Direct, Comparative, Aggregate, Unanswerable for initial turns; Follow-up, Clarification, Correction for subsequent turns) combined with CoT prompts to steer LLM query generation. This replaces pure LM sampling with structured query types.
- Core assumption: CoT prompting with taxonomy constraints effectively controls LLM output without degrading naturalness.
- Evidence anchors:
  - [abstract] "we control the overall dialog flow using taxonomy-driven user queries that are generated with Chain-of-Thought (CoT) prompting"
  - [section] "we incorporate question-type specific CoT prompts that instruct an LLM to reason through the grounding document" and "using 3 in-context learning (ICL) examples of a question type... resulted in only 5% of the intended query type, whereas CoT prompt for a specific query type... resulted in more than 90% of the generated queries adhering to the intended query type"

### Mechanism 2
- Claim: Retriever-augmented multi-document grounding mimics real-world RAG deployments and improves task performance.
- Mechanism: The system dynamically retrieves top-k passages after each user turn and uses them as grounding for subsequent agent answers, updating the retrieval context throughout the dialog.
- Core assumption: Real-world RAG systems retrieve relevant documents per query, so synthetic data should mirror this dynamic retrieval behavior.
- Evidence anchors:
  - [abstract] "support the generation of multi-document grounded dialogs by mimicking real-world use of retrievers to update the grounding documents after every user-turn in the dialog"
  - [section] "For multi-document grounded dialog, we integrate a retriever that dynamically selects top-k relevant passages from a pre-constructed document index" and "the model fine-tuned on the synthetic data generated with the RAG setting out-performs the one fine-tuned on the synthetic data generated with single document"

### Mechanism 3
- Claim: LLM-as-a-Judge filtering ensures answer correctness while maintaining high diversity and coherence.
- Mechanism: After generating dialogs, an LLM judges each context-response pair for correctness and filters out incorrect answers, improving the quality of training data.
- Core assumption: LLMs can reliably judge their own answer correctness when given proper CoT reasoning context.
- Evidence anchors:
  - [abstract] "we apply LLM-as-a-Judge to filter out queries with incorrect answers" and "synthetic-filtered is particularly effective for OR-QuAC, indicating that synthetic dialogs generated from retrieved passages are noisier than those generated from human generated single document and therefore, post-filtering improves the model accuracy"
  - [section] "We incorporate an LLM-as-a-Judge module to filter out queries with incorrect answers" and evaluation shows "Answer correctness is 73.1%"

## Foundational Learning

- Concept: Chain-of-Thought reasoning
  - Why needed here: CoT enables the LLM to systematically process documents and generate queries/answers that follow logical reasoning patterns rather than random sampling
  - Quick check question: How does CoT prompting improve the adherence of generated queries to the intended taxonomy types compared to standard prompting?

- Concept: Retriever-augmented generation (RAG) architecture
  - Why needed here: The system needs to simulate real-world RAG deployments where relevant documents are retrieved dynamically per query rather than using static grounding
  - Quick check question: What is the key difference between single-document grounding and multi-document grounding in this system?

- Concept: Few-shot learning with in-context examples
  - Why needed here: The system uses ICL examples within CoT prompts to demonstrate desired query types to the LLM
  - Quick check question: According to the paper, what percentage improvement in query type adherence was achieved by using CoT prompts versus ICL examples alone?

## Architecture Onboarding

- Component map: Taxonomy generator → CoT prompt templates → LLM query/answer generator → Retriever (for multi-doc) → LLM-as-a-Judge → Training data pipeline
- Critical path: Document → Query generation (taxonomy+CoT) → Answer generation (CoT) → (Multi-doc: Retrieval update) → Judge filtering → Training data
- Design tradeoffs: Single-doc vs multi-doc grounding (simplicity vs realism), Judge strictness vs data volume, Taxonomy coverage vs prompt complexity
- Failure signatures: Low diversity scores (taxonomy too restrictive), Low correctness (judge too lenient or retriever poor), Low coherency (CoT prompts poorly designed)
- First 3 experiments:
  1. Generate single-turn dialogs with each taxonomy type to verify CoT prompt effectiveness
  2. Compare multi-doc vs single-doc grounding on a small subset to measure retriever impact
  3. Test judge filtering threshold by varying judge strictness and measuring correctness/diversity tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of models trained on synthetic data change as the complexity of the question taxonomy increases beyond the current three categories (direct, comparative, aggregate)?
- Basis in paper: [inferred] The paper uses a specific question taxonomy to generate synthetic data, but does not explore the impact of more complex or numerous question types on model performance.
- Why unresolved: The study only evaluates a limited set of question types. There is no exploration of how adding more nuanced or complex question categories might affect the quality and diversity of synthetic data and subsequent model performance.
- What evidence would resolve it: Experiments comparing model performance using synthetic data generated with expanded or more complex question taxonomies against current results, showing whether increased taxonomy complexity leads to better generalization or introduces new challenges.

### Open Question 2
- Question: What is the optimal balance between single-document and multi-document grounded synthetic data for training models that perform well on both single and multi-document benchmark tasks?
- Basis in paper: [explicit] The paper shows that multi-document grounded data generation is more effective for multi-document grounded tasks (OR-QuAC), but does not explore the optimal mix of single and multi-document data for general performance.
- Why unresolved: The study focuses on either single or multi-document grounded data generation separately, without investigating how combining both types might affect overall model performance across different benchmark tasks.
- What evidence would resolve it: Comparative experiments training models on various ratios of single to multi-document synthetic data, measuring performance on both single-document (CoQA, QuAC) and multi-document (MultiDoc2Dial, OR-QuAC) benchmarks to identify the optimal balance.

### Open Question 3
- Question: How does the quality and diversity of synthetic dialogs change when using different large language models (LLMs) for generation and evaluation?
- Basis in paper: [explicit] The study uses MIXTRAL -8X7B-INSTRUCT throughout but acknowledges that the framework is not tied to any specific model, suggesting potential variability with different LLMs.
- Why unresolved: The research is limited to one specific LLM for both data generation and evaluation, leaving open questions about how results might differ with other models, particularly regarding diversity, coherency, and correctness metrics.
- What evidence would resolve it: Human and automatic evaluations of synthetic dialogs generated using different LLMs (e.g., GPT-4, Claude, LLaMA variants) with the same framework, comparing quality metrics to determine if certain models consistently produce better synthetic data across different metrics.

## Limitations

- The 73.1% correctness rate indicates that nearly 27% of generated dialogs contain errors, which could propagate to downstream model training
- The CoT prompts and LLM-as-a-Judge templates are not fully specified in the main paper, making exact reproduction difficult
- The comparison with human-generated data doesn't fully address whether synthetic data captures the full complexity and nuance of human dialog patterns

## Confidence

**High confidence** in the mechanism that taxonomy-driven CoT prompting controls dialog flow and ensures query diversity, supported by quantitative evidence showing 90%+ adherence to intended query types versus 5% with ICL alone.

**Medium confidence** in the effectiveness of LLM-as-a-Judge filtering for ensuring answer correctness, as the 73.1% correctness rate is reasonable but leaves significant room for improvement.

**Medium confidence** in the superiority of multi-document grounding over single-document for real-world RAG tasks, though the paper doesn't provide extensive analysis of retriever performance or failure modes.

## Next Checks

1. **Test CoT Prompt Robustness**: Generate dialogs using both the specified CoT prompts and standard ICL examples, then measure query type adherence and dialog quality to validate the claimed 85% improvement in taxonomy adherence.

2. **Analyze Retriever Performance**: Conduct ablation studies comparing dialogs generated with perfect retriever (oracle) versus the actual ELSER retriever, measuring the impact of retrieval noise on answer correctness and overall dialog quality.

3. **Validate Judge Consistency**: Have multiple human annotators assess the same set of LLM-judged dialogs to measure inter-annotator agreement and determine if the LLM-as-a-Judge is reliably identifying incorrect answers versus filtering valid but unconventional responses.