---
ver: rpa2
title: 'From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers for
  Underrepresented Languages'
arxiv_id: '2410.18836'
source_url: https://arxiv.org/abs/2410.18836
tags:
- language
- tokens
- ukrainian
- arabic
- vocabulary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a model-agnostic, cost-effective method for
  creating bilingual large language models (LLMs) supporting English and a target
  language. The approach involves vocabulary extension, embedding initialization,
  and continual pre-training.
---

# From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers for Underrepresented Languages

## Quick Facts
- **arXiv ID**: 2410.18836
- **Source URL**: https://arxiv.org/abs/2410.18836
- **Reference count**: 22
- **Primary result**: A model-agnostic, cost-effective method for creating bilingual LLMs supporting English and target underrepresented languages through vocabulary extension, embedding initialization, and continual pre-training

## Executive Summary
This paper presents a methodology for transforming English-centric LLMs into effective bilingual models supporting underrepresented languages. The approach involves extending the vocabulary to include language-specific tokens, initializing embeddings for new tokens, and performing continual pre-training on bilingual corpora. Experiments with Ukrainian, Arabic, and Georgian demonstrate that the method improves language-specific performance while maintaining English capabilities, with the added benefit of reduced computational costs compared to training from scratch.

## Method Summary
The methodology employs a three-step process to create bilingual LLMs. First, it extends the existing vocabulary by identifying and adding tokens specific to the target underrepresented language, using a frequency-based approach on a small corpus (3-4GB). Second, it initializes embeddings for the new tokens through a combination of copying from existing tokens and random initialization for truly novel tokens. Third, it performs continual pre-training on a mix of English and target language data to adapt the model to the extended vocabulary and bilingual context.

## Key Results
- Improved language performance on Ukrainian, Arabic, and Georgian while maintaining English performance on MMLU benchmark
- Reduced computational costs compared to training bilingual models from scratch
- Better handling of code-switching and grammar correctness in the target languages
- Vocabulary extension reduced tokenizer fertility and improved generation efficiency

## Why This Works (Mechanism)
The method works by addressing the fundamental mismatch between English-centric tokenizers and underrepresented languages. By extending the vocabulary with language-specific tokens, the tokenizer can more efficiently represent the target language's morphology and orthography. The embedding initialization ensures smooth integration of new tokens without catastrophic forgetting, while continual pre-training adapts the model's parameters to the bilingual context. This approach leverages the existing knowledge in the English model while efficiently incorporating language-specific patterns.

## Foundational Learning
- **Vocabulary Extension**: Why needed - English-centric tokenizers poorly represent underrepresented languages; Quick check - Compare tokenization of target language before/after extension
- **Embedding Initialization**: Why needed - New tokens require meaningful embeddings to integrate smoothly; Quick check - Verify embedding similarity between copied and original tokens
- **Continual Pre-training**: Why needed - Adapts model parameters to bilingual context without catastrophic forgetting; Quick check - Monitor language-specific perplexity during training
- **Tokenizer Fertility**: Why needed - High fertility indicates inefficient tokenization; Quick check - Compare average tokens per word between original and extended tokenizers
- **Code-switching**: Why needed - Real-world usage often involves language mixing; Quick check - Test model's ability to handle bilingual prompts

## Architecture Onboarding
**Component Map**: Original LLM -> Vocabulary Extension -> Embedding Initialization -> Continual Pre-training -> Bilingual LLM

**Critical Path**: Vocabulary extension and embedding initialization must occur before continual pre-training to ensure proper integration of new tokens.

**Design Tradeoffs**: The method trades off some English-specific optimization for better target language performance, but this is offset by the ability to maintain overall bilingual capability. The vocabulary size increase is minimal compared to training a completely new bilingual model.

**Failure Signatures**: Poor target language performance may indicate insufficient vocabulary coverage or inadequate embedding initialization. Degraded English performance suggests over-adaptation to the target language during continual pre-training.

**First Experiments**:
1. Tokenize sample text from target language with original and extended tokenizers to verify vocabulary coverage
2. Compare perplexity on held-out target language data before and after continual pre-training
3. Test code-switching capability with bilingual prompts to verify model integration

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, focusing instead on demonstrating the effectiveness of the proposed methodology.

## Limitations
- Relatively small target language corpora used for vocabulary expansion (3-4GB per language)
- Lack of ablation studies on vocabulary size effects
- Limited analysis of how custom tokenizers affect overall model capabilities beyond MMLU benchmark
- Computational cost comparisons based on theoretical calculations rather than measured training runs

## Confidence
- **High confidence**: Vocabulary extension methodology and embedding initialization approach
- **Medium confidence**: Claimed improvements in grammar correctness and code-switching handling
- **Medium confidence**: Computational efficiency claims based on theoretical calculations

## Next Checks
1. Conduct ablation studies varying vocabulary sizes to determine optimal parameters for different language families and corpus sizes
2. Perform controlled experiments measuring actual training time and memory usage to validate the claimed computational efficiency improvements
3. Test the methodology on additional underrepresented languages with different morphological complexity and writing systems to assess generalizability