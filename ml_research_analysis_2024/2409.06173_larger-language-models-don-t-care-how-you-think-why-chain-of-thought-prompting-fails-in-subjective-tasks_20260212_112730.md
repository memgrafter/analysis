---
ver: rpa2
title: 'Larger Language Models Don''t Care How You Think: Why Chain-of-Thought Prompting
  Fails in Subjective Tasks'
arxiv_id: '2409.06173'
source_url: https://arxiv.org/abs/2409.06173
tags:
- reasoning
- priors
- language
- task
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Chain-of-Thought (CoT) prompting
  can overcome the limitations of In-Context Learning (ICL) in subjective tasks like
  emotion recognition and morality classification. The authors find that larger language
  models rely heavily on reasoning priors that ossify predictions, similar to the
  label priors observed in ICL.
---

# Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks

## Quick Facts
- arXiv ID: 2409.06173
- Source URL: https://arxiv.org/abs/2409.06173
- Reference count: 27
- Larger language models rely on reasoning priors that ossify predictions in subjective tasks, limiting CoT effectiveness.

## Executive Summary
This paper investigates whether Chain-of-Thought (CoT) prompting can overcome the limitations of In-Context Learning (ICL) in subjective tasks like emotion recognition and morality classification. The authors find that larger language models rely heavily on reasoning priors that ossify predictions, similar to the label priors observed in ICL. For complex subjective tasks, CoT does not significantly improve performance over ICL, and the generated reasoning remains coherent but fails to incorporate evidence from the prompt. Larger models show stronger pull from reasoning priors, while smaller models exhibit more flexibility. The results suggest that CoT alone cannot bridge the gap between prior knowledge and task-specific evidence in subjective domains.

## Method Summary
The study evaluates Chain-of-Thought (CoT) prompting against In-Context Learning (ICL) on subjective tasks such as emotion recognition and morality classification. The authors compare model performance across different sizes, analyzing how reasoning priors influence predictions. They assess whether CoT-generated reasoning incorporates evidence from prompts or relies on pre-existing knowledge. The analysis focuses on the coherence of reasoning and its alignment with task-specific evidence, particularly in subjective domains where interpretation varies.

## Key Results
- Larger language models rely heavily on reasoning priors that ossify predictions, similar to label priors in ICL.
- CoT does not significantly improve performance over ICL in subjective tasks, and reasoning often fails to incorporate prompt evidence.
- Larger models show stronger pull from reasoning priors, while smaller models exhibit more flexibility in adapting to task-specific evidence.

## Why This Works (Mechanism)
Chain-of-Thought prompting relies on the model's ability to generate intermediate reasoning steps to arrive at a final answer. In subjective tasks, this mechanism is undermined by the model's reliance on pre-existing reasoning priors, which dominate over task-specific evidence. The study shows that larger models, which have stronger priors due to their extensive training, are more likely to ossify predictions based on these priors rather than adapt to the nuances of the prompt. This suggests that CoT's effectiveness is limited when the task requires interpretation that conflicts with the model's learned reasoning patterns.

## Foundational Learning
- **Chain-of-Thought (CoT) Prompting**: A technique where models generate intermediate reasoning steps before producing a final answer. Why needed: To improve reasoning in complex tasks. Quick check: Does the model produce coherent intermediate steps?
- **In-Context Learning (ICL)**: A method where models learn from examples provided in the prompt without fine-tuning. Why needed: To evaluate CoT against a baseline approach. Quick check: Does the model perform well with few examples?
- **Reasoning Priors**: Pre-existing knowledge embedded in the model from training data. Why needed: To understand how models rely on prior knowledge. Quick check: Does the model's reasoning align with its training data?
- **Subjective Tasks**: Tasks like emotion recognition and morality classification where interpretation varies. Why needed: To test CoT's effectiveness in ambiguous domains. Quick check: Is the task inherently subjective or objective?

## Architecture Onboarding
- **Component Map**: Prompt -> Reasoning Generation -> Final Prediction
- **Critical Path**: The reasoning generation step is critical, as it determines whether the model incorporates prompt evidence or relies on priors.
- **Design Tradeoffs**: CoT prioritizes interpretability but may sacrifice adaptability in subjective tasks. ICL is simpler but lacks explicit reasoning steps.
- **Failure Signatures**: Reasoning remains coherent but fails to address prompt-specific evidence, especially in larger models.
- **First Experiments**: 1) Test CoT on objective tasks to compare performance. 2) Vary prompt complexity to assess reasoning adaptability. 3) Explore dynamic CoT strategies to mitigate prior reliance.

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on subjective tasks, limiting generalizability to other domains where CoT may be more effective.
- The analysis assumes a framework for quantifying reasoning quality but does not rigorously define it.
- The study does not explore alternative CoT strategies, such as few-shot prompting or dynamic reasoning adjustments.

## Confidence
- **High Confidence**: Larger models show stronger pull from reasoning priors, supported by empirical results.
- **Medium Confidence**: CoT does not significantly improve performance over ICL in subjective tasks, but this may depend on specific tasks and datasets.
- **Low Confidence**: CoT alone cannot bridge the gap between prior knowledge and task-specific evidence in subjective domains, as this claim lacks exploration of alternative strategies.

## Next Checks
1. Validate findings on a broader range of subjective tasks to assess generalizability.
2. Test dynamic CoT prompting or few-shot examples to overcome observed limitations.
3. Develop a framework to measure the degree to which reasoning incorporates prompt-specific evidence versus prior knowledge.