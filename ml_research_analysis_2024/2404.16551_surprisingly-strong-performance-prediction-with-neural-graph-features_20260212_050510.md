---
ver: rpa2
title: Surprisingly Strong Performance Prediction with Neural Graph Features
arxiv_id: '2404.16551'
source_url: https://arxiv.org/abs/2404.16551
tags:
- graf
- features
- neural
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines biases in zero-cost proxies and proposes neural
  graph features (GRAF) to address their limitations. GRAF are simple to compute properties
  of neural architecture graphs, such as operation counts and node degrees.
---

# Surprisingly Strong Performance Prediction with Neural Graph Features

## Quick Facts
- **arXiv ID:** 2404.16551
- **Source URL:** https://arxiv.org/abs/2404.16551
- **Reference count:** 40
- **Primary result:** GRAF outperforms zero-cost proxies and other encodings for neural architecture performance prediction across accuracy, hardware, and robustness tasks

## Executive Summary
This work examines biases in zero-cost proxies and proposes neural graph features (GRAF) to address their limitations. GRAF are simple to compute properties of neural architecture graphs, such as operation counts and node degrees. The method uses GRAF as input to performance predictors like random forests, yielding strong and interpretable predictions across various tasks including accuracy, hardware metrics, and robustness. GRAF outperforms zero-cost proxies and other common encodings, especially when combined with them. The approach demonstrates that different tasks favor diverse network properties and can improve existing models like BRP-NAS. Overall, GRAF provides a fast, interpretable, and effective solution for neural architecture search performance prediction.

## Method Summary
The method extracts graph-based features from neural architecture graphs including operation counts, path lengths, and node degrees. These GRAF features are then used as input to performance predictors such as random forests or XGBoost. The approach can be combined with zero-cost proxies to improve prediction accuracy. The framework is evaluated across multiple NAS benchmarks (NB101, NB201, NB301) and tasks including accuracy prediction, hardware metrics, and robustness. The method is designed to be interpretable, allowing analysis of which architectural properties are important for different tasks.

## Key Results
- GRAF outperforms zero-cost proxies and one-hot encodings across all tested benchmarks and tasks
- Combining GRAF with zero-cost proxies yields the best overall performance
- Different tasks favor different network properties (e.g., skip-connections important for CIFAR-10, average pooling for autoencoders)
- GRAF can improve existing models like BRP-NAS when used as input features
- The method is interpretable, revealing which architectural properties drive performance for specific tasks

## Why This Works (Mechanism)

### Mechanism 1
Zero-cost proxies have strong correlations with validation accuracy on certain tasks (like CIFAR-10) but these correlations are driven by direct dependencies on specific network properties (e.g., number of convolutions) rather than true architectural merit. Zero-cost proxies like `nwot` and `l2 norm` correlate strongly with validation accuracy because the score directly depends on the number of convolutional operations. For example, in NAS-Bench-201, each cluster of architectures with the same number of convolutions forms a line with `nwot` increasing with the number of convolutions. The apparent correlation is not a true reflection of performance but rather an artifact of the proxy's sensitivity to operation count.

### Mechanism 2
Neural Graph Features (GRAF) provide a more interpretable and effective input to performance predictors by capturing diverse network properties beyond simple operation counts. GRAF uses features like operation counts, minimum/maximum path lengths, and node degrees. These features provide a richer representation of the network's structure, allowing the predictor to learn which architectural properties are important for different tasks. For example, on NB201 CIFAR-10, skip-connection and convolution min path length (shortcuts) are the most important features, while on TNB101-micro autoencoder, shortcuts with skip-connection and average pooling are important.

### Mechanism 3
Combining zero-cost proxies with GRAF features leads to the best performance prediction results because they capture complementary information. Zero-cost proxies capture certain network properties (like operation counts) that are important for some tasks, while GRAF features capture a broader range of architectural properties. By combining them, the predictor can leverage the strengths of both, leading to improved accuracy. For example, on NB201 CIFAR-10, ZCP + GRAF yields the overall best results.

## Foundational Learning

- **Concept:** Graph Theory (nodes, edges, paths, degrees)
  - Why needed here: GRAF features are based on graph properties of neural architecture graphs. Understanding graph theory is essential for comprehending how these features are computed and interpreted.
  - Quick check question: What is the difference between a path and a cycle in a graph? What does the degree of a node represent?

- **Concept:** Neural Architecture Search (NAS) Benchmarks
  - Why needed here: The paper evaluates GRAF on various NAS benchmarks (NAS-Bench-101, NAS-Bench-201, etc.). Understanding these benchmarks and their search spaces is crucial for interpreting the results.
  - Quick check question: What is the difference between a cell-based and a macro search space in NAS?

- **Concept:** Performance Prediction Methods (Random Forest, GNNs)
  - Why needed here: GRAF is used as input to performance predictors like random forests. Understanding these methods is important for understanding how GRAF's features are used to predict network performance.
  - Quick check question: What is the advantage of using a random forest over a single decision tree for performance prediction?

## Architecture Onboarding

- **Component Map:** Neural architecture graph -> GRAF Feature Extractor -> Performance Predictor (Random Forest/XGBoost) -> Predicted performance

- **Critical Path:**
  1. Load neural architecture graph from benchmark
  2. Compute GRAF features
  3. Train performance predictor on GRAF features (and optionally zero-cost proxies)
  4. Use predictor to estimate performance of new architectures

- **Design Tradeoffs:**
  - GRAF vs. Zero-cost proxies: GRAF provides richer features but requires computing graph properties, while zero-cost proxies are faster but may have biases.
  - GRAF vs. GNN-based predictors: GRAF is simpler and more interpretable, but GNNs may capture more complex patterns.
  - Feature selection: Including all GRAF features vs. a subset of linearly independent features.

- **Failure Signatures:**
  - Poor performance on tasks where network performance does not correlate with operation counts (e.g., TNB101-micro class scene).
  - Inability to handle search spaces significantly different from cell-based or macro search spaces.
  - Overfitting when using a large number of GRAF features with limited training data.

- **First 3 Experiments:**
  1. Compute GRAF features for a small set of architectures from NAS-Bench-201 and visualize their distribution to understand which features capture different network properties.
  2. Train a random forest predictor on GRAF features alone and evaluate its performance on CIFAR-10 accuracy prediction in NAS-Bench-201. Compare the results with using zero-cost proxies.
  3. Combine GRAF features with zero-cost proxies and retrain the random forest predictor. Evaluate the performance and analyze the feature importance to understand which features contribute most to the prediction.

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness currently validated primarily on cell-based search spaces with potential limitations for macro or more complex architectures
- Feature computation relies on predefined graph properties that may not capture all relevant architectural characteristics
- Interpretation of feature importance using SHAP values may not always align with actual performance drivers

## Confidence
- **High Confidence:** GRAF's ability to capture diverse network properties beyond operation counts, and its effectiveness in combining with zero-cost proxies for improved prediction
- **Medium Confidence:** Task-specific feature importance patterns, as these may vary across different datasets and search spaces
- **Low Confidence:** Generalization to search spaces significantly different from those tested, as the current evaluation is limited to specific benchmark architectures

## Next Checks
1. Evaluate GRAF on macro search spaces and more complex architectures to assess its generalization beyond cell-based designs
2. Test the feature importance patterns on additional tasks and datasets to verify consistency across different domains
3. Implement cross-validation with varying training set sizes to determine GRAF's robustness to limited data scenarios