---
ver: rpa2
title: 'REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis
  via Large Language Models'
arxiv_id: '2402.07016'
source_url: https://arxiv.org/abs/2402.07016
tags:
- data
- knowledge
- clinical
- entities
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents REALM, a Retrieval-Augmented Generation (RAG)-driven
  framework that enhances multimodal electronic health records (EHR) analysis via
  large language models (LLMs). REALM extracts task-relevant medical entities from
  clinical notes and time-series data, matches them with a professionally labeled
  external knowledge graph (PrimeKG), and fuses the retrieved knowledge with the original
  data modalities using an adaptive multimodal fusion network.
---

# REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models

## Quick Facts
- arXiv ID: 2402.07016
- Source URL: https://arxiv.org/abs/2402.07016
- Reference count: 17
- 1.09-2.06% relative improvement in AUROC and 2.75-4.75% improvement in AUPRC for mortality and readmission prediction tasks

## Executive Summary
REALM presents a Retrieval-Augmented Generation (RAG)-driven framework for enhancing multimodal electronic health records (EHR) analysis using large language models (LLMs). The framework addresses limitations of previous approaches that focused only on structured data by extracting task-relevant medical entities from clinical notes and time-series data, matching them with an external knowledge graph (PrimeKG), and fusing the retrieved knowledge with original data modalities using an adaptive multimodal fusion network. Experimental results on MIMIC-III mortality and readmission prediction tasks demonstrate REALM's superior performance over baseline models.

## Method Summary
REALM processes multimodal EHR data by first extracting embeddings from time-series data using GRU networks and from clinical notes using LLM encoders. It then employs a RAG-driven enhancement pipeline that extracts medical entities from both modalities using prompted LLMs, matches these entities to the PrimeKG knowledge graph using cosine similarity, and encodes the retrieved knowledge using LLM. The enhanced representations are integrated with the original EHR embeddings through an adaptive multimodal fusion network consisting of self-attention and cross-attention layers. Finally, the fused representations are passed through an MLP for binary classification tasks.

## Key Results
- Achieved 1.09-2.06% relative improvement in AUROC over baseline models
- Demonstrated 2.75-4.75% improvement in AUPRC for mortality and readmission prediction
- Showed superior performance particularly in data-sparse scenarios with enhanced robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG-driven entity extraction reduces hallucination and improves retrieval relevance by matching LLM-extracted entities to a professionally labeled external knowledge graph.
- Mechanism: The framework extracts disease entities from clinical notes and time-series data using a carefully prompted LLM, then filters and validates these entities against the PrimeKG knowledge graph using cosine similarity and a threshold-based matching process.
- Core assumption: The LLM can extract clinically relevant entities when properly prompted, and the PrimeKG contains accurate, comprehensive medical entity definitions that can be reliably matched.
- Evidence anchors:
  - [abstract] "we prompt LLM to extract task-relevant medical entities and match entities in professionally labeled external knowledge graph (PrimeKG) with corresponding medical knowledge"
  - [section] "By matching and aligning with clinical standards, our framework eliminates hallucinations and ensures consistency"
  - [corpus] Weak evidence - corpus only shows related papers but doesn't provide direct validation of hallucination elimination
- Break condition: If the LLM fails to extract accurate entities, or if PrimeKG lacks coverage of relevant medical entities, the matching process breaks down and hallucinations persist.

### Mechanism 2
- Claim: Adaptive multimodal fusion with self- and cross-attention improves clinical prediction accuracy by learning modality-specific and cross-modal relationships.
- Mechanism: The framework uses self-attention to capture intra-modal relationships and cross-attention to fuse information between time-series and text modalities, creating a unified representation for downstream prediction tasks.
- Core assumption: The time-series and text modalities contain complementary information that can be effectively integrated through attention mechanisms to improve prediction performance.
- Evidence anchors:
  - [abstract] "we propose an adaptive multimodal fusion network to integrate extracted knowledge with multimodal EHR data"
  - [section] "we proposed an attention-based fusion network mainly consisting of self-attention layers and cross-attention layers"
  - [corpus] Weak evidence - corpus shows related papers but doesn't directly validate the effectiveness of this specific fusion approach
- Break condition: If the attention mechanisms fail to capture meaningful relationships between modalities, or if one modality dominates the fusion process, prediction accuracy may not improve.

### Mechanism 3
- Claim: Encoding retrieved knowledge with LLM and combining it with original EHR data provides richer semantic medical context for clinical predictions.
- Mechanism: The framework retrieves detailed entity definitions and descriptions from PrimeKG, encodes them with LLM, and fuses this knowledge with the original EHR embeddings to create enhanced representations.
- Core assumption: The additional semantic medical knowledge from PrimeKG contains task-relevant information that can improve the model's understanding of clinical contexts.
- Evidence anchors:
  - [abstract] "By extending the definition and description of entities beyond simple triples, REALM captures more complex semantic medical background knowledge"
  - [section] "we encode them using LLM. Firstly, we concatenate each node details together in format like (entity name, entity definition, entity description)"
  - [corpus] Weak evidence - corpus doesn't directly validate the effectiveness of this knowledge encoding approach
- Break condition: If the retrieved knowledge is not relevant to the prediction task, or if the encoding process fails to preserve meaningful information, the enhancement may not improve prediction accuracy.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG combines the benefits of information retrieval and text generation, allowing the model to access external knowledge bases while maintaining the generation capabilities of LLMs.
  - Quick check question: What is the primary advantage of using RAG over traditional fine-tuning approaches for incorporating external knowledge?

- Concept: Multimodal Fusion with Attention Mechanisms
  - Why needed here: Different EHR modalities (time-series and text) contain complementary information that needs to be effectively integrated to improve clinical predictions.
  - Quick check question: How do self-attention and cross-attention mechanisms differ in their approach to modality fusion?

- Concept: Cosine Similarity for Entity Matching
  - Why needed here: Cosine similarity provides a quantitative measure to compare the semantic similarity between extracted entities and knowledge graph nodes, enabling reliable entity matching.
  - Quick check question: Why is cosine similarity preferred over other distance metrics for comparing high-dimensional embedding vectors?

## Architecture Onboarding

- Component map: Multimodal EHR Embedding Extraction -> RAG-Driven Enhancement Pipeline -> Multimodal Fusion Network -> Prediction Module
- Critical path: Time-series/text embedding → RAG enhancement → Fusion → Prediction
- Design tradeoffs: Using LLM for entity extraction vs. rule-based approaches; RAG enhancement vs. direct KG integration; complex fusion vs. simple concatenation
- Failure signatures: Poor entity matching (low retrieval accuracy), attention mechanism failures (mode collapse), knowledge encoding issues (hallucinations)
- First 3 experiments:
  1. Test entity extraction accuracy with different LLM prompts and PrimeKG matching thresholds
  2. Evaluate fusion performance with different attention mechanisms and fusion strategies
  3. Measure prediction accuracy improvement with vs. without RAG enhancement across different sparsity levels

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several critical areas remain unresolved:

Open Question 1
- Question: How does REALM perform when evaluated on external datasets beyond MIMIC-III, such as eICU or other international EHR databases?
- Basis in paper: [inferred] The paper demonstrates robustness to data sparsity but only tests on MIMIC-III dataset.
- Why unresolved: The authors only validate their framework on a single dataset, limiting generalizability claims.
- What evidence would resolve it: Comparative performance metrics (AUROC, AUPRC, F1) on at least 2-3 additional independent EHR datasets.

Open Question 2
- Question: What is the computational overhead of the RAG-driven enhancement pipeline compared to traditional multimodal EHR models?
- Basis in paper: [inferred] The paper mentions offline operations but doesn't quantify latency or resource requirements.
- Why unresolved: Real-world clinical deployment requires understanding trade-offs between accuracy gains and computational costs.
- What evidence would resolve it: Runtime comparison (inference time, memory usage, GPU requirements) between REALM and baseline models.

Open Question 3
- Question: How does REALM handle conflicting information between clinical notes and time-series data when making predictions?
- Basis in paper: [inferred] The fusion network is described but conflict resolution mechanisms are not detailed.
- Why unresolved: In practice, different data modalities may provide contradictory clinical information requiring explicit resolution strategies.
- What evidence would resolve it: Analysis of conflict cases and explanation of how the model prioritizes or reconciles contradictory inputs.

## Limitations

- The framework's performance relies heavily on the quality and coverage of the PrimeKG knowledge graph, which may not be available or complete for all medical domains
- The paper lacks detailed implementation specifications for critical components like the LLM encoder and PrimeKG matching process, hindering faithful reproduction
- Evaluation is limited to mortality and readmission prediction tasks on MIMIC-III, restricting generalizability to other clinical applications

## Confidence

- Mechanism 1 (RAG-driven entity matching): Medium - while the approach is theoretically sound, the effectiveness depends heavily on LLM accuracy and PrimeKG coverage
- Mechanism 2 (Adaptive multimodal fusion): Medium - attention mechanisms are well-established, but their specific implementation and performance impact require further validation
- Mechanism 3 (Knowledge encoding and integration): Low - the encoding process and knowledge integration approach lack sufficient empirical validation

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of entity matching quality, knowledge encoding, and fusion mechanisms to overall performance
2. Test the framework's robustness across different clinical prediction tasks and datasets beyond mortality and readmission prediction
3. Implement a comprehensive error analysis to identify failure modes in entity extraction, matching, and fusion processes