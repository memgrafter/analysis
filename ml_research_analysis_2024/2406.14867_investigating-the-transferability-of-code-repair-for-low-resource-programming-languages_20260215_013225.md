---
ver: rpa2
title: Investigating the Transferability of Code Repair for Low-Resource Programming
  Languages
arxiv_id: '2406.14867'
source_url: https://arxiv.org/abs/2406.14867
tags:
- code
- repair
- distilrr
- pass
- rationale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the transfer of code repair capabilities
  from high-resource to low-resource programming languages using distillation. The
  proposed method, DistiLRR, trains a student model to both generate rationales and
  code repairs by learning from a teacher model's outputs on synthetically created
  repair examples.
---

# Investigating the Transferability of Code Repair for Low-Resource Programming Languages

## Quick Facts
- arXiv ID: 2406.14867
- Source URL: https://arxiv.org/abs/2406.14867
- Reference count: 20
- Low-resource languages show 77-145% improvements in pass@1 rates using DistiLRR

## Executive Summary
This paper introduces DistiLRR, a distillation-based approach for transferring code repair capabilities from high-resource to low-resource programming languages. The method trains student models to generate both rationales and code repairs by learning from a teacher model's outputs on synthetically created repair examples. Experiments across six programming languages show that DistiLRR consistently outperforms baseline iterative repair approaches, particularly for low-resource languages where improvements range from 77% to 145% in pass@1 rates. The authors find that the correlation between rationale quality and code correctness is weaker than previously thought, especially in low-resource settings, and that DistiLRR's effectiveness stems from improving models' ability to convert feedback into specific code modifications.

## Method Summary
The DistiLRR approach involves creating synthetic training data by having a teacher model (GPT-3.5-Turbo) generate rationales and correct repairs for incorrect code samples from low-resource languages. Student models (CodeLlama-7b-Instruct, CodeLlama-7b, or Mistral-7b) are then fine-tuned on this data using LoRA with specific hyperparameters. During evaluation, the fine-tuned models generate initial code samples, execute them to collect errors, and apply iterative repair using the distilled model's ability to convert feedback into language-specific modifications. The process continues for up to 4 rounds or until tests pass.

## Key Results
- DistiLRR improves pass@1 rates for low-resource languages by 77-145% compared to baseline iterative repair
- The correlation between rationale quality and code correctness is weaker than previously perceived, especially for low-resource languages
- DistiLRR outperforms baseline approaches on both MBXP and HumanEval benchmarks across all six tested languages
- The method shows particular effectiveness for low-resource languages (Perl, Go, Swift) while maintaining similar performance on high-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DistiLRR improves responsiveness to feedback by teaching models to connect rationales to specific code modifications
- Mechanism: Fine-tuning on paired (rationale, code) examples teaches the student model to convert feedback into executable repairs, while in-context learning only provides rationales without this mapping
- Core assumption: A model's ability to generate correct code is bottlenecked not just by rationale quality but by its capacity to translate suggestions into language-specific modifications
- Evidence anchors:
  - [abstract] "contrary to preexisting beliefs, the correlation between rationale quality and code correctness is weaker than previously perceived"
  - [section] "DistiLRR mitigates this effect, increasing the rate of converting a good rationale into correct code by 31% relative to baselines"
  - [corpus] Weak corpus evidence; no directly comparable distillation approaches found
- Break condition: If the student model already has sufficient language knowledge or if rationale quality is extremely poor

### Mechanism 2
- Claim: DistiLRR transfers deeper language knowledge to low-resource languages through synthetic data creation
- Mechanism: The teacher model generates correct repairs on synthetic examples, and the student learns both the repair logic and language-specific patterns from these examples
- Core assumption: Low-resource languages lack sufficient training data, so synthetic examples created by a capable teacher can fill this gap
- Evidence anchors:
  - [abstract] "our student models are CodeLlama-7b-Instruct, CodeLlama-7b, and Mistral-7b"
  - [section] "The fine-tuning datasets are constructed from MBXP...which consists of multiple language specific benchmarks, each containing around 960 questions"
  - [corpus] Weak corpus evidence; only one similar paper found but focused on different approach
- Break condition: If the teacher model lacks sufficient knowledge of the target language or if synthetic data doesn't generalize

### Mechanism 3
- Claim: DistiLRR achieves better efficiency than i.i.d. sampling for code repair
- Mechanism: By learning to iteratively improve code through feedback, the distilled model can achieve higher pass rates with fewer inference calls compared to sampling multiple independent attempts
- Core assumption: Iterative refinement is more efficient than independent sampling when the model can learn to incorporate feedback
- Evidence anchors:
  - [abstract] "DistiLRR consistently outperforms baselines on low-resource languages, but has similar performance on high-resource languages"
  - [section] "DistiLRR outperforms the initial pass@10, with the exception of HRPLs on HumanEval"
  - [corpus] Weak corpus evidence; no directly comparable efficiency studies found
- Break condition: If the model fails to learn effective repair patterns or if feedback quality is too low

## Foundational Learning

- Concept: Distillation in machine learning
  - Why needed here: Understanding how knowledge transfers from teacher to student models is fundamental to grasping DistiLRR's approach
  - Quick check question: What's the difference between knowledge distillation and fine-tuning on human-annotated data?

- Concept: Low-resource vs high-resource languages
  - Why needed here: The paper's core contribution is addressing the low-resource language gap, so understanding what makes a language "low-resource" is essential
  - Quick check question: How does the availability of training data differ between Python and Perl according to the paper?

- Concept: Code repair and iterative refinement
  - Why needed here: The paper builds on existing code repair frameworks, so understanding the baseline approach is crucial
  - Quick check question: What are the main components of a typical code repair framework according to the paper?

## Architecture Onboarding

- Component map:
  Teacher model (GPT-3.5-Turbo) -> generates rationales and correct repairs
  Student model (CodeLlama or Mistral) -> learns from teacher's outputs
  Code executor -> runs test cases and provides error feedback
  Fine-tuning pipeline -> converts MBXP data into (instruction, question, incorrect answer, error, repair) tuples

- Critical path:
  1. Create synthetic training data using teacher model
  2. Fine-tune student model on this data
  3. Generate initial code samples
  4. Execute and collect errors
  5. Apply DistiLRR model to generate repairs
  6. Iterate until tests pass or max rounds reached

- Design tradeoffs:
  - Synthetic vs human-annotated data: Synthetic is cheaper but may have quality issues
  - Number of training examples: 400 examples may be limiting but shows promise
  - Teacher model choice: GPT-3.5-Turbo balances capability and cost

- Failure signatures:
  - Student model generates syntactically incorrect code frequently
  - Repairs don't improve code correctness despite good rationales
  - Performance similar to baseline on high-resource languages

- First 3 experiments:
  1. Compare DistiLRR vs baseline repair on a single low-resource language
  2. Measure syntax error reduction after DistiLRR fine-tuning
  3. Test transferability by applying DistiLRR trained on one low-resource language to another

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of DistiLRR scale with the size of the fine-tuning dataset for low-resource languages?
- Basis in paper: [inferred] The paper acknowledges that fine-tuning datasets contain only around 400 examples due to construction constraints, and mentions that training with larger datasets could lead to new observations.
- Why unresolved: The paper only evaluates DistiLRR with limited dataset sizes due to construction pipeline constraints and resource limitations.
- What evidence would resolve it: Systematic experiments varying fine-tuning dataset sizes from 100 to 10,000 examples for LRPLs, measuring pass@1 improvements across multiple rounds of repair.

### Open Question 2
- Question: Does the weak correlation between rationale quality and code correctness persist in more challenging multilingual code generation benchmarks?
- Basis in paper: [inferred] The paper uses HumanEval and MBXP benchmarks but notes these are not as challenging as other benchmarks like APPS or CodeContests, which are only available in high-resource languages.
- Why unresolved: The evaluation is limited to existing multilingual datasets that may not require the same level of complex reasoning needed for more difficult problems.
- What evidence would resolve it: Evaluation on more challenging multilingual benchmarks (once available) or on reasoning-heavy problems in low-resource languages, comparing rationale quality to code correctness.

### Open Question 3
- Question: How does DistiLRR performance change when using different teacher models beyond GPT-3.5-Turbo?
- Basis in paper: [explicit] The paper uses GPT-3.5-Turbo as the teacher model but does not explore how different teacher models might affect distillation quality or knowledge transfer effectiveness.
- Why unresolved: The study only uses one teacher model, leaving open whether the observed improvements are specific to GPT-3.5-Turbo or generalizable to other larger models.
- What evidence would resolve it: Experiments using different teacher models (e.g., GPT-4, Claude, open-source alternatives) to generate repair examples and measuring resulting student model performance on LRPLs.

## Limitations
- The study relies entirely on synthetic data for training, which may not fully capture the diversity of real-world coding errors in low-resource languages
- The choice of GPT-3.5-Turbo as teacher model may have limitations in understanding certain language-specific idioms or patterns
- The analysis of rationale quality vs. code correctness correlation is based on limited manual inspection of 50-100 samples per language

## Confidence
- **High Confidence**: The quantitative improvements in pass@1 rates for low-resource languages (77-145% gains) are well-supported by experimental results across two benchmarks (MBXP and HumanEval)
- **Medium Confidence**: The claim about DistiLRR's efficiency advantage over i.i.d. sampling is supported by results but would benefit from direct runtime comparisons
- **Low Confidence**: The generalizability of findings to other low-resource languages not included in the study remains untested

## Next Checks
1. Measure and compare the actual number of inference calls and wall-clock time required for DistiLRR versus baseline iterative repair across different languages to validate the claimed efficiency gains
2. Train separate models focusing only on rationale generation and only on code generation to quantify the relative contributions of each component to overall performance improvements
3. Evaluate whether DistiLRR models trained on one low-resource language (e.g., Perl) can effectively repair code in another low-resource language (e.g., Swift) without additional fine-tuning, testing the generalizability of synthetic data benefits