---
ver: rpa2
title: Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography
arxiv_id: '2407.21577'
source_url: https://arxiv.org/abs/2407.21577
tags:
- learning
- data
- datasets
- expert
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of building an echocardiography
  view classifier that maintains performance as new multi-site data becomes available
  over time, while avoiding catastrophic forgetting and accommodating label variations
  across sites. The proposed method learns an expert network for each dataset and
  combines their predictions using a score fusion model.
---

# Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography

## Quick Facts
- arXiv ID: 2407.21577
- Source URL: https://arxiv.org/abs/2407.21577
- Reference count: 27
- Primary result: Proposed method reduces training time while improving view classification performance across six multi-site echocardiography datasets through expert weighting.

## Executive Summary
This work addresses the challenge of maintaining echocardiography view classification performance as new multi-site data becomes available over time. The proposed approach learns an expert network for each dataset and combines predictions using a score fusion model with learned in-distribution weighting. This method effectively prevents catastrophic forgetting while accommodating label variations across sites, and provides transparency by making each expert's contribution visible during inference. The approach uses learned features rather than original images, facilitating easier sharing and addressing privacy concerns.

## Method Summary
The method employs a multi-expert framework where each dataset is associated with a dedicated expert network trained on that specific data. Rather than naive fine-tuning which suffers from catastrophic forgetting, the approach uses a score fusion mechanism that weights each expert's contribution based on learned in-distribution scores. This weighting minimizes the influence of "unqualified experts" whose predictions are out-of-distribution for a given input. The method uses ResNet18 architecture with 200 epochs for expert training and 50 epochs for fusion networks. Instead of original images, the method operates on learned features, addressing privacy concerns while enabling easier data sharing across institutions.

## Key Results
- Significant reductions in training time compared to naive fine-tuning approaches
- Improved view classification performance across six multi-site echocardiography datasets
- Outperforms both naive fine-tuning approaches and combined retraining oracle on average
- Demonstrates effective prevention of catastrophic forgetting across incremental learning stages

## Why This Works (Mechanism)
The method works by creating specialized expert models for each dataset, then intelligently combining their predictions based on how well-suited each expert is to the current input. The learned in-distribution scores act as a gating mechanism, allowing only the most relevant experts to contribute to the final prediction. This prevents poor performance from unqualified experts while maintaining the knowledge gained from previous datasets. The feature-based approach rather than image-based approach enables privacy-preserving knowledge transfer between sites while maintaining classification accuracy.

## Foundational Learning
- Class-incremental learning: Training models on new data without forgetting previous knowledge - needed because medical imaging data accumulates over time across multiple sites
- Catastrophic forgetting: Degradation of performance on previously learned tasks when learning new ones - critical to address for clinical deployment where historical accuracy must be maintained
- Multi-site learning: Handling data from different institutions with varying protocols and label conventions - essential for real-world medical AI that must work across healthcare systems
- Expert weighting: Dynamically adjusting the contribution of different models based on input characteristics - enables the system to leverage the most appropriate knowledge source for each case

## Architecture Onboarding

**Component Map:** Input features -> Expert networks (one per dataset) -> Score fusion network -> Weighted output -> Final classification

**Critical Path:** Feature extraction → Expert prediction → In-distribution scoring → Weighted fusion → Classification

**Design Tradeoffs:** 
- Using features instead of images enables privacy compliance but may lose spatial information
- Multiple experts increase model complexity but provide specialization for each dataset
- Learned weighting adds computational overhead but improves accuracy over static weighting

**Failure Signatures:**
- Attention scores collapsing to extreme values (0 or 1) indicating poor generalization
- Performance degradation on base dataset during incremental training suggesting catastrophic forgetting
- Inconsistent predictions across similar inputs suggesting unstable fusion weights

**First Experiments:**
1. Test feature extraction quality by comparing classification accuracy using raw images versus learned features on a single dataset
2. Evaluate expert specialization by measuring individual expert performance on their native versus external datasets
3. Validate weighting effectiveness by comparing fusion performance with learned weights versus static equal weighting

## Open Questions the Paper Calls Out
None

## Limitations
- Validation limited to six echocardiography datasets, raising questions about generalizability to other medical imaging domains
- Performance improvements, while statistically significant, may not scale proportionally to larger or more diverse datasets
- The specific weighting mechanisms may not transfer optimally to other class-incremental learning problems with different characteristics

## Confidence
**High confidence** in the feasibility of the multi-expert framework
**Medium confidence** in the superiority of the proposed fusion strategies over baselines
**Low confidence** in cross-domain generalizability without additional validation

## Next Checks
1. Test the method on external echocardiography datasets not used in training to assess true generalization capability
2. Evaluate performance across different class-incremental ordering scenarios to verify robustness to incremental sequence variations
3. Compare computational efficiency and memory requirements against alternative continual learning approaches to establish practical advantages beyond accuracy metrics