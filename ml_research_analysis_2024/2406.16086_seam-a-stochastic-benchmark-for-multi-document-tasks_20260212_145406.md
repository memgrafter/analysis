---
ver: rpa2
title: 'SEAM: A Stochastic Benchmark for Multi-Document Tasks'
arxiv_id: '2406.16086'
source_url: https://arxiv.org/abs/2406.16086
tags:
- documents
- mentions
- each
- your
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SEAM, the first benchmark designed to evaluate
  large language models on multi-document tasks. SEAM is a stochastic evaluation approach
  that samples across various design choices (e.g., prompt phrasing, document order,
  few-shot examples) to provide a more robust and meaningful assessment of model performance.
---

# SEAM: A Stochastic Benchmark for Multi-Document Tasks

## Quick Facts
- arXiv ID: 2406.16086
- Source URL: https://arxiv.org/abs/2406.16086
- Reference count: 40
- Key result: SEAM is the first stochastic benchmark evaluating LLMs on multi-document tasks, finding that even 70B parameter models struggle with these challenges.

## Executive Summary
SEAM introduces a novel stochastic evaluation approach for assessing large language models on multi-document tasks. By repeatedly sampling across arbitrary design choices such as prompt phrasing, document order, and few-shot examples, SEAM provides a distribution of performance scores rather than a single point estimate. This methodology captures both the mean capability and sensitivity of models to prompt variations. The benchmark covers three challenging task families—multi-document summarization, multi-hop question answering, and cross-document coreference resolution—using datasets from news, reviews, scientific, and wiki domains. Evaluation of seven LLMs reveals that larger models do not necessarily achieve better mean performance or more robust predictions, highlighting the unique challenges of multi-document consolidation beyond context length limitations.

## Method Summary
SEAM implements a stochastic evaluation framework that treats arbitrary prompt design choices as random variables to be resampled uniformly. For each evaluation run, the system randomly selects instruction paraphrases, document orderings, few-shot examples, their arrangement, and test instances. The benchmark integrates six existing datasets: MultiNews and OpenAsp for summarization, FuseReviews for review summarization, MuSiQue for multi-hop QA, and ECB+ and SciCo for coreference resolution. Performance is measured using task-specific metrics including ROUGE scores for summarization, F1 for QA, and CoNLL-F1 for coreference. Each model is evaluated across 10 resamples with 3 few-shot examples and up to 100 instances per dataset, using temperature 0.8 for generation. The stochastic approach produces aggregated statistics including mean performance, standard deviation, and pairwise model comparisons to assess both capability and robustness.

## Key Results
- Even state-of-the-art LLMs with 70B parameters struggle with multi-document tasks, showing limited performance improvement with scale.
- Larger model size does not guarantee superior mean performance or more robust predictions on multi-document tasks.
- Performance on multi-document tasks does not correlate with input length, indicating the challenge lies in information consolidation rather than context handling.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic evaluation exposes the true variability in LLM performance that static benchmarks mask.
- Mechanism: By resampling arbitrary design choices uniformly at random, the benchmark generates a distribution of performance scores reflecting both mean capability and sensitivity to prompt variations.
- Core assumption: Minor prompt variations are unrelated to intrinsic task difficulty and their effects average out while remaining measurable.
- Evidence anchors: [abstract] "SEAM addresses the sensitivity of LLMs to minor prompt variations through repeated evaluations"; [section] "we design SEAM as a sample generator rather than a fixed repository"
- Break condition: If prompt variations are systematically correlated with task difficulty, randomness assumption fails.

### Mechanism 2
- Claim: Stochastic resampling enables detection of model robustness differences beyond raw performance.
- Mechanism: Same model evaluated across many configurations produces score variance, which becomes a metric of robustness to input perturbations.
- Core assumption: Performance variance across resamples is a meaningful proxy for robustness, not just noise.
- Evidence anchors: [section] "a larger model size does not guarantee superior mean performance, nor more robust predictions"; [section] "we introduce a new methodological approach to address the sensitivity of LLMs"
- Break condition: If variance is dominated by random measurement noise, it ceases to be an interpretable robustness signal.

### Mechanism 3
- Claim: Aggregating across multiple multi-document tasks yields comprehensive view of LLM capabilities.
- Mechanism: Combines datasets from summarization, QA, and coreference resolution across diverse domains, surfacing domain-specific weaknesses single-task evaluation would miss.
- Core assumption: Three task families capture orthogonal dimensions of multi-document understanding, so performance differences are additive.
- Evidence anchors: [abstract] "SEAM consists of datasets addressing various multi-document processing tasks"; [section] "SEAM is a conglomeration of samples from existing datasets"
- Break condition: If tasks are not sufficiently independent, aggregation may overstate coverage.

## Foundational Learning

- Concept: Statistical sampling and distribution analysis
  - Why needed here: Understanding why resampling arbitrary choices yields meaningful variance metrics requires grasping how repeated sampling produces stable estimates of sensitivity.
  - Quick check question: If you randomly shuffle document order 100 times and measure performance each time, what statistical property of the resulting score distribution tells you about model sensitivity?

- Concept: Multi-document reasoning challenges (contradiction, omission, repetition)
  - Why needed here: Recognizing why multi-document tasks are inherently harder than single-document ones is key to interpreting why even large models struggle on SEAM.
  - Quick check question: In a multi-document summary, why might a model that simply concatenates sentences from each document fail to produce a coherent output?

- Concept: Evaluation metrics for structured vs unstructured outputs
  - Why needed here: Different tasks in SEAM require different metrics, so understanding metric suitability is essential for correct interpretation.
  - Quick check question: Why would a single-token formatting error in a coreference resolution output cause a large drop in F1 score, while a similar error in summarization might have minimal impact?

## Architecture Onboarding

- Component map: Dataset ingestion -> Prompt template engine -> Resampling scheduler -> LLM inference harness -> Post-processing module -> Scoring engine -> Reporting layer

- Critical path:
  1. Resampling scheduler generates complete prompt configuration
  2. Prompt template engine fills slots and produces final prompt
  3. LLM inference harness sends prompt to model and receives output
  4. Post-processing module normalizes output to expected format
  5. Scoring engine evaluates against gold and records metrics
  6. Repeat for all resamples; aggregate results

- Design tradeoffs:
  - Stochasticity vs reproducibility: Random seeds stored for exact replication but increase storage needs
  - Few-shot size: k=3 balances demonstration quality against prompt length constraints
  - Dataset subset size: max 100 instances limits runtime but may underrepresent rare failure modes
  - Metric choice: Task-specific metrics align with literature but make cross-task comparison indirect

- Failure signatures:
  - High variance with low mean: Model is inconsistent but occasionally correct; sensitive to prompt phrasing or ordering
  - Low variance with low mean: Model is consistently poor; lacks multi-document reasoning capability
  - High variance with high mean: Model performs well but unpredictably; may overfit to prompt patterns
  - Format parsing errors: If post-processing fails to extract expected structure, scores will be invalid

- First 3 experiments:
  1. Run SEAM with fixed seed on small subset (5 instances per dataset) to verify end-to-end pipeline correctness
  2. Execute one full resample with simple model (e.g., Gemma-2B) to confirm metric computation and AR/ARSD aggregation
  3. Compare two models (e.g., Mistral-7B vs Llama-3-8B) on single dataset to validate stochastic approach detects performance differences beyond mean score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limits of model performance improvement on multi-document tasks as model size increases?
- Basis in paper: Explicit - Paper shows even 70B parameter models struggle and larger size doesn't guarantee better performance
- Why unresolved: Paper doesn't provide theoretical upper bound or investigate diminishing returns for very large models
- What evidence would resolve it: Systematic evaluation across wide range of model sizes, including extreme scales, to identify performance plateaus

### Open Question 2
- Question: How do different stochastic sampling strategies for prompt variations affect reliability of LLM evaluations?
- Basis in paper: Explicit - Paper introduces stochastic approach but doesn't compare alternative sampling strategies
- Why unresolved: Uses uniform random sampling but doesn't investigate whether other distributions might yield more robust evaluations
- What evidence would resolve it: Comparative studies testing different sampling strategies on same benchmark tasks

### Open Question 3
- Question: What specific aspects of multi-document consolidation present greatest challenge to LLMs beyond context length limitations?
- Basis in paper: Explicit - Paper shows performance doesn't correlate with input length, suggesting challenge lies in consolidation
- Why unresolved: Identifies consolidation as key challenge but doesn't isolate which specific subtasks are most problematic
- What evidence would resolve it: Ablation studies systematically varying consolidation requirements or fine-grained error analysis

## Limitations

- Prompt sensitivity calibration may conflate sensitivity with difficulty bias if certain prompt templates inadvertently simplify or complicate tasks
- Dataset coverage may omit niche multi-document challenges (multilingual alignment, very long documents, highly technical domains), limiting generalizability
- Output format dependency highly penalizes formatting errors in structured tasks, potentially conflating syntactic compliance with reasoning ability

## Confidence

- High confidence: Core claim that stochastic resampling yields more robust performance estimate is well-supported by methodology and aligns with statistical principles
- Medium confidence: Interpretation of variance as robustness proxy is plausible but context-dependent, requiring assumption that fluctuations reflect model sensitivity
- Low confidence: Claims about cross-task comparisons of mean performance are harder to validate due to differing metrics and task-specific challenges

## Next Checks

1. Run SEAM with fixed prompt configuration across all resamples to quantify variance attributable to stochastic factors versus model behavior
2. Assess whether variance patterns persist across multiple valid metrics for tasks like summarization (ROUGE-1, ROUGE-2, ROUGE-L)
3. Have human raters assess semantic correctness of model outputs from high- and low-variance runs independent of format