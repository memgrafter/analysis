---
ver: rpa2
title: 'k-MLE, k-Bregman, k-VARs: Theory, Convergence, Computation'
arxiv_id: '2409.06938'
source_url: https://arxiv.org/abs/2409.06938
tags:
- clustering
- k-mle
- convergence
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops k-MLE, a new hard clustering method based on
  likelihood rather than distance. The authors prove convergence of the cyclic ascent
  algorithm and show that existing Bregman clustering methods are a special case.
---

# k-MLE, k-Bregman, k-VARs: Theory, Convergence, Computation

## Quick Facts
- arXiv ID: 2409.06938
- Source URL: https://arxiv.org/abs/2409.06938
- Reference count: 40
- Key outcome: New hard clustering method (k-MLE) based on likelihood maximization, with convergence proofs and application to autocorrelated time series clustering (k-VARs), showing superior performance on benchmark datasets.

## Executive Summary
This paper introduces k-MLE, a hard clustering method that maximizes classification likelihood rather than minimizing distance. The authors prove convergence of the cyclic ascent algorithm to a partial maximum in finite steps, and under additional conditions, to a local maximum. They show k-Bregman clustering is a special case of k-MLE for exponential families. The method is applied to time series clustering via k-VARs, which models autocorrelated data using vector autoregressive models and selects model order and cluster number via BIC. Experiments demonstrate k-VARs outperforms state-of-the-art algorithms on both simulated and real data.

## Method Summary
The k-MLE algorithm alternates between optimizing binary cluster labels and cluster-specific parameters to maximize classification likelihood. For time series, k-VARs models each series as a VAR process and applies k-MLE, using QR decomposition for efficient parameter updates. Model selection chooses the number of clusters K and VAR order p by minimizing BIC. The cyclic ascent procedure guarantees non-decreasing likelihood and finite convergence to a partial maximum, with local maximum convergence under MLE uniqueness conditions.

## Key Results
- k-MLE provides a unified framework for hard clustering based on likelihood maximization
- Cyclic ascent algorithm converges to a partial maximum in finite steps under compactness and boundedness conditions
- k-Bregman clustering is shown to be a special case of k-MLE for exponential family distributions
- k-VARs outperforms k-means and k-Shape on both simulated and real time series data
- BIC-based model selection effectively chooses K and p for k-VARs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: k-MLE extends clustering from similarity-based to likelihood-based, allowing hard clustering for a broader class of models.
- Mechanism: By maximizing the classification likelihood instead of a divergence, the method can handle arbitrary parametric densities, including non-exponential families like VAR models.
- Core assumption: The log-likelihood is continuous and bounded on the parameter space Ω.
- Evidence anchors:
  - [abstract]: "k-MLE is based on likelihood and thus has a far greater range of application."
  - [section II-B]: "The DL-CL depends on binary and analog (i.e. continuous valued) parameters. This leads to a hybrid maximum likelihood estimation (MLE) problem."
  - [corpus]: Weak evidence; corpus papers do not discuss likelihood-based clustering directly.
- Break condition: If the log-likelihood is not bounded or not continuous on Ω, the k-MLE problem may be ill-defined or fail to converge.

### Mechanism 2
- Claim: Cyclic ascent guarantees non-decreasing log-likelihood and convergence to a partial maximum in finite steps.
- Mechanism: Alternating between optimizing labels (τ) and parameters (Θ) ensures each step improves or maintains the objective, and because there are finitely many τ configurations, the process must terminate.
- Core assumption: The parameter space Ω is compact and the log-likelihood is bounded.
- Evidence anchors:
  - [section III-A]: "This ascent property, i.e. non-decrease of the log-likelihood, is necessary but far from sufficient for convergence of cyclic ascent."
  - [section III-A]: "Since B is a discrete, bounded set, then under Assumption 1, the k-MLE problem has at least one solution."
  - [corpus]: Weak; no corpus papers address cyclic ascent convergence for clustering.
- Break condition: If Ω is not compact or the log-likelihood is unbounded, the ascent may not converge to a finite limit.

### Mechanism 3
- Claim: Under MLE uniqueness conditions, the algorithm converges to a local maximum, not just a partial maximum.
- Mechanism: If each cluster-specific likelihood has a unique MLE, then the maximizing solution set M(τ) is a singleton, allowing the partial maximum to be promoted to a local maximum.
- Core assumption: Each cluster-specific log-likelihood satisfies conditions for unique MLE (negative definite Hessian at stationary points, gradient vanishes somewhere).
- Evidence anchors:
  - [section III-C]: "Suppose that each cluster-specific log-likelihood satisfies the uniqueness conditions of Theorem 9. Then, the k-MLE iterates converge to a local maximum point for the k-MLE problem."
  - [section III-C]: "In Proposition 10 rather than applying the conditions of Theorem 9 to establish MLE uniqueness, it is sometimes possible to establish MLE uniqueness directly."
  - [corpus]: No direct corpus evidence; assumption relies on standard MLE theory.
- Break condition: If the Hessian is not negative definite or uniqueness fails, the algorithm may stop at a saddle point or plateau.

## Foundational Learning

- Concept: Exponential family distributions and Bregman divergences
  - Why needed here: To understand when k-MLE reduces to k-Bregman and why VAR models are not covered by that reduction.
  - Quick check question: Can you write the Bregman divergence form for a Gaussian distribution and explain why it corresponds to squared Euclidean distance?

- Concept: Vector autoregressive (VAR) models and stability
  - Why needed here: k-VARs applies k-MLE to autocorrelated time series modeled as VARs; understanding VAR structure and stability is essential for implementation.
  - Quick check question: What condition must the roots of the VAR characteristic polynomial satisfy for the model to be stable?

- Concept: Bayesian Information Criterion (BIC) for model selection
  - Why needed here: BIC is used to jointly choose the number of clusters K and model order p in k-VARs.
  - Quick check question: How does BIC balance model fit and complexity, and what role does the sample size play in the penalty term?

## Architecture Onboarding

- Component map:
  - Data matrix X (d x N) or time series collection Y
  - Label matrix τ (N x K), binary membership
  - Parameter matrix Θ (K x q), cluster-specific parameters
  - Likelihood evaluation and maximization routines
  - Convergence checking and stopping logic
  - BIC computation for model selection

- Critical path:
  1. Initialize cluster parameters (random or oracle)
  2. Assign labels by maximizing per-sample log-likelihood
  3. Update parameters by solving per-cluster MLE
  4. Check stopping criterion (parameter change or log-likelihood change)
  5. If not converged, return to step 2

- Design tradeoffs:
  - Hard clustering (deterministic labels) vs. soft clustering (probabilistic memberships)
  - General likelihood-based vs. specific divergence-based methods
  - Computational cost of QR decomposition for fast VAR updates vs. direct matrix inversion
  - BIC-based joint model selection vs. fixed K and p

- Failure signatures:
  - No improvement in log-likelihood over iterations (possible local optimum or bad initialization)
  - Parameter updates blowing up (ill-conditioned matrices, non-unique MLE)
  - BIC surface flat near minimum (ambiguous model choice)

- First 3 experiments:
  1. Run k-VARs on a simple synthetic dataset with known clusters and VAR structure; verify label recovery and parameter estimates.
  2. Compare k-VARs to k-means and k-Shape on the same dataset; record ARI and NID metrics.
  3. Vary the number of clusters K and model order p; plot BIC surface and check if ground truth is recovered.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does k-MLE guarantee both criterion and parameter convergence for non-exponential family distributions?
- Basis in paper: [explicit] The paper states k-MLE requires MLE uniqueness conditions for convergence, but doesn't specify these for non-exponential families beyond VARs.
- Why unresolved: The paper proves convergence for exponential families using Theorem 12, but only establishes uniqueness for VARs through specific conditions (stable VAR, full rank noise covariance).
- What evidence would resolve it: General conditions on the likelihood function (e.g., strict concavity, unique maxima) that guarantee MLE uniqueness for arbitrary distributions in k-MLE.

### Open Question 2
- Question: How does the choice of initialization method affect the final clustering solution in k-VARs?
- Basis in paper: [explicit] The paper mentions k-VARs(rnd) and k-VARs(oracle) initializations but doesn't analyze the impact of different initialization strategies.
- Why unresolved: The paper only compares two initialization methods without exploring the space of possible initializations or their effect on convergence to local optima.
- What evidence would resolve it: Systematic comparison of different initialization strategies (e.g., k-means++, spectral methods) on k-VARs performance across various datasets.

### Open Question 3
- Question: Can BIC be efficiently extended to the general case of unequal model orders (p1,...,pK) in k-VARs?
- Basis in paper: [explicit] The paper mentions this is computationally challenging but doesn't provide a solution or complexity analysis.
- Why unresolved: The paper only addresses the equal pk case due to computational complexity, leaving the more general case unexplored.
- What evidence would resolve it: Development of an efficient algorithm or heuristic for joint model order and cluster number selection when pk's differ across clusters.

## Limitations
- Convergence proofs rely on compactness and boundedness assumptions that may not hold for all model families
- BIC-based model selection may struggle with large K or short time series relative to model order
- Empirical results based on limited comparisons and specific benchmark datasets
- k-VARs assumes VAR structure which may not capture all time series dependencies

## Confidence

- **High confidence**: Cyclic ascent algorithm guarantees non-decreasing log-likelihood and finite convergence to a partial maximum under stated assumptions. This follows from discrete optimization structure and is mathematically rigorous.
- **Medium confidence**: Extension to local maximum convergence under MLE uniqueness conditions. While theoretically sound, verification of negative definite Hessian and uniqueness for VAR models requires case-by-case analysis.
- **Medium confidence**: Empirical superiority of k-VARs on benchmark datasets. The results are promising but based on limited comparisons and specific datasets.

## Next Checks

1. **Convergence verification**: Implement k-MLE for Gaussian mixtures and verify monotonic log-likelihood increase and finite termination across multiple random initializations.
2. **VAR stability check**: For simulated VAR time series, confirm that estimated models satisfy stability conditions (roots of characteristic polynomial inside unit circle) and that clustering recovers true labels.
3. **Robustness to initialization**: Systematically compare k-VARs performance with different initialization strategies (random, k-means++, oracle) on synthetic data with varying noise levels and cluster separation.