---
ver: rpa2
title: 'ELSA: Exploiting Layer-wise N:M Sparsity for Vision Transformer Acceleration'
arxiv_id: '2409.09708'
source_url: https://arxiv.org/abs/2409.09708
tags:
- sparsity
- sparse
- supernet
- accuracy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ELSA, a methodology for exploring layer-wise
  N:M sparsity in vision transformers to accelerate inference on sparsity-supporting
  hardware. The method addresses the challenge of determining optimal layer-wise sparse
  configurations for ViTs, which typically have transformer blocks with the same number
  of parameters, making traditional heuristics ineffective.
---

# ELSA: Exploiting Layer-wise N:M Sparsity for Vision Transformer Acceleration

## Quick Facts
- arXiv ID: 2409.09708
- Source URL: https://arxiv.org/abs/2409.09708
- Reference count: 40
- Key outcome: Achieves up to 2.9× FLOPs reduction for DeiT-B and Swin-B models with only marginal accuracy loss

## Executive Summary
ELSA introduces a novel methodology for exploring layer-wise N:M sparsity in vision transformers to accelerate inference on sparsity-supporting hardware. The method addresses the challenge of determining optimal layer-wise sparse configurations for ViTs by constructing a supernet where all supported N:M sparsity levels share the same weights and use dynamic masking to extract sparse matrices. This approach reduces training overhead and improves sparse subnetwork convergence. Through a two-step sampling strategy and automatic choice filtering, the supernet is trained to focus on sparser subnets while avoiding choices that are too sparse to maintain accuracy. An evolutionary search is then used to find Pareto-optimal sparse configurations that balance accuracy and computational cost.

## Method Summary
ELSA constructs a weight-sharing supernet where all N:M sparsity levels (e.g., 1:4, 2:4, 4:4) share the same weights, using dynamic masking to extract sparse matrices. The method employs a two-step sampling strategy that discretizes computational costs into intervals and samples architectures within each interval, preventing undertraining of extremal architectures. Automatic choice filtering removes sparsity choices that are too sparse to maintain accuracy during training. After supernet training, an evolutionary search finds Pareto-optimal sparse configurations that balance accuracy and FLOPs reduction. The framework is evaluated on ImageNet using DeiT and Swin transformer models.

## Key Results
- Achieves up to 2.9× FLOPs reduction for DeiT-B and Swin-B models
- Maintains marginal accuracy loss compared to dense models
- Outperforms other sparsity configuration search methods on ImageNet benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic masking enables shared weight training across different N:M sparsity levels while maintaining optimal subnetwork convergence.
- Mechanism: The method leverages the subset-superset relationship among sparsity levels. When 1:4 sparsity is applied, the selected weights form a subset of those selected under 2:4 sparsity, which in turn forms a subset of dense weights. By sharing a single weight matrix across all sparsity levels and applying dynamic masking during training, the supernet can train all configurations simultaneously while reducing memory overhead.
- Core assumption: The subset-superset relationship holds consistently across all weight groups, allowing efficient weight sharing without compromising convergence quality.
- Evidence anchors:
  - [abstract]: "Considering not only all N:M sparsity levels supported by a given accelerator but also the expected throughput improvement"
  - [section 3.1]: "We notice the subset-superset relationship among different N:M sparsity levels and utilize it to construct the supernet"
  - [corpus]: Weak evidence - corpus lacks specific studies on N:M sparsity subset relationships
- Break condition: If the subset-superset relationship doesn't hold for certain weight patterns, or if weight sharing introduces conflicting gradient signals across sparsity levels.

### Mechanism 2
- Claim: Two-step sampling strategy ensures balanced training across sparse configurations with different computational costs.
- Mechanism: Instead of uniform sampling across all configurations, the method first discretizes computational costs into intervals, then samples architectures within each interval. This approach prevents undertraining of extremal architectures (high sparsity) that would otherwise be overshadowed by configurations clustering around the mean computational cost.
- Core assumption: Computational cost distributions follow approximately normal distributions under uniform sampling, making interval-based sampling necessary for balanced training.
- Evidence anchors:
  - [section 3.2]: "Leveraging insights from the central limit theorem [22], it becomes evident that under this vanilla sampling paradigm, the distribution of computational cost F(α) will approximately converge to a normal distribution"
  - [section 3.2]: "This tailored adjustment enables the search space to adapt to various practical constraints, such as hardware limitations"
  - [corpus]: Weak evidence - corpus lacks empirical validation of this sampling strategy's effectiveness
- Break condition: If computational cost distributions deviate significantly from normality, or if interval discretization creates artificial barriers between related configurations.

### Mechanism 3
- Claim: Automatic choice filtering removes sparsity choices that are too sparse to maintain accuracy in certain layers.
- Mechanism: After several training epochs, the method evaluates sampled sparse subnets on a proxy dataset and tracks accuracy scores for each sparsity choice per layer. Sparsity choices that consistently cause accuracy drops below a threshold are eliminated from the search space, preventing the training process from being negatively impacted by overly aggressive sparsity levels.
- Core assumption: Certain layers cannot tolerate extreme sparsity levels regardless of fine-tuning, making early elimination beneficial for overall training quality.
- Evidence anchors:
  - [section 3.2]: "When a sparsity level is chosen to sparsify a layer and induces a larger accuracy drop, its score will be low"
  - [section 3.2]: "If the score of a sparsity level is lower than the given thresholdAccth, its probability will be 0"
  - [corpus]: Weak evidence - corpus lacks ablation studies on filtering threshold sensitivity
- Break condition: If filtering thresholds are set too aggressively, potentially eliminating viable sparsity configurations, or if the proxy dataset doesn't accurately represent the validation dataset.

## Foundational Learning

- Concept: N:M sparsity and its relationship to hardware acceleration
  - Why needed here: Understanding how N:M sparsity translates to computational savings on accelerators is crucial for interpreting the FLOPs reduction claims and the motivation behind layer-wise sparsity exploration.
  - Quick check question: How does a 2:4 sparsity configuration theoretically achieve 2× speedup on hardware accelerators?

- Concept: Supernet training and weight sharing
  - Why needed here: The paper's core innovation relies on constructing a supernet where all sparsity configurations share the same weights, making understanding this concept essential for grasping how the method achieves training efficiency.
  - Quick check question: What is the key advantage of using shared weights across different sparsity levels compared to maintaining separate weight sets for each configuration?

- Concept: Evolutionary search algorithms
  - Why needed here: The paper employs evolutionary search to find Pareto-optimal sparse configurations, so understanding how mutation, crossover, and selection work is important for interpreting the search process and results.
  - Quick check question: In the context of sparse configuration search, what role does the fitness function play in the evolutionary algorithm?

## Architecture Onboarding

- Component map: Pretrained model -> Supernet construction with dynamic masking -> Two-step sampling training with automatic filtering -> Evolutionary search for optimal configuration -> Extraction of sparse subnet without fine-tuning -> Evaluation on target dataset
- Critical path: Pretrained model → Supernet construction with dynamic masking → Two-step sampling training with automatic filtering → Evolutionary search for optimal configuration → Extraction of sparse subnet without fine-tuning → Evaluation on target dataset
- Design tradeoffs: The method trades increased supernet complexity (handling multiple sparsity configurations simultaneously) for reduced overall training time compared to training each configuration separately. It also balances between aggressive sparsity for computational savings and maintaining accuracy through the search process.
- Failure signatures: Poor supernet training quality manifests as suboptimal sparse subnets with accuracy significantly below fine-tuned counterparts; ineffective search results in configurations that don't provide meaningful FLOPs reduction; filtering that's too aggressive eliminates potentially viable configurations.
- First 3 experiments:
  1. Verify supernet construction by checking that extracted sparse configurations match expected N:M patterns and that weight sharing doesn't cause convergence issues.
  2. Test the two-step sampling strategy by comparing training performance with uniform sampling across different computational cost ranges.
  3. Validate the automatic choice filtering by running experiments with and without filtering to quantify its impact on training quality and final configuration selection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ELSA framework perform when applied to other transformer-based architectures beyond ViTs and ConvNext, such as BERT or GPT models?
- Basis in paper: [inferred] The paper demonstrates ELSA's effectiveness on ViTs and ConvNext, suggesting potential applicability to other transformer-based models.
- Why unresolved: The paper does not explore the performance of ELSA on language models or other transformer variants.
- What evidence would resolve it: Experiments applying ELSA to BERT, GPT, or other transformer-based architectures, comparing accuracy and computational efficiency gains.

### Open Question 2
- Question: What is the impact of different saliency score functions (beyond weight magnitude) on the performance of the ELSA framework?
- Basis in paper: [explicit] The paper mentions that the saliency score function can be a variety of metrics, including weight magnitude and Taylor score, but only uses weight magnitude in their experiments.
- Why unresolved: The paper does not investigate the effects of alternative saliency score functions on the quality of the resulting sparse networks.
- What evidence would resolve it: Experiments comparing the performance of ELSA using different saliency score functions, such as Taylor score or other pruning criteria, on various model architectures.

### Open Question 3
- Question: How does the ELSA framework scale with increasingly large and complex vision transformer models, such as those with more layers or higher resolution input images?
- Basis in paper: [inferred] The paper demonstrates ELSA's effectiveness on models like DeiT and Swin, but does not explore its performance on larger or more complex architectures.
- Why unresolved: The paper does not investigate the scalability of ELSA to more demanding models or input resolutions.
- What evidence would resolve it: Experiments applying ELSA to larger ViT models (e.g., DeiT-Large, Swin-Large) or models with higher resolution input images, evaluating the impact on accuracy, computational efficiency, and training time.

### Open Question 4
- Question: How does the ELSA framework compare to other sparsity exploration methods, such as those based on reinforcement learning or Bayesian optimization, in terms of accuracy and computational efficiency?
- Basis in paper: [inferred] The paper compares ELSA to Erdös-Rényi and DominoSearch, but does not explore other advanced sparsity exploration techniques.
- Why unresolved: The paper does not investigate the performance of ELSA relative to other state-of-the-art sparsity exploration methods.
- What evidence would resolve it: Experiments comparing ELSA to other sparsity exploration methods, such as those based on reinforcement learning or Bayesian optimization, on various model architectures and datasets.

## Limitations

- The paper doesn't fully specify the dynamic masking implementation details, particularly the saliency score function and threshold calculation
- Evolutionary search parameters are not provided, making it difficult to reproduce the exact search process
- The automatic choice filtering mechanism's sensitivity to threshold settings is not explored

## Confidence

- **High Confidence**: The general methodology of using supernet training with weight sharing for sparsity exploration is well-established and the core concepts are sound
- **Medium Confidence**: The specific two-step sampling strategy and its claimed advantages over uniform sampling need empirical validation
- **Medium Confidence**: The claimed FLOPs reductions and accuracy preservation, while demonstrated on benchmarks, may vary depending on specific hardware implementation details

## Next Checks

1. Reproduce the supernet training with dynamic masking to verify that extracted sparse configurations match expected N:M patterns and that weight sharing doesn't cause convergence issues
2. Test the two-step sampling strategy by comparing training performance with uniform sampling across different computational cost ranges to validate the claimed advantages
3. Run ablation studies on the automatic choice filtering mechanism by training with and without filtering to quantify its impact on training quality and final configuration selection