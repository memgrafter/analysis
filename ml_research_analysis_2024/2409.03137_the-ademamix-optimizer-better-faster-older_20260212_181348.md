---
ver: rpa2
title: 'The AdEMAMix Optimizer: Better, Faster, Older'
arxiv_id: '2409.03137'
source_url: https://arxiv.org/abs/2409.03137
tags:
- ademamix
- adamw
- training
- loss
- iterations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdEMAMix improves upon AdamW by mixing two EMAs of gradients with
  different decay rates. While AdamW uses a single EMA with a moderate decay, AdEMAMix
  adds a second, slowly decaying EMA to capture information from much older gradients.
---

# The AdEMAMix Optimizer: Better, Faster, Older

## Quick Facts
- arXiv ID: 2409.03137
- Source URL: https://arxiv.org/abs/2409.03137
- Reference count: 40
- Primary result: AdEMAMix achieves comparable performance to AdamW with 95% less training data (197B → 101B tokens)

## Executive Summary
AdEMAMix improves upon AdamW by mixing two exponential moving averages (EMAs) of gradients with different decay rates. The fast EMA captures recent gradient information while the slow EMA accumulates gradients from tens of thousands of steps ago. This combination provides both rapid response to local changes and stable progress using long-term gradient information. Experiments show AdEMAMix converges faster and often to lower minima than AdamW, with particular benefits for large language models where it achieves comparable performance with significantly less training data.

## Method Summary
AdEMAMix modifies AdamW by maintaining two EMAs of gradients with different decay rates (β1 for fast EMA, β3 for slow EMA) plus the standard second moment estimate (β2). The optimizer computes parameter updates using a linear combination of the two EMAs, with a scheduler gradually increasing both the momentum of the slow EMA and its weight during training. This design allows the optimizer to maintain sensitivity to recent gradient changes while leveraging historical gradient information that remains relevant over long timescales.

## Key Results
- 110M parameter transformer trained on 101B tokens with AdEMAMix performs comparably to AdamW model trained on 197B tokens (95% efficiency gain)
- AdEMAMix converges faster than AdamW on language modeling and image classification tasks
- Slower forgetting during training: later batches have more lasting impact on the model
- Memory savings possible by setting β1=0 to eliminate the fast EMA buffer without performance loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single EMA cannot simultaneously give high weight to recent gradients and non-negligible weight to old gradients.
- Mechanism: The decay rate β in an EMA exponentially downweights past gradients. Increasing β to give more weight to older gradients also increases the decay of recent gradients, reducing sensitivity to immediate changes in the loss landscape.
- Core assumption: Gradients are local linear approximations whose relevance decreases over time, but not uniformly across all timescales.
- Evidence anchors:
  - [abstract] "a single EMA cannot simultaneously give a high weight to the immediate past, and a non-negligible weight to older gradients"
  - [section] "We observe that a single EMA cannot both give a significant weight to recent gradients, and give a non-negligible weight to older gradients (see Fig. 3a)"

### Mechanism 2
- Claim: Mixing two EMAs with different decay rates enables both rapid response to local changes and stable progress using old gradients.
- Mechanism: A fast EMA (small β) captures immediate gradient information for responsiveness, while a slow EMA (large β) accumulates long-term gradient information. Their linear combination preserves both properties.
- Core assumption: The optimization landscape has both local variations requiring quick adaptation and broader canyons where old gradient information remains useful.
- Evidence anchors:
  - [abstract] "a linear combination between a 'fast-changing' (e.g. β = 0.9 or β1 = 0) and a 'slow-changing' (e.g. β = 0.9999) EMA allows the iterate to beneficiate from (i) the great speedup provided by the larger (slow-changing) momentum, while (ii) still being reactive to small changes in the loss landscape (fast-changing)"

### Mechanism 3
- Claim: Old gradients can remain relevant for tens of thousands of steps, enabling better minima and slower forgetting.
- Mechanism: By incorporating gradients from distant past through the slow EMA, the optimizer can maintain directional information that helps escape shallow minima and preserve learned patterns.
- Core assumption: The training data distribution has persistent structure that gradients capture over long timescales, and the model parameters evolve slowly enough that old gradients remain informative.
- Evidence anchors:
  - [abstract] "gradients can stay relevant for tens of thousands of steps"
  - [section] "we show AdEMAMix forgets the training data slower when compared to Adam"

## Foundational Learning

- Concept: Exponential Moving Average (EMA) and decay rates
  - Why needed here: Understanding how β controls the trade-off between recent and old gradient information is central to why AdEMAMix works
  - Quick check question: If β = 0.9, what fraction of the weight is given to the most recent 6 gradients?

- Concept: Momentum in optimization and its generalization benefits
  - Why needed here: The work builds on established understanding that momentum helps escape shallow minima and provides better generalization
  - Quick check question: Why might momentum methods prefer lower norm solutions compared to pure gradient descent?

- Concept: Learning rate scheduling and warmup strategies
  - Why needed here: AdEMAMix requires careful scheduling of α and β3 to avoid early training instabilities when using large momentum values
  - Quick check question: What is the purpose of learning rate warmup in the context of curvature and stability?

## Architecture Onboarding

- Component map:
  - Fast EMA (m1): β1 typically 0.9, captures recent gradient information
  - Slow EMA (m2): β3 typically 0.9999, accumulates old gradient information
  - Second moment estimate (ν): β2 typically 0.999, same as AdamW
  - Weight decay (λ): Same as AdamW
  - Learning rate (η): Same as AdamW with cosine decay
  - Scheduler for α: Linear warmup to final value
  - Scheduler for β3: Exponential scheduler increasing thalf linearly

- Critical path:
  1. Initialize all EMA buffers to zero
  2. Apply learning rate warmup for first 3k steps
  3. Gradually increase β3 and α using their respective schedulers
  4. Compute gradient and update both EMAs
  5. Apply bias correction to m1 and ν
  6. Combine m1 + α·m2 for parameter update

- Design tradeoffs:
  - Memory vs performance: Setting β1=0 saves memory by eliminating m1 buffer
  - Training stability vs speed: Large β3 values require careful scheduling to avoid divergence
  - Computational overhead: Additional EMA adds negligible compute but requires extra memory
  - Generalization vs fitting: Very old gradients might cause overfitting in small data regimes

- Failure signatures:
  - Early divergence: Large updates in first few thousand steps, especially with large β3 and no scheduler
  - Loss spikes: Instability when β1 is reduced too much, particularly with large batch sizes
  - Overfitting: In small data regimes, old gradients might cause the model to overfit training patterns
  - No improvement: If the landscape doesn't benefit from long-term gradient accumulation

- First 3 experiments:
  1. Train a small transformer (110M params) on RedPajama with AdEMAMix vs AdamW, measure final perplexity
  2. Test AdEMAMix with β1=0 to verify memory savings work without performance degradation
  3. Train on ImageNet-1k with ViT to verify benefits extend beyond language modeling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical justification for why gradients remain relevant over tens of thousands of optimization steps?
- Basis in paper: [explicit] The paper observes that gradients can stay relevant for tens of thousands of steps and outperform AdamW, but does not provide a theoretical explanation for this phenomenon.
- Why unresolved: The paper focuses on empirical demonstration rather than theoretical analysis. The observation challenges the conventional wisdom that gradients quickly become outdated as the iterate moves along the loss landscape.
- What evidence would resolve it: A theoretical analysis showing conditions under which gradients maintain relevance over long optimization horizons, potentially involving properties of the loss landscape or the consistency of gradient estimates.

### Open Question 2
- Question: How does the balance between historical gradients and recent gradient sensitivity affect generalization performance?
- Basis in paper: [inferred] The paper mentions that while AdEMAMix improves optimization, it raises questions about whether leveraging very old gradients might harm generalization by reducing gradient variance.
- Why unresolved: The experiments focus on training loss and convergence speed but do not systematically study the impact on generalization or test performance across different tasks and datasets.
- What evidence would resolve it: Extensive experiments comparing generalization performance of AdEMAMix versus AdamW across diverse tasks, datasets, and model architectures, particularly examining whether the variance reduction from old gradients affects test accuracy.

### Open Question 3
- Question: Are there alternative functions beyond EMAs that could more effectively leverage past gradients while maintaining sensitivity to recent changes?
- Basis in paper: [explicit] The paper concludes by motivating further research into methods combining old and recent gradients beyond EMAs, suggesting this is an open direction.
- Why unresolved: The paper focuses specifically on EMA mixtures and does not explore other functional forms for aggregating past gradients.
- What evidence would resolve it: Comparative studies of AdEMAMix against optimizers using different aggregation functions (e.g., weighted averages with different decay patterns, attention mechanisms, or adaptive combinations) on the same benchmarks.

## Limitations
- The claim that gradients remain relevant for tens of thousands of steps assumes persistent data distribution structure that may not hold for all tasks
- The specific scheduling strategy for β3 and α hasn't been systematically explored across different scheduling approaches
- No systematic ablation studies across different data regimes to establish when the slow EMA provides benefits versus when it becomes detrimental

## Confidence
- **High confidence**: The mechanism that mixing two EMAs with different decay rates can provide both responsiveness and stability is well-established and mathematically sound.
- **Medium confidence**: The claim that AdEMAMix converges faster and to better minima than AdamW is supported by experiments, but the magnitude of improvement (95% increase in training efficiency) may be task-dependent.
- **Low confidence**: The claim that old gradients can remain relevant for tens of thousands of steps requires more systematic investigation across different model sizes, data regimes, and task types.

## Next Checks
1. **Data regime ablation**: Test AdEMAMix on datasets with varying degrees of distribution shift (e.g., CIFAR-10, CIFAR-100, and synthetic data with controlled concept drift) to determine when the slow EMA provides benefits versus when it becomes detrimental.

2. **Scheduling robustness**: Implement alternative scheduling strategies for β3 and α (e.g., cosine scheduling, step decay, or adaptive scheduling based on gradient statistics) to test whether the observed benefits depend on the specific scheduling approach used.

3. **Gradient relevance analysis**: Track the cosine similarity between current gradients and those from various historical windows (1k, 5k, 10k, 20k steps ago) across different training phases to empirically validate whether gradients remain relevant for tens of thousands of steps as claimed.