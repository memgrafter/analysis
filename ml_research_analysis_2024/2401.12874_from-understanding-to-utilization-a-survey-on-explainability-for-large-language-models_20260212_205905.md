---
ver: rpa2
title: 'From Understanding to Utilization: A Survey on Explainability for Large Language
  Models'
arxiv_id: '2401.12874'
source_url: https://arxiv.org/abs/2401.12874
tags:
- language
- methods
- llms
- explainability
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey on explainability methods
  for large language models (LLMs), focusing on pre-trained Transformer-based models
  like LLaMA. The authors categorize explainability approaches into local and global
  analyses, with local methods including feature attribution and transformer block
  dissection, while global methods encompass probing-based techniques and mechanistic
  interpretability.
---

# From Understanding to Utilization: A Survey on Explainability for Large Language Models

## Quick Facts
- arXiv ID: 2401.12874
- Source URL: https://arxiv.org/abs/2401.12874
- Authors: Haoyan Luo; Lucia Specia
- Reference count: 12
- Key outcome: Comprehensive survey of explainability methods for large language models, categorizing approaches into local/global analyses and exploring applications for model editing, capability enhancement, and controllable generation.

## Executive Summary
This paper presents a comprehensive survey on explainability methods for large language models (LLMs), focusing on pre-trained Transformer-based models like LLaMA. The authors categorize explainability approaches into local and global analyses, with local methods including feature attribution and transformer block dissection, while global methods encompass probing-based techniques and mechanistic interpretability. The survey also explores how these explainability insights are leveraged for model editing, capability enhancement, and controllable generation, particularly in addressing issues like hallucination and ethical alignment.

## Method Summary
The survey systematically reviews existing explainability techniques for large language models, organizing them into local analysis methods (feature attribution, transformer block dissection) and global analysis methods (probing-based approaches, mechanistic interpretability). The authors examine how these methods can be applied to understand model behavior and improve LLM performance through model editing, capability enhancement, and controllable generation. The work includes discussion of evaluation methods for explanation plausibility and truthfulness, highlighting the need for calibrated datasets and metrics.

## Key Results
- Vector-based attribution methods provide granular understanding of token contributions to hidden state representations across transformer layers
- Causal tracing identifies specific model components that causally influence factual predictions
- Probing-based methods uncover semantic knowledge and linguistic properties encoded in model representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vector-based attribution methods provide a more granular understanding of how input tokens contribute to hidden state representations across transformer layers.
- Mechanism: These methods decompose token representations at each layer into elemental vectors attributable to each input token, allowing researchers to track information flow through the network.
- Core assumption: The representation of a token at layer l can be accurately decomposed into contributions from all input tokens.
- Evidence anchors:
  - [section] "Consider decomposing the ith token representation in layer l ∈ {0, 1, 2, ..., L, L + 1}, i.e., xl i ∈ { xl 1, xl 2, ..., xl N }, into elemental vectors attributable to each of the N input tokens: xl i = NX k=1 xl i⇐k"
  - [section] "Empirical evaluations demonstrate the efficacy of vector-based analysis and exemplify the potential of such methods in dissecting each hidden state representation within transformers."
- Break condition: If the decomposition equation does not hold or the attribution vectors cannot be reliably computed.

### Mechanism 2
- Claim: Causal tracing identifies specific model components that causally influence factual predictions.
- Mechanism: This method compares clean runs, corrupted runs, and corrupted-with-restoration runs to quantify each activation's contribution to correct predictions.
- Core assumption: The difference in model performance between these three operational phases can isolate causal effects.
- Evidence anchors:
  - [section] "This newer method quantifies the impact of intermediate activations in neural networks on their output... specifically, (Meng et al., 2023a) assesses each activation's contribution to accurate factual predictions through three distinct operational phases"
  - [section] "Termed as causal tracing, this approach has identified crucial causal states predominantly in the middle layers"
- Break condition: If the restoration phase fails to recover the original prediction or if the method cannot distinguish correlation from causation.

### Mechanism 3
- Claim: Probing-based methods can uncover both semantic knowledge and linguistic properties encoded in model representations.
- Mechanism: A classifier (probe) is trained on network activations to predict specific features, revealing what information is captured in different layers.
- Core assumption: The presence of a linear decision boundary between classes in activation space indicates that the model has encoded that information.
- Evidence anchors:
  - [section] "Probing-based methods are employed to capture the internal representations within these networks. This approach involves training a classifier, known as a probe, on the network's activations to distinguish between various types of inputs or outputs."
  - [section] "A notable study in this area involved training linear classifiers to identify a select group of attention heads that exhibit high linear probing accuracy for truthfulness"
- Break condition: If the probe requires excessive complexity (non-linear) to achieve good performance, suggesting the information is not linearly accessible.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: Understanding how transformers process information is essential for interpreting explainability methods that analyze attention weights, FFN layers, and information flow.
  - Quick check question: How does the multi-head self-attention mechanism in transformers allow the model to focus on different positions of the input sequence?

- Concept: Feature attribution and gradient-based methods
  - Why needed here: These are fundamental techniques for understanding model predictions, and the survey compares them with newer vector-based approaches.
  - Quick check question: What is the key difference between perturbation-based methods like LIME and gradient-based methods like integrated gradients in terms of how they estimate feature importance?

- Concept: Linear probing and representation analysis
  - Why needed here: Probing methods are a major category of global analysis techniques, and understanding how to train and evaluate probes is crucial for this work.
  - Quick check question: Why might a researcher choose to use a non-linear probe instead of a linear probe when analyzing model representations?

## Architecture Onboarding

- Component map:
  - Survey structure: Local Analysis (Feature Attribution, Transformer Block Dissection) → Global Analysis (Probing Methods, Mechanistic Interpretability) → Applications (Model Editing, Capability Enhancement, Controllable Generation) → Evaluation
  - Key technical components: Transformer layers, attention mechanisms, FFN layers, embedding spaces, probing classifiers, causal analysis frameworks

- Critical path: For a new engineer, the most important path is understanding how the three main analysis categories (local, global, mechanistic) relate to each other and how they can be applied to specific LLM problems.

- Design tradeoffs:
  - Local vs. Global: Local methods provide instance-specific explanations but may not generalize; global methods offer broader insights but can miss instance-specific nuances.
  - Computational cost vs. interpretability: More granular methods (like causal tracing) are more computationally expensive but can provide deeper insights.

- Failure signatures:
  - Explanations that do not generalize across similar inputs
  - Methods that are computationally infeasible for large models
  - Probes that require non-linear classifiers to achieve good performance
  - Causal tracing that cannot distinguish correlation from causation

- First 3 experiments:
  1. Implement a basic vector-based attribution method on a small transformer model and verify the decomposition equation holds.
  2. Train a linear probe on activations from a pre-trained model to predict a simple linguistic property (e.g., part-of-speech tags).
  3. Conduct a simple causal tracing experiment by comparing model performance on clean vs. corrupted inputs with one activation restored.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can explainability methods be effectively adapted and scaled for larger language models with billions of parameters?
- Basis in paper: [explicit] The paper highlights the challenges of interpreting larger models due to their scale and complexity, noting that traditional methods like SHAP values become less practical.
- Why unresolved: The paper discusses the unique interpretability challenges posed by large models but does not provide specific solutions or methodologies for scaling explainability techniques to handle the increased complexity and parameter count.
- What evidence would resolve it: Research demonstrating effective adaptation of explainability methods for large-scale models, along with empirical results showing improved interpretability and understanding of these models.

### Open Question 2
- Question: What are the most effective ways to evaluate the truthfulness and informativeness of explanations generated by large language models?
- Basis in paper: [explicit] The paper discusses the need for calibrated datasets and metrics to evaluate the application of explainability to downstream tasks, particularly in assessing truthfulness.
- Why unresolved: While the paper mentions the importance of evaluating explanation plausibility and truthfulness, it does not provide a comprehensive framework or specific metrics for this purpose.
- What evidence would resolve it: Development and validation of standardized evaluation datasets and metrics specifically designed to assess the truthfulness and informativeness of LLM-generated explanations.

### Open Question 3
- Question: How can explainability insights be leveraged to improve the ethical alignment and reduce biases in large language models?
- Basis in paper: [explicit] The paper discusses the potential of using explainability to address issues such as model hallucinations and inherent biases, mentioning research on interpreting and mitigating social biases in LLMs.
- Why unresolved: Although the paper acknowledges the importance of ethical alignment and bias mitigation, it does not provide a clear methodology or best practices for leveraging explainability insights to achieve these goals.
- What evidence would resolve it: Studies demonstrating the successful application of explainability techniques to identify and mitigate biases in LLMs, along with empirical evidence of improved ethical alignment and reduced bias in model outputs.

## Limitations
- Feature attribution methods face challenges with faithfulness and the complexity of hidden state dynamics
- Probing methods struggle with capturing nonlinear internal representations
- Causal tracing requires extensive computational resources and may not scale well to very large models

## Confidence
- High Confidence: The categorization of explainability methods into local and global analyses is well-established and clearly supported by the literature survey.
- Medium Confidence: The effectiveness claims for vector-based attribution methods and causal tracing are supported by recent studies but require further validation across diverse model architectures.
- Medium Confidence: The applications of explainability for model editing and capability enhancement are promising but their practical impact varies significantly across different use cases.

## Next Checks
1. Conduct ablation studies on vector-based attribution methods to quantify the decomposition error and validate the attribution vectors' reliability across different transformer architectures.
2. Implement and evaluate causal tracing on a small-scale model to verify that the restoration phase can consistently recover original predictions and distinguish causal effects from correlations.
3. Test the scalability of mechanistic interpretability methods by applying circuit identification techniques to increasingly larger transformer models and measuring computational costs.