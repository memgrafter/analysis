---
ver: rpa2
title: Compact Model Parameter Extraction via Derivative-Free Optimization
arxiv_id: '2406.16355'
source_url: https://arxiv.org/abs/2406.16355
tags:
- parameters
- parameter
- optimization
- device
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extracting compact model
  parameters for semiconductor devices using derivative-free optimization. The authors
  propose a method to simultaneously extract tens of parameters by employing derivative-free
  optimization to identify a good parameter set that best fits the compact model without
  performing an exhaustive number of simulations.
---

# Compact Model Parameter Extraction via Derivative-Free Optimization

## Quick Facts
- **arXiv ID**: 2406.16355
- **Source URL**: https://arxiv.org/abs/2406.16355
- **Reference count**: 34
- **Key outcome**: Successfully extracts tens of compact model parameters using derivative-free optimization with under 6,000 trials and maintains accuracy even with 25% corrupted data

## Executive Summary
This paper presents a derivative-free optimization approach for extracting compact model parameters in semiconductor devices. The method addresses the challenge of simultaneously extracting tens of parameters by employing derivative-free optimization to identify optimal parameter sets without exhaustive simulations. The authors demonstrate their approach on both a diamond Schottky diode using the SPICE diode model and a GaN-on-SiC HEMT using the ASM-HEMT model, achieving excellent fits while being robust to measurement noise and data corruption.

## Method Summary
The method uses derivative-free optimization (specifically Optuna with TPE sampler) to extract compact model parameters by minimizing a custom loss function. The approach incorporates log transformation to focus on relative errors, threshold-based accuracy prioritization for key operational regions, and clipped penalty functions to reduce sensitivity to outliers. The optimization process includes train-test split validation to prevent overfitting, with initial optimization on 80% of data followed by re-training on the full dataset using the best parameters found.

## Key Results
- Successfully identified optimal parameters for a GaN-on-SiC HEMT with 35 parameters in under 6,000 optimization trials
- Achieved excellent model fit even when 25% of the data was purposely corrupted
- Demonstrated consistent performance across different orders of magnitude through relative error-focused loss function

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Derivative-free optimization can find near-optimal parameters without gradient calculations
- **Mechanism**: Uses stochastic sampling (TPE) to explore parameter space efficiently, focusing computational effort on promising regions
- **Core assumption**: Good parameter regions can be identified through sampling without gradient information
- **Evidence anchors**: Abstract statement about avoiding exhaustive simulations; section II description of DFO methods
- **Break condition**: When parameter space is extremely high-dimensional or loss landscape is too complex for sampling

### Mechanism 2
- **Claim**: Log-transformed loss function ensures consistent performance across orders of magnitude
- **Mechanism**: Log transformation weights errors proportionally rather than absolutely, treating small errors in low-current regions equally to those in high-current regions
- **Core assumption**: Device performance should be evaluated proportionally across operating regions
- **Evidence anchors**: Section III.A description of log transformation; abstract emphasis on relative errors
- **Break condition**: When device behavior is inherently linear or measurement noise scales proportionally with signal

### Mechanism 3
- **Claim**: Train-test split and clipped penalty prevent overfitting while handling outliers
- **Mechanism**: Train-test split validates generalization; clipped penalty caps large error contributions, treating them as outliers
- **Core assumption**: Data contains some corruption but majority is reliable
- **Evidence anchors**: Abstract mentions both techniques; section IV shows performance with 25% corruption
- **Break condition**: When corruption is systematic rather than random or model is too simple to overfit

## Foundational Learning

- **Semiconductor device physics and compact modeling**: Understanding how parameters affect device behavior is essential for interpreting optimization results and setting bounds
  - Quick check: What physical phenomenon does saturation velocity parameter in a GaN HEMT model represent?

- **Statistical model validation and overfitting prevention**: Train-test split requires understanding cross-validation principles to assess generalization
  - Quick check: Why might a model that perfectly fits training data still perform poorly on test data?

- **Optimization landscape characteristics and convergence**: Understanding DFO finds near-optimal solutions helps set appropriate expectations
  - Quick check: What factors influence whether DFO will find good solutions versus getting stuck in local optima?

## Architecture Onboarding

- **Component map**: Parameter bounds specification -> Loss function definition -> DFO optimization loop (sample -> simulate -> evaluate -> update) -> Train-test validation -> Parameter refinement
- **Critical path**: Parameter bounds → Loss function definition → DFO optimization loop → Train-test validation → Parameter refinement
- **Design tradeoffs**: Wider parameter bounds increase global optimum chances but require more simulations; stricter bounds reduce search space but may miss optimal solutions
- **Failure signatures**: Slow convergence indicates poor parameter bounds or loss function; excellent training but poor test performance indicates overfitting; poor overall performance suggests incorrect parameter bounds or loss function
- **First 3 experiments**:
  1. Single-parameter optimization on simple diode model to verify framework
  2. Two-parameter optimization on same diode model to test parameter interactions
  3. Multi-parameter optimization on GaN HEMT with synthetic clean data to establish baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the maximum number of parameters that can be effectively extracted using DFO before curse of dimensionality becomes prohibitive?
- **Basis in paper**: [explicit] Authors note DFO efficiency decreases with parameter count but acknowledge this is an emerging research area
- **Why unresolved**: Paper lacks quantitative data on parameter count thresholds or performance degradation curves
- **What evidence would resolve it**: Systematic benchmarking across models with 10, 20, 50, 100 parameters with convergence speed and accuracy metrics

### Open Question 2
- **Question**: How does DFO compare to gradient-based methods when gradients are obtainable?
- **Basis in paper**: [inferred] Authors claim DFO is more efficient but assume gradients are hard to obtain
- **Why unresolved**: No direct performance comparisons between DFO and gradient-based methods in applicable scenarios
- **What evidence would resolve it**: Head-to-head comparisons measuring computation time, simulations, and final accuracy

### Open Question 3
- **Question**: How would DFO perform for TCAD model calibration?
- **Basis in paper**: [explicit] Authors suggest DFO would be advantageous for TCAD but don't demonstrate
- **Why unresolved**: Paper focuses on compact models rather than TCAD calibration
- **What evidence would resolve it**: Case studies showing DFO-based TCAD calibration with simulation reduction metrics compared to manual tuning

## Limitations
- Validation limited to two device types (Schottky diode and HEMT), restricting generalizability
- Computational efficiency claims lack detailed runtime information and scaling analysis
- No systematic evaluation of how performance degrades with increasing parameter counts

## Confidence
- **High confidence**: Basic mechanism of using DFO for parameter extraction works and shows measurable improvement
- **Medium confidence**: Loss function design choices provide stated benefits but need more ablation studies
- **Medium confidence**: Approach generalizes well beyond tested device types, though largely unproven

## Next Checks
1. Apply methodology to at least three additional device types (MOSFET, JFET, non-silicon device) to assess generalizability
2. Systematically evaluate how trial requirements scale with parameter count using models with 5, 10, 20, and 40+ parameters
3. Conduct controlled ablation study removing each loss function component (log transformation, threshold, clipping) to quantify individual contributions