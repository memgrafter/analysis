---
ver: rpa2
title: 'Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models'
arxiv_id: '2405.15143'
source_url: https://arxiv.org/abs/2405.15143
tags:
- state
- states
- archive
- foundation
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Intelligent Go-Explore (IGE), a novel approach
  that integrates giant pretrained foundation models (FMs) into the Go-Explore framework
  to enhance exploration capabilities in complex environments. By leveraging FMs'
  internalized notions of interestingness, IGE replaces manual heuristics for state
  selection, action choice, and archive filtering, enabling more efficient and flexible
  exploration.
---

# Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models

## Quick Facts
- arXiv ID: 2405.15143
- Source URL: https://arxiv.org/abs/2405.15143
- Authors: Cong Lu; Shengran Hu; Jeff Clune
- Reference count: 40
- Primary result: IGE achieves 100% success on Game of 24 and solves long-horizon TextWorld tasks where prior FM agents fail

## Executive Summary
Intelligent Go-Explore (IGE) introduces a novel approach that integrates giant pretrained foundation models into the Go-Explore framework to enhance exploration capabilities in complex environments. By leveraging foundation models' internalized notions of interestingness, IGE replaces manual heuristics for state selection, action choice, and archive filtering, enabling more efficient and flexible exploration. The method is evaluated across diverse tasks including mathematical reasoning, partially observable gridworlds, and text-based adventure games, consistently outperforming classic reinforcement learning, graph search, and prior FM-based agents.

## Method Summary
IGE modifies the Go-Explore algorithm by replacing its handcrafted heuristics with foundation model intelligence across all exploration stages. The foundation model evaluates state interestingness, selects promising states to return to, chooses actions from those states, and filters the archive of discovered states. This approach enables IGE to maintain systematic exploration while leveraging FM intelligence to identify promising areas for further investigation. The method is evaluated on Game of 24 (mathematical reasoning), BabyAI (partially observable gridworlds), and TextWorld (text-based adventure games), comparing against baselines including random actions, naive LLM agents, ReAct, and Reflexion.

## Key Results
- Achieves 100% success rate on Game of 24, significantly faster than baselines
- Solves long-horizon TextWorld tasks (25-80 steps) where prior FM agents like Reflexion completely fail
- Only method that can find solutions in complex Coin Collector mazes in BabyAI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IGE replaces manually designed heuristics with foundation model intelligence across all stages of Go-Explore.
- Mechanism: The foundation model evaluates state interestingness, selects promising states to return to, chooses actions from those states, and filters the archive, removing the need for hand-crafted rules.
- Core assumption: Foundation models have internalized human notions of interestingness that generalize across domains.
- Evidence anchors:
  - [abstract] "replacing these handcrafted heuristics with the intelligence and internalized human notions of interestingness captured by giant pretrained foundation models (FMs)"
  - [section 3] "IGE stands on the shoulders of giant foundation models and uses their intelligence to (1) act as a judge to identify the most promising states to return to and explore from, (2) select the best actions to take from a selected state, and (3) recognize and capitalize on serendipitous discoveries"
- Break condition: Foundation models fail to generalize interestingness judgments to novel domains, or their judgments are misaligned with useful exploration.

### Mechanism 2
- Claim: IGE's use of archived states enables efficient exploration in partially observable and complex environments.
- Mechanism: By maintaining and intelligently selecting from an archive of discovered states, IGE can return to promising locations and explore from there, avoiding the need to rediscover paths and overcoming partial observability.
- Core assumption: The foundation model can accurately judge which archived states are most promising for further exploration.
- Evidence anchors:
  - [abstract] "IGE replaces these handcrafted heuristics with the intelligence and internalized human notions of interestingness captured by giant pretrained foundation models (FMs). This provides IGE with a human-like ability to instinctively identify how interesting or promising any new state is"
  - [section 4.2] "IGE can find solutions to these problems with only a tiny budget of 250 environment steps per task... and visualize the final performance in Figure 3"
- Break condition: Archive becomes too large to manage effectively, or foundation model cannot distinguish promising states from unpromising ones.

### Mechanism 3
- Claim: IGE outperforms prior FM-based agents by combining foundation model intelligence with Go-Explore's systematic exploration framework.
- Mechanism: While prior FM agents like Reflexion rely on episode-level learning and can fail in long-horizon tasks, IGE uses foundation models at a finer granularity (state and action level) within the Go-Explore framework, enabling more efficient exploration.
- Core assumption: The Go-Explore framework's systematic approach to exploration, when combined with foundation model intelligence, is superior to end-to-end FM approaches.
- Evidence anchors:
  - [abstract] "IGE strongly exceeds classic reinforcement learning and graph search baselines, and also succeeds where prior state-of-the-art FM agent frameworks like Reflexion completely fail"
  - [section 4.3] "In Coin Collector, IGE is the only method that can find the solution in the maze, with all other methods completely failing"
- Break condition: Foundation model intelligence at the state/action level provides no advantage over episode-level learning in certain domains.

## Foundational Learning

- Concept: Partially observable environments
  - Why needed here: IGE is evaluated on environments like BabyAI and TextWorld where the agent has incomplete information about the environment state.
  - Quick check question: What is the difference between fully observable and partially observable environments in reinforcement learning?

- Concept: Go-Explore algorithm
  - Why needed here: IGE is a modification of the Go-Explore algorithm that replaces heuristics with foundation model intelligence.
  - Quick check question: What are the three main stages of the Go-Explore algorithm?

- Concept: Foundation models and prompting strategies
  - Why needed here: IGE relies on foundation models (like GPT-4) and uses various prompting strategies (few-shot, chain-of-thought) to guide exploration.
  - Quick check question: What is the difference between zero-shot, few-shot, and chain-of-thought prompting?

## Architecture Onboarding

- Component map:
  Foundation model (FM) -> State selection -> Action selection -> Environment step -> Archive filtering -> Archive update

- Critical path: FM state selection → FM action selection → Environment step → FM archive filtering → Archive update

- Design tradeoffs:
  - FM choice vs performance: Different FMs (GPT-4, Claude, Llama) yield varying performance
  - Archive size vs computational cost: Larger archives provide more exploration options but increase FM query costs
  - Prompt engineering vs generalization: More specific prompts may improve performance but reduce domain generality

- Failure signatures:
  - Archive grows too large: FM filtering becomes ineffective, exploration slows
  - FM consistently selects poor states: Exploration gets stuck in unpromising areas
  - Action history not maintained: FM repeats ineffective actions from same states

- First 3 experiments:
  1. Test IGE on Game of 24 with default parameters to verify basic functionality
  2. Compare IGE with and without archive filtering on BabyAI to measure filtering impact
  3. Evaluate IGE with different FMs (GPT-4, Claude, Llama) on Game of 24 to assess FM dependence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IGE's performance scale with increasingly large state spaces and longer horizon tasks?
- Basis in paper: [explicit] The paper discusses IGE's performance across various environments with different horizon lengths (3 for Game of 24, 64-128 for BabyAI, 25-80 for TextWorld) but doesn't explore scaling to much larger state spaces.
- Why unresolved: The current evaluation focuses on relatively contained environments. The paper mentions potential real-world applications like synthetic biology or material science, but doesn't test IGE's capabilities in these much larger domains.
- What evidence would resolve it: Experiments testing IGE on domains with exponentially larger state spaces, such as procedurally generated environments with millions of possible states or real-world scientific discovery tasks.

### Open Question 2
- Question: How robust is IGE to biases in foundation models' notions of "interestingness"?
- Basis in paper: [explicit] The authors acknowledge that foundation models have biases from training data and discuss potential biases in the "Robustness and Potential Biases" section, but don't provide systematic evaluation of how these biases affect exploration.
- Why unresolved: While the paper shows IGE works well with different foundation models (GPT-4, Claude, Llama), it doesn't investigate whether certain types of biases systematically affect exploration patterns or prevent discovery of certain types of states.
- What evidence would resolve it: Systematic studies varying the types of environments and tasks to identify systematic biases in what states FMs consider interesting, plus experiments testing whether fine-tuning FMs to adjust their interestingness judgments improves exploration.

### Open Question 3
- Question: Can IGE be effectively combined with traditional reinforcement learning methods?
- Basis in paper: [inferred] The paper mentions that IGE could be used to generate trajectories for downstream reinforcement learning or imitation learning, but doesn't explore this integration. The discussion mentions combining IGE with exploration strategies based on intrinsic motivation or curiosity-driven policies.
- Why unresolved: The paper treats IGE as a standalone exploration method but doesn't investigate how its discovered trajectories could be used to bootstrap or improve traditional RL algorithms, or how combining IGE's FM-guided exploration with model-based RL might perform.
- What evidence would resolve it: Experiments showing how IGE-generated trajectories can be used to pretrain RL agents, or how combining IGE with model-based RL methods performs compared to either approach alone.

## Limitations

- The paper's claims about foundation models having "internalized human notions of interestingness" remains largely theoretical without direct empirical validation
- Computational cost of querying foundation models at each exploration step is not fully addressed, potentially limiting real-world scalability
- Robustness claims regarding different FMs and prompts are based on limited testing (3 FMs on 2 tasks)

## Confidence

- High confidence: IGE's performance improvements over baselines on tested tasks (Game of 24, BabyAI, TextWorld)
- Medium confidence: Claims about FM robustness to choice and prompt variations (limited empirical support)
- Low confidence: Theoretical claims about FMs having "internalized human notions of interestingness" (largely assumed)

## Next Checks

1. Test IGE with 5+ additional foundation models across all three benchmark tasks to verify robustness claims
2. Measure and report the computational cost (API calls, latency, tokens) of IGE versus baseline methods
3. Conduct ablation studies removing each FM component (state selection, action selection, archive filtering) to quantify individual contributions