---
ver: rpa2
title: Evaluating Self-Generated Documents for Enhancing Retrieval-Augmented Generation
  with Large Language Models
arxiv_id: '2410.13192'
source_url: https://arxiv.org/abs/2410.13192
tags:
- document
- authoritative
- self-docs
- should
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of self-generated documents
  (Self-Docs) in retrieval-augmented generation (RAG) systems. The authors propose
  a taxonomy based on Systemic Functional Linguistics (SFL) to classify Self-Docs
  by interpersonal tone, ideational content, and textual structure.
---

# Evaluating Self-Generated Documents for Enhancing Retrieval-Augmented Generation with Large Language Models

## Quick Facts
- **arXiv ID**: 2410.13192
- **Source URL**: https://arxiv.org/abs/2410.13192
- **Reference count**: 40
- **Primary result**: Task-specific self-generated documents significantly enhance RAG performance, with conversational styles excelling in long-form QA and authoritative fine-grained documents improving fact verification.

## Executive Summary
This paper investigates how self-generated documents (Self-Docs) can enhance retrieval-augmented generation (RAG) systems with large language models. The authors introduce a taxonomy based on Systemic Functional Linguistics (SFL) that classifies Self-Docs by interpersonal tone, ideational content, and textual structure. Through extensive experiments across four knowledge-intensive tasks—long-form question answering, entity linking, fact verification, and multi-document summarization—they demonstrate that while scaling model capacity improves performance, gains diminish for complex reasoning tasks. The study provides actionable guidelines showing that tailoring Self-Docs to specific task demands yields significant performance improvements, and integrating Self-Docs with external sources like Wikipedia further enhances results when stylistic alignment is applied.

## Method Summary
The authors conduct comprehensive experiments using four knowledge-intensive tasks: long-form question answering, entity linking, fact verification, and multi-document summarization. They employ the LLaMA model family with varying parameter counts (7B, 13B, 33B, 65B) to evaluate scaling effects. A taxonomy based on Systemic Functional Linguistics categorizes Self-Docs into interpersonal (tone), ideational (content), and textual (structure) dimensions. The experiments systematically compare different Self-Doc styles against each other and against external knowledge sources like Wikipedia, measuring performance improvements through task-specific metrics.

## Key Results
- Model capacity scaling improves RAG performance, but diminishing returns occur for complex reasoning tasks beyond certain model sizes
- Task-specific Self-Doc tailoring yields significant gains: conversational, unstructured documents excel at long-form QA, while authoritative, fine-grained documents improve fact verification accuracy
- Integrating Self-Docs with external sources like Wikipedia provides additional performance benefits, particularly when stylistic alignment between sources is maintained

## Why This Works (Mechanism)
Self-generated documents provide domain-specific contextual knowledge that bridges the gap between general model pretraining and task-specific requirements. By generating documents that match the linguistic and structural demands of particular tasks, the system can retrieve more relevant information during inference. The SFL-based taxonomy ensures that Self-Docs are optimized not just for content accuracy but also for their functional role in the RAG pipeline—whether providing conversational context for open-ended questions or authoritative evidence for fact verification. This alignment between document characteristics and task requirements creates a more effective retrieval mechanism than generic external knowledge bases.

## Foundational Learning
**Systemic Functional Linguistics (SFL)**: A linguistic framework analyzing language through interpersonal, ideational, and textual metafunctions. Why needed: Provides theoretical foundation for categorizing Self-Docs beyond simple content matching. Quick check: Can the three SFL dimensions be independently manipulated to measure their isolated effects on RAG performance?

**Knowledge-intensive tasks**: Tasks requiring extensive external knowledge beyond model pretraining, including long-form QA, entity linking, fact verification, and summarization. Why needed: Defines the scope of problems where RAG systems provide value over standard generation. Quick check: Do the performance patterns hold across different knowledge domains (e.g., biomedical vs. general knowledge)?

**Model scaling characteristics**: The relationship between parameter count and task performance, particularly the observation of diminishing returns for complex reasoning. Why needed: Helps determine optimal model size for cost-effective deployment. Quick check: Is there a specific model size threshold where additional parameters cease to provide meaningful RAG improvements?

## Architecture Onboarding

**Component map**: User Query -> Document Retriever -> Self-Doc Generator -> Document Store -> Reranker -> LLM Generator

**Critical path**: The retrieval pipeline (Retriever → Store → Reranker) is most critical, as it determines which documents are available for the LLM to condition on. Self-Doc generation quality directly impacts retrieval effectiveness.

**Design tradeoffs**: Balancing Self-Doc generation computational cost against retrieval quality gains. Conversational, unstructured Self-Docs are cheaper to generate but may provide less precise information than authoritative, fine-grained documents. The choice depends on task requirements and inference-time constraints.

**Failure signatures**: Poor performance occurs when Self-Doc style mismatches task requirements (e.g., using authoritative documents for conversational QA) or when generated documents contain hallucinated information that propagates through the retrieval pipeline. Over-reliance on Self-Docs without external validation sources leads to accuracy degradation in fact verification tasks.

**First experiments**:
1. Ablation study removing each SFL dimension (interpersonal, ideational, textual) to quantify individual contributions
2. Cross-domain validation testing the taxonomy on at least two additional knowledge domains
3. Integration testing combining Self-Docs with Wikipedia at different mixing ratios to find optimal configuration

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- The evaluation is limited to four specific knowledge-intensive tasks, raising questions about generalizability to other RAG applications or specialized domains
- The SFL-based taxonomy may not map perfectly to all document types or cultural contexts, particularly for the subjective interpersonal tone dimension
- Results show strong model capacity dependence with diminishing returns, but the analysis doesn't explore whether alternative architectures could mitigate these scaling limitations

## Confidence
- **High Confidence**: Model capacity scaling correlates with RAG performance gains across multiple tasks and model sizes
- **Medium Confidence**: Task-specific Self-Doc tailoring effectiveness (conversational for QA vs. authoritative for fact verification)
- **Medium Confidence**: Performance benefits from integrating Self-Docs with external sources like Wikipedia

## Next Checks
1. **Cross-domain Generalization Study**: Evaluate the Self-Doc taxonomy and task-specific recommendations across at least 10 additional knowledge domains (e.g., biomedical, legal, technical documentation) to assess transferability of the proposed guidelines

2. **Ablation of Linguistic Components**: Systematically remove or modify individual SFL dimensions (interpersonal, ideational, textual) to quantify their independent contributions to RAG performance, rather than relying on the current holistic taxonomy approach

3. **Alternative Model Architecture Comparison**: Test whether specialized RAG architectures (such as dense passage retrieval with reranking, or learned sparse representations) show different scaling characteristics or respond differently to Self-Doc optimization compared to the baseline models used in this study