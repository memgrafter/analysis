---
ver: rpa2
title: Quality or Quantity? On Data Scale and Diversity in Adapting Large Language
  Models for Low-Resource Translation
arxiv_id: '2408.12780'
source_url: https://arxiv.org/abs/2408.12780
tags:
- data
- languages
- parallel
- language
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work studies how to adapt large language models (LLMs) for
  low-resource machine translation (MT). While high-resource LLM-MT research shows
  that parallel data and data diversity are less important, this paper finds the opposite
  for low-resource languages: parallel data is critical during both pre-training and
  supervised fine-tuning (SFT), and diversity in SFT tasks causes interference rather
  than beneficial transfer.'
---

# Quality or Quantity? On Data Scale and Diversity in Adapting Large Language Models for Low-Resource Translation

## Quick Facts
- arXiv ID: 2408.12780
- Source URL: https://arxiv.org/abs/2408.12780
- Reference count: 40
- Primary result: Parallel data is critical for low-resource LLM-MT; focused multilingual MT fine-tuning yields +16.5 average chrF++ improvement over few-shot prompting

## Executive Summary
This paper challenges the conventional wisdom from high-resource LLM-MT research by demonstrating that low-resource translation requires fundamentally different approaches. While high-resource settings show that parallel data and diversity are less important, this work finds the opposite for low-resource languages: parallel data is critical during both pre-training and supervised fine-tuning, and diversity in SFT tasks causes interference rather than beneficial transfer. Through experiments with three LLMs across Indigenous American and North-East Indian language groups, the authors show that focused multilingual MT fine-tuning for more epochs yields the best results, achieving significant improvements over few-shot prompting.

## Method Summary
The authors employ a two-stage training approach: Continued Pre-Training (CPT) followed by Supervised Fine-Tuning (SFT). During CPT, models are trained on a mixture of monolingual and parallel data with various mixing strategies, including concatenated source-target pairs. SFT involves fine-tuning on multilingual MT data, with experiments varying task diversity and data scaling. The study uses three LLMs (Gemma 2B, Mistral 7B, Llama3 8B) and evaluates on AmericasNLP 2024 and WMT 2023 datasets, focusing on chrF++ improvements over few-shot prompting baselines.

## Key Results
- Parallel data is critical during both pre-training and SFT, with concatenated source-target pairs showing significant gains
- Task diversity in SFT causes interference rather than beneficial transfer in low-resource settings
- Scaling parallel data during SFT yields continued performance improvements, with gains plateauing around 1M sentences
- Focused multilingual MT fine-tuning for 2-5 epochs on 250K-1M examples is more effective than diverse task mixtures

## Why This Works (Mechanism)

### Mechanism 1
Parallel data is critical during both pre-training and SFT for low-resource LLM-MT. Concatenating source and target sentences during pre-training teaches the model the task of translation, not just language modeling. Low-resource languages benefit from explicit translation examples during pre-training, unlike high-resource languages that may have already learned translation from pre-training data. Break condition: If pre-training data already contains sufficient parallel data for low-resource languages, or if concatenation disrupts language modeling too much.

### Mechanism 2
Diversity in SFT tasks causes interference rather than beneficial transfer for low-resource LLM-MT. At low-resource scales, the model is still learning basic language generation and translation. Adding diverse tasks introduces conflicting objectives that harm MT performance. Low-resource LLMs need focused training on the target task rather than general capability enhancement. Break condition: If the model has sufficient capacity and pre-training exposure to handle diverse tasks, or if diversity is carefully controlled.

### Mechanism 3
Scaling LRL parallel data during SFT yields continued performance improvements unlike high-resource LLM-MT. Low-resource languages need more explicit translation examples during fine-tuning to achieve quality comparable to high-resource languages. Performance plateaus only after extensive scaling. The relationship between data scale and performance in low-resource settings is fundamentally different from high-resource settings. Break condition: If the model architecture or pre-training provides sufficient cross-lingual transfer, or if data quality degrades with scale.

## Foundational Learning

- **Concept**: Continued Pre-Training (CPT)
  - Why needed here: Low-resource languages are underrepresented in pre-training data. CPT teaches the LLM to model these languages effectively
  - Quick check question: Why would pre-training on monolingual data alone be insufficient for low-resource MT?

- **Concept**: Supervised Fine-Tuning (SFT)
  - Why needed here: Few-shot prompting and instruction-tuning alone are insufficient for low-resource languages. SFT with parallel data is needed for quality translation
  - Quick check question: What is the key difference between instruction-tuning and SFT in the context of this work?

- **Concept**: LoRA fine-tuning
  - Why needed here: Full fine-tuning is computationally expensive. LoRA provides parameter-efficient adaptation while maintaining performance
  - Quick check question: How does LoRA differ from full fine-tuning in terms of parameter count and computational cost?

## Architecture Onboarding

- **Component map**: Pre-trained LLM → CPT stage (monolingual + parallel data) → SFT stage (MT data) → Evaluation
- **Critical path**: Parallel data → CPT (concatenated) → SFT scaling → Performance improvement
- **Design tradeoffs**: Full fine-tuning vs LoRA (performance vs efficiency), concatenated vs separate parallel data (task learning vs language modeling)
- **Failure signatures**: If using separate parallel data instead of concatenated, if using too much task diversity in SFT, if not scaling parallel data sufficiently
- **First 3 experiments**:
  1. Compare concatenated vs separate parallel data during CPT
  2. Test SFT scaling on parallel data (50K to 1M examples)
  3. Compare bilingual vs multilingual SFT approaches

## Open Questions the Paper Calls Out

1. What is the optimal ratio of parallel to monolingual data for continued pre-training in low-resource LLM-MT?
2. How do different types of linguistic diversity (related vs. unrelated languages) impact low-resource LLM-MT performance during supervised fine-tuning?
3. What is the long-term impact of data repetition (multiple epochs) on low-resource LLM-MT performance, and at what point does overfitting occur?
4. How does the performance of low-resource LLM-MT systems compare when using synthetic vs. human-generated parallel data at scale?

## Limitations

- Moderate novelty overlap with existing literature (FMR=0.443) suggests core mechanisms have been explored in high-resource settings
- Focus on specific low-resource language families may limit generalizability to other language groups
- chrF++ as primary evaluation metric may not capture all aspects of translation quality

## Confidence

- **High Confidence**: Parallel data is critical during both pre-training and SFT for low-resource LLM-MT
- **Medium Confidence**: Diversity in SFT tasks causes interference rather than beneficial transfer
- **Medium Confidence**: Scaling LRL parallel data during SFT yields continued improvements unlike high-resource settings

## Next Checks

1. Replicate core experiments with low-resource languages from different families (e.g., African, Southeast Asian) to test generalizability
2. Conduct parallel experiments using multiple evaluation metrics (BLEU, COMET, TER) alongside chrF++ to verify consistent improvements
3. Perform extended scaling experiments beyond 1M parallel sentences during SFT to identify plateaus and compare scaling curves with high-resource LLM-MT studies