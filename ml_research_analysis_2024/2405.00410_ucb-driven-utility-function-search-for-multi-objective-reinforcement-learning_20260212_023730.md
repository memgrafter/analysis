---
ver: rpa2
title: UCB-driven Utility Function Search for Multi-objective Reinforcement Learning
arxiv_id: '2405.00410'
source_url: https://arxiv.org/abs/2405.00410
tags:
- scalarisation
- vectors
- learning
- policy
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UCB-MOPPO, a novel multi-objective reinforcement
  learning method that uses Upper Confidence Bound (UCB) to efficiently search for
  promising weight vectors during training. The approach decomposes the multi-objective
  problem into multiple scalar sub-problems, each conditioned on specific weight vectors,
  and employs a UCB acquisition function to select the most promising weights for
  maximizing the hypervolume of the resulting Pareto front.
---

# UCB-driven Utility Function Search for Multi-objective Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.00410
- Source URL: https://arxiv.org/abs/2405.00410
- Authors: Yucheng Shi; David Lynch; Alexandros Agapitos
- Reference count: 40
- Key outcome: UCB-MOPPO uses UCB to efficiently search weight vectors in MORL, outperforming state-of-the-art baselines on Mujoco benchmarks while maintaining memory efficiency through a fixed set of policies.

## Executive Summary
This paper introduces UCB-MOPPO, a novel multi-objective reinforcement learning method that uses Upper Confidence Bound (UCB) to efficiently search for promising weight vectors during training. The approach decomposes the multi-objective problem into multiple scalar sub-problems, each conditioned on specific weight vectors, and employs a UCB acquisition function to select the most promising weights for maximizing the hypervolume of the resulting Pareto front. The method is evaluated on six Mujoco benchmark problems and demonstrates consistent performance, outperforming state-of-the-art baselines in terms of hypervolume and expected utility metrics.

## Method Summary
UCB-MOPPO implements a two-layer decomposition of the scalarization vector space, dividing it into K sub-spaces each trained with a separate weight-conditioned Actor-Critic policy. At each training stage, a surrogate model predicts expected changes in objective values, and a UCB acquisition function balances exploration and exploitation when selecting scalarization vectors for training. The policies are updated using PPO, and the approach maintains a small, fixed set of policies while achieving high-quality Pareto front approximations.

## Key Results
- UCB-MOPPO consistently outperforms state-of-the-art baselines on six Mujoco benchmarks
- The method achieves higher hypervolume and expected utility metrics compared to existing approaches
- Maintains memory efficiency through a small, fixed set of policies while approximating high-quality Pareto fronts
- Generalizes effectively to new scalarization vectors without requiring additional training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UCB acquisition function efficiently selects scalarisation vectors to maximise hypervolume of the Pareto front.
- Mechanism: The algorithm maintains a surrogate model that predicts expected changes in objective values for each policy conditioned on a given scalarisation vector. At each training stage, UCB balances exploration (high uncertainty) and exploitation (high mean improvement) when choosing which vectors to train on.
- Core assumption: The surrogate model's predictions and uncertainty estimates are sufficiently accurate to guide effective selection.
- Evidence anchors:
  - [abstract]: "employs a UCB acquisition function to select the most promising weights for maximizing the hypervolume of the resulting Pareto front"
  - [section]: "An acquisition function based on UCB [28] is used to select scalarisation vectors for training from each sub-spaceWk"
  - [corpus]: Weak - neighboring papers discuss similar MORL approaches but don't directly validate UCB for hypervolume maximisation
- Break condition: If surrogate predictions become inaccurate (e.g., due to non-stationary dynamics or poor model capacity), the UCB selection will no longer effectively target promising vectors.

### Mechanism 2
- Claim: Two-layer decomposition enables scalable parallel training with compact policies.
- Mechanism: The overall scalarisation weight space W is divided into K sub-spaces, each trained with a separate policy. Within each sub-space, M scalarisation vectors are defined, creating K×M scalar RL sub-problems. This decomposition allows policies to specialise in different regions while maintaining generalisation across local neighbourhoods.
- Core assumption: Policies can effectively generalise across scalarisation vector neighbourhoods within their assigned sub-space.
- Evidence anchors:
  - [section]: "The overall scalarisation weightW is divided into K sub-spaces... A separate policyπk is trained for each sub-problem, conditioned on scalarisation vectors sampled from the corresponding sub-spaceWk"
  - [abstract]: "maintains a small, fixed set of policies while achieving high-quality Pareto front approximations"
  - [corpus]: Moderate - the decomposition approach is validated through experiments showing consistent performance across benchmarks
- Break condition: If the sub-space partitioning is too coarse, policies may fail to capture important trade-offs within each region.

### Mechanism 3
- Claim: Scalarisation-vector-conditioned Actor-Critic architecture enables efficient generalisation to new scalarisation vectors.
- Mechanism: Both the actor and critic networks are conditioned on the scalarisation vector through concatenation and residual connections. This allows a single policy to express different trade-offs between objectives by generalising across a neighbourhood of scalarisation vectors.
- Core assumption: The neural network architecture can effectively learn the mapping from state, action, and scalarisation vector to value and policy.
- Evidence anchors:
  - [section]: "both πθ(s,w) and vπϕ(s,w) are conditioned onw. This enables a single policy to express different trade-off between objectives by generalising across a neighbourhood of scalarisation vectors"
  - [abstract]: "generalizes effectively to new scalarization vectors without requiring additional training"
  - [corpus]: Weak - neighbouring papers discuss similar conditioning approaches but don't specifically validate this architecture for MORL generalisation
- Break condition: If the conditioning mechanism fails to capture the relationship between scalarisation vectors and policy/value functions, generalisation performance will degrade.

## Foundational Learning

- Concept: Multi-objective Markov Decision Process (MOMDP)
  - Why needed here: The paper's entire framework operates within the MOMDP formalism, where multiple objectives must be traded off during decision-making
  - Quick check question: What distinguishes a MOMDP from a standard MDP, and how are multiple objectives typically represented in this framework?

- Concept: Pareto optimality and Convex Coverage Set (CCS)
  - Why needed here: The algorithm aims to approximate the CCS of policies, which requires understanding dominance relationships and the geometric properties of Pareto fronts
  - Quick check question: How does the CCS differ from the full Pareto front, and why is this distinction important for linear utility functions?

- Concept: Hypervolume (HV) metric
  - Why needed here: HV is the primary quality metric used to evaluate Pareto front approximations and guide the UCB acquisition function
  - Quick check question: What properties make hypervolume a suitable metric for comparing Pareto front approximations in MORL?

## Architecture Onboarding

- Component map:
  - K pivot policies (πk) -> Scalarisation-vector-conditioned Actor-Critic networks (πθ, vπϕ) -> Surrogate model (f k,j bagging) -> UCB acquisition function -> PPO optimizer -> Memory buffer E

- Critical path:
  1. Warm-up phase: Train K pivot policies on fixed pivot weights
  2. Periodic evaluation: Collect objective values for all policies
  3. Surrogate construction: Build prediction models for objective changes
  4. UCB selection: Choose N scalarisation vectors per sub-space
  5. PPO training: Update policies on selected vectors
  6. Repeat steps 2-5 until convergence

- Design tradeoffs:
  - Fixed vs. adaptive number of policies (trade-off between memory efficiency and coverage)
  - Granularity of sub-space partitioning (trade-off between specialisation and generalisation)
  - Surrogate model complexity vs. prediction accuracy
  - UCB exploration parameter β (trade-off between exploration and exploitation)

- Failure signatures:
  - Poor HV growth despite training (surrogate model inaccurate or UCB not selecting effectively)
  - Mode collapse (policies converging to similar regions of objective space)
  - Unstable learning curves (PPO hyperparameters or conditioning mechanism problematic)
  - High variance across seeds (insufficient exploration or sensitive hyperparameters)

- First 3 experiments:
  1. Run UCB-MOPPO on Swimmer-v2 with default hyperparameters and verify HV growth over time
  2. Compare Fixed-MOPPO vs. UCB-MOPPO on HalfCheetah-v2 to validate UCB selection benefits
  3. Test interpolation capability by evaluating trained policies on finer-grained scalarisation vectors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does UCB-MOPPO perform on MORL problems with non-linear utility functions?
- Basis in paper: [inferred] The paper focuses on linear utility functions and mentions that future work will explore non-linear utility function spaces.
- Why unresolved: The current implementation is designed for linear utility functions, and no experiments or analysis are provided for non-linear cases.
- What evidence would resolve it: Experiments comparing UCB-MOPPO's performance on non-linear utility functions versus linear ones, along with analysis of convergence and hypervolume quality.

### Open Question 2
- Question: What is the impact of the number of sub-spaces (K) and sub-space vectors (M) on UCB-MOPPO's performance?
- Basis in paper: [explicit] The paper states that the decomposition setup varies for different problems and mentions step sizes, but does not provide a systematic analysis of how K and M affect performance.
- Why unresolved: The paper uses fixed values for K and M based on problem type, but does not explore how these hyperparameters influence the quality of the Pareto front or computational efficiency.
- What evidence would resolve it: A comprehensive ablation study varying K and M across different MORL problems, measuring hypervolume, expected utility, and computational cost.

### Open Question 3
- Question: How does UCB-MOPPO's performance scale with an increasing number of objectives?
- Basis in paper: [inferred] The paper only evaluates on two and three-objective problems, with no analysis of scalability beyond this.
- Why unresolved: The exponential growth of the preference space with more objectives is mentioned, but no experiments are conducted to test UCB-MOPPO's effectiveness in higher-dimensional objective spaces.
- What evidence would resolve it: Experiments applying UCB-MOPPO to MORL problems with four or more objectives, analyzing hypervolume, expected utility, and computational requirements.

### Open Question 4
- Question: How does the UCB acquisition function compare to other acquisition functions in terms of Pareto front quality and exploration efficiency?
- Basis in paper: [explicit] The paper introduces UCB as the acquisition function and compares it to mean-based selection, but does not compare it to other acquisition functions like Expected Improvement or Thompson Sampling.
- Why unresolved: The paper only provides a comparison between UCB and mean-based selection, leaving open the question of whether UCB is optimal or if other acquisition functions could yield better results.
- What evidence would resolve it: Experiments replacing the UCB acquisition function with other acquisition functions (e.g., Expected Improvement, Thompson Sampling) and comparing their performance in terms of hypervolume, expected utility, and convergence speed.

## Limitations

- The empirical validation lacks clear quantification of performance gaps and statistical significance across benchmarks
- The approach relies heavily on accurate surrogate models for UCB selection, which may fail with non-stationary dynamics
- Memory efficiency claims depend on fixed policy count K, but the impact of this parameter on coverage quality is not explored

## Confidence

- High confidence in the theoretical framework and algorithmic design
- Medium confidence in the decomposition approach based on experimental results
- Low confidence in the generalization claims without additional empirical validation

## Next Checks

1. Test UCB-MOPPO on a simple synthetic MORL problem where the Pareto front is known analytically to verify HV maximization claims
2. Compare training stability and convergence speed against Fixed-MOPPO across multiple random seeds to quantify robustness
3. Evaluate policy interpolation performance on scalarization vectors between trained points to verify the generalization claims experimentally