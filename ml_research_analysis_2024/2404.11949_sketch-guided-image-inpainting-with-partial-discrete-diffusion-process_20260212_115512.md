---
ver: rpa2
title: Sketch-guided Image Inpainting with Partial Discrete Diffusion Process
arxiv_id: '2404.11949'
source_url: https://arxiv.org/abs/2404.11949
tags:
- image
- inpainting
- sketch
- diffusion
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel partial discrete diffusion process
  (PDDP) for sketch-guided image inpainting. The method uses a discrete latent space
  learned via a VQ-VAE, then corrupts only the masked regions in the latent space
  and reconstructs them conditioned on hand-drawn sketches via a bi-directional transformer.
---

# Sketch-guided Image Inpainting with Partial Discrete Diffusion Process

## Quick Facts
- arXiv ID: 2404.11949
- Source URL: https://arxiv.org/abs/2404.11949
- Authors: Nakul Sharma; Aditay Tripathi; Anirban Chakraborty; Anand Mishra
- Reference count: 40
- Primary result: Achieves state-of-the-art FID of 7.72, LPIPS of 0.107, and local LPIPS of 0.414 for sketch-guided image inpainting

## Executive Summary
This work introduces a novel partial discrete diffusion process (PDDP) for sketch-guided image inpainting. The method uses a discrete latent space learned via a VQ-VAE, then corrupts only the masked regions in the latent space and reconstructs them conditioned on hand-drawn sketches via a bi-directional transformer. A dataset was synthesized from MS-COCO with objects masked and sketchified. The approach achieves state-of-the-art performance on multiple metrics and demonstrates strong qualitative results.

## Method Summary
The method employs a two-stage approach: first, a VQ-VAE learns a discrete latent space representation of images. Second, a partial discrete diffusion process (PDDP) corrupts only the masked regions in this latent space, which are then reconstructed conditioned on sketch embeddings using a bi-directional transformer. The transformer integrates sketch features with image tokens using positional embeddings and multi-head attention. The method was trained on a synthesized dataset of 860K object sketches from MS-COCO.

## Key Results
- Achieves state-of-the-art FID of 7.72 on sketch-guided inpainting
- Obtains LPIPS score of 0.107 and local LPIPS of 0.414
- Outperforms baselines including ControlNet and Palette
- Demonstrates strong qualitative results and positive user study feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partial Discrete Diffusion Process (PDDP) allows controlled corruption and reconstruction of only the masked region in latent space, aligning forward and backward processes for inpainting.
- Mechanism: Instead of diffusing all tokens, PDDP applies the diffusion transition matrix only to masked tokens (ML⊙v(zt)⊤Qtv(zt−1)+(1−ML)⊙v(zt−1)). This preserves non-masked content while iteratively denoising the masked region conditioned on sketch embeddings.
- Core assumption: Masked and non-masked tokens evolve independently under the transition matrix, so selective application does not corrupt valid content.
- Evidence anchors:
  - [section] "During inference for inpainting, this means denoising our corrupted latent representation zm to z0 in T timesteps. The fact that the single-step transition probability for each token at each timestep is independent of each other allows us to incorporate the position of masked regions into the transition distribution..."
  - [abstract] "The forward pass of the PDDP corrupts the masked regions of the image and the backward pass reconstructs these masked regions conditioned on hand-drawn sketches..."

### Mechanism 2
- Claim: Sketch-guided bi-directional transformer conditions the reverse diffusion on sketch embeddings to generate content faithful to the provided shape and pose.
- Mechanism: Sketch features s are extracted via a ResNet50 encoder, combined with positional embeddings, and concatenated with image tokens [s; vzt] at each timestep. Transformer blocks then predict the distribution of the noiseless target variable ~z0 conditioned on s.
- Core assumption: The sketch embedding captures sufficient shape and pose information to guide token prediction, and the transformer can integrate it with image context without introducing domain gap artifacts.
- Evidence anchors:
  - [section] "We use a sketch of the object to be inpainted as a conditioning signal... We add 2D-learnable positional embeddings to the feature map fs... to condition the denoising step on the sketch embeddings s, we concatenate a linear projection of latent representation s with the vector representation of zt..."
  - [abstract] "This strategy effectively addresses the domain gap between sketches and natural images, thereby, enhancing the quality of inpainting results."

### Mechanism 3
- Claim: Using discrete latent space via VQ-VAE allows robust handling of the sketch-image domain gap and stable training of the diffusion process.
- Mechanism: Images are encoded into a sequence of codebook indices z0 ∈ {1, 2, ..., C}^K. Masked regions are replaced with a special [MASK] token index (C+1). The discrete diffusion then operates on this compact representation, avoiding issues with continuous pixel-level noise.
- Core assumption: The discrete latent space preserves perceptually important features and the codebook can represent both masked and unmasked regions adequately.
- Evidence anchors:
  - [section] "In the first stage, we train an encoder E, a codebook Z, and a decoder D to learn a perceptually compressed discrete latent space of images... Our novelty lies in the second stage where we first project the ground truth image and the masked image in the discrete latent space using the learned encoder and the codebook..."
  - [abstract] "The proposed novel transformer module accepts two inputs – the image containing the masked region to be inpainted and the query sketch to model the reverse diffusion process."

## Foundational Learning

- Concept: Markov Chain Monte Carlo and diffusion probabilistic models
  - Why needed here: The PDDP is based on discrete diffusion processes that iteratively corrupt and denoise data; understanding MCMC helps grasp how the reverse process samples from the posterior.
  - Quick check question: In a discrete diffusion model, what does the transition matrix Q_t represent, and why is it crucial for the forward process?

- Concept: Vector Quantized Variational Autoencoders (VQ-VAE)
  - Why needed here: The method relies on learning a discrete latent codebook to represent images compactly before applying diffusion; knowing VQ-VAE helps understand how images are discretized.
  - Quick check question: How does a VQ-VAE encoder map an image to a sequence of codebook indices, and what role does the decoder play?

- Concept: Transformer attention mechanisms and positional embeddings
  - Why needed here: The sketch-guided bi-directional transformer uses multi-head self-attention to integrate sketch and image features; understanding attention is key to seeing how conditioning is applied.
  - Quick check question: In a transformer block, how are positional embeddings combined with token embeddings, and why is this necessary for image data?

## Architecture Onboarding

- Component map: VQ-VAE (Encoder E, codebook Z, decoder D) -> PDDP (Forward corruption, Reverse denoising) -> Sketch-guided bi-directional transformer (Input [s; vzt], series of transformer blocks, output ~z0 distribution)

- Critical path:
  1. Encode image → discrete latent tokens z0
  2. Apply mask → zm (replace masked tokens with [MASK])
  3. Forward PDDP: Corrupt masked tokens to zT=zm
  4. Reverse PDDP: Iteratively denoise with transformer conditioned on sketch s
  5. Decode ~z0 → inpainted image

- Design tradeoffs:
  - Discrete vs. continuous diffusion: Discrete is more stable and handles the sketch-image domain gap better but may lose fine detail.
  - Partial vs. full diffusion: Partial reduces computation and preserves valid content but assumes independence of masked/non-masked tokens.
  - Sketch conditioning via transformer vs. AdaLN: Transformer can integrate spatial info better but is heavier; AdaLN is lighter but may not capture pose/shape details.

- Failure signatures:
  - Poor FID/LPIPS: Likely issues in codebook quality, sketch conditioning, or diffusion steps.
  - Artifacts in non-masked regions: Indicates incorrect masking in latent space or token dependency violations.
  - Sketch inconsistency: Sketch conditioning not effective; check sketch encoder or transformer conditioning.
  - Blurry or unrealistic output: VQ-VAE decoder issues or insufficient diffusion steps.

- First 3 experiments:
  1. Train VQ-VAE on MS-COCO and verify reconstruction quality on masked/unmasked images.
  2. Implement PDDP with a simple transformer (1-2 layers) and test on synthetic mask/sketches to confirm selective corruption works.
  3. Train full model on small dataset, evaluate FID/LPIPS, and visualize intermediate denoising steps to debug conditioning and token prediction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would incorporating stroke-level details in sketch embeddings impact the quality of inpainting results?
- Basis in paper: [explicit] The authors mention that "Future research could explore more sophisticated sketch embeddings capturing stroke-level details."
- Why unresolved: The current sketch information embedding uses a straightforward ResNet50 encoder, which may not capture fine-grained sketch features.
- What evidence would resolve it: Comparative experiments using different sketch embedding architectures (e.g., stroke-based encoders, graph neural networks) and measuring their impact on inpainting metrics like FID and LPIPS.

### Open Question 2
- Question: What is the impact of using partial sketches instead of complete object sketches on the performance of the proposed method?
- Basis in paper: [inferred] The authors note that their model is designed for whole-object sketches and may not perform well with partial sketches, stating "This may be addressed by training the proposed model using the partial sketch."
- Why unresolved: The model's training paradigm assumes complete object sketches, so partial sketches could lead to misinterpretations during inference.
- What evidence would resolve it: Experiments training the model on datasets with partial sketches and comparing performance metrics to the original approach.

### Open Question 3
- Question: How does the proposed partial discrete diffusion process compare to other conditioning mechanisms for integrating sketch information with image representations?
- Basis in paper: [explicit] The authors state that "Another area of exploration involves refining the conditioning mechanisms that merge sketch embeddings with image representations to synthesize the inpainted image."
- Why unresolved: The current conditioning mechanism uses sketch embeddings combined with image features in the self-attention blocks of the transformer, but other approaches may yield better results.
- What evidence would resolve it: Comparative experiments using different conditioning mechanisms (e.g., cross-attention, feature fusion) and measuring their impact on inpainting metrics and qualitative results.

### Open Question 4
- Question: How does the proposed method perform on real-world datasets with natural user-generated sketches?
- Basis in paper: [explicit] The authors mention that "we aim to develop diffusion models for their generative capabilities and leverage transformer models for their robust modeling capabilities, building upon the discrete diffusion process."
- Why unresolved: The current experiments are conducted on a synthesized dataset, and the performance on real-world data with natural sketches remains unknown.
- What evidence would resolve it: Experiments on real-world datasets with user-generated sketches and measuring performance using appropriate metrics and user studies.

## Limitations
- Critical implementation details including transformer architecture specifics and training hyperparameters are underspecified
- The independence assumption for partial diffusion is stated but not rigorously proven for the specific CNN-based VQ-VAE latent space
- Performance on real-world datasets with natural user-generated sketches remains untested
- The method is designed for whole-object sketches and may not handle partial sketches effectively

## Confidence

**High Confidence**: The core methodological approach is clearly described and internally consistent. The VQ-VAE for discrete latent space representation, the partial discrete diffusion process operating only on masked tokens, and the sketch-guided bi-directional transformer conditioning are all logically sound and well-motivated by the literature.

**Medium Confidence**: The reported quantitative results (FID 7.72, LPIPS 0.107) appear reasonable for sketch-guided inpainting tasks based on similar diffusion model work, though exact comparisons are difficult without knowing all implementation details. The user study methodology seems appropriate, though sample sizes and exact question phrasing are not specified.

**Low Confidence**: The exact mechanisms by which the discrete diffusion process handles complex token dependencies are not fully demonstrated. While the independence assumption is stated, no ablation studies or theoretical analysis prove this holds for the specific CNN-based VQ-VAE latent space used. The claim that this approach better handles the sketch-image domain gap than continuous diffusion methods is asserted but not rigorously compared.

## Next Checks

1. **Token Dependency Analysis**: Implement the VQ-VAE and analyze the correlation structure between neighboring tokens in the latent space. Measure how often masked token corruption during PDDP affects non-masked region reconstruction quality. This validates the independence assumption critical to partial diffusion.

2. **Sketch Conditioning Ablation**: Create controlled experiments where the sketch encoder is removed or replaced with random noise while keeping all other components identical. Measure changes in FID and LPIPS scores, and perform qualitative analysis of sketch consistency in results. This quantifies how much the sketch conditioning actually contributes versus the diffusion process alone.

3. **VQ-VAE Reconstruction Quality**: Train VQ-VAE with varying codebook sizes (e.g., 128, 256, 512) and evaluate reconstruction fidelity on masked vs. unmasked regions. Compare the trade-off between compression ratio and inpainting quality to determine if the chosen architecture size was optimal or simply sufficient.