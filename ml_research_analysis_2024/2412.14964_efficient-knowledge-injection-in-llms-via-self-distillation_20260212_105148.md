---
ver: rpa2
title: Efficient Knowledge Injection in LLMs via Self-Distillation
arxiv_id: '2412.14964'
source_url: https://arxiv.org/abs/2412.14964
tags:
- distillation
- prompt
- answer
- knowledge
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces prompt distillation, a self-distillation-based
  method for injecting new factual knowledge into large language models (LLMs) from
  free-form documents. Unlike prior knowledge distillation approaches that require
  larger teacher models or structured formats, prompt distillation uses the same LLM
  as both teacher and student (with a LoRA adapter), eliminating style and capacity
  mismatches.
---

# Efficient Knowledge Injection in LLMs via Self-Distillation

## Quick Facts
- arXiv ID: 2412.14964
- Source URL: https://arxiv.org/abs/2412.14964
- Authors: Kalle Kujanpää; Pekka Marttinen; Harri Valpola; Alexander Ilin
- Reference count: 40
- Primary result: Prompt distillation significantly outperforms standard supervised fine-tuning on knowledge injection tasks from Squadshifts datasets, achieving closed-book performance competitive with RAG.

## Executive Summary
This paper introduces prompt distillation, a self-distillation-based method for injecting new factual knowledge into large language models (LLMs) from free-form documents. Unlike prior knowledge distillation approaches that require larger teacher models or structured formats, prompt distillation uses the same LLM as both teacher and student (with a LoRA adapter), eliminating style and capacity mismatches. The method generates question-answer pairs from target documents and trains the student to match the teacher's token-level probability distributions. Across multiple LLM sizes and families, prompt distillation significantly outperforms standard supervised fine-tuning on knowledge injection tasks and achieves closed-book performance competitive with retrieval-augmented generation.

## Method Summary
Prompt distillation uses the same LLM as both teacher and student (with a LoRA adapter) to eliminate style and capacity mismatches. The method generates question-answer pairs from target documents using the base model, then trains the student to match the teacher's token-level probability distributions using KL divergence loss. Higher temperatures during both data generation and distillation improve performance by maximizing data coverage and encouraging the student to learn from the teacher's full probability distribution. Regularization using a dataset of instruction-response pairs prevents catastrophic forgetting of general capabilities.

## Key Results
- Prompt distillation significantly outperforms standard supervised fine-tuning on Squadshifts datasets (Amazon, New Wiki, NYT, Reddit)
- Achieves closed-book performance competitive with retrieval-augmented generation (RAG)
- On HotpotQA, substantially improves base model performance and enhances RAG when combined
- More data-efficient than fine-tuning, maintaining general capabilities through regularization

## Why This Works (Mechanism)

### Mechanism 1
Self-distillation using the same model as both teacher and student eliminates style and capacity mismatches, enabling the student to focus specifically on learning new factual knowledge rather than mimicking expert answering styles. By using the same model for both roles (with a LoRA adapter), the teacher and student share identical capabilities and stylistic tendencies. This removes the need to adapt to a different model's answering style and allows the student to focus purely on internalizing the factual content from the privileged context.

### Mechanism 2
KL divergence loss with soft targets preserves the student's existing knowledge distribution where the teacher adds no new information, leading to more efficient learning and reduced catastrophic forgetting. The KL divergence loss minimizes the mutual information between the privileged context and the student's output distribution, ensuring the student only learns new information while maintaining its existing knowledge. This is more efficient than cross-entropy with hard targets, which forces alignment even when unnecessary.

### Mechanism 3
Higher temperatures during both data generation and distillation improve performance by maximizing data coverage and encouraging the student to learn from the teacher's full probability distribution rather than just the most likely tokens. High temperature sampling during question and answer generation creates more diverse training data, while high temperature distillation makes the student focus on the teacher's full probability distribution, including less probable tokens. This helps the student learn to avoid incorrect answers and understand the teacher's reasoning process.

## Foundational Learning

- **Knowledge Distillation**: Understanding how knowledge can be transferred from a teacher model to a student model through probability distributions rather than hard labels. Quick check: What is the key difference between knowledge distillation and standard supervised learning?

- **Mutual Information**: The KL divergence loss minimizes mutual information between the privileged context and the student's output, ensuring efficient learning of new knowledge. Quick check: How does minimizing KL divergence relate to reducing uncertainty about the privileged context?

- **Catastrophic Forgetting**: The regularization mechanism prevents the model from losing previously learned capabilities while acquiring new knowledge. Quick check: What regularization technique is used to prevent catastrophic forgetting during knowledge injection?

## Architecture Onboarding

- **Component map**: Base LLM (Llama-3, Qwen2.5 families) -> LoRA adapter for parameter-efficient fine-tuning -> Data generation pipeline (question and answer generation) -> Distillation training loop (teacher forward pass, student forward pass, KL loss computation) -> Regularization dataset (Tülu 3 SFT Dataset) -> Evaluation pipeline (LLM-as-a-judge grading)

- **Critical path**: 1. Generate training questions from source documents 2. Generate answers to training questions using the base model 3. For each (question, answer) pair, run teacher forward pass with privileged context 4. Run student forward pass without privileged context 5. Compute KL divergence loss between teacher and student distributions 6. Apply regularization loss on unrelated instruction-response pairs 7. Update LoRA adapter weights 8. Evaluate on test questions

- **Design tradeoffs**: Using self-distillation vs. larger expert models (simpler but requires capable base model); High temperature vs. low temperature generation (more diverse vs. more accurate data); Soft targets vs. hard targets (preserves existing knowledge vs. clearer guidance); Parameter-efficient fine-tuning vs. full fine-tuning (memory efficient vs. potentially better performance)

- **Failure signatures**: Low performance on test questions despite successful training: likely insufficient data generation capability or wrong temperature settings; Catastrophic forgetting of general capabilities: likely insufficient regularization or too aggressive fine-tuning; High variance in generated answers: likely temperature too high during data generation; No improvement over base model: likely insufficient training data or LoRA rank too low

- **First 3 experiments**: 1. Implement basic self-distillation with LoRA adapter on a small dataset, verify that student can match teacher performance on training data 2. Test different temperature settings for data generation and distillation, measure impact on training data diversity and student performance 3. Add regularization component, verify that general capabilities are preserved while learning new knowledge

## Open Questions the Paper Calls Out

- What is the optimal temperature for data generation during prompt distillation? The paper tests temperatures of 1.5 and 2.0 for question generation and distillation but does not systematically explore the full range of possible temperatures to find the optimal value.

- How does prompt distillation performance scale with the number of training questions per document? The paper tests up to 200 training questions per document but does not explore whether performance continues to improve beyond this point or plateaus.

- What is the impact of using different question-generation models on prompt distillation performance? The paper only compares two specific question-generation approaches and does not explore whether other models or approaches might yield better results.

## Limitations

- Style mismatch elimination assumption lacks rigorous validation - experiments show self-distillation outperforms larger models, but alternative explanations exist
- Temperature optimization may be specific to experimental setup - optimal settings lack sensitivity analysis and confidence intervals
- Regularization approach effectiveness not extensively validated - lacks ablation studies and comparison with alternative techniques

## Confidence

- High Confidence: Core experimental results showing prompt distillation outperforming standard supervised fine-tuning on Squadshifts datasets and improving HotpotQA performance
- Medium Confidence: Claim that self-distillation eliminates style and capacity mismatches - experimental results support but mechanism not rigorously proven
- Medium Confidence: Assertion that higher temperatures improve performance - supporting evidence but lacks comprehensive sensitivity analysis
- Low Confidence: Generalizability of optimal temperature settings and specific regularization approach to scenarios beyond those tested

## Next Checks

1. **Style Mismatch Quantification**: Conduct experiments comparing self-distillation with different teacher-student pairs while measuring style similarity through perplexity, n-gram overlap, or other stylistic metrics to validate whether style elimination is the primary driver of self-distillation's success.

2. **Temperature Sensitivity Analysis**: Systematically vary temperature parameters across multiple domains and knowledge types to determine the robustness of the optimal settings, including statistical analysis of performance variance to establish confidence intervals.

3. **Regularization Ablation Study**: Compare prompt distillation with and without regularization across different knowledge injection tasks, including alternative regularization strategies, to quantify the regularization's contribution to both knowledge injection performance and retention of general capabilities.