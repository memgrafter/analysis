---
ver: rpa2
title: 'The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry'
arxiv_id: '2402.04347'
source_url: https://arxiv.org/abs/2402.04347
tags:
- attention
- hedgehog
- linear
- softmax
- attentions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the performance gap between linear and standard
  softmax attention in Transformers. The authors identify two key properties of softmax
  attention that prior linear attentions lack: low-entropy "spikiness" and dot-product
  monotonicity.'
---

# The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry

## Quick Facts
- arXiv ID: 2402.04347
- Source URL: https://arxiv.org/abs/2402.04347
- Reference count: 40
- One-line primary result: Hedgehog recovers over 99% of standard Transformer quality in train-from-scratch and finetuned-conversion settings

## Executive Summary
This paper addresses the performance gap between linear and standard softmax attention in Transformers. The authors identify two key properties of softmax attention that prior linear attentions lack: low-entropy "spikiness" and dot-product monotonicity. They propose Hedgehog, a learnable linear attention trained to mimic softmax attention using single-layer MLPs and attention weight distillation. Experiments show Hedgehog recovers over 99% of standard Transformer quality in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions on WikiText-103 and GLUE benchmarks.

## Method Summary
Hedgehog uses trainable single-layer MLPs with exponential activation to produce attention weights that mimic softmax attention. The model is trained using attention weight distillation, where the goal is to minimize the cross-entropy loss between computed linear attention weights and softmax attention weights. The method is evaluated across three regimes: training-from-scratch, finetuned-conversion, and pretrained-conversion. The approach maintains linear complexity while recovering the spikiness and monotonicity properties of softmax attention.

## Key Results
- Recovers over 99% of standard Transformer quality in train-from-scratch and finetuned-conversion settings
- Achieves state-of-the-art perplexity on WikiText-103 for subquadratic models
- Improves ROUGE scores on Llama-2 summarization tasks when using pretrained-conversion

## Why This Works (Mechanism)

### Mechanism 1
Hedgehog's learnable MLP feature map recovers the spiky attention property lost in prior linear attentions. By using an exponential activation function in the MLP, Hedgehog maps query-key dot products to high-weight spikes for relevant tokens and near-zero weights for irrelevant ones, mimicking softmax's spikiness.

### Mechanism 2
Hedgehog's attention weight distillation loss trains the feature map to produce attention weights that closely match softmax attention weights. By minimizing the cross-entropy loss between linear attention weights and softmax attention weights, Hedgehog learns a feature map that approximates softmax's behavior.

### Mechanism 3
Hedgehog's feature map is monotonic with respect to query-key dot products, addressing a limitation of prior linear attentions. The exponential activation function in the MLP ensures that attention weights increase as query-key dot products increase, maintaining monotonicity.

## Foundational Learning

- Concept: Linear attention and kernel functions
  - Why needed here: Understanding linear attention and kernel functions is crucial for grasping Hedgehog's approach to improving attention efficiency.
  - Quick check question: How does replacing softmax with a kernel function feature map reduce attention's time and space complexity?

- Concept: Taylor polynomial approximations
  - Why needed here: The paper explores Taylor polynomial approximations as a potential approach to recover softmax's properties in linear attention.
  - Quick check question: What are the trade-offs between using a higher-degree Taylor polynomial and computational efficiency in linear attention?

- Concept: Attention weight distillation
  - Why needed here: Attention weight distillation is a key component of Hedgehog's training process, guiding the learned feature map to mimic softmax attention weights.
  - Quick check question: How does minimizing the cross-entropy loss between linear attention weights and softmax attention weights guide the learning of the feature map?

## Architecture Onboarding

- Component map: Queries/Keys/Values -> MLP feature map -> Attention computation -> Distillation loss -> Output
- Critical path: Input → MLP feature map → Attention computation → Distillation loss → Output
- Design tradeoffs:
  - Spikiness vs. monotonicity: Balancing the ability to capture relevant tokens with maintaining stable gradients
  - Efficiency vs. expressivity: Ensuring the learned feature map maintains linear complexity while closely approximating softmax
- Failure signatures:
  - Low-entropy attention weights not recovered: Model may underperform on tasks requiring selective attention
  - Non-monotonic attention weights: May lead to unstable gradients and poor performance during training and finetuning
  - Inefficient feature map computation: Model may not achieve the desired speed and memory improvements
- First 3 experiments:
  1. Train Hedgehog from scratch on a next-token prediction task and compare attention weight entropy to softmax and prior linear attentions.
  2. Convert a finetuned BERT model to use Hedgehog attention and evaluate task performance recovery compared to the original model.
  3. Convert a pretrained GPT-2 model to use Hedgehog attention and finetune on a language modeling task, comparing perplexity to the original model and prior linear attentions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Hedgehog's learned feature maps perform when generalizing to entirely different model architectures beyond BERT?
- Basis in paper: [inferred] The paper discusses generalization of Hedgehog feature maps to new data and longer contexts, but only within the same BERT model architecture.
- Why unresolved: The experiments only evaluate Hedgehog feature maps on BERT models, leaving open the question of cross-model generalization.
- What evidence would resolve it: Experiments applying Hedgehog feature maps trained on BERT to other architectures like GPT or Vision Transformers, measuring attention fidelity and downstream task performance.

### Open Question 2
- Question: What is the impact of different activation functions in Hedgehog's MLP feature map on attention quality and computational efficiency?
- Basis in paper: [explicit] The paper mentions using exponential activation but also notes that softmax over MLP outputs "seems to work but with better stability."
- Why unresolved: The paper primarily uses one activation function without comprehensive comparison to alternatives like ReLU, GELU, or other stable activations.
- What evidence would resolve it: Systematic ablation studies comparing different activations in Hedgehog's MLP across multiple benchmarks, measuring both attention fidelity and computational efficiency.

### Open Question 3
- Question: Can Hedgehog's attention distillation technique be effectively combined with quantization methods to further improve efficiency?
- Basis in paper: [inferred] The paper mentions future work on combining Hedgehog with quantization, but doesn't explore this combination.
- Why unresolved: The paper focuses on linear attention efficiency but doesn't investigate synergistic effects with quantization techniques.
- What evidence would resolve it: Experiments applying both Hedgehog conversion and quantization (e.g., 8-bit, 4-bit) to pretrained models, measuring quality retention and efficiency gains compared to each method individually.

## Limitations

- Evaluation is limited to perplexity and GLUE benchmarks without extensive testing on tasks requiring complex reasoning or long-range dependencies
- The claim of "recovering 99% of Transformer quality" is based primarily on specific tasks without broader downstream application validation
- Computational efficiency claims lack comprehensive benchmarking across different hardware configurations and sequence lengths

## Confidence

**High Confidence**: The core architectural contribution of using learnable MLPs with exponential activation for linear attention is well-defined and technically sound. The attention weight distillation training procedure is clearly specified and reproducible.

**Medium Confidence**: The experimental results showing Hedgehog's performance improvements over prior linear attentions are convincing for the specific tasks and datasets tested. However, the generalizability of these results to other domains and model scales requires further validation.

**Low Confidence**: The assertion that Hedgehog achieves "state-of-the-art perplexity" for subquadratic models is limited by the narrow scope of comparison - it's evaluated against other linear attention methods rather than all subquadratic approaches.

## Next Checks

1. **Statistical Validation of Key Properties**: Conduct ablation studies measuring the impact of spikiness and monotonicity on downstream performance by systematically varying these properties in the feature map. Quantify the relationship between attention weight entropy and task accuracy across multiple benchmarks.

2. **Extended Hardware Benchmarking**: Profile Hedgehog's runtime performance and memory usage across different hardware configurations (GPU vs CPU), sequence lengths (from 512 to 16k tokens), and batch sizes. Compare against both standard Transformers and other linear attention methods under identical conditions.

3. **Generalization Testing**: Evaluate Hedgehog on a broader range of tasks including code generation, mathematical reasoning, and multimodal applications. Test across different model scales (1B, 7B, 70B parameters) and pretraining objectives to assess robustness of the conversion approach.