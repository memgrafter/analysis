---
ver: rpa2
title: 'Interpreting Context Look-ups in Transformers: Investigating Attention-MLP
  Interactions'
arxiv_id: '2402.15055'
source_url: https://arxiv.org/abs/2402.15055
tags:
- head
- attention
- explanation
- neuron
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a methodology to understand the interactions
  between attention heads and next-token neurons in large language models. The approach
  involves identifying next-token neurons, finding prompts that highly activate them,
  attributing neuron activation to specific attention heads, and generating explanations
  for the activity of these heads using GPT-4.
---

# Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions

## Quick Facts
- **arXiv ID**: 2402.15055
- **Source URL**: https://arxiv.org/abs/2402.15055
- **Reference count**: 34
- **Primary result**: Proposes methodology to understand attention-MLP interactions in transformers by identifying next-token neurons, attributing activation to attention heads, and generating automated explanations using GPT-4

## Executive Summary
This paper introduces a novel methodology to interpret how attention heads in transformer models interact with MLP layers to predict specific tokens. The approach identifies next-token neurons, finds prompts that maximally activate them, attributes neuron activation to specific attention heads, and generates automated explanations using GPT-4. The authors demonstrate that attention heads can recognize specific contexts relevant to predicting tokens and activate downstream neurons accordingly, with results consistent across different model sizes and variants.

## Method Summary
The methodology works by first identifying next-token neurons using a congruence score based on output weights aligned with token embeddings. For each identified neuron, the system finds prompts that maximally activate it and calculates attribution scores to determine which attention heads contribute to the activation. GPT-4 then generates explanations for the attention head activity patterns by analyzing differences between activating and non-activating prompts. The quality of these explanations is evaluated through zero-shot classification, where GPT-4 predicts head activity on new prompts using only the generated explanation.

## Key Results
- Attention heads can capture specific phrases or contexts to predict tokens, with behavior consistent across different model sizes
- GPT-4 successfully generates explanations for attention head activity patterns, though it struggles with certain patterns like induction heads
- The head explanation score (average of true positive and true negative rates) effectively measures explanation quality
- Different attention heads capture various uses of the same word that activate its predicting neuron

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention heads can recognize specific contexts relevant to predicting a token and activate downstream token-predicting neurons accordingly.
- Mechanism: The model identifies next-token neurons with output weights aligned to token embeddings. Attention heads contribute to these neurons via residual connections. The dot product between attention head outputs and neuron input weights determines head attribution. High attribution indicates the head is helping predict that token in that context.
- Core assumption: The interaction between attention heads and next-token neurons is measurable through dot product calculations in residual space, and this interaction meaningfully impacts token prediction.
- Evidence anchors:
  - [abstract] "We propose a methodology to identify next-token neurons, find prompts that highly activate them, and determine the upstream attention heads responsible."
  - [section 4.3] "we calculate their head attribution score as: A(i,k),(j,ℓ) = (⟨hi,k, ej,ℓ⟩, if k ≤ ℓ, 0, otherwise)"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.529, average citations=0.0. Weak corpus evidence for specific mechanisms but relevant papers exist on attention head interpretation.

### Mechanism 2
- Claim: GPT-4 can generate accurate explanations for attention head activity patterns by analyzing differences between activating and non-activating prompts.
- Mechanism: The system collects prompts that maximally activate a neuron, categorizes them by attention head activity, and feeds these two classes to GPT-4. GPT-4 identifies distinguishing features between the classes, creating an explanation. This explanation is then used as a zero-shot classifier to predict head activity on new prompts.
- Core assumption: GPT-4 can identify semantic or syntactic patterns that distinguish between prompt classes without being explicitly programmed to do so, and these patterns correspond to actual attention head behavior.
- Evidence anchors:
  - [abstract] "We then generate and evaluate explanations for the activity of these attention heads in an automated manner."
  - [section 4.4] "we use GPT-4 to generate explanations for the activity of these attention heads. Finally, we evaluate the explanations' quality by using GPT-4 as a zero-shot classifier to predict head activity on new prompts"
  - [corpus] Weak corpus evidence for GPT-4's explanation quality, but related work on automated interpretability exists (e.g., Bills et al., 2023).

### Mechanism 3
- Claim: Head explanation scores measure explanation quality by evaluating GPT-4's classification accuracy on unseen prompts.
- Mechanism: The system uses GPT-4 with only the explanation to classify whether attention heads would be active for new prompts. It compares these predictions to ground truth from actual head activity measurements. The head explanation score averages true positive and true negative rates.
- Core assumption: GPT-4's zero-shot classification accuracy using only the explanation is a valid proxy for explanation quality and faithfulness to actual attention head behavior.
- Evidence anchors:
  - [section 4.4] "To measure explanation quality, we define the head explanation score of head i at layer k, E_k^i as the average of the true positive rate and true negative rate of the classification results"
  - [section 5.1] "Different heads capture various uses of the same word that activate its predicting neuron"
  - [corpus] Weak corpus evidence for explanation scoring methods, but related work on interpretability evaluation exists.

## Foundational Learning

- Concept: Transformer architecture and residual connections
  - Why needed here: The methodology relies on understanding how attention heads output values that flow through residual connections to influence MLP layers, specifically next-token neurons.
  - Quick check question: How does a residual connection work in a transformer layer, and why is it crucial for the attention-MLP interaction this paper studies?

- Concept: Attention mechanism computation
  - Why needed here: The attribution score calculation requires understanding how attention heads compute outputs using key, query, and value matrices.
  - Quick check question: What is the mathematical formula for computing an attention head's output, and how do the key, query, and value matrices contribute to this?

- Concept: Neuron activation and embedding spaces
  - Why needed here: The methodology identifies next-token neurons based on their output weights' alignment with token embeddings, requiring understanding of both activation patterns and embedding spaces.
  - Quick check question: How are token embeddings related to vocabulary predictions, and why would a neuron's output weights being aligned with an embedding indicate it predicts that token?

## Architecture Onboarding

- Component map:
  - Input text → Embedding layer → N transformer layers (each: attention sublayer → MLP sublayer) → Unembedding layer → Output logits
  - Key components: Attention heads (key/query/value matrices), MLP neurons (weights), residual connections, embedding/unembedding matrices
  - Data flow: Residual streams carry information between layers, enabling attention head outputs to influence downstream MLP neurons

- Critical path:
  1. Identify next-token neurons using output weight congruence scores
  2. Find max-activating prompts for each neuron
  3. Calculate head attribution scores for each prompt-head-neuron combination
  4. Generate explanations using GPT-4 based on attribution patterns
  5. Validate explanations through zero-shot classification on test prompts

- Design tradeoffs:
  - Prompt selection: Using 20 prompts balances computational cost with explanation quality
  - Layer selection: Focusing on last 5 layers targets neurons most aligned with vocabulary space
  - Model variants: Testing across GPT-2 and Pythia variants assesses generalizability
  - GPT-4 usage: Zero-shot generation vs. few-shot generation tradeoffs cost vs. explanation quality

- Failure signatures:
  - Low head explanation scores across all neurons suggest the methodology isn't capturing meaningful patterns
  - Random neuron analysis showing similar score distributions to actual neurons indicates the approach may be finding spurious correlations
  - GPT-4 consistently failing to explain certain prompt patterns (like induction heads) reveals limitations in the explanation generation
  - Ablation experiments showing minimal probability changes suggest explanations may not capture causal relationships

- First 3 experiments:
  1. Run the full methodology on a single next-token neuron to verify each component works end-to-end
  2. Compare head explanation scores for random neurons vs. actual next-token neurons to establish baseline
  3. Perform ablation on a high-scoring attention head to verify the causal relationship between head activity and token prediction

## Open Questions the Paper Calls Out

- Question: How consistent are attention head activations across different datasets when predicting the same token?
  - Basis in paper: [inferred] from the discussion of the "interpretability illusion" and the authors' use of The Pile dataset to mitigate this issue
  - Why unresolved: The paper acknowledges that neurons can exhibit different activation patterns across datasets, but only uses one diverse dataset (The Pile) for their experiments
  - What evidence would resolve it: Testing the same attention heads on multiple distinct datasets and comparing activation patterns for consistent prediction of the same tokens

- Question: Do attention heads that predict the same token work independently or cooperatively with each other?
  - Basis in paper: [explicit] from the ablation study section which notes that "similar heads active in the same contexts" might explain why token probability decreases are relatively small
  - Why unresolved: The ablation study shows statistically significant but small decreases in token probability, suggesting either redundancy or cooperation among heads
  - What evidence would resolve it: Systematic ablation studies removing multiple heads simultaneously and measuring synergistic effects on token prediction accuracy

- Question: Does the relationship between attention heads and next-token neurons extend beyond the max-activating range of neuron behavior?
  - Basis in paper: [explicit] from the Limitations section which states "we focused solely on the max-activating behavior of neurons"
  - Why unresolved: The methodology specifically targets max-activating prompts, leaving open whether similar relationships exist at lower activation levels
  - What evidence would resolve it: Analyzing attention head activity across the full spectrum of neuron activation levels, not just the maximum range

## Limitations

- The methodology's heavy reliance on GPT-4 for explanation generation and validation introduces potential biases and limitations in the interpretability framework
- The approach may not capture the full complexity of transformer behavior, focusing primarily on attention-MLP interactions for next-token prediction
- The quality of explanations depends on GPT-4's performance, which may vary across different prompt types and contexts

## Confidence

**High confidence**: The core methodology for identifying next-token neurons through congruence scores and attributing attention head activity via dot product calculations is well-founded and reproducible.

**Medium confidence**: The automated explanation generation using GPT-4 and the head explanation score as a metric for evaluation are innovative but depend on the quality and consistency of GPT-4's outputs.

**Low confidence**: The claim that this methodology provides a comprehensive understanding of attention-MLP interactions in transformers is overstated, given the limitations and dependencies identified.

## Next Checks

1. **Cross-model validation**: Apply the methodology to a diverse set of transformer architectures (e.g., BERT, LLaMA, Code models) and task types to assess generalizability. Compare head explanation scores across different architectures and identify patterns that are consistent versus model-specific.

2. **Human evaluation study**: Conduct a rigorous human evaluation of the GPT-4 generated explanations. Have domain experts rate explanation quality, faithfulness to actual behavior, and usefulness for understanding attention head functionality. Compare these ratings to the automated head explanation scores to validate the scoring methodology.

3. **Causal intervention analysis**: Design a series of controlled experiments that go beyond simple ablation. Use techniques like causal mediation analysis to quantify the exact contribution of each attention head to next-token neuron activation and subsequent token prediction. This would help establish the strength and nature of the causal relationships identified by the methodology.