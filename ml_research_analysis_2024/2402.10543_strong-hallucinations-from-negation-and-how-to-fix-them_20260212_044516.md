---
ver: rpa2
title: Strong hallucinations from negation and how to fix them
arxiv_id: '2402.10543'
source_url: https://arxiv.org/abs/2402.10543
tags:
- negation
- then
- probability
- logical
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates strong hallucinations in language models,\
  \ which arise from logical incoherence stemming from faulty representations of logical\
  \ operators, particularly negation. The authors prove that such hallucinations are\
  \ inevitable under standard LM frameworks and propose a novel hybrid approach called\
  \ \u039B, which treats negation as a constraint on the evolution of latent representations\
  \ rather than a simple token."
---

# Strong hallucinations from negation and how to fix them

## Quick Facts
- arXiv ID: 2402.10543
- Source URL: https://arxiv.org/abs/2402.10543
- Authors: Nicholas Asher; Swarnadeep Bhar
- Reference count: 40
- The paper proves strong hallucinations in LMs are inevitable from logical incoherence in negation representations and proposes Λ to fix them

## Executive Summary
This paper investigates a fundamental problem in language models: strong hallucinations arising from logical incoherence, particularly in handling negation. The authors prove that such hallucinations are inevitable under standard LM frameworks where negation is treated as just another token. They propose Λ, a novel hybrid approach that treats negation as a constraint on the evolution of latent representations rather than a simple token, ensuring logical consistency by reversing probabilities of relevant alternatives and enforcing coherence in continuations.

## Method Summary
The Λ approach modifies how language models handle negation by treating it as an operation over latent representations that constrains their evolution. It uses dynamic semantics to map input probability distributions over positive continuations to output distributions for negative continuations, ensuring logical consistency. The method requires only training on positive data, avoiding reliance on sparse negative examples. For NLI tasks, Λ infers labels for negated contexts using logical relations from positive data through entailment relationships and logical equivalences.

## Key Results
- Achieves 0% exact match errors in MKR tasks by ensuring logical consistency between positive and negative contexts
- Increases NLI accuracy by up to 91% on one dataset and 13% on another through improved handling of negation
- Demonstrates that logical coherence in LM representations can be achieved without training on negative examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Logical incoherence in LM representations of negation leads to strong hallucinations
- Mechanism: LMs treat negation as just another token, computing probabilities independently for positive and negative continuations, violating probability axioms and leading to inconsistent representations
- Core assumption: LMs assign probabilities to strings based on learned distributions over training corpora, not over propositions or truth values
- Evidence anchors:
  - [abstract] "we prove that they follow from an LM's computation of its internal representations for logical operators and outputs from those representations"
  - [section] "Proposition 1. Every LM ˆf whose outputs are governed by µ ˆf and Definition 1 must strongly hallucinate if either: (i) P x∈V n\{ϕ,¬ϕ} µ ˆf(x) > 0; (ii) µ ˆf assigns values to strings in V n that are logical truths or deductively valid reasoning steps"
  - [corpus] FMR scores 0.56-0.62 indicate moderate similarity to related work on negation and hallucination

### Mechanism 2
- Claim: Λ treats negation as a constraint on probability distribution evolution rather than a token
- Mechanism: Λ uses dynamic semantics to map input probability distributions over positive continuations to output distributions for negative continuations, ensuring logical consistency
- Core assumption: Continuations of a string σ have representations where the representation of σ is a subset of the representation of its continuation
- Evidence anchors:
  - [abstract] "negation is treated not as another element of a latent representation, but as an operation over an LM's latent representations that constrains how they may evolve"
  - [section] "Definition 5. Let A1 ⪯γ A2 with µ an LM. A2 is an admissible continuation of A1 relative to µ only if: (i) If {¬Ak} ∪ PA1 = PA2 with µ(Ak|A1) = α then γ = 1 − α"
  - [corpus] No direct corpus evidence, FMR scores suggest moderate relatedness

### Mechanism 3
- Claim: Λ enables inference of labels for negated contexts using logical relations from positive data
- Mechanism: Λ uses entailment relationships and logical equivalences to compute labels for (¬C, ¬h), (C, ¬h), and (¬C, h) configurations based on labels for (C, h), (P, h), and (h, C′) from positive data
- Core assumption: Entailment relations involving negation can be computed from entailment relations in positive contexts using logical equivalences
- Evidence anchors:
  - [abstract] "we increase accuracy by 13% on one NLI dataset and by 91% on another"
  - [section] "From the meaning of entailment and Definition 5, it follows that µ(E|C, ¬h) = 1 − µ(E|C, h). Thus, if the model predicts (C, h):E, it should predict (C, ¬h): ¬E"
  - [corpus] FMR scores 0.56-0.62 suggest moderate similarity to related NLI and negation work

## Foundational Learning

- Concept: Probability distributions over strings vs. propositions
  - Why needed here: Understanding why LMs assign inconsistent probabilities to logically related statements requires distinguishing between distributions over strings and distributions over truth values
  - Quick check question: If P(ϕ) = 0.7 for a statement ϕ, what should P(¬ϕ) equal in a logically consistent distribution?

- Concept: Dynamic semantics and embeddings
  - Why needed here: Λ's approach relies on treating logical operators as operations on embeddings, requiring understanding of how meaning representations can be transformed
  - Quick check question: In a dynamic semantics framework, what does it mean for one representation to be a subset of another?

- Concept: Logical relations in NLI tasks
  - Why needed here: The NLI experiments depend on understanding how entailment, contradiction, and neutrality relate when negation is involved
  - Quick check question: If C entails h, what is the relationship between C and ¬h?

## Architecture Onboarding

- Component map:
  - Input encoder (BERT/RoBERTa) → CLS representation → probability distribution over continuations
  - Λ constraint application layer → modified probability distribution → output predictions
  - Algorithm layer for NLI label inference based on positive data labels

- Critical path:
  1. Compute probability distribution over continuations from positive context using LM
  2. Apply Λ constraints to transform distribution for negative context
  3. Select top predictions from modified distribution
  4. For NLI: Use positive data labels and logical relations to infer labels for negated contexts

- Design tradeoffs:
  - Using only top-k predictions for Λ vs. full distribution
  - Precomputing logical relations vs. computing on-the-fly
  - Scope handling: full sentence negation vs. partial scope

- Failure signatures:
  - High exact match scores on both positive and negative contexts (indicates no understanding of negation)
  - Inconsistent predictions across similar contexts with negation
  - Poor performance on NLI tasks with negation despite good performance on positive data

- First 3 experiments:
  1. Test BERT/RoBERTa on simple yes/no questions with positive and negative contexts, measure exact match on both
  2. Apply Λ to MKR task, compare exact match scores before and after constraint application
  3. Test NLI performance on (C, h) pairs, then use Λ to infer labels for (¬C, h) and (¬C, ¬h) pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Λ approach be adapted to handle multi-step reasoning chains in longer text generation tasks?
- Basis in paper: [inferred] The paper discusses the limitations of Λ in handling longer reasoning chains and mentions the need for future work to apply Λ to multiple reasoning steps in LMs.
- Why unresolved: The current Λ approach is demonstrated on tasks with short strings, and the paper acknowledges that tasks with long string output require complex evaluations that are beyond the current scope.
- What evidence would resolve it: Experimental results showing the effectiveness of Λ on tasks involving multi-step reasoning chains in longer text generation, along with a detailed analysis of the challenges and solutions.

### Open Question 2
- Question: How can pragmatic constraints be integrated into the Λ framework to ensure the generation of contextually relevant and appropriate continuations?
- Basis in paper: [inferred] The paper suggests that while Λ can generate high-probability strings, these may not always be pragmatically relevant or appropriate, and mentions the need to explore how to supplement and refine the distribution with pragmatic constraints.
- Why unresolved: The current Λ approach focuses on logical coherence but does not address the pragmatic appropriateness of generated continuations.
- What evidence would resolve it: Development and testing of a method to incorporate pragmatic constraints into Λ, along with empirical results demonstrating improved pragmatic relevance and appropriateness of generated continuations.

### Open Question 3
- Question: How can the Λ approach be extended to handle the complex interactions between scopes of multiple logical operators?
- Basis in paper: [explicit] The paper mentions that Λ does not capture the complex interactions between scopes of multiple logical operators and did not train models to do scoping.
- Why unresolved: The current Λ approach does not fully address the nuances of logical operator scope, which can lead to inaccuracies in NLI tasks.
- What evidence would resolve it: Experimental results showing the effectiveness of Λ on tasks involving multiple logical operators with varying scopes, along with a detailed analysis of the challenges and solutions in handling scope interactions.

## Limitations
- The theoretical proof relies on specific assumptions about probability distributions that may not capture the complexity of modern neural architectures
- The approach requires careful handling of negation scope and presupposition projection that may not generalize to all linguistic phenomena
- Reliance on top-k predictions rather than full distributions could miss important probability mass in edge cases

## Confidence
- **High Confidence**: The core claim that logical incoherence in LM representations of negation leads to strong hallucinations is well-supported by the theoretical proof and experimental results
- **Medium Confidence**: The Λ approach's effectiveness in specific tasks is well-demonstrated, but confidence is lower regarding its general applicability to all types of negation and logical operators
- **Low Confidence**: The assertion that Λ will completely eliminate strong hallucinations in all contexts is not fully supported

## Next Checks
1. **Generalization Test**: Apply Λ to a broader range of negation types and logical operators beyond the controlled datasets used in the paper, including naturally occurring negation in diverse text corpora
2. **Scalability Assessment**: Evaluate Λ's performance on larger language models (e.g., GPT-4, Claude) and different model architectures to determine if the approach scales effectively
3. **Robustness Analysis**: Conduct adversarial testing with carefully crafted examples designed to expose weaknesses in Λ's handling of negation scope and presupposition, including edge cases like embedded negations and contrastive contexts