---
ver: rpa2
title: 'Outlier-Oriented Poisoning Attack: A Grey-box Approach to Disturb Decision
  Boundaries by Perturbing Outliers in Multiclass Learning'
arxiv_id: '2411.00519'
source_url: https://arxiv.org/abs/2411.00519
tags:
- poisoning
- dataset
- attack
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel outlier-oriented poisoning (OOP) attack
  that manipulates labels of most distanced samples from decision boundaries in multiclass
  learning. The attack aims to degrade performance of various machine learning algorithms
  by exploiting outliers.
---

# Outlier-Oriented Poisoning Attack: A Grey-box Approach to Disturb Decision Boundaries by Perturbing Outliers in Multiclass Learning

## Quick Facts
- arXiv ID: 2411.00519
- Source URL: https://arxiv.org/abs/2411.00519
- Reference count: 40
- Primary result: Outlier-oriented poisoning attack degrades multiclass classifier performance, with KNN and Gaussian Naive Bayes most affected (22.81% and 56.07% accuracy drops on IRIS with 15% poisoning)

## Executive Summary
This paper introduces a novel grey-box poisoning attack that targets outliers in multiclass learning by manipulating the labels of data points most distant from decision boundaries. The attack exploits the vulnerability of non-parametric algorithms to boundary shifts caused by mislabeled outliers. Experiments on IRIS, MNIST, and ISIC datasets demonstrate significant performance degradation across various classifiers, with optimal poisoning rates identified at 10-15%. The analysis reveals that algorithm sensitivity, class count, and dataset balance all influence attack effectiveness.

## Method Summary
The attack follows a surrogate model approach where a model is trained on clean data to estimate decision boundaries. Distances from these boundaries are calculated for all data points, and the farthest points (outliers) are selected for label poisoning. The poisoned dataset is then used to retrain the target model, and performance is evaluated on clean test data across multiple metrics. The process is repeated at different poisoning levels (5%, 10%, 15%, 20%, 25%) to assess attack effectiveness.

## Key Results
- KNN and Gaussian Naive Bayes are most vulnerable, with accuracy drops of 22.81% and 56.07% respectively on IRIS with 15% poisoning
- Decision Trees and Random Forests show highest resilience to the attack
- Performance degradation is inversely proportional to the number of classes in the dataset
- Imbalanced datasets exacerbate poisoning effects, acting as catalysts for attack success
- Optimal poisoning rates are found to be 10-15%, with higher rates showing diminishing returns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Poisoning distant outliers shifts decision boundaries more effectively than poisoning nearby points.
- Mechanism: By targeting data points maximally distant from decision boundaries, the attack introduces mislabeled samples that lie in regions where the classifier has low confidence. This forces the model to adjust its boundary to accommodate these mislabeled points, degrading overall classification accuracy.
- Core assumption: Outliers are more influential in boundary shifts because they are less constrained by existing class density.
- Evidence anchors:
  - [abstract] "manipulates labels of most distanced samples from the decision boundaries"
  - [section 4] "We are focusing on manipulating labels of most distanced data points from the class boundaries to shift the classification predictions."
  - [corpus] No direct match; corpus focuses on poisoning broadly, not outlier-specific methods.
- Break condition: If the surrogate model fails to correctly estimate distances from decision boundaries, the attack cannot reliably identify outliers to poison.

### Mechanism 2
- Claim: Non-parametric algorithms (KNN, GNB) are more sensitive to outlier poisoning than parametric ones (SVM, RF).
- Mechanism: Non-parametric models rely directly on data distribution and local proximity for classification. Poisoning distant outliers distorts these relationships, causing misclassification. Parametric models have fixed assumptions and regularization that provide some resilience.
- Core assumption: The attack's effectiveness is inversely related to the algorithm's reliance on local data structure.
- Evidence anchors:
  - [abstract] "KNN and GNB are the most affected algorithms with a decrease in accuracy of 22.81% and 56.07% while increasing false positive rate to 17.14% and 40.45% for IRIS dataset with 15% poisoning."
  - [section 5.3.1] "Our findings indicate that the KNN algorithm was particularly vulnerable, experiencing the most significant accuracy disruption... This vulnerability stems from KNN being a non-parametric algorithm that relies on the proximity of data points."
  - [corpus] No direct match; corpus focuses on general poisoning, not algorithm-specific sensitivity.
- Break condition: If the poisoned dataset becomes too noisy, even parametric models may degrade significantly, reducing the observed difference.

### Mechanism 3
- Claim: Fewer dataset classes lead to greater performance degradation under poisoning.
- Mechanism: With fewer classes, each class occupies a larger portion of the feature space, making outlier manipulation more impactful per class. More classes distribute the effect across more boundaries, reducing the impact per boundary.
- Core assumption: The relative impact of boundary shifts is inversely proportional to the number of classes.
- Evidence anchors:
  - [abstract] "Our analysis highlighted that number of classes are inversely proportional to the performance degradation, specifically the decrease in accuracy of the models, which is normalized with increasing number of classes."
  - [section 5.3.2] "An inverse relationship was observed between the number of classes in the dataset and the rate of performance degradation."
  - [corpus] No direct match; corpus focuses on general poisoning, not class count effects.
- Break condition: If the dataset is perfectly balanced and the poisoning is uniformly distributed, the class count effect may be less pronounced.

## Foundational Learning

- Concept: Decision boundary distance calculation in multiclass classifiers.
  - Why needed here: The attack relies on identifying outliers based on their distance from decision boundaries, which requires understanding how different algorithms compute these distances.
  - Quick check question: How does a KNN classifier determine the distance of a point from a decision boundary compared to an SVM?

- Concept: Non-parametric vs. parametric machine learning algorithms.
  - Why needed here: The attack's effectiveness varies significantly between these algorithm types, so understanding their fundamental differences is crucial.
  - Quick check question: What are the key assumptions of Gaussian Naive Bayes that make it vulnerable to outlier manipulation?

- Concept: Multiclass classification evaluation metrics (precision, recall, FPR, accuracy).
  - Why needed here: The attack's impact is measured across multiple metrics, requiring understanding of how each metric reflects different aspects of model performance degradation.
  - Quick check question: Why might a high false positive rate indicate successful poisoning even if accuracy remains relatively stable?

## Architecture Onboarding

- Component map: Surrogate model trainer -> Distance calculator -> Outlier selector -> Label manipulator -> Performance evaluator
- Critical path: Surrogate model → Distance calculation → Outlier selection → Label poisoning → Performance evaluation
- Design tradeoffs:
  - Accuracy vs. stealth: Higher poisoning rates increase effectiveness but risk detection
  - Computation vs. precision: More accurate distance calculations require more computation
  - Generalization vs. specificity: Attack designed for multiclass may be less effective on binary
- Failure signatures:
  - Surrogate model distance estimates diverge from target model
  - Poisoned points cluster in specific regions rather than dispersing
  - Performance metrics show inconsistent degradation across classes
- First 3 experiments:
  1. Implement surrogate model training and distance calculation on IRIS dataset, verify distance estimates match expected patterns
  2. Add label manipulation for top 5% farthest points, measure impact on KNN accuracy
  3. Scale to MNIST dataset, compare poisoning effectiveness between 3-class and 10-class scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the OOP attack vary across different multiclass classification algorithms when the number of classes in the dataset increases?
- Basis in paper: [explicit] The paper states "Our analysis highlighted that number of classes are inversely proportional to the performance degradation" and shows results for datasets with 3, 4, and 10 classes.
- Why unresolved: While the paper demonstrates this inverse relationship, it does not quantify the exact rate of degradation reduction per additional class or provide a mathematical model for this relationship.
- What evidence would resolve it: Experiments measuring performance degradation rates across datasets with varying numbers of classes (e.g., 2, 3, 5, 10, 20) would establish the exact relationship and enable modeling.

### Open Question 2
- Question: What is the optimal poisoning rate that balances attack effectiveness with stealth, considering both performance degradation and anomaly detection?
- Basis in paper: [explicit] The paper identifies 10-15% as optimal poisoning rates but notes that rates above 15% show "impractical success" and mentions the need to avoid overfitting.
- Why unresolved: The paper does not investigate the trade-off between poisoning effectiveness and detectability, nor does it explore whether lower poisoning rates (e.g., 5%) could be more effective when combined with sophisticated attack strategies.
- What evidence would resolve it: Comparative studies measuring both attack success rates and anomaly detection rates across various poisoning levels (0-25%) using different detection methods would clarify this trade-off.

### Open Question 3
- Question: How does dataset imbalance specifically affect the OOP attack's ability to penetrate decision boundaries compared to balanced datasets?
- Basis in paper: [explicit] The paper states "imbalanced dataset distribution can aggravate the impact of poisoning" and shows that imbalanced datasets "work as catalysts" for poisoning effects.
- Why unresolved: The paper provides qualitative observations about imbalanced datasets enhancing poisoning effects but does not quantify the specific mechanisms or provide metrics on how imbalance affects attack success versus balanced datasets.
- What evidence would resolve it: Controlled experiments comparing OOP attack success rates on balanced versus imbalanced versions of the same dataset, with varying degrees of imbalance, would quantify this relationship.

## Limitations

- The paper lacks detailed methodology for calculating decision boundary distances across different algorithms, particularly for non-linear classifiers like Random Forests and neural networks
- Limited evaluation to only three datasets (one small, one medium, one small medical dataset) may not generalize to all multiclass scenarios
- No comparison with state-of-the-art poisoning attacks or defense mechanisms
- The surrogate model approach assumes the attack has access to a similar model architecture, which may not hold in real-world scenarios

## Confidence

- High Confidence: The inverse relationship between class count and poisoning effectiveness (Mechanism 3) is well-supported by experimental results across all datasets
- Medium Confidence: Algorithm-specific sensitivity (Mechanism 2) is demonstrated but could benefit from additional algorithm types and larger-scale experiments
- Medium Confidence: The core mechanism of poisoning distant outliers (Mechanism 1) is theoretically sound but the implementation details for distance calculation remain unclear

## Next Checks

1. Implement and verify the decision boundary distance calculation method for KNN and SVM on the IRIS dataset, comparing results with expected theoretical distances
2. Test the attack's effectiveness on a binary classification problem to validate the claim that fewer classes lead to greater degradation
3. Evaluate the attack's stealthiness by measuring detection rates using anomaly detection methods on the poisoned datasets