---
ver: rpa2
title: 'MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language
  Models'
arxiv_id: '2401.16745'
source_url: https://arxiv.org/abs/2401.16745
tags:
- multi-turn
- task
- performance
- dialogue
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MT-Eval is a new benchmark for evaluating multi-turn conversational
  abilities of large language models. It categorizes interaction patterns into four
  types: recollection, expansion, refinement, and follow-up.'
---

# MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models

## Quick Facts
- **arXiv ID**: 2401.16745
- **Source URL**: https://arxiv.org/abs/2401.16745
- **Reference count**: 26
- **Key outcome**: MT-Eval is a new benchmark for evaluating multi-turn conversational abilities of large language models across four interaction patterns, revealing significant performance degradation in multi-turn settings that is uncorrelated with fundamental model capabilities.

## Executive Summary
MT-Eval introduces a comprehensive benchmark for evaluating multi-turn conversational abilities of large language models across four interaction patterns: recollection, expansion, refinement, and follow-up. The benchmark includes 1170 queries across 168 dialogues and reveals that most models perform significantly worse in multi-turn settings compared to single-turn settings, with performance gaps uncorrelated with fundamental model capabilities. Key factors identified as influencing multi-turn performance include distance to relevant content and susceptibility to error propagation.

## Method Summary
The authors constructed MT-Eval by augmenting existing datasets and creating new examples with GPT-4, resulting in 1170 queries across 168 dialogues covering four interaction patterns. They evaluated 11 models including GPT-4, GPT-3.5-Turbo, Vicuna, Llama-2, Qwen, Mistral, and Mixtral in both single-turn and multi-turn settings. Performance was measured using GPT-4 with chain-of-thought evaluation prompts, scoring responses from 1-10. The study compared performance across settings, analyzed failure modes, and conducted ablation studies to identify key factors affecting multi-turn performance.

## Key Results
- Most models show significant performance degradation in multi-turn versus single-turn settings, with performance gaps uncorrelated with fundamental model capabilities
- Distance to relevant content and susceptibility to error propagation are identified as key factors influencing multi-turn performance
- Llama-2-chat models outperform Vicuna in single-turn settings but noticeably lag behind in multi-turn dialogues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance degradation in multi-turn settings is not correlated with fundamental model capabilities
- Mechanism: The ability to maintain coherent dialogue across turns involves additional cognitive load beyond single-turn task performance
- Core assumption: Models that excel at single-turn tasks may still struggle with context retention and error propagation in multi-turn settings
- Evidence anchors:
  - [abstract]: "We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities."
  - [section 4.4]: "While Llama-2-chat models outperform Vicuna models in the single-turn setting, they noticeably lag behind in multi-turn dialogues."
  - [corpus]: Weak evidence - corpus neighbors discuss multi-turn limitations but don't specifically address uncorrelated performance gaps
- Break condition: If future models demonstrate strong correlation between single-turn and multi-turn performance, this mechanism would need revision

### Mechanism 2
- Claim: Increasing distance to relevant content negatively impacts multi-turn performance
- Mechanism: As the number of turns increases between a relevant context element and current query, the probability of losing track of that context increases
- Core assumption: LLMs have limited context window effectiveness that degrades with distance from key information
- Evidence anchors:
  - [abstract]: "we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance"
  - [section 4.4]: "Our study reveals that LLMs often underperform in tasks requiring information from earlier dialogue turns"
  - [corpus]: Weak evidence - corpus neighbors mention "cumulative contextual decay" but don't specifically address distance effects
- Break condition: If models with larger context windows or better attention mechanisms show no distance-related degradation, this mechanism would need revision

### Mechanism 3
- Claim: Error propagation causes compounding performance degradation in multi-turn dialogues
- Mechanism: Initial errors in earlier turns create a cascading effect where subsequent responses are built on incorrect foundations
- Core assumption: LLMs are highly sensitive to the quality of their input context and cannot effectively recover from earlier mistakes
- Evidence anchors:
  - [abstract]: "susceptibility to error propagation as the key factors influencing multi-turn performance"
  - [section 4.5]: "Accumulated errors from preceding dialogue turns often confuse the models, leading to more incorrect responses"
  - [section 4.6]: "Table 7 presents the results of three dialogue tasks, conditioned on dialogue history of self-generated responses (i.e. the main results) or gold responses from human verified GPT-4 outputs"
- Break condition: If models develop robust error-correction mechanisms or become less sensitive to context quality, this mechanism would need revision

## Foundational Learning

- Concept: Context window and attention mechanisms
  - Why needed here: Understanding how LLMs process and maintain information across turns is crucial for interpreting performance differences
  - Quick check question: If a model has a context window of 4096 tokens and each turn averages 100 tokens, how many turns can it theoretically maintain before earlier information becomes inaccessible?

- Concept: In-context learning and few-shot prompting
  - Why needed here: The paper discusses how dialogue history serves as in-context examples, affecting model performance
  - Quick check question: How does the quality of in-context examples affect a model's ability to follow instructions in subsequent turns?

- Concept: Chain-of-thought reasoning in evaluation
  - Why needed here: The paper uses GPT-4 with chain-of-thought prompting for evaluation, which affects the reliability of results
  - Quick check question: Why might chain-of-thought prompting improve the consistency of LLM-based evaluation compared to direct scoring?

## Architecture Onboarding

- Component map: Data collection pipeline (human-LLM conversations → pattern identification → task construction) -> Model evaluation framework (single-turn vs multi-turn comparison) -> Error analysis system (identifying types of failures) -> Ablation study framework (testing specific factors)

- Critical path: Construct multi-turn benchmark → Evaluate models in both settings → Analyze performance gaps → Identify contributing factors → Validate findings through ablation studies

- Design tradeoffs:
  - Using GPT-4 for evaluation vs human evaluation (scalability vs reliability)
  - Constructing new data vs using existing datasets (data leakage risk vs representativeness)
  - Single-turn vs multi-turn evaluation (comprehensive assessment vs practical constraints)

- Failure signatures:
  - Performance drop specifically in tasks requiring long-distance context retrieval
  - Consistent failure to follow global instructions across multiple turns
  - Accumulation of errors from earlier turns affecting later performance

- First 3 experiments:
  1. Compare performance of same model on single-turn vs multi-turn versions of identical queries
  2. Test whether providing gold responses as context improves performance (testing error propagation hypothesis)
  3. Measure performance degradation as a function of distance from relevant context

## Open Questions the Paper Calls Out

## Question 1: Impact of Model Size on Multi-Turn Performance
- **Question:** Does increasing the size of open-source models improve their multi-turn conversational performance to match or surpass that of closed-source models?
- **Basis in paper:** [inferred] The paper mentions that larger models like Llama2-chat-70B were not included due to computational limits and suggests further studies could investigate whether larger LLMs exhibit similar findings.
- **Why unresolved:** The current study only evaluates models up to 13B parameters, leaving the performance of larger models untested in multi-turn settings.
- **What evidence would resolve it:** Evaluating the performance of larger open-source models (e.g., 70B+ parameters) in the MT-Eval benchmark would provide insights into whether model size correlates with improved multi-turn capabilities.

## Question 2: Role of Dialogue History in Multi-Turn Interactions
- **Question:** How does the inclusion of detailed dialogue history as in-context examples affect the performance of LLMs in complex multi-turn interactions?
- **Basis in paper:** [explicit] The paper discusses the impact of dialogue history as in-context examples in the Follow-up task and notes that models perform significantly better with dialogue history.
- **Why unresolved:** While the paper shows that dialogue history improves performance, it does not fully explore the extent to which detailed and structured dialogue history can enhance model responses.
- **What evidence would resolve it:** Conducting experiments that systematically vary the amount and quality of dialogue history provided to models could elucidate its role in improving multi-turn interactions.

## Question 3: Error Propagation Mitigation Strategies
- **Question:** What strategies can be implemented to reduce error propagation in multi-turn dialogues, thereby improving overall model performance?
- **Basis in paper:** [explicit] The paper identifies error propagation as a key factor affecting multi-turn performance and suggests that accumulated errors from earlier turns lead to incorrect responses.
- **Why unresolved:** The paper identifies the problem but does not propose or test specific strategies to mitigate error propagation.
- **What evidence would resolve it:** Implementing and testing error correction mechanisms or models that can dynamically adjust based on accumulated errors would provide insights into effective mitigation strategies.

## Limitations

- Benchmark construction relies heavily on synthetic data generated with GPT-4, which may not fully capture the diversity and complexity of real human conversations
- Study focuses on English-language dialogues, limiting applicability to other languages or cultural contexts
- Use of GPT-4 for evaluation introduces potential biases, as evaluation models may share similar limitations to the evaluated models

## Confidence

- **High confidence**: The observation that most models perform worse in multi-turn versus single-turn settings is well-supported by consistent experimental results across multiple model families and architectures.
- **Medium confidence**: The claim that performance gaps are uncorrelated with fundamental model capabilities is supported by comparative analysis showing no clear relationship between single-turn excellence and multi-turn performance, though this could benefit from additional validation across more diverse model architectures.
- **Medium confidence**: The identification of distance to relevant content and error propagation as key factors is supported by ablation studies and comparative analysis, though the relative importance of these factors and their interaction effects could be more precisely quantified.

## Next Checks

1. **Cross-linguistic validation**: Evaluate MT-Eval across non-English languages to test whether observed performance patterns and mechanisms generalize beyond English-language dialogues and whether certain languages show different susceptibility to multi-turn degradation.

2. **Human evaluation correlation**: Conduct a small-scale human evaluation study to measure correlation between GPT-4-based evaluation scores and human judgments of multi-turn dialogue quality, particularly focusing on cases where GPT-4 scores indicate significant performance gaps.

3. **Architecture ablation study**: Systematically test whether architectural modifications specifically targeting context retention (such as attention mechanisms, context window size, or episodic memory) can reduce the identified performance gaps, distinguishing between architectural limitations versus fundamental reasoning capabilities.