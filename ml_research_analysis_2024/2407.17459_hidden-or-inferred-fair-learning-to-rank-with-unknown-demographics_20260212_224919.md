---
ver: rpa2
title: 'Hidden or Inferred: Fair Learning-To-Rank with Unknown Demographics'
arxiv_id: '2407.17459'
source_url: https://arxiv.org/abs/2407.17459
tags:
- inference
- fairness
- fair
- protected
- ndcg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how errors in demographic inference impact
  the fairness performance of fair learning-to-rank (LTR) strategies. It examines
  seven strategies ranging from fair LTR with inferred demographics to fairness-unaware
  LTR followed by fair re-ranking.
---

# Hidden or Inferred: Fair Learning-To-Rank with Unknown Demographics

## Quick Facts
- arXiv ID: 2407.17459
- Source URL: https://arxiv.org/abs/2407.17459
- Reference count: 40
- Key outcome: Re-ranking strategies are more robust to demographic inference errors than fair LTR models, with fair re-ranking maintaining better fairness metrics as inference noise increases.

## Executive Summary
This paper investigates how errors in demographic inference impact the fairness performance of fair learning-to-rank (LTR) strategies. The study examines seven strategies ranging from fair LTR with inferred demographics to fairness-unaware LTR followed by fair re-ranking. Using controlled experiments with systematic perturbation of inferred attributes and real-world datasets with popular inference tools, the research finds that re-ranking strategies are more robust to inference errors than fair LTR models. Specifically, as inference noise increases, fair LTR models may increase bias while fair re-ranking maintains better fairness metrics. The study also reveals that fair strategies achieve better fairness than oblivious models even with inaccurate inference, up to about 10% error rate.

## Method Summary
The study evaluates seven LTR strategies across four real-world datasets using both controlled systematic perturbation and real-world inference services. Models are trained with/without protected attributes, and inference errors are simulated at rates from 0-100% in 10% increments. Fairness is measured using Dadv/Adv exposure ratio and NDKL metrics, while utility is measured with NDCG. The experimental pipeline involves data preprocessing, model training, inference with controlled perturbations, ranking, optional re-ranking, and metric computation. The study systematically varies inference error rates to analyze how different strategies respond to increasing noise in demographic information.

## Key Results
- Fair re-ranking strategies (e.g., DetConstSort) are more robust to demographic inference errors than fair LTR models (e.g., DELTR)
- Inference errors can improve fairness for fairness-unaware models by misclassifying disadvantaged candidates as advantaged
- Hiding demographic attributes during testing performs worse than using inaccurate inference up to ~10% error threshold
- Fair strategies achieve better fairness than oblivious models even with inference errors up to about 10% error rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair re-ranking strategies are more robust to demographic inference errors than fair LTR models.
- Mechanism: Fair re-ranking operates on already scored lists and enforces proportional group representation at cutoff positions, so incorrect group labels still yield correct group proportions and maintain fairness. In contrast, fair LTR adjusts scores based on inferred protected attributes, and incorrect inferences reinforce bias through these adjustments.
- Core assumption: The re-ranking algorithm relies only on relative group proportions, not on accuracy of individual group assignments.
- Evidence anchors: Abstract shows fair re-ranking is more robust; section 5.1 demonstrates that LTR+FAIR RR counteracts score increases/decreases from wrong protected attributes when group proportions change.

### Mechanism 2
- Claim: Inference errors can improve fairness for fairness-unaware models.
- Mechanism: In fairness-unaware models, demographic attributes act as features that influence scores. Misclassifying a disadvantaged candidate as advantaged applies the model's higher-scoring coefficients for the advantaged group, raising that candidate's score and exposure.
- Core assumption: The ranking model uses demographic attributes as input features and assigns higher average scores to one group over the other.
- Evidence anchors: Section 5.1 shows group fairness emerges as a byproduct of erroneous inferences; for ε ≥ 30%, the advantaged group gets less exposure due to reversed disparity.

### Mechanism 3
- Claim: Hiding demographics performs worse than using inaccurate inference up to ~10% error threshold.
- Mechanism: Hiding removes any direct effect of demographic attributes from the model, preventing group-aware adjustments for fairness. With low inference error, inferred attributes still allow some group-aware adjustments that improve fairness compared to hiding.
- Core assumption: The ranking model benefits from demographic information for fairness when that information is at least partially accurate.
- Evidence anchors: Abstract states it's better to utilize a fairness-aware model with inferred attributes than to ignore them up to 10% error; section 5.1 shows OBLIVIOUS has exposure ratio farther from 1.0 than fair interventions for up to 15-20% error.

## Foundational Learning

- Concept: Learning-to-rank (LTR) and its fairness extensions
  - Why needed here: The paper compares multiple LTR strategies (fair/unaware, with/without demographics, with re-ranking), so understanding LTR basics is prerequisite to grasping the experimental setup and results.
  - Quick check question: What is the difference between a fairness-unaware LTR model and a fairness-aware LTR model in terms of training objective?

- Concept: Demographic inference error modeling
  - Why needed here: The study simulates inference errors in two ways—systematic perturbation of known attributes and use of third-party inference services—so knowing how error rates are defined and measured is key to interpreting results.
  - Quick check question: In the controlled study, how is the error parameter ε defined and applied?

- Concept: Fairness metrics (DAdv/Adv exposure ratio, NDKL) and utility metric (NDCG)
  - Why needed here: Results are evaluated using these metrics; understanding their definitions and what values indicate good/bad fairness/utility is essential for assessing the findings.
  - Quick check question: What does an exposure ratio of 1.0 signify in the context of the Dadv/Adv exposure ratio metric?

## Architecture Onboarding

- Component map: Data preprocessing -> Train/test split -> Model training (fair/unaware, with/without demographics) -> Inference (controlled perturbation or real service) -> Ranking -> (Optional) Re-ranking -> Metric computation
- Critical path: Train model -> Generate rankings with inferred/hidden demographics -> Apply fair re-ranker if applicable -> Compute fairness/utility metrics
- Design tradeoffs: Fair LTR requires demographic attributes at training and inference; fair re-ranking can work with inferred attributes only at inference; hiding avoids inference but may hurt fairness; re-ranking adds post-processing overhead but is more robust to inference errors.
- Failure signatures: High variance in fairness metrics across repeated runs indicates sensitivity to inference errors; convergence issues in DELTR training suggest poor parameter choices; DetConstSort producing rankings identical to input suggests group proportion handling is flawed.
- First 3 experiments:
  1. Train ListNet without demographics; test on data with ground-truth demographics hidden -> measure fairness/utility baseline.
  2. Train ListNet with demographics; test on data with 10% systematic inference error -> measure impact on fairness/utility.
  3. Train DELTR with demographics; test on data with 10% inference error -> compare fairness/utility to experiment 2.

## Open Questions the Paper Calls Out
The paper identifies several open questions including how fair LTR strategies perform with multi-valued demographic attributes, the optimal balance point between inference accuracy and fairness-utility trade-off, and how demographic inference errors affect individual fairness outcomes in addition to group fairness.

## Limitations
- Conclusions primarily based on controlled experiments with synthetic inference errors rather than real-world error patterns
- Focus on binary gender limits generalizability to multi-valued demographic attributes
- Results evaluated on four specific datasets that may not represent production system diversity
- Limited to two specific re-ranking algorithms, restricting generalizability to other fair ranking approaches

## Confidence
- **High confidence**: The comparative robustness of fair re-ranking versus fair LTR under increasing inference errors
- **Medium confidence**: The claim that hiding demographics performs worse than using inaccurate inference up to ~10% error
- **Medium confidence**: The finding that inference errors can improve fairness in fairness-unaware models

## Next Checks
1. Test the same seven strategies using inference error patterns from actual deployed inference services rather than systematic uniform perturbation
2. Evaluate how inference error rates and patterns change over time with updated training data for inference models, and whether the robustness hierarchy remains consistent
3. Replicate the experiments with datasets containing more than two demographic groups to verify whether observed robustness patterns hold with complex group proportions