---
ver: rpa2
title: Learning payoffs while routing in skill-based queues
arxiv_id: '2412.10168'
source_url: https://arxiv.org/abs/2412.10168
tags:
- algorithm
- action
- payoff
- lemma
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses a learning problem in skill-based queueing\
  \ systems with customer\u2013server dependent payoffs. The goal is to adaptively\
  \ learn the unknown payoff parameters while routing customers to maximize the long-term\
  \ expected reward rate."
---

# Learning payoffs while routing in skill-based queues

## Quick Facts
- arXiv ID: 2412.10168
- Source URL: https://arxiv.org/abs/2412.10168
- Authors: Sanne van Kempen; Jaron Sanders; Fiona Sloothaak; Maarten G. Wolf
- Reference count: 40
- Key outcome: Polylogarithmic regret upper bound of O(ln^β(t)) for any β > 1 for the UCB QR algorithm in skill-based queueing systems

## Executive Summary
This paper addresses the problem of online learning and routing in skill-based queueing systems with unknown customer-server dependent payoffs. The authors propose the UCB QR algorithm that leverages basic feasible solutions of a static linear program as actions in a multi-armed bandit framework. The algorithm uses Upper Confidence Bounds for payoff parameters and routes customers according to the chosen action's rates. Theoretical analysis establishes both a regret lower bound of Ω(ln(t)) and a polylogarithmic regret upper bound for the proposed algorithm. Numerical experiments demonstrate the algorithm's effectiveness in both small and large systems, showing quick convergence to optimal reward rates and robustness against time-varying parameters.

## Method Summary
The UCB QR algorithm operates in episodes, selecting actions based on Upper Confidence Bounds of unknown payoff parameters. Each episode begins by choosing the action (basic feasible solution of a static LP) with the highest UCB index. Customers are then routed according to this action's rates using a First-Come-First-Served with Randomized Routing policy. At episode end, observed payoff samples update the UCB indices for future selections. The regret is decomposed into queueing and learning components, with each bounded separately using concentration inequalities and convergence analysis. The algorithm handles virtual queues and customer reallocation when actions change between episodes, ensuring stability constraints are maintained throughout operation.

## Key Results
- Establishes an asymptotic regret lower bound of Ω(ln(t)) for a class of routing policies satisfying stability constraints
- Proves a polylogarithmic regret upper bound of O(ln^β(t)) for any β > 1 for the UCB QR algorithm
- Demonstrates convergence to optimal reward rate in both small and large queueing systems through numerical experiments
- Shows robustness against time-varying payoff parameters, with the algorithm adapting to changing optimal actions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The UCB QR algorithm learns payoff parameters efficiently by leveraging basic feasible solutions of a static LP as actions in an MAB setting.
- Mechanism: At each episode start, the algorithm selects the action (basic feasible solution) with the highest UCB index, routes customers according to the action's rates during the episode, and updates UCB indices using observed payoff samples at episode end. This allows simultaneous exploration-exploitation and adapts to queue dynamics.
- Core assumption: The finite set of basic feasible solutions forms a finite action space suitable for MAB techniques, and the queue length process converges to stationary behavior within episodes.
- Evidence anchors:
  - [abstract] "The algorithm leverages the basic feasible solutions of a static linear program as the action space."
  - [section 4.1] "We consider the basic feasible solutions of LP(θ,ε) in (10) as the different actions in an MAB setting."
  - [corpus] Weak evidence: no neighboring papers explicitly confirm LP-based action spaces in queueing MABs.
- Break condition: If the queue length process fails to converge quickly enough, insufficient payoff samples are collected, breaking the learning rate guarantees.

### Mechanism 2
- Claim: The algorithm bounds regret by decomposing it into queueing and learning components and bounding each separately.
- Mechanism: Regret decomposition separates losses from suboptimal routing (learning regret) and from deviating from oracle routing rates due to queue dynamics (queueing regret). The learning regret is bounded using concentration inequalities on UCB index overestimation, while queueing regret is bounded using convergence analysis of the queue length process to its stationary measure.
- Core assumption: Stability constraints (7)-(8) ensure queue lengths remain finite, and the warmup period in episodes ensures sufficient convergence to stationarity.
- Evidence anchors:
  - [section 3.1] "Lemma 5 presents a regret decomposition based on the line classification."
  - [section 4.2] "We split the regret into a queueing and a learning component."
  - [corpus] No direct neighboring evidence; this decomposition approach is inferred from related MAB and queueing literature.
- Break condition: If stability constraints are violated or convergence to stationarity is too slow, the regret bounds no longer hold.

### Mechanism 3
- Claim: Transfer learning occurs because rewards of actions are related through shared unknown payoff parameters, allowing efficient inference across actions.
- Mechanism: The UCB index of an action is computed using UCB indices of its constituent lines. This means observing departures in one episode (under one action) updates belief about payoffs for multiple actions, reducing the number of actions that need to be sampled directly.
- Core assumption: Actions share common payoff parameters, so payoff information transfers across actions.
- Evidence anchors:
  - [section 4.4.2] "Our regret bound in Theorem 1 is pessimistic: in analyzing the convergence of the UCB index Ua in (33), we only account for departures that are obtained in episodes where action a was chosen. However, we possibly observe type-(ij) departures as well in episodes where another action ˜a with x˜a ij > 0 was chosen."
  - [section 5.1] "Therefore, it suffices to only sample a subset of actions, as long as we observe sufficient payoff samples from all lines."
  - [corpus] No direct evidence in neighboring papers; this is a novel insight from the paper.
- Break condition: If the action space is too large or payoff parameters are too decoupled, transfer learning benefits diminish.

## Foundational Learning

- Concept: Upper Confidence Bounds (UCB) in multi-armed bandits
  - Why needed here: UCB balances exploration and exploitation in learning unknown payoff parameters while routing customers to maximize long-term reward.
  - Quick check question: How does the UCB index formula in (32) ensure that suboptimal actions are still explored enough to refine their payoff estimates?

- Concept: Linear programming duality and complementary slackness
  - Why needed here: The optimal routing problem is formulated as a linear program; duality and complementary slackness are used to characterize optimal actions, derive regret decompositions, and bound suboptimality gaps.
  - Quick check question: How does strict complementarity in the LP dual ensure that suboptimal lines have strictly positive suboptimality gaps?

- Concept: Queueing theory - stability and positive recurrence
  - Why needed here: Stability constraints ensure queue lengths remain finite; positive recurrence of the queue length process guarantees convergence to a stationary distribution, which is essential for the learning analysis.
  - Quick check question: Why does constraint (8) prevent critically loaded servers and how does this relate to positive recurrence?

## Architecture Onboarding

- Component map:
  - Episode start module -> Action selection module -> Routing module -> Queue management module -> Learning module -> Episode end module

- Critical path:
  1. Episode start: select action with max UCB index
  2. Route customers according to action's rates using FCFS RR
  3. Observe payoff samples upon service completions
  4. Episode end: update UCB indices using observed samples
  5. Repeat

- Design tradeoffs:
  - Episode length vs. learning speed: longer episodes may improve sample quality but slow adaptation
  - Action space size vs. computational complexity: more actions increase routing flexibility but slow decision-making
  - Exploration bonus magnitude vs. convergence speed: larger bonuses encourage exploration but may slow convergence

- Failure signatures:
  - Regret grows faster than polylogarithmic: indicates UCB index updates or convergence analysis is too loose
  - Queue lengths grow unbounded: indicates stability constraints (7)-(8) are violated
  - Algorithm gets stuck in suboptimal actions: indicates insufficient exploration or poor UCB index updates

- First 3 experiments:
  1. Implement UCB QR on the small example in Figure 7 with θ={0.4, 0.1, 0.3, 0.01}, verify convergence to optimal action 2 and payoff rate ~5.4
  2. Test robustness by changing θ12 from 0.1 to 0.5 at t=1000, verify algorithm adapts to new optimal action 6
  3. Scale up to larger system in Figure 13 with 88 actions, verify convergence to Oracle policy and reasonable queue lengths across scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the regret lower bound in Theorem 1 be extended to a class of policies satisfying weaker constraints than (7) and (8)?
- Basis in paper: [inferred] The authors mention in Section 3.2 that "It remains an open question whether a similar regret lower bound can be proven for a class of policies satisfying a weaker constraint such as lim t→∞Eθπ(txijθ−Dij(t))/tb = 0 for any b> 0 and (ij)∈L, in combination with assumptions (7) and (8)."
- Why unresolved: The authors do not provide a proof or counterexample for this weaker class of policies.
- What evidence would resolve it: A mathematical proof or counterexample demonstrating whether the regret lower bound holds for this weaker class of policies.

### Open Question 2
- Question: Can the regret bound in Theorem 2 be improved by exploiting the transfer learning effect mentioned in Section 4.4.2?
- Basis in paper: [explicit] The authors state "Our regret bound in Theorem 1 is pessimistic: in analyzing the convergence of the UCB index Ua in (33), we only account for departures that are obtained in episodes where action a was chosen. However, we possibly observe type-(ij) departures as well in episodes where another action â withxâij > 0 was chosen. This insight can be used in future work to improve the bound in Lemma 8 and in turn improve the regret bound in Theorem 2."
- Why unresolved: The authors do not provide a proof or numerical results demonstrating the improved regret bound.
- What evidence would resolve it: A mathematical proof or numerical experiments showing the improved regret bound when accounting for transfer learning.

### Open Question 3
- Question: How can the UCB QR algorithm be extended to handle time-varying payoff parameters more effectively?
- Basis in paper: [explicit] The authors mention in Section 5.2 that "We expect that robustness against changing parameter values can be improved by decreasing the episode length, or altering the empirical estimators in (31) by decreasing the weight of observations from the past."
- Why unresolved: The authors do not provide a concrete algorithm or theoretical analysis for handling time-varying parameters.
- What evidence would resolve it: A modified version of the UCB QR algorithm with theoretical guarantees or numerical experiments demonstrating improved performance against time-varying parameters.

## Limitations
- The action space grows exponentially with problem size, potentially becoming computationally intractable for very large systems
- Theoretical regret bounds depend on concentration inequalities that may be loose in practice
- Numerical experiments are limited to specific parameter regimes, leaving behavior in highly heterogeneous or adversarial environments unexplored

## Confidence
- **High confidence**: The core algorithmic framework leveraging LP basic feasible solutions as actions is well-founded and computationally implementable.
- **Medium confidence**: The regret decomposition and polylogarithmic bounds are mathematically rigorous, but may be conservative in practice.
- **Medium confidence**: Numerical experiments demonstrate convergence to optimal policies, but the parameter sensitivity and scalability to much larger systems need further validation.

## Next Checks
1. **Scalability test**: Implement UCB QR on a larger queueing system (e.g., I=5, J=5, L=20) and measure computational time for action selection and convergence behavior compared to smaller systems.
2. **Robustness to non-stationarity**: Modify the algorithm to handle time-varying payoff parameters (as suggested in Section 6.2) and test its performance on a system where θ12 changes periodically between 0.1 and 0.5.
3. **Sensitivity analysis**: Vary the exploration parameter α across multiple orders of magnitude and measure its impact on regret growth rate, convergence speed, and queue stability to identify the optimal tradeoff.