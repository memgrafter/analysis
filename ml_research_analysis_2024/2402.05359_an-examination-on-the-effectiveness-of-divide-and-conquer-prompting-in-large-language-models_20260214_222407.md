---
ver: rpa2
title: An Examination on the Effectiveness of Divide-and-Conquer Prompting in Large
  Language Models
arxiv_id: '2402.05359'
source_url: https://arxiv.org/abs/2402.05359
tags:
- prompting
- task
- large
- sub-task
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the divide-and-conquer prompting strategy for
  improving large language model performance on tasks with repetitive sub-tasks and
  deceptive content. The core idea is to disentangle task decomposition, sub-task
  resolution, and solution assembly into separate stages guided by a program rather
  than the LLM itself.
---

# An Examination on the Effectiveness of Divide-and-Conquer Prompting in Large Language Models

## Quick Facts
- arXiv ID: 2402.05359
- Source URL: https://arxiv.org/abs/2402.05359
- Reference count: 40
- Primary result: Divide-and-conquer prompting outperforms standard strategies on tasks with repetitive sub-tasks and deceptive content

## Executive Summary
This paper examines the divide-and-conquer prompting strategy for improving large language model performance on tasks with repetitive sub-tasks and deceptive content. The core idea is to disentangle task decomposition, sub-task resolution, and solution assembly into separate stages guided by a program rather than the LLM itself. Theoretical analysis shows this approach can expand the expressive power of fixed-depth transformers. Experiments on large integer multiplication, hallucination detection, and fact verification demonstrate that the divide-and-conquer method outperforms standard prompting strategies like Chain-of-Thought, Tree-of-Thoughts, and Least-to-Most.

## Method Summary
The divide-and-conquer prompting strategy involves three distinct stages: task decomposition where complex tasks are split into parallel sub-tasks, sub-task resolution where each sub-task is handled independently, and solution merge where sub-task results are combined into a final answer. The approach uses a control program to coordinate these stages and enforce disentanglement, preventing intermediate errors from propagating. The method was compared against baselines including IO prompting, Chain-of-Thought, Tree-of-Thoughts, Least-to-Most, and Decomposed Prompting across three tasks: large integer multiplication, hallucination detection, and fact verification.

## Key Results
- Outperforms standard prompting strategies (CoT, ToT, LtM) on large integer multiplication with higher accuracy and lower edit distance
- Better handles deceptive content in hallucination detection and fact verification tasks
- Theoretical analysis shows potential to expand transformer expressive power beyond TC0 complexity class

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating task decomposition, sub-task resolution, and solution assembly into distinct stages prevents intermediate errors from propagating.
- Mechanism: The disentangled-sub-process principle ensures that upstream processes only forward final answers to downstream processes, not intermediate steps or inputs. This isolation prevents errors in sub-task generation from corrupting the overall solution path.
- Core assumption: Intermediate errors in LLMs are primarily caused by entangled reasoning processes where sub-tasks influence each other's generation.
- Evidence anchors:
  - [abstract] "disentangles task decomposition, sub-task resolution, and solution assembly process"
  - [section] "every upstream sub-process (e.g. sub-task resolution) only forward the final answer to its downstream sub-processes (e.g. solution merge) and does not forward its input and intermediate steps"
  - [corpus] Weak - corpus neighbors don't directly address this specific mechanism
- Break Condition: When sub-tasks are not truly independent and require intermediate results from other sub-tasks

### Mechanism 2
- Claim: Parallel sub-task execution reduces susceptibility to deceptive content compared to sequential approaches.
- Mechanism: By handling sub-tasks in parallel rather than sequentially, the LLM doesn't follow the flow of potentially deceptive context, making it less prone to being influenced by misleading information.
- Core assumption: Sequential processing allows deceptive content to guide the LLM's reasoning path, while parallel processing maintains independence.
- Evidence anchors:
  - [abstract] "When tackling deceptive text like hallucination and fake news, such intertwined process often guides the LLM to sequentially tackle the corpus and thus follow the context's flow"
  - [section] "Such design makes the sub-problem generation process lack control from the human or agent side and susceptible to disruptions"
  - [corpus] Weak - corpus neighbors don't directly compare parallel vs sequential sub-task approaches
- Break Condition: When the task inherently requires sequential dependencies between sub-tasks

### Mechanism 3
- Claim: The divide-and-conquer approach expands the expressive power of fixed-depth transformers beyond TC0 complexity class.
- Mechanism: By recursively decomposing problems and using program-guided prompts, the method enables solving problems that exceed the expressive power of standard IO-prompting transformers.
- Core assumption: The theoretical limitation of fixed-depth transformers can be overcome through algorithmic decomposition strategies.
- Evidence anchors:
  - [abstract] "Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer"
  - [section] "We provide a theoretic analysis to validate that the proposed strategy can expand the expressive capacity of Transformers"
  - [corpus] Weak - corpus neighbors don't address the theoretical expressive power analysis
- Break Condition: When the problem cannot be decomposed into independent sub-tasks that can be solved separately

## Foundational Learning

- Concept: Transformer expressive power limitations
  - Why needed here: Understanding why standard prompting fails on tasks requiring long reasoning paths or repetitive sub-tasks
  - Quick check question: What complexity class bounds the expressive power of fixed-depth log-precision transformers according to Merrill & Sabharwal?

- Concept: Divide-and-conquer algorithmic paradigm
  - Why needed here: The method applies divide-and-conquer principles to problem decomposition and solution assembly
  - Quick check question: How does the recursive decomposition in divide-and-conquer differ from dynamic programming approaches?

- Concept: Program-guided prompting vs autoregressive prompting
  - Why needed here: The method uses symbolic programs to guide LLM behavior rather than relying on autoregressive generation
  - Quick check question: What are the key differences between least-to-most prompting and the parallel sub-task approach proposed here?

## Architecture Onboarding

- Component map:
  - Task decomposition module: Prompts LLM to split complex tasks into parallel sub-tasks
  - Sub-task resolution module: Handles each sub-task independently
  - Solution merge module: Combines sub-task results into final answer
  - Control program: Coordinates the three modules and enforces disentanglement

- Critical path:
  1. Receive complex task input
  2. Task decomposition module generates sub-tasks
  3. Sub-task resolution module processes all sub-tasks in parallel
  4. Solution merge module combines results
  5. Return final answer

- Design tradeoffs:
  - Parallel vs sequential sub-task execution: Parallel reduces deception susceptibility but may miss dependencies
  - Fixed vs adaptive decomposition depth: Fixed depth simplifies implementation but may not handle all problem sizes optimally
  - Program-guided vs LLM-guided control: Program guidance reduces hallucination but may limit flexibility

- Failure signatures:
  - Low accuracy when sub-tasks have hidden dependencies
  - Performance degradation on tasks requiring sequential reasoning
  - Increased computational cost for very large problem decompositions

- First 3 experiments:
  1. Compare single-level vs multi-level decomposition on large integer multiplication
  2. Test parallel vs sequential sub-task execution on hallucination detection
  3. Evaluate performance on tasks with varying degrees of deceptive content

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of how many sub-tasks can be effectively parallelized using the Divide-and-Conquer approach before performance degrades due to attention mechanism constraints?
- Basis in paper: [explicit] The paper mentions "When tackling deceptive text like hallucination and fake news, such intertwined process often guides the LLM to sequentially tackle the corpus and thus follow the context's flow, making LLM prone to deception" suggesting there may be practical limits to parallel decomposition.
- Why unresolved: The paper does not provide theoretical analysis of the maximum effective parallel decomposition size, nor does it explore how transformer attention mechanisms might limit the scalability of parallel sub-task processing.
- What evidence would resolve it: Experiments varying the number of parallel sub-tasks across different task types while measuring performance, memory usage, and attention pattern analysis would provide evidence.

### Open Question 2
- Question: How does the Divide-and-Conquer approach perform on tasks that cannot be cleanly decomposed into parallel sub-tasks versus those that can?
- Basis in paper: [inferred] The paper shows successful results on tasks like large integer multiplication and hallucination detection which have natural decomposition points, but doesn't explore tasks that resist clean parallel decomposition.
- Why unresolved: The experimental evaluation focuses on tasks that fit the divide-and-conquer paradigm well, leaving open the question of whether the approach provides advantages for tasks requiring sequential or interdependent reasoning.
- What evidence would resolve it: Comparative experiments applying DaC to both easily decomposable and non-decomposable tasks, with baseline comparisons to sequential approaches.

### Open Question 3
- Question: What is the optimal strategy for deciding when to use Single-Level versus Multi-Level Divide-and-Conquer solvers?
- Basis in paper: [explicit] The paper introduces both Single-Level and Multi-Level variants but doesn't provide clear guidance on when to apply each, stating only that "Single-level Divide-and-Conquer Solver may acquire sub-tasks with large problem sizes that will still trigger intermediate errors."
- Why unresolved: While the paper presents both variants and mentions that Multi-Level can handle larger inputs, it doesn't provide criteria for selecting between them or analyzing the trade-offs in terms of efficiency versus accuracy.
- What evidence would resolve it: Empirical studies measuring performance, computation time, and error rates across different problem sizes and complexities for both variants, potentially leading to a decision framework for choosing between them.

## Limitations
- Theoretical analysis lacks rigorous mathematical proofs connecting program-guided prompting to complexity class extensions
- Experiments focus on three specific tasks with particular dataset sizes, limiting generalizability
- Critical implementation details for DaC strategy and baseline methods are not fully specified

## Confidence
- High Confidence: The core observation that parallel sub-task execution can reduce susceptibility to deceptive content is well-supported by experimental results across multiple datasets.
- Medium Confidence: The claim that separating decomposition, resolution, and merging stages improves performance is supported but could benefit from ablation studies isolating each component's contribution.
- Low Confidence: The theoretical claim about expanding transformer expressive power beyond TC0 complexity lacks rigorous proof and requires further validation.

## Next Checks
1. Conduct ablation studies isolating each stage of the DaC pipeline (decomposition, parallel resolution, merging) to quantify their individual contributions to performance improvements.
2. Apply the DaC strategy to additional problem domains beyond the three tested (e.g., code generation, mathematical reasoning, logical inference) to assess cross-domain effectiveness.
3. Develop formal proofs or more rigorous theoretical analysis connecting the program-guided decomposition approach to complexity class extensions, particularly for the claimed TC0 expansion.