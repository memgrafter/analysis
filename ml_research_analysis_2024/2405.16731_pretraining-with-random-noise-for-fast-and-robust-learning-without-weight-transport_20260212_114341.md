---
ver: rpa2
title: Pretraining with Random Noise for Fast and Robust Learning without Weight Transport
arxiv_id: '2405.16731'
source_url: https://arxiv.org/abs/2405.16731
tags:
- training
- random
- network
- noise
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that pretraining neural networks with random
  noise significantly improves learning efficiency and generalization compared to
  training with data alone. Using a feedback alignment algorithm without weight transport,
  the authors show that random noise training aligns forward weights with fixed random
  feedback weights, enabling more precise credit assignment.
---

# Pretraining with Random Noise for Fast and Robust Learning without Weight Transport

## Quick Facts
- arXiv ID: 2405.16731
- Source URL: https://arxiv.org/abs/2405.16731
- Reference count: 40
- Key outcome: Random noise pretraining significantly improves learning efficiency and generalization in neural networks using feedback alignment without weight transport

## Executive Summary
This study introduces a novel pretraining strategy using random noise that dramatically improves the efficiency and robustness of neural network learning. The method addresses the weight transport problem by pretraining networks with random inputs and labels, which modifies forward weights to align with fixed random feedback weights. This alignment enables precise credit assignment without requiring symmetric weights, allowing the network to learn faster and achieve higher accuracy. Additionally, the pretraining acts as a form of regularization that reduces weight dimensionality and improves out-of-distribution generalization. The approach also enhances meta-learning capabilities, enabling rapid adaptation to various tasks.

## Method Summary
The method involves pretraining a two-layer feedforward network with ReLU activation using random Gaussian noise inputs and random uniform labels. The forward weights are initialized with He initialization, while fixed random feedback weights are used for the feedback alignment algorithm. During pretraining, the network modifies its forward weights to align with the transpose of the fixed feedback weights. After pretraining, the network is trained on actual data using the same feedback alignment approach. The method is compared against standard feedback alignment without pretraining and backpropagation baselines across multiple datasets including MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and STL-10.

## Key Results
- Networks pretrained with random noise achieve up to 92.86% test accuracy compared to 77.45% for standard feedback alignment
- Random noise pretraining reduces convergence time by 50% or more compared to untrained networks
- The effective rank of weight matrices decreases significantly during random noise training, indicating pre-regularization
- Pretrained networks show improved out-of-distribution generalization on USPS and rotated MNIST datasets
- Random noise pretraining reduces meta-loss, enhancing the network's ability to adapt to various tasks

## Why This Works (Mechanism)

### Mechanism 1
Random noise pretraining aligns forward weights with fixed random feedback weights, enabling precise credit assignment without weight transport. During random noise training, the network modifies its forward weights to match the transpose of the fixed random feedback weights. This alignment allows the network to compute error signals similar to backpropagation without needing symmetric weights.

### Mechanism 2
Random noise pretraining reduces the effective dimensionality of weights, creating a low-rank solution space that enhances generalization. Random noise training prunes and regularizes the weight space, causing most singular values to approach zero while a few dominate. This creates a lower-dimensional weight space that predisposes the network to learn simple, low-rank solutions.

### Mechanism 3
Random noise pretraining functions as meta-learning, enabling rapid adaptation to new tasks without task-specific data. The network learns general adaptation strategies during random noise training, reducing the meta-loss when adapting to new tasks. This enables faster learning on subsequent tasks compared to untrained networks.

## Foundational Learning

- **Weight transport problem**: The fundamental limitation that feedback alignment and random noise pretraining address - the inability to transport weights from later to earlier layers in biological networks. Quick check: Why can't biological neurons simply copy weights from downstream layers to update their own weights?

- **Singular value decomposition and effective rank**: The paper uses SVD to measure effective dimensionality of weight matrices, which is crucial for understanding the pre-regularization effect. Quick check: How does effective rank differ from traditional rank, and why is it more informative for measuring weight dimensionality?

- **Cosine similarity for vector alignment**: The paper uses cosine similarity to measure alignment between forward weights and feedback weights. Quick check: What does it mean when the angle between two vectors approaches 90 degrees versus 0 degrees in terms of their similarity?

## Architecture Onboarding

- **Component map**: Random noise input generator → Forward pass through ReLU network → Cross-entropy loss → Feedback alignment weight update → Weight alignment with feedback

- **Critical path**: Random noise input → Forward pass → Cross-entropy loss → Weight update (feedback alignment) → Weight alignment with feedback

- **Design tradeoffs**: Random noise pretraining adds initial training time but reduces overall convergence time; lower effective rank improves generalization but may limit capacity for complex tasks; feedback alignment avoids weight transport but may have slower convergence than backpropagation

- **Failure signatures**: If alignment angle remains near 90 degrees after pretraining, credit assignment won't work; if test accuracy plateaus significantly below baseline, pre-regularization may be too aggressive; if meta-loss doesn't decrease during pretraining, the network isn't learning adaptation strategies

- **First 3 experiments**:
  1. Verify weight alignment: Train with random noise for varying durations and measure alignment angle between forward and feedback weights.
  2. Test learning speed: Compare convergence speed of networks with/without random noise pretraining on MNIST classification.
  3. Measure generalization: Train on MNIST and test on out-of-distribution datasets (USPS, rotated MNIST) to verify enhanced generalization.

## Open Questions the Paper Calls Out

### Open Question 1
How does random noise pretraining performance scale with deeper convolutional neural networks compared to standard backpropagation? The paper only tested feedforward networks, leaving the effectiveness on CNNs unexplored. Systematic experiments comparing random noise pretraining with backpropagation on various CNN architectures across multiple image datasets would resolve this.

### Open Question 2
What is the optimal duration and intensity of random noise pretraining for different task complexities? The authors mention they estimated "an optimal duration for random noise pretraining" but don't provide specific guidelines or theoretical framework. A mathematical relationship between pretraining duration, task complexity, and final performance would resolve this.

### Open Question 3
What specific mechanisms in biological neural development correspond to the weight alignment observed during random noise training? While the authors draw analogies to biological processes, they don't establish specific neural mechanisms that would produce similar weight alignment in biological brains. Neurophysiological studies showing weight alignment patterns in developing neural circuits would resolve this.

### Open Question 4
Can random noise pretraining be effectively combined with other biologically plausible learning algorithms beyond feedback alignment? The paper only demonstrates the method with feedback alignment, leaving open whether it generalizes to other biologically plausible approaches. Comparative experiments applying random noise pretraining to weight mirror, direct feedback alignment, and other biologically plausible algorithms across multiple tasks would resolve this.

## Limitations

- The analysis is primarily focused on two-layer networks with ReLU activations, leaving open questions about scalability to deeper architectures.
- The optimal duration and schedule for random noise pretraining is not fully characterized, and over-pretraining could potentially lead to reduced model capacity.
- The weight alignment mechanism may face challenges in more complex architectures where the relationship between forward and feedback weights becomes less straightforward.

## Confidence

**High Confidence Claims:**
- Random noise pretraining accelerates learning speed and improves accuracy in two-layer networks using feedback alignment
- The effective rank of weight matrices decreases during random noise training, indicating pre-regularization
- Networks pretrained with random noise show improved out-of-distribution generalization

**Medium Confidence Claims:**
- Random noise pretraining enhances meta-learning ability for task adaptation
- The alignment mechanism between forward weights and fixed random feedback weights generalizes across different datasets
- The low-rank solution space created by pretraining consistently improves generalization

**Low Confidence Claims:**
- The specific duration of random noise pretraining needed for optimal performance
- Whether the alignment mechanism scales effectively to deeper networks
- The relationship between pretraining duration and subsequent task performance

## Next Checks

1. **Scalability Test**: Validate the random noise pretraining method on deeper networks (3+ layers) to assess whether the alignment mechanism and generalization benefits persist with increased architectural complexity.

2. **Pretraining Duration Analysis**: Systematically vary pretraining duration (e.g., 500, 2000, 5000, 10000 iterations) to identify optimal pretraining length and determine if there's a point of diminishing returns or overfitting to random noise.

3. **Cross-Dataset Generalization**: Test the pretraining method on a diverse set of datasets (e.g., adding SVHN, ImageNet subsets) to verify that the generalization benefits are not dataset-specific and that the low-rank bias doesn't limit performance on complex visual tasks.