---
ver: rpa2
title: Leveraging Self-Supervised Learning for Fetal Cardiac Planes Classification
  using Ultrasound Scan Videos
arxiv_id: '2407.21738'
source_url: https://arxiv.org/abs/2407.21738
tags:
- data
- methods
- training
- learning
- fetal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the application of self-supervised learning\
  \ (SSL) to fetal cardiac planes classification using ultrasound scan videos, addressing\
  \ the challenge of limited labeled data in medical imaging. The research evaluates\
  \ seven SSL methods\u2014AutoEncoder, Inpainting, SimCLR-v2, MoCo-v2, BYOL, VICReg,\
  \ and BarlowTwins\u2014on a large private fetal ultrasound dataset comprising 17,209\
  \ labeled images and 575 unlabeled videos."
---

# Leveraging Self-Supervised Learning for Fetal Cardiac Planes Classification using Ultrasound Scan Videos

## Quick Facts
- arXiv ID: 2407.21738
- Source URL: https://arxiv.org/abs/2407.21738
- Reference count: 31
- Key outcome: BarlowTwins SSL pretraining with 1% labeled data outperforms ImageNet initialization by 12% F1-score for fetal cardiac planes classification

## Executive Summary
This study investigates self-supervised learning (SSL) methods for fetal cardiac planes classification using ultrasound scan videos, addressing the challenge of limited labeled data in medical imaging. The research evaluates seven SSL approaches—AutoEncoder, Inpainting, SimCLR-v2, MoCo-v2, BYOL, VICReg, and BarlowTwins—on a large private fetal ultrasound dataset with 17,209 labeled images and 575 unlabeled videos. The findings demonstrate that dataset variance is more critical than dataset size for effective SSL training, and that BarlowTwins consistently outperforms other methods when used for downstream task initialization. Notably, full fine-tuning with just 1% of labeled data achieves a 12% higher F1-score compared to ImageNet initialization, showcasing the potential of SSL for transfer learning from ultrasound video to image data in medical applications.

## Method Summary
The study employs seven distinct self-supervised learning methods to pretrain models on unlabeled fetal ultrasound videos before fine-tuning on a small labeled dataset. The SSL methods include contrastive approaches (SimCLR-v2, MoCo-v2, BYOL), clustering-based methods (VICReg, BarlowTwins), and reconstruction-based methods (AutoEncoder, Inpainting). Pretrained models are then evaluated on downstream fetal cardiac planes classification using various fine-tuning strategies, including linear probing and full fine-tuning with different fractions of labeled data (1%, 10%, 100%). The dataset consists of 17,209 labeled images for downstream tasks and 575 unlabeled videos for SSL pretraining, allowing assessment of how pretraining on video data transfers to image-based classification.

## Key Results
- BarlowTwins consistently outperforms other SSL methods for fetal cardiac planes classification
- Dataset variance proves more crucial than dataset size for effective SSL training
- Full fine-tuning with 1% labeled data achieves 12% higher F1-score than ImageNet initialization
- SSL pretraining from video data successfully transfers to image-based downstream tasks

## Why This Works (Mechanism)
None provided

## Foundational Learning
- **Self-Supervised Learning (SSL)**: Learning from unlabeled data by creating supervisory signals from the data itself - needed to address limited labeled medical imaging data; quick check: model learns useful representations without explicit labels
- **Contrastive Learning**: Method that learns by comparing similar and dissimilar examples - needed to create meaningful embeddings; quick check: augmented views of same image pulled closer in representation space
- **Transfer Learning**: Using knowledge gained from one task to improve performance on another - needed to leverage video pretraining for image classification; quick check: pretraining task is different but complementary to downstream task
- **Fine-tuning**: Adapting pretrained model weights to a specific downstream task - needed to specialize general representations; quick check: last layers typically modified while keeping earlier layers frozen or partially updated
- **Data Variance**: Diversity of patterns and features in training data - needed for models to learn generalizable representations; quick check: dataset covers wide range of fetal cardiac anatomical variations

## Architecture Onboarding

**Component Map**: Unlabeled Videos -> SSL Pretraining -> Pretrained Backbone -> Labeled Images -> Downstream Fine-tuning -> Classification Output

**Critical Path**: Unlabeled videos undergo SSL pretraining using one of seven methods, producing a pretrained backbone. This backbone is then fine-tuned on labeled images for the downstream fetal cardiac planes classification task.

**Design Tradeoffs**: The study balances between contrastive methods (requiring large batch sizes and memory) versus non-contrastive methods (more memory efficient). Reconstruction-based methods trade computational efficiency for potentially less semantically meaningful representations. The choice of fine-tuning strategy (linear vs full) represents a tradeoff between parameter efficiency and performance.

**Failure Signatures**: Poor SSL pretraining performance manifests as similar F1-scores across all methods, indicating the pretraining provides little benefit. Overfitting to unlabeled data shows as poor transfer to labeled downstream tasks. Insufficient variance in pretraining data results in representations that don't generalize across fetal cardiac planes.

**First 3 Experiments**: 1) Compare all seven SSL methods using linear probing with 10% labeled data to establish baseline performance differences. 2) Test BarlowTwins with varying fractions of labeled data (1%, 10%, 100%) to determine minimum annotation requirements. 3) Evaluate transfer learning capability by pretraining on video data and fine-tuning on image data versus direct image training.

## Open Questions the Paper Calls Out
None provided

## Limitations
- Results based on single private fetal ultrasound dataset, limiting generalizability to other medical imaging contexts
- Claims about dataset variance being more important than size require validation across different medical imaging domains
- Focus on fetal cardiac ultrasound may not translate to other ultrasound applications or medical imaging modalities

## Confidence
- High confidence in BarlowTwins outperforming other SSL methods for fetal cardiac planes classification
- Medium confidence in transferability of SSL video pretraining to image-based downstream tasks
- Medium confidence in variance being more crucial than size for effective SSL training
- Low confidence in generalization to other medical imaging applications beyond fetal cardiac ultrasound

## Next Checks
1. Replicate the study using publicly available fetal ultrasound datasets (e.g., from MICCAI challenges) to validate the variance vs size hypothesis across different data distributions.

2. Test the SSL pretraining approach on other medical imaging modalities (MRI, CT, X-ray) to assess generalizability beyond ultrasound applications.

3. Conduct ablation studies varying the amount of unlabeled video data while holding dataset variance constant to quantify the relative importance of size vs variance empirically.