---
ver: rpa2
title: Large Language Models for Dysfluency Detection in Stuttered Speech
arxiv_id: '2406.11025'
source_url: https://arxiv.org/abs/2406.11025
tags:
- acoustic
- speech
- features
- language
- dysfluency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of detecting dysfluencies in stuttered
  speech by leveraging large language models (LLMs) as universal processors of acoustic
  and lexical inputs. The authors present acoustic features extracted from wav2vec
  2.0 and lexical features from automatic speech recognition (ASR) hypotheses to an
  LLM, finetuning it via low-rank adaptation (LoRA) to predict multi-label dysfluency
  categories.
---

# Large Language Models for Dysfluency Detection in Stuttered Speech

## Quick Facts
- arXiv ID: 2406.11025
- Source URL: https://arxiv.org/abs/2406.11025
- Reference count: 0
- Primary result: LLM-based system combining acoustic and lexical features achieves competitive F1-scores for dysfluency detection across three English and German stuttered speech datasets

## Executive Summary
This work addresses dysfluency detection in stuttered speech by leveraging large language models as universal processors of acoustic and lexical inputs. The authors extract acoustic features from wav2vec 2.0 and lexical features from ASR hypotheses, then fine-tune an LLM via LoRA to predict multi-label dysfluency categories. The experimental results demonstrate that combining acoustic and lexical information yields competitive performance across three datasets in English and German, with best results achieved using domain-adapted acoustic features from the last layer of wav2vec 2.0 and 1-best ASR hypotheses.

## Method Summary
The method extracts acoustic features from wav2vec 2.0 (projecting them into the LLM's token space) and lexical features from Whisper ASR outputs, then concatenates these representations for LLM processing. The system fine-tunes a pretrained Llama 2 model using LoRA with AdamW optimization, predicting multi-label dysfluency categories including blocks, interjections, prolongations, and various repetition types. Multiple decoding strategies (greedy, beam search, MBR) generate ASR hypotheses, while domain adaptation of the acoustic encoder aims to capture stuttering-specific patterns.

## Key Results
- Combined acoustic and lexical features outperform single-modality approaches across all three datasets
- Domain-adapted wav2vec 2.0 features from the 24th layer yield best performance
- 1-best ASR hypotheses with greedy decoding perform as well as or better than n-best and MBR decoding variants
- System shows language bias, with significantly lower performance on German KSoF dataset compared to English datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM effectively combines acoustic and lexical features for dysfluency detection.
- Mechanism: Acoustic features from wav2vec 2.0 are projected into the token space of the LLM and concatenated with ASR-derived lexical tokens, allowing the LLM to jointly process multimodal information for dysfluency prediction.
- Core assumption: The LLM's pretraining on large-scale text enables it to learn meaningful representations when given concatenated acoustic+lexical inputs.
- Evidence anchors:
  - [abstract] "We present hypotheses candidates generated with an automatic speech recognition system and acoustic representations extracted from an audio encoder model to an LLM, and finetune the system to predict dysfluency labels"
  - [section] "We hypothesize that lexical and acoustic features are complementary, each offering distinct advantages in enhancing dysfluency detection"
  - [corpus] Weak/no direct corpus evidence that this specific combination outperforms other feature fusion approaches
- Break condition: If the LLM cannot meaningfully integrate acoustic projections with lexical tokens, performance will degrade to levels similar to using either modality alone.

### Mechanism 2
- Claim: Domain-adapted acoustic features extracted from the last layer of wav2vec 2.0 yield best performance for dysfluency detection.
- Mechanism: Fine-tuning wav2vec 2.0 on stuttering-specific data allows later layers to specialize in capturing stuttering patterns, making last-layer features more discriminative than out-of-domain features from middle layers.
- Core assumption: The last layer of a domain-adapted model captures task-specific acoustic characteristics better than earlier layers or non-adapted models.
- Evidence anchors:
  - [section] "As the model is finetuned, the layers closer to the output become more specialized to the specific task...Finetuning adjusts the parameters of these layers to better fit the characteristics of the dataset, yielding representations better suited to detecting stuttering patterns"
  - [section] "However, acoustic features from domain-adapted models yielded the best performance when extracted at the 24th layer"
  - [corpus] No corpus evidence that last-layer features universally outperform middle-layer features across all dysfluency types
- Break condition: If domain adaptation does not sufficiently capture stuttering-specific patterns, the performance advantage of last-layer features disappears.

### Mechanism 3
- Claim: MBR decoding of ASR hypotheses provides more lexical diversity than 1-best decoding without significant performance loss.
- Mechanism: By sampling multiple hypotheses weighted by utility (negative WER), MBR decoding captures alternative lexical interpretations that the LLM can leverage for better dysfluency detection.
- Core assumption: The LLM can effectively process and benefit from multiple lexical hypothesis candidates rather than just the single most likely one.
- Evidence anchors:
  - [section] "ASR systems typically utilize maximum a posteriori probability (MAP) decoding...We employ MBR decoding not only in the hope to mitigate some of the shortcomings of MAP decoding, but also to add more lexical diversity, by presenting a sequence of multiple hypothesis candidates to the LLM"
  - [section] "Generally, n-best lists and MBR hypotheses candidates did not yield significant enhancements over 1-best hypotheses"
  - [corpus] Corpus shows that n-best and MBR only marginally improved performance on FluencyBank dataset
- Break condition: If the LLM cannot effectively utilize multiple hypotheses or if the additional lexical diversity does not translate to better dysfluency detection, MBR decoding provides no benefit over simpler decoding methods.

## Foundational Learning

- Concept: Multimodal learning with LLMs
  - Why needed here: The system combines acoustic and lexical inputs for a single prediction task, requiring understanding of how LLMs can process non-textual modalities
  - Quick check question: How does projecting acoustic features into the token space enable the LLM to process them alongside lexical tokens?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Enables efficient fine-tuning of the large LLM backbone by optimizing small adapter matrices instead of full model weights
  - Quick check question: What is the mathematical relationship between the original weight matrix W, the update matrix Δ, and the low-rank decomposition matrices A and B?

- Concept: Minimum Bayes Risk (MBR) decoding
  - Why needed here: Provides an alternative to MAP decoding that considers multiple hypothesis candidates weighted by their utility, potentially capturing more relevant lexical variations
  - Quick check question: How does MBR decoding differ from beam search in terms of hypothesis selection criteria?

## Architecture Onboarding

- Component map:
  - wav2vec 2.0 (acoustic encoder) → FFN projection → LLM (with LoRA) ← Whisper (lexical encoder)
  - MBR decoding generates multiple ASR hypotheses as lexical input
  - Training uses cross-entropy loss on dysfluency token prediction

- Critical path:
  - Audio waveform → wav2vec 2.0 → FFN → concatenated with ASR tokens → LLM → dysfluency tokens
  - MBR decoding → multiple hypothesis candidates → tokenized → concatenated with acoustic features

- Design tradeoffs:
  - Using last-layer vs. middle-layer acoustic features (specialization vs. general acoustic patterns)
  - MBR vs. 1-best decoding (lexical diversity vs. computational efficiency)
  - Phonetic vs. orthographic transcriptions (fine-grained vs. coarse lexical representation)

- Failure signatures:
  - Poor performance on word repetitions suggests Whisper's pretraining may not capture exact token repetition
  - Weak performance on German KSoF dataset indicates potential language model bias toward English
  - N-best and MBR not improving over 1-best suggests LLM may not effectively utilize multiple hypotheses

- First 3 experiments:
  1. Compare acoustic-only vs. lexical-only vs. combined features to verify complementarity
  2. Test different wav2vec 2.0 layer extractions (12th vs. 24th) with domain-adapted vs. non-adapted models
  3. Evaluate MBR vs. beam search vs. greedy decoding with varying numbers of hypothesis candidates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the LLM-based dysfluency detection system scale with increasing model size beyond the 7B parameter Llama 2 model used in this study?
- Basis in paper: [explicit] The authors mention "Future work will explore lexical encoder alternatives, the impact of varying LLM sizes" as a direction for future research.
- Why unresolved: The paper only tested with the 7B parameter Llama 2 model, leaving open the question of whether larger or smaller LLM models would perform better or worse for this task.
- What evidence would resolve it: Comparative experiments testing the system with different LLM sizes (e.g., 13B, 34B, or 70B parameter models) on the same datasets, measuring F1-scores for each dysfluency type.

### Open Question 2
- Question: What is the impact of using end-to-end fine-tuning of both the LLM backbone and the acoustic encoder, compared to the current approach of freezing the LLM weights and only fine-tuning LoRA modules?
- Basis in paper: [explicit] The authors state "Future work will explore...full end-to-end finetuning of both the LLM backbone and the acoustic encoder."
- Why unresolved: The current approach freezes the LLM weights, which may limit the system's ability to learn task-specific representations that could improve performance.
- What evidence would resolve it: Experiments comparing the current LoRA-based approach with full end-to-end fine-tuning, measuring improvements in F1-scores for each dysfluency type.

### Open Question 3
- Question: How does the performance of the system change when using alternative lexical encoder models, such as different ASR systems or phonetic transcription models?
- Basis in paper: [explicit] The authors mention "Future work will explore lexical encoder alternatives" as a direction for future research.
- Why unresolved: The study only tested with Whisper and a wav2vec 2.0-based phonetic transcription system, leaving open the question of whether other lexical encoders might yield better performance.
- What evidence would resolve it: Experiments replacing the current lexical encoders with alternative models (e.g., other ASR systems or phonetic transcription models) and measuring the resulting F1-scores for each dysfluency type.

## Limitations
- The LLM's ability to truly integrate multimodal information remains unclear, with acoustic features potentially dominating performance
- Minimal performance differences between MBR and 1-best decoding suggest the LLM may not effectively utilize multiple lexical hypotheses
- Significant language bias observed, with substantially worse performance on German data compared to English datasets

## Confidence

- **Medium**: Claims about LLM effectiveness in combining acoustic and lexical features - supported by quantitative results but mechanism unclear
- **Medium**: Claims about domain-adapted acoustic features superiority - empirical support present but limited cross-validation
- **Low**: Claims about MBR decoding providing meaningful lexical diversity - marginal improvements observed across datasets

## Next Checks

1. **Ablation study with isolated modalities**: Run the system with only acoustic features, only lexical features, and the combined approach on all three datasets to quantify the actual contribution of each modality and determine if the LLM truly integrates them or favors one over the other.

2. **Cross-linguistic validation**: Test the domain-adapted wav2vec 2.0 features on additional non-English stuttering datasets to verify whether the claimed language independence holds or if the model remains English-biased.

3. **Alternative hypothesis processing**: Implement and compare beam search and MBR decoding variants with different numbers of candidates (n=5, n=10) to determine if the LLM's apparent inability to utilize multiple hypotheses is due to the decoding method itself or fundamental limitations in processing lexical alternatives.