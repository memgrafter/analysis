---
ver: rpa2
title: Explore the Hallucination on Low-level Perception for MLLMs
arxiv_id: '2409.09748'
source_url: https://arxiv.org/abs/2409.09748
tags:
- mllms
- low-level
- visual
- self-awareness
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QL-Bench, a benchmark designed to evaluate
  the self-awareness of Multi-modality Large Language Models (MLLMs) in low-level
  visual perception tasks. The benchmark includes a dataset of 2,990 single images
  and 1,999 image pairs, each paired with open-ended questions about low-level visual
  attributes like clarity and lighting.
---

# Explore the Hallucination on Low-level Perception for MLLMs

## Quick Facts
- arXiv ID: 2409.09748
- Source URL: https://arxiv.org/abs/2409.09748
- Reference count: 33
- Key outcome: QL-Bench benchmark reveals MLLMs have strong low-level visual capabilities but underdeveloped self-awareness, especially for simpler questions

## Executive Summary
This paper introduces QL-Bench, a benchmark designed to evaluate the self-awareness of Multi-modality Large Language Models (MLLMs) in low-level visual perception tasks. The benchmark includes a dataset of 2,990 single images and 1,999 image pairs, each paired with open-ended questions about low-level visual attributes like clarity and lighting. The study evaluates 15 MLLMs, including both open-source and closed-source models, using metrics like scorecc (correct answers), scorerc (appropriate refusals), and scoresa (overall self-awareness). Results show that while some models exhibit strong low-level visual capabilities, their self-awareness remains underdeveloped, particularly in simpler questions. Models perform better in multi-image tasks but still struggle to balance accuracy and appropriate refusal. The findings highlight the need for further research to enhance MLLMs' self-awareness in low-level visual tasks.

## Method Summary
The study creates QL-Bench, a benchmark evaluating MLLMs' self-awareness in low-level visual perception through three metrics: scorecc for correct answers, scorerc for appropriate refusals, and scoresa (sum of scorecc and scorerc) for overall self-awareness. The LLSA VisionQA dataset contains 2,990 single images and 1,999 image pairs from 10 diverse sources, with open-ended questions about low-level visual attributes. The evaluation tests 15 MLLMs on three question types ("Yes-or-No," "What," "How") across single-image and multi-image tasks. The study compares open-source models (LLaVA-Next, mPLUG-Owl, InstructBLIP) with closed-source models (GPT-4V, GPT-4O, Gemini-1.5-Pro) to assess self-awareness patterns.

## Key Results
- MLLMs show strong low-level visual capabilities but underdeveloped self-awareness, particularly for simpler questions
- Self-awareness improves when addressing more challenging questions, contrary to accuracy patterns
- Multi-image tasks enhance self-awareness compared to single-image tasks, despite decreased accuracy
- Open-source and closed-source models show different self-awareness patterns across question types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark measures self-awareness by requiring models to either answer correctly or refuse when uncertain.
- Mechanism: The evaluation uses two metrics—scorecc for correct answers and scorerc for appropriate refusals—to capture whether models can distinguish between what they know and don't know.
- Core assumption: A model's self-awareness can be quantified by how well it balances accurate responses with appropriate refusals.
- Evidence anchors: [abstract] "we introduce three metrics to measure the self-awareness of MLLMs" and "scoresa: It is the sum of scorecc and scorerc, representing the overall self-awareness of the model"

### Mechanism 2
- Claim: Low-level visual perception self-awareness differs from high-level understanding in that simpler questions often yield lower self-awareness.
- Mechanism: The study found that for the same model, simpler questions are often answered more accurately than complex ones, but self-awareness appears to improve when addressing more challenging questions.
- Core assumption: The relationship between question difficulty and self-awareness follows a non-monotonic pattern where complex questions trigger better recognition of knowledge boundaries.
- Evidence anchors: [abstract] "Notably, for the same model, simpler questions are often answered more accurately than complex ones. However, self-awareness appears to improve when addressing more challenging questions"

### Mechanism 3
- Claim: Multi-image tasks improve MLLMs' self-awareness compared to single-image tasks, even when accuracy decreases.
- Mechanism: The benchmark shows that models display stronger self-awareness in multi-image tasks compared to single-image tasks, suggesting that comparative visual analysis enhances self-recognition of knowledge boundaries.
- Core assumption: Visual comparison across multiple images provides additional context that helps models better recognize their limitations.
- Evidence anchors: [abstract] "Comparison between single-image and multi-image tasks. Most models display stronger self-awareness in multi-image tasks compared to single-image tasks, although the accuracy of their responses decreased"

## Foundational Learning

- Concept: Visual perception hierarchy (low-level vs high-level)
  - Why needed here: Understanding the distinction between low-level visual attributes (clarity, lighting) and high-level semantic understanding is crucial for interpreting why this benchmark focuses on low-level perception
  - Quick check question: What distinguishes low-level visual perception from high-level visual understanding in the context of MLLMs?

- Concept: Hallucination detection and mitigation
  - Why needed here: The paper's core premise is that hallucinations stem from lack of self-awareness, so understanding hallucination mechanisms is essential for evaluating the benchmark's effectiveness
  - Quick check question: How does the proposed self-awareness metric attempt to address hallucination in MLLMs?

- Concept: Multimodal model evaluation metrics
  - Why needed here: The benchmark introduces specific metrics (scorecc, scorerc, scoresa) that require understanding of how multimodal model evaluation differs from unimodal evaluation
  - Quick check question: What makes evaluating self-awareness in MLLMs more complex than evaluating accuracy alone?

## Architecture Onboarding

- Component map: LLSA VisionQA dataset (2,990 single images + 1,999 image pairs) -> Three question types (Yes-or-No, What, How) -> Evaluation metrics (scorecc, scorerc, scoresa) -> 15 MLLMs (open-source and closed-source)
- Critical path: Dataset creation → Question formulation → Model evaluation → Metric calculation → Result analysis
- Design tradeoffs: Single-image vs multi-image tasks trade off accuracy for self-awareness; simpler questions yield higher accuracy but lower self-awareness; open-source vs closed-source models show different self-awareness patterns
- Failure signatures: Models achieving high scorecc but low scorerc indicate overconfidence without proper uncertainty recognition; consistently low scoresa across all models suggest fundamental architectural limitations
- First 3 experiments:
  1. Run baseline evaluation using only 'Yes-or-No' questions on single images to establish minimum self-awareness scores
  2. Test the same models with 'How' questions to measure difficulty-based self-awareness variation
  3. Compare open-source vs closed-source models on multi-image tasks to identify architectural differences in self-awareness development

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications could enhance MLLMs' self-awareness in low-level visual tasks, and how do these modifications impact their performance across different question types?
- Basis in paper: [inferred] The paper highlights the need for further research to enhance MLLMs' self-awareness in low-level visual tasks, particularly in simpler questions where self-awareness is weaker.
- Why unresolved: The paper identifies the problem but does not propose specific architectural changes or evaluate their effectiveness.
- What evidence would resolve it: Empirical studies comparing MLLMs with and without proposed architectural modifications, focusing on self-awareness metrics across different question types.

### Open Question 2
- Question: How do different training datasets and data augmentation techniques affect the self-awareness of MLLMs in low-level visual perception tasks?
- Basis in paper: [inferred] The paper suggests that some models exhibit strong low-level visual capabilities without specialized training, implying that dataset composition and augmentation might play a role in self-awareness.
- Why unresolved: The study evaluates existing models but does not explore the impact of training data variations on self-awareness.
- What evidence would resolve it: Comparative analysis of MLLMs trained on diverse datasets and with different augmentation strategies, measuring self-awareness scores.

### Open Question 3
- Question: What is the relationship between the complexity of visual tasks and the self-awareness of MLLMs, and how can this relationship be leveraged to improve model reliability?
- Basis in paper: [explicit] The paper notes that simpler questions are often answered more accurately than complex ones, yet self-awareness appears to improve when addressing more challenging questions.
- Why unresolved: The study observes this pattern but does not investigate the underlying reasons or potential applications for improving model reliability.
- What evidence would resolve it: In-depth analysis of model behavior across a range of task complexities, identifying factors that influence self-awareness and strategies to enhance it.

## Limitations

- Dataset size and diversity may not capture full range of low-level visual perception scenarios
- Evaluation framework assumes appropriate refusal equals self-awareness, potentially oversimplifying uncertainty recognition
- Controlled benchmark questions may limit generalizability to real-world applications

## Confidence

- **High confidence**: The benchmark construction methodology and the basic evaluation framework (scorecc, scorerc, scoresa) are well-specified and reproducible.
- **Medium confidence**: The comparative analysis between single-image and multi-image tasks, and the relationship between question difficulty and self-awareness scores, shows consistent patterns but requires larger sample sizes for robust generalization.
- **Low confidence**: The generalizability of findings to real-world applications remains uncertain due to the controlled nature of the benchmark questions and the limited representation of diverse visual scenarios.

## Next Checks

1. **Dataset Diversity Validation**: Test the QL-Bench evaluation framework on an independently constructed dataset of low-level visual perception tasks to verify whether the observed patterns in model self-awareness hold across different data distributions.

2. **Cross-Domain Transfer Analysis**: Evaluate whether models that perform well on QL-Bench show corresponding improvements in self-awareness for high-level visual reasoning tasks, or if the low-level focus limits transferability.

3. **Temporal Stability Assessment**: Re-run the benchmark evaluation after fine-tuning the same models on additional low-level visual perception data to determine if self-awareness scores improve proportionally with accuracy, or if they exhibit non-monotonic relationships.