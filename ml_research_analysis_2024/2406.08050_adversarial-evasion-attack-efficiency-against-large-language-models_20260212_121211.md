---
ver: rpa2
title: Adversarial Evasion Attack Efficiency against Large Language Models
arxiv_id: '2406.08050'
source_url: https://arxiv.org/abs/2406.08050
tags:
- attacks
- adversarial
- llms
- attack
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzed adversarial evasion attack efficiency against
  large language models (LLMs) used for sentiment classification. Three attack methods
  (BERTAttack, ChecklistAttack, TypoAttack) were evaluated against five LLMs (BERT,
  RoBERTa, DistilBERT, ALBERT, XLNet) using the RottenTomatoes dataset.
---

# Adversarial Evasion Attack Efficiency against Large Language Models

## Quick Facts
- arXiv ID: 2406.08050
- Source URL: https://arxiv.org/abs/2406.08050
- Reference count: 28
- Primary result: BERTAttack achieved highest misclassification rates (80-100%) but required 85-135 queries, showing a tradeoff between attack effectiveness and practicality

## Executive Summary
This paper analyzes adversarial evasion attacks against large language models (LLMs) used for sentiment classification. Three attack methods (BERTAttack, ChecklistAttack, TypoAttack) were evaluated against five LLMs (BERT, RoBERTa, DistilBERT, ALBERT, XLNet) using the RottenTomatoes dataset. The results reveal a significant tradeoff between attack effectiveness and practicality, with BERTAttack achieving highest misclassification rates but requiring many queries, while ChecklistAttack was most practical but least effective. The study highlights the need for real-time query monitoring to detect and prevent exploitation of deployed LLMs.

## Method Summary
The researchers evaluated three adversarial attack methods against five transformer-based LLMs using the RottenTomatoes movie review dataset. BERTAttack generates word-level perturbations using a pre-trained BERT model, ChecklistAttack performs constrained word-level perturbations from a predefined checklist, and TypoAttack executes character-level perturbations. The attacks were measured using Misclassification Rate (MR), Average Perturbed Words (APW), and Average Required Queries (ARQ). All experiments were implemented using the textattack library with pre-trained models from HuggingFace.

## Key Results
- BERTAttack achieved 80-100% misclassification rates with 85-135 queries and 2-4 word replacements
- TypoAttack caused 50-90% misclassifications with minimal perturbations but needed 355-381 queries
- ChecklistAttack was most practical (2-2.25 queries) but least effective (10-40% misclassifications)
- A clear tradeoff exists between attack effectiveness and practicality across all methods and models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERTAttack's high effectiveness comes from using a BERT model to generate adversarial examples that exploit the target model's weaknesses.
- Mechanism: BERTAttack leverages a pre-trained BERT model as an adversary to generate word-level perturbations. It iteratively modifies text samples by replacing words with semantically similar alternatives that the target LLM misclassifies. The attack uses the target model's confidence scores to guide the search for effective perturbations.
- Core assumption: The attacking BERT model can generate perturbations that are semantically close enough to fool the target LLM but different enough to change the classification.
- Evidence anchors:
  - [abstract] "BERTAttack achieved highest misclassification rates (80-100%) but required 85-135 queries and replaced 2-4 words."
  - [section] "The word-level BERTAttack and the character-level TypoAttack obtained very high misclassiï¬cation rates, which demonstrated that both types of perturbations can be used to attack LLMs."
- Break condition: If the target model uses robust adversarial training or has strong semantic understanding that prevents semantic equivalence attacks, BERTAttack's effectiveness would degrade significantly.

### Mechanism 2
- Claim: TypoAttack's effectiveness stems from exploiting character-level tokenization and stemming processes in LLMs.
- Mechanism: TypoAttack performs character-level perturbations by adding, removing, or swapping characters in words, exploiting the tokenization and stemming processes of LLMs. These subtle modifications are often overlooked by humans but can significantly affect the model's classification by altering how words are tokenized or processed.
- Core assumption: The tokenization and stemming mechanisms of LLMs are vulnerable to minor character-level perturbations that don't significantly change human interpretation but affect model processing.
- Evidence anchors:
  - [abstract] "TypoAttack caused 50-90% misclassifications with minimal perturbations but needed 355-381 queries."
  - [section] "On the other hand, the TypoAttack only modified a few characters in one or two words of a text sample. It was able to cause 50% of misclassifications in the bigger and more complex LLMs like BERT and RoBERTa, and even reached 80% against the smaller ALBERT and up to 90% against XLNet."
- Break condition: If the LLM implements robust character-level validation or uses character-aware tokenization that is resistant to minor typos, TypoAttack's effectiveness would diminish.

### Mechanism 3
- Claim: ChecklistAttack's practicality comes from its constrained data generation process that limits perturbations to predefined acceptable alternatives.
- Mechanism: ChecklistAttack performs word-level perturbations but restricts replacements to words from a predefined checklist, ensuring that the modified text remains semantically coherent and grammatically correct. This constrained approach reduces the search space and computational cost compared to unrestricted attacks.
- Core assumption: A predefined checklist of semantically equivalent words can generate effective adversarial examples while maintaining text coherence and reducing computational overhead.
- Evidence anchors:
  - [abstract] "ChecklistAttack was most practical (2-2.25 queries) but least effective (10-40% misclassifications)."
  - [section] "Since it only modified one or two words with similar ones from a predefined checklist, the attack didn't transfer well to the dataset used in this work."
- Break condition: If the checklist is too restrictive or doesn't contain effective adversarial alternatives for the specific dataset, ChecklistAttack's effectiveness will remain low despite its practicality.

## Foundational Learning

- Concept: Adversarial examples in machine learning
  - Why needed here: Understanding how small perturbations can cause misclassification is fundamental to analyzing the attack methods and their effectiveness.
  - Quick check question: What is an adversarial example and how does it differ from random noise in input data?

- Concept: Natural Language Processing and text classification
  - Why needed here: The attacks target LLMs used for sentiment classification, requiring understanding of how these models process and classify text.
  - Quick check question: How do transformer-based models like BERT and RoBERTa process text for classification tasks?

- Concept: Query efficiency and computational cost in adversarial attacks
  - Why needed here: The paper emphasizes the tradeoff between attack effectiveness and practicality, measured through average queries required.
  - Quick check question: Why is the number of queries an important metric for evaluating adversarial attack practicality in real-world scenarios?

## Architecture Onboarding

- Component map:
  - RottenTomatoes dataset -> Five LLMs (BERT, RoBERTa, DistilBERT, ALBERT, XLNet) -> Three attack methods (BERTAttack, ChecklistAttack, TypoAttack) -> Evaluation metrics (MR, APW, ARQ)

- Critical path:
  1. Load dataset and models using huggingface
  2. Implement each attack method using textattack
  3. Run attacks against each model
  4. Collect metrics (MR, APW, ARQ)
  5. Analyze and compare results

- Design tradeoffs:
  - Effectiveness vs. Practicality: High effectiveness attacks (BERTAttack) require many queries, while practical attacks (ChecklistAttack) are less effective
  - Perturbation size vs. Detection: Smaller perturbations (TypoAttack) are less noticeable but may require more queries
  - Computational cost vs. Attack success rate: More queries increase attack success but also computational overhead

- Failure signatures:
  - Low misclassification rates across all attacks suggest model robustness
  - High query requirements with low effectiveness indicate inefficient attack methods
  - Inconsistent results across different LLMs suggest model-specific vulnerabilities

- First 3 experiments:
  1. Run BERTAttack against BERT model and verify high misclassification rate (>90%) with moderate query count (80-140)
  2. Run TypoAttack against XLNet model and confirm high misclassification rate (>80%) with high query count (>350)
  3. Run ChecklistAttack against RoBERTa model and observe practical query count (<3) with moderate effectiveness (20-40%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do adversarial attacks impact the robustness of large language models in real-world deployment scenarios?
- Basis in paper: [explicit] The paper discusses the effectiveness, efficiency, and practicality of three different types of adversarial attacks against five different LLMs in a sentiment classification task.
- Why unresolved: The paper provides insights into the impacts of different types of perturbations but does not fully explore the real-world implications of these attacks on deployed LLMs.
- What evidence would resolve it: Conducting real-world deployment tests of LLMs under adversarial attacks and analyzing the impact on their performance and security.

### Open Question 2
- Question: What are the most effective and practical adversarial attack methods against large language models for text classification tasks?
- Basis in paper: [explicit] The paper evaluates three attack methods (BERTAttack, ChecklistAttack, TypoAttack) and their effectiveness against five LLMs, highlighting the tradeoff between attack effectiveness and practicality.
- Why unresolved: The paper does not determine which attack method is universally the most effective and practical across all types of LLMs and text classification tasks.
- What evidence would resolve it: Extensive testing of various attack methods against a wider range of LLMs and text classification tasks to identify the most effective and practical approach.

### Open Question 3
- Question: How can adversarial defenses be developed to enhance the robustness of large language models against different types of attacks?
- Basis in paper: [inferred] The paper discusses the need for real-time query monitoring to detect suspicious patterns and prevent exploitation of deployed LLMs, suggesting a potential defense strategy.
- Why unresolved: The paper does not provide a comprehensive solution for developing adversarial defenses that can effectively protect LLMs against various types of attacks.
- What evidence would resolve it: Developing and testing various defense mechanisms against different types of adversarial attacks to determine their effectiveness in enhancing the robustness of LLMs.

## Limitations
- The study is limited to sentiment classification on a single dataset (RottenTomatoes), which may not generalize to other NLP tasks
- Real-world applicability is uncertain due to potential detection systems and user feedback loops that weren't modeled
- Computational costs of some attacks (particularly BERTAttack) may be impractical for real-time applications

## Confidence
- Effectiveness of BERTAttack: **High**
- Practicality of ChecklistAttack: **Medium**
- Computational cost of TypoAttack: **Medium**

## Next Checks
1. Test the attack methods on a different sentiment analysis dataset to verify the generalizability of the results.
2. Implement a real-time query monitoring system to detect and mitigate suspicious patterns in LLM usage.
3. Explore the effectiveness of these attacks against LLMs with adversarial training to assess the robustness of the models.