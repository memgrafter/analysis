---
ver: rpa2
title: Evaluation of Google Translate for Mandarin Chinese translation using sentiment
  and semantic analysis
arxiv_id: '2409.04964'
source_url: https://arxiv.org/abs/2409.04964
tags:
- translation
- translate
- google
- translations
- chapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates Google Translate\u2019s performance in Mandarin\
  \ Chinese to English translation using sentiment and semantic analysis. The authors\
  \ developed a framework comparing Google Translate translations of Lu Xun\u2019\
  s classic novel \u201CThe True Story of Ah Q\u201D with two expert human translations\
  \ across nine chapters."
---

# Evaluation of Google Translate for Mandarin Chinese translation using sentiment and semantic analysis

## Quick Facts
- arXiv ID: 2409.04964
- Source URL: https://arxiv.org/abs/2409.04964
- Reference count: 40
- Google Translate achieves lower sentiment (0.548-0.577) and semantic similarity (0.548-0.577) scores compared to human expert translations (average 0.653) when translating Lu Xun's "The True Story of Ah Q"

## Executive Summary
This study evaluates Google Translate's performance in Mandarin Chinese to English translation using a comprehensive framework combining sentiment and semantic analysis. The researchers compared Google Translate translations of Lu Xun's classic novel "The True Story of Ah Q" with two expert human translations across nine chapters, using BERT-based models fine-tuned on the SenWave dataset for sentiment analysis and MPNet for semantic similarity assessment. Results showed Google Translate produced translations with significantly lower sentiment and semantic similarity scores compared to human experts, with qualitative analysis revealing difficulties in handling Chinese idioms, vernacular expressions, and cultural references.

## Method Summary
The study developed a comparative framework that involved extracting the original Mandarin text of "The True Story of Ah Q" and obtaining two expert English translations, then using Google Translate API to create a third translation. The researchers fine-tuned a BERT-base uncased model on the SenWave dataset (10,000 tweets with 9 emotions) to analyze sentiment patterns across all three translations, calculating Jaccard similarity scores. They also employed MPNet to generate sentence embeddings and compute cosine similarity for semantic analysis. The framework included bigram/trigram analysis for vocabulary comparison and qualitative expert assessment of specific translation failures, particularly focusing on culturally specific phrases and idioms.

## Key Results
- Google Translate achieved lower sentiment similarity scores (average 0.548-0.577) compared to two human expert translations (average 0.653)
- Semantic similarity analysis showed Google Translate translations were less semantically similar to human translations than human translations were to each other
- Expert qualitative analysis revealed Google Translate struggled with Chinese idioms and vernacular expressions, often producing literal translations that missed intended meaning
- The mistranslations were attributed to lack of contextual understanding and historical knowledge of Chinese culture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Google Translate's mistranslations stem from lack of contextual and cultural understanding of Chinese idioms and vernacular expressions.
- Mechanism: The model fails to preserve semantic meaning when encountering culturally specific phrases, resulting in literal translations that miss intended meaning.
- Core assumption: Chinese idioms and vernacular expressions require cultural and historical context for accurate translation.
- Evidence anchors:
  - [abstract]: "Google Translate is unable to translate some of the specific words or phrases in Chinese, such as Chinese traditional allusions. The mistranslations may be due to lack of contextual significance and historical knowledge of China."
  - [section]: "Google Translate is unable to understand the context of words and lack of contextual significance... Google Translate did not convey the intended meaning; it merely translated the allusion from Chinese characters into pinyin."
  - [corpus]: Weak evidence - corpus mentions idiom translation evaluation but no specific evidence about Google Translate's idiom handling.
- Break condition: If translations improve significantly on idioms and cultural references, this mechanism would need revision.

### Mechanism 2
- Claim: Sentiment analysis using BERT-based models reveals Google Translate's inability to capture emotional nuance in Chinese-to-English translations.
- Mechanism: The BERT model fine-tuned on SenWave dataset identifies sentiment patterns that show Google Translate produces translations with lower sentiment similarity scores compared to human translations.
- Core assumption: Sentiment patterns in text are preserved through accurate translation and can be detected using BERT-based sentiment analysis.
- Evidence anchors:
  - [abstract]: "Results showed Google Translate achieved lower sentiment similarity scores (average 0.548-0.577) compared to the two human expert translations (average 0.653)."
  - [section]: "We found that thankful and empathetic were not expressed in any of the chapters and Google Translate led sentiments that expressed sad and joking."
  - [corpus]: Moderate evidence - corpus includes studies on LLM translation evaluation using sentiment and semantic analysis.
- Break condition: If sentiment analysis shows comparable results between Google Translate and human translations, this mechanism would need revision.

### Mechanism 3
- Claim: MPNet-based semantic analysis reveals Google Translate's structural translation differences compared to human expert translations.
- Mechanism: MPNet sentence embeddings capture semantic similarity between translations, showing Google Translate produces less semantically similar outputs compared to human translations.
- Core assumption: Semantic similarity can be quantified using MPNet embeddings and cosine similarity, revealing translation quality differences.
- Evidence anchors:
  - [abstract]: "semantic similarity scores (average 0.548-0.577) compared to the two human expert translations (average 0.653)."
  - [section]: "In terms of semantic analysis, we found that Chapter 9 was the most semantically similar, whereas Chapter 6 is the least semantically in the three translations."
  - [corpus]: Moderate evidence - corpus includes studies using MPNet for semantic analysis in translation evaluation.
- Break condition: If semantic similarity scores improve significantly for Google Translate, this mechanism would need revision.

## Foundational Learning

- Concept: BERT-based sentiment analysis
  - Why needed here: The study uses BERT models fine-tuned on SenWave dataset to analyze sentiment patterns across translations
  - Quick check question: How does BERT differ from traditional sentiment analysis models, and why is fine-tuning on domain-specific data important?

- Concept: MPNet-based semantic similarity
  - Why needed here: The study employs MPNet to compute semantic similarity scores between different translations
  - Quick check question: What advantages does MPNet have over other sentence embedding models for cross-lingual semantic analysis?

- Concept: N-gram analysis for vocabulary comparison
  - Why needed here: The study uses bigrams and trigrams to identify common vocabulary patterns across different translations
  - Quick check question: How can N-gram frequency analysis reveal translation style differences and common themes?

## Architecture Onboarding

- Component map: Text extraction -> Google Translate API -> BERT sentiment analysis -> MPNet semantic analysis -> Expert evaluation
- Critical path: Text extraction -> translation -> sentiment analysis -> semantic analysis -> expert review
- Design tradeoffs: Automated analysis vs. human expert evaluation; computational efficiency vs. accuracy
- Failure signatures: Low sentiment similarity scores, high semantic distance, inconsistent translation of cultural references
- First 3 experiments:
  1. Test BERT sentiment analysis on small sample of translations to verify sentiment detection accuracy
  2. Compare MPNet semantic similarity scores between human translations to establish baseline
  3. Analyze specific mistranslations of Chinese idioms to understand pattern failures

## Open Questions the Paper Calls Out

- How does Google Translate's performance vary when translating different types of Chinese text (e.g., novels, news articles, academic papers) beyond the novella studied?
- How does the performance of Google Translate compare to other machine translation systems like DeepL when translating Mandarin Chinese to English?
- How would using a sentiment analysis dataset more appropriate for literary texts (rather than COVID-19 tweets) affect the evaluation of Google Translate's translation quality?

## Limitations

- The exact version of "The True Story of Ah Q" used in the study is not clearly specified, which could affect reproducibility
- Using a SenWave dataset of tweets to analyze sentiment in literary Chinese text may not be optimal due to domain mismatch
- The study does not address how corresponding sentences/verses were aligned across the three translations
- Qualitative expert assessment methodology is not detailed, making it difficult to assess interpretation of quantitative results

## Confidence

- **High Confidence**: The comparative framework using BERT sentiment analysis and MPNet semantic similarity is methodologically sound for evaluating translation quality differences between Google Translate and human translations.
- **Medium Confidence**: The finding that Google Translate produces lower sentiment and semantic similarity scores compared to human translations is supported by the quantitative results, though domain mismatch in sentiment analysis introduces uncertainty.
- **Low Confidence**: The attribution of translation errors specifically to lack of contextual and cultural understanding is based on qualitative expert assessment that lacks detailed methodology and may be subjective.

## Next Checks

1. Reproduce baseline similarity by comparing sentiment and semantic similarity scores between the two human expert translations to establish a baseline maximum similarity score, then compare Google Translate's performance against this baseline.

2. Test domain-specific sentiment analysis by creating or identifying a sentiment analysis model trained on literary text or Chinese novels rather than tweets, then re-run the sentiment analysis to see if results change significantly.

3. Analyze specific translation failures by conducting a detailed case study of specific mistranslations (particularly Chinese idioms and cultural references) to validate whether the attribution to lack of contextual understanding is accurate, using cross-linguistic expert review.