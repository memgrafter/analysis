---
ver: rpa2
title: 'Gradual Learning: Optimizing Fine-Tuning with Partially Mastered Knowledge
  in Large Language Models'
arxiv_id: '2410.05802'
source_url: https://arxiv.org/abs/2410.05802
tags:
- knowledge
- fine-tuning
- data
- known
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of large language models (LLMs)
  struggling to learn new knowledge during fine-tuning, leading to overfitting and
  hallucinations. The core idea is that LLMs can infer new knowledge from existing
  knowledge during fine-tuning, even if the new knowledge was not present in the original
  training data.
---

# Gradual Learning: Optimizing Fine-Tuning with Partially Mastered Knowledge in Large Language Models

## Quick Facts
- **arXiv ID**: 2410.05802
- **Source URL**: https://arxiv.org/abs/2410.05802
- **Reference count**: 8
- **Primary result**: 24% increase in knowledge classified as "Highly Known" after fine-tuning on WikiQA dataset, with improved overall test accuracy

## Executive Summary
This paper addresses the challenge of large language models (LLMs) struggling to acquire new knowledge during fine-tuning, which leads to overfitting and hallucinations. The authors propose that LLMs can infer new knowledge from existing partially mastered knowledge during fine-tuning due to the interconnected nature of knowledge within the model. Based on this hypothesis, they develop a two-stage fine-tuning strategy: first fine-tuning on partially mastered knowledge, then reassessing knowledge categories and fine-tuning again on improved mastery data while replaying fully mastered knowledge to prevent forgetting. The method achieves a 24% increase in knowledge classified as "Highly Known" along with improved overall test accuracy on the WikiQA dataset.

## Method Summary
The method employs a two-stage fine-tuning approach built on knowledge classification. First, knowledge is categorized into four types (Highly Known, Maybe Known, Weakly Known, Unknown) using a decoding strategy-based detection method. The first fine-tuning stage uses only Maybe Known data, with continual evaluation of knowledge categories at each epoch. After this stage, knowledge types are reassessed to identify improved mastery. The second stage fine-tunes on data showing improved mastery (originally Weakly Known or Unknown, now Maybe Known) while replaying 20% of Highly Known data to prevent forgetting. The approach uses LoRA for parameter-efficient fine-tuning with AdamW optimizer, cosine scheduler, and reduced learning rates (3e-4 for stage one, 15e-5 for stage two) along with weight decay (0.01) to prevent overfitting.

## Key Results
- 24% increase in knowledge classified as "Highly Known" after fine-tuning on WikiQA dataset
- Improved overall test accuracy compared to baseline fine-tuning methods
- Effective prevention of catastrophic forgetting through replay of fully mastered knowledge
- Significant portion of previously unmastered knowledge showed improved mastery after fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can infer new knowledge from partially mastered knowledge during fine-tuning
- Mechanism: As model mastery of partially known knowledge improves, it leverages interconnected knowledge structures to infer and master additional concepts not present in training data
- Core assumption: Knowledge within LLMs is structured and interconnected, allowing reasoning across related concepts
- Evidence anchors: [abstract] "Given the model's intrinsic reasoning abilities and the interconnectedness of different knowledge areas..."; [section] "Knowledge is interconnected, and it is possible to infer new knowledge from existing information."
- Break condition: If knowledge structures are not sufficiently interconnected or if reasoning abilities are limited, the inference mechanism fails

### Mechanism 2
- Claim: Fine-tuning on partially mastered knowledge prevents catastrophic forgetting of fully mastered knowledge
- Mechanism: Maintaining exposure to fully mastered knowledge through replay strategies while fine-tuning on partially mastered knowledge retains previously learned information
- Core assumption: Continual exposure to mastered knowledge during fine-tuning prevents overwriting of learned parameters
- Evidence anchors: [section] "This approach not only improves the model's overall test accuracy and knowledge retention but also preserves its accuracy on previously mastered content."
- Break condition: If replay ratio is insufficient or learning rate too high, forgetting occurs despite the strategy

### Mechanism 3
- Claim: Two-stage fine-tuning with knowledge category reassessment improves overall knowledge mastery
- Mechanism: Re-evaluating knowledge categories after initial fine-tuning identifies improved mastery, enabling targeted fine-tuning on newly mastered knowledge while preventing forgetting through replay
- Core assumption: Knowledge mastery categories are dynamic and can change significantly during fine-tuning
- Evidence anchors: [abstract] "In the second stage of fine-tuning, we used data categorized as 'Maybe Known' at the end of the first stage..."; [section] "We observed that following fine-tuning, a significant portion of knowledge showed improved mastery..."
- Break condition: If knowledge categories don't change meaningfully between stages, the reassessment adds no value

## Foundational Learning

- **Concept**: Knowledge classification based on decoding strategy performance
  - Why needed here: The entire method depends on accurately categorizing knowledge into Highly Known, Maybe Known, Weakly Known, and Unknown based on model probability of correct answers under different decoding strategies
  - Quick check question: How does the paper define the boundary between "Maybe Known" and "Weakly Known" knowledge?

- **Concept**: Catastrophic forgetting in continual learning
  - Why needed here: Understanding why models lose previously mastered knowledge during fine-tuning is essential for designing effective replay strategies
  - Quick check question: What specific techniques does the paper use to prevent catastrophic forgetting?

- **Concept**: Knowledge interconnectedness and reasoning in LLMs
  - Why needed here: The core hypothesis relies on the idea that LLMs can infer new knowledge from existing knowledge through reasoning across interconnected concepts
  - Quick check question: What evidence does the paper provide that knowledge within LLMs is truly interconnected rather than isolated?

## Architecture Onboarding

- **Component map**: Knowledge detection → First stage fine-tuning → Knowledge reassessment → Second stage fine-tuning with replay → Evaluation
- **Critical path**: The system processes knowledge through classification, applies two-stage fine-tuning with dynamic category reassessment, and uses replay to maintain previously mastered knowledge while acquiring new knowledge
- **Design tradeoffs**: The method trades computational cost of multiple generations and knowledge reassessment for improved knowledge acquisition and retention. Lower learning rates and replay strategies reduce forgetting but may slow convergence
- **Failure signatures**: Test accuracy plateaus or declines after second stage, knowledge categories don't change meaningfully between stages, or accuracy on Highly Known knowledge decreases despite replay
- **First 3 experiments**:
  1. Verify knowledge classification accuracy by testing the same model twice without fine-tuning to establish baseline variability
  2. Test single-stage fine-tuning on Maybe Known data only, with early stopping to find optimal epoch count
  3. Implement two-stage fine-tuning with various replay ratios to find optimal balance between new knowledge acquisition and forgetting prevention

## Open Questions the Paper Calls Out
None

## Limitations
- Knowledge classification method relies heavily on prompt construction and decoding strategies that are not fully specified
- Two-stage fine-tuning process requires substantial computational resources due to multiple knowledge evaluations and fine-tuning iterations
- Effectiveness depends on assumption that knowledge is truly interconnected within LLMs, which remains to be definitively proven

## Confidence
**High Confidence**: The general framework of two-stage fine-tuning with knowledge reassessment and replay strategies is well-established in continual learning literature. The 24% improvement in knowledge mastery is a specific, measurable outcome.

**Medium Confidence**: The specific mechanism by which LLMs infer new knowledge from partially mastered knowledge during fine-tuning is plausible but not definitively proven. While knowledge interconnectedness is supported by related work, the paper doesn't provide direct evidence of inference during fine-tuning.

**Low Confidence**: Exact implementation details for knowledge classification, including prompt construction and decoding parameters, are not fully specified, making exact replication challenging. Criteria for determining when to transition between fine-tuning stages are unclear.

## Next Checks
1. **Knowledge Classification Validation**: Implement the knowledge classification method and test its consistency by classifying the same model's knowledge twice without fine-tuning to establish baseline variability.

2. **Single-Stage Baseline Comparison**: Implement single-stage fine-tuning on Maybe Known data with early stopping to determine optimal epoch count and compare this baseline to the proposed two-stage approach.

3. **Generalization Testing**: Apply the gradual learning method to a different dataset (e.g., Natural Questions or TriviaQA) to test whether the 24% improvement in knowledge mastery generalizes beyond WikiQA.