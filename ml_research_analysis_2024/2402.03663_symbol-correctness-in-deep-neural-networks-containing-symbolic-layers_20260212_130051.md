---
ver: rpa2
title: Symbol Correctness in Deep Neural Networks Containing Symbolic Layers
arxiv_id: '2402.03663'
source_url: https://arxiv.org/abs/2402.03663
tags:
- symbol
- neural
- symbolic
- output
- correctness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of symbol correctness, a necessary
  property for explainability and transfer learning in neurosymbolic deep neural networks
  (NS-DNNs). Symbol correctness captures the correctness of intermediate symbols predicted
  by neural layers with respect to ground-truth symbolic representations.
---

# Symbol Correctness in Deep Neural Networks Containing Symbolic Layers

## Quick Facts
- arXiv ID: 2402.03663
- Source URL: https://arxiv.org/abs/2402.03663
- Reference count: 40
- One-line primary result: Symbol correctness is a necessary property for explainability and transfer learning in NS-DNNs, but cannot be trained for in general due to alternative mappings indistinguishable at the output level.

## Executive Summary
This paper introduces the concept of symbol correctness, a necessary property for explainability and transfer learning in neurosymbolic deep neural networks (NS-DNNs). Symbol correctness captures the correctness of intermediate symbols predicted by neural layers with respect to ground-truth symbolic representations. The authors formalize NS-DNNs and define symbol correctness, showing it cannot be trained for in general due to the existence of alternative mappings indistinguishable at the output level. They develop a model of NS-DNN training that frames real-world algorithms as attempting to simulate an ideal, oracular synthesizer by reconciling neural beliefs and symbolic information about ground-truth symbols. Experiments on visual addition demonstrate that three state-of-the-art synthesizers can lead to models with much higher output accuracy than symbol accuracy, and the circumstances under which this happens are consistent with the model.

## Method Summary
The paper introduces a framework for analyzing symbol correctness in NS-DNNs. The authors define NS-DNNs as networks containing a neural component, a symbolic layer, and a symbolic program. They formalize the concept of symbol correctness and show that it is necessary for explainability and transfer learning, but cannot be trained for in general due to alternative mappings indistinguishable at the output level. The authors develop a model of NS-DNN training that frames real-world algorithms as attempting to simulate an ideal, oracular synthesizer by reconciling neural beliefs and symbolic information about ground-truth symbols. Experiments on visual addition demonstrate that three state-of-the-art synthesizers can lead to models with much higher output accuracy than symbol accuracy, and the circumstances under which this happens are consistent with the model.

## Key Results
- Symbol correctness is a necessary property for NS-DNN explainability and transfer learning.
- Symbol correctness cannot be trained for in general due to alternative mappings indistinguishable at the output level.
- Experiments on visual addition show that state-of-the-art synthesizers can lead to models with much higher output accuracy than symbol accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symbol correctness is a necessary property for NS-DNN explainability and transfer learning.
- Mechanism: A symbol-correct model produces explanations consistent with both the output and the input data, enabling trust and modularity.
- Core assumption: The symbolic layer is correct and maps ground-truth symbols to correct outputs.
- Evidence anchors:
  - [abstract] "We demonstrate that symbol correctness is a necessary property for NS-DNN explainability and transfer learning"
  - [section] "A symbol-correct model can provide a satisfactory explanation for a correct prediction, an explanation that is consistent not only with the output, but also with the input data."
  - [corpus] Weak evidence - related papers focus on neural-symbolic systems but don't directly address symbol correctness as a necessary property.
- Break condition: If the symbolic layer is not correct, symbol correctness no longer guarantees output correctness.

### Mechanism 2
- Claim: Symbol correctness cannot be trained for in general due to alternative mappings indistinguishable at the output level.
- Mechanism: Multiple different mappings from inputs to symbols can produce the same output, making it impossible to learn the correct mapping without supervision at the neural-symbolic boundary.
- Core assumption: The symbolic layer allows for multiple inputs to produce the same output.
- Evidence anchors:
  - [abstract] "We demonstrate that symbol correctness is a necessary property for NS-DNN explainability and transfer learning (despite being in general impossible to train for)."
  - [section] "Symbol correctness requires learning a particular mapping from raw inputs to symbols, but there can exist alternative mappings that are indistinguishable from the desired mapping at the level of the model output."
  - [corpus] Weak evidence - related papers discuss neural-symbolic systems but don't address the fundamental impossibility of training for symbol correctness.
- Break condition: If the symbolic layer is injective (one-to-one mapping), symbol correctness becomes learnable.

### Mechanism 3
- Claim: NS-DNN training algorithms attempt to simulate an ideal oracular synthesizer by reconciling neural beliefs and symbolic information.
- Mechanism: Real-world synthesizers use strategies ranging from trusting neural predictions to skepticism of neural beliefs, integrating these with symbolic information about possible ground-truth symbols.
- Core assumption: The synthesizer has access to neural predictions and symbolic information about possible ground-truth symbols.
- Evidence anchors:
  - [abstract] "we develop an abstract model of training at the neural-symbolic boundary—in which a training algorithm attempts to learn a symbol-correct representation by integrating neural beliefs and symbolic information about the possible ground-truth symbol"
  - [section] "Different synthesizers use different strategies to synthesize this information into gradients... These algorithms (Wang et al., 2023; Li et al., 2023b) were designed for symbolic layers consisting of SMT formulas; by replacing SMT solving with answer set programming (ASP) solving (Brewka et al., 2011), the algorithms can be adapted to reason about Datalog (Bembenek et al., 2023)."
  - [corpus] Moderate evidence - the related papers discuss different synthesizers and their strategies for neuro-symbolic systems.
- Break condition: If either neural beliefs or symbolic information is highly informative, simpler synthesizer strategies may be sufficient.

## Foundational Learning

- Concept: Neural-symbolic boundary
  - Why needed here: Understanding how information flows between neural and symbolic components is crucial for grasping symbol correctness and training algorithms.
  - Quick check question: What happens at the neural-symbolic boundary in an NS-DNN?

- Concept: Ground-truth symbol function
  - Why needed here: The ground-truth symbol function represents the ideal mapping from inputs to symbols, which is the target for symbol correctness.
  - Quick check question: How is the ground-truth symbol function defined in relation to the input domain and symbolic representation?

- Concept: Symbol correctness vs output correctness
  - Why needed here: Distinguishing between these two types of correctness is essential for understanding the limitations of end-to-end training and the importance of symbol correctness.
  - Quick check question: Can a model be output correct but not symbol correct? Why or why not?

## Architecture Onboarding

- Component map: Raw input → Neural network → Grounding function → Symbolic program → Output
- Critical path: Raw input → Neural network → Grounding function → Symbolic program → Output
  - Symbol correctness depends on the correctness of both the neural network and the grounding function
- Design tradeoffs:
  - Trusting neural beliefs vs. skepticism of neural predictions in synthesizer design
  - Complexity of symbolic program vs. ease of training
  - Expressiveness of symbolic representation vs. tractability of reasoning
- Failure signatures:
  - High output accuracy but low symbol accuracy indicates the model learned an alternative mapping
  - Inconsistent performance across different initializations suggests sensitivity to neural network initialization
  - Poor performance on skewed data distributions indicates reliance on statistical patterns rather than symbolic reasoning
- First 3 experiments:
  1. Implement a simple visual addition task with MNIST digits and test different synthesizers
  2. Vary the initial neural network parameters to observe sensitivity to initialization
  3. Skew the input distribution to test robustness to distribution mismatch

## Open Questions the Paper Calls Out
None

## Limitations
- The practical feasibility of achieving symbol correctness in real-world NS-DNNs is uncertain.
- The paper relies on specific synthesizers (MULTIPLE, CLOSEST, RANDOM) in experiments, not exploring the full space of possible strategies.
- Experiments focus on a relatively simple visual addition task, which may not capture challenges of more complex neurosymbolic reasoning.

## Confidence
- High Confidence: The theoretical framework for symbol correctness and its relationship to explainability and transfer learning is well-established and logically sound.
- Medium Confidence: The claim that symbol correctness cannot be trained for in general due to alternative mappings is supported by the visual addition experiments, but the extent to which this generalizes to other domains is uncertain.
- Low Confidence: The practical implications of symbol correctness for real-world NS-DNN applications and the effectiveness of different synthesizer strategies are not fully explored.

## Next Checks
1. Extend experiments to more complex tasks that require more sophisticated symbolic reasoning, such as natural language inference or visual question answering, to assess the generalizability of symbol correctness.
2. Investigate additional synthesizer algorithms that combine neural beliefs and symbolic information in different ways, and evaluate their effectiveness in achieving symbol correctness across various NS-DNN architectures.
3. Conduct experiments with intentionally skewed input distributions to understand how the ratio of symbol-correct to symbol-incorrect training data affects the likelihood of learning a symbol-correct model, as suggested by the theoretical framework.