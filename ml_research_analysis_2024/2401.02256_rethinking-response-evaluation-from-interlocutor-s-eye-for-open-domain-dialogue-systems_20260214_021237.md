---
ver: rpa2
title: Rethinking Response Evaluation from Interlocutor's Eye for Open-Domain Dialogue
  Systems
arxiv_id: '2401.02256'
source_url: https://arxiv.org/abs/2401.02256
tags:
- dialogue
- interlocutor
- evaluation
- systems
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how to better evaluate open-domain dialogue
  systems from the perspective of the interlocutor. The key findings are: Predicting
  interlocutor scores requires personalizing the evaluation model to the target interlocutor,
  in addition to using interlocutor scores as reference.'
---

# Rethinking Response Evaluation from Interlocutor's Eye for Open-Domain Dialogue Systems

## Quick Facts
- arXiv ID: 2401.02256
- Source URL: https://arxiv.org/abs/2401.02256
- Authors: Yuma Tsuta; Naoki Yoshinaga; Shoetsu Sato; Masashi Toyoda
- Reference count: 17
- One-line primary result: Personalizing dialogue response evaluators to interlocutors improves correlation with human judgments, and dialogue continuity prediction can train such evaluators without costly annotations.

## Executive Summary
This paper addresses the challenge of evaluating open-domain dialogue systems from the perspective of the interlocutor, focusing on engagement rather than traditional reference-based metrics. The key insight is that different interlocutors have varying preferences for what constitutes an engaging response, necessitating personalized evaluation models. The authors propose training interlocutor-aware response evaluators using a dialogue continuity prediction (DCP) task, which leverages naturally occurring conversation stop signals as training labels, eliminating the need for costly human annotations.

Experiments on Twitter conversations demonstrate that models aware of the target interlocutor's identity show significantly higher correlation with interlocutor scores compared to generic evaluators. The DCP-trained evaluator effectively captures interlocutor perspective for human responses, though it still struggles with system-generated responses. The work highlights the importance of considering interlocutor perspective in dialogue system evaluation and provides a practical approach to training personalized evaluators using naturally occurring data.

## Method Summary
The paper proposes training interlocutor-aware response evaluators using dialogue continuity prediction (DCP) as a training task. The method involves collecting conversation logs from X (formerly Twitter) and preprocessing them to create training samples where responses followed by no reply indicate low engagement. A BERT-based classifier is fine-tuned on the DCP task, predicting whether an interlocutor will continue the conversation. Personalization is achieved through speaker-specific tokens or profile text integration. The trained evaluator is then validated on human responses from annotated conversations, measuring correlation with interlocutor scores. The approach eliminates the need for costly human annotations by leveraging naturally occurring conversation stop signals as labels.

## Key Results
- Personalizing evaluators to interlocutors significantly improves correlation with interlocutor scores compared to generic models
- Dialogue continuity prediction effectively trains interlocutor-aware evaluators without human feedback
- The DCP-trained evaluator correlates well with interlocutor scores for human responses but struggles with system responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Personalizing the evaluator to the target interlocutor improves correlation with interlocutor scores.
- Mechanism: Using a speaker-specific token for each interlocutor allows the evaluator to adapt its scoring criteria to individual preferences and conversational patterns.
- Core assumption: Different interlocutors have distinct preferences for what constitutes an engaging response, and these preferences can be captured by speaker-specific embeddings.
- Evidence anchors:
  - [abstract] "Models that are aware of the target interlocutor's identity show much higher correlation with actual interlocutor scores."
  - [section] "These results suggest that automatic interlocutor evaluation requires us to not only take the interlocutors' view (here, scores) into account but also to be aware of the target interlocutor."
  - [corpus] Weak evidence - corpus contains related works on persona-grounded dialogue but no direct evidence on interlocutor-specific evaluation correlation.
- Break condition: If interlocutor preferences are too diverse or inconsistent to be captured by a single token embedding.

### Mechanism 2
- Claim: Dialogue continuity prediction (DCP) can train an interlocutor-aware evaluator without human feedback.
- Mechanism: DCP uses naturally occurring conversation stop signals as labels, where a response followed by no reply indicates low engagement, allowing the model to learn interlocutor preferences from real conversation data.
- Core assumption: The decision to continue or end a conversation is a reliable proxy for response engagement from the interlocutor's perspective.
- Evidence anchors:
  - [abstract] "The second experiment using massive conversations on X (formerly Twitter) confirmed that dialogue continuity prediction can train an interlocutor-aware response evaluator without human feedback."
  - [section] "This task of estimating whether the target speaker will continue speaking or not can take advantage of labels (conversation stop signals) that are naturally annotated by the interlocutor in the conversation log."
  - [corpus] No direct corpus evidence - related works discuss dialogue continuity but not as a training signal for evaluation.
- Break condition: If conversation continuation is influenced by factors unrelated to response quality (e.g., time constraints, external interruptions).

### Mechanism 3
- Claim: Evaluator performance depends on training sample size for the target interlocutor.
- Mechanism: More conversation samples per interlocutor provide better personalization, allowing the evaluator to learn individual preferences more accurately.
- Core assumption: Interlocutor preferences are learnable from sufficient conversation data and will improve evaluator accuracy with more samples.
- Evidence anchors:
  - [section] "We confirmed that, with the exception of a peak around 400 samples, the accuracy changed only slightly below 1200 samples, improved above 1200 samples, and overall, the personalized models outperformed the BERT-DCP."
  - [corpus] Weak evidence - corpus contains related works on dialogue evaluation but no direct evidence on sample size requirements.
- Break condition: If interlocutor preferences are too complex to be captured even with large amounts of data, or if preferences change over time.

## Foundational Learning

- Concept: Dialogue evaluation metrics and their correlation with human judgments
  - Why needed here: Understanding the landscape of existing evaluation methods and their limitations is crucial for appreciating why interlocutor-aware evaluation is necessary.
  - Quick check question: Why do traditional reference-based metrics like BLEU fail to correlate with human judgments in open-domain dialogue?

- Concept: Personalization in dialogue systems
  - Why needed here: The paper's core insight is that personalization improves evaluation, so understanding how personalization works in dialogue generation is essential.
  - Quick check question: How do speaker tokens help personalize dialogue responses to individual interlocutors?

- Concept: Supervised learning with limited human annotations
  - Why needed here: The paper proposes using DCP as an alternative to costly human annotations, so understanding how to leverage naturally occurring labels is important.
  - Quick check question: What are the advantages and limitations of using naturally occurring conversation data as training labels?

## Architecture Onboarding

- Component map:
  - Input: Conversation context + speaker information (token or profile) -> BERT-based classifier -> Output: Probability of conversation continuation (engagement score) -> Personalization layer: Speaker-specific embeddings or profile text integration

- Critical path:
  1. Preprocess conversation data with speaker labels
  2. Train BERT-DCP model on dialogue continuity task
  3. Personalize model with speaker tokens or profiles
  4. Evaluate correlation with interlocutor scores

- Design tradeoffs:
  - Speaker token vs. profile text for personalization: Tokens are simpler but profiles may capture richer information
  - Binary classification vs. regression for DCP: Classification is simpler but regression might capture nuanced engagement levels
  - Model complexity vs. data efficiency: More complex personalization may require more training data

- Failure signatures:
  - Poor correlation with interlocutor scores despite personalization
  - Performance degradation when evaluating system responses vs. human responses
  - Overfitting to specific interlocutors with limited training data

- First 3 experiments:
  1. Ablation study: Compare evaluator performance with/without interlocutor awareness using Hazumi dataset
  2. DCP training: Train BERT-DCP model on X dataset and evaluate dialogue continuity prediction accuracy
  3. Correlation validation: Test personalized evaluator's correlation with interlocutor scores on human vs. system responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of conversation logs for a target interlocutor affect the performance of interlocutor-aware response evaluators?
- Basis in paper: [inferred] The paper discusses that the performance of interlocutor-aware evaluators will be affected by the size of conversation logs given by the target interlocutor, and mentions investigating the relationship between training sample size and performance.
- Why unresolved: The paper mentions that the performance could be poor for users with few conversations in the training data, but does not provide detailed analysis or conclusions on how sample size affects performance.
- What evidence would resolve it: Conducting experiments to measure the performance of interlocutor-aware evaluators across different sample sizes for target interlocutors, and identifying the minimum sample size required for reliable performance.

### Open Question 2
- Question: Can the proposed dialogue continuity prediction (DCP) task be extended to evaluate other aspects of dialogue systems beyond engagement, such as coherence or informativeness?
- Basis in paper: [inferred] The paper mentions that existing studies evaluate dialogue systems using various metrics such as understandability and informativeness, and suggests investigating interlocutor-aware evaluation for other metrics.
- Why unresolved: The paper focuses on evaluating dialogue systems in terms of engagement using the DCP task, but does not explore its applicability to other evaluation metrics.
- What evidence would resolve it: Extending the DCP task to predict other aspects of dialogue quality (e.g., coherence, informativeness) and evaluating its effectiveness in capturing these aspects compared to existing methods.

### Open Question 3
- Question: How can the proposed interlocutor-aware response evaluator be improved to better handle system-generated responses, which currently show lower correlation with human judgments compared to human responses?
- Basis in paper: [explicit] The paper mentions that the interlocutor-aware evaluators show lower correlation with human judgments for system responses compared to human responses, indicating a need for further improvements.
- Why unresolved: The paper identifies the challenge of evaluating system responses but does not provide specific strategies or solutions to address this issue.
- What evidence would resolve it: Developing and testing new techniques or modifications to the interlocutor-aware evaluator that specifically target the evaluation of system-generated responses, and measuring the improvement in correlation with human judgments.

## Limitations
- The approach requires substantial conversation data per interlocutor (1200+ samples) for optimal performance, which may be impractical in many applications
- The evaluator struggles significantly with system-generated responses, showing much lower correlation with human judgments compared to human responses
- The methodology relies on Twitter conversation data, which may not generalize well to other conversational contexts due to platform-specific norms and behaviors

## Confidence

**High Confidence:** The finding that personalizing evaluation models to specific interlocutors improves correlation with human judgments is well-supported by the experimental results. The ablation studies clearly demonstrate that speaker-aware models outperform generic evaluators across multiple datasets.

**Medium Confidence:** The effectiveness of dialogue continuity prediction as a training signal for interlocutor-aware evaluation is promising but requires further validation. While the correlation results with human responses are positive, the methodology's limitations with system responses and potential domain bias in the Twitter data warrant caution in generalizing these findings.

**Low Confidence:** The claim that DCP can fully replace human feedback for training interlocutor-aware evaluators is premature. The methodology shows promise but has significant gaps, particularly in handling system responses and the dependency on large amounts of interlocutor-specific training data.

## Next Checks

1. **Cross-domain validation:** Test the personalized evaluator on conversation datasets from multiple platforms (e.g., Reddit, customer service logs, in-person dialogues) to assess whether the interlocutor personalization approach generalizes beyond Twitter conversations.

2. **System response evaluation:** Conduct targeted experiments to understand why the evaluator struggles with system responses, including controlled tests comparing system responses to human responses across multiple dialogue systems to identify specific failure patterns.

3. **Profile enrichment study:** Compare the speaker token personalization approach against models that incorporate richer interlocutor profile information (bio text, posting history, interaction patterns) to determine if more sophisticated personalization methods yield better evaluation performance.