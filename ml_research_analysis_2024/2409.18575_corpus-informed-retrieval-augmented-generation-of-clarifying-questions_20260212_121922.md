---
ver: rpa2
title: Corpus-informed Retrieval Augmented Generation of Clarifying Questions
arxiv_id: '2409.18575'
source_url: https://arxiv.org/abs/2409.18575
tags:
- evidence
- facets
- questions
- clarifying
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating corpus-informed
  clarifying questions for web search, ensuring that questions align with the available
  information in the retrieval corpus. The authors propose using Retrieval Augmented
  Generation (RAG) models, specifically Fusion-in-Decoder (FiD), to jointly model
  user queries and the retrieval corpus, eliminating the need for intermediate feature
  extraction steps.
---

# Corpus-informed Retrieval Augmented Generation of Clarifying Questions

## Quick Facts
- arXiv ID: 2409.18575
- Source URL: https://arxiv.org/abs/2409.18575
- Reference count: 40
- This paper proposes using Retrieval Augmented Generation (RAG) with Fusion-in-Decoder (FiD) to generate corpus-informed clarifying questions for web search, addressing the problem of "hallucinations" where models suggest intents not present in the retrieval corpus.

## Executive Summary
This paper addresses the challenge of generating clarifying questions that are informed by the available information in a retrieval corpus for web search applications. The authors demonstrate that current datasets often suffer from a disconnect between document evidence and ground truth clarification questions, leading to models that "hallucinate" by suggesting search intents not present in the corpus. To address this, they propose using Retrieval Augmented Generation (RAG) with Fusion-in-Decoder (FiD) models, which can jointly model user queries and the retrieval corpus to generate corpus-informed clarifications. The paper also explores dataset augmentation methods to align ground truth clarifications with the retrieval corpus and techniques to enhance evidence pool relevance during inference.

## Method Summary
The authors propose using Fusion-in-Decoder (FiD) models for generating corpus-informed clarifying questions. They train FiD models on the MIMICS dataset using different evidence sets (BING snippets, BM25, and Contriever retrieved documents) and evaluate performance on facet generation using metrics like Term Overlap, Exact Match, Set-BERT, and Set-BLEU. The approach includes dataset augmentation to align ground truth clarifications with the retrieval corpus, and exploration of evidence grounding techniques to improve relevance during inference. The method aims to ensure that generated clarifications remain faithful to the actual content available in the corpus rather than suggesting unsupported intents.

## Key Results
- FiD models outperform baselines in generating corpus-informed clarifying questions across precision and semantic similarity metrics
- Evidence grounding and dataset alignment are critical for preventing model hallucinations and ensuring generated questions remain faithful to corpus content
- Current evaluation frameworks and datasets are not well-suited for assessing corpus-informed clarification generation due to the disconnect between ground truth facets and available corpus evidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG enables joint modeling of user queries and the retrieval corpus to pinpoint uncertainty and ask for clarifications end-to-end.
- Mechanism: RAG combines language models for text generation with retrieval systems that provide access to a large non-parametric memory (document collection). This allows the model to generate clarifying questions informed by actual corpus content.
- Core assumption: The retrieval corpus contains information relevant to the user's query and potential clarifying questions.
- Evidence anchors:
  - [abstract] "We demonstrate the effectiveness of Retrieval Augmented Language Models (RAG) in this process, emphasising their ability to (i) jointly model the user query and retrieval corpus to pinpoint the uncertainty and ask for clarifications end-to-end"
  - [section] "Since the generation model cannot read the entire corpus, Retrieval Augmentation helps to inform the generator of the possible information needs present in the collection."

### Mechanism 2
- Claim: FiD models are computationally efficient in modeling multiple evidence documents, allowing for broader corpus coverage when generating questions.
- Mechanism: FiD is an encoder-decoder model where the encoder models input documents independently (no cross-attentions), producing individual embeddings. The decoder then fuses information from those embeddings to generate an output answer.
- Core assumption: The efficiency gains of FiD allow for modeling a larger portion of the corpus, leading to more informed clarifying questions.
- Evidence anchors:
  - [abstract] "we propose using Retrieval Augmented Generation for generating clarifying questions end-to-end and show that Fusion-in-Decoder is an effective approach on this task"
  - [section] "To apply Retrieval Augmentation effectively, it is crucial to (a) select a sample of the corpus that is representative of users' potential information needs, and (b) account for as many relevant documents as possible to cover those. For this reason, we propose using Fusion-in-Decoders (FiD), a family of models [12] that are computationally efficient in modelling multiple evidence documents."

### Mechanism 3
- Claim: Aligning ground truth clarifications with the retrieval corpus during training prevents model hallucinations and ensures generated questions remain faithful to evidence documents.
- Mechanism: By ensuring evidence documents contain ground truth facets (possible search intents), the model learns to generate clarifying questions grounded in the corpus rather than suggesting unsupported intents.
- Core assumption: Alignment between evidence documents and ground truth facets is necessary for the model to learn to generate corpus-informed clarifications.
- Evidence anchors:
  - [abstract] "we observe that in current datasets search intents are largely unsupported by the corpus, which is problematic both for training and evaluation. This causes question generation models to 'hallucinate', ie. suggest intents that are not in the corpus, which can have detrimental effects in performance."
  - [section] "To address this, we propose dataset augmentation methods that align the ground truth clarifications with the retrieval corpus."

## Foundational Learning

- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: RAG combines language models for text generation with retrieval systems that provide access to a large non-parametric memory (document collection), allowing for generating clarifying questions informed by actual corpus content.
  - Quick check question: How does RAG differ from traditional language models in terms of access to information?

- Concept: Fusion-in-Decoder (FiD)
  - Why needed here: FiD is computationally efficient in modeling multiple evidence documents, allowing for broader corpus coverage when generating questions, potentially leading to more informed clarifying questions.
  - Quick check question: What is the key architectural difference between FiD and other encoder-decoder models that enables its efficiency?

- Concept: Dataset alignment
  - Why needed here: Aligning ground truth clarifications with the retrieval corpus during training prevents model hallucinations and ensures generated questions remain faithful to evidence documents.
  - Quick check question: Why is it important to ensure that evidence documents contain the ground truth facets during training?

## Architecture Onboarding

- Component map: User query -> Retrieval system (BM25, Contriever, or Bing snippets) -> Fusion-in-Decoder (FiD) model -> Generated clarifying question and facets
- Critical path:
  1. User query is received
  2. Retrieval system fetches relevant documents from the corpus
  3. FiD model processes the query and retrieved documents
  4. FiD model generates a clarifying question and associated facets
  5. Clarifying question and facets are presented to the user
- Design tradeoffs:
  - Using FiD allows for modeling more evidence documents but may sacrifice some cross-document interactions compared to other encoder-decoder models
  - Aligning the dataset requires additional preprocessing but can improve the quality and faithfulness of generated questions
- Failure signatures:
  - Generated clarifying questions are not relevant to the user's query or the corpus
  - Model "hallucinates" by suggesting intents that are not present in the corpus
  - Performance metrics (e.g., Exact Match) are low
- First 3 experiments:
  1. Compare FiD with other encoder-decoder models (e.g., BART) on the task of generating clarifying questions
  2. Investigate the effect of aligning the dataset on the quality and faithfulness of generated questions
  3. Explore the use of different retrieval methods (e.g., BM25, Contriever) and their impact on generated questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal evidence set size and composition for training and evaluating corpus-informed clarifying question generation models?
- Basis in paper: [explicit] The authors state "It is crucial to (a) select a sample of the corpus that is representative of users' potential information needs, and (b) account for as many relevant documents as possible to cover those" and explore increasing the number of evidence documents in their experiments.
- Why unresolved: The paper shows that increasing the number of evidence documents can improve performance when the evidence is well-aligned with the target facets, but also notes that adding irrelevant documents can deteriorate performance. The optimal balance between evidence set size, quality, and alignment is not definitively established.
- What evidence would resolve it: Systematic experiments varying evidence set size, retrieval methods, and alignment metrics across different query types and domains could identify the optimal evidence set composition for various scenarios.

### Open Question 2
- Question: How can we develop evaluation frameworks that better reflect the objective of generating corpus-informed clarifications, given the disconnect between ground truth facets and the retrieval corpus?
- Basis in paper: [explicit] The authors conclude that "current test collections and evaluation frameworks are not suitable for evaluating Corpus-Informed clarifying question generation" and highlight the gap between ground truth facets (constructed from query reformulations and static taxonomies) and those present in search collections.
- Why unresolved: The paper identifies the problem but does not propose a concrete solution for developing new evaluation frameworks that account for the multiple valid facets that could exist in a corpus.
- What evidence would resolve it: Development and validation of new evaluation metrics that consider the diversity of valid facets in a corpus, perhaps through human judgments of facet relevance and corpus alignment, or through automated methods that assess the presence of generated facets in the actual retrieval corpus.

### Open Question 3
- Question: What are the most effective methods for inducing novelty in the evidence pool during inference to capture ground truth facets in an open-domain setting?
- Basis in paper: [explicit] The authors explore two methods for introducing novelty (MMR and knowledge distillation from reader to retriever) but conclude that "bringing up documents related to the ground truth facets remains a challenge in this dataset and task."
- Why unresolved: While the authors test specific novelty methods, the paper suggests that more effective approaches may exist, particularly given the limitations of MMR (which increases diversity but hurts relevance) and the modest improvements from knowledge distillation.
- What evidence would resolve it: Comparative experiments testing a wider range of novelty methods, including probabilistic retrieval approaches, hybrid lexical-semantic methods, and advanced diversity-aware ranking algorithms, could identify more effective strategies for capturing ground truth facets during inference.

## Limitations
- The analysis reveals that only 20% of clarification facets in MIMICS-Click are supported by the MSMarco corpus, suggesting a fundamental mismatch between training data and retrieval content
- The evaluation relies heavily on the MIMICS dataset, which appears to have systematic biases (e.g., low query diversity, taxonomy-like facets)
- The paper doesn't provide human evaluation or assess whether generated clarifications actually improve downstream search performance

## Confidence
- **High Confidence**: The effectiveness of FiD for modeling multiple evidence documents is well-established in the literature and the experimental results align with expectations. The computational efficiency gains are measurable and reproducible.
- **Medium Confidence**: The dataset alignment approach shows promise, but the relatively small improvement in supported facets (from 20% to 28%) suggests limited practical impact. The reliance on a single dataset makes generalization uncertain.
- **Low Confidence**: Claims about the general applicability of corpus-informed clarification generation across different domains and datasets are not well-supported by the experimental evidence.

## Next Checks
1. **Cross-Dataset Validation**: Evaluate the trained models on alternative clarification datasets (e.g., from other domains or languages) to assess generalization beyond MIMICS.
2. **Human Evaluation**: Conduct user studies to measure whether corpus-informed clarifications actually improve search satisfaction and whether generated questions are perceived as helpful and natural.
3. **Corpus Coverage Analysis**: Perform a detailed analysis of what types of information needs cannot be supported by the corpus and whether this represents a fundamental limitation or an opportunity for corpus expansion.