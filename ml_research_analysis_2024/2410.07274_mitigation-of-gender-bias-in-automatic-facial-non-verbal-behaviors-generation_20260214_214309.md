---
ver: rpa2
title: Mitigation of gender bias in automatic facial non-verbal behaviors generation
arxiv_id: '2410.07274'
source_url: https://arxiv.org/abs/2410.07274
tags:
- behaviors
- non-verbal
- gender
- data
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses gender bias in the automatic generation of
  facial non-verbal behaviors for socially interactive agents. The authors propose
  FairGenderGen, a new model that integrates a gender classifier with a gradient reversal
  layer into their previous FaceGen model to generate facial non-verbal behaviors
  from speech features while mitigating gender sensitivity.
---

# Mitigation of gender bias in automatic facial non-verbal behaviors generation

## Quick Facts
- arXiv ID: 2410.07274
- Source URL: https://arxiv.org/abs/2410.07274
- Reference count: 40
- Primary result: FairGenderGen reduces gender classifier accuracy from 80.69% to 48.61% while maintaining speech-behavior coordination

## Executive Summary
This paper addresses gender bias in automatic generation of facial non-verbal behaviors for socially interactive agents. The authors propose FairGenderGen, which integrates a gender classifier with a gradient reversal layer into their previous FaceGen model to generate behaviors independent of speaker gender. The model successfully reduces gender bias as measured by classifier accuracy while preserving coordination with speech, though subjective evaluations reveal a disparity in perceived believability between male and female behaviors.

## Method Summary
FairGenderGen builds upon the FaceGen architecture by adding a gender classifier branch connected via a gradient reversal layer to the latent representation. During training, the model generates facial non-verbal behaviors from speech features while the gradient reversal forces the encoder to produce gender-invariant latent representations. The model is trained using reconstruction loss, Wasserstein adversarial loss with gradient penalty, and binary cross-entropy loss for the gender classifier. Behavioral features (head pose, gaze, and AUs) are extracted from the Trueness corpus using OpenFace, and speech features are obtained using Hubert.

## Key Results
- Gender classifier accuracy drops from 80.69% on FaceGen outputs to 48.61% on FairGenderGen outputs
- Subjective evaluation shows preserved coordination between generated behaviors and speech
- Female-generated behaviors receive lower believability ratings than male behaviors, despite bias mitigation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FairGenderGen reduces gender bias by using gradient reversal in the latent space to make male and female behavior distributions indistinguishable.
- Mechanism: A gender classifier is attached to the latent representation of FaceGen's encoder. During training, gradient reversal multiplies gradients by a negative factor, forcing the encoder to produce gender-invariant features. This directly minimizes the ability of the classifier to distinguish male vs. female, while preserving speech-to-behavior mapping.
- Core assumption: Gender bias in generated behaviors stems from identifiable features in the latent space that can be suppressed without destroying naturalness.
- Evidence anchors:
  - [abstract] "We present a new model, FairGenderGen, which integrates a gender discriminator and a gradient reversal layer into our previous behavior generation model."
  - [section] "The gender classifier is connected to the encoder via a gradient reversal layer... Gradient reversal ensures that the distributions over the two genders are made as indistinguishable as possible."
- Break condition: If the gender classifier's accuracy does not drop below 50% on FairGenderGen outputs, the gradient reversal is ineffective.

### Mechanism 2
- Claim: The adversarial training between generator and discriminator preserves temporal coordination between speech and generated behavior.
- Mechanism: Both FaceGen and FairGenderGen include a discriminator that evaluates the plausibility of (speech, behavior) pairs. The generator is optimized to fool this discriminator while minimizing reconstruction error, which enforces synchronization.
- Core assumption: The discriminator can effectively judge temporal alignment without needing explicit linguistic or prosodic alignment cues.
- Evidence anchors:
  - [section] "The model is build upon the work of Delbosc et al. [...] The generator generates data by sampling from a noise z and speech features Fs... both modules receive speech features Fs [0:T], allowing the discriminator to evaluate the believability of the temporal alignment between behavioral and speech features."
- Break condition: If generated behaviors fail to align with speech prosody or rhythm in subjective evaluations, the adversarial loss is insufficient.

### Mechanism 3
- Claim: Reducing gender bias may shift the perceived believability of behaviors due to societal expectations, not model quality.
- Mechanism: By forcing gender distributions to converge, FairGenderGen generates behaviors that are less stereotypical. This may lead to lower believability scores for female behaviors because they no longer conform to higher societal expectations, while male behaviors improve.
- Core assumption: Perceived believability is influenced by stereotypical expectations rather than just naturalness or coordination.
- Evidence anchors:
  - [section] "Society has higher expectations of women when it comes to non-verbal behavior... Our efforts to mitigate gender bias in generated non-verbal behaviors resulted in a notable disparity in perceived believability performance between males and females."
- Break condition: If a new subjective study shows no difference in believability after explicitly controlling for stereotypical expectations, the mechanism is invalid.

## Foundational Learning

- Concept: Gradient reversal layer
  - Why needed here: Enables domain adaptation by making the latent representation invariant to gender while still being discriminative for behavior generation.
  - Quick check question: What happens to the gradients flowing into the encoder when the gender classifier is attached with a gradient reversal layer?

- Concept: Adversarial training (GAN-style)
  - Why needed here: Ensures generated behaviors are temporally coherent with speech by having a discriminator judge (speech, behavior) pairs.
  - Quick check question: Why does the generator receive both the adversarial loss and the reconstruction loss?

- Concept: Domain adaptation
  - Why needed here: The goal is to adapt the behavior generation model so that its outputs are invariant to gender, treating gender as a domain to be neutralized.
  - Quick check question: How does the model ensure that gender-invariant features still preserve the core speech-to-behavior mapping?

## Architecture Onboarding

- Component map:
  Speech features → Encoder → Gradient reversal → Gender classifier (training only)
  Latent representation → Three parallel decoders (head, gaze, AUs) → Generated behavior
  Speech + behavior → Discriminator → Adversarial loss

- Critical path:
  Speech → Encoder → Latent → Decoder → Generated behavior → Discriminator

- Design tradeoffs:
  - Smaller discriminator vs. larger one: Faster training but may miss subtle misalignment cues.
  - Noise injection in latent space vs. none: Improves diversity but may reduce fidelity.
  - Gradual increase of gradient reversal factor vs. immediate: Stabilizes training but may slow bias reduction.

- Failure signatures:
  - Gender classifier still >50% accurate on FairGenderGen outputs → gradient reversal ineffective.
  - DTW between male/female distributions not reduced → bias mitigation not working.
  - Subjective believability drops sharply → over-regularization.

- First 3 experiments:
  1. Run gender classifier on FairGenderGen outputs to verify accuracy <50%.
  2. Compute DTW distances between male/female distributions to confirm convergence.
  3. Conduct a small subjective study (10 participants) on 2-3 generated clips to verify coordination is preserved.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can generative models be designed to produce non-verbal behaviors that maintain both fairness and perceived believability across genders without relying on gender stereotypes?
- Basis in paper: [explicit] The paper discusses the challenge of balancing fairness (reducing gender bias) with maintaining perceived believability, particularly noting that efforts to mitigate bias resulted in female behaviors being rated as less believable.
- Why unresolved: The paper identifies the tension between reducing bias and maintaining perceived believability, but does not provide a solution that successfully balances both aspects.
- What evidence would resolve it: Development and testing of a generative model that produces non-verbal behaviors with equal perceived believability across genders while effectively reducing gender bias, validated through both subjective and objective evaluations.

### Open Question 2
- Question: What are the specific non-verbal behavior patterns that contribute most significantly to gender bias in automatic facial behavior generation, and how can these be isolated and modified without compromising overall naturalness?
- Basis in paper: [inferred] The paper's findings suggest that certain non-verbal behaviors are more gender-associated, but does not specify which features contribute most to bias or how to modify them.
- Why unresolved: The paper demonstrates the presence of gender bias but does not identify the specific non-verbal features driving this bias or provide methods to selectively modify them.
- What evidence would resolve it: Detailed analysis of which specific non-verbal features (e.g., gaze patterns, head movements, facial expressions) are most gender-associated, followed by successful modification of these features in a way that maintains naturalness.

### Open Question 3
- Question: How can fairness in non-verbal behavior generation be defined and measured in a way that goes beyond gender and encompasses other sensitive attributes such as cultural or racial differences?
- Basis in paper: [explicit] The paper mentions that the reflections on gender bias could be extended to other sensitive variables such as cultural or racial differences, but does not explore this.
- Why unresolved: The paper focuses specifically on gender bias and acknowledges the need to extend this to other attributes, but does not provide a framework for doing so.
- What evidence would resolve it: Development of a comprehensive framework for defining and measuring fairness in non-verbal behavior generation that includes multiple sensitive attributes, along with empirical validation across diverse datasets.

## Limitations
- The study uses binary gender classification, not capturing the full spectrum of gender identities
- Subjective evaluation relies on Mechanical Turk participants who may not represent diverse perspectives on gender expression
- The model only generates behaviors for listening scenarios, limiting generalizability to other conversational contexts

## Confidence

- **High Confidence**: The mechanism of using gradient reversal to reduce gender classifier accuracy is technically sound and well-implemented
- **Medium Confidence**: The subjective evaluation results showing coordination preservation are reliable, though the believability disparity warrants further investigation
- **Low Confidence**: The interpretation that believability disparities stem from societal expectations rather than model quality is speculative without additional controlled studies

## Next Checks
1. Test gender classifier on FairGenderGen outputs with stratified evaluation across different speaker demographics to verify consistent performance below 50% accuracy
2. Conduct a controlled subjective study with explicit instructions about stereotypical expectations to determine if believability differences persist when participants are primed to evaluate naturalness rather than social expectations
3. Evaluate FairGenderGen on non-listening scenarios (speaking, turn-taking) to assess generalizability of bias mitigation across different conversational contexts