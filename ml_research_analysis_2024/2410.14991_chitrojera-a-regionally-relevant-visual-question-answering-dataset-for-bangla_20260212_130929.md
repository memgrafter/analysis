---
ver: rpa2
title: 'ChitroJera: A Regionally Relevant Visual Question Answering Dataset for Bangla'
arxiv_id: '2410.14991'
source_url: https://arxiv.org/abs/2410.14991
tags:
- question
- bangla
- answer
- image
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChitroJera, the first large-scale, regionally
  relevant Bangla visual question answering (VQA) dataset. It addresses the lack of
  culturally specific data in Bangla VQA by curating over 15k samples from locally
  relevant sources and synthesizing questions using GPT-4 Turbo.
---

# ChitroJera: A Regionally Relevant Visual Question Answering Dataset for Bangla

## Quick Facts
- **arXiv ID:** 2410.14991
- **Source URL:** https://arxiv.org/abs/2410.14991
- **Reference count:** 40
- **Primary result:** Introduces the first large-scale, regionally relevant Bangla VQA dataset with 15k samples, demonstrating improved performance through culturally specific data and dual-encoder architectures.

## Executive Summary
This paper introduces ChitroJera, the first large-scale, regionally relevant Bangla visual question answering (VQA) dataset. It addresses the lack of culturally specific data in Bangla VQA by curating over 15k samples from locally relevant sources and synthesizing questions using GPT-4 Turbo. The dataset emphasizes cultural and geographic context specific to Bangla-speaking regions, with rigorous annotation by linguistic experts.

The authors evaluate a range of models including unimodal encoders, multimodal VLMs, and novel dual-encoder architectures that outperform existing models of similar scale. They also benchmark large vision-language models via zero-shot prompting, achieving the best overall performance. Experiments reveal that pretraining dual encoders improves accuracy by 2-3%, and that proprietary models significantly outperform open-source ones. The dataset and experiments highlight the importance of regional relevance and strong modality alignment for low-resource languages like Bangla.

## Method Summary
ChitroJera was created by curating locally relevant images from Bangla-speaking regions and generating corresponding questions using GPT-4 Turbo. The dataset underwent rigorous annotation by linguistic experts to ensure cultural and geographic accuracy. The authors evaluated multiple model architectures including unimodal encoders, multimodal vision-language models (VLMs), and dual-encoder architectures specifically designed for the dataset. They also tested large VLMs through zero-shot prompting. The dataset construction emphasizes regional specificity in both visual content and question formulation, addressing the gap in culturally relevant Bangla VQA data.

## Key Results
- Dual-encoder architectures outperform existing models of similar scale on the ChitroJera dataset
- Pretraining dual encoders improves accuracy by 2-3% over baseline approaches
- Large VLMs achieve best performance through zero-shot prompting, though proprietary models significantly outperform open-source alternatives
- The dataset successfully captures culturally and geographically relevant content specific to Bangla-speaking regions

## Why This Works (Mechanism)
The success of ChitroJera stems from addressing the fundamental limitation in existing Bangla VQA datasets: lack of regional and cultural relevance. By curating images and questions specific to Bangla-speaking regions, the dataset provides training data that better aligns with the actual visual and linguistic contexts that Bangla speakers encounter. The use of GPT-4 Turbo for question synthesis ensures high-quality, diverse question generation while maintaining cultural appropriateness through expert annotation. The dual-encoder architecture leverages the regional specificity by learning stronger representations that capture both the visual and linguistic nuances of Bangla culture, leading to improved modality alignment and reasoning capabilities.

## Foundational Learning

**Regional VQA relevance**: Understanding why culturally specific datasets matter - needed because generic datasets fail to capture local visual contexts and language patterns; quick check: compare performance on regional vs generic datasets.

**Dual-encoder architectures**: Learning how separate modality encoders can be jointly optimized - needed for handling language-vision alignment in low-resource settings; quick check: measure modality-specific performance gains.

**Zero-shot prompting**: Understanding how large VLMs can perform without fine-tuning - needed for leveraging existing models in new domains; quick check: compare zero-shot vs fine-tuned performance.

**Cultural annotation quality**: Recognizing the importance of expert validation - needed to ensure dataset authenticity and relevance; quick check: measure inter-annotator agreement.

## Architecture Onboarding

**Component map**: Image encoder (CNN/Transformer) -> Text encoder (Transformer) -> Fusion layer -> Output classifier

**Critical path**: Image → Encoder → Feature extraction → Fusion → Classification decision

**Design tradeoffs**: Regional specificity vs dataset size tradeoff - larger datasets capture more diversity but may dilute cultural specificity; architectural complexity vs performance gains in dual-encoder designs.

**Failure signatures**: Models struggle with questions requiring cultural context not present in training images; performance degrades on dialectical variations outside sampled regions; zero-shot models fail on culturally specific visual elements.

**3 first experiments**:
1. Compare dual-encoder vs single-encoder performance on culturally specific vs generic questions
2. Test model robustness across different Bangla dialects and regional variations
3. Evaluate zero-shot prompting success rate on questions requiring cultural knowledge

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of 15k samples remains modest compared to major VQA benchmarks in high-resource languages
- Evaluation focused primarily on accuracy metrics without deeper error type analysis
- Limited assessment of model robustness to linguistic variation across different Bangla dialects

## Confidence

**High confidence**: The dataset's regional relevance claim and general methodology of combining local imagery with GPT-4 Turbo synthesis; comparative performance improvements of dual-encoder architectures over baseline models.

**Medium confidence**: The 2-3% accuracy improvement from pretraining dual encoders requires scrutiny of experimental conditions; zero-shot prompting results depend heavily on prompt engineering quality.

**Medium confidence**: Claims about proprietary models outperforming open-source alternatives should be interpreted cautiously given potential confounding factors.

## Next Checks

1. Conduct ablation studies to isolate the contribution of regional relevance versus dataset size by testing models trained on ChitroJera versus comparable-sized but less regionally specific Bangla datasets.

2. Perform robustness testing across dialectical variations within Bangla to assess whether the dataset adequately captures linguistic diversity beyond the sampled regions.

3. Extend evaluation to include error analysis categorizing failure modes (visual reasoning, language understanding, cultural context) to better understand where models struggle and how this relates to dataset composition.