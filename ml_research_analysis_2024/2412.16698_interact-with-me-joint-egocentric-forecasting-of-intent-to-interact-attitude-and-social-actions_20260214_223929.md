---
ver: rpa2
title: 'Interact with me: Joint Egocentric Forecasting of Intent to Interact, Attitude
  and Social Actions'
arxiv_id: '2412.16698'
source_url: https://arxiv.org/abs/2412.16698
tags: []
core_contribution: The authors address the challenge of enabling social agents to
  proactively recognize users and anticipate interactions by jointly forecasting a
  person's intent to interact, their attitude towards the agent, and the action they
  will perform, using only 1 second of egocentric video. They propose SocialEgoNet,
  a graph-based spatiotemporal framework that extracts whole-body keypoints and uses
  a hierarchical multitask learning approach to exploit task dependencies.
---

# Interact with me: Joint Egocentric Forecasting of Intent to Interact, Attitude and Social Actions

## Quick Facts
- arXiv ID: 2412.16698
- Source URL: https://arxiv.org/abs/2412.16698
- Reference count: 30
- Jointly forecasts intent to interact, attitude, and social actions from 1 second of egocentric video

## Executive Summary
This paper introduces SocialEgoNet, a graph-based spatiotemporal framework that enables social agents to proactively recognize users and anticipate interactions by jointly forecasting a person's intent to interact, their attitude towards the agent, and the action they will perform. The system extracts whole-body keypoints from 1 second of egocentric video and uses hierarchical multitask learning to exploit dependencies between forecasting tasks. The approach significantly outperforms competitive baselines including R3D-18 and ST-GCN on the augmented JPL-Social dataset.

## Method Summary
SocialEgoNet employs a hierarchical multitask learning approach that leverages task dependencies for improved forecasting accuracy. The model extracts whole-body keypoints from egocentric video frames and processes them through a graph-based spatiotemporal network. By jointly forecasting intent, attitude, and actions rather than predicting each independently, the system achieves better performance while maintaining computational efficiency. The framework demonstrates faster inference speeds (2.5x faster than ST-GCN) and a smaller model size (66.24% smaller than ST-GCN) while maintaining high accuracy across all tasks.

## Key Results
- Achieves 83.15% average accuracy across all tasks on the augmented JPL-Social dataset
- Inference speed is 2.5x faster than ST-GCN baseline
- Model size is 66.24% smaller than ST-GCN baseline
- Outperforms competitive baselines including R3D-18 and ST-GCN

## Why This Works (Mechanism)
The hierarchical multitask learning approach exploits natural dependencies between forecasting tasks - intent to interact, attitude, and social actions are inherently correlated in human interactions. By jointly modeling these tasks, the system can leverage shared features and contextual information across tasks, reducing redundant computation and improving overall accuracy. The graph-based spatiotemporal architecture effectively captures the spatial relationships between body keypoints and their temporal evolution, which is crucial for understanding social interactions from egocentric perspectives.

## Foundational Learning
- Egocentric vision: First-person video understanding is essential for social agent interaction scenarios
  - Why needed: Provides the perspective of the agent experiencing the interaction
  - Quick check: Verify that the dataset uses head-mounted or body-worn cameras
- Multitask learning: Joint prediction of related tasks improves performance
  - Why needed: Social interaction components are interdependent
  - Quick check: Compare performance against single-task baselines
- Graph neural networks: Effective for modeling spatial relationships between keypoints
  - Why needed: Human body poses have inherent structural relationships
  - Quick check: Ensure the graph topology captures meaningful body part connections

## Architecture Onboarding
**Component Map:** Keypoint Extraction -> Graph Construction -> Spatiotemporal Processing -> Multitask Forecasting
**Critical Path:** The keypoint extraction and graph construction stages are critical as they directly impact the quality of spatial representations fed into the spatiotemporal processing
**Design Tradeoffs:** Joint forecasting versus independent task prediction; computational efficiency versus model accuracy; 1-second input constraint versus longer temporal context
**Failure Signatures:** Poor keypoint extraction quality, incorrect graph topology assumptions, failure to capture temporal dependencies, task interference in multitask learning
**First Experiments:**
1. Baseline comparison with single-task versus joint forecasting approaches
2. Ablation study on graph architecture variations
3. Evaluation of different keypoint extraction algorithms

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may not generalize to diverse real-world scenarios beyond the controlled JPL-Social dataset environment
- 1-second input constraint may limit effectiveness in scenarios requiring longer-term interaction understanding
- Study does not address performance degradation with occlusions, varying lighting conditions, or multiple simultaneous interaction partners

## Confidence
- High: Model architecture claims and performance metrics (83.15% accuracy, 2.5x faster inference, 66.24% smaller model size)
- Medium: Real-world applicability and deployment readiness claims (dependent on untested generalization factors)

## Next Checks
1. Conduct cross-dataset validation using publicly available egocentric interaction datasets to assess generalization capabilities beyond JPL-Social
2. Perform ablation studies to quantify the contribution of each component in the hierarchical multitask learning framework, particularly the impact of joint forecasting versus individual task prediction
3. Evaluate model performance with varying input durations (beyond the 1-second constraint) to determine optimal trade-offs between processing speed and prediction accuracy for different application scenarios