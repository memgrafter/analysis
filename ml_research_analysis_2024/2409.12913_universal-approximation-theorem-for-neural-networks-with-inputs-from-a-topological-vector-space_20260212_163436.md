---
ver: rpa2
title: Universal approximation theorem for neural networks with inputs from a topological
  vector space
arxiv_id: '2409.12913'
source_url: https://arxiv.org/abs/2409.12913
tags:
- neural
- networks
- function
- layer
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proves a universal approximation theorem for feedforward\
  \ neural networks (TVS-FNNs) that accept inputs from any topological vector space\
  \ (TVS) with the Hahn-Banach extension property. The key result shows that for any\
  \ non-polynomial continuous activation function \u03C3, the set of single-hidden-layer\
  \ TVS-FNNs is dense in the space of continuous functions on any compact subset of\
  \ the TVS."
---

# Universal approximation theorem for neural networks with inputs from a topological vector space

## Quick Facts
- arXiv ID: 2409.12913
- Source URL: https://arxiv.org/abs/2409.12913
- Authors: Vugar Ismailov
- Reference count: 26
- Key outcome: Proves that single-hidden-layer TVS-FNNs are dense in C(K) for compact K in any TVS with HBEP using non-polynomial continuous activation functions

## Executive Summary
This paper extends classical universal approximation theorems to feedforward neural networks that accept inputs from general topological vector spaces (TVSs) rather than Euclidean space. The key result demonstrates that for any TVS with the Hahn-Banach extension property, single-hidden-layer networks with non-polynomial continuous activation functions can approximate any continuous function on compact subsets. The proof leverages the Stone-Weierstrass theorem by showing that exponential functions formed from continuous linear functionals form a dense subalgebra in the space of continuous functions. Specific applications include matrix spaces, sequence spaces, function spaces, and other mathematical structures beyond standard neural network inputs.

## Method Summary
The proof establishes that TVS-FNNs with single hidden layer can approximate any continuous function on compact subsets of a topological vector space X with the Hahn-Banach extension property. The approach involves four key steps: (1) showing that activation functions can approximate univariate functions on compact intervals, (2) proving that the set of exponential functions generated by continuous linear functionals is dense in C(K), (3) demonstrating that single-hidden-layer networks can approximate these exponential functions, and (4) combining these results to show overall density. The construction relies critically on the existence of separating functionals guaranteed by the HBEP and uses classical approximation theorems like Stone-Weierstrass and Weierstrass.

## Key Results
- Single-hidden-layer TVS-FNNs are dense in C(K) for any compact K in a TVS with HBEP
- The result extends classical universal approximation theorems beyond Euclidean spaces
- Applications demonstrated for matrix spaces, sequence spaces (ℓp, c0), function spaces Lp, and continuous function spaces C(X)
- The activation function must be continuous and non-polynomial for the approximation to work

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The set of single-hidden-layer TVS-FNNs is dense in the space of continuous functions on any compact subset of a topological vector space with the Hahn-Banach extension property.
- Mechanism: The proof uses the Stone-Weierstrass theorem by showing that the set of exponential functions {er(x) : r ∈ X*} forms an algebra that separates points and contains constants, then approximates any continuous function on a compact set by first approximating it with exponentials and then approximating each exponential with single-hidden-layer networks using a non-polynomial continuous activation function.
- Core assumption: The topological vector space X has the Hahn-Banach extension property (HBEP), which ensures that for any distinct points in X there exists a continuous linear functional that separates them.
- Evidence anchors:
  - [abstract]: "The proof relies on density arguments in C(K) for compact sets K, using the Hahn-Banach theorem and the Weierstrass approximation theorem."
  - [section 2]: "Step 3: Exponential functions. Let us show that the set E = span{er(x) : r ∈ X*} is dense in C(K) for every compact set K ⊂ X. It is not difficult to see that E is a subalgebra of C(X)... By the Stone-Weierstrass theorem [23], for any compact K ⊂ X, the algebra E restricted to K is dense in C(K)."
  - [corpus]: The corpus contains several related papers on universal approximation theorems for various neural network architectures, supporting the general validity of such approaches.
- Break condition: If the activation function σ is a polynomial, the approximation fails. If the TVS does not have the HBEP, the separation of points argument fails.

### Mechanism 2
- Claim: Any continuous non-polynomial activation function can approximate any univariate continuous function on a compact interval.
- Mechanism: The proof shows that the set span{σ(wx - θ) : w ∈ R, θ ∈ R} is dense in C[a,b] for any compact interval [a,b]. This is proven by showing that the set contains all monomials (by taking derivatives with respect to w and evaluating at specific points), and since it contains all polynomials, the Weierstrass approximation theorem implies density.
- Core assumption: The activation function σ is continuous and not a polynomial on the open interval Θ.
- Evidence anchors:
  - [abstract]: "The key result shows that for any non-polynomial continuous activation function σ, the set of single-hidden-layer TVS-FNNs is dense in the space of continuous functions on any compact subset of the TVS."
  - [section 2]: "Step 1: The case when X = R and σ ∈ C^∞(R). We first prove that if σ ∈ C^∞(R), then the set M(σ) = span{σ(wx - θ) : w ∈ R, θ ∈ Θ} is dense in C(K) for every compact set K ⊂ R... Obviously, dk/dwk σ(wx - θ) = xkσ^(k)(wx - θ) and since σ is not a polynomial, there exists -θk ∈ Θ such that σ^(k)(-θk) ≠ 0."
  - [corpus]: Weak evidence - the corpus contains related work but no direct evidence for this specific mechanism.
- Break condition: If σ is a polynomial function, the set cannot approximate all continuous functions.

### Mechanism 3
- Claim: The approximation of exponential functions er(x) by single-hidden-layer networks with activation σ is possible because univariate functions can be approximated by such networks.
- Mechanism: For any continuous univariate function on a compact set (like aiei(x) where ai are constants and ri are continuous functionals), there exists a single-hidden-layer network with activation σ that approximates it arbitrarily well. This is because the proof shows that any continuous univariate function can be approximated by networks with activation σ, and then this is applied component-wise to the exponential functions.
- Core assumption: The univariate function aiei(t) can be approximated by networks with activation σ for any compact set of t values.
- Evidence anchors:
  - [abstract]: "The proof relies on density arguments in C(K) for compact sets K, using the Hahn-Banach theorem and the Weierstrass approximation theorem."
  - [section 2]: "Step 4: The general case. According to Step 3 for any compact K ⊂ X, any g ∈ C(K) and any ε > 0, there exist finitely many functionals ri ∈ X* and numbers ai ∈ R such that |g(x) - Σai er i(x)| < ε/2... Since ri are continuous, the images ri(K) are compact sets in R. Put R = ∪ri(K). Note that R is also compact. By Step 2, each univariate function aite, t ∈ R, can be approximated by single hidden layer networks with the activation σ."
  - [corpus]: The corpus contains related work on operator approximation and universal approximation theorems, supporting the general approach.
- Break condition: If the univariate approximation fails (which would happen if σ were a polynomial), the overall approximation fails.

## Foundational Learning

- Concept: Hahn-Banach Extension Property (HBEP)
  - Why needed here: The HBEP ensures that for any two distinct points in the TVS, there exists a continuous linear functional that separates them, which is crucial for the Stone-Weierstrass argument to work.
  - Quick check question: If X is a TVS without the HBEP, can the set of exponential functions still separate points in X?

- Concept: Stone-Weierstrass Theorem
  - Why needed here: The Stone-Weierstrass theorem is used to prove that the algebra of exponential functions is dense in C(K) for any compact K ⊂ X, which is the key step in showing that TVS-FNNs can approximate any continuous function.
  - Quick check question: What are the three conditions required for an algebra of functions to be dense in C(X) according to the Stone-Weierstrass theorem?

- Concept: Weierstrass Approximation Theorem
  - Why needed here: The Weierstrass theorem is used in the proof to show that the set of functions generated by the activation function σ is dense in C[a,b] for any compact interval [a,b], which is then used to approximate the exponential functions.
  - Quick check question: According to the Weierstrass approximation theorem, what class of functions is dense in C[a,b] for any continuous function on [a,b]?

## Architecture Onboarding

- Component map:
  - Input Layer: An element x ∈ X from the topological vector space
  - Hidden Layer: Each neuron applies a continuous linear functional f ∈ X* to x, then applies shift θ and activation function σ
  - Output Layer: Weighted sum of hidden layer outputs
  - Key components: Continuous linear functionals (dual space elements), non-polynomial continuous activation function, compact set K ⊂ X

- Critical path: The critical path for implementing this theorem involves:
  1. Verifying that the input space X has the HBEP
  2. Implementing the dual space X* (continuous linear functionals on X)
  3. Implementing a non-polynomial continuous activation function σ
  4. Constructing the network architecture with single hidden layer
  5. Implementing the approximation algorithm based on the proof steps

- Design tradeoffs:
  - The choice of activation function σ affects approximation quality - it must be continuous and non-polynomial
  - The complexity of computing continuous linear functionals from X* may vary depending on the structure of X
  - The number of hidden neurons needed depends on the desired approximation accuracy ε
  - For specific TVSs like matrix spaces or function spaces, specialized implementations of the dual space may be more efficient

- Failure signatures:
  - If the activation function is a polynomial, the network cannot approximate all continuous functions
  - If the TVS does not have the HBEP, the network cannot separate all points, limiting approximation capability
  - If the input space is not compact, the theorem does not apply directly
  - Poor approximation may indicate that the chosen activation function is not suitable for the specific problem

- First 3 experiments:
  1. Implement a simple case with X = R and test approximation of basic functions like polynomials and trigonometric functions using different non-polynomial activation functions (ReLU, sigmoid, tanh)
  2. Test with X = R^n and verify that the network can approximate multivariate functions by using linear functionals as weighted sums of inputs
  3. Implement a case with X = C([0,1]) (continuous functions on [0,1]) and test approximation of simple functional transformations using integration functionals as the dual space elements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the universal approximation theorem extend to deep TVS-FNNs with multiple hidden layers?
- Basis in paper: [explicit] The paper explicitly states "This paper does not address the approximation capabilities of deep TVS-FNNs" and mentions that "TVS-FNNs with more than one hidden layer are defined recursively" but are not studied.
- Why unresolved: The paper focuses solely on single-hidden-layer networks and does not provide any analysis or proof for deeper architectures in the TVS context.
- What evidence would resolve it: A proof showing that for TVS-FNNs with l hidden layers (l > 1), the set of functions they can represent is dense in C(K) for compact K ⊂ X, or a counterexample demonstrating limitations.

### Open Question 2
- Question: Can the theorem be extended to TVSs without the Hahn-Banach extension property?
- Basis in paper: [explicit] The theorem explicitly requires X to have the HBEP, with the paper noting "According to the Hahn-Banach theorem, every (locally) convex TVS has the HBEP" but also mentioning "there also exist nonconvex TVSs that possess the HBEP."
- Why unresolved: The proof relies critically on the HBEP to ensure existence of functionals that separate points, and the paper does not explore alternative approaches for TVSs lacking this property.
- What evidence would resolve it: Either a modified proof that works for broader classes of TVSs without HBEP, or a counterexample showing the theorem fails when HBEP is not satisfied.

### Open Question 3
- Question: What quantitative convergence rates can be established for TVS-FNN approximations?
- Basis in paper: [inferred] The paper establishes density results but does not address convergence rates, while the reference [14] is cited as having obtained "quantitative estimates (i.e., convergence rates) for the approximation of nonlinear operators using single-hidden layer networks acting between infinite-dimensional Banach spaces."
- Why unresolved: The proof uses density arguments without providing explicit bounds on the number of neurons needed for a given approximation accuracy ε.
- What evidence would resolve it: Explicit bounds on the number of neurons r in terms of ε and properties of the function being approximated, similar to the finite-dimensional results in classical approximation theory.

## Limitations

- The proof requires the topological vector space to have the Hahn-Banach extension property, which is not satisfied by all TVSs
- Practical implementation details for specific TVSs are not fully developed
- The computational complexity of evaluating continuous linear functionals for different TVSs is not analyzed

## Confidence

**High Confidence:** The core theoretical framework and proof structure are sound. The extension of classical universal approximation theorems to general topological vector spaces is mathematically rigorous and follows logically from established theorems in functional analysis.

**Medium Confidence:** The applicability to specific TVSs (matrix spaces, sequence spaces, function spaces) is demonstrated, but the practical implementation details and computational complexity for these specific cases are not fully explored.

**Low Confidence:** The practical implications and computational feasibility of implementing TVS-FNNs in real-world applications remain uncertain, as the paper focuses primarily on theoretical foundations rather than practical considerations.

## Next Checks

1. **Empirical Validation:** Implement a concrete example using a specific TVS (e.g., ℓ^p sequence space) and test the approximation capabilities of TVS-FNNs on standard benchmark functions to verify the theoretical claims.

2. **HBEP Characterization:** Conduct a systematic study to characterize which common topological vector spaces possess the Hahn-Banach Extension Property and develop efficient methods to verify this property for new TVSs.

3. **Computational Complexity Analysis:** Analyze the computational complexity of implementing TVS-FNNs for different types of topological vector spaces, particularly focusing on the cost of evaluating continuous linear functionals in the dual space.