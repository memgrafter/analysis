---
ver: rpa2
title: Offline Reinforcement Learning for Learning to Dispatch for Job Shop Scheduling
arxiv_id: '2409.10589'
source_url: https://arxiv.org/abs/2409.10589
tags:
- offline-ld
- offline
- learning
- time
- mqrdqn-dexpert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Offline Learned Dispatching (Offline-LD) introduces the first offline
  reinforcement learning approach for Job Shop Scheduling Problems (JSSP), learning
  dispatching policies from historical data without environment interaction. The method
  adapts maskable variants of Quantile Regression DQN and Soft Actor-Critic with Conservative
  Q-Learning to handle offline training and maskable action spaces.
---

# Offline Reinforcement Learning for Learning to Dispatch for Job Shop Scheduling

## Quick Facts
- arXiv ID: 2409.10589
- Source URL: https://arxiv.org/abs/2409.10589
- Authors: Jesse van Remmerden; Zaharah Bukhsh; Yingqian Zhang
- Reference count: 36
- Offline-LD introduces the first offline RL approach for JSSP, outperforming online RL when trained on only 100 expert solutions

## Executive Summary
Offline-LD presents the first offline reinforcement learning method for Job Shop Scheduling Problems (JSSP), enabling dispatching policy learning from historical data without environment interaction. The approach adapts maskable variants of Quantile Regression DQN and Soft Actor-Critic with Conservative Q-Learning to handle offline training and maskable action spaces. By leveraging expert solutions from Constraint Programming, Offline-LD achieves competitive performance while requiring significantly less data than online RL methods. The method introduces novel entropy regularization for maskable actions and reward normalization based on expert solutions, demonstrating strong generalization capabilities and robustness to noisy training data.

## Method Summary
Offline-LD adapts maskable variants of Quantile Regression DQN (mQRDQN) and Soft Actor-Critic (d-mSAC) for offline RL in JSSP. The method incorporates Conservative Q-Learning to handle distributional shift, enabling training on fixed datasets without exploration. State representations use Graph Isomorphism Network encoders to capture job-machine relationships, while maskable action spaces dynamically remove infeasible operations at each timestep. Reward normalization divides makespan by expert solution makespan, and novel entropy regularization is applied for maskable actions. The approach is trained on expert solutions generated by Constraint Programming and evaluated on generated instances and benchmark sets across multiple problem sizes.

## Key Results
- Offline-LD trained on only 100 expert solutions outperforms online RL approaches like L2D
- mQRDQN variant generalizes well to larger instances when trained on smaller ones, while d-mSAC struggles with such generalization
- Performance remains stable with datasets as small as 10 instances, demonstrating high data efficiency
- Method maintains strong performance even with noisy training data containing 50% noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offline-LD achieves better performance than online RL (L2D) because it learns from high-quality expert data rather than through trial-and-error interaction.
- Mechanism: The method leverages pre-collected optimal or near-optimal solutions from Constraint Programming (CP), eliminating the need for millions of environment interactions required by online RL. By training on expert trajectories, the policy inherits good decision-making patterns from the start.
- Core assumption: Expert datasets contain sufficient diversity and quality to train a robust policy that generalizes to unseen instances.
- Evidence anchors:
  - [abstract]: "Offline-LD trained on only 100 expert solutions outperforms online RL approaches"
  - [section 1]: "Our approach is motivated by scenarios where historical scheduling data and expert solutions are available"
  - [corpus]: Weak evidence - related papers focus on online RL approaches for JSSP, but no direct comparisons to offline methods
- Break condition: If expert datasets are too small or lack diversity, the policy may overfit and fail to generalize to new instances.

### Mechanism 2
- Claim: The use of Conservative Q-Learning (CQL) enables effective offline RL by preventing overestimation of out-of-distribution actions.
- Mechanism: CQL adds a regularization term to the Q-network loss that penalizes high Q-values for actions not present in the training data. This prevents the policy from selecting actions that were never or rarely seen during training, addressing the distributional shift problem inherent in offline RL.
- Core assumption: The training data contains sufficient coverage of relevant state-action pairs to learn a useful policy without exploration.
- Evidence anchors:
  - [section 4.1]: "Conservative Q-learning (CQL) Kumar et al. (2020) alleviates the issue of OOD actions by being pessimistic about Q values of OOD actions"
  - [section 1]: "Traditional online RL methods cannot be used directly to train with these existing datasets due to the distributional shift"
  - [corpus]: Explicit mention in related work of distributional shift challenges in offline RL
- Break condition: If training data is highly suboptimal or contains many errors, CQL's pessimism may prevent learning useful policies.

### Mechanism 3
- Claim: Maskable action spaces enable efficient policy learning by focusing computation only on feasible actions at each state.
- Mechanism: The action space is dynamically masked based on the current state, removing infeasible operations (e.g., operations whose preceding operations haven't completed). This reduces the effective action space size and allows the neural network to focus on valid decisions.
- Core assumption: The maskable action space structure aligns with the JSSP problem structure where only certain operations can be scheduled at any given time.
- Evidence anchors:
  - [section 4]: "We propose two Q-learning methods capable of masking infeasible actions"
  - [section 3.1]: "At each timestep t, the available actions at ∈ A(st) are the current operations that can be scheduled"
  - [corpus]: Weak evidence - related papers mention action masking but don't provide detailed mechanisms
- Break condition: If the masking logic is incorrect or incomplete, the policy may be trained on invalid actions or miss valid ones.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: The JSSP is formalized as an MDP to apply reinforcement learning techniques
  - Quick check question: What are the state, action, and reward components in the JSSP MDP formulation?

- Concept: Conservative Q-Learning (CQL)
  - Why needed here: CQL addresses the distributional shift problem that prevents direct application of online RL methods to offline data
  - Quick check question: How does CQL's regularization term prevent overestimation of out-of-distribution actions?

- Concept: Graph Neural Networks for state representation
  - Why needed here: The JSSP can be naturally represented as a disjunctive graph, and GNNs can effectively encode this structure for the RL policy
  - Quick check question: What are the two types of embeddings output by the GIN network in Offline-LD?

## Architecture Onboarding

- Component map: GIN encoder → Q-network(s) or Policy network → CQL regularization → Training loop with target network updates
- Critical path: State representation → Q-value/policy estimation → CQL penalty calculation → Loss computation → Network parameter updates
- Design tradeoffs: Offline RL vs online RL (data efficiency vs exploration capability), value-based vs actor-critic methods (stability vs sample efficiency)
- Failure signatures: Poor generalization (overfitting to training data), instability in training (inappropriate reward scaling or αCQL values), masked action space errors
- First 3 experiments:
  1. Train on 6×6 expert dataset and evaluate on same size to verify basic functionality
  2. Test generalization by training on 6×6 and evaluating on 10×10 to assess scaling
  3. Compare performance with and without CQL regularization to verify its importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Offline-LD performance scale when trained on datasets generated by simple priority dispatching rules (PDRs) versus CP-generated solutions?
- Basis in paper: [explicit] "future work could explore training using datasets generated by alternative methods, including simple priority dispatching rules (PDRs) such as MOR, SPT, and MWKR"
- Why unresolved: The paper only tested with CP-generated expert datasets, leaving the question of whether performance depends strongly on high-quality solutions or can be achieved with simpler heuristics unanswered
- What evidence would resolve it: Direct comparison experiments training Offline-LD on datasets generated by MOR, SPT, and MWKR rules versus CP solutions, measuring performance gap across multiple instance sizes

### Open Question 2
- Question: What is the theoretical explanation for mQRDQN's superior generalization to larger instances compared to d-mSAC when trained on smaller instances?
- Basis in paper: [explicit] "mQRDQN is able to generalize well to larger sizes, even if trained on smaller sizes... d-mSAC is not able to generalize well, especially with Dnoisy"
- Why unresolved: The paper observes this phenomenon but doesn't provide theoretical analysis of why quantile regression in mQRDQN enables better generalization than actor-critic approaches
- What evidence would resolve it: Formal analysis of how quantile regression affects function approximation bias and variance across different instance sizes, potentially explaining the generalization gap

### Open Question 3
- Question: What is the optimal dataset size and composition for Offline-LD to achieve the best performance-quality tradeoff?
- Basis in paper: [explicit] "Our results demonstrate competitive performance, even when training on as few as 10 instances" and "increasing the dataset size does not lead to a statistically significant improvement"
- Why unresolved: While the paper shows that very small datasets (10-100 instances) suffice, it doesn't systematically explore the tradeoff between dataset size, composition (expert vs noisy), and performance across different instance types
- What evidence would resolve it: Systematic ablation studies varying dataset size (10, 50, 100, 500, 1000 instances) and composition ratios (expert vs noisy), measuring performance and training efficiency across multiple benchmark sets

### Open Question 4
- Question: How does Offline-LD perform on real-world job shop scheduling datasets that include machine breakdowns and other disturbances?
- Basis in paper: [explicit] "for many of these COPs, such as JSSP, there are real-world datasets, that contains events like machine breakdowns"
- Why unresolved: The paper only tested on synthetic and benchmark datasets, not on real-world industrial data that includes the complexity of real disturbances
- What evidence would resolve it: Experiments applying Offline-LD to real-world JSSP datasets from manufacturing environments, comparing performance against traditional methods and measuring robustness to real disturbances

## Limitations

- Limited analysis of training data coverage and diversity, making it unclear if 100 expert solutions per instance size provide sufficient representation
- Sparse implementation details for maskable action space handling, complicating verification of correct infeasible action removal
- No systematic exploration of dataset size tradeoffs beyond showing performance with 10-100 instances

## Confidence

- High: Offline-LD can train on small expert datasets and generalize to larger instances
- Medium: CQL effectively prevents distributional shift issues in this specific JSSP context
- Low: Expert data quality is sufficient for robust policy learning across all instance sizes

## Next Checks

1. **Dataset Coverage Analysis**: Quantitatively measure the coverage of state-action pairs in the training data versus encountered during testing. Calculate the percentage of test-time states that fall within the training data distribution and correlate this with performance degradation.

2. **Maskable Action Space Verification**: Implement a systematic test where the maskable action mechanism is disabled, and observe if the policy selects infeasible actions during evaluation. This would validate whether the masking implementation is functioning correctly.

3. **Generalization Stress Test**: Train mQRDQN on 6×6 instances and evaluate on 30×20 benchmark instances, measuring performance gap and runtime. This extreme generalization test would validate the claimed scaling capability beyond the tested instance sizes.