---
ver: rpa2
title: Associative Memories in the Feature Space
arxiv_id: '2402.10814'
source_url: https://arxiv.org/abs/2402.10814
tags:
- memory
- data
- space
- function
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of associative memory models
  failing to retrieve images under corruption by proposing semantic memory models
  that compute similarities in an embedding space rather than raw pixel space. The
  core idea involves using pre-trained neural networks with contrastive loss as feature
  maps to create embeddings where similarity calculations are more meaningful for
  semantic features.
---

# Associative Memories in the Feature Space

## Quick Facts
- arXiv ID: 2402.10814
- Source URL: https://arxiv.org/abs/2402.10814
- Reference count: 28
- Primary result: Semantic memory models achieve >70% retrieval accuracy on corrupted images using embedding space similarity

## Executive Summary
This paper addresses a fundamental limitation of associative memory models: their failure to retrieve images under corruption when using pixel-space similarity. The authors propose computing similarities in embedding space using pre-trained neural networks with contrastive loss, which captures semantic rather than pixel-level features. Experiments on CIFAR10 and STL10 datasets show significant performance improvements over universal Hopfield networks, with retrieval accuracies exceeding 70% for various corruption types including cropping, masking, rotation, and Gaussian noise.

## Method Summary
The method uses pre-trained neural networks (ResNet18/50 with SimCLR contrastive loss) as feature maps to create semantic embeddings of images. These embeddings, typically 512-2048 dimensions, replace the original high-dimensional pixel space (3072-27648 dimensions) for similarity computations. The universal Hopfield network architecture is adapted to work in this embedding space, using standard similarity metrics (L1, L2, cosine, dot product) with softmax separation. A fully-semantic variant stores only low-dimensional embeddings and uses generative networks for reconstruction, demonstrated on MNIST with autoencoders.

## Key Results
- Retrieval accuracy exceeds 70% for various corruption types using L1 norm similarity in embedding space
- Computational efficiency improves significantly due to lower-dimensional embedding spaces (512-2048 vs 3072-27648 dimensions)
- Memory-efficient variant demonstrated on MNIST, storing only low-dimensional embeddings instead of full images
- Negative L1 norm consistently outperforms other similarity metrics across different corruption types

## Why This Works (Mechanism)

### Mechanism 1
Semantic similarity in embedding space enables robust retrieval under pixel-level corruption. The pre-trained neural network with contrastive loss learns to map semantically similar images to nearby points in embedding space, while dissimilar images are pushed apart. This creates a representation where standard similarity metrics can effectively distinguish between corrupted versions of the same image and different images.

### Mechanism 2
Lower-dimensional embedding space provides computational efficiency. By mapping high-dimensional pixel data (3072-27648 dimensions) to a much lower-dimensional embedding space (512-2048 dimensions), similarity computations become faster while maintaining or improving retrieval accuracy.

### Mechanism 3
Memory-efficient storage through semantic embeddings. Instead of storing full-resolution images, the system stores only low-dimensional embeddings of the data points. Retrieval then requires a generative network to reconstruct images from these embeddings, trading exact retrieval for significant memory savings.

## Foundational Learning

- Concept: Contrastive learning and SimCLR
  - Why needed here: Understanding how pre-trained networks create meaningful embeddings is crucial for implementing the feature map component
  - Quick check question: How does the contrastive loss encourage similar images to be close in embedding space while pushing dissimilar images apart?

- Concept: Universal Hopfield Networks and decomposition into score, separation, projection functions
  - Why needed here: The paper builds on this formalism to create semantic variants, so understanding the basic architecture is essential
  - Quick check question: What role does each of the three components (score, separation, projection) play in the retrieval process?

- Concept: Autoencoder architecture and bottleneck layers
  - Why needed here: For the fully-semantic variant, understanding how encoders and decoders work together to compress and reconstruct data is critical
  - Quick check question: Why is a bottleneck layer necessary in an autoencoder, and what would happen without it?

## Architecture Onboarding

- Component map:
  Input corrupted image -> ResNet feature map -> embedding -> similarity scores -> softmax separation -> projection function -> output retrieved image

- Critical path:
  1. Input corrupted image → ResNet feature map → embedding
  2. Compute similarity scores with all stored embeddings
  3. Apply softmax separation to emphasize most similar
  4. Project back to pixel space using stored data or generative network
  5. Output retrieved/reconstructed image

- Design tradeoffs:
  - Exact vs. semantic retrieval: Storing full images enables exact retrieval but requires more memory; storing embeddings saves memory but requires generative reconstruction
  - Embedding dimensionality: Higher dimensions preserve more information but reduce computational benefits; lower dimensions increase efficiency but may lose discriminative power
  - Pre-trained model selection: Larger models (ResNet50x4) provide better embeddings but increase computation time

- Failure signatures:
  - Poor retrieval accuracy despite correct implementation: Likely indicates insufficient semantic information in embeddings or inappropriate similarity metric for the corruption type
  - Slow inference: Check if embedding dimensionality is too high or if similarity computations aren't optimized
  - Unrecognizable reconstructions: Generative network may be too simple or not properly trained for the data distribution

- First 3 experiments:
  1. Implement semantic Hopfield network with pre-trained ResNet18 on CIFAR10, test retrieval on rotated images
  2. Compare performance of different similarity metrics (L1, L2, cosine) on masked images
  3. Implement fully-semantic variant with simple autoencoder on MNIST, test reconstruction from Gaussian noise inputs

## Open Questions the Paper Calls Out

### Open Question 1
How do semantic memory models perform when using generative models beyond autoencoders, such as GANs or diffusion models, for the fully-semantic variant? The paper demonstrates a proof of concept using autoencoders on MNIST but suggests that applications in practice would need more powerful generative models, picked according to the needed task and data.

### Open Question 2
What is the theoretical capacity limit of semantic memory models when using different embedding functions and similarity metrics? The paper shows empirical performance improvements but does not provide theoretical analysis of capacity limits, unlike previous work on Hopfield networks that established exponential capacity bounds.

### Open Question 3
How do semantic memory models handle out-of-distribution queries that share semantic features with stored memories but differ in appearance? The experiments focus on known corruption types applied to training data, but don't explore retrieval performance on truly novel queries that share semantic content but differ in visual features.

## Limitations

- Performance heavily depends on the quality of the pre-trained contrastive model, with no exploration of what happens when corruption types exceed the feature extractor's training distribution
- Memory-efficient variant requires a separate generative model training that isn't fully evaluated on complex datasets beyond MNIST
- Choice of L1 norm as the best similarity metric is asserted but not thoroughly justified theoretically

## Confidence

- **High confidence**: The core mechanism of using embedding space for semantic similarity is well-supported by the empirical results showing 70%+ retrieval accuracy across multiple corruption types
- **Medium confidence**: The computational efficiency claims are reasonable given the dimensionality reduction, but exact speedups would depend on implementation details not fully specified
- **Low confidence**: The memory efficiency of the fully-semantic variant is theoretically sound but only demonstrated on MNIST, which may not generalize to more complex datasets

## Next Checks

1. Test the retrieval performance when corruptions exceed the training distribution of the contrastive model (e.g., extreme rotations or high-variance noise) to identify the break point of the semantic similarity assumption
2. Implement the memory-efficient variant on a more complex dataset (e.g., CIFAR10) to validate whether generative reconstruction can maintain retrieval quality with significant memory savings
3. Conduct ablation studies varying the embedding dimensionality to identify the optimal tradeoff between computational efficiency and retrieval accuracy for different corruption types