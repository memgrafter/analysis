---
ver: rpa2
title: Domain Generalization with Small Data
arxiv_id: '2402.06150'
source_url: https://arxiv.org/abs/2402.06150
tags:
- domain
- probabilistic
- domains
- proposed
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for domain generalization with small
  data, specifically addressing the problem of insufficient samples from source domains.
  The key idea is to learn domain-invariant representations using probabilistic embeddings,
  rather than deterministic ones.
---

# Domain Generalization with Small Data

## Quick Facts
- arXiv ID: 2402.06150
- Source URL: https://arxiv.org/abs/2402.06150
- Reference count: 25
- This paper proposes a method for domain generalization with small data, specifically addressing the problem of insufficient samples from source domains.

## Executive Summary
This paper addresses the challenge of domain generalization when source domain data is limited. The key insight is to use probabilistic embeddings rather than deterministic ones to capture richer distributional information. By mapping each data point into a probabilistic embedding using a Bayesian neural network, the method learns domain-invariant representations more effectively under data scarcity. The approach introduces two novel components: a probabilistic maximum mean discrepancy (P-MMD) for global alignment and a probabilistic contrastive semantic alignment (P-CSA) loss for local alignment.

## Method Summary
The method uses Bayesian neural networks to map inputs to probabilistic embeddings (distributions) rather than point estimates. A probabilistic maximum mean discrepancy (P-MMD) measures global alignment between source domains by comparing mixture distributions of latent distributions. A probabilistic contrastive semantic alignment (P-CSA) loss aligns embeddings locally by encouraging positive pairs to be closer and negative pairs farther apart, using MMD as the distance metric. The total loss combines classification loss, KL divergence regularization, P-CSA, and P-MMD. The approach is evaluated on three medical imaging datasets showing improved performance under insufficient data conditions.

## Key Results
- Probabilistic embeddings outperform deterministic embeddings for domain generalization with small data
- P-MMD effectively measures discrepancy between mixture distributions compared to standard MMD
- P-CSA loss provides better local alignment than standard contrastive losses when using probabilistic embeddings
- Extensive experiments on three medical imaging datasets demonstrate state-of-the-art performance under insufficient data scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic embeddings improve domain generalization under insufficient data by capturing richer distributional information than deterministic point embeddings.
- Mechanism: By mapping each data point into a probabilistic embedding (latent distribution) via Bayesian neural networks, the model learns higher-order statistics beyond the mean, which helps align distributions across domains more effectively.
- Core assumption: The distribution over distributions (mixture distributions) contains more discriminative and generalizable information than point embeddings, especially when training data is scarce.
- Evidence anchors:
  - [abstract]: "Instead of extracting latent feature embeddings based on deterministic models, we propose to learn a domain-invariant representation based on the probabilistic framework by mapping each data point into probabilistic embeddings."
  - [section 3.1]: "Instead of the individual latent embeddings zl1,..., we have latent probabilistic embeddings Π l1 := p(z|xl1, W), ... For a source domain Dl, we have the associated distribution over distributions Pl = {Πl1, ···, Πlnl}."
  - [corpus]: Weak - corpus papers focus on other probabilistic embedding uses (e.g., speech, video) not directly on domain generalization under small data.
- Break condition: If the Bayesian neural network fails to approximate the true posterior well (e.g., due to insufficient MC samples or poor variational approximation), the probabilistic embeddings may not capture meaningful distributional information.

### Mechanism 2
- Claim: The proposed Probabilistic Maximum Mean Discrepancy (P-MMD) measures discrepancy between mixture distributions more effectively than empirical MMD on point embeddings.
- Mechanism: P-MMD extends MMD by using a level-2 kernel to compare distributions over distributions, preserving higher moments of the latent distributions and providing a more sensitive global alignment metric.
- Core assumption: The level-2 kernel K(Πli, Πtj) = κ(µΠli, µΠtj) can meaningfully compare distributions by comparing their kernel mean embeddings in RKHS, and this comparison is sensitive to higher-order statistics.
- Evidence anchors:
  - [abstract]: "we first extend empirical maximum mean discrepancy (MMD) to a novel probabilistic MMD that can measure the discrepancy between mixture distributions (i.e., source domains) consisting of a series of latent distributions rather than latent points."
  - [section 3.1]: "Consider a level-1 kernel κ on H and its reproducing kernel Hilbert space (RKHS) Hκ. Define K as K(Πli, Πtj) = κ(µΠli, µΠtj) = ⟨ψ(µΠli), ψ(µΠtj)⟩Hκ..."
  - [corpus]: Weak - no direct corpus evidence; related work on MMD extensions but not specifically for probabilistic embeddings in DG.
- Break condition: If the level-2 kernel is not characteristic or the kernel bandwidth is poorly chosen, P-MMD may fail to distinguish between distributions effectively.

### Mechanism 3
- Claim: The Probabilistic Contrastive Semantic Alignment (P-CSA) loss aligns probabilistic embeddings locally by encouraging positive pairs to be closer and negative pairs farther apart, adapted to the distributional nature of the embeddings.
- Mechanism: Instead of Euclidean distance between point embeddings, P-CSA uses MMD to measure discrepancy between probabilistic embeddings, enabling reliable contrastive learning even when embeddings are distributions.
- Core assumption: MMD can serve as a valid distance metric between probabilistic embeddings, and the contrastive margin ξ is appropriately set to enforce separation.
- Evidence anchors:
  - [abstract]: "instead of imposing the contrastive semantic alignment (CSA) loss based on pairs of latent points, a novel probabilistic CSA loss encourages positive probabilistic embedding pairs to be closer while pulling other negative ones apart."
  - [section 3.2]: "Lpos local = 1/2 || 1/T Σi ϕ(MΘ(zni)) - 1/T Σj ϕ(MΘ(zqj)) ||^2_H..." and "Lneg local = 1/2 max[0, ξ - MMD(Πn, Πq)^2]..."
  - [corpus]: Weak - corpus lacks direct evidence of MMD-based contrastive loss for probabilistic embeddings in DG.
- Break condition: If the number of MC samples T is too small, the MMD estimates become noisy, degrading the quality of the contrastive alignment.

## Foundational Learning

- Concept: Bayesian neural networks and variational inference
  - Why needed here: BNNs provide the probabilistic framework to map inputs to distributions (probabilistic embeddings) rather than point estimates, which is crucial for capturing uncertainty and distributional information under insufficient data.
  - Quick check question: What is the role of the variational distribution qθ(w) in approximating the true posterior p(w|D) in a BNN?

- Concept: Maximum Mean Discrepancy (MMD) and kernel methods
  - Why needed here: MMD is used as a distribution distance metric for both global alignment (P-MMD) and local alignment (P-CSA), requiring understanding of RKHS, characteristic kernels, and empirical estimation.
  - Quick check question: Why must the kernel k used in MMD be characteristic for the distance to be a proper metric between distributions?

- Concept: Contrastive learning and metric learning
  - Why needed here: The P-CSA loss is a contrastive loss adapted to probabilistic embeddings, requiring understanding of positive/negative pair construction and distance metrics in embedding space.
  - Quick check question: In standard contrastive loss, what is the effect of the margin ξ on the separation between negative pairs?

## Architecture Onboarding

- Component map:
  Input -> Probabilistic Extractor (Qϕ, BNN) -> Probabilistic Embeddings (Π) -> Metric Network (MΘ) -> Output
  Classifier (Cω, BNN) for final prediction
  Loss components: Classification loss + KL divergence regularization + P-CSA loss (Llocal) + P-MMD loss (Lglobal)

- Critical path:
  1. Forward pass through Qϕ with T MC samples to get {z_ti} for each input
  2. Compute probabilistic embeddings Π_i = p(z|x_i, W) for each sample
  3. Compute P-MMD between domains for Lglobal
  4. Sample positive/negative pairs, compute MMD between their Π for Llocal
  5. Forward pass through Cω for classification
  6. Compute total loss and backpropagate

- Design tradeoffs:
  - T (MC samples) vs. computational cost: Higher T gives better approximation of Π but increases cost quadratically for P-MMD
  - Kernel bandwidth in Gaussian RBF kernels: Affects sensitivity of MMD/P-MMD to distributional differences
  - Distance margin ξ in P-CSA: Too small → insufficient separation; too large → over-separation and potential instability

- Failure signatures:
  - High variance in P-MMD/P-CSA loss across batches → MC sample size T may be too small
  - Degraded classification accuracy despite low alignment losses → KL divergence term may be too weak or BNN posterior poorly approximated
  - Slow convergence or unstable training → learning rates for BNN layers may need adjustment or variational approximation may be poor

- First 3 experiments:
  1. Ablation: Replace probabilistic embeddings with mean embeddings (E[Π]) and use standard MMD/contrastive loss; compare performance to verify benefit of distributional information
  2. Sensitivity: Vary T (e.g., 5, 10, 20) and measure impact on both performance and training time to find optimal tradeoff
  3. Kernel sensitivity: Test different Gaussian RBF kernel bandwidths for P-MMD and observe effect on domain alignment and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of probabilistic MMD (P-MMD) compare to mean embedding methods when using different numbers of Monte Carlo samples (T)?
- Basis in paper: [explicit] The paper discusses the comparison between mean embedding and kernel mean embedding methods with different Monte Carlo samples in Section 4.5 and Figure 4.
- Why unresolved: While the paper provides some experimental results comparing P-MMD and mean embedding methods, it does not thoroughly investigate the impact of varying the number of Monte Carlo samples (T) on the performance of these methods.
- What evidence would resolve it: Further experiments comparing the performance of P-MMD and mean embedding methods with different values of T would help determine the optimal number of samples for each method and provide insights into their relative strengths and weaknesses.

### Open Question 2
- Question: How does the proposed probabilistic framework perform on larger-scale benchmark datasets compared to small-scale ones?
- Basis in paper: [explicit] The paper mentions the scalability of the proposed method to benchmark datasets in Section 4.6 but does not provide extensive experimental results on larger-scale datasets.
- Why unresolved: The paper primarily focuses on evaluating the proposed method on small-scale datasets due to the focus on the insufficient data scenario. The performance on larger-scale datasets remains unexplored.
- What evidence would resolve it: Experiments on larger-scale benchmark datasets, such as DomainNet and Wilds, would provide insights into the scalability and generalization ability of the proposed probabilistic framework.

### Open Question 3
- Question: What is the impact of using different approximate posterior distributions in the Bayesian neural network (BNN) on the performance of the proposed method?
- Basis in paper: [inferred] The paper mentions the use of mean-field variational inference (MFVI) for approximating the posterior distribution in the BNN but does not explore alternative approximate posterior distributions.
- Why unresolved: While MFVI is a commonly used method for approximate inference in BNNs, other approaches like normalizing flows or more expressive posterior distributions might lead to improved performance.
- What evidence would resolve it: Experiments comparing the performance of the proposed method using different approximate posterior distributions in the BNN would help determine the impact of the choice of posterior on the overall performance and provide insights into potential improvements.

## Limitations
- Effectiveness relies heavily on quality of Bayesian neural network's posterior approximation, which may degrade with very limited training data
- Computational overhead of MC sampling (T samples) for probabilistic embeddings could be prohibitive for large-scale applications
- Kernel hyperparameters for P-MMD and P-CSA require careful tuning, and suboptimal choices may undermine performance
- Method validated primarily on medical imaging datasets, so generalizability to other domains remains to be tested

## Confidence

High confidence: The fundamental framework of using probabilistic embeddings for domain generalization is sound and theoretically grounded in BNN literature.

Medium confidence: The specific implementation of P-MMD and P-CSA losses is technically correct, though empirical validation across diverse domains is limited.

Medium confidence: The experimental results demonstrate effectiveness on medical imaging tasks, but the sample size (three datasets) is modest.

## Next Checks

1. Cross-domain generalization test: Evaluate the method on non-medical datasets (e.g., natural images, text) to assess domain transferability of the probabilistic embedding approach.

2. Ablation study on MC samples: Systematically vary T (number of Monte Carlo samples) to quantify the tradeoff between approximation quality and computational cost.

3. Robustness to prior specification: Test sensitivity to different prior distributions in the Bayesian layers to understand how prior choices affect domain generalization performance.