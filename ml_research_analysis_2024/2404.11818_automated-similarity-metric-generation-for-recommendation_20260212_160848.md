---
ver: rpa2
title: Automated Similarity Metric Generation for Recommendation
arxiv_id: '2404.11818'
source_url: https://arxiv.org/abs/2404.11818
tags:
- metrics
- metric
- similarity
- recommendation
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoSMG, the first method to automatically
  generate similarity metrics for recommender systems. Instead of using handcrafted
  metrics like inner product or L2 distance, AutoSMG constructs a metric space from
  basic embedding operators (e.g., addition, normalization) and uses evolutionary
  algorithms to search for optimal metrics.
---

# Automated Similarity Metric Generation for Recommendation

## Quick Facts
- arXiv ID: 2404.11818
- Source URL: https://arxiv.org/abs/2404.11818
- Reference count: 40
- This paper introduces AutoSMG, the first method to automatically generate similarity metrics for recommender systems, achieving up to 14.5% improvement in NDCG@20.

## Executive Summary
This paper presents AutoSMG, a novel approach for automatically generating similarity metrics in recommender systems. Instead of relying on handcrafted metrics like inner product or L2 distance, AutoSMG constructs a metric space from basic embedding operators and uses evolutionary algorithms to search for optimal metrics. The method employs early stopping and surrogate models to efficiently evaluate candidate metrics without full training. Tested across three datasets and different recommendation architectures, AutoSMG consistently outperforms handcrafted metrics and other automated search methods.

## Method Summary
AutoSMG operates by first defining a space of basic embedding operators (addition, subtraction, inner product, cosine similarity, etc.) that serve as building blocks. These operators are assembled into computational graphs representing candidate similarity metrics through evolutionary search. The search process uses early stopping (training for a fixed small number of epochs) or surrogate models (LSTM-based predictors) to approximate metric fitness without full training. The method is model-agnostic and can be integrated into different recommendation architectures like NCF and LightGCN. The evolutionary algorithm maintains a population of 50 candidate metrics across 100 generations, with mutation ratio of 0.7.

## Key Results
- AutoSMG outperforms handcrafted metrics and other automated search methods on MovieLens-1M, Gowalla, and LastFM datasets
- Achieves improvements of up to 14.5% in NDCG@20 compared to inner product baseline
- Successfully integrates with both NCF and LightGCN recommendation architectures
- Demonstrates efficient search through early stopping and surrogate model approximations

## Why This Works (Mechanism)

### Mechanism 1
AutoSMG constructs a space of possible similarity metrics by sampling from basic embedding operators and assembling them into computational graphs, enabling discovery of domain-specific similarity patterns that handcrafted metrics miss. The method defines fundamental vector operations as building blocks and uses evolutionary search to sample combinations into directed acyclic graphs representing computable similarity metrics.

### Mechanism 2
The method uses early stopping and surrogate models to approximate metric fitness without full training, dramatically reducing computational cost while maintaining search quality. Instead of fully training each candidate metric, AutoSMG either trains for a fixed small number of epochs or trains a surrogate LSTM model on previously evaluated metrics to predict performance.

### Mechanism 3
AutoSMG is model-agnostic and can be plugged into different recommendation architectures, enabling transfer of learned similarity metrics across encoder types. The method treats the similarity metric as a standalone component that takes user and item embeddings as input and outputs a similarity score, which can be inserted into any encoder-decoder framework without modifying the encoder architecture.

## Foundational Learning

- **Evolutionary algorithms for search in discrete/combinatorial spaces**: Needed to explore vast space of possible computational graphs representing similarity metrics. Quick check: How does an evolutionary algorithm maintain diversity in the population to avoid premature convergence?

- **Computational graph representation of mathematical expressions**: Required to construct, evaluate, and mutate DAGs representing candidate metrics. Quick check: How do you ensure type compatibility between nodes when inserting or deleting operators in a computational graph?

- **Surrogate model training for performance prediction**: Essential for predicting metric fitness without full training. Quick check: What loss function and architecture would you use to predict a scalar fitness score from a variable-length sequence of operator embeddings?

## Architecture Onboarding

- **Component map**: Operator space → Metric space construction → Evolutionary search loop → Fitness evaluation (early stopping/surrogate) → Metric selection → Integration into encoder

- **Critical path**: Operator space → Metric space construction → Evolutionary search loop → Fitness evaluation (early stopping/surrogate) → Metric selection → Integration into encoder

- **Design tradeoffs**: Operator set size vs. search space complexity; Early stopping epochs vs. evaluation accuracy; Surrogate model capacity vs. training data requirements

- **Failure signatures**: Search converges to handcrafted metrics (operator set too limited); Performance degrades after integration (metric overfit to specific encoder); Search too slow (early stopping not correlated with final performance)

- **First 3 experiments**: 1) Verify operator space can reconstruct known handcrafted metrics as computational graphs; 2) Test early stopping correlation by comparing early-phase vs final performance; 3) Validate surrogate model accuracy by predicting performance on held-out metrics

## Open Questions the Paper Calls Out

### Open Question 1
How do the performance characteristics of similarity metrics generated through AutoSMG vary across different recommendation tasks beyond Top-K recommendation, such as rating prediction or sequential recommendation? The paper's experimental scope is limited to Top-K recommendation, leaving the generalizability of AutoSMG to other recommendation paradigms untested.

### Open Question 2
What is the relationship between the structural similarity of top-performing metrics and their effectiveness across different datasets and model architectures? While the paper notes structural similarities among top metrics, it does not investigate the underlying reasons for these similarities or their implications for metric design.

### Open Question 3
How does the integration of AutoSMG with joint search for both similarity metrics and loss functions impact the overall performance of recommender systems compared to optimizing them separately? The paper focuses on optimizing similarity metrics independently, leaving the potential benefits of simultaneous optimization unexplored.

## Limitations

- Several key implementation details remain unspecified, particularly around operator space definition and surrogate model architecture
- Method relies heavily on evolutionary search, which introduces stochastic variability that may affect reproducibility
- Claim of model-agnostic integration requires further validation across diverse encoder architectures beyond NCF and LightGCN

## Confidence

**High confidence**: The core claim that automated metric generation can outperform handcrafted metrics is well-supported by empirical results showing consistent improvements across three datasets and two encoder architectures.

**Medium confidence**: The mechanism of using early stopping and surrogate models for efficient fitness approximation appears sound, but specific implementation details are underspecified.

**Low confidence**: The assertion that AutoSMG can be seamlessly integrated into any recommendation model architecture is the weakest claim, as the paper only demonstrates integration with two specific architectures.

## Next Checks

1. **Operator Space Verification**: Implement the full operator space and verify that known handcrafted metrics (inner product, L2 distance) can be exactly reconstructed as computational graphs within the search space.

2. **Early Stopping Correlation Analysis**: Systematically vary the number of training epochs used for early stopping evaluation and measure the correlation between early-phase performance and final performance across a range of candidate metrics.

3. **Surrogate Model Transferability**: Train the surrogate model on metrics evaluated from one dataset and test its predictive accuracy on metrics from a different dataset to assess generalization capability across domains.