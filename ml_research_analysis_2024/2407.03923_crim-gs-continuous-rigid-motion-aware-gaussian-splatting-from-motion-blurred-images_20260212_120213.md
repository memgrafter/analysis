---
ver: rpa2
title: 'CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion-Blurred
  Images'
arxiv_id: '2407.03923'
source_url: https://arxiv.org/abs/2407.03923
tags:
- camera
- neural
- motion
- images
- transformation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CRiM-GS addresses the problem of reconstructing 3D scenes from
  motion-blurred images by proposing a novel framework that uses continuous rigid
  motion-aware Gaussian splatting. The core idea is to model continuous camera trajectories
  using neural ordinary differential equations (ODE), combined with rigid body transformations
  and an adaptive distortion-aware transformation to handle nonlinear distortions
  like rolling shutter effects.
---

# CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion-Blurred Images

## Quick Facts
- arXiv ID: 2407.03923
- Source URL: https://arxiv.org/abs/2407.03923
- Authors: Jungho Lee; Donghyeong Kim; Dogyoon Lee; Suhwan Cho; Minhyeok Lee; Sangyoun Lee
- Reference count: 40
- Achieves PSNR of 30.46, SSIM of 0.9095, and LPIPS of 0.0441 on synthetic scenes, and PSNR of 27.35, SSIM of 0.8315, and LPIPS of 0.0633 on real-world scenes

## Executive Summary
CRiM-GS introduces a novel framework for reconstructing 3D scenes from motion-blurred images by leveraging continuous rigid motion-aware Gaussian splatting. The method uses neural ordinary differential equations (ODE) to model smooth camera trajectories, combined with rigid body transformations and adaptive distortion-aware transformations to handle rolling shutter effects. Extensive experiments demonstrate state-of-the-art performance on both synthetic and real-world datasets, achieving high-quality novel view synthesis with effective deblurring.

## Method Summary
CRiM-GS models continuous camera trajectories using neural ODEs, which output rigid body transformations via screw axis decomposition. The framework employs an adaptive distortion-aware transformation to compensate for nonlinear distortions like rolling shutter effects, with regularization to maintain physical plausibility. Multiple camera poses are sampled along the trajectory, and 3D Gaussian splatting renders images from each pose. A pixel-wise weighted sum, inspired by traditional deblurring, combines these renders, with a per-pixel mask blending sharp and blurry regions. The method is trained end-to-end with L1, D-SSIM, and regularization losses.

## Key Results
- Achieves PSNR of 30.46, SSIM of 0.9095, and LPIPS of 0.0441 on synthetic scenes
- Achieves PSNR of 27.35, SSIM of 0.8315, and LPIPS of 0.0633 on real-world scenes
- Outperforms traditional deblurring approaches and demonstrates robustness to inaccurate camera poses

## Why This Works (Mechanism)

### Mechanism 1
Neural ODEs provide continuous, differentiable camera motion trajectories that capture complex motion blur more accurately than discrete or spline-based approaches. The camera trajectory is modeled as a time-continuous function using a neural network parameterized by ODE, which outputs a latent state that evolves smoothly over the exposure time and is decoded into rigid body transformations. By sampling this trajectory at multiple time points, the method generates multiple camera poses that together represent the motion blur. Core assumption: Camera motion during exposure is smooth and can be represented as a continuous function in the latent space. Evidence anchors: [abstract], [section 4.2]. Break condition: If camera motion is discontinuous or highly non-smooth (e.g., sudden jerks), the continuous ODE assumption fails.

### Mechanism 2
The combination of rigid body transformation and adaptive distortion-aware transformation allows the model to handle both predictable linear motion and unpredictable nonlinear distortions like rolling shutter effects. The rigid body transformation ensures the shape and size of static objects remain consistent during motion, using screw axis decomposition. The adaptive distortion-aware transformation adds a learned offset to this rigid transformation, with regularization to keep it close to SE(3) space while allowing flexibility for distortions. Core assumption: Static objects maintain their shape/size during camera motion, and nonlinear distortions are local deviations from rigid motion. Evidence anchors: [abstract], [section 4.3]. Break condition: If distortions are so severe that they violate the rigid body assumption fundamentally, or if the regularization prevents necessary flexibility.

### Mechanism 3
Pixel-wise weighted sum of rendered images with learned weights more accurately reconstructs the motion blur than simple averaging or kernel-based approaches. Multiple images are rendered from the sampled camera poses along the trajectory. A lightweight CNN computes pixel-wise weights for these images, and a weighted sum creates the final blurry image. A per-pixel mask blends sharp and blurry regions. This approach is inspired by traditional blind deblurring but applied in 3D rendering. Core assumption: The motion blur in each pixel can be accurately represented by a weighted combination of sharp renders from different poses. Evidence anchors: [abstract], [section 4.4]. Break condition: If the blur kernel is too complex for a linear combination of renders to capture, or if the CNN cannot learn appropriate weights for all pixel locations.

## Foundational Learning

- Concept: Neural Ordinary Differential Equations (Neural ODEs)
  - Why needed here: To model continuous camera motion trajectories that capture complex motion blur patterns more accurately than discrete or spline-based methods.
  - Quick check question: What is the main advantage of using Neural ODEs over traditional discrete-time models for representing camera motion?

- Concept: Rigid Body Transformations and Screw Theory
  - Why needed here: To ensure that static objects maintain their shape and size during camera motion, which is a fundamental physical constraint that must be preserved for accurate 3D reconstruction.
  - Quick check question: What are the three components required to fully describe a rigid body transformation using screw theory?

- Concept: Gaussian Splatting and Differentiable Rendering
  - Why needed here: To achieve real-time rendering performance while maintaining high-quality novel view synthesis, which is essential for practical applications and efficient training.
  - Quick check question: How does Gaussian splatting differ from volumetric rendering approaches like NeRF in terms of computational efficiency?

## Architecture Onboarding

- Component map:
  Image index → Encoder → Neural ODE → Decoder → Camera poses → 3DGS renderer → Multiple renders → Weight CNN → Weighted sum → Final output

- Critical path:
  Image index → Encoder → Neural ODE → Decoder → Camera poses → 3D Gaussian Splatting Renderer → Multiple renders → Weight CNN → Weighted sum → Final output

- Design tradeoffs:
  - Neural ODE vs. splines: More expressive but potentially harder to train
  - Adaptive transformation regularization: Balances flexibility vs. physical plausibility
  - Number of poses N: More poses = better blur modeling but slower rendering
  - Pixel-wise weighting vs. simple averaging: More accurate but requires additional CNN

- Failure signatures:
  - Training instability: Likely due to Neural ODE initialization or learning rate issues
  - Blurry regions too sharp/soft: Indicates incorrect camera trajectory or weighting
  - Geometric distortions: Suggests insufficient adaptive transformation or regularization
  - Slow convergence: May need curriculum learning or better initialization

- First 3 experiments:
  1. Ablation study with only rigid body transformation (remove adaptive distortion) to verify its contribution
  2. Vary number of poses N (e.g., 5, 7, 9, 11) to find optimal balance between quality and speed
  3. Compare pixel-wise weighted sum vs. simple averaging on a simple scene to validate the weighting mechanism

## Open Questions the Paper Calls Out
None explicitly stated in the provided text.

## Limitations
- Evaluation framework lacks comparison with recently published motion-aware reconstruction methods, primarily benchmarking against traditional deblurring approaches
- Claims of robustness to inaccurate camera poses are not systematically validated with controlled experiments on degraded pose accuracy
- Computational efficiency claims lack quantitative runtime and memory usage benchmarks compared to baseline Gaussian splatting approaches

## Confidence
- **Medium** on state-of-the-art performance claims: Competitive metrics reported but lacks comparison with contemporary motion-aware Gaussian splatting methods
- **Medium** regarding robustness to inaccurate camera poses: Claims supported by COLMAP estimates but no systematic evaluation of pose error sensitivity
- **Low** on computational efficiency claims: Real-time rendering mentioned but actual runtime overhead of 9 poses + weight CNN not quantified

## Next Checks
1. **Ablation study on adaptive distortion-aware transformation**: Systematically evaluate the contribution of the adaptive transformation component by comparing performance with and without it across scenes with varying rolling shutter characteristics.

2. **Pose accuracy sensitivity analysis**: Conduct controlled experiments with synthetically degraded camera poses to quantify the framework's robustness to pose estimation errors and identify failure thresholds.

3. **Computational complexity benchmarking**: Measure and report the actual runtime performance, memory usage, and rendering speed compared to baseline Gaussian splatting approaches across different scene complexities.