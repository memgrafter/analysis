---
ver: rpa2
title: Meta-prompting Optimized Retrieval-augmented Generation
arxiv_id: '2407.03955'
source_url: https://arxiv.org/abs/2407.03955
tags:
- content
- prompt
- query
- optimization
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method that improves retrieval-augmented
  generation by refining the retrieved content before it is passed to the generation
  model. The refinement is done using a transformation language model, guided by an
  instruction optimized via meta-prompting.
---

# Meta-prompting Optimized Retrieval-augmented Generation

## Quick Facts
- arXiv ID: 2407.03955
- Source URL: https://arxiv.org/abs/2407.03955
- Reference count: 40
- Primary result: 34.69% accuracy on StrategyQA, outperforming baseline RAG (26.12%) by over 30%

## Executive Summary
This paper introduces a method to improve retrieval-augmented generation by refining retrieved content before it reaches the generation model. The refinement is performed by a transformation language model guided by an instruction that is optimized through meta-prompting. Experiments on the StrategyQA dataset demonstrate a significant improvement over baseline RAG performance, validating the effectiveness of optimizing retrieved content rather than the query or instruction.

## Method Summary
The method uses an iterative meta-prompting optimization process where an optimizer language model generates and evaluates tentative refinement instructions based on their contribution to RAG performance. The best-performing instructions are retained and used to refine retrieved content via a transformation model before passing it to the generation model. The approach was tested on StrategyQA with Llama-2-70b models, using 100 optimization steps with 3 new instructions per step evaluated on 6 random training examples each.

## Key Results
- Achieved 34.69% accuracy on StrategyQA, outperforming baseline RAG (26.12%) by over 30%
- Meta-prompting optimization converges to better instructions than random selection
- Refining retrieved content proves more effective than optimizing queries or instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing retrieval-augmented generation by refining retrieved content improves accuracy.
- Mechanism: The method uses a transformation language model to refine retrieved content based on an instruction optimized via meta-prompting. This refined content is then passed to the generation model, leading to more accurate answers.
- Core assumption: The transformation model can effectively refine retrieved content to better align with the query and improve downstream task performance.
- Evidence anchors:
  - [abstract] "This paper introduces a method that improves retrieval-augmented generation by refining the retrieved content before it is passed to the generation model. The refinement is done using a transformation language model, guided by an instruction optimized via meta-prompting."
  - [section] "This reﬁnement is accomplished by means of an auxiliary transformation-LLM that is entered with a prompt containing the pieces of retrieved contents, preceded by an instruction with the request for the sought reﬁnement."
  - [corpus] Weak evidence - related papers discuss various RAG improvements but don't specifically address content refinement via meta-prompting optimization.
- Break condition: If the transformation model fails to effectively refine the content or if the meta-prompting optimization doesn't find better instructions, the accuracy improvement may not be realized.

### Mechanism 2
- Claim: Meta-prompting optimization iteratively improves the refinement instruction.
- Mechanism: A meta-prompt containing a meta-instruction and a list of tentative instructions is used as input to an optimizer-LLM. This optimizer-LLM generates new tentative instructions, scores them based on their contribution to RAG performance, and updates the meta-prompt with the top-performing instructions. This process is iterated until the best scoring instruction is found.
- Core assumption: The optimizer-LLM can generate and evaluate instructions that improve RAG performance through iterative refinement.
- Evidence anchors:
  - [abstract] "The refinement is done using a transformation language model, guided by an instruction optimized via meta-prompting."
  - [section] "This meta-prompting optimization is undertaken by an optimizer-LLM that is entered with a (meta-)prompt that includes a (meta-)instruction and a list of tentative reﬁnement instructions and respective performance scores."
  - [corpus] Weak evidence - while related papers discuss prompt optimization, none specifically describe the meta-prompting approach used here for instruction optimization.
- Break condition: If the optimizer-LLM fails to generate better instructions or if the scoring mechanism doesn't accurately reflect instruction quality, the iterative improvement process may stall or converge to suboptimal solutions.

### Mechanism 3
- Claim: Refining retrieved content before generation is more effective than refining the query or instruction.
- Mechanism: Instead of optimizing the query or instruction as in previous approaches, this method focuses on optimizing the version of the retrieved content that is included in the prompt entered into the generation-LLM. This is achieved by using a transformation-LLM to refine the content based on an optimized instruction.
- Core assumption: The retrieved content is a more significant factor in RAG performance than the query or instruction, and refining it can lead to greater improvements.
- Evidence anchors:
  - [abstract] "Differently from previous approaches, our method focuses instead on optimizing the version of the retrieved content that is included in the prompt entered into the generation-LLM."
  - [section] "A prompt for RAG includes a query and the content retrieved from external sources on the basis of that query... Related work for RAG enhanced with prompt optimization has concentrated on optimizing the instruction and/or the query. Differently from previous approaches, our method focuses instead on optimizing the version of the retrieved content that is included in the prompt entered into the generation-LLM."
  - [corpus] Weak evidence - while related papers discuss various RAG enhancements, none specifically compare the effectiveness of refining retrieved content versus refining the query or instruction.
- Break condition: If the retrieved content is already highly relevant and well-structured, or if the generation model is robust to noisy input, the benefits of content refinement may be minimal.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG is the foundation of the proposed method, which aims to improve its performance by refining retrieved content.
  - Quick check question: What are the key components of a RAG system, and how does it differ from standard language model generation?

- Concept: Meta-prompting
  - Why needed here: Meta-prompting is used to optimize the instruction for refining retrieved content, which is a key part of the proposed method.
  - Quick check question: How does meta-prompting differ from standard prompt engineering, and what are its potential benefits for optimizing RAG systems?

- Concept: Language model fine-tuning
  - Why needed here: Understanding language model fine-tuning is important for grasping the context of the proposed method, which uses pre-trained and fine-tuned models.
  - Quick check question: What are the differences between pre-training, fine-tuning, and in-context learning for language models, and when might each approach be most appropriate?

## Architecture Onboarding

- Component map:
  Query input -> Retrieval model -> Transformation model -> Generation model -> Answer output

- Critical path:
  1. User enters query
  2. Retrieval model retrieves relevant content
  3. Transformation model refines retrieved content using optimized instruction
  4. Generation model generates answer based on query and refined content
  5. Answer is returned to user

- Design tradeoffs:
  - Using a separate transformation model adds complexity but allows for more targeted refinement of retrieved content
  - Meta-prompting optimization can be computationally expensive but may lead to better instructions and improved performance
  - The choice of retrieval model and its parameters can significantly impact the quality of retrieved content and downstream performance

- Failure signatures:
  - Poor retrieval quality leading to irrelevant or noisy content
  - Transformation model failing to effectively refine content or introducing errors
  - Optimizer model converging to suboptimal instructions or failing to improve performance
  - Generation model producing verbose or incorrect answers despite refined content

- First 3 experiments:
  1. Evaluate the impact of content refinement on RAG performance using a fixed instruction
  2. Compare the effectiveness of meta-prompting optimization versus other instruction optimization methods
  3. Assess the contribution of each component (retrieval, transformation, generation) to overall performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the meta-prompting optimized method scale with larger language models?
- Basis in paper: [inferred] The paper only tested the method with Llama-2-70b and Llama-2-70b-chat models. It mentions "exploring further evaluation functions" and "scaling up with larger models" as future work.
- Why unresolved: The experiments were limited to specific model sizes, and the paper explicitly identifies scaling as future work.
- What evidence would resolve it: Empirical results comparing the method's performance across different model sizes (e.g., 7B, 13B, 70B parameters) using the same dataset and evaluation metrics.

### Open Question 2
- Question: What is the optimal number of iterations for the meta-prompting optimization process?
- Basis in paper: [explicit] The paper states "The instruction optimization was iterated over 100 steps" but notes "a high fluctuation of the evaluation scores can be observed along the iteration steps" and identifies this as an area for future exploration.
- Why unresolved: The paper chose 100 iterations arbitrarily and observed score fluctuations, suggesting there may be an optimal number that hasn't been determined.
- What evidence would resolve it: Systematic experiments varying the number of iterations (e.g., 10, 50, 100, 200) and analyzing the trade-off between optimization quality and computational cost.

### Open Question 3
- Question: How does the proposed method perform on non-English languages, particularly Portuguese?
- Basis in paper: [explicit] The paper mentions "It will be interesting also to study the interaction of our proposed method with the Portuguese language" as future work, and the authors have developed Portuguese language models.
- Why unresolved: All experiments were conducted on English language data, and the paper explicitly identifies multilingual evaluation as future work.
- What evidence would resolve it: Empirical results showing the method's performance on Portuguese datasets (such as those mentioned in the references) using Portuguese language models, compared to English results.

## Limitations

- The evaluation framework relies on binary exact-match scoring which may not capture partial correctness in multi-hop reasoning
- Meta-prompting optimization shows high score volatility across iterations, suggesting potential instability in instruction quality assessment
- The "brute force" optimization baseline (300 instructions generated at once) is included but its poor performance raises questions about experimental design and comparison validity

## Confidence

**High Confidence**: The core mechanism of using a transformation language model to refine retrieved content before generation is clearly described and experimentally validated with a 30% improvement over baseline.

**Medium Confidence**: The claim that refining retrieved content is more effective than refining queries or instructions is supported by comparative results, but the experimental design doesn't isolate this factor from other differences between approaches.

**Low Confidence**: The scalability of the meta-prompting approach to larger datasets or more complex reasoning tasks is unclear, and the computational cost of the iterative optimization process is not discussed in terms of practical deployment considerations.

## Next Checks

1. **Ablation Study**: Remove the transformation model and pass retrieved content directly to the generation model to quantify the specific contribution of content refinement versus instruction optimization.

2. **Robustness Testing**: Evaluate the method on queries where retrieved content contains conflicting information or multiple plausible interpretations to assess how well the refinement process handles ambiguity.

3. **Generalization Test**: Apply the optimized instruction to a different multi-hop QA dataset (e.g., HotpotQA) without further optimization to measure cross-dataset transfer performance.