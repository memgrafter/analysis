---
ver: rpa2
title: Can we teach language models to gloss endangered languages?
arxiv_id: '2406.18895'
source_url: https://arxiv.org/abs/2406.18895
tags:
- language
- examples
- languages
- computational
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can be effective at interlinear glossing
  of endangered languages using in-context learning, without any traditional training.
  We propose new approaches for selecting examples to provide in-context, observing
  that targeted selection can significantly improve performance.
---

# Can we teach language models to gloss endangered languages?

## Quick Facts
- arXiv ID: 2406.18895
- Source URL: https://arxiv.org/abs/2406.18895
- Authors: Michael Ginn; Mans Hulden; Alexis Palmer
- Reference count: 29
- Large language models can perform interlinear glossing of endangered languages using in-context learning without traditional training

## Executive Summary
This paper explores whether large language models (LLMs) can be used for interlinear glossing (IGT) of endangered languages through in-context learning, without requiring traditional training. The authors propose targeted example selection strategies for few-shot prompting and find that these significantly outperform random selection and transformer baselines. While LLM-based approaches still underperform state-of-the-art supervised systems, they offer a practical solution for researchers outside the NLP community, requiring minimal effort to implement.

## Method Summary
The authors use in-context learning with the Cohere Command R+ 104B parameter model to generate IGT for four endangered languages from the SIGMORPHON 2023 shared task dataset. They test six retrieval strategies for selecting in-context examples: random sampling, word recall/precision, chrF score (character n-gram F-score), max coverage, and morpheme recall using Morfessor segmentation. The approach varies the number of examples from 0 to 100 shots and compares performance against a baseline transformer model and the state-of-the-art Tü-CL system using morpheme accuracy as the evaluation metric.

## Key Results
- Targeted example selection significantly improves performance over random selection across all tested languages
- The relationship between number of few-shot examples and accuracy follows a roughly logarithmic pattern
- chrF-based retrieval (character n-gram F-score) is the most effective strategy for selecting in-context examples
- LLM-based approaches outperform transformer baselines but underperform state-of-the-art supervised systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can perform interlinear glossing of endangered languages using in-context learning without traditional training.
- Mechanism: LLMs leverage their multilingual knowledge and learn patterns in rare languages through exposure to examples in the prompt context.
- Core assumption: The model has some underlying multilingual knowledge that can be activated through few-shot examples.
- Evidence anchors:
  - [abstract] "As large language models (LLMs) have showed promising results across multilingual tasks, even for rare, endangered languages (Zhang et al., 2024), it is natural to wonder whether they can be utilized for the task of generating IGT."
  - [section 2.2] "Though LLMs generally have limited understanding of rare and low-resource languages (Ebrahimi et al., 2022), they can often achieve significantly better performance through crosslingual in-context learning (X-ICL)"
  - [corpus] Weak - only 25 related papers found, suggesting limited existing research on this specific combination of LLMs and endangered language glossing
- Break condition: The LLM has no prior knowledge of the target language at all, even at a subword level, making pattern learning impossible.

### Mechanism 2
- Claim: Selecting relevant in-context examples significantly improves LLM glossing performance compared to random selection.
- Mechanism: By retrieving examples that share morphological or character-level similarity with the target sentence, the model receives more relevant context for pattern recognition.
- Core assumption: Similarity between training examples and target sentences correlates with usefulness for the glossing task.
- Evidence anchors:
  - [abstract] "We propose new approaches for selecting examples to provide in-context, observing that targeted selection can significantly improve performance."
  - [section 5.2] "Comparison with Random Retrieval Across all languages, we observe clear and significant improvements over the random selection method"
  - [corpus] Weak - only 25 related papers, limited evidence on retrieval strategies for IGT generation
- Break condition: The retrieval metrics (word recall, chrF, morpheme recall) fail to capture true task-relevant similarity.

### Mechanism 3
- Claim: The relationship between number of few-shot examples and glossing accuracy follows a roughly logarithmic pattern.
- Mechanism: Initial examples provide substantial learning gains, but each additional example yields diminishing returns as the model approaches its capacity limit.
- Core assumption: The learning curve follows a predictable mathematical relationship that can be modeled.
- Evidence anchors:
  - [abstract] "The relationship between performance and number of few-shot examples is roughly logarithmic."
  - [section 4.2] "We observe extremely strong correlation values across all settings. This indicates that the logarithmic model is a good fit for the data"
  - [corpus] Weak - only 25 related papers, limited evidence on scaling relationships for few-shot learning in low-resource languages
- Break condition: The model reaches a plateau where additional examples provide no benefit or actually degrade performance due to noise.

## Foundational Learning

- Concept: Interlinear glossed text (IGT) format
  - Why needed here: The entire task involves generating IGT, so understanding its structure is fundamental
  - Quick check question: What are the three lines in a typical IGT example and what information does each contain?

- Concept: Crosslingual in-context learning (X-ICL)
  - Why needed here: The approach relies on providing examples in the target language to activate multilingual knowledge
  - Quick check question: How does X-ICL differ from standard few-shot prompting?

- Concept: Character-level n-gram similarity metrics (chrF)
  - Why needed here: The most effective retrieval strategy uses chrF to find morphologically similar examples
  - Quick check question: Why might character-level similarity be more useful than word-level similarity for morphologically rich languages?

## Architecture Onboarding

- Component map: Prompt template → Example retrieval system → LLM inference → Evaluation metrics
- Critical path: Retrieve relevant examples → Construct prompt with examples → Send to LLM → Parse and evaluate output
- Design tradeoffs: Longer prompts with more examples improve accuracy but increase cost and may hit context limits
- Failure signatures: Zero-shot performance is very poor, indicating the model lacks prior knowledge of the language
- First 3 experiments:
  1. Run zero-shot prompting on a small language to establish baseline performance
  2. Test random example selection with 5, 10, and 30 examples to observe scaling effects
  3. Implement chrF-based retrieval and compare against random selection with same number of examples

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the discussion and limitations, several important questions emerge:

- How does the performance of LLM-based interlinear glossing systems scale with increasing context length beyond the tested limits?
- How does the effectiveness of different example selection strategies vary across languages with different morphological typologies?
- How would the inclusion of additional linguistic information in prompts (beyond the gloss list) affect the performance of LLM-based interlinear glossing systems?

## Limitations

- The four tested languages are not representative of all the world's languages, particularly lacking languages with non-Latin writing systems
- The approach still underperforms state-of-the-art supervised systems by a considerable margin
- The logarithmic scaling relationship observed may not hold for very large or very small numbers of examples

## Confidence

- **High Confidence**: Targeted example selection improves performance over random selection across all four tested languages
- **Medium Confidence**: LLM approaches are practical for researchers outside the NLP community, though real-world applicability may be limited by API costs and context window constraints
- **Low Confidence**: The assertion that these approaches can "significantly improve performance" compared to baselines is somewhat overstated given the modest absolute gains

## Next Checks

1. **Cross-model validation**: Test the retrieval strategies across multiple LLM architectures (different sizes, different companies) to verify that the improvements are not specific to the Command R+ model used in the experiments.

2. **Robustness to example quality**: Systematically vary the quality of in-context examples (e.g., by introducing errors or using examples from different dialects) to understand the sensitivity of the approach to example quality versus example relevance.

3. **Generalization to new languages**: Apply the best-performing retrieval strategy to a completely different set of endangered languages not included in the SIGMORPHON 2023 dataset to test whether the observed improvements generalize beyond the specific languages studied.