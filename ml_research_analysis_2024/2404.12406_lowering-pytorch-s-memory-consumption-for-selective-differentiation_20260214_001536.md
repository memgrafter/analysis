---
ver: rpa2
title: Lowering PyTorch's Memory Consumption for Selective Differentiation
arxiv_id: '2404.12406'
source_url: https://arxiv.org/abs/2404.12406
tags:
- layers
- differentiable
- memory
- layer
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses memory inefficiencies in PyTorch's automatic
  differentiation engine during selective differentiation, where gradients are computed
  for only a subset of model parameters. The authors identify that PyTorch unnecessarily
  stores inputs to linear layers (convolutions, fully-connected, and batch normalization
  in evaluation mode) even when those layers' parameters are marked as non-differentiable.
---

# Lowering PyTorch's Memory Consumption for Selective Differentiation

## Quick Facts
- arXiv ID: 2404.12406
- Source URL: https://arxiv.org/abs/2404.12406
- Reference count: 40
- Primary result: Memory reductions up to 6x for selective differentiation scenarios without runtime overhead

## Executive Summary
This paper addresses a significant memory inefficiency in PyTorch's automatic differentiation engine during selective differentiation, where gradients are computed for only a subset of model parameters. The authors identify that PyTorch unnecessarily stores inputs to linear layers (convolutions, fully-connected, and batch normalization in evaluation mode) even when those layers' parameters are marked as non-differentiable. They propose a drop-in implementation of these layers that is agnostic to parameter differentiability, discarding unnecessary inputs during forward passes. Their method significantly reduces memory consumption without affecting runtime performance, with experiments showing memory reductions of up to 6x in selective differentiation scenarios while maintaining equivalent computation speed.

## Method Summary
The authors developed custom implementations of PyTorch's linear layers (Conv2d, Linear, ConvTranspose2d, BatchNorm2d) that check parameter differentiability during the forward pass and conditionally store inputs only when needed for gradient computation. They created a converter function that replaces standard layers with these memory-optimized versions. The approach leverages the fact that Jacobians of linear layers depend on weights, inputs, or neither, allowing selective storage based on which parameters require gradients. The method requires careful handling of layer interactions (ReLU, dropout) through custom implementations to achieve full memory savings.

## Key Results
- Memory reductions of up to 6x in selective differentiation scenarios on CNNs and Transformers
- Runtime performance remains equivalent to standard PyTorch implementation
- Easy to use via a converter function that replaces standard layers with memory-optimized versions
- Memory savings apply across different selective differentiation patterns (all input parameters differentiable, surgical fine-tuning, norm-bounded optimization)

## Why This Works (Mechanism)

### Mechanism 1
PyTorch stores convolution layer inputs even when parameters are non-differentiable, leading to unnecessary memory usage. The automatic differentiation engine saves inputs to layers acting linearly in parameters (convolutions, fully-connected, batch normalization) regardless of whether gradients are needed for those parameters. By checking parameter differentiability during the forward pass, unnecessary inputs can be discarded.

### Mechanism 2
The memory savings apply to selective differentiation scenarios where gradients are computed for only a subset of parameters. When only input parameters are differentiable (e.g., adversarial example generation), storing all layer inputs is unnecessary since no weight gradients are needed. Our implementation detects this and stores only required tensors.

### Mechanism 3
Layer interactions (ReLU, dropout) can prevent memory savings even when linear layers are optimized. Activations and other layers may store outputs that serve as inputs to subsequent linear layers, making our optimizations ineffective. Custom implementations (mask-based ReLU, dropout state saving) resolve this.

## Foundational Learning

- Concept: Automatic Differentiation and Computation Graphs
  - Why needed here: Understanding how PyTorch stores computation graphs for backpropagation is fundamental to identifying memory inefficiencies.
  - Quick check question: What information does PyTorch store in the computation graph during a forward pass, and how is it used during backpropagation?

- Concept: Linear Layer Jacobians
  - Why needed here: Knowing how Jacobians depend on weights and inputs allows determining which tensors must be stored for gradient computation.
  - Quick check question: For a convolution layer Z = W * X + b, which tensors are required to compute the Jacobians with respect to W, X, and b?

- Concept: Selective Differentiation Patterns
  - Why needed here: Understanding common scenarios where only subsets of parameters need gradients helps identify optimization opportunities.
  - Quick check question: What are three common deep learning tasks that use selective differentiation, and which parameters are typically marked as non-differentiable?

## Architecture Onboarding

- Component map: Conv2d -> Linear -> ConvTranspose2d -> BatchNorm2d -> Custom torch.autograd.Function implementations
- Critical path: 1. Forward pass with differentiability checks 2. Selective tensor storage based on parameter requirements 3. Backward pass using stored tensors 4. Memory measurement and comparison
- Design tradeoffs: Memory vs. implementation complexity (custom layers vs. standard PyTorch), Runtime overhead vs. memory savings (dropout state regeneration), Compatibility vs. optimization (torch.compile support)
- Failure signatures: No memory savings observed (layer interactions preventing optimization), Runtime slowdown (inefficient custom implementations), Memory leaks (improper tensor management), Compatibility issues (torch.compile breaking custom layers)
- First 3 experiments: 1. Deep CNN with all parameters differentiable vs. all non-differentiable to verify baseline memory behavior 2. Selective differentiation with only input differentiable to test memory savings 3. Layer interaction test with ReLU/dropout preceding linear layers to verify custom implementations work

## Open Questions the Paper Calls Out

The paper identifies several open questions: (1) How the approach performs on hybrid architectures combining convolution-based and attention-based components like Vision Transformers, (2) Memory-saving potential for extremely large models (100B+ parameters) where memory constraints are most severe, and (3) Impact on distributed training scenarios where memory optimization at the node level interacts with communication and synchronization overhead.

## Limitations

- Limited experimental scope to standard CNNs and Transformers with mini-batch sizes of 64
- Uncertain effectiveness on larger models, different batch sizes, or more complex layer combinations
- Potential compatibility issues with torch.compile and other PyTorch optimization passes
- Requires replacing standard layers with custom implementations which may break complex training pipelines

## Confidence

High confidence in the core memory optimization mechanism and experimental results for tested scenarios. Medium confidence in generalizability to all PyTorch models and training setups. Medium confidence in runtime performance claims across diverse hardware and PyTorch configurations.

## Next Checks

1. Test memory savings on larger models (e.g., LLaMA-70B, GPT-2 XL) with varying batch sizes to verify scalability of the optimization.
2. Evaluate compatibility with torch.compile and other PyTorch optimization passes to ensure the custom layers don't break standard training workflows.
3. Measure memory impact when combining selective differentiation with other memory optimization techniques like gradient checkpointing or model parallelism.