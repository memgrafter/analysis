---
ver: rpa2
title: Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language
  Models
arxiv_id: '2402.14714'
source_url: https://arxiv.org/abs/2402.14714
tags:
- tokens
- korean
- arxiv
- language
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EEVE-Korean-v1.0, a Korean adaptation of
  large language models (LLMs) using an efficient and effective vocabulary expansion
  (EEVE) method. The EEVE method combines parameter freezing and subword-based embedding
  initialization to integrate new linguistic tokens from languages beyond the initial
  English-centric training scope.
---

# Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models

## Quick Facts
- arXiv ID: 2402.14714
- Source URL: https://arxiv.org/abs/2402.14714
- Reference count: 7
- Primary result: Korean adaptation of LLMs using EEVE method outperforms other Korean LLMs with only 2B training tokens

## Executive Summary
This paper introduces EEVE-Korean-v1.0, a Korean adaptation of large language models using an efficient vocabulary expansion method. The approach combines parameter freezing with subword-based embedding initialization to integrate new linguistic tokens from languages beyond the initial English-centric training scope. The method uses a seven-stage training process, focusing initially on input embeddings and progressively expanding to full parameters. This approach allows for significantly improved Korean text processing with only 2 billion training tokens, compared to the trillions required by previous methods.

## Method Summary
The EEVE method expands vocabulary by adding new tokens to an existing English-centric tokenizer, then training through seven progressive stages. New input embeddings are initialized as averages of their constituent subword embeddings, while output embeddings use the first subword token's embedding. Training begins with only input embeddings trainable, then gradually expands to output embeddings, then all embeddings, and finally all layers. This selective parameter freezing approach preserves existing capabilities while efficiently learning new tokens.

## Key Results
- EEVE-Korean-10.8B-v1.0 and EEVE-Korean-2.8B-v1.0 outperform other pre-trained Korean LLMs on the Open Ko-LLM Leaderboard
- Models maintain strong English capabilities while significantly improving Korean text processing
- Achieved results using only 2 billion training tokens versus trillions required by previous methods
- Public release of models supports research in various languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage training with selective parameter freezing allows efficient vocabulary expansion by leveraging pre-trained model knowledge.
- Mechanism: The model starts with only input embeddings trainable, then gradually expands to output embeddings, then all embeddings, and finally all layers. Each stage builds on the previous one's alignment between input and output representations.
- Core assumption: The pre-trained model already has strong language understanding and reasoning capabilities that can be transferred to new tokens without full retraining.
- Evidence anchors: [abstract] "we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization"; [section 2.3] "Our core assumption is that foundational models, having been extensively trained in English texts, possess a substantial level of understanding and reasoning capabilities"
- Break condition: If the pre-trained model lacks sufficient understanding of subword relationships, the initialization may not provide meaningful starting points for new tokens.

### Mechanism 2
- Claim: Subword-based embedding initialization provides semantically meaningful starting points for new tokens.
- Mechanism: New input embeddings are initialized as the average of their constituent subword embeddings, while new output embeddings are initialized with the first subword token's embedding. This creates semantic continuity between old and new vocabulary.
- Core assumption: Subword embeddings capture meaningful semantic information that can be aggregated to represent whole words.
- Evidence anchors: [section 2.2] "For the input embeddings of the newly added tokens, we adopt the approach of using the average embeddings of the subword tokens that make up these new tokens"; [section 2.2] "Conversely, the output embeddings for the newly added tokens are initialized with the embeddings of the first subword token"
- Break condition: If subword segmentation is poor or if the language has many words not decomposable into meaningful subwords, the initialization may be semantically incorrect.

### Mechanism 3
- Claim: Using distinct tokenizers for input and output during Stage 1 enables more efficient learning of new vocabulary embeddings.
- Mechanism: By using the old tokenizer for output while learning new input embeddings, the model can leverage existing embedding spaces to align representations for generating new tokens.
- Core assumption: The model can learn to map between old subword tokens and new whole-word tokens when trained to predict subwords given whole words.
- Evidence anchors: [section 2.3] "our principal hypothesis here is that if the input and output token sequences in causal language modeling can be differentiated, by utilizing both the old and new tokenizers at the same time, the model can more efficiently and effectively learn new vocabulary embeddings"; [section 2.3] "Here, the subword-based embedding initialization provides a proxy for using the old tokenizer for output sequences"
- Break condition: If the alignment between subwords and whole words is too complex, the model may fail to learn the mapping effectively.

## Foundational Learning

- Concept: Subword tokenization and its relationship to vocabulary efficiency
  - Why needed here: Understanding how subword tokenization reduces token count and computational cost is crucial for appreciating the vocabulary expansion approach
  - Quick check question: Why does Korean text typically require more tokens than English text for equivalent semantic content?

- Concept: Parameter freezing and its impact on training efficiency
  - Why needed here: The multi-stage training approach relies heavily on selectively freezing parameters to balance learning new tokens while preserving existing capabilities
  - Quick check question: What is the computational advantage of freezing parameters during certain training stages?

- Concept: Embedding initialization strategies and their effect on convergence
  - Why needed here: The subword-based initialization is critical for providing good starting points that accelerate learning and improve final performance
  - Quick check question: How does initializing new embeddings as averages of subword embeddings differ from random initialization?

## Architecture Onboarding

- Component map: embed_tokens (input embeddings) -> transformer layers (encoder/decoder blocks) -> lm_head (output embeddings)
- Critical path: Stage 1 (input embeddings) → Stage 2 (output embeddings) → Stage 3 (both embeddings) → Stage 4 (all output embeddings) → Stage 5 (new input and all output) → Stage 6 (all layers) → Stage 7 (internal layers)
- Design tradeoffs: Multi-stage training trades off training time for better alignment and preservation of existing capabilities. The approach is slower than full fine-tuning but requires far fewer tokens (2B vs trillions).
- Failure signatures: Poor performance on new tokens, degradation of original language capabilities, training instability during later stages, or convergence issues in early stages.
- First 3 experiments:
  1. Implement Stage 1 only with input embeddings trainable - verify new tokens can be processed and check loss reduction
  2. Add Stage 2 with output embeddings - test generation of new tokens and measure alignment with input embeddings
  3. Run full pipeline through Stage 3 - evaluate both understanding and generation of new tokens while monitoring English capability preservation

Assumption: The seven-stage training