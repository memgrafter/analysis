---
ver: rpa2
title: Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom
  dataset and mBART
arxiv_id: '2403.00212'
source_url: https://arxiv.org/abs/2403.00212
tags:
- audio
- voice
- speech
- hindi
- wav2vec2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of training personalized Automatic
  Speech Recognition (ASR) models for low-resource languages like Hindi with minimal
  data. Using just 14 minutes of custom audio from a YouTube video, the researchers
  employed Retrieval-Based Voice Conversion (RVC) to create a personalized Common
  Voice 16.0 corpus.
---

# Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART

## Quick Facts
- arXiv ID: 2403.00212
- Source URL: https://arxiv.org/abs/2403.00212
- Reference count: 16
- Key outcome: Fine-tuned XLSR Wav2Vec2 achieves WER of 0.53 on 14 minutes of custom Hindi audio

## Executive Summary
This study demonstrates that personalized Automatic Speech Recognition (ASR) for low-resource languages like Hindi can be achieved with minimal data. Using just 14 minutes of custom audio, the researchers employed Retrieval-Based Voice Conversion (RVC) to create a speaker-matched training corpus from Common Voice 16.0. A fine-tuned XLSR Wav2Vec2 model was then used for transcription, followed by mBART for Hindi-to-English translation. The system successfully generates English subtitles for Hindi videos through a web-based GUI, providing an accessible solution for multilingual video content transcription and translation.

## Method Summary
The researchers used 14 minutes of custom audio from a YouTube video to create a personalized Common Voice 16.0 corpus using RVC. They then fine-tuned the XLSR Wav2Vec2 model on this dataset for 40 epochs with a learning rate of 1×10^-4 and weight decay of 2.5×10^-6. The fine-tuned model was integrated with mBART for translation, and speaker diarization was implemented using Pyannote. A web-based GUI was developed using Gradio to enable user interaction with the transcription and translation pipeline.

## Key Results
- XLSR Wav2Vec2 achieved a training accuracy of 0.80 and WER of 0.53 on the personalized dataset
- The system successfully generates English subtitles for Hindi videos
- The approach demonstrates feasibility of personalized ASR with minimal data for low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning XLSR Wav2Vec2 on a small, personalized dataset yields usable ASR performance for low-resource languages. The pre-trained model's cross-lingual representations from 56,000 hours of audio across 50 languages generalize well to Hindi and the target speaker's voice, requiring only 14 minutes of targeted data for meaningful adaptation.

### Mechanism 2
RVC enables creation of a custom, speaker-matched training corpus from a short target audio sample. The model learns to map generic Hindi corpus audio to the target speaker's voice characteristics, allowing fine-tuning of the ASR model on personalized data without requiring hours of manual recording.

### Mechanism 3
mBART effectively translates Hindi transcriptions to English subtitles, enabling multilingual video accessibility. Its pre-training on 25 languages and denoising autoencoding objective allows it to learn robust multilingual representations that can translate Hindi ASR output to coherent English subtitles.

## Foundational Learning

- Self-supervised speech representation learning (e.g., Wav2Vec 2.0)
  - Why needed: Enables XLSR Wav2Vec2 to learn from unlabeled audio and leverage large cross-lingual pre-training corpus
  - Quick check: What is the core pre-training objective used by Wav2Vec 2.0 to learn speech representations without labels?

- Voice conversion and speaker adaptation
  - Why needed: RVC transforms generic Hindi speech data into the target speaker's voice, creating personalized dataset without extensive manual recording
  - Quick check: How does RVC differ from traditional text-to-speech in terms of input and output?

- End-to-end multilingual translation with transformer models
  - Why needed: mBART's pre-trained multilingual representations enable direct Hindi-to-English translation without separate transliteration or alignment steps
  - Quick check: What is the advantage of denoising autoencoding pre-training for multilingual translation?

## Architecture Onboarding

- Component map: Input Hindi video → Audio extraction → MP3 → Speaker diarization (Pyannote) → (start, end, speaker) segments → XLSR Wav2Vec2 fine-tuned on RVC-generated Hindi corpus → Hindi text → mBART fine-tuned on Hindi-English pairs → English subtitles → VTT generation → Overlay on video → Output: Hindi video with English subtitles

- Critical path: Audio extraction → Diarization → XLSR transcription → mBART translation → VTT generation → Video overlay

- Design tradeoffs:
  - Using RVC trades data quantity for synthesis quality; poor RVC output degrades ASR performance
  - Fine-tuning XLSR on a tiny dataset risks overfitting (observed WER 0.53, training accuracy 0.80)
  - mBART translation is fast but sensitive to ASR transcription errors

- Failure signatures:
  - High WER after fine-tuning → RVC data or XLSR fine-tuning issues
  - Garbled subtitles → mBART translation errors or poor ASR input
  - Misaligned subtitles → Diarization or VTT generation bugs

- First 3 experiments:
  1. Validate RVC synthesis: Input generic Hindi audio, check if output resembles target speaker and remains intelligible
  2. Test XLSR fine-tuning: Run on a small subset of the RVC-generated corpus, monitor training loss and WER on a held-out validation set
  3. End-to-end pipeline sanity check: Process a short Hindi video through all components, inspect intermediate outputs (diarization segments, Hindi text, English translation) before generating final subtitles

## Open Questions the Paper Calls Out

### Open Question 1
How does the accuracy of the XLSR Wav2Vec2 model trained on 14 minutes of personalized data compare to models trained on larger, more diverse datasets like LibriSpeech for low-resource languages?

### Open Question 2
What is the optimal number of epochs for training the RVC model to achieve the best balance between voice conversion quality and training efficiency?

### Open Question 3
How do the parameter settings for the RVC model, such as volume envelope scaling, filter radius, and search feature ratios, affect the quality of voice conversion for different types of audio inputs?

## Limitations
- Extremely small training dataset (14 minutes) raises concerns about overfitting and generalization
- RVC-based data augmentation lacks independent validation of synthetic vs real speech quality
- Translation quality assessment is limited to indirect evaluation through end-to-end pipeline

## Confidence
- High Confidence: General feasibility of using pre-trained XLSR Wav2Vec2 for Hindi ASR through fine-tuning
- Medium Confidence: Effectiveness of RVC for creating speaker-matched training data
- Medium Confidence: Overall end-to-end system functionality
- Low Confidence: Translation quality assessment

## Next Checks
1. Test the fine-tuned XLSR model on a held-out Hindi speech dataset not used in training or RVC generation to assess true generalization capability
2. Conduct a human evaluation comparing RVC-generated audio against real recordings of the target speaker for intelligibility and speaker similarity
3. Evaluate mBART's translation output independently using standard machine translation metrics (BLEU, TER) on reference Hindi-English parallel corpora