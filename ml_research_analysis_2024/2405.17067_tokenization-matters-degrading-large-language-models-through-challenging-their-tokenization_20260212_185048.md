---
ver: rpa2
title: Tokenization Matters! Degrading Large Language Models through Challenging Their
  Tokenization
arxiv_id: '2405.17067'
source_url: https://arxiv.org/abs/2405.17067
tags:
- tokenization
- llms
- response
- word
- wrong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how tokenization errors in large language
  models (LLMs) can degrade their performance. The authors construct an adversarial
  dataset, ADT, with manually and automatically generated examples designed to challenge
  LLMs' tokenization abilities.
---

# Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization

## Quick Facts
- arXiv ID: 2405.17067
- Source URL: https://arxiv.org/abs/2405.17067
- Reference count: 40
- Key outcome: Tokenization errors significantly degrade LLM performance, with Chinese data showing error rates often exceeding 90% on open-source models.

## Executive Summary
This paper investigates how tokenization errors in large language models can severely impact their performance. The authors construct an adversarial dataset called ADT with examples specifically designed to challenge LLMs' tokenization abilities. They demonstrate that incorrect tokenization directly causes incorrect responses, with particularly high error rates observed for Chinese text. The paper introduces both manual and automatic methods for generating challenging examples, showing that while automatic generation is efficient, manual curation produces more effective test cases. The findings highlight tokenization as a critical bottleneck in LLM performance and call for improved tokenizer robustness.

## Method Summary
The authors construct an adversarial dataset ADT with two subsets: ADT-Human (manually curated) and ADT-Auto (automatically generated). They export vocabularies from various open-source LLMs to identify "trap words" that can influence tokenization. For ADT-Human, they manually insert special character spans into tokens to create ambiguous tokenization cases. For ADT-Auto, they use GPT-4 to generate sentences containing these ambiguous patterns, then filter based on whether trap words appear in both sentence and response tokenizations. The dataset is evaluated across multiple open-source and closed-source LLMs to measure error rates when tokenization fails.

## Key Results
- Incorrect tokenization directly causes incorrect LLM responses, with Chinese data showing error rates often exceeding 90% on open-source models
- ADT-Human dataset poses significant challenges to both open-source and closed-source LLMs, demonstrating that tokenization issues are universal across mainstream models
- Automatic generation framework (ADT-Auto) is efficient but less effective than manual curation, producing more formal and regular syntax that's easier for models to handle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorrect tokenization directly causes incorrect responses in LLMs.
- Mechanism: When the tokenizer splits input text incorrectly, the model receives a corrupted sequence of tokens that no longer matches the intended meaning. The model then generates output based on this corrupted input, leading to nonsensical or wrong answers.
- Core assumption: The LLM's understanding is entirely dependent on its tokenization output.
- Evidence anchors:
  - [abstract] "incorrect tokenization is the critical point that hinders LLMs in understanding the input precisely, thus leading to unsatisfactory output"
  - [section 1] "our empirical studies found that this flaw not only exists in some specific LLMs, but also is a universal issue across many mainstream LLMs"
- Break condition: If the LLM has strong context understanding that can overcome tokenization errors, or if post-tokenization processing can correct errors.

### Mechanism 2
- Claim: ADT dataset construction effectively challenges LLM tokenization by using vocabulary-specific trap words.
- Mechanism: The dataset uses words from the LLM's own vocabulary but inserts characters that create ambiguous tokenization boundaries. When the LLM encounters these ambiguous cases, it often tokenizes incorrectly, leading to wrong responses.
- Core assumption: LLMs cannot resolve ambiguous tokenization cases using context alone.
- Evidence anchors:
  - [section 3.2] "We adopt one of the three approaches... to convert it into a challenging span through inserting a special character span before or (and) after it"
  - [section 4.2] "ADT-Human poses a significant challenge to both open-source and closed-source LLMs on their tokenization, resulting in very high rates of inaccurate responses"
- Break condition: If LLMs develop better context-aware tokenization or if tokenizers are improved to handle ambiguous cases.

### Mechanism 3
- Claim: Automatic dataset generation using GPT-4 can efficiently create challenging tokenization examples.
- Mechanism: The framework exports vocabulary, identifies trap words, uses GPT-4 to generate sentences containing ambiguous tokenization cases, then filters based on whether trap words appear in both sentence and response tokenizations.
- Core assumption: GPT-4 can generate linguistically coherent sentences that contain the desired ambiguous tokenization patterns.
- Evidence anchors:
  - [section 3.3] "we harness GPT-4 to generate an instance of ADT-Auto... The generated sentence is required to include the concatenation of Word 1 and Word 2"
  - [section 4.3] "ADT-Auto is less challenging to LLMs, since the sentences generated by GPT-4 have more formal, regular or simple syntaxes"
- Break condition: If GPT-4's generated sentences become too predictable or if the filtering criteria miss challenging cases.

## Foundational Learning

- Concept: Subword tokenization algorithms (BPE, WordPiece, Unigram)
  - Why needed here: Understanding how different tokenization algorithms work is crucial to designing effective adversarial examples
  - Quick check question: What's the key difference between BPE and WordPiece in how they merge token pairs?

- Concept: Token vocabulary and its relationship to model performance
  - Why needed here: The paper shows that tokens from an LLM's vocabulary can still cause problems, so understanding vocabulary structure is important
  - Quick check question: Why does the paper focus on Chinese tokenization more than English?

- Concept: Adversarial dataset construction methodology
  - Why needed here: The paper's main contribution is creating a systematic way to test tokenization weaknesses
  - Quick check question: What are the three approaches used to create challenging spans in ADT-Human?

## Architecture Onboarding

- Component map: Tokenizer → LLM → Response Generator
- Critical path: Input text → Tokenizer → Token sequence → LLM processing → Output generation. The bottleneck is the tokenizer's ability to correctly split input text.
- Design tradeoffs: Fixed vocabulary vs. dynamic tokenization, subword units vs. character-level processing, efficiency vs. accuracy
- Failure signatures: High error rates on ADT dataset, presence of trap words in tokenization lists, mismatch between human and LLM tokenization
- First 3 experiments:
  1. Test a simple example from ADT-Human on an LLM and verify incorrect tokenization leads to wrong response
  2. Compare tokenization results between human and LLM for a challenging span example
  3. Run ADT-Auto generation framework on a small vocabulary to understand the automatic process

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do tokenization errors in LLMs affect the models' performance on downstream tasks beyond simple question answering, such as reasoning, translation, or code generation?
- Basis in paper: [inferred] The paper demonstrates that tokenization errors lead to incorrect responses in simple question-answering tasks. It is reasonable to assume that these errors could propagate and impact more complex tasks.
- Why unresolved: The paper focuses on simple question-answering tasks to illustrate the impact of tokenization errors. The effect on downstream tasks is not explicitly investigated.
- What evidence would resolve it: Empirical studies comparing LLM performance on various downstream tasks with and without tokenization errors would provide insights into the broader impact of tokenization on model capabilities.

### Open Question 2
- Question: What are the specific characteristics of Chinese language that make it more susceptible to tokenization errors compared to English, and how can these challenges be addressed?
- Basis in paper: [explicit] The paper explicitly states that tokenization errors are more obvious in Chinese scenarios and provides examples of challenging tokenization cases in Chinese.
- Why unresolved: While the paper identifies the issue, it does not delve into the linguistic reasons behind the increased vulnerability of Chinese to tokenization errors.
- What evidence would resolve it: Linguistic analysis of Chinese and English tokenization, focusing on differences in word structure, morphology, and the use of spaces, could shed light on the specific challenges faced by Chinese tokenization.

### Open Question 3
- Question: How can the automatic generation framework for challenging tokenization examples be improved to create more diverse and effective adversarial datasets?
- Basis in paper: [explicit] The paper presents an automatic generation framework for creating challenging tokenization examples but acknowledges that the manually constructed dataset is more effective.
- Why unresolved: The paper does not explore ways to enhance the automatic generation framework to match the quality and effectiveness of manually constructed examples.
- What evidence would resolve it: Comparative studies evaluating the effectiveness of automatically generated examples against manually constructed ones, along with analysis of the limitations of the current framework, would guide improvements.

## Limitations

- Dataset Generality: The evaluation focuses heavily on Chinese language, with uncertain generalization to other languages with different tokenization characteristics
- Model Coverage: Limited sample size and diversity of model architectures tested, with insufficient systematic exploration of different tokenization algorithms
- Root Cause Attribution: Correlation between tokenization errors and wrong responses established, but causation not definitively proven

## Confidence

- High Confidence: The core finding that tokenization errors can degrade LLM performance is well-supported by empirical evidence
- Medium Confidence: The claim that this is a "universal issue across many mainstream LLMs" is supported but could benefit from broader testing
- Low Confidence: The paper's assertions about specific failure mechanisms at the attention and positional encoding level are largely theoretical

## Next Checks

1. **Cross-Lingual Validation**: Test the ADT methodology on multiple languages with different tokenization characteristics (e.g., agglutinative languages like Turkish, logographic languages like Japanese) to assess whether the findings generalize beyond Chinese.

2. **Model Architecture Analysis**: Systematically compare how different tokenization algorithms (BPE vs WordPiece vs Unigram) and model architectures respond to the same adversarial examples, isolating whether the issue stems from tokenization methodology or model training.

3. **Error Propagation Analysis**: Design experiments to trace exactly how tokenization errors propagate through the model layers to affect final outputs, using techniques like attention visualization and intermediate layer analysis to validate the proposed mechanism.