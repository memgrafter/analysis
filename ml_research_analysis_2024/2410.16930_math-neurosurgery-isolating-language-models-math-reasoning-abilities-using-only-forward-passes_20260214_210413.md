---
ver: rpa2
title: 'Math Neurosurgery: Isolating Language Models'' Math Reasoning Abilities Using
  Only Forward Passes'
arxiv_id: '2410.16930'
source_url: https://arxiv.org/abs/2410.16930
tags:
- parameters
- mmlu
- race
- math
- mathneuro
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Math Neurosurgery (MathNeuro), a method to
  isolate math-specific parameters in Large Language Models (LLMs) using only forward
  passes. The method builds on existing work by using weights and activations to calculate
  parameter importance, but uniquely filters out parameters important for general
  language tasks to focus on math-specific ones.
---

# Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes

## Quick Facts
- arXiv ID: 2410.16930
- Source URL: https://arxiv.org/abs/2410.16930
- Authors: Bryan R. Christ; Zack Gottesman; Jonathan Kropko; Thomas Hartvigsen
- Reference count: 40
- Key outcome: Method to isolate math-specific parameters in LLMs using only forward passes, improving math performance by 4-17% (GSM8K) and 5-35% (MATH) while preserving general language ability

## Executive Summary
Math Neurosurgery (MathNeuro) is a novel method for isolating math-specific parameters in Large Language Models using only forward passes, without requiring gradient computation or fine-tuning. The approach calculates parameter importance using weight×activation products, then filters out parameters important for general language tasks to focus on those specific to mathematical reasoning. Through experiments on five LLMs ranging from 1B to 8B parameters, the authors demonstrate that MathNeuro effectively isolates math-specific parameters that, when pruned, eliminate math reasoning ability while preserving general language performance. When these parameters are scaled by small constants, GSM8K and MATH performance improves significantly without altering non-math behavior, and the method maintains effectiveness even when identifying parameters from a single sample.

## Method Summary
MathNeuro builds on existing parameter importance estimation techniques by calculating the product of weights and activations for each parameter during forward passes. The method computes parameter importance separately for math and non-math samples, identifies the top-K% most important parameters for each task, and takes the set difference to isolate math-specific parameters. The authors evaluate their approach through two complementary experiments: pruning identified parameters (setting them to zero) to test if math performance degrades while preserving non-math performance, and scaling identified parameters by small constants (1.1 for smaller models, 1.01 for larger ones) to improve math reasoning without affecting general language tasks. The approach is data-efficient, maintaining effectiveness when identifying math-specific parameters using only a single sample.

## Key Results
- Pruning math-specific parameters eliminated GSM8K performance while preserving RACE and MMLU accuracy across five models (1B-8B parameters)
- Scaling math-specific parameters improved GSM8K accuracy by 4-17% and MATH performance by 5-35% without degrading non-math tasks
- Method maintained effectiveness when identifying math-specific parameters from just a single sample
- MathNeuro outperformed baseline methods (Wanda, LAPE, random) in both pruning and scaling experiments

## Why This Works (Mechanism)
The method works by exploiting the differential importance of parameters across task types. Mathematical reasoning in LLMs requires specific parameter configurations that differ from those used for general language understanding. By computing |W_ij| x ||X_j||_2 for each parameter during forward passes and comparing importance across math and non-math samples, MathNeuro identifies parameters that are disproportionately critical for mathematical reasoning. The set difference operation effectively filters out parameters that serve dual purposes, isolating those truly specific to math. This isolation enables targeted manipulation—pruning eliminates math reasoning by removing critical math-specific parameters, while scaling enhances it by amplifying the influence of parameters most important for mathematical tasks.

## Foundational Learning
- **Parameter importance estimation**: Computing |W_ij| x ||X_j||_2 to measure each parameter's contribution during forward passes. Needed to identify which parameters are most influential for specific tasks without requiring backward passes or gradients. Quick check: verify importance scores correlate with ablation impact on task performance.
- **Task-specific parameter isolation**: Using set difference operations on top-K% important parameters to identify parameters unique to math reasoning. Needed to separate parameters serving general language from those specific to mathematical reasoning. Quick check: confirm isolated parameters show high importance for math but low importance for non-math tasks.
- **Forward-pass only computation**: Leveraging activations and weights during inference rather than requiring gradient computation. Needed to make the method computationally efficient and applicable to frozen models. Quick check: ensure computational complexity scales linearly with model size and sequence length.
- **Top-K% selection**: Choosing the most important parameters for each task based on a percentage threshold. Needed to balance between capturing all relevant parameters and avoiding noise from less important ones. Quick check: sweep K values to find optimal trade-off between coverage and precision.

## Architecture Onboarding

**Component Map**: Forward pass → Weight×Activation computation → Per-parameter importance scoring → Top-K% selection per task → Set difference → Isolated math-specific parameters

**Critical Path**: The core computational path involves processing each sample through the model to obtain activations, computing importance scores using stored weights and activations, aggregating importance across samples for each task, selecting top parameters per task, and computing the set difference to isolate math-specific parameters.

**Design Tradeoffs**: The method trades precision for efficiency by using only forward passes and simple importance metrics rather than more sophisticated techniques like integrated gradients or attention-based methods. This makes it computationally lightweight but may miss nuanced parameter interactions. The choice of K% involves balancing between capturing all relevant math-specific parameters versus including non-math parameters that happen to be important for some math samples.

**Failure Signatures**: 
- Poor math performance after pruning despite high confidence in parameter isolation indicates either incorrect importance scoring or insufficient K% coverage
- Significant non-math performance degradation after pruning or scaling suggests contamination from non-math parameters in the isolated set
- Minimal math performance improvement after scaling indicates either incorrect parameter isolation or insufficient scaling factor

**3 First Experiments**:
1. Compute parameter importance scores for a small subset of math and non-math samples, visualize distributions, and verify math-specific parameters have higher importance for math tasks
2. Apply MathNeuro to a toy model with known math/non-math parameter separation to validate the isolation process
3. Perform ablation study by pruning small subsets of identified parameters and measuring incremental impact on math vs non-math performance

## Open Questions the Paper Calls Out
None

## Limitations
- The scaling factor selection (1.1 vs 1.01) appears empirically derived without clear theoretical justification for why smaller models tolerate larger perturbations
- Computational overhead of parameter importance calculation across full datasets may limit practical applicability for very large models, though single-sample effectiveness partially mitigates this
- The core assumption that |W_ij| x ||X_j||_2 reliably isolates math-specific parameters across diverse model architectures remains empirically validated but theoretically underexplored

## Confidence
**Parameter isolation effectiveness (High)**: Pruning experiments consistently show GSM8K performance degradation to near-zero while preserving RACE/MMLU performance across five models, providing strong empirical validation.

**Performance improvement via scaling (Medium)**: The 4-17% GSM8K and 5-35% MATH improvements are demonstrated, but the choice of scaling factors appears somewhat arbitrary, and results may be sensitive to hyperparameter selection.

**Data efficiency (Medium)**: Single-sample identification maintaining most effectiveness is compelling, but the paper doesn't extensively explore sample size trade-offs or characterize when performance begins degrading significantly.

## Next Checks
1. **Scaling factor sensitivity**: Systematically sweep scaling factors (0.9, 1.0, 1.01, 1.05, 1.1, 1.2) on each model to determine optimal values and characterize performance curves, particularly examining if gains plateau or reverse at higher values.

2. **Cross-architecture generalization**: Apply MathNeuro to transformer architectures not represented in the original study (e.g., OPT, Mistral) to test whether the parameter importance estimation generalizes beyond the tested family of models.

3. **Long-context behavior**: Evaluate whether scaled parameters maintain math reasoning advantages in long-context scenarios (e.g., GSM8K problems requiring multi-step reasoning across extended contexts) to test if improvements are superficial or represent genuine capability enhancement.