---
ver: rpa2
title: 'Symbotunes: unified hub for symbolic music generative models'
arxiv_id: '2410.20515'
source_url: https://arxiv.org/abs/2410.20515
tags:
- music
- symbotunes
- training
- symbolic
- file
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Symbotunes, an open-source Python framework
  designed to address compatibility and reproducibility challenges in symbolic music
  generative models. The framework re-implements foundational models like Folk-RNN,
  MusicVAE, and ABC GPT2 using modern tools (PyTorch 2.2, PyTorch Lightning 2.2, MidiTok
  3.0) to ensure compatibility with current machine learning practices.
---

# Symbotunes: unified hub for symbolic music generative models

## Quick Facts
- arXiv ID: 2410.20515
- Source URL: https://arxiv.org/abs/2410.20515
- Authors: Paweł Skierś; Maksymilian Łazarski; Michał Kopeć; Mateusz Modrzejewski
- Reference count: 0
- One-line primary result: A unified, open-source Python framework for symbolic music generative models using modern ML tools

## Executive Summary
Symbotunes addresses compatibility and reproducibility challenges in symbolic music generative models by providing a unified framework that re-implements foundational models like Folk-RNN, MusicVAE, and ABC GPT2 using modern tools (PyTorch 2.2, PyTorch Lightning 2.2, MidiTok 3.0). The framework offers a standardized pipeline for training and generating music, supports multiple datasets, and provides customizable configuration files for flexible experimentation. Designed for both researchers and educators, Symbotunes facilitates exploration, adaptation, and further development of symbolic music models through a GPL-licensed open-source platform.

## Method Summary
Symbotunes re-implements foundational symbolic music models using modern deep learning frameworks to ensure compatibility with current machine learning practices. The framework uses a YAML-based configuration system that allows users to specify model types, hyperparameters, datasets, data transforms, and training parameters in a single file. All datasets inherit from a common BaseDataset interface, and logging is handled via Weights and Biases for standardized experiment tracking. The implementation uses PyTorch 2.2, PyTorch Lightning 2.2, and MidiTok 3.0, with a unified pipeline accessible through Python scripts for training and sampling.

## Key Results
- Re-implementation of Folk-RNN, MusicVAE, and ABC GPT2 models using modern PyTorch and PyTorch Lightning frameworks
- Unified YAML configuration system enabling flexible experimentation across different models and datasets
- Standardized data handling through BaseDataset interface and centralized logging via Weights and Biases
- Support for LAKH MIDI dataset and Folk RNN ABC dataset with corresponding tokenizers and transforms

## Why This Works (Mechanism)

### Mechanism 1
Re-implementing foundational symbolic music models in modern frameworks reduces compatibility barriers for new researchers. Legacy models like Folk-RNN and MusicVAE were built using obsolete frameworks (e.g., Theano), making them difficult to run or extend with current tools. Symbotunes re-implements these models using PyTorch 2.2, PyTorch Lightning 2.2, and MidiTok 3.0, aligning them with modern machine learning practices and tooling.

### Mechanism 2
A unified configuration system streamlines experimentation and model comparison. Symbotunes uses a YAML-based configuration parser that allows users to specify model types, hyperparameters, datasets, data transforms, and training parameters in a single file. This eliminates the need to modify multiple scripts or manually align disparate project structures across models.

### Mechanism 3
Standardized data handling and logging improve reproducibility and experiment tracking. All datasets inherit from a common BaseDataset interface, and logging is handled via Weights and Biases. This standardization ensures consistent data preprocessing and centralized experiment monitoring across all models.

## Foundational Learning

- Concept: Symbolic music representation formats (e.g., ABC notation, MIDI)
  - Why needed here: Symbotunes works with ABC and MIDI formats, so understanding these representations is essential for dataset preparation and model input/output handling
  - Quick check question: What are the key differences between ABC notation and MIDI in terms of structure and use cases?

- Concept: Deep learning model architectures (LSTM, VAE, Transformer)
  - Why needed here: The framework implements models based on LSTM (Folk-RNN), VAE (MusicVAE), and Transformer (ABC GPT2) architectures, so familiarity with these is required to modify or extend them
  - Quick check question: How does a variational autoencoder differ from a standard autoencoder in its objective and output?

- Concept: Data tokenization and preprocessing for sequence models
  - Why needed here: Symbolic music models require tokenization of sequences (e.g., ABC tokens, MIDI events) before feeding them into neural networks, and Symbotunes provides tokenizers for this purpose
  - Quick check question: Why is tokenization necessary for training sequence models on symbolic music data?

## Architecture Onboarding

- Component map:
  scripts/ -> entry points for training (train.py) and sampling (sample.py)
  models/ -> implementations of symbolic music models (Folk-RNN, MusicVAE, ABC GPT2)
  data/ -> dataset loaders, tokenizers, and data transforms
  callbacks/ -> training callbacks (checkpointing, logging, CUDA monitoring)
  config system -> YAML-based configuration for experiments

- Critical path:
  1. Define experiment in YAML config (model, dataset, hyperparameters)
  2. Run train.py with config path to start training
  3. Use sample.py with trained checkpoint to generate music

- Design tradeoffs:
  - Using YAML for config offers readability but may lack type safety compared to programmatic config builders
  - Abstracting datasets via BaseDataset simplifies interface but may limit dataset-specific optimizations
  - GPL licensing encourages open collaboration but may restrict commercial adoption

- Failure signatures:
  - Training fails with "module not found" → dependency mismatch in conda environment
  - Model produces invalid ABC/MIDI → tokenizer or data transform bug
  - Checkpoints not loading → incompatible model architecture or PyTorch version

- First 3 experiments:
  1. Train Folk-RNN on Folk RNN dataset with default config; verify output ABC validity
  2. Sample from pre-trained MusicVAE checkpoint; compare generated sequences to training data
  3. Modify MusicVAE config to change latent dimension; observe impact on generated music diversity

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Symbotunes' re-implemented models compare to the original implementations, particularly in terms of musical quality and generation capabilities? The paper states that Symbotunes re-implements foundational models using modern tools, but does not provide direct performance comparisons with original implementations.

### Open Question 2
What are the specific advantages and limitations of using Symbotunes for educational purposes compared to learning from individual model implementations? The paper mentions that Symbotunes is "designed for both researchers and educators" but doesn't elaborate on educational benefits or limitations.

### Open Question 3
How scalable is Symbotunes when implementing additional models, datasets, and tokenizers, and what are the technical challenges involved? The paper mentions future plans to add more models, datasets, and tokenizers, but doesn't discuss scalability challenges or technical limitations.

## Limitations

- No quantitative comparisons between re-implemented models and their original versions to validate functional equivalence
- Limited to only three models and two datasets, with unclear scalability for future additions
- No discussion of computational requirements or expected training times for different configurations

## Confidence

- Confidence in technical implementation details (High): The use of PyTorch 2.2, PyTorch Lightning 2.2, and MidiTok 3.0 is explicitly stated and verifiable
- Confidence in claimed benefits (Medium): The mechanisms for compatibility and unification are sound, but empirical validation of model behavior parity is missing
- Confidence in educational utility claims (Low): The paper states educational benefits but provides no evidence or case studies demonstrating learning outcomes

## Next Checks

1. Verify that the re-implemented Folk-RNN, MusicVAE, and ABC GPT2 models produce outputs statistically similar to their original versions using the same datasets
2. Test the YAML configuration system with edge cases (missing fields, invalid types) to assess robustness and error handling
3. Evaluate the BaseDataset abstraction by implementing a new dataset class to determine if the interface is sufficiently flexible without sacrificing functionality