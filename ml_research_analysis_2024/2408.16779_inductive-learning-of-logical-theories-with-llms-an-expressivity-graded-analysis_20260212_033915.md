---
ver: rpa2
title: 'Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded Analysis'
arxiv_id: '2408.16779'
source_url: https://arxiv.org/abs/2408.16779
tags:
- noise
- theory
- llms
- chain
- drdg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel methodology to evaluate inductive
  learning capabilities of large language models (LLMs) for logic theory generation.
  The approach combines LLM-generated theories with formal Prolog inference engine
  feedback through iterative refinement, enabling graded analysis across rule expressivity
  levels.
---

# Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded Analysis

## Quick Facts
- arXiv ID: 2408.16779
- Source URL: https://arxiv.org/abs/2408.16779
- Authors: João Pedro Gandarela; Danilo S. Carvalho; André Freitas
- Reference count: 12
- Primary result: LLMs achieve competitive F1 scores against state-of-the-art ILP system Popper, particularly at higher noise levels, but tracking long predicate chains proves more challenging than handling complex rule dependencies.

## Executive Summary
This paper introduces a novel methodology for evaluating inductive learning capabilities of large language models (LLMs) in logic theory generation. The approach combines LLM-generated theories with formal Prolog inference engine feedback through iterative refinement, enabling graded analysis across rule expressivity levels. Experiments with multiple LLM architectures (GPT-4o, GPT-3.5 Turbo, Llama3-8B, Mixtral-8x7B, Gemma-7B-IT) on synthetic datasets show that larger LLMs achieve competitive performance against established ILP systems while demonstrating significantly faster processing times. However, the study reveals that long predicate chains present a more significant challenge than rule complexity itself.

## Method Summary
The methodology employs an iterative refinement loop where LLMs generate Prolog theories from synthetic datasets with varying expressivity levels and noise parameters. The process alternates between LLM theory generation and Prolog interpreter evaluation, using misclassified examples to guide subsequent refinement prompts. The evaluation framework grades performance across seven rule expressivity categories (CHAIN, CHAIN REC, RDG, RDG REC, DRDG, DRDG REC, MIXED) at three noise levels (0.1, 0.2, 0.3), comparing results against the Popper ILP system baseline. Performance metrics include accuracy, precision, recall, F1 scores, and processing times.

## Key Results
- Larger LLMs (GPT-4o, GPT-3.5 Turbo) demonstrate superior noise resilience and consistent F1 scores across expressivity levels compared to smaller models
- LLMs achieve 3 orders of magnitude faster processing times than Popper while maintaining competitive accuracy
- Tracking long predicate chains proves more challenging than handling complex rule dependencies, representing a fundamental limitation in LLM reasoning capabilities
- Performance degradation occurs with additional refinement iterations, indicating potential overfitting or hallucination issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative refinement with formal feedback enables LLMs to progressively improve logic theories
- Mechanism: The approach alternates between LLM-generated theories and Prolog interpreter evaluation, using misclassified examples as targeted refinement prompts
- Core assumption: Each iteration provides actionable feedback that guides the LLM toward better theory generation
- Evidence anchors:
  - [section]: "The refinement aims to improve the theory's performance by addressing misclassifications and enhancing its predictive capabilities"
  - [abstract]: "combines LLM-generated theories with formal Prolog inference engine feedback through iterative refinement"
- Break condition: Performance plateaus or degrades when LLM starts hallucinating non-existent predicates or introducing logical inconsistencies

### Mechanism 2
- Claim: Graded expressivity analysis reveals specific LLM limitations in logic theory induction
- Mechanism: By varying rule dependency structures (CHAIN, RDG, DRDG, MIXED) and noise levels, the framework isolates which structural complexities challenge LLM reasoning
- Core assumption: Different rule structures impose distinct cognitive demands on LLMs that can be systematically measured
- Evidence anchors:
  - [abstract]: "analysis is complexity-graded w.r.t. rule dependency structure, allowing quantification of specific inference challenges on LLM performance"
  - [section]: "to quantify the extent in which LLMs can address ILP tasks, the evaluation is graded w.r.t. the expressivity of the target rule-sets"
- Break condition: Performance becomes too noisy to distinguish between structural complexity effects and random variation

### Mechanism 3
- Claim: Larger LLMs demonstrate superior noise resilience and consistent performance across expressivity levels
- Mechanism: Model capacity enables better semantic representation and context handling, allowing larger models to maintain theory quality despite data perturbations
- Core assumption: Model size correlates with the ability to maintain coherent logical reasoning under challenging conditions
- Evidence anchors:
  - [section]: "The larger scale models (GPT3.5, 4) demonstrate more resilience to noise and consistent F1 across the different categories"
  - [section]: "Inducing theories on long relation chains is the major obstacle for LLMs, rather than the expressivity level"
- Break condition: Performance gains from increased model size diminish or become outweighed by computational costs

## Foundational Learning

- Concept: Inductive Logic Programming (ILP) fundamentals
  - Why needed here: Understanding how ILP systems derive theories from examples is crucial for comparing LLM performance against established baselines
  - Quick check question: What are the three components required for an ILP system to generate a hypothesis (H)?

- Concept: Prolog syntax and semantics
  - Why needed here: The evaluation framework relies on Prolog interpreter feedback, requiring understanding of how logical rules are structured and evaluated
  - Quick check question: How does Prolog handle the completeness and consistency conditions when evaluating a generated theory?

- Concept: Rule dependency structures and expressivity classes
  - Why needed here: The graded analysis framework depends on understanding how different rule structures (CHAIN, RDG, DRDG, MIXED) affect learning complexity
  - Quick check question: What distinguishes a Rooted Directed Graph (RDG) from a Chain structure in terms of predicate dependencies?

## Architecture Onboarding

- Component map:
  Prompt Generator -> LLM Set -> Prolog Interpreter -> Evaluation Module -> Prompt Generator (iterative loop)

- Critical path: Prompt Generation → LLM Theory Generation → Prolog Validation → Evaluation → Refinement Prompt Generation (iterative loop)

- Design tradeoffs:
  - Model size vs. computational cost: Larger models perform better but require more resources
  - Iteration count vs. diminishing returns: More iterations don't guarantee better results
  - Expressivity level vs. noise tolerance: More complex structures are more sensitive to noise

- Failure signatures:
  - Syntactic errors: Invalid Prolog grammar that causes interpreter failures
  - Logical errors: Grammatically correct but semantically incorrect theories
  - Hallucination artifacts: Introduction of non-existent predicates in generated rules
  - Performance degradation: F1 scores decrease with additional iterations

- First 3 experiments:
  1. Run baseline test with CHAIN category and 0.1 noise level across all models to establish performance baseline
  2. Test RDG category with 0.3 noise level to identify model sensitivity to complex structures
  3. Run MIXED category with 0.2 noise level to evaluate recursive rule handling across models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs scale with increasing dataset size and complexity beyond the XS configuration used in this study?
- Basis in paper: [explicit] The paper mentions plans to use larger datasets as future work to test scalability
- Why unresolved: The current experiments only used datasets with 50-100 facts (XS size), limiting conclusions about performance on larger, more complex datasets
- What evidence would resolve it: Systematic experiments testing LLMs on datasets of increasing size and complexity (S, M, L configurations) while tracking performance metrics

### Open Question 2
- Question: What is the optimal iteration limit for the refinement process across different LLM architectures and expressivity levels?
- Basis in paper: [explicit] The paper notes that increasing iterations beyond 3 did not improve results monotonically, but used a fixed limit of 4
- Why unresolved: The study used a single iteration limit across all experiments without exploring optimal limits for different models or expressivity levels
- What evidence would resolve it: Comparative experiments testing different iteration limits (1-10) across models and expressivity levels to identify optimal stopping points

### Open Question 3
- Question: How do different prompting strategies and template designs affect the quality and validity of generated theories?
- Basis in paper: [explicit] The paper used minimal, objective prompt templates designed to reduce syntax errors, but acknowledges this as a parameter
- Why unresolved: Only one prompt template was tested across all models, without exploring alternative prompting strategies
- What evidence would resolve it: Systematic comparison of different prompt designs (chain-of-thought, step-by-step, few-shot) across models and expressivity levels

### Open Question 4
- Question: What specific architectural features enable larger LLMs to better handle long predicate chains compared to smaller models?
- Basis in paper: [inferred] The paper notes that tracking long predicate chains is more challenging than rule complexity, and larger models perform better
- Why unresolved: The study did not investigate the internal mechanisms behind the performance differences
- What evidence would resolve it: Architectural analysis comparing attention patterns, context window utilization, and internal representations across model sizes

### Open Question 5
- Question: How do domain-specific datasets affect the performance of LLMs in theory induction compared to synthetic datasets?
- Basis in paper: [explicit] The paper mentions plans to investigate domain-specific scenarios as future work
- Why unresolved: All experiments used synthetic datasets generated by RuDaS, without testing on real-world domain data
- What evidence would resolve it: Experiments comparing performance on synthetic vs. real-world datasets across different domains (biology, medicine, finance)

## Limitations

- The methodology faces fundamental limitations in tracking long predicate chains, which proves more challenging than handling complex rule dependencies
- Performance degradation with additional refinement iterations suggests potential overfitting or hallucination issues that limit practical applicability
- Evaluation is limited to synthetic datasets with controlled complexity, raising questions about generalization to real-world scenarios

## Confidence

- **High Confidence:** Model size correlation with performance (Larger models consistently outperform smaller ones across metrics)
- **Medium Confidence:** Iterative refinement effectiveness (Mixed results with performance sometimes degrading after iterations)
- **Low Confidence:** Generalization to real-world datasets (Evaluation limited to synthetic data with controlled complexity)

## Next Checks

1. Test the methodology on real-world ILP benchmarks to verify synthetic dataset performance translates to practical applications
2. Implement hallucination detection mechanisms to quantify the frequency and impact of non-existent predicate generation
3. Conduct ablation studies varying iteration counts and refinement strategies to optimize the refinement loop parameters