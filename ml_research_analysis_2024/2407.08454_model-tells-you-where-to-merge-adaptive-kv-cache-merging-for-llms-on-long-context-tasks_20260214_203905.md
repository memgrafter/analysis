---
ver: rpa2
title: 'Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context
  Tasks'
arxiv_id: '2407.08454'
source_url: https://arxiv.org/abs/2407.08454
tags:
- merging
- cache
- states
- similarity
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of efficiently serving large language
  models (LLMs) during autoregressive generation, specifically addressing the substantial
  memory consumption of the KV cache in long-context scenarios. The proposed solution,
  KVMerger, is a novel KV cache merging approach that achieves adaptive compression
  by exploiting the high similarity between key states at the token level within a
  sequence.
---

# Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks

## Quick Facts
- arXiv ID: 2407.08454
- Source URL: https://arxiv.org/abs/2407.08454
- Authors: Zheng Wang; Boxiao Jin; Zhongzhi Yu; Minjia Zhang
- Reference count: 21
- Primary result: KVMerger outperforms existing KV cache compression techniques (H2O, CaM) on long-context tasks with 50% and 35% memory budgets

## Executive Summary
This paper addresses the challenge of efficient large language model (LLM) serving during autoregressive generation by proposing KVMerger, a novel KV cache merging approach that achieves adaptive compression. The method exploits high token-level similarity within key states to identify merging sets using a constrained clustering algorithm and employs a Gaussian kernel weighted merging function to combine states. Extensive experiments on Llama2 models across LongBench and ZeroScrolls benchmarks demonstrate that KVMerger achieves superior performance under constrained memory budgets compared to existing compression techniques.

## Method Summary
KVMerger is a KV cache merging approach that reduces memory consumption during LLM inference by identifying and merging similar key states at the token level. The method uses a variant of Agglomerative Hierarchical Clustering to identify merging sets based on cosine similarity thresholds, then applies a Gaussian kernel weighted merging algorithm to combine states within each set. The approach is inspired by the observation that key states exhibit high similarity within sequences, and that this similarity pattern is persistent across different samples and tasks at the model level.

## Key Results
- On LongBench with 50% KV cache budget, KVMerger achieves average score of 35.02 vs 34.00 for H2O and 33.67 for CaM
- Under 35% budget on LongBench, KVMerger maintains competitive performance while significantly reducing memory usage
- The method demonstrates consistent compression ratios across different samples and tasks, validating the observation of persistent KV cache sparsity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KVMerger improves LLM performance on long-context tasks by merging KV cache states with high token-level similarity, reducing memory usage while retaining critical context information
- Mechanism: Exploits high cosine similarity between key states at token level within sequences across different attention heads and layers, using Gaussian kernel weighted merging that assigns greater weight to nearby states
- Core assumption: Key states exhibit high and persistent similarity within sequences, enabling layer-wise KV cache compression without significant information loss
- Evidence anchors: Similarity visualization on Llama2-7B-chat using SynthWiki dataset; consistent compression ratios across different samples and tasks
- Break condition: If key states don't exhibit high similarity, merging may introduce noise and cause performance degradation

### Mechanism 2
- Claim: KV cache sparsity for different samples is persistent at the model level, enabling determination of layer-wise compression ratios by adjusting cosine similarity threshold
- Mechanism: Observation that KV cache sparsity is independent of dataset and remains persistent at model level allows identification of merging sets based on cosine similarity thresholds
- Core assumption: KV cache sparsity resulting from high similarity in key states is independent of dataset and remains persistent at model level
- Evidence anchors: Analysis showing consistent layer-wise compression ratios across different samples from same task and across different tasks
- Break condition: If KV cache sparsity varies significantly across datasets or models, method may not achieve consistent compression ratios

### Mechanism 3
- Claim: Gaussian kernel weighted merging algorithm effectively conserves information after merging without significantly degrading LLM generation performance
- Mechanism: Gaussian kernel assigns weights to states within merging set based on distance from pivotal state, ensuring merged result is shaped by nearby states while minimizing impact of distant, possibly noisy states
- Core assumption: Pivotal state within each merging set is most informative one, and greater weight to nearby elements conserves more information during merging
- Evidence anchors: Comparison with average weighted merging algorithm showing better performance on selected LongBench datasets under 50% cache budget
- Break condition: If pivotal state is not most informative one, substantial weight allocation may lead to information loss and bias

## Foundational Learning

- Concept: Cosine similarity
  - Why needed here: Used to measure similarity between key states at token level within sequences, crucial for identifying merging sets and determining layer-wise compression ratios
  - Quick check question: How is cosine similarity calculated between two vectors?

- Concept: Constrained clustering problem
  - Why needed here: KV cache merging formulated as constrained clustering where all elements in cluster should be consecutive in sequence, preventing merging of distant but similar states
  - Quick check question: What is the objective function to be maximized in the constrained clustering problem for KV cache merging?

- Concept: Gaussian kernel function
  - Why needed here: Assigns weights to each state within merging set based on distance from pivotal state, ensuring merged result is shaped by nearby states
  - Quick check question: How does Gaussian kernel function assign weights to elements based on their distance from pivotal state?

## Architecture Onboarding

- Component map: Input (LLM model, benchmarks) -> Merging set identification (cosine similarity, constrained clustering) -> Gaussian kernel weighted merging -> Output (compressed KV cache)
- Critical path: 1) Identify key states with high similarity at token level within sequences, 2) Formulate KV cache merging as constrained clustering problem, 3) Apply variant of AHC algorithm to solve problem, 4) Define Gaussian kernel weighted merging algorithm for each merging set, 5) Retain KV states with top-k aggregated attention scores
- Design tradeoffs: Memory usage vs. performance (reduce memory by merging while maintaining performance), simplicity vs. effectiveness (simple yet effective merging set identification and merging functions)
- Failure signatures: Performance degradation if merging introduces noise or information loss, inconsistent compression ratios if KV cache sparsity varies significantly
- First 3 experiments: 1) Evaluate cosine similarity between key states at token level using Llama2-7B-chat and visualize similarity map, 2) Apply merging set identification algorithm on Llama2-7B-chat and calculate layer-wise compression ratios for different samples and tasks, 3) Compare Gaussian kernel weighted merging with average weighted merging on selected LongBench datasets under 50% cache budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of KVMerger compare to quantization-based approaches when both are applied together?
- Basis in paper: The paper discusses KVMerger as a complementary approach to quantization, mentioning that combining them could lead to better improvements, but does not provide empirical results on their joint application
- Why unresolved: Paper focuses on comparing KVMerger against other compression techniques but does not explore combined effect of merging and quantization
- What evidence would resolve it: Experiments comparing performance using both KVMerger and quantization methods together versus using either method alone on long-context tasks

### Open Question 2
- Question: What is the impact of different clustering algorithms and similarity measurements on the effectiveness of KVMerger?
- Basis in paper: Paper suggests future work could investigate impact of different clustering algorithms and similarity measurements to optimize merging sets, indicating this aspect is not fully explored
- Why unresolved: Paper uses variant of AHC and cosine similarity based on observed token-level similarity but does not explore alternative methods
- What evidence would resolve it: Empirical studies comparing KVMerger's performance using different clustering algorithms and similarity measures on various long-context tasks and models

### Open Question 3
- Question: How does KVMerger perform on other LLMs, including those fine-tuned for long-context tasks, and on datasets beyond those tested?
- Basis in paper: Paper mentions future work could apply KVMerger to other LLMs including long-context fine-tuned models and datasets to assess generalizability and robustness
- Why unresolved: Current evaluation limited to Llama2-7B-chat and Llama2-13B-chat models on specific benchmarks without testing on other models or datasets
- What evidence would resolve it: Comprehensive testing of KVMerger on diverse set of LLMs including those specifically fine-tuned for long-context tasks and on broader range of datasets

## Limitations

- Method relies heavily on assumption of high token-level similarity in key states, which may not hold consistently across all model architectures and tasks
- Similarity threshold (ε) used for merging set identification is determined empirically and may require re-tuning for different scenarios
- Current implementation appears specific to Llama2 models, with generalization to other architectures (like GPT-style models) remaining unverified

## Confidence

**High Confidence Claims:**
- Observation of high token-level similarity within key states for Llama2 models is well-supported by similarity visualization and compression ratio analysis
- KVMerger outperforms H2O and CaM under both 50% and 35% KV cache budgets on tested benchmarks
- Persistence of KV cache sparsity at model level across different samples and tasks is demonstrated through empirical analysis

**Medium Confidence Claims:**
- Effectiveness of Gaussian kernel weighted merging algorithm in preserving information during compression
- Generalizability of method to other LLM architectures beyond Llama2
- Optimal selection of hyperparameters (similarity threshold, σ value) across different scenarios

**Low Confidence Claims:**
- Performance on extremely long sequences (>8K tokens) beyond tested benchmarks
- Effectiveness across all types of long-context tasks without further empirical validation
- Computational efficiency gains beyond memory reduction

## Next Checks

1. **Cross-Architecture Validation**: Test KVMerger on GPT-style models (e.g., GPT-3, GPT-4) and other transformer architectures to verify generalizability of high token-level similarity observation and merging effectiveness across different model families

2. **Ablation Study on Merging Components**: Conduct comprehensive ablation study isolating contributions of merging set identification versus Gaussian kernel weighted merging function to quantify individual impacts on performance and compression ratios

3. **Extreme Long-Context Evaluation**: Evaluate KVMerger on sequences exceeding 16K tokens to assess robustness and identify potential failure modes when dealing with exceptionally long contexts, particularly focusing on degradation in information preservation