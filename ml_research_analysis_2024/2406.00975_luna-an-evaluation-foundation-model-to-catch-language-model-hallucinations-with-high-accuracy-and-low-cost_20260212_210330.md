---
ver: rpa2
title: 'Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations
  with High Accuracy and Low Cost'
arxiv_id: '2406.00975'
source_url: https://arxiv.org/abs/2406.00975
tags:
- luna
- context
- computational
- language
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Luna is a DeBERTa-large encoder model for detecting hallucinations
  in RAG systems. It addresses the challenge of hallucination detection by framing
  the task as Natural Language Inference, identifying supported tokens in LLM responses
  given a query and retrieved context.
---

# Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost

## Quick Facts
- arXiv ID: 2406.00975
- Source URL: https://arxiv.org/abs/2406.00975
- Authors: Masha Belyi; Robert Friel; Shuai Shao; Atindriyo Sanyal
- Reference count: 34
- Primary result: Luna achieves 97% and 91% reductions in cost and latency for hallucination detection compared to GPT-3.5

## Executive Summary
Luna is a DeBERTa-large encoder model designed to detect hallucinations in RAG systems with high accuracy and low computational cost. It frames hallucination detection as a Natural Language Inference task, identifying supported tokens in LLM responses given a query and retrieved context. Luna outperforms GPT-3.5 and commercial evaluation frameworks on hallucination detection while maintaining strong performance on long contexts and across multiple industry verticals.

## Method Summary
Luna fine-tunes a DeBERTa-v3-Large NLI checkpoint with a shallow hallucination classifier on each response token. The model is trained on a RAG QA dataset constructed from open-book QA datasets covering five industry verticals, annotated using GPT-4-turbo. At inference, Luna processes long contexts using overlapping windows and aggregates token-level support probabilities to produce example-level hallucination predictions.

## Key Results
- Achieves 97% and 91% reductions in cost and latency respectively compared to GPT-3.5
- Maintains 88% and 68% of its performance on medium and long context lengths (exceeding 16k tokens)
- Outperforms commercial evaluation frameworks on hallucination detection metrics

## Why This Works (Mechanism)

### Mechanism 1
Luna's span-level hallucination detection is more precise than example-level detection for long contexts. By predicting support probabilities at the token level and aggregating with a min-over-tokens strategy, Luna can isolate exactly which parts of a response are hallucinated, even when relevant context is scattered across long documents.

### Mechanism 2
Luna's long-context chunking approach prevents false positives from scattered supporting information. By processing overlapping windows of context that each include the full question and response, Luna ensures that supporting information appearing anywhere in the long context can influence the support probability for each response token.

### Mechanism 3
Pre-training on NLI and fine-tuning on cross-domain RAG data enables Luna to generalize across industry verticals. The NLI pre-training provides a foundation for reasoning about relationships between claims and evidence, while the diverse RAG fine-tuning data exposes the model to various domain-specific language patterns and hallucination types.

## Foundational Learning

- Concept: Natural Language Inference (NLI) and entailment relationships
  - Why needed here: Luna builds on NLI pre-training to reason about whether response tokens are supported by the context, similar to determining if a hypothesis is entailed by a premise
  - Quick check question: How would you modify an NLI model to output token-level support probabilities instead of just sentence-level entailment labels?

- Concept: Long-context processing and window-based approaches
  - Why needed here: Luna uses overlapping windows to process long RAG contexts that exceed the model's maximum sequence length, requiring understanding of how to maintain context across window boundaries
  - Quick check question: What are the trade-offs between using larger windows (more context per window) versus smaller windows (more windows, better aggregation) for long-context processing?

- Concept: Cross-domain generalization and transfer learning
  - Why needed here: Luna is trained on diverse RAG data from multiple industries and must perform well on unseen domains, requiring understanding of how pre-training and fine-tuning data distributions affect generalization
  - Quick check question: How would you evaluate whether Luna's performance degradation on a new domain is due to domain shift or insufficient training data for that domain type?

## Architecture Onboarding

- Component map: DeBERTa-v3-Large encoder -> NLI pre-trained weights -> Token-level support prediction head -> Long-context chunking module -> Aggregation layer -> Training pipeline
- Critical path: Input RAG example → Long-context chunking → Token-level support prediction → Aggregation → Output hallucination probability
- Design tradeoffs: Span-level vs example-level detection (more precise but more complex), window size (larger windows capture more context but reduce aggregation quality), token-level vs sentence-level labels (more granular but noisier annotations)
- Failure signatures: High false positive rate (issues with min-over-tokens aggregation or window overlap), performance degradation on long contexts (insufficient window overlap or token-level label noise), poor cross-domain generalization (insufficient diversity in training data or inadequate NLI pre-training)
- First 3 experiments:
  1. Ablation study: Compare span-level detection vs example-level detection on long-context RAG data to quantify precision gains
  2. Window size optimization: Systematically vary window overlap and size to find optimal parameters for long-context processing
  3. Cross-domain evaluation: Test Luna on a new industry vertical not seen during training to measure generalization capability and identify failure patterns

## Open Questions the Paper Calls Out
- How does Luna's performance change when trained with token-level annotations instead of sentence-level annotations?
- How does Luna's performance degrade on long-context inputs beyond 16k tokens, and what are the theoretical limits of its context length handling?
- How does Luna's performance compare to fine-tuned models of similar size when evaluated on the same benchmarks?
- How robust is Luna to variations in the quality and format of retrieved context, such as when documents are noisy, incomplete, or contain irrelevant information?

## Limitations
- Performance on extremely long contexts beyond 16k tokens is untested
- Potential distribution shift between synthetic training data and real-world deployment scenarios
- Reliance on single-token level predictions may miss context-dependent hallucinations

## Confidence
- High Confidence: Luna's superiority over GPT-3.5 and commercial frameworks on standard hallucination detection metrics
- Medium Confidence: Luna's ability to generalize across industry verticals
- Medium Confidence: The cost and latency improvements

## Next Checks
1. Test Luna on additional real-world RAG datasets beyond RAGTruth to verify generalization claims and assess performance on naturally occurring hallucinations versus synthetic ones.
2. Conduct detailed analysis of Luna's performance on the longest context examples (16k+ tokens) to identify specific failure modes and determine whether they stem from windowing strategy or token-level prediction limitations.
3. Compare Luna's performance against a version trained from scratch (without NLI pre-training) on the same RAG data to quantify the contribution of NLI reasoning capabilities to cross-domain generalization.