---
ver: rpa2
title: 'GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic
  Evaluations'
arxiv_id: '2402.12348'
source_url: https://arxiv.org/abs/2402.12348
tags:
- llms
- opponent
- games
- reasoning
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates LLMs'' strategic and logical reasoning abilities
  in competitive game-theoretic scenarios. The authors propose GTBench, a language-driven
  environment with 10 games spanning a comprehensive taxonomy: complete vs.'
---

# GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations

## Quick Facts
- arXiv ID: 2402.12348
- Source URL: https://arxiv.org/abs/2402.12348
- Reference count: 40
- Key outcome: LLMs fail in complete and deterministic games but remain competitive in incomplete and probabilistic scenarios

## Executive Summary
This paper introduces GTBench, a comprehensive evaluation framework for assessing large language models' strategic and logical reasoning capabilities in competitive game-theoretic scenarios. The framework encompasses 10 games spanning a taxonomy of complete vs. incomplete information, dynamic vs. static, and probabilistic vs. deterministic games. Through systematic comparisons against conventional solvers (MCTS/TfT) and other LLMs, the study reveals that LLMs struggle in complete and deterministic games but remain competitive in incomplete and probabilistic scenarios. The research also identifies code-pretraining as a key factor for strategic reasoning success and finds that advanced reasoning methods don't always improve performance.

## Method Summary
GTBench employs a language-driven evaluation environment where LLM agents interact with game environments through prompt adapters that convert observations to prompts and responses to actions. The framework includes 10 game-theoretic tasks with modular prompt structures (System Prompt, Head Prompt, Observation Prompt, Reasoning Prompt) and four reasoning methods (Prompt, Chain-of-Thought, Self-Consistent CoT, Tree-of-Thought). Experiments measure performance via Normalized Relative Advantage (NRA) and Elo ratings across 50 valid matches per competition, with temperature set to 0.2 and max tokens to 1024. The evaluation compares LLM agents against MCTS and Tit-for-Tat conventional solvers as well as against each other.

## Key Results
- LLMs fail in complete and deterministic games against simple MCTS opponents but remain competitive in incomplete and probabilistic scenarios
- Code-pretrained models like CodeLlama-34b-Instruct outperform larger chat-optimized models in strategic reasoning tasks
- Advanced reasoning methods (CoT, SC-CoT, ToT) don't always improve performance and sometimes hurt results
- Most open-source LLMs lag commercial ones in complex games, except Llama-3-70b-Instruct
- Error analysis reveals LLMs can plan but struggle with action selection and miss endgame situations

## Why This Works (Mechanism)

### Mechanism 1
Code-pretrained LLMs exhibit superior strategic reasoning compared to larger chat-optimized models because code-pretraining exposes models to structured logical patterns, algorithmic thinking, and formal problem-solving, which transfer to strategic reasoning in game-theoretic tasks. The core assumption is that code structure and logic patterns are generalizable to strategic reasoning in competitive games. Evidence shows CodeLlama-34b-Instruct achieves comparable results as GPT-3.5-turbo and significantly outperforms Llama-2-70b-chat. This mechanism would break if the code-pretraining benefits are specific to syntax rather than generalizable reasoning patterns.

### Mechanism 2
Advanced reasoning methods (CoT, SC-CoT, ToT) do not always improve LLM performance in game-theoretic tasks because complex reasoning methods can introduce additional error points and reasoning complexity that outweigh their benefits for certain tasks and model capabilities. The core assumption is that additional reasoning steps don't always translate to better decision quality. Evidence shows that only Mistral-7b-Orca has substantial improvement with CoT while other LLMs perform worse with advanced reasoning. This mechanism would break if the reasoning methods are tuned specifically for each game type.

### Mechanism 3
LLMs fail in complete and deterministic games but remain competitive in incomplete and probabilistic scenarios because complete and deterministic games have well-defined optimal strategies that simple search algorithms (like MCTS) can approximate, making LLM reasoning unnecessary and error-prone. The core assumption is that search algorithms outperform LLM reasoning in deterministic, complete-information domains. Evidence shows all LLM agents achieve NRA of -1 when competing against MCTS Agent in complete and deterministic games. This mechanism would break if the games are not truly deterministic or complete-information.

## Foundational Learning

- **Game-theoretic evaluation frameworks**: Understanding how to systematically evaluate strategic reasoning requires knowledge of game theory taxonomy and evaluation metrics. Quick check: What are the key dimensions for classifying games in game theory?

- **Monte Carlo Tree Search (MCTS)**: MCTS is used as the baseline conventional agent for comparison with LLM-driven agents. Quick check: How does MCTS balance exploration and exploitation in search trees?

- **Nash Equilibrium and Pareto Efficiency**: These concepts are used to characterize the game-theoretic properties of LLM behaviors. Quick check: What is the relationship between regret and Nash Equilibrium approximation?

## Architecture Onboarding

- **Component map**: Environment (game hosting, observation providing, action execution) → Prompt Adapter (observation to prompt, response to action) → Participant (LLM agents with various reasoning methods)
- **Critical path**: Game state → Environment observation → Prompt Adapter → LLM generation → Action parsing → Game execution
- **Design tradeoffs**: Language-driven approach vs. API-based game interfaces; flexibility vs. performance; standardized prompts vs. game-specific optimization
- **Failure signatures**: Completion rate <90%, consistent NRA of -1 against MCTS, illegal move generation, reasoning errors in endgame detection
- **First 3 experiments**:
  1. Run LLM agents against Random Agent across all 10 games to establish baseline performance
  2. Compare LLM agents against MCTS Agent in complete and deterministic games
  3. Evaluate same LLM agents in incomplete and probabilistic games to identify capability gaps

## Open Questions the Paper Calls Out

### Open Question 1
What are the specific factors that contribute to LLM failures in complete and deterministic games, and how can these be mitigated to improve LLM performance in such scenarios? The paper states that LLMs fail in complete and deterministic games against simple MCTS opponents but does not provide a detailed analysis of the specific factors leading to these failures. Resolving this would require experiments that isolate and analyze the impact of different factors (e.g., game complexity, LLM architecture, reasoning methods) on LLM performance in complete and deterministic games.

### Open Question 2
How does code-pretraining specifically enhance strategic reasoning abilities in LLMs, and can this be generalized to other reasoning tasks beyond game-theoretic scenarios? The paper shows that code-pretrained LLMs like CodeLlama-34b-Instruct outperform larger chat LLMs in strategic reasoning tasks but does not delve into the mechanisms by which code-pretraining improves reasoning abilities. Resolving this would require investigating the specific features of code-pretraining that contribute to improved reasoning and testing the generalization of these improvements to other reasoning tasks.

### Open Question 3
Why do advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) sometimes lead to worse performance in game-theoretic scenarios, and how can these methods be optimized for better results? The paper finds that advanced reasoning methods do not always help and can sometimes hurt performance but does not provide a detailed analysis of the reasons behind the ineffectiveness of these methods in certain scenarios. Resolving this would require conducting experiments that systematically vary the application of CoT and ToT methods across different game types and analyzing the resulting performance to identify optimal usage strategies.

## Limitations

- Findings about LLM limitations in complete and deterministic games rely on comparisons against MCTS with fixed 1000 simulations, but sensitivity to MCTS parameter tuning remains unclear
- Error analysis conclusions are qualitative and based on manual inspection of game logs, making them harder to verify and potentially subject to interpretation bias
- Observation that advanced reasoning methods sometimes harm performance suggests potential issues with prompt engineering that weren't fully explored

## Confidence

- **High confidence**: Systematic comparison framework and evaluation methodology (NRA and Elo ratings) are well-established and reproducible; finding that code-pretrained models outperform larger chat models is supported by direct experimental comparisons
- **Medium confidence**: Conclusions about LLM performance gaps between complete/deterministic versus incomplete/probabilistic games are supported by data, though generalizability to other game types requires validation; observation that reasoning methods don't always help is based on controlled experiments but may depend on prompt quality
- **Low confidence**: Error analysis conclusions are qualitative and based on manual inspection of game logs, making them harder to verify and potentially subject to interpretation bias

## Next Checks

1. **Parameter sensitivity analysis**: Re-run the MCTS comparisons with varying simulation counts (100-5000) to determine if the observed LLM failures persist across different computational budgets
2. **Prompt engineering ablation**: Systematically vary the prompt structure for each reasoning method to determine if performance degradation is due to reasoning method choice versus prompt quality
3. **Cross-domain generalization**: Test the same LLM agents on a different set of complete-information games (chess, checkers) to verify if the observed limitations generalize beyond the current game selection