---
ver: rpa2
title: 'THESAURUS: Contrastive Graph Clustering by Swapping Fused Gromov-Wasserstein
  Couplings'
arxiv_id: '2412.11550'
source_url: https://arxiv.org/abs/2412.11550
tags:
- class
- graph
- cluster
- clustering
- thesaurus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: THESAURUS tackles the problem of poor cluster separability in deep
  graph clustering by introducing semantic prototypes to provide contextual information
  for distinguishing similar nodes. It uses a cross-view assignment prediction pretext
  task aligned with clustering and fuses graph structure information via Gromov-Wasserstein
  Optimal Transport.
---

# THESAURUS: Contrastive Graph Clustering by Swapping Fused Gromov-Wasserstein Couplings

## Quick Facts
- arXiv ID: 2412.11550
- Source URL: https://arxiv.org/abs/2412.11550
- Reference count: 40
- Key outcome: Achieves F1 scores up to 5.5 percentage points higher than SOTA on Cora and 11.65 points higher on Pubmed by improving cluster separability

## Executive Summary
THESAURUS addresses the critical challenge of poor cluster separability in deep graph clustering, particularly the Uniform Effect and Cluster Assimilation issues that plague existing methods. The method introduces semantic prototypes to provide contextual information for distinguishing similar nodes from different classes, and employs a cross-view assignment prediction pretext task that aligns well with the downstream clustering task. By fusing graph structure information via Gromov-Wasserstein Optimal Transport and using a momentum module to adapt to diverse data, THESAURUS significantly improves clustering performance on multiple benchmark datasets.

## Method Summary
THESAURUS is a contrastive learning framework for graph node clustering that introduces semantic prototypes and uses cross-view assignment prediction as a pretext task. The method employs GCN encoders to generate node representations, constructs prototype graphs, and matches them with data graphs using Fused Gromov-Wasserstein Optimal Transport. A momentum module updates the prototype graph and marginal distribution to handle diverse data. The framework is trained end-to-end with a loss that predicts optimal clustering assignments across different data augmentation views, directly optimizing for cluster separability rather than indirect reconstruction or contrastive objectives.

## Key Results
- Achieves F1 scores up to 5.5 percentage points higher than prior SOTA on Cora dataset
- Improves F1 scores by 11.65 percentage points on Pubmed compared to previous methods
- Effectively mitigates the Uniform Effect (all nodes assigned to one cluster) and Cluster Assimilation (minority clusters absorbed by majority clusters)
- Demonstrates strong performance across nine benchmark graph datasets including Cora, Citeseer, Pubmed, and Amazon-Photo

## Why This Works (Mechanism)

### Mechanism 1
Semantic prototypes provide contextual information that helps distinguish synonymous nodes from different classes. By representing each semantic category as a prototype and computing context-aware representations based on distances to these prototypes, the method captures subtle differences between similar nodes that share similar structural neighborhoods but belong to different classes.

### Mechanism 2
The pretext task of predicting cross-view clustering assignments aligns well with the downstream clustering objective. By treating semantic prototypes as centroids and learning to predict swapped clustering assignments across different data augmentation views, the model directly optimizes for cluster separability rather than indirect reconstruction or contrastive objectives.

### Mechanism 3
Gromov-Wasserstein Optimal Transport extracts and exploits cluster information from graph structure more effectively than standard graph convolutions. By computing optimal transport between the prototype graph and data graph using GW-OT, the method captures structural cluster information without suffering from oversmoothing or over-squashing issues that plague GCN-based methods.

## Foundational Learning

- Concept: Optimal Transport and Wasserstein Distance
  - Why needed here: THESAURUS uses Fused Gromov-Wasserstein OT to align prototype graphs with data graphs and to fuse attribute and structure information
  - Quick check question: What is the key advantage of using Wasserstein distance over KL divergence for comparing probability distributions?

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: The method uses GCN encoders to generate node representations before applying the OT-based clustering framework
  - Quick check question: What is the fundamental operation performed by each layer of a GCN?

- Concept: Self-Supervised Learning and Pretext Tasks
  - Why needed here: THESAURUS employs a pretext task (cross-view assignment prediction) to learn meaningful representations without labels
  - Quick check question: How does a pretext task in self-supervised learning differ from supervised learning objectives?

## Architecture Onboarding

- Component map: Data Augmentation → GCN Encoder → Prototype Context Generation → FGW-OT Assignment → Cross-View Prediction Loss → Momentum Updates
- Critical path: 1) Generate two augmented views of the graph 2) Encode views using GCN to get Z1 and Z2 3) Compute context-aware representations R1, R2 4) Construct prototype graphs B1, B2 5) Solve FGW-OT to get assignments Q1, Q2 6) Predict cross-view assignments and compute loss 7) Update prototypes and marginal distributions with momentum
- Design tradeoffs: Number of prototypes S vs. clustering quality, Temperature τ in softmax vs. prediction sharpness, Momentum weights β1, β2 vs. stability vs. adaptability
- Failure signatures: All clusters merging into one, One cluster dominating, Minority clusters being absorbed, Training loss not decreasing
- First 3 experiments: 1) Run on Cora with default hyperparameters to verify basic functionality 2) Test with different numbers of prototypes (S=5, 10, 20) to find optimal setting 3) Compare with and without the momentum module to validate its contribution

## Open Questions the Paper Calls Out

### Open Question 1
How does THESAURUS handle extremely large graphs with millions of nodes where the Fused Gromov-Wasserstein OT becomes computationally prohibitive? The paper mentions scalability concerns but does not address how the method would scale to massive graphs where even Sinkhorn becomes intractable.

### Open Question 2
What is the optimal number of semantic prototypes S for different types of graph data, and how sensitive is THESAURUS to this hyperparameter? The paper varies S across datasets but does not provide a principled method for selecting S or analyze its sensitivity to clustering performance.

### Open Question 3
How does THESAURUS perform when the underlying graph structure is noisy or adversarial, such as when edges are randomly added or removed? The paper uses data augmentation through edge and feature dropout but doesn't systematically evaluate robustness to structural noise or adversarial attacks on the graph topology.

## Limitations

- Computational complexity of Gromov-Wasserstein Optimal Transport limits scalability to massive graphs
- Heavy reliance on multiple hyperparameters (S, β1/β2, τ) without systematic sensitivity analysis
- Lack of empirical validation for the specific mechanisms claimed to address Uniform Effect and Cluster Assimilation

## Confidence

- High confidence: Overall clustering performance improvements (F1 scores 5.5-11.65 percentage points higher than SOTA)
- Medium confidence: The three proposed mechanisms (semantic prototypes, cross-view assignment prediction, GW-OT) contribute to performance gains
- Low confidence: The specific claims about how each mechanism addresses the Uniform Effect and Cluster Assimilation issues

## Next Checks

1. Conduct ablation study validation by removing each major component (semantic prototypes, cross-view prediction, GW-OT) to quantify individual contributions to performance improvements.

2. Use t-SNE or UMAP to visualize learned representations and prototype distributions, specifically examining whether synonymous nodes from different classes show meaningful separation based on prototype distances.

3. Test THESAURUS on larger graphs (100K+ nodes) to empirically validate computational efficiency claims and identify practical limitations for real-world deployment.