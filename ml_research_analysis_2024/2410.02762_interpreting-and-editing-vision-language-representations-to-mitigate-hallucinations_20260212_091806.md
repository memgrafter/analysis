---
ver: rpa2
title: Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations
arxiv_id: '2410.02762'
source_url: https://arxiv.org/abs/2410.02762
tags:
- image
- objects
- hallucinations
- representations
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to interpret and edit vision-language
  models (VLMs) to mitigate hallucinations. The authors project internal image representations
  onto the language vocabulary and find that real objects have higher confidence scores
  than hallucinated ones.
---

# Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations

## Quick Facts
- **arXiv ID**: 2410.02762
- **Source URL**: https://arxiv.org/abs/2410.02762
- **Reference count**: 29
- **Primary result**: Achieves up to 25.7% hallucination reduction in image captions while maintaining performance

## Executive Summary
This paper introduces a method to interpret and edit vision-language models (VLMs) to mitigate hallucinations. The authors project internal image representations onto the language vocabulary and find that real objects have higher confidence scores than hallucinated ones. Using this insight, they develop PROJECT AWAY, an algorithm that reduces hallucinations by orthogonally editing out hallucinated object features. Applied to the COCO2014 dataset, the method achieves up to 25.7% hallucination reduction in image captions while maintaining performance. The approach also enables zero-shot segmentation, demonstrating that VLMs' internal representations can be understood and edited to improve reliability and enable new capabilities.

## Method Summary
The authors propose a two-step approach: first, they use logit lens to project VLM internal image representations through the language model's unembedding matrix, revealing interpretable object probability distributions. They observe that real objects exhibit higher internal confidence scores than hallucinated objects. Second, they develop PROJECT AWAY, an algorithm that orthogonally edits image features to remove hallucinated objects. The method applies linear orthogonalization by subtracting a weighted version of the hallucinated object's text embedding from relevant image feature activations, reducing their influence on subsequent token generation.

## Key Results
- PROJECT AWAY achieves up to 25.7% hallucination reduction on COCO2014 dataset
- Method maintains caption quality while reducing hallucinations
- Enables zero-shot segmentation capabilities by editing internal representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting VLM internal image representations through the language model's unembedding matrix yields interpretable object probability distributions.
- Mechanism: The logit lens technique applies the unembedding matrix to intermediate layer activations, effectively translating image features into text vocabulary logits.
- Core assumption: The intermediate image representations in VLMs retain enough semantic structure to be meaningfully mapped to text tokens via the language model's unembedding matrix.
- Evidence anchors:
  - [abstract] "We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects."
  - [section 3.2] "We apply logit lens on the image representations in the VLM... For a given image embedding ki, we find the latent representation of the image embedding at layer l, hl(ki), taking the logit lens to get the probability distribution over the vocabulary."
  - [corpus] Weak evidence: Related works focus on hallucination detection/mitigation but not on logit lens interpretation of internal image features.
- Break condition: If the mapping from image features to text vocabulary via the unembedding matrix produces random or uninformative distributions, the method fails.

### Mechanism 2
- Claim: Hallucinated objects manifest with lower internal confidence in VLM image representations than real objects.
- Mechanism: The model's internal confidence score (maximum softmax probability over layers and image features) is systematically lower for objects not present in the image.
- Core assumption: The VLM's internal representations encode a confidence signal that distinguishes between real and hallucinated objects.
- Evidence anchors:
  - [abstract] "We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects."
  - [section 3.2] "Figure 2 exhibits the internal confidences for objects present and not present in the image. We empirically find that the VLMs' internal confidences are higher for present objects than not present ones."
  - [corpus] Weak evidence: Related works discuss hallucination detection but not specifically the confidence gap between real and hallucinated objects in internal representations.
- Break condition: If hallucinated and real objects show similar internal confidence distributions, the detection method fails.

### Mechanism 3
- Claim: Linear orthogonalization of image features with respect to text embeddings of hallucinated objects effectively removes those objects from captions.
- Mechanism: PROJECT AWAY subtracts a weighted version of the target object's text embedding from relevant image feature activations, reducing their influence on subsequent token generation.
- Core assumption: Image features and text embeddings exist in compatible spaces where linear operations can meaningfully remove semantic content.
- Evidence anchors:
  - [abstract] "Building on this approach, we introduce a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features."
  - [section 4.1] "We present an algorithm, PROJECT AWAY... Given an image and an object to remove, we edit the latent representations... We compute the dot product, p, of hlI(ki) and the object's text embedding ⃗t, subtracting a weighted ⃗t from hlI(ki) only if the dot product is positive."
  - [corpus] Weak evidence: Related works discuss editing mechanisms but not specifically linear orthogonalization of image features for hallucination removal.
- Break condition: If the linear editing operation fails to reduce hallucination rates or significantly degrades caption quality, the method fails.

## Foundational Learning

- Concept: Logit lens technique for interpreting intermediate layer representations
  - Why needed here: This is the core interpretability method that enables understanding what VLMs "see" internally
  - Quick check question: What does applying the unembedding matrix to intermediate layer activations reveal about the model's current state?

- Concept: Vector projection and orthogonalization in embedding spaces
  - Why needed here: The PROJECT AWAY algorithm relies on understanding how to project vectors to remove specific semantic content
  - Quick check question: How does subtracting a scaled version of one vector from another achieve orthogonality?

- Concept: Softmax probability distributions and confidence scoring
  - Why needed here: The internal confidence metric is based on softmax probabilities over the vocabulary
  - Quick check question: Why is the maximum softmax probability across layers and image features a meaningful confidence score?

## Architecture Onboarding

- Component map: Vision encoder -> Mapping network -> Language model decoder with L layers
- Critical path: Input image -> vision encoder -> mapping network -> image embeddings -> language model layers -> token generation. The logit lens operates on the language model layers.
- Design tradeoffs: Linear editing (PROJECT AWAY) is simple and interpretable but may not capture complex semantic relationships. More sophisticated editing might work better but be harder to interpret.
- Failure signatures: If internal confidence doesn't distinguish real/hallucinated objects, if linear editing degrades caption quality, or if spatial localization doesn't correlate with visual object positions.
- First 3 experiments:
  1. Apply logit lens to random image embeddings and visualize the top predicted tokens - verify interpretability.
  2. Compute internal confidence scores for objects in sample images and compare distributions for real vs hallucinated objects.
  3. Apply PROJECT AWAY with different weight factors (α) and measure hallucination reduction vs caption quality degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the PROJECT AWAY algorithm's effectiveness vary across different VLM architectures (e.g., transformer-based vs. CNN-based models) and what architectural features make certain models more amenable to hallucination reduction through linear edits?
- Basis in paper: [inferred] The paper applies PROJECT AWAY to two transformer-based VLMs (InstructBLIP and LLaVA) and observes different effectiveness rates (25.7% vs. 23.8% hallucination reduction). The ablation study shows hidden layer selection significantly impacts performance, suggesting architectural differences matter.
- Why unresolved: The paper only examines two transformer-based models with similar architectures. It doesn't explore whether these findings generalize to other VLM architectures or what specific architectural features (attention mechanisms, layer depths, etc.) influence the algorithm's success.
- What evidence would resolve it: Systematic testing of PROJECT AWAY across diverse VLM architectures including CNN-based models, attention-based models with different configurations, and newer architectures would reveal architectural dependencies and optimal parameters for each type.

### Open Question 2
- Question: What is the relationship between the internal confidence threshold used for hallucination detection and the false positive rate for correctly detected objects, and how can this threshold be optimized dynamically based on image content or task requirements?
- Basis in paper: [explicit] The paper mentions that "our chosen threshold prioritizes precision over recall (i.e. we allow classification of some CD objects as hallucinations)" and uses fixed thresholds (co < 0.2 for InstructBLIP and co < 0.1 for LLaVA). The logit lens analysis shows internal confidence distributions differ between real and hallucinated objects.
- Why unresolved: The paper uses static, empirically chosen thresholds without exploring the precision-recall tradeoff or adaptive thresholding strategies. It doesn't investigate how image complexity, object density, or task context might influence optimal threshold selection.
- What evidence would resolve it: Comprehensive analysis of precision-recall curves across varying thresholds, testing dynamic threshold adjustment based on image features or downstream task requirements, and exploring whether threshold optimization improves both hallucination reduction and preservation of correctly detected objects.

### Open Question 3
- Question: How does the PROJECT AWAY algorithm affect the long-term stability and generalization capabilities of VLMs when applied iteratively or across multiple editing sessions, and what are the potential unintended consequences of repeated knowledge editing?
- Basis in paper: [inferred] The paper demonstrates immediate effects of PROJECT AWAY on hallucination reduction but doesn't examine long-term impacts. The ablation study shows that high weight factors can make captions "nonsensical," suggesting potential instability with aggressive editing.
- Why unresolved: The paper focuses on single-session editing effects without investigating whether repeated application degrades model performance, creates new failure modes, or affects the model's ability to learn from new data. The relationship between editing intensity and model degradation is not explored.
- What evidence would resolve it: Longitudinal studies tracking model performance across multiple editing sessions, testing whether edited models maintain or improve performance on downstream tasks, and investigating whether edited models exhibit new types of errors or degraded generalization capabilities would reveal long-term stability implications.

## Limitations
- Method requires manual identification of hallucinated objects to edit, limiting practical applicability
- Evaluation limited to COCO2014 dataset and a single VLM architecture, raising generalizability concerns
- Linear orthogonalization approach may be too crude to capture complex semantic relationships

## Confidence

**High Confidence**: The core observation that logit lens reveals interpretable object probability distributions from VLM internal representations. This is directly supported by the empirical evidence showing distinct confidence patterns for real versus hallucinated objects.

**Medium Confidence**: The effectiveness of the PROJECT AWAY algorithm for hallucination reduction. While the method achieves measurable improvements (up to 25.7% reduction), the evaluation is limited to a specific dataset and model, and the trade-off between hallucination reduction and caption quality degradation needs more thorough investigation.

**Low Confidence**: The generalizability of this approach to other VLMs, datasets, and types of hallucinations. The paper doesn't explore how well the method works with different architectures, the impact on other hallucination types, or performance in more challenging real-world scenarios.

## Next Checks

1. **Cross-Architecture Validation**: Apply the logit lens and PROJECT AWAY methodology to at least two additional VLM architectures (e.g., CLIP, BLIP) to assess generalizability. Measure whether the internal confidence gap between real and hallucinated objects persists and whether the linear editing approach remains effective.

2. **Hallucination Type Analysis**: Systematically categorize hallucinations in the COCO dataset (object hallucinations, attribute errors, contextual inconsistencies) and evaluate PROJECT AWAY's effectiveness on each category. This will reveal whether the method addresses the full spectrum of hallucination types or is limited to object-level hallucinations.

3. **Ablation Study on Weight Parameter α**: Conduct a comprehensive ablation study varying the weight parameter α in PROJECT AWAY across a wider range (e.g., 0.1 to 2.0) and measure the precise trade-off curve between hallucination reduction and caption quality degradation. Include quantitative metrics beyond ROUGE (e.g., CLIPScore, human evaluation) to assess caption quality more comprehensively.