---
ver: rpa2
title: 'Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training'
arxiv_id: '2402.08344'
source_url: https://arxiv.org/abs/2402.08344
tags:
- noise
- bias
- implicit
- noisy-sgd
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the implicit bias of noisy SGD in the context
  of differentially private training. It demonstrates that the performance drop observed
  in large-batch DP-SGD persists even without gradient clipping (Noisy-SGD), suggesting
  that stochasticity (not clipping) is responsible for the implicit bias.
---

# Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training

## Quick Facts
- arXiv ID: 2402.08344
- Source URL: https://arxiv.org/abs/2402.08344
- Authors: Tom Sander; Maxime Sylvestre; Alain Durmus
- Reference count: 40
- Primary result: Performance drop in large-batch DP-SGD persists even without gradient clipping, indicating stochasticity (not clipping) drives implicit bias

## Executive Summary
This paper analyzes the implicit bias of noisy SGD in differentially private training, demonstrating that the performance degradation observed in large-batch DP-SGD persists even when removing gradient clipping (creating Noisy-SGD). The results show that stochasticity, rather than the clipping operation, is responsible for the implicit bias that causes small batches to outperform large batches. Theoretical analysis using continuous SDEs for linear least squares and diagonal linear networks reveals that added Gaussian noise can actually amplify the implicit bias compared to standard SGD, though the optimal noise level involves a trade-off between bias enhancement and perturbation effects.

## Method Summary
The paper combines theoretical analysis using stochastic differential equations as continuous approximations of SGD dynamics with empirical validation on ImageNet and synthetic datasets. For the theoretical component, the authors analyze how adding isotropic Gaussian noise to SGD updates affects the implicit bias in linear least squares and diagonal linear network settings. The empirical validation uses noisy SGD (DP-SGD without clipping) on ImageNet with varying batch sizes and step sizes, comparing performance to standard SGD. Additionally, synthetic experiments with diagonal linear networks demonstrate how different noise levels affect the sparsity of learned solutions.

## Key Results
- Performance degradation with large batches persists in Noisy-SGD (DP-SGD without clipping), showing stochasticity rather than clipping drives implicit bias
- In diagonal linear networks, added Gaussian noise can amplify implicit bias, producing sparser solutions than standard SGD
- Theoretical analysis shows that Noisy-SGD converges to solutions with similar implicit bias properties as SGD, with the added noise perturbing but not destroying the gradient geometry

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gradient noise structure inherent to SGD remains robust under Gaussian perturbations, preserving its implicit bias
- Mechanism: When Gaussian noise is added to SGD's stochastic gradient updates, the resulting dynamics still converge to solutions with similar implicit bias properties. The added noise perturbs the gradient geometry but doesn't fundamentally alter the convergence behavior toward solutions with favorable generalization properties
- Core assumption: The natural SGD noise geometry is robust enough that isotropic Gaussian perturbations don't completely destroy the implicit bias effect
- Evidence anchors:
  - [abstract]: "the performance issues of large-batch DP-SGD training are rooted in the same underlying principles as SGD, offering hope for potential improvements in large batch training strategies."
  - [section 3.2]: "We observe that adding isotropic noise changes the limiting distribution: its shape depends on the training data. This simple example shows that geometry variation of the noise in the linear least square setting implies a controlled variation of the limiting process."
  - [corpus]: Weak - corpus neighbors focus on DP-SGD implementations rather than theoretical analysis of noise geometry robustness
- Break condition: If the added Gaussian noise overwhelms the structured SGD noise, the implicit bias could be completely destroyed

### Mechanism 2
- Claim: The implicit bias in Noisy-SGD can be amplified compared to standard SGD under certain conditions
- Mechanism: Adding Gaussian noise to SGD updates can effectively reduce the initialization parameter in the mirror flow formulation, leading to solutions that are closer to sparse solutions. The trade-off between smaller effective initialization and perturbation from additional noise determines whether the implicit bias is enhanced
- Core assumption: The benefit from smaller effective initialization outweighs the perturbation introduced by additional Gaussian noise
- Evidence anchors:
  - [section 3.3]: "Noisy-SGD can even exhibit an enhanced implicit bias in comparison to SGD... Using continuous modelings, we theoretically demonstrate that a favorable implicit bias indeed exists: It leads to the same solution as SGD, albeit with a distinct effective initialization."
  - [section 4.2]: "we show that noisy-SGD produces sparser solutions than the one obtained with SGD, as it is closer to the sparse interpolator βl0."
  - [corpus]: Weak - no direct corpus evidence about implicit bias amplification through noise addition
- Break condition: If the Gaussian noise magnitude becomes too large relative to the structured SGD noise, the perturbation effect could dominate and worsen the implicit bias

### Mechanism 3
- Claim: The performance drop in large-batch DP-SGD is caused by stochasticity rather than gradient clipping
- Mechanism: By removing the clipping component from DP-SGD (creating Noisy-SGD), the same performance degradation with large batches is observed. This demonstrates that the stochastic nature of gradient updates, not the clipping operation, is responsible for the implicit bias that causes small batches to perform better
- Core assumption: The clipping operation in DP-SGD is not the primary cause of the implicit bias effect
- Evidence anchors:
  - [abstract]: "We first show that the phenomenon extends to Noisy-SGD (DP-SGD without clipping), suggesting that the stochasticity (and not the clipping) is the cause of this implicit bias, even with additional isotropic Gaussian noise."
  - [section 1]: "We first observe a persistent manifestation of the implicit bias associated with DP-SGD observed by Sander et al. (2023) when we remove the gradient clipping component, as depicted in Figure 1."
  - [corpus]: Moderate - corpus neighbors discuss DP-SGD implementations but don't specifically address the clipping vs. stochasticity distinction
- Break condition: If clipping had a non-trivial effect on the gradient direction that wasn't captured by the theoretical analysis

## Foundational Learning

- Concept: Stochastic Differential Equations (SDEs) as continuous approximations of SGD
  - Why needed here: The paper uses SDEs to theoretically analyze how adding Gaussian noise affects SGD's implicit bias properties. Understanding this mathematical framework is essential for following the proofs and results
  - Quick check question: In the SDE dθt = -∇R(θt)dt + σ(θt)dWt, what does the term σ(θt)dWt represent in the context of SGD?

- Concept: Mirror flow dynamics for diagonal linear networks
  - Why needed here: The analysis of DLNs relies on understanding how the mirror flow framework describes SGD's convergence behavior, and how adding noise modifies this flow to change the effective initialization
  - Quick check question: How does the potential function ϕα(β) in the mirror flow framework relate to the l1 and l2 norms for different values of α?

- Concept: Differential Privacy and DP-SGD mechanics
  - Why needed here: Understanding how DP-SGD adds Gaussian noise to clipped gradients is crucial for interpreting why the performance drop occurs even without clipping (Noisy-SGD)
  - Quick check question: In DP-SGD, why does increasing the batch size help reduce the effective noise magnitude while maintaining privacy guarantees?

## Architecture Onboarding

- Component map: Theoretical analysis (SDEs) -> Linear least squares experiments -> Diagonal linear network analysis -> ImageNet validation -> Noise robustness conclusions
- Critical path: Theoretical analysis → Empirical validation → Noise robustness conclusions → Practical implications for DP-SGD
- Design tradeoffs: Balancing theoretical rigor (continuous SDEs) with empirical relevance (ImageNet experiments), and between simplicity (linear models) and complexity (DLNs)
- Failure signatures: If added Gaussian noise completely destroys implicit bias, or if Noisy-SGD shows no performance degradation with large batches
- First 3 experiments:
  1. Implement Noisy-SGD on ImageNet with varying batch sizes and compare to standard SGD to verify the implicit bias persists
  2. Run DLN experiments with different noise levels (σ) to quantify the trade-off between implicit bias enhancement and perturbation
  3. Analyze the convergence behavior of SDEs for linear least squares with and without additional noise to verify theoretical predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the implicit bias of Noisy-SGD compare to DP-SGD in large-batch settings, considering the clipping operation in DP-SGD?
- Basis in paper: [explicit] The paper discusses that Noisy-SGD only aligns with DP-SGD if gradients are bounded, which is not the case for classical neural networks or DLNs
- Why unresolved: The theoretical analysis focuses on Noisy-SGD as a proxy for DP-SGD, but the impact of gradient clipping in DP-SGD is not fully explored
- What evidence would resolve it: Experimental comparison of Noisy-SGD and DP-SGD on large-batch training of DNNs with bounded and unbounded gradients, measuring the implicit bias and performance

### Open Question 2
- Question: What is the optimal noise level (σ) for Noisy-SGD to achieve the best implicit bias in DLNs?
- Basis in paper: [inferred] The paper discusses a trade-off between noise level and implicit bias in DLNs, where increasing σ can lead to sparser solutions but also increases the distance between the obtained solution and the minimizer of the potential
- Why unresolved: The paper shows empirical results for specific values of σ, but does not determine the optimal noise level
- What evidence would resolve it: A systematic study varying σ in Noisy-SGD training of DLNs, measuring the sparsity of the obtained solutions and the distance from the minimizer of the potential, to find the optimal σ

### Open Question 3
- Question: How does the implicit bias of Noisy-SGD change with different network architectures and loss functions?
- Basis in paper: [inferred] The paper analyzes the implicit bias of Noisy-SGD for Linear Least Squares and DLNs, but does not explore other architectures or loss functions
- Why unresolved: The analysis is limited to specific settings, and it is unclear how the results generalize to other scenarios
- What evidence would resolve it: Experimental and theoretical analysis of Noisy-SGD's implicit bias for various network architectures (e.g., convolutional networks, transformers) and loss functions (e.g., cross-entropy, hinge loss), comparing the results to those obtained in the paper

## Limitations

- The analysis relies on continuous SDE approximations that may not fully capture discrete SGD dynamics
- Experiments focus on relatively simple architectures (linear models, DLNs) and one real-world dataset (ImageNet)
- The threshold where added Gaussian noise overwhelms structured SGD noise remains unclear

## Confidence

- **High Confidence**: The core claim that stochasticity (not clipping) drives the implicit bias in DP-SGD, supported by direct ablation experiments showing Noisy-SGD exhibits the same large-batch degradation as DP-SGD
- **Medium Confidence**: The theoretical analysis of implicit bias amplification in DLNs, which relies on continuous approximations that may not fully capture discrete dynamics, though empirical validation provides supporting evidence
- **Medium Confidence**: The claim about gradient noise geometry robustness, based on theoretical analysis of linear least squares but limited empirical validation beyond simple settings

## Next Checks

1. Test Noisy-SGD with various noise magnitudes σ to identify the threshold where implicit bias breaks down, providing empirical bounds on noise geometry robustness
2. Extend the analysis to deeper neural networks beyond DLNs (e.g., ResNets with multiple layers) to validate whether the implicit bias amplification mechanism generalizes
3. Compare continuous SDE predictions with discrete SGD trajectories in the linear least squares setting to quantify the approximation error and validate the theoretical framework