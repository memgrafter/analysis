---
ver: rpa2
title: 'Disentangling Rich Dynamics from Feature Learning: A Framework for Independent
  Measurements'
arxiv_id: '2410.04264'
source_url: https://arxiv.org/abs/2410.04264
tags:
- feature
- learning
- training
- features
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to analyze feature learning in deep
  neural networks by decomposing them into a forward feature map and a final linear
  layer. The key innovation is measuring feature learning independently of performance
  by tracking changes in the eigenvalues and eigenfunctions of the forward feature
  map during training.
---

# Disentangling Rich Dynamics from Feature Learning: A Framework for Independent Measurements

## Quick Facts
- arXiv ID: 2410.04264
- Source URL: https://arxiv.org/abs/2410.04264
- Reference count: 40
- Primary result: Introduces a method to measure feature learning independently of performance by tracking eigenvalue and eigenfunction changes in the forward feature map during training.

## Executive Summary
This paper presents a novel framework for analyzing feature learning in deep neural networks by decomposing them into a forward feature map and a final linear layer. The key innovation is the ability to measure feature learning independently of performance by tracking how the eigenvalues and eigenfunctions of the feature map change during training. The authors introduce interpretable metrics like quality and utility to quantify how features align with target functions and learned representations. They identify three distinct regimes of feature learning - coefficient learning, minimal feature (MF), and extended feature (EF) - and demonstrate that the MF regime correlates with better generalization, though not guaranteed.

## Method Summary
The method decomposes a DNN into a forward feature map Φ and a final linear layer, then analyzes feature learning by computing eigenvalues and eigenfunctions of the feature map with respect to the gradient descent operator. The integral operator T[f](x') is constructed from Φ, and its eigenfunctions e_k and eigenvalues ρ_k capture natural data directions in feature space. Changes in these during training indicate feature learning. The authors introduce measures like cumulative quality and utility to quantify feature alignment with target and learned functions, distinguishing between regimes where learning occurs in coefficients only versus actual feature learning.

## Key Results
- Demonstrates three distinct regimes: coefficient learning (no feature learning), minimal feature (MF) regime (few features needed), and extended feature (EF) regime (many features used)
- Shows MF regime correlates with better generalization but isn't guaranteed - random labeling can also produce MF regimes
- Reveals how training set size, learning rate, and batch normalization affect feature learning dynamics
- Extends framework successfully to regression tasks beyond classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Measures feature learning by diagonalizing the forward feature map with respect to the gradient descent operator and tracking changes in eigenfunctions and eigenvalues during training.
- Core assumption: Eigenfunctions of the integral operator T provide a stable basis for analyzing feature learning, and their evolution under training reflects meaningful changes in the feature map.
- Evidence: [abstract] states "We diagonalizeΦ with respect to the gradient descent operator and track feature learning by measuring how the eigenfunctions and eigenvalues ofΦ change during training."
- Break condition: If feature map changes don't affect eigenfunctions meaningfully, or if T doesn't represent training dynamics well.

### Mechanism 2
- Claim: Quality (Q*_k) and utility (ˆQ_k) measures quantify how well features align with target and learned functions.
- Core assumption: Inner products between eigenfunctions and target/learned functions accurately reflect alignment and utilization.
- Evidence: [abstract] introduces "measures that quantify the rich regime independently of performance" and [section 2.3] provides formal definitions.
- Break condition: If target or learned functions aren't well-represented by eigenfunctions, or inner products don't capture alignment effectively.

### Mechanism 3
- Claim: Effective dimension measure (Def_f) quantifies effective number of features needed to express target or learned function.
- Core assumption: Entropy of eigenvalue or quality distribution accurately reflects effective number of features.
- Evidence: [section 2.4] defines effective dimension measure, [section 3.1] uses it to measure features needed for target function.
- Break condition: If entropy distribution doesn't accurately reflect effective features, or distribution isn't well-behaved.

## Foundational Learning

- Concept: Eigenfunctions and eigenvalues of integral operators
  - Why needed here: Required for diagonalizing the forward feature map with respect to the gradient descent operator
  - Quick check question: Can you explain how the integral operator T is constructed from the feature map Φ, and what its eigenfunctions and eigenvalues represent?

- Concept: Gradient descent dynamics
  - Why needed here: Essential for understanding how eigenfunctions and eigenvalues change during training
  - Quick check question: Can you explain how gradient descent updates parameters of a linear model, and how this relates to dynamics of eigenfunctions and eigenvalues?

- Concept: Kernel methods and feature maps
  - Why needed here: Required for using kernel methods to analyze the feature map
  - Quick check question: Can you explain how a kernel function represents a feature map, and how this relates to the integral operator T?

## Architecture Onboarding

- Component map: Input data -> Forward feature map Φ (post-activations of penultimate layer) -> Final linear layer (classifies data)
- Critical path: Calculate eigenfunctions and eigenvalues of forward feature map, then track changes during training to analyze feature learning
- Design tradeoffs: Computationally efficient Nyström method approximation introduces some error; choice of loss function (MSE) and optimization (SGD) affects results
- Failure signatures: If feature map doesn't change significantly during training, or eigenfunctions/eigenvalues don't capture meaningful changes, method may fail to detect feature learning
- First 3 experiments:
  1. Train simple FCN on 1D function and visualize eigenfunctions/eigenvalues vs epoch
  2. Compare feature learning measures for CNN on MNIST with frozen features vs full SGD
  3. Analyze effect of training set size on feature learning for ResNet18 on CIFAR10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the causal relationship between the minimal feature (MF) regime and improved generalization performance?
- Basis in paper: [explicit] Authors observe MF regime typically correlates with better generalization but acknowledge MF can occur for poor generalization (e.g., random labeling)
- Why unresolved: While empirical evidence shows correlation, no causal link is established; authors suggest feature quality measured by Π*(k) is more critical than minimal number
- What evidence would resolve it: Controlled experiments varying feature learning independently (e.g., adversarial training) and measuring direct impact on generalization

### Open Question 2
- Question: How do feature learning measures behave for intermediate layers in deep neural networks?
- Basis in paper: [explicit] Authors acknowledge analyzing only final layer features is insufficient and provide preliminary examples for intermediate layers
- Why unresolved: Initial experiments suggest intermediate layer features can be strongly aligned to target but may not lead to better performance; further analysis left to future work
- What evidence would resolve it: Systematic analysis of intermediate layer features across different architectures and datasets

### Open Question 3
- Question: How does the proposed framework extend to tasks beyond classification, such as regression or reinforcement learning?
- Basis in paper: [explicit] Authors demonstrate extension to regression tasks and mention potential for reinforcement learning extension
- Why unresolved: While proof-of-concept for regression is provided, behavior in other domains and required adaptations are unexplored
- What evidence would resolve it: Application to diverse tasks (e.g., object detection, NLP) and analysis of resulting feature learning dynamics

## Limitations

- Empirical validation limited to specific architectures (FCNs, CNNs, ResNets) and datasets, limiting generalizability claims
- Computational cost of eigenfunction approximation using Nyström method for very large datasets is not discussed
- Sensitivity of measures to hyperparameter choices (learning rate, batch size) needs more systematic exploration

## Confidence

- High confidence in mathematical framework and decomposition methodology
- Medium confidence in empirical findings across different architectures
- Medium confidence in generalization claims, particularly correlation between MF regimes and generalization

## Next Checks

1. Test framework on more diverse architectures (Transformers, Graph Neural Networks) and datasets to verify generalizability
2. Conduct ablation studies on hyperparameter sensitivity, particularly examining how learning rate and batch size affect feature learning measures
3. Compare computational efficiency and approximation accuracy of Nyström method against exact eigendecomposition for smaller-scale problems to establish error bounds