---
ver: rpa2
title: 'Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem
  Solving'
arxiv_id: '2405.12205'
source_url: https://arxiv.org/abs/2405.12205
tags:
- skill
- skills
- question
- math
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores whether LLMs possess metacognitive knowledge\u2014\
  intuitive understanding of their own reasoning processes\u2014specifically in mathematical\
  \ problem solving. The authors develop a method to extract such knowledge by having\
  \ a strong LLM (GPT-4) assign fine-grained skill labels to math questions, cluster\
  \ them into broader interpretable skills, and build a skill exemplar repository."
---

# Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving

## Quick Facts
- **arXiv ID**: 2405.12205
- **Source URL**: https://arxiv.org/abs/2405.12205
- **Reference count**: 40
- **Primary result**: Skill-based prompting using GPT-4-extracted metacognitive knowledge improves LLM mathematical reasoning by +1.3% on GSM8K and +11.6% on MATH datasets

## Executive Summary
This paper investigates whether LLMs possess metacognitive knowledge—intuitive understanding of their own reasoning processes—specifically in mathematical problem solving. The authors develop a method to extract such knowledge by having a strong LLM (GPT-4) assign fine-grained skill labels to math questions, cluster them into broader interpretable skills, and build a skill exemplar repository. When solving new questions, LLMs are guided to select relevant skill labels and use corresponding exemplars as in-context examples. This skill-based prompting significantly improves accuracy on GSM8K (+1.3%) and MATH (+11.6%) datasets compared to standard Chain-of-Thought baselines, and also boosts weaker LLMs like Mixtral. The approach is general and transferable across datasets, showing that metacognitive knowledge extracted from powerful models can enhance reasoning in others.

## Method Summary
The method consists of three stages: First, GPT-4 labels training questions with fine-grained skill names and reasons. Second, GPT-4 clusters these fine-grained skills into interpretable coarse skills (22 for GSM8K, 117 for MATH). Third, a skill exemplar repository is built containing questions and answers for each coarse skill. During inference, the target LLM selects the relevant skill from the repository and uses corresponding exemplars as in-context examples to solve new questions. This approach can be seamlessly integrated with existing prompting strategies like Chain of Thought and self-consistency.

## Key Results
- Skill-based prompting improves GSM8K accuracy by +1.3% and MATH accuracy by +11.6% over Chain-of-Thought baselines
- The approach also boosts weaker LLMs like Mixtral, demonstrating transferability across model capabilities
- Skill exemplars transfer effectively across datasets, improving performance on SVAMP, ASDIV, and MAWPS beyond just GSM8K
- The method is domain-agnostic and can be integrated with numerous existing prompting strategies

## Why This Works (Mechanism)

### Mechanism 1
Skill exemplars selected by a strong LLM (GPT-4) improve in-context learning for both strong and weaker LLMs. GPT-4 extracts fine-grained skills from training data, clusters them into interpretable coarse skills, and assigns skill exemplars. When solving new problems, the target LLM selects the relevant skill and receives exemplars from that skill as in-context examples, guiding reasoning. This works because the LLM being tested can interpret the skill labels and exemplars provided by GPT-4 and apply them to the new problem.

### Mechanism 2
Fine-grained skill labels, when clustered into interpretable coarse skills, are more effective than broad topic labels for in-context learning. GPT-4 initially labels questions with very specific skills (e.g., "understanding of triangle properties and circle radius calculation"), then clusters similar skills into broader, more generally applicable categories (e.g., "understanding of triangles"). This reduces overfitting to specific question phrasing while retaining semantic relevance, making the skill exemplars more broadly useful.

### Mechanism 3
Skill exemplars transfer effectively across different LLMs and even different but related math datasets. Because the skill labels are derived from the LLM's own reasoning processes, they are interpretable to other LLMs. The exemplars act as semantic bridges, allowing a skill repository from one dataset (e.g., GSM8K) to improve performance on another (e.g., SVAMP, ASDIV). This transfer works because the underlying mathematical reasoning processes are similar enough across LLMs and datasets for skill exemplars to be transferable.

## Foundational Learning

- **Concept**: Skill clustering and semantic generalization
  - Why needed here: Raw skill labels from GPT-4 are too fine-grained (e.g., 5000 for MATH), leading to overfitting and poor generalization. Clustering them into interpretable coarse skills makes them practically useful.
  - Quick check question: Why does clustering skills into broader categories improve the utility of skill exemplars for in-context learning?

- **Concept**: In-context learning and few-shot prompting
  - Why needed here: The method relies on providing relevant exemplars as in-context examples to guide the LLM's reasoning. Understanding how few-shot prompting works is essential to grasp why skill-based exemplars are effective.
  - Quick check question: How does providing skill-relevant exemplars as in-context examples differ from standard few-shot prompting, and why is it more effective?

- **Concept**: Metacognition in LLMs
  - Why needed here: The core idea is that LLMs have metacognitive knowledge (understanding of their own reasoning processes) that can be extracted and used. Understanding what metacognition means in the context of LLMs is crucial.
  - Quick check question: What evidence does the paper provide that LLMs possess metacognitive knowledge, and how is this knowledge extracted?

## Architecture Onboarding

- **Component map**: Skill Labeling Phase -> Skill Clustering Phase -> Skill Exemplar Repository -> Inference Phase -> Transfer Mechanism
- **Critical path**:
  1. Label training data with fine-grained skills (GPT-4)
  2. Cluster skills into interpretable coarse categories (GPT-4)
  3. Build skill exemplar repository from clustered skills
  4. For new question: target LLM selects relevant skill
  5. Retrieve and provide exemplars for that skill as in-context examples
  6. Target LLM solves the question using exemplars
- **Design tradeoffs**:
  - Fine-grained vs. coarse skills: Fine-grained skills are more specific but risk overfitting; coarse skills are more generalizable but may lose specificity
  - Skill labeling model: Using GPT-4 gives high-quality labels but is costly; weaker models may not capture metacognitive knowledge as well
  - Transfer scope: Transferring across datasets is powerful but may degrade if source and target are too different
- **Failure signatures**:
  - No improvement over baseline: Skill labels may not be interpretable to target LLM, or exemplars may not be relevant
  - Worse performance than baseline: Clustering may be too coarse, losing specificity; exemplars may mislead reasoning
  - Inconsistent results across runs: Random selection of exemplars may introduce variance; skill selection may be noisy
- **First 3 experiments**:
  1. Skill labeling and clustering on GSM8K: Verify that GPT-4 can label questions with interpretable skills and cluster them into broad categories
  2. Skill-based prompting on GSM8K test set: Compare accuracy of target LLM using skill exemplars vs. standard CoT prompting
  3. Transfer to MATH dataset: Use GSM8K skill repository to improve MATH problem solving, validating cross-dataset transfer

## Open Questions the Paper Calls Out

### Open Question 1
Can the skill discovery methodology be extended to create hierarchical skill structures where multiple skills are assigned to each question? The current methodology assigns only one named skill to each math question for simplicity and to allow simple clustering. This limitation prevents capturing the full complexity of how skills interact in problem-solving. Mathematical problems often require a combination of a primary skill and various secondary skills, suggesting the need for a more advanced approach using an LLM to create hierarchies of skills.

### Open Question 2
How transferable are skills across different mathematical domains and problem types? While the paper shows transferability from GSM8K to other math word problem datasets (SVAMP, ASDIV, MAWPS suite), the extent to which this transfer works for datasets with significantly different problem types or domains remains unclear. The methodology is described as "domain-agnostic" but this claim remains untested outside mathematical problem-solving.

### Open Question 3
Can the skill exemplar repository be used to improve models through fine-tuning rather than just in-context learning? All current experiments focus on in-context learning using the skill exemplar repository. The paper suggests that using skills to fine-tune GPT-4 "may raise its capabilities" but provides no experimental evidence. The future goal is to extend these methodologies to improve all models through fine-tuning processes.

## Limitations

- The method's effectiveness may vary with different LLM architectures or less capable models, as the study relies heavily on GPT-4's ability to accurately identify and cluster metacognitive skills
- While skill transfer across datasets is demonstrated, the extent to which this transfer works for datasets with significantly different problem types or domains remains unclear
- The paper doesn't explore how sensitive the skill clustering is to the choice of base LLM, which could affect generalizability

## Confidence

- **High Confidence**: The improvement in accuracy for GSM8K (+1.3%) and MATH (+11.6%) datasets using skill-based prompting compared to CoT baselines is well-supported by experimental results
- **Medium Confidence**: The claim that skill exemplars improve weaker LLMs like Mixtral is plausible but relies on the assumption that skill labels are interpretable across different LLM architectures, which isn't thoroughly validated
- **Medium Confidence**: The assertion that the method is domain-agnostic is based on results from math datasets; broader domain validation is needed to confirm this claim

## Next Checks

1. **Architecture Sensitivity Test**: Evaluate the skill labeling and clustering process using different LLM architectures (e.g., Claude, Llama) to assess robustness and generalizability
2. **Cross-Domain Transfer**: Apply the skill exemplar repository to non-math domains (e.g., logical reasoning, code generation) to test domain-agnostic claims
3. **Skill Clustering Variability**: Analyze how different clustering thresholds or algorithms affect the quality and interpretability of coarse skills, and their impact on downstream performance