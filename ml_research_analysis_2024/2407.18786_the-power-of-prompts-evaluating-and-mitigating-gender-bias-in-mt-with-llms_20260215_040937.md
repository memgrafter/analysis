---
ver: rpa2
title: 'The power of Prompts: Evaluating and Mitigating Gender Bias in MT with LLMs'
arxiv_id: '2407.18786'
source_url: https://arxiv.org/abs/2407.18786
tags:
- english
- gender
- inst
- catalan
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates gender bias in machine translation using
  Large Language Models (LLMs) compared to Neural Machine Translation (NMT) models
  for English to Catalan and Spanish translation. Base LLMs show more gender bias
  than NMT models across all tested datasets.
---

# The power of Prompts: Evaluating and Mitigating Gender Bias in MT with LLMs

## Quick Facts
- arXiv ID: 2407.18786
- Source URL: https://arxiv.org/abs/2407.18786
- Reference count: 40
- Base LLMs exhibit more gender bias than NMT models; simplified chain-of-thought prompting reduces bias by up to 12%

## Executive Summary
This paper investigates gender bias in machine translation comparing Large Language Models (LLMs) with Neural Machine Translation (NMT) models for English to Catalan and Spanish. Base LLMs consistently show more gender bias than NMT models across all tested datasets. The study explores prompt engineering techniques to mitigate this bias in instruction-tuned LLMs, finding that a simplified chain-of-thought prompt with anti-stereotypical examples and increased female representation achieved up to 12% reduction in gender bias on the WinoMT dataset compared to baseline prompts.

## Method Summary
The study evaluates gender bias in LLMs and NMT models using 5-shot prompting for LLMs with various prompt templates on four test sets (FLoRes-200, WinoMT, Gold BUG, MuST-SHE). The researchers tested different prompt structures including baseline prompts, detailed chain-of-thought instructions, and simplified schematic prompts with anti-stereotypical examples. They measured translation quality using BLEU and COMET metrics, and gender bias using Gender Accuracy, F1-male, F1-female, ∆G, and ∆S metrics. The most effective prompt combined simplified chain-of-thought structure with 5 anti-stereotypical examples showing female engineers and male nurses.

## Key Results
- Base LLMs show higher gender bias than NMT models across all tested datasets
- Simplified chain-of-thought prompting with anti-stereotypical examples reduces gender bias by up to 12% on WinoMT
- The bias reduction comes at a slight cost to overall translation quality (COMET decrease of 0.13-0.18 points)

## Why This Works (Mechanism)

### Mechanism 1
- Chain-of-thought prompting forces explicit gender resolution reasoning in LLMs
- Core assumption: LLMs can follow multi-step reasoning instructions when properly prompted
- Evidence anchors: WinoMT results show 12% bias reduction with chain-of-thought prompts
- Break condition: If model cannot parse multi-step instructions

### Mechanism 2
- Anti-stereotypical examples in few-shot prompting recalibrate model's gender priors
- Core assumption: Few-shot learning can override pre-existing gender stereotypes
- Evidence anchors: Performance correlates with gender stereotypes, better for anti-stereotypical content
- Break condition: If model's pre-training exposure is too strong to be overridden

### Mechanism 3
- Simplified schematic instructions work better than detailed procedural descriptions
- Core assumption: LLMs respond better to pattern-based guidance than explicit instruction lists
- Evidence anchors: Llama-2-7B-chat comprehends schematic prompts better than elaborate ones
- Break condition: If model requires more explicit guidance for complex reasoning

## Foundational Learning

- Concept: Winograd schema gender resolution
  - Why needed here: Core evaluation metric for gender bias requires understanding pronoun resolution
  - Quick check question: How does a Winograd schema differ from regular coreference resolution in terms of gender ambiguity?

- Concept: Few-shot prompting mechanics
  - Why needed here: The prompting strategy relies on providing examples to guide translation behavior
  - Quick check question: What is the minimum number of examples typically needed for effective few-shot learning in LLMs?

- Concept: Chain-of-thought prompting structure
  - Why needed here: The most effective bias mitigation uses this reasoning approach
  - Quick check question: What are the key differences between standard prompting and chain-of-thought prompting?

## Architecture Onboarding

- Component map: Prompt template with instruction header -> 5-shot examples -> schematic reasoning steps -> Translation input/output pipeline -> Evaluation framework

- Critical path: 1) Construct prompt with anti-stereotypical Winograd examples 2) Apply schematic chain-of-thought structure 3) Process input sentence through LLM 4) Extract and evaluate translation for gender accuracy 5) Calculate gender bias metrics

- Design tradeoffs: Translation quality vs gender bias reduction, Prompt complexity vs model comprehension, Number of examples vs prompt length limits

- Failure signatures: Gender bias scores not improving despite prompt changes, Translation quality dropping significantly, Model ignoring schematic steps and generating defaults

- First 3 experiments: 1) Test baseline prompt without chain-of-thought to establish control metrics 2) Implement full schematic chain-of-thought with anti-stereotypical examples 3) Vary example gender ratios to find optimal balance between quality and bias reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the chain-of-thought prompting approach work for gender bias mitigation in LLMs across languages beyond English → Catalan and English → Spanish?
- Basis in paper: The authors note their findings are specific to En → Ca and En → Es translation directions and suggest testing the prompt's effectiveness in other language pairs.
- Why unresolved: The paper only evaluates the top-performing prompt on two language pairs.
- What evidence would resolve it: Applying the same chain-of-thought prompting approach to translation tasks between English and other gendered languages and measuring gender bias reduction.

### Open Question 2
- Question: What is the mechanism by which chain-of-thought prompting reduces gender bias in LLMs?
- Basis in paper: The authors observe that chain-of-thought prompting significantly reduces gender bias but do not provide a detailed explanation of why this approach is effective.
- Why unresolved: The paper demonstrates the effectiveness of chain-of-thought prompting but does not investigate the underlying cognitive or linguistic processes.
- What evidence would resolve it: Conducting controlled experiments comparing different chain-of-thought structures or analyzing model attention patterns.

### Open Question 3
- Question: Does the reduction in gender bias through prompting come at the cost of reduced accuracy in other aspects of translation quality beyond BLEU and COMET scores?
- Basis in paper: The authors note that using the gender-bias mitigating prompt leads to a slight loss in overall translation quality, but only measure this through BLEU and COMET metrics.
- Why unresolved: The paper acknowledges a trade-off between bias reduction and translation quality but does not explore what other aspects might be affected.
- What evidence would resolve it: Conducting human evaluations of translations for fluency, adequacy, and other quality metrics.

## Limitations
- Results are specific to English→Catalan and English→Spanish translation pairs
- The simplified schematic prompts reduce translation quality (COMET decrease of 0.13-0.18 points)
- Performance variation across test sets suggests mitigation strategy may not generalize well to all translation scenarios

## Confidence
- **High confidence**: Base LLM gender bias is consistently higher than NMT models across all tested datasets
- **Medium confidence**: The simplified chain-of-thought prompt reduces gender bias by up to 12% on WinoMT
- **Low confidence**: The anti-stereotypical examples reliably recalibrate gender priors in few-shot learning

## Next Checks
1. Test the prompt engineering approach across additional language pairs beyond English→Catalan/Spanish to assess cross-linguistic generalizability
2. Conduct ablation studies varying the number of anti-stereotypical examples (1-10 shots) to determine optimal few-shot learning parameters
3. Measure long-term performance by evaluating model behavior on sentences with mixed stereotypical and anti-stereotypical content to test robustness of bias mitigation