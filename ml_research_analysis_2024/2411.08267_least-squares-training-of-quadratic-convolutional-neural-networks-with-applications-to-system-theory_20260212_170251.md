---
ver: rpa2
title: Least Squares Training of Quadratic Convolutional Neural Networks with Applications
  to System Theory
arxiv_id: '2411.08267'
source_url: https://arxiv.org/abs/2411.08267
tags:
- training
- network
- squares
- networks
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a least squares formulation for training a
  2-layer quadratic convolutional neural network (CQNN), which provides an analytic
  expression for globally optimal weights and a quadratic input-output equation. The
  method transforms the CQNN into an equivalent quadratic neural network (QNN) problem,
  drastically reducing the number of learned weights compared to traditional formulations.
---

# Least Squares Training of Quadratic Convolutional Neural Networks with Applications to System Theory

## Quick Facts
- arXiv ID: 2411.08267
- Source URL: https://arxiv.org/abs/2411.08267
- Authors: Zachary Yetman Van Egmond; Luis Rodrigues
- Reference count: 15
- Key outcome: Least squares formulation for 2-layer CQNNs achieves global optimality, reduces training time by 99% while maintaining accuracy

## Executive Summary
This paper presents a least squares formulation for training 2-layer quadratic convolutional neural networks (CQNNs) with quadratic activation functions. The method transforms the CQNN into an equivalent quadratic neural network (QNN) problem, enabling an analytic expression for globally optimal weights and a quadratic input-output equation. Applied to system identification and GPS signal emulation problems, the proposed LS-CQNN achieved significantly reduced training times (0.019s vs 9.092s for back-propagation CNN) while maintaining minimal compromises on prediction accuracy. The analytic quadratic model enables further analysis such as sensitivity of outputs to input perturbations, making it particularly valuable for safety-critical systems.

## Method Summary
The method involves transforming a 2-layer CQNN with quadratic activation functions into an equivalent QNN problem that can be solved analytically via least squares. The transformation exploits the overlapping patch structure of convolutional operations to create sparse weight matrices, drastically reducing the number of learned weights compared to traditional formulations. The training reduces to solving a convex optimization problem under specific conditions (a > 0, c > 0, b² - 4ac ≥ 0), yielding globally optimal weights and an analytic input-output equation. The approach was validated on system identification (robot arm data) and GPS signal emulation (drone trajectory data) problems, comparing performance against back-propagation CNN baselines.

## Key Results
- Training time reduced by 99% (0.019s vs 9.092s) for system identification while maintaining MSE performance
- Analytic quadratic input-output equation enables sensitivity analysis for safety-critical applications
- Weight reduction from structured transformation eliminates redundant parameters in convolutional operations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The least squares formulation achieves global optimality for 2-layer CQNNs under quadratic activation functions.
- Mechanism: By transforming the CQNN into an equivalent QNN problem with constrained structure, the training reduces to a convex optimization problem that can be solved analytically via least squares.
- Core assumption: The quadratic activation function satisfies the conditions a > 0, c > 0, and b² - 4ac ≥ 0, and the network has exactly 2 layers.
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If the network has more than 2 layers, or if the quadratic activation parameters don't satisfy the required conditions, the global optimality guarantee no longer holds.

### Mechanism 2
- Claim: The structured transformation reduces the number of learned weights compared to the original CQNN formulation.
- Mechanism: The transformation exploits the overlapping patch structure of convolutional operations, creating a sparse pattern in the weight matrices that eliminates redundant parameters.
- Core assumption: The convolutional operation with stride 1 creates predictable overlap patterns that can be captured in the equivalent QNN formulation.
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If stride > 1 or non-sequential patch extraction is used, the weight reduction benefit diminishes.

### Mechanism 3
- Claim: The analytic input-output equation enables sensitivity analysis for safety-critical applications.
- Mechanism: The quadratic nature of the network output (y = xᵀ aZ̄₁x + bZ̄₂x + cZ̄₄) allows direct computation of gradients with respect to inputs.
- Core assumption: The analytic form preserves all necessary information about the network's behavior for derivative computation.
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If the network needs to handle discontinuous or highly nonlinear phenomena beyond quadratic approximation, the sensitivity analysis becomes less meaningful.

## Foundational Learning

- Concept: Convex optimization and duality theory
  - Why needed here: The proof that the non-convex CQNN training problem can be transformed into a convex problem relies on understanding duality and constraint relaxation techniques
  - Quick check question: What conditions must hold for a non-convex problem to share global optima with its convex relaxation?

- Concept: Convolutional neural network architecture and patch extraction
  - Why needed here: Understanding how convolutional layers extract overlapping patches is crucial for the transformation that reduces the weight count
  - Quick check question: How does the number of unique patches relate to input length and filter size in a standard CNN?

- Concept: Least squares solution and regularization
  - Why needed here: The final training step solves a least squares problem, requiring knowledge of both the analytical solution and how regularization affects it
  - Quick check question: How does adding L2 regularization change the closed-form solution of a least squares problem?

## Architecture Onboarding

- Component map: Input preprocessing -> Patch extraction -> Matrix construction (H, y) -> Least squares solve -> Weight extraction -> Sensitivity analysis

- Critical path: Data → Patch extraction → Matrix construction (H, y) → Least squares solve → Weight extraction → Sensitivity analysis

- Design tradeoffs:
  - Filter size vs. weight reduction: Larger filters create more sparse patterns but increase individual matrix size
  - Regularization strength vs. overfitting: Too much regularization may underfit, too little may overfit
  - Input dimensionality vs. computational tractability: High-dimensional inputs make matrix operations expensive

- Failure signatures:
  - Poor training accuracy with zero regularization suggests the quadratic model is insufficient
  - Sensitivity analysis producing extreme values may indicate numerical instability in matrix inversion
  - Training time unexpectedly high despite LS formulation suggests implementation inefficiency in matrix construction

- First 3 experiments:
  1. Compare training times between LS-CQNN and BP CNN on a simple system identification problem with known quadratic dynamics
  2. Test sensitivity analysis by adding small perturbations to inputs and comparing predicted output changes with analytical gradients
  3. Vary filter size (f) and input length (n) to quantify the weight reduction benefit empirically across different problem scales

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the least squares formulation be extended to train deeper quadratic convolutional neural networks (CQNNs) beyond the 2-layer case?
- Basis in paper: [explicit] The paper explicitly states that "these results only apply to 2-layer networks" and motivates "the exploration of deeper quadratic networks in the context of system theory."
- Why unresolved: The paper only presents a method for 2-layer CQNNs and does not address how to extend the least squares approach to networks with more layers.
- What evidence would resolve it: A successful extension of the least squares formulation to train CQNNs with 3 or more layers, demonstrating reduced training times and maintaining analytic properties while achieving comparable or improved performance.

### Open Question 2
- Question: How does the performance of the least squares CQNN compare to other analytic neural network training methods that use regularization, such as the convex training method presented in [2]?
- Basis in paper: [explicit] The paper mentions that "the regularization is being applied differently in (4) and (23) they will only share a solution when β = 0" and presents a regularized least squares formulation.
- Why unresolved: While the paper compares the least squares CQNN to the convex training method from [2] without regularization, it does not explore the impact of regularization on the least squares formulation or compare it to regularized versions of other analytic methods.
- What evidence would resolve it: A comprehensive comparison of the least squares CQNN with various regularization terms to other analytic neural network training methods that use regularization, evaluating both training efficiency and prediction accuracy.

### Open Question 3
- Question: Can the analytic input-output equation of the least squares CQNN be used to derive formal stability guarantees for control applications, similar to the Lyapunov-based stability guarantees mentioned for QNNs in [5]?
- Basis in paper: [explicit] The paper states that the network provides "an analytic input-output relationship" which enables "further analysis" such as sensitivity to input perturbations, and mentions that QNNs have been used for "controller design with Lyapunov-based stability guarantees."
- Why unresolved: While the paper demonstrates the sensitivity analysis capability of the analytic equation, it does not explore whether this can be extended to derive formal stability guarantees for control systems.
- What evidence would resolve it: A demonstration of how the analytic input-output equation of the least squares CQNN can be used to derive Lyapunov-based stability guarantees for a specific control application, showing that the network's predictions remain stable under certain conditions.

## Limitations

- Restricted to exactly 2-layer networks with specific quadratic activation conditions
- Weight reduction benefit diminishes for higher-dimensional convolutions or non-sequential patch extraction
- Sensitivity analysis assumes quadratic model adequately captures system dynamics

## Confidence

- Global optimality guarantee for 2-layer CQNNs: **High**
- Weight reduction through structured transformation: **Medium-High**
- Sensitivity analysis utility for safety-critical systems: **Medium**

## Next Checks

1. Test the method on a 3-layer network to quantify the breakdown of global optimality guarantees
2. Evaluate weight reduction benefits across different stride values (1, 2, 3) and filter sizes to map the boundary conditions
3. Apply sensitivity analysis to a real safety-critical system with known nonlinear dynamics to assess prediction accuracy under perturbations