---
ver: rpa2
title: Are Large Vision Language Models up to the Challenge of Chart Comprehension
  and Reasoning? An Extensive Investigation into the Capabilities and Limitations
  of LVLMs
arxiv_id: '2406.00257'
source_url: https://arxiv.org/abs/2406.00257
tags:
- chart
- gpt-4v
- charts
- data
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive evaluation of large
  vision language models (LVLMs) for chart understanding and reasoning tasks. The
  authors assess both closed and open-source LVLMs (GPT-4V, Gemini, Claude-3, Phi-3)
  across five major chart reasoning tasks using seven benchmark datasets.
---

# Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning? An Extensive Investigation into the Capabilities and Limitations of LVLMs

## Quick Facts
- arXiv ID: 2406.00257
- Source URL: https://arxiv.org/abs/2406.00257
- Reference count: 40
- Primary result: Comprehensive evaluation of LVLMs across five chart reasoning tasks reveals strong abilities in high-level insights but common problems with hallucinations, factual errors, and data bias

## Executive Summary
This paper presents the first comprehensive evaluation of large vision language models for chart understanding and reasoning tasks. The authors assess both closed and open-source LVLMs across five major chart reasoning tasks using seven benchmark datasets. Their analysis reveals that while LVLMs demonstrate strong abilities in generating fluent texts with high-level data insights, they encounter common problems like hallucinations, factual errors, and data bias. GPT-4V generally performs best in discriminative tasks like factoid question answering and fact-checking, while Gemini excels in open-ended generation tasks. The study also finds that LVLMs struggle with extracting data from charts without explicit labels and have difficulty covering contextual and domain-specific information.

## Method Summary
The paper evaluates large vision language models (GPT-4V, Gemini, Claude-3, Phi-3) across seven benchmark datasets covering chart QA, chart-to-text, open-ended question answering, fact-checking, chart-to-table, and chart-to-text tasks. Experiments use zero-shot Chain-of-Thought prompting, Program-aided Language prompting, and qualitative analysis using hallucination detection frameworks and semantic coverage evaluation. The authors create a modified ChartQA* dataset to test performance on charts without explicit data labels, and employ both automatic metrics (BLEURT, CIDEr, F1) and human evaluation for comprehensive assessment.

## Key Results
- GPT-4V outperforms other models in discriminative tasks like factoid question answering and fact-checking
- Gemini excels in open-ended generation tasks, outperforming GPT-4V by 3% in zero-shot CoT setup
- All models experience significant performance drops on ChartQA* when data labels are absent
- LVLMs demonstrate strong semantic coverage for Levels 1-3 but struggle with Level 4 (contextual/domain-specific information)
- Entity and relation hallucinations are most frequent across all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LVLMs perform better on discriminative chart tasks when using Chain-of-Thought prompting
- Mechanism: CoT prompting encourages LVLMs to break down reasoning steps, improving accuracy on structured visual reasoning tasks
- Core assumption: The model can interpret the chart correctly in initial stages of reasoning
- Evidence anchors:
  - GPT-4V generally performs best in discriminative tasks like factoid question answering
  - Gemini outperformed GPT-4V by 3% in zero-shot CoT setup
- Break condition: If chart data cannot be extracted accurately from visual elements, CoT reasoning cannot recover correct answers

### Mechanism 2
- Claim: LVLMs generate more semantically rich chart summaries using four-level semantic frameworks
- Mechanism: The four-level framework provides structured categories for semantic depth, guiding models to include low-level details, statistics, trends, and domain context
- Core assumption: The model's training data includes examples of these semantic levels
- Evidence anchors:
  - GPT-4V produces longer summaries of chart-specific visual information while Gemini produces concise summaries with some statistical and domain-specific information
- Break condition: If the model lacks training examples covering all four semantic levels

### Mechanism 3
- Claim: LVLMs struggle with extracting data values from charts without explicit labels
- Mechanism: Without text labels showing exact data values, models must estimate values from visual elements relative to axis scales
- Core assumption: Visual interpretation accuracy depends on clear alignment between visual elements and axis labels
- Evidence anchors:
  - Performance drops drastically when data labels are not annotated
  - GPT-4V frequently declines to respond when data labels are absent
- Break condition: If chart design includes clear visual encoding that aids value estimation

## Foundational Learning

- **Visual encoding interpretation**: Mapping visual features to data semantics for accurate chart understanding
  - Why needed: LVLMs must interpret how colors, positions, sizes represent data
  - Quick check: Can you identify which visual channels encode which data dimensions in a bar chart?

- **Chain-of-Thought reasoning structure**: Breaking down chart reasoning into sequential steps
  - Why needed: Improves accuracy on complex visual questions
  - Quick check: How would you decompose "What's the median value of Japan graph from 2013 to 2015?" into reasoning steps?

- **Semantic content classification**: Using four-level framework for evaluating chart summaries
  - Why needed: Distinguishes between visual details, statistics, trends, and domain context
  - Quick check: Which semantic level does "The unemployment rate increased sharply from 3.3% in November 2019 to 15.7% in April 2020" belong to?

## Architecture Onboarding

- **Component map**: Chart image + task prompt → LVLM inference → Text generation → Evaluation (automatic metrics + human review) → Optional: Python code generation → Execution → Answer generation

- **Critical path**:
  1. Chart image preprocessing (format, resolution)
  2. Prompt construction (task-specific instructions)
  3. LVLM inference (closed-source API or local model)
  4. Response post-processing (formatting, error checking)
  5. Evaluation (BLEURT, CIDEr, F1, hallucination detection)

- **Design tradeoffs**:
  - Zero-shot vs. few-shot prompting: Zero-shot is faster but less accurate
  - Automatic vs. human evaluation: Automatic is scalable but misses nuanced errors
  - Closed-source vs. open-source models: Closed-source more accurate but expensive

- **Failure signatures**:
  - No response when data labels missing
  - Incorrect value extraction from visual elements
  - Hallucinations in entity and relation categories
  - Factual errors in trend descriptions
  - Failure to identify chart type or axis ranges

- **First 3 experiments**:
  1. Test zero-shot CoT vs. PAL prompting on ChartQA with labeled charts
  2. Evaluate semantic coverage using the four-level framework on Chart-to-Text summaries
  3. Measure performance drop on ChartQA* (charts without data labels)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LVLMs reliably extract and interpret data from charts without explicit data labels?
- Basis: The paper introduces ChartQA* to test LVLMs' ability to estimate data values from visual elements
- Why unresolved: Does not provide threshold for how much visual detail is necessary for accurate data extraction
- What evidence would resolve it: Experimental results showing minimum visual information required for accurate data extraction

### Open Question 2
- Question: How can hallucinations and factual errors in LVLMs' chart comprehension be effectively detected and mitigated?
- Basis: Uses FaV A method to categorize hallucinations into entity, relation, subjective, contradictory, unverifiable, and invented types
- Why unresolved: Does not propose or test methods to reduce these errors
- What evidence would resolve it: Comparative analysis of hallucination rates before and after applying error detection techniques

### Open Question 3
- Question: Can LVLMs generate more contextually rich and domain-specific insights when summarizing charts?
- Basis: Evaluates ability to cover four levels of semantic content, finding struggle with Level 4 (contextual/domain-specific)
- Why unresolved: Does not investigate whether domain-specific training or retrieval-augmented generation could improve contextual insights
- What evidence would resolve it: Experimental results comparing summaries from domain-specific vs. general-purpose models

## Limitations

- Performance drops significantly when data labels are absent, limiting practical utility
- Hallucinations and factual errors occur frequently, particularly in entity and relation categories
- Difficulty covering contextual and domain-specific information (Level 4 semantic content)
- Human evaluation introduces potential subjectivity that may affect reproducibility

## Confidence

- **High confidence**: GPT-4V's superior performance in discriminative tasks is well-supported by quantitative metrics
- **Medium confidence**: Gemini's strength in open-ended generation tasks requires further validation across diverse chart types
- **Medium confidence**: Limitations around data extraction without labels are consistent across experiments
- **Low confidence**: Specific mechanisms behind performance differences between prompting approaches need more investigation

## Next Checks

1. **Cross-dataset generalization test**: Evaluate model performance on additional chart datasets (FigureQA, DVQA) to assess whether findings generalize beyond seven benchmark datasets

2. **Label-robustness analysis**: Systematically vary amount and quality of data labels to quantify exact threshold where performance degrades, and test whether visual encoding improvements can mitigate this limitation

3. **Semantic framework validation**: Conduct blind evaluation with multiple human annotators using the four-level semantic framework to measure inter-rater reliability and validate framework effectiveness