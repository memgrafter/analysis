---
ver: rpa2
title: Robust image classification with multi-modal large language models
arxiv_id: '2412.10353'
source_url: https://arxiv.org/abs/2412.10353
tags:
- multi-shield
- adversarial
- image
- robust
- rejection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Multi-Shield, a defense mechanism that combines
  adversarial training with multimodal information from large language models (specifically
  CLIP) to detect and reject adversarial examples in image classification. Multi-Shield
  operates by comparing predictions from an image classifier with alignment scores
  computed between input images and textual prompts representing class descriptions
  using CLIP's zero-shot capabilities.
---

# Robust image classification with multi-modal large language models

## Quick Facts
- arXiv ID: 2412.10353
- Source URL: https://arxiv.org/abs/2412.10353
- Reference count: 13
- Key outcome: Multi-Shield combines adversarial training with CLIP's multimodal capabilities to significantly improve robust accuracy against adversarial examples

## Executive Summary
Multi-Shield is a defense mechanism that leverages multimodal information from large language models (specifically CLIP) to detect and reject adversarial examples in image classification. The approach compares predictions from an image classifier with alignment scores computed between input images and textual prompts representing class descriptions using CLIP's zero-shot capabilities. When predictions disagree, Multi-Shield abstains from classification, indicating potential adversarial manipulation.

The method is evaluated on six diverse datasets using both robust and non-robust image classifiers, demonstrating substantial improvements in robust accuracy compared to baseline defenses. Under worst-case adaptive attacks where attackers have complete knowledge of the defense mechanism, Multi-Shield maintains significant improvements in robust accuracy across all tested models.

## Method Summary
Multi-Shield integrates an adversarially trained image classifier with CLIP's zero-shot multimodal capabilities to detect adversarial examples. The system computes alignment scores between input images and textual prompts representing class descriptions, comparing these with the image classifier's predictions. When predictions disagree, Multi-Shield abstains from classification. The rejection score is computed as the maximum difference between the alignment score of the predicted class and all other classes. The approach is evaluated using AutoAttack with perturbation size ε = 8/255 under three scenarios: baseline (no Multi-Shield), Multi-Shield (with detection), and Multi-Shield with adaptive attack.

## Key Results
- Multi-Shield raises robust accuracy from 0% to over 90% for non-robust models on CIFAR-10 and ImageNet
- Provides average improvements of 32% on CIFAR-10 and 65% on ImageNet compared to baseline defenses
- Maintains substantial improvements in robust accuracy (up to 50% on CIFAR-10) even under worst-case adaptive attacks

## Why This Works (Mechanism)
Multi-Shield exploits the complementary nature of visual and textual semantic information. Adversarial examples that fool image classifiers often exploit vulnerabilities in the visual feature space, but these perturbations may not be effective in the multimodal semantic space captured by CLIP's text-image alignment. By requiring agreement between predictions from two different modalities (visual-only classification and multimodal alignment), the defense creates a second barrier that adversarial examples must overcome. The rejection mechanism based on prediction disagreement provides a principled way to identify inputs where the two modalities disagree, which often indicates adversarial manipulation.

## Foundational Learning
- **Adversarial training**: Training models to be robust against adversarial examples by including perturbed inputs in the training process. Why needed: Provides baseline robustness that Multi-Shield builds upon. Quick check: Verify that robust models show higher baseline accuracy on adversarial examples compared to non-robust models.
- **Multimodal learning with CLIP**: Using pre-trained models that can compute similarity between images and text. Why needed: Provides semantic understanding that complements visual-only classification. Quick check: Test CLIP's zero-shot classification accuracy on clean validation data.
- **Zero-shot classification**: Performing classification without task-specific training using natural language prompts. Why needed: Enables flexible application across different datasets without retraining. Quick check: Verify that text prompts are correctly formatted and aligned with class names.
- **Adversarial attacks (AutoAttack)**: White-box attacks that combine multiple attack strategies to find adversarial examples. Why needed: Provides standardized evaluation of defense effectiveness. Quick check: Confirm that AutoAttack successfully generates adversarial examples for baseline models.
- **Robust accuracy metrics**: Evaluating model performance on adversarial examples, including cases where the model abstains. Why needed: Captures the trade-off between security and usability. Quick check: Calculate clean accuracy, robust accuracy, and rejection ratio for baseline models.
- **Adaptive attack scenarios**: Evaluating defenses against attackers who know the defense mechanism. Why needed: Provides worst-case security assessment. Quick check: Verify that adaptive attacks maintain higher success rates than random guessing.

## Architecture Onboarding

**Component map**: Image Classifier -> CLIP Zero-shot Model -> Multi-Shield Decision Module

**Critical path**: Input image → Image classifier prediction → CLIP alignment scores → Comparison module → Abstain or output prediction

**Design tradeoffs**: The defense trades some clean accuracy for improved robust accuracy by abstaining on uncertain inputs. The choice of text prompts affects performance - too generic prompts may lead to excessive rejections, while too specific prompts may miss subtle adversarial perturbations. The rejection threshold determines the balance between security and usability.

**Failure signatures**: 
- High rejection ratio on clean data indicates poor alignment between image classifier and CLIP predictions
- Low rejection ratio on adversarial data indicates the attack successfully fooled both modalities
- Significant drop in robust accuracy under adaptive attack suggests the defense can be overcome with sufficient attack sophistication

**First experiments to run**:
1. Evaluate clean accuracy and alignment scores on validation data to establish baseline performance
2. Test Multi-Shield's rejection behavior on adversarial examples generated by FGSM with ε = 8/255
3. Compare robust accuracy with and without Multi-Shield under AutoAttack with ε = 8/255

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on white-box adaptive attacks, with limited exploration of black-box attacks or attacks targeting the multimodal alignment mechanism specifically
- Effectiveness depends heavily on the quality and coverage of text prompts, which may vary across datasets and require careful prompt engineering
- Computational overhead introduced by CLIP inference during evaluation is not extensively discussed, raising questions about scalability to real-time applications

## Confidence
- High confidence in the core claim that Multi-Shield improves robust accuracy across multiple datasets and model architectures
- Medium confidence in the generalization of results to datasets beyond those tested, as the paper does not explore significantly different image domains
- Medium confidence in the claim that Multi-Shield maintains effectiveness under adaptive attacks, though the specific attack methodology could be further detailed

## Next Checks
1. Test Multi-Shield's performance on a dataset with significantly different characteristics (e.g., medical imaging or satellite imagery) to evaluate robustness to domain shifts
2. Evaluate the impact of different text prompt formulations on Multi-Shield's performance to identify optimal prompt engineering strategies
3. Investigate the computational overhead introduced by Multi-Shield during inference and its scalability to real-time applications