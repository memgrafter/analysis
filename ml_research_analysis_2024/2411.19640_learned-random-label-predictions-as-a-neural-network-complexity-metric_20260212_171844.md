---
ver: rpa2
title: Learned Random Label Predictions as a Neural Network Complexity Metric
arxiv_id: '2411.19640'
source_url: https://arxiv.org/abs/2411.19640
tags:
- random
- label
- learning
- memorization
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a multi-head network architecture that learns
  class labels and randomly generated labels in parallel to assess memorization and
  regularization in deep neural networks. By treating random label accuracy as an
  empirical measure of model complexity, the authors demonstrate that common regularizers
  like dropout, weight decay, and label smoothing effectively reduce memorization.
---

# Learned Random Label Predictions as a Neural Network Complexity Metric

## Quick Facts
- arXiv ID: 2411.19640
- Source URL: https://arxiv.org/abs/2411.19640
- Authors: Marlon Becker; Benjamin Risse
- Reference count: 18
- One-line primary result: Random label prediction accuracy serves as an empirical complexity metric that effectively measures memorization in deep neural networks.

## Executive Summary
This paper introduces a multi-head network architecture that learns class labels and randomly generated labels in parallel to assess memorization and regularization in deep neural networks. The authors demonstrate that common regularizers like dropout, weight decay, and label smoothing effectively reduce memorization as measured by random label accuracy. Additionally, the study challenges conventional understanding of feature extraction and classification transitions in CNNs, showing that sample-specific information persists deeper into convolutional layers than previously assumed. However, while the proposed regularization successfully reduces memorization, it does not improve generalization, contradicting classical learning theory predictions.

## Method Summary
The proposed method uses a multi-head network architecture where a feature extractor is shared between a main classification head and multiple random prediction heads (one per class). During training, the network predicts both true class labels and random labels in parallel. The accuracy of random label predictions serves as an empirical measure of model complexity and memorization. A novel regularization loss is introduced that encourages the feature extractor to reduce random label accuracy while maintaining class label accuracy. The model is trained on CIFAR100 using WideResNet-16-4 and VGG16 architectures with SGD, momentum 0.9, cosine learning rate scheduler, batch size 256, and 200 epochs.

## Key Results
- Random label accuracy effectively measures memorization in deep neural networks
- Common regularizers (dropout, weight decay, label smoothing) reduce memorization as measured by random label accuracy
- Sample-specific information persists deeper into convolutional layers than previously assumed
- Reducing memorization through regularization does not improve generalization, challenging classical learning theory

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning random labels in parallel with true labels enables empirical measurement of memorization and model complexity.
- Mechanism: The multi-head architecture generates predictions for all combinations of true and random labels. The accuracy of random label predictions reflects the amount of sample-specific information retained in the feature maps, which is a proxy for Rademacher complexity.
- Core assumption: Random label accuracy is a reliable empirical approximation of Rademacher complexity, and reducing it correlates with reduced model capacity.
- Evidence anchors:
  - [abstract]: "we introduce a multi-head network architecture... allows for the unlearning of random labels, preventing the network from memorizing individual samples"
  - [section]: "The accuracy of the random prediction heads can be used to define a complexity metric that serves as an empirical approximation to Rademacher complexity"
  - [corpus]: Weak or missing; corpus neighbors focus on label noise and memorization detection but not on random label metrics.
- Break condition: If the random label prediction heads fail to capture sample-specific information or if memorization shifts to earlier layers undetected, the metric becomes unreliable.

### Mechanism 2
- Claim: The proposed regularization loss effectively reduces memorization without harming class prediction performance.
- Mechanism: By selecting the random prediction head corresponding to the correct class and enforcing uniform distribution over random labels, the regularization encourages feature abstraction and prevents sample-specific overfitting.
- Core assumption: The feature extractor can be trained to reduce random label accuracy while maintaining class label accuracy through careful loss formulation.
- Evidence anchors:
  - [section]: "The regularization loss Lreg seeks to equalize the probabilities for all random labels while ensuring accurate class label predictions"
  - [abstract]: "we propose a novel regularizer that effectively reduces sample memorization"
  - [corpus]: Weak; corpus neighbors discuss memorization but not targeted regularization via random labels.
- Break condition: If the regularization loss becomes unstable or the feature extractor collapses to a trivial solution, memorization reduction fails.

### Mechanism 3
- Claim: Memorization and generalization are not directly causally linked, contrary to classical learning theory predictions.
- Mechanism: Despite reducing random label accuracy (and thus memorization) through regularization, no improvement in test accuracy is observed, suggesting memorization does not directly impair generalization.
- Core assumption: Rademacher complexity bounds on generalization are not tight for deep neural networks in practice.
- Evidence anchors:
  - [abstract]: "contrary to the predictions of classical statistical learning theory, we do not observe improvements in generalization"
  - [section]: "While our results show that our proposed regularization effectively reduces memorization, we do not observe improved generalization, raising questions about the direct causal relationship between memorization and generalization"
  - [corpus]: Weak; corpus neighbors focus on memorization detection and effects but not the memorization-generalization disconnect.
- Break condition: If future experiments show consistent generalization improvements when memorization is reduced, the causal disconnect hypothesis is invalidated.

## Foundational Learning

- Concept: Rademacher complexity
  - Why needed here: Provides theoretical grounding for using random label accuracy as a complexity metric and for expected generalization benefits from regularization.
  - Quick check question: How does Rademacher complexity relate to the ability of a model to fit random labels?

- Concept: PAC learning theory
  - Why needed here: Underlies the expectation that reducing model complexity (memorization) should improve generalization bounds.
  - Quick check question: What role do generalization bounds play in connecting memorization to generalization performance?

- Concept: Adversarial training and fair AI
  - Why needed here: Inspires the multi-head architecture and regularization loss design to suppress unwanted memorization without harming primary task performance.
  - Quick check question: How does adversarial loss formulation in fair AI translate to the proposed random label regularization?

## Architecture Onboarding

- Component map:
  - Feature extractor (base CNN) -> Main classification head (predicts true labels) -> Multiple random prediction heads (one per class, predict random labels) -> Loss functions (L_class, L_rnd, L_reg)

- Critical path:
  1. Extract features from input
  2. Predict true label probabilities
  3. Predict random label probabilities for each class head
  4. Compute and backpropagate all three losses
  5. Update feature extractor and heads jointly

- Design tradeoffs:
  - Multi-head structure allows class-preserving regularization but increases parameter count and memory usage
  - Random label regularization may slow convergence or require careful learning rate tuning
  - Metric only measures memorization at one layer depth; deeper shifts undetected

- Failure signatures:
  - Random label accuracy plateaus early or does not correlate with generalization
  - Training becomes unstable with high regularization strength
  - Memorization shifts to earlier layers without detection

- First 3 experiments:
  1. Train baseline WRN-16-4 on CIFAR-100 with random labels only; record random label accuracy vs. epochs
  2. Add random label heads to baseline; train with L_class only; compare random label accuracy to baseline
  3. Introduce L_reg with varying λ; measure random label accuracy reduction and test accuracy changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does memorization shift to earlier layers when regularization is applied, rather than being eliminated?
- Basis in paper: [inferred] The paper notes that the proposed architecture only measures memorization at the layer where additional prediction heads are attached, and potential shifts in memorization to earlier layers remain undetected.
- Why unresolved: The study's architecture design does not allow for monitoring of memorization across all layers simultaneously.
- What evidence would resolve it: Experiments using a multi-scale architecture that monitors memorization at multiple layers throughout the network would reveal whether memorization is being shifted to earlier layers or truly eliminated.

### Open Question 2
- Question: What is the causal relationship between memorization and generalization in deep neural networks?
- Basis in paper: [explicit] The paper explicitly states that the proposed regularization effectively reduces memorization but does not improve generalization, challenging the predictions of classical learning theory and raising questions about the direct causal relationship between memorization and generalization.
- Why unresolved: The experimental results contradict the theoretical expectations from PAC learning theory and Rademacher complexity bounds.
- What evidence would resolve it: Systematic experiments varying memorization levels across different datasets and network architectures while measuring generalization performance would help clarify whether memorization directly impacts generalization or if other factors are at play.

### Open Question 3
- Question: Does the proposed random label regularization technique have an implicit effect similar to label smoothing, or does it introduce a separate detrimental effect?
- Basis in paper: [explicit] The paper discusses how the proposed regularizer shows similar effects to label smoothing but also suggests a second detrimental effect that counteracts label smoothing at high regularization factors.
- Why unresolved: The experiments show that while the regularizer can improve performance without label smoothing, it degrades performance when optimal label smoothing is already applied, suggesting complex interactions between the two techniques.
- What evidence would resolve it: Controlled experiments isolating the effects of the regularizer and label smoothing, including ablation studies and analysis of the learned representations, would help distinguish between the implicit label smoothing-like effect and any separate detrimental effects.

## Limitations

- Random label accuracy as a proxy for Rademacher complexity lacks rigorous theoretical justification
- Multi-head architecture increases model complexity and parameter count
- Memorization reduction does not translate to generalization improvements, contradicting classical learning theory
- Regularization effectiveness appears sensitive to implementation details not fully specified

## Confidence

- **High**: The multi-head architecture successfully learns random labels in parallel with true labels, and common regularizers demonstrably reduce random label accuracy.
- **Medium**: The proposed regularization effectively reduces memorization without harming class prediction performance.
- **Low**: The claim that memorization and generalization are not directly causally linked, challenging classical learning theory predictions.

## Next Checks

1. Conduct controlled experiments varying the regularization strength λ and measuring both random label accuracy reduction and test accuracy changes to establish the memorization-generalization relationship more definitively.
2. Implement ablation studies comparing the proposed random label regularization to standard regularization techniques (weight decay, dropout) while controlling for model capacity and training dynamics.
3. Extend the analysis to deeper layers and different architectures to verify that sample-specific information persistence and memorization patterns are consistent across network depths and model families.