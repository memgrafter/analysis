---
ver: rpa2
title: Expert Routing with Synthetic Data for Continual Learning
arxiv_id: '2412.17009'
source_url: https://arxiv.org/abs/2412.17009
tags:
- learning
- domain
- data
- samples
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generate to Discriminate (G2D), a method
  for domain-incremental continual learning that uses synthetic data to train a domain
  discriminator for expert routing. The key insight is that synthetic data is more
  effective for training a domain discriminator than for augmenting training data
  for the downstream classifier, which is the common approach in generative replay
  methods.
---

# Expert Routing with Synthetic Data for Continual Learning

## Quick Facts
- **arXiv ID:** 2412.17009
- **Source URL:** https://arxiv.org/abs/2412.17009
- **Authors:** Yewon Byun, Sanket Vaibhav Mehta, Saurabh Garg, Emma Strubell, Michael Oberst, Bryan Wilder, Zachary C. Lipton
- **Reference count:** 40
- **Primary result:** G2D achieves 70.03% average accuracy on DomainNet compared to 60.16% for generative replay and 57.45% for sequential fine-tuning

## Executive Summary
This paper introduces Generate to Discriminate (G2D), a novel method for domain-incremental continual learning that uses synthetic data to train a domain discriminator for expert routing. The key insight is that synthetic data is more effective for training a domain discriminator than for augmenting training data for the downstream classifier, which is the common approach in generative replay methods. G2D leverages this insight to route inference-time samples to domain-specific expert classifiers without requiring access to real data from previous domains. The method demonstrates strong performance across vision (DomainNet, CORe50, DermCL) and text (Question Answering) modalities, outperforming competitive methods in continual learning scenarios.

## Method Summary
G2D trains a domain discriminator on synthetic samples generated from current domain data, which routes test samples to the appropriate domain-specific expert classifier. The method uses diffusion models (Stable Diffusion) for vision and prompt tuning for text to generate synthetic data per domain. At each domain, experts are finetuned on current domain data and kept frozen thereafter. The domain discriminator is trained on the union of synthetic samples from all seen domains to predict domain identity. At inference, the discriminator routes each test sample to the most likely domain's expert classifier, achieving domain-incremental continual learning without data sharing across domains.

## Key Results
- G2D achieves 70.03% average accuracy on DomainNet (6 domains) compared to 60.16% for generative replay and 57.45% for sequential fine-tuning
- Strong out-of-distribution performance on CORe50 (8 domains) with average accuracy of 61.45%
- Outperforms state-of-the-art prompt-based methods on DermCL (4 dermatology tasks) with 0.729 average AUC
- Domain discriminator demonstrates accurate routing capabilities, closely matching performance of discriminators trained on real data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data is more effective for training a domain discriminator than for augmenting training data for the downstream classifier
- Mechanism: When synthetic samples are used to train a domain discriminator, the model learns to distinguish between domains rather than modeling complex class distributions. This domain-level discrimination is simpler than class-level generation, which requires capturing subtle variations within each class.
- Core assumption: The generative model can capture domain-level features more accurately than class-level features
- Evidence anchors:
  - [abstract]: "Surprisingly, we find that leveraging synthetic data in this capacity is more effective than using the samples to directly train the downstream classifier"
  - [section]: "Our results demonstrate that using the same set of synthetic samples for domain discrimination consistently outperforms their use for augmenting training data for downstream classification"

### Mechanism 2
- Claim: Expert routing with domain discriminators enables effective continual learning without access to real data from previous domains
- Mechanism: By training a domain discriminator on synthetic data, the system can route test samples to the appropriate expert classifier based on domain similarity, effectively avoiding catastrophic forgetting without requiring access to real historical data
- Core assumption: The synthetic data sufficiently represents the real domain distributions to enable accurate routing
- Evidence anchors:
  - [abstract]: "G2D leverages synthetic data to train a domain-discriminator that routes inference-time samples to a set of domain-specific expert classifiers"
  - [section]: "At inference time, we use our domain discriminator to predict the most likely domain identity t, and the test sample is routed to the corresponding expert classifier"

### Mechanism 3
- Claim: Modular expert architectures with routing mechanisms outperform single-model approaches in domain-incremental learning
- Mechanism: Separate expert classifiers for each domain can adapt more closely to individual domain characteristics without interference from other domains, while the routing mechanism ensures the right expert is used at inference time
- Core assumption: Domain-specific experts can learn more specialized representations than a single general model
- Evidence anchors:
  - [abstract]: "While any single model may struggle to achieve this goal, learning an ensemble of domain-specific experts offers the potential to adapt more closely to each individual institution"
  - [section]: "The key challenge in such approaches is determining which expert or module to invoke at inference time"

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: The paper addresses catastrophic forgetting in continual learning by using expert routing rather than single-model approaches
  - Quick check question: What happens to model performance on previous tasks when training on new tasks without special techniques?

- Concept: Domain adaptation
  - Why needed here: The method operates in domain-incremental learning where the label space is fixed but data distributions change across domains
  - Quick check question: How does domain adaptation differ from standard supervised learning?

- Concept: Generative models and synthetic data
  - Why needed here: The approach relies on generative models to create synthetic data for training domain discriminators
  - Quick check question: What are the key challenges in generating synthetic data that faithfully represents real data distributions?

## Architecture Onboarding

- Component map: Generator → Domain discriminator training → Expert classifier training → Inference routing
- Critical path: Synthetic data generation → Domain discriminator training → Expert finetuning → Test-time routing
- Design tradeoffs:
  - Linear scaling of experts with domains vs. parameter efficiency
  - Quality of synthetic data vs. computation cost of generation
  - Domain discrimination accuracy vs. routing complexity
- Failure signatures:
  - Poor domain discrimination accuracy leading to incorrect expert selection
  - Low-quality synthetic data causing inaccurate domain representations
  - Expert interference when domains have overlapping characteristics
- First 3 experiments:
  1. Train a single expert without routing on a multi-domain dataset to establish baseline catastrophic forgetting
  2. Train the domain discriminator on synthetic data and evaluate its accuracy in distinguishing domains
  3. Implement the full G2D pipeline and compare performance against sequential fine-tuning baseline

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations section, several open questions emerge regarding the scalability of the approach with increasing numbers of domains, the applicability to other types of continual learning scenarios beyond domain-incremental learning, and the comparison to more sophisticated generative replay methods.

## Limitations
- The number of experts scales linearly with the number of domains, introducing potential computational overhead
- The approach is limited to domain-incremental learning and may not generalize to class-incremental or task-incremental scenarios
- The synthetic data generation pipeline lacks full specification of key hyperparameters, affecting reproducibility

## Confidence

- **High confidence:** G2D's effectiveness in domain-incremental learning (supported by strong quantitative results across multiple benchmarks)
- **Medium confidence:** The superiority of synthetic data for domain discrimination over classification (based on ablation studies but limited comparative analysis)
- **Medium confidence:** The routing mechanism's accuracy (empirical results show good performance, but theoretical guarantees are limited)

## Next Checks

1. **Ablation study on synthetic data quality:** Systematically vary the quality and quantity of synthetic data to quantify its impact on domain discrimination accuracy and downstream performance.

2. **Expert routing robustness evaluation:** Test the domain discriminator's routing accuracy under domain overlap scenarios and measure the impact on overall system performance when routing errors occur.

3. **Scalability analysis:** Evaluate G2D's performance as the number of domains increases to determine if the linear scaling of expert classifiers remains practical and effective.