---
ver: rpa2
title: 'From Link Prediction to Forecasting: Addressing Challenges in Batch-based
  Temporal Graph Learning'
arxiv_id: '2406.04897'
source_url: https://arxiv.org/abs/2406.04897
tags:
- temporal
- time
- edges
- link
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Batch-based evaluation in temporal graph learning causes inconsistent
  prediction tasks due to varying time window durations and information loss from
  temporal ordering. The authors reformulate dynamic link prediction as link forecasting
  using fixed-length time windows, eliminating dependency on batch size.
---

# From Link Prediction to Forecasting: Addressing Challenges in Batch-based Temporal Graph Learning

## Quick Facts
- arXiv ID: 2406.04897
- Source URL: https://arxiv.org/abs/2406.04897
- Reference count: 40
- Authors reformulate dynamic link prediction as link forecasting using fixed-length time windows, eliminating dependency on batch size and showing substantial performance differences for memory-based models.

## Executive Summary
This paper identifies fundamental problems with batch-based evaluation in temporal graph learning, where fixed-size batching causes inconsistent prediction tasks due to varying time window durations and information loss from temporal ordering. The authors propose reformulating dynamic link prediction as link forecasting using fixed-length time windows, eliminating batch size dependency. Experiments on 14 real-world datasets show substantial performance differences between traditional batch-based evaluation and the proposed time-window approach, particularly for memory-based models.

## Method Summary
The authors reformulate dynamic link prediction as link forecasting by using fixed-length time windows instead of fixed batch sizes. They implement a SnapshotLoader that groups edges by time windows of duration h, replacing the standard TemporalDataLoader. The evaluation uses a horizon parameter to define prediction windows, sampling positive edges that occur within each window and negative edges that don't occur. This approach is implemented as extensions to PyTorch Geometric and DyGLib, allowing existing TGNN architectures to be evaluated using time-window-based metrics while maintaining their original training procedures.

## Key Results
- Performance differences of up to 0.37 AUC-ROC between batch-based and time-window-based evaluation, especially for memory-based models
- Information loss measured by decreasing NMI between batch indices and timestamps as batch size increases
- The proposed time-window approach eliminates batch size dependency while providing consistent prediction tasks across different datasets

## Why This Works (Mechanism)

### Mechanism 1
Fixed-size batching causes inconsistent prediction tasks because batch size determines the time window length, which varies across datasets due to non-uniform edge density. When edges are grouped into fixed-size batches, each batch implicitly defines a time window. In datasets with non-uniform temporal activity, these windows have different durations, changing the nature of the prediction task (e.g., predicting interactions within minutes vs. hours).

### Mechanism 2
Fixed-size batching discards temporal ordering information within batches, leading to information loss that varies with batch size. When edges within a batch are processed in parallel, their temporal order is lost. Larger batches contain more edges with different timestamps, resulting in greater information loss as measured by decreasing NMI between batch indices and timestamps.

### Mechanism 3
In discrete-time temporal graphs, fixed-size batching imposes an artificial ordering on edges within snapshots that doesn't exist in the data. When snapshots contain many more edges than the batch size, batches fall entirely within snapshots. Processing these batches sequentially imposes an ordering on edges that should be considered simultaneous, violating the data's temporal structure.

## Foundational Learning

- **Temporal graph representation learning**: TGNN architectures learn representations capturing both temporal and topological patterns. Understanding how these models process temporal information is essential to grasp why batch-based evaluation is problematic.
  - Quick check: How do temporal graph neural networks typically incorporate time information into node representations?

- **Information theory and normalized mutual information (NMI)**: NMI is used to quantify the information loss when edges are grouped into batches. Understanding this metric is crucial for interpreting the empirical results showing how batching affects temporal information retention.
  - Quick check: What does an NMI value of 0.5 between batch indices and timestamps indicate about the relationship between batching and temporal information?

- **Negative sampling in link prediction**: The paper discusses different negative sampling strategies (random, historic, inductive) that affect how models are trained and evaluated. Understanding these approaches is important for interpreting the experimental results.
  - Quick check: Why is random negative sampling considered "ill-suited for dynamic link prediction" according to the paper?

## Architecture Onboarding

- **Component map**: DataLoader (SnapshotLoader) -> Model (TGNN) -> Evaluation (time window-based) -> Training loop (edges from fixed time windows with negative sampling)
- **Critical path**: 1) Load temporal graph data using SnapshotLoader with specified horizon h, 2) For each time window [t, t+h), sample positive edges that occur and negative edges that don't occur, 3) Feed these edges through the TGNN to obtain predictions, 4) Compute evaluation metrics (AUC-ROC, average precision) per time window, 5) Aggregate results across all time windows
- **Design tradeoffs**: Fixed time windows vs. fixed batch sizes (consistent tasks vs. computational efficiency), horizon selection (longer horizons capture more interactions but may reduce precision), negative sampling strategy (historical vs. inductive)
- **Failure signatures**: Performance degradation when horizon h doesn't match natural interaction patterns, high variance in batch sizes with non-uniform edge density, memory issues with large horizon values
- **First 3 experiments**: 1) Implement SnapshotLoader and verify consistent time windows, 2) Run TGNN with both batch-based and window-based evaluation on a small dataset, 3) Measure NMI between batch indices and timestamps for different batch sizes

## Open Questions the Paper Calls Out

- **Optimal forecasting horizon**: How does the choice of forecasting horizon h affect TGNN performance, and what is the optimal horizon for different temporal graphs? The authors only provide one example of choosing h per dataset without exploring its impact systematically.
- **Training process modifications**: Can batch-based evaluation issues be addressed by modifying training rather than evaluation? The paper mentions a correction technique but doesn't evaluate its effectiveness.
- **Weighting scheme effects**: How do different weighting schemes for aggregating performance scores across time windows affect relative performance? Only equal weight per time window is presented, not exploring alternatives.

## Limitations

- Performance differences depend on precise implementation details and hyperparameter choices
- The proposed fixed-horizon evaluation may introduce new biases when natural interaction patterns don't align with chosen window sizes
- Paper doesn't address how evaluation metrics should be adjusted when batch sizes become highly variable under fixed-time-window sampling

## Confidence

- **High**: The fundamental observation that batch size determines time window length, creating inconsistent prediction tasks across datasets
- **Medium**: The specific performance differences reported between batch-based and time-window-based evaluation
- **Medium**: The claim that information loss from temporal ordering varies with batch size

## Next Checks

1. **NMI Validation**: Systematically measure normalized mutual information between batch indices and timestamps across different batch sizes (10, 100, 1000) on multiple datasets to verify the claimed information loss pattern
2. **Model-Specific Analysis**: Compare performance degradation patterns between memory-based and non-memory-based TGNN models when switching from batch-based to time-window-based evaluation
3. **Horizon Sensitivity**: Test model performance across multiple horizon values (h=50, h=200, h=500) on a representative dataset to assess stability and identify optimal window sizing strategies