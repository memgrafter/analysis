---
ver: rpa2
title: Test-time Adaptation for Cross-modal Retrieval with Query Shift
arxiv_id: '2410.15624'
source_url: https://arxiv.org/abs/2410.15624
tags:
- query
- retrieval
- modality
- shift
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the query shift problem in cross-modal retrieval,
  where online queries follow a different distribution than the source domain, leading
  to performance degradation. The authors propose Test-time adaptation for Cross-modal
  Retrieval (TCR), a method that adapts pre-trained models to handle query shift.
---

# Test-time Adaptation for Cross-modal Retrieval with Query Shift

## Quick Facts
- arXiv ID: 2410.15624
- Source URL: https://arxiv.org/abs/2410.15624
- Reference count: 38
- Primary result: TCR achieves up to 68.9% Recall@1 on COCO-C, significantly outperforming state-of-the-art methods

## Executive Summary
This paper addresses the query shift problem in cross-modal retrieval, where online queries follow different distributions than the source domain, causing performance degradation. The authors propose Test-time Adaptation for Cross-modal Retrieval (TCR), which adapts pre-trained models to handle query shift through a novel query prediction refinement module and a joint objective function. TCR employs three key losses: intra-modality uniformity learning, inter-modality gap learning, and noise-robust adaptation, achieving significant improvements over existing methods across multiple benchmarks.

## Method Summary
TCR adapts pre-trained cross-modal retrieval models at test time to handle query shift without access to source data. The method employs a query prediction refinement module that selects candidate gallery items using nearest neighbors and refines predictions based on a constraint estimation mechanism. The joint objective function includes three losses: intra-modality uniformity learning (enhancing query discrimination through contrast with centers), inter-modality gap learning (aligning query and gallery modalities by rectifying the modality gap), and noise-robust adaptation (filtering noisy predictions using entropy-based thresholds). The model updates only normalization layers in the query encoder using AdamW optimizer.

## Key Results
- Achieves up to 68.9% Recall@1 on COCO-C, outperforming state-of-the-art methods
- Consistently improves performance across various gallery sizes while baseline TTA methods degrade
- Demonstrates robustness across multiple datasets including COCO, Flickr, Fashion-Gen, and CUHK-PEDES

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query shift diminishes intra-modality uniformity in the query modality
- Mechanism: Query shift causes samples from the query modality to cluster together, reducing their spread in the embedding space. This clustering makes it harder to distinguish between different queries during retrieval.
- Core assumption: The pre-trained model's common space maintains good uniformity in the source domain, but query shift violates this property.
- Evidence anchors: [abstract] "query shift would not only diminish the uniformity (namely, within-modality scatter) of the query modality" and [section] "query shift would diminish the uniformity of the query modality, resulting in confused queries with lower discrimination in the common space."

### Mechanism 2
- Claim: Query shift amplifies the modality gap between query and gallery modalities
- Mechanism: The distribution shift causes the query embeddings to move away from their gallery counterparts in the embedding space, increasing the distance between modalities. This breaks the cross-modal alignment established by pre-trained models.
- Core assumption: Pre-trained models establish good cross-modal alignment in the source domain that query shift disrupts.
- Evidence anchors: [abstract] "query shift would not only diminish the uniformity... but also amplify the gap between query and gallery modalities" and [section] "query shift would amplify the modality gap between query and gallery modalities, disrupting the cross-modal alignment established by the source model."

### Mechanism 3
- Claim: TCR achieves robustness by manipulating both uniformity and modality gap while preventing overfitting to noisy predictions
- Mechanism: TCR uses a three-loss objective that (1) increases query modality uniformity through contrast with centers, (2) reduces modality gap to the source model's estimated gap, and (3) filters noisy predictions using self-adaptive thresholds based on entropy.
- Core assumption: The source model's modality gap is a good estimate of the desired target gap, and entropy-based filtering can effectively identify noisy predictions.
- Evidence anchors: [abstract] "TCR employs a novel module to refine the query predictions... and a joint objective to prevent query shift from disturbing the common space" and [section] "the joint objective is composed of three individual losses that embrace the following merits."

## Foundational Learning

- **Cross-modal retrieval fundamentals**: Understanding how embeddings from different modalities are compared and matched is crucial for grasping why query shift affects retrieval performance. Quick check: How does a cross-modal retrieval system determine if a query matches a gallery item?

- **Test-time adaptation principles**: TCR builds upon TTA concepts but adapts them for cross-modal retrieval; understanding online adaptation without source data access is key. Quick check: What's the key difference between standard domain adaptation and test-time adaptation?

- **Entropy-based uncertainty estimation**: The noise-robust adaptation loss uses entropy to filter noisy predictions; understanding this metric is essential for the mechanism. Quick check: How does entropy relate to prediction confidence in classification tasks?

## Architecture Onboarding

- **Component map**: Query and gallery encoders -> Query prediction refinement module -> Three-loss objective function -> Parameter update mechanism (normalization layers only)

- **Critical path**: 
  1. Query embeddings → Candidate selection → Refined predictions
  2. Refined predictions → Entropy estimation → Noise filtering
  3. Filtered data → Loss computation → Parameter updates
  4. Updated model → Improved retrieval performance

- **Design tradeoffs**:
  - Candidate selection vs. full gallery comparison: Faster but potentially less accurate
  - Fixed vs. adaptive thresholds: More robust but potentially more complex
  - Update only normalization layers vs. full model: Less catastrophic forgetting risk but potentially less effective adaptation

- **Failure signatures**:
  - Performance degradation despite adaptation attempts
  - Mode collapse (all queries mapping to similar regions)
  - Overfitting to specific query patterns

- **First 3 experiments**:
  1. Ablation study removing the query prediction refinement module to measure its impact
  2. Test with different candidate selection strategies (e.g., k-NN vs. threshold-based)
  3. Evaluate performance on varying degrees of distribution shift severity to find breaking points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TCR's performance scale with increasing gallery size in cross-modal retrieval tasks?
- Basis in paper: [explicit] The paper mentions that TCR consistently achieves performance improvements across various gallery sizes, while baseline TTA methods suffer from increasing performance degradation as gallery size increases.
- Why unresolved: The paper provides a comparison for a few specific gallery sizes but doesn't systematically study the relationship between gallery size and TCR performance.
- What evidence would resolve it: A comprehensive study plotting TCR's Recall@1 (or other metrics) against gallery size across multiple datasets, demonstrating the scalability of TCR's approach.

### Open Question 2
- Question: Can TCR's approach be extended to handle more than two modalities in cross-modal retrieval?
- Basis in paper: [inferred] The paper focuses on image-text retrieval but mentions TCR is a general TTA framework that could endow most existing pre-trained models with robustness against query shift. The methods employed (query prediction refinement, intra-modality uniformity, inter-modality gap learning) seem applicable to more than two modalities.
- Why unresolved: The paper only evaluates TCR on image-text retrieval tasks and doesn't explore its applicability to other multimodal scenarios.
- What evidence would resolve it: Experiments applying TCR to retrieval tasks involving three or more modalities (e.g., image-text-audio) and demonstrating its effectiveness in those scenarios.

### Open Question 3
- Question: What is the computational overhead of TCR compared to baseline TTA methods during the adaptation process?
- Basis in paper: [inferred] The paper mentions that TCR performs the objective function for each coming mini-batch of queries and updates parameters within normalization layers. It also maintains a queue for estimating constraints. These operations likely introduce computational overhead.
- Why unresolved: The paper doesn't provide a quantitative comparison of the computational cost of TCR versus baseline methods.
- What evidence would resolve it: A detailed analysis comparing the runtime and memory usage of TCR and baseline TTA methods during adaptation, ideally across different hardware configurations and batch sizes.

## Limitations
- The empirical validation of individual mechanisms is limited, with ablation studies showing performance improvements but not directly measuring impact on uniformity or modality gap metrics
- Query shift scenarios are simulated through synthetic corruptions, with effectiveness on naturally occurring query shifts in real-world applications remaining to be validated
- TCR relies on several critical hyperparameters (SI threshold, queue size, temperature scaling) whose sensitivity and optimal values across different datasets are not thoroughly explored

## Confidence

**High Confidence**: The experimental results demonstrating TCR's superiority over baseline methods (up to 68.9% Recall@1 on COCO-C) are well-supported with statistical significance testing across multiple datasets and corruption types.

**Medium Confidence**: The theoretical framework explaining how query shift disrupts uniformity and modality gap is logically sound and consistent with established cross-modal retrieval principles, though direct empirical validation of these mechanisms could be strengthened.

**Low Confidence**: The noise-robust adaptation mechanism's effectiveness depends heavily on the entropy-based filtering threshold calculation, which involves several design choices not fully justified or tested across diverse scenarios.

## Next Checks

1. **Mechanism-specific Metrics**: Implement and track uniformity (intra-modality scatter) and modality gap metrics during TCR adaptation to empirically validate that the method achieves its stated goals of enhancing uniformity and rectifying the modality gap.

2. **Real-world Query Shift Evaluation**: Test TCR on naturally occurring query distribution shifts (e.g., temporal drift in user queries, domain-specific terminology changes) rather than only synthetic corruptions to assess real-world applicability.

3. **Hyperparameter Robustness Analysis**: Conduct a systematic sensitivity analysis of TCR's critical hyperparameters (SI threshold, queue size, temperature scaling) across different datasets and shift severities to identify robust default values and failure conditions.