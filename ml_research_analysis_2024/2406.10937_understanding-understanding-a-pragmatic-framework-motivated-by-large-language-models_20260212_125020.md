---
ver: rpa2
title: 'Understanding Understanding: A Pragmatic Framework Motivated by Large Language
  Models'
arxiv_id: '2406.10937'
source_url: https://arxiv.org/abs/2406.10937
tags:
- questions
- agent
- understanding
- probability
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a pragmatic framework for testing whether an
  agent (human or AI) understands a domain by assessing its ability to answer questions.
  The framework requires high average scores across all questions, with rare ridiculous
  answers, and allows "I don't know" responses.
---

# Understanding Understanding: A Pragmatic Framework Motivated by Large Language Models

## Quick Facts
- arXiv ID: 2406.10937
- Source URL: https://arxiv.org/abs/2406.10937
- Reference count: 0
- Primary result: Proposes a pragmatic framework for testing understanding by question-answering performance, showing current LLMs don't understand nontrivial domains but providing a recipe for building agents that do

## Executive Summary
This paper presents a pragmatic framework for determining whether an agent understands a domain by evaluating its ability to answer questions with high average scores and rare ridiculous answers. The framework acknowledges that direct exhaustive testing is infeasible for large domains, so it uses random sampling with probabilistic confidence bounds to establish high-confidence assessments. The authors demonstrate that explanations accompanying answers can significantly reduce the number of samples needed by covering multiple similar questions. According to this framework, current LLMs cannot be said to understand nontrivial domains, but the framework provides a practical recipe for building AI agents that do understand.

## Method Summary
The framework defines understanding through observable consequences, specifically an agent's ability to answer questions from a domain with high average scores and rare ridiculous answers. For domains too large to test exhaustively, the method uses random sampling with Chernoff bounds to establish probabilistic confidence in the agent's performance. When explanations are provided, they can cover multiple similar questions, dramatically reducing the required sample size. The approach aligns with pragmatic philosophy by defining understanding solely through observable question-answering competence rather than internal states or representations.

## Key Results
- Random sampling with Chernoff bounds enables high-confidence testing of understanding for large domains
- Explanations accompanying answers can reduce sample complexity by covering multiple similar questions
- Current LLMs fail to demonstrate understanding of nontrivial domains according to this framework
- The framework provides a practical recipe for building AI agents that do understand

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Random sampling with probabilistic confidence bounds can effectively test understanding even for large domains.
- **Mechanism:** By drawing questions independently from the distribution ∆Q and applying Chernoff bounds, we can establish high-probability confidence intervals for both average scores and ridiculous answer probabilities.
- **Core assumption:** Questions are drawn independently from the distribution ∆Q, and the scoring function S provides meaningful quantitative feedback.
- **Evidence anchors:**
  - [abstract] "Reaching certainty about these conditions requires exhaustive testing of the questions which is impossible for nontrivial scopes, but we show how high confidence can be achieved via random sampling and the application of probabilistic confidence bounds."
  - [section] "Although our definition of understanding implies an obvious way of assessing whether an agent understands—namely, asking every question in the scope—all scopes of interest are so large (indeed, often infinite) that it is not possible in practice to verify our criteria with certainty. The natural alternative is to forego certainty and settle for obtaining high confidence, by asking a reasonable number of questions taken at random from the scope."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.503" - Weak evidence of related work on testing frameworks and benchmarks.
- **Break condition:** The sampling becomes ineffective if the question distribution is heavily skewed or if the scoring function S is unreliable.

### Mechanism 2
- **Claim:** Explanations accompanying answers can dramatically reduce the number of samples needed to test understanding.
- **Mechanism:** When an agent provides an explanation αi for its answer, this implies the ability to answer all questions in Qαi similarly, effectively covering multiple questions with one sample.
- **Core assumption:** The explanation procedure αi is reliable and would be applied consistently to all questions in Qαi.
- **Evidence anchors:**
  - [abstract] "We also show that accompanying answers with explanations can improve the sample complexity required to achieve acceptable bounds, because an explanation of an answer implies the ability to answer many similar questions."
  - [section] "We define this formally and show that it can meaningfully reduce the number of required samples."
  - [corpus] "Towards a Benchmark for Causal Business Process Reasoning with LLMs" - Related work on evaluating reasoning capabilities.
- **Break condition:** The explanation procedure is not reliable or does not cover a significant portion of the question space.

### Mechanism 3
- **Claim:** The pragmatic framework provides a practical recipe for building AI agents that do understand.
- **Mechanism:** By requiring high average scores, rare ridiculous answers, and explanations, the framework guides the development of AI agents that demonstrate understanding through observable consequences.
- **Core assumption:** Understanding can be defined solely by observable consequences, aligning with the pragmatic tradition.
- **Evidence anchors:**
  - [abstract] "According to our framework, current LLMs cannot be said to understand nontrivial domains, but as the framework provides a practical recipe for testing understanding, it thus also constitutes a tool for building AI agents that do understand."
  - [section] "Our approach falls in the camp of pragmatism within philosophy... According to this principle, seeking some deeper notion of what understanding 'actually is'... provides no useful information beyond the observed agent's competence in answering questions."
  - [corpus] "SWE-QA: Can Language Models Answer Repository-level Code Questions?" - Related work on evaluating code understanding.
- **Break condition:** The framework fails if understanding requires internal states or representations that are not captured by observable performance.

## Foundational Learning

- **Concept: Probabilistic confidence bounds**
  - Why needed here: To establish high-probability confidence intervals for average scores and ridiculous answer probabilities when exhaustive testing is infeasible.
  - Quick check question: Given a sample mean of 0.9 from 100 questions, what is the lower bound for the true mean at 95% confidence using Chernoff bounds?

- **Concept: Chernoff bounds**
  - Why needed here: To derive tighter confidence intervals than Hoeffding bounds, especially when the true probability is close to 0 or 1.
  - Quick check question: Why are Chernoff bounds preferred over Hoeffding bounds when testing for rare ridiculous answers?

- **Concept: Pragmatic definition of understanding**
  - Why needed here: To define understanding solely by observable consequences (question-answering performance) rather than internal states or representations.
  - Quick check question: How does the pragmatic approach differ from internalism and referentialism in defining understanding?

## Architecture Onboarding

- **Component map:**
  - Question generator -> QA system -> Scoring function -> Confidence calculator -> Understanding determination
  - Explanation verifier (optional component)

- **Critical path:**
  1. Sample n questions from ∆Q
  2. Agent answers questions, optionally with explanations
  3. Score answers using S(q,a)
  4. Calculate empirical mean score and fraction of ridiculous answers
  5. Apply Chernoff bounds to derive confidence intervals
  6. Compare confidence intervals to passing grade PG and ridiculousness threshold RID
  7. Determine understanding based on Good-Grade, Good-Rid, Bad-Grade, and Bad-Rid conditions

- **Design tradeoffs:**
  - Sample size n vs. confidence level δ: Larger n provides tighter bounds but is more expensive
  - Explanation quality vs. coverage: Better explanations cover more questions but may be harder to generate
  - Scoring granularity vs. reliability: Finer-grained scores provide more information but may be harder to assess consistently

- **Failure signatures:**
  - High variance in scores: Indicates unreliable scoring function or inconsistent agent performance
  - Explanations that don't cover many questions: Indicates low coverage and limited benefit from explanations
  - Confidence intervals that never exclude passing grade: Indicates need for more samples or lower PG

- **First 3 experiments:**
  1. Implement the basic testing procedure with a simple domain (e.g., arithmetic) and a synthetic QA agent
  2. Add explanation capability to the agent and measure the reduction in required samples
  3. Test the procedure with a real LLM (e.g., GPT-4) on a benchmark dataset (e.g., MMLU) and analyze the results

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we formally define and measure the "ridiculousness" of an answer beyond a binary 0/1 score?
- **Basis in paper:** [explicit] The paper defines a ridiculous answer as one receiving a score of 0, but notes this prevents assessing degrees of ridiculousness.
- **Why unresolved:** The paper chose to model all ridiculous answers equally for simplicity, but acknowledges this limits granularity in assessment.
- **What evidence would resolve it:** Empirical studies comparing performance of AI systems using a continuous ridiculousness scale vs binary scale across various domains.

### Open Question 2
- **Question:** What is the optimal balance between sample size and confidence threshold when testing understanding in practice?
- **Basis in paper:** [inferred] The paper shows sample sizes needed for high confidence (1000-10000 questions) but acknowledges this may be impractical in many real-world settings.
- **Why unresolved:** The paper doesn't provide specific guidance on when to accept lower confidence vs investing in more samples, which depends on domain and use case.
- **What evidence would resolve it:** Cost-benefit analysis across multiple domains showing performance trade-offs between sample sizes and acceptable confidence levels.

### Open Question 3
- **Question:** How can we verify that an AI system is genuinely applying an explanation procedure rather than just claiming to do so?
- **Basis in paper:** [explicit] The paper assumes the agent would apply procedures reliably to all applicable questions, but notes this is "optimistic" and suggests this needs further investigation.
- **Why unresolved:** The paper acknowledges this assumption is difficult to verify in practice, especially for complex procedures.
- **What evidence would resolve it:** Experimental framework testing whether explanations actually improve performance on unseen questions in the same category vs control groups.

## Limitations

- The framework assumes reliable scoring functions and explanation procedures, which may be difficult to implement in practice
- It may be too strict for current LLMs, potentially underestimating their capabilities due to inherent technological limitations
- The assumption of independent question sampling may not hold for domains with highly interdependent or context-dependent questions

## Confidence

- **Framework Design and Theoretical Foundation**: High
- **Sample Complexity Analysis**: Medium
- **Practical Applicability**: Low

## Next Checks

1. **Empirical Validation on Benchmark Datasets**: Apply the framework to established benchmarks like MMLU or SuperGLUE with various LLMs to measure understanding as defined by the framework. Compare results against human performance to validate the scoring function and passing grade thresholds.

2. **Analysis of Explanation Coverage**: For domains where explanations are applicable, conduct experiments to measure the actual coverage of Qαi sets and quantify the reduction in sample complexity. This will validate whether the theoretical benefits of explanations translate to practical improvements.

3. **Stress Testing on Edge Cases**: Test the framework's robustness by evaluating agents on domains with ambiguous questions, adversarial examples, or where the question distribution is unknown or non-stationary. This will reveal limitations in the framework's assumptions about question sampling and scoring.