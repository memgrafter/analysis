---
ver: rpa2
title: 'LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal Classification'
arxiv_id: '2408.03359'
source_url: https://arxiv.org/abs/2408.03359
tags:
- lampo
- llms
- shot
- each
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LAMPO, a new method for few-shot ordinal
  classification using large language models (LLMs). LAMPO treats the LLM as a preference
  machine that compares test instances to demonstrations pairwise, then aggregates
  these comparisons into final ordinal predictions.
---

# LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal Classification

## Quick Facts
- arXiv ID: 2408.03359
- Source URL: https://arxiv.org/abs/2408.03359
- Reference count: 12
- Authors: Zhen Qin, Junru Wu, Jiaming Shen, Tianqi Liu, Xuanhui Wang
- One-line primary result: LAMPO achieves over 20% absolute improvement on hate classification using pairwise preference comparisons

## Executive Summary
LAMPO addresses the challenge of few-shot ordinal classification with large language models by transforming the task from pointwise estimation to pairwise preference comparison. Rather than having LLMs directly predict ordinal labels, LAMPO uses LLMs to compare test instances against demonstrations pairwise, then aggregates these comparisons into final predictions using self-supervised thresholds. This approach overcomes limitations of traditional in-context learning methods, including context length constraints and ordering bias, while achieving competitive performance across seven datasets including Twitter, SST-5, Yelp-5, Lap14, Hate, Offensive, and Irony.

## Method Summary
LAMPO transforms few-shot ordinal classification by using LLMs as preference machines that compare test instances to demonstrations pairwise rather than performing direct ordinal prediction. For each test instance, LAMPO makes LLM calls comparing it to every demonstration, obtaining binary preference judgments. These pairwise comparisons are aggregated using self-supervised thresholds learned from a probing set, eliminating the need for labeled development data. The method avoids context length constraints by using short prompts with only two examples per LLM call, and mitigates ordering bias by treating all pairwise comparisons independently before aggregation.

## Key Results
- Achieved over 20% absolute improvement on hate classification compared to state-of-the-art methods
- Demonstrated competitive performance across seven diverse datasets (Twitter, SST-5, Yelp-5, Lap14, Hate, Offensive, Irony)
- Works effectively with both black-box LLMs (PaLM2-S) and white-box LLMs (Flan-T5-XXL)
- Eliminates context length constraints while substantially reducing ordering bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LAMPO transforms ordinal classification from pointwise estimation to pairwise preference comparison
- Mechanism: Instead of having LLM directly predict an ordinal label, LAMPO uses LLM to compare test instance with each demonstration and determine which is more positive/hateful/etc. These binary comparisons are then aggregated using self-supervised thresholds
- Core assumption: Pairwise preference judgments are more robust than absolute ordinal predictions, especially when LLM lacks strong prior knowledge of specific label space
- Evidence anchors:
  - [abstract] "LAMPO uses the LLM as a preference machine that makes a relative comparative decision between the test instance and each demonstration"
  - [section 4.1] "We score each test instance x as follows: S(x) ≜ ∑(xi,yi)∈C s(F(x, xi), l(yi))"
  - [corpus] Weak - no direct neighbor evidence for preference-based ordinal classification
- Break condition: If LLM is not instruction-following or if demonstrations are too few to provide meaningful comparisons

### Mechanism 2
- Claim: LAMPO avoids context length constraints by eliminating the need to pack all demonstrations into a single prompt
- Mechanism: Each comparison uses only two examples (one demonstration and one test instance) in separate LLM calls, allowing arbitrary number of demonstrations regardless of context limits
- Core assumption: LLMs can maintain performance with shorter, focused prompts containing only two examples rather than many demonstrations
- Evidence anchors:
  - [abstract] "it avoids packing all demonstrations in a single prompt and thus enables LLMs to leverage an arbitrary number of demonstrations"
  - [section 1] "This constraint limits the number of demonstrations that can be included in a prompt, adversely affecting LLM performance"
  - [corpus] Weak - no direct neighbor evidence for context length mitigation
- Break condition: If LLM has strict context limits that still affect individual prompt size or if demonstration quality degrades with separation

### Mechanism 3
- Claim: LAMPO substantially mitigates ordering bias by treating each pairwise comparison independently
- Mechanism: Since each comparison involves only two examples, there's no combinatorial explosion of demonstration orderings. All pairwise comparisons are made independently and aggregated later
- Core assumption: Removing demonstration ordering from individual comparisons eliminates the primary source of ordering bias
- Evidence anchors:
  - [abstract] "it includes only two examples (one demonstration and one test instance) in each prompt and treats all prompts independently during the aggregation stage, which substantially mitigates the demonstration ordering bias in LLMs"
  - [section 1] "the arrangement of demonstrations within the prompt can drastically affect the LLM performance"
  - [corpus] Weak - no direct neighbor evidence for ordering bias mitigation
- Break condition: If other forms of bias (e.g., position bias within individual comparisons) become dominant

## Foundational Learning

- Concept: In-Context Learning (ICL) with few-shot demonstrations
  - Why needed here: LAMPO builds upon ICL but transforms it from pointwise to pairwise comparison approach
  - Quick check question: What is the key difference between traditional ICL and LAMPO's approach to using demonstrations?

- Concept: Ordinal classification and its challenges
  - Why needed here: LAMPO specifically addresses ordinal classification where labels have natural order (e.g., sentiment from negative to positive)
  - Quick check question: Why is ordinal classification more challenging than standard multi-class classification for LLMs?

- Concept: Self-supervised threshold learning
  - Why needed here: LAMPO uses probing set with entropy-based ranking to find optimal thresholds without labeled development data
  - Quick check question: How does LAMPO find decision thresholds without access to a development set?

## Architecture Onboarding

- Component map:
  Scoring Engine -> Threshold Learner -> Aggregator -> LLM Interface

- Critical path:
  1. For each demonstration, make LLM call comparing test instance to demonstration
  2. Convert comparison results to local scores and sum for total score
  3. Use pre-computed thresholds to map total score to ordinal label

- Design tradeoffs:
  - More LLM API calls (linear in demonstrations) vs. elimination of context length limits
  - Pairwise comparisons require more calls but shorter prompts
  - Self-supervised thresholds avoid need for development data but may have higher variance

- Failure signatures:
  - Poor performance with very few demonstrations (insufficient comparison data)
  - Degradation when LLM is not instruction-following
  - Suboptimal thresholds if probing set doesn't represent true data distribution

- First 3 experiments:
  1. Compare LAMPO vs. standard ICL on a small ordinal dataset with 5 demonstrations
  2. Test LAMPO with varying numbers of demonstrations (2, 5, 10) to find scaling behavior
  3. Evaluate LAMPO with different threshold methods (expected vs. self-supervised) to assess impact on performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Performance may degrade with very few demonstrations due to insufficient comparison data
- Method requires multiple LLM API calls (linear in demonstrations), increasing computational cost
- Sensitive to demonstration quality and LLM instruction-following capabilities

## Confidence
- High: LAMPO's experimental performance on the seven tested datasets
- Medium: Claims about preference-based comparison being more robust than pointwise estimation
- Medium: Claims about context length and ordering bias mitigation

## Next Checks
1. Implement LAMPO with varying numbers of demonstrations (2, 5, 10, 20) on a small ordinal dataset to characterize the scaling behavior and identify the optimal demonstration count
2. Compare LAMPO's performance with different threshold learning methods (expected vs. self-supervised) on the same dataset to isolate the impact of the aggregation strategy
3. Test LAMPO's sensitivity to demonstration quality by using corrupted or noisy demonstrations to assess robustness to demonstration selection