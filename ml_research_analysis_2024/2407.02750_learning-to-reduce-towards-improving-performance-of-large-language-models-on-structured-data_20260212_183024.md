---
ver: rpa2
title: 'Learning to Reduce: Towards Improving Performance of Large Language Models
  on Structured Data'
arxiv_id: '2407.02750'
source_url: https://arxiv.org/abs/2407.02750
tags:
- language
- table
- context
- data
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Learning to Reduce, a framework that fine-tunes
  a language model using On-Policy Learning to generate reduced versions of structured
  data inputs, specifically tables, by identifying the most relevant rows and columns
  for a given question-answering task. The method addresses the challenge of long-context
  inference in large language models on structured data by training a model to select
  relevant evidence before passing the reduced context to an LLM.
---

# Learning to Reduce: Towards Improving Performance of Large Language Models on Structured Data

## Quick Facts
- **arXiv ID:** 2407.02750
- **Source URL:** https://arxiv.org/abs/2407.02750
- **Reference count:** 15
- **Primary result:** Achieved 91.82% column recall and 96.17% row recall on WikiTableQuestions, significantly improving LLM accuracy on table QA tasks

## Executive Summary
This paper introduces Learning to Reduce, a framework that fine-tunes language models to generate reduced versions of structured data inputs, specifically tables, by identifying the most relevant rows and columns for a given question-answering task. The method addresses the challenge of long-context inference in large language models on structured data by training a model to select relevant evidence before passing the reduced context to an LLM. Evaluated on the WikiTableQuestions dataset, Learning to Reduce achieved higher recall in context reduction compared to baselines including GPT-4 and fine-tuned T5 models, and showed strong generalizability on an unseen dataset. The reduced tables generated by the framework significantly improved the accuracy of LLMs on table QA tasks, especially for longer contexts, demonstrating its effectiveness in enhancing LLM performance on structured data.

## Method Summary
The Learning to Reduce framework fine-tunes a FLAN-T5-Large model using On-Policy Learning with Proximal Policy Optimization (PPO) to identify relevant rows and columns from structured data. It employs two separate models: one for column reduction and one for row reduction. The models are trained on WikiTableQuestions dataset with text-to-SQL annotations from SQUALL to identify relevant items. A reward function penalizes type II errors (missing relevant information) more heavily than type I errors (including irrelevant information). The framework generates reduced tables that are then passed to LLMs for inference, improving performance on table QA tasks by reducing context length while maintaining critical information.

## Key Results
- Achieved 91.82% recall for column reduction and 96.17% recall for row reduction on WikiTableQuestions
- Outperformed GPT-4 and fine-tuned T5 models in context reduction tasks
- Demonstrated strong generalizability with effective performance on the unseen Hybrid QA dataset
- Significantly improved LLM accuracy on table QA tasks, particularly for longer contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a language model with On-Policy Learning to generate reduced structured data inputs improves LLM performance on table QA tasks.
- Mechanism: The framework trains a model to identify and retain only the most relevant rows and columns from a table based on a given question, reducing the input context size before passing it to an LLM for inference.
- Core assumption: LLMs struggle with long-context structured data, and reducing the context to only relevant information will improve their performance on table QA tasks.
- Evidence anchors:
  - [abstract] "This paper proposes a framework, Learning to Reduce, that fine-tunes a language model with On-Policy Learning to generate a reduced version of an input structured data."
  - [section 2.1] "We use a sequence-to-sequence language model with FLAN-T5-Large (Chung et al., 2022) pre-trained checkpoint as a base model."
- Break condition: If the reduced context loses critical information necessary for answering the question, LLM performance may degrade despite the shorter input.

### Mechanism 2
- Claim: Using a reward function that penalizes type II errors (missing relevant information) more heavily than type I errors (including irrelevant information) leads to better context reduction.
- Mechanism: The reward function is designed to give higher penalties for failing to select relevant rows and columns, encouraging the model to be more conservative in its selection to avoid missing critical information.
- Core assumption: It is more detrimental for the reduced context to miss relevant information than to include some irrelevant information, as the LLM can still perform reasonably well if all necessary information is present.
- Evidence anchors:
  - [section 2.2] "Type II errors need to be considered more seriously, as this happens when the models fail to generate relevant rows and columns."
  - [section 2.2] "The model gets positive rewards for selecting the correct relevant items, and gets negative rewards for incorrect selection."
- Break condition: If the model becomes too conservative and includes too much irrelevant information, the benefits of context reduction may be diminished.

### Mechanism 3
- Claim: The Learning to Reduce framework is generalizable and can be applied to different structured data QA tasks without requiring fine-tuning on the specific dataset.
- Mechanism: The framework uses a heuristics approach to identify relevant rows and columns based on SQL query annotations, allowing it to learn general patterns of relevance that can be applied to new datasets.
- Core assumption: The patterns of relevance learned from one structured data QA dataset (WikiTableQuestions) can be generalized to other datasets with different distributions and table structures.
- Evidence anchors:
  - [abstract] "When compared to state-of-the-art LLMs like GPT-4, Learning to Reduce not only achieves outstanding performance in reducing the input, but shows generalizability on different datasets."
  - [section 3.1] "This result implies that policy networks can capture more general knowledge about context reduction and opens the possibility of applying Learning to Reduce on other structured data QA tasks without fine-tuning again."
- Break condition: If the new dataset has significantly different characteristics or requires a different type of reasoning, the generalizability of the framework may be limited.

## Foundational Learning

- Concept: On-Policy Learning
  - Why needed here: On-Policy Learning is used to fine-tune the language model based on the rewards it receives for its predictions, allowing it to learn an optimal policy for context reduction.
  - Quick check question: How does On-Policy Learning differ from Off-Policy Learning in terms of the data used for training?

- Concept: Reinforcement Learning (RL) in NLP
  - Why needed here: The Learning to Reduce framework uses RL techniques, such as Proximal Policy Optimization (PPO), to optimize the language model's performance on the context reduction task.
  - Quick check question: What are the key components of an RL framework, and how are they applied in the context of language model fine-tuning?

- Concept: Structured Data Processing
  - Why needed here: The framework is designed to handle structured data inputs, such as tables, and identify relevant information within them based on a given question.
  - Quick check question: What are the challenges associated with processing structured data, and how does the Learning to Reduce framework address them?

## Architecture Onboarding

- Component map: Question -> Column reduction model -> Row reduction model -> Reduced table -> LLM -> Answer

- Critical path:
  1. Column reduction model generates relevant columns
  2. Row reduction model generates relevant rows based on reduced columns
  3. Reduced structured data is passed to LLM for inference
  4. Reward is computed based on the quality of the reduced context and LLM performance

- Design tradeoffs:
  - Using separate models for column and row reduction vs. a single model for both tasks
  - Balancing the trade-off between recall and precision in context reduction
  - Choosing the appropriate reward function to optimize for

- Failure signatures:
  - LLM performance degrades despite reduced context (missing critical information)
  - Context reduction model generates irrelevant or incomplete outputs
  - Reward function fails to capture the true quality of the reduced context

- First 3 experiments:
  1. Evaluate the performance of the column and row reduction models separately on a held-out dataset.
  2. Test the end-to-end pipeline with a small set of questions and tables to ensure the models are working together as expected.
  3. Compare the performance of the Learning to Reduce framework with a baseline approach (e.g., using GPT-4 for context reduction) on a larger dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Learning to Reduce vary across different types of structured data beyond tables, such as knowledge graphs or databases?
- Basis in paper: [inferred] The paper suggests future work on applying the framework to other types of structured data like knowledge bases and databases.
- Why unresolved: The current framework is only tested on table data, leaving uncertainty about its effectiveness on other structured data types.
- What evidence would resolve it: Experiments evaluating the framework's performance on knowledge graphs or database query tasks would clarify its generalizability to different structured data formats.

### Open Question 2
- Question: What is the impact of using task-specific rewards on the accuracy of context reduction, particularly in cases where the model generates a very small number of rows and columns?
- Basis in paper: [explicit] The discussion section mentions that adding task-specific rewards could improve the model's learning, especially when it generates too few relevant items.
- Why unresolved: The current reward function focuses on recall-based metrics, which may not adequately penalize overly reduced contexts that confuse LLMs.
- What evidence would resolve it: Testing the framework with rewards that incorporate downstream task performance metrics would demonstrate if task-specific rewards improve context reduction accuracy.

### Open Question 3
- Question: How does the performance of Learning to Reduce scale with larger and more complex table QA datasets compared to the relatively simple WikiTableQuestions dataset?
- Basis in paper: [inferred] The paper notes that WTQ questions are simple enough to be translated into SQL queries, but LLMs' performance on this dataset is not reliable, suggesting potential limitations with more complex datasets.
- Why unresolved: The framework's effectiveness on more complex table QA tasks with intricate questions and larger tables remains untested.
- What evidence would resolve it: Experiments on more challenging table QA datasets like MultiWOZ or Spider would reveal if the framework maintains its performance on complex reasoning tasks.

## Limitations

- The framework relies on text-to-SQL annotations from SQUALL for training data, which may not perfectly align with human interpretation of relevant information in tables.
- The heuristics for identifying relevant rows and columns from SQL queries are described but not fully specified, creating potential reproducibility challenges.
- The framework is evaluated primarily on WikiTableQuestions with limited testing on diverse table structures or question types, raising questions about generalizability to real-world applications.

## Confidence

**High Confidence:** The mechanism of context reduction through fine-tuning is sound and the recall metrics (91.82% for columns, 96.17% for rows) are well-documented and directly measurable from the reported results.

**Medium Confidence:** The generalizability claims are supported by testing on Hybrid QA, but this is a single additional dataset with potentially similar characteristics to WikiTableQuestions.

**Low Confidence:** The specific impact of reward function hyperparameters (λp, λn1, λn2) on final performance is not extensively explored, and the choice of these values appears somewhat arbitrary without ablation studies demonstrating their sensitivity.

## Next Checks

1. **Cross-domain generalization test:** Apply the pre-trained reduction models to a dataset with fundamentally different table structures (e.g., financial tables, scientific datasets) and evaluate both recall metrics and downstream LLM performance to verify true generalizability.

2. **Ablation study on reward function components:** Systematically vary λp, λn1, and λn2 values to quantify their impact on context reduction quality and LLM accuracy, particularly examining whether the heavier penalty for type II errors is optimal across different question types.

3. **Human evaluation of reduced tables:** Conduct expert annotation of randomly sampled reduced tables to assess whether the model's selections align with human judgment of relevance, particularly for edge cases where the SQL-based heuristics might fail.