---
ver: rpa2
title: Measuring Progress in Dictionary Learning for Language Model Interpretability
  with Board Game Models
arxiv_id: '2408.00113'
source_url: https://arxiv.org/abs/2408.00113
tags:
- board
- saes
- sparse
- learning
- dictionary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating sparse autoencoders
  (SAEs) trained on language models by proposing a novel approach using board game
  models. The authors leverage the interpretable nature of board game states, such
  as chess and Othello, to create ground-truth board state properties (BSPs).
---

# Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models

## Quick Facts
- arXiv ID: 2408.00113
- Source URL: https://arxiv.org/abs/2408.00113
- Reference count: 27
- Primary result: Introduces novel metrics for evaluating sparse autoencoders using board game models, demonstrating that coverage and board reconstruction metrics reveal differences in SAE quality not captured by existing unsupervised metrics

## Executive Summary
This paper addresses the challenge of evaluating sparse autoencoders (SAEs) trained on language models by proposing a novel approach using board game models. The authors leverage the interpretable nature of board game states, such as chess and Othello, to create ground-truth board state properties (BSPs). They introduce two metrics to measure SAE quality: coverage, which quantifies how well SAE features capture BSPs, and board reconstruction, which evaluates the ability to reconstruct the game board from SAE features. The authors also propose a new SAE training technique called p-annealing, which improves performance on both their proposed metrics and traditional unsupervised metrics. They find that SAEs can effectively capture board state information, with p-annealing showing promising results comparable to more computationally expensive methods.

## Method Summary
The paper proposes a novel evaluation framework for sparse autoencoders using board game models. The method involves training SAEs on residual stream activations from chess and Othello language models, then evaluating them using two proposed metrics: coverage (mean F1-scores of features as classifiers for predefined board state properties) and board reconstruction (ability to reconstruct board state from SAE features). The authors introduce p-annealing as a new training technique, which gradually transitions from convex L1 minimization to non-convex Lp minimization to better approximate true sparsity. The framework also uses traditional unsupervised metrics like L0 and loss recovered for comparison.

## Key Results
- Coverage and board reconstruction metrics reveal differences in SAE quality not captured by existing unsupervised metrics
- p-annealing improves SAE performance on both proposed metrics and traditional unsupervised metrics
- SAEs can effectively capture board state information, with p-annealing showing performance comparable to more computationally expensive methods
- The metrics demonstrate that SAEs trained with p-annealing perform better at recovering interpretable features than those trained with standard L1 regularization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The board reconstruction metric works because SAE features, when high-precision classifiers for BSPs, can be aggregated to recover the full board state.
- Mechanism: The metric assumes that interpretable SAE features will be high-precision (but not necessarily high-recall) classifiers for some subset of BSPs. By identifying which features are high-precision for which BSPs on training data, the metric creates a prediction rule that combines these features to reconstruct the board state.
- Core assumption: Interpretable SAE features tend to be high-precision classifiers for subsets of BSPs, even if they don't capture all instances of those BSPs.
- Evidence anchors:
  - [abstract]: "board reconstruction, which evaluates the ability to reconstruct the game board from SAE features"
  - [section]: "we identify, for each SAE feature fi, all of the BSPs g ∈ G for which ϕfi,t is a high precision (of at least 0.95) classifier"
- Break condition: If SAE features are not high-precision classifiers for any BSPs, or if multiple features overlap in what they classify, the reconstruction will fail.

### Mechanism 2
- Claim: The coverage metric works because it measures how many ground-truth BSPs are captured by SAE features by finding the best classifier for each BSP.
- Mechanism: For each BSP g, the metric finds the SAE feature that best classifies g (highest F1-score) and then averages these best scores across all BSPs. This captures how comprehensively the SAE captures the ground-truth features.
- Core assumption: For each BSP, there exists at least one SAE feature that can serve as a reasonable classifier, and the best classifier across all features represents the SAE's coverage of that BSP.
- Evidence anchors:
  - [abstract]: "coverage, which quantifies how well SAE features capture BSPs"
  - [section]: "we take, for each g ∈ G, the F1-score of the feature that best serves as a classifier for g, and then take the mean of these maximal F1-scores"
- Break condition: If SAE features are completely unrelated to the BSPs, or if multiple features equally capture the same BSP while missing others, the coverage will be misleading.

### Mechanism 3
- Claim: p-annealing improves SAE quality by gradually transitioning from convex L1 minimization to non-convex Lp minimization, better approximating true sparsity.
- Mechanism: Starting with p=1 (convex L1 norm) allows the optimization to find a good initial region, then gradually decreasing p to values less than 1 makes the sparsity penalty closer to the true L0 norm while maintaining some convexity benefits during training.
- Core assumption: The non-convex Lp norms with p<1 provide better approximations of true sparsity than L1, and the annealing schedule prevents the optimization from getting stuck in poor local optima.
- Evidence anchors:
  - [abstract]: "we introduce a new SAE training technique, p-annealing, which improves performance on prior unsupervised metrics as well as our new metrics"
  - [section]: "we propose the use of nonconvex Lp minimization, with p < 1, as an alternative to the standard L1 minimization"
- Break condition: If the annealing schedule is too aggressive or the initial L1 phase doesn't find a good region, p-annealing could perform worse than constant p.

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs)
  - Why needed here: The paper evaluates SAEs on board game models, so understanding how SAEs work is fundamental to understanding the metrics and experiments.
  - Quick check question: What is the key difference between SAEs and standard autoencoders?

- Concept: Dictionary Learning
  - Why needed here: SAEs are inspired by sparse dictionary learning, and the paper frames the evaluation in terms of whether SAEs recover ground-truth features (the dictionary).
  - Quick check question: In the context of SAEs, what does "dictionary" refer to?

- Concept: Board State Properties (BSPs)
  - Why needed here: The novel metrics are built around BSPs, which are formal specifications of interpretable features in board games.
  - Quick check question: What is the difference between a "board state property" and a general feature?

## Architecture Onboarding

- Component map: Chess/Othello models -> SAE training pipeline -> Feature extraction -> Coverage metric and board reconstruction metric -> Evaluation
- Critical path: Train SAEs on board game model activations → Extract features → Apply coverage and board reconstruction metrics using ground-truth BSPs → Compare different SAE methods
- Design tradeoffs: The choice between Standard and Gated SAE architectures involves computational cost vs. feature suppression; p-annealing trades off training complexity for potentially better sparsity; the choice of BSPs affects metric validity but requires domain knowledge
- Failure signatures: Poor coverage suggests SAEs aren't capturing interpretable features; poor board reconstruction suggests features aren't combinable into a coherent representation; inconsistent results between unsupervised metrics and proposed metrics suggest metrics measure different aspects
- First 3 experiments:
  1. Train a Standard SAE without p-annealing on chess model activations and compute coverage and board reconstruction scores
  2. Train a Gated SAE and compare its performance to the Standard SAE on both metrics
  3. Train a Standard SAE with p-annealing and compare to the other two methods

## Open Questions the Paper Calls Out

- How generalizable are the proposed board state property metrics to other domains beyond chess and Othello?
- Can the proposed p-annealing technique be effectively combined with other SAE training methodologies beyond the Gated SAE architecture?
- How can the learned features from SAEs be effectively utilized for downstream interpretability tasks?

## Limitations
- The evaluation framework relies heavily on ground-truth BSPs that are hand-engineered and may not capture all aspects of model interpretability
- The board game setting, while offering clean supervision, may not generalize to natural language tasks where features are less formally specifiable
- The paper doesn't extensively explore the robustness of their metrics to SAE hyperparameters or different board game models

## Confidence
- Coverage metric claims: Medium
- Board reconstruction claims: Medium
- p-annealing effectiveness: Medium
- Generalizability claims: Low

## Next Checks
1. **Ablation study on p-annealing**: Test different annealing schedules (e.g., linear vs exponential decay) and final p values to determine sensitivity and identify optimal configurations
2. **Cross-model consistency**: Apply the same evaluation framework to different board game models (e.g., Go) or different architectures to assess metric consistency across domains
3. **Feature interaction analysis**: Investigate cases where coverage is high but reconstruction is low (or vice versa) to understand when features can be individually predictive but fail to combine effectively