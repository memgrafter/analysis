---
ver: rpa2
title: How Interpretable are Reasoning Explanations from Prompting Large Language
  Models?
arxiv_id: '2402.11863'
source_url: https://arxiv.org/abs/2402.11863
tags:
- reasoning
- arxiv
- explanation
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work conducts a comprehensive evaluation of interpretability
  in reasoning explanations from large language models across multiple prompting techniques,
  including Chain-of-Thought variants. The study examines three key dimensions of
  interpretability: faithfulness (whether explanations align with model reasoning),
  robustness (resilience to input perturbations), and utility (usefulness for learning).'
---

# How Interpretable are Reasoning Explanations from Prompting Large Language Models?

## Quick Facts
- arXiv ID: 2402.11863
- Source URL: https://arxiv.org/abs/2402.11863
- Authors: Wei Jie Yeo; Ranjan Satapathy; Rick Siow Mong Goh; Erik Cambria
- Reference count: 16
- Key outcome: SEA-CoT improves interpretability by 70% with 1.2% paraphrase flip rate, 3.81% counterfactual unfaithfulness, 61.24% answer change rate, and 16.97% simulatability score

## Executive Summary
This work evaluates interpretability of reasoning explanations from large language models across multiple prompting techniques, introducing SEA-CoT to improve explanation quality through self-alignment. The study examines faithfulness, robustness, and utility dimensions using perturbation tests on commonsense reasoning datasets. SEA-CoT achieves significant improvements over baselines by selecting reasoning chains that maximize entailment with context and answers.

## Method Summary
The authors evaluate interpretability across prompting techniques using Llama-2 models (7B, 13B, 70B) with 4-bit quantization. SEA-CoT introduces a selection mechanism that ranks reasoning chains based on entailment scores and token overlap with context and answers. The method is compared against Chain-of-Thought variants using perturbation experiments (paraphrasing, mistake insertion, counterfactuals) and simulatability tests across three commonsense reasoning datasets.

## Key Results
- SEA-CoT achieves over 70% improvement across interpretability metrics compared to baselines
- Joint generation of explanations and answers produces more faithful explanations than modular approaches
- Larger models (70B) outperform smaller variants across all interpretability metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-Entailment-Alignment improves interpretability by selecting reasoning chains that maximize alignment between explanation and context.
- Mechanism: SEA-CoT computes entailment scores (Se) and overlap scores (So) between the reasoning chain and the joint context (question + answer), then selects explanations with the highest combined score (ST = Se + So).
- Core assumption: Explanations that better align with both the question and answer are more interpretable and faithful.
- Evidence anchors: [abstract] "SEA-CoT achieves over 70% improvement across interpretability metrics"; [section] "We posit that a credible explanation should intrinsically align with the given context it aims to elucidate"

### Mechanism 2
- Claim: Single-model generation of explanations and answers produces more faithful explanations than modular approaches.
- Mechanism: Joint generation allows the model to better align its explanation with the final answer since both are produced by the same decision-making process.
- Core assumption: When explanation and prediction are generated by separate modules, the explanation may not accurately reflect the actual reasoning process.
- Evidence anchors: [section] "We hypothesize that a single model is in better control of aligning its explanation to the resultant outcome"; [section] "Figure 1 shows that the joint approach scores higher on both accounts of faithfulness and utility"

### Mechanism 3
- Claim: Larger models produce more interpretable explanations due to better entailment scoring and reasoning quality.
- Mechanism: The 70B model outperforms smaller variants (13B, 7B) across all interpretability metrics, suggesting scale improves explanation quality.
- Core assumption: Model capacity directly correlates with the ability to generate faithful, robust, and useful explanations.
- Evidence anchors: [section] "The scaling laws of model size primarily concern the downstream performance of LLMs but little is known regarding the influence on interpretability properties"; [section] "Table 2. Interpretability scores between different model sizes - The largest model, 70B generally outperforms the smaller sizes across all metrics"

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: The paper builds upon and extends CoT variants, so understanding how CoT works is essential for grasping SEA-CoT's improvements
  - Quick check question: How does standard CoT prompting differ from Self-Consistency CoT in terms of explanation selection?

- Concept: Faithfulness vs Plausibility in explanations
  - Why needed here: The paper distinguishes between explanations that are faithful to model reasoning versus those that merely sound plausible to humans
  - Quick check question: What key difference does the paper identify between modular and joint approaches in terms of faithfulness?

- Concept: Evaluation metrics for interpretability
  - Why needed here: The paper introduces specific perturbation tests (paraphrasing, mistake insertion, counterfactuals) to assess different aspects of interpretability
  - Quick check question: Which perturbation test would best detect if an explanation is faithful to the model's actual reasoning process?

## Architecture Onboarding

- Component map: Context → LLM reasoning generation (N sequences) → Entailment/overlap scoring → Best explanation selection → Answer generation
- Critical path: Context → LLM reasoning generation (N sequences) → Entailment/overlap scoring → Best explanation selection → Answer generation
- Design tradeoffs: Using quantized models saves resources but may impact quality; joint generation ensures faithfulness but requires more model capacity; SEA-CoT adds selection overhead but improves interpretability
- Failure signatures: Poor entailment scoring leads to suboptimal explanation selection; model size limitations cause joint generation failures; perturbation tests show low improvement rates
- First 3 experiments:
  1. Implement SEA-CoT selection mechanism with basic entailment scoring and verify it improves over random selection
  2. Compare joint vs modular generation approaches on faithfulness metrics to validate the core hypothesis
  3. Test SEA-CoT with different N values to find the optimal balance between selection quality and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SEA-CoT method perform on other large language models beyond Llama-2, such as GPT-3.5 or GPT-4?
- Basis in paper: [inferred] The paper mentions that SEA-CoT was only tested on Llama-2 due to resource constraints, but suggests extending the work to other models.
- Why unresolved: The paper explicitly states that they did not test SEA-CoT on other models like GPT-3.5 or GPT-4 due to resource limitations. This leaves open the question of whether SEA-CoT's effectiveness generalizes to other LLM architectures and sizes.
- What evidence would resolve it: Conducting the same experiments with SEA-CoT on different LLM architectures (e.g., GPT-3.5, GPT-4, or other transformer models) and comparing the results to those obtained with Llama-2 would provide evidence of its generalizability.

### Open Question 2
- Question: What is the impact of increasing the number of sequences (N) on the interpretability metrics beyond the tested values of 10, 30, and 50?
- Basis in paper: [explicit] The paper mentions that increasing N has positive effects on utility but slight negative effects on paraphrasing and counterfactual tests, but does not explore values beyond 50.
- Why unresolved: The paper only tested a limited range of N values (10, 30, 50) and observed trends, but did not explore the full spectrum of possible N values to determine the optimal number for balancing interpretability metrics.
- What evidence would resolve it: Conducting experiments with a wider range of N values (e.g., 20, 40, 60, 100) and analyzing the resulting interpretability scores would provide insights into the optimal N value for maximizing interpretability across different metrics.

### Open Question 3
- Question: How does SEA-CoT perform when grounded with external knowledge or when combined with other interpretability techniques like Neuro-symbolic AI?
- Basis in paper: [inferred] The paper mentions that it did not investigate techniques that ground LLM responses with external knowledge or hybrid approaches like Neuro-symbolic AI, leaving open the question of how SEA-CoT would perform in such scenarios.
- Why unresolved: The paper focused solely on SEA-CoT's performance without external knowledge or hybrid approaches, so its effectiveness in those contexts remains unknown.
- What evidence would resolve it: Implementing SEA-CoT with external knowledge grounding or combining it with Neuro-symbolic AI techniques and comparing the interpretability results to the original SEA-CoT would provide evidence of its potential improvements or limitations in those scenarios.

## Limitations
- The evaluation framework relies heavily on perturbation-based metrics which may not fully capture all aspects of interpretability
- Model size effects remain incompletely characterized without exploring architectural optimizations or fine-tuning strategies
- The claim of "over 70% improvement" lacks baseline performance context to assess absolute significance

## Confidence
- **High confidence**: The improvement of SEA-CoT over baseline prompting techniques is well-supported by quantitative metrics across multiple datasets
- **Medium confidence**: The superiority of joint generation over modular approaches is demonstrated but edge cases are not fully explored
- **Low confidence**: The absolute significance of "over 70% improvement" cannot be fully assessed without knowing baseline performance levels

## Next Checks
1. Replicate experiments with explicit baseline values for each interpretability metric to establish absolute improvement achieved by SEA-CoT
2. Test SEA-CoT on non-commonsense reasoning tasks (mathematical problem-solving, code generation, scientific reasoning) to verify generalization
3. Compare computational overhead of SEA-CoT against interpretability gains by measuring inference time and memory usage across different model sizes