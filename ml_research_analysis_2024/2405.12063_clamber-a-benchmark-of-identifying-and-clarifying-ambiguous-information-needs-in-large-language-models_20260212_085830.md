---
ver: rpa2
title: 'CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs
  in Large Language Models'
arxiv_id: '2405.12063'
source_url: https://arxiv.org/abs/2405.12063
tags:
- llms
- question
- ambiguous
- clarifying
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CLAMBER introduces a taxonomy to evaluate LLMs\u2019 ability to\
  \ identify and clarify ambiguous queries, organizing ambiguities into three dimensions:\
  \ Epistemic Misalignment (unfamiliar entities, contradictions), Linguistic Ambiguity\
  \ (lexical, semantic), and Aleatoric Output (missing elements: who, when, where,\
  \ what). The benchmark includes ~12K high-quality data samples."
---

# CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models

## Quick Facts
- arXiv ID: 2405.12063
- Source URL: https://arxiv.org/abs/2405.12063
- Reference count: 26
- Primary result: Current LLMs show limited ability to identify and clarify ambiguous queries, with ChatGPT achieving only ~54% accuracy and ~53% F1 scores.

## Executive Summary
CLAMBER introduces a comprehensive taxonomy and benchmark to evaluate large language models' ability to identify and clarify ambiguous user queries. The benchmark organizes ambiguities into three dimensions—Epistemic Misalignment, Linguistic Ambiguity, and Aleatoric Output—and includes ~12K high-quality data samples. Evaluation across five LLMs reveals that while ChatGPT outperforms smaller models, even it struggles significantly with ambiguity identification and clarifying question generation. Chain-of-thought and few-shot prompting provide minimal improvements and can actually induce overconfidence in smaller models, highlighting fundamental limitations in current LLM architectures for handling uncertainty.

## Method Summary
The CLAMBER benchmark employs a systematic taxonomy of ambiguity types and generates ~12K data samples covering epistemic misalignment (unfamiliar entities, contradictions), linguistic ambiguity (lexical, semantic), and aleatoric output (missing elements). Five LLMs (ChatGPT, Llama2 variants, Vicuna) are evaluated using four prompting schemes: zero-shot w/o CoT, zero-shot w/ CoT, few-shot w/o CoT, and few-shot w/ CoT. Performance is measured through accuracy and F1 scores for ambiguity identification, and BertScore plus human evaluation (Help score) for clarifying question quality.

## Key Results
- ChatGPT achieves only ~54% accuracy and ~53% F1 scores in ambiguity identification
- Small LLMs systematically overclassify queries as ambiguous, treating most inputs as uncertain
- Chain-of-thought and few-shot prompting induce overconfidence in small models without meaningful performance gains
- LLMs struggle particularly with semantic ambiguities (pronoun resolution) and contradictions
- Clarifying question generation remains weak due to inadequate conflict resolution and knowledge gap assessment

## Why This Works (Mechanism)

### Mechanism 1
LLMs struggle to identify ambiguity because they over-rely on distributional patterns and misinterpret uniform responses as confidence. When presented with ambiguous queries, small-scale LLMs default to classifying most inputs as ambiguous due to learned response distributions in fine-tuning data, which included uniform treatment of uncertain inputs. This occurs because ambiguity identification accuracy is inversely correlated with model size due to smaller models' lack of contextual reasoning capacity to differentiate subtle ambiguity cues.

### Mechanism 2
Chain-of-thought and few-shot prompting increase overconfidence in small LLMs rather than improving ambiguity detection. These techniques provide surface-level reasoning templates that small LLMs adopt without genuine understanding, leading to inflated confidence scores that don't match actual performance. The effectiveness of prompting techniques depends on the model's underlying reasoning capacity, which scales with parameter count.

### Mechanism 3
LLMs fail to generate effective clarifying questions due to inability to assess their knowledge boundaries and resolve conflicts in their inherent knowledge. When LLMs recognize ambiguity, they lack mechanisms to identify which specific knowledge gaps exist and how to formulate questions that resolve those gaps effectively. Effective clarifying questions require meta-cognitive awareness of what the model doesn't know, which current architectures don't support.

## Foundational Learning

- **Concept**: Taxonomy of ambiguity types (Epistemic Misalignment, Linguistic Ambiguity, Aleatoric Output)
  - Why needed here: The benchmark requires systematic categorization to evaluate LLM performance across different ambiguity dimensions and to identify specific failure modes.
  - Quick check question: Can you list the three primary dimensions of ambiguity in CLAMBER and give one example category from each?

- **Concept**: Chain-of-thought prompting and few-shot learning
  - Why needed here: These are the baseline techniques evaluated for improving ambiguity identification, and understanding their mechanisms is crucial for interpreting why they fail.
  - Quick check question: What is the key difference between zero-shot w/ CoT and few-shot w/ CoT prompting schemes in the context of ambiguity detection?

- **Concept**: Expected Calibration Error (ECE) and AUROC for confidence evaluation
  - Why needed here: These metrics are used to quantify overconfidence in LLM predictions, which is a key finding about why current approaches fail.
  - Quick check question: How does ECE differ from AUROC in measuring LLM prediction confidence, and why are both needed?

## Architecture Onboarding

- **Component map**: Taxonomy module → Data generation pipeline → LLM evaluation framework → Analysis module
- **Critical path**: Data collection → Taxonomy application → LLM evaluation with multiple prompting schemes → Performance analysis → Mechanism identification → Failure diagnosis
- **Design tradeoffs**: Using synthetic data for unfamiliar entities ensures control but may lack real-world complexity; rule-based templates for clarifying questions provide consistency but may miss nuanced contexts
- **Failure signatures**: Overconfidence indicators (high ECE with low accuracy improvement), uniform classification patterns (most queries classified as ambiguous), under-specified clarifying questions (generic rather than targeted)
- **First 3 experiments**:
  1. Run LLM evaluations with and without CoT prompting on a subset of CLAMBER data to reproduce overconfidence findings
  2. Test different numbers of few-shot examples (1, 2, 4, 8) to identify the point of diminishing returns in performance improvement
  3. Implement a simple knowledge boundary detection mechanism and evaluate its impact on clarifying question quality

## Open Questions the Paper Calls Out

1. **How does the sensitivity of prompts affect the performance of LLMs in identifying and clarifying ambiguous queries across different ambiguity categories?**
   - The study used three different prompts and reported average results, but did not explore the full range of prompt variations or their impact on specific ambiguity categories.

2. **Would larger LLMs (e.g., PaLM 540B) significantly outperform ChatGPT in identifying and clarifying ambiguities, particularly in epistemic misalignment categories?**
   - The study limited evaluation to five LLMs due to computational constraints and noted that frontier models were not assessed.

3. **What specific mechanisms cause chain-of-thought and few-shot prompting to induce overconfidence in small-scale LLMs, and can these effects be mitigated?**
   - While the paper measured overconfidence through ECE and AUROC metrics, it did not investigate the underlying cognitive mechanisms or test mitigation strategies.

## Limitations
- The evaluation relies on synthetic data for certain ambiguity types, which may not capture real-world complexity
- The study focuses on English-language queries and specific LLM architectures, limiting generalizability
- Human evaluation of clarifying questions is described but not fully detailed in methodology

## Confidence
- **High Confidence**: Small LLMs systematically overclassify queries as ambiguous due to distributional learning patterns
- **Medium Confidence**: CoT/few-shot prompting causes overconfidence in small models, though prompt engineering choices may influence this
- **Low Confidence**: LLMs lack meta-cognitive awareness for effective clarifying questions, difficult to verify without full methodology

## Next Checks
1. Test the CLAMBER benchmark on non-English queries to assess cross-lingual validity
2. Implement external knowledge retrieval to evaluate improvements in ambiguity identification and clarifying questions
3. Conduct detailed ECE and AUROC analysis across confidence thresholds to identify unreliable prediction ranges and test temperature tuning mitigation