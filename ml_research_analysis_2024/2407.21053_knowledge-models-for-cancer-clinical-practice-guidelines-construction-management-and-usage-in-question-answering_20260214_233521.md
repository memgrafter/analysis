---
ver: rpa2
title: 'Knowledge Models for Cancer Clinical Practice Guidelines : Construction, Management
  and Usage in Question Answering'
arxiv_id: '2407.21053'
source_url: https://arxiv.org/abs/2407.21053
tags:
- knowledge
- cancer
- algorithm
- nccn
- guideline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an improved automated knowledge modeling algorithm
  to extract and structure information from National Comprehensive Cancer Network
  (NCCN) Clinical Practice Guidelines (CPGs) in Oncology for various cancer types.
  The algorithm extracts treatment pathways and discussion content from NCCN guidelines
  and transforms them into a programmatically interactable, easy-to-update structured
  model.
---

# Knowledge Models for Cancer Clinical Practice Guidelines : Construction, Management and Usage in Question Answering

## Quick Facts
- arXiv ID: 2407.21053
- Source URL: https://arxiv.org/abs/2407.21053
- Authors: Pralaypati Ta; Bhumika Gupta; Arihant Jain; Sneha Sree C; Keerthi Ram; Mohanasankar Sivaprakasam
- Reference count: 25
- Primary result: Automated knowledge modeling algorithm extracts treatment pathways from NCCN CPGs, achieving 54.5% accuracy (treatment algorithm) and 81.8% accuracy (discussion section) in QA evaluation.

## Executive Summary
This paper presents an automated algorithm to extract and structure knowledge from NCCN Clinical Practice Guidelines in Oncology, transforming graphical treatment algorithms and textual discussions into programmatically interactable models. The approach enables systematic comparison of guideline versions and supports question answering about treatment protocols. The system was evaluated on four cancer types and demonstrated effective knowledge extraction with a hybrid retrieval mechanism combining dense and sparse approaches for improved QA performance.

## Method Summary
The method involves parsing NCCN guideline PDFs using Apache PDFBox to extract both graphical treatment algorithms and textual discussion content, which are then transformed into structured knowledge models. A hybrid retrieval mechanism combines dense embeddings (fine-tuned on synthetic questions generated from guideline content) with BM25 sparse retrieval using reciprocal rank fusion. The question-answering framework employs a RAG pipeline with Llama-2-70b-chat-hf LLM to generate answers grounded in the retrieved guideline content. The system also includes a version comparison algorithm using hash-based detection and edit-distance scoring to identify changes between guideline versions.

## Key Results
- Knowledge extraction achieved Node Formation Accuracy across four cancer types (Breast, Ovarian, Prostate, and NSCLC)
- Question-answering framework achieved 54.5% accuracy from treatment algorithm section and 81.8% accuracy from discussion section for NSCLC
- Version comparison algorithm successfully identified additions, deletions, and modifications between guideline versions using hash matching and edit-distance scoring
- Fine-tuned embedding model improved domain-specific retrieval capability compared to base embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid retrieval (dense + sparse) improves question-answering accuracy by balancing semantic relevance and lexical matching.
- Mechanism: Dense retrievers capture semantic meaning but can miss exact terms; sparse retrievers (BM25) handle exact matches but miss synonyms. Combining them with Reciprocal Rank Fusion (RRF) gives the best of both.
- Core assumption: The treatment-related content in CPGs contains both precise medical terminology and context-dependent phrasing.
- Evidence anchors:
  - [abstract] "The framework was evaluated against the question-answer pairs from one data source, and it can generate the answers with 54.5% accuracy from the treatment algorithm and 81.8% accuracy from the discussion part of the NCCN NSCLC guideline knowledge model."
  - [section] "As in the QA framework for the algorithm section, both a dense and sparse index were created using the paragraphs of the discussion section."
- Break condition: If the guideline content is overly concise (like algorithm nodes), the sparse retriever may dominate and miss semantically relevant but lexically different content.

### Mechanism 2
- Claim: Fine-tuning the embedding model on synthetic questions from CPG content improves domain-specific retrieval performance.
- Mechanism: Standard embeddings are trained on general corpora; domain-specific fine-tuning aligns the embedding space with CPG terminology and structure, improving retrieval relevance.
- Core assumption: Synthetic question generation from guideline content captures the language patterns used in real clinical queries.
- Evidence anchors:
  - [abstract] "We fine-tuned an embedding model to improve its domain-specific retrieval capability in the RAG pipeline by generating synthetic question-answers from the guideline content."
  - [section] "The retrieval model was fine-tuned using the generated questions and the matching paragraphs to adapt the model for the domain-specific content."
- Break condition: If synthetic questions are too generic or miss clinical nuance, the fine-tuning may not generalize to real-world questions.

### Mechanism 3
- Claim: Hash-based comparison of node content allows precise detection of changes between guideline versions, distinguishing additions, deletions, and modifications.
- Mechanism: Hash matching quickly identifies unchanged nodes; edit-distance-based similarity scores between differing hashes classify changes and quantify modification extent.
- Core assumption: The content of nodes is stable enough that hash collisions are rare, and edit distance meaningfully captures content change.
- Evidence anchors:
  - [abstract] "We proposed an algorithm to compare the knowledge models for different versions of a guideline to discover the specific changes introduced in the treatment protocol of a new version."
  - [section] "The hash of the node contents is matched to find the unchanged nodes. For nodes with varying hash values, a pairwise edit distance-based similarity score between the node contents is calculated."
- Break condition: If guideline updates are mostly structural (reordering, formatting) without semantic change, hash-based comparison may over-report modifications.

## Foundational Learning

- Concept: Clinical Practice Guidelines (CPGs) structure
  - Why needed here: The algorithm must parse both graphical treatment algorithms and textual discussion sections, which have different formats.
  - Quick check question: What are the two main sections of NCCN CPGs that this work extracts knowledge from?

- Concept: Graph path traversal
  - Why needed here: Treatment pathways are represented as directed graphs; finding paths from start to treatment nodes is essential for QA.
  - Quick check question: How does the algorithm determine which treatment options lead to a specific patient condition?

- Concept: Embedding fine-tuning
  - Why needed here: Off-the-shelf embeddings underperform on specialized medical text; fine-tuning aligns the embedding space with CPG language.
  - Quick check question: What is the purpose of generating synthetic question-answer pairs from guideline content?

## Architecture Onboarding

- Component map:
  PDF parser (Apache PDFBox) -> HTML extraction -> HTML parser -> Node/edge extraction (algorithm section) + paragraph extraction (discussion section) -> Graph builder -> Node content hashing -> Version comparison engine -> Embedding fine-tuner (synthetic QA generation + training) -> Hybrid retriever (dense + BM25 + RRF) -> LLM answer generator

- Critical path:
  1. Parse guideline PDF -> Extract nodes/edges + paragraphs
  2. Build knowledge model graph
  3. Fine-tune embedding on synthetic QA
  4. Index graph nodes and paragraphs (dense + sparse)
  5. For a query: retrieve top documents -> generate answer via LLM

- Design tradeoffs:
  - Dense vs. sparse retrieval: semantic richness vs. exact term matching
  - Graph vs. text-based QA: concise pathways vs. detailed explanations
  - Synthetic vs. real QA data: scalability vs. clinical realism

- Failure signatures:
  - Low retrieval recall: either dense or sparse index missing key content
  - LLM hallucinations: over-reliance on generated answers without grounding
  - Version comparison noise: structural changes flagged as semantic modifications

- First 3 experiments:
  1. Evaluate retrieval accuracy with only dense vs. only sparse vs. hybrid on a small CPG sample.
  2. Test fine-tuned vs. base embedding on a set of synthetic and real CPG questions.
  3. Run version comparison on two CPG versions with known changes to validate edit-distance scoring.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the knowledge extraction algorithm be further improved to handle even more complex variations in NCCN guideline structures across different cancer types?
- Basis in paper: [inferred] The paper mentions that the existing automated algorithms have minimal scope and cannot handle the varying complexity of the knowledge content in CPGs for different cancer types. The proposed algorithm is evaluated on four cancer types but there may be more complex variations not yet encountered.
- Why unresolved: The paper only evaluates the algorithm on four cancer types (Breast, Ovarian, Prostate, and NSCLC). There may be more complex guideline structures for other cancer types that the algorithm hasn't been tested on yet.
- What evidence would resolve it: Evaluating the algorithm on a larger and more diverse set of cancer types, including those with known complex guideline structures, and measuring its performance would provide evidence of its robustness and limitations.

### Open Question 2
- Question: What specific improvements could be made to the question-answering framework to increase its accuracy, especially when using the algorithm section of the guidelines?
- Basis in paper: [explicit] The paper states that the framework achieves 54.5% accuracy from the treatment algorithm and 81.8% accuracy from the discussion part of the NCCN NSCLC guideline knowledge model. The authors note that the performance is lower when using the algorithm section because it represents treatment options concisely.
- Why unresolved: The authors acknowledge the lower performance of the Q&A framework when using the algorithm section but do not provide specific solutions to improve it. The concise representation of treatment options in the algorithm section seems to be a key challenge.
- What evidence would resolve it: Experimenting with different techniques to better capture and represent the concise information in the algorithm section, such as more advanced embedding methods or incorporating additional context from related sections, could potentially improve the accuracy of the Q&A framework.

### Open Question 3
- Question: How can the knowledge model comparison algorithm be enhanced to provide more granular insights into the changes between guideline versions, beyond just identifying additions, deletions, and modifications?
- Basis in paper: [explicit] The paper proposes an algorithm to compare knowledge models for different versions of an NCCN guideline, which can identify the exact location of changes and categorize them as additions, deletions, or modifications. However, it doesn't provide more detailed insights into the nature of these changes.
- Why unresolved: While the current algorithm can identify the types of changes, it doesn't provide a deeper understanding of the implications or context of these changes. For example, it doesn't highlight changes that might have a significant impact on treatment decisions or identify trends in guideline evolution over time.
- What evidence would resolve it: Developing a more sophisticated comparison algorithm that not only identifies changes but also analyzes their potential impact, categorizes them based on their significance, and visualizes trends in guideline evolution could provide more valuable insights to healthcare professionals.

## Limitations

- The QA framework's 54.5% accuracy on treatment algorithm content is relatively low compared to 81.8% accuracy on discussion sections, indicating challenges with concise algorithmic representations
- The evaluation is based on a limited dataset of 32 question-answer pairs, which may not capture the full complexity and diversity of clinical queries
- The synthetic question generation process for embedding fine-tuning lacks detailed validation to ensure questions adequately represent real clinical scenarios

## Confidence

- High Confidence: Technical feasibility of PDF parsing and HTML processing for knowledge extraction; version comparison mechanism using hash matching and edit-distance scoring
- Medium Confidence: Hybrid retrieval mechanism combining dense and sparse approaches; synthetic question generation for fine-tuning embeddings
- Low Confidence: Generalizability of reported accuracy rates to broader clinical contexts; effectiveness of the approach across diverse guideline formats

## Next Checks

1. **Cross-validation with expanded dataset**: Test the QA framework on a larger, more diverse set of clinical questions (minimum 100 pairs) from multiple cancer types and guideline versions to verify the reported accuracy rates hold across different clinical contexts.

2. **Ablation study on retrieval components**: Systematically evaluate the impact of removing either the dense or sparse retrieval component, and test different fusion methods beyond reciprocal rank fusion to quantify the specific contribution of the hybrid approach.

3. **Clinical expert validation**: Conduct a blind evaluation where practicing oncologists review a sample of generated answers (at least 50) for both accuracy and clinical appropriateness, focusing on cases where the model achieved high confidence but may produce clinically questionable recommendations.