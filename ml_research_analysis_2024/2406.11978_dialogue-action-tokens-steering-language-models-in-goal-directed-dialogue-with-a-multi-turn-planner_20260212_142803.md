---
ver: rpa2
title: 'Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue
  with a Multi-Turn Planner'
arxiv_id: '2406.11978'
source_url: https://arxiv.org/abs/2406.11978
tags:
- arxiv
- dialogue
- language
- action
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dialogue Action Tokens (DAT) converts goal-directed dialogue into
  a game by treating each utterance as an action, allowing reinforcement learning
  to improve language model (LM) performance without language degradation. The approach
  freezes a pretrained LM and trains a small planner to predict continuous action
  vectors, which control generation through prefix tokens.
---

# Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner

## Quick Facts
- **arXiv ID:** 2406.11978
- **Source URL:** https://arxiv.org/abs/2406.11978
- **Reference count:** 11
- **One-line primary result:** DAT steers LLaMA-3 to significantly increase harmful response elicitation rates in red-teaming, surpassing state-of-the-art methods.

## Executive Summary
Dialogue Action Tokens (DAT) introduces a method to control language models in goal-directed dialogues by treating each utterance as an action in a reinforcement learning framework. The key innovation is freezing a pretrained language model and training a small planner to predict continuous action vectors, which are expanded into prefix tokens to steer generation. This approach avoids language degradation while enabling effective multi-turn planning. DAT is evaluated on social capability simulations (Sotopia) and red-teaming (HarmBench), showing superior performance compared to baselines and GPT-4 in some settings.

## Method Summary
DAT converts dialogue planning into a low-dimensional continuous control problem by freezing a pretrained language model and training a small planner to predict continuous action vectors. These vectors are expanded via an up-mapping matrix into prefix tokens, which are prepended to the dialogue history embeddings to guide the frozen model's generation. The method uses self-cloning to initialize the planner, followed by reinforcement learning (TD3+BC) to optimize for goal-directed dialogue outcomes. DAT is evaluated on social capability in simulated dialogues (Sotopia) and red-teaming (HarmBench), demonstrating significant improvements over baselines.

## Key Results
- DAT steers LLaMA-3 to achieve significantly higher success rates in eliciting harmful responses compared to state-of-the-art red-teaming methods.
- In social simulations on Sotopia, DAT surpasses GPT-4's performance in improving social capability scores.
- DAT avoids language degradation by freezing the pretrained language model and only training a small planner.

## Why This Works (Mechanism)

### Mechanism 1
Freezing the language model and training only a small planner avoids language degradation while enabling goal-directed dialogue control. By keeping the pretrained LM frozen, the model's natural language generation capabilities remain intact. The small planner learns to predict continuous action vectors that steer the LM without modifying its parameters, constraining optimization to a low-dimensional space and preventing catastrophic forgetting seen in full fine-tuning. **Core assumption:** The frozen LM can still generate coherent and contextually appropriate responses when guided by prefix tokens, and the planner can learn effective steering without needing to modify the LM's parameters. **Evidence anchors:** [abstract] "This design avoids the problem of language degradation under reward optimization." [section] "By embedding the conversation history with the transformer itself and guiding the decoding process with dialogue action tokens, we effectively converted the language space MDP into a vector space."

### Mechanism 2
Converting dialogue into a low-dimensional continuous control problem makes reinforcement learning tractable and effective. Each utterance is treated as an action, and the planner predicts a continuous vector that, when expanded via an up-mapping matrix and prepended as prefix tokens, guides the LM's generation. This transforms the complex discrete language space into a continuous vector space where RL algorithms like TD3+BC can be applied effectively. **Core assumption:** The continuous action space is sufficiently expressive to capture necessary variations in dialogue strategy while being low-dimensional enough for stable RL training. **Evidence anchors:** [abstract] "The key contribution of the proposed DAT framework is a series of design choices that convert the problem of planning in the language space into a low-dimensional continuous-control problem." [section] "This design constrains the influence RL training can have over the LM while incorporating multi-turn planning."

### Mechanism 3
Self-cloning the planner to match the frozen LM's behavior provides a stable initialization for subsequent RL training. The planner and up-mapping matrix are first trained to reproduce the LM's unsteered outputs, ensuring that the steered generation matches the baseline before any RL optimization. This creates a good starting point in the action space and prevents early degradation from poor initialization. **Core assumption:** The planner can learn to predict action vectors that, when expanded and prepended, produce outputs nearly identical to the frozen LM's natural generation. **Evidence anchors:** [section] "Although it does not bring about an immediate performance boost, it provides a good starting point for the subsequent RL training." [section] "After training with self-clone loss, the steered model's performance will match the unsteered baseline."

## Foundational Learning

- **Concept:** Markov Decision Process (MDP) formulation for sequential decision making
  - **Why needed here:** The dialogue problem is modeled as an MDP where states are conversation histories, actions are utterances, and rewards are based on goal achievement, providing the formal framework for RL application.
  - **Quick check question:** In the DAT formulation, what constitutes a state, an action, and a reward in the dialogue MDP?

- **Concept:** Reinforcement learning with continuous action spaces
  - **Why needed here:** DAT converts dialogue planning into continuous control by predicting action vectors, requiring RL algorithms designed for continuous rather than discrete action spaces.
  - **Quick check question:** Why does DAT use algorithms like TD3+BC instead of standard discrete-action RL algorithms?

- **Concept:** Prefix tuning and controlled language generation
  - **Why needed here:** DAT uses prefix tokens predicted by the planner to steer the frozen LM's generation, requiring understanding of how prepended continuous vectors can influence autoregressive generation.
  - **Quick check question:** How do the dialogue action tokens influence the LM's generation without modifying its parameters?

## Architecture Onboarding

- **Component map:**
  - Environment/Simulator -> Tokenizer and embedding matrix -> Planner MLP -> Up-mapping matrix W -> Frozen language model (LLM) -> Judge model

- **Critical path:**
  1. Environment provides initial scenario and dialogue history
  2. Embeddings extracted from dialogue history
  3. Planner predicts action vector from embeddings
  4. Action vector expanded by W to form prefix tokens
  5. Prefix tokens prepended to dialogue history embeddings
  6. Frozen LLM generates next utterance conditioned on modified input
  7. Dialogue partner responds, history updated
  8. Judge model evaluates and provides reward
  9. Reward used for planner training (self-cloning or RL)

- **Design tradeoffs:**
  - Number of prefix tokens (L): More tokens provide greater steering capacity but risk language degradation; fewer tokens are safer but may limit expressiveness
  - Action space dimensionality (d'): Higher dimensions allow finer control but make RL training harder and less sample efficient
  - RL algorithm choice: Offline algorithms like TD3+BC are safer and cheaper but may be less sample efficient than online methods
  - Reward signal quality: Cheap automated rewards enable large-scale training but may not perfectly capture human preferences

- **Failure signatures:**
  - Language quality degradation: Planner produces action vectors that, when expanded, create unnatural or incoherent prefix tokens
  - Reward hacking: Planner learns to exploit reward function loopholes rather than genuinely improving dialogue quality
  - Mode collapse: Planner converges to very limited action patterns, reducing dialogue diversity
  - Poor transfer: Planner trained on one partner or scenario fails to generalize to new settings

- **First 3 experiments:**
  1. **Self-cloning verification:** Train planner with self-cloning loss and verify that steered generation matches unsteered baseline across multiple scenarios
  2. **RL ablation study:** Compare DAT with and without the self-cloning initialization step to measure impact on convergence and final performance
  3. **Action space sensitivity:** Sweep across different action space dimensionalities (d') to find the optimal balance between expressiveness and training stability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the dimensionality of the action space (d') affect the performance of the Dialogue Action Tokens (DAT) method?
- **Basis in paper:** Inferred from the hyperparameter sweeps described in Appendix C, which varied the action space dimensionality (d') in the red teaming experiment.
- **Why unresolved:** The paper only provides preliminary results from varying d' and does not offer a conclusive answer on the optimal dimensionality for different tasks.
- **What evidence would resolve it:** Conducting a comprehensive study with a wider range of d' values and tasks to determine the optimal dimensionality for each scenario.

### Open Question 2
- **Question:** What is the impact of the number of prefix tokens (L) on the language generation quality and task performance?
- **Basis in paper:** Inferred from the experiments that varied the number of prefix tokens (L) in the red teaming scenario, as mentioned in Appendix C.
- **Why unresolved:** The paper suggests that increasing the number of prefix tokens may lead to degraded language quality, but does not provide a detailed analysis of the trade-off between language quality and task performance.
- **What evidence would resolve it:** Performing a detailed study that evaluates both language quality and task performance across different values of L.

### Open Question 3
- **Question:** How does the choice of RL algorithm affect the performance of DAT?
- **Basis in paper:** Inferred from the remark in Section 5.2, which states that the choice of TD3+BC is non-essential and a more carefully chosen or tuned algorithm might surpass the results presented.
- **Why unresolved:** The paper uses TD3+BC but does not explore other RL algorithms that could potentially improve performance.
- **What evidence would resolve it:** Comparing the performance of DAT using different RL algorithms on the same tasks to identify the most effective one.

## Limitations
- Evaluation is limited to synthetic environments (Sotopia, HarmBench) rather than human evaluations, making it difficult to verify claims about surpassing GPT-4 and achieving state-of-the-art results.
- The method's performance is only demonstrated on LLaMA-2 and LLaMA-3, with no exploration of generalization to other model families or domains.
- The optimal action space dimensionality (d') is treated as a hyperparameter without theoretical guidance, and the trade-off between steering capacity and language quality is not systematically analyzed.

## Confidence
- **DAT framework effectively converts goal-directed dialogue into tractable continuous control:** Medium
- **Freezing the language model prevents language degradation while enabling goal-directed control:** Low-Medium
- **DAT achieves superior performance in social simulations and red-teaming compared to state-of-the-art methods:** Low-Medium

## Next Checks
1. **Language Quality Validation:** Conduct a comprehensive comparison of language quality between DAT-steered outputs, the frozen baseline, and a fully fine-tuned baseline using both automatic metrics (perplexity, BLEU) and human evaluations across diverse dialogue scenarios to empirically verify the claim of avoiding language degradation.

2. **Action Space Sensitivity Analysis:** Perform a systematic ablation study sweeping across different action space dimensionalities (d') and numbers of prefix tokens (L) to identify the optimal configuration and analyze the trade-off between steering capacity, training stability, and language quality.

3. **Generalization and Transfer Study:** Evaluate DAT's performance when transferring the trained planner from one dialogue partner or scenario to novel partners/scenarios not seen during training, and test the method on different model families (e.g., Mistral, Mixtral) to assess its robustness and generalization capabilities.