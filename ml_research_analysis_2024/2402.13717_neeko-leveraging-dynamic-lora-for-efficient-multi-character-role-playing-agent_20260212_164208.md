---
ver: rpa2
title: 'Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing
  Agent'
arxiv_id: '2402.13717'
source_url: https://arxiv.org/abs/2402.13717
tags:
- character
- lora
- agent
- evaluation
- role-playing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of enabling language models\
  \ to role-play multiple characters effectively. It proposes Neeko, a framework that\
  \ uses dynamic LoRA blocks\u2014one per character\u2014combined with a gating network\
  \ for character selection."
---

# Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent

## Quick Facts
- arXiv ID: 2402.13717
- Source URL: https://arxiv.org/abs/2402.13717
- Authors: Xiaoyan Yu, Tongxu Luo, Yifan Wei, Fangyu Lei, Yiming Huang, Hao Peng, Liehuang Zhu
- Reference count: 40
- One-line primary result: Dynamic LoRA blocks with gating network enable efficient multi-character role-playing with strong performance across consistency metrics

## Executive Summary
This paper addresses the challenge of enabling language models to role-play multiple characters effectively. Neeko introduces a framework using dynamic LoRA blocks—one per character—combined with a gating network for character selection. The system supports both seen and unseen characters through fusion and expansion strategies for incremental learning. Experiments on the Character-LLM-Data dataset show that Neeko outperforms existing methods in multi-character role-playing tasks, achieving higher scores across multiple evaluation metrics while maintaining low training costs.

## Method Summary
Neeko employs non-overlapping LoRA blocks for each character within a shared base model, with a gating network using role embeddings to select the appropriate block during inference. The framework supports incremental learning through fusion (combining existing blocks for new characters) or expansion (adding new blocks when sufficient data is available). The approach is evaluated on the Character-LLM-Data dataset using LLaMA-2 or ChatGLM as backbone models, with performance measured across character consistency, knowledge consistency, and dialogue consistency metrics using GPT-3.5 as judge.

## Key Results
- Outperforms baseline methods (ICL, RAG, LoRA) across all evaluation metrics
- Achieves higher character consistency scores while maintaining knowledge consistency
- Supports incremental learning for both seen and unseen characters with minimal performance degradation
- Maintains low training costs through parameter-efficient LoRA adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic LoRA blocks allow independent character adaptation without interference.
- Mechanism: Each character has its own non-overlapping LoRA block within the shared B and A matrices, ensuring character-specific updates don't affect others.
- Core assumption: Non-overlapping parameter blocks guarantee isolation between character adaptations.
- Evidence anchors:
  - [abstract]: "distinct LoRA blocks for each character, alongside incorporating a gating network for role selection"
  - [section]: "Rather than randomly selecting the range of LoRA ranks, we introduce a LoRA module consisting of non-overlapping LoRA blocks for different characters"
  - [corpus]: Weak - No direct evidence in related papers about non-overlapping blocks for character isolation
- Break condition: If rank allocation is not properly managed, blocks may overlap and cause interference.

### Mechanism 2
- Claim: Gating network enables precise character selection during inference.
- Mechanism: A global role embedding matrix is created during pre-training, and the gating network selects the appropriate LoRA block based on similarity to user instructions.
- Core assumption: Role embeddings can be effectively learned from character profiles to enable accurate selection.
- Evidence anchors:
  - [abstract]: "alongside incorporating a gating network for role selection"
  - [section]: "During the inference phase, Neeko utilizes a Mix of Experts (MoE) gate mechanism to determine and activate the appropriate role-specific LoRA block"
  - [corpus]: Weak - Related papers mention role selection but lack detailed gating mechanisms
- Break condition: If role embeddings are not precise, the gating network may activate incorrect LoRA blocks.

### Mechanism 3
- Claim: Fusion and expansion strategies enable incremental learning without catastrophic forgetting.
- Mechanism: Fusion combines existing LoRA blocks for new characters, while expansion adds new blocks for new characters with sufficient data.
- Core assumption: Non-overlapping LoRA blocks allow new character learning without affecting existing character performance.
- Evidence anchors:
  - [abstract]: "For the incremental learning of unseen or novel characters, Neeko provides two strategies, fusion and expansion"
  - [section]: "In the fusion strategy, LoRA blocks for new characters are acquired by employing an element-wise method to combine corresponding parameters"
  - [corpus]: Weak - Limited evidence in related papers about incremental learning strategies for role-playing
- Break condition: If new character data is insufficient, fusion may lead to degraded performance for both new and existing characters.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Enables efficient fine-tuning of LLMs for multiple characters with minimal parameter changes
  - Quick check question: What is the primary advantage of using LoRA over full fine-tuning for character adaptation?

- Concept: Mixture of Experts (MoE)
  - Why needed here: Provides the gating mechanism for selecting appropriate LoRA blocks based on character requirements
  - Quick check question: How does the MoE gating mechanism determine which LoRA block to activate for a given character?

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding this concept is crucial for designing effective incremental learning strategies
  - Quick check question: What are the main causes of catastrophic forgetting in sequential fine-tuning scenarios?

## Architecture Onboarding

- Component map: Base LLM -> Dynamic LoRA module (non-overlapping blocks) -> Gating network (MoE-style) -> Global role embedding matrix -> Fusion/expansion modules

- Critical path:
  1. Pre-training: Create base LLM + initialize LoRA blocks
  2. Character training: Train individual LoRA blocks
  3. Gating network training: Learn role embeddings and selection weights
  4. Inference: Select and activate appropriate LoRA block
  5. Incremental learning: Apply fusion or expansion for new characters

- Design tradeoffs:
  - Non-overlapping vs. overlapping LoRA blocks (isolation vs. parameter efficiency)
  - Fusion vs. expansion for incremental learning (data efficiency vs. performance)
  - Fixed vs. dynamic gating network (simplicity vs. adaptability)

- Failure signatures:
  - Incorrect character selection: Gating network issues
  - Performance degradation: Overlapping LoRA blocks or insufficient training
  - Slow inference: Inefficient gating mechanism or large number of LoRA blocks

- First 3 experiments:
  1. Single character role-playing with baseline LoRA to establish performance baseline
  2. Multi-character role-playing with fixed LoRA blocks to test isolation
  3. Incremental learning with fusion strategy using limited data for new character

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations of using a simple similarity-based approach for role selection in the gating network, and how can more advanced role learning methods improve the performance of Neeko?
- Basis in paper: [inferred] The paper mentions that the gating network uses a similarity-based approach to select the appropriate LoRA block, but also notes that this may lead to less precise representations of roles and potentially accumulate errors.
- Why unresolved: The paper acknowledges the potential limitations of the current role selection method but does not explore alternative approaches or provide evidence of their effectiveness.
- What evidence would resolve it: Experimental results comparing Neeko with different role learning methods, such as contrastive learning or prompt-based approaches, would provide insights into the effectiveness of more advanced role selection techniques.

### Open Question 2
- Question: How does Neeko handle situations where the user provides conflicting or ambiguous instructions for role-playing, and what strategies can be employed to resolve such conflicts?
- Basis in paper: [inferred] The paper discusses the importance of handling user-specified characters and mentions that users may not always adhere to the meta prompt, but does not explicitly address how Neeko deals with conflicting or ambiguous instructions.
- Why unresolved: The paper does not provide details on how Neeko handles conflicting or ambiguous instructions, which could impact the quality of role-playing and user experience.
- What evidence would resolve it: Case studies or user studies demonstrating Neeko's performance in handling conflicting or ambiguous instructions would provide insights into its robustness and user-friendliness.

### Open Question 3
- Question: How does the performance of Neeko scale with the number of characters, and what are the practical limitations in terms of memory and computational resources?
- Basis in paper: [explicit] The paper mentions that Neeko can theoretically play an unlimited number of characters, but does not provide empirical evidence on its performance and resource usage as the number of characters increases.
- Why unresolved: The paper does not investigate the scalability of Neeko and its practical limitations in terms of memory and computational resources, which are important factors for real-world applications.
- What evidence would resolve it: Experiments evaluating Neeko's performance and resource usage with varying numbers of characters would provide insights into its scalability and practical limitations.

## Limitations
- Non-overlapping LoRA blocks mechanism lacks strong empirical validation from related work
- Gating network implementation details are sparse, particularly regarding role embedding learning
- Limited exploration of alternative role selection methods beyond similarity-based approach
- No clear criteria defined for choosing between fusion and expansion strategies in incremental learning

## Confidence

- **High confidence**: Overall framework design and multi-character role-playing task formulation; evaluation methodology using GPT-3.5 judge
- **Medium confidence**: Performance claims supported by experimental results, though individual component contributions unclear without ablation studies
- **Low confidence**: Non-overlapping LoRA blocks preventing interference mechanism (theoretically sound but limited validation); gating network effectiveness depends heavily on role embedding quality

## Next Checks

1. **Ablation study on LoRA block isolation**: Systematically test whether overlapping vs. non-overlapping LoRA blocks significantly impact multi-character performance, including quantitative measurements of parameter interference and character consistency degradation.

2. **Gating network robustness evaluation**: Test the gating mechanism's performance across diverse input prompts, including edge cases like ambiguous character descriptions or prompts that could match multiple characters, to assess selection accuracy and failure modes.

3. **Incremental learning boundary conditions**: Conduct experiments varying the amount of new character data to determine the precise thresholds where fusion becomes ineffective and expansion becomes necessary, documenting performance trade-offs across the spectrum.