---
ver: rpa2
title: Mitigating Large Language Model Hallucination with Faithful Finetuning
arxiv_id: '2406.11267'
source_url: https://arxiv.org/abs/2406.11267
tags:
- language
- hallucination
- question
- llms
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Faithful Finetuning (F2), a novel method to
  mitigate hallucinations in large language models for question-answering tasks. F2
  decomposes the QA objective into internal fact retrieval and fact-grounded QA, and
  employs weighted objectives and targeted fine-tuning on hallucination-prone spans
  and layers.
---

# Mitigating Large Language Model Hallucination with Faithful Finetuning

## Quick Facts
- arXiv ID: 2406.11267
- Source URL: https://arxiv.org/abs/2406.11267
- Reference count: 33
- This paper proposes Faithful Finetuning (F2), a novel method to mitigate hallucinations in large language models for question-answering tasks.

## Executive Summary
This paper addresses the critical problem of hallucination in large language models (LLMs) during question-answering tasks. The authors propose Faithful Finetuning (F2), a method that decomposes the QA objective into internal fact retrieval and fact-grounded QA, then employs weighted objectives and targeted fine-tuning on hallucination-prone spans and layers. The approach shows significant improvements over vanilla models and baselines on both TruthfulQA and FACTOR datasets, with F2 also demonstrated to be orthogonal to existing hallucination mitigation methods.

## Method Summary
Faithful Finetuning (F2) is a multi-component approach that first decomposes the QA task into two explicit sub-objectives: Internal Fact Retrieval and Fact-grounded QA. The method uses weighted cross-entropy loss to focus on hallucination-prone spans identified through entity-based and attention-based heuristics, and employs targeted fine-tuning on specific model layers identified as most associated with hallucination behavior. The approach uses LoRA for conservative model updates and combines these techniques to improve faithfulness in LLM responses.

## Key Results
- F2 significantly improves MC1, MC2, and MC3 scores on TruthfulQA dataset compared to vanilla LLaMA-2 7B
- F2 achieves higher selection accuracy on FACTOR dataset for factuality evaluation
- F2 demonstrates orthogonal benefits when combined with state-of-the-art hallucination mitigation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted loss functions on hallucination-prone spans improve faithfulness
- Mechanism: By assigning higher weights to spans identified via entity and attention-based heuristics, the model is trained to prioritize accurate retrieval and generation of these critical information units during fine-tuning
- Core assumption: The model's tendency to hallucinate is concentrated in specific spans rather than uniformly distributed across all tokens
- Evidence anchors:
  - [abstract] "We design a targeted fine-tuning approach that focuses on hotspots identified by the entity-based and attention-based heuristics within the retrieved fact spans"
  - [section 3.2] "we employ a targeted fine-tuning approach that focuses on hotspots identified within the retrieved fact spans in LR(Ï•), as well as layers in the LLMs"
  - [corpus] Found 25 related papers, but only 5 had direct relevance to hallucination mitigation methods

### Mechanism 2
- Claim: Multi-objective decomposition of QA tasks improves internal knowledge utilization
- Mechanism: By decomposing the QA task into fact retrieval and fact-grounded QA sub-objectives, the model learns to first access its internal knowledge before generating answers, creating a more structured reasoning process
- Core assumption: LLMs have sufficient internal knowledge but need explicit training signals to access it effectively for QA tasks
- Evidence anchors:
  - [abstract] "we propose decomposing the QA task by adding two explicit sub-objectives: Internal Fact Retrieval and Fact-grounded QA"
  - [section 3.1.3] "The Fact-grounded Question-Answering (FQA) objective is specifically designed to encourage language models to generate responses a that are firmly grounded in the fact k retrieved from their internal memory"
  - [corpus] The corpus shows several papers on knowledge distillation and retrieval methods, but limited direct evidence for multi-objective decomposition

### Mechanism 3
- Claim: Targeted fine-tuning on hallucination-prone layers reduces hallucination occurrence
- Mechanism: By selecting and fine-tuning only the top 10 layers most strongly associated with hallucination, the model can be edited without disrupting other important functionalities
- Core assumption: Hallucination behavior is localized to specific layers rather than distributed across the entire network
- Evidence anchors:
  - [section 3.2.3] "we adopt the approach proposed in TruthX (Zhang et al., 2024). This method involves fine-tuning only the top 10 modules most strongly associated with hallucination, as determined by probing accuracy on the validation set"
  - [section 3.2.3] "TruthX selects the top 10 modules with the highest probing accuracy from a total of 64 modules (32 attention modules and 32 FFN modules) for model editing"
  - [corpus] The corpus includes one relevant paper on layer selection strategies

## Foundational Learning

- Concept: Cross-entropy loss and weighted cross-entropy loss
  - Why needed here: Understanding how different loss formulations can guide model behavior during fine-tuning, particularly for hallucination mitigation
  - Quick check question: What is the mathematical difference between standard cross-entropy loss and weighted cross-entropy loss, and how does this difference affect gradient updates?

- Concept: Multi-task learning and objective decomposition
  - Why needed here: The F2 method decomposes the QA objective into multiple sub-tasks, requiring understanding of how to combine and balance multiple training objectives
  - Quick check question: How does combining multiple loss functions affect the overall training dynamics, and what are common strategies for balancing their relative importance?

- Concept: Attention mechanisms and attention-based heuristics
  - Why needed here: The method uses attention-based heuristics to identify hallucination-prone spans, requiring understanding of how attention weights relate to token importance
  - Quick check question: How can attention weights be interpreted as measures of token importance, and what are the limitations of this interpretation?

## Architecture Onboarding

- Component map: Standard LLM fine-tuning pipeline with three new components: (1) multi-objective decomposition with internal fact retrieval and fact-grounded QA tasks, (2) entity and attention-based heuristics for span identification, and (3) targeted layer selection for fine-tuning
- Critical path: The most critical path is the weighted loss computation on hallucination-prone spans, as this directly drives the model's behavior change during fine-tuning
- Design tradeoffs: The method trades computational efficiency (by fine-tuning fewer layers) for potential performance gains, and uses LoRA for conservative updates rather than full fine-tuning
- Failure signatures: If the model's hallucination behavior does not improve despite fine-tuning, or if the weighted loss causes training instability or degradation on other tasks
- First 3 experiments:
  1. Implement the basic multi-objective decomposition without any weighting strategies to establish baseline performance
  2. Add entity-based heuristics to identify and weight hallucination-prone spans
  3. Implement attention-based heuristics and combine with entity-based weighting to create the full F2 approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different weighting strategies (e.g., alpha values) impact the performance of Faithful Finetuning?
- Basis in paper: [explicit] The paper mentions that alpha is set to 1.1 for entity and attention-based heuristics but does not explore the impact of varying this value
- Why unresolved: The optimal weighting strategy is not investigated, leaving open the question of how sensitive F2 is to this hyperparameter
- What evidence would resolve it: Systematic ablation studies varying alpha and other weighting parameters, showing their impact on performance metrics like MC1, MC2, and MC3 scores

### Open Question 2
- Question: Can Faithful Finetuning be extended to other language model architectures beyond Llama2-7b?
- Basis in paper: [inferred] The paper focuses on Llama2-7b but does not discuss generalizability to other models like GPT or other transformer architectures
- Why unresolved: The method's applicability to different model architectures is unknown, limiting its broader impact
- What evidence would resolve it: Experiments applying F2 to other models (e.g., GPT-3, BERT) and comparing performance gains across architectures

### Open Question 3
- Question: What is the long-term impact of Faithful Finetuning on model generalization and performance on downstream tasks?
- Basis in paper: [inferred] The paper evaluates F2 on TruthfulQA and FACTOR datasets but does not investigate its effects on other NLP tasks or long-term model behavior
- Why unresolved: The trade-offs between improved factuality and potential performance degradation on other tasks are not explored
- What evidence would resolve it: Long-term studies evaluating F2's impact on model performance across diverse NLP tasks, including zero-shot and few-shot learning scenarios

## Limitations

- Limited external validation: Only 5 out of 25 examined papers were directly relevant to hallucination mitigation methods
- Implementation specificity: Critical components like entity-based and attention-based heuristics lack detailed implementation specifics
- Model architecture constraint: Effectiveness may be limited to LLaMA-2 7B and may not generalize to other model sizes or architectures

## Confidence

- Mechanism 1 (Weighted loss functions on hallucination-prone spans): Medium Confidence
- Mechanism 2 (Multi-objective decomposition): Low Confidence
- Mechanism 3 (Targeted fine-tuning on hallucination-prone layers): Medium Confidence

## Next Checks

1. **Ablation Study of Individual Components**: Conduct systematic ablation experiments to isolate the contribution of each mechanism (weighted loss, multi-objective decomposition, and targeted layer selection) to the overall performance improvement.

2. **Generalization Across Model Architectures**: Test the F2 method on different model sizes (e.g., LLaMA-2 13B, 33B) and architectures (e.g., Mistral, GPT-2 variants) to assess whether the approach generalizes beyond the specific model used in the original experiments.

3. **Long-term Stability Analysis**: Evaluate the model's performance over extended periods and after multiple fine-tuning sessions to assess whether the hallucination mitigation remains stable or if the model gradually reverts to previous behaviors.