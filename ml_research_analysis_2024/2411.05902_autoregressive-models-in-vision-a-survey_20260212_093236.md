---
ver: rpa2
title: 'Autoregressive Models in Vision: A Survey'
arxiv_id: '2411.05902'
source_url: https://arxiv.org/abs/2411.05902
tags:
- generation
- autoregressive
- image
- arxiv
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of autoregressive models
  in computer vision, covering foundational concepts, taxonomy, and applications.
  It categorizes visual autoregressive models into pixel-based, token-based, and scale-based
  approaches based on sequence representation strategies.
---

# Autoregressive Models in Vision: A Survey

## Quick Facts
- arXiv ID: 2411.05902
- Source URL: https://arxiv.org/abs/2411.05902
- Reference count: 40
- Primary result: Comprehensive survey of autoregressive models in computer vision covering foundational concepts, taxonomy, applications, and future directions

## Executive Summary
This survey provides a comprehensive overview of autoregressive models in computer vision, examining their evolution from NLP applications to visual domains. The paper systematically categorizes visual autoregressive models into pixel-based, token-based, and scale-based approaches based on sequence representation strategies. It explores interconnections with other generative models, provides detailed multi-perspective categorization of applications, and identifies key challenges and future research directions. The work serves as a structured reference for understanding the rapidly evolving field of visual autoregressive modeling.

## Method Summary
The survey employs a comprehensive literature review methodology, systematically analyzing existing research on autoregressive models in computer vision. It synthesizes findings from approximately 250 references to construct a taxonomy based on sequence representation strategies (pixel-level, token-level, scale-level) and autoregressive sequence modeling approaches. The survey examines interconnections between autoregressive models and other generative frameworks including VAEs, GANs, diffusion models, and MAEs, while providing detailed categorization of applications across image generation, video generation, 3D generation, and multimodal tasks.

## Key Results
- Autoregressive models can be categorized into pixel-based, token-based, and scale-based approaches based on sequence representation strategies
- Scale-based models achieve better efficiency (OT(N⁴/k⁴)) compared to pixel-based models (OT(N⁶)) through hierarchical representation
- The survey identifies integration opportunities between autoregressive models and other generative frameworks like diffusion models
- Key challenges include tokenizer design, architectural inductive biases, and adaptation to downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoregressive models excel at visual generation by capturing long-range dependencies through sequential prediction of visual elements
- Mechanism: The model predicts each visual element (pixel, token, or scale) conditioned on all previously generated elements, allowing it to build coherent structures progressively
- Core assumption: Visual data can be effectively represented as a sequence where each element depends on preceding elements
- Evidence anchors:
  - [abstract] "Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level"
  - [section] "Autoregressive models generally consist of two core components: Sequence Representation and Autoregressive Sequence Modeling"
  - [corpus] Weak - corpus doesn't contain direct evidence about this specific mechanism, though related concepts appear in neighbor papers
- Break condition: If visual data cannot be meaningfully linearized or if elements have complex bidirectional dependencies that unidirectional modeling cannot capture

### Mechanism 2
- Claim: Scale-based autoregressive models achieve better efficiency than pixel-based models by reducing sequence length through hierarchical representation
- Mechanism: By generating entire token maps at each scale level rather than individual pixels, the model reduces computational complexity from O(N⁶) to O(N⁴/k⁴)
- Core assumption: Visual information can be compressed hierarchically without losing essential generative capabilities
- Evidence anchors:
  - [section] "Generating a complete token map at each step, enables leveraging the inductive biases inherent in images to enhance the quality of visual generation"
  - [section] "The efficiency of token generation is improved by enabling parallel token generation within each token map, reducing the overall computational complexity"
  - [corpus] Weak - corpus doesn't provide specific efficiency comparisons for scale-based models
- Break condition: If hierarchical compression introduces artifacts that propagate through scales or if parallel generation within scales breaks coherence

### Mechanism 3
- Claim: Autoregressive models can be effectively integrated with other generative frameworks like diffusion models to combine their complementary strengths
- Mechanism: Autoregressive models provide sequential conditioning and long-range context while diffusion models handle local refinement and noise reduction
- Core assumption: The sequential nature of autoregressive modeling can complement the iterative refinement of diffusion processes
- Evidence anchors:
  - [section] "Kaleido Diffusion (Gu et al., 2024) maintains diffusion as the core generative framework while incorporating an autoregressive model to handle the latent conditions"
  - [section] "DART (Zhao et al., 2024) aims to enhance diffusion models by augmenting them with learnable discrete latents. They adopt an autoregressive transformer to model the distribution of these discrete latents"
  - [corpus] Moderate - neighbor papers show some integration attempts but limited direct evidence
- Break condition: If the integration creates conflicting objectives or if the sequential nature of autoregressive models interferes with diffusion's iterative refinement process

## Foundational Learning

- Concept: Sequence representation strategies (pixel-level, token-level, scale-level)
  - Why needed here: Understanding how visual data is transformed into sequences is fundamental to grasping autoregressive modeling approaches
  - Quick check question: What are the three main ways visual data can be represented as sequences in autoregressive models?

- Concept: Negative log-likelihood loss function and its role in autoregressive training
  - Why needed here: This is the core optimization objective that enables autoregressive models to learn conditional probability distributions
  - Quick check question: How is the training objective formulated for autoregressive models in terms of log-likelihood?

- Concept: Computational complexity analysis of different autoregressive paradigms
  - Why needed here: Understanding efficiency tradeoffs is crucial for choosing appropriate approaches for different applications
  - Quick check question: What is the computational complexity difference between next-pixel and next-scale prediction approaches?

## Architecture Onboarding

- Component map: Visual tokenizer → Sequence model (typically Transformer) → Generation head → (Optional) Post-processing
- Critical path: Tokenizer design → Model architecture selection → Training pipeline → Evaluation metrics
- Design tradeoffs: Resolution vs efficiency (pixel-level gives detail but is computationally expensive), discrete vs continuous representations (discrete is simpler but may lose information)
- Failure signatures: Poor codebook utilization (tokens not diverse enough), mode collapse (generated samples lack diversity), temporal inconsistency in video generation
- First 3 experiments:
  1. Implement a simple next-pixel prediction model on a small dataset to understand basic mechanics
  2. Compare pixel-level vs token-level generation on the same dataset to observe efficiency tradeoffs
  3. Test different tokenizer designs (VQGAN vs RQ-VAE) to understand impact on generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal visual tokenizer design that balances reconstruction quality, codebook utilization, and computational efficiency for large-scale autoregressive models?
- Basis in paper: [explicit] The paper discusses various visual tokenizer designs including VQGAN, ViT-VQGAN, Efficient-VQGAN, and RQ-VAE, highlighting challenges in codebook utilization and efficiency when scaling to larger codebooks.
- Why unresolved: While the paper reviews different tokenizer approaches and their trade-offs, it doesn't identify a definitive optimal design. The discussion shows ongoing challenges with codebook utilization rates, computational costs, and the balance between vector dimensions and codebook sizes.
- What evidence would resolve it: A comprehensive comparative study evaluating reconstruction quality (PSNR/SSIM), codebook utilization rates, and computational complexity across different tokenizer architectures at scale, with clear performance benchmarks on large-scale datasets.

### Open Question 2
- Question: Can continuous representations outperform discrete representations in autoregressive visual modeling, and if so, under what conditions?
- Basis in paper: [explicit] The paper discusses the debate between discrete and continuous representations, noting that while continuous representations simplify visual data compression, they pose challenges for autoregressive architecture design and multimodal integration with language models.
- Why unresolved: Recent works like MAR (Li et al., 2024c) have challenged the necessity of discrete representations, but the paper indicates that continuous representations haven't yet demonstrated decisive advantages. The integration challenges with language models and the lack of established loss functions for continuous settings remain open issues.
- What evidence would resolve it: Direct head-to-head comparisons of discrete versus continuous autoregressive models across multiple tasks (image generation, multimodal understanding) with standardized evaluation metrics, including analysis of scaling properties and multimodal fusion capabilities.

### Open Question 3
- Question: What architectural inductive biases, beyond standard causal attention, most effectively improve autoregressive visual modeling performance and scalability?
- Basis in paper: [explicit] The paper discusses various architectural approaches including bidirectional attention, masked image modeling strategies, and hierarchical multi-scale tokenization, questioning whether vanilla autoregressive models are truly optimal.
- Why unresolved: While recent works have explored different inductive biases, the paper suggests this remains an open question about whether incorporating visual-specific inductive biases provides advantages over standard transformer architectures, particularly regarding scalability and multimodal fusion.
- What evidence would resolve it: Systematic ablation studies comparing different architectural variants (causal vs. bidirectional attention, hierarchical vs. flat structures, masked vs. unmasked training) across multiple scales and tasks, measuring both performance gains and computational efficiency trade-offs.

## Limitations

- The survey is based on literature review rather than original experimental research, relying on aggregated observations rather than controlled experiments
- Efficiency comparisons between different autoregressive approaches are theoretical calculations that may not capture real-world implementation constraints
- Integration claims with other generative models are based on observed implementations but don't establish fundamental compatibility or optimal strategies

## Confidence

- **High confidence**: The taxonomy of autoregressive approaches (pixel-based, token-based, scale-based) is well-supported by existing literature and clearly defined
- **Medium confidence**: The efficiency analysis showing O(N⁶) vs O(N⁴/k⁴) complexity has theoretical grounding but lacks empirical validation across diverse hardware configurations
- **Medium confidence**: Integration claims with other generative models (diffusion, VAEs) are based on observed implementations but don't establish fundamental compatibility or optimal integration strategies

## Next Checks

1. **Empirical efficiency validation**: Implement representative models from each autoregressive category and measure actual runtime and memory usage across different hardware platforms to verify theoretical complexity claims

2. **Integration benchmarking**: Systematically compare autoregressive-diffusion hybrids against pure diffusion models on standardized image generation benchmarks to quantify the actual performance gains from integration

3. **Tokenizer impact study**: Conduct controlled experiments varying tokenizer design (VQGAN vs RQ-VAE vs other approaches) while holding autoregressive architecture constant to isolate the impact of tokenization on generation quality