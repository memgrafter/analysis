---
ver: rpa2
title: 'Investigating Continual Pretraining in Large Language Models: Insights and
  Implications'
arxiv_id: '2402.17400'
source_url: https://arxiv.org/abs/2402.17400
tags:
- domains
- continual
- training
- pretraining
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark for continual domain-adaptive
  pretraining of large language models (LLMs) across 159 domains from the M2D2 dataset,
  evaluating knowledge retention and transfer across diverse domains. The authors
  train GPT-2, Llama2-7B, and RoBERTa models on sequential domain corpora, measuring
  perplexity, forward/backward transfer, and forgetting.
---

# Investigating Continual Pretraining in Large Language Models: Insights and Implications

## Quick Facts
- arXiv ID: 2402.17400
- Source URL: https://arxiv.org/abs/2402.17400
- Reference count: 16
- This paper introduces a benchmark for continual domain-adaptive pretraining across 159 domains, finding that continual pretraining improves models under 1.5B parameters while larger models show better perplexity but higher resistance to forgetting.

## Executive Summary
This study presents a comprehensive benchmark for continual domain-adaptive pretraining of large language models across 159 domains from the M2D2 dataset. The authors systematically evaluate knowledge retention and transfer using GPT-2, Llama2-7B, and RoBERTa models trained on sequential domain corpora. Through extensive experimentation with different domain orderings and model scales, the paper reveals critical insights about how continual learning affects model performance, forgetting rates, and domain specialization. The findings demonstrate that while continual pretraining consistently benefits smaller models, larger models require sufficiently large domains to show improvement, and that training domain order significantly impacts knowledge transfer and specialization.

## Method Summary
The authors use the M2D2 dataset containing 159 hierarchically organized domains (236 total) from Wikipedia and Semantic Scholar. They train three model families (GPT-2 variants, Llama2-7B, RoBERTA) on sequential domain corpora using full fine-tuning with the original next-token prediction objective. Training employs Adam optimizer with batch size 16, DeepSpeed auto configuration, and dropout rate 0.2. The continual pretraining pipeline involves training on each domain for one epoch, checkpointing, and evaluating perplexity on all previous and future domains. Two domain orderings are tested: similar-order (semantically related domains) and random-order. The study measures zero-shot perplexity, domain adaptive pretraining perplexity, continual pretraining perplexity, forgetting rates, and forward/backward transfer across all model sizes.

## Key Results
- Continual pretraining consistently improves models under 1.5B parameters but degrades Llama2-7B due to its larger pretraining corpus
- Larger models achieve better perplexity but smaller models show higher learning and forgetting rates
- Randomizing training domain order improves final performance and reduces forgetting compared to semantically ordered training
- Llama2-7B benefits from continual pretraining only for domains larger than 100MB
- Continual pretraining enhances downstream task performance in GPT-2 models, with results correlating with perplexity trends

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Continual pretraining improves GPT2 models but degrades Llama2-7B due to domain size differences.
- **Mechanism**: Llama2-7B was pretrained on a massive corpus (2T tokens) while GPT2 models trained on smaller datasets. When adapted to domains much smaller than their original pretraining data, Llama2-7B experiences degradation while GPT2 models improve.
- **Core assumption**: Domain size relative to original pretraining corpus determines whether additional pretraining is beneficial.
- **Evidence anchors**:
  - [section] "We conjecture that additional domain adaptive and/or continual pretraining on a relatively small corpus causes Llama2-7B to perform worse."
  - [section] "Llama2-7B requires domains to be larger than 100 MB for improved adaptation."
  - [corpus] Weak - no direct corpus evidence linking specific token counts to performance changes.
- **Break condition**: When domain sizes approach or exceed the original pretraining corpus size, or when using architectures less sensitive to pretraining data scale.

### Mechanism 2
- **Claim**: Model scale correlates with final performance and inversely with forgetting rates.
- **Mechanism**: Larger models have more parameters and capacity to retain previously learned knowledge while adapting to new domains, resulting in better perplexity and less forgetting.
- **Core assumption**: Parameter count and model capacity directly influence knowledge retention during continual learning.
- **Evidence anchors**:
  - [section] "larger models always achieve better perplexity than smaller ones when continually pretrained on the same corpus"
  - [section] "smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both learning and forgetting"
  - [corpus] Moderate - scaling laws literature supports this relationship but specific evidence for forgetting rates is limited.
- **Break condition**: When models become so large that computational constraints prevent effective training, or when domain complexity exceeds model capacity.

### Mechanism 3
- **Claim**: Training domain order significantly impacts knowledge transfer and specialization.
- **Mechanism**: Similar-order training (semantically related domains) enhances specialization through gradual knowledge accumulation, while random-order training improves overall transfer and reduces forgetting by preventing catastrophic interference.
- **Core assumption**: Semantic similarity between consecutive training domains influences how knowledge transfers and accumulates.
- **Evidence anchors**:
  - [section] "continual pretraining enables LLMs to specialize better when the sequence of domains shows semantic similarity"
  - [section] "randomizing training domains leads to better transfer and final performance otherwise"
  - [section] "larger models seem to attain better CPT perplexity in similar order training than random order"
- **Break condition**: When domain similarity becomes too high (causing overfitting) or too low (preventing useful transfer), or when task order optimization becomes computationally prohibitive.

## Foundational Learning

- **Concept**: Catastrophic forgetting in neural networks
  - **Why needed here**: Understanding why models lose previously learned knowledge during sequential training is fundamental to interpreting continual learning results.
  - **Quick check question**: What happens to a neural network's performance on earlier tasks when it's trained on new tasks sequentially without any preservation mechanisms?

- **Concept**: Domain adaptation and transfer learning
  - **Why needed here**: The paper compares continual pretraining against domain-adaptive pretraining, requiring understanding of how models adapt to new domains and transfer knowledge.
  - **Quick check question**: How does pretraining on one domain affect performance on a different but related domain?

- **Concept**: Perplexity as an evaluation metric
  - **Why needed here**: Perplexity is the primary metric used throughout the paper to measure language model performance during continual learning.
  - **Quick check question**: What does perplexity measure in language models and why is lower perplexity better?

## Architecture Onboarding

- **Component map**: Pretrained LLMs (GPT2 variants, Llama2-7B, RoBERTa) -> M2D2 dataset with 159 domains -> Training pipeline with sequential domain processing -> Evaluation framework measuring perplexity, transfer, and forgetting

- **Critical path**: Pretrain base model -> Sequentially train on each domain for one epoch -> Checkpoint after each domain -> Evaluate on all previous and future domains -> Measure perplexity, transfer, and forgetting metrics

- **Design tradeoffs**: Full fine-tuning vs. parameter-efficient methods (chosen: full fine-tuning for comprehensive analysis), similar-order vs. random domain ordering (analyzed both), batch size selection (16 default, 64 ablation)

- **Failure signatures**: Perplexity degradation across all domains indicates catastrophic forgetting; inconsistent performance across different domain orders suggests sensitivity to training sequence; larger models showing worse performance than smaller ones would indicate scaling issues

- **First 3 experiments**:
  1. Compare zero-shot performance vs. domain-adaptive pretraining vs. continual pretraining on a small subset of domains to establish baseline improvements.
  2. Test different domain orderings (similar vs. random) on a single model size to observe order effects on transfer and forgetting.
  3. Vary batch sizes (16 vs. 64) while keeping domain order constant to assess impact on learning dynamics and final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does continual pretraining impact the emergence of compositional reasoning abilities in large language models across different domain sequences?
- Basis in paper: [explicit] The paper discusses continual pretraining across 159 domains and observes performance changes, but does not specifically investigate compositional reasoning abilities
- Why unresolved: The study focuses on perplexity metrics and downstream task performance but does not examine whether continual pretraining enables or inhibits the development of complex reasoning capabilities that require combining knowledge from multiple domains
- What evidence would resolve it: A systematic evaluation of compositional reasoning tasks (e.g., multi-step problem solving, analogical reasoning across domains) before and after continual pretraining on different domain sequences would reveal whether certain training orders promote or hinder the emergence of these capabilities

### Open Question 2
- Question: What is the relationship between the semantic similarity of training domains and the development of domain-specific expertise versus general knowledge in large language models?
- Basis in paper: [explicit] The paper investigates how similar-order training leads to better specialization while random order improves transfer, but does not quantify the trade-off between specialization and generalization
- Why unresolved: The study shows that semantically similar domains enhance specialization while random ordering improves transfer, but it does not measure the extent to which these approaches develop domain-specific expertise versus broad, transferable knowledge
- What evidence would resolve it: Controlled experiments measuring performance on both domain-specific and domain-general tasks after continual pretraining with varying degrees of semantic similarity between training domains would reveal the trade-off between specialization and generalization

### Open Question 3
- Question: How do the initial pretraining data characteristics (size, diversity, domain coverage) influence the effectiveness and limitations of continual pretraining across diverse domains?
- Basis in paper: [explicit] The paper observes that Llama2-7B (trained on 2T tokens) does not benefit from continual pretraining while GPT models (trained on smaller corpora) do, but does not systematically investigate how different initial pretraining characteristics affect continual learning
- Why unresolved: The study identifies that Llama2-7B's large initial pretraining corpus prevents further improvement, but it does not explore how other characteristics of the initial pretraining data (such as domain diversity or specific content types) influence continual learning effectiveness
- What evidence would resolve it: Comparative studies of continual pretraining effectiveness across models with systematically varied initial pretraining data characteristics (domain coverage, data diversity, corpus size) would reveal how these factors constrain or enable continual learning capabilities

## Limitations
- Results primarily based on English-language scientific and Wikipedia domains, limiting generalizability to other languages and domain types
- Focus on perplexity as primary metric doesn't directly measure downstream task performance across all model types
- Full fine-tuning approach used rather than parameter-efficient methods, limiting applicability to resource-constrained scenarios

## Confidence

- **High confidence**: Continual pretraining consistently improves models under 1.5B parameters; correlation between model scale and performance/perplexity; relationship between domain size and Llama2-7B effectiveness
- **Medium confidence**: Randomizing domain order improving transfer and reducing forgetting; larger models showing better similar-order training performance
- **Low confidence**: Specific 100MB threshold for Llama2-7B domain size effectiveness

## Next Checks
1. Evaluate continual pretraining approach on non-scientific domains (legal, financial, creative writing) to assess generalizability beyond current domain set
2. Implement LoRA or other parameter-efficient fine-tuning methods alongside full fine-tuning to test if benefits persist under computational constraints
3. Extend training beyond current domain sequence to measure whether forgetting compounds over time and test episodic memory mechanisms to mitigate degradation in larger models