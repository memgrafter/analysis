---
ver: rpa2
title: 'MSSDA: Multi-Sub-Source Adaptation for Diabetic Foot Neuropathy Recognition'
arxiv_id: '2409.14154'
source_url: https://arxiv.org/abs/2409.14154
tags:
- domain
- source
- feature
- data
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-sub-source adaptation method for diabetic
  foot neuropathy (DFN) recognition. The key contribution is creating a new dataset
  of continuous plantar pressure data for DFN recognition, containing data from 94
  DM patients with DFN and 41 DM patients without DFN.
---

# MSSDA: Multi-Sub-Source Adaptation for Diabetic Foot Neuropathy Recognition

## Quick Facts
- arXiv ID: 2409.14154
- Source URL: https://arxiv.org/abs/2409.14154
- Reference count: 24
- Achieves 88.9% accuracy on proposed DFN dataset

## Executive Summary
This paper proposes MSSDA, a multi-sub-source adaptation method for diabetic foot neuropathy (DFN) recognition using continuous plantar pressure data. The method addresses significant domain discrepancies in biomedical signal processing by creating a new dataset with 135 subjects and implementing a three-stage approach: contrastive learning for feature separation, Gaussian Mixture Model clustering to identify sub-source domains, and selective multi-source domain alignment. MSSDA achieves 88.9% accuracy on the proposed dataset, outperforming other domain adaptation methods.

## Method Summary
MSSDA implements a three-stage approach to address domain gaps in DFN recognition. Stage 1 uses contrastive learning to train a feature extractor that maximizes separation between all samples. Stage 2 applies GMM clustering on convolutional feature statistics (mean and standard deviation) to discover K sub-source domains. Stage 3 selects M sub-source domains closest to the target and performs parallel adversarial alignment between each selected source-target pair across multiple feature spaces, averaging classifier outputs for final predictions.

## Key Results
- Achieves 88.9% accuracy on the proposed DFN dataset
- Outperforms other domain adaptation methods on both DFN-DS and FRA datasets
- Demonstrates effectiveness of sub-source domain selection in avoiding negative transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning in Stage 1 separates all samples in feature space, improving GMM clustering in Stage 2.
- Mechanism: By maximizing distances between dissimilar samples and minimizing distances between similar ones, the feature extractor F0 produces more separable embeddings. This enhanced separability allows GMM to identify more distinct clusters in convolutional feature statistics.
- Core assumption: Greater inter-sample distances in feature space lead to clearer cluster boundaries in the second stage.
- Evidence anchors:
  - [abstract]: "The first stage trains a model to separate all the samples as well as possible using contrastive learning."
  - [section]: "We harness its influence within the feature space to widen the distance between all samples... thereby enhancing the clustering effect in the second stage."
  - [corpus]: No direct corpus evidence found for this specific contrastive learning clustering claim. Evidence is only from the paper text.
- Break condition: If the contrastive loss fails to create meaningful separations (e.g., due to data augmentation not preserving semantic similarity), GMM clustering will remain ineffective.

### Mechanism 2
- Claim: Splitting the dataset by convolutional feature statistics rather than by individual subjects mitigates domain gap.
- Mechanism: Individual subjects exhibit high intra-domain variance, creating large gaps when treated as separate domains. By clustering based on feature statistics (mean and std across channels), the method creates sub-source domains that better capture latent style variations within the data, allowing for more effective alignment.
- Core assumption: Convolutional feature statistics reflect underlying data "styles" that are more consistent within sub-domains than across subjects.
- Evidence anchors:
  - [abstract]: "We split the dataset based on convolutional feature statistics and select appropriate sub-source domains."
  - [section]: "We assume that the latent domains of data are reflected in their styles, specifically in the convolutional feature statistics (mean and standard deviation)."
  - [corpus]: No direct corpus evidence found supporting the claim that convolutional feature statistics better reflect latent domains than subject-based splits. Evidence is internal to the paper.
- Break condition: If feature statistics do not correlate with domain-relevant differences (e.g., if pressure patterns vary randomly within subjects), the clustering will not produce useful sub-domains.

### Mechanism 3
- Claim: Selecting only the closest sub-source domains to the target avoids negative transfer and improves alignment.
- Mechanism: By measuring distances between sub-source domain centers and target samples, and selecting only the closest M domains, the method ensures that only domains with similar feature distributions are aligned with the target. This avoids including domains that are too dissimilar, which could degrade performance through negative transfer.
- Core assumption: Domain alignment is only beneficial when the source and target distributions are sufficiently similar.
- Evidence anchors:
  - [abstract]: "We select appropriate sub-source domains to enhance efficiency and avoid negative transfer."
  - [section]: "We select M sub-source domains that are closest to the target domain... to avoid negative transfer which may be happened as the individual number of the dataset increases."
  - [corpus]: No direct corpus evidence found for this specific selection strategy claim. Evidence is from the paper text.
- Break condition: If the distance metric fails to capture true domain similarity (e.g., if PCA projection loses critical variance), relevant domains may be excluded and irrelevant ones included.

## Foundational Learning

- Concept: Domain adaptation and transfer learning
  - Why needed here: The paper addresses significant domain gaps between subjects in plantar pressure data, requiring techniques to transfer knowledge from source subjects to target subjects.
  - Quick check question: What is the difference between single-source and multi-source domain adaptation, and why is the latter more complex?

- Concept: Contrastive learning and representation learning
  - Why needed here: Stage 1 uses contrastive learning to create separable feature representations that improve clustering in Stage 2.
  - Quick check question: How does contrastive loss encourage separation between dissimilar samples and attraction between similar ones?

- Concept: Gaussian Mixture Models and clustering for domain discovery
  - Why needed here: Stage 2 uses GMM on convolutional feature statistics to discover latent sub-domains within the source data.
  - Quick check question: What role does the Bayesian Information Criterion (BIC) play in determining the optimal number of clusters?

## Architecture Onboarding

- Component map:
  F0 (feature extractor) -> GMM clustering on {µ(F0(x)), σ(F0(x))} -> K sub-source domains -> M selected domains -> For each selected domain k: Fk, Dk, Ck -> Parallel adversarial alignment -> Average classifier outputs

- Critical path:
  1. Train F0 with contrastive loss on all data
  2. Extract statistics and cluster to get K sub-domains
  3. Assign pseudo labels and select M closest domains
  4. For each selected domain, train adversarial alignment with target
  5. Average classifier outputs for final prediction

- Design tradeoffs:
  - M selection: Larger M may improve coverage but increases risk of negative transfer; smaller M is safer but may miss useful domains
  - Number of clusters K: Determined by BIC; too few clusters may not capture diversity, too many may overfit
  - Feature extractor architecture: Different for each stage; F0 is for clustering, F1...FM are for alignment

- Failure signatures:
  - Low clustering accuracy in Stage 2 (poor feature separability)
  - Inconsistent performance across different target subjects (selection strategy not robust)
  - Performance drops with higher thresholds (over-pursuit of transferability)

- First 3 experiments:
  1. Run Stage 1 alone: Train F0 with contrastive loss and evaluate clustering quality on held-out validation data
  2. Run Stage 2 alone: Fix F0, apply GMM clustering, visualize cluster assignments and BIC scores for different K values
  3. Run Stage 3 ablation: Compare performance using all sub-source domains vs. selected domains with single feature space alignment vs. multi-feature space alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MSSDA vary when using different numbers of sub-source domains (M) for alignment, and what is the optimal value of M for different datasets?
- Basis in paper: [explicit] The paper mentions that M is set to 2 for both DFN-DS and FRA datasets, but does not explore how performance changes with different values of M.
- Why unresolved: The paper does not provide an ablation study or analysis on the impact of varying M on the model's performance.
- What evidence would resolve it: Conducting experiments with different values of M (e.g., M = 1, 3, 4, 5) and comparing the resulting accuracy, precision, recall, and F1-score on both DFN-DS and FRA datasets would provide insights into the optimal number of sub-source domains for alignment.

### Open Question 2
- Question: How does the performance of MSSDA compare to other state-of-the-art domain adaptation methods on larger and more diverse datasets, particularly those with a higher number of subjects?
- Basis in paper: [inferred] The paper mentions that existing MDA methods are typically designed for datasets with fewer than 50 subjects and that MSSDA addresses the challenge of scaling to larger datasets. However, it does not provide a comprehensive comparison with other methods on larger datasets.
- Why unresolved: The paper only evaluates MSSDA on two datasets (DFN-DS and FRA) with a limited number of subjects, and does not explore its performance on larger and more diverse datasets.
- What evidence would resolve it: Conducting experiments on larger datasets with a higher number of subjects (e.g., >100) and comparing the performance of MSSDA with other state-of-the-art domain adaptation methods would provide insights into its scalability and effectiveness on more diverse data.

### Open Question 3
- Question: How does the choice of clustering method (GMM) and the evaluation criterion (BIC) impact the performance of MSSDA, and are there alternative methods that could yield better results?
- Basis in paper: [explicit] The paper mentions that GMM is used for clustering and BIC is used to determine the optimal number of clusters, but does not explore alternative clustering methods or evaluation criteria.
- Why unresolved: The paper does not provide an ablation study or comparison with other clustering methods (e.g., K-means, hierarchical clustering) or evaluation criteria (e.g., silhouette score, Calinski-Harabasz index) to assess their impact on the performance of MSSDA.
- What evidence would resolve it: Conducting experiments with different clustering methods and evaluation criteria, and comparing their impact on the performance of MSSDA (e.g., accuracy, precision, recall, F1-score) on both DFN-DS and FRA datasets would provide insights into the effectiveness of the chosen methods and potential alternatives.

## Limitations

- The proposed mechanisms lack external validation and theoretical grounding
- No ablation studies to isolate contributions of individual components
- Dataset creation process and subject demographics are not fully detailed

## Confidence

- Confidence in the key claims is Medium. The proposed methodology is well-structured and the results on the DFN dataset are promising, but the lack of ablation studies on individual components limits confidence in the specific contributions of each stage.
- Evidence for the proposed mechanisms comes exclusively from the paper itself, with no corpus support or comparison to established domain adaptation methods beyond the tested baselines.
- The lack of detailed dataset information raises questions about generalizability.

## Next Checks

1. Conduct an ablation study to isolate the contribution of each stage (contrastive learning, GMM clustering, and multi-source adaptation) to overall performance.
2. Compare the proposed feature statistics-based clustering approach with subject-based domain splitting and alternative clustering methods (e.g., K-means, spectral clustering) on the same dataset.
3. Test the method's robustness by varying the M selection threshold and analyzing the trade-off between domain coverage and negative transfer risk.