---
ver: rpa2
title: 'Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking'
arxiv_id: '2402.14811'
source_url: https://arxiv.org/abs/2402.14811
tags:
- circuit
- heads
- group
- position
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how fine-tuning affects the internal mechanisms
  of language models, focusing on entity tracking capabilities. The study finds that
  fine-tuned models use the same circuit as the base model for entity tracking, with
  performance improvements attributed to enhanced functionality of specific components
  rather than changes in the overall mechanism.
---

# Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking

## Quick Facts
- arXiv ID: 2402.14811
- Source URL: https://arxiv.org/abs/2402.14811
- Reference count: 40
- Primary result: Fine-tuned models use the same entity tracking circuit as base models, with performance improvements coming from enhanced component functionality rather than new mechanisms

## Executive Summary
This paper investigates how fine-tuning affects the internal mechanisms of language models through a detailed case study on entity tracking capabilities. The research demonstrates that fine-tuned models employ the same entity tracking circuit as their base counterparts, with performance improvements attributed to enhanced functionality of specific components rather than fundamental mechanistic changes. Using novel methods including Cross-Model Activation Patching (CMAP), the study reveals that fine-tuning augments existing mechanisms, particularly improving the model's ability to handle positional information about entities. This work provides crucial insights into how fine-tuning affects model internals and challenges the assumption that fine-tuning necessarily introduces new mechanisms.

## Method Summary
The study employs a multi-method approach to analyze entity tracking mechanisms across base and fine-tuned models. Path Patching is used to identify the entity tracking circuit in Llama-7B, followed by evaluation of this circuit in fine-tuned models using faithfulness and completeness metrics. Desiderata-based Component Masking (DCM) identifies model components responsible for specific functionalities within the circuit, while Cross-Model Activation Patching (CMAP) compares mechanisms between base and fine-tuned models. The entity tracking task involves tracking 7 boxes with single-token objects, with circuit discovery using a synthetic dataset of 300 examples and additional samples for activation patching.

## Key Results
- Fine-tuned models use the same entity tracking circuit as base models, with 72 attention heads in Llama-7B and 175 in fine-tuned versions
- The entity tracking circuit remains consistent across models, with performance improvements attributed to enhanced sub-mechanisms rather than new mechanisms
- Cross-model activation patching reveals that fine-tuned models show enhanced ability to handle positional information, particularly through Position Transmitter and Value Fetcher heads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning enhances existing mechanisms rather than creating new ones for entity tracking
- Mechanism: Fine-tuning improves the functionality of specific components within the same circuit rather than altering the overall mechanism
- Core assumption: The entity tracking circuit remains largely unchanged during fine-tuning, with performance improvements coming from enhanced sub-mechanisms
- Evidence anchors:
  - [abstract] "fine-tuning enhances, rather than fundamentally alters, the mechanistic operation of the model"
  - [section 1] "fine-tuning enhances the existing mechanism of the original model rather than causing a fundamental shift"
  - [corpus] Weak evidence - only one related paper on mechanism changes, but not specific to entity tracking enhancement
- Break condition: If fine-tuning were to fundamentally alter the entity tracking circuit or introduce entirely new mechanisms, this claim would break down

### Mechanism 2
- Claim: The same entity tracking circuit is used across all models, with fine-tuned versions performing better through improved component functionality
- Mechanism: The entity tracking circuit identified in Llama-7B is present and functional in all fine-tuned models, with performance improvements due to enhanced capabilities of specific circuit components
- Core assumption: The circuit components identified in the base model are sufficient to explain entity tracking in fine-tuned models
- Evidence anchors:
  - [section 4.3] "fine-tuned models have good faithfulness scores for the circuit identified in Llama-7B (without any additional optimization or adaptation)"
  - [section 5.3] "the functionality of the subset of heads remains the same across fine-tuned models"
  - [corpus] Moderate evidence - one related paper on mechanism preservation during fine-tuning
- Break condition: If the circuit components differ significantly between base and fine-tuned models, or if new mechanisms are introduced, this claim would break down

### Mechanism 3
- Claim: Fine-tuning enhances the ability to handle positional information in entity tracking
- Mechanism: Fine-tuned models improve their ability to encode and utilize positional information about entities, leading to better performance in tracking entities
- Core assumption: Positional information is a key factor in entity tracking performance
- Evidence anchors:
  - [section 6] "patching the output of the Position Transmitter and Value Fetcher heads from fine-tuned models to Llama-7B improves the performance of Llama-7B beyond its default performance"
  - [section 5.1] "entity tracking is performed by tracking the position of the correct entity in both the original model and its fine-tuned versions"
  - [corpus] Weak evidence - no direct mention of positional information enhancement in related papers
- Break condition: If positional information is not a critical factor in entity tracking, or if fine-tuning does not improve positional information handling, this claim would break down

## Foundational Learning

- Concept: Circuit discovery and analysis in neural networks
  - Why needed here: Understanding how entity tracking works requires identifying the relevant circuit components and their functionality
  - Quick check question: What is the primary method used to identify the entity tracking circuit in Llama-7B?
- Concept: Cross-model activation patching
  - Why needed here: To compare mechanisms between base and fine-tuned models and identify performance improvements
  - Quick check question: How does cross-model activation patching differ from standard activation patching?
- Concept: Desiderata-based Component Masking (DCM)
  - Why needed here: To automatically identify model components responsible for specific functionalities within the entity tracking circuit
  - Quick check question: What is the purpose of using desiderata in DCM?

## Architecture Onboarding

- Component map:
  - Entity tracking circuit: 72 attention heads in Llama-7B, 175 in fine-tuned models
  - Four groups of heads: Value Fetcher (A), Position Transmitter (B), Position Detector (C), Structure Reader (D)
  - Desiderata: Object, Label, and Position desiderata for functionality identification
- Critical path:
  1. Identify entity tracking circuit using path patching
  2. Evaluate circuit in fine-tuned models using faithfulness criterion
  3. Analyze circuit functionality using DCM and activation patching
  4. Compare mechanisms between base and fine-tuned models using cross-model activation patching
- Design tradeoffs:
  - Using the same circuit across models simplifies analysis but may miss fine-tuned specific components
  - Focusing on positional information may overlook other factors contributing to performance improvements
- Failure signatures:
  - Low faithfulness scores when evaluating fine-tuned models with base circuit
  - Inconsistent functionality across models for the same circuit components
  - Inability to attribute performance improvements to specific sub-mechanisms
- First 3 experiments:
  1. Implement path patching to identify the entity tracking circuit in Llama-7B
  2. Evaluate the identified circuit in Vicuna-7B, Goat-7B, and FLoat-7B using faithfulness and completeness metrics
  3. Apply DCM to identify the functionality of circuit components in Llama-7B and fine-tuned models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do additional components in fine-tuned circuits enhance entity tracking performance beyond the base model's circuit?
- Basis in paper: [inferred] The paper notes that fine-tuned models contain the same entity tracking circuit as the base model plus additional components, with performance improvements attributed to enhanced functionality rather than new mechanisms.
- Why unresolved: While the paper identifies that additional components exist and improve performance, it does not fully characterize what these components do or how they specifically enhance the circuit's functionality.
- What evidence would resolve it: Detailed analysis of the additional components' roles through ablation studies or comparative activation analysis between base and fine-tuned models could clarify their specific contributions to performance enhancement.

### Open Question 2
- Question: Is the mechanism invariance observed in entity tracking typical across other tasks when models are fine-tuned?
- Basis in paper: [inferred] The paper suggests that understanding the mechanism in fine-tuned models provides insights into base model behavior, but acknowledges this needs validation across more tasks and models.
- Why unresolved: The study focuses specifically on entity tracking as a case study, leaving open whether similar mechanism preservation occurs in other capabilities or if entity tracking is unique in this regard.
- What evidence would resolve it: Applying the same mechanistic analysis approach to multiple tasks (e.g., arithmetic, code generation, instruction following) across different base models would reveal whether mechanism invariance is a general phenomenon or task-specific.

### Open Question 3
- Question: What specific aspects of positional information are encoded and utilized during entity tracking?
- Basis in paper: [explicit] The paper notes that models primarily track entity positions rather than values or labels, and includes additional experiments exploring different types of positional information.
- Why unresolved: While the paper establishes that positional information is crucial, it cannot fully characterize whether models track absolute positions, relative positions, or semantic associations between entities and their containers.
- What evidence would resolve it: Controlled experiments systematically varying context structure, entity order, and semantic relationships could isolate which aspects of positional information are critical for the tracking mechanism.

## Limitations

- Model Scope Uncertainty: The study focuses exclusively on Llama-7B and three specific fine-tuned variants, leaving generalizability to other base models or fine-tuning objectives unknown.
- Circuit Completeness Question: The relationship between head counts and circuit completeness is not rigorously established, potentially missing fine-tuned-specific components.
- Positional Information Focus: The emphasis on positional information may oversimplify the complex interactions within the entity tracking circuit, overlooking other potential contributing factors.

## Confidence

**High Confidence**: The claim that fine-tuning enhances rather than fundamentally alters existing mechanisms is well-supported by empirical evidence showing consistent circuit identification across models.

**Medium Confidence**: The specific attribution of performance improvements to enhanced positional information handling is supported but not definitively proven, as alternative explanations cannot be fully ruled out.

**Medium Confidence**: The claim that the same entity tracking circuit operates across all models is empirically supported but relies on the completeness assumption, which could be violated if fine-tuned-specific components exist.

## Next Checks

**Cross-Architecture Validation**: Apply the entity tracking circuit discovery methodology to a different base model architecture (e.g., OPT or Mistral) and its fine-tuned variants to test the generalizability of the enhancement hypothesis across architectures.

**Component Ablation Study**: Systematically ablate individual heads within the entity tracking circuit in both base and fine-tuned models to quantify their individual contributions to performance and reveal fine-tuned-specific critical components.

**Alternative Mechanism Testing**: Design synthetic entity tracking tasks that specifically isolate positional information from other tracking-relevant features to empirically distinguish the relative importance of positional enhancement versus other potential mechanisms.