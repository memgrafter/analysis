---
ver: rpa2
title: 'Ancient but Digitized: Developing Handwritten Optical Character Recognition
  for East Syriac Script Through Creating KHAMIS Dataset'
arxiv_id: '2408.13631'
source_url: https://arxiv.org/abs/2408.13631
tags:
- syriac
- handwritten
- will
- university
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the KHAMIS dataset, a novel resource of 624
  handwritten Syriac sentences in the East Syriac script, collected from 31 university
  students and one professor. The dataset is used to fine-tune the Tesseract-OCR engine's
  pretrained Syriac model for handwritten recognition.
---

# Ancient but Digitized: Developing Handwritten Optical Character Recognition for East Syriac Script Through Creating KHAMIS Dataset

## Quick Facts
- arXiv ID: 2408.13631
- Source URL: https://arxiv.org/abs/2408.13631
- Authors: Ameer Majeed; Hossein Hassani
- Reference count: 4
- Primary result: Handwritten Syriac OCR model achieves CER of 1.097-1.610% on training set and 8.963-10.490% on evaluation set, with 18.89-19.71% CER on unseen test data

## Executive Summary
This paper introduces KHAMIS, a novel dataset of 624 handwritten Syriac sentences in the East Syriac script, collected from 31 university students and one professor. The dataset is used to fine-tune the Tesseract-OCR engine's pretrained Syriac model for handwritten recognition, achieving significant improvements over the default model. The work addresses the critical need for digitizing handwritten Syriac texts for cultural preservation and research, providing a foundation for future work in Syriac language processing.

## Method Summary
The researchers collected handwritten Syriac sentences from 32 writers, created the KHAMIS dataset, and used it to fine-tune Tesseract's pretrained Syriac model through transfer learning. The dataset underwent preprocessing including binarization and blurring before being split into 90% training and 10% evaluation sets. The model was then tested on unseen data to assess generalization performance. The fine-tuning process leveraged Tesseract's existing capabilities for printed Syriac text, adapting them to handwritten variations.

## Key Results
- Character Error Rate (CER) on training set: 1.097-1.610%
- Character Error Rate (CER) on evaluation set: 8.963-10.490%
- Character Error Rate (CER) on unseen test data: 18.89-19.71%
- Word Error Rate (WER) on unseen test data: 62.83-65.42%

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a pretrained Syriac OCR model with a custom handwritten dataset significantly improves recognition performance over the default model. Transfer learning allows the model to leverage learned visual patterns from printed Syriac text and adapt them to handwritten variations, reducing the need for massive amounts of handwritten training data.

### Mechanism 2
Using a 90/10 train/eval split with a relatively small dataset still allows for meaningful performance evaluation. Stratified sampling ensures that the evaluation set is representative of the handwriting variability in the training set, allowing for reliable error estimation.

### Mechanism 3
Preprocessing (binarization, blurring) improves OCR accuracy by reducing noise and standardizing image quality. Noise reduction and contrast enhancement make character shapes clearer for the model, improving feature extraction and recognition.

## Foundational Learning

- **Transfer learning and fine-tuning**: Why needed here: The Syriac handwriting dataset is small; transfer learning allows leveraging a pretrained model to avoid starting from scratch. Quick check question: What is the difference between fine-tuning and training from scratch in the context of OCR?

- **Error metrics (CER and WER)**: Why needed here: CER and WER quantify recognition accuracy; CER is more granular while WER penalizes whole-word mistakes. Quick check question: Why is CER usually lower than WER in OCR evaluation?

- **Data preprocessing (binarization, noise removal)**: Why needed here: Handwritten images often contain noise and variable contrast; preprocessing standardizes input for the model. Quick check question: What could go wrong if you overapply blurring during preprocessing?

## Architecture Onboarding

- **Component map**: Data collection -> Preprocessing (OpenCV) -> Fine-tuning Tesseract LSTM -> Evaluation (CER/WER)

- **Critical path**: 
  1. Collect handwritten sentences and save as PNG + ground truth text
  2. Preprocess images (binarize, blur, extract from A4 scans)
  3. Fine-tune Tesseract using tesstrain with syr pretrained model
  4. Evaluate on held-out test set

- **Design tradeoffs**: Small dataset → high variance in results; larger dataset → better generalization; Aggressive preprocessing → cleaner images but possible loss of detail; Fine-tuning vs. training from scratch → faster but less flexible

- **Failure signatures**: CER stays high on training set → model not learning or data too noisy; CER low on training but high on test → overfitting; Segmentation errors dominate → need better preprocessing or post-processing

- **First 3 experiments**: 
  1. Run inference on test set with default Syriac model to establish baseline CER/WER
  2. Fine-tune on 90/10 split, evaluate on eval set; check for overfitting
  3. Try 80/20 split and compare training stability and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the model performance compare when using data augmentation techniques on the KHAMIS dataset? The paper mentions that data augmentation could be leveraged to extend the dataset and create variations, but does not report on its actual impact on model performance.

### Open Question 2
What is the impact of including diacritics in the training data on the model's recognition accuracy? The paper notes that most contemporary documents and historical manuscripts include diacritics and that their inclusion is of extreme importance for future research.

### Open Question 3
How would the model's performance change if trained on a more diverse set of scripts (Estrangela and West Syriac) in addition to East Syriac? The paper mentions that the current model will experience a bottleneck during inference of different scripts and recommends a more universal model that can differentiate between various scripts.

## Limitations
- Dataset size is relatively small (624 sentences from 32 writers) for deep learning applications, which may limit generalization to other handwriting styles
- The performance gap between training/evaluation sets and unseen test data indicates potential overfitting or distribution shift
- Preprocessing steps are described but not fully specified, potentially affecting reproducibility

## Confidence
- **High confidence**: The core methodology of fine-tuning Tesseract with handwritten data is technically sound and well-executed
- **Medium confidence**: The quantitative results are valid for the specific dataset used, but generalization to other East Syriac handwriting remains uncertain
- **Low confidence**: The claim that this is "the first handwritten Syriac dataset" cannot be independently verified without broader literature review

## Next Checks
1. **Dataset expansion validation**: Test model performance as the dataset grows from 100 to 1000 samples to quantify the relationship between training data volume and recognition accuracy

2. **Cross-writer generalization test**: Evaluate the fine-tuned model on handwriting samples from writers not included in the training data to assess true generalization capability

3. **Comparison with alternative approaches**: Benchmark against a model trained from scratch and other OCR architectures (e.g., CRNN, Transformer-based) on the same dataset to establish the relative advantage of transfer learning for this script