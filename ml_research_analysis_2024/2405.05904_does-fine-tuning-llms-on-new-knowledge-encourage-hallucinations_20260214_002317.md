---
ver: rpa2
title: Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?
arxiv_id: '2405.05904'
source_url: https://arxiv.org/abs/2405.05904
tags:
- knowledge
- examples
- unknown
- fine-tuning
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies whether fine-tuning LLMs on new knowledge increases
  hallucination. The authors design a controlled closed-book QA setup using SliCK,
  a method to categorize examples as Known or Unknown based on how well a model generates
  correct answers.
---

# Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?

## Quick Facts
- **arXiv ID:** 2405.05904
- **Source URL:** https://arxiv.org/abs/2405.05904
- **Reference count:** 31
- **Primary result:** Fine-tuning LLMs on new knowledge increases hallucination rates, with unknown examples learned much slower than known examples during training.

## Executive Summary
This paper investigates whether fine-tuning large language models (LLMs) on new knowledge increases hallucination rates. The authors introduce SliCK, a method to categorize examples as Known or Unknown based on how well a model generates correct answers. Using a controlled closed-book QA setup, they find that Unknown examples are learned significantly slower than Known examples during fine-tuning. As Unknown examples are eventually learned, they linearly increase the model's tendency to hallucinate relative to pre-existing knowledge. The results suggest that fine-tuning primarily teaches models to use existing knowledge more efficiently rather than acquiring new facts, highlighting the risk of introducing new knowledge during fine-tuning.

## Method Summary
The study uses a closed-book question answering setup with the ENTITY QUESTIONS dataset, which contains (subject, relation, object) triplets. The authors implement SliCK to categorize examples into four knowledge categories based on the model's ability to generate correct answers. They create fine-tuning variants with varying proportions of Unknown examples (0%, 25%, 50%, 75%, 100%) and train PaLM 2-S models using early stopping based on development set performance. The primary metric is exact match accuracy on test sets, with additional analysis of training dynamics to compare learning rates between Known and Unknown examples.

## Key Results
- Unknown examples are learned substantially slower than Known examples during fine-tuning
- Higher proportions of Unknown examples lead to performance degradation on Known facts
- Fine-tuning on MaybeKnown examples is essential for handling uncertain knowledge during inference
- The relationship between Unknown examples and hallucination is linear across different fine-tuning durations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning on unknown examples increases hallucination relative to pre-existing knowledge.
- **Mechanism:** When models encounter new knowledge during fine-tuning, they gradually adapt to generate responses not grounded in their pre-existing knowledge, leading to hallucinations when tested on known facts.
- **Core assumption:** The model's pre-existing knowledge is relatively stable during fine-tuning, and performance degradation reflects increased hallucination rather than forgetting.
- **Evidence anchors:**
  - [abstract] "as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate."
  - [section 4.1] "Higher %Unknown leads to performance degradation, regardless of the fine-tuning duration, which indicates that Unknown examples are less useful than Known."
  - [corpus] FMR neighbor evidence supports hallucination concerns but lacks direct empirical grounding.
- **Break condition:** If unknown examples are presented with strong context or grounding signals, the hallucination effect may be mitigated.

### Mechanism 2
- **Claim:** LLMs struggle to integrate new factual knowledge through fine-tuning.
- **Mechanism:** Unknown examples are learned much slower than known examples during fine-tuning, indicating that the model primarily learns to use its pre-existing knowledge more efficiently rather than acquiring new facts.
- **Core assumption:** The rate of learning new knowledge can be measured by comparing the fitting speed of known versus unknown examples during fine-tuning.
- **Evidence anchors:**
  - [abstract] "fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge."
  - [section 4.3] "M fits Unknown fine-tuning examples substantially slower than Known. In EARLY_STOP (vertical dotted line), M reaches peak performance on the development set, while fitting the majority of the Known examples but only a small fraction of the Unknown."
  - [corpus] FMR neighbor "Large Language Models Do NOT Really Know What They Don't Know" supports general knowledge boundary uncertainty.
- **Break condition:** If the fine-tuning task involves strong associative cues or retrieval mechanisms, new knowledge integration may occur more readily.

### Mechanism 3
- **Claim:** MaybeKnown examples are essential for proper handling of uncertain knowledge during inference.
- **Mechanism:** Fine-tuning on MaybeKnown examples helps the model develop appropriate response strategies for facts with intermediate certainty levels, improving overall performance on such examples during testing.
- **Core assumption:** The MaybeKnown category captures a distinct knowledge state that requires specific fine-tuning to handle correctly.
- **Evidence anchors:**
  - [section 5] "DMaybeKnown yields the best overall performance. Compared to DHighlyKnown, DMaybeKnown enhances MD's performance on MaybeKnown (60.1 → 69.9), without compromising performance on HighlyKnown (98.7 → 98.4)."
  - [section 5] "This suggests that MaybeKnown fine-tuning examples are essential for MD to correctly handle such examples during inference."
  - [corpus] FMR neighbor "Knowledge-Aware Self-Correction in Language Models via Structured Memory Graphs" suggests structured approaches to uncertain knowledge.
- **Break condition:** If the MaybeKnown category is too broad or noisy, its effectiveness may diminish.

## Foundational Learning

- **Concept:** Closed-book question answering setup
  - Why needed here: The study focuses on how models use pre-existing knowledge when answering questions without external resources, which is crucial for measuring hallucinations relative to known facts.
  - Quick check question: What distinguishes closed-book QA from open-book QA in terms of model behavior and evaluation?

- **Concept:** Knowledge categorization (SliCK)
  - Why needed here: The study requires a method to determine whether specific examples are known or unknown to the model, which is fundamental to isolating the effect of new knowledge.
  - Quick check question: How does the SliCK categorization differentiate between HighlyKnown, MaybeKnown, WeaklyKnown, and Unknown examples?

- **Concept:** Training dynamics analysis
  - Why needed here: Understanding how different types of examples are learned during fine-tuning is essential for explaining the relationship between new knowledge and hallucination.
  - Quick check question: What does it mean when unknown examples are "fitted slower" than known examples during training?

## Architecture Onboarding

- **Component map:** PaLM 2-S base model -> ENTITY QUESTIONS dataset (preprocessed) -> SliCK categorization -> fine-tuning variants (varying Unknown proportions) -> early stopping on dev set -> evaluation on test sets
- **Critical path:** Categorize examples → create fine-tuning variants → fine-tune models → measure performance degradation → analyze training dynamics → draw conclusions about knowledge integration
- **Design tradeoffs:** The study uses exact match as the primary metric for simplicity, though this may miss paraphrases; it uses a single model type to control variables, limiting generalizability; it focuses on closed-book QA, which may not capture all hallucination scenarios.
- **Failure signatures:** If unknown examples don't show slower learning rates, or if performance degradation isn't linear with unknown example proportion, the core hypothesis may be challenged.
- **First 3 experiments:**
  1. Run the basic experiment with 50% known/50% unknown examples to observe the linear relationship between unknown examples and hallucination.
  2. Test the filtering approach by removing unknown examples and comparing performance to the full dataset.
  3. Create single-category fine-tuning variants (HighlyKnown, MaybeKnown, WeaklyKnown, Unknown) to understand the contribution of each knowledge type.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the relationship between fine-tuning duration and hallucination risk vary across different model architectures (e.g., decoder-only, encoder-decoder, or hybrid models)?
- **Basis in paper:** [inferred] The paper focuses on a single decoder-only LLM (PaLM 2-S) and suggests future exploration of other architectures.
- **Why unresolved:** The study's controlled setup and computational constraints limited testing to one model, leaving open whether the observed dynamics generalize to other architectures.
- **What evidence would resolve it:** Comparative fine-tuning experiments on multiple architectures with varying Unknown proportions, measuring both convergence speed and hallucination rates.

### Open Question 2
- **Question:** Can dynamic fine-tuning strategies (e.g., adaptive learning rates or curriculum learning) mitigate the slow integration of Unknown examples without increasing hallucination risk?
- **Basis in paper:** [inferred] The paper notes that Unknown examples are fitted slower and suggests future research into parameter-efficient fine-tuning and continual pre-training.
- **Why unresolved:** The study uses fixed learning rates and full fine-tuning, leaving open whether adaptive methods could accelerate Unknown learning while preserving factual consistency.
- **What evidence would resolve it:** Controlled experiments comparing fixed vs. adaptive fine-tuning on Unknown-heavy datasets, measuring both convergence speed and test accuracy.

### Open Question 3
- **Question:** How do long-form generation tasks affect the relationship between Unknown fine-tuning examples and hallucination rates compared to closed-book QA?
- **Basis in paper:** [explicit] The authors acknowledge that their findings are limited to closed-book QA and suggest validation in long-form generation settings.
- **Why unresolved:** The study's closed-book QA setup allows precise evaluation, but long-form tasks introduce additional complexity in measuring factual consistency.
- **What evidence would resolve it:** Comparative fine-tuning experiments on both closed-book QA and long-form generation tasks, using the same Unknown proportions and evaluating factual consistency with task-appropriate metrics.

## Limitations

- The controlled closed-book QA setup may not fully capture the complexity of real-world knowledge integration scenarios
- The study uses a single model type (PaLM 2-S), limiting generalizability across different architectures and scales
- The knowledge categorization method (SliCK) relies on 4-shot exemplars and temperature sampling, which may not perfectly capture the model's true knowledge boundaries

## Confidence

- **High confidence**: The core finding that unknown examples are learned significantly slower than known examples during fine-tuning is well-supported by training dynamics analysis and consistent across multiple experimental conditions.
- **Medium confidence**: The linear relationship between unknown example proportion and hallucination rates is observed in the controlled setup but may vary in more complex real-world scenarios.
- **Medium confidence**: The claim that fine-tuning primarily teaches models to use existing knowledge more efficiently rather than acquiring new facts is supported by the evidence but could be further validated with additional model types and tasks.

## Next Checks

1. **Cross-architecture validation**: Test the same fine-tuning protocol with different model families (e.g., Llama, GPT, Claude) to assess whether the knowledge integration patterns generalize beyond PaLM 2-S.

2. **Alternative knowledge categorization**: Implement a second, independent method for categorizing examples as known or unknown (e.g., using multiple annotators or different prompting strategies) to verify that the SliCK categorization doesn't introduce systematic biases.

3. **Real-world knowledge integration test**: Design a fine-tuning experiment where models are trained on a small set of truly new facts (verified through external sources) rather than relying on the SliCK categorization, to directly measure hallucination rates when introducing verified new knowledge.