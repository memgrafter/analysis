---
ver: rpa2
title: 'Evaluating Language Model Context Windows: A "Working Memory" Test and Inference-time
  Correction'
arxiv_id: '2407.03651'
source_url: https://arxiv.org/abs/2407.03651
tags:
- context
- documents
- long
- document
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SWiM is a benchmark framework for evaluating long-context language
  models on real-world use cases, addressing the limitations of existing tests like
  needle-in-a-haystack. It measures position effects and context size impact on retrieval
  accuracy using custom document and task pairs.
---

# Evaluating Language Model Context Windows: A "Working Memory" Test and Inference-time Correction

## Quick Facts
- **arXiv ID:** 2407.03651
- **Source URL:** https://arxiv.org/abs/2407.03651
- **Reference count:** 13
- **Primary result:** Medoid voting improves long-context model accuracy by up to 24% on single-document QA tasks

## Executive Summary
This paper introduces SWiM, a benchmark framework for evaluating long-context language models on real-world use cases. The framework addresses limitations of existing tests like needle-in-a-haystack by measuring position effects and context size impact on retrieval accuracy using custom document and task pairs. Experiments on eight models reveal a "lost-in-the-middle" effect where performance degrades significantly when information is in the middle of the context window. To mitigate this, the paper introduces medoid voting, a training-free inference-time method that randomly permutes documents, generates multiple responses, and selects the medoid answer by embedding similarity. Medoid voting achieves up to 24% accuracy lift on single-document QA tasks, even with as few as 3 runs.

## Method Summary
The paper presents SWiM (Scalable Working Memory), a framework that evaluates long-context models through four steps: task generation using LLMs to create QA pairs from documents, task validation with human-in-the-loop for quality, task completion where models generate responses at varying document positions and context sizes, and response evaluation using LLM-as-a-judge. The framework specifically tests for the "lost-in-the-middle" effect by positioning answer documents at different locations within the context window (0%, 25%, 50%, 75%, 100%). The medoid voting method addresses this issue by running tasks multiple times with randomly permuted document order, then selecting the response with minimal average embedding distance to all others as the final answer.

## Key Results
- The "lost-in-the-middle" effect is confirmed across eight models including GPT-4 and Claude 3 Opus, with significant performance degradation when answer documents are positioned in the middle of the context window
- Medoid voting achieves up to 24% accuracy improvement on single-document QA tasks without requiring model fine-tuning
- As few as 3 runs of medoid voting are sufficient to see substantial performance gains
- The approach is effective across different model architectures and context sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Medoid voting improves accuracy by mitigating position-dependent performance variance in long-context LLMs
- Mechanism: Randomly permute document order, generate multiple responses, select the response with minimal average embedding distance to all others (the medoid)
- Core assumption: LLM responses to the same query with permuted context will cluster around the correct answer in embedding space, even if individual runs vary
- Break condition: If the model's stochasticity does not correlate with position effects, or if the medoid does not correspond to the correct answer, the method fails

### Mechanism 2
- Claim: The "lost-in-the-middle" effect causes significant performance degradation when answer documents are positioned in the middle of the context window
- Mechanism: LLMs have positional bias in attention mechanisms, leading to better retrieval at beginning and end positions than middle positions
- Core assumption: Attention scores dilute across tokens, making middle tokens less accessible than edge tokens
- Break condition: If the model uses attention mechanisms that are position-invariant or if the model's architecture explicitly mitigates positional bias

### Mechanism 3
- Claim: SWiM framework provides more realistic evaluation of long-context capabilities than NIAH test by using domain-specific documents and tasks
- Mechanism: Custom task generation and validation with human-in-the-loop ensures relevance and accuracy of evaluation data
- Core assumption: Real-world documents and tasks are more complex and relevant than synthetic needles in unrelated haystacks
- Break condition: If the custom tasks do not accurately represent real-world use cases or if the human validation step is insufficient

## Foundational Learning

- **Attention mechanisms in transformers**: Understanding how attention scores dilute across tokens explains the "lost-in-the-middle" effect. Quick check: How does self-attention in transformers handle long sequences, and what are the computational limitations?

- **Embedding spaces and similarity measures**: Medoid voting relies on computing embedding distances to select the best response. Quick check: How do embedding models convert text to vectors, and what similarity measures are commonly used?

- **Few-shot learning and in-context learning**: SWiM uses LLM-as-a-judge for evaluation, which relies on few-shot prompting capabilities. Quick check: How do LLMs perform few-shot learning, and what are the limitations of this approach?

## Architecture Onboarding

- **Component map**: Document → Task Generation → Task Validation → Task Completion → Task Evaluation → Results
- **Critical path**: Document → Task Generation → Task Validation → Task Completion → Task Evaluation → Results
- **Design tradeoffs**: Custom vs. standard benchmarks (SWiM provides more realistic evaluation but requires more setup), Human-in-the-loop vs. fully automated (validation improves data quality but adds cost and time), Medoid voting vs. single run (improves accuracy but increases computation and latency)
- **Failure signatures**: High variance in LLM-as-a-judge evaluations indicates inconsistent or ambiguous tasks, No improvement with medoid voting suggests position effects are not the primary issue, Poor performance across all models suggests tasks are too difficult or domain-specific
- **First 3 experiments**: 1) Run SWiM on a simple dataset (e.g., Huggingface Cosmopedia) to verify the framework works end-to-end, 2) Test a single model on single document QA with varying context sizes to observe the effect of distractors, 3) Implement medoid voting on a model that shows lost-in-the-middle effect and measure accuracy improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do long-context language models perform on complex reasoning tasks that require synthesizing information across multiple documents, as opposed to the single-document QA tasks evaluated in this study?
- Basis in paper: The paper mentions that future development of the SWiM framework should address more complex scenarios including tasks requiring reasoning over multiple documents
- Why unresolved: The current SWiM framework and experimental results focus primarily on single-document QA tasks. The paper acknowledges that models may perform differently on more complex scenarios but doesn't provide data on multi-document reasoning tasks
- What evidence would resolve it: Experimental results comparing model performance on single vs. multi-document reasoning tasks using the SWiM framework, with metrics on accuracy, hallucination rates, and citation accuracy

### Open Question 2
- Question: How does the effectiveness of medoid voting vary with different embedding models, and is there an optimal embedding model that maximizes the performance lift?
- Basis in paper: The paper uses an embedding model in the medoid voting algorithm but doesn't explore the impact of different embedding models on the effectiveness of the method
- Why unresolved: The paper uses a single embedding model for medoid voting without comparing it to alternatives or optimizing the choice of embedding model
- What evidence would resolve it: Comparative analysis of medoid voting performance using different embedding models (e.g., sentence transformers, LLM-based embeddings) and identification of the optimal embedding model for this task

### Open Question 3
- Question: What is the relationship between the position of relevant information in the context window and the likelihood of hallucination or incorrect information generation by long-context models?
- Basis in paper: The paper discusses the "lost-in-the-middle" effect where models perform poorly when relevant information is in the middle of the context window, but doesn't explore the relationship with hallucination
- Why unresolved: The paper focuses on retrieval accuracy but doesn't analyze whether models are more likely to hallucinate when information is in unfavorable positions within the context window
- What evidence would resolve it: Analysis correlating the position of relevant information with rates of hallucination, including examples of hallucinated responses and their relationship to document position in the context

## Limitations
- Limited model diversity: Only eight models were tested, potentially missing important variations in architecture that could affect the "lost-in-the-middle" phenomenon
- Synthetic data constraints: The use of synthetically generated QA pairs, even with human validation, may not fully capture the complexity and nuance of real-world document collections and queries
- Evaluation methodology concerns: The reliance on LLM-as-a-judge for response evaluation introduces potential biases and inconsistencies

## Confidence
- **High Confidence**: The existence of the "lost-in-the-middle" effect in long-context language models is well-supported by the experimental results across multiple models and document positions
- **Medium Confidence**: The effectiveness of medoid voting in mitigating the "lost-in-the-middle" effect is demonstrated, but the magnitude of improvement may vary with different datasets and query types
- **Medium Confidence**: The SWiM framework provides a more realistic evaluation of long-context capabilities compared to NIAH tests, though the extent of this improvement depends on the quality of the custom task generation and validation process

## Next Checks
1. **Cross-dataset validation**: Test medoid voting and the "lost-in-the-middle" effect using diverse real-world document collections (e.g., legal documents, scientific papers, news articles) to assess generalizability beyond synthetic data
2. **Architecture-specific analysis**: Conduct experiments focusing on models with different attention mechanisms (e.g., sparse attention, linear attention) to determine if the "lost-in-the-middle" effect and medoid voting effectiveness are architecture-dependent
3. **Extended evaluation metrics**: Implement additional evaluation metrics beyond retrieval accuracy, such as response coherence, factual consistency, and hallucination detection, to comprehensively assess the impact of medoid voting on overall response quality