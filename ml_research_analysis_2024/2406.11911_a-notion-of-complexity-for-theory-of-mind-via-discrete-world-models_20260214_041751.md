---
ver: rpa2
title: A Notion of Complexity for Theory of Mind via Discrete World Models
arxiv_id: '2406.11911'
source_url: https://arxiv.org/abs/2406.11911
tags:
- workshop
- complexity
- answer
- isabella
- theory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework to measure the complexity of Theory
  of Mind (ToM) tasks by quantifying the number of states needed to correctly solve
  a problem. Inspired by cognitive load theory, the authors define stateful and stateless
  complexity to characterize the inherent difficulty of tracking agents' beliefs and
  environmental changes.
---

# A Notion of Complexity for Theory of Mind via Discrete World Models

## Quick Facts
- arXiv ID: 2406.11911
- Source URL: https://arxiv.org/abs/2406.11911
- Reference count: 40
- Primary result: DWM outperforms standard prompting methods on ToM benchmarks by explicitly tracking environmental states

## Executive Summary
This paper introduces a complexity framework for Theory of Mind (ToM) tasks based on the number of states needed to solve problems, inspired by cognitive load theory. The authors propose Discrete World Models (DWM), a prompting technique that splits inputs into sequential parts and asks LLMs to describe each state event. Experiments on five ToM benchmarks demonstrate that DWM outperforms standard methods like Chain of Thought and Tree of Thoughts, particularly on tasks requiring explicit state tracking. The results show a strong correlation between task complexity and model performance, validating the framework as a measure of ToM task difficulty.

## Method Summary
The method involves a complexity framework that quantifies task difficulty through stateful and stateless complexity measures, then applies DWM prompting to improve LLM performance on ToM tasks. DWM splits prompts into sequential chunks, queries the LLM for state descriptions after each split, and uses these descriptions to inform the final answer. The framework is evaluated against standard prompting methods across five ToM benchmarks using multiple LLM models, measuring both accuracy and the correlation between calculated complexity and actual performance.

## Key Results
- DWM outperforms Chain of Thought and Tree of Thoughts on tasks requiring explicit state tracking
- Strong correlation exists between task complexity and model performance across all tested models
- Stateful complexity is more predictive of performance than stateless complexity
- DWM shows particular effectiveness on tasks where belief tracking is critical to correct answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting prompts into sequential state events forces explicit tracking of relevant environmental changes
- Mechanism: DWM technique divides input into chunks and prompts LLM to describe current state and agent beliefs after each chunk
- Core assumption: LLMs can generate accurate state descriptions when explicitly asked to track environmental changes
- Evidence anchors:
  - [abstract] "DWM outperforms standard prompting methods like Chain of Thought and Tree of Thoughts, particularly on tasks requiring explicit state tracking"
  - [section 3.2.1] "DWM inserts, after each 'split' pt, an additional prompt w like 'Now, provide a succinct description of the state of the environment and each agent's belief.'"
  - [corpus] Weak evidence - no direct corpus support for state tracking mechanism

### Mechanism 2
- Claim: Stateful complexity (tracking necessary states) is more critical than stateless complexity for task performance
- Mechanism: Complexity framework weights stateful events more heavily than stateless events in difficulty calculation
- Core assumption: Correct answers require accurate tracking of specific state changes rather than general environmental awareness
- Evidence anchors:
  - [section 3.1.2] "Our framework naturally extends to multiple objects by considering their union" and "We finally introduce the notion of partition function"
  - [section 3.1.4] "Our framework, summarized in figure 2, has two main parts: stateful and stateless complexity"
  - [corpus] Weak evidence - no direct corpus support for complexity weighting

### Mechanism 3
- Claim: Strong correlation exists between task complexity and model performance
- Mechanism: Higher complexity tasks require more computational steps, revealing model limitations
- Core assumption: Model performance degrades predictably as task complexity increases
- Evidence anchors:
  - [abstract] "The results demonstrate a strong correlation between task complexity and model performance"
  - [section 4.2] "We observe a strong correlation between the error-rate and the complexity of a task"
  - [corpus] Moderate evidence - corpus shows related work on ToM benchmarks but limited complexity-performance correlation studies

## Foundational Learning

- Concept: Theory of Mind (ToM) in artificial intelligence
  - Why needed here: The entire framework and experiments are based on measuring and improving ToM capabilities
  - Quick check question: What is the primary difference between first-order and higher-order beliefs in ToM tasks?

- Concept: Cognitive load theory and its application to computational tasks
  - Why needed here: The complexity framework draws inspiration from cognitive load theory's intrinsic, extraneous, and germane loads
  - Quick check question: How does the framework's "stateful complexity" relate to cognitive load theory's "intrinsic load"?

- Concept: Prompt engineering techniques (Chain of Thought, Tree of Thoughts)
  - Why needed here: DWM is compared against these standard prompting methods
  - Quick check question: What is the key difference between how CoT and DWM handle input prompts?

## Architecture Onboarding

- Component map:
  - Input: ToM problem prompt with multiple agents and environmental changes
  - Processing: DWM splits prompt into sequential chunks, queries LLM for state descriptions
  - Output: Final answer with LLM-generated state tracking information
  - Complexity calculator: Measures stateful and stateless complexity of tasks
  - Evaluation: Compares DWM performance against baseline methods

- Critical path:
  1. Split input prompt into state-changing events
  2. Query LLM for state description after each split
  3. Concatenate state descriptions with original prompt
  4. Query LLM for final answer
  5. Measure accuracy and complexity correlation

- Design tradeoffs:
  - More splits → better state tracking but higher computational cost
  - Simpler state descriptions → faster processing but potentially less accurate tracking
  - Complexity discounting factor τ → balances stateful/stateless importance but requires tuning

- Failure signatures:
  - Performance doesn't improve with additional splits (state tracking limit reached)
  - Complexity scores don't correlate with error rates (framework not capturing relevant difficulty factors)
  - State descriptions become repetitive or inaccurate (LLM struggling with tracking)

- First 3 experiments:
  1. Run DWM with varying split counts on a simple ToM task to find optimal split number
  2. Compare state descriptions generated by DWM vs. CoT on identical tasks to measure tracking differences
  3. Test complexity framework on tasks with known difficulty levels to validate correlation predictions

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does the DWM technique scale to higher-order belief tracking tasks with more than 5-6 states?
- Basis in paper: [inferred] The authors note that their theoretical framework extends to tasks with much higher complexity, but they haven't tested this in settings with higher state complexity.
- Why unresolved: The paper only evaluates on tasks requiring 1-5 states to be correctly answered. There's no empirical evidence for how DWM performs on more complex tasks involving kth-order belief tracking.
- What evidence would resolve it: Testing DWM on benchmarks specifically designed for higher-order belief tracking (e.g., tasks requiring 6+ states to solve correctly) and comparing performance to other prompting techniques.

Open Question 2
- Question: Can the optimal task splits for DWM be automatically determined rather than manually defined?
- Basis in paper: [explicit] The authors mention that it's not straightforward to automatically find the correct task splits that correctly describe the state, and they attempted a simple approach where the model splits every sentence.
- Why unresolved: Current DWM requires manual definition of state splits, which limits its scalability and practical application. The simple approach of letting the model split every sentence resulted in noisier and less accurate descriptions.
- What evidence would resolve it: Developing and evaluating an automated method for determining optimal splits that produces cleaner state descriptions and maintains or improves upon manual split performance.

Open Question 3
- Question: How does memorization of ToM benchmarks affect the validity of performance comparisons across different prompting techniques?
- Basis in paper: [explicit] The authors discovered that ToMi and FANToM benchmarks were heavily memorized by GPT-3.5-Instruct, with entire portions retrievable word-for-word, and note that memorization doesn't clearly correlate with performance drops in DWM.
- Why unresolved: While the paper quantifies memorization rates and notes that performance differences persist despite memorization, it doesn't fully address how this affects the validity of ToM benchmark evaluations or what proportion of "success" is due to memorization versus genuine reasoning.
- What evidence would resolve it: A systematic study comparing performance on memorized vs. novel prompts across multiple prompting techniques and model families to quantify the impact of memorization on benchmark validity.

## Limitations

- Manual labeling requirement for state event identification creates scalability bottleneck
- Optimal split point determination lacks automatic methodology
- Limited evaluation scope to specific ToM task types without testing nested beliefs or counterfactual reasoning

## Confidence

**High Confidence:** The correlation between task complexity and model performance is well-supported by experimental results across multiple benchmarks and models. The statistical relationship appears robust and reproducible.

**Medium Confidence:** The mechanism by which DWM improves state tracking is plausible but not definitively proven. While the correlation with stateful complexity is strong, the causal relationship between explicit state descriptions and improved performance requires further validation.

**Low Confidence:** The scalability of the complexity framework to new task types without manual intervention is questionable. The framework's generalizability beyond the tested benchmarks and its ability to automatically identify state events remain significant concerns.

## Next Checks

1. **Automated State Event Detection Test:** Develop and evaluate an automated method for identifying state-changing events in prompts, then compare its accuracy against manual labeling across a diverse set of ToM tasks. Measure the impact on complexity calculations and performance predictions.

2. **Split Point Optimization Study:** Systematically vary the number of prompt splits in DWM across different task types and measure the resulting accuracy. Identify patterns in optimal split counts relative to task complexity metrics to develop guidelines for automatic split determination.

3. **Framework Generalizability Assessment:** Apply the complexity framework to a broader range of ToM tasks, including those with nested beliefs, counterfactual reasoning, and dynamic social interactions. Evaluate whether the stateful/stateless complexity distinction remains meaningful and predictive of performance in these more complex scenarios.