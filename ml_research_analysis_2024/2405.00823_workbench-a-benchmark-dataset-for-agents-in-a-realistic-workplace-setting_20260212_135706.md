---
ver: rpa2
title: 'WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting'
arxiv_id: '2405.00823'
source_url: https://arxiv.org/abs/2405.00823
tags:
- email
- task
- tasks
- event
- customer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WorkBench, a benchmark dataset for evaluating
  autonomous agents in realistic workplace settings. WorkBench contains a sandbox
  environment with five databases, 26 tools, and 690 tasks representing common business
  activities like sending emails and scheduling meetings.
---

# WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting

## Quick Facts
- arXiv ID: 2405.00823
- Source URL: https://arxiv.org/abs/2405.00823
- Reference count: 40
- Primary result: 690 realistic workplace tasks across 5 domains with outcome-centric evaluation showing LLM agents achieve 3-43% success

## Executive Summary
WorkBench is a benchmark dataset designed to evaluate autonomous agents in realistic workplace settings. It contains 690 tasks across five domains (Email, Calendar, CRM, Project Management, Analytics) with a sandbox environment featuring five databases and 26 tools. The tasks require planning, tool selection, and multiple actions to complete common business activities. Agents are evaluated using outcome-centric assessment, measuring whether final database states match ground truth regardless of the action path taken. Testing five existing ReAct agents revealed significant limitations, with success rates ranging from 3% to 43%, highlighting the gap between current agent capabilities and workplace task requirements.

## Method Summary
The WorkBench benchmark uses a sandbox environment with five simulated databases and 26 tools representing common business systems. Tasks are generated from human-curated templates across five domains, with programmatic generation creating multiple variations. The ReAct framework guides agents through reasoning and action selection. Evaluation is outcome-centric, measuring whether agents achieve the correct final state of the sandbox databases rather than specific action sequences. Five existing agents (GPT-4, GPT-3.5, Claude-2, Llama2-70B, Mixtral-8x7B) were tested on the full task set, with performance measured by accuracy and side effects.

## Key Results
- GPT-4 achieved the highest accuracy at 43%, while Llama2-70B scored only 3% success
- Agent performance dropped significantly when all 26 tools were available versus just required tools
- Common errors included sending emails to wrong people and unintended database modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Outcome-centric evaluation enables robust, automated assessment of agents performing workplace tasks
- Mechanism: Each task has a unique, unambiguous ground truth outcome defined by the expected final state of sandbox databases. Agents are evaluated solely on whether their actions result in matching this state, regardless of the specific action path taken
- Core assumption: The sandbox environment accurately represents the real-world systems the agent would interact with, and the ground truth outcomes are correctly defined for each task
- Evidence anchors:
  - [abstract] "If a task has been successfully executed, one (or more) of the database values may change. The correct outcome for each task is unique and unambiguous, which allows for robust, automated evaluation."
  - [section 3.1] "All tasks in WorkBench have a unique, unambiguous outcome, which is the expected state of the sandbox databases after successful completion of the task."

### Mechanism 2
- Claim: The combination of task templates with programmatic task generation creates diverse, realistic workplace tasks
- Mechanism: Human-curated templates capture common business activities, then programmatic generation creates multiple variations per template with different data combinations, yielding 690 varied tasks across five domains
- Core assumption: Human templates accurately represent realistic workplace tasks, and the programmatic generation maintains task validity while creating diversity
- Evidence anchors:
  - [section 3.2] "We manually create task templates that represent realistic workplace tasks. Templates are split into two categories: 1) Single domain: these only require tools from one domain... 2) Multi-domain: these require tools from multiple domains..."
  - [section 3.2] "We can define a simple function to automatically find the correct outcome. However, the agent must combine Analytics, Calendar and Email tools to complete this task."

### Mechanism 3
- Claim: The sandbox environment with multiple interconnected databases creates realistic multi-step planning requirements
- Mechanism: Five domain-specific databases (Calendar, Email, Analytics, CRM, Project Management) with 26 tools enable tasks requiring planning across multiple domains, tool selection, and multi-action execution
- Core assumption: The simulated databases capture essential complexity of real business systems while remaining manageable for evaluation
- Evidence anchors:
  - [section 3.1] "We create a sandbox environment consisting of five simulated databases: Calendar, Email, Website analytics, Customer relationship management (CRM), Project management"
  - [section 3.4] "Some tools have negative consequences if used incorrectly, such as sending emails to the wrong person."

## Foundational Learning

- Concept: ReAct framework for tool-using agents
  - Why needed here: WorkBench evaluates agents using the ReAct framework (Reason + Act), where agents must reason about tasks, select appropriate tools, and execute actions iteratively
  - Quick check question: What are the three key components of the ReAct framework that agents must implement when using WorkBench?

- Concept: Outcome-centric evaluation methodology
  - Why needed here: Unlike prior benchmarks that evaluate function calls, WorkBench evaluates whether the final state matches the ground truth, allowing multiple valid action paths
  - Quick check question: How does outcome-centric evaluation differ from traditional function-call-based evaluation in agent benchmarks?

- Concept: Multi-domain task planning
  - Why needed here: Many WorkBench tasks require combining tools from different domains (e.g., using both Calendar and Email tools), requiring agents to plan across system boundaries
  - Quick check question: What challenges do agents face when planning tasks that require tools from multiple domains simultaneously?

## Architecture Onboarding

- Component map: Sandbox databases (5) <- Tools (26) <- Agent (LLM with ReAct) <- Task templates -> Programmatic task generation -> Outcome evaluation
- Critical path: Task receipt -> Tool selection -> Action execution -> Database state update -> Outcome comparison
- Design tradeoffs: Simulated databases vs. real APIs (simplicity vs. realism), outcome-centric vs. function-call evaluation (flexibility vs. granularity), template-based vs. fully manual task creation (scalability vs. nuance)
- Failure signatures: Side effects (unintended database modifications), ReAct framework violations (missing ACTION keyword), tool selection errors (wrong arguments or tool choice), context window limitations (incomplete tool descriptions)
- First 3 experiments:
  1. Run a simple single-domain task with all tools available to test basic functionality
  2. Execute a multi-domain task to verify cross-domain tool coordination
  3. Test a task requiring multiple actions to evaluate sequential reasoning capabilities

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but identifies several areas needing investigation:

1. The impact of increasing the number of tools in WorkBench on agent accuracy
2. How human baselines compare to LLM agents on WorkBench tasks
3. The relationship between task complexity (number of required actions) and agent success rate

## Limitations

- The simulated sandbox environment may not capture all real-world complexity and edge cases
- Limited to 690 tasks across 5 domains, potentially missing important workplace scenarios
- Performance varies dramatically between models, suggesting framework implementation details significantly impact results

## Confidence

- Medium: The benchmark methodology is well-specified and reproducible, but real-world applicability remains uncertain
- Core evaluation claims: Medium confidence - outcome-centric evaluation is sound but assumes correct ground truth specification
- Performance results: Medium confidence - significant variation between models suggests implementation sensitivity
- Task coverage: Medium confidence - 690 tasks provide breadth but may miss critical scenarios

## Next Checks

1. **Sandbox vs Real-World Validation**: Test a subset of WorkBench tasks using real APIs instead of simulated databases to measure performance degradation and identify simulation artifacts.

2. **Cross-Framework Performance**: Implement the same tasks using alternative agent frameworks (e.g., AutoGPT, LangChain) to isolate whether performance differences stem from model capabilities versus framework design.

3. **Task Coverage Analysis**: Conduct a systematic review of the 690 tasks to identify missing workplace scenarios and evaluate whether the current distribution (single vs multi-domain, tool combinations) reflects actual business needs.