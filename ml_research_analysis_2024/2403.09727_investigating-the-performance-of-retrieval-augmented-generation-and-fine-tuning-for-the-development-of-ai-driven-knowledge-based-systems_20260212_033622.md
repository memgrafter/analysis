---
ver: rpa2
title: Investigating the performance of Retrieval-Augmented Generation and fine-tuning
  for the development of AI-driven knowledge-based systems
arxiv_id: '2403.09727'
source_url: https://arxiv.org/abs/2403.09727
tags:
- dataset
- datasets
- used
- text
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper compares two approaches\u2014fine-tuning (FN) and Retrieval-Augmented\
  \ Generation (RAG)\u2014for building AI-driven knowledge systems using large language\
  \ models. FN involves further training on domain-specific data, while RAG uses semantic\
  \ search to inject relevant context into the model."
---

# Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems

## Quick Facts
- arXiv ID: 2403.09727
- Source URL: https://arxiv.org/abs/2403.09727
- Authors: Robert Lakatos; Peter Pollner; Andras Hajdu; Tamas Joo
- Reference count: 24
- Key outcome: RAG-based systems outperform fine-tuning by 16% (ROUGE), 15% (BLEU), and 53% (cosine similarity), with lower hallucination risk

## Executive Summary
This paper investigates the performance of Retrieval-Augmented Generation (RAG) versus fine-tuning (FN) for building AI-driven knowledge-based systems using large language models. The study compares these two approaches across four models (GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2) on three datasets (COVID, CORN, and UB). Results demonstrate that RAG-based systems consistently outperform fine-tuned models in accuracy metrics while maintaining lower hallucination risk, with the optimal configuration using Llama-2-7B and sentence-level indexed datasets.

## Method Summary
The study employed two main approaches: fine-tuning models on domain-specific Q&A datasets using standard Hugging Face training procedures (batch size 4, learning rate 2e-4, epochs 5), and implementing RAG with semantic search using sentence transformers (MiniLM L6 v2) and cosine similarity filtering. Datasets were preprocessed from JSON, PDF, and Word formats, with text converted to raw form and split into paragraphs. Performance was evaluated using ROUGE, BLEU, METEOR, and cosine similarity metrics across multiple threshold values for context filtering.

## Key Results
- RAG-based systems outperformed fine-tuning by 16% (ROUGE), 15% (BLEU), and 53% (cosine similarity)
- Fine-tuning improved creativity (8% higher METEOR) but did not enhance RAG performance when combined
- Best RAG configuration achieved ROUGE 0.3, BLEU 0.063, and cosine similarity 0.57 using Llama-2-7B with sentence-level indexed datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG improves accuracy by injecting relevant context from indexed datasets
- Mechanism: RAG uses semantic search to find relevant documents, converts them to vectors using sentence transformers, and injects top matches into model context for answer generation
- Core assumption: Transformer architecture can effectively use injected context without forgetting pre-trained knowledge
- Evidence anchors: RAG outperforms FN by 16% (ROUGE), 15% (BLEU), and 53% (cosine similarity); does not require continuous retraining
- Break condition: If semantic search fails to retrieve relevant documents or context window is too small

### Mechanism 2
- Claim: Fine-tuning adapts models to domains but risks increased hallucination
- Mechanism: Further training on domain-specific Q&A pairs adjusts model parameters to target domain
- Core assumption: Parameters can be adjusted without catastrophic forgetting
- Evidence anchors: Fine-tuning improves creativity (8% higher METEOR) but increases hallucination risk
- Break condition: If dataset is too small or noisy, leading to overfitting or incorrect associations

### Mechanism 3
- Claim: Cosine similarity thresholding controls quality and size of injected context
- Mechanism: After embedding question and dataset sentences, only those above threshold are included in context
- Core assumption: Higher threshold filters irrelevant context but may exclude useful information if too high
- Evidence anchors: Threshold values defined 0-1 with 0.1 intervals; fine-tuned LlaMA-2-7b showed 100% rejection at threshold 1.0
- Break condition: If threshold is too low (irrelevant context injected) or too high (useful context excluded)

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how model processes context is crucial for grasping why RAG works
  - Quick check question: How does multi-head attention mechanism allow transformer to selectively focus on different parts of input sequence?

- Concept: Semantic search and vector embeddings
  - Why needed here: RAG relies on converting text to vector embeddings and using cosine similarity to find relevant documents
  - Quick check question: What is purpose of using sentence transformer like MiniLM L6 v2 in RAG pipeline?

- Concept: Evaluation metrics for text generation (ROUGE, BLEU, METEOR, cosine similarity)
  - Why needed here: Paper uses these metrics to compare RAG and FN performance
  - Quick check question: What is key difference between BLEU and ROUGE metrics?

## Architecture Onboarding

- Component map: User question -> Semantic search (sentence transformer + cosine similarity) -> Context retrieval -> Context injection -> LLM generation -> Answer output

- Critical path: User question -> Semantic search -> Context retrieval -> Context injection -> LLM generation -> Answer output

- Design tradeoffs:
  - RAG vs. FN: RAG offers lower hallucination risk and easier knowledge expansion but may be slower; FN adapts to domain but requires retraining for new knowledge
  - Context window size: Larger windows allow more context but increase computational cost and may dilute focus
  - Threshold tuning: Higher thresholds reduce noise but may exclude useful information

- Failure signatures:
  - RAG: Poor semantic search results, context window overflow, or model ignoring injected context
  - FN: Overfitting to training data, increased hallucination, or catastrophic forgetting

- First 3 experiments:
  1. Implement basic RAG pipeline with small dataset and evaluate performance on simple question-answering task; compare with base model without RAG
  2. Fine-tune model on domain-specific dataset and evaluate performance on same task; compare results with RAG approach
  3. Experiment with different threshold values in RAG pipeline to find optimal balance between relevance and context coverage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of RAG-based systems scale with increasingly larger and more diverse knowledge bases?
- Basis in paper: [inferred] Experiments limited to fixed datasets of moderate size
- Why unresolved: Limited to three specific datasets without exploring scalability
- What evidence would resolve it: Systematic evaluation across datasets of varying sizes and diverse domains

### Open Question 2
- Question: Can combining RAG with fine-tuning improve performance beyond either approach alone?
- Basis in paper: [explicit] Paper states combination is not trivial and did not enhance RAG performance
- Why unresolved: Only tested limited combination strategy without exploring alternative architectures
- What evidence would resolve it: Experiments testing hybrid architectures and multi-stage fine-tuning across multiple datasets

### Open Question 3
- Question: What is impact of hallucination risk in fine-tuned models when deployed in high-stakes domains?
- Basis in paper: [explicit] Notes RAG reduces hallucination but does not quantify risk in critical applications
- Why unresolved: Used general datasets without analyzing consequences in sensitive domains
- What evidence would resolve it: Comparative analysis of hallucination frequency and severity in high-stakes domains

## Limitations
- Study uses only four models on three specific datasets, limiting generalizability
- Preprocessing steps for converting PDF and Word documents are not fully specified
- Hallucination risk claims rely primarily on cosine similarity scores rather than systematic detection methods

## Confidence
- High Confidence: Experimental methodology and performance metrics are standard and reliable; RAG outperforms fine-tuning by specific percentages is well-supported
- Medium Confidence: RAG's lower hallucination risk is supported by cosine similarity but lacks direct detection analysis; fine-tuning improves creativity claim is reasonable but relationship to METEOR is not explicit
- Low Confidence: Insufficient detail on optimal threshold tuning process; claim that combining RAG with fine-tuning doesn't enhance performance needs further investigation

## Next Checks
1. Implement systematic hallucination detection framework to validate RAG's lower hallucination risk compared to fine-tuning beyond cosine similarity scores
2. Conduct comprehensive sensitivity analysis of cosine similarity thresholds across different dataset types and model sizes
3. Replicate experiments on additional domain-specific datasets beyond COVID, CORN, and UB to assess generalizability across different knowledge domains