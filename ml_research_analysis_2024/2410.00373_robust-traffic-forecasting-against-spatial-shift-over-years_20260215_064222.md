---
ver: rpa2
title: Robust Traffic Forecasting against Spatial Shift over Years
arxiv_id: '2410.00373'
source_url: https://arxiv.org/abs/2410.00373
tags:
- graph
- traffic
- data
- spatial
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance of state-of-the-art traffic
  forecasting models over extended time periods and finds significant degradation
  due to their inability to adapt to unobserved spatial dependencies. To address this,
  the authors propose a novel Mixture of Experts (MoE) framework that learns a set
  of graph generators (graphons) during training and adaptively combines them to generate
  new graphs under novel environmental conditions during testing.
---

# Robust Traffic Forecasting against Spatial Shift over Years

## Quick Facts
- arXiv ID: 2410.00373
- Source URL: https://arxiv.org/abs/2410.00373
- Reference count: 40
- Primary result: Novel Mixture of Experts (MoE) framework with learnable graph generators (graphons) that adaptively handles spatial distribution shifts in traffic forecasting, achieving substantial improvements on PEMS and NYC datasets.

## Executive Summary
This paper addresses the critical challenge of traffic forecasting under spatial distribution shifts over time. Traditional spatiotemporal graph neural networks degrade significantly when tested on data from different years due to unobserved spatial dependencies. The authors propose a Mixture of Experts framework that learns multiple graph generators (graphons) during training and adaptively combines them to handle novel environmental conditions during testing. The approach integrates seamlessly with existing ST-GNN architectures and is extended to Transformers, demonstrating substantial performance improvements across multiple benchmark datasets.

## Method Summary
The proposed approach employs a Mixture of Experts framework with learnable graph generators (graphons) to handle spatial distribution shifts in traffic forecasting. During training, the model learns K expert graphons, each capturing distinct graph relationships. An episodic training policy simulates distribution shifts by treating each expert as optimal for specific data segments while combining others to approximate it. During testing, mixup weights dynamically combine the expert graphons based on input signals to generate new graphs adapted to unseen conditions. The framework integrates with ST-GNNs through a learnable expert graphons layer and extends to Transformer architectures using Gumbel-softmax sampling for differentiable graph structure sampling.

## Key Results
- Significant improvements on PEMS datasets: 56.72% on PEMS07 and 45.15% on PEMS08 in out-of-distribution scenarios
- Effective handling of spatial dynamics with parsimonious approach requiring fewer parameters than ensemble methods
- Seamless integration with conventional ST-GNNs and Transformers while maintaining computational efficiency
- Outperforms existing approaches including LSTM, GWNet, AGCRN, MTGNN, and STAEformer in handling spatial distribution shifts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Mixture of Experts (MoE) framework learns a set of graph generators (graphons) that can adapt to novel environmental conditions by combining multiple graph relations.
- Mechanism: The framework trains K expert graphons during training, each capturing distinct graph relationships. During testing, it adaptively combines these graphons using learned weights to generate new graphs tailored to unseen data.
- Core assumption: Traffic data exhibits distinct graph relationships across different time periods, and these relationships can be effectively modeled by a finite set of graph generators.
- Evidence anchors:
  - [abstract] "The proposed Mixture of Experts (MoE) framework, which learns a set of graph generators (i.e., graphons) during training and adaptively combines them to generate new graphs under novel environmental conditions during testing."
  - [section] "Our method integrates seamlessly with conventional ST -GNNs by incorporating a learnable expert graphons layer."
- Break condition: If traffic patterns are too complex or non-stationary to be captured by a finite set of graph generators, or if the adaptive combination mechanism fails to learn meaningful weights.

### Mechanism 2
- Claim: The episodic training policy enhances the model's ability to handle out-of-distribution (OOD) scenarios by exposing it to simulated distribution shifts during training.
- Mechanism: During training, each expert is treated as the optimal solution for a specific data segment, while the other experts are combined to approximate it. This forces the model to learn robust representations that can generalize to unseen graph relations.
- Core assumption: Simulating distribution shifts during training can improve the model's robustness to real-world distribution shifts in traffic data.
- Evidence anchors:
  - [abstract] "We further extend this concept to the Transformer architecture, yielding significant performance improvements."
  - [section] "To address the spatial dynamic nature of traffic datasets, we draw inspiration from the successful implementation of the mixture of experts model [52] in domain generalization [53]."
- Break condition: If the episodic training policy does not effectively simulate real-world distribution shifts, or if the model overfits to the training distribution.

### Mechanism 3
- Claim: The Gumbel-softmax sampling technique enables differentiable sampling of discrete graph structures from continuous graphon representations.
- Mechanism: The graphon probability matrix is perturbed with Gumbel noise and passed through a softmax function to obtain a differentiable approximation of discrete sampling. This allows gradients to flow through the sampling process during training.
- Core assumption: Differentiable sampling of discrete graph structures is necessary for training the MoE framework end-to-end.
- Evidence anchors:
  - [section] "Subsequently, we can sample a graph Gk from graphon Pk incorporating with the Gumbel softmax [40] for reparameterizing the graph's probability distribution."
  - [section] "The graphs within one expert are produced under the same generator (i.e., graphon)."
- Break condition: If the Gumbel-softmax approximation is too crude or introduces excessive noise, hindering the learning process.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: The MoE framework is designed to be integrated with any ST-GNN that includes learnable graph modules, enhancing their ability to handle spatial distribution shifts.
  - Quick check question: Can you explain how GNNs capture spatial dependencies in graph-structured data?

- Concept: Domain Generalization
  - Why needed here: The paper addresses the challenge of traffic forecasting in out-of-distribution (OOD) scenarios, where the model needs to generalize to unseen graph relations.
  - Quick check question: What is the difference between domain adaptation and domain generalization?

- Concept: Mixture of Experts (MoE)
  - Why needed here: The MoE framework is the core innovation of the paper, enabling adaptive combination of multiple graph generators to handle spatial distribution shifts.
  - Quick check question: How does the MoE framework differ from traditional ensemble methods?

## Architecture Onboarding

- Component map: Input data -> Expert graphons layer (K graph generators) -> Mixup weights computation -> Graph combination -> Spatiotemporal module -> Forecast output

- Critical path:
  1. Input data is passed through the expert graphons layer to generate K graphons.
  2. Mixup weights are computed based on the input signal.
  3. The graphons are combined using the mixup weights to generate a new graph.
  4. The new graph is fed into the spatiotemporal module for processing.
  5. The output is the forecasted traffic flow.

- Design tradeoffs:
  - Number of experts (K): Increasing K allows for more diverse graph relations but increases computational complexity and memory requirements.
  - Temperature hyperparameter (s) in Gumbel-softmax: Higher values lead to smoother approximations but may introduce excessive noise.
  - Minimum sequence length for each expert (Î±1): Longer sequences allow for more stable graph relations but may reduce the model's ability to capture short-term dynamics.

- Failure signatures:
  - Poor performance on in-distribution data: The model may be overfitting to the training distribution or the expert graphons may not be capturing meaningful graph relations.
  - Poor performance on out-of-distribution data: The episodic training policy may not be effectively simulating real-world distribution shifts, or the mixup weights may not be learning meaningful combinations of graphons.
  - High computational cost: The number of experts (K) may be too large, or the spatiotemporal module may be too complex.

- First 3 experiments:
  1. Train the model with a small number of experts (e.g., K=2) on a simple traffic dataset to verify the basic functionality of the MoE framework.
  2. Evaluate the model's performance on both in-distribution and out-of-distribution data to assess its ability to handle spatial distribution shifts.
  3. Experiment with different temperature values in the Gumbel-softmax sampling to find the optimal balance between smoothness and noise.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of experts (K) for different traffic datasets and how does this choice affect model performance and computational efficiency?
- Basis in paper: [explicit] The paper mentions that "blindingly increasing the number of experts is not a reasonable choice" due to increased memory burden and potential insufficient training samples, and that "each sub-figure exhibits an obvious turning point" when evaluating different divisions.
- Why unresolved: The paper conducts a grid search to find optimal K values but does not provide a systematic method for determining the optimal number of experts across different datasets or traffic scenarios.
- What evidence would resolve it: A comprehensive study comparing model performance and computational costs across various K values for multiple traffic datasets, along with a principled approach for determining the optimal K based on dataset characteristics.

### Open Question 2
- Question: How does the proposed MoE framework handle sudden, extreme changes in traffic patterns (e.g., accidents, natural disasters) that differ significantly from both training and historical patterns?
- Basis in paper: [inferred] While the paper addresses spatial distribution shifts and OOD scenarios, it focuses on gradual changes and does not explicitly discuss handling extreme, unexpected events that cause drastic deviations from normal traffic pattern.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis of the model's performance under extreme, rare events that create completely novel traffic patterns.
- What evidence would resolve it: Testing the model's performance on datasets containing extreme events (e.g., data with significant disruptions) and comparing it to traditional methods in these scenarios.

### Open Question 3
- Question: Can the expert graphons framework be extended to handle multi-modal transportation data (e.g., integrating traffic, public transit, and pedestrian flows) and what would be the impact on performance and interpretability?
- Basis in paper: [inferred] The paper focuses on single-mode traffic forecasting and does not discuss the potential extension to multi-modal scenarios, which would be valuable for comprehensive urban transportation planning.
- Why unresolved: The paper does not explore the framework's ability to handle heterogeneous data sources or discuss the challenges and benefits of such an extension.
- What evidence would resolve it: Empirical studies comparing the model's performance on multi-modal datasets versus single-mode datasets, along with analysis of how the expert graphons adapt to different transportation modes.

## Limitations
- The framework assumes spatial dependencies can be effectively captured by a finite set of graph generators, which may not hold for extremely complex or non-stationary traffic patterns.
- The Gumbel-softmax sampling technique introduces approximation error in sampling discrete graph structures, potentially limiting the quality of generated graphs for complex traffic networks.
- The computational overhead of maintaining and sampling from multiple graph generators increases complexity compared to single-graph approaches.

## Confidence
- **High Confidence**: The framework's ability to improve OOD performance on benchmark datasets (PEMS07: 56.72%, PEMS08: 45.15% improvement) is well-supported by experimental results.
- **Medium Confidence**: The claim that the episodic training policy effectively simulates real-world distribution shifts is supported by the methodology but could benefit from more detailed analysis of how well the simulated shifts match actual temporal variations.
- **Medium Confidence**: The assertion that the framework can seamlessly integrate with any ST-GNN is theoretically sound but requires empirical validation across a broader range of architectures beyond the Transformer extension shown.

## Next Checks
1. Conduct ablation studies removing the episodic training policy to quantify its exact contribution to OOD performance improvements across different time horizons.
2. Measure and report the diversity of learned graphons using graph similarity metrics to verify that the experts capture genuinely distinct spatial relationships rather than redundant patterns.
3. Evaluate the framework's performance and computational efficiency on larger traffic networks (e.g., city-scale datasets) to assess its practical applicability beyond the regional PEMS datasets used.