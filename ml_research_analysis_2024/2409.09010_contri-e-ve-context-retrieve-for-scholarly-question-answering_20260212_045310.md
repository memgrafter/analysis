---
ver: rpa2
title: 'Contri(e)ve: Context + Retrieve for Scholarly Question Answering'
arxiv_id: '2409.09010'
source_url: https://arxiv.org/abs/2409.09010
tags:
- question
- knowledge
- scholarly
- information
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a hybrid question answering system for scholarly
  data that combines context extraction from structured and unstructured sources with
  prompt engineering using Llama3.1. The approach extracts relevant information about
  authors from DBLP and SemOpenAlex knowledge graphs and Wikipedia text, then uses
  this context in prompts to answer questions from the Scholarly-QALD dataset.
---

# Contri(e)ve: Context + Retrieve for Scholarly Question Answering

## Quick Facts
- arXiv ID: 2409.09010
- Source URL: https://arxiv.org/abs/2409.09010
- Reference count: 12
- Achieved 40% F1 score, ranking second in Scholarly-QALD challenge

## Executive Summary
Contri(e)ve presents a hybrid question answering system for scholarly data that combines context extraction from structured knowledge graphs (DBLP, SemOpenAlex) and unstructured Wikipedia text with prompt engineering using Llama3.1. The system extracts relevant author information through SPARQL queries and fuzzy search, then uses this context in prompts to answer questions from the Scholarly-QALD dataset. Despite achieving competitive performance (40% F1, 2nd place), the approach reveals significant challenges including LLM hallucinations, inconsistent responses, and data quality issues with missing ORCID identifiers across knowledge graphs.

## Method Summary
The approach uses a two-step methodology: first extracting context from multiple sources through SPARQL queries and fuzzy matching, then applying prompt engineering with Llama3.1-8b-Instruct. Context is grouped into author, institute, and publication categories, with information from DBLP, SemOpenAlex, and Wikipedia combined into structured prompts. The system processes questions from the Scholarly-QALD dataset (5000 training, 702 test samples) and evaluates performance using F1-score and Exact Match metrics, achieving 40% F1 and 32.05% Exact Match.

## Key Results
- Achieved 40% F1 score and 32.05% Exact Match on Scholarly-QALD test set
- Ranked second in the scholarly question answering challenge
- Identified LLM hallucination rate of approximately 40% when correct context was provided

## Why This Works (Mechanism)
The system leverages the complementary strengths of structured knowledge graphs and unstructured text to provide comprehensive context for LLM reasoning. By extracting specific author information from DBLP and SemOpenAlex using ORCID identifiers, then supplementing with Wikipedia text through fuzzy search, the approach creates rich prompts that guide the LLM toward accurate answers. The grouping of context into author, institute, and publication categories helps organize information for the model, while prompt engineering techniques attempt to mitigate hallucination through clear instructions and output indicators.

## Foundational Learning
- **SPARQL queries for KG extraction**: Needed to retrieve structured author information from DBLP and SemOpenAlex. Quick check: verify query returns expected author data.
- **Fuzzy string matching**: Required for finding relevant Wikipedia text segments. Quick check: confirm matches capture correct information.
- **Prompt engineering principles**: Essential for structuring LLM inputs to minimize hallucinations. Quick check: test different prompt templates.
- **LLM evaluation metrics**: F1-score and Exact Match needed to quantify performance. Quick check: ensure metric calculations match ground truth.
- **Knowledge graph reconciliation**: Important for handling missing ORCID identifiers between sources. Quick check: measure cross-source linking success rate.

## Architecture Onboarding

**Component Map**: SPARQL queries -> Context extraction -> Prompt template -> Llama3.1-8b-Instruct -> Evaluation metrics

**Critical Path**: Question parsing → Context retrieval (KG + Wikipedia) → Prompt construction → LLM inference → Answer evaluation

**Design Tradeoffs**: The system balances context richness (combining multiple sources) against LLM processing limitations (long prompts degrade performance), with trade-offs between comprehensive information extraction and manageable prompt lengths.

**Failure Signatures**: LLM hallucinations (40% of cases), missing cross-source linking due to absent ORCID identifiers, poor performance on numeric answers, and inconsistent responses to identical prompts.

**First Experiments**:
1. Test context extraction completeness by measuring information coverage across all three sources for sample questions
2. Evaluate prompt template variations on a subset of questions to identify optimal structure
3. Measure LLM response consistency across multiple runs with identical prompts to quantify hallucination rates

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the attention mechanism in LLMs affect their ability to correctly process and utilize context provided in prompts for scholarly question answering tasks?
- **Basis in paper**: [explicit] The paper discusses LLM inconsistencies in responding to questions despite correct context being provided, noting random behavior and hallucinations. It concludes that further study of attention paid to context in prompts is needed.
- **Why unresolved**: The paper identifies that LLMs show inconsistent performance with context processing but doesn't investigate the underlying mechanisms or attention patterns that cause this behavior.
- **What evidence would resolve it**: Experimental analysis of attention weights during LLM processing of scholarly QA prompts, comparison of attention patterns between successful and failed responses, and correlation studies between attention mechanisms and answer accuracy.

### Open Question 2
- **Question**: What is the optimal prompt length and structure for maximizing LLM performance on hybrid scholarly question answering tasks?
- **Basis in paper**: [explicit] The paper notes that both shorter and longer prompts negatively impact LLM performance - shorter prompts lead to hallucinations while longer prompts increase inference time and produce wrong answers.
- **Why unresolved**: The paper acknowledges this trade-off but doesn't systematically investigate the relationship between prompt length, information density, and LLM performance to find optimal configurations.
- **What evidence would resolve it**: Controlled experiments varying prompt length and structure while measuring F1 scores, analysis of information retention vs. processing efficiency, and identification of critical information thresholds for accurate responses.

### Open Question 3
- **Question**: How can the issue of missing ORCID identifiers between DBLP and SemOpenAlex knowledge graphs be systematically addressed to improve context extraction accuracy?
- **Basis in paper**: [explicit] The paper identifies that many ORCIDs are missing in SemOpenAlex KG, leading to incomplete context extraction, and notes attempts to use different SPARQL endpoints resulted in updated values that caused mismatches.
- **Why unresolved**: While the paper acknowledges this as a problem, it doesn't propose or evaluate systematic solutions for handling missing identifiers or reconciling data inconsistencies between knowledge graph versions.
- **What evidence would resolve it**: Development and evaluation of identifier reconciliation algorithms, comparison of different knowledge graph versions for completeness, and assessment of fallback strategies when primary identifiers are missing.

## Limitations
- LLM hallucination rate of approximately 40% when correct context is provided
- Missing ORCID identifiers between DBLP and SemOpenAlex knowledge graphs reduce cross-source linking effectiveness
- Long Wikipedia text substrings degrade LLM performance, suggesting need for better text selection methods

## Confidence
- **High confidence**: Experimental methodology and dataset usage (Scholarly-QALD with 5000 training and 702 test samples)
- **Medium confidence**: F1-score results (40%) and ranking (2nd place) given competitive challenge nature
- **Low confidence**: Generalizability of findings due to specific domain focus and limited failure case analysis

## Next Checks
1. **Context Selection Validation**: Test whether shorter, more focused Wikipedia text segments improve LLM performance by conducting controlled experiments with different text selection strategies
2. **Knowledge Graph Completeness Analysis**: Systematically measure the impact of missing ORCID identifiers by comparing model performance on questions where complete author information is available versus questions requiring cross-source linking
3. **Prompt Engineering Robustness Test**: Evaluate the consistency of LLM responses across multiple runs with identical prompts to quantify the extent of hallucination and instruction-following issues, testing whether prompt modifications can reduce these failures below the reported 40% rate