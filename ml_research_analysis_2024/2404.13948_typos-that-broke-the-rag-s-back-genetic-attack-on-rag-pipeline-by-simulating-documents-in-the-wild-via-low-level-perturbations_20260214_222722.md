---
ver: rpa2
title: 'Typos that Broke the RAG''s Back: Genetic Attack on RAG Pipeline by Simulating
  Documents in the Wild via Low-level Perturbations'
arxiv_id: '2404.13948'
source_url: https://arxiv.org/abs/2404.13948
tags:
- adversarial
- documents
- document
- attack
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates the robustness of Retrieval-Augmented Generation\
  \ (RAG) systems against low-level textual perturbations such as typos in retrieval\
  \ corpora, an underexplored vulnerability. The authors propose two new objectives\u2014\
  Relevance Score Ratio (RSR) and Generation Probability Ratio (GPR)\u2014to evaluate\
  \ how perturbations affect both the retriever and reader components of RAG."
---

# Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations

## Quick Facts
- **arXiv ID**: 2404.13948
- **Source URL**: https://arxiv.org/abs/2404.13948
- **Reference count**: 40
- **Primary result**: Genetic algorithm-based attack (GARAG) generates adversarial documents that achieve over 70% attack success rate against RAG systems using only low-level textual perturbations like typos.

## Executive Summary
This work investigates the robustness of Retrieval-Augmented Generation (RAG) systems against low-level textual perturbations such as typos in retrieval corpora, an underexplored vulnerability. The authors propose two new objectives—Relevance Score Ratio (RSR) and Generation Probability Ratio (GPR)—to evaluate how perturbations affect both the retriever and reader components of RAG. To exploit these vulnerabilities, they introduce GARAG, a genetic algorithm-based attack that generates adversarial documents by iteratively refining a population through crossover, mutation, and selection to simultaneously minimize RSR and GPR. Experiments across three QA datasets with diverse retrievers and LLMs show that GARAG achieves high attack success rates (over 70%) and significantly degrades both component-level and end-to-end performance, with even small perturbations causing substantial harm. The findings highlight that minor textual errors pose serious risks to RAG systems and that larger models do not necessarily exhibit better robustness.

## Method Summary
The GARAG attack method generates adversarial documents through a genetic algorithm that iteratively refines a population via crossover, mutation, and selection operations. The algorithm targets two objectives simultaneously: Relevance Score Ratio (RSR) for the retriever component and Generation Probability Ratio (GPR) for the reader component. Low-level perturbations including inner-shuffling, truncation, and keyboard/natural typos are applied to tokens. The attack is evaluated on three QA datasets (Natural Questions, TriviaQA, and SQuAD) using DPR and Contriever retrievers with Llama2, Vicuna, and Mistral LLMs. The method achieves high attack success rates while maintaining document stealth through controlled perturbation rates.

## Key Results
- GARAG achieves attack success rates over 70% across three QA datasets and multiple retriever/LLM combinations
- Small textual perturbations cause substantial harm to both retriever and reader components, with Component Error rates exceeding 60% in many cases
- Larger LLMs (13B) do not exhibit better robustness to low-level perturbations compared to smaller models (7B)
- Even minimal perturbation rates (0.1-0.2) achieve significant attack success while maintaining document stealth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-level perturbations like typos in retrieval corpora significantly degrade both retriever and reader performance in RAG systems.
- Mechanism: Minor textual errors alter token embeddings, shifting relevance scores and confusing the LLM's grounding process.
- Core assumption: Perturbations directly impact the token-level embeddings used in similarity scoring and language model token prediction.
- Evidence anchors:
  - [abstract] "minor textual errors pose serious risks to RAG systems" and "small perturbations causing substantial harm"
  - [section] "low-level textual errors found in retrieval corpora, often originating from human mistakes or preprocessing inaccuracies"
  - [corpus] Weak; no direct evidence from related work on typos in RAG
- Break condition: If token embeddings are robust to minor changes (e.g., through normalization or synonym handling), the attack impact would be reduced.

### Mechanism 2
- Claim: GARAG's genetic algorithm effectively explores the adversarial search space to find documents that simultaneously reduce retrieval relevance and reader grounding.
- Mechanism: Crossover and mutation operations on token-level perturbations allow the algorithm to iteratively refine adversarial documents toward the Pareto front of low LRSR and LGPR.
- Core assumption: The search space defined by LRSR and LGPR is sufficiently continuous for genetic operators to converge on optimal solutions.
- Evidence anchors:
  - [abstract] "GARAG achieves high attack success rates" and "significantly devastates the performance of each component and their synergy"
  - [section] "adversarial document population is initially generated by injecting low-level perturbations" and "iterative crossover, mutation, and selection processes"
  - [corpus] Weak; related work focuses on high-level semantic perturbations, not low-level genetic search
- Break condition: If the search space is too sparse or discontinuous, genetic operations may fail to find effective adversarial documents.

### Mechanism 3
- Claim: Larger LLMs do not inherently exhibit better robustness to low-level perturbations in RAG systems.
- Mechanism: Model size does not correlate with resilience to token-level noise, as both 7B and 13B models show similar vulnerability levels.
- Core assumption: Token-level perturbations affect both small and large models similarly in terms of embedding distortion and grounding failure.
- Evidence anchors:
  - [abstract] "even small perturbations causing substantial harm" and "larger models do not necessarily exhibit better robustness"
  - [section] "results reveal that an increase in model size does not necessarily enhance robustness to adversarial attacks"
  - [corpus] Weak; no direct comparison of model sizes against typos in related work
- Break condition: If larger models incorporate stronger tokenization or error correction, they might resist low-level perturbations better.

## Foundational Learning

- Concept: Genetic algorithms for multi-objective optimization
  - Why needed here: GARAG uses NSGA-II to balance two competing objectives (LRSR and LGPR) without gradient signals
  - Quick check question: What is the role of non-dominated sorting in NSGA-II when optimizing adversarial documents?

- Concept: Token-level perturbations and their impact on embeddings
  - Why needed here: Understanding how typos shift token embeddings is key to predicting retriever and reader failures
  - Quick check question: How do inner-shuffling, truncation, and keyboard typos each affect token embedding similarity?

- Concept: Retrieval-Augmented Generation pipeline architecture
  - Why needed here: Knowing the sequential flow from retriever to reader clarifies where low-level errors cause cascading failures
  - Quick check question: At which step in the RAG pipeline does a perturbed document most likely break the system?

## Architecture Onboarding

- Component map:
  - Retriever (DPR or Contriever) -> Reader (LLM) -> Answer generation

- Critical path:
  1. Document retrieval via similarity scoring
  2. Document grounding via LLM generation
  3. Adversarial document refinement via GARAG

- Design tradeoffs:
  - Perturbation rate vs. stealth: Lower perturbation rates increase stealth but may reduce attack success
  - Search space size vs. computational cost: Larger populations and more iterations improve coverage but increase runtime
  - Attack scope: Targeting only retriever vs. both retriever and reader affects system robustness assessment

- Failure signatures:
  - High LRSR with low LGPR: Retrieval error dominates
  - Low LRSR with high LGPR: Grounding error dominates
  - Both LRSR and LGPR low: Holistic system failure

- First 3 experiments:
  1. Vary perturbation rate (0.1, 0.2, 0.3) and measure ASR and component errors to find optimal stealth vs. impact tradeoff
  2. Compare attack success on DPR vs. Contriever to identify retriever-specific vulnerabilities
  3. Test GARAG with and without selection step to quantify its importance in balancing dual objectives

## Open Questions the Paper Calls Out
None

## Limitations
- The attack effectiveness is primarily evaluated on three QA datasets with Wikipedia-derived documents, which may not represent the full diversity of real-world retrieval corpora
- The genetic algorithm's effectiveness depends heavily on hyperparameters like population size and mutation rates that may require tuning for different retriever architectures or document domains
- Limited model size comparisons (7B vs 13B) provide insufficient evidence for claims about LLM robustness to token-level perturbations

## Confidence

**High confidence**: The mechanism that low-level perturbations affect token embeddings and similarity scoring is well-established in NLP literature. The empirical observation that GARAG achieves high attack success rates across multiple datasets and model combinations is supported by concrete metrics presented in the paper.

**Medium confidence**: The claim that larger LLMs do not inherently exhibit better robustness to token-level perturbations is based on limited model size comparisons (7B vs 13B). While the experimental results support this, broader validation across more model scales would strengthen this conclusion.

**Medium confidence**: The effectiveness of the genetic algorithm in exploring the adversarial search space is demonstrated through results, but the paper does not provide ablation studies showing how critical each genetic operator (crossover, mutation, selection) is to the attack success.

## Next Checks

1. **Dataset diversity validation**: Test GARAG on non-Wikipedia retrieval corpora with different document types (scientific papers, news articles, legal documents) to assess whether the attack generalizes beyond the current evaluation set.

2. **Model architecture ablation**: Conduct experiments comparing GARAG's effectiveness against retrievers with different similarity mechanisms (sparse vs dense) and readers with different tokenization strategies to identify which components are most vulnerable to low-level perturbations.

3. **Perturbation stealth analysis**: Systematically vary the perturbation rate and measure not only attack success but also the perceptual similarity of adversarial documents to original documents, determining the minimum perturbation level that still achieves meaningful attack success.