---
ver: rpa2
title: 'ReFT: Reasoning with Reinforced Fine-Tuning'
arxiv_id: '2401.08967'
source_url: https://arxiv.org/abs/2401.08967
tags:
- reft
- reward
- arxiv
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Reinforced Fine-Tuning (ReFT), a method that
  enhances the reasoning ability of large language models (LLMs) by combining supervised
  fine-tuning (SFT) with reinforcement learning (RL). ReFT first performs SFT to warm
  up the model, then applies RL (specifically PPO) to further refine the model by
  exploring multiple reasoning paths and using rewards derived from ground-truth answers.
---

# ReFT: Reasoning with Reinforced Fine-Tuning

## Quick Facts
- **arXiv ID:** 2401.08967
- **Source URL:** https://arxiv.org/abs/2401.08967
- **Reference count:** 22
- **Key result:** ReFT improves LLM reasoning accuracy on math datasets by up to 12 points over SFT alone.

## Executive Summary
ReFT introduces a two-stage fine-tuning approach that enhances the reasoning ability of large language models by combining supervised fine-tuning with reinforcement learning. The method first uses SFT to warm up the model, then applies RL (specifically PPO) to refine the model by exploring multiple reasoning paths and using rewards derived from ground-truth answers. Evaluated on three math problem datasets (GSM8K, MathQA, SVAMP) using two foundation models, ReFT significantly outperforms SFT and shows benefits from majority voting and reward model reranking at inference time.

## Method Summary
ReFT is a two-stage fine-tuning method for enhancing LLM reasoning capabilities. First, a supervised fine-tuning (SFT) stage warms up the model using (question, CoT) pairs for 1-2 epochs. Second, an RL stage applies PPO to further refine the model by sampling diverse reasoning paths, extracting answers, and computing rewards based on ground-truth correctness. The method uses KL divergence regularization to keep updates close to the warm-up policy and employs on-policy sampling for exploration.

## Key Results
- ReFT achieves up to 12-point improvements on GSM8K over SFT alone.
- Majority voting and reward model reranking at inference further improve performance.
- ReFT generalizes well across three math datasets (GSM8K, MathQA, SVAMP) and two foundation models (Galactica-6.7B, CodeLLAMA-7B).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ReFT's RL stage explores multiple reasoning paths per question, giving richer supervision than SFT's single annotated path.
- **Mechanism:** PPO samples diverse CoTs, each scored by a reward derived from the ground-truth answer; policy updates based on advantage estimates.
- **Core assumption:** Multiple correct CoTs exist for each question and the policy can learn from their distribution.
- **Evidence anchors:** [abstract] "an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers." [section 3.1] "In this stage, the policy improves its performance via a form of online self-learning using a dataset comprising of (question, answer) tuples: (x, y)."
- **Break condition:** If the reward function is noisy (e.g., reward hacking in MCQ), RL can reinforce incorrect CoTs.

### Mechanism 2
- **Claim:** The warm-up SFT stage constrains the RL policy to reasonable CoT space, preventing catastrophic divergence.
- **Mechanism:** Initial SFT produces a policy that already solves problems correctly enough; KL penalty in RL loss keeps updates close to this prior.
- **Core assumption:** Warm-up SFT yields a policy within the basin of attraction for correct reasoning.
- **Evidence anchors:** [abstract] "ReFT first warmups the model with SFT, and then employs on-line reinforcement learning... to further fine-tune the model." [section 3.1] "As mentioned in §3.1, our total reward is the sum of the reward function score and the Kullback-Leibler (KL) divergence between the learned RL policy and initial policy scaled by a coefficient factor β."
- **Break condition:** If β=0, policy collapses to degenerate outputs (accuracy drops to zero).

### Mechanism 3
- **Claim:** On-policy sampling plus PPO's clipped objective prevents large, destabilizing policy updates while exploring.
- **Mechanism:** PPO uses importance sampling with clipping to bound policy changes; advantage estimates guide updates toward high-reward CoTs.
- **Core assumption:** Advantage estimates are reliable indicators of CoT quality.
- **Evidence anchors:** [section 3.1] "We employ PPO (Schulman et al., 2017) with a clipped objective algorithm for training." [section 3.1] "The generalized advantage estimate... is used for advantage calculation."
- **Break condition:** If reward is sparse (e.g., numeric-only), learning is slow; partial rewards mitigate this.

## Foundational Learning

- **Concept:** Proximal Policy Optimization (PPO)
  - **Why needed here:** PPO balances exploration with stable updates, critical when CoT sampling is noisy and rewards are sparse.
  - **Quick check question:** What does the clipping parameter ε in PPO prevent, and why is that important for fine-tuning LLMs?
- **Concept:** Kullback-Leibler (KL) divergence as regularization
  - **Why needed here:** KL penalty keeps RL updates from drifting too far from the warm-up SFT policy, maintaining coherence.
  - **Quick check question:** If β=0 in the KL term, what training failure mode is observed in the ablation study?
- **Concept:** Generalized Advantage Estimation (GAE)
  - **Why needed here:** GAE reduces variance in advantage estimates, making RL updates more stable when rewards are binary.
  - **Quick check question:** How does the λ parameter in GAE trade off bias vs. variance in advantage estimation?

## Architecture Onboarding

- **Component map:** (question, answer) tuples -> warm-up SFT -> warm-up checkpoint -> RL stage (PPO) -> policy, value model -> reward from ground truth
- **Critical path:** 1. Load dataset -> warm-up SFT -> warm-up checkpoint; 2. Initialize RL: policy, value, optimizer; 3. Loop: sample CoTs -> extract answers -> compute rewards -> update PPO; 4. Evaluate on test set
- **Design tradeoffs:** Shared vs. separate value model: shared saves compute but may slow early RL convergence; Partial vs. binary reward: partial speeds learning but may introduce bias; On-policy vs. offline RL: on-policy explores more but is costlier
- **Failure signatures:** KL divergence spikes -> policy collapse; Reward hacking -> policy reinforces incorrect reasoning; Slow improvement -> sparse rewards, consider partial reward
- **First 3 experiments:** 1. Warm-up SFT with 2 epochs, verify CoT generation quality; 2. RL with binary reward only, monitor KL and accuracy curves; 3. RL with partial reward, compare convergence speed and final accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Reward function is simple and binary, which may limit performance on complex reasoning tasks where partial credit would be beneficial.
- No evidence of scalability to larger models or different domains beyond math problem solving.
- Computational overhead of on-policy RL sampling is not fully characterized.

## Confidence
- **High Confidence:** ReFT improves over SFT in tested math domains, as demonstrated by consistent accuracy gains across three datasets and two foundation models.
- **Medium Confidence:** The two-stage approach (SFT + RL) is effective, though the exact contribution of each stage is not fully isolated through ablation studies.
- **Low Confidence:** Generalization to non-math domains or larger models is speculative; the paper provides no evidence beyond the tested setup.

## Next Checks
1. **Reward Function Robustness Test:** Evaluate ReFT with a partial credit reward (e.g., step-wise correctness) on a subset of GSM8K to measure impact on learning speed and final accuracy.
2. **Ablation of Warm-up Stage:** Train a version of ReFT with RL only (no SFT warm-up) to quantify the contribution of the initial supervised fine-tuning to final performance.
3. **Domain Generalization Experiment:** Apply ReFT to a non-math reasoning dataset (e.g., CommonsenseQA) to assess whether the method generalizes beyond mathematical problem solving.