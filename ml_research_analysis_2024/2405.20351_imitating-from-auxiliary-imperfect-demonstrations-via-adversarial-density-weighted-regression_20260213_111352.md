---
ver: rpa2
title: Imitating from auxiliary imperfect demonstrations via Adversarial Density Weighted
  Regression
arxiv_id: '2405.20351'
source_url: https://arxiv.org/abs/2405.20351
tags:
- policy
- learning
- arxiv
- preprint
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in imitation learning from imperfect
  demonstrations, particularly the cumulative offsets from suboptimal rewards in multi-step
  Bellman updates and the Out-of-Distribution (OOD) state-action issues. The authors
  propose Adversarial Density Regression (ADR), a one-step supervised learning framework
  that directly aligns policy distribution with expert demonstrations while diverging
  from unknown-quality datasets, avoiding Bellman operator dependency and eliminating
  OOD concerns.
---

# Imitating from auxiliary imperfect demonstrations via Adversarial Density Weighted Regression

## Quick Facts
- arXiv ID: 2405.20351
- Source URL: https://arxiv.org/abs/2405.20351
- Reference count: 40
- ADR achieves state-of-the-art performance across Gym-Mujoco, Adroit, and Kitchen domains, notably improving 89.5% over IQL on Adroit and Kitchen tasks with ground truth rewards

## Executive Summary
This paper addresses the challenge of imitation learning from imperfect demonstrations, particularly the cumulative offsets from suboptimal rewards in multi-step Bellman updates and Out-of-Distribution (OOD) state-action issues. The authors propose Adversarial Density Regression (ADR), a one-step supervised learning framework that directly aligns policy distribution with expert demonstrations while diverging from unknown-quality datasets. ADR uses density-weighted Behavioral Cloning with adversarial density estimation via VAEs to estimate behavior densities of expert and imperfect datasets. The method provably converges to expert policy distribution and achieves near-optimal value bounds, while demonstrating superior training stability and OOD robustness compared to traditional IL and offline RL approaches.

## Method Summary
ADR operates as a one-step supervised learning paradigm that avoids Bellman operator dependency by directly minimizing KL divergence between the policy and expert density while maximizing divergence from unknown-quality dataset density. The method uses VAEs to estimate behavior densities for both expert and imperfect datasets, then applies density-weighted behavioral cloning where the regression loss is scaled by the ratio of unknown-quality to expert densities. This approach eliminates OOD concerns by training entirely on data from the union of demonstrations and unknown-quality datasets, ensuring all training samples are within distribution.

## Key Results
- ADR achieves state-of-the-art performance across Gym-Mujoco, Adroit, and Kitchen domains
- Notable 89.5% improvement over IQL on Adroit and Kitchen tasks with ground truth rewards
- Demonstrates superior training stability and OOD robustness compared to traditional IL and offline RL approaches
- Achieves better performance with limited demonstrations based on adversarial density-weighted regression

## Why This Works (Mechanism)

### Mechanism 1
ADR avoids cumulative offsets from suboptimal rewards by using a one-step supervised learning objective instead of multi-step Bellman updates. The ADR framework directly minimizes KL divergence between the policy and expert density while maximizing divergence from unknown-quality dataset density, eliminating the need for reward estimation or Q-function bootstrapping that accumulate errors over multiple steps. Core assumption: The one-step supervised learning objective can effectively correct policy distribution without relying on value function estimation.

### Mechanism 2
ADR eliminates OOD state-action issues by using only in-sample data from the combined dataset and demonstrations. By training entirely on data from the union of demonstrations and unknown-quality datasets, ADR ensures all training samples are within the distribution, avoiding the extrapolation errors that plague offline RL methods. Core assumption: The combined dataset contains sufficient coverage of the expert state-action space.

### Mechanism 3
The density-weighted regression objective provides more effective policy correction than standard behavioral cloning or reward-based IL methods. ADR uses density weights to scale the regression loss based on the ratio of unknown-quality to expert densities, focusing more on regions where the policy needs correction while avoiding over-correction in well-covered regions. Core assumption: The density estimation via VAEs provides accurate relative weights for different state-action pairs.

## Foundational Learning

- **Concept: KL Divergence**
  - Why needed here: ADR uses KL divergence minimization to align policy distribution with expert distribution while diverging from unknown-quality distribution
  - Quick check question: How does KL divergence differ from L2 distance when measuring distribution similarity?

- **Concept: Variational Autoencoders (VAEs)**
  - Why needed here: ADR uses VAEs to estimate the density of expert and unknown-quality datasets, which are then used for the density-weighted regression objective
  - Quick check question: What is the relationship between VAE ELBO and the true log density of data?

- **Concept: Behavioral Cloning**
  - Why needed here: ADR builds on behavioral cloning by adding density weighting to make it more effective with limited demonstrations
  - Quick check question: Why does standard behavioral cloning perform poorly with limited demonstrations?

## Architecture Onboarding

- **Component map:** VAE Pre-training -> Density Weighting -> Policy Network -> Training Loop
- **Critical path:**
  1. Pre-train VAEs on expert and unknown-quality datasets
  2. Compute density weights using VAE outputs
  3. Train policy using weighted regression objective
  4. Evaluate policy on held-out expert data

- **Design tradeoffs:**
  - VAE complexity vs. density estimation accuracy
  - Batch size vs. training stability
  - Weighting scheme vs. over-correction risk

- **Failure signatures:**
  - Poor density estimation → noisy or unstable density weights
  - Insufficient dataset coverage → OOD issues despite in-sample training
  - Over-weighted corrections → policy instability

- **First 3 experiments:**
  1. Train ADR on medium-replay dataset with 5 demonstrations to verify basic functionality
  2. Compare ADR vs. standard BC on same data to validate density weighting benefit
  3. Test ADR robustness by adding increasing amounts of noise to demonstrations

## Open Questions the Paper Calls Out

### Open Question 1
How does ADR's performance scale with the number of demonstrations when moving beyond the tested range (5 for Gym-Mujoco, 1 for Kitchen/Adroit)? The paper tests LfD(5) on Gym-Mujoco and LfD(1) on Kitchen/Adroit, noting ADR's strong performance, but doesn't explore performance with varying numbers of demonstrations.

### Open Question 2
What is the theoretical limit of ADR's policy distribution alignment with the expert when the expert demonstrations are scarce or noisy? Proposition 5.2 provides a bound on policy convergence but assumes the expert distribution is known or estimable, and doesn't account for demonstration scarcity or noise.

### Open Question 3
How does ADR's computational efficiency compare to other IL methods when scaling to high-dimensional action spaces or complex environments? The paper claims ADR is computationally efficient compared to ADE-divergence in Figure 6, but only tests on relatively simple Gym-Mujoco tasks with moderate action dimensions.

## Limitations
- The effectiveness of VAE-based density estimation may degrade in high-dimensional action spaces
- Performance depends on sufficient coverage of expert state-action space in the combined dataset
- Theoretical convergence guarantees assume accurate density estimation, which may be challenging for complex distributions

## Confidence

**High confidence:** ADR's mechanism for avoiding Bellman operator dependency and its one-step supervised learning framework

**Medium confidence:** The effectiveness of density-weighted regression in correcting policy distribution

**Low confidence:** The scalability of VAE-based density estimation to extremely complex state-action spaces

## Next Checks

1. **Ablation study on density estimation accuracy:** Systematically vary VAE architecture complexity and training duration to quantify the relationship between density estimation quality and ADR performance across all benchmark domains.

2. **Coverage analysis of combined datasets:** Measure the overlap between expert state-action distributions and the union of expert + imperfect datasets to verify the assumption that in-sample training provides sufficient coverage.

3. **Sensitivity analysis to demonstration quality:** Test ADR performance across a spectrum of demonstration quality levels (from near-optimal to highly suboptimal) to determine the method's breaking point and identify when density weighting becomes ineffective.