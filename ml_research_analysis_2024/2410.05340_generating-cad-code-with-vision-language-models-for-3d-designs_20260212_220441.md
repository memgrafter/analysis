---
ver: rpa2
title: Generating CAD Code with Vision-Language Models for 3D Designs
arxiv_id: '2410.05340'
source_url: https://arxiv.org/abs/2410.05340
tags:
- code
- object
- generated
- cadcodeverify
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CADCodeVerify, a novel approach for refining
  Computer-Aided Design (CAD) code generated by Vision-Language Models (VLMs) to create
  3D objects. The method employs a two-step self-correction process: generating and
  answering validation questions about the generated object, then producing corrective
  feedback based on discrepancies.'
---

# Generating CAD Code with Vision-Language Models for 3D Designs

## Quick Facts
- arXiv ID: 2410.05340
- Source URL: https://arxiv.org/abs/2410.05340
- Authors: Kamel Alrashedy; Pradyumna Tambwekar; Zulfiqar Zaidi; Megan Langwasser; Wei Xu; Matthew Gombolay
- Reference count: 40
- One-line primary result: CADCodeVerify achieved 7.30% reduction in Point Cloud distance and 5.5% improvement in successful object generation compared to prior work

## Executive Summary
This paper introduces CADCodeVerify, a novel approach for refining Computer-Aided Design (CAD) code generated by Vision-Language Models (VLMs) to create 3D objects. The method employs a two-step self-correction process: generating and answering validation questions about the generated object, then producing corrective feedback based on discrepancies. To evaluate CADCodeVerify, the authors introduce CADPrompt, the first benchmark for CAD code generation, containing 200 expert-annotated 3D objects with natural language prompts and Python code. When applied to GPT-4, CADCodeVerify achieved a 7.30% reduction in Point Cloud distance and a 5.5% improvement in successful object generation compared to prior work.

## Method Summary
CADCodeVerify is a novel approach for refining CAD code generated by VLMs to create accurate 3D objects. The method involves a three-step process: first, generating CAD code from natural language prompts using VLMs; second, executing the code with a CADQuery compiler to create 3D objects; and third, refining the code through self-initiated question-answering and feedback generation. This iterative refinement process aims to improve the quality of the generated 3D objects by identifying and addressing discrepancies between the intended and actual designs.

## Key Results
- Achieved 7.30% reduction in Point Cloud distance compared to prior work
- Improved successful object generation by 5.5% over existing methods
- Demonstrated superior performance on complex and difficult-to-generate objects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The self-correction loop through visual question-answering improves 3D object generation by identifying and addressing specific structural, dimensional, and positional discrepancies.
- **Mechanism**: CADCodeVerify generates targeted "Yes/No" questions about the generated object's properties, answers them using visual inspection, and creates actionable feedback to refine the CAD code iteratively.
- **Core assumption**: VLMs can accurately assess 3D objects through images and generate meaningful questions and answers about their properties.
- **Evidence anchors**:
  - [abstract] "CADCodeVerify achieved a 7.30% reduction in Point Cloud distance and a 5.5% improvement in successful object generation compared to prior work"
  - [section] "CADCodeVerify not only enhances the quality of generated 3D objects but also improves the performance of LLMs and VLMs by increasing the compile rate of compiled programs"
  - [corpus] Weak evidence - no direct comparison of question-answering accuracy in related work
- **Break condition**: If VLMs cannot reliably answer visual questions about 3D objects, or if the generated questions don't capture critical aspects of the design requirements.

### Mechanism 2
- **Claim**: The question-answer generation process enables model-agnostic feedback that can be applied across different LLMs/VLMs.
- **Mechanism**: By generating questions based on natural language descriptions and answering them using visual inspection, CADCodeVerify creates a feedback mechanism independent of the underlying model's architecture.
- **Core assumption**: The feedback generated through visual question-answering is universally applicable regardless of which LLM/VLM generated the initial code.
- **Evidence anchors**:
  - [abstract] "CADCodeVerify demonstrates superior performance on more challenging data" and "CADCodeVerify generates model-agnostic feedback to improve 3D object generation"
  - [section] "Our results indicate that CADCodeVerify improves the quality of the generated object across all three LLMs/VLMs as measured by IoGT, Point Cloud distance and Hausdorff distance"
  - [corpus] Moderate evidence - related work focuses on specific model architectures rather than model-agnostic approaches
- **Break condition**: If different LLMs/VLMs interpret feedback differently or if the feedback mechanism is not truly model-agnostic.

### Mechanism 3
- **Claim**: The iterative refinement process with visual feedback outperforms methods relying solely on geometric solver feedback.
- **Mechanism**: CADCodeVerify uses visual inspection and reasoning to generate feedback, while geometric solver feedback requires ground truth objects, making it less practical for real-world applications.
- **Core assumption**: Visual reasoning can identify discrepancies that geometric solvers might miss, particularly in complex structural and design aspects.
- **Evidence anchors**:
  - [abstract] "CADCodeVerify sets a new state-of-the-art 3D design via CAD scripting, achieving a 7.30% reduction in Point Cloud distance"
  - [section] "CADCodeVerify outperforms 3D-Premise across VLMs" and "achieve comparable performance without requiring access to the target object"
  - [corpus] Strong evidence - geometric solver baseline provides upper bound comparison
- **Break condition**: If geometric solver feedback consistently outperforms visual reasoning-based feedback, or if visual reasoning fails to capture critical geometric properties.

## Foundational Learning

- **Concept**: Point Cloud Distance and Hausdorff Distance
  - **Why needed here**: These metrics measure the geometric similarity between generated and ground truth 3D objects, providing quantitative evaluation of CADCodeVerify's performance.
  - **Quick check question**: What is the maximum possible Point Cloud distance between two points in a unit cube?

- **Concept**: Iterative Closest Point (ICP) Algorithm
  - **Why needed here**: ICP algorithm aligns the generated 3D object with the ground truth object for optimal comparison, ensuring accurate distance measurements.
  - **Quick check question**: Why is object alignment necessary before calculating distance metrics between 3D objects?

- **Concept**: Parametric CAD vs. Non-parametric 3D Representations
  - **Why needed here**: Understanding the difference is crucial for appreciating why CAD code generation is valuable for manufacturing applications compared to point clouds or voxels.
  - **Quick check question**: What are the advantages of parametric CAD representations over point clouds for manufacturing applications?

## Architecture Onboarding

- **Component map**: VLM Code Generator → CADQuery Compiler → Visual Inspector (CADCodeVerify) → Feedback Generator → VLM Code Refiner (iterative loop)
- **Critical path**: Natural language prompt → Initial code generation → Compilation → Visual inspection → Question generation → Answer generation → Feedback generation → Code refinement → Repeat until convergence or max iterations
- **Design tradeoffs**: 
  - Tradeoff between number of refinement iterations (computational cost) vs. improvement in object quality
  - Tradeoff between question specificity (detailed feedback) vs. generalization (applicable to various object types)
  - Tradeoff between using visual feedback vs. geometric solver feedback (practicality vs. precision)
- **Failure signatures**: 
  - Low compile rate indicates syntax or semantic errors in generated code
  - High "Unclear" answer rate suggests VLM struggles with visual interpretation
  - Convergence to local optima where further refinements don't improve object quality
  - Large Point Cloud distance despite multiple refinement iterations
- **First 3 experiments**:
  1. Run CADCodeVerify on a simple object (e.g., basic cube) to verify the complete pipeline works
  2. Test with a moderately complex object to evaluate question generation and answering accuracy
  3. Compare CADCodeVerify performance against baseline on a challenging object to measure improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CADCodeVerify perform on real-world CAD datasets compared to synthetic benchmarks like CADPrompt?
- Basis in paper: Inferred from the paper's discussion of CADPrompt as a synthetic benchmark and the acknowledgment that real-world data may differ in complexity and compilation difficulty.
- Why unresolved: The paper evaluates CADCodeVerify primarily on the synthetic CADPrompt dataset, which may not fully represent the challenges of real-world CAD code generation tasks.
- What evidence would resolve it: Testing CADCodeVerify on real-world CAD datasets with diverse complexity levels and comparing performance metrics (compile rate, point cloud distance, etc.) against those obtained on CADPrompt.

### Open Question 2
- Question: Can CADCodeVerify's question-answering mechanism be improved to reduce hallucinations and increase accuracy?
- Basis in paper: Explicit mention of the current accuracy of generated answers (64.6% for Refine 1 and 68.2% for Refine 2) and the strategy of instructing the model to respond with "Unclear" when uncertain.
- Why unresolved: The paper acknowledges that the accuracy of generated answers could be improved, but does not explore methods to enhance the question-answering mechanism.
- What evidence would resolve it: Developing and testing techniques to improve the VLM's ability to interpret 3D objects and self-verify its responses, such as incorporating additional geometric constraints or using external verification tools.

### Open Question 3
- Question: How sensitive is CADCodeVerify to the quality and specificity of the initial language prompts?
- Basis in paper: Inferred from the paper's discussion of prompt creation for CADPrompt and the acknowledgment that different prompts could lead to valid but functionally different interpretations of the same 3D object.
- Why unresolved: The paper mentions the importance of prompt quality but does not investigate how variations in prompt specificity or structure affect CADCodeVerify's performance.
- What evidence would resolve it: Conducting experiments with multiple prompts for the same 3D object to assess how prompt variations impact compile rate, point cloud distance, and the effectiveness of the refinement process.

## Limitations

- Data dependency and bias: The CADPrompt benchmark contains only 200 expert-annotated objects, which may not capture the full diversity of real-world CAD design scenarios.
- VLM visual interpretation accuracy: The method relies heavily on VLMs' ability to visually inspect 3D objects and generate meaningful questions and answers, but lacks quantitative measures of this capability.
- Model selection and hyperparameters: The paper doesn't specify exact VLMs used, their configurations, or hyperparameters for the refinement process, making reproduction difficult.

## Confidence

**High confidence claims**:
- The CADCodeVerify method can generate and refine CAD code for 3D objects using vision-language models
- The approach improves object quality metrics (Point Cloud distance, IoGT) compared to baseline methods
- The method is model-agnostic and can work across different VLMs

**Medium confidence claims**:
- The 7.30% reduction in Point Cloud distance and 5.5% improvement in successful generation are robust across different object types
- Visual question-answering is more practical than geometric solver feedback for real-world applications
- The iterative refinement process converges to improved designs

**Low confidence claims**:
- The method will generalize well to highly complex or domain-specific CAD designs beyond the benchmark
- Visual reasoning consistently outperforms geometric solver feedback across all design scenarios
- The approach is computationally efficient for practical deployment

## Next Checks

**Validation Check 1**: Conduct ablation studies to isolate the contribution of each component (question generation, answer generation, feedback generation) to overall performance improvements. This will help determine whether all three steps are necessary or if certain components provide most of the benefit.

**Validation Check 2**: Test the method on a significantly larger and more diverse dataset of 3D objects (beyond the 200 in CADPrompt) to evaluate generalization performance and identify potential failure modes with complex or unusual designs.

**Validation Check 3**: Measure and report the computational cost of the iterative refinement process, including average number of iterations needed for convergence and total processing time, to assess practical feasibility for real-world applications.