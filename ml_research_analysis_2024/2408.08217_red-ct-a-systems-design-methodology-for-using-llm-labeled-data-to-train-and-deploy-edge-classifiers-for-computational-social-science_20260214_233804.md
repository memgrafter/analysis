---
ver: rpa2
title: 'RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train and
  Deploy Edge Classifiers for Computational Social Science'
arxiv_id: '2408.08217'
source_url: https://arxiv.org/abs/2408.08217
tags:
- data
- edge
- labels
- tasks
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RED-CT, a systems design methodology that
  uses LLM-generated labels with minimal human annotation to train edge classifiers
  for computational social science tasks. The method incorporates confidence-informed
  sampling to select data for expert labeling and soft labels to account for annotation
  uncertainty during training.
---

# RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train and Deploy Edge Classifiers for Computational Social Science

## Quick Facts
- arXiv ID: 2408.08217
- Source URL: https://arxiv.org/abs/2408.08217
- Reference count: 7
- Primary result: RED-CT outperformed LLM labels in 7 of 8 experiments, achieving 6.5% average improvement over baseline classifiers using only 10% expert annotation

## Executive Summary
RED-CT introduces a systems design methodology that uses LLM-generated labels with minimal human annotation to train edge classifiers for computational social science tasks. The approach incorporates confidence-informed sampling to select data for expert labeling and soft labels to account for annotation uncertainty during training. Tested across four CSS tasks with two LLMs and three BERT-based models, RED-CT demonstrates that it's possible to approximate or outperform LLMs on CSS tasks with minimal human data labeling (10% of dataset), enabling effective deployment of NLP models in resource-constrained edge environments while reducing dependency on costly LLM APIs.

## Method Summary
RED-CT is a methodology that leverages LLM-generated labels as a base for training edge classifiers, then applies confidence-informed sampling to identify high-value examples for expert annotation, and finally uses soft labels derived from LLM probabilities during training to account for uncertainty. The process involves using LLMs to label large datasets, computing confidence scores from logit differences, sampling the lowest-confidence examples for expert annotation, generating soft labels from LLM probabilities, and training edge classifiers with these soft labels. The methodology was tested with two LLMs (GPT-3.5-turbo and Mistral-7B-Instruct-v0.2) and three BERT-based models (DistilBERT, RoBERTa, RoBERTa-Large) across four CSS tasks: stance detection, misinformation detection, ideology classification, and humor classification.

## Key Results
- RED-CT outperformed LLM-generated labels in 7 of 8 experimental comparisons
- Achieved an average 6.5% improvement over baseline classifiers using only 10% expert annotation
- Demonstrated effectiveness across four CSS tasks with different BERT model sizes and two prompting styles
- Showed that learning with soft labels significantly improved edge classifier performance compared to using only hard LLM labels

## Why This Works (Mechanism)

### Mechanism 1: Confidence-informed sampling
LLM label confidence scores (derived from logit differences) are inversely correlated with labeling errors. Sampling from the lowest confidence strata increases the probability of selecting mislabeled examples, making expert annotation more effective.

### Mechanism 2: Learning with soft labels
Soft labels derived from LLM probabilities weight the training loss, giving less influence to examples where the LLM is uncertain or likely wrong, reducing overfitting to noisy LLM labels.

### Mechanism 3: Efficient human-in-the-loop labeling
Combining LLM labels with minimal expert annotation creates effective training data without requiring full manual labeling. The LLM provides initial labels at scale, then expert annotation focuses only on high-value examples (low confidence), creating a high-quality labeled dataset that outperforms using LLM labels alone.

## Foundational Learning

- **Knowledge distillation**: Transfers knowledge from LLMs to smaller edge models that can run in resource-constrained environments. Quick check: What is the difference between standard fine-tuning and knowledge distillation?
- **Active learning/uncertainty sampling**: Selects which examples human experts should label based on where the LLM is most uncertain. Quick check: How does uncertainty sampling differ from random sampling in active learning?
- **Soft labels and label smoothing**: Incorporates LLM confidence into the training process, reducing the impact of potentially incorrect labels. Quick check: How do soft labels affect the loss function during training compared to hard labels?

## Architecture Onboarding

- **Component map**: Data pipeline → LLM labeling → Confidence scoring → Expert annotation (stratified sampling) → Soft label generation → Edge model training → Model deployment
- **Critical path**: Expert annotation (bottleneck) → Soft label generation → Edge model training
- **Design tradeoffs**: Higher expert annotation percentage improves performance but increases cost; larger edge models perform better but consume more resources
- **Failure signatures**: Poor confidence score calibration → ineffective sampling; biased LLM labels → systematic errors; insufficient expert data → marginal improvements
- **First 3 experiments**:
  1. Compare base classifier (LLM labels only) vs soft labels vs random sampling to establish individual intervention effects
  2. Vary expert annotation percentage (5%, 10%, 20%) to find optimal tradeoff between performance and cost
  3. Test different BERT model sizes (DistilBERT, RoBERTa, RoBERTa-Large) to evaluate performance-resource tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
How do different prompting techniques influence logit values and resulting confidence scores in LLM-generated labels? The paper only tested two prompting styles (zero-shot and zero-shot chain of thought) and did not conduct a comprehensive analysis of how various prompting techniques affect the distribution of logit values and confidence scores.

### Open Question 2
Are there better logit transformation techniques for confidence-informed sampling beyond the simple expit function used in this study? The study only used a basic expit function to transform probabilities into soft labels, and more sophisticated transformations might better capture the uncertainty in LLM predictions.

### Open Question 3
How does the proposed RED-CT methodology generalize to other domains beyond CSS tasks, such as image classification or spam detection? The paper only evaluated the approach on four CSS tasks, and the effectiveness of the system interventions for non-text data or different classification domains remains unexplored.

## Limitations

- Dataset representativeness: Evaluation relies on four specific CSS tasks, limiting generalizability to other NLP domains
- Expert annotation quality: Consistency and quality control procedures for expert annotations are not detailed
- Confidence score calibration: Calibration of LLM confidence scores across different tasks and models is not examined

## Confidence

- **High confidence**: The core RED-CT methodology is technically sound and the 6.5% average improvement over baseline classifiers is meaningful
- **Medium confidence**: The claim that 10% expert annotation provides optimal performance-cost tradeoff is supported but may be task-dependent
- **Low confidence**: The practical deployment benefits on actual edge devices, including specific resource consumption metrics, are not validated

## Next Checks

1. Apply RED-CT to 2-3 non-CSS NLP tasks (sentiment analysis, named entity recognition, question answering) to assess generalizability
2. Measure confidence score calibration quality using Expected Calibration Error (ECE) and test calibration methods
3. Deploy trained edge classifiers on representative edge hardware (Raspberry Pi, mobile device) to measure actual memory usage, latency, and battery consumption