---
ver: rpa2
title: Efficient Visual Transformer by Learnable Token Merging
arxiv_id: '2407.15219'
source_url: https://arxiv.org/abs/2407.15219
tags:
- token
- merging
- tokens
- transformer
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel transformer block, LTM-Transformer,
  that learns to merge tokens in a way that reduces the Information Bottleneck (IB)
  loss. The key idea is to generate an informative token merging mask through a learnable
  mask module, inspired by the principle of reducing redundancy while preserving important
  information.
---

# Efficient Visual Transformer by Learnable Token Merging
## Quick Facts
- arXiv ID: 2407.15219
- Source URL: https://arxiv.org/abs/2407.15219
- Authors: Yancheng Wang; Yingzhen Yang
- Reference count: 40
- Key outcome: Novel LTM-Transformer learns to merge tokens by reducing Information Bottleneck loss, achieving better efficiency than state-of-the-art methods while maintaining or improving accuracy

## Executive Summary
This paper proposes LTM-Transformer, a novel visual transformer block that learns to merge tokens in a task-aware manner to reduce computational complexity. The method generates an informative token merging mask through a learnable mask module, inspired by the principle of reducing redundancy while preserving important information. The approach is compatible with various popular visual transformer architectures including MobileViT, EfficientViT, ViT, and Swin, and demonstrates significant reductions in FLOPs and inference time while maintaining or improving prediction accuracy across image classification, object detection, and instance segmentation tasks.

## Method Summary
LTM-Transformer replaces standard transformer blocks with learnable token merging blocks that generate merging masks based on a variational upper bound of the Information Bottleneck loss. The mask module takes previous merging mask and current features to generate new merging weights, which are applied via matrix multiplication to produce compressed token representations. The method maintains compatibility with both regular transformers (self-attention followed by MLP) and efficient transformers (self-attention with convolutional operations) by merging both input and self-attention outputs. Training uses standard SGD optimization with cross-entropy loss, and Gumbel-Softmax is employed for binarized mask generation during training.

## Key Results
- LTM-MobileViT-S achieves 79.7% top-1 accuracy with 1.17G FLOPs, outperforming original MobileViT-S (78.4% accuracy, 1.4G FLOPs)
- LTM-EfficientViT-B1 achieves 82.5% top-1 accuracy when trained from scratch on ImageNet-1k
- On MS-COCO, LTM-Swin-S achieves 54.4% box AP and 47.1% mask AP with 260G FLOPs, comparable to original Swin-S (54.1% box AP, 46.5% mask AP, 310G FLOPs)
- Inference time reduced by 18.3% on MobileViT-S and 26.4% on EfficientViT-B1 compared to their original versions

## Why This Works (Mechanism)
### Mechanism 1
- Claim: The LTM-Transformer reduces Information Bottleneck (IB) loss by generating an informative token merging mask that preserves task-relevant information while reducing redundancy.
- Mechanism: The mask module generates merging weights based on a variational upper bound of the IB loss, encouraging merged tokens to be more correlated with class labels while being less correlated with input features. The merging process is formulated as one step of gradient descent on this bound.
- Core assumption: Tokens can be meaningfully weighted based on their informativeness to the task, and merging them according to these weights reduces redundancy without losing critical information.
- Evidence anchors: [abstract] "The key idea is to generate an informative token merging mask through a learnable mask module, inspired by the principle of reducing redundancy while preserving important information."

### Mechanism 2
- Claim: The LTM-Transformer maintains or improves accuracy while reducing FLOPs by merging tokens in a learnable, task-aware manner rather than uniform or similarity-based merging.
- Mechanism: Instead of uniform averaging or similarity-based merging, LTM uses learned merging weights that prioritize tokens contributing more to the task. This preserves task-relevant features while eliminating redundancy, maintaining representational power with fewer tokens.
- Core assumption: Task-relevant tokens have distinguishable importance that can be learned and exploited for merging decisions.
- Evidence anchors: [abstract] "Experiments on image classification, object detection, and instance segmentation tasks demonstrate that LTM-Transformer reduces FLOPs and inference time while maintaining or even improving prediction accuracy"

### Mechanism 3
- Claim: The LTM-Transformer framework is compatible with various visual transformer architectures (MobileViT, EfficientViT, ViT, Swin) while maintaining architectural efficiency.
- Mechanism: LTM blocks can replace standard transformer blocks in different architectures by adapting to their specific designs (regular vs efficient transformers). For efficient transformers, both input and self-attention outputs are merged using the same mask to maintain compatibility with residual connections and convolutional operations.
- Core assumption: The token merging mechanism can be adapted to different transformer architectures without breaking their fundamental design principles.
- Evidence anchors: [abstract] "The method is compatible with various popular visual transformer architectures, including MobileViT, EfficientViT, ViT, and Swin."

## Foundational Learning
- Concept: Information Bottleneck (IB) theory
  - Why needed here: The LTM-Transformer is explicitly motivated by reducing IB loss, which measures the trade-off between compression and prediction. Understanding IB helps explain why the merging mask prioritizes task-relevant information.
  - Quick check question: What is the mathematical expression for IB loss, and how does it relate to the trade-off between compression and prediction accuracy?

- Concept: Variational inference and upper bounds
  - Why needed here: The paper derives a variational upper bound for the IB loss that makes optimization tractable. Understanding variational methods is crucial for grasping how the mask module generates merging weights.
  - Quick check question: How does a variational upper bound make optimization of the IB loss possible in practice?

- Concept: Gradient descent in neural network layers
  - Why needed here: The mask module's update rule is formulated as one step of gradient descent on the IB bound. Understanding this helps explain the iterative nature of the merging process across layers.
  - Quick check question: How does treating each transformer layer as one step of gradient descent on the IB bound inform the design of the mask module?

## Architecture Onboarding
- Component map: Input tokens → Self-attention module → Mask module → Token merging → Merged tokens → MLP → Output
- Critical path: Token merging occurs after self-attention and before MLP in each LTM block; mask generation depends on both previous mask state and current features; merging weights are applied via matrix multiplication to produce compressed token representation
- Design tradeoffs: Compression ratio vs. accuracy (higher compression reduces FLOPs but risks losing information); mask module complexity vs. parameter efficiency (more sophisticated masking could improve quality but adds parameters); end-to-end trainability vs. specialized optimization (trades specialized optimization for standard SGD compatibility)
- Failure signatures: Accuracy degradation despite FLOP reduction indicates poor merging decisions; training instability could indicate improper mask module initialization or learning rate issues; memory issues during training might indicate incorrect handling of merged token dimensions
- First 3 experiments: 1) Replace one transformer block in MobileViT-XS with LTM block, keeping all other parameters frozen, and measure accuracy/FLOPs change; 2) Vary compression ratio (0.3 to 0.9) on LTM-ViT-B to find the sweet spot where accuracy is maintained while FLOPs are significantly reduced; 3) Compare LTM merging weights visualization against uniform merging to verify that informative tokens receive higher weights in practice

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical mechanism of reducing IB loss through variational upper bound optimization lacks empirical validation - no direct measurement of IB loss reduction during training
- Some key token merging baselines (HST, DGT, NDF) are missing from object detection and instance segmentation experiments
- Inference time measurements are reported only for batch size 1, without analysis of how performance scales with different batch sizes or hardware configurations

## Confidence
- High confidence: The empirical results showing LTM-Transformer reduces FLOPs and inference time while maintaining/improving accuracy across multiple tasks and architectures
- Medium confidence: The claim that LTM outperforms existing token merging methods, though some key baselines are missing from certain experiments
- Low confidence: The theoretical mechanism of reducing IB loss through variational upper bound optimization, which lacks empirical validation

## Next Checks
1. Instrument the training code to measure actual IB loss (or proxy metrics) during training to confirm that the variational upper bound optimization is reducing information bottleneck loss as claimed
2. Implement missing token merging baselines (HST, DGT, NDF) for object detection and instance segmentation experiments to ensure comprehensive comparison across all tasks
3. Test LTM-Transformer on additional transformer architectures not mentioned in the paper (e.g., PVT, ConvNeXt) to verify the claimed architectural compatibility extends beyond the four specified architectures