---
ver: rpa2
title: An Evolved Universal Transformer Memory
arxiv_id: '2410.13166'
source_url: https://arxiv.org/abs/2410.13166
tags:
- namms
- namm
- performance
- cache
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Neural Attention Memory Models (NAMMs), a
  learned memory management system for transformers that evolves atop pre-trained
  models to selectively prune KV cache tokens. By conditioning exclusively on attention
  matrix values, NAMMs provide different latent contexts for individual layers and
  attention heads, focusing on the most relevant information.
---

# An Evolved Universal Transformer Memory

## Quick Facts
- arXiv ID: 2410.13166
- Source URL: https://arxiv.org/abs/2410.13166
- Reference count: 40
- One-line primary result: NAMMs achieve up to 75% cache reduction while improving performance across 36 benchmarks through evolved attention-based token pruning

## Executive Summary
This paper introduces Neural Attention Memory Models (NAMMs), a learned memory management system for transformers that evolves atop pre-trained models to selectively prune KV cache tokens. By conditioning exclusively on attention matrix values, NAMMs provide different latent contexts for individual layers and attention heads, focusing on the most relevant information. Evolved on long-context language tasks, NAMMs achieve substantial performance improvements across 36 benchmarks while reducing cache size by up to 75%. Notably, NAMMs trained only on language tasks can be zero-shot transferred to entirely new transformer architectures across different modalities (vision, reinforcement learning), consistently outperforming hand-designed KV cache eviction methods that typically trade off efficiency for performance.

## Method Summary
NAMMs extract features from attention matrices using Short-Time Fourier Transform with Hann window, reduce them via exponential moving average, and use a small neural network to score each token. Tokens with negative scores are evicted from the KV cache. The system is optimized using Covariance Matrix Adaptation Evolution Strategy (CMA-ES) on long-context language modeling tasks, enabling direct optimization of non-differentiable token selection operations. The backward attention memory (BAM) architecture allows cross-token communication through counter-causal masking, helping the model learn diversity metrics and avoid redundant information.

## Key Results
- Achieved up to 75% reduction in KV cache size while maintaining or improving performance across 36 benchmarks
- Zero-shot transferred from language tasks to vision (Llava Next Video 7B) and reinforcement learning (Decision Transformer) domains
- Consistently outperformed hand-designed eviction methods that typically sacrifice performance for efficiency gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NAMMs improve transformer performance by selectively pruning the KV cache based on learned attention patterns
- Mechanism: NAMMs extract features from the attention matrix using STFT, reducing them via EMA, and use a small neural network to score each token. Tokens with negative scores are evicted, allowing the transformer to focus on the most relevant information
- Core assumption: Attention matrix values contain sufficient information to determine token importance for downstream performance
- Evidence anchors: [abstract] "NAMMs are universally applicable to any model using self-attention as they condition exclusively on the values in the produced attention matrices"; [section 3.1] "To meaningfully compress this unbounded vector signal, we process it via an STFT with a fixed-sized Hann window"
- Break condition: If attention matrix values do not correlate with token importance for downstream tasks, or if the STFT/EMA reduction loses critical information

### Mechanism 2
- Claim: The backward attention memory (BAM) architecture enables cross-token communication, allowing NAMMs to learn diversity metrics and avoid redundant information
- Mechanism: BAM uses a counter-causal mask in its self-attention layer, allowing each token to attend only to its future relatives in the KV cache. This creates an asymmetric relationship that distinguishes older and newer tokens, enabling the model to learn diversity metrics
- Core assumption: Sharing information from all tokens in memory is key for assessing their importance, especially in scenarios with repeated words or sentences
- Evidence anchors: [section 3.2] "We design the backward attention memory architecture (BAM) for parameter-efficient sharing of information while making use of the powerful inductive biases enabled by the masked self-attention operation"
- Break condition: If cross-token communication does not improve token importance assessment, or if the backward mask does not create the intended asymmetric relationship

### Mechanism 3
- Claim: Evolution-based optimization overcomes the non-differentiability of memory management operations, enabling direct optimization of downstream performance
- Mechanism: NAMMs use CMA-ES to evolve the weights of their small neural network, optimizing for normalized performance relative to the base model on a subset of long-context language modeling tasks
- Core assumption: Evolution is effective for optimizing non-differentiable token selection tasks that gradient-based methods cannot handle
- Evidence anchors: [abstract] "Evolution inherently overcomes the non-differentiability of memory management operations with binary outcomes (selecting tokens to preserve/discard) which renders gradient-based optimization incompatible"
- Break condition: If evolution does not converge to effective solutions, or if the optimized NAMMs do not generalize to new tasks and architectures

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding how self-attention works is crucial for grasping how NAMMs condition on attention matrix values to assess token importance
  - Quick check question: How does the attention matrix A = softmax(QK^T / sqrt(d)) represent the relative importance of tokens in processing each other's representations?

- Concept: Short-Time Fourier Transform (STFT) and spectrogram representation
  - Why needed here: NAMMs use STFT to extract features from the attention matrix, so understanding this technique is essential for comprehending the feature extraction process
  - Quick check question: How does the STFT with a Hann window convert a time-domain signal into a frequency-domain representation, and why is this useful for compressing the attention matrix?

- Concept: Covariance Matrix Adaptation Evolution Strategy (CMA-ES)
  - Why needed here: NAMMs are optimized using CMA-ES, an evolutionary algorithm, so understanding its principles is necessary for grasping the optimization process
  - Quick check question: How does CMA-ES sample, evaluate, and update a population of neural networks to optimize for a black-box objective function like downstream performance?

## Architecture Onboarding

- Component map: STFT feature extraction -> EMA reduction -> BAM/MLP scoring -> Token selection
- Critical path:
  1. Extract attention matrix A from the latest nup queries
  2. For each token in the KV cache, compute its spectrogram representation ωt_i using STFT
  3. Reduce the spectrogram over the time axis using EMA: ω_i = (Σt γ^t ωt_i) + γ^nT ω'_i
  4. Concatenate positional embedding to ω_i and feed to the memory model m_φ
  5. Apply BAM or MLP to compute score s_i = m_φ(ω_i)
  6. Evict tokens with s_i < 0 from the KV cache
  7. Repeat every nup steps

- Design tradeoffs:
  - Simplicity vs. expressivity: Using a small neural network with few parameters (4000) enables efficient training and transfer, but may limit the model's ability to capture complex patterns
  - Compression vs. information loss: STFT and EMA reduction compress the attention matrix, but may lose some information that could be useful for token selection
  - Frequency vs. time information: Using spectrogram features captures frequency information, but loses the temporal ordering of attention values

- Failure signatures:
  - Poor performance on training tasks: Indicates the memory model is not learning effective token selection strategies
  - Inability to transfer to new architectures or domains: Suggests the learned strategies are too specific to the training setup
  - Degradation in performance with longer prompts: May indicate the memory model struggles to handle increased complexity or information density
  - Overfitting to training tasks: Can be detected by evaluating on held-out test tasks and comparing performance to the base model

- First 3 experiments:
  1. Evaluate the impact of different EMA coefficients (γ) on memory size and performance to understand the trade-off between compression and information preservation
  2. Compare the performance of BAM vs. MLP architectures to validate the importance of cross-token communication for token selection
  3. Test the zero-shot transfer capability of NAMMs to a different transformer architecture (e.g., Mistral 7B) to assess generalization and identify potential failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal EMA reduction coefficient (γ) for NAMMs across different long-context tasks and modalities?
- Basis in paper: [explicit] The paper notes that using γ = 0.9999sw (instead of γ = 0.99sw) improved performance on longer prompts in the Needle In A Haystack task, suggesting current coefficient might be suboptimal
- Why unresolved: The paper only tested two values of γ, and the optimal coefficient likely varies based on task characteristics (e.g., prompt length, information density, modality) and NAMM architecture
- What evidence would resolve it: Systematic experiments varying γ across diverse long-context tasks and modalities, potentially with γ learned through evolution rather than fixed

### Open Question 2
- Question: How do NAMMs perform when trained on multiple base transformer architectures and input modalities simultaneously?
- Basis in paper: [inferred] The paper shows NAMMs trained only on language tasks can zero-shot transfer to vision and reinforcement learning transformers, but suggests training on multiple architectures/modalities could yield more robust transfer
- Why unresolved: All NAMMs in the paper were trained on a single Llama 3 8B language model, leaving unexplored how multi-architecture training affects performance and transferability
- What evidence would resolve it: Training NAMMs on a curriculum including multiple transformer architectures (different sizes, modalities) and evaluating zero-shot transfer performance compared to single-architecture training

### Open Question 3
- Question: What is the relationship between NAMM performance and the underlying transformer's architectural features (e.g., number of layers, attention heads, GQA implementation)?
- Basis in paper: [inferred] The paper transfers NAMMs to transformers with different architectures (Llama 3 70B, Llava Next Video 7B, Decision Transformer) but doesn't analyze how architectural differences affect NAMM performance
- Why unresolved: The paper demonstrates successful transfer but doesn't investigate whether certain architectural features make transformers more or less amenable to NAMM optimization
- What evidence would resolve it: Controlled experiments varying transformer architectural parameters while keeping NAMMs constant, measuring performance changes to identify architectural features that enhance or limit NAMM effectiveness

## Limitations

- Architectural generalization remains uncertain, as zero-shot transfer results are limited to transformers within the same architectural family and narrow RL applications
- The STFT-EMA feature extraction pipeline may lose critical information through compression, with limited ablation studies on alternative approaches
- The evolutionary optimization method's stability and convergence are not thoroughly analyzed, raising questions about whether solutions represent global optima

## Confidence

- High Confidence: The core claim that NAMMs can reduce KV cache size by up to 75% while maintaining or improving performance on long-context language tasks is well-supported by the experimental results across 36 benchmarks
- Medium Confidence: The zero-shot transfer capability to new transformer architectures across different modalities is demonstrated but limited to two additional models
- Low Confidence: The assertion that evolution is the only viable optimization method for this problem is not rigorously defended

## Next Checks

1. **Architectural Transfer Stress Test**: Evaluate NAMMs on fundamentally different transformer architectures including hybrid CNN-Transformer models, state-space models (Mamba), and sparse attention variants (BigBird, Longformer) to test the limits of architectural generalization

2. **Feature Extraction Ablation**: Systematically compare STFT-EMA against alternative feature extraction pipelines including raw attention values, temporal convolutional networks, and learned linear projections to isolate the contribution of the specific feature representation

3. **Optimization Method Comparison**: Implement and compare alternative optimization approaches such as proximal policy optimization (PPO) or evolutionary strategies with different mutation operators to assess whether CMA-ES is truly necessary or simply one viable option