---
ver: rpa2
title: 'MU-Bench: A Multitask Multimodal Benchmark for Machine Unlearning'
arxiv_id: '2406.14796'
source_url: https://arxiv.org/abs/2406.14796
tags:
- unlearning
- machine
- learning
- https
- proceedings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MU-Bench, the first comprehensive benchmark
  for machine unlearning (MU) that unifies evaluation settings and expands coverage
  to previously unexplored tasks and modalities like speech and video. The authors
  propose a taxonomy of MU methods based on knowledge measurement, corruption, and
  retention strategies, and evaluate five representative approaches across nine datasets.
---

# MU-Bench: A Multitask Multimodal Benchmark for Machine Unlearning

## Quick Facts
- arXiv ID: 2406.14796
- Source URL: https://arxiv.org/abs/2406.14796
- Authors: Jiali Cheng; Hadi Amiri
- Reference count: 40
- Primary result: Introduces first comprehensive benchmark for machine unlearning across tasks, modalities, and architectures

## Executive Summary
This paper introduces MU-Bench, the first comprehensive benchmark for machine unlearning (MU) that unifies evaluation settings and expands coverage to previously unexplored tasks and modalities like speech and video. The authors propose a taxonomy of MU methods based on knowledge measurement, corruption, and retention strategies, and evaluate five representative approaches across nine datasets. Key findings include: RAND LABEL and SALUN are the most effective general MU methods, achieving close-to-random performance on deleted data while preserving test accuracy; BAD-T and SCRUB can achieve random performance on deletion sets but may compromise overall performance; existing MU methods struggle with audio and video tasks due to strong inter-sample correlations; and parameter-efficient fine-tuning (PEFT) benefits MU but requires larger trainable parameter thresholds than typical fine-tuning tasks.

## Method Summary
MU-Bench provides standardized datasets, models, implementations, and evaluation metrics to enable fair comparisons of MU methods. The benchmark includes nine publicly available datasets covering discriminative (image, text, graph, speech) and generative tasks, evaluated across 20 architectures and 34 scales. Five representative MU methods (NEGGRAD, RAND LABEL, BAD-T, SCRUB, SALUN) are implemented and evaluated using unified metrics including performance on test set DTest (↑), deletion set Df (↓), remaining set Dr (↑), unlearning time (↓), and membership inference attack success rate (↓). The benchmark also includes a leaderboard for submitting and comparing results.

## Key Results
- RAND LABEL and SALUN achieve close-to-random performance on deleted data while preserving test accuracy across most tasks
- BAD-T and SCRUB can achieve random performance on deletion sets but may compromise overall performance
- Existing MU methods struggle with audio and video tasks due to strong inter-sample correlations
- PEFT benefits MU but requires at least 50% of parameters to be trainable, unlike standard fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
MU-Bench enables fair and scalable comparisons of MU methods by unifying evaluation settings across tasks, modalities, and architectures. Standardizing deleted sample sets (1-10% increments), baseline models, and evaluation metrics removes inconsistencies that previously hindered meaningful comparisons. This allows researchers to isolate method effectiveness from confounding variables like architecture choice or deletion strategy.

### Mechanism 2
The teacher-student framework taxonomy reveals that knowledge measurement method (loss vs representation vs logits) is the key differentiator