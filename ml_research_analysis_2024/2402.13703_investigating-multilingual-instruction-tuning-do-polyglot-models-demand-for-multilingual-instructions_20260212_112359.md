---
ver: rpa2
title: 'Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for
  Multilingual Instructions?'
arxiv_id: '2402.13703'
source_url: https://arxiv.org/abs/2402.13703
tags:
- language
- multilingual
- evaluation
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether multilingual pre-trained Large
  Language Models require multilingual instructions for optimal cross-lingual performance.
  The authors translate MT-Bench into five Indo-European languages and create parallel
  instruction-tuning datasets of varying sizes.
---

# Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?

## Quick Facts
- arXiv ID: 2402.13703
- Source URL: https://arxiv.org/abs/2402.13703
- Reference count: 40
- Mid-sized multilingual models require large-scale instruction-tuning datasets, contrary to the Superficial Alignment Hypothesis

## Executive Summary
This study investigates whether multilingual pre-trained Large Language Models require multilingual instructions for optimal cross-lingual performance. The authors translate MT-Bench into five Indo-European languages and create parallel instruction-tuning datasets of varying sizes. They systematically instruction-tune both mid-sized (7B) and large (8x7B) multilingual models on different language mixtures, including monolingual, parallel, and sampled datasets. Results show that instruction-tuning on parallel datasets improves cross-lingual instruction-following capabilities by up to 9.9% compared to monolingual training. The study also finds that the Superficial Alignment Hypothesis does not hold for mid-sized models, which require large-scale instruction-tuning datasets.

## Method Summary
The authors translate MT-Bench into English, German, French, Italian, and Spanish to create MT-Bench-X. They create parallel instruction-tuning datasets (Lima-X with 1030 instructions per language and Bactrian-X with 64K samples per language) and test different language mixtures including monolingual, parallel (ENDEFRITES), and sampled variants. Two model sizes are fine-tuned: 24EU-7B (mid-sized) and Mixtral-8x7B-v0.1 (large) using learning rate 1e-5, batch size 64, Adam optimizer, and weight decay 0.1. Models are evaluated on MT-Bench-X using GPT-4-as-a-judge with single score and pair-wise modes, supplemented by human evaluation for selected models.

## Key Results
- Parallel instruction-tuning improves cross-lingual instruction-following by up to 9.9% compared to monolingual training
- Mid-sized 7B models require large-scale instruction-tuning datasets, contradicting the Superficial Alignment Hypothesis
- Synthetic datasets can outperform human-curated datasets for multilingual instruction-tuning when sufficiently large
- Including the predominant pre-training language (English) during parallel instruction-tuning is crucial for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuning on parallel datasets improves cross-lingual instruction-following by up to 9.9% compared to monolingual training.
- Mechanism: Parallel datasets ensure semantic alignment across languages, allowing models to learn instruction-response patterns that generalize across languages rather than overfitting to monolingual context.
- Core assumption: Semantic meaning is preserved when translating instructions and responses across the five Indo-European languages.
- Evidence anchors:
  - [abstract] "instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 9.9%"
  - [section] "the performance improvement when including the predominant pre-training language during parallel instruction-tuning (ENDEFRITES vs DEFRITES) highlights the importance of its inclusion"
  - [corpus] Weak - the corpus section only provides neighbor scores, not direct evidence about translation quality or semantic preservation
- Break condition: If translation introduces significant semantic drift or if models rely on surface-level patterns rather than semantic understanding.

### Mechanism 2
- Claim: Mid-sized multilingual models (7B) require large-scale instruction-tuning datasets, contrary to the Superficial Alignment Hypothesis.
- Mechanism: Mid-sized models have not developed sufficient world knowledge during pre-training to infer instruction patterns from few examples, requiring extensive fine-tuning data.
- Core assumption: Pre-training data distribution and model capacity determine the effectiveness of the Superficial Alignment Hypothesis.
- Evidence anchors:
  - [abstract] "we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets"
  - [section] "our results show that the Superficial Alignment Hypothesis (Kirstain et al., 2022; Zhou et al., 2023) does not generally hold for mid-sized LLMs"
  - [corpus] Weak - no corpus evidence directly addresses the hypothesis or pre-training data specifics
- Break condition: If larger models consistently show similar requirements for large-scale datasets, or if specific task types benefit from few-shot learning regardless of model size.

### Mechanism 3
- Claim: Synthetic datasets can outperform human-curated datasets for multilingual instruction-tuning when sufficiently large.
- Mechanism: Large synthetic datasets provide more diverse training signals and exposure to varied instruction formats, compensating for potential quality differences compared to human-curated data.
- Core assumption: Dataset size and diversity can compensate for potential quality gaps between synthetic and human-curated data.
- Evidence anchors:
  - [section] "the Bactrian-X-based synthetic datasets outperform LIMA-X for both dataset magnitudes" and "we note consistent improvement of parallel instruction-tuning, i.e., for the sampled as well as full-sized datasets"
  - [abstract] "we show that the Superficial Alignment Hypothesis does not hold in general" (implying synthetic data requires more examples)
  - [corpus] No direct evidence - corpus section doesn't address dataset quality or synthetic vs human-curated comparisons
- Break condition: If quality differences between synthetic and human-curated data create harmful biases or if models overfit to synthetic patterns.

## Foundational Learning

- Concept: Cross-lingual semantic alignment
  - Why needed here: Understanding how parallel datasets maintain meaning across languages is crucial for interpreting the 9.9% improvement
  - Quick check question: If an instruction about "writing an email" is translated from English to German, what linguistic elements must be preserved to maintain semantic equivalence?

- Concept: Pre-training data distribution impact
  - Why needed here: The 43.88% English dominance in pre-training data explains why English instruction-tuning shows strong monolingual performance
  - Quick check question: Given the pre-training language distribution (43.88% EN, 8.65% FR, 7.63% ES, 8.48% DE, 4.64% IT), which language would you expect to show the strongest baseline performance without instruction-tuning?

- Concept: Model capacity vs. instruction-tuning requirements
  - Why needed here: Explains why the Superficial Alignment Hypothesis holds for Mixtral-8x7B but not 24EU-7B models
  - Quick check question: If a 7B model needs 1000+ instruction examples but a 70B model needs only 10, what aspect of model architecture or pre-training is likely responsible?

## Architecture Onboarding

- Component map: Pre-trained multilingual model → Instruction-tuning dataset (parallel/monolingual/synthetic) → Evaluation on MT-Bench-X → Human/GPT-4 evaluation → Performance metrics
- Critical path: Dataset creation → Model fine-tuning → Evaluation setup → Result analysis
- Design tradeoffs: Parallel datasets offer cross-lingual benefits but require translation quality; synthetic datasets offer scale but may introduce noise
- Failure signatures: Inconsistent cross-lingual improvements across language pairs; large performance gaps between synthetic and human-curated datasets; positional bias in evaluation
- First 3 experiments:
  1. Fine-tune 24EU-7B on monolingual English dataset, evaluate on all five languages to establish baseline
  2. Fine-tune same model on parallel ENDEFRITES dataset, compare cross-lingual performance to monolingual
  3. Fine-tune on DEFRITES (excluding English), evaluate to test importance of predominant pre-training language inclusion

## Open Questions the Paper Calls Out

Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?

Open Question 1
- Question: What is the minimum dataset size required for effective multilingual instruction-tuning across different language families?
- Basis in paper: [inferred] The paper investigates dataset size effects but only examines Indo-European languages
- Why unresolved: The study focuses on high-resource European languages but doesn't explore low-resource languages or more distant language families
- What evidence would resolve it: Comparative studies testing instruction-tuning effectiveness across language families with varying resource levels and dataset sizes

Open Question 2
- Question: How does the choice of translation engine affect the quality and performance of multilingual instruction-tuning datasets?
- Basis in paper: [explicit] The authors chose DeepL for translation but acknowledge potential issues
- Why unresolved: The paper only uses one translation engine and doesn't compare alternatives or evaluate translation quality systematically
- What evidence would resolve it: Systematic comparison of different translation engines' impact on instruction-tuning performance and quality metrics

Open Question 3
- Question: Does the Superficial Alignment Hypothesis hold for larger multilingual models beyond 8x7B parameters?
- Basis in paper: [explicit] The authors find the hypothesis doesn't hold for 7B models but works for 8x7B models
- Why unresolved: The study only tests two model sizes, leaving the hypothesis's applicability to even larger models unknown
- What evidence would resolve it: Instruction-tuning experiments with progressively larger multilingual models to identify the parameter threshold where the hypothesis becomes valid

Open Question 4
- Question: What is the optimal language mixture ratio for multilingual instruction-tuning across different model sizes?
- Basis in paper: [inferred] The study tests different language compositions but doesn't optimize mixture ratios
- Why unresolved: The paper examines parallel vs. monolingual training but doesn't investigate optimal proportions of each language
- What evidence would resolve it: Systematic experiments varying language proportions to find optimal mixtures for different model sizes and language combinations

## Limitations

- Translation quality and semantic preservation across languages are not directly evaluated, which could impact cross-lingual performance
- Human evaluation component uses a relatively small sample size (32 instructions) that may not capture systematic evaluation biases
- Findings are based on specific model architectures and may not generalize to other multilingual models with different pre-training distributions

## Confidence

**High Confidence**:
- Cross-lingual performance improvements from parallel instruction-tuning (9.9% improvement)
- Importance of including the predominant pre-training language (English) in multilingual instruction-tuning
- Consistency between human and GPT-4-based evaluation for multilingual chat scenarios

**Medium Confidence**:
- The failure of the Superficial Alignment Hypothesis for mid-sized models
- Synthetic datasets outperforming human-curated datasets when sufficiently large
- Performance differences between single-score and pair-wise evaluation modes

**Low Confidence**:
- Generalization of findings to other model architectures beyond the two tested
- Impact of dataset size thresholds for the Superficial Alignment Hypothesis
- Specific mechanisms driving language-specific performance variations

## Next Checks

1. **Translation Quality Validation**: Conduct human evaluation of translation quality across all instruction-response pairs in MT-Bench-X to quantify semantic preservation and identify potential sources of cross-lingual performance variation.

2. **Dataset Size Threshold Analysis**: Systematically vary instruction-tuning dataset sizes (e.g., 100, 500, 1000, 2000 examples) for both mid-sized and large models to precisely identify when the Superficial Alignment Hypothesis begins to hold or break down.

3. **Cross-Architecture Replication**: Replicate key experiments with different multilingual model architectures (e.g., BLOOM, XGLM) to test the generalizability of findings about the Superficial Alignment Hypothesis and parallel dataset benefits across diverse model families.