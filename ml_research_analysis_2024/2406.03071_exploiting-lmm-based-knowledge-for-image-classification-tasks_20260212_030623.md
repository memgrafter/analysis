---
ver: rpa2
title: Exploiting LMM-based knowledge for image classification tasks
arxiv_id: '2406.03071'
source_url: https://arxiv.org/abs/2406.03071
tags:
- image
- clip
- classification
- text
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to improve image classification performance
  by incorporating knowledge from Large Multimodal Models (LMMs). The approach leverages
  MiniGPT-4 to generate semantic descriptions for each image, which are then encoded
  using CLIP's text encoder.
---

# Exploiting LMM-based knowledge for image classification tasks

## Quick Facts
- arXiv ID: 2406.03071
- Source URL: https://arxiv.org/abs/2406.03071
- Authors: Maria Tzelepi; Vasileios Mezaris
- Reference count: 27
- Primary result: Proposed method achieves 91.753%, 85.909%, and 95.566% test accuracy on UCF-101, ERA, and BAR datasets respectively

## Executive Summary
This paper presents a method to improve image classification performance by incorporating knowledge from Large Multimodal Models (LMMs). The approach leverages MiniGPT-4 to generate semantic descriptions for each image, which are then encoded using CLIP's text encoder. These text embeddings are combined with CLIP's image embeddings and used as input to a linear classifier. Experiments on three datasets demonstrate that the proposed method significantly improves classification accuracy compared to using CLIP image embeddings alone.

## Method Summary
The proposed method uses MiniGPT-4 to generate 10 semantic descriptions for each image in the dataset. CLIP's text encoder converts these descriptions into 768-dimensional embeddings, which are averaged to produce a single text embedding per image. These text embeddings are concatenated with CLIP's 768-dimensional image embeddings to form a 1536-dimensional vector. A linear classifier is then trained on top of these concatenated embeddings using cross-entropy loss. The method is evaluated on three datasets (UCF-101, ERA, and BAR) and compared against CLIP image-only baselines.

## Key Results
- Test accuracy of 91.753% on UCF-101 dataset (vs 89.981% baseline)
- Test accuracy of 85.909% on ERA dataset (vs 79.825% baseline)
- Test accuracy of 95.566% on BAR dataset (vs 90.622% baseline)
- Averaging 10 MiniGPT-4 descriptions outperforms using image embeddings alone
- Concatenation of image and text embeddings performs better than using either modality alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP image embeddings alone provide a strong baseline for image classification but miss richer semantic context.
- Mechanism: The proposed method adds a second feature stream—text embeddings derived from MiniGPT-4's semantic descriptions—to capture complementary, high-level conceptual information that the image encoder alone might not fully represent.
- Core assumption: The multimodal knowledge encoded in MiniGPT-4 can be effectively projected into CLIP's text embedding space, and that this projection preserves discriminative semantics for classification.
- Evidence anchors:
  - [abstract] "the proposed approach achieves test accuracies of 91.753%, 85.909%, and 95.566% on UCF-101, ERA, and BAR datasets, respectively, outperforming the baseline CLIP model."
  - [section 3.3] "Instead, we use MiniGPT-4 for obtaining semantic descriptions... and then we extract the text embeddings using the CLIP text encoder g."
- Break condition: If MiniGPT-4's descriptions are vague, repetitive, or misaligned with visual content, the text embeddings will add noise rather than signal, degrading performance.

### Mechanism 2
- Claim: Averaging the 10 CLIP text embeddings from MiniGPT-4's descriptions yields a stable, representative text embedding for each image.
- Mechanism: MiniGPT-4 produces multiple descriptions per image; averaging their CLIP text embeddings smooths out idiosyncratic wording and reduces the impact of any single noisy description.
- Core assumption: The set of 10 descriptions per image is diverse enough that averaging yields a meaningful consensus embedding, not just a blur of unrelated tokens.
- Evidence anchors:
  - [section 3.3] "the average pooling operation is applied and a unique text embedding tM i ∈ ℜ d is obtained for each image Xi."
  - [section 4.4] Ablation shows the mean embedding approach also improves over image-only baseline, though concatenation performs slightly better.
- Break condition: If MiniGPT-4 repeatedly produces near-identical descriptions for an image, averaging offers no benefit and concatenation of identical embeddings is redundant.

### Mechanism 3
- Claim: Concatenating image and text embeddings preserves complementary information while keeping the model simple (single linear classifier).
- Mechanism: The concatenated vector [image_embedding, text_embedding] is fed to a linear layer that learns to weight each modality according to its discriminative power for each class.
- Core assumption: The two embedding spaces (image and text) are semantically aligned enough that a linear combination can learn useful decision boundaries without deep multimodal fusion.
- Evidence anchors:
  - [section 3.3] "we generally use concatenation, that is [y⊺ i , tM ⊺ i ]⊺ ∈ Rd+d."
  - [section 4.4] "using the proposed methodology we can accomplish significant improvement over the approach of using only the image embeddings."
- Break condition: If the modalities are poorly aligned (e.g., text embeddings encode irrelevant or misleading concepts), the concatenated vector may confuse the classifier rather than help it.

## Foundational Learning

- Concept: Multimodal embedding spaces (CLIP)
  - Why needed here: CLIP's pretrained image and text encoders provide aligned representations that enable the fusion of visual and semantic information without requiring large-scale paired training data.
  - Quick check question: What is the dimensionality of CLIP's embeddings used in the experiments, and why is that important for concatenation?

- Concept: Prompt engineering for vision-language models
  - Why needed here: MiniGPT-4 is prompted with a specific instruction ("Give 10 semantic descriptions of the image") to elicit structured, high-level textual descriptions that can be mapped into CLIP's text space.
  - Quick check question: How would changing the prompt wording affect the diversity and usefulness of the generated descriptions?

- Concept: Linear probe classification
  - Why needed here: The final classifier is a single linear layer trained on top of frozen CLIP embeddings, which is a standard, efficient way to evaluate representation quality without overfitting.
  - Quick check question: What would happen to performance if the linear classifier were replaced with a deeper network?

## Architecture Onboarding

- Component map: Image → CLIP image encoder → embedding → concatenation → linear layer → loss. MiniGPT-4 runs offline to generate descriptions before training begins.

- Critical path: Image → CLIP image encoder → embedding → concatenation → linear layer → loss. MiniGPT-4 runs offline to generate descriptions before training begins.

- Design tradeoffs:
  - Offline description generation avoids runtime latency but requires storage of 10 descriptions per image.
  - Concatenation is simpler than multimodal attention but may not fully exploit cross-modal interactions.
  - Using CLIP's frozen encoders preserves generalization but limits fine-tuning of representation space.

- Failure signatures:
  - If text embeddings dominate the concatenated vector, the model may ignore visual cues; check class activation maps.
  - If descriptions are repetitive or irrelevant, accuracy gain over image-only baseline will be marginal or negative.
  - If the linear layer overfits, validation accuracy will plateau or drop while training accuracy rises.

- First 3 experiments:
  1. Train only with CLIP image embeddings (baseline) to confirm the 89.981% UCF-101 score.
  2. Train only with averaged CLIP text embeddings (from MiniGPT-4) to measure standalone text performance (~80.333% on UCF-101).
  3. Train with concatenated embeddings but reduce the number of descriptions per image (e.g., 5 instead of 10) to see if fewer descriptions suffice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform on datasets with a larger number of classes compared to UCF-101, ERA, and BAR?
- Basis in paper: [inferred] The paper evaluates the method on three datasets with varying numbers of classes (101, 25, and 6 respectively), but does not explore performance on datasets with a significantly larger number of classes.
- Why unresolved: The paper does not provide any analysis or discussion on the scalability of the proposed method to datasets with a larger number of classes.
- What evidence would resolve it: Conducting experiments on datasets with a larger number of classes, such as ImageNet or COCO, and comparing the performance of the proposed method to other state-of-the-art methods.

### Open Question 2
- Question: How does the quality and diversity of the semantic descriptions generated by MiniGPT-4 affect the classification performance?
- Basis in paper: [explicit] The paper mentions that accurate and meaningful descriptions are generally obtained from MiniGPT-4, but does not explore the impact of description quality and diversity on classification performance.
- Why unresolved: The paper does not provide any analysis on the relationship between description quality/diversity and classification performance, nor does it explore techniques to improve description generation.
- What evidence would resolve it: Conducting experiments to evaluate the impact of description quality and diversity on classification performance, and exploring techniques to improve description generation, such as using different prompts or fine-tuning MiniGPT-4 on specific domains.

### Open Question 3
- Question: How does the proposed method compare to other approaches that combine image and text embeddings for image classification?
- Basis in paper: [inferred] The paper compares the proposed method to other CLIP-based approaches, but does not explore other methods that combine image and text embeddings for image classification, such as dual-stream networks or transformer-based models.
- Why unresolved: The paper does not provide any comparison to other methods that combine image and text embeddings, nor does it discuss the potential advantages or disadvantages of the proposed approach compared to these methods.
- What evidence would resolve it: Conducting experiments to compare the proposed method to other approaches that combine image and text embeddings, such as dual-stream networks or transformer-based models, and analyzing the results to identify the strengths and weaknesses of each approach.

## Limitations
- The method relies on MiniGPT-4's ability to generate meaningful and diverse semantic descriptions, which may not always be optimal
- No ablation studies are provided for the number of descriptions per image (fixed at 10)
- The evaluation is limited to three relatively small datasets, with no cross-dataset generalization tests
- No qualitative analysis of description quality or diversity is provided

## Confidence

- High confidence: The improvement over CLIP image-only baseline is well-documented across all three datasets with specific accuracy numbers.
- Medium confidence: The mechanism by which MiniGPT-4's descriptions improve classification is plausible but not rigorously proven - alternative explanations (e.g., increased dimensionality, regularization effects) are not ruled out.
- Medium confidence: The averaging of 10 descriptions is shown to work better than image-only baseline, but whether 10 is optimal or whether other aggregation methods might work better is not established.

## Next Checks

1. Conduct ablation studies varying the number of MiniGPT-4 descriptions per image (e.g., 1, 5, 10, 20) to determine if 10 is optimal or if fewer descriptions suffice.

2. Replace the linear classifier with a small MLP (1-2 hidden layers) to test whether simple concatenation limits the model's ability to learn cross-modal interactions.

3. Perform qualitative analysis of MiniGPT-4's generated descriptions by comparing them with ground truth labels and measuring description diversity using metrics like BLEU or BERTScore.