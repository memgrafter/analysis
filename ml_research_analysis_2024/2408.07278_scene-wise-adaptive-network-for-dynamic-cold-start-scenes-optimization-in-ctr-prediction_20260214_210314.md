---
ver: rpa2
title: Scene-wise Adaptive Network for Dynamic Cold-start Scenes Optimization in CTR
  Prediction
arxiv_id: '2408.07278'
source_url: https://arxiv.org/abs/2408.07278
tags:
- scenes
- uni00000013
- scene
- uni00000011
- uni0000001c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the cold-start problem in multi-scene recommendation
  systems for dynamic online environments, where new scenes frequently emerge without
  historical data. The authors propose Scene-wise Adaptive Network (SwAN), which leverages
  scene similarity learning, user-specific scene transition cognition, and adaptive
  model structure allocation to optimize recommendations for new scenes.
---

# Scene-wise Adaptive Network for Dynamic Cold-start Scenes Optimization in CTR Prediction

## Quick Facts
- arXiv ID: 2408.07278
- Source URL: https://arxiv.org/abs/2408.07278
- Reference count: 34
- Online CTR improvement: 5.64% over baselines

## Executive Summary
This paper addresses the cold-start problem in multi-scene recommendation systems where new scenes emerge frequently without historical data. The authors propose Scene-wise Adaptive Network (SwAN), which leverages scene similarity learning, user-specific scene transition cognition, and adaptive model structure allocation to optimize recommendations for new scenes. SwAN was deployed in Meituan's online catering recommendation service and achieved significant improvements in both click-through rate and order volume.

## Method Summary
SwAN introduces three key capabilities: scene similarity learning through a Scene Relation Graph (SRG) that captures similarities between scenes based on attributes and user interactions, user-specific scene transition cognition through a Similarity Attention Network (SAN) that incorporates user attention on scene similarities, and adaptive model structure allocation through an Adaptive Ensemble-experts Module (AEM) that dynamically allocates model structures. The method uses a combination of Cross-Entropy, Cosine Loss, and variance loss functions for training, with evaluation on both public (Taobao) and industrial (Meituan) datasets.

## Key Results
- Achieved 5.64% improvement in CTR and 5.19% increase in daily order volume proportion in online A/B testing
- Demonstrated superior performance over existing MSR approaches in offline AUC evaluation
- Successfully deployed in Meituan's online catering recommendation service for cold-start scene optimization

## Why This Works (Mechanism)

### Mechanism 1
Scene Relation Graph (SRG) transfers prior information from similar scenes to cold-start scenes using feature similarity. SRG constructs a graph based on scene attribute features and user interaction features, computing similarity weights between the target scene and existing scenes. These weights are used to retrieve and aggregate prior information from similar scenes.

### Mechanism 2
Similarity Attention Network (SAN) incorporates user cognition to learn latent similarity between scenes beyond explicit attribute similarity. SAN uses user features, target scene features, and similar scene features to compute attention weights that capture user-specific perceptions of scene similarity. These weights refine the prior information aggregation.

### Mechanism 3
Adaptive Ensemble-experts Module (AEM) dynamically allocates model structures to cold-start scenes based on similar scene information. AEM consists of Adaptive Expert Group (AEG) and Shared Expert Group (SEG). AEG uses Expert Selector with differentiable conditional selection (DicS) to choose experts based on similar scene representations.

## Foundational Learning

- **Scene similarity learning**: Why needed here: Cold-start scenes lack historical data, so leveraging similar scenes' information is essential for performance. Quick check question: How does SRG compute similarity weights between scenes?
- **User attention mechanisms**: Why needed here: Different users perceive scene similarity differently, and incorporating this cognition improves recommendation quality. Quick check question: What features does SAN use to compute attention weights?
- **Mixture-of-Experts (MoE) architectures**: Why needed here: Dynamic allocation of model structures to scenes based on similarity and shared logic improves performance for both cold-start and existing scenes. Quick check question: How does AEM differ from traditional MoE architectures?

## Architecture Onboarding

- **Component map**: Input features → SRG → similarity weights → SAN → refined attention weights → CFR → cross-scene feature representation → AEM (AEG + SEG) → expert outputs → Decision Layer → final prediction
- **Critical path**: 1) Input features → SRG → similarity weights, 2) Input features + user features → SAN → refined attention weights, 3) Attention weights + scene features → CFR → cross-scene feature representation, 4) Cross-scene representation → AEM → expert outputs, 5) Expert outputs → Decision Layer → final prediction
- **Design tradeoffs**: Static vs dynamic model structure allocation (dynamic improves cold-start but adds complexity), number of similar scenes (more scenes provide more information but increase computation), expert diversity (more diverse experts improve scene-specific learning but may reduce shared knowledge)
- **Failure signatures**: Poor performance on cold-start scenes (check SRG similarity computation and SAN attention weights), degradation on existing scenes (check AEM expert selection and gating mechanisms), training instability (check temperature coefficient τ and loss function weights)
- **First 3 experiments**: 1) Test SRG similarity computation: Compute similarity weights for known scene pairs and verify they match expectations, 2) Test SAN attention refinement: Compare attention weights with and without user features to verify user cognition incorporation, 3) Test AEM expert selection: Visualize expert selection patterns for different scenes to verify dynamic allocation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SwAN's performance compare to traditional multi-scene recommendation models in cold-start scenarios with limited historical data?
- Basis in paper: [explicit] The paper states that SwAN outperforms existing MSR approaches by seamlessly adapting to new scenes and providing more accurate and high-quality recommendations, achieving up to 5.64% online CTR improvement relative to the baselines and up to 5.19% increase in daily order volume proportion.
- Why unresolved: The paper does not provide a direct comparison of SwAN's performance with traditional models in cold-start scenarios with limited historical data.
- What evidence would resolve it: A controlled experiment comparing SwAN's performance with traditional models in cold-start scenarios with varying amounts of historical data.

### Open Question 2
- Question: What is the impact of the Scene Relation Graph (SRG) on the model's ability to capture scene similarities and improve recommendations for new scenes?
- Basis in paper: [explicit] The paper introduces the SRG as a crucial component of SwAN, stating that it captures graph-structured similarities between scenes based on inherent attributes and user interaction features.
- Why unresolved: The paper does not provide a detailed analysis of the SRG's impact on the model's performance, such as ablation studies or visualizations of the learned scene similarities.
- What evidence would resolve it: Ablation studies comparing SwAN's performance with and without the SRG, as well as visualizations of the learned scene similarities and their correlation with user preferences.

### Open Question 3
- Question: How does the Adaptive Expert Group (AEG) in the Adaptive Ensemble-experts Module (AEM) dynamically allocate model structures for different scenes, and what is the impact of this allocation on the model's performance?
- Basis in paper: [explicit] The paper describes the AEG as a component that improves the model's ability to extract scene-specific information by dynamically allocating model structures based on scene similarity information.
- Why unresolved: The paper does not provide a detailed explanation of how the AEG dynamically allocates model structures or a comprehensive analysis of its impact on the model's performance.
- What evidence would resolve it: A detailed explanation of the AEG's dynamic allocation mechanism, along with ablation studies and visualizations of the allocated model structures for different scenes and their impact on the model's performance.

## Limitations

- Cold-start evaluation relies on artificial scene isolation in offline experiments, which may not fully capture true cold-start complexity
- Similarity-based approach assumes scene attributes and user interaction patterns are reliable indicators of functional similarity
- Dynamic model allocation mechanism adds computational overhead that isn't fully quantified in terms of latency or resource costs

## Confidence

- **High confidence**: Overall framework design and individual components (SRG, SAN, AEM) are technically sound with concrete online A/B testing results showing 5.64% CTR improvement
- **Medium confidence**: Offline AUC improvements are promising but artificial cold-start evaluation methodology introduces uncertainty about real-world generalizability
- **Low confidence**: Lack of detailed analysis of computational overhead, model complexity trade-offs, and sensitivity analysis for key hyperparameters

## Next Checks

1. **Similarity Reliability Test**: Evaluate SRG similarity weights on held-out scene pairs with known relationships to verify that the graph captures meaningful scene similarities beyond superficial attribute correlations.

2. **User Attention Ablation**: Conduct controlled experiments comparing SAN performance with and without user features across different user segments to quantify the contribution of user cognition to recommendation quality.

3. **Cold-start Robustness Analysis**: Test SwAN's performance under varying degrees of cold-start severity (0-10% historical data) to establish the method's effectiveness range and identify failure thresholds.