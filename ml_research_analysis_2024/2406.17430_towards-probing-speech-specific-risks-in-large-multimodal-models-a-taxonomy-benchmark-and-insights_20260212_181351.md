---
ver: rpa2
title: 'Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy,
  Benchmark, and Insights'
arxiv_id: '2406.17430'
source_url: https://arxiv.org/abs/2406.17430
tags:
- speaker
- speech
- risk
- person
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a speech-specific risk taxonomy that addresses
  the detection of paralinguistic cues in speech, which can transform low-risk textual
  content into high-risk speech. The taxonomy covers 8 risk categories under hostility
  (malicious sarcasm and threats), malicious imitation (age, gender, ethnicity), and
  stereotypical biases (age, gender, ethnicity).
---

# Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights

## Quick Facts
- arXiv ID: 2406.17430
- Source URL: https://arxiv.org/abs/2406.17430
- Authors: Hao Yang; Lizhen Qu; Ehsan Shareghi; Gholamreza Haffari
- Reference count: 12
- Key outcome: Large multimodal models perform near random baseline on detecting speech-specific risks involving paralinguistic cues like tone and emotion

## Executive Summary
This paper addresses a critical gap in large multimodal model evaluation by focusing on speech-specific risks that arise from paralinguistic cues. The authors introduce a comprehensive taxonomy covering 8 risk categories under hostility, malicious imitation, and stereotypical biases. They create a synthetic speech dataset using advanced text-to-speech systems and evaluate 5 recent speech-supported LMMs, revealing that even state-of-the-art models struggle significantly with detecting risks conveyed through tone, emotion, and speaker identity. The work highlights the importance of developing specialized benchmarks and evaluation methods for speech risk detection.

## Method Summary
The authors develop a speech-specific risk taxonomy with 8 categories and create a synthetic dataset using GPT-4 to expand seed text samples and Audiobox/Google TTS to generate speech with specified paralinguistic cues. They evaluate 5 recent LMMs (Qwen-Audio-Chat, SALMONN-7/13B, WavLLM, Gemini-1.5-Pro) using zero-shot prompting strategies including Yes/No, Multi-choice, Chain-of-thought, and Pre-task approaches. The evaluation pipeline calculates accuracy and macro-averaged F1 scores across risk sub-categories, comparing results against random baseline performance.

## Key Results
- All 5 evaluated LMMs perform near random baseline (50% accuracy) on speech risk detection tasks
- Pre-task prompting strategy consistently improves performance across models and risk categories
- SALMONN models show severe misalignment on Yes/No prompts but recover with Multi-choice prompting
- Even advanced models like Gemini 1.5 Pro struggle with detecting paralinguistic risks in speech

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Paralinguistic cues in speech can fundamentally alter the perceived risk level of otherwise neutral text.
- Mechanism: The taxonomy captures risks where tone, emotion, or speaker identity (gender, age, ethnicity) transforms low-risk transcripts into high-risk speech by conveying implicit hostility, bias, or mockery that is absent from the text alone.
- Core assumption: LMMs must integrate both textual content and paralinguistic audio features to accurately detect speech-specific risks; focusing only on transcription misses these risks.
- Evidence anchors:
  - [abstract] "However, in speech-based interactions, paralinguistic cues in audio can significantly alter the intended meaning behind utterances."
  - [section 3] "To delineate the risks associated with paralinguistic cues, we establish 3 primary categories of risk speech... we emphasise the significance of paralinguistic cues, including tone, emotion, and speaker information."
  - [corpus] Weak evidence: related works focus on red-teaming or benchmarking but not specifically on paralinguistic risk detection; no direct evidence found.
- Break condition: If LMMs can reliably extract paralinguistic features but fail to integrate them with text reasoning, the taxonomy will not improve detection.

### Mechanism 2
- Claim: Advanced TTS systems can generate realistic speech with controllable paralinguistic cues for risk evaluation.
- Mechanism: The data collection pipeline uses GPT-4 to expand seed text samples and Audiobox/Google TTS to synthesize speech with specified emotional tone, gender, age, or accent, creating a scalable dataset that isolates the effect of paralinguistic cues.
- Core assumption: TTS systems can produce speech that reliably conveys the intended paralinguistic cues without introducing confounding artefacts.
- Evidence anchors:
  - [section 4.2] "To convert these transcripts into audio, we used advanced text-to-speech (TTS) systems, Audiobox... and Google TTS... to generate various synthetic speeches with paralinguistic cues."
  - [section 4.1] "Manually creating samples is a time-consuming and costly process... we leverage the human-curated samples as seed templates, and prompt GPT-4 to generate more samples."
  - [corpus] Weak evidence: no direct mention of TTS quality or controllability in related works; assumption based on system descriptions.
- Break condition: If TTS-generated speech fails to convey paralinguistic cues accurately, model evaluations will not reflect true detection capabilities.

### Mechanism 3
- Claim: Evaluation prompts that explicitly request paralinguistic cue recognition improve LMM detection performance.
- Mechanism: The Pre-task + MC prompting strategy first asks the model to identify paralinguistic features (sentiment, speaker attributes) before judging risk, guiding the model to attend to multimodal cues rather than text alone.
- Core assumption: LMMs can be steered to focus on specific multimodal aspects through carefully designed prompts, overcoming their default tendency to rely on transcription.
- Evidence anchors:
  - [section 5.2] "We observe that... the adoption of Pre-task activates most of models to achieve a better result on various sub-categories."
  - [section 5.2] "It suggests the implicit signal from paralinguistic cues help models integrating multimodal cues."
  - [corpus] Weak evidence: related works on red-teaming or robustness do not address prompting for paralinguistic cue detection.
- Break condition: If LMMs consistently ignore paralinguistic cues even with Pre-task prompts, the prompting strategy will not improve detection.

## Foundational Learning

- Concept: Multimodal representation alignment between audio and text modalities.
  - Why needed here: LMMs must fuse speech embeddings with language representations to detect risks conveyed by tone, emotion, or speaker identity.
  - Quick check question: How do Whisper and other audio encoders map speech to embeddings that can be aligned with LLM token embeddings?

- Concept: Paralinguistic feature extraction (emotion, tone, speaker attributes).
  - Why needed here: Risk categories depend on recognizing non-lexical cues such as angry tone indicating sarcasm or accent indicating ethnicity-based bias.
  - Quick check question: What paralinguistic features can be reliably extracted from speech, and how are they encoded for LMM input?

- Concept: Prompt engineering for multimodal reasoning.
  - Why needed here: LMMs require explicit guidance to integrate multimodal cues; naive text-based prompts fail to trigger proper speech understanding.
  - Quick check question: How does the Pre-task prompt structure influence LMM attention to paralinguistic cues versus textual content?

## Architecture Onboarding

- Component map: Audio encoder (e.g., Whisper, BEATs) → Fusion layer with LLM → Risk classifier head; TTS generation pipeline (GPT-4 → TTS → speech); Evaluation prompt processor
- Critical path: Speech input → Audio encoder → Multimodal fusion → Prompt processing → Risk detection output
- Design tradeoffs: End-to-end audio-LLM training vs. frozen encoders with adapters; TTS quality vs. scalability; prompt complexity vs. model performance
- Failure signatures: Random or near-random accuracy on risk tasks; model refusal or misalignment on Y/N prompts; inconsistent performance across prompting strategies
- First 3 experiments:
  1. Evaluate LMM on speech risk tasks using only Y/N prompts to establish baseline
  2. Test Pre-task + MC prompts to assess improvement from explicit paralinguistic cue requests
  3. Compare LMM performance on paralinguistic subtasks (sentiment, speaker counting) to diagnose failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LMMs on detecting paralinguistic risks vary across different languages and cultural contexts?
- Basis in paper: [inferred] The paper evaluates LMMs on a dataset primarily focused on English and American accents, but does not explore performance across diverse linguistic and cultural backgrounds.
- Why unresolved: The study is limited to a specific language and accent, which may not generalize to other languages or cultural contexts where paralinguistic cues differ significantly.
- What evidence would resolve it: Conducting experiments with datasets in multiple languages and accents, and evaluating LMM performance across these diverse contexts, would provide insights into the generalizability of the findings.

### Open Question 2
- Question: What are the long-term implications of LMMs' inability to detect paralinguistic risks on user trust and safety in real-world applications?
- Basis in paper: [inferred] The paper highlights that even advanced LMMs perform near random baseline in detecting paralinguistic risks, but does not discuss the broader impact on user trust and safety.
- Why unresolved: The study focuses on technical performance without addressing the potential consequences of LMM failures in real-world scenarios where trust and safety are critical.
- What evidence would resolve it: Longitudinal studies assessing user trust and safety in applications using LMMs, coupled with qualitative feedback from users, would help understand the broader implications of LMM limitations.

### Open Question 3
- Question: How can the proposed speech-specific risk taxonomy be adapted or extended to include emerging risks associated with new paralinguistic cues introduced by evolving speech technologies?
- Basis in paper: [explicit] The paper introduces a taxonomy covering current paralinguistic risks but acknowledges the need for future extensions as speech technologies evolve.
- Why unresolved: The taxonomy is static and does not account for new paralinguistic cues that may emerge as speech technologies advance, such as those introduced by synthetic voices or AI-generated speech.
- What evidence would resolve it: Continuous monitoring of emerging speech technologies and their associated paralinguistic cues, followed by iterative updates to the taxonomy, would ensure its relevance and comprehensiveness.

## Limitations
- The synthetic dataset may not capture the full complexity and variability of real-world speech risks
- Reliance on TTS systems introduces uncertainty about whether generated speech accurately conveys paralinguistic cues
- Evaluation focuses on zero-shot prompting strategies, potentially missing performance gains from fine-tuning

## Confidence

- **High confidence**: The core finding that current LMMs perform near random baseline on speech-specific risk detection tasks is well-supported by experimental results across multiple models and prompting strategies.
- **Medium confidence**: The effectiveness of the Pre-task prompting strategy for improving detection performance is supported by results, but the mechanism (explicitly requesting paralinguistic cue recognition) needs further validation across more diverse models and tasks.
- **Low confidence**: The assumption that TTS-generated speech reliably conveys intended paralinguistic cues is not empirically validated within the paper, relying instead on the capabilities of commercial TTS systems.

## Next Checks

1. **Real-world speech validation**: Evaluate the taxonomy and benchmark using naturally occurring speech data from social media, podcasts, or call-center recordings to verify that synthetic data captures genuine speech risk patterns.
2. **Cross-linguistic and cultural validation**: Test the taxonomy across multiple languages and cultural contexts to assess whether paralinguistic cues and associated risks are universal or culturally specific.
3. **Fine-tuning impact study**: Compare zero-shot LMM performance against models fine-tuned on the synthetic speech dataset to determine whether current detection failures stem from architectural limitations or insufficient training data.