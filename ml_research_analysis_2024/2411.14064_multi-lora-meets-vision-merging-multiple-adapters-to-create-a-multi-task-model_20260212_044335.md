---
ver: rpa2
title: 'Multi LoRA Meets Vision: Merging multiple adapters to create a multi task
  model'
arxiv_id: '2411.14064'
source_url: https://arxiv.org/abs/2411.14064
tags:
- adapters
- performance
- lora
- task
- merged
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores merging multiple LoRA adapters trained on different
  computer vision tasks into a single multi-task model. It trains LoRA adapters on
  six tasks across five datasets, then merges them using concatenation.
---

# Multi LoRA Meets Vision: Merging multiple adapters to create a multi task model

## Quick Facts
- arXiv ID: 2411.14064
- Source URL: https://arxiv.org/abs/2411.14064
- Reference count: 32
- Merging LoRA adapters generally leads to performance loss compared to individual adapters

## Executive Summary
This paper investigates merging multiple LoRA adapters trained on different computer vision tasks into a single multi-task model. The authors train LoRA adapters on six tasks across five datasets, then merge them using concatenation. Results show that merged models lose performance compared to individual task-specific adapters, though some combinations outperform a frozen-backbone head-finetuning baseline. The study finds that adapters trained on dissimilar data preserve performance better when merged, and merging up to three adapters is feasible with slight performance drops but still beats the baseline in most cases.

## Method Summary
The authors train individual LoRA adapters on six computer vision tasks across five datasets using LLaVA-NeXT-7B. They then merge these adapters using concatenation, testing various combinations of 2-3 adapters. Performance is evaluated against a frozen-backbone head-finetuning baseline. The merging process involves concatenating the weight matrices of individual LoRA adapters along the rank dimension. Task similarity is analyzed based on dataset characteristics and performance degradation patterns observed after merging.

## Key Results
- Merged adapters generally underperform individual task-specific adapters
- Some merged combinations outperform frozen-backbone head-finetuning baseline
- Adapters trained on dissimilar datasets show better performance preservation when merged
- Merging up to three adapters is feasible with acceptable performance degradation

## Why This Works (Mechanism)
LoRA adapters introduce low-rank modifications to pre-trained model weights, allowing efficient task-specific fine-tuning while maintaining the original model's capabilities. When merged through concatenation, these modifications combine additively, but interference between task-specific features can cause performance degradation. The effectiveness of merging depends on task similarity - dissimilar tasks create orthogonal modifications that interfere less, while similar tasks may overwrite or conflict with each other's learned representations.

## Foundational Learning

**Low-Rank Adaptation (LoRA)**: Efficient fine-tuning method using low-rank matrices to approximate weight updates. Why needed: Enables task-specific adaptation without full fine-tuning. Quick check: Verify adapter ranks and dimensions match requirements.

**Adapter Merging**: Combining multiple LoRA adapters into a single model. Why needed: Creates multi-task models from task-specific adapters. Quick check: Confirm concatenation preserves weight structure and dimensions.

**Task Interference**: Performance degradation when combining task-specific modifications. Why needed: Explains why merged adapters underperform individual ones. Quick check: Measure performance drop between individual and merged adapters.

## Architecture Onboarding

Component map: Input -> Backbone -> LoRA Adapters -> Output
Critical path: Input → Backbone → LoRA Adapters → Task-specific head → Output
Design tradeoffs: Efficiency vs. performance degradation when merging adapters
Failure signatures: Performance drops indicate task interference or incompatible modifications
First experiments: 1) Test individual adapter performance, 2) Merge two adapters from similar datasets, 3) Merge two adapters from dissimilar datasets

## Open Questions the Paper Calls Out
The paper identifies several key open questions including the extent of performance degradation when merging more than three adapters, the optimal merging strategy beyond simple concatenation, and the impact of merge order on final performance. It also questions how to systematically quantify dataset similarity for predicting merge success.

## Limitations
- Limited exploration of merging more than three adapters
- Single merging strategy (concatenation) without comparison to alternatives
- No systematic analysis of merge order effects on performance
- Limited scope to LLaVA-NeXT-7B architecture only

## Confidence

**High confidence**: Merged adapters generally underperform individual task-specific adapters
**Medium confidence**: Dissimilar dataset training improves merge stability
**Medium confidence**: Baseline comparison results showing some merged models outperform head-finetuning

## Next Checks

1. Test merging 4-6 adapters systematically to establish performance scaling limits
2. Implement and compare alternative merging strategies (weighted averaging, attention-based fusion)
3. Conduct ablation studies varying dataset similarity thresholds and merge order permutations