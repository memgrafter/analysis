---
ver: rpa2
title: 'Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging'
arxiv_id: '2406.16330'
source_url: https://arxiv.org/abs/2406.16330
tags:
- merging
- layer
- compression
- layers
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MKA, a novel compression method for large language
  models (LLMs) that leverages manifold learning and information bottleneck measures
  to merge similar layers while preserving model performance. The method first uses
  diffusion kernel algorithms to learn low-dimensional manifold representations of
  layer activations, capturing the nonlinear structure in the activations.
---

# Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging

## Quick Facts
- arXiv ID: 2406.16330
- Source URL: https://arxiv.org/abs/2406.16330
- Reference count: 40
- This paper proposes MKA, a novel compression method for large language models (LLMs) that leverages manifold learning and information bottleneck measures to merge similar layers while preserving model performance.

## Executive Summary
This paper introduces MKA (Manifold Kernel Alignment), a novel method for compressing large language models by merging similar layers based on manifold learning and information bottleneck principles. The approach uses diffusion kernel algorithms to learn low-dimensional manifold representations of layer activations, capturing the nonlinear structure in the activations. It then employs the Information Bottleneck (IB) measure to construct a similarity matrix, identifying the most similar layer pairs for merging. Extensive experiments demonstrate that MKA achieves significant compression ratios (up to 43.75% on Llama3-8B with only 2.82% performance drop) while outperforming traditional pruning methods and maintaining competitive performance when combined with quantization techniques.

## Method Summary
MKA operates through a multi-stage pipeline: first extracting layer activations from a trained LLM, then applying diffusion kernel algorithms to learn low-dimensional manifold representations that capture the intrinsic geometry of these activations. The Information Bottleneck framework is used to compute normalized mutual information between diffusion map embeddings, creating a similarity matrix that identifies redundant layers. The method iteratively merges the most similar layer pairs using weighted parameter combination, where weights are determined by the normalized mutual information. This process continues until no layer pairs exceed the similarity threshold, resulting in a compressed model with fewer layers but preserved functionality.

## Key Results
- MKA achieves up to 43.75% compression on Llama3-8B with only 2.82% performance drop on MMLU benchmark
- Outperforms traditional pruning methods across multiple LLMs (Llama2, Llama3, Llama3.2, Mistral) and tasks (MMLU, PIQA, HellaSwag, RACE-H, BoolQ)
- When combined with quantization techniques, MKA achieves even higher compression ratios with minimal accuracy loss
- Demonstrates consistent effectiveness across different model architectures, including MoE and hybrid transformer-Mamba models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion kernel-based manifold learning extracts the intrinsic low-dimensional structure of layer activations, enabling more meaningful layer similarity comparisons than raw activation similarity.
- Mechanism: The diffusion kernel algorithm transforms high-dimensional activation vectors into low-dimensional diffusion map embeddings that preserve the manifold's intrinsic geometry. This allows mutual information calculations between layers to capture true functional redundancy rather than superficial activation similarity.
- Core assumption: Layer activations lie on a low-dimensional manifold embedded in the high-dimensional activation space, and this manifold structure captures the essential representational patterns.
- Evidence anchors:
  - [abstract] "diffusion kernel algorithms to learn low-dimensional manifold representations of layer activations, capturing the nonlinear structure in the activations"
  - [section] "The diffusion map is designed to preserve the intrinsic geometry of the activation manifold by capturing multi-scale connectivity"
- Break condition: If layer activations are not manifold-structured (e.g., completely random or noise-dominated), the diffusion kernel will fail to extract meaningful lower-dimensional representations, making the similarity comparisons unreliable.

### Mechanism 2
- Claim: Information Bottleneck (IB) measure quantifies the similarity between layers by balancing mutual information maximization with entropy minimization, identifying layers that carry redundant information.
- Mechanism: IB measures the normalized pairwise mutual information between diffusion map embeddings of different layers. Layers with high normalized mutual information carry similar information content and can be merged without significant performance loss.
- Core assumption: The IB framework appropriately captures functional similarity between layers when applied to diffusion map embeddings of layer activations.
- Evidence anchors:
  - [abstract] "employs the Information Bottleneck (IB) measure to construct a similarity matrix, identifying the most similar layer pairs for merging"
  - [section] "the normalized mutual information (NMI) as Slm = I(Ψl; Ψm)pH(Ψl)H(Ψm)"
- Break condition: If the IB measure fails to capture the true functional relationship between layers (e.g., if layers have complementary rather than redundant information), merging based on this similarity will degrade performance.

### Mechanism 3
- Claim: Iterative layer merging with adaptive weight assignment preserves more information than one-shot merging by allowing the merged layer to integrate knowledge progressively across the network.
- Mechanism: The method iteratively merges the most similar layer pairs, where each merge creates a new fused layer that can then be merged with other layers. The merging weights are determined by the normalized mutual information, giving more weight to the layer with higher shared information.
- Core assumption: The order of layer merging matters and that iterative merging with adaptive weights preserves more essential information than static, one-shot merging approaches.
- Evidence anchors:
  - [abstract] "the number of models in the merging process can be gradually and naturally reduced"
  - [section] "we conduct experiments on the Llama3-8B model using the MMLU dataset demonstrate that MKA's iterative approach yields superior performance"
- Break condition: If layer dependencies are highly non-linear or if merging order creates information bottlenecks, iterative merging could propagate errors or lose critical information that one-shot approaches might preserve.

## Foundational Learning

- Concept: Manifold learning and diffusion geometry
  - Why needed here: The core innovation relies on extracting low-dimensional representations of high-dimensional activations to compare layer similarity meaningfully
  - Quick check question: What is the difference between PCA and diffusion maps in terms of the geometric structure they preserve?

- Concept: Information Bottleneck principle
  - Why needed here: IB provides the theoretical framework for quantifying when two layers carry redundant information suitable for merging
  - Quick check question: How does the IB objective balance compression against preserving relevant information?

- Concept: Layer-wise activation analysis in transformers
  - Why needed here: Understanding how transformer layers transform activations is crucial for interpreting why certain layers can be merged
  - Quick check question: What is the typical pattern of activation similarity between consecutive layers in a well-trained transformer?

## Architecture Onboarding

- Component map:
  Activation extraction -> Manifold learning pipeline -> Similarity computation -> Merging engine -> Evaluation

- Critical path: Activation extraction → Manifold learning → Similarity computation → Merging decision → Parameter update → Evaluation

- Design tradeoffs:
  - Manifold learning granularity vs. computational cost (more samples → better manifold approximation but slower)
  - Similarity threshold selection (too low → excessive merging, too high → insufficient compression)
  - Merging weight strategy (adaptive vs. fixed) affecting information preservation
  - Iterative vs. one-shot merging affecting final performance

- Failure signatures:
  - Performance collapse after merging indicates poor similarity measurement or inappropriate merging order
  - No compression achieved despite high similarity suggests threshold is too conservative
  - Excessive memory usage during activation extraction indicates need for batch processing
  - Slow convergence suggests inefficient similarity computation or inappropriate bandwidth parameter

- First 3 experiments:
  1. Baseline similarity heatmap: Run activation extraction and similarity computation without merging to visualize layer relationships and validate manifold learning works
  2. Controlled merging: Merge the two most similar layers and measure performance drop to establish baseline for acceptable similarity thresholds
  3. Iterative compression test: Apply full MKA pipeline on a small model (e.g., Llama2-7B) to verify the end-to-end workflow and identify bottlenecks

## Open Questions the Paper Calls Out

- Question: How does the performance of MKA compare when applied to non-LLM architectures like CNNs or vision transformers?
  - Basis in paper: [explicit] The paper mentions MKA shows promising potential for broader adoption across various deep learning architectures including CNNs and vision transformers, with initial experiments reinforcing viability.
  - Why unresolved: The paper only provides preliminary results showing MKA can be generalized to other model types but lacks detailed quantitative comparisons across different architectures.
  - What evidence would resolve it: Comprehensive benchmarking of MKA against existing compression methods on multiple non-LLM architectures (CNNs, vision transformers, etc.) with quantitative performance metrics.

- Question: What is the optimal number of iterations for the MKA algorithm to balance compression ratio and performance preservation?
  - Basis in paper: [explicit] The paper discusses that MKA incorporates an iterative process where layers are progressively merged, but doesn't provide a systematic analysis of iteration effects.
  - Why unresolved: The paper mentions iterative nature but doesn't analyze how different iteration strategies affect final performance or compression efficiency.
  - What evidence would resolve it: Systematic experiments varying iteration strategies and quantifying their impact on compression ratios, accuracy retention, and computational efficiency.

- Question: How do different manifold learning techniques (beyond diffusion kernels) affect MKA's performance and compression capabilities?
  - Basis in paper: [explicit] The paper uses diffusion kernel algorithm for manifold learning but mentions other techniques like PCA, t-SNE, and UMAP in related work section.
  - Why unresolved: The paper only evaluates one manifold learning approach without comparing it to alternatives or analyzing how different techniques might affect compression performance.
  - What evidence would resolve it: Comparative analysis of MKA using different manifold learning techniques (PCA, t-SNE, UMAP, etc.) with performance metrics and compression ratios for each approach.

## Limitations
- The method's effectiveness depends on layer activations being manifold-structured, which may not hold for all model types or tasks
- Computational overhead during activation extraction and manifold learning phases could be prohibitive for very large models
- The iterative merging process assumes layer dependencies are relatively local, but this may not hold for deeper models with complex information flow

## Confidence
- High Confidence: The core mechanism of using diffusion maps to extract low-dimensional representations of layer activations is well-established in manifold learning literature
- Medium Confidence: The specific application of the Information Bottleneck framework to diffusion map embeddings for layer similarity is novel and theoretically sound, but empirical validation is limited
- Low Confidence: The method's scalability to extremely large models (>70B parameters) and performance on specialized domains are not demonstrated

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary the diffusion kernel bandwidth (σ), embedding dimension (k), and merging threshold (τ) on a medium-sized model (e.g., Llama2-13B) to quantify their impact on compression ratio and performance degradation

2. **Ablation Study on Manifold Learning**: Compare MKA's performance against a baseline that uses raw activation similarity (e.g., cosine similarity) instead of diffusion map embeddings to validate whether the manifold learning step provides meaningful improvements

3. **Long-Range Dependency Test**: Evaluate MKA on a task that requires capturing long-range dependencies (e.g., logical reasoning or multi-step inference) to test whether the iterative merging process preserves the model's ability to maintain context over extended sequences