---
ver: rpa2
title: Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language
  Models
arxiv_id: '2401.05861'
source_url: https://arxiv.org/abs/2401.05861
tags:
- translation
- performance
- zero-shot
- lora
- finetuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how prompt strategies affect zero-shot
  multilingual translation performance in large language models. It shows that different
  prompt strategies yield comparable supervised translation performance but vary significantly
  in zero-shot directions.
---

# Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models

## Quick Facts
- arXiv ID: 2401.05861
- Source URL: https://arxiv.org/abs/2401.05861
- Reference count: 23
- Key outcome: Cross-lingual consistency regularization (XConST) improves zero-shot multilingual translation by up to 10.8 COMET points on average

## Executive Summary
This paper addresses the challenge of improving many-to-many multilingual translation in large language models, particularly focusing on zero-shot translation performance where the model translates between language pairs not seen during training. The authors demonstrate that different prompt strategies significantly affect zero-shot translation while having minimal impact on supervised directions, and introduce XConST - a regularization method that aligns semantic-equivalent sentence representations across languages via KL divergence. XConST consistently improves zero-shot translation performance across different prompt strategies and model sizes, with up to 10.8 COMET points improvement on average for 13B models.

## Method Summary
The method involves instruction finetuning pretrained LLM models (ALMA-7B/13B, TowerBase-7B, LLaMA-2-7B/13B) on English-centric parallel datasets using five different prompt strategies (T-ENC, T-DEC, S-ENC-T-ENC, S-ENC-T-DEC, GPT-MT). The key innovation is the cross-lingual consistency regularization (XConST), which adds a KL divergence term to the standard causal language modeling loss. This regularization forces the model to produce consistent output distributions for semantically equivalent sentences across languages. The training uses LoRA adapters or full-weight finetuning with AdamW optimizer, and evaluation is performed using SacreBLEU and COMET scores on WMT and FLORES-200 datasets.

## Key Results
- XConST improves zero-shot translation performance by 3.46 COMET points on average for 7B models and 10.8 points for 13B models
- Zero-shot translation performance varies significantly across different prompt strategies (up to 20.5 COMET points difference)
- XConST consistently boosts performance across all prompt strategies, with T-ENC and S-ENC-T-DEC strategies benefiting most
- The method demonstrates strong generalization beyond English-centric scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt strategies significantly affect zero-shot translation performance while having minimal impact on supervised directions.
- Mechanism: The model's internal language representation alignment varies based on how source and target languages are indicated in the prompt, influencing its ability to generalize to unseen language pairs.
- Core assumption: The language tag format affects semantic representation consistency across languages in the model's latent space.
- Evidence anchors:
  - [abstract] "We demonstrate that prompt strategies adopted during finetuning are crucial to zero-shot translation..."
  - [section 4.1.1] "The zero-shot translation performance varies significantly across different prompt strategies..."
  - [corpus] Weak evidence - the corpus doesn't directly address prompt strategy effects.
- Break condition: If the model has perfect cross-lingual alignment or the prompt strategy doesn't affect the attention patterns over language tokens.

### Mechanism 2
- Claim: Cross-lingual consistency regularization improves zero-shot translation by aligning semantic-equivalent sentence representations across languages.
- Mechanism: By applying KL divergence between the model's output distributions for a sentence pair and its source-only counterpart, the model learns to produce consistent representations for semantically equivalent sentences across languages.
- Core assumption: Forcing consistent output distributions for semantic-equivalent sentences will improve the model's cross-lingual alignment and zero-shot generalization.
- Evidence anchors:
  - [abstract] "We introduce a cross-lingual consistency regularization, XConST, to bridge the representation gap among different languages..."
  - [section 4.2.1] "XConST consistently boosts the zero-shot translation performance across different prompt strategies..."
  - [corpus] Weak evidence - the corpus doesn't discuss consistency regularization approaches.
- Break condition: If the KL regularization is too strong (α too high) causing degradation in supervised performance or if the model's cross-attention mechanism doesn't benefit from this alignment.

### Mechanism 3
- Claim: The off-target issue in zero-shot translation is related to the model's failure to follow translation instructions properly.
- Mechanism: When the model generates translations in the wrong language, it indicates that the instruction-following capability needs improvement, which can be addressed through better prompt design or regularization.
- Core assumption: Poor zero-shot performance is partially due to the model not adhering to the translation instruction format rather than purely representational issues.
- Evidence anchors:
  - [section 4.1.3] "We can see that the bad translation performance in the zero-shot directions is highly related to the off-target issue..."
  - [abstract] "...instruction finetuning on multilingual large language models (LLMs) with high-quality translation pairs."
  - [corpus] Weak evidence - the corpus doesn't specifically address off-target issues in translation.
- Break condition: If the off-target issue is resolved through other means (like better pre-training) or if the problem is purely representational rather than instruction-following.

## Foundational Learning

- Concept: Cross-entropy loss and KL divergence
  - Why needed here: The training objective combines standard causal language modeling (cross-entropy) with cross-lingual consistency regularization (KL divergence) to balance translation quality and representation alignment.
  - Quick check question: What happens to the training objective when α=0 in the XConST formula?

- Concept: Prompt engineering for multilingual translation
  - Why needed here: Different prompt strategies (T-ENC, T-DEC, S-ENC-T-ENC, S-ENC-T-DEC, GPT-MT) have varying effects on zero-shot performance, requiring understanding of how language tags influence model behavior.
  - Quick check question: How does the T-ENC strategy differ from S-ENC-T-DEC in terms of language tag placement?

- Concept: Representation alignment in multilingual models
  - Why needed here: The core hypothesis is that improving cross-lingual representation consistency will enhance zero-shot translation, requiring understanding of how models represent different languages in their latent space.
  - Quick check question: What visualization technique was used to show language representation alignment before and after XConST?

## Architecture Onboarding

- Component map: LLaMA-2 decoder-only transformer with LoRA adapters → prompt template processor → cross-lingual consistency regularizer → loss computation → parameter updates
- Critical path: Prompt → Model forward pass → KL regularization computation → Combined loss → Backward pass → Parameter update
- Design tradeoffs: Full-weight finetuning vs LoRA (parameter efficiency vs performance), α regularization strength (supervised vs zero-shot performance), prompt strategy selection (simplicity vs effectiveness)
- Failure signatures: Off-target translations indicate instruction-following issues, poor zero-shot performance suggests representational misalignment, supervised performance degradation indicates over-regularization
- First 3 experiments:
  1. Compare vanilla finetuning with XConST using different α values on a small language pair to find optimal regularization strength
  2. Test all five prompt strategies with and without XConST to identify which strategies benefit most from regularization
  3. Evaluate the effect of XConST on representation alignment using t-SNE visualization on a multi-way parallel corpus

## Open Questions the Paper Calls Out

- **Cross-linguistic generalizability**: The paper notes that future research could consider more diverse language families beyond the Indo-European and East Asian languages tested.
- **Optimal data composition**: The paper mentions using monolingual datasets for continual pretraining followed by instruction finetuning, but doesn't systematically explore the optimal ratio of monolingual to parallel data.
- **Scaling behavior**: While the paper demonstrates effectiveness on 7B and 13B models, it doesn't explore how XConST performs on larger models beyond 13B parameters.

## Limitations

- **Language coverage**: Experiments limited to five languages (Czech, German, English, Russian, Chinese) with only 8 supervised and 12 zero-shot directions, leaving generalizability to truly low-resource or typologically diverse languages uncertain.
- **Evaluation focus**: Primarily relies on automatic metrics (COMET, BLEU) with limited comprehensive human evaluation, especially for zero-shot directions.
- **Prompt strategy sensitivity**: Zero-shot performance varies significantly across prompt strategies (up to 20.5 COMET points difference), suggesting results may be sensitive to implementation details of prompt templates.

## Confidence

- **High confidence**: The effectiveness of XConST in improving zero-shot translation performance across different prompt strategies and model sizes, supported by consistent improvements and ablation studies.
- **Medium confidence**: The mechanism by which prompt strategies affect zero-shot performance, as the theoretical explanation for why certain strategies work better than others is somewhat speculative.
- **Medium confidence**: The claim that off-target issues indicate instruction-following failures rather than purely representational problems, as the correlation between prompt strategy and off-target rates is shown but causation requires more controlled experiments.

## Next Checks

1. **Cross-linguistic generalizability test**: Evaluate XConST on a broader set of language pairs including truly low-resource languages (e.g., from FLORES-101) and typologically diverse languages (e.g., Japanese, Swahili, Quechua).

2. **Ablation of XConST components**: Systematically test the contribution of each component in XConST by ablating the KL regularization term, the source-target pairing mechanism, or the temperature scaling.

3. **Attention visualization analysis**: Generate and analyze attention weight visualizations across different prompt strategies (both with and without XConST) to understand how language tokens influence the model's cross-attention patterns.