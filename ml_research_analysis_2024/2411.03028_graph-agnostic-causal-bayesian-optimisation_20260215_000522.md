---
ver: rpa2
title: Graph Agnostic Causal Bayesian Optimisation
arxiv_id: '2411.03028'
source_url: https://arxiv.org/abs/2411.03028
tags:
- causal
- graph
- bayesian
- function
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses causal Bayesian optimisation (CBO) when the
  causal graph is unknown, focusing on both hard and soft interventions to maximize
  cumulative reward. The authors propose Graph Agnostic Causal Bayesian Optimisation
  (GACBO), which balances exploiting known high-reward actions and exploring causal
  structure and functions.
---

# Graph Agnostic Causal Bayesian Optimisation

## Quick Facts
- arXiv ID: 2411.03028
- Source URL: https://arxiv.org/abs/2411.03028
- Authors: Sumantrak Mukherjee; Mengyan Zhang; Seth Flaxman; Sebastian Josef Vollmer
- Reference count: 40
- One-line primary result: GACBO achieves competitive performance compared to baselines, reaching similar performance to MCBO with the true graph within 100 rounds across synthetic and real-world experiments.

## Executive Summary
This paper addresses causal Bayesian optimisation (CBO) when the causal graph is unknown, focusing on both hard and soft interventions to maximize cumulative reward. The authors propose Graph Agnostic Causal Bayesian Optimisation (GACBO), which balances exploiting known high-reward actions and exploring causal structure and functions. The method models surrogate functions using Gaussian processes for all plausible graphs and selects interventions based on an Upper Confidence Bound (UCB) acquisition function that incorporates uncertainty from both function space and graph structure. GACBO achieves competitive performance compared to baselines, reaching similar performance to MCBO with the true graph within 100 rounds across synthetic and real-world experiments.

## Method Summary
GACBO addresses causal Bayesian optimisation with unknown causal graphs by maintaining plausible models (graphs and functions) within confidence intervals and using an Upper Confidence Bound (UCB) acquisition function. The algorithm models surrogate functions using Gaussian processes for all plausible graphs, quantifies uncertainty in both function space and graph structure, and selects interventions that maximize the upper confidence bound of the target variable. The method naturally unifies causal discovery and Bayesian optimisation by making causal discovery a subtask only relevant when it can lead to better rewards, and it handles both soft and hard interventions through appropriate function modeling approaches.

## Key Results
- GACBO reaches similar performance to MCBO with the true graph within 100 rounds across synthetic environments
- The method achieves competitive performance on real-world Epidemiology application (HIV viral load minimization)
- GACBO naturally handles both soft and hard interventions while unifying causal discovery with Bayesian optimisation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GACBO achieves competitive performance by balancing exploitation of known high-reward actions and exploration of causal structure and functions.
- Mechanism: The algorithm maintains plausible models (graphs and functions) within confidence intervals and uses an Upper Confidence Bound (UCB) acquisition function that incorporates uncertainty from both function space and graph structure.
- Core assumption: The true data-generating mechanism lies within the prior space of plausible models, and the confidence intervals capture the true SCM with high probability.
- Evidence anchors:
  - [abstract]: "GACBO seeks to balance exploiting the actions that give the best rewards against exploring the causal structures and functions."
  - [section 3.3]: "Our method quantifies all the sources of uncertainty by following a model-based approach. We construct a confidence interval and consider graphs and functions within the confidence interval."
- Break condition: If the true graph lies outside the confidence intervals, or if the confidence intervals become too wide (high uncertainty), the algorithm may waste samples exploring incorrect structures.

### Mechanism 2
- Claim: Causal discovery is made a subtask only relevant when it can lead to better rewards, naturally unifying causal discovery and Bayesian optimisation.
- Mechanism: The acquisition function selects interventions that maximize the upper confidence bound of the target variable across plausible graphs, only exploring causal structure when it can distinguish between graphs and yield better results.
- Core assumption: The optimal intervention can be found among the plausible models, and causal structure learning is only beneficial when it leads to higher expected rewards.
- Evidence anchors:
  - [abstract]: "We introduce an Upper Confidence Bound based acquisition function that makes causal discovery a subtask only relevant when distinguishing between graphs yields better results."
  - [section 3.3]: "We design our acquisition function to incorporate uncertainty stemming from the absence of a priori knowledge regarding the underlying graph structure, by selecting actions giving the highest possible values according to the plausible graphs and functions."
- Break condition: If the reward function is insensitive to causal structure (e.g., linear functions), or if the number of plausible graphs is too large, the algorithm may not benefit from causal discovery.

### Mechanism 3
- Claim: GACBO naturally handles both soft and hard interventions by modeling surrogate functions using Gaussian processes for all plausible graphs and selecting interventions based on the highest reward among all plausible graphs.
- Mechanism: For soft interventions, the algorithm treats actions as additional inputs to the Gaussian process, while for hard interventions, it uses the notion of Minimal Intervention Sets (MIS) to simplify the optimization problem.
- Core assumption: The functions relating nodes to their parents and actions belong to a Reproducible Kernel Hilbert Space (RKHS) with bounded norm, ensuring smoothness and compactness of the input space.
- Evidence anchors:
  - [section 2.1]: "For a particular set of indices I the intervention space is denoted using AI = {Ai}i∈I and therefore the total space of interventions is given byAI = {AI }I∈P(I)"
  - [section 3.3]: "The acquisition for hard intervention... is based on the notion of Minimal Intervention Sets (MIS) [28]."
- Break condition: If the regularity assumptions are violated (e.g., non-smooth functions, unbounded noise), the Gaussian process modeling may be inappropriate, leading to poor performance.

## Foundational Learning

- Concept: Causal Bayesian Optimisation (CBO)
  - Why needed here: CBO leverages known causal relationships through targeted interventions to optimize an outcome variable, improving sample efficiency compared to standard Bayesian optimization.
  - Quick check question: What is the main difference between CBO and standard Bayesian optimization, and how does causal knowledge improve optimization efficiency?

- Concept: Gaussian Processes (GPs)
  - Why needed here: GPs are used to model surrogate functions for all plausible graphs, quantifying uncertainty in both function space and graph structure.
  - Quick check question: How do GPs incorporate prior beliefs and quantify uncertainty in the surrogate modeling process?

- Concept: Upper Confidence Bound (UCB) Acquisition Function
  - Why needed here: The UCB acquisition function balances exploration and exploitation by selecting interventions that maximize the upper confidence bound of the target variable across plausible graphs.
  - Quick check question: How does the UCB acquisition function incorporate uncertainty from both function space and graph structure in the decision-making process?

## Architecture Onboarding

- Component map: Surrogate models (Gaussian processes for all plausible graphs) -> Plausible models (graphs and functions within confidence intervals) -> Acquisition function (UCB based on reparametrization trick) -> Causal subgraph discovery (recursive identification of parent variables)

- Critical path:
  1. Construct confidence bounds for plausible functions
  2. Update plausible graphs based on new data
  3. Select interventions using UCB acquisition function
  4. Observe node values and update dataset
  5. Update GP posteriors for all plausible functions

- Design tradeoffs:
  - Exploration vs. exploitation: Balancing learning causal structure and exploiting known high-reward actions
  - Computational complexity: Enumerating all possible graphs vs. using MCMC sampling or differential approaches for scalability
  - Regularity assumptions: Bounded RKHS norm and compact input space vs. relaxing assumptions for more general functions

- Failure signatures:
  - Poor performance when true graph lies outside confidence intervals
  - Suboptimal results when causal structure is not beneficial (e.g., linear reward functions)
  - Scalability issues with large graphs due to superexponential growth of DAGs

- First 3 experiments:
  1. Synthetic environment (Dropwave, Alpine3, Rosenbrock, or ToyGraph): Compare GACBO performance with MCBO (true graph) and GP-UCB (no causal structure)
  2. Real-world application (Epidemiology): Evaluate GACBO on HIV viral load minimization problem with unknown causal graph
  3. Ablation study: Remove causal subgraph discovery and observe impact on performance to validate the importance of learning causal structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GACBO compare to MCBO when the causal graph contains cycles or feedback loops?
- Basis in paper: [inferred] The paper focuses on DAGs and does not address cyclic causal structures, which are common in real-world systems.
- Why unresolved: The theoretical framework and algorithm are built on DAG assumptions, making extension to cyclic graphs non-trivial.
- What evidence would resolve it: Experimental results comparing GACBO and MCBO on benchmark problems with known cyclic causal structures.

### Open Question 2
- Question: What is the theoretical regret bound for GACBO when the causal graph is unknown, and how does it compare to the known-graph case?
- Basis in paper: [explicit] The authors acknowledge the lack of theoretical guarantees for finite-sample convergence to the true graph and defer regret analysis to future work.
- Why unresolved: Proving regret bounds requires handling the additional uncertainty from graph structure learning, which complicates the analysis.
- What evidence would resolve it: A formal proof establishing regret bounds for GACBO under various assumptions about graph structure and function smoothness.

### Open Question 3
- Question: How sensitive is GACBO to the choice of kernel and its hyperparameters when modeling the functions relating nodes?
- Basis in paper: [inferred] The paper uses Gaussian processes with generic kernels but does not explore the impact of kernel choice on performance.
- Why unresolved: Kernel selection can significantly affect function approximation and uncertainty quantification in GP-based methods.
- What evidence would resolve it: Systematic experiments varying kernel types and hyperparameters across different problem domains and graph structures.

## Limitations

- Computational complexity grows superexponentially with number of nodes due to full enumeration of all possible DAGs
- Performance heavily depends on quality of confidence intervals - may fail if true data-generating mechanism lies outside bounds
- Regularity assumptions (bounded RKHS norm, compact input space) may not hold for all real-world applications

## Confidence

- **High confidence**: Theoretical framework for soft interventions and Gaussian process surrogate modeling
- **Medium confidence**: Performance claims on synthetic benchmarks, though real-world application lacks implementation details
- **Medium confidence**: Unification of causal discovery and Bayesian optimization through UCB acquisition function

## Next Checks

1. **Scalability validation**: Implement a variant using MCMC sampling instead of full enumeration for graphs with >10 nodes to assess practical scalability beyond toy examples
2. **Robustness testing**: Systematically vary the RKHS norm bounds and input space constraints to determine the method's sensitivity to these regularity assumptions
3. **Baseline comparison**: Conduct head-to-head comparisons with pure causal discovery methods that ignore reward optimization to quantify the benefits of the unified approach