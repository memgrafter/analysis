---
ver: rpa2
title: Toward a More Complete OMR Solution
arxiv_id: '2409.00316'
source_url: https://arxiv.org/abs/2409.00316
tags:
- notation
- music
- assembly
- detection
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-stage approach to optical music recognition
  (OMR) that jointly considers object detection and notation assembly, rather than
  treating them separately. The authors propose a YOLOv8-based detector trained on
  MUSCIMA++ v2.0 with preprocessing adaptations, and a supervised training pipeline
  that learns notation assembly directly from detection outputs using a maximum-weight
  matching strategy.
---

# Toward a More Complete OMR Solution

## Quick Facts
- arXiv ID: 2409.00316
- Source URL: https://arxiv.org/abs/2409.00316
- Reference count: 0
- Primary result: Jointly trained OMR pipeline achieves 84.79% mAP detection and 3.13% Match+AUC improvement over baseline

## Executive Summary
This paper presents a multi-stage approach to optical music recognition (OMR) that integrates object detection and notation assembly rather than treating them as separate problems. The authors develop a YOLOv8-based detector trained on MUSCIMA++ v2.0 with preprocessing adaptations, and introduce a supervised training pipeline that learns notation assembly directly from detection outputs using maximum-weight matching. They propose a novel evaluation metric, Match+AUC, that accounts for both detection and assembly errors through bipartite matching and precision-recall area under curve. Results show the detector achieves 84.79% mAP on all 163 classes, outperforming prior work. The joint training pipeline improves Match+AUC by 3.13% (essential classes) and 1.79% (all classes), demonstrating the benefit of considering both stages together.

## Method Summary
The method employs a two-stage OMR pipeline with YOLOv8-based object detection as the first stage, trained on MUSCIMA++ v2.0 dataset with random cropping augmentation. The second stage, notation assembly, uses an MLP trained on detection outputs rather than ground truth. A maximum-weight matching algorithm creates training pairs by aligning detected objects with ground truth, accounting for detection errors. The evaluation uses a novel Match+AUC metric that first matches detected objects to ground truth using bipartite matching, then evaluates assembly performance using area under precision-recall curve. The pipeline is trained for 500 epochs with AdamW optimizer, using 1216×1216 crops resized to 640×640.

## Key Results
- YOLOv8 detector achieves 84.79% mAP on all 163 classes, outperforming previous approaches
- Joint training pipeline improves Match+AUC by 3.13% for essential classes and 1.79% for all classes
- The pipeline successfully handles realistic detection errors through maximum-weight matching training
- Match+AUC evaluation metric provides more comprehensive assessment than traditional metrics

## Why This Works (Mechanism)

### Mechanism 1
Training notation assembly on imperfect detector outputs improves system performance compared to training on perfect detection. The notation assembly model learns to handle real-world detection errors during training, making it more robust when deployed in a full pipeline. This avoids the distribution shift between training and deployment.

### Mechanism 2
The maximum-weight matching approach creates accurate training data for notation assembly from imperfect detections. By finding optimal matches between detected objects and ground truth, the system creates pseudo-ground-truth edges that account for detection errors, allowing supervised training of the assembly model.

### Mechanism 3
The Match+AUC evaluation metric accurately measures end-to-end pipeline performance by accounting for both detection and assembly errors. The metric first matches detected objects to ground truth using maximum-weight matching, then evaluates assembly performance using area under precision-recall curve, providing a comprehensive view of system performance.

## Foundational Learning

- **Object detection fundamentals**: Understanding bounding boxes, IoU, and mAP is essential since the paper builds a YOLOv8-based detector as the first stage. Quick check: What is the difference between mAP and weighted mAP, and why might weighted mAP be more appropriate for this dataset?

- **Graph theory and matching algorithms**: The paper uses maximum-weight matching in bipartite graphs to align detected objects with ground truth. Quick check: How does the Hungarian algorithm (maximum-weight matching) work, and why is it suitable for this object alignment problem?

- **Precision-recall curves and AUC metrics**: The novel evaluation metric uses area under the precision-recall curve to assess notation assembly performance. Quick check: What does the area under the precision-recall curve represent, and how does it differ from F1-score in evaluating model performance?

## Architecture Onboarding

- **Component map**: Image → YOLOv8 Detector → Maximum-weight Matching → Notation Assembly MLP → Match+AUC Evaluation
- **Critical path**: The detector must be reliable enough to provide usable inputs for assembly; the matching must accurately align detections with ground truth; the assembly must handle imperfect inputs effectively
- **Design tradeoffs**: YOLOv8 vs. other detectors (YOLOv8 offers good performance but may need adaptation for high-resolution music scores); matching threshold (0.05) vs. assembly accuracy (lower thresholds may include more matches but increase noise); number of classes (163 vs. essential 73) affects model complexity
- **Failure signatures**: Low mAP in detection stage → Assembly receives poor inputs; high variance in Match+AUC scores → Inconsistent matching or assembly performance; large gap between training and test performance → Overfitting to specific detection patterns
- **First 3 experiments**: Train detector on cropped images and evaluate mAP on test set; implement maximum-weight matching and verify alignment accuracy; compare baseline assembly (trained on perfect detection) vs. pipelined assembly (trained on detector output) using Match+AUC

## Open Questions the Paper Calls Out

### Open Question 1
Does the Match+AUC metric generalize to other OMR datasets beyond MUSCIMA++ v2.0, and how does its performance compare to traditional metrics in those contexts? The paper introduces Match+AUC as a novel evaluation metric but does not explore its applicability to other OMR datasets or benchmark it against traditional metrics in different contexts.

### Open Question 2
Can the joint training approach for detection and assembly stages be extended to end-to-end OMR systems that bypass explicit object detection? The paper focuses on a multi-stage pipeline and suggests future research could explore applying Match+AUC within a joint training objective for both stages, but does not address end-to-end systems.

### Open Question 3
How does the soft label approach in the notation assembly stage affect the model's robustness to class imbalance in the MUSCIMA++ dataset? The paper introduces a "soft" variant of pipelined training that uses soft class labels from the detection model, but does not analyze its impact on handling class imbalance.

### Open Question 4
What is the computational overhead of the Match+AUC evaluation metric compared to traditional metrics, and does it scale efficiently with larger datasets? The paper introduces Match+AUC as a novel metric but does not discuss its computational complexity or scalability.

## Limitations
- The approach relies heavily on the MUSCIMA++ v2.0 dataset, which may limit generalizability to other notation styles or engraving traditions
- The maximum-weight matching threshold of 0.05 appears arbitrary and may not be optimal across different datasets or detection qualities
- While the Match+AUC metric provides comprehensive evaluation, it may be computationally expensive for large-scale applications

## Confidence
- **High confidence**: YOLOv8 detector performance improvements (84.79% mAP) are well-supported by quantitative results
- **Medium confidence**: Joint training benefits (3.13% Match+AUC improvement) are demonstrated but may be dataset-specific
- **Medium confidence**: Maximum-weight matching approach for training data creation is theoretically sound but lacks extensive validation across different scenarios

## Next Checks
1. **Cross-dataset validation**: Test the complete pipeline on non-MUSCIMA++ datasets (e.g., DeepScores, Printed Music) to verify generalization beyond the training domain
2. **Ablation on matching threshold**: Systematically evaluate different matching thresholds (0.01-0.1) to determine optimal values for various detection quality levels
3. **Runtime performance analysis**: Measure end-to-end inference time and memory requirements to assess practical deployment feasibility on standard hardware