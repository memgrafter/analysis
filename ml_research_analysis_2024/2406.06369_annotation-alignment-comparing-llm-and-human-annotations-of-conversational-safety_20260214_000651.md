---
ver: rpa2
title: 'Annotation alignment: Comparing LLM and human annotations of conversational
  safety'
arxiv_id: '2406.06369'
source_url: https://arxiv.org/abs/2406.06369
tags:
- annotators
- gpt-4
- chatbot
- safety
- unsafe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates how well leading large language models (GPT-3.5,\
  \ GPT-4, GPT-4o, Gemini 1.5 Pro, Llama 3.1 405B) align with human perceptions of\
  \ safety in user-chatbot conversations. Using the DICES dataset with 350 conversations\
  \ rated by 112 annotators across 10 race-gender groups, the authors measure annotation\
  \ alignment\u2014how closely LLM safety ratings match human ratings."
---

# Annotation alignment: Comparing LLM and human annotations of conversational safety

## Quick Facts
- arXiv ID: 2406.06369
- Source URL: https://arxiv.org/abs/2406.06369
- Reference count: 19
- Key outcome: GPT-4 with chain-of-thought prompt achieves highest alignment with human safety ratings (r = 0.61) among tested LLMs

## Executive Summary
This paper evaluates how well leading large language models align with human perceptions of safety in user-chatbot conversations. Using the DICES dataset with 350 conversations rated by 112 annotators across 10 race-gender groups, the authors measure annotation alignment—how closely LLM safety ratings match human ratings. GPT-4 with a chain-of-thought prompt achieves the highest alignment, correlating at r = 0.61 with the average human rating, outperforming most individual annotators. The dataset lacks power to detect demographic differences in alignment, and GPT-4 cannot reliably predict when demographic groups disagree about safety. The study reveals both strong overall alignment and systematic disagreements between LLMs and humans, highlighting opportunities to improve safety annotation consistency.

## Method Summary
The study evaluates annotation alignment between five leading LLMs (GPT-3.5, GPT-4, GPT-4o, Gemini 1.5 Pro, Llama 3.1 405B) and human annotators using the DICES-350 dataset. The authors generate LLM safety ratings for 350 conversations using zero-shot prompting with both analyze-rate and rating-only variants. They compute Pearson correlations between LLM ratings and average human ratings, perform bootstrap resampling for statistical significance, and analyze alignment patterns across demographic groups. The study also tests LLMs' ability to predict demographic disagreements by comparing paired ratings for conversations where demographic groups disagree.

## Key Results
- GPT-4 with chain-of-thought prompting achieves highest alignment (r = 0.61) with average human ratings
- GPT-4 alignment varies substantially within demographic groups, with no significant differences between groups
- GPT-4 cannot reliably predict demographic disagreements, with only 61% accuracy in predicting group-level safety judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's chain-of-thought prompt significantly improves alignment with human annotations compared to rating-only prompts.
- Mechanism: The analyze-rate prompt structure forces the model to first process and reason about each safety criterion before providing a final rating, mimicking human deliberation processes.
- Core assumption: The model's reasoning quality improves when explicitly asked to analyze criteria before rating.
- Evidence anchors:
  - [abstract] "GPT-4 with a chain-of-thought prompt achieves the highest alignment, correlating at r = 0.61 with the average human rating"
  - [section] "Chiang and Lee (2023b) find that prompting the model to generate an explanation alongside its rating can increase correlation with human annotators"
  - [corpus] Weak - no direct corpus evidence, only validation experiments mentioned
- Break condition: If the model's reasoning process doesn't improve with analysis, or if the added cognitive load degrades performance.

### Mechanism 2
- Claim: The dataset's high per-item replication rate (112 annotators per conversation) provides reliable human consensus measurements.
- Mechanism: High replication allows averaging to reduce individual annotator noise, creating a stable reference point for comparing LLM alignment.
- Core assumption: Individual annotator ratings are noisy but their average converges to a stable consensus.
- Evidence anchors:
  - [section] "the average of 3 random humans achieves correlation r = 0.72 with the human consensus"
  - [section] "the average over multiple raters exhibits high construct reliability (Carmines, 1979)"
  - [corpus] Moderate - corpus shows related work on annotation reliability but no direct evidence about this dataset's power
- Break condition: If individual annotator variance is too high or if the underlying construct being measured is truly subjective with no stable consensus.

### Mechanism 3
- Claim: LLM alignment varies systematically with annotator demographics, but the current dataset lacks statistical power to detect these differences.
- Mechanism: Demographic groups may have different safety standards and perspectives, and LLMs may align more with some groups' perspectives than others.
- Core assumption: Annotator demographics capture meaningful differences in safety perception that could create alignment disparities.
- Evidence anchors:
  - [section] "we observe that human-LLM alignment varies substantially for individuals within demographic subgroups"
  - [section] "there is substantial idiosyncratic variation in correlation within groups"
  - [corpus] Weak - corpus shows related work on demographic alignment but no direct evidence about this dataset's power
- Break condition: If demographic differences in alignment are small relative to individual variation, or if the dataset lacks sufficient examples per demographic group.

## Foundational Learning

- Concept: Pearson correlation coefficient
  - Why needed here: The paper uses Pearson correlation to quantify alignment between LLM ratings and human ratings
  - Quick check question: If two annotators have ratings that move in opposite directions for all conversations, what would their Pearson correlation be?

- Concept: Statistical power and confidence intervals
  - Why needed here: The paper uses permutation tests and confidence intervals to determine whether observed differences in alignment are statistically significant
  - Quick check question: If a 99% confidence interval for a group's correlation with an LLM spans [0.44, 0.64], can we conclude the correlation is significantly different from 0.5?

- Concept: Chain-of-thought prompting
  - Why needed here: The paper tests whether prompting LLMs to explain their reasoning before rating improves alignment with human annotations
  - Quick check question: What is the key difference between a "rating-only" prompt and an "analyze-rate" prompt?

## Architecture Onboarding

- Component map: Dataset ingestion -> Prompt generation -> LLM API calls -> Correlation computation -> Statistical analysis -> Qualitative analysis
- Critical path: The correlation computation step is most critical as it determines the primary evaluation metric
- Design tradeoffs: Zero-shot prompting vs. few-shot learning (zero-shot used due to better validation performance), Likert vs. binary ratings (Likert chosen for better correlation)
- Failure signatures: Low correlation values indicate alignment problems; inconsistent ratings across model generations suggest inference instability
- First 3 experiments:
  1. Run correlation analysis between GPT-4 analyze-rate ratings and human average ratings
  2. Test demographic alignment by computing correlations between GPT-4 ratings and different demographic groups
  3. Evaluate disagreement prediction by comparing GPT-4's paired ratings with actual group disagreements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design LLM safety annotations to better capture individual annotator variation that is not explained by race or gender demographics?
- Basis in paper: [explicit] The paper finds that alignment with GPT-4 varies substantially within demographic groups, suggesting race and gender do not fully capture differences in alignment.
- Why unresolved: Current demographic groupings (race and gender) show wide confidence intervals and substantial within-group variation, but the paper does not identify what other characteristics or context might better explain individual alignment differences.
- What evidence would resolve it: A larger dataset with richer annotator metadata (e.g., country of origin, prior online harm experience, education level, political views) that shows correlations between these additional factors and alignment with LLM safety ratings.

### Open Question 2
- Question: Can LLM prompts be optimized to reduce systematic disagreements between LLM safety ratings and human annotator ratings?
- Basis in paper: [explicit] The paper identifies systematic patterns of disagreement, such as LLMs being more permissive of sensitive advice while being more stringent about hate speech and stereotypes compared to annotators.
- Why unresolved: While the paper provides qualitative examples of systematic disagreements, it does not test prompt modifications to address these specific disagreement patterns.
- What evidence would resolve it: Experimental results showing improved alignment after prompt modifications that specifically address the identified disagreement patterns (e.g., clarifying stance on sensitive advice, adjusting thresholds for hate speech).

### Open Question 3
- Question: What is the minimum dataset size required to detect meaningful demographic differences in LLM-human alignment?
- Basis in paper: [explicit] The paper concludes the dataset is underpowered to detect demographic differences, with wide confidence intervals (e.g., r ∈ [0.44, 0.64] for the Latinx female group).
- Why unresolved: The current DICES-350 dataset (350 conversations, 112 annotators) cannot resolve whether demographic alignment differences exist or not, but the paper does not quantify how much larger a dataset would be needed.
- What evidence would resolve it: Statistical power analysis showing the sample size needed to achieve narrow enough confidence intervals to detect specific effect sizes in demographic alignment differences.

## Limitations
- Dataset lacks statistical power to detect demographic differences in alignment
- Relies exclusively on Pearson correlation, potentially missing complex agreement patterns
- Uses specific model versions that may have changed since study, affecting reproducibility

## Confidence

**High Confidence**: The finding that GPT-4 with chain-of-thought prompting achieves the highest overall correlation (r = 0.61) with human ratings is well-supported by the experimental design and consistent results across multiple runs.

**Medium Confidence**: Claims about GPT-4's inability to reliably predict demographic disagreements are supported by the data but limited by the dataset's statistical power.

**Low Confidence**: Assertions about systematic demographic alignment differences are tentative given the acknowledged lack of statistical power.

## Next Checks

1. **Bootstrap Resampling Validation**: Perform bootstrap resampling (1000+ samples) on the correlation estimates to establish confidence intervals for each LLM's alignment with human ratings.

2. **Cross-Dataset Replication**: Apply the same alignment measurement framework to an independent safety annotation dataset with larger demographic group sizes.

3. **Alternative Alignment Metrics**: Compute additional alignment measures such as Cohen's kappa, mean absolute error, and normalized discounted cumulative gain alongside Pearson correlation.