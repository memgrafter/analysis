---
ver: rpa2
title: 'MoRA: LoRA Guided Multi-Modal Disease Diagnosis with Missing Modality'
arxiv_id: '2408.09064'
source_url: https://arxiv.org/abs/2408.09064
tags:
- missing
- mora
- modalities
- multi-modal
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoRA addresses missing modalities in multi-modal disease diagnosis
  by introducing a computationally efficient fine-tuning method based on LoRA. It
  projects inputs to a low intrinsic dimension and uses modality-aware up-projections
  to adapt to missing modalities.
---

# MoRA: LoRA Guided Multi-Modal Disease Diagnosis with Missing Modality

## Quick Facts
- arXiv ID: 2408.09064
- Source URL: https://arxiv.org/abs/2408.09064
- Authors: Zhiyi Shi; Junsik Kim; Wanhua Li; Yicong Li; Hanspeter Pfister
- Reference count: 21
- Primary result: MoRA achieves state-of-the-art performance on multi-modal disease diagnosis with missing modalities, requiring less than 1.6% of trainable parameters

## Executive Summary
MoRA addresses the challenge of missing modalities in multi-modal disease diagnosis by introducing a computationally efficient fine-tuning method based on LoRA. The method projects inputs to a low intrinsic dimension and uses modality-aware up-projections to adapt to missing modalities. By integrating into the first block of a pre-trained multi-modal model (ViLT) and freezing most parameters, MoRA achieves strong performance while requiring minimal computational resources.

## Method Summary
MoRA introduces Modality-aware Low-Rank Adaptation for handling missing modalities in multi-modal disease diagnosis. The method uses low-rank matrix factorization to project inputs to a shared low-dimensional space, then applies separate up-projection matrices for each modality. When a modality is missing, only the corresponding up-projection is omitted. MoRA is inserted into the first transformer block of a pre-trained ViLT model, with all other parameters frozen. This approach requires training less than 1.6% of the total model parameters while achieving state-of-the-art performance on medical imaging datasets.

## Key Results
- Achieves F1-Macro scores of 80.73% on ODIR and 38.19% on CXR datasets
- Requires less than 1.6% of trainable parameters compared to full model training
- Demonstrates superior robustness and training efficiency compared to existing methods, particularly in scenarios with high missing modality rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modality-aware up-projections enable targeted adaptation for each modality
- Mechanism: The method projects all inputs to a shared low intrinsic dimension using a common down-projection matrix A, then applies separate up-projection matrices (Bm1, Bm2) for each modality. When a modality is missing, only the corresponding up-projection is omitted from the adaptation, preserving modality-specific information.
- Core assumption: The intrinsic dimensionality of the modality-specific features is low enough to be captured by the rank r projection
- Evidence anchors:
  - [abstract]: "MoRA projects each input to a low intrinsic dimension but uses different modality-aware up-projections for modality-specific adaptation in cases of missing modalities."
  - [section]: Equations (2)-(4) show how the adaptation depends on which modalities are present in each subset
- Break condition: If the rank r is too small to capture modality-specific information, the adaptation becomes ineffective. If r is too large, computational benefits diminish.

### Mechanism 2
- Claim: Inserting MoRA into the first block provides sufficient adaptation with minimal computational cost
- Mechanism: By placing the adaptation early in the model architecture, MoRA can modify the input features before they propagate through subsequent layers, achieving strong performance while only requiring training of a small number of parameters.
- Core assumption: Early feature modification is sufficient to guide subsequent processing when modalities are missing
- Evidence anchors:
  - [abstract]: "MoRA integrates into the first block of the model, significantly improving performance when a modality is missing."
  - [section]: "We empirically plug MoRA into the first block (which is a transformer block in ViLT) of the pre-trained model."
- Break condition: If the first block's adaptation is insufficient to capture the missing modality effects, performance gains may be limited.

### Mechanism 3
- Claim: Freezing the pre-trained model and only training MoRA parameters drastically reduces computational requirements
- Mechanism: The method keeps all ViLT parameters frozen and only updates the MoRA matrices (A, Bm1, Bm2) and the classifier, reducing trainable parameters to less than 1.6% of the total model.
- Core assumption: The pre-trained model's general knowledge is sufficient and only needs modality-specific adaptation
- Evidence anchors:
  - [abstract]: "It requires minimal computational resources, with less than 1.6% of the trainable parameters needed compared to training the entire model."
  - [section]: "During the fine-tuning, all parameters that need to be trained are only MoRA and the classifier."
- Break condition: If the pre-trained model's frozen weights are insufficient for the target task, performance may degrade compared to full fine-tuning.

## Foundational Learning

- Concept: Low-rank matrix factorization
  - Why needed here: MoRA relies on representing weight updates as low-rank matrices (BA) to reduce parameter count while maintaining expressiveness
  - Quick check question: What is the relationship between the rank r and the dimensions of the original matrix in low-rank adaptation?

- Concept: Multi-modal feature fusion
  - Why needed here: The method must understand how different modalities contribute to the final prediction and how to handle cases where some modalities are missing
  - Quick check question: How does the ViLT model process multi-modal inputs before MoRA modification?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: MoRA is inserted into the first transformer block, so understanding how self-attention works is crucial for grasping the adaptation's impact
  - Quick check question: What is the role of the class token in the ViLT transformer architecture?

## Architecture Onboarding

- Component map: Input embedding layer → MoRA adaptation (first transformer block) → Remaining ViLT transformer blocks → Classifier
- Critical path: 1. Tokenize input images and text (including dummy tokens for missing modalities) 2. Apply input embeddings 3. Pass through MoRA adaptation in first block 4. Process through remaining ViLT layers 5. Extract class token and classify
- Design tradeoffs: Rank r: Higher values increase capacity but reduce computational efficiency; Plugged block location: Earlier blocks provide more influence but may require more careful initialization; Number of modality-specific up-projections: More modalities increase parameter count but improve specificity
- Failure signatures: Poor performance with high missing rates: Likely indicates rank r is too small or modality-specific projections are inadequate; Instability during training: May indicate improper initialization of up-projection matrices; Minimal improvement over baseline: Could suggest MoRA is not effectively modifying the first block's output
- First 3 experiments: 1. Test different rank r values (1, 4, 16) on ODIR dataset with 65% missing rate for both modalities 2. Compare performance when plugging MoRA into different blocks (first block vs. first three blocks) 3. Evaluate performance degradation when varying the missing rate from 0% to 100% for each modality individually

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MoRA perform when integrated into larger pre-trained models beyond ViLT, such as those used for more complex multi-modal tasks?
- Basis in paper: [explicit] The authors mention in their conclusion that future work will extend MoRA to larger pre-trained models and explore its feasibility in disease diagnosis with such models.
- Why unresolved: The current study is limited to the ViLT model, and the performance of MoRA on larger models remains untested.
- What evidence would resolve it: Experimental results comparing MoRA's performance across various large-scale multi-modal pre-trained models, such as CLIP or ALIGN, in different disease diagnosis tasks.

### Open Question 2
- Question: Can MoRA be adapted to handle more than two modalities, such as adding audio or additional imaging types, without significant performance degradation?
- Basis in paper: [inferred] The paper focuses on two modalities (image and text) and does not explore scenarios with more than two modalities, which are common in clinical settings.
- Why unresolved: The current implementation and experiments are limited to two modalities, and the method's scalability to more modalities is not demonstrated.
- What evidence would resolve it: Experiments testing MoRA on datasets with three or more modalities, such as combining images, text, and audio, to assess its adaptability and performance.

### Open Question 3
- Question: What is the impact of different imputation strategies on MoRA's performance when dealing with missing modalities?
- Basis in paper: [explicit] The paper briefly mentions that imputation methods are non-robust in small modality scenarios but does not explore alternative imputation strategies or their effects on MoRA.
- Why unresolved: The study does not compare MoRA with different imputation methods or investigate how imputation affects its performance.
- What evidence would resolve it: Comparative studies using various imputation techniques alongside MoRA to evaluate their combined impact on model performance and robustness.

## Limitations

- The theoretical justification for choosing specific rank values is not provided, making it unclear whether the method would scale to datasets with more than two modalities or different types of missing data patterns.
- Evaluation is restricted to two medical imaging datasets with binary or small-scale multi-class classification tasks, leaving the method's performance on more complex diagnostic scenarios untested.
- The approach depends on the pre-trained ViLT model's ability to handle the target domain after minimal adaptation, which may limit performance on tasks that significantly differ from the pre-training data distribution.

## Confidence

**High confidence** in the computational efficiency claims (less than 1.6% trainable parameters) and the general approach of using low-rank adaptation for missing modality handling.

**Medium confidence** in the performance claims relative to baselines. The F1-Macro scores show improvement, but the absolute performance values suggest the problem remains challenging.

**Low confidence** in the method's scalability and robustness across diverse scenarios. The paper lacks ablation studies on rank selection sensitivity, missing pattern variations, and cross-domain generalization tests.

## Next Checks

1. **Rank sensitivity analysis**: Systematically evaluate MoRA's performance across a wider range of rank values (r = 1, 2, 4, 8, 16, 32) on both datasets to identify optimal values and determine the trade-off between efficiency and accuracy.

2. **Missing pattern robustness**: Test the method with non-uniform missing patterns (e.g., correlated modality missingness, missing at random vs. missing not at random) to assess real-world applicability beyond the simulated missing rates.

3. **Cross-dataset generalization**: Evaluate MoRA's performance when pre-trained on one medical domain (e.g., ODIR) and fine-tuned on another (e.g., CXR), or when applied to a different medical specialty entirely, to test the frozen pre-trained model assumption.