---
ver: rpa2
title: "Segmenting Action-Value Functions Over Time-Scales in SARSA via TD($\u0394\
  $)"
arxiv_id: '2411.14783'
source_url: https://arxiv.org/abs/2411.14783
tags:
- sarsa
- learning
- function
- k-step
- action-value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper extends the TD(\u0394) framework to SARSA, proposing\
  \ SARSA(\u0394) to address the bias-variance trade-off in long-horizon reinforcement\
  \ learning. By decomposing the action-value function into delta estimators across\
  \ multiple discount factors, SARSA(\u0394) enables learning at different time scales,\
  \ improving stability and efficiency."
---

# Segmenting Action-Value Functions Over Time-Scales in SARSA via TD($Δ$)

## Quick Facts
- arXiv ID: 2411.14783
- Source URL: https://arxiv.org/abs/2411.14783
- Reference count: 40
- Primary result: SARSA(Δ) improves long-horizon RL by decomposing Q(s,a) into delta estimators across time-scales, reducing variance while controlling bias

## Executive Summary
This paper extends the TD(Δ) framework from Q-learning to SARSA, proposing SARSA(Δ) to address the bias-variance trade-off in long-horizon reinforcement learning. By decomposing the action-value function into delta estimators across multiple discount factors, SARSA(Δ) enables learning at different time scales, improving stability and efficiency. The method shows theoretical guarantees for variance reduction and demonstrates empirical improvements across tabular MDPs, OpenAI Gym environments, and Atari games.

## Method Summary
SARSA(Δ) decomposes the action-value function Q(s,a) into delta estimators D_m across multiple discount factors η_m. Each D_m estimates the difference between successive discount-level action-values, allowing learning at different time scales. The method uses multi-step returns with km ≈ 1/(1-ηm) to balance bias-variance trade-off for each component. In the phased update setting where all km are identical, variance reduction is preserved without increasing bias. The approach extends to deep RL through PPO-based variants with generalized advantage estimation.

## Key Results
- Outperforms standard SARSA and PPO baselines in terms of higher average rewards across all tested environments
- Demonstrates faster convergence and better stability, particularly in deterministic environments like FrozenLake and CliffWalking
- Shows robustness in stochastic settings like Blackjack while maintaining variance reduction benefits
- Excels in long-horizon tasks by effectively segmenting action-value functions across time scales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing Q(s,a) into delta estimators D_m across discount factors η_m reduces variance by learning short-term and long-term returns separately.
- Mechanism: Each D_m estimates the difference Q_ηm(s,a) - Q_ηm-1(s,a), isolating components that bootstrap over different time scales. This localizes variance to individual components rather than propagating it across all time scales.
- Core assumption: The sum of delta components reconstructs the full action-value function exactly: Q_ηm(s,a) = Σ_{x=0}^m D_x(s,a).
- Evidence anchors:
  - [abstract] "By splitting the action-value function down into components that are linked to specific discount factors, SARSA(Δ) makes learning easier across a range of time scales."
  - [section] "The delta function Dm(sn,an) is defined as the difference between action-value functions associated with successive discount factors from Eq. 9."
  - [corpus] Weak anchor - related work focuses on Q-learning decomposition, not SARSA-specific delta estimators.
- Break condition: If η_m are not ordered (η_0 ≤ η_1 ≤ ... ≤ η_M), the decomposition no longer isolates increasing time scales, breaking variance reduction.

### Mechanism 2
- Claim: Multi-step returns with km ≈ 1/(1-ηm) balance bias-variance trade-off for each time scale.
- Mechanism: Shorter km values reduce variance by limiting bootstrap horizon; km values matched to 1/(1-ηm) ensure each component's effective planning horizon matches its discount factor's decay rate.
- Core assumption: km ≈ 1/(1-ηm) provides near-optimal horizon for TD(λ) convergence in each delta component.
- Evidence anchors:
  - [section] "km ≈ 1/(1-ηm) to establish a suitable balance between bias and variance over all time scales significantly less than T."
  - [section] "Each increase in Dm facilitates the doubling of the effective horizon, hence ensuring logarithmic scaling with the quantity of action-value functions."
  - [corpus] Weak anchor - the cited works (Kearns & Singh) address state-value functions, not action-value delta estimators.
- Break condition: If km is too small for large ηm, bias dominates and long-term returns are severely underestimated.

### Mechanism 3
- Claim: Phased updates with synchronized km across all m reduce cumulative variance without increasing bias.
- Mechanism: When all km are identical, the variance reduction from TD(Δ) is preserved and bias does not increase because bootstrapping uses consistent horizons across time scales.
- Core assumption: In the phased setting, if km = k for all m, then the upper bound on error does not increase compared to single-estimator SARSA.
- Evidence anchors:
  - [section] "It is observed that when all km values are identical, both algorithms yield the same upper bound."
  - [section] "By establishing km for each ηm, where η_1/(1-ηm)_m ≤ 1/e, the approach guarantees that the effective horizon and variance are maintained within limits without requiring extensive parameter optimization."
  - [corpus] No direct anchor - corpus lacks phased update analysis.
- Break condition: If km are staggered (km < km+1), the variance reduction term diminishes and bias increases from mismatched horizons.

## Foundational Learning

- Concept: Temporal difference learning and bootstrapping
  - Why needed here: SARSA(Δ) builds on TD updates for each delta component; understanding bootstrapping is essential to see how each D_m learns from its own time scale.
  - Quick check question: In a one-step TD update Q ← Q + α[r + ηQ' - Q], which term is the bootstrap target and why does it introduce bias?

- Concept: Discount factor and planning horizon
  - Why needed here: Different η_m values correspond to different effective horizons (1/(1-η)), and km values are chosen relative to these horizons.
  - Quick check question: If η = 0.99, what is the approximate effective planning horizon, and how does this justify km ≈ 100?

- Concept: Bias-variance trade-off in RL
  - Why needed here: SARSA(Δ) explicitly trades off bias and variance by decomposing Q(s,a) and choosing km per time scale.
  - Quick check question: Why does using a smaller discount factor reduce variance but increase bias in long-horizon tasks?

## Architecture Onboarding

- Component map:
  - Delta estimators D_0, D_1, ..., D_M, each associated with discount factor η_m
  - Update rule for each D_m: Dm ← Dm + α_m[multi-step return - Dm]
  - Multi-step return aggregates rewards over km steps, adjusted by (η_x^m - η_x^{m-1}) for x < km, and bootstraps from prior D_q or Q_ηm-1
  - Reconstruction: Q_ηm = Σ_{x=0}^m D_x

- Critical path:
  1. Initialize all D_m(s,a) = 0
  2. At each step, for each m:
     - Collect km-step trajectory rewards
     - Compute return Gm using Eq. 18/16
     - Update D_m with TD error (Gm - D_m)
  3. Reconstruct Q_ηm from delta components as needed

- Design tradeoffs:
  - Memory vs. accuracy: O(M × |S| × |A|) storage for all D_m
  - km scheduling: synchronized km (low variance, same bias) vs. staggered km (lower variance for short scales, higher bias)
  - Learning rates: shared α_m = α (Theorem 1 equivalence) vs. per-scale α_m (practical flexibility)

- Failure signatures:
  - Divergence: Check if η_m are ordered; unordered η_m breaks decomposition assumptions
  - High bias: Verify km ≈ 1/(1-ηm); too-small km underestimates long-term returns
  - Instability in stochastic environments: Adaptive η_m scheduling may be needed

- First 3 experiments:
  1. Ring MDP with η_m = {0.0, 0.5, 0.996}, km = {1, 2, 1000}; compare absolute error vs. k-step SARSA TD
  2. FrozenLake-v1 with η_m = {0.99, 0.992, 0.996}, km = {100, 125, 250}; measure convergence speed and average reward
  3. Seaquest Atari with PPO-TD(λ, SARSA(Δ)); compare reward curves against PPO-Standard over 100K episodes

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis focuses on single-state MDPs but experiments use complex environments with function approximation
- Parameter sensitivity to ηm sequence and km values is critical but under-specified for deep RL experiments
- Atari experiments lack comparison against established baselines beyond PPO and lack statistical significance testing

## Confidence
**High Confidence**: The core mechanism of decomposing Q(s,a) into delta components across discount factors is mathematically sound and the tabular MDP results are reproducible.

**Medium Confidence**: The bias-variance trade-off improvements are theoretically justified but the practical impact depends heavily on parameter choices that are not fully specified.

**Low Confidence**: Claims about deep RL performance improvements lack sufficient empirical validation, particularly regarding hyperparameter sensitivity and comparison with modern RL baselines.

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary ηm sequences and km values in FrozenLake to quantify performance degradation when theoretical conditions are violated.

2. **Statistical Validation**: Add confidence intervals and significance testing to all Atari learning curves, comparing against Rainbow DQN and IMPALA in addition to PPO.

3. **Ablation Study**: Remove the delta decomposition (use single η=0.996) while keeping multi-step returns to isolate the benefit of time-scale segmentation vs. longer horizons.