---
ver: rpa2
title: 'TAMS: Translation-Assisted Morphological Segmentation'
arxiv_id: '2403.14840'
source_url: https://arxiv.org/abs/2403.14840
tags:
- tsez
- language
- data
- segmentation
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TAMS, a method for improving morphological
  segmentation by incorporating translation data. TAMS uses a character-level sequence-to-sequence
  model with LSTM and integrates representations from high-resource language models
  trained on translations.
---

# TAMS: Translation-Assisted Morphological Segmentation

## Quick Facts
- arXiv ID: 2403.14840
- Source URL: https://arxiv.org/abs/2403.14840
- Authors: Enora Rice; Ali Marashian; Luke Gessler; Alexis Palmer; Katharina von der Wense
- Reference count: 23
- One-line primary result: TAMS improves morphological segmentation accuracy by 1.99-2.87 percentage points in super-low resource settings

## Executive Summary
This paper introduces TAMS (Translation-Assisted Morphological Segmentation), a character-level sequence-to-sequence model that leverages translation data to improve canonical morphological segmentation, particularly in low-resource settings. The model incorporates BERT embeddings of aligned translation words as additional signal, achieving significant performance gains when training data is extremely limited (100 samples). Notably, TAMS can achieve strong results even without requiring word-level alignments, making it practical for resource-constrained language documentation scenarios. The approach shows particular promise for languages with complex morphology where traditional segmentation methods struggle due to limited training data.

## Method Summary
TAMS uses a pointer-generator LSTM encoder-decoder architecture that incorporates translation information through multiple strategies. The model takes character sequences of surface forms as input and produces canonical segmented forms as output. Translation data is processed through pretrained BERT models to obtain embeddings of aligned translation words, which are then incorporated into the encoder and decoder using CLS-Concat, Init-State, and Concat-Half strategies. The system is trained on IGT data from SIGMORPHON 2023 Shared Task for three languages, using varying training set sizes to evaluate performance in different resource settings.

## Key Results
- TAMS outperforms baseline by 1.99 percentage points on average in the n=100 training setting
- Maximum improvement of 2.87 percentage points achieved even with poor quality automatic alignments
- CLS-Only variant (sentence-level embeddings only) often outperforms full alignment-based TAMS
- Mixed results on larger training sets (500+ samples) suggest translation signal less useful with more data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TAMS leverages high-quality pretrained language model embeddings from translation data to improve canonical morphological segmentation, especially in low-resource settings.
- Mechanism: The model uses BERT embeddings of English translations as additional signal, which captures morphological cues not available in the low-resource target language.
- Core assumption: English BERT embeddings contain morphological information useful for segmenting morphologically complex target language words.
- Evidence anchors:
  - [abstract] "We propose a character-level sequence-to-sequence model that incorporates representations of translations obtained from pretrained high-resource monolingual language models as an additional signal."
  - [section] "Previous work has suggested that distributional similarity is an informative cue for morphology... Other work has suggested that BERT embeddings could encode grammatical and morphological information."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.468, average citations=0.0." (Weak evidence of related work on translation-assisted morphological tasks)
- Break condition: If BERT embeddings don't capture relevant morphological patterns for the target language family, or if the translation is poor quality.

### Mechanism 2
- Claim: TAMS can achieve strong performance without requiring word-level alignments, making it more practical for resource-constrained language documentation.
- Mechanism: The model uses sentence-level CLS token embeddings from BERT as a fixed-length representation of the translation, bypassing the need for word alignments.
- Core assumption: Sentence-level embeddings capture sufficient information for morphological segmentation without explicit word alignments.
- Evidence anchors:
  - [abstract] "Additionally, we find that we can achieve strong performance even without needing difficult-to-obtain word level alignments."
  - [section] "For the CLS-None strategy, we discard d0 and average pool dalign before using a model parameter Wtrans... The intuition behind including the CLS token in our representation is that it may allow us to capture sentence-level dynamics better than word-level alignments alone."
  - [corpus] (No direct evidence in corpus, weak support)
- Break condition: If sentence-level embeddings lose critical alignment information needed for accurate segmentation.

### Mechanism 3
- Claim: TAMS is particularly effective in super-low resource settings where training data is extremely limited.
- Mechanism: The model's ability to incorporate external translation information becomes more valuable as the amount of available training data decreases.
- Core assumption: External translation information provides more relative benefit when native training data is scarce.
- Evidence anchors:
  - [abstract] "Our model outperforms the baseline in a super-low resource setting but yields mixed results on training splits with more data."
  - [section] "In the extremely low data setting (n=100), our approach outperforms the baseline by an average of 1.99 percentage points and as high as 2.87 points, even with poor quality automatic alignments."
  - [corpus] (No direct evidence in corpus, weak support)
- Break condition: If translation quality is poor or if the translation language is too distantly related to the target language.

## Foundational Learning

- Concept: Canonical morphological segmentation vs surface segmentation
  - Why needed here: The paper specifically addresses canonical segmentation (underlying morpheme forms) which is different from surface segmentation and requires understanding of linguistic concepts like morpheme alternation and allomorphy.
  - Quick check question: What's the difference between segmenting "cylindrically" as "cylinder-ly" vs "cylindric-ly"?

- Concept: Pointer-generator networks in sequence-to-sequence models
  - Why needed here: TAMS uses a pointer-generator LSTM which allows copying characters directly from input to output, crucial for morphological segmentation tasks.
  - Quick check question: How does a pointer-generator network differ from a standard LSTM encoder-decoder?

- Concept: Multilingual word alignment techniques
  - Why needed here: The paper experiments with both automatic (awesome-align) and manual alignment methods to connect target language words with translation words.
  - Quick check question: What are the trade-offs between using automatic alignment tools vs manual expert alignment?

## Architecture Onboarding

- Component map:
  Input characters -> LSTM encoder -> Pointer-generator decoder -> Output characters
  Translation BERT embeddings -> Translation vector creation -> Encoder/Decoder integration

- Critical path:
  1. Tokenize and normalize input text
  2. Align target words with translation words
  3. Generate BERT embeddings for aligned translation words
  4. Create fixed-length translation vector (CLS-Concat strategy)
  5. Incorporate into encoder (Init-State) and decoder (Concat-Half)
  6. Train pointer-generator LSTM to segment words

- Design tradeoffs:
  - Alignment quality vs practicality: Manual alignment gives better results but is expensive; automatic alignment is practical but lower quality
  - Translation incorporation strategy: Different ways to incorporate translation vectors affect performance
  - Model complexity: More layers and parameters vs training efficiency

- Failure signatures:
  - Poor alignment F1 scores (e.g., 0.1735 on Tsez manual alignment) indicating translation signal may be noisy
  - Performance gains only in low-resource settings, suggesting translation assistance is less useful with more data
  - Standard deviation in accuracy metrics ranging 1.11-2.33 across settings

- First 3 experiments:
  1. Test TAMS with gold-aligned data vs automatic alignment on Tsez to measure alignment quality impact
  2. Compare CLS-Only (sentence-level) vs word-level alignment strategies across different training sizes
  3. Evaluate performance on Arapaho (polysynthetic language) to test cross-family generalization

## Open Questions the Paper Calls Out

1. Does the performance benefit of TAMS require word-level alignments, or can sentence-level embeddings alone provide similar improvements? The paper only tests this on Tsez with 100 training samples.

2. Why does TAMS show significant improvements in the 100-sample setting but mixed results in higher-resource settings? The paper notes this pattern but doesn't provide a theoretical explanation.

3. Would providing additional explicit morphological information (like POS tags) alongside translations further improve segmentation performance? The paper suggests this as a future direction.

## Limitations
- Reliance on translation data quality, with automatic alignments showing poor F1 scores that could introduce noise
- Mixed results on larger training sets suggest translation signal may become less useful or even detrimental as native data increases
- Experiments limited to three languages from two families, raising questions about cross-linguistic generalizability

## Confidence

**High Confidence**: The core mechanism of using translation data to improve morphological segmentation in super-low resource settings (n=100) is well-supported by the experimental results showing 1.99-2.87 percentage point improvements.

**Medium Confidence**: The claim that TAMS can achieve strong performance without word-level alignments is partially supported, as the paper shows promising results with sentence-level CLS embeddings.

**Low Confidence**: The assertion that TAMS will be particularly valuable for language documentation scenarios is speculative, as the paper doesn't demonstrate performance on actual under-documented languages.

## Next Checks

1. **Alignment Quality Impact Study**: Conduct controlled experiments comparing TAMS performance with varying alignment quality levels (gold vs automatic vs no alignment) across multiple language families.

2. **Cross-Linguistic Generalization Test**: Evaluate TAMS on a broader set of morphologically diverse languages (including agglutinative, fusional, and polysynthetic types) to assess whether the translation signal benefits generalize beyond the current scope.

3. **Translation Language Dependency Analysis**: Systematically test TAMS with translations from different high-resource languages (e.g., Russian, Turkish, Chinese) to determine whether effectiveness depends on language family proximity or translation quality.