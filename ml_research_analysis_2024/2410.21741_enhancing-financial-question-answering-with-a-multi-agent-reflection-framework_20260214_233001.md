---
ver: rpa2
title: Enhancing Financial Question Answering with a Multi-Agent Reflection Framework
arxiv_id: '2410.21741'
source_url: https://arxiv.org/abs/2410.21741
tags:
- financial
- agent
- reasoning
- arxiv
- critic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a multi-agent framework for financial question
  answering that uses a critic agent to refine reasoning and numerical calculations.
  The framework applies LLM-based agents in single-, two-, and three-agent configurations
  to analyze financial documents and answer hybrid tabular-textual questions.
---

# Enhancing Financial Question Answering with a Multi-Agent Reflection Framework

## Quick Facts
- arXiv ID: 2410.21741
- Source URL: https://arxiv.org/abs/2410.21741
- Reference count: 37
- Primary result: Multi-agent framework with critic agents improves financial QA performance by 10-18% on FinQA, ConvFinQA, and TAT-QA datasets

## Executive Summary
This paper introduces a multi-agent framework for financial question answering that uses LLM-based critic agents to refine reasoning and numerical calculations. The system employs specialized agents that collaborate to analyze financial documents and answer hybrid tabular-textual questions. Experiments demonstrate that adding critic agents significantly improves performance over single-agent setups, with gains increasing as more specialized critics are used. The approach is particularly effective for smaller models like LLaMA3-8B and LLaMA3-70B, achieving results comparable to state-of-the-art fine-tuned models and larger proprietary models while requiring less computational overhead.

## Method Summary
The framework implements three configurations: single-agent (expert agent only), two-agent (expert + critic), and three-agent (expert + two specialized critics). The expert agent performs data extraction and reasoning using chain-of-thought prompting, while critic agents review outputs and provide feedback on data extraction accuracy and calculation correctness. The system uses AutoGen for agent communication, with system messages defining agent roles and capabilities. The expert agent generates initial answers with reasoning steps, critics provide targeted feedback, and the expert incorporates this feedback to produce refined answers. Experiments evaluate performance using Exact Match accuracy across three financial QA datasets.

## Key Results
- Adding critic agents improves performance by 10-18% over single-agent setups
- Three-agent configuration outperforms two-agent configuration on all datasets
- LLaMA3-70B with three agents achieves comparable results to GPT-4o-mini
- Smaller models (LLaMA3-8B) show the largest relative improvements from multi-agent refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Critic agents improve numerical reasoning by detecting and correcting calculation errors in the expert agent's outputs.
- Mechanism: The critic agent receives the expert's answer and performs independent verification of numerical calculations and logic. If errors are detected, it provides specific feedback that prompts the expert to recalculate and refine its answer.
- Core assumption: LLMs can reliably detect their own or others' calculation errors when given the appropriate system prompt and context.
- Evidence anchors: [abstract]: "a critic agent that reflects on the reasoning steps and final answers for each question"; [section]: "the critic agent identifies that the last step in the financial expert's reasoning is incorrect. Subsequently, upon receiving this feedback, the financial expert agent revises its answer, correcting the identified error"
- Break condition: The critic agent fails to detect errors when the initial interpretation of the question is incorrect, as it cannot provide constructive feedback on fundamentally misunderstood queries.

### Mechanism 2
- Claim: Dividing refinement tasks between specialized critic agents improves performance by allowing focused attention on distinct aspects of the answer.
- Mechanism: The framework uses two critic agents with specialized system messages - one focusing on data extraction from tables and text, the other focusing on calculation accuracy and unit consistency. This specialization allows each agent to develop expertise in its assigned domain.
- Core assumption: Specialized agents can provide more thorough and accurate feedback than a generalist critic when examining specific aspects of a response.
- Evidence anchors: [abstract]: "we enhance our system by adding multiple critic agents, each focusing on a specific aspect of the answer"; [section]: "The refinement process is divided into two sub-tasks, with each critic agent specializing in one aspect"
- Break condition: The performance gain from specialization diminishes if the tasks aren't clearly separable or if the agents overlap significantly in their areas of focus.

### Mechanism 3
- Claim: Iterative refinement through multi-agent communication reduces hallucination and improves answer accuracy in complex financial reasoning tasks.
- Mechanism: The expert agent generates an initial answer, the critic agent provides feedback, and the expert incorporates this feedback to produce a revised answer. This cycle can repeat, allowing the system to correct errors that might not be caught in a single pass.
- Core assumption: LLMs can effectively use external feedback to improve their reasoning without falling into repetitive error patterns.
- Evidence anchors: [abstract]: "This iterative, collaborative approach enhances the accuracy of financial analysis"; [section]: "This iterative refinement process, demonstrating how the interplay between the critic and financial expert agents leads to more accurate answers"
- Break condition: The system enters a feedback loop where the critic and expert keep correcting the same types of errors without converging on a correct answer.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Enables the expert agent to break down complex financial reasoning into sequential steps, making it easier for the critic to identify where errors occur
  - Quick check question: How does CoT prompting help the critic agent identify calculation errors in the expert's reasoning?

- Concept: Numerical reasoning in hybrid tabular-textual contexts
  - Why needed here: Financial QA requires extracting relevant numbers from both tables and text, then performing mathematical operations on them
  - Quick check question: What makes numerical reasoning in financial documents more challenging than pure text-based reasoning?

- Concept: Multi-agent communication frameworks
  - Why needed here: The system relies on structured communication between agents to pass feedback and refined answers
  - Quick check question: How does the AutoGen framework facilitate communication between the financial expert and critic agents?

## Architecture Onboarding

- Component map:
  - UserProxyAgent -> FinancialExpertAgent -> CriticAgent1 (Data Extraction) -> CriticAgent2 (Calculation) -> FinancialExpertAgent -> Final Answer

- Critical path:
  1. Financial expert agent receives question, text, and table data
  2. Expert generates initial answer with reasoning steps
  3. Critic agent reviews answer and provides feedback
  4. Expert incorporates feedback to produce refined answer
  5. Final answer is returned to user

- Design tradeoffs:
  - Single vs. multiple critic agents: Multiple critics provide specialized feedback but increase complexity and computational cost
  - Model size: Larger models (LLaMA3-70B) have better inherent reasoning but benefit less from refinement
  - Temperature setting: Lower temperature (0.1) ensures consistency but may reduce creativity in problem-solving approaches

- Failure signatures:
  - Critic agent provides vague or unhelpful feedback
  - Expert agent fails to incorporate feedback correctly
  - System gets stuck in repetitive correction cycles
  - Initial question interpretation is fundamentally wrong, making correction impossible

- First 3 experiments:
  1. Implement single-agent baseline with LLaMA3-8B to establish performance floor
  2. Add single critic agent and measure performance improvement on FinQA dataset
  3. Split critic role into data extraction and calculation specialists, measure incremental gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-agent framework perform on datasets with more complex multi-hop reasoning compared to FinQA and ConvFinQA?
- Basis in paper: [inferred] The paper mentions that ConvFinQA presents each intermediate step as a separate question, minimizing incorrect information extraction, and notes that TAT-QA includes simpler questions. This suggests that more complex multi-hop reasoning scenarios haven't been fully explored.
- Why unresolved: The current evaluation focuses on datasets with varying but relatively limited complexity in multi-hop reasoning. More complex scenarios could reveal additional strengths or weaknesses of the framework.
- What evidence would resolve it: Experiments on datasets specifically designed for complex multi-hop reasoning, comparing the multi-agent framework's performance to single-agent approaches and larger models.

### Open Question 2
- Question: What is the impact of using different types of critic agents (e.g., domain experts vs. general critics) on the framework's performance?
- Basis in paper: [explicit] The paper mentions that future work includes experimenting with an iterative (multi-turn) refinement process and developing a multi-step expert agent for extracting and performing calculation steps.
- Why unresolved: The current framework uses a general critic agent. Exploring specialized critic agents could provide insights into the most effective types of feedback for improving numerical reasoning.
- What evidence would resolve it: Comparative experiments using different types of critic agents (e.g., financial experts, general critics, domain-specific critics) to measure their impact on performance across various financial QA tasks.

### Open Question 3
- Question: How does the framework's performance scale with the complexity of the financial documents and the length of the questions?
- Basis in paper: [inferred] The paper discusses the framework's ability to handle hybrid tabular-textual data but doesn't extensively explore performance variations with document complexity or question length.
- Why unresolved: The study doesn't provide detailed analysis of how the framework handles increasingly complex financial documents or longer, more intricate questions.
- What evidence would resolve it: Experiments varying the complexity of financial documents (e.g., number of tables, length of text) and question length, measuring the framework's performance and identifying potential bottlenecks.

## Limitations

- System prompt content for agents is not specified, making exact reproduction difficult
- Performance evaluation lacks comprehensive error analysis showing failure patterns
- Framework tested only on financial datasets without validation of cross-domain generalizability

## Confidence

**High Confidence (8-10/10)**: The framework architecture is clearly specified with well-defined agent roles and communication patterns. The implementation using AutoGen framework is straightforward and the three experimental configurations (single-agent, two-agent, three-agent) are explicitly described. The performance improvements of 10-18% are reported consistently across multiple datasets, suggesting robust empirical findings.

**Medium Confidence (5-7/10)**: The core claim that specialized critic agents improve performance over generalist critics is supported by the reported results, but the underlying mechanism could use more detailed analysis. The paper shows that performance increases with more specialized critics, but doesn't provide sufficient evidence about whether this improvement comes from better error detection, more focused feedback, or some combination of factors.

**Low Confidence (2-4/10)**: The claim that the framework achieves "comparable" performance to fine-tuned models and larger proprietary models requires more careful scrutiny. The comparison is limited to a small set of benchmarks and doesn't account for potential differences in training data, model size, or implementation details. The statement that results are "comparable" may overstate the case given that Claude-3.5 Sonnet still outperforms the proposed system.

## Next Checks

**Validation Check 1: Prompt Ablation Study**: Systematically test the impact of different system prompt formulations on the financial expert agent's performance. Compare prompts that emphasize different aspects (calculation accuracy, data extraction completeness, reasoning transparency) to identify which prompt elements contribute most to the observed performance gains. This would help isolate whether improvements come from the multi-agent architecture itself or from the quality of agent instructions.

**Validation Check 2: Error Pattern Analysis**: Conduct a detailed error analysis on a subset of questions where the multi-agent system succeeds versus fails. Categorize errors into types (data extraction failures, calculation mistakes, reasoning gaps, question misinterpretation) and assess whether the critic agents successfully address specific error types. This would validate whether the claimed error-correction mechanism is actually functioning as described.

**Validation Check 3: Cross-Domain Transferability**: Apply the multi-agent framework to a non-financial dataset requiring numerical reasoning (e.g., scientific papers with numerical data or technical manuals with specifications). Compare performance against both single-agent baselines and domain-specific performance to assess whether the framework's benefits generalize beyond the financial domain where it was developed.