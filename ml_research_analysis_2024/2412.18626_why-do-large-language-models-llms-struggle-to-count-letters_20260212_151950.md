---
ver: rpa2
title: Why Do Large Language Models (LLMs) Struggle to Count Letters?
arxiv_id: '2412.18626'
source_url: https://arxiv.org/abs/2412.18626
tags:
- letters
- llms
- word
- errors
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates why large language models (LLMs) struggle
  to count letter occurrences in words, focusing on the widely known "strawberry"
  problem. The research evaluates eight representative LLMs (including GPT-4o, Llama3.1,
  Mistral, and Gemma) on 10,000 randomly selected words, analyzing their counting
  errors against various word features.
---

# Why Do Large Language Models (LLMs) Struggle to Count Letters?

## Quick Facts
- arXiv ID: 2412.18626
- Source URL: https://arxiv.org/abs/2412.18626
- Reference count: 28
- LLMs struggle to count letter occurrences when letters appear multiple times within words

## Executive Summary
This study investigates why large language models fail at a seemingly simple task: counting letter occurrences in words. The research examines eight prominent LLMs including GPT-4o, Llama3.1, Mistral, and Gemma across 10,000 randomly selected English words. The key discovery is that counting errors correlate strongly with letter multiplicity rather than tokenization or word frequency. While models accurately detect which letters are present, they struggle significantly when letters appear more than once in a word. This fundamental limitation persists even in larger models, suggesting counting mechanisms represent a core architectural constraint rather than a training data issue.

## Method Summary
The study evaluates eight representative LLMs on counting letter occurrences in 10,000 randomly selected English words. The researchers systematically analyze errors against multiple factors including word frequency, token frequency, and letter multiplicity. They distinguish between detection accuracy (identifying which letters are present) and counting accuracy (determining how many times each letter appears). The analysis includes token-level examination to assess whether errors stem from tokenization artifacts or represent fundamental counting limitations. The study also compares model performance across different sizes to evaluate the relationship between model scale and counting capability.

## Key Results
- Counting errors increase dramatically when letters appear more than once in a word, with most models failing on letters appearing more than twice
- Models can accurately detect which letters are present but struggle with counting multiplicity
- Error rates for letters appearing exactly twice remain similar whether occurrences are in same or different tokens
- Larger models generally show better counting performance but still struggle significantly with multi-occurrence letters

## Why This Works (Mechanism)
The study reveals that LLMs possess functional letter detection capabilities but have fundamental limitations in quantitative counting operations. The counting mechanism appears to fail specifically when tracking multiple instances of the same symbol, suggesting architectural constraints in how models represent and update symbol occurrence information during processing. The consistent failure across different tokenization schemes and word frequencies indicates this is an inherent limitation in how LLMs perform sequential counting rather than a surface-level implementation issue.

## Foundational Learning
- **Letter detection vs counting**: Understanding that models can identify presence but not quantity is crucial for interpreting results. Quick check: Verify detection accuracy separately from counting accuracy.
- **Multiplicity effect**: The strong correlation between error rates and letter repetition frequency is the central finding. Quick check: Analyze error distribution across different multiplicity levels.
- **Tokenization impact**: While not the primary cause, tokenization still influences performance. Quick check: Compare performance across different tokenization schemes.

## Architecture Onboarding
**Component Map**: Input Tokenization -> Letter Detection -> Counting Mechanism -> Output Generation

**Critical Path**: Tokenization → Letter Detection → Counting → Response Formation

**Design Tradeoffs**: The study reveals a fundamental tradeoff between letter identification accuracy and counting precision, suggesting architectural constraints in how models track symbol occurrences during sequential processing.

**Failure Signatures**: 
- Accurate detection but incorrect counts for multi-occurrence letters
- Similar error rates for same-token vs different-token multiple occurrences
- Strong correlation between letter multiplicity and error probability

**First Experiments**:
1. Test counting accuracy on single-occurrence vs multi-occurrence letters within the same words
2. Compare detection accuracy versus counting accuracy on identical word sets
3. Analyze token-level processing to identify where counting information is lost

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to languages with different orthographic or morphological structures
- Limited exploration of how different tokenization schemes specifically impact counting performance
- Does not investigate whether counting limitations extend to numerical digit sequences or other quantitative tasks

## Confidence
- Counting difficulty correlation with multiplicity: High
- Tokenization not primary cause: Medium
- Model size correlation with performance: High

## Next Checks
1. Test the same counting task across multiple languages and scripts to determine if the multiplicity-counting difficulty generalizes beyond English orthography
2. Conduct controlled experiments with custom tokenization schemes to isolate tokenization effects from inherent counting limitations
3. Compare counting performance on numerical digit sequences versus letter sequences to determine if the limitation is specific to orthographic symbols or reflects broader quantitative reasoning constraints