---
ver: rpa2
title: When Do Off-Policy and On-Policy Policy Gradient Methods Align?
arxiv_id: '2402.12034'
source_url: https://arxiv.org/abs/2402.12034
tags:
- policy
- objective
- state
- distribution
- stationary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes theoretical conditions under which off-policy\
  \ policy gradient methods align with on-policy methods, addressing a critical gap\
  \ in understanding off-policy optimization. The core contribution is showing that\
  \ when the discount factor \u03B3 approaches 1 and the underlying Markov chain is\
  \ irreducible and aperiodic, the excursion objective used in off-policy learning\
  \ becomes arbitrarily close to the on-policy objective."
---

# When Do Off-Policy and On-Policy Policy Gradient Methods Align?

## Quick Facts
- arXiv ID: 2402.12034
- Source URL: https://arxiv.org/abs/2402.12034
- Reference count: 38
- Key outcome: This paper establishes theoretical conditions under which off-policy policy gradient methods align with on-policy methods, addressing a critical gap in understanding off-policy optimization.

## Executive Summary
This paper addresses a fundamental question in reinforcement learning: when do off-policy policy gradient methods produce results comparable to on-policy methods? The authors provide theoretical analysis showing that off-policy and on-policy objectives align when the discount factor approaches 1 and the underlying Markov chain satisfies certain irreducibility and aperiodicity conditions. The key insight is that the excursion objective used in off-policy learning converges to the on-policy objective under these conditions.

Empirically, the work demonstrates that using discount factors below 0.95 in practical continuous control environments can lead to significant misalignment between objectives, sometimes resulting in worse-than-random performance rankings. This provides theoretical justification for the common practice of using high discount factors in off-policy policy gradient methods while quantifying the trade-offs involved.

## Method Summary
The paper establishes conditions for alignment between off-policy and on-policy policy gradient methods through theoretical analysis of the excursion objective. The authors derive an upper bound on the gradient difference between these objectives, showing it scales with (1-γ)² and the total variation distance between state distributions. The analysis assumes finite state spaces and uses concentration inequalities to bound the difference between on-policy and off-policy state distributions.

## Key Results
- The excursion objective used in off-policy learning becomes arbitrarily close to the on-policy objective when γ → 1 and the Markov chain is irreducible and aperiodic
- The gradient difference bound scales as (1-γ)² multiplied by the total variation distance between state distributions
- Using γ < 0.95 in continuous control environments can lead to significant misalignment, sometimes worse than random selection
- High discount factors are necessary (but not sufficient) for reliable off-policy optimization

## Why This Works (Mechanism)
The alignment occurs because as the discount factor approaches 1, the agent's effective planning horizon extends to infinity, making the off-policy state distribution (which depends on the behavior policy) converge to the on-policy state distribution (which depends on the current policy). The irreducibility and aperiodicity of the Markov chain ensure that the state space is fully explored, preventing the off-policy distribution from getting "stuck" in regions that don't represent the true on-policy distribution.

## Foundational Learning

1. **Excursion objective**: Measures the expected return under a behavior policy while optimizing for the current policy. Why needed: Forms the basis of off-policy policy gradient methods. Quick check: Verify understanding of how this differs from standard policy gradient objectives.

2. **Total variation distance**: Measures the difference between two probability distributions. Why needed: Quantifies how much the off-policy and on-policy state distributions differ. Quick check: Can you explain why this metric is appropriate for comparing state distributions?

3. **Irreducibility and aperiodicity**: Properties of Markov chains ensuring all states are reachable and visitation times don't have periodic patterns. Why needed: Guarantees that the state distribution mixes properly across the entire state space. Quick check: Can you identify whether a given Markov chain satisfies these properties?

4. **Discount factor role**: Controls the effective planning horizon and affects the alignment between objectives. Why needed: Central to understanding when off-policy methods approximate on-policy behavior. Quick check: How does changing γ affect the theoretical bounds derived in the paper?

## Architecture Onboarding

**Component map**: On-policy objective <-(TV distance)-> Off-policy excursion objective <-(γ, MC properties)-> Alignment conditions

**Critical path**: Policy evaluation -> State distribution analysis -> Gradient alignment verification

**Design tradeoffs**: High γ provides better alignment but reduces the agent's effective planning horizon and can make learning more difficult; low γ allows longer planning but risks misalignment.

**Failure signatures**: 
- Significant performance gap between on-policy and off-policy evaluation
- Rankings that contradict on-policy performance metrics
- Instability in policy updates when using moderate discount factors

**First experiments**:
1. Verify the theoretical bound by computing gradient differences across different γ values in a simple grid world
2. Test the alignment conditions on a small MDP with known mixing properties
3. Implement a basic off-policy policy gradient algorithm and compare performance across different discount factors

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes finite state spaces, which may not hold for many continuous control problems
- Requires careful verification of mixing properties and total variation bounds in practice
- The concentration inequality assumes bounded rewards and action spaces, limiting applicability to unbounded scenarios
- Does not address function approximation, which is essential for scaling to complex problems

## Confidence
- High confidence: The theoretical derivation of the gradient difference bound scaling as (1-γ)² is rigorously proven
- Medium confidence: The empirical demonstration of misalignment for γ < 0.95 is based on specific benchmarks and may not generalize
- Medium confidence: The claim that high discount factors are necessary for reliable off-policy optimization, as the analysis shows sufficient but not necessary conditions

## Next Checks
1. Extend the theoretical analysis to infinite state spaces with function approximation to assess real-world applicability
2. Conduct systematic experiments varying mixing times and ergodicity properties to validate the total variation distance bounds empirically
3. Test the framework on non-stationary environments where the Markov chain assumptions may be violated to understand robustness limitations