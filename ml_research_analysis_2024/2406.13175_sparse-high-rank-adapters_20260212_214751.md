---
ver: rpa2
title: Sparse High Rank Adapters
arxiv_id: '2406.13175'
source_url: https://arxiv.org/abs/2406.13175
tags:
- shira
- lora
- adapter
- adapters
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Sparse High Rank Adapters (SHiRA), a new paradigm
  that enables rapid switching of adapters and significantly reduces concept loss
  during multi-adapter fusion. SHiRA achieves this by finetuning only 1-2% of the
  base model weights, resulting in a highly sparse adapter that can be switched directly
  in the fused mode.
---

# Sparse High Rank Adapters

## Quick Facts
- arXiv ID: 2406.13175
- Source URL: https://arxiv.org/abs/2406.13175
- Reference count: 40
- Key outcome: Enables rapid adapter switching and reduces concept loss during multi-adapter fusion by finetuning only 1-2% of base model weights

## Executive Summary
SHiRA introduces a novel approach to parameter-efficient fine-tuning that achieves both rapid adapter switching and effective multi-adapter fusion. By training only a small fraction (1-2%) of base model weights, SHiRA creates highly sparse adapters that can be directly overwritten onto the base model without computationally expensive fusion steps. The method demonstrates significant performance improvements over LoRA while enabling memory-efficient implementation and faster inference switching.

## Method Summary
SHiRA involves masking gradients during backpropagation to finetune only 1-2% of the base model weights while leaving others unchanged. The method uses various sparse mask strategies (SHiRA-Struct, SHiRA-Rand, SHiRA-WM, SHiRA-Grad, SHiRA-SNIP) to determine which parameters to train. Trained adapters are stored as sparse weights with indices, enabling rapid inference loading through scatter operations. The approach is implemented using the PEFT library and validated on commonsense reasoning tasks for LLMs and style transfer tasks for LVMs.

## Key Results
- Achieves 5-16x faster adapter switching compared to LoRA fusion on CPU
- Maintains competitive performance while training only 1-2% of base model parameters
- Reduces peak GPU memory consumption by up to 16% compared to LoRA
- Demonstrates improved orthogonality between adapters, reducing concept loss during multi-adapter fusion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHiRA enables rapid adapter switching by directly overwriting a small fraction of base model weights without requiring fusion computation.
- Mechanism: By training only 1-2% of base weights, SHiRA adapters can be stored as sparse weights plus indices. At inference, these weights are scattered directly onto the base model using a scatter operation, avoiding the computationally expensive fusion step required by LoRA.
- Core assumption: The base model weights changed by SHiRA are a small enough subset that scatter-based overwriting is faster than fusion computation.
- Evidence anchors:
  - [abstract] "To demonstrate rapid switching benefits during inference, we show that loading SHiRA on a base model can be 5x-16x faster than LoRA fusion on a CPU."
  - [section 3.2] "Since very few base weights change during the SHiRA training, we can simply extract them out and store them as sparse weights and their indices."
  - [corpus] Weak - related papers focus on LoRA variants but don't directly address scatter-based rapid switching mechanisms.

### Mechanism 2
- Claim: SHiRA's high sparsity reduces concept loss during multi-adapter fusion by increasing orthogonality between adapters.
- Mechanism: High sparsity means fewer overlapping non-zero elements between different adapters. This increases the number of zero elements in the product AT1A2, making adapters more orthogonal and reducing interference when combined.
- Core assumption: Sparse adapters with fewer overlapping non-zero elements will have less multiplicative interference when fused.
- Evidence anchors:
  - [section 4.2] "We theoretically and empirically analyze the high rank vs. sparsity properties of SHiRA and why that helps with adapter performance."
  - [section 4.2] "SHiRA can in fact create near-orthogonal adapters" and provides AWOM/AWOR metrics showing SHiRA has higher orthogonality than LoRA.
  - [corpus] Weak - related papers discuss adapter orthogonality but don't provide the specific sparsity-orthogonality relationship SHiRA demonstrates.

### Mechanism 3
- Claim: SHiRA achieves competitive performance with minimal parameter updates by leveraging the high rank property of sparse masks.
- Mechanism: While SHiRA uses very few parameters (1-2%), the structured sparsity patterns (like SHiRA-Struct with diagonal + structured rows/columns) create high rank adapters that maintain expressive power without the parameter overhead of full dense training.
- Core assumption: High rank sparse patterns can maintain sufficient model capacity even with minimal parameter updates.
- Evidence anchors:
  - [abstract] "SHiRA can be trained by directly tuning only 1-2% of the base model weights while leaving others unchanged."
  - [section 4.1] "SHiRA is a combination of a high rank but very sparse adapter and a rank 1 adapter."
  - [corpus] Weak - related papers focus on low-rank adaptations but don't explore high-rank sparse alternatives.

## Foundational Learning

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: SHiRA builds on PEFT principles to achieve adaptation with minimal parameter updates, making it crucial to understand the trade-offs between parameter count and model performance.
  - Quick check question: What is the primary advantage of PEFT methods like LoRA and SHiRA compared to full fine-tuning?

- Concept: Low-Rank Matrix Approximation
  - Why needed here: Understanding how LoRA works through low-rank decomposition helps contrast with SHiRA's high-rank sparse approach and explains why SHiRA can maintain expressive power with fewer parameters.
  - Quick check question: How does LoRA's rank-r decomposition differ from SHiRA's approach to parameter efficiency?

- Concept: Sparsity and Model Expressiveness
  - Why needed here: The relationship between sparsity patterns and model capacity is central to SHiRA's design - understanding this helps explain why structured sparsity can outperform unstructured approaches.
  - Quick check question: What is the trade-off between sparsity level and model expressiveness in adapter-based fine-tuning?

## Architecture Onboarding

- Component map:
  - Base model weights (frozen) -> Sparse mask (determines which parameters to train) -> SHiRA adapter (sparse weights after training) -> Scatter operation (for efficient inference loading) -> Orthogonality metrics (AWOM/AWOR for multi-adapter analysis)

- Critical path:
  1. Create sparse mask based on chosen strategy (Struct, Rand, WM, Grad, SNIP)
  2. Apply mask during gradient computation via gradient hook or PEFT implementation
  3. Train only masked parameters while keeping base model frozen
  4. Extract modified weights as sparse adapter (weights + indices)
  5. At inference, use scatter operation to load adapter onto base model
  6. For multi-adapter scenarios, evaluate orthogonality metrics before fusion

- Design tradeoffs:
  - Sparsity level vs. performance: Higher sparsity (1%) vs. lower sparsity (2%) affects both training speed and final accuracy
  - Mask structure vs. task complexity: Structured masks (SHiRA-Struct) work well for vision but may underperform on complex language tasks
  - Memory efficiency vs. implementation complexity: PEFT-based implementation offers 16% memory savings but requires understanding of Pytorch hooks

- Failure signatures:
  - Poor performance despite high sparsity: Likely indicates the mask strategy doesn't align with task requirements (e.g., using SHiRA-Struct for complex reasoning tasks)
  - Slow inference despite scatter optimization: May indicate too many parameters being modified (>5%) or inefficient scatter implementation
  - Concept loss in multi-adapter scenarios: Suggests insufficient orthogonality between adapters, possibly due to overlapping mask patterns

- First 3 experiments:
  1. Single adapter baseline: Compare SHiRA-WM vs LoRA on a simple vision task (style transfer) with 1% parameter budget
  2. Mask strategy comparison: Test all five SHiRA mask types on the same task to identify which performs best
  3. Multi-adapter orthogonality: Train two SHiRA adapters with different masks and measure AWOM/AWOR to validate orthogonality benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sparse mask strategy for SHiRA that maximizes both single-adapter performance and multi-adapter fusion capabilities?
- Basis in paper: [inferred] The paper discusses various SHiRA masking strategies (SHiRA-Struct, SHiRA-Rand, SHiRA-WM, SHiRA-Grad, SHiRA-SNIP) and notes that SHiRA-Struct performs well for vision but not language tasks, while SHiRA-SNIP works across both domains. The paper also notes that non-overlapping masks achieve lower single-adapter accuracy than overlapping masks, suggesting a tradeoff between single and multi-adapter performance.
- Why unresolved: The paper demonstrates that different masks work better for different tasks and that there's a tradeoff between single and multi-adapter performance, but doesn't identify a single optimal mask strategy that works well across all scenarios.
- What evidence would resolve it: Systematic experiments comparing all mask strategies across a wide range of tasks (both vision and language), measuring both single-adapter and multi-adapter performance, would help identify optimal mask strategies or reveal fundamental tradeoffs.

### Open Question 2
- Question: How do the orthogonality properties of SHiRA adapters translate to actual interference in the feature space during multi-adapter fusion?
- Basis in paper: [explicit] The paper provides theoretical analysis showing that SHiRA-Struct adapters can be nearly orthogonal, and presents metrics (AWOM and AWOR) measuring adapter weight orthogonality. However, it explicitly states that this analysis focuses on orthogonality of adapter weights, not orthogonality of subspaces.
- Why unresolved: While the paper demonstrates that SHiRA adapters have better weight orthogonality than LoRA, it doesn't investigate whether this weight orthogonality actually translates to reduced interference in the feature space where the adapters actually operate.
- What evidence would resolve it: Empirical studies measuring feature space interference during multi-adapter fusion, comparing SHiRA with LoRA across various tasks, would reveal whether weight orthogonality translates to practical benefits.

### Open Question 3
- Question: What is the relationship between SHiRA adapter sparsity and the required dataset size for effective finetuning?
- Basis in paper: [explicit] The paper provides Lemma 4.1 stating that the learning complexity of SHiRA equals the number of non-zero elements, and Lemma 4.2 suggesting LoRA can approximate SHiRA with bounded error. However, it doesn't empirically investigate how sparsity affects dataset requirements.
- Why unresolved: While the paper provides theoretical bounds on learning complexity, it doesn't empirically validate whether highly sparse SHiRA adapters actually require smaller datasets than LoRA, or how sparsity affects convergence and generalization.
- What evidence would resolve it: Controlled experiments varying dataset sizes for different sparsity levels of SHiRA adapters, measuring convergence speed and final performance, would reveal the practical relationship between sparsity and data requirements.

## Limitations
- Implementation complexity: Exact implementation details for SHiRA masks are not fully specified, creating uncertainty about achieving claimed memory and speed benefits
- Task generalization: Empirical results focus on specific task types (commonsense reasoning and style transfer) with limited evidence across diverse domains
- Orthogonality assumptions: Theoretical analysis relies on idealized conditions that may not hold in practice due to data correlations and training dynamics

## Confidence
- High confidence: SHiRA enables rapid adapter switching through scatter-based weight overwriting
- Medium confidence: SHiRA achieves competitive performance with 1-2% parameter updates
- Low confidence: SHiRA's high sparsity guarantees improved multi-adapter fusion through increased orthogonality

## Next Checks
1. Train three SHiRA adapters on increasingly related tasks and measure AWOM/AWOR degradation to establish orthogonality limits
2. Evaluate a single SHiRA adapter trained on one task when applied to a different but related task to test zero-shot transfer
3. Test SHiRA performance across different model scales (1B, 7B, 70B parameters) to verify parameter update effectiveness