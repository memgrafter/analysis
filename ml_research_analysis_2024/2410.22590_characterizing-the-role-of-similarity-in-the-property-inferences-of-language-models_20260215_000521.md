---
ver: rpa2
title: Characterizing the Role of Similarity in the Property Inferences of Language
  Models
arxiv_id: '2410.22590'
source_url: https://arxiv.org/abs/2410.22590
tags:
- similarity
- property
- premise
- conclusion
- last
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the interplay between taxonomic relations
  and categorical similarity in the property inheritance behavior of large language
  models (LMs). Using both behavioral and causal interpretability analyses, the authors
  show that LMs are more likely to project novel properties from one category to another
  when they are taxonomically related and highly similar.
---

# Characterizing the Role of Similarity in the Property Inferences of Language Models

## Quick Facts
- arXiv ID: 2410.22590
- Source URL: https://arxiv.org/abs/2410.22590
- Authors: Juan Diego Rodriguez; Aaron Mueller; Kanishka Misra
- Reference count: 40
- Language models are more likely to project novel properties from one category to another when they are taxonomically related and highly similar.

## Executive Summary
This paper investigates how large language models (LMs) determine property inheritance between categories, examining the interplay between taxonomic relations and categorical similarity. Using both behavioral experiments and causal interpretability methods (DAS), the authors show that LMs exhibit property inheritance patterns that depend on both hierarchical taxonomic relationships and semantic/visual similarity between categories. The findings challenge the assumption that LMs rely purely on taxonomic principles and suggest that taxonomic and similarity features are fundamentally entangled in model representations.

## Method Summary
The authors use the THINGS dataset to create premise-conclusion pairs with nonce properties (e.g., "is/are daxable") to isolate inference behavior from prior knowledge. They test four instruction-tuned LMs (Mistral 7B, Gemma 2 2B/9B, Llama 3 8B) using two similarity metrics (Word-Sense and SPoSE). Behavioral analysis measures taxonomic sensitivity, property sensitivity, and mismatch sensitivity across different data slices. For causal analysis, they employ Distributed Alignment Search (DAS) to identify subspaces responsible for property inheritance and evaluate Interchange Intervention Accuracy (IIA) to determine whether these subspaces encode taxonomic vs. similarity features.

## Key Results
- LMs show positive correlations between property inheritance judgments and both taxonomic relations and categorical similarity
- Causal analysis reveals that property inheritance subspaces are sensitive to both taxonomic and similarity features, indicating fundamental entanglement
- Instruction-tuned models show stronger taxonomic sensitivity than non-instruction-tuned models
- Property inheritance behavior cannot be explained by pure taxonomic principles alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models use a combined taxonomic-similarity feature space to determine property inheritance
- Mechanism: When predicting whether a novel property should transfer from category A to category B, the model accesses a subspace that encodes both hierarchical taxonomic relations and similarity-based relations. These features are entangled rather than separate.
- Core assumption: The model's internal representations do not cleanly separate taxonomic knowledge from similarity-based knowledge
- Evidence anchors:
  - [abstract] "our findings provide insight into the conceptual structure of language models and may suggest new psycholinguistic experiments for human subjects"
  - [section 4.1] "There is a substantial drop from Ambiguous-Test to Ambiguous-Gen for all models and for every intervention position"
- Break condition: If future experiments show property inheritance can be localized to subspaces that respond only to taxonomic features

### Mechanism 2
- Claim: Property inheritance behavior is mediated by causal variables that represent category membership relationships
- Mechanism: The model implements a causal graph where category membership (B is a kind of A) is a mediating variable that determines whether property inheritance occurs
- Core assumption: The model uses an explicit representation of "category membership" as a causal variable
- Evidence anchors:
  - [section 4] "We hypothesize that language models rely on this causal graph to perform the categorical inference task"
  - [section 4.1] "the learned rotations tend to rely on similarity rather than taxonomic relations"
- Break condition: If experiments show property inheritance can be explained purely by surface-level pattern matching

### Mechanism 3
- Claim: Instruction-tuned models show stronger taxonomic sensitivity than non-instruction-tuned models
- Mechanism: The fine-tuning process on instruction datasets reinforces taxonomic category membership as a principle for reasoning
- Core assumption: Instruction tuning provides exposure to examples that emphasize taxonomic relationships
- Evidence anchors:
  - [section 3.3] "Initial experiments showed these LMs' non instruction-tuned counterparts to perform worse at property inheritance"
  - [section 3.4] All four models used are instruction-tuned, showing consistent taxonomic sensitivity
- Break condition: If non-instruction-tuned models of similar size show equivalent taxonomic sensitivity

## Foundational Learning

- Concept: Causal interpretability methods (DAS)
  - Why needed here: To identify which internal model subspaces are responsible for property inheritance behavior and determine whether they encode taxonomic vs similarity features
  - Quick check question: What is the difference between DAS and neuron-based interpretability methods in terms of what they can capture?

- Concept: Taxonomic relations vs similarity relations
  - Why needed here: The paper investigates how both types of relations influence property inheritance, requiring understanding of the distinction and how they might interact
  - Quick check question: In what way does the Sloman (1998) finding about human property inheritance challenge purely taxonomic explanations?

- Concept: Distributed alignment search (DAS) and interchange intervention accuracy (IIA)
  - Why needed here: These are the specific methods used to measure causal alignment between hypothesized variables and model representations
  - Quick check question: How does IIA quantify the success of a DAS intervention in aligning a model subspace with a hypothesized causal variable?

## Architecture Onboarding

- Component map: Stimulus generation -> LM inference -> Similarity computation -> Behavioral analysis -> DAS training -> Causal analysis -> Interpretation
- Critical path: Generate premise-conclusion pairs with nonce properties → Run LMs to collect responses → Compute behavioral metrics → Train DAS on balanced dataset → Evaluate IIA on test sets → Analyze sensitivity to taxonomic/similarity features
- Design tradeoffs: Using nonce properties isolates inference from parametric knowledge but may reduce ecological validity; using multiple similarity metrics captures different aspects but increases complexity; DAS provides causal insights but requires careful interpretation
- Failure signatures: Low IIA values indicating poor causal alignment; inconsistent results across similarity metrics; failure to replicate taxonomic sensitivity in behavioral experiments
- First 3 experiments:
  1. Run behavioral analysis with both similarity metrics on a smaller subset of stimuli to verify basic pattern
  2. Implement DAS training on the balanced dataset and evaluate IIA at different token positions and layers
  3. Test learned DAS interventions on ambiguous test sets to determine whether subspaces rely on taxonomic or similarity features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language models show sensitivity to both taxonomic relations and categorical similarity in their property inheritance behavior?
- Basis in paper: [explicit] The paper demonstrates that LMs are more likely to project novel properties when categories are taxonomically related and highly similar
- Why unresolved: The paper does not fully explore the relative strength of taxonomic vs. similarity effects across different types of properties or contexts
- What evidence would resolve it: Experiments varying property types while controlling for taxonomic and similarity relationships

### Open Question 2
- Question: Are taxonomic information and noun similarity fundamentally entangled in language model representations?
- Basis in paper: [explicit] Causal analysis using DAS shows subspaces responsible for property inheritance are sensitive to both taxonomic and similarity features
- Why unresolved: The paper shows entanglement exists but doesn't determine whether this reflects human-like conceptual organization or is an artifact of training data
- What evidence would resolve it: Comparing entanglement patterns across models trained on different data distributions

### Open Question 3
- Question: Do language models show sensitivity to the directionality of property inheritance?
- Basis in paper: [explicit] Behavioral analysis of directional sensitivity shows mixed results across models
- Why unresolved: The paper doesn't systematically explore what factors influence directional sensitivity
- What evidence would resolve it: Systematic experiments varying prompt templates, model families, and property types

## Limitations

- The behavioral analysis shows only moderate effect sizes (correlation coefficients typically below 0.4), suggesting similarity is only one factor among many
- The use of nonce properties, while controlling for prior knowledge, may not capture how LMs handle real-world property inheritance tasks
- The causal analysis could potentially reflect correlated features in training data rather than truly entangled representations

## Confidence

- **High Confidence**: Behavioral findings showing LMs are more likely to project properties between taxonomically related and highly similar categories
- **Medium Confidence**: Claim that taxonomic and similarity features are fundamentally entangled in model representations
- **Low Confidence**: Psycholinguistic implications suggesting these findings should inform human subject experiments

## Next Checks

1. **Feature Ablation Study**: Conduct systematic ablation experiments where either taxonomic or similarity features are explicitly controlled or removed from input to determine whether observed entanglement is causal or merely correlational

2. **Cross-Lingual Validation**: Test property inheritance patterns across multiple languages to determine whether taxonomic-similarity interplay is universal or specific to English-language training data

3. **Human Comparison Study**: Design and conduct direct comparison between LM property inheritance judgments and human judgments on the same stimuli to test whether LM behavior actually mirrors human reasoning patterns