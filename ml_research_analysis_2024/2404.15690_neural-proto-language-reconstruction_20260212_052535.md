---
ver: rpa2
title: Neural Proto-Language Reconstruction
arxiv_id: '2404.15690'
source_url: https://arxiv.org/abs/2404.15690
tags:
- daughter
- reconstruction
- wikihan
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores three approaches to improve neural proto-language
  reconstruction on the WikiHan dataset: data augmentation via reflex prediction using
  CNN and transformer models, a variational autoencoder (VAE) structure added to the
  Transformer model for proto-to-language prediction, and a modified NMT model. The
  VAE-Transformer model achieved the best performance with an edit distance of 0.883
  and accuracy of 54.05%, outperforming the baseline Transformer model.'
---

# Neural Proto-Language Reconstruction

## Quick Facts
- arXiv ID: 2404.15690
- Source URL: https://arxiv.org/abs/2404.15690
- Authors: Chenxuan Cui; Ying Chen; Qinxin Wang; David R. Mortensen
- Reference count: 5
- Key outcome: VAE-Transformer model achieved edit distance 0.883 and accuracy 54.05% on WikiHan dataset, outperforming baseline Transformer

## Executive Summary
This paper addresses the challenge of neural proto-language reconstruction by proposing three approaches to improve performance on the WikiHan dataset. The authors explore data augmentation through reflex prediction, a variational autoencoder (VAE) structure added to the Transformer model, and modifications to neural machine translation architectures. The VAE-Transformer model achieves the best results, demonstrating that incorporating the Neogrammarian hypothesis through forward reconstruction improves proto-form prediction. The study also reveals that the model relies heavily on languages with more data (Cantonese, Hokkien, Mandarin), suggesting room for improvement in utilizing all daughter languages effectively.

## Method Summary
The paper proposes three approaches to improve neural proto-language reconstruction. First, data augmentation recovers missing reflexes using character-level CNN and transformer models that predict missing daughter forms from proto-forms. Second, a VAE structure is added to the Transformer model, incorporating a daughter decoder that reconstructs daughter forms from the latent space, enforcing the Neogrammarian hypothesis. Third, the authors modify a neural machine translation model by adapting its encoder-decoder architecture to handle the proto-language reconstruction task. The VAE-Transformer model combines these approaches, with the VAE component ensuring the latent space contains information necessary for both proto-form and daughter-form reconstruction.

## Key Results
- VAE-Transformer model achieved edit distance of 0.883 and accuracy of 54.05% on WikiHan dataset
- Data augmentation with transformer-based reflex predictor improved training stability and reduced variance
- Attention visualization revealed model heavily relies on Cantonese, Hokkien, and Mandarin (languages with most data)
- VAE structure successfully enforced Neogrammarian hypothesis, improving proto-form prediction quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The VAE structure improves the quality of the latent space by enforcing the Neogrammarian hypothesis, which states that sound change is normally regular and deterministic.
- Mechanism: The VAE component includes a daughter decoder that reconstructs daughter forms from the latent space. This forces the latent space to contain all the information needed to forward-reconstruct any daughter form, making it more regular and meaningful for the proto-form reconstruction task.
- Core assumption: The model can learn a latent space that captures the deterministic sound change rules and is sufficient for both proto-form and daughter-form reconstruction.
- Evidence anchors:
  - [abstract]: "Adding a VAE structure to the Transformer model for proto-to-language prediction"
  - [section 4.1]: "We aim to improve the existing architecture by incorporating the Neogrammarian hypothesis in the form of adding a forward-reconstruction module."
  - [corpus]: Weak evidence. The corpus does not directly discuss the VAE mechanism in proto-language reconstruction.
- Break condition: If the daughter decoder fails to reconstruct daughter forms accurately, the latent space will not improve, and the model's performance will not increase.

### Mechanism 2
- Claim: Data augmentation via reflex prediction stabilizes training and reduces variance by filling in missing entries in the WikiHan dataset.
- Mechanism: The model predicts missing daughter forms using either a CNN-based reflex predictor or a transformer-based transducer. These predictions are added to the training data, especially for languages with fewer data points, making the model less prone to overfitting and overemphasizing languages with more data.
- Core assumption: The reflex prediction models can accurately predict missing daughter forms, and these augmented data points are useful for training the proto-form reconstruction model.
- Evidence anchors:
  - [abstract]: "data augmentation to recover missing reflexes"
  - [section 3.1]: "We augmented the training set of the WikiHan dataset by predicting the missing entries based on the proto-form and optionally the existing daughter forms in the same cognate set."
  - [corpus]: Weak evidence. The corpus mentions data augmentation but does not specifically discuss reflex prediction in the context of proto-language reconstruction.
- Break condition: If the reflex prediction models are inaccurate, the augmented data will be noisy and could degrade the model's performance.

### Mechanism 3
- Claim: The transformer-based reflex predictor is more effective than the CNN-based reflex predictor for data augmentation in the WikiHan dataset.
- Mechanism: The transformer model takes the proto-form and the target language as input and predicts the missing daughter form. It outperforms the CNN model on the WikiHan dataset because the reflexes are descended from the proto-form, making it natural to apply the transducer.
- Core assumption: The transformer model can effectively learn the patterns of reflex prediction from the proto-form to the daughter forms in the WikiHan dataset.
- Evidence anchors:
  - [section 3.4.1]: "The transducer is favorable when there are clear patterns to match from the input to the output phonemes. For the WikiHan dataset, the reflexes to predict are descended from the proto-form input, which make it natural to apply the transducer."
  - [table 2]: "Transducer 64 45" (WikiHan accuracy)
  - [corpus]: No direct evidence in the corpus for this specific claim.
- Break condition: If the patterns between proto-forms and daughter forms are not clear or consistent, the transformer model may not perform well.

## Foundational Learning

- Concept: Comparative method in historical linguistics
  - Why needed here: Understanding the steps and goals of the comparative method helps in framing the proto-language reconstruction task and evaluating the model's performance.
  - Quick check question: What are the major steps of the comparative method, and how do they relate to the computational approaches described in the paper?

- Concept: Variational Autoencoder (VAE)
  - Why needed here: The VAE structure is a key component of the proposed model, and understanding its principles and how it differs from standard autoencoders is crucial for grasping the model's architecture and motivation.
  - Quick check question: How does a VAE differ from a standard autoencoder, and what is the role of the reparameterization trick in training a VAE?

- Concept: Neural Machine Translation (NMT)
  - Why needed here: The paper modifies an NMT model for the proto-language reconstruction task, so understanding the basics of NMT and how it can be adapted to this task is important.
  - Quick check question: How can the encoder-decoder architecture of an NMT model be modified to handle the input and output formats of the proto-language reconstruction task?

## Architecture Onboarding

- Component map:
  - Daughter forms → Transformer encoder → Latent space → Transformer decoder → Proto-form
  - VAE component (daughter decoder) works in parallel to enforce Neogrammarian hypothesis

- Critical path: Daughter forms → Transformer encoder → Latent space → Transformer decoder → Proto-form
  - The VAE component (daughter decoder) works in parallel to enforce the Neogrammarian hypothesis.

- Design tradeoffs:
  - Model size: Balancing expressive power and overfitting due to the limited size of the dataset.
  - Teacher forcing ratio: Disabling teacher forcing with some probability prevents the task from becoming too easy, especially for short Sinitic words.
  - Warmup epochs: Tuning the learning rate warmup to prevent performance degradation from too fast or too slow warmup.

- Failure signatures:
  - High variance in model performance across runs: Indicates sensitivity to hyperparameters or insufficient regularization.
  - Poor reconstruction of daughter forms by the daughter decoder: Suggests the latent space is not meaningful or regular enough.
  - Model overemphasizing languages with more data: Implies the model is not effectively leveraging all available daughter languages.

- First 3 experiments:
  1. Train the base Transformer model on the WikiHan dataset and evaluate its performance using edit distance and accuracy metrics.
  2. Implement the VAE structure by adding a daughter decoder and attention average block to the Transformer model, and train it on the WikiHan dataset.
  3. Experiment with different data augmentation methods (CNN vs. transformer-based reflex prediction) and evaluate their impact on the model's performance and training stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the VAE-Transformer architecture consistently outperform the baseline Transformer model across different proto-language reconstruction datasets with varying sequence lengths?
- Basis in paper: [explicit] The paper notes that the Transformer model outperforms RNN on datasets with long concatenated sequences (romance dataset, 39 Chinese topolects) but not on WikiHan with shorter sequences. The authors suggest more rigorous analysis is needed on performance across different datasets and sequence lengths.
- Why unresolved: The study only evaluated on WikiHan dataset. Performance comparison across datasets with different characteristics (sequence lengths, language families) was not conducted.
- What evidence would resolve it: Systematic evaluation of VAE-Transformer and baseline models across multiple proto-language reconstruction datasets with varying sequence lengths and language family structures.

### Open Question 2
- Question: Why does adding a VAE structure to the RNN model (Meloni et al., 2021) result in worse performance than the baseline RNN?
- Basis in paper: [explicit] The authors conducted preliminary experiments adding VAE to the RNN model but found it performed worse than the baseline RNN. They state "A more thorough analysis of the underlying causes is required."
- Why unresolved: The authors only mention preliminary experiments were conducted without detailing the experimental setup or potential causes for the performance degradation.
- What evidence would resolve it: Detailed ablation studies comparing different VAE configurations with RNN, analysis of latent space quality, and investigation of how VAE affects the RNN's ability to handle short sequences.

### Open Question 3
- Question: Can the model architecture be improved to better utilize all daughter languages rather than relying heavily on languages with more data (Cantonese, Hokkien, Mandarin)?
- Basis in paper: [explicit] Attention visualization revealed the model relies heavily on three languages with the least missing data, and the authors state "Trying to leverage all the daughter languages to produce the best reconstruction is left as future work."
- Why unresolved: The study only visualized attention weights and noted the imbalance but did not experiment with methods to encourage more balanced attention across all languages.
- What evidence would resolve it: Experimental results showing improved performance when implementing techniques to balance attention across languages (weighted attention, data augmentation for underrepresented languages, or architectural modifications).

## Limitations
- The model's heavy reliance on languages with more data (Cantonese, Hokkien, Mandarin) suggests poor utilization of underrepresented daughter languages
- The VAE mechanism's effectiveness is theoretically sound but lacks direct validation of latent space quality and Neogrammarian hypothesis enforcement
- The approach may not generalize well to non-Sinitic language families due to reliance on specific phonological patterns in the WikiHan dataset

## Confidence
- **High Confidence**: Baseline performance metrics (edit distance 0.883, accuracy 54.05%) are directly reported and reproducible. Data augmentation methodology is clearly specified and stabilizing effect on training is demonstrated.
- **Medium Confidence**: VAE structure's enforcement of Neogrammarian hypothesis is theoretically sound but lacks empirical validation. Transformer reflex predictor's superiority over CNN is demonstrated on WikiHan but may not generalize.
- **Low Confidence**: Generalizability to non-Sinitic language families is uncertain given heavy reliance on specific phonological patterns. Attention visualization provides suggestive but not conclusive evidence about language utilization.

## Next Checks
1. Perform t-SNE or PCA visualization of the VAE latent space to verify that it captures meaningful phonological patterns and that daughter decoder reconstructions align with expected sound change rules.

2. Independently evaluate the CNN and transformer reflex prediction models on a held-out test set to quantify their accuracy and identify failure patterns before using them for data augmentation.

3. Test the complete VAE-Transformer pipeline on a different proto-language reconstruction dataset (e.g., Indo-European languages) to assess whether the approach generalizes beyond the Sinitic family.