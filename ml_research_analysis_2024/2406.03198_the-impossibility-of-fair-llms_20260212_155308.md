---
ver: rpa2
title: The Impossibility of Fair LLMs
arxiv_id: '2406.03198'
source_url: https://arxiv.org/abs/2406.03198
tags:
- fairness
- such
- llms
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes whether technical fairness frameworks like group
  fairness and fair representations can be applied to general-purpose large language
  models (LLMs). It argues that due to the scale, flexibility, and multimodal nature
  of LLMs, inherent challenges make these frameworks intractable.
---

# The Impossibility of Fair LLMs

## Quick Facts
- arXiv ID: 2406.03198
- Source URL: https://arxiv.org/abs/2406.03198
- Reference count: 27
- The paper argues that technical fairness frameworks are inherently intractable for general-purpose LLMs due to their scale, flexibility, and multimodal nature.

## Executive Summary
This paper analyzes whether established technical fairness frameworks can be applied to large language models. The authors argue that due to the scale, flexibility, and multimodal nature of LLMs, inherent challenges make traditional fairness frameworks intractable. They examine four key areas where fairness becomes impossible: fairness through unawareness fails because unstructured training data contains pervasive sensitive attributes; producer-side fairness becomes obsolete as LLMs can consume and redistribute content at scale; general-purpose LLMs cannot be made fair across many contexts due to combinatorial explosion of populations and use cases; and fairness fails to compose when models are chained. The authors conclude that even if empirical challenges were addressed, these inherent limitations would persist.

## Method Summary
The paper conducts a conceptual analysis comparing the characteristics of general-purpose LLMs against the requirements and assumptions of established technical fairness frameworks. The authors systematically examine how LLM affordances (unstructured training data, content synthesis capabilities, multimodal flexibility, and general-purpose design) conflict with fairness framework requirements like group fairness, fair representations, fairness through unawareness, and multi-sided fairness. Rather than proposing new solutions, the paper demonstrates why existing approaches are fundamentally incompatible with LLM characteristics.

## Key Results
- Fairness through unawareness is impossible for LLMs due to pervasive sensitive attributes in unstructured training data
- Producer-side fairness becomes obsolete when LLMs can synthesize and redistribute content without attribution
- General-purpose LLMs cannot achieve fairness across diverse contexts due to combinatorial explosion of requirements
- Fairness metrics fail to compose when LLMs are chained or used in sequence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fairness through unawareness is impossible for LLMs because their training data is unstructured and contains pervasive sensitive attributes.
- Mechanism: The LLM training corpus includes natural language text that inherently encodes sensitive attributes through indirect cues (e.g., pronoun usage, dialect, cultural references). Unlike structured data where sensitive attributes can be explicitly removed, unstructured text requires removing or distorting meaningful content to achieve unawareness.
- Core assumption: Sensitive attributes in natural language are pervasive and cannot be cleanly separated from non-sensitive content without information loss.
- Evidence anchors:
  - [section] "LLMs are trained on massive amounts of unstructured data, primarily natural language... By design, LLMs are trained on massive amounts of unstructured data... Efforts to remove sensitive attributes can produce incoherence or distortion."
  - [abstract] "fairness through unawareness of sensitive attributes is made impossible by the unstructured training data and limited transparency of LLMs"
- Break condition: If LLMs could be trained on truly anonymized, attribute-free synthetic data or if advanced detection/removal techniques could extract sensitive information without content loss.

### Mechanism 2
- Claim: Producer-side fairness criteria become obsolete when LLMs can consume and redistribute content at scale without directing users to original sources.
- Mechanism: Traditional producer fairness metrics assume users access original content producers (e.g., websites, creators). LLMs can synthesize information from multiple sources and present consolidated answers, eliminating direct exposure to individual producers and their content.
- Core assumption: LLM systems will increasingly function as information intermediaries that synthesize rather than direct users to original sources.
- Evidence anchors:
  - [section] "However, if users search for information via the LLM system... then LLMs can entirely circumvent the producers and upend the conventional notion of producer-side fairness."
  - [abstract] "Standards for the fair treatment of content producers can be rendered obsolete by the LLM capacity for large-scale consumption and redistribution of content"
- Break condition: If LLM systems maintain attribution mechanisms or if regulatory frameworks mandate content source visibility.

### Mechanism 3
- Claim: General-purpose LLMs cannot be made fair across many contexts due to combinatorial explosion of populations, use cases, and sensitive attributes.
- Mechanism: Each combination of population, use case, and sensitive attribute may require different fairness metrics. The number of required fairness adjustments grows combinatorially, making comprehensive fairness infeasible while maintaining model utility.
- Core assumption: Fairness requirements vary meaningfully across different contexts and cannot be unified into a single framework.
- Evidence anchors:
  - [section] "General-purpose LLMs cannot be made fair across many contexts because of the combinations of populations, use cases, and other factors that impose different fairness requirements."
  - [abstract] "general-purpose LLMs cannot be made fair across many contexts because of the combinations of populations, use cases, and other factors that impose different fairness requirements"
- Break condition: If context-adaptive fairness mechanisms could dynamically adjust fairness criteria based on deployment context.

## Foundational Learning

- Concept: Group fairness metrics and their mathematical definitions
  - Why needed here: The paper critiques specific fairness frameworks like demographic parity, equalized odds, and calibration. Understanding these metrics is essential to grasp why they fail for LLMs.
  - Quick check question: What is the mathematical difference between demographic parity and equalized odds in binary classification?

- Concept: Fairness through unawareness and its limitations
  - Why needed here: This framework is explicitly discussed as impossible for LLMs due to unstructured training data. Understanding its assumptions and implementation helps explain why LLMs cannot achieve it.
  - Quick check question: How does fairness through unawareness typically work in structured data settings, and why does this approach fail with natural language?

- Concept: Multistakeholder fairness in recommender systems
  - Why needed here: The paper extends producer-side fairness concepts from recommender systems to LLMs, explaining how LLMs disrupt traditional fairness assumptions for content creators.
  - Quick check question: What are the three main stakeholder groups in multistakeholder fairness frameworks, and how do their fairness requirements typically conflict?

## Architecture Onboarding

- Component map:
  - Training data pipeline (unstructured text collection, preprocessing, tokenization)
  - Model architecture (transformer-based LLM with attention mechanisms)
  - Context adaptation layer (instruction tuning, RLHF, etc.)
  - Evaluation framework (fairness metrics, context-specific testing)
  - Deployment interface (chat, API, integration with other systems)

- Critical path: Data → Model training → Context adaptation → Evaluation → Deployment
  - Each stage can introduce fairness challenges that compound across the pipeline

- Design tradeoffs:
  - Transparency vs. performance: More transparent training data improves fairness evaluation but may reduce competitive advantage
  - General-purpose vs. specialized: More general models are harder to make fair across contexts
  - Static vs. dynamic fairness: Pre-defined fairness metrics vs. context-adaptive approaches

- Failure signatures:
  - Systematic bias amplification across different user populations
  - Context-dependent fairness violations (fair in one domain, biased in another)
  - Inability to audit fairness due to training data opacity

- First 3 experiments:
  1. Test fairness through unawareness by attempting to remove sensitive attributes from LLM training data and measuring content distortion vs. fairness improvement
  2. Evaluate producer-side fairness by comparing LLM outputs with and without attribution mechanisms to original content sources
  3. Assess context-dependent fairness by deploying the same LLM across different use cases and measuring fairness metric variations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM developers create scalable evaluation methods that account for the vast number of combinations of populations, use cases, and sensitive attributes?
- Basis in paper: [explicit] The paper discusses the combinatorial explosion of contexts and the need for scalable evaluation in Section 4.3 and 5.3.
- Why unresolved: The paper acknowledges the challenge but does not provide a concrete solution, stating that "any realistic amount of effort is insufficient."
- What evidence would resolve it: Development and demonstration of a scalable evaluation framework that can efficiently test fairness across diverse contexts and sensitive attributes.

### Open Question 2
- Question: To what extent can AI-assisted evaluation methods, such as using LLMs to generate or validate fairness tests, mitigate bias without introducing new biases?
- Basis in paper: [explicit] The paper discusses the potential and risks of AI-assisted evaluation in Section 5.3.
- Why unresolved: The paper highlights the risk of "bias all the way down" but does not provide empirical evidence on the effectiveness of AI-assisted methods.
- What evidence would resolve it: Empirical studies comparing the performance and bias of AI-assisted evaluation methods against human-led evaluations.

### Open Question 3
- Question: What specific standards of developer responsibility can ensure transparency and accountability in LLM development, particularly regarding training data and model behavior?
- Basis in paper: [explicit] The paper emphasizes the need for developer responsibility in Section 5.1.
- Why unresolved: The paper calls for standards but does not specify what they should entail or how they can be enforced.
- What evidence would resolve it: Case studies or policy proposals outlining specific standards and their implementation in real-world LLM development.

## Limitations

- The analysis relies primarily on theoretical arguments rather than empirical validation of fairness framework intractability
- The paper focuses on technical aspects while not deeply exploring social and political dimensions of fairness
- Limited consideration of how hybrid approaches might partially address fairness challenges

## Confidence

- High confidence: The impossibility of fairness through unawareness for LLMs (supported by multiple examples of sensitive attribute pervasiveness in natural language)
- Medium confidence: The obsolescence of producer-side fairness due to LLM content synthesis capabilities (the argument is logical but depends on future LLM development trajectories)
- Medium confidence: The combinatorial explosion of fairness requirements across contexts (mathematically sound but assumes no unifying fairness framework can emerge)

## Next Checks

1. Conduct controlled experiments measuring content distortion when attempting to remove sensitive attributes from LLM training data, quantifying the tradeoff between fairness improvement and information loss.

2. Implement and test attribution mechanisms in LLM outputs to empirically evaluate whether producer-side fairness can be preserved despite content synthesis capabilities.

3. Design and run cross-context fairness evaluations comparing the same LLM across multiple deployment scenarios to measure the actual variation in fairness metric performance.