---
ver: rpa2
title: 'Gemma 2: Improving Open Language Models at a Practical Size'
arxiv_id: '2408.00118'
source_url: https://arxiv.org/abs/2408.00118
tags:
- gemma
- language
- arxiv
- table
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Gemma 2, a new family of open language models
  ranging from 2B to 27B parameters. The authors apply architectural modifications
  such as interleaving local-global attention and group-query attention, and train
  2B and 9B models with knowledge distillation from a larger model instead of next
  token prediction.
---

# Gemma 2: Improving Open Language Models at a Practical Size

## Quick Facts
- arXiv ID: 2408.00118
- Source URL: https://arxiv.org/abs/2408.00118
- Reference count: 28
- Primary result: Gemma 2 models achieve state-of-the-art performance for their size, with the 27B model competitive with models 2.5× larger

## Executive Summary
Gemma 2 introduces a new family of open language models (2B, 9B, 27B parameters) that achieve state-of-the-art performance for their size through architectural innovations. The models use interleaving local-global attention and group-query attention, with smaller models trained via knowledge distillation from larger models rather than next-token prediction. The 27B model outperforms similar-sized open models and remains competitive with models 2-3× larger despite using 2/3 less training data. Gemma 2 models also demonstrate improved safety metrics and lower memorization rates compared to previous versions.

## Method Summary
Gemma 2 employs a transformer architecture with alternating local (4096-token sliding window) and global (8192-token span) attention layers, combined with group-query attention (2 groups) for improved inference efficiency. The 2B and 9B models are trained using knowledge distillation from larger models rather than standard next-token prediction, while the 27B model is trained from scratch. Models are trained on primarily English data ranging from 2T to 13T tokens, using the same tokenizer as Gemma 1 and Gemini (256k vocabulary). The training infrastructure leverages TPUv5p with extensive parallelization (6144 chips, 768-way data replication, 8-way model sharding).

## Key Results
- Gemma 2 27B outperforms similar-sized open models and is only a few percent below models 2.5× larger
- Gemma 2 9B shows up to 10% gains over previous versions on some benchmarks
- Distilled 2B and 9B models outperform from-scratch training on the same data volume
- Models demonstrate lower memorization rates and better safety metrics compared to prior versions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaving local and global attention layers reduces computational cost while maintaining context understanding.
- Mechanism: Local sliding window attention (4096 tokens) handles fine-grained, nearby dependencies; global attention (8192 tokens) captures long-range relationships. Alternating these in every other layer balances efficiency and expressiveness.
- Core assumption: The dataset contains both local and global dependencies that benefit from different attention strategies.
- Evidence anchors:
  - [abstract] Mentions "interleaving local-global attentions" as a key modification.
  - [section] Details sliding window size of 4096 tokens and global span of 8192 tokens.
- Break condition: If the data lacks long-range dependencies, global attention becomes unnecessary overhead.

### Mechanism 2
- Claim: Grouped-Query Attention (GQA) speeds inference without sacrificing accuracy.
- Mechanism: GQA reduces the number of key-value heads compared to multi-head attention, decreasing memory usage and computation during inference while preserving model performance.
- Core assumption: Fewer KV heads still capture sufficient information for the task.
- Evidence anchors:
  - [abstract] States GQA is used with num_groups = 2.
  - [section] Reports ablation comparing GQA vs MHA with similar performance but faster inference.
- Break condition: If tasks require highly diverse attention patterns, fewer KV heads may lose critical information.

### Mechanism 3
- Claim: Knowledge distillation from a larger model improves small model performance beyond what scaling training data alone achieves.
- Mechanism: Instead of predicting next tokens from raw data, small models learn from the softened probability distribution output by a larger teacher model, providing richer gradients.
- Core assumption: The teacher model has learned useful patterns that transfer to smaller models.
- Evidence anchors:
  - [abstract] Explains distillation is used instead of next token prediction for 2B and 9B models.
  - [section] Shows distilled 2B model outperforms from-scratch training on 500B tokens.
- Break condition: If the teacher model is poor or misaligned, distillation could propagate errors.

## Foundational Learning

- Concept: Transformer architecture fundamentals (self-attention, feed-forward layers, layer norms).
  - Why needed here: Gemma 2 builds directly on the decoder-only transformer; understanding its components is essential for grasping the architectural changes.
  - Quick check question: What is the role of multi-head attention in a transformer decoder?
- Concept: Knowledge distillation in machine learning.
  - Why needed here: The paper trains smaller models using outputs from a larger teacher model rather than raw next-token prediction.
  - Quick check question: How does distillation differ from standard supervised learning in terms of target labels?
- Concept: Attention mechanisms and their variants (local, global, sliding window).
  - Why needed here: The paper modifies the attention layers to improve efficiency and performance.
  - Quick check question: What is the computational advantage of using local sliding window attention over full self-attention?

## Architecture Onboarding

- Component map: Embedding layer → Alternating local/global attention blocks → Feed-forward layers → RMSNorm → Output projection
- Critical path: Data flows through embeddings, attention layers (alternating local/global), feed-forward, normalization, and output projection
- Design tradeoffs: Local attention reduces computation but may miss long-range context; global attention captures it but is costlier. GQA speeds inference but may reduce modeling flexibility.
- Failure signatures: Degraded performance on tasks requiring long-range reasoning (if global attention is insufficient); slow inference (if GQA harms expressiveness).
- First 3 experiments:
  1. Train a small model with only local attention; compare perplexity on short vs. long sequences.
  2. Replace GQA with standard MHA in a 9B model; measure inference speed and benchmark scores.
  3. Train a 2B model with and without distillation from a 7B teacher; evaluate on MMLU.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Gemma 2 models change when trained on languages other than English?
- Basis in paper: [inferred] The paper mentions that the tokenizer is designed to work on a large number of languages, but the models are not specifically trained for multilingual capabilities.
- Why unresolved: The paper focuses on English data for training, and the impact of multilingual training is not explored.
- What evidence would resolve it: Training and evaluating Gemma 2 models on multilingual datasets and comparing their performance to English-only training.

### Open Question 2
- Question: What is the optimal balance between model size and training data for achieving the best performance in Gemma 2 models?
- Basis in paper: [explicit] The paper discusses training models of different sizes with varying amounts of data, but the optimal balance is not explicitly stated.
- Why unresolved: The paper provides performance metrics for specific model sizes and data amounts, but does not explore the full range of possibilities.
- What evidence would resolve it: Conducting a systematic study of model performance across a wide range of model sizes and training data amounts.

### Open Question 3
- Question: How do Gemma 2 models perform on tasks requiring long-term memory or reasoning over extended contexts?
- Basis in paper: [inferred] The paper mentions the use of a context length of 8192 tokens, but does not explore the impact of longer contexts on performance.
- Why unresolved: The paper focuses on short-to-medium range tasks, and the impact of longer contexts is not investigated.
- What evidence would resolve it: Evaluating Gemma 2 models on tasks that require reasoning over long contexts, such as multi-document question answering or story generation.

## Limitations

- Architectural effectiveness at scale remains uncertain beyond 27B parameters
- Limited ablation studies on distillation techniques and hyperparameters
- Lack of transparency about dataset composition and filtering methods

## Confidence

**High Confidence Claims**:
- The Gemma 2 models demonstrate state-of-the-art performance for their parameter counts (2B, 9B, 27B).
- The architectural modifications (local-global attention interleaving, GQA) are implemented as described and contribute to efficiency gains.
- The models show measurable improvements over Gemma 1.0 in benchmarks and safety metrics.

**Medium Confidence Claims**:
- The 27B model's performance relative to models 2.5× larger is genuinely competitive rather than an artifact of dataset differences.
- Knowledge distillation provides consistent benefits across different model sizes.
- The claimed reduction in memorization rates is both meaningful and directly attributable to the architectural changes.

**Low Confidence Claims**:
- The specific mechanism by which interleaved attention improves performance is fully understood and optimized.
- The safety improvements generalize robustly across all deployment scenarios.
- The architectural choices would maintain their effectiveness if scaled to significantly larger models.

## Next Checks

1. **Ablation Study on Distillation**: Train an additional 2B model using the same training data but with next-token prediction instead of distillation, ensuring identical hyperparameters except for the training objective. Compare performance on MMLU, BIG-bench Hard, and human preference evaluations to quantify the exact contribution of distillation versus architectural improvements.

2. **Attention Pattern Analysis**: Extract and visualize the attention patterns from the 27B model on sequences requiring both local (syntactic dependencies) and global (coreference, long-form reasoning) understanding. Compare these patterns against a baseline transformer with uniform attention to empirically validate that the interleaving strategy captures the intended local-global balance.

3. **Cross-Dataset Generalization Test**: Evaluate Gemma 2 models on non-English benchmarks and specialized domains (medical, legal, code) not represented in the training data. Measure performance degradation relative to models trained on more diverse datasets to assess whether the reported gains are due to superior architecture or simply better-matched training data.