---
ver: rpa2
title: 'LLM Teacher-Student Framework for Text Classification With No Manually Annotated
  Data: A Case Study in IPTC News Topic Classification'
arxiv_id: '2411.19638'
source_url: https://arxiv.org/abs/2411.19638
tags:
- news
- topic
- data
- iptc
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an LLM-based teacher-student framework for
  developing multilingual news topic classification models without manually annotated
  data. The approach uses GPT-4o as a teacher model to automatically annotate news
  articles in four languages (Catalan, Croatian, Greek, Slovenian) with IPTC Media
  Topic labels.
---

# LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Study in IPTC News Topic Classification

## Quick Facts
- arXiv ID: 2411.19638
- Source URL: https://arxiv.org/abs/2411.19638
- Authors: Taja Kuzman; Nikola Ljubešić
- Reference count: 40
- Primary result: First multilingual IPTC Media Topic classifier trained without manual annotation

## Executive Summary
This paper introduces an innovative teacher-student framework that enables development of high-quality multilingual text classification models without requiring any manually annotated training data. The approach leverages GPT-4o as a teacher model to automatically annotate news articles in four languages (Catalan, Croatian, Greek, Slovenian) with IPTC Media Topic labels. The resulting automatically labeled datasets are then used to fine-tune smaller XLM-RoBERTa student models, achieving performance comparable to the teacher model while being orders of magnitude smaller and faster to run.

The framework demonstrates that LLM-generated annotations can achieve quality comparable to human annotators, with agreement rates between GPT-4o and human annotators matching inter-annotator agreement levels. The resulting multilingual student model achieves micro-F1 of 0.734 and macro-F1 of 0.746, and has been openly published on Hugging Face, providing the first available multilingual IPTC Media Topic classifier. The study shows that this approach enables rapid development of high-quality news topic classifiers across multiple languages without the traditional bottleneck of manual annotation.

## Method Summary
The teacher-student framework operates in two phases: first, GPT-4o serves as a teacher model to automatically annotate news articles with IPTC Media Topic labels across four languages. The authors validate this annotation quality by comparing GPT-4o's labels against human annotations on validation sets, finding agreement rates comparable to inter-human agreement. In the second phase, these automatically annotated datasets are used to fine-tune smaller XLM-RoBERTa student models. The framework is evaluated across different training set sizes (from 1,000 to 15,000 instances) and compared against zero-shot teacher performance. Cross-lingual transfer capabilities are also assessed by testing models on languages not included in training.

## Key Results
- GPT-4o achieves strong zero-shot performance across all languages (average macro-F1 0.731, micro-F1 0.722)
- Student models achieve performance comparable to teacher model when fine-tuned on 15,000+ instances
- Even with only 1,000 training instances, student models achieve strong results (micro-F1 0.678-0.704, macro-F1 0.692-0.716)
- Multilingual student model outperforms language-specific models, achieving micro-F1 0.734 and macro-F1 0.746
- Strong zero-shot cross-lingual capabilities demonstrated across all languages

## Why This Works (Mechanism)
The framework works because large language models like GPT-4o possess strong zero-shot classification capabilities when provided with appropriate task instructions and examples. The teacher-student approach leverages this capability to generate high-quality annotations that capture complex relationships between text and labels without requiring human labeling effort. By using the same LLM to generate training data and as a performance benchmark, the approach creates a self-consistent evaluation framework. The automatic annotation process scales efficiently across multiple languages, enabling development of multilingual models that benefit from shared linguistic patterns across languages.

## Foundational Learning
- **IPTC Media Topics taxonomy**: Why needed - Provides standardized news topic classification schema; Quick check - 25 top-level categories covering news domains
- **Zero-shot classification**: Why needed - Enables annotation without training data; Quick check - GPT-4o achieves 0.731 macro-F1 without examples
- **Teacher-student learning**: Why needed - Transfers knowledge from large to small models; Quick check - Student models match teacher performance at scale
- **Cross-lingual transfer**: Why needed - Enables models to work on unseen languages; Quick check - Zero-shot performance on all test languages
- **Automatic data annotation**: Why needed - Eliminates manual labeling bottleneck; Quick check - 6,000+ articles annotated automatically
- **Fine-tuning XLM-RoBERTa**: Why needed - Adapts multilingual model to specific task; Quick check - Best model achieves 0.734 micro-F1

## Architecture Onboarding

**Component Map:**
GPT-4o (teacher) -> Automatic annotation generation -> XLM-RoBERTa fine-tuning -> Student model deployment

**Critical Path:**
1. GPT-4o annotates raw news articles with IPTC labels
2. Annotated data used to fine-tune XLM-RoBERTa models
3. Fine-tuned models evaluated on held-out test sets
4. Cross-lingual zero-shot performance assessed

**Design Tradeoffs:**
- Large teacher model (GPT-4o) vs. small student models (XLM-RoBERTa) - balances annotation quality with deployment efficiency
- Automatic vs. manual annotation - trades potential human oversight for scalability and cost-effectiveness
- Multilingual vs. monolingual models - multilingual approach shows better performance but may be more complex to train
- Zero-shot vs. fine-tuned performance - framework aims to close the gap between these approaches

**Failure Signatures:**
- Poor teacher annotation quality would propagate to student models
- Limited cross-lingual transfer would indicate poor multilingual model training
- Performance plateau below teacher level would suggest annotation quality issues
- High variance across languages would indicate uneven annotation quality

**First 3 Experiments:**
1. Evaluate GPT-4o zero-shot performance on each language individually
2. Fine-tune XLM-RoBERTa on 1,000 annotated instances per language
3. Test cross-lingual zero-shot transfer on languages not in training set

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on GPT-4o annotations without independent human validation on test sets
- Limited to four languages and one specific news domain (IPTC Media Topics)
- Framework effectiveness demonstrated primarily through internal comparisons rather than external benchmarks

## Confidence
- **High Confidence**: GPT-4o's effectiveness as teacher model for zero-shot classification
- **Medium Confidence**: Student model performance when fine-tuned on automatically labeled data
- **Medium Confidence**: Cross-lingual zero-shot transfer capabilities

## Next Checks
1. Conduct blind human annotation validation on final test sets across all four languages
2. Test framework on non-news domains and languages from different families
3. Implement ablation studies comparing teacher-student approach against traditional few-shot learning