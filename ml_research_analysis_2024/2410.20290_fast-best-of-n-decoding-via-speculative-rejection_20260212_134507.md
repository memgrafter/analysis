---
ver: rpa2
title: Fast Best-of-N Decoding via Speculative Rejection
arxiv_id: '2410.20290'
source_url: https://arxiv.org/abs/2410.20290
tags:
- arxiv
- rejection
- reward
- should
- best-of-n
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPECULATIVE REJECTION, a decoding strategy
  designed to maximize a given metric of interest, such as reward or probability,
  in large language models. The core idea is to dynamically adjust the batch size
  during auto-regressive generation by using a reward model to infer which responses
  are unlikely to achieve high scores, allowing early termination of unpromising responses.
---

# Fast Best-of-N Decoding via Speculative Rejection

## Quick Facts
- arXiv ID: 2410.20290
- Source URL: https://arxiv.org/abs/2410.20290
- Reference count: 40
- One-line primary result: Achieves higher reward scores with 16-32x fewer GPUs compared to Best-of-N while maintaining similar latency

## Executive Summary
This paper introduces SPECULATIVE REJECTION, a decoding strategy designed to maximize reward metrics in large language models. The method dynamically adjusts batch size during auto-regressive generation by using a reward model to identify and terminate low-quality responses early. By leveraging the positive correlation between partial and final rewards, the algorithm can generate fewer total tokens while achieving better or comparable reward scores. Empirically, SPECULATIVE REJECTION demonstrates significant computational efficiency, requiring substantially fewer GPUs than traditional Best-of-N approaches while maintaining similar latency and generating responses with lower perplexity.

## Method Summary
SPECULATIVE REJECTION operates by generating multiple responses in parallel during auto-regressive decoding, then using a reward model to evaluate partial responses at a decision token. If a response's partial reward falls below a threshold quantile, it is terminated early to save computation. The algorithm starts with a large initial batch size that would normally cause memory overflow, then dynamically reduces the batch size by rejecting low-scoring responses when memory pressure builds. This approach achieves computational efficiency through reduced token generation while maintaining or improving reward scores through selective continuation of promising responses.

## Key Results
- Achieves higher reward scores with 16-32x fewer GPUs compared to Best-of-N
- Maintains similar latency while requiring significantly less computational resources
- Consistently generates responses with lower perplexity than baseline methods
- Demonstrates effectiveness across multiple model pairs and reward model combinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Speculative Rejection leverages positive correlation between partial and final rewards to identify low-quality responses early
- Mechanism: During auto-regressive generation, the algorithm evaluates partial responses at a decision token using a reward model. If the partial reward falls below a threshold quantile, the response is terminated early
- Core assumption: The reward model can reliably distinguish high-quality from low-quality responses based on partial generations
- Evidence anchors: [abstract] "the scores of partial utterances are positively correlated to the scores of full utterances"
- Break condition: The correlation breaks when partial responses mislead the reward model, causing premature termination of high-quality responses

### Mechanism 2
- Claim: Speculative Rejection maximizes GPU memory utilization by dynamically adjusting batch size
- Mechanism: The algorithm starts with a large initial batch size that would normally cause OOM, then reduces the batch size by rejecting low-scoring responses when memory pressure builds
- Core assumption: GPU memory consumption is linear with the number of active responses and token count
- Evidence anchors: [abstract] "S PECULATIVE REJECTION dynamically reduces the batch size during generation, preventing memory exhaustion"
- Break condition: The mechanism breaks when memory consumption patterns deviate from assumptions, such as non-linear KV cache growth

### Mechanism 3
- Claim: Speculative Rejection achieves computational efficiency through reduced token generation
- Mechanism: By terminating low-quality responses early, the algorithm generates fewer total tokens while maintaining or improving reward scores
- Core assumption: Terminating unpromising responses saves more computation than the overhead of reward model evaluation
- Evidence anchors: [abstract] "S PECULATIVE REJECTION achieves higher reward scores with fewer computational resources compared to Best-of-N, requiring 16 to 32 times fewer GPUs"
- Break condition: The efficiency gains disappear when reward model evaluation overhead exceeds saved computation from early termination

## Foundational Learning

- Concept: Auto-regressive generation
  - Why needed here: The algorithm relies on token-by-token generation where partial responses can be evaluated and decisions made about continuation
  - Quick check question: What happens if the EOS token is reached before the decision token in Speculative Rejection?

- Concept: Reward model evaluation
  - Why needed here: The algorithm uses a reward model to score partial responses and make rejection decisions
  - Quick check question: How does the reward model handle incomplete responses versus complete ones?

- Concept: Quantile-based thresholding
  - Why needed here: The algorithm uses quantile thresholds to determine which responses to reject based on their partial reward scores
  - Quick check question: What happens if all responses have similar partial reward scores?

## Architecture Onboarding

- Component map: Generator (LLM) -> Reward Model -> Memory Manager -> Rejection Controller
- Critical path: Generate -> Score -> Decide -> Continue/Terminate -> Output
- Design tradeoffs: Aggressive rejection (higher speedup, lower quality) vs conservative rejection (lower speedup, higher quality)
- Failure signatures: OOM crashes, reward degradation, latency spikes
- First 3 experiments:
  1. Baseline: Run Best-of-N with N=120 to establish memory limits
  2. Correlation test: Generate responses and plot partial vs final rewards to verify assumption
  3. Rejection rate sweep: Test different α values to find optimal tradeoff between speedup and quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Speculative Rejection vary with different reward model architectures (e.g., small vs. large models, different training objectives)?
- Basis in paper: [explicit] The paper mentions using different reward models but does not systematically explore the impact of reward model architecture on performance
- Why unresolved: The experiments use a fixed set of reward models without analyzing how performance changes with different architectures
- What evidence would resolve it: Systematic experiments varying reward model architecture and measuring performance across these variations

### Open Question 2
- Question: Can Speculative Rejection be extended to handle multimodal prompts or other non-textual reward functions?
- Basis in paper: [inferred] The paper focuses on text-based prompts and reward models that evaluate text responses
- Why unresolved: The current algorithm is designed for text-based generation and evaluation
- What evidence would resolve it: Experiments demonstrating performance on multimodal prompts or with non-textual reward functions

### Open Question 3
- Question: What is the optimal strategy for setting the rejection rate (α) dynamically during generation, based on prompt characteristics or reward model behavior?
- Basis in paper: [explicit] The paper discusses the effect of the rejection rate α but uses a fixed value for each experiment
- Why unresolved: The paper uses a fixed rejection rate without investigating dynamic adjustment strategies
- What evidence would resolve it: A method for dynamically adjusting α during generation along with experiments showing improved performance

## Limitations

- The method relies on the correlation between partial and final rewards, which is not perfect and can break down
- Memory optimization depends on specific GPU memory characteristics that are not fully specified
- Empirical validation is limited to specific model pairs without establishing broad generality across different architectures and domains

## Confidence

- **High confidence**: The computational efficiency claims (fewer GPUs needed) - these are directly measurable and the methodology is sound
- **Medium confidence**: The reward maximization claims - while supported by experiments, the mechanism's reliability depends on the partial-final reward correlation which has acknowledged limitations
- **Low confidence**: The generality of the method across different model pairs and domains - limited empirical validation beyond the tested configurations

## Next Checks

1. **Correlation Robustness Test**: Systematically measure the partial-final reward correlation across diverse model pairs (different base models, different reward models, different task domains) to identify conditions under which the correlation breaks down. This should include both in-distribution and out-of-distribution test cases.

2. **Memory Consumption Validation**: Conduct controlled experiments to verify the claimed memory efficiency by precisely measuring GPU memory usage across different batch sizes and response lengths, comparing against theoretical predictions based on KV cache scaling.

3. **Rejection Rate Sensitivity Analysis**: Perform a comprehensive sweep of the rejection threshold parameter α across multiple tasks to characterize the tradeoff frontier between computational speedup and quality degradation, identifying optimal operating points for different use cases.