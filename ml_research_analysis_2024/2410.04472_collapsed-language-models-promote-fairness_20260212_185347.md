---
ver: rpa2
title: Collapsed Language Models Promote Fairness
arxiv_id: '2410.04472'
source_url: https://arxiv.org/abs/2410.04472
tags:
- language
- arxiv
- preprint
- fairness
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores the connection between neural collapse and fairness
  in language models. The authors observe that debiased language models exhibit more
  collapsed token representations for gender-related words, indicated by better alignment
  between classifiers (word embeddings) and class means (token representations).
---

# Collapsed Language Models Promote Fairness

## Quick Facts
- arXiv ID: 2410.04472
- Source URL: https://arxiv.org/abs/2410.04472
- Reference count: 40
- Primary result: Neural collapse regularization improves fairness metrics by up to 1.83 ICAT score on StereoSet while preserving language modeling performance

## Executive Summary
This paper establishes a novel connection between neural collapse in deep networks and fairness in language models. The authors observe that debiased language models naturally exhibit more collapsed token representations for gender-related words, with better alignment between word embeddings and token representations. Based on this observation, they propose a principled regularization method that explicitly enforces neural collapse by minimizing the standard deviation of cosine similarities between word embeddings and token representations for gender-related words. The method is simple, agnostic to fine-tuning approaches, and consistently improves fairness across multiple debiasing baselines while maintaining general language modeling performance.

## Method Summary
The method introduces a regularization term (U)N C3 that minimizes the standard deviation of cosine similarities between word embeddings wc and class means µc for gender-related words. During fine-tuning, this regularization is added to the standard language model loss function, encouraging more collapsed alignments specifically for gender-sensitive vocabulary. The approach leverages the observation that debiased models naturally exhibit such collapsed patterns and makes this behavior explicit through regularization. The method is universally applicable across different debiasing approaches and model architectures.

## Key Results
- Up to 1.83 improvement in ICAT score on StereoSet
- Consistent reductions in gender bias across WinoBias and Bias-in-Bios benchmarks
- Preservation of language modeling performance on GLUE tasks
- Improved fairness metrics (BEC-Pro association score differences) while maintaining general performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Collapsed representations reduce bias by enforcing uniform similarity between word embeddings and token representations for gender-related words
- Mechanism: The regularization term LN C3 minimizes the standard deviation of cosine similarities between word embeddings wc and class means µc, creating more collapsed alignments that correlate with reduced bias
- Core assumption: More collapsed alignments between token representations and word embeddings directly indicate reduced bias in language models
- Evidence anchors:
  - [abstract] "debiased language models exhibit collapsed alignment between token representations and word embeddings"
  - [section 3.2.1] "debiased language models exhibit neural collapse in certain perspectives: N C3 is consistently improved (minimized) in debiased models"
  - [corpus] Weak evidence - no direct corpus support found for collapsed alignments reducing bias

### Mechanism 2
- Claim: The regularization is effective because it operates on the final classification layer where neural collapse naturally occurs
- Mechanism: Language models perform next-token prediction as classification; the word embedding layer acts as classifier weights, making neural collapse metrics applicable for debiasing
- Core assumption: Language model pretraining and fine-tuning are essentially classification tasks where neural collapse phenomena apply
- Evidence anchors:
  - [section 3.1] "the pretraining and fine-tuning of language model are actually classification tasks, and the word embedding layer E = {wc ∈ Rd|c = 1, · · · , C} is typically the classifier"
  - [section 4] "Our method is simple, principled, and is agnostic to any pretraining or fine-tuning methods for fairness"
  - [corpus] No corpus evidence found supporting classification framework for language models

### Mechanism 3
- Claim: The method preserves model performance because it only regularizes on gender-related vocabulary subsets
- Mechanism: By limiting regularization to Vgender (gender-related words), the method avoids disrupting general language modeling capabilities while targeting bias
- Core assumption: Gender-related words are a small enough subset that regularization won't interfere with general language understanding
- Evidence anchors:
  - [section 4.2] "fine-tuning with Eq. 3 preserves the performance of language models on general tasks with minimal difference from each baseline model"
  - [section 3.3] "we study the impact of different choices of fairness-sensitive words on neural collapse"
  - [corpus] No corpus evidence found about subset regularization preserving performance

## Foundational Learning

- Concept: Neural collapse in deep networks
  - Why needed here: The paper's entire approach is based on understanding and leveraging neural collapse behavior in language models
  - Quick check question: What are the four perspectives of neural collapse defined in the paper, and how do they differ?

- Concept: Language model classification framework
  - Why needed here: Understanding that language models perform next-token prediction as classification is crucial for applying neural collapse metrics
  - Quick check question: How does the paper reinterpret language model training as a classification problem?

- Concept: Fairness metrics in NLP
  - Why needed here: The paper evaluates debiasing effectiveness using both intrinsic metrics (StereoSet, BEC-Pro) and extrinsic metrics (WinoBias, Bias-in-Bios)
  - Quick check question: What is the difference between intrinsic and extrinsic fairness metrics, and why are both needed?

## Architecture Onboarding

- Component map: Word embedding layer (E) -> Penultimate layer representations (h) -> Neural collapse regularization term LN C3 -> Standard language model fine-tuning objectives

- Critical path:
  1. Compute class means µc for gender-related words
  2. Calculate standard deviation of cosine similarities between wc and µc
  3. Add LN C3 to existing loss function
  4. Fine-tune language model as normal

- Design tradeoffs:
  - Subset size vs. effectiveness: Smaller gender-related subsets minimize interference but may reduce debiasing impact
  - Regularization strength vs. performance: Higher α improves fairness but risks harming language modeling
  - Generic vs. tailored: Method works across multiple debiasing approaches but may not be optimal for any single one

- Failure signatures:
  - ICAT score decreases despite higher SS score
  - General language modeling performance drops significantly
  - No improvement in fairness metrics despite regularization
  - Inconsistent results across different debiasing baselines

- First 3 experiments:
  1. Apply (U)N C3 regularization to Mabel with α = 10 and measure StereoSet improvements
  2. Compare performance on gender-related words vs. random word subsets to validate subset selection
  3. Test different α values (1, 10, 30, 50) to find optimal tradeoff between fairness and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does neural collapse consistently predict debiasing effectiveness across different model architectures and training objectives?
- Basis in paper: [explicit] The paper observes that debiased models show more collapsed representations for gender words, but this pattern varies across metrics and datasets.
- Why unresolved: The paper only tests this relationship on BERT variants and specific debiasing methods, leaving open whether the connection holds for other architectures (like GPT, T5) or different training objectives (like contrastive learning, adversarial training).
- What evidence would resolve it: Systematic testing of neural collapse patterns across diverse model families, training paradigms, and fairness definitions would clarify whether this is a general principle or specific to the studied cases.

### Open Question 2
- Question: What is the theoretical mechanism linking neural collapse to fairness in language models?
- Basis in paper: [inferred] The paper observes correlations between neural collapse metrics and fairness outcomes but does not explain why this relationship exists.
- Why unresolved: While the empirical connection is established, the paper does not provide a causal explanation for why collapsed representations would lead to reduced bias, leaving open questions about whether this is a beneficial side effect or an artifact of specific training dynamics.
- What evidence would resolve it: Formal analysis of how collapsed representations affect the model's ability to encode and retrieve stereotypical associations, possibly through information-theoretic or geometric arguments about the embedding space.

### Open Question 3
- Question: How does neural collapse regularization interact with other forms of bias (e.g., intersectional, cultural, or domain-specific biases)?
- Basis in paper: [explicit] The paper focuses exclusively on gender-related words and binary gender distinctions.
- Why unresolved: The method is only evaluated on gender bias, leaving unclear whether enforcing neural collapse would similarly help with other types of bias that may have different structural properties in the embedding space.
- What evidence would resolve it: Testing the regularization method on debiasing tasks targeting race, age, disability, or intersectional identities would reveal whether neural collapse is a general fairness mechanism or specific to gender bias.

### Open Question 4
- Question: Does neural collapse in token representations trade off with other desirable properties like compositional generalization or reasoning capabilities?
- Basis in paper: [inferred] The paper shows that neural collapse regularization preserves standard NLU performance but does not examine more complex cognitive capabilities.
- Why unresolved: While basic language modeling performance is maintained, the paper does not investigate whether enforcing collapsed representations might impair the model's ability to handle compositional tasks, analogical reasoning, or abstract problem-solving.
- What evidence would resolve it: Comprehensive evaluation of models with and without neural collapse regularization on benchmarks for compositional generalization (like SCAN), reasoning (like BIG-bench), and abstract problem-solving would reveal potential trade-offs.

## Limitations

- The connection between neural collapse metrics and actual fairness outcomes remains the primary uncertainty
- The subset-based regularization approach may not capture all forms of bias beyond gender-related vocabulary
- The theoretical framework connecting neural collapse to fairness needs more rigorous validation

## Confidence

**High confidence**: The empirical results showing consistent improvements in fairness metrics (StereoSet, BEC-Pro, WinoBias, Bias-in-Bios) when applying the (U)N C3 regularization, and the preservation of general language modeling performance.

**Medium confidence**: The theoretical framework connecting neural collapse to fairness, and the assumption that language model fine-tuning can be treated as a standard classification task where neural collapse metrics apply meaningfully.

**Low confidence**: The long-term stability of the debiasing effects and whether the regularization might introduce unintended biases in other semantic dimensions not measured by the current evaluation suite.

## Next Checks

1. **Cross-model validation**: Apply the (U)N C3 regularization to additional language model architectures (e.g., RoBERTa, GPT-style models) beyond the current BERT and Mabel experiments to test generalizability across different pretraining objectives and architectures.

2. **Temporal stability analysis**: Measure the evolution of fairness metrics and N C3 values throughout the fine-tuning process, not just at convergence, to understand whether the collapsed alignments emerge gradually or require specific training dynamics.

3. **Ablation on vocabulary size**: Systematically vary the size of the gender-related vocabulary subset (Vgender) from 50 to 500 words while monitoring both fairness improvements and performance degradation to precisely characterize the tradeoff between subset size and effectiveness.