---
ver: rpa2
title: 'Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive
  Prompting for Large-Language-Models'
arxiv_id: '2408.05093'
source_url: https://arxiv.org/abs/2408.05093
tags:
- prompt
- answer
- reasoning
- first
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the hallucination problem in large language
  models (LLMs), specifically the issue where models generate factually incorrect
  but plausible-sounding responses, such as incorrectly comparing 9.11 and 9.9. The
  core observation is that the order in which LLMs generate answers and reasoning
  impacts their consistency.
---

# Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models

## Quick Facts
- arXiv ID: 2408.05093
- Source URL: https://arxiv.org/abs/2408.05093
- Authors: Zikai Xie
- Reference count: 23
- Key outcome: Reflexive prompting strategy improves reasoning performance by 0.5-2.8% by reconciling outputs from answer-first and logic-first prompts

## Executive Summary
This paper addresses the hallucination problem in large language models (LLMs), specifically the issue where models generate factually incorrect but plausible-sounding responses, such as incorrectly comparing 9.11 and 9.9. The core observation is that the order in which LLMs generate answers and reasoning impacts their consistency. The paper introduces a new benchmark method, "Reasoning Order as Benchmark," which compares responses generated by asking models to output the answer first versus the reasoning first. Additionally, it proposes a "Reflexive Prompting" strategy that queries the model twice with different prompt orders and then uses a third prompt to reconcile the differences. Experimental results across multiple datasets and LLMs show that reflexive prompting improves reasoning performance, with accuracy improvements ranging from 0.5% to 2.8% compared to baseline methods. The consistency measured by the new benchmark also correlates strongly with model accuracy, suggesting it is a useful diagnostic tool for LLM reliability.

## Method Summary
The paper introduces two main contributions: Reasoning Order as Benchmark and Reflexive Prompting. Reasoning Order as Benchmark evaluates LLM consistency by comparing outputs from answer-first prompts (where models answer before reasoning) and logic-first prompts (where models reason before answering). Reflexive Prompting is a three-step strategy that first generates both answer-first and logic-first responses, then uses a third prompt to reconcile any differences between them. The method was tested across four multiple-choice reasoning datasets (MMLU, TruthfulQA, LogiQA, BigBench) using four different LLMs (GPT-4o-mini, Llama-3.1-70b, Claude-3.5-sonnet, Gemini-1.5-flash), comparing accuracy improvements and consistency correlations.

## Key Results
- Reflexive prompting improved accuracy by 0.5-2.8% compared to baseline methods across multiple datasets
- Strong positive correlation (Pearson correlation) between consistency scores and accuracy in most cases
- The reasoning order benchmark successfully identified models with higher hallucination rates
- In 1.3% of cases, reflexive prompts produced completely new outputs that were 42.1% correct

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The order in which LLMs generate answers and reasoning affects the consistency of their outputs.
- Mechanism: Sequential token generation without visibility into future outputs leads to context-dependent hallucinations. When asked to answer first, the model commits to an answer before reasoning is generated, potentially locking in an incorrect response that is then rationalized. When asked to reason first, the model has a more complete logical context before committing to an answer, often producing different (and sometimes correct) results.
- Core assumption: LLMs generate text autoregressively, meaning each token depends only on previously generated tokens, not on future ones.
- Evidence anchors:
  - [abstract] "We discovered that the order in which LLMs generate answers and reasoning impacts their consistency."
  - [section 3] "We noticed that in every response from large language models with incorrect answer, the first sentence always presents the answer."
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism.
- Break condition: If LLMs were modified to have bidirectional context or could "see" future tokens during generation, this mechanism would break.

### Mechanism 2
- Claim: Reflexive prompting improves reasoning performance by leveraging contradictory outputs from different prompt orders.
- Mechanism: By generating both answer-first and logic-first responses and then using a third prompt to reconcile differences, the model effectively performs self-consistency checking. This ensemble-like approach allows the model to detect and correct inconsistencies, improving overall accuracy compared to any single prompt strategy.
- Core assumption: LLMs can detect inconsistencies between their own outputs when presented together and use this information to revise their reasoning.
- Evidence anchors:
  - [abstract] "Experimental results demonstrate that this strategy improves performance across various LLMs compared to direct questioning."
  - [section 4.3] "the reflexive prompt generally performs slightly better than other prompt methods across almost all tasks"
  - [corpus] Weak - no direct corpus evidence supporting this specific ensemble mechanism.
- Break condition: If the model consistently produces identical outputs regardless of prompt order, or if it cannot effectively reconcile contradictory information, this mechanism would fail.

### Mechanism 3
- Claim: Consistency measured by reasoning order benchmarking correlates with model accuracy, making it a useful diagnostic tool.
- Mechanism: When a model's answer-first and logic-first responses are consistent, it indicates reliable reasoning capability. When they differ, it reveals potential hallucinations. The degree of consistency across many questions serves as a proxy for overall model reliability.
- Core assumption: Consistent outputs across different prompt orders indicate robust reasoning, while inconsistency signals potential hallucination.
- Evidence anchors:
  - [abstract] "the accuracies achieved using different prompting strategies show strong correlation with the results of the Reasoning Order as Benchmark test result"
  - [section 4.3] "we calculated Pearson correlation coefficients between consistency and accuracy... In most cases, we observe a strong positive correlation"
  - [corpus] Weak - no direct corpus evidence supporting this specific correlation claim.
- Break condition: If consistency and accuracy become decoupled (e.g., in cases where consistent but systematically wrong reasoning occurs), this mechanism would break.

## Foundational Learning

- Concept: Autoregressive text generation
  - Why needed here: Understanding that LLMs generate text sequentially, with each token depending only on previous tokens, is fundamental to why prompt order affects outputs.
  - Quick check question: If a model generates "The capital of France is..." what information is available to it at that point in the generation process?

- Concept: Chain-of-Thought reasoning
  - Why needed here: The logic-first prompting approach is essentially a zero-shot CoT prompting strategy, which is crucial for understanding why reasoning-first approaches can improve accuracy.
  - Quick check question: How does requiring the model to generate reasoning before an answer differ from standard direct prompting?

- Concept: Ensemble learning principles
  - Why needed here: The reflexive prompting strategy leverages the idea that combining multiple "weak" predictors (different prompt strategies) can produce a stronger overall result.
  - Quick check question: Why might averaging or reconciling outputs from different prompt strategies improve overall accuracy?

## Architecture Onboarding

- Component map: Question normalization → Answer-first prompt generation → LLM call 1 → Logic-first prompt generation → LLM call 2 → Output comparison → Reflexive prompt generation → LLM call 3 → Final answer extraction

- Critical path: Question → Generate answer-first prompt → LLM call 1 → Generate logic-first prompt → LLM call 2 → Compare outputs → Generate reflexive prompt → LLM call 3 → Return final answer

- Design tradeoffs:
  - Accuracy vs. cost: Reflexive prompting requires 3x the inference calls of single-prompt approaches
  - Consistency vs. creativity: Strict consistency checking might suppress novel but correct answers that differ from standard reasoning
  - Complexity vs. interpretability: Adding the reconciliation step makes the system harder to debug

- Failure signatures:
  - All three prompt strategies consistently produce the same incorrect answer (model has systematic bias)
  - Answer-first and logic-first outputs are consistently identical (no benefit from reflexivity)
  - Reflexive output is completely different from both initial outputs but is incorrect (over-correction problem)

- First 3 experiments:
  1. Test on a simple factual question set (like the 9.11 vs 9.9 comparison) to verify the basic inconsistency mechanism
  2. Test on a reasoning dataset with known answers to measure accuracy improvement from reflexive prompting
  3. Test on a dataset with ambiguous questions to evaluate how the system handles cases where both prompt orders might be valid

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reasoning order inconsistency serve as a reliable diagnostic tool for detecting hallucinations across different LLM architectures and datasets?
- Basis in paper: [explicit] The paper proposes Reasoning Order as Benchmark to evaluate LLM consistency by comparing answer-first and logic-first prompts, finding strong correlation between consistency and accuracy.
- Why unresolved: The current experiments are limited to specific datasets (MMLU, TruthfulQA, LogiQA, BigBench) and four LLMs. The generalizability across diverse domains, languages, and model families remains untested.
- What evidence would resolve it: Extensive cross-domain testing across multiple languages, specialized domains, and a broader range of LLM architectures (both decoder-only and encoder-decoder models) showing consistent correlation between reasoning order inconsistency and hallucination rates.

### Open Question 2
- Question: Does the reflexive prompting strategy provide genuine error correction or merely ensemble selection between two potentially flawed outputs?
- Basis in paper: [explicit] The paper notes that reflexive prompting improves accuracy by 0.5-2.8% compared to baselines, but acknowledges it doesn't correct erroneous answers—only selects between answer-first and logic-first results.
- Why unresolved: The mechanism by which LLMs reconcile contradictory outputs is not fully understood. The 1.3% of cases where reflexive prompts produce completely new outputs that are 42.1% correct suggests potential emergent error correction capabilities that need investigation.
- What evidence would resolve it: Controlled experiments isolating cases where both answer-first and logic-first prompts are wrong, measuring whether reflexive prompting can generate correct answers through independent reasoning, and analyzing the reasoning process differences.

### Open Question 3
- Question: Would architectural modifications that allow LLMs to "see" future outputs during generation significantly reduce hallucination compared to prompt-based approaches?
- Basis in paper: [explicit] The paper hypothesizes that LLMs' sequential generation without visibility into future content contributes to hallucinations, and references work showing performance gains from inference-time delays and continuous latent space reasoning.
- Why unresolved: The paper couldn't retrain LLMs due to computational constraints and only explored prompt engineering solutions. The potential benefits of architectural changes versus prompt strategies remain theoretical.
- What evidence would resolve it: Comparative experiments between modified decoder architectures with bidirectional visibility during generation and prompt-based approaches across identical tasks, measuring hallucination rates and accuracy improvements.

## Limitations
- The reflexive prompting strategy requires 3x the inference calls, creating significant cost and latency overhead
- Results are primarily validated on multiple-choice questions, limiting generalizability to open-ended reasoning tasks
- The correlation between consistency and accuracy, while strong in most cases, may not hold for all types of reasoning tasks or model architectures

## Confidence
- **High confidence**: The empirical observation that prompt order affects consistency (Mechanism 1) is well-supported by the experimental results across multiple datasets and models.
- **Medium confidence**: The reflexive prompting strategy improves accuracy (Mechanism 2) and the consistency benchmark correlates with model reliability (Mechanism 3), though the effect sizes are modest and the causal mechanisms could be further validated.
- **Low confidence**: The generalizability of these findings to non-multiple-choice tasks and the cost-benefit tradeoff of the reflexive prompting approach in production settings.

## Next Checks
1. **Cost-Benefit Analysis**: Conduct a thorough evaluation of the accuracy improvements versus the tripled inference costs, particularly for production applications where latency and cost are critical factors.

2. **Cross-Task Generalization**: Test the reflexive prompting approach on open-ended reasoning tasks and creative generation tasks to determine if the consistency-accuracy correlation holds beyond multiple-choice questions.

3. **Alternative Consistency Measures**: Explore whether other forms of consistency checking (such as Bayesian model combination or contrastive sampling) could achieve similar or better results with fewer inference calls.