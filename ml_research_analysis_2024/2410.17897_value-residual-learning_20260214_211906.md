---
ver: rpa2
title: Value Residual Learning
arxiv_id: '2410.17897'
source_url: https://arxiv.org/abs/2410.17897
tags:
- attention
- training
- layer
- arxiv
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of attention concentration in
  deep Transformer models, where token representations become increasingly similar
  as network depth increases. The authors propose ResFormer, which adds residual connections
  from the first layer's value vectors to all subsequent layers before attention computation,
  preserving initial token information.
---

# Value Residual Learning

## Quick Facts
- arXiv ID: 2410.17897
- Source URL: https://arxiv.org/abs/2410.17897
- Authors: Zhanchao Zhou; Tianyi Wu; Zhiyun Jiang; Fares Obeid; Zhenzhong Lan
- Reference count: 14
- Key outcome: ResFormer achieves equivalent validation loss with 16.11% fewer parameters and 20.3% less training data on 20B SlimPajama subset

## Executive Summary
This paper addresses the problem of attention concentration in deep Transformer models, where token representations become increasingly similar as network depth increases. The authors propose ResFormer, which adds residual connections from the first layer's value vectors to all subsequent layers before attention computation, preserving initial token information. They also introduce SVFormer, a variant where all layers share the first layer's value embedding, reducing KV cache size by nearly half. Experiments on a 20B SlimPajama subset show that ResFormer achieves equivalent validation loss with 16.11% fewer parameters and 20.3% less training data compared to standard Transformers, while SVFormer significantly speeds up training and maintains performance when sequence length is long.

## Method Summary
The paper introduces ResFormer, which adds residual connections from the first layer's value vectors to all subsequent layers before attention computation, preserving initial token information. The SVFormer variant shares the first layer's value embedding across all layers, reducing KV cache size by nearly half. These approaches address attention concentration by maintaining token diversity throughout the network depth, where standard Transformers tend to produce increasingly similar representations.

## Key Results
- ResFormer achieves equivalent validation loss with 16.11% fewer parameters and 20.3% less training data on 20B SlimPajama subset
- SVFormer significantly speeds up training and maintains performance when sequence length is long
- Both methods outperform DenseFormer and NeuTRENO

## Why This Works (Mechanism)
The mechanism works by preventing attention concentration through value residual connections. In standard Transformers, deeper layers produce increasingly similar token representations due to attention mechanisms that progressively focus on specific patterns. By adding residuals from the first layer's value vectors, ResFormer preserves the initial diversity of token representations throughout the network. SVFormer takes this further by sharing value embeddings across layers, which not only maintains representation diversity but also reduces memory overhead through smaller KV caches.

## Foundational Learning

**Attention Concentration**: The phenomenon where token representations become increasingly similar as network depth increases in Transformers. *Why needed*: Understanding this problem is crucial because it limits model expressivity and efficiency. *Quick check*: Compare cosine similarity between token embeddings at different depths in a standard Transformer.

**Value Residual Connections**: Adding residual connections specifically to value vectors before attention computation. *Why needed*: Value vectors contain the actual content information that attention mechanisms combine. *Quick check*: Visualize how value residuals affect attention weight distributions.

**KV Cache Optimization**: Techniques to reduce memory usage in autoregressive decoding by sharing or compressing key-value caches. *Why needed*: Large KV caches become a bottleneck for long sequences and large models. *Quick check*: Measure memory usage differences between standard and shared KV implementations.

## Architecture Onboarding

**Component Map**: Input -> Embedding -> ResFormer Blocks (with value residuals) -> Output, where each block contains attention with residual value connections.

**Critical Path**: Token embeddings → Value extraction (first layer) → Value residual additions (all layers) → Attention computation → Output.

**Design Tradeoffs**: ResFormer trades slightly increased computation in each layer for reduced model size and training data requirements. SVFormer trades some flexibility in layer-specific value learning for significant KV cache reduction.

**Failure Signatures**: If value residuals are added too early or too late in the attention computation, they may not effectively prevent attention concentration. If SVFormer's shared values are poorly initialized, it can lead to training instability.

**First Experiments**:
1. Compare token representation diversity (via similarity metrics) between standard Transformers and ResFormer at different depths
2. Measure KV cache size reduction when switching from standard to SVFormer implementation
3. Evaluate training efficiency (FLOPs, wall-clock time) of SVFormer versus standard Transformers on long sequences

## Open Questions the Paper Calls Out
None

## Limitations

- Experimental validation is limited to a single 20B SlimPajama subset, raising questions about generalizability
- Focus on pretraining efficiency metrics with limited discussion of downstream task performance
- Theoretical grounding for why value residuals specifically address attention concentration could be more rigorous

## Confidence

- **High confidence**: ResFormer achieves equivalent validation loss with fewer parameters and training data
- **Medium confidence**: SVFormer speeds up training while maintaining performance for long sequences
- **Medium confidence**: ResFormer outperforms DenseFormer and NeuTRENO based on limited comparative experiments

## Next Checks

1. Evaluate ResFormer and SVFormer on diverse downstream tasks to verify pretraining efficiency translates to practical performance gains
2. Conduct ablation studies testing alternative residual connection strategies to isolate value residual contributions
3. Test methods on smaller model scales (1B-8B parameters) to assess practical applicability for resource-constrained settings