---
ver: rpa2
title: Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language
  Models for Fact Completion
arxiv_id: '2410.14405'
source_url: https://arxiv.org/abs/2410.14405
tags:
- samples
- fact
- prediction
- recall
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of precisely interpreting how
  language models perform fact completion by distinguishing between four prediction
  scenarios: generic language modeling, random guesswork, heuristics recall, and exact
  fact recall. The authors introduce PRISM, a model-specific method for constructing
  datasets that isolate these scenarios using diagnostic criteria based on fact completion,
  prediction confidence, and heuristic dependence.'
---

# Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion

## Quick Facts
- arXiv ID: 2410.14405
- Source URL: https://arxiv.org/abs/2410.14405
- Reference count: 40
- This paper introduces PRISM to precisely interpret language model fact completion by distinguishing between four prediction scenarios: generic language modeling, random guesswork, heuristics recall, and exact fact recall.

## Executive Summary
This paper addresses the fundamental challenge of precisely interpreting how language models perform fact completion by distinguishing between four distinct prediction scenarios. The authors introduce PRISM, a model-specific method that constructs diagnostic datasets isolating each scenario using criteria based on fact completion, prediction confidence, and heuristic dependence. Through causal tracing and information flow analysis on GPT-2 XL, Llama 2 7B, and Llama 2 13B models, the study reveals distinct interpretability patterns for each scenario. The findings demonstrate that previous interpretability studies may have been confounded by mixed prediction scenarios, and provide new insights into the neural mechanisms underlying different types of fact completion behavior.

## Method Summary
The PRISM method constructs model-specific diagnostic datasets by filtering LAMA queries based on three criteria: fact completion (whether the model produces factual answers), confident prediction (measured through agreement across paraphrased queries), and no dependence on heuristics (detected through lexical overlap, person name bias, and prompt bias filters). The authors apply two interpretability methods to these datasets: causal tracing, which measures the importance of different neural components by corrupting and restoring representations, and information flow analysis using attention knockout and attribute extraction to understand information propagation patterns. The approach systematically isolates four prediction scenarios to enable precise interpretation of language model behavior during fact completion.

## Key Results
- Causal tracing reveals that exact fact recall shows critical activity in mid-layer MLP sublayers at last subject token positions, while guesswork and heuristics rely more on late last token position MLP sublayers.
- Information flow analysis confirms distinct patterns across scenarios, with exact fact recall showing strong subject position layer dependence while guesswork shows early non-subject position layer importance.
- The study validates that previous interpretability findings may have been influenced by mixed prediction scenarios, and demonstrates that PRISM enables more nuanced and accurate interpretations of language model behavior during fact completion.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PRISM datasets enable precise separation of prediction scenarios by isolating specific combinations of fact completion, confidence, and heuristic dependence.
- Mechanism: The method constructs datasets by applying three diagnostic criteria—fact completion, confident prediction, and no dependence on heuristics—to systematically filter samples into four distinct prediction scenarios.
- Core assumption: Each prediction scenario exhibits unique interpretability patterns when analyzed in isolation, and mixing scenarios obscures these patterns.
- Evidence anchors:
  - [abstract] "We apply two popular interpretability methods to the scenarios: causal tracing (CT) and information flow analysis. We find that both yield distinct results for each scenario."
  - [section] "The scenarios are: 1) Generic language modeling, when the model does not respond with facts, such as when generating a story. 2) Guesswork, when the model responds with a fact but is uncertain. 3) Heuristics recall, when the model uses shallow heuristics... 4) Exact fact recall, when the model has indeed memorized the correct answer."

### Mechanism 2
- Claim: Different prediction scenarios activate distinct neural pathways during fact completion, as evidenced by varying importance of MLP sublayers at different token positions.
- Mechanism: Causal tracing reveals that exact fact recall relies on mid-layer MLP sublayers at last subject token positions for memory access, while guesswork and heuristics rely more on late last token position MLP sublayers.
- Core assumption: The importance of different neural components varies systematically across prediction scenarios, reflecting different computational strategies.
- Evidence anchors:
  - [abstract] "Results for exact fact recall and generic language modeling scenarios confirm previous conclusions about the importance of mid-range MLP sublayers for fact recall, while results for guesswork and heuristics indicate a critical role of late last token position MLP sublayers."
  - [section] "The exact fact recall results show a clear peak in AIE in (last subject token, mid layer) MLP states."

### Mechanism 3
- Claim: Information flow analysis reveals distinct patterns of information propagation for each prediction scenario, with exact fact recall showing strong subject position layer dependence while guesswork shows early non-subject position layer importance.
- Mechanism: Attention knockout and attribute extraction methods demonstrate that exact fact recall critically depends on information flowing from middle-upper subject position layers, while guesswork relies more on early non-subject position layers.
- Core assumption: The patterns of information flow within the model differ systematically across prediction scenarios, reflecting different inference mechanisms.
- Evidence anchors:
  - [abstract] "Our in-depth analysis of information flow confirms that models employ distinct inference mechanisms for the PRISM scenarios."
  - [section] "We observe contrasting results for exact fact recall compared to generic language modeling samples."

## Foundational Learning

- Concept: Causal tracing methodology
  - Why needed here: To measure the importance of different neural components for specific predictions across different scenarios
  - Quick check question: How does perturbing and restoring representations at different positions help identify critical model components?

- Concept: Information flow analysis techniques
  - Why needed here: To understand how information propagates through the model during different types of fact completion
  - Quick check question: What is the difference between attention knockout and attribute extraction methods in studying information flow?

- Concept: Model confidence estimation
  - Why needed here: To distinguish between confident and unconfident predictions across different scenarios
  - Quick check question: Why might using multiple paraphrased queries be a better proxy for model confidence than single-query confidence scores?

## Architecture Onboarding

- Component map: Embedding layers -> Multiple transformer blocks with multi-head self-attention (MHSA) and multi-layer perceptron (MLP) sublayers -> Output layers. Critical components for fact completion are identified at specific (token, layer) positions.
- Critical path: For exact fact recall, the critical path involves subject position embeddings flowing through mid-layer MLP sublayers to extract factual associations. For guesswork, the path involves early non-subject position layers and late last token position MLP sublayers.
- Design tradeoffs: Using synthetic data to simulate no memorization trades realism for control in creating heuristics recall samples. Using popularity scores as memorization proxies trades accuracy for scalability in dataset creation.
- Failure signatures: Similar interpretability patterns across scenarios might indicate insufficient diagnostic criteria. Over-reliance on single measurement methods might miss scenario-specific patterns.
- First 3 experiments:
  1. Run causal tracing on a mixed dataset and compare results to PRISM-isolated scenarios to verify scenario-specific patterns
  2. Test different confidence threshold values to assess sensitivity of scenario separation
  3. Apply attention knockout to synthetic fact tuples to verify no memorization assumption

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for measuring model confidence in fact completion scenarios, given that current approaches (like paraphrase agreement) have limitations and inconsistencies?
- Basis in paper: [explicit] The paper discusses the lack of comprehensive studies on model confidence metrics and notes that different methods exist without clear consensus
- Why unresolved: The paper observes that their results are sensitive to whether they investigate predictions with high or low probabilities from each partition, suggesting that qualitative differences exist in how models behave in different confidence regimes that current metrics cannot capture
- What evidence would resolve it: A comprehensive evaluation framework comparing multiple confidence metrics across diverse fact completion scenarios, with clear criteria for when each metric is most appropriate and validation against human judgments of confidence

### Open Question 2
- Question: How do model behaviors differ fundamentally between low-probability and high-probability fact completion predictions, and what causes these qualitative differences?
- Basis in paper: [inferred] The paper notes that their results are sensitive to probability levels and observes that bottom-ranked probability samples show larger effects for last token states compared to top-ranked samples, suggesting fundamentally different inference mechanisms
- Why unresolved: The paper observes these differences but does not investigate the underlying causes or whether this represents a fundamental property of language model inference or an artifact of current measurement approaches
- What evidence would resolve it: Controlled experiments systematically varying prediction probability levels while holding other factors constant, combined with interpretability analyses to identify what computational mechanisms change between low and high probability regimes

### Open Question 3
- Question: What is the relationship between the four prediction scenarios (generic language modeling, guesswork, heuristics recall, exact fact recall) and the underlying computational mechanisms in transformer models?
- Basis in paper: [explicit] The paper shows that different scenarios yield distinct interpretability results with exact fact recall showing critical mid-layer MLP activity while guesswork and heuristics rely more on late last token position MLP sublayers
- Why unresolved: While the paper identifies these patterns, it does not provide a comprehensive theoretical framework explaining why these scenarios should correspond to different computational mechanisms or how they relate to the overall architecture of transformer models
- What evidence would resolve it: A unified theoretical model explaining how transformer architectures naturally give rise to these four distinct computational patterns, supported by additional empirical validation across different model architectures and scales

### Open Question 4
- Question: How can we develop more comprehensive and accurate heuristics detection methods beyond the three filters (lexical overlap, person name bias, prompt bias) currently used in the paper?
- Basis in paper: [explicit] The paper acknowledges that their heuristics filters are leaky and cannot detect all instances of heuristics, particularly noting issues with non-person subjects and more subtle linguistic correlations
- Why unresolved: The paper demonstrates the limitations of current heuristics detection but does not propose solutions or investigate what additional types of heuristics might exist that current methods cannot capture
- What evidence would resolve it: Development and validation of more sophisticated heuristics detection algorithms that can identify a broader range of surface-level cues and spurious correlations, with systematic evaluation showing improved coverage and precision compared to current methods

## Limitations
- The diagnostic criteria used to construct PRISM datasets rely on proxies (popularity scores, paraphrased queries) that may not perfectly capture the underlying prediction mechanisms, potentially leaving residual scenario mixing.
- The interpretability analyses are conducted on a limited set of models (GPT-2 XL, Llama 2 7B, Llama 2 13B), and it's unclear whether the observed patterns would hold for other architectures or training regimes.
- The study focuses on cloze-style fact completion tasks from LAMA, which may not generalize to open-ended generation or other knowledge-intensive tasks.

## Confidence
- **High confidence**: The existence of distinct interpretability patterns for different prediction scenarios when analyzed in isolation.
- **Medium confidence**: The claim that previous interpretability studies may have been influenced by mixed prediction scenarios.
- **Medium confidence**: The assertion that exact fact recall relies on mid-layer MLP sublayers while guesswork relies on late last token position MLP sublayers.
- **Low confidence**: The effectiveness of the diagnostic criteria in completely isolating the four prediction scenarios.

## Next Checks
1. Apply PRISM methodology to a different knowledge-intensive task (such as open-ended question answering or multi-hop reasoning) to test whether the same scenario-specific interpretability patterns emerge, or whether task complexity introduces additional prediction mechanisms.

2. Systematically vary the confidence thresholds, bias filter parameters, and popularity score cutoffs used in PRISM dataset construction to quantify how sensitive the observed interpretability patterns are to these methodological choices.

3. Design targeted interventions that selectively impair either memory access (to test exact fact recall) or heuristic computation (to test heuristics recall), and verify whether the predicted scenario-specific failure modes occur as expected based on the interpretability findings.