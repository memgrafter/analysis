---
ver: rpa2
title: 'OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to construct
  Observer-Thinker-Conceiver-Expresser'
arxiv_id: '2406.16495'
source_url: https://arxiv.org/abs/2406.16495
tags:
- information
- state
- experts
- positional
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OTCE, a hybrid architecture combining Selective
  State Space Models (Mamba) with Transformer-based self-attention, enhanced by cross-domain
  Mixture of Experts. The key innovation is a relative positional information injection
  method that bridges SSM and attention mechanisms, allowing effective handling of
  long-range dependencies.
---

# OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to construct Observer-Thinker-Conceiver-Expresser

## Quick Facts
- arXiv ID: 2406.16495
- Source URL: https://arxiv.org/abs/2406.16495
- Reference count: 31
- Primary result: 27.06% improvement on natural long context tasks compared to Jamba

## Executive Summary
This paper introduces OTCE, a hybrid architecture combining Selective State Space Models (Mamba) with Transformer-based self-attention, enhanced by cross-domain Mixture of Experts. The key innovation is a relative positional information injection method that bridges SSM and attention mechanisms, allowing effective handling of long-range dependencies. Following a biomimetic Observer-Thinker-Conceiver-Expresser structure, OTCE achieves superior performance on various language tasks including semantic similarity, text classification, and associative recall tasks.

## Method Summary
OTCE integrates Mamba SSM and Transformer attention with cross-domain Mixture of Experts through a novel relative positional information injection mechanism. The architecture uses rotational positional encoding (RoPE) to convert discrete positional information from SSM inner products into continuous form for attention weighting. Four distinct modules process information sequentially: Observer (SSM for selective filtering), Thinker (attention for long-range dependencies), Conceiver (SSM for state aggregation), and Expresser (attention-weighted combination of context-aware and summary states). Cross-domain experts share parameters either cohesively (small-scale models) or expansively (large-scale models) to balance specialization and efficiency.

## Key Results
- OTCE achieves 27.06% improvement on natural long context tasks compared to Jamba
- Superior performance on semantic similarity, text classification, and associative recall tasks
- Effective handling of long-range dependencies up to 128K context length

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relative positional information injection bridges SSM and attention by converting discrete positional information from SSM to continuous form for attention weighting.
- Mechanism: SSM inner product between output weight matrix (C) and state (h) produces discrete relative positional information, converted to continuous form using rotational positional encoding before attention calculation.
- Core assumption: Inner product operation between C and h can be represented as a function of word embeddings and their relative positional difference.
- Evidence anchors: Abstract mentions position information injection connecting SSM with quadratic attention; section describes discrete positional information becoming continuous for long-term dependencies.
- Break condition: If conversion between discrete and continuous positional information is mathematically unsound or attention cannot utilize converted positional information.

### Mechanism 2
- Claim: Cross-domain mixture of experts with parameter sharing reduces redundancy while maintaining domain specialization.
- Mechanism: Cohesive experts share linear layer parameters; expansive experts share complete MLP parameters with common domain knowledge gate controlling parameter flow.
- Core assumption: Different domains share common foundational knowledge that can be efficiently represented through parameter sharing.
- Evidence anchors: Abstract mentions hybrid experts with cross-sharing domains; section describes experts storing and transferring common foundational knowledge through parameter sharing.
- Break condition: If shared parameters become bottleneck for domain-specific learning or gating mechanism fails to control knowledge flow.

### Mechanism 3
- Claim: Biomimetic OTCE architecture optimizes information processing by combining SSM and attention strengths.
- Mechanism: Four-stage processing: Observer (SSM selective filtering), Thinker (attention long-range dependencies), Conceiver (SSM state aggregation), Expresser (attention-weighted combination of context-aware and summary states).
- Core assumption: Four-stage biological information processing pattern can be effectively mapped to computational modules.
- Evidence anchors: Abstract mentions biomimetic Observer-Thinker-Conceiver-Expresser architecture; section describes each stage's function.
- Break condition: If sequential processing creates bottlenecks or information flow between modules is suboptimal.

## Foundational Learning

- Concept: Selective State Space Models (Mamba)
  - Why needed here: OTCE relies on Mamba's ability to selectively process information and maintain compact state representation
  - Quick check question: What are the key differences between traditional state space models and selective state space models like Mamba?

- Concept: Rotational Positional Encoding (RoPE)
  - Why needed here: RoPE injects positional information into both SSM and attention, bridging their different position handling approaches
  - Quick check question: How does RoPE differ from traditional absolute positional encoding in capturing relative positional information?

- Concept: Mixture of Experts (MoE)
  - Why needed here: Cross-domain MoE requires understanding expert selection and parameter sharing across domains
  - Quick check question: What is the purpose of the gating network in MoE, and how does it differ between traditional MoE and cross-domain MoE?

## Architecture Onboarding

- Component map: Input → Observer (SSM with RoPE) → Thinker (Attention with RoPE) → Conceiver (SSM) → Expresser (Attention + SSM) → Output

- Critical path: Input → Observer (SSM) → Thinker (Attention) → Conceiver (SSM) → Expresser (Attention + SSM) → Output

- Design tradeoffs:
  - Parameter sharing in experts reduces redundancy but may limit domain-specific specialization
  - Combining SSM and attention increases complexity but leverages complementary strengths
  - Rotational positional encoding adds computation but enables effective bridging of SSM and attention

- Failure signatures:
  - Poor performance on long-range dependency tasks indicates issues with positional encoding or attention mechanism
  - Suboptimal results on selective filtering tasks suggest problems with Observer module
  - Unstable training or poor generalization may indicate issues with mixture of experts parameter sharing

- First 3 experiments:
  1. Ablation study: Remove rotational positional encoding from either SSM or attention to quantify its impact on bridging the two mechanisms
  2. Parameter sharing analysis: Compare performance of cohesive vs expansive cross-domain experts with different sharing ratios
  3. Module isolation: Evaluate each OTCE module (Observer, Thinker, Conceiver, Expresser) independently to identify bottlenecks or suboptimal components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does cross-domain MoE architecture perform when scaled to 10B+ parameter models compared to 1B parameter models tested?
- Basis in paper: Paper discusses Cohesive Cross-Domain Experts being more suitable for small-scale models with fewer experts, while Expansive Cross-Domain Experts are better for large-scale models with more experts. Largest tested was 1B parameters.
- Why unresolved: Paper only tests MoE architectures up to 1B parameters. Scaling effects on performance, efficiency, and knowledge transfer across domains at much larger scales remain unexplored.
- What evidence would resolve it: Comprehensive benchmarking of cross-domain MoE architectures across multiple scales (1B, 10B, 100B parameters) on diverse downstream tasks.

### Open Question 2
- Question: What is the optimal balance between SSM layers and attention layers in OTCE for different task types?
- Basis in paper: Paper mentions ablation experiments comparing different combinations of MOE and MLP, and notes that OTCE's Expresser module reweights recursively aggregated SSM information with attention. Optimal ratio for different task categories is not explored.
- Why unresolved: Paper only tests one configuration (8 layers with specific SSM:Attention:MLP ratios) and doesn't investigate how different layer compositions affect performance across task types.
- What evidence would resolve it: Systematic ablation studies varying number of SSM vs. attention layers for different task families, measuring performance improvements on each category.

### Open Question 3
- Question: How does RoPE strategy affect long-range dependency capture in SSM beyond 128K context length, and are there diminishing returns or performance plateaus?
- Basis in paper: Paper tests RoPE effectiveness at 128K context length and shows significant improvements, but doesn't explore beyond this limit or analyze performance scaling.
- Why unresolved: Evaluation only extends to 128K context length. Paper doesn't investigate whether RoPE benefits continue to scale, plateau, or potentially degrade at extreme context lengths.
- What evidence would resolve it: Extended evaluation of OTCE with RoPE at 256K, 512K, and 1M context lengths, measuring accuracy on both long-context and standard tasks.

## Limitations

- The mathematical foundation for converting discrete positional information from SSM to continuous form for attention weighting is not rigorously proven
- The 27.06% improvement claim over Jamba lacks detailed ablation studies showing which components contribute most to this gain
- Cross-domain mixture of experts with parameter sharing may create optimization challenges not fully explored

## Confidence

**High Confidence**: The general framework of combining SSM (Mamba) with Transformer attention is well-established and the paper correctly identifies complementary strengths of both approaches.

**Medium Confidence**: The specific mechanism for bridging SSM and attention through relative positional information injection appears plausible but lacks rigorous mathematical proof. Parameter sharing strategies in cross-domain mixture of experts are reasonable but not thoroughly validated.

**Low Confidence**: The claimed 27.06% improvement over Jamba requires independent verification, as does the effectiveness of the specific four-stage biomimetic architecture. The paper provides limited ablation studies to isolate contribution of individual innovations.

## Next Checks

1. **Ablation study on positional encoding connection**: Remove the relative positional information injection mechanism and retrain OTCE to quantify its exact contribution to performance improvements. Compare perplexity on long sequences with and without this bridging component.

2. **Cross-domain expert parameter sharing analysis**: Systematically vary the ratio of shared to private parameters in the mixture of experts (0%, 25%, 50%, 75%, 100%) and measure impact on both pre-training perplexity and downstream task performance. This will reveal whether the proposed parameter sharing strategy is optimal.

3. **Independent replication on alternative long-context datasets**: Reimplement OTCE architecture and evaluate on different long-context benchmarks (e.g., PG-19, long-range Arena) to verify the 27.06% improvement claim against Jamba is reproducible and not dataset-specific.