---
ver: rpa2
title: 'Building Expressive and Tractable Probabilistic Generative Models: A Review'
arxiv_id: '2402.00759'
source_url: https://arxiv.org/abs/2402.00759
tags:
- probabilistic
- learning
- networks
- tractable
- circuits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of tractable probabilistic
  generative models, with a primary focus on Probabilistic Circuits (PCs). The authors
  discuss the trade-offs between expressivity and tractability in generative modeling
  and present various techniques to build expressive and efficient PCs.
---

# Building Expressive and Tractable Probabilistic Generative Models: A Review

## Quick Facts
- arXiv ID: 2402.00759
- Source URL: https://arxiv.org/abs/2402.00759
- Reference count: 4
- Primary result: Comprehensive survey of tractable probabilistic generative models focusing on Probabilistic Circuits (PCs), covering learning algorithms, structural properties, and neural extensions while highlighting trade-offs between expressivity and tractability.

## Executive Summary
This paper provides a comprehensive survey of tractable probabilistic generative models, with a primary focus on Probabilistic Circuits (PCs). The authors discuss the trade-offs between expressivity and tractability in generative modeling and present various techniques to build expressive and efficient PCs. They cover learning algorithms, structural properties, and recent extensions that integrate ideas from deep neural networks to improve PCs. The survey also highlights the challenges and open questions in the field, aiming to guide future research towards building hybrid models that combine the strengths of both tractable and deep generative models.

## Method Summary
The paper provides a high-level overview of the field of tractable probabilistic generative models, particularly focusing on Probabilistic Circuits (PCs). It outlines various learning algorithms for PCs, including parameter learning through gradient-based optimization, expectation maximization, and closed-form solutions under determinism, as well as structure learning through heuristics-based and Bayesian approaches. The survey also discusses extensions and modifications to improve expressivity and robustness, such as incorporating neural networks, variational autoencoders, and normalizing flows. However, the paper does not provide specific implementation details or code for the various algorithms and extensions discussed.

## Key Results
- PCs achieve tractability through structural constraints like smoothness, decomposability, and determinism over sum and product nodes
- Parameter learning in PCs can be done exactly via closed-form solutions under determinism, or via gradient-based, EM, or signomial programming approaches otherwise
- PCs can be made expressive by integrating deep neural network components while preserving tractability via structural compatibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic Circuits (PCs) achieve tractability by enforcing structural constraints like smoothness, decomposability, and determinism over sum and product nodes.
- Mechanism: These properties ensure that probabilistic inference (e.g., marginal, conditional, MAP) can be computed exactly and efficiently by decomposing integrals and sums over the circuit structure without approximation.
- Core assumption: The computational graph is a valid hierarchy of tractable univariate distributions (leaves) combined via sum and product nodes.
- Evidence anchors:
  - [section] "Definition 2 (Smoothness)... Intuitively, smoothness implies that the sum nodes represent valid mixture distributions... circuits that are smooth and decomposable additionally support tractable computation of marginal and conditional queries."
  - [section] "Definition 3 (Decomposability)... smoothness and decomposability are not sufficient for tractably computing MAP queries... we can perform MAP inference tractably over smooth, decomposable, and deterministic PCs."
- Break condition: If the structure becomes cyclic or violates smoothness/decomposability, tractability is lost and exact inference becomes intractable.

### Mechanism 2
- Claim: Parameter learning in PCs can be done exactly via closed-form solutions under determinism, or via gradient-based, EM, or signomial programming approaches otherwise.
- Mechanism: Determinism collapses the PC to a weighted product of factorized distributions, enabling closed-form MLE; otherwise, the PC's differentiability and EM's latent variable interpretation enable iterative optimization.
- Core assumption: Parameters correspond to mixture weights (sum nodes) and leaf distribution parameters; under determinism, these weights become sparse.
- Evidence anchors:
  - [section] "In the presence of determinism, the output of the root node reduces to a weighted product of simple factorized distributions, and the parameters can be estimated in closed form."
  - [section] "An alternative optimization scheme proposed for PCs is expectation-maximization (EM)... As PCs are essentially hierarchical mixture models, their sum nodes can be viewed as marginalizing out an unobserved discrete latent variable."
- Break condition: If the PC is not smooth/decomposable, EM and gradient-based methods may not converge to valid parameter estimates or may not even be applicable.

### Mechanism 3
- Claim: PCs can be made expressive by integrating deep neural network components (e.g., convolutions, attention, residual links) while preserving tractability via structural compatibility.
- Mechanism: Neural modules are embedded as valid PC nodes (e.g., convolutional sum nodes, attention-based sum nodes) that respect smoothness/decomposability, enabling deep hybrid architectures.
- Core assumption: The added neural components can be formulated as operations over probability densities (not data) and maintain the PC's structural properties.
- Evidence anchors:
  - [section] "Butz et al., 2019 demonstrated that the sum nodes in a PC are similar in essence to convolutions, while product nodes resemble pooling operations... they formalized the properties that such neural operations need to satisfy in order to result in a valid PC."
  - [section] "Yu et al., 2022a proposed the integration of the self attention mechanism... within PCs, while [Ventola et al., 2020] proposed the use of residual links, thus developing a probabilistic analog to ResNets."
- Break condition: If the neural module introduces non-linear dependencies that violate decomposability or smoothness, the PC loses tractability guarantees.

## Foundational Learning

- Concept: Probabilistic inference tasks (evidential, marginal, conditional, MAP)
  - Why needed here: These queries define the tractability spectrum; understanding which PCs support which queries is essential for model design.
  - Quick check question: What structural property must a PC have to support exact marginal and conditional inference?

- Concept: Sum-product network structure and semantics
  - Why needed here: PCs are fundamentally hierarchical mixtures of factorized distributions; grasping this is key to understanding learning and extensions.
  - Quick check question: In a PC, what does a sum node represent in terms of probability distributions?

- Concept: Parameter vs. structure learning trade-offs
  - Why needed here: Deciding when to optimize parameters, learn structure, or both impacts expressiveness and tractability.
  - Quick check question: Why is structure learning non-trivial for PCs, and what are common heuristic approaches?

## Architecture Onboarding

- Component map: Leaf nodes (univariate distributions) -> Sum nodes (mixture weights) -> Product nodes (factorized products) -> Root node (joint density)

- Critical path:
  1. Define variable scopes and partition into leaf scopes
  2. Build decomposable, smooth (and optionally deterministic) structure
  3. Initialize parameters (weights, leaf params)
  4. Optimize parameters via MLE (closed-form, gradient, EM, or signomial)
  5. Validate tractability for target inference queries

- Design tradeoffs:
  - Determinism vs. expressiveness: deterministic PCs are less expressive but enable closed-form learning and MAP inference
  - Random vs. learned structure: random structures scale better and parallelize well but may be less task-optimized
  - Hybrid neural modules vs. pure PC: hybrids gain inductive biases but require careful structural integration to preserve tractability

- Failure signatures:
  - Non-smooth sum nodes → invalid mixture, broken evidential inference
  - Non-decomposable product nodes → intractable marginals/conditionals
  - Invalid neural module integration → structural violations, loss of tractability

- First 3 experiments:
  1. Implement a simple binary PC with Gaussian leaves, smooth/decomposable structure, and test exact evidential inference
  2. Extend the PC to support marginal inference by ensuring decomposability, then compute marginals and compare to ground truth
  3. Add a deterministic sum node and verify that closed-form parameter learning works, then test MAP inference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can overparameterized probabilistic circuits be optimized more effectively, given the observed performance plateau despite increasing model size?
- Basis in paper: [explicit] The paper discusses that scaling the parameters of a PC does not result in a corresponding performance improvement and attributes this to the increase in latent information associated with sum nodes.
- Why unresolved: Current optimization approaches for PCs rely on heuristics without a theoretically grounded understanding of why overparameterization fails to improve performance.
- What evidence would resolve it: Developing a theoretical framework that explains the behavior of overparameterized PCs, possibly borrowing concepts from the study of overparameterized neural networks, and demonstrating improved optimization techniques based on this understanding.

### Open Question 2
- Question: How can semantically meaningful and disentangled latent representations be learned from the latent variables introduced by sum nodes in probabilistic circuits?
- Basis in paper: [explicit] The paper notes that learning useful data representations using the latent variables associated with sum nodes is non-trivial and less explored, despite recent works on distilling information from VAEs within PCs.
- Why unresolved: Current approaches to learning latent representations in PCs are limited, and there is a need for scalable and differentiable methods to learn such representations effectively.
- What evidence would resolve it: Proposing and validating a scalable and differentiable approach to learning latent representations in PCs, demonstrating improved performance and interpretability compared to existing methods.

### Open Question 3
- Question: How can adversarial training be effectively applied to probabilistic circuits to improve sample quality and expressiveness, given the challenge of non-differentiable sampling in PCs?
- Basis in paper: [explicit] The paper mentions that while adversarial losses have been explored in DGMs to improve sample quality, their application to PCs is relatively unexplored due to the non-differentiable nature of sampling in PCs.
- Why unresolved: The non-differentiability of sampling in PCs poses a challenge for backpropagating the output of an adversarial discriminator, making it difficult to apply adversarial training effectively.
- What evidence would resolve it: Developing differentiable sampling strategies for PCs and demonstrating the successful application of adversarial training to improve sample quality and expressiveness in PCs.

## Limitations

- The survey primarily focuses on structural properties but does not extensively address scalability limitations when PCs are scaled to high-dimensional data or complex distributions.
- While neural extensions are claimed to preserve tractability, the specific conditions under which various neural modules maintain structural properties are not rigorously proven.

## Confidence

- High confidence: The fundamental mechanisms of tractability through smoothness, decomposability, and determinism are well-established in the PC literature and supported by multiple citations.
- Medium confidence: The learning algorithms (gradient-based, EM, closed-form under determinism) are described accurately, though practical convergence and stability may vary depending on implementation details.
- Medium confidence: The claims about neural extensions preserving tractability are plausible based on cited work, but require careful validation to ensure structural properties are maintained in practice.

## Next Checks

1. Implement a minimal PC with smoothness and decomposability constraints, then systematically violate each property to empirically verify when tractability breaks (e.g., test marginal inference on non-decomposable structures).
2. Compare parameter learning performance between closed-form solutions (deterministic case) and iterative methods (non-deterministic case) on a controlled dataset to validate the claimed computational advantages.
3. Integrate a simple neural module (e.g., convolutional sum node) into a PC and verify through structural analysis that smoothness and decomposability properties are preserved, then test inference tractability.