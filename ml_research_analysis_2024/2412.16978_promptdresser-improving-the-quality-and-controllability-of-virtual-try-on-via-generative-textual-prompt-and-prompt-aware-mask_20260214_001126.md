---
ver: rpa2
title: 'PromptDresser: Improving the Quality and Controllability of Virtual Try-On
  via Generative Textual Prompt and Prompt-aware Mask'
arxiv_id: '2412.16978'
source_url: https://arxiv.org/abs/2412.16978
tags:
- clothing
- mask
- try-on
- virtual
- pose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PromptDresser, a virtual try-on model that
  leverages large multimodal models (LMMs) to generate rich, well-aligned text descriptions
  for both person and clothing images, enabling text-driven clothing manipulation.
  The method addresses three key challenges: (1) designing detailed yet compact text
  prompts for paired person-clothing data, (2) avoiding textual conflicts between
  original and new clothing during generation, and (3) adaptively adjusting inpainting
  masks to align with text prompts.'
---

# PromptDresser: Improving the Quality and Controllability of Virtual Try-On via Generative Textual Prompt and Prompt-aware Mask

## Quick Facts
- **arXiv ID**: 2412.16978
- **Source URL**: https://arxiv.org/abs/2412.16978
- **Reference count**: 40
- **Primary result**: Achieves FID scores of 8.54 (VITON-HD) and 11.00 (DressCode), significantly outperforming baselines on virtual try-on quality and text alignment.

## Executive Summary
PromptDresser is a virtual try-on model that leverages large multimodal models (LMMs) to generate rich, well-aligned text descriptions for both person and clothing images, enabling text-driven clothing manipulation. The method addresses three key challenges: (1) designing detailed yet compact text prompts for paired person-clothing data, (2) avoiding textual conflicts between original and new clothing during generation, and (3) adaptively adjusting inpainting masks to align with text prompts. Experiments show PromptDresser significantly outperforms baselines on VITON-HD and DressCode datasets, achieving FID scores of 8.54 and 11.00 respectively, while enabling versatile text-based editing and high-fidelity clothing manipulation.

## Method Summary
PromptDresser uses LMMs (GPT-4o/LLaVA) to generate attribute-specific captions for person and clothing via in-context learning with few-shot exemplars. These captions are combined into a unified text prompt for a fine-tuned diffusion inpainting model (SDXL). Training incorporates random dilation mask augmentation to enable flexible inpainting boundaries. During inference, a coarse-to-fine mask refinement process (prompt-aware mask generation) adapts the inpainting area to the new clothing shape while preserving the original person's appearance. The method is trained on paired person-clothing images from VITON-HD and DressCode datasets.

## Key Results
- **State-of-the-art FID scores**: 8.54 on VITON-HD and 11.00 on DressCode datasets, significantly outperforming baseline methods.
- **Improved text alignment**: High accuracy in matching generated images to text descriptions (e.g., "untucked" vs "fully tucked").
- **Versatile editing capabilities**: Enables text-based clothing manipulation (length, style, fit) while preserving person appearance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating person and clothing captions avoids textual conflicts during clothing replacement.
- Mechanism: LMMs generate person-specific and clothing-specific captions independently via in-context learning, then combine them into a unified prompt for inpainting.
- Core assumption: Person and clothing features can be disentangled into distinct attribute sets without loss of essential details.
- Evidence anchors:
  - [abstract]: "we propose designing pre-defined attributes and tasking the LMM with generating captions based on these attributes."
  - [section 3.3]: "Therefore, we designed our approach to separate information by pre-defining distinct attributes for the person and clothing."
  - [corpus]: Weak/no direct evidence of disentanglement benefit; no corpus entry explicitly supports this claim.
- Break condition: If attributes overlap significantly (e.g., clothing length affects person silhouette), disentanglement introduces errors.

### Mechanism 2
- Claim: Prompt-aware mask generation improves clothing shape fidelity by reducing bias toward original garment form.
- Mechanism: Random dilation mask augmentation creates a range of inpainting masks from coarse to fine, enabling the model to learn adaptive editing areas.
- Core assumption: Training with varied mask sizes helps the model generalize to arbitrary clothing shapes during inference.
- Evidence anchors:
  - [section 3.4]: "To address these issues, we propose random dilation mask augmentation. As illustrated in Fig 2, we introduce a coarse mask mc and a fine mask mf to enable learning across a diverse range of masked images."
  - [section 3.4]: "This approach allows for a prompt-aware mask generation (PMG) during the inference, providing preservation of the original person's appearance irrelevant to the reference clothing."
  - [corpus]: No corpus entry discusses mask dilation or adaptive masking.
- Break condition: If the coarse-to-fine process oversmooths fine details or introduces artifacts.

### Mechanism 3
- Claim: In-context learning with few-shot exemplars improves caption specificity for pose and style attributes.
- Mechanism: LMMs are prompted with a small set of labeled attribute examples to generate detailed, consistent descriptions for new images.
- Core assumption: LMMs can generalize from few examples to produce high-quality attribute labels for unseen images.
- Evidence anchors:
  - [section 3.3]: "we utilize the in-context learning capability [7] of LMM models to generate rich, free-form captions for pre-defined attributes. Based on the observation that multi-modal models are proficient at predicting head categories such as gender [33], we carefully select N few-shot exemplar images..."
  - [section 3.3]: "Human annotators then label each caption to capture these specific details accurately."
  - [corpus]: No corpus entry provides evidence for in-context learning efficacy in virtual try-on.
- Break condition: If the few-shot set is too small or unrepresentative, LMMs produce noisy or biased captions.

## Foundational Learning

- **Concept**: In-context learning
  - Why needed here: Enables LMMs to generate attribute-specific captions without fine-tuning.
  - Quick check question: Can the LMM produce consistent attribute labels given only a few examples?

- **Concept**: Latent diffusion inpainting
  - Why needed here: Provides the generative backbone for realistic clothing transfer and editing.
  - Quick check question: Does the model preserve non-target regions (face, background) during inpainting?

- **Concept**: Mask dilation and erosion
  - Why needed here: Allows flexible inpainting boundaries to adapt to new clothing shapes.
  - Quick check question: Does varying the dilation radius change the generated clothing length?

## Architecture Onboarding

- **Component map**: LMM caption generator -> Frozen SDXL U-Net (feature extractor) -> Fine-tuned SDXL inpainting U-Net (generator) -> Random dilation module -> Prompt-aware mask generator
- **Critical path**:
  1. Generate person and clothing captions via LMM
  2. Build unified text prompt
  3. Apply random dilation to training mask
  4. Inpaint with fine-tuned U-Net
  5. Refine mask at inference via coarse-to-fine generation
- **Design tradeoffs**:
  - Caption granularity vs. token length limits
  - Mask coarseness vs. detail preservation
  - Inference speed vs. denoising steps for prompt-aware mask
- **Failure signatures**:
  - Blurry person details -> mask too large or denoising insufficient
  - Clothing shape mismatch -> mask not aligned with prompt
  - Caption errors -> in-context examples too few or ambiguous
- **First 3 experiments**:
  1. Test caption generation quality with varying few-shot sizes.
  2. Measure FID/KID with and without random dilation augmentation.
  3. Compare text alignment accuracy (e.g., "untucked" vs "fully tucked") between PromptDresser and baseline models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model perform on extreme pose variations or occlusions not present in the training data?
- Basis in paper: [inferred] The paper mentions pose descriptions are used instead of DensePose to avoid pose network errors, but does not explicitly evaluate performance on challenging poses or occluded body parts.
- Why unresolved: The ablation study focuses on adding/removing pose information but does not test model robustness to extreme or occluded poses not seen during training.
- What evidence would resolve it: Quantitative evaluation on a dataset with diverse and challenging poses/occlusions, comparing model performance with and without DensePose.

### Open Question 2
- Question: What is the upper limit of the number of attributes that can be effectively handled by the LMM in-context learning for captioning?
- Basis in paper: [explicit] The paper mentions selecting a subset of attributes due to the CLIP text encoder's 77-token limit, but does not explore the maximum number of attributes that can be effectively used.
- Why unresolved: The paper selects a fixed set of attributes but does not experiment with varying the number of attributes to find the optimal or maximum limit for effective captioning.
- What evidence would resolve it: Systematic experiments varying the number of attributes used in in-context learning and measuring the impact on caption quality and virtual try-on performance.

### Open Question 3
- Question: How does the model handle clothing categories beyond those in the training datasets (e.g., swimwear, formalwear)?
- Basis in paper: [inferred] The model is trained on VITON-HD and DressCode datasets which include upper body, lower body, and dresses, but there is no mention of testing on out-of-distribution clothing categories.
- Why unresolved: The paper focuses on performance within the training dataset categories and does not evaluate generalization to novel clothing types not seen during training.
- What evidence would resolve it: Testing the model on datasets containing diverse clothing categories like swimwear, formalwear, or cultural attire, and measuring performance metrics like FID and user preference.

### Open Question 4
- Question: What is the computational overhead introduced by the LMM-driven captioning and prompt-aware mask generation during inference?
- Basis in paper: [explicit] The paper mentions applying early stopping for computational efficiency in prompt-aware mask generation, but does not provide detailed timing comparisons between different model variants.
- Why unresolved: While inference times are mentioned in the ablation study, there is no comprehensive analysis of the computational cost of the LMM captioning and mask refinement steps relative to the overall pipeline.
- What evidence would resolve it: Detailed profiling of inference time for each component (LMM captioning, mask refinement, generation) and comparison with baseline models to quantify the overhead.

## Limitations
- **Reproducibility concerns**: Insufficient detail on exact template format and pre-defined attributes for in-context learning.
- **Hyperparameter uncertainty**: Lack of specific settings for random dilation (kernel size, iteration count).
- **Computational overhead**: No quantification of inference time added by LMM captioning and prompt-aware mask generation.

## Confidence

**High Confidence**: The overall architecture combining LMM captions with diffusion-based inpainting is well-grounded in existing literature. The quantitative improvements on VITON-HD and DressCode datasets (FID scores of 8.54 and 11.00) are substantial and directly comparable to baseline methods.

**Medium Confidence**: The mechanism of separating person and clothing captions to avoid textual conflicts is intuitively sound, but lacks direct empirical validation in the paper. The claim that random dilation augmentation improves generalization to arbitrary clothing shapes is plausible but not conclusively demonstrated.

**Low Confidence**: The effectiveness of in-context learning with few-shot exemplars for caption generation is assumed rather than proven. The paper does not provide ablation studies showing how caption quality degrades with fewer examples or how caption errors propagate to final output quality.

## Next Checks

1. **Caption Quality Validation**: Generate captions using varying numbers of few-shot examples (1, 3, 5, 10) and measure inter-annotator agreement on attribute accuracy. Compare text alignment scores between captions generated with minimal vs. extensive few-shot sets.

2. **Mask Dilation Sensitivity**: Systematically vary dilation kernel sizes and iteration counts during training. Measure FID/KID scores and clothing shape fidelity (e.g., sleeve length accuracy) across different dilation settings to identify optimal parameters.

3. **Inference Speed Analysis**: Profile inference time with and without prompt-aware mask generation. Measure the trade-off between mask refinement iterations and final output quality metrics (FID, SSIM) to determine practical limits for real-time applications.