---
ver: rpa2
title: 'Neuromorphic Drone Detection: an Event-RGB Multimodal Approach'
arxiv_id: '2409.16099'
source_url: https://arxiv.org/abs/2409.16099
tags:
- drone
- detection
- event
- fusion
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present NeRDD, a novel multimodal neuromorphic-RGB dataset for
  drone detection, and propose an end-to-end fusion model for joint Event-RGB processing.
  We study the impact of different fusion strategies, including pooling-based fusion,
  asymmetric modality injection and symmetric fusion.
---

# Neuromorphic Drone Detection: an Event-RGB Multimodal Approach

## Quick Facts
- arXiv ID: 2409.16099
- Source URL: https://arxiv.org/abs/2409.16099
- Reference count: 40
- Introduces NeRDD dataset and shows event-based models outperform RGB counterparts with further gains from fusion

## Executive Summary
This paper presents NeRDD, a novel multimodal dataset for drone detection using both neuromorphic event cameras and RGB sensors. The authors propose an end-to-end fusion model that combines event and RGB modalities through different fusion strategies. Experiments demonstrate that event-based models significantly outperform RGB-only approaches, while fusion further improves detection accuracy. The work establishes a new benchmark for drone detection and provides insights into effective multimodal fusion strategies.

## Method Summary
The authors develop a multimodal detection system using DETR architecture with ResNet-50 backbone. They implement three fusion strategies: pooling-based fusion where features are averaged before decoder, asymmetric modality injection using cross-attention where events inform RGB features, and symmetric fusion allowing bidirectional information flow. Event frames are generated by accumulating spikes over 33ms intervals (30fps) to match RGB temporal resolution. The model is trained for 30 epochs with AdamW optimizer and evaluated on the NeRDD dataset containing 3.5 hours of synchronized event-RGB data.

## Key Results
- Event-based models achieve AP50 of 83.2, significantly outperforming RGB counterparts
- Pooling-based fusion improves AP50 by 4.3 points over event-only baseline
- Symmetric fusion provides the best performance but with increased parameters
- Asymmetric fusion achieves good results while maintaining efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Event frames preserve high temporal resolution and robustness to motion blur
- Mechanism: Event cameras record only pixel-level brightness changes asynchronously, producing dense event streams for fast-moving objects that are accumulated into discriminative event frames
- Core assumption: Drone motion generates sufficient events in short accumulation windows
- Evidence anchors: Abstract mentions resilience to high-speed moving objects; Section 3 describes 33ms accumulation windows
- Break condition: Slow drone movement or static scenes reduce event density and discriminative power

### Mechanism 2
- Claim: Encoder-level feature pooling allows effective cross-modal interaction
- Mechanism: Separate backbone feature extraction followed by pooling preserves spatial structure while enabling the model to learn complementary cues
- Core assumption: Convolutional features share sufficient spatial alignment for meaningful pooling
- Evidence anchors: Section 3 details pooling-based fusion strategy; Section 5.3 shows only 0.5 AP50 drop with reduced precision
- Break condition: Severe misalignment or resolution mismatch corrupts spatial cues during pooling

### Mechanism 3
- Claim: Asymmetric cross-attention lets events guide RGB feature attention
- Mechanism: Event tokens serve as queries while RGB features act as keys/values, highlighting regions aligned with event activity
- Core assumption: Event activity reliably indicates drone movement locations
- Evidence anchors: Section 3 describes one-way information injection; Section 5.2 shows significant AP improvement with cross-attention
- Break condition: Noisy or sparse events cause attention to irrelevant regions

## Foundational Learning

- Concept: Neuromorphic event cameras vs frame cameras
  - Why needed here: To understand why events excel at fast motion and low-light detection
  - Quick check question: What is the main difference in data representation between an event camera and a conventional RGB camera?

- Concept: Transformer-based object detection (DETR architecture)
  - Why needed here: To grasp how modality fusion integrates into a set-based detection framework
  - Quick check question: In DETR, how does the Hungarian matching loss replace the need for NMS?

- Concept: Multimodal fusion strategies (early, middle, late)
  - Why needed here: To reason about trade-offs in fusing event and RGB streams
  - Quick check question: What is a key advantage of fusing at an intermediate encoder layer compared to early or late fusion?

## Architecture Onboarding

- Component map: Event accumulation -> Backbone feature extraction -> Encoder-level pooling -> Decoder processing -> Bounding box prediction
- Critical path: Event accumulation → backbone feature extraction → encoder-level pooling → decoder processing → bounding box prediction
- Design tradeoffs: Event accumulation window length vs temporal precision; fusion layer choice vs parameter count; object query number vs computational efficiency
- Failure signatures: Low AP75 with moderate AP50 indicates RGB motion blur issues; asymmetric fusion underperforming suggests misalignment; symmetric fusion degrading indicates overfitting
- First 3 experiments: 1) Baseline DETR on event frames only, 2) Early fusion pooling event and RGB features, 3) Cross-attention fusion with asymmetric event-to-RGB attention

## Open Questions the Paper Calls Out

- What is the optimal temporal resolution for event frame aggregation in drone detection tasks?
  - Basis: Paper mentions 30fps constraint could be relaxed for finer-grained motion analysis
  - Why unresolved: No exploration of different temporal resolutions
  - What evidence would resolve it: Comparative experiments across different aggregation intervals (10ms, 33ms, 100ms)

- How does incorporating temporal information from event streams affect detection performance?
  - Basis: Authors state they "completely disregard time by processing frames individually"
  - Why unresolved: Focus on spatial features only
  - What evidence would resolve it: Experiments comparing single-frame processing against temporal models using recurrent networks or optical flow

- What is the impact of object query number on detection performance for different drone sizes and numbers?
  - Basis: Experiments with 5, 10, 25, 100 queries but only tested on maximum 2 drones
  - Why unresolved: Limited evaluation scenarios
  - What evidence would resolve it: Experiments varying drone count and size while testing different query numbers

## Limitations
- Dataset size (92 training videos) may limit generalizability to new drone types and environments
- Event-RGB synchronization relies on precise calibration without detailed methodology
- Asymmetric fusion's one-way information flow may miss complementary RGB-to-event cues

## Confidence
- **High confidence:** Event frames preserve motion information for fast-moving drones; baseline DETR performance on event modality (AP50: 83.2, AP50:95: 57.6)
- **Medium confidence:** Fusion strategy effectiveness; real-world robustness beyond controlled lab conditions
- **Low confidence:** Cross-attention mechanism implementation details; generalization to different drone sizes and lighting conditions

## Next Checks
1. Test model performance on temporally disjoint subsets to assess temporal generalization and detect overfitting to specific motion patterns
2. Evaluate detection latency and processing speed to verify real-time capability claims under varying event densities
3. Conduct ablation studies with different event accumulation window sizes (Δt) to quantify the tradeoff between temporal precision and event density