---
ver: rpa2
title: 'Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad'
arxiv_id: '2403.02648'
source_url: https://arxiv.org/abs/2403.02648
tags:
- adagrad
- should
- data
- learning
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KA TE, a new scale-invariant version of the
  AdaGrad algorithm. KA TE removes the square root from the denominator of the AdaGrad
  stepsize and compensates with an increasing sequence in the numerator to maintain
  convergence.
---

# Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad

## Quick Facts
- arXiv ID: 2403.02648
- Source URL: https://arxiv.org/abs/2403.02648
- Authors: Sayantan Choudhury; Nazarii Tupitsa; Nicolas Loizou; Samuel Horvath; Martin Takac; Eduard Gorbunov
- Reference count: 40
- Introduces KA TE, a scale-invariant AdaGrad variant that removes the square root from the denominator and achieves O(log T / √T) convergence for smooth non-convex problems

## Executive Summary
This paper introduces KA TE, a new scale-invariant version of the AdaGrad algorithm that removes the square root from the denominator of the stepsize and compensates with an increasing sequence in the numerator to maintain convergence. For smooth non-convex problems, KA TE achieves an O(log T / √T) convergence rate, matching the best-known rates for AdaGrad and Adam. The method is proven to be scale-invariant for Generalized Linear Models, meaning its performance is independent of data scaling. Numerical experiments on logistic regression, image classification, and text classification show that KA TE consistently outperforms AdaGrad and matches or surpasses Adam in all considered scenarios.

## Method Summary
KA TE modifies the standard AdaGrad algorithm by removing the square root from the denominator of the stepsize formula and introducing an increasing sequence in the numerator. This modification maintains the convergence guarantees while achieving scale-invariance for Generalized Linear Models. The key insight is that by removing the square root, the method becomes invariant to data scaling, which is particularly beneficial for machine learning problems where feature magnitudes can vary significantly. The increasing sequence in the numerator compensates for the removal of the square root to ensure proper convergence behavior. The algorithm maintains the adaptive learning rate property of AdaGrad while achieving better theoretical guarantees and practical performance across various optimization tasks.

## Key Results
- Achieves O(log T / √T) convergence rate for smooth non-convex problems, matching the best-known rates for AdaGrad and Adam
- Proven to be scale-invariant for Generalized Linear Models, ensuring performance independence from data scaling
- Consistently outperforms AdaGrad and matches or surpasses Adam across logistic regression, image classification, and text classification tasks

## Why This Works (Mechanism)
KA TE works by removing the square root from the denominator of AdaGrad's stepsize while introducing an increasing sequence in the numerator. This modification preserves the adaptive learning rate property while making the algorithm scale-invariant. The increasing sequence compensates for the removal of the square root, ensuring that the algorithm maintains proper convergence behavior. For Generalized Linear Models, this modification ensures that the algorithm's performance is independent of the scale of the input data, which is a significant advantage in practical applications where feature magnitudes can vary widely.

## Foundational Learning

**Smooth Non-Convex Optimization**: Understanding convergence rates for non-convex problems
*Why needed*: The paper targets smooth non-convex optimization problems where gradient-based methods are commonly applied
*Quick check*: Verify that the objective function satisfies L-smoothness and is bounded below

**Adaptive Gradient Methods**: Knowledge of AdaGrad and its variants
*Why needed*: KA TE is a modification of AdaGrad, requiring understanding of its mechanics
*Quick check*: Compare stepsize formulas between AdaGrad and KA TE

**Scale-Invariance**: Concept of algorithm performance being independent of data scaling
*Why needed*: The main contribution is achieving scale-invariance for GLMs
*Quick check*: Test algorithm performance on datasets with different feature scalings

**Convergence Analysis**: Understanding of how to prove convergence rates
*Why needed*: The paper provides theoretical convergence guarantees
*Quick check*: Verify that the proof technique follows standard Lyapunov function analysis

## Architecture Onboarding

**Component Map**: Initialize parameters -> Compute gradient -> Update accumulator (without sqrt) -> Compute stepsize with increasing sequence -> Update parameters -> Check convergence
**Critical Path**: The stepsize computation is critical, involving the accumulator update and the increasing sequence calculation
**Design Tradeoffs**: Removing the square root improves scale-invariance but requires careful design of the increasing sequence to maintain convergence
**Failure Signatures**: Poor convergence if the increasing sequence is not properly tuned; instability if the accumulator grows too quickly
**First Experiments**: 1) Test KA TE on a simple logistic regression problem with differently scaled features 2) Compare convergence rates on a smooth non-convex synthetic problem 3) Evaluate performance on a small image classification dataset with varying feature scales

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical convergence rate of O(log T / √T) has not been compared against the best-known lower bounds for non-convex smooth problems
- Scale-invariance claim for GLMs needs more extensive validation across diverse datasets and problem structures
- Evaluation is limited to three specific tasks (logistic regression, image classification, and text classification) and may not generalize to other problem domains

## Confidence
High: Theoretical contributions with rigorous convergence analysis and proof of scale-invariance
Medium: Empirical superiority with limited scope of experiments and absence of comparisons with more recent adaptive methods

## Next Checks
1. Test KA TE on a broader range of optimization problems including deep learning architectures with different activation functions and loss landscapes
2. Conduct a systematic ablation study varying the increasing sequence parameters to understand their impact on convergence and performance
3. Compare KA TE against other recent adaptive gradient methods like Yogi, QHAdam, and Padam to establish its relative standing in the current state-of-the-art