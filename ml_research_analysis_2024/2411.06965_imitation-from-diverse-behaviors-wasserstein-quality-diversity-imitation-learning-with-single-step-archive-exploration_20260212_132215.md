---
ver: rpa2
title: 'Imitation from Diverse Behaviors: Wasserstein Quality Diversity Imitation
  Learning with Single-Step Archive Exploration'
arxiv_id: '2411.06965'
source_url: https://arxiv.org/abs/2411.06965
tags:
- learning
- reward
- archive
- adversarial
- wasserstein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning diverse and high-quality
  behaviors from limited expert demonstrations in imitation learning. The key issue
  is that traditional adversarial imitation learning methods suffer from training
  instability and behavior-overfitted rewards, limiting their ability to discover
  diverse behaviors beyond the demonstrations.
---

# Imitation from Diverse Behaviors: Wasserstein Quality Diversity Imitation Learning with Single-Step Archive Exploration

## Quick Facts
- arXiv ID: 2411.06965
- Source URL: https://arxiv.org/abs/2411.06965
- Authors: Xingrui Yu; Zhenglin Wan; David Mark Bossens; Yueming Lyu; Qing Guo; Ivor W. Tsang
- Reference count: 40
- Primary result: Significantly outperforms state-of-the-art imitation learning baselines, achieving near-expert or beyond-expert quality diversity performance on continuous control tasks from MuJoCo environments

## Executive Summary
This paper addresses the challenge of learning diverse and high-quality behaviors from limited expert demonstrations in imitation learning. Traditional adversarial imitation learning methods suffer from training instability and behavior-overfitted rewards, limiting their ability to discover diverse behaviors beyond the demonstrations. The authors propose Wasserstein Quality Diversity Imitation Learning (WQDIL) with Single-Step Archive Exploration, which stabilizes reward learning through latent Wasserstein adversarial training in a Wasserstein Auto-Encoder (WAE) and encourages exploration using a measure-conditioned reward with a single-step archive exploration bonus. The method significantly outperforms state-of-the-art imitation learning baselines, achieving near-expert or beyond-expert quality diversity performance on continuous control tasks from MuJoCo environments.

## Method Summary
WQDIL combines Wasserstein Auto-Encoder (WAE) with latent Wasserstein adversarial training and measure-conditioned rewards with single-step archive exploration bonus. The method is built on top of PPGA algorithm and addresses behavior overfitting by conditioning rewards on single-step measures while encouraging exploration through an archive-based bonus. Expert demonstrations are generated from a policy archive obtained by PPGA, with 4 diverse demonstrations selected from the top 500 high-performance elites per environment. The framework trains a WAE to encode expert and policy data into a shared latent space, where a latent Wasserstein discriminator minimizes the Wasserstein distance between distributions. The reward model combines discriminator output with exploration bonus, and a QD optimizer maintains diverse policy archive.

## Key Results
- WQDIL achieves near-expert or beyond-expert quality diversity performance on HalfCheetah, Walker2d, and Humanoid environments
- Substantial improvements in QD-Score metrics compared to GAIL, PWIL, AIRL, MaxEntIRL, and GIRIL baselines
- Successfully mitigates behavior-overfitting issue by learning diverse behaviors beyond expert demonstrations
- Single-step archive exploration bonus effectively promotes coverage of the full behavior space

## Why This Works (Mechanism)

### Mechanism 1: Latent Wasserstein Adversarial Training
- Claim: Latent Wasserstein adversarial training stabilizes reward learning by minimizing the Wasserstein distance in the latent space of a Wasserstein Auto-Encoder (WAE).
- Mechanism: The WAE encodes expert demonstrations and policy-generated data into a shared latent space. A latent Wasserstein discriminator then minimizes the 1-Wasserstein distance between these distributions, providing more stable gradients than standard GAN-based adversarial training.
- Core assumption: The latent space of the WAE provides a meaningful, stable manifold for comparing distributions, and the Wasserstein distance yields smoother gradients than Jensen-Shannon divergence.
- Evidence anchors:
  - [abstract] "improves the stability of imitation learning in the quality diversity setting with latent adversarial training based on a Wasserstein Auto-Encoder (WAE)"
  - [section 3.2] "WAE keeps the good properties of VAEs (stable training and a nice latent manifold structure) while generating better-quality images than GAN [47]"
  - [corpus] Weak evidence - related papers focus on diverse behaviors but do not specifically validate WAE-based stability improvements.
- Break condition: If the latent space does not capture meaningful behavioral differences, or if the Wasserstein discriminator fails to converge, stability gains disappear.

### Mechanism 2: Single-Step Archive Exploration Bonus
- Claim: The single-step archive exploration bonus encourages behavior space exploration by rewarding visits to underrepresented behavioral regions.
- Mechanism: The algorithm maintains a discretized archive of single-step measure states. Each time a state activates a cell, its visitation count increments. The bonus formula 1/(1 + p(δ(s))) ensures underexplored regions always receive a non-zero bonus, promoting coverage of the full behavior space.
- Core assumption: Behavioral diversity is best encouraged by directly rewarding exploration of the measure space rather than the state-action space, and the bonus remains effective throughout training.
- Evidence anchors:
  - [section 3.1] "The exploration bonus assigns higher rewards to regions in A_single that are less frequently visited, thereby promoting the agent to explore unseen behavior patterns."
  - [section 2.5] "Contrasting to UCRL type bonuses, due to dividing by the proportion of visits, rather than the count, our measure bonus does not shrink to zero such that underexplored behaviors will continue to receive a bonus."
  - [corpus] Weak evidence - related work mentions exploration bonuses but not specifically measure-conditioned single-step exploration.
- Break condition: If the discretization of the measure space is too coarse or too fine, the bonus may either miss opportunities or become noisy and ineffective.

### Mechanism 3: Measure Conditioning for Behavior Sensitivity
- Claim: Measure conditioning makes the reward function sensitive to local behavior patterns, preventing behavior-overfitted rewards.
- Mechanism: The reward model is conditioned on both state-action pairs and the single-step measure proxy, creating different reward functions for different behavioral regions. This allows the agent to learn distinct policies for different behavior patterns rather than converging to a single behavior.
- Core assumption: Different behaviors require different reward functions to be effectively learned, and conditioning on the measure provides sufficient information to distinguish these behaviors.
- Evidence anchors:
  - [section 3.1] "the reward model is formulated as... This contributes to a solution to behavior overfitting: since the goal is to form behaviorally diverse policies, different behaviors require different policies to be imitated – and hence correspond to a different reward function."
  - [abstract] "mitigates a behavior-overfitting issue using a measure-conditioned reward function with a single-step archive exploration bonus"
  - [corpus] Moderate evidence - related work on QDIL mentions measure conditioning but lacks detailed validation of this specific approach.
- Break condition: If the measure conditioning does not capture sufficient behavioral information, or if the policy cannot effectively use the conditioned rewards, the diversity benefits disappear.

## Foundational Learning

- Concept: Wasserstein distance and optimal transport theory
  - Why needed here: The core stability mechanism relies on minimizing Wasserstein distance in the latent space rather than using standard GAN objectives.
  - Quick check question: Can you explain why Wasserstein distance provides more stable gradients than Jensen-Shannon divergence in adversarial training?

- Concept: Quality Diversity optimization and archive-based exploration
  - Why needed here: The method builds on QDRL frameworks that maintain archives of diverse, high-performing solutions across behavior space.
  - Quick check question: How does the archive structure in QDRL differ from standard reinforcement learning approaches, and why is this important for learning diverse behaviors?

- Concept: Auto-encoders and latent space representations
  - Why needed here: The WAE encodes demonstrations and policy data into a shared latent space where distribution matching occurs.
  - Quick check question: What properties make the WAE's latent space particularly suitable for stable adversarial training compared to standard auto-encoders or GANs?

## Architecture Onboarding

- Component map: WAE encoder/decoder -> Latent Wasserstein discriminator -> Single-step archive -> Reward model -> QD optimizer (PPGA)
- Critical path: Demonstrations → WAE encoding → Latent Wasserstein training → Reward model update → Policy optimization with measure conditioning and exploration bonus → Archive update
- Design tradeoffs: 
  - WAE vs. VAE: WAE provides better image quality and stable training but may require more careful hyperparameter tuning
  - Single-step vs. episodic measures: Single-step provides finer granularity but requires more storage and computation
  - Archive discretization: Coarser grids are computationally cheaper but may miss behavioral nuances
- Failure signatures:
  - Training instability: Discriminator loss oscillations or WAE reconstruction failures
  - Poor diversity: Archive coverage remains low despite exploration bonus
  - Behavior overfitting: Policies converge to narrow behavior regions despite measure conditioning
- First 3 experiments:
  1. Verify WAE stability: Train WAE on demonstration data and visualize latent space organization
  2. Test exploration bonus: Run policy with only exploration bonus (no measure conditioning) and measure archive coverage
  3. Validate measure conditioning: Compare policy performance with and without measure conditioning on a simple behavior space

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of WQDIL scale with the number and diversity of expert demonstrations?
- Basis in paper: [inferred] The paper focuses on a limited number of diverse demonstrations (4 per environment) and notes the behavior-overfitting issue with limited demonstrations, but does not systematically explore scaling to more demonstrations or less diverse ones.
- Why unresolved: The experiments only use 4 demonstrations per environment, so the effect of increasing demonstration count or decreasing diversity is untested.
- What evidence would resolve it: Experiments varying the number and diversity of demonstrations while measuring QD-Score, Coverage, and stability metrics.

### Open Question 2
- Question: What is the computational overhead of WQDIL compared to standard adversarial IL methods like GAIL?
- Basis in paper: [explicit] The paper mentions WAE optimization and latent Wasserstein adversarial training as key components but does not provide runtime or computational complexity comparisons.
- Why unresolved: While the paper demonstrates superior performance, it does not quantify the additional computational cost of the WAE and measure-conditioning components.
- What evidence would resolve it: Detailed runtime measurements comparing wall-clock time and computational resources for WQDIL versus GAIL, PWIL, and other baselines.

### Open Question 3
- Question: How sensitive is WQDIL's performance to the choice of hyperparameters for the WAE, measure-conditioning, and exploration bonus?
- Basis in paper: [explicit] The paper provides hyperparameter tables but does not conduct sensitivity analysis or ablation studies on these specific components.
- Why unresolved: The ablation studies focus on architectural choices (Wasserstein vs standard GAN, measure conditioning vs not) but not on tuning the individual hyperparameters within these components.
- What evidence would resolve it: Sensitivity analysis varying key hyperparameters (regularization coefficient, learning rates, exploration bonus weight) and measuring performance impact.

### Open Question 4
- Question: Can WQDIL effectively handle environments with high-dimensional state spaces or continuous action spaces beyond the MuJoCo benchmarks?
- Basis in paper: [inferred] The paper demonstrates success on 3 MuJoCo continuous control tasks but does not test on more complex or higher-dimensional environments.
- Why unresolved: The experiments are limited to standard MuJoCo benchmarks, which may not fully capture the challenges of more complex real-world scenarios.
- What evidence would resolve it: Testing WQDIL on environments with higher-dimensional states/actions, such as robotics manipulation tasks or more complex locomotion environments, and comparing performance.

## Limitations
- Single-step archive exploration may face scalability challenges in high-dimensional behavior spaces due to curse of dimensionality in discretization
- WAE-based stability improvements rely on empirical validation rather than rigorous mathematical proofs of convergence
- Measure conditioning approach assumes single-step measures adequately capture behavioral diversity, which may not hold for all task types

## Confidence
- High confidence: The overall framework combining WAE with latent Wasserstein training and measure-conditioned rewards is technically sound and follows established principles in adversarial imitation learning and quality diversity optimization.
- Medium confidence: The specific implementation details of the single-step archive exploration bonus and its integration with the PPGA framework are reasonable but lack complete specification for independent reproduction.
- Medium confidence: The reported performance improvements are substantial and consistent across multiple metrics, though the comparison is limited to specific baselines and environments.

## Next Checks
1. Test WQDIL on additional environments with different behavioral characteristics to verify generalization beyond the MuJoCo control tasks used in the paper.
2. Conduct ablation studies to isolate the contributions of each component (WAE stability, measure conditioning, exploration bonus) by systematically removing them.
3. Evaluate the method's performance when provided with varying amounts of expert demonstrations (e.g., 1, 2, 8 demonstrations) to understand its robustness to demonstration scarcity.