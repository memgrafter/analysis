---
ver: rpa2
title: Large Language Models Memorize Sensor Datasets! Implications on Human Activity
  Recognition Research
arxiv_id: '2406.05900'
source_url: https://arxiv.org/abs/2406.05900
tags:
- data
- sensor
- llms
- datasets
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers investigated whether GPT-4 has been trained on public
  wearable sensor datasets by applying memorization tests, specifically the row completion
  test. This involved prompting GPT-4 with contiguous sensor data rows and instructing
  it to complete the next row verbatim.
---

# Large Language Models Memorize Sensor Datasets! Implications on Human Activity Recognition Research

## Quick Facts
- **arXiv ID**: 2406.05900
- **Source URL**: https://arxiv.org/abs/2406.05900
- **Reference count**: 40
- **Primary result**: GPT-4 accurately reproduced Daphnet FoG dataset data with Levenshtein ratio of 0.8074, suggesting memorization

## Executive Summary
This study investigates whether large language models, specifically GPT-4, have been trained on public wearable sensor datasets used for Human Activity Recognition (HAR) research. The researchers applied memorization tests, particularly the row completion test, to five public HAR datasets by prompting GPT-4 with contiguous sensor data rows and measuring its ability to reproduce subsequent rows verbatim. Results showed that GPT-4 demonstrated significant memorization of the Daphnet FoG dataset, raising important questions about the validity of current HAR evaluation protocols and the potential for data contamination in LLM training.

## Method Summary
The researchers employed a row completion test where GPT-4 was prompted with ten consecutive rows of sensor data from five public HAR datasets (Capture-24, HHAR, PAMAP2, MHEALTH, and Daphnet FoG) and instructed to complete the next row verbatim. The Levenshtein ratio was used to compare generated rows against ground truth rows, with higher ratios indicating closer matches. The test was repeated 25 times for each dataset with randomly sampled starting points. Few-shot examples were provided as context to guide GPT-4's predictions. The approach aimed to detect whether GPT-4 had memorized specific sensor data patterns from training.

## Key Results
- GPT-4 accurately reproduced Daphnet FoG dataset data with a Levenshtein ratio of 0.8074
- Other datasets showed lower memorization ratios, with Capture-24 being the next most accurately reproduced
- Datasets with high repetition rates (due to sensor issues or high sampling frequencies) posed challenges for memorization detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 memorizes sensor data through direct reproduction of contiguous timesteps
- Mechanism: The Row Completion test prompts GPT-4 with ten consecutive rows of sensor data and asks it to generate the next row verbatim. When GPT-4 accurately reproduces sensor values across multiple rows, this indicates memorization of those data patterns from training.
- Core assumption: If GPT-4 can reproduce data with high Levenshtein ratio, the data must have been seen during training.
- Evidence anchors:
  - [abstract] "Results showed GPT-4 accurately reproduced Daphnet FoG dataset data, suggesting memorization, with a Levenshtein ratio of 0.8074."
  - [section] "When comparing the LLM-generated output to the original data we found a non-negligible amount of matches which suggests that the LLM under investigation seems to indeed have seen wearable sensor data from the benchmark datasets during training."
- Break condition: If sensor data contains many repeated values, GPT-4 may simply copy from context rather than generate from memory.

### Mechanism 2
- Claim: Memorization is more detectable when sensor data has low repetition across rows
- Mechanism: Datasets with high variability between consecutive timesteps provide clearer signals of memorization. When GPT-4 reproduces accurate values in such datasets, it indicates genuine recall rather than simple copying of similar context.
- Core assumption: Datasets with repeating values can artificially inflate memorization detection scores.
- Evidence anchors:
  - [section] "In the course of our exploration, we observed that completion-based memorization tests are difficult to apply to wearable sensor data... many rows with repeating values, potentially due to the relatively high sampling frequency."
- Break condition: If a dataset has systematic repetition patterns, GPT-4 can predict these without memorization.

### Mechanism 3
- Claim: Levenshtein ratio provides quantitative evidence of memorization by measuring string similarity between generated and ground truth rows
- Mechanism: The Levenshtein ratio normalizes the edit distance between generated and actual sensor data rows by their lengths, producing a 0-1 score where higher values indicate closer matches.
- Core assumption: High Levenshtein ratios in sensor data reproduction indicate memorization rather than chance or context copying.
- Evidence anchors:
  - [section] "We compare the row of data generated by the LLM against the ground truth row, using the Levenshtein ratio [31]."
  - [section] "The Levenshtein ratios obtained by all files in each dataset are averaged and presented in Tab. 1."
- Break condition: If GPT-4's tokenizer splits numerical values into multiple tokens, Levenshtein ratio may not accurately reflect memorization quality.

## Foundational Learning

- Concept: Tabular data representation and CSV format structure
  - Why needed here: Sensor datasets are typically stored as CSV/TSV files with header rows followed by sensor readings
  - Quick check question: What is the difference between a header row and a data row in a typical sensor dataset CSV file?

- Concept: Levenshtein distance and similarity metrics
  - Why needed here: The Levenshtein ratio is used to quantify how closely GPT-4's generated rows match the ground truth
  - Quick check question: How does the Levenshtein ratio differ from simple character-by-character comparison?

- Concept: Few-shot prompting in LLMs
  - Why needed here: The memorization test uses seven examples as few-shot context to guide GPT-4's predictions
  - Quick check question: What is the purpose of providing multiple examples as few-shot context in the Row Completion test?

## Architecture Onboarding

- Component map:
  - Data source -> Row Completion test implementation -> OpenAI API -> Levenshtein ratio computation -> Analysis
  - Public HAR datasets (CSV/TSV) -> Random sampling of contiguous rows -> Prompt construction with few-shot examples -> GPT-4 API call -> Row comparison and ratio calculation

- Critical path:
  1. Load dataset files and randomly sample contiguous rows
  2. Construct prompt with few-shot examples and test query
  3. Send prompt to GPT-4 via API
  4. Compute Levenshtein ratio between generated and ground truth rows
  5. Analyze patterns in reproduction accuracy

- Design tradeoffs:
  - Using verbatim reproduction vs. semantic similarity - verbatim is stricter but may miss semantic memorization
  - Number of examples in few-shot context - more examples improve guidance but may introduce bias
  - Sampling strategy - random sampling vs. systematic coverage of dataset

- Failure signatures:
  - High Levenshtein ratios across all datasets may indicate GPT-4 is simply copying from context rather than memorizing
  - Low ratios in datasets with repeating values may mask actual memorization
  - Consistent errors in timestamp or label columns suggest GPT-4 is relying on pattern recognition rather than data recall

- First 3 experiments:
  1. Test GPT-4 on Daphnet FoG dataset with varying numbers of few-shot examples (1, 3, 7, 10) to find optimal guidance level
  2. Apply Row Completion test to a synthetic dataset with known memorization vs. unknown dataset to validate test sensitivity
  3. Compare Levenshtein ratio results with alternative metrics like BLEU or ROUGE to assess robustness of memorization detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise extent and nature of GPT-4's memorization of Daphnet FoG dataset sensor data, including which specific rows or patterns were reproduced most accurately?
- Basis in paper: [explicit] The paper explicitly states that GPT-4 can "reproduce blocks of sensor readings" from the Daphnet FoG dataset with a Levenshtein ratio of 0.8074
- Why unresolved: While the paper demonstrates GPT-4's ability to reproduce Daphnet data, it does not provide a detailed breakdown of which specific rows, patterns, or segments were most accurately reproduced
- What evidence would resolve it: A detailed analysis of the specific rows and patterns from the Daphnet FoG dataset that GPT-4 reproduced most accurately, including statistical breakdowns of Levenshtein ratios for different segments

### Open Question 2
- Question: What are the specific causes and patterns of data duplication in wearable sensor datasets, and how do these patterns affect the reliability of memorization detection methods?
- Basis in paper: [explicit] The paper identifies data duplication as a significant challenge, noting that "large portions of data across rows can be identical" due to factors like high sampling frequencies
- Why unresolved: While the paper highlights the problem of data duplication, it does not systematically investigate the underlying causes across different datasets
- What evidence would resolve it: A comprehensive study analyzing the causes of data duplication across multiple wearable sensor datasets, along with a methodological framework for distinguishing between true memorization and replication of duplicated data patterns

### Open Question 3
- Question: How effective are current perturbation and data watermarking techniques in preventing LLM memorization of sensor datasets, and what are their limitations?
- Basis in paper: [explicit] The paper mentions that perturbation with small values has limited effectiveness against memorization
- Why unresolved: The paper briefly mentions these techniques but does not provide a detailed evaluation of their effectiveness or explore their limitations
- What evidence would resolve it: Systematic testing of various perturbation and watermarking techniques on sensor datasets, comparing their effectiveness in preventing memorization while maintaining data utility

## Limitations
- Memorization detection is confounded by data repetition patterns in sensor datasets, making it difficult to distinguish between true memorization and context copying
- The Levenshtein ratio metric may miss semantic memorization where LLMs generate semantically similar but syntactically different outputs
- Verbatim reproduction requirements may not capture more subtle forms of memorization

## Confidence
- **High Confidence**: GPT-4 accurately reproduced Daphnet FoG dataset data with Levenshtein ratio of 0.8074
- **Medium Confidence**: Standard HAR evaluation protocols should be reconsidered due to potential memorization effects
- **Low Confidence**: GPT-4 has been trained on public wearable sensor datasets based solely on Daphnet FoG results

## Next Checks
1. **Dataset Purification Test**: Preprocess each dataset to remove repeated values and sensor artifacts, then re-run the memorization test to isolate true memorization effects from noise-related confounds
2. **Semantic Memorization Assessment**: Implement an alternative evaluation using semantic similarity metrics (e.g., BERTScore or embedding-based cosine similarity) alongside Levenshtein ratio to detect paraphrased memorization
3. **Cross-LLM Comparison**: Apply the same memorization test to multiple LLM architectures (GPT-3.5, Claude, LLaMA) to determine if memorization patterns are consistent across models or specific to GPT-4's training data