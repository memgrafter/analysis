---
ver: rpa2
title: Experiences from Creating a Benchmark for Sentiment Classification for Varieties
  of English
arxiv_id: '2410.11216'
source_url: https://arxiv.org/abs/2410.11216
tags:
- sentiment
- reviews
- language
- english
- varieties
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the need for benchmarks that capture linguistic
  diversity across English varieties (Australian, Indian, British) in sentiment classification.
  The authors collected Google Places reviews and applied various sampling strategies
  based on review length and sentiment-bearing word density.
---

# Experiences from Creating a Benchmark for Sentiment Classification for Varieties of English

## Quick Facts
- arXiv ID: 2410.11216
- Source URL: https://arxiv.org/abs/2410.11216
- Authors: Dipankar Srirag; Jordan Painter; Aditya Joshi; Diptesh Kanojia
- Reference count: 18
- Primary result: Label semantics significantly impact sentiment classification performance, with simpler labels yielding higher F1-scores (up to 97.2%) compared to harder labels (average drop of 13.7 points)

## Executive Summary
This paper addresses the need for benchmarks that capture linguistic diversity across English varieties (Australian, Indian, British) in sentiment classification. The authors collected Google Places reviews and applied various sampling strategies based on review length and sentiment-bearing word density. They evaluated three BERT-based models (BERT-base, DistilBERT-base, RoBERTa-base) on these samples, finding that label semantics significantly impact performance, with simpler labels yielding higher F1-scores compared to harder labels. The study highlights the importance of diverse sampling and careful label definition in creating robust benchmarks for sentiment classification across linguistic varieties.

## Method Summary
The authors collected Google Places reviews from Australia, India, and UK, filtering out non-English text and tourist attractions. They created two label semantic configurations (SIMPLE: 1vs5, 2vs5; HARD: 2vs5, 3vs4) based on star ratings. Three sampling strategies were applied: length-based (LEN-75, LEN-50, LEN-25), sentiment distribution-based (SENT-75, SENT-50, SENT-25), and a combined approach. Three BERT-based models (BERT-base, DistilBERT-base, RoBERTa-base) were fine-tuned with 70%/15%/15% train/validation/test splits using Adam optimizer and early stopping. Performance was evaluated using macro-averaged Precision, Recall, and F1-score.

## Key Results
- Label semantics significantly impact performance, with simpler labels yielding higher F1-scores (up to 97.2%) compared to harder labels (average drop of 13.7 points)
- Models generally perform better on longer reviews, but performance varies across English varieties
- Inner-circle varieties (en-AU, en-UK) consistently outperform Indian English (en-IN) in sentiment classification tasks
- Higher density of sentiment-bearing words increases ambiguity, making it harder for models to infer sentiment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance degrades significantly when sentiment-bearing words are densely packed in reviews
- Mechanism: Higher density of sentiment-bearing words increases ambiguity, making it harder for models to infer sentiment
- Core assumption: Models rely on clear sentiment signals and struggle when those signals are mixed or ambiguous
- Evidence anchors:
  - [abstract] "Models generally perform better on longer reviews, but performance varies across English varieties..."
  - [section 4.2] "As sentiment proportion increased in the HARD samples, performance degraded significantly, with the lowest F1-score of 64.8..."
  - [corpus] Weak evidence: corpus doesn't directly address sentiment-bearing word density impact
- Break condition: If sentiment-bearing words are uniformly distributed and polarity is clear, the degradation may not occur

### Mechanism 2
- Claim: Models perform better on well-separated sentiment labels (SIMPLE) than on closely grouped labels (HARD)
- Mechanism: Clear sentiment distinctions reduce ambiguity in classification, leading to higher accuracy
- Core assumption: Label semantics significantly influence model learning and generalization
- Evidence anchors:
  - [abstract] "label semantics significantly impact performance, with simpler labels yielding higher F1-scores (up to 97.2%) compared to harder labels (average drop of 13.7 points)."
  - [section 4.1] "switching from SIMPLE to HARD labels resulted in a significant performance drop across all locales, with an average F1-score decrease of 13.7 points."
  - [corpus] No direct evidence; inference based on abstract and section data
- Break condition: If the dataset has very distinct sentiment classes, the performance gap between SIMPLE and HARD labels may be minimal

### Mechanism 3
- Claim: Inner-circle English varieties (en-AU, en-UK) consistently outperform Indian English (en-IN) in sentiment classification tasks
- Mechanism: Models are better attuned to linguistic patterns and cultural nuances of inner-circle varieties due to more training data or inherent biases
- Core assumption: Model performance is influenced by the linguistic diversity and representativeness of the training data
- Evidence anchors:
  - [abstract] "Models generally perform better on longer reviews, but performance varies across English varieties, with inner-circle varieties (en-AU, en-UK) consistently outperforming Indian English."
  - [section 4.2] "However, the performance of models trained on Indian English reviews was consistently lower across different review lengths..."
  - [corpus] Weak evidence: corpus neighbors focus on related benchmarks but don't provide direct evidence for this mechanism
- Break condition: If models are explicitly trained on diverse English varieties, the performance gap may diminish

## Foundational Learning

- Concept: Label semantics and their impact on model performance
  - Why needed here: Understanding how label definitions affect classification accuracy is crucial for designing effective benchmarks
  - Quick check question: How does changing label semantics from SIMPLE to HARD affect model performance?

- Concept: Sentiment-bearing word density and its influence on classification
  - Why needed here: Recognizing how the proportion of sentiment-bearing words impacts model ambiguity is essential for data sampling strategies
  - Quick check question: What happens to model performance when sentiment-bearing words are densely packed in reviews?

- Concept: Linguistic diversity and model generalization across English varieties
  - Why needed here: Ensuring models are robust to different English varieties is key for creating inclusive and accurate benchmarks
  - Quick check question: Why do models perform differently across Australian, Indian, and British English varieties?

## Architecture Onboarding

- Component map: Data Collection -> Data Preprocessing -> Label Definition -> Sampling Strategies -> Model Training -> Evaluation
- Critical path:
  1. Collect and preprocess reviews
  2. Define label semantics (SIMPLE vs HARD)
  3. Apply sampling strategies
  4. Train models on sampled data
  5. Evaluate model performance across varieties
- Design tradeoffs:
  - Longer reviews vs. shorter reviews: Balancing contextual richness with real-world data representation
  - Simple labels vs. hard labels: Trade-off between classification ease and benchmark challenge
  - Inner-circle vs. outer-circle varieties: Ensuring model robustness vs. focusing on high-resource varieties
- Failure signatures:
  - High variance in F1-scores across different sampling strategies
  - Significant performance drops when switching from SIMPLE to HARD labels
  - Consistent underperformance on Indian English reviews
- First 3 experiments:
  1. Compare model performance on SIMPLE vs. HARD labels to quantify the impact of label semantics
  2. Evaluate models on length-based samples (LEN-75, LEN-50, LEN-25) to assess the effect of review length
  3. Test models on sentiment distribution-based samples (SENT-75, SENT-50, SENT-25) to understand the influence of sentiment-bearing word density

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the authors define the boundaries between different sentiment-bearing words in their spaCy-TextBlob-based sentiment distribution calculation?
- Basis in paper: [explicit] The paper mentions using spaCy-TextBlob to count positive and negative words based on polarity scores, but doesn't specify how they handle cases where words might have mixed or ambiguous polarity
- Why unresolved: The methodology section describes the sentiment distribution calculation but lacks detail on how edge cases (e.g., words with neutral polarity or mixed sentiment) are treated, which could significantly impact the results
- What evidence would resolve it: Detailed explanation of the spaCy-TextBlob configuration used, including threshold values for positive/negative polarity and handling of neutral/mixed-sentiment words

### Open Question 2
- Question: What specific linguistic features or patterns did the annotators use to distinguish between en-AU and en-UK varieties when they disagreed?
- Basis in paper: [inferred] The low inter-annotator agreement (Cohen's kappa of 0.191) between ant-IN and ant-UK for en-AU and en-UK reviews suggests these varieties share similar features that make them difficult to distinguish
- Why unresolved: While the paper reports the disagreement rate, it doesn't analyze the specific linguistic features that caused confusion between these two varieties
- What evidence would resolve it: Linguistic analysis of the disputed reviews showing which features (vocabulary, spelling, syntax, etc.) led to different annotations, or a more detailed breakdown of which specific linguistic markers were used for classification

### Open Question 3
- Question: How would the results change if the authors used a different BERT-based model architecture (e.g., ALBERT or ELECTRA) instead of BERT-base, DistilBERT-base, and RoBERTa-base?
- Basis in paper: [inferred] The paper uses three BERT-based models but doesn't explore how different transformer architectures might affect performance across English varieties
- Why unresolved: The choice of model architecture could impact performance differently across varieties due to factors like model size, pretraining objectives, or architectural differences
- What evidence would resolve it: Experimental results comparing the three models used with additional models like ALBERT or ELECTRA, showing whether the observed patterns (e.g., inner-circle varieties outperforming Indian English) persist across different architectures

## Limitations
- The exclusive use of Google Places reviews may not capture sentiment expression patterns in other domains like social media or product reviews
- Binary sentiment labeling approach oversimplifies nuanced sentiment expressions that could be better captured with multi-class or continuous scales
- Focus on three English varieties represents only a subset of global English diversity

## Confidence
- Label semantics impact: **High** - Consistently observed 13.7-point performance drop across multiple model architectures and sampling strategies
- Length-based performance patterns: **Medium** - Generally positive effect of review length, but shows variability across different sampling strategies and label types
- Cross-variety performance claims: **Low** - Study doesn't fully explore potential confounding factors like review topic distribution or demographic differences

## Next Checks
1. **Domain Generalization Test**: Evaluate the same models and sampling strategies on sentiment classification tasks using reviews from different domains (e.g., Amazon product reviews, Twitter posts) to assess whether the observed patterns hold across contexts
2. **Multi-class Sentiment Analysis**: Replicate the study using a multi-class sentiment labeling scheme (e.g., 1-5 star ratings as distinct classes) to determine if the label semantics effect persists with more granular sentiment categories
3. **Cultural Context Analysis**: Conduct a detailed analysis of review content to identify potential cultural or topical differences between English varieties that might explain performance variations beyond linguistic factors