---
ver: rpa2
title: Toward a Method to Generate Capability Ontologies from Natural Language Descriptions
arxiv_id: '2406.07962'
source_url: https://arxiv.org/abs/2406.07962
tags:
- capability
- ontology
- language
- llms
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LLM4Cap, a method to automatically generate
  capability ontologies from natural language descriptions using Large Language Models
  (LLMs). The approach employs few-shot prompting to transform capability descriptions
  into OWL ontologies and incorporates automated verification through syntax checking,
  logical reasoning, and SHACL validation to detect hallucinations and inconsistencies.
---

# Toward a Method to Generate Capability Ontologies from Natural Language Descriptions

## Quick Facts
- arXiv ID: 2406.07962
- Source URL: https://arxiv.org/abs/2406.07962
- Authors: Luis Miguel Vieira da Silva; Aljosha Köcher; Felix Gehlhoff; Alexander Fay
- Reference count: 17
- Primary result: LLM4Cap automatically generates capability ontologies from natural language descriptions using few-shot prompting and automated verification

## Executive Summary
This paper presents LLM4Cap, a method for automatically generating capability ontologies from natural language descriptions using Large Language Models (LLMs). The approach employs few-shot prompting with structured examples to guide LLMs in producing syntactically correct OWL ontologies. A key innovation is the automated verification pipeline that checks for syntax errors, logical inconsistencies, and hallucinations through Apache Jena, Pellet reasoning, and SHACL validation. The method significantly reduces manual effort by requiring only the initial natural language description and final human review, while the iterative loop between generation and verification ensures high-quality outputs.

## Method Summary
LLM4Cap uses a few-shot prompting technique where a predefined prompt containing instructions, context from the CaSk ontology, and three capability examples guides the LLM in generating OWL ontologies from natural language descriptions. The generated ontology undergoes automated verification through three steps: syntax checking using Apache Jena, logical reasoning using Pellet to detect contradictions, and SHACL validation to identify hallucinations and missing elements. If errors are detected, they are backprompted to the LLM for correction in an iterative loop until the ontology passes all verification steps or requires manual intervention. The Java implementation supports multiple LLMs (GPT-4 and Claude 3) and integrates Apache Jena for ontology validation.

## Key Results
- Few-shot prompting with structured examples produces accurate capability ontologies from natural language descriptions
- Automated verification through syntax checking, reasoning, and SHACL validation effectively detects and corrects hallucinations and inconsistencies
- The iterative generation-verification loop minimizes manual effort while ensuring ontology quality
- Implementation as a Java framework supports multiple LLMs and integrates with Apache Jena for validation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting enables LLMs to generate accurate capability ontologies by providing structured examples.
- Mechanism: The method uses a predefined prompt containing short instructions, context from the CaSk ontology, and three capability examples (coffee-making, mathematical operation, distillation) to guide the LLM in generating syntactically and semantically correct OWL ontologies.
- Core assumption: LLMs can learn from in-context examples to produce structured outputs matching the provided patterns.
- Evidence anchors:
  - [abstract]: "Our approach requires only a natural language description of a capability, which is then automatically inserted into a predefined prompt using a few-shot prompting technique."
  - [section III]: "In our previous study in [5], we examined the suitability of LLMs for generating capabilities and compared different prompting techniques — zero-shot, one-shot and few-shot prompting technique — with few-shot providing the best results."
- Break condition: If the LLM fails to generalize from examples or if the examples don't adequately represent the complexity of the target capability.

### Mechanism 2
- Claim: Automated verification through syntax checking, reasoning, and SHACL validation ensures the generated ontology's correctness and completeness.
- Mechanism: The method employs three verification steps: (1) syntax check using Apache Jena to identify character errors and syntax violations, (2) logical reasoning using Pellet reasoner to detect contradictions and inconsistencies, and (3) SHACL validation to check for hallucinations and missing ontology elements.
- Core assumption: LLMs can be guided to correct errors when provided with specific feedback about their mistakes.
- Evidence anchors:
  - [abstract]: "After prompting an LLM, the resulting capability ontology is automatically verified through various steps in a loop with the LLM to check the overall correctness of the capability ontology. First, a syntax check is performed, then a check for contradictions, and finally a check for hallucinations and missing ontology elements."
  - [section III]: "Should an error be detected in any of these steps, the error is automatically backprompted to LLM for correction, as LLMs show good results in correcting errors with the inclusion of feedback."
- Break condition: If the LLM enters an infinite loop of error generation and correction, or if the verification steps cannot identify certain types of errors.

### Mechanism 3
- Claim: The iterative loop between ontology generation and verification minimizes manual effort while ensuring high-quality outputs.
- Mechanism: The system generates an ontology, verifies it through automated steps, and if errors are found, backprompts the LLM with specific error feedback. This loop continues until the ontology passes all verification steps or requires manual intervention.
- Core assumption: LLMs can effectively incorporate feedback to improve their outputs across multiple iterations.
- Evidence anchors:
  - [abstract]: "Our method greatly reduces manual effort, as only the initial natural language description and a final human review and possible correction are necessary."
  - [section III]: "Should an error be detected in any of these steps, the error is automatically backprompted to LLM for correction... If an error recurs in the same step, the result is passed directly to the human for manual verification, thus preventing a potentially infinite cycle."
- Break condition: If the LLM consistently fails to correct errors even with feedback, or if the verification steps themselves introduce new errors.

## Foundational Learning

- Concept: Large Language Models (LLMs) and Few-shot Prompting
  - Why needed here: Understanding how LLMs can be guided to generate structured ontologies using examples rather than fine-tuning.
  - Quick check question: What is the difference between zero-shot, one-shot, and few-shot prompting, and why was few-shot chosen for this method?

- Concept: Web Ontology Language (OWL) and Turtle Syntax
  - Why needed here: The generated capability ontologies must conform to OWL standards and be expressed in Turtle syntax for machine interpretation.
  - Quick check question: What are the key elements of OWL ontology syntax, and how does Turtle syntax represent these elements?

- Concept: Apache Jena, Pellet Reasoner, and SHACL Validation
  - Why needed here: These tools are used to verify the generated ontologies for syntax correctness, logical consistency, and constraint adherence.
  - Quick check question: How do Pellet reasoning and SHACL validation complement each other in verifying ontology correctness?

## Architecture Onboarding

- Component map: Natural Language Input → Prompt Generator → LLM (GPT-4 or Claude 3) → Generated Ontology → Syntax Checker (Apache Jena) → Reasoner (Pellet) → SHACL Validator → Loop back to LLM (if errors) → Manual Verification
- Critical path: Natural Language Description → Prompt Generation → LLM Ontology Generation → Syntax Check → Reasoning → SHACL Validation → Manual Verification
- Design tradeoffs:
  - Using few-shot prompting instead of fine-tuning allows for flexibility but may require more careful prompt engineering
  - The iterative verification loop ensures quality but could increase processing time and costs
  - Supporting multiple LLMs (GPT-4 and Claude 3) provides options but adds complexity to the framework
- Failure signatures:
  - Syntax errors: Malformed Turtle syntax or missing prefixes
  - Logical inconsistencies: Instances belonging to disjoint classes or contradictory properties
  - Hallucinations: Addition of invalid relations or missing mandatory elements
  - Loop failures: LLM fails to correct errors after multiple iterations
- First 3 experiments:
  1. Test the system with a simple capability (e.g., coffee-making) to verify basic functionality
  2. Introduce a known syntax error in the generated ontology to test the verification loop
  3. Create a capability with complex dependencies to evaluate the reasoning and SHACL validation steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can prompt optimization techniques be systematically evaluated to improve LLM-generated capability ontologies?
- Basis in paper: [explicit] The paper mentions future work on prompt optimization but doesn't detail evaluation methods
- Why unresolved: The authors acknowledge that prompt optimization is needed but don't specify how to measure improvement or what metrics to use
- What evidence would resolve it: Comparative studies showing ontology quality improvements with different prompting strategies, using standardized metrics like ontology completeness, logical consistency scores, or reduction in required human corrections

### Open Question 2
- Question: What embedding techniques would most effectively reduce token costs while maintaining ontology quality in LLM4Cap?
- Basis in paper: [explicit] The authors state that token costs are high and embedding techniques "are worth investigating"
- Why unresolved: While embedding approaches are mentioned as promising, the paper doesn't identify which specific techniques (semantic embeddings, knowledge graph embeddings, etc.) would work best for this application
- What evidence would resolve it: Comparative analysis of different embedding methods showing cost reduction and quality maintenance across multiple capability ontology generation tasks

### Open Question 3
- Question: How can the CaSk ontology be enhanced with annotations to improve LLM understanding without increasing complexity for human users?
- Basis in paper: [explicit] The authors suggest adding labels, comments, and annotations to improve LLM understanding
- Why unresolved: The paper identifies this as future work but doesn't explore the trade-off between adding helpful context for LLMs and maintaining human readability
- What evidence would resolve it: User studies comparing ontology usability with and without enhanced annotations, measuring both LLM performance and human expert comprehension time and accuracy

## Limitations
- The approach's effectiveness may vary significantly across different capability domains and complexity levels
- The iterative verification loop could potentially lead to increased processing time and costs
- Success depends on the specific SHACL constraints used, which are not fully detailed in the paper

## Confidence

- **High Confidence:** The mechanism of using few-shot prompting for structured ontology generation is well-supported by the literature and the authors' previous work.
- **Medium Confidence:** The automated verification process using syntax checking, logical reasoning, and SHACL validation is likely effective, but the specific implementation details and error handling strategies may impact its reliability.
- **Low Confidence:** The iterative loop between generation and verification, while promising, may face challenges with complex capabilities or LLMs that struggle to incorporate feedback effectively.

## Next Checks

1. **Example Generalization Test:** Evaluate the system's performance across a diverse set of capability descriptions (simple, complex, and edge cases) to assess the generalizability of the few-shot examples.
2. **Error Correction Robustness:** Introduce controlled errors at each verification step to test the LLM's ability to correct them consistently across multiple iterations.
3. **SHACL Constraint Validation:** Verify the completeness and accuracy of the SHACL constraints used for capability validation by testing them against known valid and invalid capability descriptions.