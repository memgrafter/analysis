---
ver: rpa2
title: Benchmarking LLMs via Uncertainty Quantification
arxiv_id: '2401.12794'
source_url: https://arxiv.org/abs/2401.12794
tags:
- uncertainty
- llms
- prediction
- conformal
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces uncertainty quantification into LLM benchmarking
  using conformal prediction. The method produces prediction sets that statistically
  cover the true answer with user-specified probability, with set size serving as
  the uncertainty metric.
---

# Benchmarking LLMs via Uncertainty Quantification

## Quick Facts
- **arXiv ID**: 2401.12794
- **Source URL**: https://arxiv.org/abs/2401.12794
- **Reference count**: 40
- **Primary result**: Conformal prediction provides statistically guaranteed uncertainty quantification for LLM benchmarking, outperforming perplexity and entropy baselines.

## Executive Summary
This paper introduces uncertainty quantification into LLM benchmarking using conformal prediction, producing prediction sets that statistically cover the true answer with user-specified probability. The method is evaluated across nine LLMs (6B-72B parameters) and five NLP tasks using 10K instances each, all converted to multiple-choice format. Results reveal that higher accuracy doesn't guarantee lower uncertainty, larger models sometimes exhibit greater uncertainty, and instruction-finetuning tends to increase uncertainty. The study demonstrates that conformal prediction provides more reliable uncertainty estimation than traditional metrics like perplexity and entropy.

## Method Summary
The paper adapts conformal prediction to LLM benchmarking by converting five NLP tasks (QA, RC, CI, DRS, DS) into multiple-choice format with six options each. Nine open-source LLMs are evaluated using three prompting strategies and two conformal score functions (LAC and APS). The method constructs prediction sets that guarantee coverage at a user-specified error rate α=0.1, with set size serving as the uncertainty metric. Evaluation uses three metrics: Prediction Accuracy (Acc), Prediction Uncertainty (Set Size - SS), and Coverage Rate (CR). The framework is also extended to closed-source LLMs and free-form text generation scenarios.

## Key Results
- Higher accuracy does not guarantee lower uncertainty - some less accurate models show lower uncertainty
- Larger models sometimes exhibit greater uncertainty than smaller models
- Instruction-finetuning tends to increase uncertainty compared to base models
- Conformal prediction is superior to perplexity and entropy for reliable uncertainty estimation

## Why This Works (Mechanism)

### Mechanism 1
Conformal prediction produces statistically guaranteed prediction sets that cover the true label with a user-specified error rate. It uses a held-out calibration set to compute a conformal threshold, then includes in the prediction set only those labels whose conformal score is below this threshold. The core assumption is that calibration data are drawn from the same distribution as test data with exchangeable model outputs.

### Mechanism 2
Set size from conformal prediction serves as a reliable indicator of uncertainty. Larger prediction sets correspond to higher uncertainty because more candidate labels are included to meet the coverage guarantee. The conformal score function must meaningfully rank labels by their plausibility for this to work effectively.

### Mechanism 3
Averaging results over multiple conformal score functions (LAC and APS) and prompting strategies yields a more robust uncertainty measure. Different score functions and prompts can produce different prediction set sizes; averaging reduces variance and mitigates strategy-specific bias.

## Foundational Learning

- **Concept: Conformal prediction and its coverage guarantee property**
  - Why needed here: Provides the theoretical foundation for uncertainty quantification in LLM benchmarking
  - Quick check question: What is the role of the calibration set in conformal prediction, and how does it relate to the user-specified error rate α?

- **Concept: Exchangeability and distribution-free assumptions**
  - Why needed here: Ensures that the coverage guarantee holds without requiring knowledge of the underlying data distribution
  - Quick check question: What could happen to the coverage guarantee if the calibration and test sets are drawn from different distributions?

- **Concept: Conformal score functions (LAC vs APS)**
  - Why needed here: Different score functions affect the size and quality of prediction sets, influencing uncertainty measurement
  - Quick check question: Why might APS produce larger prediction sets than LAC on average, and what are the trade-offs?

## Architecture Onboarding

- **Component map**: Data preparation → prompt generation → model inference → conformal threshold calculation → prediction set construction → metric calculation
- **Critical path**: Data preparation → prompt generation → model inference → conformal threshold calculation → prediction set construction → metric calculation
- **Design tradeoffs**: LAC gives smaller sets but may under-cover; APS gives larger sets but better coverage; averaging balances these
- **Failure signatures**: Coverage rate far below 1-α; large variance in set size across prompt strategies; prediction sets empty or containing too many options
- **First 3 experiments**:
  1. Run conformal prediction with LAC on a small QA dataset and check if coverage rate meets 90%
  2. Repeat with APS and compare average set size
  3. Vary the calibration/test split ratio and observe impact on coverage and set size

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of conformal score function (LAC vs APS) impact the reliability of uncertainty quantification across different LLM sizes? The paper shows varying performance between LAC and APS across tasks but doesn't provide clear guidance on when one should be preferred over the other, especially for different model sizes or task types.

### Open Question 2
What is the optimal calibration data ratio for achieving the best balance between uncertainty quantification accuracy and resource efficiency? The paper uses 50% calibration data as standard but doesn't explore whether smaller ratios (e.g., 10-20%) could provide sufficient accuracy while reducing computational costs.

### Open Question 3
How does uncertainty quantification via conformal prediction generalize to non-multiple-choice formats and multi-modal tasks? The paper only applies conformal prediction to multiple-choice formats and acknowledges significant challenges in applying it to free-form generation.

## Limitations

- Limited model comparisons for instruction-tuning effects - only a few model pairs tested
- Multiple-choice conversion may introduce artifacts, especially for tasks like summarization and dialogue
- Comparison only against perplexity and entropy, not other LLM-specific uncertainty methods

## Confidence

**High Confidence**: Conformal prediction provides statistically guaranteed coverage when calibration and test data are from the same distribution.

**Medium Confidence**: Conformal prediction produces more reliable uncertainty estimates than perplexity and entropy for LLM benchmarking.

**Low Confidence**: Larger models exhibit higher uncertainty and instruction-finetuning increases uncertainty based on limited model comparisons.

## Next Checks

1. Replicate the uncertainty comparison between instruction-tuned and base models using a larger sample of model pairs to determine if the instruction-tuning effect is consistent across different architectures.

2. Validate the multiple-choice conversion process by comparing uncertainty measurements on the original task format versus the converted format for at least one task where conversion may introduce significant artifacts.

3. Compare conformal prediction uncertainty quantification against at least two additional LLM-specific uncertainty methods (e.g., Monte Carlo dropout, deep ensembles) on the same benchmark tasks.