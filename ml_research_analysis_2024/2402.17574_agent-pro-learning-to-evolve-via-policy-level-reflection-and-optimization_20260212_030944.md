---
ver: rpa2
title: 'Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization'
arxiv_id: '2402.17574'
source_url: https://arxiv.org/abs/2402.17574
tags:
- player
- game
- agent-pro
- action
- hand
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Agent-Pro enables LLM-based agents to learn and evolve their behavioral
  policies through policy-level reflection in complex interactive environments. The
  method uses dynamic self-belief and world-belief modeling combined with iterative
  prompt optimization via depth-first search, allowing the agent to learn from past
  trajectories and refine its strategies without parameter tuning.
---

# Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization

## Quick Facts
- arXiv ID: 2402.17574
- Source URL: https://arxiv.org/abs/2402.17574
- Reference count: 40
- Agent-Pro enables LLM-based agents to learn and evolve their behavioral policies through policy-level reflection in complex interactive environments, achieving up to 4% higher win rates in Blackjack and up to 2.9 more chips in Texas Hold'em.

## Executive Summary
Agent-Pro introduces a novel approach for LLM-based agents to learn and evolve their behavioral policies in complex, interactive environments like Blackjack and Texas Hold'em. By combining dynamic self-belief and world-belief modeling with iterative prompt optimization via depth-first search, Agent-Pro enables agents to reflect on past trajectories, refine their strategies, and adapt without parameter tuning. The method significantly outperforms vanilla LLMs and specialized models, demonstrating robust policy improvement through interaction and reflection.

## Method Summary
Agent-Pro operates through a cycle of belief-aware decision-making, policy-level reflection, and DFS-based evolution. The agent maintains dynamic self-belief (own state, plan, risk) and world-belief (opponent style, environment rules) to inform decisions. Failed trajectories trigger reflection, where irrational beliefs are distilled into behavioral guidelines and world modeling. These updates are verified via replay, and a depth-first search is employed to explore improved policies. The process iterates, enabling continual enhancement without parameter tuning.

## Key Results
- Agent-Pro achieves up to 4% higher win rates in Blackjack compared to vanilla LLMs.
- In Texas Hold'em, Agent-Pro gains up to 2.9 more chips than specialized models.
- The method demonstrates robust adaptation and strategic skill development through interaction and reflection.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agent-Pro improves policy through belief calibration rather than action-level correction.
- Mechanism: The agent identifies irrational beliefs from failed trajectories, distills them into behavioral guidelines and world modeling, and verifies improved policies via replay.
- Core assumption: Policy-level reflection can generalize better than immediate action correction in long-horizon games.
- Evidence anchors:
  - [abstract] "Agent-Pro iteratively reflects on past trajectories and beliefs, 'fine-tuning' its irrational beliefs for a better policy."
  - [section 3.2] "Agent-Pro examines the rationality of these beliefs based on the final results and reflects on the reasons for the final failure."
  - [corpus] Weak/no direct evidence; relies on the paper's internal analysis.
- Break condition: If the belief calibration produces vague or verbose instructions, policy improvement stalls.

### Mechanism 2
- Claim: Dynamic belief-aware decision-making improves action coherence in imperfect-information games.
- Mechanism: Agent-Pro maintains self-belief (own state, plan, risk) and world-belief (opponent style, environment rules) updated each decision cycle, enabling more context-aware choices.
- Core assumption: Maintaining separate belief streams for self and environment yields better decisions than static prompts alone.
- Evidence anchors:
  - [abstract] "dynamic self-belief and world-belief modeling combined with iterative prompt optimization."
  - [section 3.1] "Agent-Pro first generates a dynamic belief ξ about itself and opponents in natural language, then predicts an action based on the latest beliefs."
  - [corpus] Weak/no direct evidence; assumption based on described design.
- Break condition: If belief updates lag behind rapid environment changes, decisions become outdated.

### Mechanism 3
- Claim: DFS-based policy evolution with K+1 hand-card and position swaps yields unbiased performance estimates.
- Mechanism: After policy update, Agent-Pro evaluates on (K+1)² permuted games to cancel luck and order effects, then searches for improved policies via DFS.
- Core assumption: Systematic permutation eliminates randomness, giving true policy strength.
- Evidence anchors:
  - [section 3.3] "we concurrently use these (K+1)² games to evaluate Agent-Pro’s new policy... The ∆ assesses its gains relative to the strongest opponent."
  - [abstract] "a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs."
  - [corpus] Weak/no direct evidence; described method lacks external validation.
- Break condition: If opponent policies change unpredictably, permutation-based evaluation may not reflect true relative strength.

## Foundational Learning

- Concept: Imperfect-information games
  - Why needed here: Both Blackjack and Texas Hold'em involve hidden information; understanding this is key to grasping belief modeling.
  - Quick check question: In Blackjack, what information is hidden from the agent at the start of each round?

- Concept: Policy-level vs. action-level reflection
  - Why needed here: Agent-Pro updates entire behavioral strategies rather than single moves; distinguishing this prevents misapplication of RL methods.
  - Quick check question: How does Agent-Pro decide whether to keep a new policy after reflection?

- Concept: Prompt optimization via in-context learning
  - Why needed here: Agent-Pro updates its behavior by modifying prompts instead of fine-tuning weights; this enables rapid adaptation without training.
  - Quick check question: What format does Agent-Pro use to embed updated behavioral guidelines into its prompt?

## Architecture Onboarding

- Component map: Observation -> belief update -> action -> trajectory storage -> reflection (if loss) -> policy update -> verification -> DFS search
- Critical path: Observation → belief update → action → trajectory storage → reflection (if loss) → policy update → verification → DFS search
- Design tradeoffs: Belief modeling adds coherence but increases latency; DFS exploration improves robustness but raises computational cost; prompt-based updates avoid fine-tuning but depend on LLM reasoning quality.
- Failure signatures: Vague or verbose behavioral guidelines; stagnant policy scores across iterations; beliefs inconsistent with actual outcomes.
- First 3 experiments:
  1. Run Agent-Pro in Blackjack with empty initial beliefs; observe win rate and belief evolution.
  2. Disable belief module, keep learning; compare performance drop.
  3. Replace DFS search with random policy sampling; measure policy improvement stability.

## Open Questions the Paper Calls Out
None

## Limitations
- Belief calibration may produce vague or verbose guidelines that do not translate into actionable improvements.
- Belief updates may lag in fast-changing environments, leading to outdated decisions.
- The evaluation method may not fully account for non-stationarity in opponent strategies.

## Confidence
- Policy-level reflection and belief calibration: Medium
- Dynamic belief modeling: Medium
- DFS-based policy evolution: Medium

## Next Checks
1. Run ablation studies comparing belief-aware vs. action-level reflection in both Blackjack and Texas Hold'em.
2. Test belief update frequency sensitivity to determine optimal reflection timing.
3. Compare DFS-based policy search against random policy sampling and gradient-based methods in terms of convergence speed and final performance.