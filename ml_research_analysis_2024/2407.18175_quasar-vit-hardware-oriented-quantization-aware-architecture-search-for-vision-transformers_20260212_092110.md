---
ver: rpa2
title: 'Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision
  Transformers'
arxiv_id: '2407.18175'
source_url: https://arxiv.org/abs/2407.18175
tags:
- quantization
- search
- hardware
- accuracy
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Quasar-ViT, a hardware-oriented quantization-aware
  architecture search framework for Vision Transformers (ViTs). The core method involves
  training a supernet using row-wise flexible mixed-precision quantization, mixed-precision
  weight entanglement, and supernet layer scaling, followed by an efficient hardware-oriented
  search algorithm integrated with hardware latency and resource modeling to determine
  optimal subnets.
---

# Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers

## Quick Facts
- **arXiv ID:** 2407.18175
- **Source URL:** https://arxiv.org/abs/2407.18175
- **Reference count:** 40
- **Primary result:** Proposed framework achieves 101.5, 159.6, and 251.6 FPS on ZCU102 FPGA with 80.4%, 78.6%, and 74.9% ImageNet top-1 accuracy, respectively

## Executive Summary
This paper presents Quasar-ViT, a hardware-oriented quantization-aware architecture search framework specifically designed for efficient Vision Transformer deployment on FPGAs. The approach addresses the challenge of balancing accuracy with hardware constraints by integrating flexible mixed-precision quantization, mixed-precision weight entanglement, and hardware-aware search into a unified framework. The authors demonstrate that their method achieves significantly higher inference speeds than prior works while maintaining competitive accuracy on the ImageNet benchmark, specifically targeting the AMD/Xilinx ZCU102 FPGA platform.

## Method Summary
Quasar-ViT trains a supernet using row-wise flexible mixed-precision quantization where different ViT layers use varying ratios of 4-bit and 8-bit weights, combined with mixed-precision weight entanglement to reduce memory costs. The framework incorporates supernet layer scaling and employs hardware latency and resource modeling integrated into the search process to ensure selected subnets meet actual FPGA performance constraints. The searched models are implemented on FPGA with 4-bit atomic computation and hybrid DSP packing optimizations. The framework uses evolution-based search with hardware-aware metrics rather than theoretical MAC count reduction.

## Key Results
- Achieved 101.5 FPS (80.4% top-1 accuracy), 159.6 FPS (78.6% top-1 accuracy), and 251.6 FPS (74.9% top-1 accuracy) on ZCU102 FPGA
- Consistently outperforms prior works in FPS-accuracy trade-offs for ViT models
- Demonstrates effective bridging of theoretical computation reduction and practical inference speedup through model-adaptive FPGA designs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Row-wise flexible mixed-precision quantization allows each ViT layer to use different ratios of 4-bit and 8-bit weights, improving accuracy over fixed-ratio schemes
- **Mechanism:** Fine-grained channel-wise quantization within layers with flexible mix-ratios preserves accuracy better than fixed inter-layer quantization while maintaining hardware uniformity
- **Core assumption:** Different layers have varying sensitivity to quantization precision
- **Evidence anchors:** Abstract and section 3.1 comparisons with FILM-QNN and Auto-ViT-Acc
- **Break condition:** Hardware cannot efficiently handle variable bit-width operations or quantization noise becomes too severe

### Mechanism 2
- **Claim:** Mixed-precision weight entanglement reduces training memory cost and improves convergence by allowing different transformer blocks to share common weights
- **Mechanism:** Shared weight storage across subnets for common parts, with smaller blocks extracting weights from largest block's shared pool
- **Core assumption:** Transformer blocks within same layer have overlapping parameter requirements
- **Evidence anchors:** Section 3.2 description of mixed-precision weight entanglement block
- **Break condition:** Parameter sharing creates too much interference between subnet searches

### Mechanism 3
- **Claim:** Hardware latency and resource modeling ensures selected subnets meet actual FPGA performance constraints
- **Mechanism:** Models FPGA resource usage and inference latency based on actual hardware design including 4-bit atomic computation and DSP packing optimizations
- **Core assumption:** Theoretical MAC count reduction doesn't directly translate to real-world speedup due to hardware constraints
- **Evidence anchors:** Abstract mention of model-adaptive FPGA designs, section 3.4.3 hardware modeling details
- **Break condition:** Hardware modeling is inaccurate or doesn't capture real-world factors like memory access patterns

## Foundational Learning

- **Concept:** Mixed-precision quantization
  - **Why needed here:** ViT models are large and computationally expensive; mixed-precision allows aggressive quantization of less sensitive parts while preserving accuracy in sensitive areas
  - **Quick check question:** What is the difference between inter-layer and intra-layer mixed-precision quantization?

- **Concept:** Neural Architecture Search (NAS)
  - **Why needed here:** Manual design of efficient ViT architectures is difficult; NAS automates the search for optimal architectures under hardware constraints
  - **Quick check question:** What is the key advantage of one-shot NAS over other NAS strategies?

- **Concept:** FPGA hardware design for deep learning
  - **Why needed here:** The framework targets FPGA deployment, requiring understanding of DSP utilization, memory tiling, and bit-width specific optimizations
  - **Quick check question:** Why is DSP packing important for low-bit precision computations on FPGAs?

## Architecture Onboarding

- **Component map:** Supernet training -> Hardware modeling -> Evolution search -> FPGA implementation
- **Critical path:** 1. Supernet training (700 epochs) 2. Hardware modeling 3. Evolution search for subnets 4. FPGA implementation of searched model
- **Design tradeoffs:**
  - Flexible mix-ratios vs. hardware uniformity: Allows flexible mix-ratios but maintains uniformity through 4-bit atomic computation and DSP packing
  - Memory efficiency vs. search flexibility: Weight entanglement saves memory but might limit search space
  - Accuracy vs. inference speed: Different models trade accuracy for speed based on hardware constraints
- **Failure signatures:**
  - Poor accuracy: Likely due to quantization noise or insufficient search space
  - Low FPS despite low MAC count: Indicates hardware modeling didn't capture bottlenecks
  - Long training time: Could be from inefficient weight sharing or insufficient GPU resources
- **First 3 experiments:**
  1. Run supernet training with a single ViT layer and verify weight sharing across candidate blocks
  2. Implement a simple FPGA design with 4-bit atomic computation and measure DSP utilization for different packing factors
  3. Perform a small-scale evolution search on a toy ViT model to validate hardware modeling accuracy

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- No ablation studies showing individual contribution of each mechanism to final performance
- No comparison with other hardware-aware NAS frameworks using different search strategies or hardware targets
- Scalability to larger ViT models or different vision tasks is not explored

## Confidence

- **High confidence** in FPGA implementation results and hardware optimization techniques, as these are specific and measurable
- **Medium confidence** in overall effectiveness of search framework, as methodology is well-described but lacks comparative ablation studies
- **Low confidence** in claims about generalizability to other hardware platforms or vision tasks, as evaluation is limited to single FPGA platform and ImageNet classification

## Next Checks
1. Perform ablation study isolating contribution of row-wise flexible mixed-precision quantization versus fixed mixed-precision schemes on accuracy and hardware efficiency
2. Compare search efficiency and final model quality against other hardware-aware NAS frameworks (FBNet, ProxylessNAS) using same hardware constraints and target platform
3. Test transferability of searched architectures to different vision tasks (object detection, semantic segmentation) and different hardware platforms (GPU, NPU) to assess generalizability