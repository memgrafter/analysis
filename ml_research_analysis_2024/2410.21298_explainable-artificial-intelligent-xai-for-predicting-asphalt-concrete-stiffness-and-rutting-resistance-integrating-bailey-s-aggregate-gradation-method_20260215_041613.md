---
ver: rpa2
title: 'Explainable Artificial Intelligent (XAI) for Predicting Asphalt Concrete Stiffness
  and Rutting Resistance: Integrating Bailey''s Aggregate Gradation Method'
arxiv_id: '2410.21298'
source_url: https://arxiv.org/abs/2410.21298
tags:
- aggregate
- performance
- asphalt
- gradation
- ratio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study developed an explainable AI model using deep learning\
  \ and SHAP values to predict asphalt concrete stiffness (MR) and rutting resistance\
  \ (DS) based on Bailey\u2019s aggregate gradation method. The diamond-shaped MLP\
  \ architecture achieved MAPE of 4.61% for MR and 14.15% for DS predictions, outperforming\
  \ traditional ML models."
---

# Explainable Artificial Intelligent (XAI) for Predicting Asphalt Concrete Stiffness and Rutting Resistance: Integrating Bailey's Aggregate Gradation Method

## Quick Facts
- arXiv ID: 2410.21298
- Source URL: https://arxiv.org/abs/2410.21298
- Reference count: 0
- Key outcome: Diamond-shaped MLP achieves MAPE of 4.61% for MR and 14.15% for DS predictions using Bailey's aggregate gradation method

## Executive Summary
This study develops an explainable AI model that predicts asphalt concrete stiffness (MR) and rutting resistance (DS) using deep learning and SHAP values. The model leverages Bailey's aggregate gradation method to create a diamond-shaped multi-layer perceptron architecture that achieves high predictive accuracy. SHAP analysis reveals critical aggregate size thresholds, particularly at 0.6 mm, that significantly influence asphalt performance. The research also demonstrates how aggregate lithology impacts rutting resistance and provides web-based interfaces for practical application by engineers.

## Method Summary
The study employs a diamond-shaped MLP architecture with 14 input features representing Bailey's method parameters, expanding through hidden layers of 200, 1000, 200, 20, and 5 neurons before converging to a single output. The model uses LeakyReLU activations, Adam optimizer with L2 regularization, and k-fold cross-validation (k=10) to address the limited sample size (n=27). SHAP values are applied to interpret predictions and identify feature importance, while web-based interfaces enable practical application for engineers.

## Key Results
- Diamond-shaped MLP achieves MAPE of 4.61% for MR and 14.15% for DS predictions
- SHAP analysis identifies 0.6 mm sieve size as critical threshold affecting both properties
- Coarse aggregates primarily influence rutting resistance while medium-fine aggregates affect stiffness
- Aggregate lithology significantly impacts rutting resistance
- Web-based interfaces successfully deployed for practical application

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAP values enable decomposition of individual model predictions into feature contributions.
- Mechanism: SHAP applies game theory to fairly distribute prediction changes among features, ensuring local accuracy and consistency.
- Core assumption: Model predictions are additive combinations of feature effects.
- Evidence anchors:
  - [abstract] "SHAP (SHapley Additive exPlanations) values were applied to interpret the model's predictions, providing insights into the relative importance and impact of different gradation characteristics"
  - [section] "SHAP values, introduced by Lundberg and Lee (2017), provide a mathematically rigorous approach to interpreting machine learning model outputs. Grounded in coalitional game theory, SHAP values distribute the model's prediction f(x) among input features according to the equation f(x) = φ₀ + Σᵢ φᵢ(x)"

### Mechanism 2
- Claim: Diamond-shaped MLP architecture enhances feature extraction through expansion-contraction structure.
- Mechanism: Network expands from 14 inputs through 200→1000→200 neurons to capture complex patterns, then contracts through 20→5 neurons to distill features before final prediction.
- Core assumption: Complex relationships between aggregate gradation and asphalt properties require hierarchical feature learning.
- Evidence anchors:
  - [section] "This study employs a deep learning model with a diamond-shaped multi-layer perceptron (MLP) architecture, as illustrated in Fig. 4. Implemented in Python using the PyTorch library, the network expands from 14 input features through successive hidden layers of 200, 1000, 200, 20, and 5 neurons, before converging to a single output neuron."

### Mechanism 3
- Claim: K-fold cross-validation ensures robust model evaluation with limited sample size.
- Mechanism: Data partitioned into k equal folds; model trained on k-1 folds and validated on remaining fold, repeated k times to use all data for both training and testing.
- Core assumption: Limited sample size (n=27) requires careful validation to avoid overfitting and ensure generalization.
- Evidence anchors:
  - [section] "K-fold cross-validation (Jung and Hu 2015) was implemented to address potential biases in dataset partitioning and assess model generalization capability, given the limited sample size (n = 27)"
  - [section] "The dataset was partitioned into 10 equal folds. The validation process underwent 10 iterations, with each iteration designating a unique fold as the test set and the remaining 9 folds as the training set."

## Foundational Learning

- Concept: Aggregate gradation control sieves and Bailey's method ratios
  - Why needed here: Model inputs are Bailey's method parameters (CA, FAc, FAf ratios) derived from specific control sieve sizes; understanding these relationships is crucial for interpreting SHAP results
  - Quick check question: What is the relationship between Primary Control Sieve (PCS) and Nominal Maximum Particle Size (NMPS) in Bailey's method?

- Concept: SHAP value properties (local accuracy, missingness, consistency)
  - Why needed here: These properties ensure SHAP values provide reliable feature importance interpretations for the asphalt concrete predictions
  - Quick check question: How does the "missingness" property of SHAP values handle features that are not present in a particular prediction?

- Concept: LeakyReLU vs standard ReLU activation functions
  - Why needed here: Model uses LeakyReLU to avoid "dying ReLU" problem while maintaining non-linearity in hidden layers
  - Quick check question: What is the mathematical difference between LeakyReLU and standard ReLU, and why is this important for deep networks?

## Architecture Onboarding

- Component map: Data preprocessing → Standard scaler normalization → Diamond MLP (14→200→1000→200→20→5→1) with LeakyReLU activations → Sigmoid output → SHAP interpretation → Web interface
- Critical path: Data → Model training → SHAP analysis → Web deployment
- Design tradeoffs: Deeper network (more layers) could capture more complex patterns but risks overfitting with small dataset; shallower network may underfit
- Failure signatures: High MAPE values (>10%) indicate poor predictive performance; inconsistent SHAP values across similar inputs suggest model instability
- First 3 experiments:
  1. Test model performance with different MLP architectures (vary number of layers/neurons) while keeping dataset constant
  2. Compare SHAP interpretation results when using different aggregate lithologies (limestone vs basalt) as separate models
  3. Validate web interface predictions against laboratory test results for new aggregate gradations not in training set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the identified critical aggregate size thresholds, particularly the 0.6 mm sieve size, mechanistically influence asphalt concrete performance at the microstructural level?
- Basis in paper: [explicit] The study identified the 0.6 mm sieve size as a critical threshold affecting both resilience modulus and dynamic stability, but the underlying mechanisms are not fully explained.
- Why unresolved: While the SHAP analysis revealed the importance of this threshold, the paper does not delve into the specific physical or chemical processes that make this size range so influential.
- What evidence would resolve it: Microscopic analysis of aggregate-binder interactions at different size ranges, coupled with mechanical testing at the micro-scale, could elucidate the mechanisms behind this size-dependent performance.

### Open Question 2
- Question: How does the model's performance generalize to different environmental conditions and traffic loads not represented in the original dataset?
- Basis in paper: [inferred] The study used a specific dataset with limestone and basalt aggregates, but did not extensively test the model's robustness across diverse environmental conditions or traffic scenarios.
- Why unresolved: The paper acknowledges that the model's predictions are based on a specific dataset and may not generalize to all types of asphalt mixtures or environmental conditions.
- What evidence would resolve it: Testing the model with data from different climates, traffic intensities, and aggregate types would provide insights into its generalizability and robustness.

### Open Question 3
- Question: What is the optimal balance between Coarse Aggregate (CA) and Fine Aggregate (FA) ratios to simultaneously optimize both resilience modulus and dynamic stability?
- Basis in paper: [explicit] The study found differential impacts of CA and FA ratios on DS and MR, suggesting potential trade-offs in optimizing for one performance parameter versus another.
- Why unresolved: While the study identifies the differential impacts, it does not provide a clear framework for balancing these ratios to achieve optimal performance for both parameters simultaneously.
- What evidence would resolve it: A systematic optimization study varying both CA and FA ratios across a wide range, coupled with multi-objective optimization techniques, could identify the Pareto-optimal solutions for balancing these performance criteria.

## Limitations
- Study based on relatively small dataset (n=27) limiting generalizability
- Model may overfit to specific aggregate combinations tested
- SHAP interpretations may overstate causal relationships in complex material systems
- Does not address temporal variations in material properties or environmental conditions

## Confidence
- **High Confidence**: Technical implementation of diamond-shaped MLP architecture and SHAP analysis methodology
- **Medium Confidence**: Reported MAPE values and model performance metrics are plausible given dataset size and problem complexity
- **Low Confidence**: Generalizability of 0.6 mm sieve threshold finding and specific aggregate lithology impacts require verification with independent datasets

## Next Checks
1. **External Dataset Validation**: Test the model's predictive accuracy on a separate, larger dataset of asphalt concrete mixtures with different aggregate sources and gradation combinations to verify the generalizability of the 4.61% and 14.15% MAPE values.

2. **SHAP Value Consistency Analysis**: Conduct sensitivity analysis by systematically varying individual sieve sizes around the identified 0.6 mm threshold to confirm the stability and robustness of SHAP-based feature importance rankings across similar material compositions.

3. **Real-World Application Testing**: Deploy the web interface with a pilot group of asphalt mixture design engineers to evaluate the practical utility of the model predictions and SHAP explanations in actual project decision-making scenarios, comparing predicted versus observed MR and DS values.