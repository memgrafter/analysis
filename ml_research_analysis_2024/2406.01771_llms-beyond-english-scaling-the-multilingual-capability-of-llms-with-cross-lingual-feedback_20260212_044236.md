---
ver: rpa2
title: 'LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual
  Feedback'
arxiv_id: '2406.01771'
source_url: https://arxiv.org/abs/2406.01771
tags:
- high
- latn
- languages
- translate
- google
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of extending large language models
  (LLMs) to support more natural languages, especially low-resource ones, while also
  aligning them with human preferences. The core method involves constructing two
  datasets: a multilingual instruction dataset in 100 languages and a cross-lingual
  human feedback dataset in 30 languages.'
---

# LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback

## Quick Facts
- arXiv ID: 2406.01771
- Source URL: https://arxiv.org/abs/2406.01771
- Reference count: 20
- Models xLLaMA-100 and xBLOOM-100 outperform existing models on five multilingual benchmarks

## Executive Summary
This paper addresses the challenge of extending large language models to support more natural languages, particularly low-resource ones, while aligning with human preferences. The authors construct two datasets: a multilingual instruction dataset in 100 languages and a cross-lingual human feedback dataset in 30 languages. These datasets are used to perform multilingual instruction tuning and align the models with human feedback using the DPO algorithm. The resulting models, xLLaMA-100 and xBLOOM-100, demonstrate significant improvements in multilingual understanding and generation capabilities across five benchmarks.

## Method Summary
The authors construct a multilingual instruction dataset by translating the Alpaca dataset into 100 languages and generating responses using ChatGPT, Google Translate API, or NLLB based on translation quality. They perform supervised fine-tuning with LoRA on this dataset. Then, they create a cross-lingual human feedback dataset with instructions in one language and responses in another across 30 languages. Finally, they align the models using the DPO algorithm on this feedback dataset, resulting in the xLLaMA-100 and xBLOOM-100 models.

## Key Results
- xLLaMA-100 and xBLOOM-100 outperform existing models across five multilingual benchmarks
- Models achieve new state-of-the-art performance in multilingual understanding and generation
- Significant improvements demonstrated in both high-resource and low-resource languages
- Effectively addresses the off-target problem (generating text in wrong language)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual human feedback dataset improves multilingual generation capability by simulating multilingual generation scenarios
- Mechanism: Instructions and outputs in different languages teach models to generate outputs in various target languages when given instructions in a different source language
- Core assumption: Human feedback quality is high enough in 30 target languages to effectively guide model alignment
- Evidence anchors: Cross-lingual human feedback data covering 30 languages to enhance generating capability; DPO algorithm fine-tuning

### Mechanism 2
- Claim: Multilingual instruction dataset with 100 languages enables models to understand instructions in diverse languages
- Mechanism: Translating Alpaca instructions into 100 languages and generating responses in same language trains model to comprehend instructions across wide language spectrum
- Core assumption: Translation quality preserves instruction semantics across languages
- Evidence anchors: Multilingual instruction dataset including 100 languages; SFT on constructed instruction data

### Mechanism 3
- Claim: DPO algorithm with cross-lingual feedback is more efficient than RLHF for aligning multilingual models
- Mechanism: DPO fine-tunes model using supervised learning on human preference data, avoiding instability and computational cost of reinforcement learning
- Core assumption: Supervised learning on preference data can effectively capture reward signal that RLHF would provide
- Evidence anchors: DPO algorithm used on cross-lingual human feedback dataset; computational resource efficiency

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: Full fine-tuning of large models is computationally expensive; PEFT allows efficient adaptation to multilingual tasks
  - Quick check question: What is the primary advantage of using LoRA (a PEFT method) over full fine-tuning?

- Concept: Human feedback alignment
  - Why needed here: LLMs need to generate outputs that align with human preferences, not just be fluent in multiple languages
  - Quick check question: How does DPO differ from traditional RLHF in terms of computational requirements?

- Concept: Multilingual evaluation metrics
  - Why needed here: Standard metrics like BLEU and ROUGE need to be applied across diverse language pairs to assess true multilingual capability
  - Quick check question: Why might BLEU scores vary significantly between high-resource and low-resource languages?

## Architecture Onboarding

- Component map: Alpaca instructions -> Translation to 100 languages -> Response generation -> SFT with LoRA -> Cross-lingual human feedback -> DPO with LoRA -> xLLaMA-100/xBLOOM-100
- Critical path: Construct multilingual instruction dataset -> SFT with LoRA -> Construct cross-lingual human feedback dataset -> DPO with LoRA -> Evaluate on all benchmarks
- Design tradeoffs:
  - Dataset size vs. coverage: 100 languages for instructions but only 30 for feedback due to quality constraints
  - Translation vs. generation for responses: Hybrid approach balances quality and native speaker style
  - Model size: 7B models chosen for computational efficiency, though larger models may perform better
- Failure signatures:
  - Off-target generation (wrong language output)
  - Degradation in non-target languages after fine-tuning
  - Inconsistent performance across high-resource vs. low-resource languages
  - Poor translation quality affecting instruction understanding
- First 3 experiments:
  1. Fine-tune base model on multilingual instruction dataset only, evaluate on PAWS-X and XCOPA
  2. Fine-tune base model on cross-lingual human feedback dataset only, evaluate on FLORES and XL-Sum
  3. Compare performance of instruction-tuned vs. feedback-tuned models on all benchmarks to identify which capability improves most

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology and results raise several important research directions regarding optimal data combinations, model size scaling effects, and practical deployment challenges.

## Limitations
- Translation quality dependence: Entire multilingual instruction dataset relies on translation quality from Google Translate API and NLLB
- Human feedback bottlenecks: Cross-lingual human feedback dataset covers only 30 languages despite instruction dataset covering 100 languages
- Dataset construction opacity: Quality thresholds for choosing response generation methods are not explicitly specified

## Confidence
- High Confidence: Basic methodology of using parameter-efficient fine-tuning (LoRA) for multilingual instruction tuning is well-established and technically sound
- Medium Confidence: Performance improvements on five benchmarks are reported consistently, but underlying data quality and potential evaluation biases are not fully transparent
- Medium Confidence: Claim about addressing the off-target problem is supported by reported metrics, but practical significance in real-world deployment scenarios is not evaluated

## Next Checks
1. **Translation Quality Validation**: Conduct independent assessment of translation quality across all 100 languages using human evaluation on random sample of instructions
2. **Cross-Lingual Feedback Completeness**: Perform ablation studies by fine-tuning models with feedback data for only 10, 20, and all 30 languages covered by feedback dataset
3. **Real-World Deployment Testing**: Deploy xLLMs-100 models in actual multilingual customer service scenarios where instructions may be in one language and responses expected in another