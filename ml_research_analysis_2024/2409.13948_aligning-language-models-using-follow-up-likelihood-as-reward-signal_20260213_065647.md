---
ver: rpa2
title: Aligning Language Models Using Follow-up Likelihood as Reward Signal
arxiv_id: '2409.13948'
source_url: https://arxiv.org/abs/2409.13948
tags:
- reward
- language
- data
- arxiv
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes "Follow-up Likelihood as Reward" (FLR), a novel
  method that leverages user follow-up utterances as implicit feedback signals to
  evaluate and improve LLM response quality without relying on human or LLM-based
  preference annotations. The core idea is to use the conditional likelihood of curated
  positive and negative follow-ups given a prompt-response pair as a reward signal.
---

# Aligning Language Models Using Follow-up Likelihood as Reward Signal

## Quick Facts
- **arXiv ID**: 2409.13948
- **Source URL**: https://arxiv.org/abs/2409.13948
- **Reference count**: 40
- **Primary result**: FLR achieves competitive performance with strong reward models on multiple benchmarks, improving helpfulness of base policy models like Llama-3-8B-Instruct and Qwen2-7B-Instruct.

## Executive Summary
This paper proposes Follow-up Likelihood as Reward (FLR), a novel method that leverages user follow-up utterances as implicit feedback signals to evaluate and improve LLM response quality without relying on human or LLM-based preference annotations. The core idea is to use the conditional likelihood of curated positive and negative follow-ups given a prompt-response pair as a reward signal. FLR achieves competitive performance with strong reward models on multiple benchmarks, and its effectiveness is further enhanced by fine-tuning the language model with natural language feedback data. The method enables automatic preference data mining for direct alignment, significantly improving the helpfulness of base policy models like Llama-3-8B-Instruct and Qwen2-7B-Instruct.

## Method Summary
FLR uses the likelihood of follow-up utterances as rewards to differentiate preferred responses from less favored ones. The method involves curating positive and negative follow-up utterances, computing their conditional log probabilities given prompt-response pairs using an instruction-tuned LLM, and aggregating scores across understanding, engagingness, and instruction-following categories. To align base models, FLR automatically mines preference data from online generations by selecting the highest-scoring response as positive and the lowest-scoring as negative, then fine-tunes the base model using DPO or KTO on these synthetic preference pairs. The follow-up LLM itself can be enhanced through fine-tuning with natural language feedback data to improve reward modeling performance.

## Key Results
- FLR achieves competitive accuracy with established reward models on 8 pairwise preference datasets (e.g., HH-RLHF, BeaverTails, MT-Bench Pairwise)
- FLR demonstrates strong Pearson correlation on 4 rating-based benchmarks (e.g., RewardBench Chat Easy/Hard, Preference Bench)
- FLR-enhanced models show improved helpfulness metrics including Alpaca-Eval V2 LC win rate and WildBench V2 win rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Follow-up utterances can serve as implicit feedback signals to evaluate response quality.
- Mechanism: Instruction-tuned LLMs, trained on high-quality conversational data, generate follow-up utterances whose likelihood given a prompt-response pair indicates response quality. Positive follow-ups are more likely after helpful responses; negative follow-ups after unhelpful ones.
- Core assumption: Strong instruction-tuned LLMs have learned conversational dynamics that reflect human feedback patterns.
- Evidence anchors:
  - [abstract] "we propose using the likelihood of follow-up utterances as rewards to differentiate preferred responses from less favored ones"
  - [section 2] "many powerful instruction-tuned LLMs...are fine-tuned on high-quality conversational data that inherently reflect human-like feedback dynamics"
  - [corpus] Weak evidence - related papers focus on preference optimization and reward modeling but don't specifically address follow-up likelihood

### Mechanism 2
- Claim: Fine-tuning the LLM with natural language feedback data enhances its ability to distinguish helpful from unhelpful responses.
- Mechanism: The LLM learns to generate more appropriate follow-up likelihoods by being trained to produce natural language feedback for prompt-response pairs, effectively teaching it what constitutes helpful responses.
- Core assumption: Natural language feedback contains signal about response quality that can be learned.
- Evidence anchors:
  - [abstract] "fine-tuning the language model that provides follow-up likelihood with natural language feedback significantly enhances FLR's performance"
  - [section 3.2] "we finetune M using natural language feedback data...the model is optimized to generate the natural language feedback"
  - [corpus] Moderate evidence - related papers discuss reward modeling from natural language feedback

### Mechanism 3
- Claim: Automatic preference data mining from online generations enables direct alignment of base policy models.
- Mechanism: The policy model generates multiple response candidates, FLR scores them using follow-up likelihoods, and the highest-scoring response becomes the positive example while the lowest becomes negative. This creates preference pairs for DPO/KTO fine-tuning.
- Core assumption: The highest-scoring response according to FLR is genuinely better than lower-scoring ones.
- Evidence anchors:
  - [abstract] "we propose to automatically mine preference data from the online generations of a base policy model"
  - [section 3.3] "The candidate with the highest score is treated as the positive response...The candidate with the lowest score is treated as the negative response"
  - [corpus] Moderate evidence - related papers discuss online DPO and self-improvement

## Foundational Learning

- **Concept**: Reward modeling and preference optimization
  - Why needed here: The entire approach relies on using follow-up likelihoods as reward signals to guide model alignment
  - Quick check question: What's the difference between reward modeling and preference optimization in the context of LLM alignment?

- **Concept**: Language model fine-tuning techniques (SFT, DPO, KTO)
  - Why needed here: The paper uses multiple fine-tuning approaches - SFT for the follow-up LLM and DPO/KTO for the base policy model
  - Quick check question: How does DPO differ from KTO in terms of data requirements and optimization objectives?

- **Concept**: Conversational dynamics and human feedback patterns
  - Why needed here: The core assumption is that instruction-tuned LLMs have learned these patterns from training data
  - Quick check question: What types of conversational data are most likely to teach an LLM about helpful response feedback?

## Architecture Onboarding

- **Component map**: Nectar prompts → Base model (Llama-3-8B-Instruct/Qwen2-7B-Instruct) → FLR scores → Preference pairs → DPO/KTO fine-tuning → Aligned model
- **Critical path**: Prompt → Base model generates k responses → FLR scores each response → Select best/worst → Create preference pair → Fine-tune base model with DPO/KTO
- **Design tradeoffs**:
  - Using follow-up likelihood vs. traditional reward models: no need for human/GPT-4 annotations but depends on quality of instruction-tuned LLM
  - Single LLM vs. separate models: using the base model as M saves resources but may introduce bias
  - Number of response candidates (k): more candidates provide better coverage but increase computation
- **Failure signatures**:
  - FLR scores don't correlate with human judgments → Follow-up utterances aren't capturing quality signals
  - Alignment doesn't improve helpfulness metrics → Preference pairs are poor quality or fine-tuning isn't effective
  - Model performance degrades on general tasks → Alignment is overfitting to specific patterns
- **First 3 experiments**:
  1. Test FLR scoring on a small set of manually labeled prompt-response pairs to verify correlation with human judgments
  2. Compare FLR performance with and without natural language feedback fine-tuning on the reward modeling benchmarks
  3. Run alignment on a small prompt set and measure improvement on a simple helpfulness metric before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FLR vary when applied to different instruction-tuned LLM architectures beyond Llama-3-8B-Instruct and Qwen2-7B-Instruct?
- Basis in paper: The paper experiments with Llama-3-8B-Instruct and Qwen2-7B-Instruct as the instruction-tuned LLM (M) used in FLR. It is mentioned that FLR's performance may be sensitive to factors like the training method and type of training data used in the underlying language model.
- Why unresolved: The paper only tests FLR with two specific LLM architectures. It does not explore how FLR's effectiveness changes when applied to other instruction-tuned LLMs, such as GPT-4, Gemini, or open-source alternatives like Mistral or Falcon.
- What evidence would resolve it: Testing FLR with a diverse set of instruction-tuned LLMs, including both proprietary and open-source models, and comparing their performance on the same reward modeling benchmarks would provide insights into FLR's generalizability across different architectures.

### Open Question 2
- Question: Can FLR be effectively adapted to leverage real-time user feedback from interactive human-machine conversations, rather than relying on pre-curated follow-up utterances?
- Basis in paper: The paper uses manually curated sets of positive and negative follow-up utterances to compute the FLR reward signal. It is noted that conducting an exhaustive search for all contextually relevant follow-ups is intractable, and the automatic search for suitable follow-ups is left to future work.
- Why unresolved: The paper does not explore the feasibility of dynamically generating or selecting follow-up utterances based on the specific context of a conversation, nor does it investigate the potential benefits of using real-time user feedback.
- What evidence would resolve it: Developing and testing an adaptive FLR mechanism that can generate or select contextually relevant follow-up utterances on-the-fly during a conversation, and evaluating its performance compared to the static approach, would demonstrate the viability of real-time user feedback integration.

### Open Question 3
- Question: What is the optimal strategy for fine-tuning the instruction-tuned LLM (M) used in FLR with natural language feedback data to maximize its reward modeling performance?
- Basis in paper: The paper fine-tunes M using natural language feedback data to enhance its ability to assign high likelihoods to positive follow-ups and low likelihoods to negative follow-ups for helpful responses. However, it is acknowledged that more effective ways to leverage NL feedback or higher-quality NL feedback could further enhance FLR's performance.
- Why unresolved: The paper uses a straightforward supervised fine-tuning approach with a fixed set of natural language feedback data. It does not explore different fine-tuning strategies, such as few-shot learning, reinforcement learning, or curriculum learning, nor does it investigate the impact of feedback data quality and diversity.
- What evidence would resolve it: Experimenting with various fine-tuning strategies and analyzing their impact on FLR's performance, as well as curating and testing higher-quality and more diverse natural language feedback data, would provide insights into the optimal approach for enhancing M's reward modeling capabilities.

## Limitations

- The method's effectiveness depends heavily on the quality and representativeness of the natural language feedback data used for fine-tuning the follow-up LLM.
- FLR's performance may not generalize well to domains outside the instruction-following domain where base models were trained.
- The approach doesn't address potential trade-offs between optimizing for follow-up likelihood and other important dimensions like factuality or safety.

## Confidence

**High Confidence Claims**:
- FLR can achieve competitive performance with established reward models on benchmark datasets
- Fine-tuning with natural language feedback improves FLR performance
- Automatic preference data mining enables direct alignment without human annotations

**Medium Confidence Claims**:
- FLR generalizes across different base models (tested on Llama-3 and Qwen2, but with limited architectural diversity)
- The three categories of follow-up utterances comprehensively capture response quality
- FLR's improvements are due to the follow-up likelihood mechanism specifically

## Next Checks

1. **Cross-domain robustness test**: Evaluate FLR on prompts from domains not represented in the training data (e.g., medical, legal, technical) to verify whether the follow-up likelihood signal remains reliable outside the instruction-following domain.

2. **Safety and factuality assessment**: Measure FLR-aligned models on benchmarks that specifically test factual accuracy and safety constraints (e.g., TruthfulQA, RealToxicityPrompts) to determine whether optimizing for follow-up likelihood inadvertently compromises these critical dimensions.

3. **Human preference validation**: Conduct a controlled human study where annotators rate the same prompt-response pairs using both traditional pairwise comparison and follow-up likelihood prediction tasks, to directly verify whether the likelihood signal correlates with human quality judgments beyond automated benchmarks.