---
ver: rpa2
title: Do Pretrained Contextual Language Models Distinguish between Hebrew Homograph
  Analyses?
arxiv_id: '2405.07099'
source_url: https://arxiv.org/abs/2405.07099
tags:
- noun
- hebrew
- homograph
- homographs
- cons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates whether pretrained contextual language models\
  \ can effectively disambiguate Hebrew homographs, which are words with multiple\
  \ analyses due to omitted vowels, affixes, and lack of capitalization. The researchers\
  \ developed a large dataset of 150,000 sentences covering 75 high-frequency Hebrew\
  \ homographs with 2\u20135 possible analyses each."
---

# Do Pretrained Contextual Language Models Distinguish between Hebrew Homograph Analyses?

## Quick Facts
- arXiv ID: 2405.07099
- Source URL: https://arxiv.org/abs/2405.07099
- Reference count: 24
- Primary result: AlephBERT achieves high accuracy on Hebrew homograph disambiguation even with few training examples

## Executive Summary
This study evaluates whether pretrained contextual language models can effectively disambiguate Hebrew homographs - words with multiple analyses due to omitted vowels, affixes, and lack of capitalization. The researchers developed a large dataset of 150,000 sentences covering 75 high-frequency Hebrew homographs with 2-5 possible analyses each. They tested existing Hebrew BERT models (mBERT, HeBERT, AlephBERT) and a previous word2vec-based BiLSTM method using 10-fold cross-validation and few-shot learning scenarios. Results show that contextualized embeddings (AlephBERT) significantly outperform non-contextualized models and previous SOTA, especially for segmentation and morphosyntactic ambiguity, but are less effective for pure sense disambiguation. AlephBERT maintains high accuracy even with few training examples (5-100 sentences per analysis) and performs well for both balanced and skewed homograph distributions.

## Method Summary
The researchers developed a dataset of 150,000 sentences containing 75 high-frequency Hebrew homographs with 2-5 possible analyses each. They used 10-fold cross-validation to test four models: mBERT, HeBERT, AlephBERT, and a previous word2vec-based BiLSTM method. For each homograph, they extracted contextual embeddings from the models and trained MLP classifiers to predict the correct analysis. They also tested few-shot learning scenarios with 5, 10, 25, and 100 training examples per analysis. Performance was evaluated using F1 scores, and they conducted probing experiments to understand how models make disambiguation decisions.

## Key Results
- AlephBERT achieves F1 scores of 0.85-0.96 on binary homograph classification
- Contextualized embeddings significantly outperform non-contextualized models and previous SOTA methods
- AlephBERT maintains high accuracy even with few training examples (5-100 sentences per analysis)
- Masked tokenization performs similarly to unmasked for AlephBERT, unlike non-contextualized models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AlephBERT's contextualized embeddings outperform non-contextualized models because they capture word-internal structure through word-piece tokenization.
- Mechanism: The BERT architecture learns hierarchical representations where subword units combine to encode morphological and syntactic patterns specific to Hebrew homographs.
- Core assumption: Word-piece tokenization, despite arbitrary splits, still captures sufficient morphological structure when combined with contextual attention mechanisms.
- Evidence anchors:
  - [abstract]: "contextualized embeddings (AlephBERT) significantly outperform non-contextualized models"
  - [section]: "contextualized language models do effectively capture Hebrew homograph distinctions, even those based on word-pieces"
  - [corpus]: Weak - corpus doesn't directly address tokenization effectiveness
- Break condition: If word-piece splits become too granular (3+ pieces) or homographs have 4+ analyses, performance degrades significantly.

### Mechanism 2
- Claim: AlephBERT maintains high accuracy even with few training examples because its pretraining provides rich semantic priors.
- Mechanism: The model's attention mechanisms can generalize from limited examples by leveraging distributional patterns learned during pretraining on 98.7M Hebrew tokens.
- Core assumption: Pretraining on large unlabeled data transfers sufficient linguistic knowledge to handle rare homograph analyses.
- Evidence anchors:
  - [abstract]: "AlephBERT maintains high accuracy even with few training examples (5–100 sentences per analysis)"
  - [section]: "AlephBERT embeddings provide a highly accurate solution even on this few-shot basis"
  - [corpus]: Weak - corpus doesn't provide direct evidence about few-shot performance
- Break condition: If training examples drop below 5 per analysis or homographs have highly skewed distributions (>50:1 ratio).

### Mechanism 3
- Claim: Masked scenario performs similarly to unmasked because AlephBERT's attention mechanisms don't rely heavily on the target word itself.
- Mechanism: The contextual attention heads can infer homograph meaning from surrounding context without needing the actual token, preventing distribution bias.
- Core assumption: Contextual patterns are sufficiently informative to disambiguate without explicit token presence.
- Evidence anchors:
  - [abstract]: "AlephBERT achieves high scores both with balanced homographs as well as with homographs of highly skewed distribution"
  - [section]: "Using a [MASK] token instead of the actual word does not generally improve performance"
  - [corpus]: Weak - corpus doesn't directly test masked vs unmasked performance
- Break condition: If homographs have ambiguous surrounding contexts or if distribution bias is severe.

## Foundational Learning

- Concept: Word-piece tokenization and subword segmentation
  - Why needed here: Hebrew homographs are broken into word pieces in mBERT, affecting disambiguation performance
  - Quick check question: How does word-piece tokenization handle morphologically complex words differently than whole-word approaches?

- Concept: Contextual attention mechanisms in transformer models
  - Why needed here: AlephBERT uses attention to disambiguate homographs based on surrounding context
  - Quick check question: What role do attention heads play in distinguishing between homograph analyses?

- Concept: Few-shot learning and transfer learning principles
  - Why needed here: AlephBERT performs well with minimal training data due to pretraining
  - Quick check question: How does pretraining on large corpora enable effective few-shot performance?

## Architecture Onboarding

- Component map: AlephBERT model -> MLP classifier (768→100→2-5 outputs) -> F1 score evaluation
- Critical path: 1. Load sentence through AlephBERT 2. Extract homograph embedding (768-dim) 3. Pass through MLP for classification 4. Evaluate with F1 score across folds 5. Aggregate results across homographs
- Design tradeoffs: Using raw embeddings vs. contextual attention; Masked vs unmasked token processing; 2-way vs 3-way vs 4-way ambiguity handling; Balanced vs skewed distribution datasets
- Failure signatures: Sharp performance drop when word-piece splits >2; Degraded accuracy for homographs with 4+ analyses; Poor performance on purely semantic ambiguities; Sensitive to skewed distributions without masking
- First 3 experiments: 1. Test AlephBERT on binary homograph classification (baseline) 2. Compare masked vs unmasked performance on skewed distributions 3. Evaluate few-shot performance (5, 10, 25 examples per analysis)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Hebrew BERT models perform on homographs with 4+ possible analyses compared to 2-3 way ambiguities?
- Basis in paper: [explicit] The paper shows AlephBERT performs well on 2-way and 3-way ambiguities but accuracy declines for 4-way or 5-way classification (Figure 3).
- Why unresolved: The study only tested up to 5-way ambiguities with limited data points for 4-way cases, leaving uncertainty about performance trends for higher ambiguity levels.
- What evidence would resolve it: Testing AlephBERT on a larger dataset of homographs with 4+ analyses would clarify performance degradation patterns and whether there's a threshold beyond which disambiguation becomes ineffective.

### Open Question 2
- Question: Does the effectiveness of masked vs unmasked tokenization depend on the specific morphosyntactic features being disambiguated?
- Basis in paper: [explicit] The study compared masked and unmasked scenarios but found no general improvement with masking across different homograph distributions (Figure 5).
- Why unresolved: The analysis didn't examine whether masking helps specifically for certain types of ambiguity (segmentation vs morphosyntactic vs semantic) or particular morphosyntactic features.
- What evidence would resolve it: A fine-grained analysis comparing masked vs unmasked performance across different ambiguity types and morphosyntactic features would reveal if masking has selective benefits.

### Open Question 3
- Question: What is the minimum number of training examples needed for AlephBERT to maintain high accuracy on homograph disambiguation?
- Basis in paper: [explicit] The study tested few-shot scenarios with 5-100 examples per analysis and found AlephBERT maintained accuracy even with only 5 examples (Figure 6).
- Why unresolved: The experiments only tested down to 5 examples, leaving uncertainty about whether this represents the absolute minimum or if accuracy would drop significantly with fewer examples.
- What evidence would resolve it: Testing AlephBERT on even fewer examples (1-4 per analysis) would establish the true minimum threshold for effective few-shot learning.

## Limitations
- Relies on crowdsourced morphological analyses that may not capture all valid interpretations
- 75 high-frequency homographs may not be representative of all Hebrew homographs
- Uses static MLP classifiers on top of contextual embeddings rather than fine-tuning entire model

## Confidence

**High confidence** (95%):
- AlephBERT outperforms non-contextualized models and previous SOTA methods
- Contextualized embeddings are effective for segmentation and morphosyntactic ambiguity
- AlephBERT maintains strong performance with few training examples (5-100 per analysis)
- Results are statistically significant (paired t-tests, p < 0.05)

**Medium confidence** (80%):
- Masked scenario performs similarly to unmasked for AlephBERT
- Performance degradation occurs with 4+ analyses or highly skewed distributions
- Word-piece tokenization captures sufficient morphological structure for disambiguation

**Low confidence** (60%):
- Generalization to other Semitic languages with similar morphological systems
- Performance on homographs outside the 75 studied (particularly low-frequency items)
- Long-term stability of results across different BERT variants or training epochs

## Next Checks
1. Cross-linguistic validation: Test the same methodology on Modern Standard Arabic or other morphologically rich Semitic languages using their respective pretrained contextual models.

2. Fine-tuning comparison: Implement end-to-end fine-tuning of AlephBERT (rather than frozen embeddings with MLP classifier) to determine if this architectural change improves performance.

3. Dynamic sampling analysis: Conduct experiments with controlled homograph distributions (e.g., 10:1, 20:1, 50:1 ratios) while systematically varying training set sizes to identify precise break points where few-shot performance degrades.