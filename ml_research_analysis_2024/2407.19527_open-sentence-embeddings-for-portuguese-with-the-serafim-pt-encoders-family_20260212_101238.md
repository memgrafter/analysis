---
ver: rpa2
title: Open Sentence Embeddings for Portuguese with the Serafim PT* encoders family
arxiv_id: '2407.19527'
source_url: https://arxiv.org/abs/2407.19527
tags:
- sentence
- encoders
- portuguese
- data
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Serafim PT, a family of open-source sentence
  encoders for Portuguese with three sizes (100M, 335M, and 900M parameters) that
  achieve state-of-the-art performance on semantic text similarity and information
  retrieval tasks. The models are trained using a combination of supervised and unsupervised
  methods including contrastive learning, GISTEmbed, and task-specific fine-tuning.
---

# Open Sentence Embeddings for Portuguese with the Serafim PT* encoders family

## Quick Facts
- arXiv ID: 2407.19527
- Source URL: https://arxiv.org/abs/2407.19527
- Reference count: 30
- Three open-source Portuguese sentence encoders (100M, 335M, 900M parameters) achieve state-of-the-art performance on semantic tasks

## Executive Summary
The paper introduces Serafim PT*, a family of three open-source sentence encoders for Portuguese designed to advance semantic text understanding in the language. These models, available in small (100M), medium (335M), and large (900M) sizes, demonstrate superior performance on semantic similarity and information retrieval tasks compared to both previous Portuguese-specific encoders and multilingual models. The training methodology combines supervised and unsupervised approaches, including contrastive learning and task-specific fine-tuning, resulting in significant improvements in evaluation metrics. All models are openly available under a permissive license, addressing the need for high-quality Portuguese language models in the research community.

## Method Summary
Serafim PT* models are developed using a hybrid training approach that combines supervised and unsupervised learning techniques. The training pipeline incorporates contrastive learning methods, GISTEmbed for semantic understanding, and task-specific fine-tuning on downstream applications. Three model sizes are created to balance performance with computational efficiency, allowing users to select appropriate models based on their specific requirements. The models are evaluated on Portuguese semantic text similarity (STS) tasks and information retrieval benchmarks, demonstrating state-of-the-art performance across all sizes.

## Key Results
- Achieved 0.02-0.04 Spearman correlation improvement on Portuguese STS tasks over previous models
- Reached MRR@10 scores above 0.85 on Portuguese information retrieval tasks
- Outperformed multilingual models like mBERT and XLM-R on Portuguese-specific benchmarks

## Why This Works (Mechanism)
The models leverage contrastive learning to learn meaningful semantic representations by distinguishing between similar and dissimilar sentence pairs. The combination of supervised fine-tuning on specific tasks with unsupervised pretraining creates robust embeddings that generalize well across different applications. The multi-size approach allows for optimal performance-to-efficiency trade-offs, where larger models capture more nuanced semantic relationships while smaller models provide adequate performance for less complex tasks.

## Foundational Learning
- **Contrastive learning**: Why needed - to learn discriminative semantic representations by comparing similar and dissimilar examples; Quick check - evaluate embedding separation in embedding space using nearest neighbor analysis
- **GISTEmbed**: Why needed - provides task-agnostic semantic understanding before fine-tuning; Quick check - measure semantic clustering quality on held-out data
- **Task-specific fine-tuning**: Why needed - adapts general representations to downstream application requirements; Quick check - compare performance with and without fine-tuning on target tasks

## Architecture Onboarding
- **Component map**: Tokenization -> Transformer encoder -> Pooling layer -> Output embeddings
- **Critical path**: Input text → tokenization → contextual embedding generation → pooling → final sentence representation
- **Design tradeoffs**: Larger models provide better performance but require more computational resources; smaller models offer faster inference at some accuracy cost
- **Failure signatures**: Performance degradation on out-of-domain texts; sensitivity to input length variations
- **First experiments**: 1) Test embedding quality on STS benchmarks, 2) Evaluate retrieval performance on IR datasets, 3) Measure inference speed across different hardware configurations

## Open Questions the Paper Calls Out
The paper acknowledges that while the models show strong performance on Portuguese-language tasks, their cross-lingual transfer capabilities remain untested. The exact contribution of each training component to final performance is not fully isolated through ablation studies. Additionally, the models' performance on specialized domains such as technical documentation or legal texts has not been systematically evaluated.

## Limitations
- Evaluation focused primarily on Portuguese tasks with limited cross-lingual testing
- Model selection analysis could benefit from more systematic ablation studies
- Performance improvements may vary across domains not covered in evaluation

## Confidence
- Performance claims (state-of-the-art results): High
- Model selection guidance: Medium
- Training methodology contributions: Medium
- Cross-lingual capabilities: Low

## Next Checks
1. Test model performance on out-of-domain Portuguese texts (e.g., technical documentation, legal texts) to validate generalizability
2. Conduct ablation studies to isolate the contribution of supervised vs. unsupervised training components
3. Evaluate cross-lingual transfer to related languages (Spanish, Italian) to assess zero-shot performance beyond Portuguese