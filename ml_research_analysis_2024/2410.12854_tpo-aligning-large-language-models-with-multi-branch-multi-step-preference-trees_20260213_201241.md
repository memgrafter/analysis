---
ver: rpa2
title: 'TPO: Aligning Large Language Models with Multi-branch & Multi-step Preference
  Trees'
arxiv_id: '2410.12854'
source_url: https://arxiv.org/abs/2410.12854
tags:
- preference
- reasoning
- reward
- step
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Tree Preference Optimization (TPO), a method
  designed to enhance the long-chain reasoning capabilities of large language models
  (LLMs) by directly learning from multi-branch and multi-step preference trees, rather
  than sampling binary preference pairs. TPO formulates the alignment task as a Preference
  List Ranking problem, enabling models to learn from ranked preference lists with
  varying reward values.
---

# TPO: Aligning Large Language Models with Multi-branch & Multi-step Preference Trees

## Quick Facts
- arXiv ID: 2410.12854
- Source URL: https://arxiv.org/abs/2410.12854
- Authors: Weibin Liao; Xu Chu; Yasha Wang
- Reference count: 40
- Key outcome: TPO improves LLM reasoning accuracy by 1.44%-4.49% over DPO across 5 models and 4 datasets

## Executive Summary
TPO introduces a novel approach to LLM alignment that learns directly from multi-branch and multi-step preference trees rather than sampling binary preference pairs. By formulating alignment as a Preference List Ranking problem and employing an Adaptive Step Reward mechanism based on semantic similarity, TPO enables more effective learning from ranked preference lists with varying reward values. The method demonstrates consistent improvements over Direct Preference Optimization (DPO) on mathematical reasoning tasks while showing better generalization to out-of-distribution tasks, though it may cause more catastrophic forgetting in some cases.

## Method Summary
TPO directly learns from entire preference trees using Preference List Ranking optimization with Adaptive Step Reward. The method parses preference trees into ranked lists, computes lambda weights for ranking pairs, adjusts reward margins based on semantic similarity between steps using cosine similarity, and optimizes a listwise ranking loss. TPO uses standard supervised fine-tuning with CoT prompting, no demonstrations, and is trained using AdamW optimizer with cosine learning rate scheduler. The approach is designed to capture multi-branch and multi-step reasoning information that binary sampling methods like DPO discard.

## Key Results
- TPO consistently outperforms DPO on mathematical reasoning tasks across 5 different LLMs and 4 datasets
- Average accuracy improvements range from 1.44% to 4.49% compared to DPO
- TPO shows better generalization to out-of-distribution tasks like coding and reasoning
- The method is particularly effective when dealing with preference lists of varying sizes and reward distributions

## Why This Works (Mechanism)

### Mechanism 1
TPO improves preference learning by directly modeling entire preference trees rather than sampling binary pairs. Instead of sampling paired preferences from trees, TPO uses the full preference tree and treats alignment as a Preference List Ranking problem, enabling learning from ranked lists with varying reward values. The core assumption is that preference trees contain valuable multi-branch and multi-step information that binary sampling discards, and this full information can be leveraged for better alignment.

### Mechanism 2
Adaptive Step Reward improves discrimination between similar reasoning steps by adjusting rewards based on semantic similarity. For each pair of steps, TPO computes cosine similarity between their semantic embeddings and uses this to scale the reward margin, giving less weight to shared or semantically similar steps. The core assumption is that many reasoning trajectories share sub-trajectories, which reduces the reward margin between them; adjusting rewards based on semantic similarity can restore discriminative power.

### Mechanism 3
Lambda Weight in the ranking loss improves optimization by considering absolute positions in the ranked list. TPO incorporates lambda weights that depend on both reward differences and ranking positions, making the loss sensitive to the absolute position of preferences in the list. The core assumption is that the absolute position of preferences in a ranked list contains additional information beyond pairwise comparisons that can improve optimization.

## Foundational Learning

- Concept: Bradley-Terry model for pairwise preferences
  - Why needed here: Forms the theoretical foundation for how preferences are modeled and converted to rewards in DPO and TPO
  - Quick check question: What does the Bradley-Terry model assume about the probability of preferring one response over another?

- Concept: Listwise ranking loss vs pairwise ranking loss
  - Why needed here: TPO uses listwise ranking (Preference List Ranking) rather than pairwise ranking, which is fundamental to understanding its approach
  - Quick check question: How does listwise ranking differ from pairwise ranking in terms of the information it uses?

- Concept: Semantic similarity and embedding space
  - Why needed here: Adaptive Step Reward relies on computing semantic similarity between reasoning steps using their embeddings
  - Quick check question: Why might cosine similarity between embeddings be useful for adjusting reward margins between reasoning steps?

## Architecture Onboarding

- Component map: Preference tree -> Ranked list -> Lambda weight computation -> Adaptive Step Reward -> Listwise ranking loss -> Backpropagation
- Critical path: 1. Parse preference tree into ranked list 2. Compute lambda weights for ranking pairs 3. For each step pair, compute semantic similarity and adjust reward margin 4. Compute listwise ranking loss 5. Backpropagate and update model
- Design tradeoffs: Full tree vs sampled pairs: TPO uses full tree (better information utilization) but higher computational cost; Semantic similarity metric: Cosine similarity is simple but may not capture all relevant differences; Lambda weight formulation: More complex than simple pairwise ranking but provides position sensitivity
- Failure signatures: Poor performance despite more data: Could indicate that preference trees are too noisy or rewards are poorly calibrated; Catastrophic forgetting: TPO may overfit to in-distribution tasks and lose generalization; Training instability: Complex loss function with multiple components may require careful hyperparameter tuning
- First 3 experiments: 1. Compare TPO vs DPO on a simple preference tree dataset with clear ranking structure 2. Test Adaptive Step Reward effectiveness by creating pairs with varying semantic similarity 3. Evaluate lambda weight impact by comparing with and without lambda weighting on the same data

## Open Questions the Paper Calls Out

### Open Question 1
How does the Adaptive Step Reward mechanism affect catastrophic forgetting when applying TPO to out-of-distribution tasks? The paper mentions that TPO may introduce stronger catastrophic forgetting compared to DPO, particularly on out-of-distribution datasets like HumanEval, and suggests that this could be mitigated by incorporating memory replay, regularization constraints, or meta-learning techniques. This remains unresolved because the paper acknowledges this limitation but does not experimentally test whether incorporating these mitigation strategies would reduce catastrophic forgetting while maintaining TPO's performance benefits on in-distribution tasks.

### Open Question 2
How does the quality and reliability of the reward values assigned by ChatGPT affect TPO's performance? The paper discusses using ChatGPT to score reasoning trajectories with rewards ranging from -100 to 100, acknowledges potential issues with ChatGPT's scoring reliability, and mentions using ReACT to improve score credibility, but also notes that autoregressive LLMs tend to assign extreme values and that the current data generation strategy leads to imbalanced reward distributions. This remains unresolved because the paper doesn't investigate how variations in ChatGPT's scoring accuracy or consistency would impact TPO's learning effectiveness, nor does it explore alternative scoring methods or evaluate the robustness of TPO to noisy reward signals.

### Open Question 3
Would alternative ToT strategies beyond the current step-by-step generation from correct trajectories produce preference trees with more diverse reward distributions? The paper acknowledges that the current data generation strategy (starting from correct trajectories and generating additional responses) leads to imbalanced reward distributions because subsequent steps become easily inferable once key steps are present, and suggests that introducing more effective ToT strategies like MCTS could ensure higher-quality data. This remains unresolved because the paper doesn't experimentally compare TPO's performance using preference trees generated by different ToT strategies, so it's unclear whether alternative generation methods would produce more challenging and diverse training examples that could further improve TPO's generalization.

## Limitations

- Data Quality Dependency: TPO's effectiveness heavily depends on the quality of preference trees generated through Tree-of-Thoughts, with poor reward calibration potentially significantly impacting performance
- Generalization Concerns: While showing better in-distribution performance, TPO demonstrates more catastrophic forgetting on out-of-distribution tasks like coding, suggesting potential overfitting
- Computational Overhead: Processing entire preference trees rather than sampled pairs could introduce significant computational overhead not fully analyzed in the paper

## Confidence

- High Confidence: The core technical contribution of TPO (formulating alignment as Preference List Ranking with Adaptive Step Reward) is well-defined and the experimental setup is clearly described
- Medium Confidence: The effectiveness of Adaptive Step Reward mechanism relies on semantic similarity metrics that may not capture all relevant task differences
- Low Confidence: The paper's claims about TPO's advantages for multi-branch reasoning are based on mathematical reasoning tasks, with uncertain generalization to other domains

## Next Checks

1. **Reward Calibration Analysis**: Perform ablation studies varying the quality and distribution of rewards in preference trees to determine how sensitive TPO is to reward quality versus quantity

2. **Computational Efficiency Benchmarking**: Compare wall-clock training time and memory usage of TPO versus DPO across different preference tree sizes to quantify the computational overhead trade-off

3. **Cross-Domain Generalization Test**: Evaluate TPO on non-mathematical reasoning tasks (e.g., commonsense reasoning, scientific reasoning) to determine if the multi-branch reasoning advantages extend beyond mathematical domains