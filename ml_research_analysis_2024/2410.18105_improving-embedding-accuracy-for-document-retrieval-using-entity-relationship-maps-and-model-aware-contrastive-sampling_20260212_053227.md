---
ver: rpa2
title: Improving Embedding Accuracy for Document Retrieval Using Entity Relationship
  Maps and Model-Aware Contrastive Sampling
arxiv_id: '2410.18105'
source_url: https://arxiv.org/abs/2410.18105
tags:
- training
- arxiv
- document
- factual
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: APEX-Embedding-7B achieves state-of-the-art retrieval accuracy
  by using structured entity relationship maps and model-aware contrastive sampling
  to bias factual content focus, improving rank@1 accuracy to 90.86% (6.26% above
  prior best) while reducing input size by 37.71%.
---

# Improving Embedding Accuracy for Document Retrieval Using Entity Relationship Maps and Model-Aware Contrastive Sampling

## Quick Facts
- arXiv ID: 2410.18105
- Source URL: https://arxiv.org/abs/2410.18105
- Authors: Thea Aviss
- Reference count: 24
- Primary result: Achieves 90.86% rank@1 accuracy on query/document retrieval

## Executive Summary
APEX-Embedding-7B introduces a novel approach to document retrieval by combining Structured Entity Relationship Maps with Model-Aware Contrastive Sampling. The method addresses the fundamental challenge in RAG systems where embeddings capture semantic style rather than factual content. By using entity relationship maps as structured training data and implementing a sophisticated contrastive sampling strategy that leverages the base model's embedding capabilities, the approach achieves state-of-the-art retrieval accuracy while reducing input size by 37.71%. The model demonstrates a 6.26% improvement over previous best results, reaching 90.86% rank@1 accuracy.

## Method Summary
The approach uses two key innovations: Structured Entity Relationship Maps and Model-Aware Contrastive Sampling. Structured Entity Relationship Maps transform plain text documents into structured representations of entities and their relationships using GPT-4o, creating focused training data that emphasizes factual content. Model-Aware Contrastive Sampling precomputes embeddings using the base model, then classifies negative samples into hard and soft categories based on cosine similarity scores, creating a balanced training set. The model is fine-tuned using 4-bit QLoRA with specific hyperparameters including learning rate of 2e-5, LoRA rank of 8, and alpha of 16, with training interrupted at pre-convergence to maintain factual content focus.

## Key Results
- Achieves rank@1 accuracy of 90.86% on query/document pair retrieval
- Improves accuracy by 6.26% compared to previous state-of-the-art
- Reduces training data input context size by 37.71% compared to plain text

## Why This Works (Mechanism)
The method works by shifting the model's attention bias from semantic style to factual content through structured data representation. Entity Relationship Maps provide a distilled version of documents that highlights factual relationships between entities, making it easier for the model to learn what's important for retrieval. The Model-Aware Contrastive Sampling creates more effective training pairs by using the base model's own embedding capabilities to identify meaningful hard and soft negatives, rather than relying on random or heuristic-based negative sampling. Pre-convergence interruption prevents the model from over-fitting to the structured format and maintains the factual content focus.

## Foundational Learning

**Structured Entity Relationship Maps**
- Why needed: Transforms unstructured text into focused representations highlighting factual content
- Quick check: Verify generated maps capture key entities and relationships from source documents

**Model-Aware Contrastive Sampling**
- Why needed: Creates more effective training pairs by leveraging base model's embedding capabilities
- Quick check: Confirm hard and soft negative classifications based on similarity thresholds

**4-bit QLoRA Fine-tuning**
- Why needed: Enables efficient fine-tuning of large models while preserving performance
- Quick check: Monitor memory usage and training stability during fine-tuning

## Architecture Onboarding

**Component Map**
GPT-4o -> Entity Relationship Map Generator -> Structured Training Data -> Model-Aware Contrastive Sampler -> Training Pairs -> Mistral-7B (4-bit QLoRA) -> Fine-tuned APEX-Embedding-7B

**Critical Path**
Structured Entity Relationship Map generation → Model-Aware Contrastive Sampling → Fine-tuning with pre-convergence interruption

**Design Tradeoffs**
- Structured data reduces input size but may lose contextual nuance
- Model-aware sampling improves quality but requires additional computation
- Pre-convergence interruption preserves factual focus but may limit maximum performance

**Failure Signatures**
- Overfitting to structured format (monitor plain text performance)
- Insufficient negative sample diversity (check hard/soft negative balance)
- Loss of semantic understanding (evaluate on semantic similarity tasks)

**3 First Experiments**
1. Generate entity relationship maps from sample documents and verify factual content extraction
2. Implement contrastive sampling with base model embeddings and test negative classification
3. Run small-scale fine-tuning with pre-convergence stopping and measure accuracy trends

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How does the pre-convergence interruption point specifically affect the model's factual content prioritization bias, and is there an optimal stopping point that maximizes this effect?
- Basis in paper: The paper mentions that pre-convergence interrupted fine-tuning is used to shift the model's attention bias toward factual content, but it does not specify the optimal stopping point or the exact impact of the interruption timing.
- Why unresolved: The paper does not provide detailed analysis or experimentation on how different interruption points affect the model's performance or the nature of the bias introduced.
- What evidence would resolve it: Experimental results comparing models fine-tuned with different interruption points, showing the impact on factual accuracy and plain text performance.

**Open Question 2**
- Question: Can the Model-Aware Contrastive Sampling methodology be generalized to other types of tasks beyond document retrieval, such as image or audio retrieval?
- Basis in paper: The paper focuses on document retrieval but does not explore the applicability of the Model-Aware Contrastive Sampling methodology to other data types.
- Why unresolved: The methodology is presented in the context of text embeddings, and there is no discussion of its potential adaptation to non-textual data.
- What evidence would resolve it: Successful application and evaluation of the methodology on image or audio retrieval tasks, demonstrating its effectiveness across different data modalities.

**Open Question 3**
- Question: How does the reduction in training data input context size by 37.71% impact the model's performance on extremely long documents or documents with complex structures?
- Basis in paper: The paper states that the model reduces training data input context size by an average of 37.71% compared to plain text, but it does not discuss the impact on performance with very long or complex documents.
- Why unresolved: The evaluation does not include tests with documents that exceed the typical context length or have intricate structures, leaving the model's scalability untested.
- What evidence would resolve it: Performance metrics on a dataset with significantly longer or more complex documents, showing how the model handles increased context and structural complexity.

## Limitations

**Unknown Implementation Details**
- Exact GPT-4o prompt for generating synthetic queries is not provided
- Specific threshold values for classifying hard versus soft negatives are unspecified

**Statistical Uncertainty**
- No confidence intervals or standard deviations reported for the 90.86% accuracy claim
- Unclear whether evaluation was performed on held-out test set versus validation set

**Generalizability Concerns**
- Results may not transfer to other model architectures beyond Mistral-7B
- Effectiveness on extremely long or complex documents is untested

## Confidence

**High Confidence**: The core methodology of using Structured Entity Relationship Maps as training data and Model-Aware Contrastive Sampling for negative selection is clearly specified and technically sound. The QLoRA fine-tuning setup with detailed hyperparameters is fully reproducible.

**Medium Confidence**: The claim of 90.86% rank@1 accuracy is reported with specific numbers, but without confidence intervals or standard deviations, making the statistical significance uncertain. The comparison to previous methods showing 6.26% improvement is specific but lacks error margins.

**Low Confidence**: The generalizability of results to other domains or larger models is unclear since only Mistral-7B was tested. The impact of the unspecified prompt and threshold values on final performance cannot be assessed.

## Next Checks

1. **Baseline Reproduction**: Implement the APEX-Embedding-7B training pipeline using the specified hyperparameters and a simplified prompt for generating Structured Entity Relationship Maps, then compare rank@1 accuracy on the same dataset to validate the core methodology.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary the learning rate (e.g., 1e-5, 2e-5, 5e-5) and LoRA rank (e.g., 4, 8, 16) to determine their impact on final accuracy and identify the optimal configuration.

3. **Statistical Validation**: Run the complete training and evaluation pipeline three times with different random seeds, reporting mean rank@1 accuracy with standard deviation to establish statistical significance of the claimed 90.86% performance.