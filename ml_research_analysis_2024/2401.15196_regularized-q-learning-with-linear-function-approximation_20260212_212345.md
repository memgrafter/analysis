---
ver: rpa2
title: Regularized Q-Learning with Linear Function Approximation
arxiv_id: '2401.15196'
source_url: https://arxiv.org/abs/2401.15196
tags:
- function
- lemma
- regularized
- where
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a single-loop algorithm for regularized Q-learning
  with linear function approximation that achieves finite-time convergence guarantees.
  The algorithm operates on two time-scales with different step sizes for updating
  the main solution (approximating Bellman's optimality condition) and target solution
  (projection onto basis vectors).
---

# Regularized Q-Learning with Linear Function Approximation

## Quick Facts
- arXiv ID: 2401.15196
- Source URL: https://arxiv.org/abs/2401.15196
- Reference count: 40
- Key outcome: Single-loop algorithm achieves finite-time convergence to stationary point at rate O(T^{-1/4}(log T)^{1/2}) under linear function approximation

## Executive Summary
This paper addresses the challenge of regularized Q-learning with linear function approximation by proposing a single-loop algorithm that achieves finite-time convergence guarantees. The key innovation is operating on two time-scales with different step sizes for updating the main solution (approximating Bellman's optimality condition) and target solution (projection onto basis vectors). This approach resolves the fundamental issue that the composition of regularized Bellman operator and projection is not a contraction with respect to any norm, which can cause divergence in standard Q-learning.

## Method Summary
The algorithm uses a bilevel optimization formulation where the lower level identifies a value function approximation satisfying Bellman's recursive optimality condition, and the upper level finds the projection of this solution onto the span of basis vectors. A key technical contribution is the smooth truncation operator Kδ, which bounds the convex conjugate of the regularizer to ensure bounded iterates while preserving differentiability for gradient-based updates. The algorithm operates on two time-scales with slower projection steps to stabilize convergence, using stochastic gradient updates for both levels.

## Key Results
- Achieves O(T^{-1/4}(log T)^{1/2}) convergence rate to stationary point in ℓ2 norm
- Provides performance guarantees for derived policies under linear function approximation
- Demonstrates that two-timescale updates prevent divergence caused by non-contraction of ΠBτ
- Introduces smooth truncation operator Kδ to stabilize Bellman operator updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-timescale update with slower projection steps stabilizes convergence
- Mechanism: Updates to target network (θ) use smaller step size than main network (ω), smoothing the non-contracting projection step
- Core assumption: α ≪ β (step sizes sufficiently separated)
- Evidence anchors: Abstract and Section 4 describe the two-timescale approach
- Break condition: If α and β are not sufficiently separated, algorithm may diverge

### Mechanism 2
- Claim: Smooth truncation operator Kδ stabilizes Bellman operator and ensures bounded iterates
- Mechanism: Bounds convex conjugate of regularizer, preventing large value estimates from destabilizing learning while preserving differentiability
- Core assumption: Threshold δ chosen large enough to avoid bias but small enough to prevent overestimation
- Evidence anchors: Section 3.1 defines Kδ, Section 6 discusses its role in bounding values
- Break condition: If δ too small, truncation bias dominates; if too large, variance increases

### Mechanism 3
- Claim: Bilevel optimization decouples Bellman error minimization from projection step
- Mechanism: Lower level finds projection of Bellman backup onto linear basis, upper level minimizes mean squared projected Bellman error
- Core assumption: Lower-level objective strongly convex in ω for any fixed θ
- Evidence anchors: Section 3.2 defines g(θ, ω), Proposition 3.1 establishes strong convexity
- Break condition: If strong convexity constant λg too small, lower-level problem becomes ill-conditioned

## Foundational Learning

- Concept: Strongly convex functions and their role in optimization stability
  - Why needed here: Ensures unique minimizer and provides lower bound on eigenvalue of Σ, critical for convergence rates
  - Quick check question: Why does strong convexity of g(θ, ω) in ω imply that ω*(θ) is Lipschitz in θ?

- Concept: Contraction mappings and their failure under composition
  - Why needed here: Explains why ΠBτ is not a contraction, causing divergence in naive Q-learning
  - Quick check question: What property of projection operator Π fails to make ΠBτ a contraction?

- Concept: Two-timescale stochastic approximation
  - Why needed here: Convergence relies on classic two-timescale framework where one update is much slower
  - Quick check question: What condition on step sizes α and β ensures fast update tracks slow one?

## Architecture Onboarding

- Component map: Behavioral policy πbhv -> MDP transition (s, a, s') -> Lower-level update ht g -> Main network ωt+1 -> Upper-level update ht f -> Target network θt+1

- Critical path:
  1. Sample transition (s, a, s′) from πbhv
  2. Compute stochastic gradient for lower-level update ht g
  3. Update main network: ωt+1 = P[ωt − βht g]
  4. Compute stochastic surrogate gradient for upper-level update ht f
  5. Update target network: θt+1 = θt − α‖θt−ωt+1‖⁻¹ ht f (if θt ≠ ωt+1)
  6. Repeat

- Design tradeoffs:
  - Step size separation (α ≪ β) vs. convergence speed
  - Threshold δ vs. truncation bias and variance
  - Basis function choice vs. approximation error Eapprox
  - Regularizer choice (e.g., entropy vs. Tsallis) vs. policy sparsity/robustness

- Failure signatures:
  - Divergence: If α and β not sufficiently separated
  - High variance: If δ too large
  - Slow convergence: If λg too small or δ too small
  - Poor performance: If basis functions inadequate (high Eapprox)

- First 3 experiments:
  1. GridWorld with varying δ: Verify convergence speed vs. truncation bias trade-off
  2. MountainCar-v0 with different regularizers: Compare entropy vs. Tsallis regularization on policy performance
  3. Sensitivity to step size ratio: Test convergence with α/β = 0.1, 0.01, 0.001 to find optimal separation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between smooth truncation threshold δ and convergence rate?
- Basis in paper: Paper discusses tradeoff between larger δ reducing bias but increasing variance
- Why unresolved: Notes tradeoff but lacks detailed analysis of optimal δ scaling with problem parameters
- What evidence would resolve it: Theoretical analysis showing optimal δ as function of problem parameters with empirical validation

### Open Question 2
- Question: Can convergence rate be improved beyond O(T^{-1/4}(log T)^{1/2})?
- Basis in paper: Achieves O(T^{-1/4}(log T)^{1/2}) but notes this is constrained by lower-level problem's convergence rate
- Why unresolved: Doesn't explore whether alternative designs or parameter choices could yield faster convergence
- What evidence would resolve it: Theoretical analysis showing tighter bounds under specific conditions or empirical results with modified algorithms

### Open Question 3
- Question: How does choice of regularizer G affect convergence properties and performance?
- Basis in paper: Mentions regularizer belongs to general class including negative Shannon and Tsallis entropy
- Why unresolved: Lacks detailed analysis of how different regularizers impact convergence rates or policy quality
- What evidence would resolve it: Theoretical analysis comparing convergence rates for different regularizers with empirical evaluation

## Limitations
- Convergence rate of O(T^{-1/4}(log T)^{1/2}) is relatively slow compared to finite-state Q-learning
- Analysis requires restrictive assumptions including bounded state-action values and strongly convex regularizers
- Algorithm depends on careful tuning of step size ratio and truncation threshold, which may be challenging in practice

## Confidence
- High confidence: Theoretical framework and proof techniques are sound, building on established two-timescale stochastic approximation results
- Medium confidence: Convergence rate analysis relies on several boundedness assumptions that may not hold in all practical scenarios
- Medium confidence: Performance guarantees depend on quality of linear function approximation and choice of regularizer

## Next Checks
1. Empirical validation of step size sensitivity: Systematically test convergence behavior across different α/β ratios (0.1, 0.01, 0.001) to verify theoretical requirement that α ≪ β
2. Truncation threshold ablation: Evaluate policy performance and convergence speed across different δ values to quantify trade-off between truncation bias and variance
3. Comparison with TD-based methods: Benchmark proposed algorithm against state-of-the-art regularized TD learning methods on standard control tasks to assess practical performance gains