---
ver: rpa2
title: 'PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs'
arxiv_id: '2402.12835'
source_url: https://arxiv.org/abs/2402.12835
tags:
- panda
- expert
- llms
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PANDA, a tuning-free method to enhance the
  domain-specific abilities of large language models (LLMs) by learning from expert
  models. PANDA extracts expert preferences from a domain expert model, uses the LLM
  to generate explanations for these preferences to form an "insight pool," and then
  retrieves relevant insights during inference to adapt the LLM's preferences.
---

# PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs

## Quick Facts
- arXiv ID: 2402.12835
- Source URL: https://arxiv.org/abs/2402.12835
- Reference count: 18
- Key outcome: Tuning-free method using expert preference explanations improves LLM performance across 20 ScienceWorld and 7 TweetEval tasks, with LLM outperforming expert model in 4 tasks

## Executive Summary
PANDA is a tuning-free approach that enhances domain-specific abilities of large language models by learning from expert models through preference explanations. The method extracts expert preferences, prompts LLMs to generate explanations for these preferences to form an "insight pool," and retrieves relevant insights during inference to adapt the LLM's preferences. Experiments show PANDA significantly improves performance across text classification and interactive decision tasks, demonstrating the potential for weak-to-strong generalization without fine-tuning.

## Method Summary
PANDA extracts expert preferences from a domain expert model, prompts an LLM (gpt-3.5-turbo) to generate explanations for these preferences to create an insight pool, and retrieves relevant insights during inference to guide the LLM's responses through in-context learning. The method involves two stages: learning stage where insights are generated and stored, and inference stage where relevant insights are retrieved and used to adapt LLM preferences for specific queries. The approach leverages the LLM's language understanding to verbalize expert preferences and uses retrieval-augmented generation to provide context during inference.

## Key Results
- PANDA significantly improves LLM performance across 20 tasks in ScienceWorld and 7 tasks in TweetEval
- The LLM with PANDA outperforms the expert model in 4 ScienceWorld tasks, demonstrating weak-to-strong generalization
- PANDA achieves consistent improvement by leveraging summary insights from expert preferences compared to raw prediction results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PANDA leverages LLMs' language understanding to generate explanations ("insights") for expert model preferences, enabling preference alignment without fine-tuning.
- Mechanism: The LLM is prompted with preference pairs (A > B) and the query context, then generates an explanation for why the expert prefers A. These insights are stored in an "insight pool" and retrieved during inference to guide the LLM's responses.
- Core assumption: LLMs can accurately interpret and verbalize the rationale behind expert model preferences when given appropriate prompts.
- Evidence anchors:
  - [abstract]: "PANDA extracts expert preferences from a domain expert model, uses the LLM to generate explanations for these preferences to form an 'insight pool,' and then retrieves relevant insights during inference to adapt the LLM's preferences."
  - [section 2.2]: "Leveraging the powerful language understanding and generation ability, we prompt LLMs to generate the explanation of the preferences of the expert, which is a process of learning expert knowledge."
- Break condition: If the LLM cannot generate coherent or accurate explanations for preferences, the insight pool becomes ineffective.

### Mechanism 2
- Claim: PANDA improves performance by aligning LLM preferences with expert model preferences through in-context learning during inference.
- Mechanism: During inference, PANDA retrieves relevant insights from the insight pool using the current query context as the retrieval key. The LLM is then prompted with these insights to adapt its preference ranking for the current query.
- Core assumption: In-context learning with retrieved insights can effectively shift the LLM's preference ranking to match that of the expert model.
- Evidence anchors:
  - [abstract]: "PANDA retrieves the most relevant insights from the insight pool to assist in completing the current query, followed by standard inference using the retrieved insights."
  - [section 2.3]: "PANDA firstly retrieves related insights from the insight pool using the current query context as the retrieval key... we then incorporate the retrieved insights as part of the prompt."
- Break condition: If retrieved insights are irrelevant or misleading, the LLM's performance may degrade rather than improve.

### Mechanism 3
- Claim: PANDA achieves weak-to-strong generalization by learning generalizable knowledge from expert preferences rather than just mimicking expert behavior.
- Mechanism: By prompting the LLM to explain why the expert prefers A over B, PANDA encourages the LLM to learn the underlying principles or patterns that guide expert preferences, rather than simply memorizing preference pairs.
- Core assumption: Explanations for preferences capture generalizable knowledge that can be applied to novel queries, not just the specific training instances.
- Evidence anchors:
  - [abstract]: "LLM with PANDA even outperforms the expert model that being learned on 4 tasks of ScienceWorld, demonstrating the potential of tuning-free approaches to achieve weak-to-strong generalization."
  - [section 4.3]: "PANDA achieves consistent improvement by leveraging summary insights from the preferences of the expert. These insights better represent the understanding of the expert for the specific task compared to raw prediction results."
- Break condition: If explanations only capture surface-level patterns specific to training instances, generalization to novel queries will be limited.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: PANDA uses RAG to retrieve relevant insights from the insight pool during inference, which is crucial for guiding the LLM's responses.
  - Quick check question: How does PANDA differ from standard RAG approaches in terms of the retrieval content (insights vs. examples)?

- Concept: In-context learning
  - Why needed here: PANDA relies on the LLM's ability to adapt its behavior based on the context provided by retrieved insights during inference.
  - Quick check question: What are the limitations of in-context learning that could impact PANDA's performance?

- Concept: Preference learning
  - Why needed here: PANDA learns from expert model preferences (A > B pairs) to generate insights and align LLM preferences with expert preferences.
  - Quick check question: How does learning from preferences differ from learning from expert behavior or labels?

## Architecture Onboarding

- Component map:
  Expert model -> Preference pairs -> LLM (PANDA-Learning) -> Insight pool <- Retrieval mechanism <- LLM (PANDA-Inference)

- Critical path:
  1. Expert model generates preference pairs and response samples on training data
  2. LLM generates insights from preference pairs using PANDA-Learning prompt
  3. Insights are stored in the insight pool
  4. During inference, retrieval mechanism finds relevant insights using current query context
  5. LLM adapts preferences using retrieved insights via PANDA-Inference prompt

- Design tradeoffs:
  - Using a pre-trained LLM vs. fine-tuning: PANDA is tuning-free but relies on the LLM's existing capabilities
  - Number of preference pairs per query: More pairs provide more comprehensive insights but increase computation
  - Number of retrieved insights: More insights provide better guidance but increase prompt length and computation

- Failure signatures:
  - Poor retrieval quality: Irrelevant or misleading insights are retrieved, leading to degraded performance
  - Inadequate insight generation: LLM fails to generate coherent or accurate explanations for preferences
  - Limited generalization: Insights only capture surface-level patterns specific to training instances

- First 3 experiments:
  1. Validate insight generation: Test LLM's ability to generate coherent explanations for preference pairs on a small dataset
  2. Validate retrieval quality: Test retrieval mechanism's ability to find relevant insights for a set of queries
  3. Validate preference adaptation: Test LLM's ability to adapt preferences using retrieved insights on a small task

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications arise from the research:

1. What is the theoretical upper limit of performance improvement achievable through PANDA compared to fine-tuning methods, particularly for tasks where the expert model is significantly weaker than the LLM?

2. How does PANDA's performance scale with the size and diversity of the insight pool, and what is the optimal balance between insight quantity and quality?

3. What are the computational and memory requirements of PANDA compared to traditional fine-tuning approaches, and how does this impact its practical applicability for real-time or resource-constrained scenarios?

## Limitations

- Retrieval Quality Dependence: PANDA's performance heavily relies on the quality of insights retrieved from the insight pool, with no detailed analysis of retrieval failure modes.
- Scalability Concerns: The paper does not address scalability challenges such as insight pool maintenance, retrieval efficiency for large-scale applications, or performance degradation with increased task complexity.
- Insight Generation Quality: The effectiveness depends on the LLM's ability to generate coherent and accurate explanations for expert preferences, which is not validated for complex domains.

## Confidence

- High Confidence: The mechanism by which PANDA retrieves and uses insights to guide LLM responses during inference is well-documented and reproducible.
- Medium Confidence: The claim that PANDA achieves weak-to-strong generalization is supported by results but lacks systematic analysis of what makes tasks amenable to this improvement.
- Low Confidence: The paper does not adequately address potential failure modes where PANDA could degrade performance with misleading insights.

## Next Checks

1. Conduct a retrieval ablation study to systematically vary the number and quality of retrieved insights and determine the relationship between retrieval quality and performance improvement.

2. Perform human evaluations of the insights generated by the LLM to assess whether they capture meaningful domain knowledge versus superficial patterns.

3. Design experiments that intentionally provide misleading or irrelevant insights to the LLM during inference to measure performance degradation and establish PANDA's robustness.