---
ver: rpa2
title: Return Augmented Decision Transformer for Off-Dynamics Reinforcement Learning
arxiv_id: '2410.23450'
source_url: https://arxiv.org/abs/2410.23450
tags:
- dataset
- target
- source
- medium
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of offline off-dynamics reinforcement
  learning, where policies must be learned from source domain data to perform well
  in a target domain with different dynamics. The proposed method, Return Augmented
  Decision Transformer (RADT), enhances the standard Decision Transformer (DT) by
  augmenting returns in the source domain to better align with the target domain's
  return distribution.
---

# Return Augmented Decision Transformer for Off-Dynamics Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.23450
- Source URL: https://arxiv.org/abs/2410.23450
- Authors: Ruhan Wang; Yu Yang; Zhishuai Liu; Dongruo Zhou; Pan Xu
- Reference count: 4
- Key outcome: RADT achieves same suboptimality as target-only learning while leveraging source data

## Executive Summary
This paper addresses offline off-dynamics reinforcement learning, where policies must be learned from source domain data to perform well in a target domain with different dynamics. The proposed Return Augmented Decision Transformer (RADT) enhances standard Decision Transformer by augmenting returns in the source domain to better align with target domain's return distribution. Two practical implementations are introduced: RADT-DARA using reward augmentation via dynamic-aware reward augmentation, and RADT-MV using direct mean-variance matching of return distributions. Theoretical analysis shows RADT achieves the same suboptimality level as policies learned directly in the target domain under standard data coverage assumptions. Extensive experiments on D4RL datasets demonstrate RADT outperforms baseline algorithms in off-dynamics settings, particularly under BodyMass and JointNoise distribution shifts.

## Method Summary
RADT enhances Decision Transformer for off-dynamics RL by augmenting returns from source domain data to match target domain return distributions. RADT-DARA uses reward augmentation based on learned binary classifiers that estimate the log-likelihood ratio of transition dynamics between domains. RADT-MV directly matches mean and variance of return distributions under Gaussian approximation, using CQL for value function estimation. The method theoretically achieves the same suboptimality as target-only learning when source data is sufficiently large and covers target domain states.

## Key Results
- RADT outperforms baseline algorithms in off-dynamics settings across Walker2D, Hopper, and HalfCheetah environments
- RADT achieves same suboptimality level as policies learned directly in target domain under standard data coverage assumptions
- RADT-DARA and RADT-MV both show significant improvements over source-only and target-only baselines
- Performance gains are particularly pronounced under BodyMass and JointNoise distribution shifts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Return augmentation in RADT-DARA aligns trajectory distributions between source and target domains, reducing the dynamics shift.
- Mechanism: The method estimates the log-likelihood ratio of transition dynamics between source and target environments using learned binary classifiers. This ratio is then added to the reward, creating augmented returns that make source domain trajectories resemble target domain optimal trajectories.
- Core assumption: The learned binary classifiers can accurately estimate the KL divergence between transition dynamics of source and target environments.
- Evidence anchors:
  - [section 4.2]: "We use a pair of learned binary classifiers which infers whether the transitions come from the source or target environments."
  - [abstract]: "Previous works tackle the dynamics shift problem by augmenting the reward in the trajectory from the source domain to match the optimal trajectory in the target domain."
- Break condition: If the classifier cannot accurately distinguish between source and target transitions, the augmentation becomes ineffective.

### Mechanism 2
- Claim: RADT-MV achieves return distribution alignment by matching mean and variance of return distributions.
- Mechanism: Under Gaussian approximation of return distributions, RADT-MV transforms source returns by subtracting source mean, scaling by the ratio of standard deviations, and adding target mean. This preserves return-conditioned policy structure while aligning distributions.
- Core assumption: Return distributions in both domains can be well-approximated by Gaussian distributions conditioned on state-action pairs.
- Evidence anchors:
  - [section 4.3]: "We use Laplace approximation to approximate both GT β and GS β by Gaussian distributions"
  - [abstract]: "RADT-MV, which directly matches the mean and variance of return distributions"
- Break condition: If return distributions are highly non-Gaussian or have multi-modal structure, the Gaussian approximation fails.

### Mechanism 3
- Claim: Theoretical analysis shows RADT achieves the same suboptimality level as policies learned directly in target domain.
- Mechanism: The sample complexity analysis demonstrates that with sufficient source data and appropriate return augmentation, the regret bound matches what would be achieved with target-only data.
- Core assumption: Standard data coverage assumptions hold for both source and target datasets.
- Evidence anchors:
  - [abstract]: "Theoretical analysis shows that RADT achieves the same suboptimality level as policies learned directly in the target domain under standard data coverage assumptions."
  - [section 4.4]: "Theorem 4.5... J T (π⋆) − J T (ˆπf) = O(1/(N T + N S)1/4)"
- Break condition: If data coverage assumptions are violated, the theoretical guarantees no longer apply.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: The entire problem formulation and theoretical analysis relies on MDP framework with states, actions, rewards, and transitions.
  - Quick check question: What are the key components of an MDP and how do they relate to the off-dynamics RL problem?

- Concept: Return-conditioned supervised learning (RCSL)
  - Why needed here: DT and RADT are based on RCSL framework, which conditions policies on desired returns rather than value functions.
  - Quick check question: How does RCSL differ from traditional RL approaches like Q-learning?

- Concept: Dynamic programming vs. supervised learning in RL
  - Why needed here: The paper contrasts RADT (supervised learning approach) with DARA (dynamic programming approach) and explains why traditional reward augmentation doesn't directly apply to DT.
  - Quick check question: What are the key differences between dynamic programming-based RL algorithms and supervised learning-based approaches like DT?

## Architecture Onboarding

- Component map: Source dataset (DS) -> Augmentation module (RADT-DARA or RADT-MV) -> Augmented dataset (D) -> Decision Transformer -> Policy for target environment

- Critical path:
  1. Collect source and target datasets
  2. Apply return augmentation (RADT-DARA or RADT-MV)
  3. Train Decision Transformer on augmented dataset
  4. Evaluate policy in target environment

- Design tradeoffs:
  - RADT-DARA requires training binary classifiers but aligns trajectories
  - RADT-MV requires value function estimation but is simpler to implement
  - Tradeoff between theoretical guarantees and practical performance

- Failure signatures:
  - Poor performance despite augmentation → classifier estimation error or Gaussian approximation failure
  - High variance in results → clipping parameters need adjustment
  - Degradation on source-only data → augmentation is too aggressive

- First 3 experiments:
  1. Baseline: Train DT on target-only dataset (1T)
  2. Compare: Train DT on source-only dataset (10S)
  3. Test: Train DT on mixed dataset without augmentation (1T10S)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the clipping technique ratio in RADT-MV affect its performance across different environments and shift types?
- Basis in paper: [explicit] Section 5.3 and Figure 6 mention clipping technique with ratios θ1 and θ2, but do not systematically vary these ratios to determine optimal values
- Why unresolved: The paper uses fixed clipping ratios (0.9<θ<1.25 for Walker2D, 0.9<θ<1 for Hopper, 0.67<θ<1.5 for HalfCheetah) without exploring the sensitivity of performance to these parameters
- What evidence would resolve it: Systematic experiments varying clipping ratios across environments and shift types to identify optimal ranges and understand trade-offs

### Open Question 2
- Question: What is the theoretical relationship between the source dataset size and the effectiveness of return augmentation in RADT?
- Basis in paper: [inferred] Theorem 4.5 suggests improved performance when NS >> NT, but does not quantify the exact relationship or identify diminishing returns
- Why unresolved: While the paper shows theoretical improvement, it does not provide quantitative bounds on how much larger NS needs to be relative to NT for return augmentation to be beneficial
- What evidence would resolve it: Theoretical analysis providing explicit bounds on the ratio NS/NT required for effective return augmentation, validated through empirical studies

### Open Question 3
- Question: How does RADT perform when the source and target domains have non-overlapping state-action distributions?
- Basis in paper: [explicit] Assumption 4.3 requires domain occupancy overlap (dTβ(s) ≤ γf dSβ(s)), but the paper does not test scenarios violating this assumption
- Why unresolved: The theoretical guarantees rely on this assumption, yet real-world scenarios often involve domains with minimal overlap
- What evidence would resolve it: Experiments testing RADT performance in scenarios with varying degrees of state-action distribution overlap, including cases with no overlap, to understand failure modes and potential remedies

## Limitations
- Theoretical analysis relies on strong assumptions about data coverage and classifier accuracy that may not hold in practice
- Gaussian approximation for return distributions in RADT-MV may fail for non-Gaussian or multi-modal distributions
- Paper does not extensively explore sensitivity of performance to source-to-target data size ratio

## Confidence
- High confidence: Empirical results showing RADT outperforming baseline methods on D4RL datasets
- Medium confidence: Theoretical analysis showing suboptimality bounds matching target-only learning given strong assumptions
- Low confidence: Effectiveness of RADT-DARA's binary classifier approach in complex, high-dimensional environments

## Next Checks
1. **Ablation study on source data quantity**: Systematically vary the ratio of source to target data (e.g., 5S:1T, 20S:1T) to identify the optimal balance and understand performance degradation when source data is limited

2. **Cross-environment generalization test**: Evaluate RADT performance when source and target domains are from different but related environments (e.g., Walker2d to HalfCheetah) to assess robustness to distribution shifts

3. **Binary classifier accuracy analysis**: Measure the accuracy of the learned classifiers in RADT-DARA for distinguishing between source and target transitions, and correlate classifier performance with policy performance to validate the key assumption of this approach