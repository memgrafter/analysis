---
ver: rpa2
title: Steering Large Language Models between Code Execution and Textual Reasoning
arxiv_id: '2410.03524'
source_url: https://arxiv.org/abs/2410.03524
tags:
- code
- task
- text
- arxiv
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to effectively steer large language
  models (LLMs) between code execution and textual reasoning across diverse tasks.
  It reveals that current methods struggle to optimally choose between the two modes,
  with no single best approach.
---

# Steering Large Language Models between Code Execution and Textual Reasoning

## Quick Facts
- arXiv ID: 2410.03524
- Source URL: https://arxiv.org/abs/2410.03524
- Reference count: 40
- Primary result: No single best method exists for steering LLMs between code execution and textual reasoning; hybrid approaches (Code+Text+Sum, multi-turn refinement) achieve the best average normalized scores but incur higher token and runtime costs.

## Executive Summary
This paper investigates how to effectively steer large language models (LLMs) between code execution and textual reasoning across diverse tasks. It reveals that current methods struggle to optimally choose between the two modes, with no single best approach. Surprisingly, smaller models sometimes outperform larger ones (inverse scaling behavior), and forcing code generation does not always improve accuracy. The authors propose methods like assembling code and text responses, and multi-turn refinement, which notably improve performance. Experimental results on 14 tasks with 6 LLM types show that combining code and textual reasoning achieves the best average normalized scores. However, this and other proposed methods incur higher token and runtime costs. The study highlights the complexity of steering LLMs for code vs. text generation and points to significant room for further improvement.

## Method Summary
The paper evaluates 10 methods for steering code/text generation in LLMs, including baseline methods (Only Question, All Text, All Code, All Code + CoT) and proposed methods (AutoGen Conca., AutoGen System, Code Interpreter, Code Interpreter+, Code + Text + Sum., Self-estimate Score). Experiments are conducted on 14 tasks using 6 types of LLMs. The primary metric is Average Normalized Score, with additional analysis of token lengths and runtime. The Code Interpreter+ method prompts the Code Interpreter the same way as in the All Code method, and the proposed methods aim to improve steering by assembling code and text responses or using multi-turn refinement.

## Key Results
- No single method consistently outperforms others in steering LLMs between code execution and textual reasoning.
- Smaller models (e.g., GPT-3.5) sometimes outperform larger ones (e.g., GPT-4o) when augmented with Code Interpreter, demonstrating inverse scaling behavior.
- Forcing LLMs to always use code does not guarantee better accuracy than textual reasoning.
- Hybrid methods (Code + Text + Sum., multi-turn refinement) achieve the best average normalized scores but consume more tokens and runtime.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code Interpreter (CI) usage varies with task complexity, leading to a non-monotonic accuracy pattern.
- Mechanism: CI decides whether to use code or textual reasoning based on its internal confidence estimate. For simple tasks, it uses text; for very complex tasks, it uses code; for medium complexity, it sometimes chooses text when code would be better, leading to failures.
- Core assumption: CI’s confidence estimate correlates with task difficulty, but is miscalibrated for medium-difficulty tasks.
- Evidence anchors:
  - [abstract] "GPT-4o CI tends to handle simple Number Multiplying tasks with text and complex tasks with code. However, it often fails with medium-difficulty questions, where it is overconfident and chooses not to use code when needed."
  - [section] "GPT-4o answers all multiplication questions correctly using text for small numbers (left side) and generates code for very large numbers (right side). Errors occur when the numbers are neither too large nor too small, causing GPT-4o to struggle with deciding whether to use code."
- Break condition: If CI’s confidence calibration is improved or it is forced to always use code for medium-difficulty tasks, the failure pattern may disappear.

### Mechanism 2
- Claim: Smaller CI-enabled models sometimes outperform larger ones (inverse scaling behavior) due to differing reliance on textual reasoning.
- Mechanism: Larger models (e.g., GPT-4o) have higher confidence in their textual reasoning abilities and are more likely to use text even when code is better, whereas smaller models (e.g., GPT-3.5) are more conservative and default to code more often in complex tasks.
- Core assumption: Model size correlates with overconfidence in textual reasoning, not just absolute capability.
- Evidence anchors:
  - [abstract] "GPT-4o-mini and GPT-3.5 achieve 100% success rates across all the task complexity... Compared to GPT-4o, GPT-4o-mini and GPT-3.5 are more conservative so that they generate code all the time when encountering slightly complex questions."
  - [section] "GPT-4o tends to rely more on its textual reasoning abilities, often becoming overconfident and opting not to use code, even when coding would be more effective."
- Break condition: If larger models are retrained or prompted to default to code in ambiguous cases, the inverse scaling effect may reverse.

### Mechanism 3
- Claim: Simply prompting LLMs to "always answer with code" does not guarantee better accuracy than textual reasoning.
- Mechanism: Code output is not always functionally correct; it can resemble textual reasoning without efficient computation, and the strict format can limit token space and reasoning diversity.
- Core assumption: Code generation requires both syntactic correctness and semantic efficiency; LLMs may generate code that is syntactically valid but semantically equivalent to text-based reasoning.
- Evidence anchors:
  - [abstract] "We also discover that results from LLM written code are not always better than using textual reasoning, even if the task could be solved through code."
  - [section] "In Game 24, prompting LLMs to answer with code often leads to code versions similar to textual reasoning, lacking the efficiency of true code execution."
  - [section] "The coding format will limit the space of generated tokens so that the reasoning ability is undermined."
- Break condition: If code generation is coupled with execution and validation (as in CI), or if the task is structured to require true computation, the pattern may shift.

## Foundational Learning

- Concept: Confidence calibration in decision-making under uncertainty.
  - Why needed here: CI models must decide between code and text; miscalibration leads to suboptimal choices.
  - Quick check question: If a model is 90% confident in its answer, but is wrong 40% of the time, is it well-calibrated? (No.)

- Concept: Inverse scaling and the limits of scaling laws.
  - Why needed here: Demonstrates that scaling model size does not always improve performance; architecture and prompting matter.
  - Quick check question: If a smaller model outperforms a larger one on a task, what could explain this besides random chance? (e.g., overconfidence in the larger model.)

- Concept: The distinction between syntactic code generation and semantic code execution.
  - Why needed here: Code that looks valid may not compute correctly; functional correctness requires execution and validation.
  - Quick check question: Can a model generate code that passes syntax checks but fails at runtime? (Yes.)

## Architecture Onboarding

- Component map: LLM core -> Code Interpreter (optional) -> Execution engine -> Result parser -> Multi-turn refinement loop (optional) -> Aggregator (for Code+Text+Sum)
- Critical path: Prompt -> LLM generation -> (optional: Code Interpreter) -> (optional: Code execution) -> Answer extraction -> Evaluation
- Design tradeoffs:
  - Code Interpreter vs. direct code generation: CI adds execution overhead but ensures functional correctness; direct code is faster but riskier.
  - Single-turn vs. multi-turn: Multi-turn allows refinement but increases latency and cost.
  - Code-only vs. hybrid: Code is more precise for computation; text is better for semantic reasoning.
- Failure signatures:
  - CI fails on medium-difficulty tasks -> overconfidence in textual reasoning.
  - Forcing code leads to worse accuracy -> code output resembles text or is inefficient.
  - Larger models underperform smaller ones -> inverse scaling due to overconfidence.
- First 3 experiments:
  1. Compare accuracy of GPT-4o vs. GPT-3.5 on a task with varying difficulty, with and without CI.
  2. Measure code vs. text usage ratios across task complexities for a given model.
  3. Evaluate accuracy when prompting "always answer with code" vs. CI-enabled generation on a mixed task set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does prompting LLMs to always respond with code sometimes lead to worse overall accuracy compared to textual reasoning, even for tasks that can be solved through code?
- Basis in paper: [explicit] The paper states that "prompting LLMs to always respond with code can sometimes result in incorrect code outputs, which resembles more of textual reasoning" and observes that "code format can limit the space of output tokens, potentially hindering the reasoning ability of LLMs."
- Why unresolved: The underlying mechanisms for why code format restricts reasoning diversity and why LLMs generate code that resembles textual reasoning are not fully explained. The paper only provides hypotheses without definitive evidence.
- What evidence would resolve it: Experiments comparing the token diversity and reasoning patterns in code vs. text outputs for the same tasks, and studies on how code formatting constraints affect LLM reasoning processes.

### Open Question 2
- Question: What causes the inverse scaling behavior observed in some tasks, where smaller models outperform larger ones when augmented with Code Interpreter?
- Basis in paper: [explicit] The paper observes that "when augmented with CI, smaller models like GPT-3.5 sometimes outperform larger ones like GPT-4o, illustrating an inverse scaling behavior" and hypothesizes this is linked to "the varied LLM's confidence in its textual reasoning ability."
- Why unresolved: The exact relationship between model size, confidence in textual reasoning, and code generation effectiveness is not fully understood. The paper does not provide a comprehensive explanation for this phenomenon.
- What evidence would resolve it: Detailed analysis of model confidence scores across different task complexities, and experiments isolating the impact of code generation vs. textual reasoning confidence on task performance.

### Open Question 3
- Question: How can we develop a more efficient method to improve LLM performance in steering between code and textual reasoning while controlling token and runtime costs?
- Basis in paper: [explicit] The paper notes that "Code + Text + Sum. and All Code with multi-turn achieve relatively higher performance, but they also consume more tokens and runtime" and suggests there is "a much broader space for further improvement in the future."
- Why unresolved: While the paper proposes some methods to improve steering, it acknowledges these methods are resource-intensive and calls for more efficient solutions, which have not yet been developed.
- What evidence would resolve it: Development and testing of new methods that achieve similar or better performance improvements with reduced token and runtime costs, and comparative studies of cost-effectiveness across different approaches.

## Limitations

- Confidence calibration in CI models remains a core limitation, leading to systematic errors on medium-difficulty tasks.
- The inverse scaling phenomenon is likely context-dependent and may not generalize beyond tested tasks and model families.
- High token and runtime costs of hybrid methods may limit practical deployment, with no cost-benefit optimization explored.

## Confidence

- **High confidence**: Code Interpreter models exhibit non-monotonic accuracy patterns tied to task complexity; hybrid approaches achieve the best average normalized scores.
- **Medium confidence**: Proposed mechanisms (overconfidence in larger models, inverse scaling, code resembling text) explain observed behaviors but need further validation.
- **Low confidence**: Generalization of inverse scaling and mode-switching failures to other model families, domains, or larger model releases is uncertain.

## Next Checks

1. **Cross-task and cross-model validation**: Test the same steering methods and mode-switching analysis on a broader set of tasks (e.g., mathematical reasoning, code repair, multi-modal inputs) and newer or alternative model families to assess the robustness of the inverse scaling and calibration findings.

2. **Prompt sensitivity and calibration**: Systematically vary the prompts for Code Interpreter+ and multi-turn refinement to measure their impact on accuracy and mode-switching behavior, and quantify the effect of prompt engineering on confidence calibration.

3. **Cost-benefit and optimization study**: Measure token usage and runtime for each method across tasks, and explore prompt or architectural optimizations (e.g., adaptive turn limits, selective code execution) to reduce overhead while preserving accuracy gains.