---
ver: rpa2
title: 'LLM-ABBA: Understanding time series via symbolic approximation'
arxiv_id: '2411.18506'
source_url: https://arxiv.org/abs/2411.18506
tags:
- series
- time
- abba
- symbolic
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-ABBA, a method that integrates adaptive
  Brownian bridge-based symbolic aggregation (ABBA) with large language models (LLMs)
  for time series analysis tasks including classification, regression, and forecasting.
  The approach addresses the challenge of bridging the gap between numerical time
  series data and LLMs by transforming time series into symbolic representations that
  capture amplitude and period patterns while preserving semantic information.
---

# LLM-ABBA: Understanding time series via symbolic approximation

## Quick Facts
- arXiv ID: 2411.18506
- Source URL: https://arxiv.org/abs/2411.18506
- Authors: Xinye Chen; Erin Carson; Cheng Kang
- Reference count: 40
- Primary result: LLM-ABBA achieves state-of-the-art performance on Time Series Extrinsic Regression (TSER) benchmarks and competitive results on medical time series classification tasks

## Executive Summary
This paper introduces LLM-ABBA, a method that integrates adaptive Brownian bridge-based symbolic aggregation (ABBA) with large language models (LLMs) for time series analysis tasks including classification, regression, and forecasting. The approach addresses the challenge of bridging the gap between numerical time series data and LLMs by transforming time series into symbolic representations that capture amplitude and period patterns while preserving semantic information. LLM-ABBA employs a fixed-point adaptive piecewise linear continuous approximation (FAPCA) trick to mitigate cumulative errors during reconstruction, particularly beneficial for forecasting tasks. The method achieves state-of-the-art performance on Time Series Extrinsic Regression (TSER) benchmarks and competitive results on medical time series classification tasks.

## Method Summary
LLM-ABBA transforms numerical time series into symbolic representations using ABBA (adaptive Brownian bridge-based symbolic aggregation), which first applies adaptive piecewise linear continuous approximation (APCA) to compress time series into polygonal chain representations, then uses mean-based clustering to map each piece to a symbol. These symbolic sequences are then processed by LLMs through fine-tuning using QLoRA with shunting inhibition. For forecasting and regression tasks, the method employs inverse symbolization to reconstruct numerical values from the symbolic outputs. A key innovation is the fixed-point adaptive piecewise linear continuous approximation (FAPCA) trick that mitigates cumulative errors during reconstruction by anchoring reconstructed points at their original positions rather than using increments.

## Key Results
- Achieves state-of-the-art performance on Time Series Extrinsic Regression (TSER) benchmarks with RMSE of 5.45
- Competitive results on UCR classification tasks, achieving accuracy rates within narrow margins of current best performers (91.8% vs 91.8% best)
- Demonstrates forecasting performance comparable to Informer architecture on ETTh1/ETTh2/ETTm1/ETTm2 datasets
- Shows strong performance on medical time series classification tasks across MIT-BIH, EEG eye states, and UCI medical datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ABBA's symbolic representation preserves the semantic structure of time series by compressing them into a sequence of symbols that capture amplitude and period patterns.
- Mechanism: ABBA first applies adaptive piecewise linear continuous approximation (APCA) to compress the time series into a polygonal chain representation, then uses mean-based clustering to map each piece to a symbol. This creates a compact symbolic sequence that retains the essential shape features of the original series.
- Core assumption: The polygonal chain approximation with bounded reconstruction error (controlled by tolerance parameter) sufficiently preserves the critical shape information needed for downstream tasks.
- Evidence anchors:
  - [abstract] "ABBA shows outstanding efficacy in preserving salient time series features by modeling time series patterns in terms of amplitude and period"
  - [section] "The symbolic time series approximation (STSA) method called adaptive Brownian bridge-based symbolic aggregation (ABBA) shows outstanding efficacy in preserving salient time series features by modeling time series patterns in terms of amplitude and period"
  - [corpus] Weak - corpus doesn't directly address ABBA's shape preservation mechanism
- Break condition: If the time series contains features finer than the tolerance parameter allows, or if the clustering fails to maintain consistent symbol semantics across different series.

### Mechanism 2
- Claim: LLMs can learn the chain-of-patterns (COP) from symbolic sequences generated by ABBA through fine-tuning.
- Mechanism: The symbolic sequences created by ABBA represent the temporal patterns of time series as discrete symbols. When LLMs are fine-tuned on these sequences, they learn to recognize and predict patterns in the symbolic domain, which can then be mapped back to numerical values through inverse symbolization.
- Core assumption: The symbolic representation contains sufficient information about the temporal relationships and patterns in the original time series for LLMs to learn meaningful representations.
- Evidence anchors:
  - [abstract] "Utilizing a symbolic time series representation, one can efficiently bridge the gap between LLMs and time series"
  - [section] "By symbolizing time series, LLM-ABBA compares favorably to the recent state-of-the-art (SOTA) in UCR and three medical time series classification tasks"
  - [corpus] Missing - corpus doesn't provide evidence about LLMs learning from symbolic representations
- Break condition: If the symbolic sequences become too abstract or if the inverse symbolization introduces significant reconstruction errors that accumulate during forecasting.

### Mechanism 3
- Claim: The fixed-point adaptive piecewise linear continuous approximation (FAPCA) mitigates cumulative errors during forecasting by anchoring reconstructed points.
- Mechanism: FAPCA modifies the standard APCA by using fixed endpoint values rather than increments, which prevents the accumulation of small errors that would otherwise compound during sequential reconstruction of forecasted values.
- Core assumption: Anchoring points at their original positions prevents the drift that occurs when small errors in each step accumulate over long sequences.
- Evidence anchors:
  - [section] "Afixed-point polygonal chaintrick is introduced to mitigate this issue. We still partition the time series into pieces following (1) whilep j = (len j,inc j)is replaced withp j = (len j, tij )before normalization"
  - [section] "Fig. 4 shows that FAPCA eliminates the cumulative errors arising from the preceding mistaken symbol and improves the recovery"
  - [corpus] Weak - corpus doesn't discuss FAPCA or error accumulation in forecasting
- Break condition: If the fixed points introduce discontinuities or if the error bounds from Theorem III.5 become too large for the specific forecasting horizon.

## Foundational Learning

- Concept: Symbolic time series approximation and dimensionality reduction
  - Why needed here: Understanding how ABBA compresses time series while preserving essential features is fundamental to grasping why this approach works
  - Quick check question: How does ABBA's tolerance parameter control the trade-off between compression ratio and reconstruction accuracy?

- Concept: Large language model fine-tuning and adapter methods
  - Why needed here: The method relies on QLoRA with shunting inhibition to adapt LLMs to time series tasks without full fine-tuning
  - Quick check question: What is the difference between full fine-tuning and adapter-based fine-tuning like QLoRA in terms of parameter efficiency?

- Concept: Error analysis and reconstruction bounds
  - Why needed here: The theoretical guarantees about reconstruction error bounds are critical for understanding when and why the method succeeds
  - Quick check question: According to Theorem III.3, what parameter directly controls the maximum reconstruction error in ABBA?

## Architecture Onboarding

- Component map:
  - Time series input → ABBA compression (APCA/FAPCA) → ABBA digitization (clustering) → Symbolic sequence → LLM tokenizer → LLM fine-tuning (QLoRA) → Task-specific output layer → Inverse symbolization (if needed)
  - Key components: ABBA implementation, LLM model with QLoRA adapter, tokenizer, inverse symbolization module

- Critical path:
  1. ABBA symbolization (compression + digitization)
  2. Tokenization of symbolic sequence
  3. LLM fine-tuning with task-specific adapter
  4. (For forecasting/regression) Inverse symbolization to reconstruct numerical values

- Design tradeoffs:
  - Higher compression (lower tolerance) → better reconstruction but more symbols → longer LLM sequences
  - Using pretrained tokens as symbols → better LLM compatibility but potential semantic drift
  - FAPCA vs APCA → reduced cumulative error vs potentially less smooth approximations

- Failure signatures:
  - Poor classification accuracy → symbolic representation losing discriminative features
  - High reconstruction error → tolerance parameter too large or clustering too coarse
  - Forecasting drift → cumulative errors in inverse symbolization or insufficient model capacity
  - Semantic drift → pretrained tokens not well-aligned with time series semantics

- First 3 experiments:
  1. Implement ABBA on a simple synthetic time series (e.g., sine wave) and verify reconstruction accuracy vs tolerance parameter
  2. Fine-tune a small LLM on symbolic sequences from UCR dataset and measure classification accuracy
  3. Implement FAPCA and compare forecasting accuracy vs standard APCA on a univariate time series with clear trends

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the symbolic consistency across different time series under the same symbolization scheme affect the learning of consistent knowledge by large language models?
- Basis in paper: [explicit] The paper discusses symbolic consistency issues that exist in STSA methods and questions whether LLMs will learn consistent knowledge from the transformed symbols that contain the time series pattern logic.
- Why unresolved: The paper acknowledges this as a challenge but does not provide empirical evidence or theoretical analysis on how symbolic consistency affects LLM learning outcomes.
- What evidence would resolve it: Comparative experiments showing performance differences between consistent and inconsistent symbolization schemes, along with analysis of LLM attention patterns on different symbolization approaches.

### Open Question 2
- Question: What is the optimal balance between the tolerance parameter (tol) and the digitization parameter (α) for achieving the best reconstruction accuracy while minimizing the number of symbols used?
- Basis in paper: [inferred] The paper discusses the reconstruction error analysis and mentions that decreasing tol or α tends to result in smaller reconstruction errors, but also leads to more symbols being used. It also shows that there is a trade-off between runtime and performance.
- Why unresolved: The paper does not provide a systematic study or guidelines on how to optimally set these parameters for different types of time series data or tasks.
- What evidence would resolve it: A comprehensive empirical study varying tol and α across different datasets and tasks, with quantitative analysis of reconstruction accuracy, symbol count, and downstream task performance.

### Open Question 3
- Question: How does the hallucination tendency of large language models impact the accuracy of long-term time series forecasting tasks?
- Basis in paper: [explicit] The paper mentions that because LLMs are born with hallucination, more generated contents would contain more "hallucination knowledge," and therefore LLM-ABBA performs better on short-term forecasting tasks.
- Why unresolved: The paper identifies this as a limitation but does not quantify the impact of hallucination on forecasting accuracy or explore methods to mitigate this effect.
- What evidence would resolve it: Comparative studies of forecasting accuracy at different time horizons, analysis of the nature and frequency of hallucinated content in generated sequences, and experiments testing hallucination mitigation techniques.

## Limitations

- Theoretical gap exists between ABBA's reconstruction error bounds and downstream LLM performance guarantees
- Claims about "state-of-the-art performance" are primarily validated on specific benchmarks (UCR, TSER, medical datasets) without broader generalization testing
- Tokenization assumptions create potential semantic mismatch since LLM tokens were trained on language data, not time series patterns

## Confidence

- High Confidence: Claims about ABBA's effectiveness in preserving salient time series features and reducing dimensionality while maintaining reconstruction accuracy
- Medium Confidence: Claims about competitive performance on UCR classification tasks and medical time series classification
- Medium Confidence: Claims about forecasting performance being "comparable to Informer"

## Next Checks

**Validation Check 1**: Reproduce the reconstruction accuracy vs. tolerance parameter relationship on synthetic time series (e.g., sine waves with varying frequencies, sawtooth patterns, and step functions) to verify Theorem III.3's bounds hold across different signal types.

**Validation Check 2**: Implement cross-dataset generalization tests where an LLM-ABBA model trained on UCR datasets is evaluated on the ETTh1/ETTh2 datasets (or vice versa) to assess whether symbolic representations learned on one domain transfer to others.

**Validation Check 3**: Conduct ablation studies comparing FAPCA vs. standard APCA for forecasting tasks on multiple time series with varying trend strengths (strong trends, seasonal patterns, chaotic signals) to quantify the cumulative error mitigation benefit claimed in Mechanism 3.