---
ver: rpa2
title: Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition
  of Misordered Context
arxiv_id: '2410.07103'
source_url: https://arxiv.org/abs/2410.07103
tags:
- context
- documents
- core
- reasoning
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a "misordered context problem" in large language
  models (LLMs) where the order of supporting documents affects multi-hop reasoning
  performance. The authors propose a simple method called context repetition (CoRe)
  that repeats the context multiple times, ensuring that contiguous reasoning segments
  are presented in an optimal order.
---

# Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context

## Quick Facts
- **arXiv ID**: 2410.07103
- **Source URL**: https://arxiv.org/abs/2410.07103
- **Authors**: Sangwon Yu; Ik-hwan Kim; Jongyoon Song; Saehyung Lee; Junsung Park; Sungroh Yoon
- **Reference count**: 22
- **Key outcome**: CoRe improves F1 scores by up to 30 percentage points on multi-hop QA tasks and accuracy by up to 70 percentage points on synthetic tasks

## Executive Summary
Large language models (LLMs) struggle with multi-hop reasoning when supporting documents are presented in suboptimal orders, leading to performance drops of up to 26% in F1 score. This paper identifies the "misordered context problem" and proposes context repetition (CoRe) as a simple solution. By repeating the context k times (where k is the number of supporting documents), CoRe ensures that all possible orders of presenting documents are covered, increasing the probability that the model encounters an optimal reasoning chain. The method also mitigates the "lost-in-the-middle" problem and can be combined with Chain-of-Thought approaches.

## Method Summary
CoRe involves prompting the model by repeatedly presenting the context k times, where k is the number of supporting documents. This repetition ensures that certain contiguous reasoning segments within supporting documents are presented in optimal order, covering all possible permutations of document presentation. The approach is particularly effective for decoder-only LLMs that process context in a fixed left-to-right order due to causal attention. CoRe can be combined with retrieval-based approaches and helps mitigate the "lost-in-the-middle" problem by increasing the visibility of documents that would otherwise appear in the middle of long contexts.

## Key Results
- CoRe improves F1 scores by up to 30 percentage points on multi-hop QA tasks
- CoRe improves accuracy by up to 70 percentage points on synthetic tasks
- Performance gains are most pronounced when supporting documents are in the middle of long contexts
- CoRe effectively mitigates the "lost-in-the-middle" problem in LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Context repetition ensures that all possible orders of supporting documents are presented to the model at least once.
- **Mechanism**: By repeating the context k times, the model encounters each supporting document in every possible position within the reasoning chain.
- **Core assumption**: The model can recognize and utilize the optimal ordering of documents when encountered, even if presented non-contiguously across repetitions.
- **Evidence anchors**: [abstract] "we propose a simple yet effective method called context repetition (CoRe), which involves prompting the model by repeatedly presenting the context"; [section] "we demonstrate that by repeating the context containing k supporting documents k times, all possible orders of presenting the documents can be covered"
- **Break condition**: When the number of supporting documents becomes too large, making k repetitions computationally infeasible

### Mechanism 2
- **Claim**: Repetition improves performance by exposing the model to the same reasoning chain in different orders, increasing the probability of encountering an optimal configuration.
- **Mechanism**: As the context is repeated, the probability that the model encounters the supporting documents in an order that facilitates correct reasoning increases.
- **Core assumption**: The model's reasoning performance is sensitive to the order of supporting documents, and this sensitivity can be mitigated by repetition.
- **Evidence anchors**: [abstract] "LLMs' performance is also sensitive to the order, relative position, in which the supporting documents are presented"; [section] "we observe that, even in ideal situations without noisy documents, LLMs exhibit up to a 26%p difference in F1 score depending on the order, relative position, of supporting documents"
- **Break condition**: When noisy documents overwhelm the context, making the signal-to-noise ratio too low even with repetition

### Mechanism 3
- **Claim**: CoRe mitigates the "lost-in-the-middle" problem by increasing the number of times each document appears in the context.
- **Mechanism**: By repeating the context, documents that would normally appear in the middle of a long context get presented multiple times, increasing their visibility to the model.
- **Core assumption**: The "lost-in-the-middle" problem is caused by absolute positional bias, which can be partially mitigated by repetition.
- **Evidence anchors**: [abstract] "CoRe helps mitigate the well-known 'lost-in-the-middle' problem in LLMs"; [section] "we observe that the performance gap between the baseline and CoRe increases as the position of the supporting documents moves toward the middle"
- **Break condition**: When the context becomes too long, causing computational constraints that outweigh the benefits of repetition

## Foundational Learning

- **Concept: Multi-hop reasoning**
  - **Why needed here**: Understanding how models chain multiple reasoning steps is crucial for grasping why document order matters
  - **Quick check question**: What is the difference between single-hop and multi-hop reasoning in the context of question answering?

- **Concept: Causal attention in decoder-only models**
  - **Why needed here**: The left-to-right processing nature of these models creates sensitivity to document order
  - **Quick check question**: How does causal attention affect the way a model processes information in a sequence?

- **Concept: In-context learning**
  - **Why needed here**: CoRe relies on the model's ability to learn from repeated context without parameter updates
  - **Quick check question**: What distinguishes in-context learning from traditional fine-tuning approaches?

## Architecture Onboarding

- **Component map**: Retriever -> Context formatter (applies CoRe repetition) -> LLM (performs reasoning on repeated context) -> Evaluation module
- **Critical path**: 
  1. Retrieve or construct context with supporting documents
  2. Apply CoRe repetition (k times)
  3. Feed repeated context to LLM with appropriate prompt
  4. Process model output and evaluate
- **Design tradeoffs**: 
  - Higher k values increase the probability of optimal ordering but also increase computational cost
  - Balance between repetition count and context length limits
  - Trade-off between performance gains and inference time
- **Failure signatures**: 
  - Performance plateaus or degrades with high repetition counts (likely due to noisy documents)
  - No improvement when k = 1 (baseline case)
  - Diminishing returns as k increases beyond a certain point
- **First 3 experiments**: 
  1. Baseline test: Run without CoRe (k = 1) on a small multi-hop QA dataset
  2. Simple repetition test: Apply CoRe with k = 2 on the same dataset
  3. Order sensitivity test: Create permuted contexts and measure performance variance with and without CoRe

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the optimal number of repetitions (k-hat) vary across different model architectures and reasoning task complexities?
- **Basis in paper**: [explicit] The paper identifies that k-hat needs to be predetermined and discusses its practical limitations, noting that "the model leads to the more appropriate order of reasoning chains as repetition progresses multiple times" but also observing performance degradation with excessive repetition in some models.
- **Why unresolved**: The paper only evaluates k-hat values in the range [1,3] and notes that "future research could explore ways to repeat only the core contents of the context" but doesn't provide a systematic method for determining the optimal k-hat value for different scenarios.
- **What evidence would resolve it**: Empirical studies testing a wider range of k-hat values (e.g., 1-10) across diverse model architectures and task complexities, combined with theoretical analysis of the trade-off between improved reasoning and computational costs.

### Open Question 2
- **Question**: Can CoRe be combined with retrieval techniques to dynamically determine which documents to repeat based on their relevance to the reasoning chain?
- **Basis in paper**: [inferred] The paper mentions that CoRe "can be effectively combined with retrieval-based approaches" and shows improved performance in retrieve-and-reason tasks, but doesn't explore selective repetition of retrieved documents.
- **Why unresolved**: The current implementation repeats all documents equally, which may include unnecessary repetition of noisy documents that could degrade performance, as observed in the synthetic task experiments.
- **What evidence would resolve it**: Experiments comparing uniform repetition versus relevance-weighted repetition, measuring both performance improvements and computational efficiency.

### Open Question 3
- **Question**: What is the relationship between context length, document order sensitivity, and model architecture (decoder-only vs encoder-decoder) in multi-hop reasoning?
- **Basis in paper**: [explicit] The paper states that "decoder-only LLMs, operating with causal attention, interpret the context in a fixed left-to-right order" and demonstrates that performance varies significantly with document order.
- **Why unresolved**: The study focuses on decoder-only models and doesn't compare how different architectures handle document ordering or how context length affects sensitivity to misordered contexts.
- **What evidence would resolve it**: Comparative experiments across different model architectures (decoder-only, encoder-decoder, hybrid) with varying context lengths, measuring document order sensitivity and performance gaps.

### Open Question 4
- **Question**: How does CoRe affect the interpretability and transparency of multi-hop reasoning chains in LLMs?
- **Basis in paper**: [inferred] The paper shows that CoRe improves performance by ensuring "certain contiguous reasoning segments within supporting documents are presented in the optimal order," but doesn't analyze whether this makes the reasoning process more interpretable.
- **Why unresolved**: While the method improves performance, it's unclear whether repeating contexts makes the model's reasoning more traceable or if it introduces additional complexity that obscures the reasoning process.
- **What evidence would resolve it**: Analysis of attention patterns, intermediate reasoning steps, and error cases with and without CoRe, measuring both performance and interpretability metrics.

## Limitations

- CoRe's computational scalability is limited as the number of supporting documents increases, creating quadratic growth in context length
- The approach's effectiveness depends on the quality of supporting documents, with noisy documents potentially overwhelming the benefits of repetition
- The paper's empirical evaluation relies heavily on synthetic tasks and controlled experiments with clean contexts, raising questions about real-world applicability

## Confidence

**High Confidence**: The identification of order sensitivity in multi-hop reasoning (up to 26% F1 difference) is well-supported by controlled experiments on clean contexts. The mechanism explaining why repetition helps - by increasing the probability of encountering optimal document orders - is logically sound and consistent with the empirical results.

**Medium Confidence**: The claim that CoRe mitigates the "lost-in-the-middle" problem is supported by observations but the causal connection between repetition and positional bias mitigation could be more thoroughly established. The synthetic task results showing up to 70% accuracy improvement are impressive but may not generalize to real-world question answering scenarios.

**Low Confidence**: The scalability claims for large k values are based on limited experimental evidence. The paper's suggestion that CoRe can be combined with Chain-of-Thought approaches is mentioned but not empirically validated, making this a speculative benefit.

## Next Checks

1. **Scalability Validation**: Test CoRe on multi-hop reasoning tasks with varying numbers of supporting documents (k = 2, 3, 4, 5, 6) to establish the point at which computational costs outweigh performance benefits. Measure both accuracy gains and inference time increases across different model sizes.

2. **Noise Tolerance Experiment**: Create datasets with controlled levels of noisy documents (0%, 25%, 50%, 75%) and evaluate how CoRe's performance degrades as noise increases. Compare against alternative approaches like document filtering or re-ranking to establish CoRe's robustness profile.

3. **Real-World Retrieval Test**: Implement CoRe on open-domain question answering using standard retrieval systems (e.g., BM25, dense retrieval) to measure performance when supporting documents are not pre-filtered. Compare against retrieval-augmented generation baselines to assess practical value.