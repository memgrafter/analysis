---
ver: rpa2
title: 'KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph
  Completion'
arxiv_id: '2402.02389'
source_url: https://arxiv.org/abs/2402.02389
tags:
- knowledge
- kicgpt
- entities
- entity
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KICGPT, a framework that integrates a large
  language model (LLM) and a triple-based knowledge graph completion (KGC) retriever.
  It addresses the long-tail problem in KGC by encoding structural knowledge into
  demonstrations to guide the LLM through in-context learning.
---

# KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2402.02389
- Source URL: https://arxiv.org/abs/2402.02389
- Authors: Yanbin Wei; Qiushi Huang; James T. Kwok; Yu Zhang
- Reference count: 34
- One-line primary result: KICGPT achieves MRR of 0.412 and Hits@1 of 0.327 on FB15k-237, outperforming existing triple-based and text-based methods while being more efficient than text-based approaches.

## Executive Summary
KICGPT is a novel framework that integrates a large language model (LLM) with a triple-based knowledge graph completion (KGC) retriever to address the long-tail entity problem in KGC. The framework uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. By formalizing link prediction as a re-ranking task, KICGPT constrains the LLM's output and leverages its internal knowledge base to improve performance on long-tail entities. Experimental results on FB15k-237 and WN18RR datasets demonstrate state-of-the-art performance with low training overhead.

## Method Summary
KICGPT combines a triple-based retriever (RotatE) with an LLM (ChatGPT) using a Knowledge Prompt in-context learning strategy. The retriever generates initial rankings of entities, while the LLM performs re-ranking based on demonstrations constructed from the knowledge graph. The framework constructs two pools of triples - analogy and supplement pools - and orders them using diversity-based and BM25 score-based strategies respectively. The LLM's output is constrained to re-ranking the top-m entities from the retriever, addressing the issue of generating out-of-scope entities. KICGPT achieves state-of-the-art performance on FB15k-237 and WN18RR datasets with minimal training overhead.

## Key Results
- KICGPT achieves MRR of 0.412 and Hits@1 of 0.327 on FB15k-237 dataset.
- Outperforms existing triple-based and text-based methods on both FB15k-237 and WN18RR datasets.
- More efficient than text-based approaches that require expensive fine-tuning, with low training overhead.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KICGPT alleviates long-tail entity problems by encoding structural knowledge into demonstrations to guide the LLM through in-context learning.
- Mechanism: The framework uses a retriever to obtain top-m candidates, then performs re-ranking using the LLM with the proposed Knowledge Prompt strategy, which encodes structural knowledge into demonstrations.
- Core assumption: The LLM's internal knowledge base can be effectively leveraged to complement structural information in knowledge graphs for long-tail entities.
- Evidence anchors:
  - [abstract] "KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM."
  - [section 3.3] "By encoding part of the KG into demonstrations, Knowledge Prompt boosts the performance of LLM on link prediction."
  - [corpus] Weak evidence - no direct corpus studies on this specific mechanism, though related works like "Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion" suggest similar approaches.
- Break condition: If the LLM's internal knowledge base does not contain relevant information for the long-tail entities, or if the retriever fails to provide relevant candidates.

### Mechanism 2
- Claim: KICGPT constrains LLM outputs by formalizing link prediction as a re-ranking task for the given sequence Rretriever.
- Mechanism: Instead of generating unconstrained outputs, KICGPT limits the LLM to re-ranking the top-m entities from the retriever's output.
- Core assumption: Constraining the LLM to re-rank a pre