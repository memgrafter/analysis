---
ver: rpa2
title: A Survey of Large Language Models for European Languages
arxiv_id: '2408.15040'
source_url: https://arxiv.org/abs/2408.15040
tags:
- language
- https
- text
- languages
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey presents the first comprehensive review of large language
  models (LLMs) for European languages, categorizing them into high-, mid-, and low-resource
  groups. It summarizes monolingual and multilingual pretraining datasets, including
  OSCAR, Wikipedia, CCNet, mC4, and others, with detailed statistics on corpus sizes
  and token counts.
---

# A Survey of Large Language Models for European Languages

## Quick Facts
- arXiv ID: 2408.15040
- Source URL: https://arxiv.org/abs/2408.15040
- Authors: Wazir Ali; Sampo Pyysalo
- Reference count: 40
- Primary result: First comprehensive review of large language models for European languages, categorizing them by resource availability and detailing monolingual and multilingual pretraining datasets.

## Executive Summary
This survey presents the first comprehensive review of large language models (LLMs) for European languages, categorizing them into high-, mid-, and low-resource groups. It summarizes monolingual and multilingual pretraining datasets, including OSCAR, Wikipedia, CCNet, mC4, and others, with detailed statistics on corpus sizes and token counts. The paper provides an overview of existing LLMs for European languages, highlighting encoder-only, decoder-only, and encoder-decoder architectures, and serves as a foundational resource for researchers developing LLMs tailored to European languages.

## Method Summary
The paper synthesizes existing literature on LLMs for European languages, categorizing them by resource availability and detailing monolingual and multilingual pretraining datasets. It reviews existing LLM architectures and their applications for different language categories, compiling an overview of existing monolingual and multilingual LLMs for European languages. The methodology involves collecting and categorizing datasets for 24 official EU languages and summarizing the existing work on LLM development for European languages.

## Key Results
- Comprehensive review of large language models for European languages, categorizing them into high-, mid-, and low-resource groups
- Detailed summary of monolingual and multilingual pretraining datasets, including OSCAR, Wikipedia, CCNet, mC4, and others
- Overview of existing LLMs for European languages, highlighting encoder-only, decoder-only, and encoder-decoder architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs learn to understand and generate language by training billions of parameters on vast volumes of text data.
- Mechanism: Large-scale self-supervised pretraining on massive corpora enables LLMs to capture complex linguistic patterns, long-range dependencies, and generalized representations applicable to diverse downstream tasks.
- Core assumption: Increased model size and training data directly correlate with improved performance on NLP tasks.
- Evidence anchors:
  - [abstract]: "The LLMs learn to understand and generate language by training billions of model parameters on vast volumes of text data."
  - [section]: "Larger pretrained LMs have led to the development of LLMs by significantly increasing the size of the training datasets (many gigabytes to terabytes) and the number of parameters (billions to trillions)."
  - [corpus]: Found 25 related papers with strong FMR scores, indicating substantial academic interest and research activity in this area.
- Break Condition: Diminishing returns on performance gains despite continued increases in model size and training data, suggesting other bottlenecks such as architecture limitations or data quality issues.

### Mechanism 2
- Claim: Pretraining followed by task-specific fine-tuning allows LLMs to generalize across a wide range of NLP tasks.
- Mechanism: Pretraining on large, diverse corpora builds general language understanding, while fine-tuning adapts the model to specific task formats and objectives, enabling effective zero-shot and few-shot learning.
- Core assumption: The general representations learned during pretraining transfer effectively to downstream tasks with minimal task-specific data.
- Evidence anchors:
  - [abstract]: "Unlike traditional LMs, transformer-based LLMs are not task-specific and can be adapted for a wide range of NLP applications by adding specific output layers on top of the core transformer architecture."
  - [section]: "Unlike early neural LMs, pretrained LMs introduced the paradigm of pretraining and fine-tuning using recurrent neural networks [8] for general NLP tasks."
  - [corpus]: High FMR scores for survey papers indicate strong community interest in evaluating and understanding the pretraining and fine-tuning paradigm.
- Break Condition: Poor transfer performance due to domain mismatch between pretraining and fine-tuning data, or insufficient fine-tuning data for the target task.

### Mechanism 3
- Claim: Specialized architectures (encoder-only, decoder-only, encoder-decoder) enable LLMs to excel at specific types of NLP tasks.
- Mechanism: Encoder-only models (e.g., BERT) are optimized for understanding tasks like classification, decoder-only models (e.g., GPT) for generation tasks, and encoder-decoder models (e.g., T5) for sequence-to-sequence tasks like translation and summarization.
- Core assumption: Architectural design choices are aligned with the computational and representational needs of different NLP tasks.
- Evidence anchors:
  - [abstract]: "This survey presents the first comprehensive review of large language models (LLMs) for European languages, categorizing them into high-, mid-, and low-resource groups."
  - [section]: "This section provides an overview of small and large language models, including encoder, decoder, encoder-decoder, and Sparse models along with a discussion of remarkable findings that have contributed to enhance the performance in NLU and NLG tasks."
  - [corpus]: Presence of multiple LLM survey papers suggests active research into architectural innovations and their impact on task performance.
- Break Condition: Architectural specialization limits model flexibility, requiring multiple models for different tasks or hindering performance on tasks that don't fit neatly into one category.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: The transformer architecture is the foundation of modern LLMs, enabling efficient parallelization, handling of variable input lengths, and capture of long-range dependencies.
  - Quick check question: What are the key differences between the transformer encoder and decoder components, and how do these differences impact their suitability for different NLP tasks?

- Concept: Pretraining and fine-tuning paradigm
  - Why needed here: Understanding the pretraining and fine-tuning process is crucial for developing and applying LLMs effectively, as it determines how general representations are learned and adapted to specific tasks.
  - Quick check question: What are the advantages and disadvantages of different pretraining objectives (e.g., masked language modeling, autoregressive modeling) and how do they impact downstream task performance?

- Concept: Multilingual modeling
  - Why needed here: The survey focuses on LLMs for European languages, which requires understanding how to effectively train and evaluate models across multiple languages with varying resource availability.
  - Quick check question: What are the challenges and strategies for training multilingual LLMs, and how do they differ from monolingual approaches?

## Architecture Onboarding

- Component map:
  - Data collection and preprocessing -> Pretraining module -> Fine-tuning module -> Evaluation module

- Critical path:
  1. Data collection and preprocessing
  2. Pretraining on massive multilingual corpora
  3. Task-specific fine-tuning
  4. Evaluation and benchmarking

- Design tradeoffs:
  - Model size vs. computational efficiency: Larger models generally perform better but require more resources to train and deploy.
  - Pretraining data size and diversity vs. data quality: More data can improve generalization, but low-quality data can introduce noise and biases.
  - Multilingual modeling vs. language-specific optimization: Multilingual models offer broader coverage but may underperform on low-resource languages compared to specialized models.

- Failure signatures:
  - Poor pretraining performance: Indicates issues with data quality, model architecture, or training hyperparameters.
  - Ineffective fine-tuning: Suggests domain mismatch between pretraining and fine-tuning data or insufficient fine-tuning data.
  - Catastrophic forgetting: Model loses previously learned knowledge when fine-tuned on new tasks.

- First 3 experiments:
  1. Train a small multilingual BERT model on a subset of the OSCAR corpus for a few European languages and evaluate on a simple classification task.
  2. Fine-tune the pretrained BERT model on a task-specific dataset (e.g., sentiment analysis) and compare performance to a randomly initialized model.
  3. Experiment with different pretraining objectives (e.g., masked language modeling vs. autoregressive modeling) and evaluate their impact on downstream task performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective strategies for developing large language models for low-resource European languages, given the limited availability of high-quality monolingual datasets?
- Basis in paper: [explicit] The paper identifies several low-resource EU languages (e.g., Maltese, Lithuanian, Slovak, Slovene, Irish, Latvian, Estonian, and Croatian) and notes the limited availability of online textual resources and annotated data for training LLMs in these languages.
- Why unresolved: The paper primarily focuses on summarizing existing resources and models for EU languages, but does not delve into specific strategies for addressing the challenges of low-resource languages. It mentions that these languages are often included in multilingual pretrained models but lack dedicated LLMs trained specifically for them.
- What evidence would resolve it: Empirical studies comparing different data augmentation techniques, transfer learning approaches, and multilingual pretraining strategies for low-resource EU languages would provide insights into the most effective methods.

### Open Question 2
- Question: How do the performance characteristics of monolingual LLMs for European languages compare to those of multilingual models when evaluated on language-specific tasks?
- Basis in paper: [explicit] The paper presents an overview of both monolingual and multilingual LLMs for European languages, highlighting the strengths and weaknesses of each approach. It mentions that multilingual models often prioritize English and may not perform as well on specific European languages.
- Why unresolved: While the paper provides examples of both monolingual and multilingual models for various EU languages, it does not offer a comprehensive comparative analysis of their performance on language-specific tasks. The focus is more on summarizing the existing work rather than conducting a detailed performance comparison.
- What evidence would resolve it: Systematic evaluations of monolingual and multilingual models on a wide range of language-specific benchmarks for different EU languages would shed light on their relative strengths and weaknesses.

### Open Question 3
- Question: What are the ethical considerations and potential biases associated with developing and deploying large language models for European languages, particularly in relation to cultural and linguistic diversity?
- Basis in paper: [inferred] The paper highlights the linguistic diversity of the EU and the importance of preserving and promoting this diversity through the development of LLMs for European languages. However, it does not explicitly address the ethical implications and potential biases that may arise from such development.
- Why unresolved: The paper focuses primarily on the technical aspects of LLM development and does not delve into the broader societal and ethical implications. The potential for bias in training data, the impact on language preservation, and the equitable access to LLM technology for different linguistic communities are not discussed.
- What evidence would resolve it: Research on the ethical considerations and potential biases associated with LLM development for European languages, including studies on data collection practices, model evaluation methods, and the impact on language communities, would provide valuable insights.

## Limitations
- The survey synthesizes existing literature rather than presenting novel experimental results, limiting direct verification of claims.
- Exact performance metrics for the reviewed models are not provided, and training procedures and hyperparameters remain unspecified.
- The categorization of languages into high-, mid-, and low-resource groups may not capture nuanced resource availability for specific tasks or domains within each language.

## Confidence
- **High Confidence**: The existence and general characteristics of major European language models (GBERT, CamemBERT, etc.) and their architectural classifications (encoder-only, decoder-only, encoder-decoder) are well-established and verifiable through referenced literature.
- **Medium Confidence**: The categorization of languages by resource availability and the identification of relevant pretraining datasets are based on reasonable assessments but may vary depending on specific task requirements and evolving data availability.
- **Low Confidence**: Claims about relative model performance across languages and tasks lack quantitative support in the survey, making direct comparison and validation difficult.

## Next Checks
1. Cross-reference the language resource categorizations with independent assessments from language technology communities to verify the accuracy of high-, mid-, and low-resource classifications.
2. Compare the identified pretraining datasets (OSCAR, Wikipedia, CCNet, mC4) against recent corpus studies to ensure completeness and identify any significant omissions.
3. Validate the architectural classifications of specific models by examining their original papers to confirm encoder-only, decoder-only, or encoder-decoder designations and their implications for task performance.