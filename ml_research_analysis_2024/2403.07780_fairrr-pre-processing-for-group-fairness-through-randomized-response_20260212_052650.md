---
ver: rpa2
title: 'FairRR: Pre-Processing for Group Fairness through Randomized Response'
arxiv_id: '2403.07780'
source_url: https://arxiv.org/abs/2403.07780
tags:
- fairness
- fair
- fairrr
- group
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FairRR, a pre-processing method for achieving
  group fairness in machine learning models through randomized response. The method
  formulates group fairness as an optimal design matrix problem where a response variable
  is modified according to sensitive attributes to achieve fairness constraints.
---

# FairRR: Pre-Processing for Group Fairness through Randomized Response

## Quick Facts
- **arXiv ID**: 2403.07780
- **Source URL**: https://arxiv.org/abs/2403.07780
- **Authors**: Xianli Zeng; Joshua Ward; Guang Cheng
- **Reference count**: 11
- **Primary result**: FairRR achieves controlled group fairness through pre-processing with label flipping while maintaining model utility

## Executive Summary
This paper introduces FairRR, a pre-processing method for achieving group fairness in machine learning through randomized response mechanisms. The approach formulates group fairness as an optimal design matrix problem where response variables are modified based on sensitive attributes to satisfy fairness constraints. FairRR employs an imbalanced randomized response technique that flips labels according to sensitive attributes, enabling control over demographic parity, equality of opportunity, and predictive equality metrics. The method demonstrates effectiveness across three benchmark datasets (Adult, COMPAS, and Law School), showing that it can directly control disparity levels while preserving model accuracy and F1 scores, outperforming or matching existing pre-processing techniques.

## Method Summary
FairRR operates as a pre-processing technique that modifies the response variable in a dataset to achieve group fairness constraints before training any machine learning model. The method formulates fairness as an optimal design matrix problem where sensitive attributes guide a randomized response mechanism for label flipping. An imbalanced randomization approach is applied where the probability of flipping a label depends on both the true label and the sensitive attribute group membership. This creates a modified dataset where statistical independence between predictions and sensitive attributes is achieved while maintaining reasonable model utility. The technique allows direct control over specific disparity levels, providing flexibility in balancing accuracy and fairness requirements.

## Key Results
- FairRR achieves controlled disparity levels on Adult, COMPAS, and Law School datasets while maintaining model accuracy
- The method outperforms or matches existing pre-processing techniques including Fair Sampling, FAWOS, and TabFairGAN
- FairRR provides direct control over specific fairness metrics (demographic parity, equality of opportunity, predictive equality)
- The approach maintains competitive F1 scores while reducing fairness disparities

## Why This Works (Mechanism)
FairRR leverages the theoretical foundation that group fairness can be achieved by modifying the joint distribution between sensitive attributes, predictions, and outcomes. By applying randomized response to labels based on sensitive attributes, the method breaks statistical dependencies that cause bias while preserving the underlying structure necessary for useful predictions. The imbalanced nature of the flipping mechanism ensures that the modification is proportional to existing disparities, allowing for controlled correction rather than over-correction.

## Foundational Learning
- **Randomized Response**: A privacy-preserving technique where responses are randomly altered to protect sensitive information while maintaining statistical properties. Why needed: Provides the mathematical framework for controlled label manipulation. Quick check: Verify that the expected value of flipped labels matches target distributions.
- **Design Matrix Optimization**: The mathematical formulation of fairness constraints as optimization over feature-label relationships. Why needed: Enables precise control over fairness metrics through systematic modifications. Quick check: Confirm that the optimization converges and satisfies constraint bounds.
- **Imbalanced Flipping Probabilities**: The use of different flipping rates for different sensitive attribute groups. Why needed: Allows targeted correction of specific fairness disparities. Quick check: Validate that flipping probabilities align with observed base rates.

## Architecture Onboarding

Component Map: Sensitive Attributes -> Randomized Response Mechanism -> Modified Labels -> Training Data

Critical Path: The core workflow processes sensitive attributes through the randomized response mechanism to generate modified labels, which are then combined with original features to create the pre-processed dataset for model training.

Design Tradeoffs: The method trades off between perfect accuracy preservation and fairness achievement, requiring careful calibration of flipping probabilities to balance these competing objectives.

Failure Signatures: Poor performance manifests as either insufficient fairness improvement (flipping probabilities too conservative) or significant accuracy loss (flipping probabilities too aggressive).

First Experiments:
1. Apply FairRR with varying flipping probability ranges to identify the optimal trade-off point for each dataset
2. Test the method on a held-out validation set to verify that fairness improvements generalize beyond training data
3. Implement FairRR without sensitive attributes during testing to evaluate real-world applicability constraints

## Open Questions the Paper Calls Out
None

## Limitations
- The method assumes access to sensitive attributes during both training and testing phases, which may not be feasible in privacy-constrained environments
- The relationship between label flipping and fairness achievement may not generalize across all data distributions and fairness constraints
- The approach may not perform optimally in scenarios with complex, multi-dimensional sensitive attributes or online/streaming data contexts

## Confidence

**High confidence**: The method's ability to achieve controlled disparity levels in experimental settings; the mathematical formulation of fairness constraints as an optimization problem.

**Medium confidence**: The generalization of results across different datasets and fairness metrics; the trade-off between accuracy and fairness being optimal for all use cases.

**Low confidence**: The practical applicability in scenarios with complex, multi-dimensional sensitive attributes; the method's performance in online or streaming data contexts.

## Next Checks

1. **Ablation study on flipping probability**: Conduct experiments varying the probability of label flipping across different ranges to identify the optimal trade-off point between fairness and accuracy for each dataset.

2. **Cross-dataset robustness test**: Evaluate FairRR's performance on additional diverse datasets, particularly those with different class imbalance ratios and sensitive attribute distributions.

3. **Post-processing comparison**: Implement and compare FairRR against post-processing methods under identical conditions to validate the claimed advantages of pre-processing approaches.