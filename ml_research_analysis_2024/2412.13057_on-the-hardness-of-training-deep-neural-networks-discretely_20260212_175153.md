---
ver: rpa2
title: On the Hardness of Training Deep Neural Networks Discretely
arxiv_id: '2412.13057'
source_url: https://arxiv.org/abs/2412.13057
tags:
- d-nnt
- such
- network
- holds
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the computational complexity of training deep
  neural networks. It shows that training neural networks discretely (with parameters
  from a finite set) is significantly harder than in the continuous case.
---

# On the Hardness of Training Deep Neural Networks Discretely

## Quick Facts
- arXiv ID: 2412.13057
- Source URL: https://arxiv.org/abs/2412.13057
- Authors: Ilan Doron-Arad
- Reference count: 17
- Primary result: Discrete neural network training is not in NP even for simple deep architectures, making it harder than NP-complete problems

## Executive Summary
This paper establishes fundamental computational complexity results for training deep neural networks with discrete parameters. The key finding is that discrete neural network training (D-NNT) is not in the complexity class NP even for very simple deep architectures, making it harder than any NP-complete problem. This result is proven under standard complexity assumptions and demonstrates a dramatic difference between discrete and continuous training. The paper also shows that this hardness extends to the continuous case via a polynomial reduction, and proves that discrete training remains NP-hard even for simple two-layer networks under severely restricted conditions.

## Method Summary
The paper studies the decision problem of neural network training (NNT), which asks whether there exist parameters that achieve training loss below a threshold. It analyzes both discrete NNT (D-NNT) with finite parameter spaces and continuous NNT (C-NNT) with real-valued parameters. The main technical approach involves reducing known hard problems (PosSLP and subset sum) to instances of D-NNT, then using these reductions to establish complexity class separations. A pseudo-polynomial dynamic programming algorithm is developed for two-layer networks in the over-parametrized case with fixed dataset size.

## Key Results
- D-NNT is not in NP even for deep networks with fixed dimensions and dataset size
- D-NNT remains NP-hard for two-layer networks with severely restricted parameters (one hidden layer, one data point, identity activation, sum of squares loss)
- Polynomial reduction exists from D-NNT to C-NNT, showing C-NNT inherits hardness
- Pseudo-polynomial algorithm exists for two-layer D-NNT with fixed dataset size, running in time M^O(n₀) · d · k

## Why This Works (Mechanism)

### Mechanism 1
- Claim: D-NNT is not in NP even for deep networks with very restricted parameters.
- Mechanism: The proof encodes an NP-hard problem (PosSLP) into a D-NNT instance where the correctness of a solution depends on the exact computation of intermediate variables in a straight-line program. Because verifying a solution requires checking that each layer's computation matches the program's variables, and the network's depth allows representing large integers through activation functions, the verification cannot be done in polynomial time.
- Core assumption: The Constructive Univariate Radical Conjecture holds and NP ⊈ BPP.
- Evidence anchors:
  - [abstract] "D-NNT is not in the complexity class NP even for instances with fixed dimensions and dataset size, having a deep architecture."
  - [section] Lemma 3.5 shows reduction from PosSLP and Theorem 1.1 concludes D-NNT ∉ NP under assumptions.
  - [corpus] Weak - corpus papers focus on continuous training or Lipschitzness, not on discrete NP-hardness.
- Break condition: If PosSLP were in BPP or if the conjecture failed, the hardness result would not hold.

### Mechanism 2
- Claim: D-NNT remains NP-hard even for two-layer networks under severely restricted conditions.
- Mechanism: Reduction from subset sum where each neuron corresponds to an item and weights encode selection (0 or the item value). The loss function forces exact matching of the sum to the target, making verification NP-hard.
- Core assumption: P ≠ NP.
- Evidence anchors:
  - [abstract] "complement these results with a comprehensive list of NP-hardness lower bounds for D-NNT on two-layer networks"
  - [section] Theorem 1.3 proves NP-hardness with one hidden layer, one data point, identity activation, sum of squares loss.
  - [corpus] Weak - corpus discusses Lipschitzness and optimization, not subset sum reductions.
- Break condition: If subset sum were in P, the reduction would fail.

### Mechanism 3
- Claim: A pseudo-polynomial algorithm exists for D-NNT on two-layer networks when dataset size is fixed.
- Mechanism: Dynamic programming tracks all possible intermediate values (bounded by M) for each neuron and data point, building up solutions layer by layer. The algorithm's runtime is polynomial in M (pseudo-polynomial in input size) and exponential in dataset size n₀.
- Core assumption: Weights and biases are natural numbers (after scaling).
- Evidence anchors:
  - [abstract] "we obtain a pseudo-polynomial algorithm for D-NNT on a two-layer network with a fixed dataset size"
  - [section] Lemma 5.3 proves correctness and runtime M^O(n₀) · d · k.
  - [corpus] Weak - no corpus papers discuss pseudo-polynomial DP for discrete training.
- Break condition: If weights/biases were arbitrary reals or if dataset size varied, runtime would not be pseudo-polynomial.

## Foundational Learning

- Concept: Complexity classes (NP, BPP, ETH)
  - Why needed here: To understand what it means for D-NNT to be "not in NP" and why ETH-based lower bounds matter.
  - Quick check question: If a problem is in NP, what does that imply about how we can verify a solution?

- Concept: Straight-line programs and PosSLP
  - Why needed here: The main hardness reduction encodes PosSLP into D-NNT, so understanding SLPs is essential.
  - Quick check question: In a straight-line program, can you ever reuse a variable's value after it's been overwritten?

- Concept: Subset sum and exact set cover problems
  - Why needed here: These classic NP-hard problems are used in the reductions proving two-layer D-NNT hardness.
  - Quick check question: If you had weights {3, 5, 7} and target 10, could you select a subset summing to exactly 10?

## Architecture Onboarding

- Component map: Input vertex → Hidden layer neurons (each with activation function) → Output vertex. Each edge has weight and bias from finite sets.
- Critical path: For D-NNT, the hardest part is verifying that a given parameter set achieves the required loss, especially in deep networks where intermediate values grow.
- Design tradeoffs: Discrete parameter spaces make verification harder (not in NP) but allow exhaustive search; continuous spaces are easier to verify but harder to optimize globally.
- Failure signatures: If your algorithm claims a solution but the loss exceeds γ, you likely miscomputed intermediate values or activation function behavior.
- First 3 experiments:
  1. Implement a tiny D-NNT instance (1 hidden neuron, 1 data point) and verify NP-hardness reduction from subset sum.
  2. Build the dynamic programming solver for fixed dataset size and test on small synthetic instances.
  3. Attempt the PosSLP reduction with a very shallow network to see why depth is crucial for the "not in NP" result.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we prove a lower bound for continuous neural network training (C-NNT) that is strictly stronger than NP-completeness, without requiring an exponential increase in output dimensions?
- Basis in paper: [explicit] The paper shows a polynomial reduction from D-NNT to C-NNT, but this increases the output dimension. The authors state it would be interesting to reproduce the lower bound of Theorem 1.1 for C-NNT with a smaller number of outputs.
- Why unresolved: The current reduction requires a large output dimension, and achieving a stronger lower bound with fewer outputs would require more structured instances with potentially larger datasets and more expressive activation functions.
- What evidence would resolve it: A polynomial reduction from D-NNT to C-NNT that maintains the same or similar output dimensions while preserving the hardness properties.

### Open Question 2
- Question: Can we develop a pseudo-polynomial lower bound for discrete neural network training (D-NNT) on deep networks, as opposed to the current NP-hardness result?
- Basis in paper: [explicit] The authors mention that pseudo-polynomial lower bounds, especially for deeper networks, would be interesting. They note that Theorem 1.1 is based on representing fast-increasing numbers in the network, but in practice numbers used in deep networks do not usually grow as fast.
- Why unresolved: Current lower bounds for deep networks are based on worst-case scenarios with rapid number growth, which may not reflect practical implementations. A pseudo-polynomial lower bound would better capture the complexity in realistic settings.
- What evidence would resolve it: A proof that D-NNT on deep networks requires time exponential in the maximum number computed by the network, even when the network depth is fixed.

### Open Question 3
- Question: Can we establish lower bounds for neural network training in a learning context, where the goal is to minimize generalization loss on unseen data rather than training loss on a static dataset?
- Basis in paper: [explicit] The authors note that while there are connections between training and learning, it would be interesting to find non-trivial assumptions on the data distribution and design analogous lower bounds to Theorem 1.1 in a learning perspective.
- Why unresolved: The current lower bounds focus on minimizing training loss on a given dataset, which is a simplification of the actual learning problem. Establishing lower bounds for generalization error would provide more insight into the fundamental limitations of neural network learning.
- What evidence would resolve it: A proof that, under certain realistic assumptions about data distribution, there is no polynomial-time algorithm that can train a neural network to achieve a certain generalization error bound.

## Limitations

- The hardness results rely on standard complexity assumptions that cannot be proven unconditionally
- The reductions use non-standard activation functions that may not correspond to practical training scenarios
- The pseudo-polynomial algorithm is limited to over-parametrized settings with fixed dataset size

## Confidence

- D-NNT ∉ NP (even for deep networks): High - follows from well-established complexity theory and explicit PosSLP reduction
- D-NNT NP-hardness for two-layer networks: High - concrete subset sum reduction with standard assumptions
- Polynomial reduction from D-NNT to C-NNT: Medium - proof details are less explicit and rely on deeper technical arguments
- Pseudo-polynomial algorithm runtime: High - explicit dynamic programming formulation with clear complexity bound

## Next Checks

1. Implement the subset sum reduction to verify NP-hardness for two-layer D-NNT instances with small parameter values
2. Test the pseudo-polynomial dynamic programming algorithm on synthetic two-layer instances with varying M and n₀ to confirm M^O(n₀) scaling
3. Attempt to construct explicit PosSLP instances encoded as deep D-NNT problems to verify the "not in NP" claim for small cases