---
ver: rpa2
title: Experiment Planning with Function Approximation
arxiv_id: '2401.05193'
source_url: https://arxiv.org/abs/2401.05193
tags:
- function
- least
- where
- policy
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the first algorithms for experiment planning\
  \ in contextual bandits with general function approximation. The authors propose\
  \ an EluderPlanner algorithm that generates a static policy sequence for data collection,\
  \ which can be used to produce an \u03B5-optimal policy with sample complexity matching\
  \ the adaptive OptLS algorithm."
---

# Experiment Planning with Function Approximation

## Quick Facts
- arXiv ID: 2401.05193
- Source URL: https://arxiv.org/abs/2401.05193
- Authors: Aldo Pacchiano; Jonathan N. Lee; Emma Brunskill
- Reference count: 40
- Key outcome: First algorithms for experiment planning in contextual bandits with general function approximation, showing fundamental gap between planning and adaptive learning

## Executive Summary
This paper addresses the problem of experiment planning in contextual bandits with general function approximation. The authors introduce EluderPlanner, an algorithm that generates a static policy sequence for data collection, achieving sample complexity matching the adaptive OptLS algorithm. They also demonstrate that uniform sampling achieves the same sample complexity as the adaptive SquareCB algorithm for small action spaces. Critically, the authors construct a structured bandit problem where adaptive learning can require significantly fewer samples than static planning, revealing a fundamental limitation of experiment planning approaches. The work provides both algorithmic contributions and theoretical insights into the statistical complexity of experiment planning.

## Method Summary
The paper introduces two main algorithmic approaches for experiment planning in contextual bandits with function approximation. EluderPlanner uses confidence intervals derived from least squares optimization to construct a static set of policies that minimize simple regret, leveraging the eluder dimension to bound uncertainty. The uniform sampling strategy directly solves least squares regression on uniformly collected data, extracting an ε-optimal policy. Both approaches assume realizability of the true reward function in the function class F and bounded noise. The authors also present a model selection algorithm with logarithmic dependence on the number of models and the complexity of the optimal model class, using a test/train split approach.

## Key Results
- EluderPlanner achieves sample complexity matching the adaptive OptLS algorithm using eluder dimension bounds
- Uniform sampling achieves the same sample complexity as SquareCB for small action spaces (|A| log|F|/ε² samples)
- Constructed example shows adaptive learning can require significantly fewer samples than static planning in structured bandit problems
- Model selection algorithm provides logarithmic dependence on number of models with sample complexity matching optimal model class

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EluderPlanner can achieve the same sample complexity as OptLS adaptive learning
- Mechanism: Uses Eluder dimension to bound uncertainty in function approximation, then constructs optimistic policies that minimize simple regret
- Core assumption: The true reward function is realizable in the function class F and bounded noise
- Evidence anchors:
  - [abstract] "The first is an eluder planning and sampling procedure that can recover optimality guarantees depending on the eluder dimension of the reward function class"
  - [section] "We introduce and analyze two experimental planning algorithms. The PlannerEluder algorithm... utilizes confidence intervals derived from least squares optimization to construct a static set of policies"
  - [corpus] Weak evidence - corpus doesn't discuss eluder dimension or function approximation in experiment planning
- Break condition: If realizability assumption fails or eluder dimension grows too quickly with complexity

### Mechanism 2
- Claim: Uniform sampling achieves the same sample complexity as SquareCB adaptive learning
- Mechanism: Directly solves least squares regression on uniformly collected data, then extracts ε-optimal policy
- Core assumption: Number of actions is small enough that uniform sampling provides sufficient exploration
- Evidence anchors:
  - [abstract] "For the second, we show that a uniform sampler achieves competitive optimality rates in the setting where the number of actions is small"
  - [section] "we show that collecting T = Ω(|A|log|F|/ε²) uniform samples is sufficient to obtain an ε-optimal policy"
  - [corpus] Weak evidence - corpus doesn't discuss uniform sampling or action-dependent sample complexity
- Break condition: When number of actions becomes large, uniform sampling becomes inefficient

### Mechanism 3
- Claim: Static planning can be fundamentally less sample-efficient than adaptive learning
- Mechanism: Constructs a structured bandit problem where adaptive algorithms can exploit problem structure while static planning cannot
- Core assumption: The problem has a specific tree-structured action space that adaptive algorithms can exploit
- Evidence anchors:
  - [abstract] "we demonstrate that there exist structured bandit problems where adaptive learning can require significantly fewer samples than static planning"
  - [section] "We present a certain structured class of bandit problems where a different adaptive algorithm can require significantly less samples"
  - [corpus] No direct evidence - corpus papers don't discuss this specific gap between planning and adaptive learning
- Break condition: When problem structure doesn't match the constructed example or when structure is unknown

## Foundational Learning

- Eluder Dimension
  - Why needed here: Provides complexity measure for function classes that enables sample complexity bounds in contextual bandits
  - Quick check question: What does it mean for a point to be ε-dependent on a sequence of previous points in the eluder dimension definition?

- Realizability Assumption
  - Why needed here: Guarantees that the true reward function exists in the function class F, enabling consistent estimation
  - Quick check question: How does realizability differ from agnostic learning in contextual bandits?

- Online-to-Batch Conversion
  - Why needed here: Allows converting cumulative regret bounds from adaptive algorithms into simple regret bounds for experiment planning
  - Quick check question: What is the key difference between cumulative regret and simple regret in bandit problems?

## Architecture Onboarding

- Component map: EluderPlanner -> Sampler -> Policy extraction -> Model selection (optional)
- Critical path: Context samples -> Policy generation -> Data collection -> Regression -> Optimal policy
- Design tradeoffs: EluderPlanner has better complexity bounds but requires more planning computation vs Uniform Sampling which is simpler but requires small action space
- Failure signatures: Suboptimal policies when eluder dimension grows too fast, poor performance with large action spaces, failure when realizability assumption violated
- First 3 experiments:
  1. Implement EluderPlanner with linear function class and verify sample complexity matches linear experiment planning
  2. Test Uniform Sampling on small action space problem and measure simple regret
  3. Construct the tree-structured problem from section 6 and compare planning vs adaptive performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the true statistical complexity of experiment planning for contextual bandits with general function approximation?
- Basis in paper: [inferred] The authors demonstrate that the eluder dimension is not the sharpest statistical complexity measure for this problem, and they identify a gap between the sample complexity of experiment planning and adaptive learning.
- Why unresolved: The paper provides a lower bound showing that adaptive learning can require significantly fewer samples than static planning, but does not identify a more accurate complexity measure or an algorithm that achieves the true lower bound.
- What evidence would resolve it: A new complexity measure that accurately characterizes the sample complexity of experiment planning, along with an algorithm that achieves the corresponding lower bound.

### Open Question 2
- Question: Can existing adaptive learning algorithms like OptLS and SquareCB be improved to achieve optimal sample complexity for experiment planning?
- Basis in paper: [explicit] The authors show that these algorithms are suboptimal for certain structured bandit problems, implying a gap between their upper bounds and the true complexity.
- Why unresolved: The paper demonstrates the suboptimality of these algorithms but does not provide improved algorithms or tighter analysis that closes the gap.
- What evidence would resolve it: Either a tighter analysis of OptLS and SquareCB that matches the lower bound for experiment planning, or a new adaptive learning algorithm with improved sample complexity guarantees for experiment planning.

### Open Question 3
- Question: How does the sample complexity of experiment planning scale with the number of models in the model selection setting?
- Basis in paper: [explicit] The authors provide a model selection algorithm with logarithmic dependence on the number of models, but it's unclear if this is optimal.
- Why unresolved: The paper presents an algorithm with logarithmic dependence but does not establish whether this is the best possible scaling or if polynomial dependence is unavoidable.
- What evidence would resolve it: Either a matching lower bound showing that logarithmic dependence on the number of models is necessary, or an algorithm with better scaling (e.g., sub-logarithmic) for model selection in experiment planning.

## Limitations
- Computational complexity of eluder dimension calculations may render the approach infeasible for complex function classes
- Realizability assumption is quite strong and may not hold in many practical settings
- Constructed example showing gap between planning and adaptive learning lacks empirical validation
- Uniform sampling strategy becomes inefficient when number of actions is large

## Confidence
- Sample complexity bounds: Medium to High (derived from established bandit techniques)
- Eluder dimension computations: Low (no implementation details provided)
- Model selection approach: Medium (sound in principle but untested)
- Gap between planning and adaptive learning: Low (theoretical construction only)

## Next Checks
1. Implement a concrete example of EluderPlanner with a simple function class (e.g., linear functions) and empirically verify the sample complexity bounds match theoretical predictions.
2. Test the uniform sampling strategy on a small-scale contextual bandit problem and measure how the simple regret scales with the number of actions |A|.
3. Construct the tree-structured bandit problem from section 6 and empirically demonstrate the sample complexity gap between the planning approach and adaptive learning algorithms.