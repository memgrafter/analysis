---
ver: rpa2
title: 'MedExQA: Medical Question Answering Benchmark with Multiple Explanations'
arxiv_id: '2406.06331'
source_url: https://arxiv.org/abs/2406.06331
tags:
- medical
- explanations
- language
- datasets
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedExQA, a novel medical question-answering
  benchmark designed to evaluate large language models' (LLMs) understanding of medical
  knowledge through explanations. The benchmark addresses a major gap in current medical
  QA benchmarks by incorporating two sets of explanations for each question-answer
  pair across five underrepresented medical specialties.
---

# MedExQA: Medical Question Answering Benchmark with Multiple Explanations

## Quick Facts
- arXiv ID: 2406.06331
- Source URL: https://arxiv.org/abs/2406.06331
- Authors: Yunsoo Kim; Jinge Wu; Yusuf Abdulle; Honghan Wu
- Reference count: 10
- Primary result: Introduces MedExQA benchmark with multiple explanations, evaluates 18 baseline models and 3 GPT APIs, proposes MedPhi-2 (2.7B) outperforming 70B medical LLMs in explanation generation

## Executive Summary
This paper introduces MedExQA, a novel medical question-answering benchmark designed to evaluate large language models' understanding of medical knowledge through explanations. The benchmark addresses a major gap in current medical QA benchmarks by incorporating two sets of explanations for each question-answer pair across five underrepresented medical specialties. The study evaluates 18 baseline open-source models and three OpenAI GPT models, demonstrating that generation evaluation with multiple explanations aligns better with human assessment. The authors also propose MedPhi-2, a new medical model based on Phi-2 (2.7B), which outperformed medical LLMs based on Llama2-70B in generating explanations.

## Method Summary
The authors created MedExQA datasets across five medical specialties with two sets of explanations per question-answer pair. They evaluated 18 baseline open-source models and 3 GPT APIs using zero-shot classification accuracy and explanation generation metrics (BLEU, ROUGE, METEOR, BERTScore). The MedPhi-2 model was trained using medical guidelines, SNOMED CT descriptions, biomedical article abstracts, Wikipedia medical terms, and PMC patient notes with pretraining and fine-tuning. Human evaluation of generated explanations was conducted by annotators with MSc degrees in health-related subjects.

## Key Results
- Classification accuracy using logits and chat generation shows model robustness across different output formats
- Generation evaluation with multiple explanations achieves higher correlation (0.9385) with human assessment compared to single explanations (0.9347)
- MedPhi-2 (2.7B) outperforms 70B medical LLMs in explanation generation metrics
- Speech Language Pathology questions posed challenges for all models, including GPT4, suggesting domain-specific knowledge gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple explanations improve alignment between automated and human assessment of model comprehension.
- Mechanism: By providing two semantically aligned yet lexically distinct explanations per question-answer pair, the benchmark captures different valid ways of expressing the same rationale, reducing false negatives in automated evaluation.
- Core assumption: The two explanations are sufficiently different to expose model's ability to generate varied, coherent rationales while still covering the same conceptual content.
- Evidence anchors:
  - [abstract] "Our results show generation evaluation with multiple explanations aligns better with human assessment"
  - [section 5.5] "When we used just one set of explanations the correlation was 0.9347, and this correlation increased to 0.9385 when we used two versions of explanations together."
  - [corpus] Weak: only 5 related papers found, none directly on multiple-explanation benchmarks.
- Break condition: If the two explanations are too similar, the added diversity is negligible and no improvement in correlation occurs.

### Mechanism 2
- Claim: Supervised fine-tuning on medical instructions improves explanation generation quality more than continued pretraining alone.
- Mechanism: Instruction-tuning teaches the model to follow structured reasoning patterns and generate coherent, context-aware explanations, whereas continued pretraining only boosts domain knowledge.
- Core assumption: The instruction-tuning datasets used (e.g., Asclepius synthetic clinical notes, AlpaCare medical self-instruct) are high quality and representative of medical explanation tasks.
- Evidence anchors:
  - [abstract] "Our model outperformed medical LLMs based on Llama2-70B in generating explanations"
  - [section 5.3] MedPhi-2 (2.7B) outperformed 70B medical LLMs in explanation metrics.
  - [section 4.2] Training pipeline includes both pretraining and finetuning with instruction datasets.
- Break condition: If the instruction datasets are noisy or misaligned with explanation task, fine-tuning may degrade performance.

### Mechanism 3
- Claim: Smaller, instruction-tuned models can outperform larger general-domain models on medical explanation tasks.
- Mechanism: Domain-specific fine-tuning compensates for parameter deficit by aligning model behavior with medical reasoning patterns.
- Core assumption: Instruction-tuning data quality and domain relevance outweigh the advantage of larger parameter counts in general-domain models.
- Evidence anchors:
  - [section 5.3] SOLAR (10.7B, general) and Yi (6B, general) performed competitively with 70B medical LLMs; MedPhi-2 (2.7B) outperformed 70B medical LLMs.
  - [section 5.2] MedPhi-2 was most robust in chat-based classification.
  - [corpus] No direct corpus evidence; inferred from benchmark results.
- Break condition: If the general-domain models receive domain fine-tuning, the performance gap may reverse.

## Foundational Learning

- Concept: Cosine similarity for measuring lexical overlap between explanation pairs.
  - Why needed here: To quantify how different the two explanations are, ensuring they are distinct yet semantically equivalent.
  - Quick check question: If two explanations have a cosine similarity of 0.9, are they sufficiently different for this benchmark?
- Concept: String matching for classification accuracy in generative models.
  - Why needed here: To evaluate whether the generated answer matches the correct choice, since logits are unavailable in chat-style outputs.
  - Quick check question: Does the presence of the answer phrase in the generated text guarantee the explanation is also correct?
- Concept: BLEU, ROUGE, METEOR, BERTScore as explanation quality metrics.
  - Why needed here: To measure lexical and semantic similarity between generated and reference explanations, capturing both surface form and meaning.
  - Quick check question: If a model gets high BERTScore but low BLEU, what does that imply about its explanations?

## Architecture Onboarding

- Component map: Data pipeline: raw collection → preprocessing (deduplication, filtering) → two-explanation validation → train/dev/test split → Model zoo: 18 baseline open-source models (various sizes) + 3 GPT APIs + MedPhi-2 → Evaluation stack: classification accuracy (logits vs chat), explanation generation metrics, human evaluation → Storage: Hugging Face model hub for weights, GitHub for code/datasets
- Critical path: Dataset creation → baseline evaluation → MedPhi-2 training → integrated evaluation → release
- Design tradeoffs:
  - Open-source vs closed-source models: open-source allows reproducibility but lags in performance
  - Single vs multiple explanations: multiple improves evaluation fidelity but doubles annotation effort
  - Zero-shot vs few-shot: zero-shot ensures fair comparison but may underutilize model capacity
- Failure signatures:
  - Low correlation between automated metrics and human scores → explanations too similar or metrics misaligned
  - Models failing on SLP consistently → domain knowledge gap in pretraining corpus
  - Performance drops in chat mode → generation artifacts or prompt sensitivity
- First 3 experiments:
  1. Run all baseline models on the development set with both logits and chat classification to identify robust models
  2. Generate explanations for a subset of dev questions and compute all four automated metrics against both reference versions
  3. Conduct human evaluation on generated explanations for top-5 baseline models to validate metric alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does training on Speech Language Pathology-specific text significantly improve LLM performance in this domain?
- Basis in paper: [explicit] The paper notes that Speech Language Pathology questions posed challenges for all models, including GPT4, and suggests this could be due to the absence of relevant text in training corpora, as this is a highly specialized field.
- Why unresolved: The paper hypothesizes that the lack of Speech Language Pathology text in training data is a key factor, but the proprietary nature of LLM training corpora makes it impossible to confirm this definitively.
- What evidence would resolve it: Training a new LLM from scratch or fine-tuning an existing model with a large, diverse corpus of Speech Language Pathology text, then evaluating its performance on the MedExQA benchmark compared to the original model.

### Open Question 2
- Question: How does the performance of MedExQA compare to other medical QA benchmarks when using different evaluation metrics (e.g., classification accuracy vs. explanation quality)?
- Basis in paper: [explicit] The paper introduces MedExQA with a focus on explanation quality and finds that generation evaluation with multiple explanations aligns better with human assessment. It also notes that classification accuracy alone may not adequately assess nuanced medical expertise.
- Why unresolved: The paper primarily evaluates MedExQA using its own proposed methodology and does not provide a direct comparison with other benchmarks using the same evaluation metrics.
- What evidence would resolve it: Evaluating other medical QA benchmarks (e.g., MedQA, MedMCQA) using the same explanation quality metrics as MedExQA and comparing the results.

### Open Question 3
- Question: What is the optimal size and architecture for medical LLMs when considering both classification accuracy and explanation quality?
- Basis in paper: [explicit] The paper finds that larger models generally perform better in classification accuracy, but smaller models like SOLAR, Yi, and Mistral show competitive performance. It also finds that MedPhi-2 (2.7B) outperforms larger medical LLMs in generating explanations.
- Why unresolved: The paper presents results for a range of model sizes but does not explore the full spectrum of possible architectures or conduct a systematic study to determine the optimal balance between size, accuracy, and explanation quality.
- What evidence would resolve it: A comprehensive study evaluating various model sizes and architectures on both classification accuracy and explanation quality metrics, potentially using techniques like neural architecture search.

## Limitations
- Limited human evaluation sample size (150 explanations) raises questions about generalizability of correlation improvements
- Benchmark focuses on five underrepresented specialties, potentially limiting broader applicability to mainstream medical domains
- Does not address potential biases in source medical textbooks used for dataset creation

## Confidence

- **High Confidence**: The core methodology of using multiple explanations per question-answer pair is well-justified and technically sound. The architectural choices for MedPhi-2, including the use of Phi-2 as a base model and the specified training pipeline, are clearly documented and reproducible.

- **Medium Confidence**: The claim that MedPhi-2 (2.7B) outperforms 70B medical LLMs in explanation generation is supported by benchmark results but requires independent validation. The human evaluation correlation improvements, while statistically present, are based on a limited sample size.

- **Low Confidence**: The assertion that instruction-tuning on synthetic clinical notes and medical self-instruct datasets is the primary driver of MedPhi-2's performance, as the paper does not provide ablation studies or detailed analysis of individual training components' contributions.

## Next Checks

1. **Replication Study**: Conduct an independent evaluation of the MedExQA benchmark using at least 500 generated explanations across multiple annotator pools to verify the correlation improvements between automated metrics and human assessment when using multiple explanations.

2. **Domain Transfer Test**: Evaluate MedPhi-2's performance on mainstream medical specialties (e.g., internal medicine, surgery) to assess the generalizability of its explanation generation capabilities beyond the five underrepresented specialties in the benchmark.

3. **Bias Analysis**: Perform a systematic analysis of potential demographic and clinical biases in the MedExQA datasets by examining the source medical textbooks' representation of diverse patient populations and clinical scenarios.