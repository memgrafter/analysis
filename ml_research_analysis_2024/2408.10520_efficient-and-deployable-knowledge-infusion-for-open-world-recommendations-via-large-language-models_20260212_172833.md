---
ver: rpa2
title: Efficient and Deployable Knowledge Infusion for Open-World Recommendations
  via Large Language Models
arxiv_id: '2408.10520'
source_url: https://arxiv.org/abs/2408.10520
tags:
- knowledge
- user
- recommendation
- llms
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REKI is a framework that augments recommender systems with open-world
  knowledge from LLMs while preserving industrial efficiency. It addresses the gap
  between closed-loop recommender systems and external world knowledge by extracting
  user preference reasoning and item factual knowledge from LLMs via factorization
  prompting.
---

# Efficient and Deployable Knowledge Infusion for Open-World Recommendations via Large Language Models

## Quick Facts
- arXiv ID: 2408.10520
- Source URL: https://arxiv.org/abs/2408.10520
- Reference count: 40
- 7% improvement in online CTR for news recommendation, 1.99% for music recommendation

## Executive Summary
REKI is a framework that augments recommender systems with open-world knowledge from LLMs while preserving industrial efficiency. It addresses the gap between closed-loop recommender systems and external world knowledge by extracting user preference reasoning and item factual knowledge from LLMs via factorization prompting. To handle large-scale scenarios, REKI employs individual and collective knowledge extraction, clustering users and items to reduce offline resource consumption. The extracted knowledge is transformed into augmented vectors using a hybridized expert-integrated network, enabling seamless integration with any conventional recommendation model. REKI has been deployed in Huawei's news and music platforms, achieving significant online improvements.

## Method Summary
REKI extracts knowledge from LLMs through factorization prompting, breaking complex reasoning into simpler sub-questions to avoid compositional gaps. For large-scale scenarios, it clusters users and items to generate shared knowledge, reducing offline resource consumption. The extracted knowledge is encoded and transformed into augmented vectors via a hybridized expert-integrated network (HEIN), which mixes shared and dedicated experts to denoise and condense LLM embeddings. These augmented vectors are then integrated into conventional recommendation models, enabling open-world knowledge infusion without sacrificing online efficiency.

## Key Results
- 7% improvement in online CTR for news recommendation and 1.99% for music recommendation
- Outperforms state-of-the-art baselines across multiple recommendation tasks in offline experiments
- Maintains low online inference latency suitable for industrial deployment

## Why This Works (Mechanism)

### Mechanism 1
Factorization prompting breaks complex reasoning into simpler sub-questions to avoid compositional gap. Decomposes user preference and item knowledge into scenario-specific factors before querying LLMs, so each factor can be answered correctly by the LLM. Core assumption: LLMs can reliably answer individual factor-based questions even if they fail on the full compositional prompt.

### Mechanism 2
Individual and collective knowledge extraction enable scalability by reducing the number of LLM calls. For small-scale scenarios, generate individual knowledge per user/item; for large-scale, cluster users/items first, then generate shared knowledge per cluster. Core assumption: Knowledge is sufficiently homogeneous within clusters to justify shared reasoning, and cluster boundaries do not cut important preference differences.

### Mechanism 3
Hybridized expert integration network (HEIN) transforms LLM semantic embeddings into recommendation-compatible vectors while reducing noise. Mixes shared and dedicated experts (MoE-style) to reduce dimensionality and denoise LLM embeddings, producing augmented vectors suitable for any CRM. Core assumption: LLM embeddings contain enough relevant signal for recommendation even after dimensionality reduction; gating mechanisms can effectively suppress noise.

## Foundational Learning

- Concept: Factorization Machines and their role in decomposing complex interactions into manageable factors.
  - Why needed here: Provides theoretical justification for breaking preference reasoning into scenario-specific factors; parallels how FM decomposes feature interactions.
  - Quick check question: What is the key idea behind factorization machines that makes them applicable to this factorization prompting strategy?

- Concept: Compositional gap in language models and its empirical evidence.
  - Why needed here: Explains why naive end-to-end prompting fails and motivates the factorization approach.
  - Quick check question: What is the compositional gap, and how does factorization prompting specifically address it?

- Concept: Streaming hierarchical clustering (StreaKHC) algorithm and its complexity properties.
  - Why needed here: Enables efficient user/item grouping in large-scale scenarios without recomputing from scratch.
  - Quick check question: What is the time complexity of StreaKHC, and why is it preferable over standard k-means for streaming data?

## Architecture Onboarding

- Component map: Factorization Prompt Generator → LLM API → Knowledge Encoder → HEIN → Augmented Vectors → CRM Backbone
- Critical path: Factorization Prompt Generator → LLM API → Knowledge Encoder → HEIN → CRM (for training/inference)
- Design tradeoffs:
  - Individual vs collective extraction: accuracy vs offline efficiency
  - Encoder choice (ChatGLM vs BERT) impacts representation quality vs speed
  - Number of experts in HEIN: more experts → more expressive but risk overfitting/noise
- Failure signatures:
  - High variance in LLM outputs → noisy augmented vectors → CRM instability
  - Poor clustering → overly generic collective knowledge → accuracy drop
  - HEIN underfitting → augmented vectors still semantically incompatible with CRM
- First 3 experiments:
  1. Baseline DIN vs REKI-I with dummy (identity) augmented vectors to confirm integration layer works
  2. REKI-I vs REKI-C on a small dataset (e.g., MovieLens-1M) to measure accuracy loss vs efficiency gain
  3. Swap knowledge encoders (ChatGLM vs BERT) in REKI-I to quantify impact of encoder quality on final AUC

## Open Questions the Paper Calls Out

### Open Question 1
How can REKI handle real-time updates to user preferences while maintaining offline knowledge generation efficiency? The paper suggests using backbone models to capture short-term preferences with timely updates, while pre-storing LLM-generated knowledge for long-term preferences, but does not provide a detailed mechanism for balancing real-time updates with offline efficiency.

### Open Question 2
How does the choice of scenario-specific factors impact the effectiveness of factorization prompting in REKI? The paper discusses the importance of identifying major scenario-specific factors but does not provide a systematic analysis of how different choices affect the quality of generated knowledge and overall performance.

### Open Question 3
Can REKI be extended to handle multi-modal data, such as images and videos, in addition to textual information? The paper focuses on leveraging LLMs for textual knowledge extraction and does not discuss the integration of multi-modal data, though the general framework could potentially be adapted for such scenarios.

## Limitations
- Lacks ablation studies on the impact of different factorization factors to determine which contribute most to performance gains
- Collective knowledge extraction relies on clustering quality but does not thoroughly validate sensitivity to clustering hyperparameters
- Does not report full pipeline latency breakdowns (including LLM API calls, encoding, and HEIN transformation), only final integration overhead

## Confidence
- High confidence: REKI outperforms baselines in offline experiments (AUC, NDCG, MAP metrics) is well-supported by results tables
- Medium confidence: Factorization prompting improves LLM knowledge extraction is logically sound but lacks direct empirical comparison to end-to-end prompting
- Medium confidence: Collective knowledge extraction is necessary for scalability is supported by theoretical efficiency argument but lacks head-to-head latency comparisons

## Next Checks
1. Conduct an ablation study varying the number and type of factorization factors to quantify their individual impact on knowledge extraction accuracy and downstream recommendation performance
2. Perform a sensitivity analysis on clustering hyperparameters (e.g., number of clusters, distance metrics) in collective extraction and measure the degradation in recommendation accuracy when clusters are suboptimal
3. Measure and report full pipeline latency (including LLM API calls, encoding, and HEIN transformation) on a large-scale dataset to verify the claimed efficiency gains over both individual and collective baselines