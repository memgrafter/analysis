---
ver: rpa2
title: Efficient LLM Scheduling by Learning to Rank
arxiv_id: '2408.15792'
source_url: https://arxiv.org/abs/2408.15792
tags:
- ranking
- requests
- length
- latency
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles head-of-line blocking in LLM inference scheduling,
  where a long request can delay short ones and increase latency. The authors show
  that while predicting exact generation lengths is hard, predicting the relative
  ranking of lengths is feasible using learning-to-rank techniques.
---

# Efficient LLM Scheduling by Learning to Rank

## Quick Facts
- arXiv ID: 2408.15792
- Source URL: https://arxiv.org/abs/2408.15792
- Authors: Yichao Fu; Siqi Zhu; Runlong Su; Aurick Qiao; Ion Stoica; Hao Zhang
- Reference count: 40
- Key result: Ranking-based scheduling reduces latency by up to 2.8× and increases throughput by 6.5× compared to FCFS

## Executive Summary
This paper addresses head-of-line blocking in LLM inference by using learning-to-rank techniques to predict relative generation lengths. Instead of predicting exact output lengths (which is infeasible), the authors train a small OPT model to predict rankings of generation lengths. This ranking information enables shortest-job-first scheduling that significantly reduces latency in chatbot serving and increases throughput in synthetic data generation. The approach integrates seamlessly with vLLM and adds minimal overhead (<2%) while preventing starvation through priority promotion mechanisms.

## Method Summary
The method trains a small OPT model (125M-350M parameters) to predict generation length rankings using ListMLE loss. The predictor scores incoming requests, which are then sorted by the scheduler to approximate shortest-job-first ordering. Batch formation respects memory constraints while maintaining the sorted order. A starvation prevention mechanism promotes long-waiting requests to higher priority. The system integrates with vLLM's continuous batching and PagedAttention infrastructure, requiring minimal modifications to existing code.

## Key Results
- Reduces average latency by 2.8× in chatbot serving compared to FCFS scheduling
- Increases synthetic data generation throughput by 6.5× over FCFS
- Achieves Kendall's Tau correlation of 0.5-0.7 between predicted and actual rankings
- Predictor overhead remains under 2% of total execution time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relative ranking of generation lengths can be predicted accurately enough to approximate shortest-job-first scheduling without needing exact length predictions.
- Mechanism: A small OPT model predicts scores for each request, which are then sorted to approximate SJF/SRTF scheduling. Higher Kendall's Tau correlation between predicted and actual rankings translates to better scheduling performance.
- Core assumption: Kendall's Tau is a reliable proxy for scheduling quality, and a small model can learn ranking patterns from training data.
- Evidence anchors: Abstract confirms ranking prediction is feasible; section 3 shows Kendall's Tau correlates with latency improvements; no corpus evidence found.

### Mechanism 2
- Claim: ListMLE loss function effectively optimizes the predictor model to improve Kendall's Tau correlation.
- Mechanism: ListMLE minimizes negative log-likelihood of correct ranking permutations, aligning with the goal of maximizing Kendall's Tau between predicted and actual rankings.
- Core assumption: Minimizing ListMLE loss will improve ordinal association measured by Kendall's Tau.
- Evidence anchors: Section 3.2 describes ListMLE optimization; section 4.2 shows -0.9 Pearson correlation between Tau and loss; no corpus evidence found.

### Mechanism 3
- Claim: Starvation prevention mechanism prevents long requests from being indefinitely delayed while maintaining latency improvements.
- Mechanism: Each request tracks a starvation count that increases when not executed. When the count exceeds a threshold, the request gets promoted with quantum time before demotion.
- Core assumption: Request-level fairness (max_waiting_time) better reflects user satisfaction than client-level approaches.
- Evidence anchors: Section 3.3 describes promotion/demotion logic; section 4.3 shows <10% latency overhead with starvation prevention; no corpus evidence found.

## Foundational Learning

- Concept: Kendall's Tau rank correlation coefficient
  - Why needed here: Measures how well predicted rankings match actual generation lengths, the core success metric for scheduling
  - Quick check question: If two rankings have 60 concordant pairs and 40 discordant pairs out of 100 total pairs, what is their Kendall's Tau?

- Concept: Learning to rank (LTR) techniques
  - Why needed here: The approach relies on training a model to predict relative rankings rather than absolute values, fundamentally different from regression
  - Quick check question: What's the key difference between pointwise, pairwise, and listwise LTR approaches, and why does this paper choose listwise?

- Concept: ListMLE loss function
  - Why needed here: The specific loss function used to train the ranking predictor, optimized for list-level ranking
  - Quick check question: How does ListMLE loss differ from pairwise ranking losses in terms of what it optimizes?

## Architecture Onboarding

- Component map: Request arrives → Predictor scores request → Scheduler inserts into priority queue → Batch formation based on memory constraints and sorted order → Execution with LLM → Result output
- Critical path: Predictor evaluation and scheduler sorting must complete within iteration boundaries to avoid performance degradation
- Design tradeoffs: Small OPT model trades prediction accuracy for computational overhead; starvation prevention trades some latency benefit for fairness
- Failure signatures: Kendall's Tau drops → latency improvements diminish; predictor overhead exceeds 2% → HOL blocking benefits negated; aggressive starvation prevention → near-FCFS behavior; memory constraint violations → system crashes
- First 3 experiments:
  1. Measure Kendall's Tau correlation between predicted and actual rankings on small test dataset
  2. Compare end-to-end latency with and without ranking scheduler on burst workload
  3. Test starvation prevention by submitting mix of short and long requests

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ranking predictor generalize to datasets with significantly different prompt and output length distributions compared to ShareGPT and LMSYS-Chat-1M?
- Basis in paper: The paper shows predictor performance degradation under distribution shifts but retains some predictive capability
- Why unresolved: Only tested on two datasets, extent of generalization unknown
- What evidence would resolve it: Experiments on wider range of datasets with varying prompt/output length distributions

### Open Question 2
- Question: What is the impact of using different sampling temperatures during generation on the predictor's performance?
- Basis in paper: Paper mentions random sampling with temperature 1.0 but notes framework insensitivity to sampling parameters
- Why unresolved: No experiments testing predictor performance across different sampling temperatures
- What evidence would resolve it: Experiments comparing predictor performance across range of sampling temperatures

### Open Question 3
- Question: How does the proposed method perform in scenarios where generation length is not correlated with prompt length, such as code generation tasks?
- Basis in paper: Paper focuses on conversational datasets where prompt and output length typically correlate
- Why unresolved: Method not evaluated on tasks where prompt and output length are uncorrelated
- What evidence would resolve it: Experiments testing method on code generation or other uncorrelated tasks

## Limitations
- Generalizability unknown for datasets beyond ShareGPT and LMSYS-Chat-1M
- Starvation prevention parameters appear tuned for specific evaluation setup
- Evaluation limited to Llama-3 models, performance with other architectures uncertain
- Predictor model size scalability for larger serving models unclear

## Confidence
**High confidence**: Core insight that relative ranking prediction is more feasible than exact length prediction; empirical demonstration of latency reduction in chatbot serving; integration with vLLM infrastructure

**Medium confidence**: Scalability claims for synthetic data generation; effectiveness of starvation prevention across diverse workloads

**Low confidence**: Approach's generalizability to non-conversational tasks; performance with different LLM architectures; multi-tenant serving environments; optimal predictor model size scaling

## Next Checks
1. **Cross-dataset generalization test**: Evaluate trained ranking predictor on diverse datasets including code generation, summarization, and non-conversational workloads
2. **Multi-tenant performance evaluation**: Test scheduling approach in multi-tenant environment with different QoS requirements and varying request arrival patterns
3. **Predictor scalability analysis**: Systematically evaluate predictor model sizes (10M-1B parameters) across different serving model scales (7B, 34B, 70B parameters) to identify optimal configurations