---
ver: rpa2
title: Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback
arxiv_id: '2410.21242'
source_url: https://arxiv.org/abs/2410.21242
tags:
- rede-rf
- retrieval
- query
- documents
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building effective dense
  retrieval systems without relevance supervision by introducing Real Document Embulsions
  from Relevance Feedback (ReDE-RF). The core method reframes hypothetical document
  generation as a relevance estimation task, using an LLM to select relevant documents
  from initial retrieval results for nearest neighbor search.
---

# Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback

## Quick Facts
- arXiv ID: 2410.21242
- Source URL: https://arxiv.org/abs/2410.21242
- Reference count: 27
- Primary result: ReDE-RF improves zero-shot dense retrieval by 6-14% over baselines while achieving 4.4-11.2x latency improvements

## Executive Summary
This paper addresses the challenge of building effective dense retrieval systems without relevance supervision. The authors propose ReDE-RF, which reframes hypothetical document generation as a relevance estimation task using an LLM to select relevant documents from initial retrieval results for nearest neighbor search. This approach eliminates the need for domain-specific LLM knowledge and reduces latency by requiring only single-token LLM outputs. The method is evaluated across multiple low-resource datasets and demonstrates consistent improvements over state-of-the-art zero-shot dense retrieval methods.

## Method Summary
ReDE-RF uses an LLM to identify relevant documents from initial retrieval results, then updates query embeddings using these relevant document embeddings for improved retrieval. The method eliminates the need for domain-specific LLM knowledge by using a generic relevance estimation prompt, reducing latency since only single-token LLM outputs are needed. Additionally, ReDE-RF can be distilled to DistillReDE, an efficient unsupervised dense retriever that improves Contriever by 33% without requiring LLM inference or index updates.

## Key Results
- ReDE-RF achieves 6-14% improvements over state-of-the-art zero-shot dense retrieval methods on low-resource datasets
- Latency improvements of 4.4-11.2x compared to baseline methods
- DistillReDE improves Contriever by 33% without requiring LLM inference or index updates

## Why This Works (Mechanism)
The paper addresses zero-shot dense retrieval by using LLM relevance feedback instead of hypothetical document generation. By reframing the problem as relevance estimation rather than generation, ReDE-RF reduces the complexity of what the LLM needs to accomplish. Instead of requiring the LLM to generate entire hypothetical documents, it only needs to identify which retrieved documents are relevant, making the task more manageable and efficient.

## Foundational Learning
- **Hybrid Retrieval (BM25 + Contriever)**: Combines sparse (BM25) and dense (Contriever) retrieval methods to improve initial document selection. Needed because neither method alone provides sufficient coverage for diverse queries.
- **LLM Relevance Feedback**: Uses an LLM to identify relevant documents from retrieved results. Needed to provide supervision signal for updating query embeddings without manual relevance annotations.
- **Query Embedding Update**: Equation 4 shows how relevant document embeddings are used to update query embeddings. Needed to transform the query representation based on relevance information.
- **Distillation Process**: Converts ReDE-RF into DistillReDE for efficient inference. Needed to eliminate LLM dependency while preserving performance gains.
- **Document Truncation (128 tokens)**: Limits document length for LLM processing. Needed to reduce computational overhead while maintaining sufficient context.
- **Nearest Neighbor Search**: Uses relevant document embeddings to update query representations. Needed to efficiently find similar documents in embedding space.

## Architecture Onboarding

Component Map: Query -> Hybrid Retrieval (BM25 + Contriever) -> Top-k Documents -> LLM Relevance Feedback -> Relevant Documents -> Updated Query Embedding -> Final Retrieval

Critical Path: The core pipeline processes queries through hybrid retrieval to obtain initial results, applies LLM relevance feedback to identify relevant documents, updates query embeddings using these documents, and performs final retrieval. The distillation process (DistillReDE) follows a separate training path from the ReDE-RF inference pipeline.

Design Tradeoffs: ReDE-RF trades initial retrieval quality for reduced LLM complexity by using only relevance estimation rather than document generation. This reduces latency but requires strong initial retrieval performance. The distillation tradeoff eliminates LLM dependency at the cost of requiring additional training.

Failure Signatures: Poor initial retrieval leads to no relevant documents found, causing ReDE-RF to default to Contriever or HyDEPRF. LLM relevance feedback producing no relevant documents indicates prompt effectiveness issues or document truncation problems. Inconsistent latency measurements suggest GPU environment or timing issues.

First Experiments:
1. Implement hybrid retrieval (BM25 + Contriever) and evaluate on TREC DL19/DL20 to verify the reported performance gap
2. Implement LLM relevance feedback with the specific prompt and document truncation, measuring both NDCG@10 and latency on A100 GPU
3. Train DistillReDE following the described distillation procedure and evaluate on BEIR datasets compared to Contriever baseline

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Performance depends on initial retrieval quality, with poor initial retrieval leading to no relevant documents found
- Specific LLM inference hyperparameters (temperature, max tokens) are not fully specified
- Latency improvements measured on specific GPU configuration (A100) may vary with different hardware
- The method requires access to an LLM, which may not be available in all deployment scenarios

## Confidence
High: Overall method performance improvements and empirical evaluation across multiple datasets with clear baselines
Medium: Exact implementation details of hybrid retrieval scoring mechanism and specific LLM inference hyperparameters
Medium: Latency improvements dependent on specific GPU configuration and may vary with different hardware setups

## Next Checks
1. Reproduce the hybrid retrieval baseline using BM25 and Contriever with the exact scoring mechanism and evaluate on TREC DL19/DL20 to verify the reported performance gap
2. Implement the ReDE-RF LLM relevance feedback pipeline with the specific prompt and document truncation, measuring both NDCG@10 and latency on the same GPU configuration (A100)
3. Train the DistillReDE model following the described distillation procedure and evaluate its performance on BEIR datasets compared to the Contriever baseline