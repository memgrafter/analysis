---
ver: rpa2
title: 'Yi: Open Foundation Models by 01.AI'
arxiv_id: '2403.04652'
source_url: https://arxiv.org/abs/2403.04652
tags:
- data
- arxiv
- performance
- language
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Yi model family consists of 6B and 34B parameter language models
  extended to chat, long-context (200K), vision-language, and depth-upscaled variants.
  Pretrained on 3.1 trillion tokens with extensive data filtering and deduplication,
  Yi achieves near GPT-3.5 performance on benchmarks like MMLU, AlpacaEval, and Chatbot
  Arena.
---

# Yi: Open Foundation Models by 01.AI

## Quick Facts
- arXiv ID: 2403.04652
- Source URL: https://arxiv.org/abs/2403.04652
- Reference count: 40
- Primary result: Yi-34B achieves 76.3 on MMLU and 94.08 win-rate on AlpacaEval, near GPT-3.5 performance

## Executive Summary
Yi is a family of open foundation models developed by 01.AI, featuring 6B and 34B parameter variants extended to chat, long-context (200K), vision-language, and depth-upscaled versions. Trained on 3.1 trillion tokens with extensive data filtering and deduplication, Yi demonstrates strong performance on academic benchmarks and human preference evaluations. The models are available under a non-commercial license and can be deployed on consumer GPUs through 4-bit quantization.

## Method Summary
Yi models are pretrained on a carefully curated corpus of 3.1 trillion tokens from English and Chinese sources, processed through a cascaded filtering pipeline that removes duplicates and low-quality content. The 34B parameter dense transformer architecture uses Grouped-Query Attention, SwiGLU activation, and RoPE with adjusted base frequency. Fine-tuning employs a small, high-quality instruction dataset (<10K examples) with manual verification. Context length is extended to 200K through lightweight continual pretraining, while vision-language capabilities are added via integration with a vision transformer encoder. Depth upscaling improves performance by duplicating middle layers and continuing pretraining.

## Key Results
- Yi-34B achieves 76.3 on MMLU and 94.08 win-rate on AlpacaEval
- 200K context length extension maintains performance
- Vision-language models integrate ViT encoders for multimodal tasks
- Models deployable on consumer GPUs via 4-bit quantization
- Near GPT-3.5 performance on human preference benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Superior data quality from cascaded filtering and deduplication leads to strong benchmark performance
- Mechanism: Multi-stage pipeline (heuristic rules → learned classifiers → cluster-based filtering → MinHash + exact-match deduplication) removes low-quality, duplicate, and unsafe content
- Core assumption: Removing noisy and redundant data increases effective model learning capacity more than scaling raw token count
- Evidence anchors: Cascaded pipeline description, comparison to prior approaches, abstract claims about data quality
- Break condition: If deduplication is too aggressive, useful semantic diversity may be lost

### Mechanism 2
- Claim: Small, meticulously polished instruction datasets outperform large-scale noisy SFT data
- Mechanism: Curating <10K high-quality, multi-turn dialog pairs iterated with human feedback produces robust alignment without memorizing low-quality patterns
- Core assumption: Quality and diversity of fine-tuning examples matter more than raw quantity for human preference alignment
- Evidence anchors: Abstract claims about small dataset, deviation from quantity-scaling approaches, iterative polishing process
- Break condition: If small dataset fails to cover key capability domains, generalization may suffer

### Mechanism 3
- Claim: Extending depth via continual pretraining improves performance without full retraining
- Mechanism: Duplicating middle layers (12-28) and continuing pretraining on 800B tokens yields better benchmark scores than base model
- Core assumption: Middle layers contain redundant representations that can be safely duplicated to increase capacity
- Evidence anchors: Abstract claims about depth extension, cosine similarity evidence showing duplicated layers' outputs nearly identical to originals
- Break condition: If duplicated layers introduce overfitting or destabilize training, gains may vanish

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Yi uses standard decoder-only Transformer with GQA, SwiGLU, and RoPE ABF; understanding these components is essential to reproduce or extend the model
  - Quick check question: What is the role of Grouped-Query Attention in Yi, and how does it differ from standard multi-head attention?

- Concept: Data filtering and deduplication techniques
  - Why needed here: Pretraining data pipeline (heuristic, learned, cluster-based filters, MinHash deduplication) is central to Yi's performance; engineers must know how to tune or rebuild it
  - Quick check question: How does MinHash deduplication work, and why is it combined with exact-match filtering?

- Concept: Instruction fine-tuning and human feedback alignment
  - Why needed here: Yi's chat model is aligned via small, high-quality SFT dataset iterated with user feedback; understanding this process is key to reproducing alignment
  - Quick check question: What are main steps in Yi's iterative SFT data polishing process, and how does it differ from large-scale instruction tuning?

## Architecture Onboarding

- Component map: Base Yi model → Chat fine-tuning → Long-context extension (200K) → Vision-language (ViT encoder + projection module) → Depth upscaling (layer duplication + continued pretraining)
- Critical path: Pretraining (3.1T tokens, cascaded filtering) → Supervised fine-tuning (small curated dataset) → Model extension (context, vision, depth)
- Design tradeoffs: Smaller models with higher-quality data vs larger models with more data; aggressive deduplication vs retaining semantic diversity; small SFT datasets vs large-scale noisy datasets
- Failure signatures: Low MMLU/CMMLU scores may indicate insufficient data quality; poor human preference scores may indicate inadequate SFT data diversity or quality; long-context failures may indicate incomplete continual pretraining
- First 3 experiments:
  1. Re-run Yi's pretraining pipeline on subset of corpus with/without deduplication to quantify quality impact
  2. Compare small curated SFT (≤10K) vs large noisy SFT on AlpacaEval/Chatbot Arena to confirm data quality advantage
  3. Test depth upscaling by duplicating layers and continuing pretraining for few steps to observe performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Yi model family exhibit emergent abilities independent of scaling, or are they primarily function of increased model size?
- Basis in paper: [explicit] Paper discusses in-context learning capability of Yi-34B, comparing to smaller models and noting only larger models achieve meaningful exact match on complex tasks
- Why unresolved: While paper provides evidence of improved performance with increased model size, it doesn't definitively establish whether improvements are due to emergent abilities or simply more parameters
- What evidence would resolve it: Systematic study comparing performance of Yi models of different sizes on range of tasks, controlling for other factors such as data quality and training time

### Open Question 2
- Question: What is optimal balance between data quantity and data quality for training large language models like Yi?
- Basis in paper: [explicit] Paper emphasizes importance of data quality while mentioning training on larger amount than Chinchilla optimal delivers clear performance gain
- Why unresolved: Paper doesn't provide quantitative analysis of trade-off between data quantity and quality, nor clear guideline for determining optimal balance
- What evidence would resolve it: Controlled experiment varying amount and quality of training data while keeping other factors constant

### Open Question 3
- Question: How does Yi model family perform on tasks requiring complex reasoning and problem-solving abilities?
- Basis in paper: [inferred] While paper reports strong performance on various benchmarks, it acknowledges open-source LLMs still lag behind GPT-4 and GPT-3.5 on various benchmarks, particularly mathematics and coding
- Why unresolved: Paper doesn't provide detailed analysis of Yi model's performance on tasks requiring complex reasoning and problem-solving, such as solving novel mathematical problems or writing complex code
- What evidence would resolve it: Comprehensive evaluation of Yi model family on range of tasks requiring complex reasoning and problem-solving, including both existing benchmarks and novel tasks

## Limitations
- Lack of detailed quantitative comparisons against baseline models with identical evaluation protocols
- Cascaded filtering pipeline specifications remain underspecified (exact thresholds, configurations)
- Small SFT dataset (<10K examples) raises questions about statistical significance
- Vision-language extension combines features from multiple architectures without clear attribution of performance contributions

## Confidence
- High Confidence: Strong benchmark performance, cascaded filtering pipeline, small curated SFT datasets, depth upscaling feasibility
- Medium Confidence: Data quality as primary performance driver, optimal filtering configuration, vision-language architectural patterns, 200K context maintenance
- Low Confidence: "Near GPT-3.5 performance" on all benchmarks, exact quantitative impact of filtering stages, statistical significance of small SFT results, specific contribution of depth upscaling

## Next Checks
1. Direct Baseline Comparison: Re-run Yi-34B and GPT-3.5 on identical hardware with same seeds across MMLU, AlpacaEval, and HumanEval to establish true performance gaps

2. Filtering Pipeline Ablation Study: Train identical models using Yi's corpus with progressive removal of filtering stages to quantify marginal benefit of each component

3. SFT Dataset Size Scaling: Systematically vary SFT dataset size (100, 1K, 5K, 10K examples) while maintaining quality standards to determine relationship between dataset size and performance