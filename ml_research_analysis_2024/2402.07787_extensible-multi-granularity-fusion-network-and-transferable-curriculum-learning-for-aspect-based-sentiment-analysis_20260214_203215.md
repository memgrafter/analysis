---
ver: rpa2
title: Extensible Multi-Granularity Fusion Network and Transferable Curriculum Learning
  for Aspect-based Sentiment Analysis
arxiv_id: '2402.07787'
source_url: https://arxiv.org/abs/2402.07787
tags:
- sentiment
- computational
- dependency
- features
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes EMGF, an extensible multi-granularity fusion
  network for aspect-based sentiment analysis. It addresses the problem of integrating
  diverse linguistic and structural features into a unified, scalable framework.
---

# Extensible Multi-Granularity Fusion Network and Transferable Curriculum Learning for Aspect-based Sentiment Analysis

## Quick Facts
- arXiv ID: 2402.07787
- Source URL: https://arxiv.org/abs/2402.07787
- Reference count: 16
- Primary result: EMGF+CL outperforms state-of-the-art ABSA models on SemEval 2014, Twitter, and MAMS datasets

## Executive Summary
This paper proposes EMGF (Extensible Multi-Granularity Fusion network), a novel approach for aspect-based sentiment analysis that integrates multiple linguistic and structural features into a unified framework. The model combines dependency syntax, constituent syntax, attention-based semantics, and external knowledge graphs using multi-anchor triplet learning and orthogonal projection. Additionally, it introduces the first task-specific curriculum learning framework for text-only ABSA, training models from easy to hard examples to improve generalization. Experiments demonstrate consistent performance improvements over state-of-the-art methods across four benchmark datasets.

## Method Summary
EMGF consists of three main modules: Text Encoding using BERT for contextual embeddings, Granularity Feature Construction using four GCN-based modules (dependency syntax, constituent syntax, semantic attention, and knowledge graphs), and Extensible Multi-Stage Fusion that combines these features through residual connections. The fusion stage employs multi-anchor triplet learning to enable mutual learning between syntactic views and orthogonal projection to decorrelate syntactic and semantic features. The curriculum learning component assigns difficulty scores using five indicators (sentiment word, distance, degree adverb, negation, punctuation) and trains the model sequentially from easy to hard examples.

## Key Results
- EMGF+CL achieves state-of-the-art performance on Laptop and Restaurant datasets from SemEval2014 Task 4
- Significant improvements on Twitter and MAMS datasets compared to baseline models
- Ablation studies confirm effectiveness of multi-granularity fusion and curriculum learning components
- Model demonstrates strong generalization capabilities with transferable curriculum learning framework

## Why This Works (Mechanism)

### Mechanism 1
Multi-anchor triplet learning enables mutual learning between dependency and constituent syntax by designating "anchor," "pos," and "neg" nodes to capture complementary syntactic information. The method selects top-K important nodes as anchors, then pulls anchors closer to homologous "pos" nodes while pushing them away from "neg" nodes across dependency and constituent views. This forces the two syntactic representations to learn complementary aspects. Core assumption: Homologous nodes in different syntactic views carry semantically equivalent information that can be mutually reinforced.

### Mechanism 2
Orthogonal projection techniques purify syntactic features by removing semantic overlap, creating more discriminative representations. The method projects dependency syntax onto semantic features, then subtracts this projection to obtain syntax-only components, and repeats for constituent syntax. This ensures syntactic and semantic features are decorrelated. Core assumption: Syntactic and semantic features contain overlapping information that reduces discriminative power when combined directly.

### Mechanism 3
Extensible multi-stage fusion with residual connections enables cumulative effects from multiple granularity features without increasing model complexity. The EMSF blocks cascade through preprocessing (triplet learning + projection) and fusion stages, with residual connections allowing each block to build upon previous fusions. The final representation averages outputs across all blocks. Core assumption: Each additional granularity feature provides incremental information that can be effectively combined through sequential fusion.

## Foundational Learning

- **Graph Neural Networks (GNNs) for syntactic structure encoding**: Needed to understand how GCNs aggregate information from graph structures. Quick check: What is the difference between message passing in GCNs versus GATs, and when would you choose one over the other for syntactic trees?

- **Attention mechanisms and multi-head attention**: Required to understand semantic graph construction and important node selection. Quick check: How does average pooling vs maximum pooling of attention scores affect the selection of important nodes in the anchor selection process?

- **Curriculum learning and difficulty scoring**: Essential for implementing the task-specific curriculum learning framework. Quick check: What are common difficulty indicators for curriculum learning in text classification tasks, and how do you normalize them for effective ordering?

## Architecture Onboarding

- **Component map**: Raw text → BERT → Four GCN modules → EMSF preprocessing (triplet learning + projection) → EMSF fusion → Average pooling → Classification

- **Critical path**: Raw text → BERT → Four GCN modules → EMSF preprocessing (triplet learning + projection) → EMSF fusion → Average pooling → Classification

- **Design tradeoffs**: More GCN layers vs computational cost (uses different layer counts: 3, 3, 9, 3), Number of EMSF blocks (le=6) vs performance gain, Top-K anchor selection vs coverage

- **Failure signatures**: Poor performance on neutral sentiment (data imbalance), Slow training with large K values (excessive computational cost), Degradation with fewer granularity features (M3, M2, M1 perform worse than M4)

- **First 3 experiments**:
  1. Verify each GCN module produces expected outputs by checking node representations and attention scores on a small example sentence
  2. Test the anchor selection mechanism by running multi-head S-pool on attention matrix and verifying Top-K selection matches expectations
  3. Validate the orthogonal projection by checking that projected syntactic features have reduced correlation with semantic features (correlation coefficient < 0.1)

## Open Questions the Paper Calls Out

- **Scalability with increasing features**: The paper claims EMGF can fuse arbitrary numbers of features at low computational cost, but doesn't provide detailed analysis of scalability. Evidence needed: Experiments with varying numbers of integrated features and analysis of performance/computational efficiency at each scale.

- **Anchor node selection in challenging scenarios**: The paper mentions challenges with neutral sentiment due to lack of strong sentiment words and data imbalance, but doesn't explain how multi-anchor triplet learning handles these cases. Evidence needed: Experiments with datasets containing varying sentiment word strength and data imbalance, analyzing model performance in these scenarios.

- **Effectiveness of orthogonal projection**: While the paper uses orthogonal projection to decorrelate syntactic and semantic features, it doesn't detail how this specifically contributes to differentiation between information types. Evidence needed: Experiments comparing performance with and without orthogonal projection, analyzing impact on model's ability to differentiate between syntactic and semantic information.

## Limitations

- The approach may not scale effectively to longer documents beyond the sentence-level focus demonstrated
- Performance appears sensitive to hyperparameter choices (K values, number of EMSF blocks) without clear theoretical guidance
- Reliance on external knowledge graphs limits generalizability when such resources are unavailable

## Confidence

- **Multi-granularity fusion effectiveness**: Medium-High - Strong ablation study evidence, but architectural choices appear somewhat arbitrary
- **Curriculum learning framework**: Medium - Innovative approach but lacks comparison to established curriculum learning strategies
- **Scalability claims**: Low - Limited experimental evidence for handling increasing numbers of features

## Next Checks

1. Verify the orthogonality constraint effectiveness by measuring feature correlation before and after projection on held-out validation data
2. Test the anchor selection mechanism with different K values to determine sensitivity of performance to this hyperparameter
3. Evaluate the model's ability to generalize to out-of-domain aspect terms not present in training data