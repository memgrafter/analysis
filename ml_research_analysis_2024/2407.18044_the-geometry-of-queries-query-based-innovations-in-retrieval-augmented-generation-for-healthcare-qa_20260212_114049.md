---
ver: rpa2
title: 'The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented Generation
  for Healthcare QA'
arxiv_id: '2407.18044'
source_url: https://arxiv.org/abs/2407.18044
tags:
- content
- questions
- answer
- qb-rag
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Query-Based Retrieval Augmented Generation
  (QB-RAG), a framework for enhancing RAG systems in healthcare question-answering
  by pre-aligning user queries with a database of curated, answerable questions derived
  from healthcare content. A key component of QB-RAG is an LLM-based filtering mechanism
  that ensures that only relevant and answerable questions are included in the database,
  enabling reliable reference query generation at scale.
---

# The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented Generation for Healthcare QA

## Quick Facts
- arXiv ID: 2407.18044
- Source URL: https://arxiv.org/abs/2407.18044
- Authors: Eric Yang; Jonathan Amar; Jong Ha Lee; Bhawesh Kumar; Yugang Jia
- Reference count: 26
- Key outcome: QB-RAG nearly doubles the exact recovery rate compared to other methods, from 45% to 89% when retrieving a single document

## Executive Summary
This work introduces Query-Based Retrieval Augmented Generation (QB-RAG), a framework for enhancing RAG systems in healthcare question-answering by pre-aligning user queries with a database of curated, answerable questions derived from healthcare content. A key component of QB-RAG is an LLM-based filtering mechanism that ensures that only relevant and answerable questions are included in the database, enabling reliable reference query generation at scale. Our empirical evaluation on a healthcare dataset demonstrates the superior performance of QB-RAG compared to existing retrieval methods, highlighting its practical value in building trustworthy digital health applications for health question-answering. QB-RAG improves both retrieval and answer quality, as measured by faithfulness, relevance, and guideline adherence.

## Method Summary
QB-RAG transforms each content document into multiple answerable questions, forming a query-centric knowledge base. When a user query arrives, it is matched to the most similar pre-generated question rather than directly to documents, leveraging semantic alignment within the query space. The process involves offline question generation using an LLM, answerability filtering using another LLM to vet each generated question, and online retrieval using cosine similarity in embedding space. The method aims to improve both retrieval accuracy and answer quality while reducing inference latency compared to online query rewriting approaches.

## Key Results
- QB-RAG nearly doubles the exact recovery rate compared to other methods, from 45% to 89% when retrieving a single document
- Improves both retrieval and answer quality, as measured by faithfulness, relevance, and guideline adherence
- Achieves 90% agreement between model and clinical experts on answerability judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-generating answerable questions from content improves retrieval by creating a query-aligned knowledge base
- Mechanism: QB-RAG transforms each content document into multiple answerable questions, forming a query-centric knowledge base. When a user query arrives, it is matched to the most similar pre-generated question rather than directly to documents, leveraging semantic alignment within the query space
- Core assumption: Generated questions accurately capture the semantic intent and content coverage of their source documents, and the embedding space aligns well enough for reliable nearest-neighbor retrieval
- Evidence anchors:
  - [abstract] "pre-aligning user queries with a database of curated, answerable questions derived from healthcare content"
  - [section] "Our core contribution is an automated, LLM-powered filter that vets each generated question for clarity and answerability, ensuring a reliable, query-centric knowledge base."
  - [corpus] Weak evidence: no directly comparable prior work found, but related methods (doc2query, QuIM-RAG) share conceptual similarity
- Break condition: If the question generation or answerability filtering produces low-quality or unrepresentative questions, or if the embedding space poorly aligns queries with their corresponding answerable questions, retrieval accuracy will degrade

### Mechanism 2
- Claim: Offline query-space alignment reduces inference latency compared to online query rewriting methods
- Mechanism: QB-RAG performs all computationally expensive LLM calls (question generation and filtering) offline, so online retrieval only requires efficient embedding similarity search, avoiding serial LLM invocation during user queries
- Core assumption: The cost of offline generation is acceptable and the retrieval step remains fast enough for real-time applications
- Evidence anchors:
  - [abstract] "enabling reliable reference query generation at scale"
  - [section] "In contrast to methods relying on online LLM calls for query rewriting, QB-RAG shifts this computational burden offline."
  - [corpus] No direct latency comparison found in corpus, but inference cost reduction is a common motivation in retrieval literature
- Break condition: If the offline question set becomes too large or the embedding search becomes inefficient, latency gains could be lost

### Mechanism 3
- Claim: LLM-based answerability filtering improves the quality of the question base by removing irrelevant or unanswerable questions
- Mechanism: For each generated question-content pair, an LLM evaluates whether the content can answer the question, filtering out mismatches before deployment
- Core assumption: The LLM can reliably judge answerability, and its judgments correlate well with human expert assessments
- Evidence anchors:
  - [abstract] "An LLM-based filtering mechanism that ensures that only relevant and answerable questions are included in the database"
  - [section] "Our LLM-based filtering mechanism that vets each generated question for clarity and answerability, ensuring a reliable, query-centric knowledge base."
  - [section] "90% of the pairs received the same answerability rating" (between model and clinical experts)
- Break condition: If the answerability model's accuracy drops or domain shifts occur, filtered questions may no longer reflect answerable content, harming retrieval

## Foundational Learning

- Concept: Semantic similarity in embedding spaces
  - Why needed here: QB-RAG relies on embedding similarity to match user queries to pre-generated questions; understanding how embeddings capture meaning is essential
  - Quick check question: If two questions have high cosine similarity in embedding space, what does that imply about their semantic relationship?

- Concept: Retrieval metrics (exact recovery, relevance)
  - Why needed here: The paper evaluates retrieval quality using exact recovery and auto-evaluator relevancy; understanding these metrics is necessary to interpret results
  - Quick check question: What is the difference between exact recovery rate and auto-evaluator relevancy rate?

- Concept: LLM-based evaluation and auto-annotation
  - Why needed here: The paper uses LLMs to judge answer quality and answerability; understanding the strengths and limitations of LLM evaluation is important for interpreting findings
  - Quick check question: What is a potential risk of relying solely on LLM-based auto-evaluation for answer quality?

## Architecture Onboarding

- Component map: Content base → Offline question generation (LLM) → Answerability filtering (LLM) → Question database → Online query embedding → Nearest-neighbor search → Retrieve content → LLM answer generation
- Critical path: User query → Embedding → Similarity search → Retrieve content → Answer generation → Output
- Design tradeoffs: Comprehensive question coverage vs. storage/compute cost; offline generation cost vs. online latency; embedding quality vs. retrieval accuracy
- Failure signatures: Low exact recovery rates; high answer decline rates; poor correlation between retrieval improvements and answer quality gains
- First 3 experiments:
  1. Generate answerable questions for a small sample of content and validate filtering accuracy against human judgments
  2. Compare retrieval performance (exact recovery, relevancy) of QB-RAG vs. naive RAG on a test set of paraphrased questions
  3. Measure end-to-end answer quality (faithfulness, relevance, guideline adherence) for both methods

## Open Questions the Paper Calls Out

- How does the performance of QB-RAG compare when using different LLM models for query generation and filtering?
- How does QB-RAG perform on knowledge bases with different characteristics, such as longer documents, more diverse topics, or less structured content?
- How does the computational cost of QB-RAG compare to other RAG methods, especially when considering the offline question generation and filtering process?
- How does QB-RAG handle queries that are not answerable by the knowledge base?

## Limitations

- The method's performance on out-of-domain questions or in languages other than English is unclear
- The knowledge base is static after generation, so it may not adapt to evolving medical knowledge or user query patterns without retraining
- The paper does not provide detailed analysis of the computational cost of QB-RAG compared to other RAG methods

## Confidence

- **High confidence**: The mechanism of pre-generating answerable questions and using an LLM-based filter to improve retrieval accuracy is well-supported by the paper's empirical results and aligns with established practices in RAG literature
- **Medium confidence**: The claim of reduced inference latency is plausible given the offline generation approach, but lacks direct empirical validation or comparison to baselines
- **Low confidence**: The generalizability of the method to out-of-domain queries and other languages is not supported by the paper's evaluation, which focuses on a specific healthcare domain

## Next Checks

1. Evaluate QB-RAG's performance on a diverse set of questions from different medical specialties or general domains to assess its generalizability and identify potential coverage gaps
2. Measure and compare the end-to-end latency and computational cost of QB-RAG against online query rewriting methods like RARR on the same hardware and dataset
3. Conduct an ablation study to quantify the impact of the LLM-based answerability filtering on retrieval accuracy and answer quality, by comparing performance with and without the filter