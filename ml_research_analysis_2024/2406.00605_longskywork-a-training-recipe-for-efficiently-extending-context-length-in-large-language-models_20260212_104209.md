---
ver: rpa2
title: 'LongSkywork: A Training Recipe for Efficiently Extending Context Length in
  Large Language Models'
arxiv_id: '2406.00605'
source_url: https://arxiv.org/abs/2406.00605
tags:
- long-context
- data
- context
- arxiv
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LongSkywork addresses the challenge of efficiently extending context
  length in large language models (LLMs) to handle up to 200,000 tokens. The key method
  involves a four-stage training recipe: standard pretraining, long-context pretraining
  with Chunk Interleaved Pretraining (CIP), standard supervised fine-tuning (SFT),
  and long-context SFT with synthetic data generation.'
---

# LongSkywork: A Training Recipe for Efficiently Extending Context Length in Large Language Models

## Quick Facts
- arXiv ID: 2406.00605
- Source URL: https://arxiv.org/abs/2406.00605
- Reference count: 9
- Key result: Achieves perfect Needle Test accuracy and outperforms GPT-4-128K and Claude2.1-200K on long-context retrieval tasks

## Executive Summary
LongSkywork addresses the challenge of efficiently extending context length in large language models (LLMs) to handle up to 200,000 tokens. The key method involves a four-stage training recipe: standard pretraining, long-context pretraining with Chunk Interleaved Pretraining (CIP), standard supervised fine-tuning (SFT), and long-context SFT with synthetic data generation. CIP reorganizes short documents into interleaved long sequences to enhance long-range dependency learning, while synthetic data methods (SynL) generate task-specific long-context question pairs to overcome data scarcity. The approach achieves perfect accuracy on the Needle Test for long-context retrieval and outperforms GPT-4-128K and Claude2.1-200K in three retrieval-based tasks in InfiniteBench. LongSkywork-13B matches Claude2.1 in real-world evaluations despite having significantly fewer parameters, demonstrating effective long-context comprehension and reasoning capabilities.

## Method Summary
LongSkywork employs a four-stage training recipe to extend context length in LLMs. First, standard pretraining is conducted on short-context data. Second, long-context pretraining uses Chunk Interleaved Pretraining (CIP), which reorganizes short documents into interleaved long sequences to enhance long-range dependency learning. Third, standard supervised fine-tuning (SFT) aligns the model to human preferences with a 4K context window. Fourth, long-context SFT uses synthetic data generated via the Synthetic Lengthy Tables (SynL) method, combining 20% normal SFT data with 80% synthetic long-context SFT data. The model uses NTK-aware RoPE positional embedding scaling with base frequency adjusted from 10000 to 260000 for 200K context extension.

## Key Results
- Achieves perfect accuracy on the Needle Test for long-context retrieval
- Outperforms GPT-4-128K and Claude2.1-200K on three retrieval-based tasks in InfiniteBench
- LongSkywork-13B matches Claude2.1 performance in real-world Tiangong AI Reading benchmark despite having significantly fewer parameters
- Synthetic long-context SFT data outperforms human-curated data in some cases

## Why This Works (Mechanism)

### Mechanism 1
Chunk Interleaved Pretraining (CIP) improves long-context retrieval by forcing the model to learn long-range dependencies during pretraining. CIP reorganizes short documents into interleaved chunks so that tokens from different documents are intermixed. This compels the model to attend across previously distant tokens, strengthening its ability to retrieve information from far-apart positions. The interleaving structure is necessary to trigger the model's attention across long distances.

### Mechanism 2
A dedicated long-context SFT stage after standard SFT is critical for strong long-context retrieval and comprehension. The long-context SFT stage trains on long-context synthetic data (e.g., synthetic tables) that require complex reasoning over extensive input, improving the model's ability to retrieve and understand long-form information. Short-context SFT alone does not provide sufficient exposure to long-form reasoning tasks; explicit long-context training is needed to extend capability.

### Mechanism 3
Synthetic long-context SFT data can outperform human-curated data for long-context tasks. Programmatically generated synthetic tables with varying lengths and task types (retrieval, reasoning, sorting) provide rich, scalable training signals without manual annotation. Synthetic data can simulate the complexity and variability of real long-context scenarios, and the model can learn robust patterns from them.

## Foundational Learning

- Concept: Long-context modeling via RoPE positional embeddings
  - Why needed here: RoPE allows scaling to long sequences by rotating token representations; understanding its interpolation/extrapolation is key to extending context length.
  - Quick check question: What happens to RoPE when the context length is extended beyond the training range, and how does NTK-aware scaling help?

- Concept: Supervised fine-tuning (SFT) alignment
  - Why needed here: SFT aligns the base model to human preferences and formats; separating short- and long-context SFT preserves alignment while enabling long-context specialization.
  - Quick check question: Why does mixing short and long SFT samples in one stage reduce long-context performance?

- Concept: Synthetic data generation for training efficiency
  - Why needed here: Manual annotation for long-context tasks is expensive; synthetic data provides scalable, diverse training signals.
  - Quick check question: How do you ensure synthetic data diversity and complexity without manual curation?

## Architecture Onboarding

- Component map: Base LLM (e.g., Skywork-13B) → Long-context pretraining (CIP) → Short-context SFT → Long-context SFT (synthetic data) → RoPE positional encoding tuned for extended context

- Critical path:
  1. CIP data preparation (chunking + interleaving)
  2. Long-context pretraining (few hundred iterations)
  3. Short-context SFT (standard alignment)
  4. Synthetic long-context SFT data generation
  5. Long-context SFT (few hundred iterations)

- Design tradeoffs:
  - CIP vs. concatenation: CIP improves long-range attention but may slightly degrade short-sequence modeling.
  - Synthetic vs. human data: Synthetic is scalable but must be carefully designed to avoid distribution shift.
  - Short vs. long SFT separation: Preserves alignment efficiency but requires two SFT stages.

- Failure signatures:
  - Long-context retrieval fails → likely CIP or long-context SFT insufficient
  - Short-context tasks degrade → CIP may be too aggressive
  - Synthetic data leads to poor generalization → synthetic data too narrow or unrealistic

- First 3 experiments:
  1. Train a small model with CIP vs. concatenation on a short corpus; evaluate retrieval on 64K context.
  2. Train with only long-context SFT vs. mixed short/long SFT; measure long-context retrieval vs. alignment retention.
  3. Generate synthetic table data with varying lengths; evaluate model reasoning and retrieval performance.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal ratio of synthetic long-context SFT data to true long-context SFT data for maximizing model performance?
- Basis in paper: The paper mentions using 40% synthetic long-context SFT data and 40% true long-context SFT data in LongSkywork-13B-S&T, but suggests this ratio may be imprecise.
- Why unresolved: The paper indicates that finding the right data mixing ratio is crucial but does not provide a systematic exploration of different ratios.
- What evidence would resolve it: Conducting experiments with various ratios of synthetic to true long-context SFT data and comparing the resulting model performance on benchmarks like InfiniteBench.

### Open Question 2
How does the performance of LongSkywork models scale with increasing model size beyond 13B parameters?
- Basis in paper: The paper only reports results for models up to 13B parameters, but does not explore scaling to larger model sizes.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the training recipe with 13B parameter models, but does not investigate the impact of scaling to larger model sizes.
- What evidence would resolve it: Training and evaluating LongSkywork models with varying parameter sizes (e.g., 30B, 70B) and comparing their performance on long-context benchmarks and real-world tasks.

### Open Question 3
Can the synthetic data generation methods be extended to create long-context data for tasks beyond information retrieval and table-based reasoning?
- Basis in paper: The paper describes synthetic data generation methods for information retrieval and table-based reasoning tasks, but does not explore their applicability to other task types.
- Why unresolved: The paper demonstrates the effectiveness of synthetic data for specific task types but does not investigate its potential for generating data for other long-context tasks, such as summarization or complex reasoning.
- What evidence would resolve it: Developing and evaluating synthetic data generation methods for a wider range of long-context tasks and assessing their impact on model performance across these tasks.

## Limitations
- The exact synthetic data generation pipeline remains underspecified, particularly the Faker-based table generation and task template design
- The interleaving mechanism in Chunk Interleaved Pretraining (CIP) is described but not empirically validated against simpler concatenation approaches
- The paper reports perfect Needle Test performance but does not provide error analysis or failure case studies for real-world long-context scenarios

## Confidence
- High confidence: The four-stage training recipe framework is clearly defined and the results on standard benchmarks (InfiniteBench, Tiangong AI Reading) are reproducible given access to the model checkpoints
- Medium confidence: The mechanism by which CIP improves long-context retrieval is plausible but not rigorously proven; the synthetic data advantage over human-curated data needs independent validation
- Low confidence: Claims about NTK-aware RoPE scaling and its interaction with extended context length are mentioned but not thoroughly explained or validated

## Next Checks
1. Implement and compare CIP against simple document concatenation on a small-scale model to verify the claimed advantage in long-range dependency learning
2. Generate a controlled synthetic dataset using the described SynL methodology and compare model performance against human-curated long-context data on the same tasks
3. Conduct ablation studies on the long-context SFT stage to determine whether the two-stage SFT approach (short then long) is truly necessary, or if a mixed SFT approach could achieve similar results