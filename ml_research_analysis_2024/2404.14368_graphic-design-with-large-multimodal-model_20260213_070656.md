---
ver: rpa2
title: Graphic Design with Large Multimodal Model
arxiv_id: '2404.14368'
source_url: https://arxiv.org/abs/2404.14368
tags:
- arxiv
- graphist
- design
- graphic
- layout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Graphist, the first Large Multimodal Model
  (LMM) for the task of Hierarchical Layout Generation (HLG). HLG addresses the limitation
  of previous Graphic Layout Generation (GLG) methods by generating layouts from unordered
  sets of design elements, providing greater creative flexibility.
---

# Graphic Design with Large Multimodal Model

## Quick Facts
- arXiv ID: 2404.14368
- Source URL: https://arxiv.org/abs/2404.14368
- Authors: Yutao Cheng; Zhao Zhang; Maoke Yang; Hui Nie; Chunyuan Li; Xinglong Wu; Jie Shao
- Reference count: 40
- Key outcome: Introduces Graphist, the first Large Multimodal Model for Hierarchical Layout Generation (HLG), achieving state-of-the-art performance on both Crello and CGL-V2 datasets with new metrics IOPR and GPT-4V Eval

## Executive Summary
This paper presents Graphist, a novel Large Multimodal Model designed for Hierarchical Layout Generation (HLG) in graphic design. Unlike traditional Graphic Layout Generation methods that require ordered input sequences, Graphist accepts unordered sets of design elements and generates optimal layer arrangements. The model achieves superior performance on established datasets while introducing new evaluation metrics that better capture layer ordering accuracy and aesthetic quality.

## Method Summary
Graphist reframes HLG as a sequence generation problem, taking RGB-A images as input and outputting JSON specifications for design layouts. The architecture consists of three components: an RGBA-Encoder (ViT-L/14 with four-channel input), a Visual Shrinker that compresses visual features, and a Large Language Model (LLM). The model is trained in three progressive stages, starting with basic visual-linguistic alignment, advancing to HLG-specific understanding, and finally expanding to broader graphic design tasks including traditional GLG.

## Key Results
- Graphist achieves state-of-the-art performance on Crello and CGL-V2 datasets for HLG
- The model outperforms traditional GLG methods while providing greater creative flexibility with unordered inputs
- New metrics (IOPR and GPT-4V Eval) demonstrate superior layer ordering accuracy and aesthetic quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reframing HLG as sequence generation with unordered inputs allows the model to learn flexible layer ordering without requiring pre-specified sequences.
- Mechanism: By training on randomly shuffled design elements with 0.75 probability, the model learns to infer optimal layering from visual and contextual cues rather than relying on user-provided order.
- Core assumption: The optimal layer order can be inferred from multimodal inputs without explicit ordering signals.
- Evidence anchors: The model efficiently reframes HLG as a sequence generation problem, utilizing RGB-A images as input; random shuffling with 0.75 probability trains the model to arrange inputs layer order and spatial coordinates.

### Mechanism 2
- Claim: Using RGB-A images with alpha channels provides crucial information for distinguishing text from backgrounds and understanding layer transparency relationships.
- Mechanism: The alpha channel allows the model to isolate text elements from complex backgrounds, enabling better placement decisions and maintaining text readability.
- Core assumption: Alpha channel information is essential for proper text-background separation and transparency handling.
- Evidence anchors: Inputs with additional alpha channel yield higher-quality outputs with more accurate layer ordering compared to RGB inputs; alpha channel allows the model to isolate text from potentially distracting backgrounds.

### Mechanism 3
- Claim: The three-stage training strategy progressively builds model capabilities from basic visual understanding to complex layout reasoning.
- Mechanism: Stage 1 calibrates the visual encoder to interpret alpha channels and align visual-linguistic features; Stage 2 emphasizes HLG understanding while maintaining captioning skills; Stage 3 expands to GLG and other variants.
- Core assumption: Progressive training from simple to complex tasks enables better model adaptation and performance.
- Evidence anchors: Stage 1 targets patch embedding and projector layers; Stage 2 expands to projector layer and full LLM; Stage 3 maintains focus on projector layer and complete LLM but aims to adapt to broader graphic design task types.

## Foundational Learning

- Concept: Multimodal representation learning with large language models
  - Why needed here: The model must process diverse input types (images, text, coordinates) and generate structured JSON output, requiring unified multimodal understanding
  - Quick check question: How does the Visual Shrinker transform the ViT output for LLM compatibility, and why is this compression necessary?

- Concept: Coordinate encoding in natural language sequences
  - Why needed here: Spatial positioning must be represented as tokens that the LLM can generate, requiring a coordinate-to-token mapping strategy
  - Quick check question: What advantages does numerical coordinate representation have over special coordinate tokens in the vocabulary?

- Concept: Diffusion-based evaluation metrics for aesthetic quality
  - Why needed here: Traditional pixel-based metrics don't capture design aesthetics; GPT-4V evaluation provides human-aligned quality assessment
  - Quick check question: How do the SDL, SGI, SIO, and ST V metrics differ in their evaluation focus, and why is this multi-metric approach necessary?

## Architecture Onboarding

- Component map: RGBA-Encoder (ViT-L/14 with 224x224 four-channel input) → Visual Shrinker (2D average pooling + MLP) → LLM (Qwen1.5-0.5B/7B) → JSON output
- Critical path: Input design elements → RGBA encoding → Visual feature compression → LLM token generation → JSON layout specification
- Design tradeoffs: Larger models (Graphist-Base) provide better performance but require more compute; Visual Shrinker reduces computational cost but may lose fine-grained spatial details
- Failure signatures: Poor layer ordering (high IOPR scores), text readability issues (low ST V scores), or element overlap problems indicate model limitations
- First 3 experiments:
  1. Test different Visual Shrinker pooling kernel sizes (2x2 vs 4x4) to evaluate impact on spatial precision vs computational efficiency
  2. Compare RGB vs RGB-A input performance on a small subset to validate alpha channel importance
  3. Evaluate different LLM sizes (0.5B vs 7B) on Crello validation set to establish performance/compute tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Graphist be extended to handle more complex design tasks beyond poster and advertisement layouts, such as UI/UX design or 3D modeling?
- Basis in paper: The paper demonstrates Graphist's effectiveness on Crello and CGL-V2 datasets, which primarily focus on 2D graphic layouts, but does not explore more complex design domains.
- Why unresolved: The current evaluation is limited to specific graphic design tasks, and the paper does not discuss the model's potential application to other design domains.
- What evidence would resolve it: Experiments applying Graphist to UI/UX design or 3D modeling tasks, along with appropriate evaluation metrics, would demonstrate its versatility in handling more complex design challenges.

### Open Question 2
- Question: How does the performance of Graphist scale with increasing model size and dataset size?
- Basis in paper: The paper mentions that Graphist was trained on different datasets and model sizes, but does not provide a comprehensive analysis of the scaling behavior.
- Why unresolved: While the paper shows that larger models (e.g., Qwen1.5-7B) outperform smaller ones, it does not explore the relationship between model size, dataset size, and performance in detail.
- What evidence would resolve it: Systematic experiments varying model size and dataset size, along with analysis of performance trends, would provide insights into the scaling properties of Graphist.

### Open Question 3
- Question: Can Graphist be adapted to handle real-time design tasks, such as interactive layout generation or dynamic content updates?
- Basis in paper: The paper focuses on generating static graphic compositions, but does not discuss the model's potential for real-time applications.
- Why unresolved: The current implementation of Graphist is designed for batch processing, and the paper does not explore its capabilities for interactive or dynamic design tasks.
- What evidence would resolve it: Experiments demonstrating Graphist's performance in real-time design scenarios, along with appropriate metrics for interactivity and responsiveness, would show its potential for practical applications.

## Limitations
- The evaluation relies on newly introduced metrics (IOPR, GPT-4V Eval) that haven't been validated across the broader research community
- The Visual Shrinker's compression from 16x16+1 tokens to just 5 tokens may lose important spatial details for complex layouts
- Computational costs increase significantly with model size, raising questions about practical deployment

## Confidence
**High Confidence**: The architectural framework of Graphist (RGBA-Encoder → Visual Shrinker → LLM) is well-specified and technically sound. The three-stage training approach is clearly described, and the use of alpha channels for better text-background separation is empirically supported.

**Medium Confidence**: The reported performance improvements over baselines are based on newly introduced metrics that haven't been validated across the broader research community. While the model shows state-of-the-art results on the tested datasets, the metrics themselves may not fully capture real-world design quality or user satisfaction.

**Low Confidence**: The scalability claims are limited by the computational costs mentioned. The paper acknowledges that larger models achieve better performance but doesn't provide a thorough analysis of the performance/compute tradeoff or explore whether the Visual Shrinker's compression strategy becomes more problematic at larger scales.

## Next Checks
1. **Alpha Channel Dependency Test**: Create a controlled experiment comparing Graphist's performance on RGB vs RGB-A inputs across varying design complexity levels (simple text-only layouts vs complex multi-element compositions with transparency). Measure both IOPR and GPT-4V scores to quantify the exact contribution of alpha channel information.

2. **Visual Shrinker Ablation Study**: Test different pooling configurations (2x2, 4x4, 8x8 kernels) and compare their impact on layout accuracy metrics. Evaluate whether the current 2x2 pooling strikes the optimal balance between computational efficiency and spatial precision, particularly for layouts with dense element arrangements.

3. **Real-World Design Validation**: Conduct user studies comparing Graphist outputs against human designers on practical design tasks from the Crello dataset. Measure both objective metrics (IOPR, ST V) and subjective assessments of aesthetic quality, functionality, and creative flexibility to validate whether the model's performance translates to real-world utility.