---
ver: rpa2
title: LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating
  Output Format Bias of LLMs
arxiv_id: '2408.08656'
source_url: https://arxiv.org/abs/2408.08656
tags:
- format
- bias
- formats
- chatgpt
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates format bias in large language
  models (LLMs), where performance varies significantly across different output formats.
  The authors propose a new framework to reliably evaluate LLM performance under format
  constraints by distinguishing between format-following scores and actual performance.
---

# LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating Output Format Bias of LLMs

## Quick Facts
- arXiv ID: 2408.08656
- Source URL: https://arxiv.org/abs/2408.08656
- Reference count: 40
- Key outcome: LLMs show substantial performance variance across output formats, with BiasFo values ranging from 4.10% to 353.80%, and three mitigation strategies successfully reduced ChatGPT's format bias from 235.33%² to 0.71%² on MMLU.

## Executive Summary
This paper systematically investigates format bias in large language models, where performance varies significantly across different output formats. The authors propose a new framework to reliably evaluate LLM performance under format constraints by distinguishing between format-following scores and actual performance. They introduce a metric to quantify format bias and evaluate it across 15 widely-used formats spanning four categories: multiple-choice answers, wrapping, lists, and mappings.

Testing on eight tasks with three state-of-the-art models reveals substantial performance variance across formats. The authors find that format-instruction following capabilities significantly impact bias and propose three mitigation strategies: demonstration-based prompting, repeated format instructions, and fine-tuning with synthesized format data. Their methods successfully reduce performance variance, demonstrating effective bias reduction.

## Method Summary
The authors develop a comprehensive framework to evaluate and mitigate format bias in LLMs. They first establish a reliable evaluation methodology that separates format-following scores from actual performance metrics. The framework introduces the BiasFo metric to quantify performance variance across different output formats. For mitigation, they propose three strategies: demonstration-based prompting (showing examples of desired format), repeated format instructions (emphasizing format requirements), and fine-tuning with synthesized format data. The evaluation spans 15 output formats across four categories, tested on eight diverse tasks using Gemma-7B-it, Mistral-7B-it-v0.2, and ChatGPT models.

## Key Results
- Performance variance across formats shows BiasFo values ranging from 4.10% to 353.80% across tested models and tasks
- ChatGPT's performance variance in wrapping formats reduced from 235.33%² to 0.71%² on MMLU using mitigation strategies
- Format-instruction following capabilities significantly impact observed format bias
- Three mitigation strategies (demonstration-based prompting, repeated instructions, fine-tuning) effectively reduce format bias

## Why This Works (Mechanism)
The effectiveness of the proposed methods stems from addressing the core issue of format bias through multiple intervention points. Demonstration-based prompting works by providing concrete examples that guide the model's generation process, reducing ambiguity in format interpretation. Repeated format instructions reinforce the importance of format compliance, making it more likely the model prioritizes format adherence. Fine-tuning with synthesized format data directly trains the model to handle various formats, improving its format-following capabilities. The framework's success lies in its ability to isolate format bias from actual performance, allowing for targeted interventions that address the specific problem of format variance rather than task performance itself.

## Foundational Learning

**Format bias in LLMs** - Why needed: Understanding how different output formats affect LLM performance is crucial for reliable deployment across applications. Quick check: Compare model performance across at least 3 different output formats on the same task.

**BiasFo metric** - Why needed: Provides quantitative measure of format bias to enable systematic evaluation and comparison. Quick check: Calculate BiasFo values for a model across different format categories.

**Format-following vs performance scores** - Why needed: Distinguishing between whether a model follows format correctly versus whether it provides correct answers is essential for proper evaluation. Quick check: Evaluate both format compliance and answer correctness separately for each format.

**Demonstration-based prompting** - Why needed: Shows how providing examples can guide model behavior and reduce format variance. Quick check: Compare performance with and without demonstration examples for complex formats.

**Synthetic data fine-tuning** - Why needed: Enables targeted training for format handling without requiring large amounts of real labeled data. Quick check: Measure format bias reduction after fine-tuning with synthetic format examples.

## Architecture Onboarding

**Component map**: Format evaluation framework -> BiasFo calculation -> Format instruction generation -> Model prompting -> Performance measurement -> Mitigation strategy application

**Critical path**: Task definition → Format selection → Instruction generation → Model inference → Format compliance checking → Performance evaluation → Bias quantification → Mitigation application

**Design tradeoffs**: The framework trades computational efficiency for comprehensive evaluation by testing multiple formats, versus focusing on single format optimization. The mitigation strategies balance prompt complexity against performance gains.

**Failure signatures**: High format bias without corresponding task performance issues indicates format-following problems. Inconsistent format compliance across similar formats suggests model confusion about format requirements.

**First experiments**:
1. Baseline evaluation of format bias across 3-5 formats on a single task
2. Apply demonstration-based prompting to reduce bias on highest-variance format
3. Test repeated instruction strategy on wrapping format category

## Open Questions the Paper Calls Out

None specified in the source material.

## Limitations

- Study focuses on only three models and eight tasks, limiting generalizability across the broader landscape of LLMs and applications
- Synthetic data used for fine-tuning raises questions about real-world applicability of mitigation strategies
- Framework relies on specific assumptions about what constitutes format bias, which may not capture all aspects of format-related performance issues

## Confidence

- High confidence in the existence of format bias across tested models and tasks
- Medium confidence in the effectiveness of proposed mitigation strategies
- Medium confidence in the generalizability of findings beyond tested conditions

## Next Checks

1. Replicate the format bias evaluation across a broader range of models (including larger models) and tasks to test generalizability
2. Validate mitigation strategies using real-world datasets rather than synthetic data to assess practical applicability
3. Conduct ablation studies to isolate the impact of format instruction quality versus model capabilities on observed bias