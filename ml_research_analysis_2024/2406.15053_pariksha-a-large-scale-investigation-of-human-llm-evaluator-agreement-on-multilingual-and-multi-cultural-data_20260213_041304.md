---
ver: rpa2
title: 'PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on
  Multilingual and Multi-Cultural Data'
arxiv_id: '2406.15053'
source_url: https://arxiv.org/abs/2406.15053
tags:
- human
- evaluation
- llama-3
- language
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates 30 large language models across 10 Indian
  languages using both human and LLM-based evaluators, conducting 90K human evaluations
  and 30K LLM-based evaluations. The research finds that models like GPT-4o and Llama-3
  70B consistently perform best for most Indic languages, while Llama-3 8B ranks in
  the middle and open-source models like Llama-2 7B, Mistral 7B, and Gemma 7B score
  at the bottom.
---

# PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data

## Quick Facts
- arXiv ID: 2406.15053
- Source URL: https://arxiv.org/abs/2406.15053
- Reference count: 32
- Primary result: Humans and LLMs agree better on pairwise comparisons than direct assessments for multilingual evaluation

## Executive Summary
This study conducts the largest investigation to date of human-LLM evaluator agreement across 10 Indian languages, using 90K human evaluations and 30K LLM-based evaluations. The research systematically evaluates 30 large language models on general and culturally nuanced prompts, revealing significant differences in agreement patterns between pairwise and direct assessment evaluation methods. The findings highlight the challenges of multilingual evaluation, including self-bias in LLM evaluators and the need for human-in-the-loop systems, particularly for culturally nuanced content and lower-resource languages like Bengali and Odia.

## Method Summary
The study employs a two-phase evaluation approach: (1) pairwise comparison where evaluators choose between two responses, and (2) direct assessment where responses are rated on linguistic acceptability, task quality, and hallucination detection. Both methods are evaluated by human annotators and a GPT-4-32K LLM evaluator. The evaluation covers 30 models across 10 Indian languages using 20 prompts per language (5 health, 5 finance, 10 culturally nuanced). Results are aggregated using Elo rating systems for pairwise comparisons and mean scores for direct assessment.

## Key Results
- GPT-4o and Llama-3 70B consistently perform best for most Indic languages, while open-source models like Llama-2 7B, Mistral 7B, and Gemma 7B score at the bottom
- Human and LLM evaluators show high agreement in pairwise settings but lower agreement in direct assessment, especially for Bengali and Odia
- GPT-based evaluator exhibits self-bias, ranking its own outputs higher than human evaluators
- LLM evaluators tend to be overly optimistic, giving higher scores and failing to detect hallucinations as effectively as humans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human and LLM evaluators achieve higher agreement on pairwise comparisons than direct assessments because pairwise evaluation reduces cognitive load and aligns with human preference structures.
- Mechanism: Pairwise comparison simplifies decision-making by forcing a binary choice between two options, whereas direct assessment requires evaluating multiple metrics independently. Humans find binary choices more intuitive and less cognitively demanding than scoring along multiple dimensions. LLMs also perform better on pairwise tasks as they can more reliably identify relative quality without needing to assign absolute scores.
- Core assumption: The cognitive load difference between binary choices and multi-metric scoring affects agreement rates.
- Evidence anchors:
  - [abstract] "We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation"
  - [section 4.4.2] "LLM evaluators tend to be more decisive and chooses fewer ties compared to humans which is along the lines of Wu and Aji (2023)"
  - [corpus] Weak evidence - related papers discuss grading scale impact but not specifically cognitive load differences
- Break condition: If evaluation prompts become too simple or if metrics are highly correlated, the advantage of pairwise evaluation may diminish.

### Mechanism 2
- Claim: LLM evaluators show self-bias toward their own outputs, particularly GPT models favoring GPT-generated responses.
- Mechanism: LLMs exhibit self-preference when evaluating their own outputs due to training on similar patterns and internal consistency preferences. This manifests as higher Elo ratings for their own generations compared to human evaluators. The effect is strongest in GPT models due to their training data and architectural tendencies.
- Core assumption: LLMs have learned patterns that make them favor outputs similar to their own generation style.
- Evidence anchors:
  - [section 4.4.4] "We check for self-bias by the GPT evaluator towards its own outputs across both types of evaluation. We calculate the average rank of GPT-4 in the Elo leaderboard given by both the evaluators as well as the rank of all other models which were evaluated for atleast 8-10 languages. We find that of the 11 selected models, the average rank of GPT-4 increases by the highest amount (1.4 places) for evaluations performed by the GPT evaluator"
  - [corpus] Moderate evidence - "Grading Scale Impact on LLM-as-a-Judge" discusses human-LLM alignment issues
- Break condition: If evaluation prompts are highly structured or if evaluators are explicitly instructed to avoid self-preference.

### Mechanism 3
- Claim: LLM evaluators consistently score higher than human evaluators on direct assessment metrics, showing an overly optimistic bias.
- Mechanism: LLMs tend to be more lenient in scoring because they lack human judgment about what constitutes "good" quality and may not detect subtle issues like grammatical errors or cultural nuances. They also struggle with hallucination detection, giving higher scores even when outputs contain factual errors.
- Core assumption: LLMs have different quality thresholds than humans due to their training objectives and limitations.
- Evidence anchors:
  - [section 4.4.2] "We observe that LLM evaluators fail to detect the hallucinations as well as tend to give higher scores for LA and TQ. This shows the overly optimistic nature of LLMs"
  - [section 4.4.2] "LLM evaluator on the other hand tends to pick one response even if both responses are gibberish or contain hallucinations"
  - [corpus] Weak evidence - related papers discuss LLM evaluation but not specifically optimism bias
- Break condition: If evaluation rubrics become more specific or if LLMs are fine-tuned on human preference data.

## Foundational Learning

- Concept: Elo rating systems and Bradley-Terry models
  - Why needed here: Understanding how pairwise comparisons are converted to rankings is crucial for interpreting leaderboard results and agreement analysis
  - Quick check question: How does the Bradley-Terry model estimate win rates between LLMs compared to standard Elo?

- Concept: Inter-annotator agreement metrics (Fleiss Kappa, Percentage Agreement)
  - Why needed here: These metrics are essential for quantifying agreement between human evaluators and between humans and LLMs
  - Quick check question: What does a Fleiss Kappa score of 0.54 indicate about human-human agreement in pairwise evaluation?

- Concept: Multilingual evaluation challenges
  - Why needed here: Understanding linguistic diversity, cultural nuances, and contamination issues is crucial for interpreting evaluation results across different languages
  - Quick check question: Why might Bengali and Odia show lower agreement between human and LLM evaluators compared to other languages?

## Architecture Onboarding

- Component map: Prompt curation -> Model response generation -> Human evaluation (pairwise + direct assessment) -> LLM evaluation -> Leaderboard construction -> Bias analysis
- Critical path: Prompt generation -> Response collection -> Human annotation (90K datapoints) -> LLM evaluation (30K datapoints) -> Agreement analysis -> Leaderboard creation
- Design tradeoffs:
  - Human evaluation provides gold standard but is expensive (90K annotations)
  - LLM evaluation scales better but shows biases and lower agreement
  - Direct assessment provides detailed metrics but lower agreement than pairwise
  - Cultural prompts capture nuances but reduce evaluator agreement
- Failure signatures:
  - Low Fleiss Kappa scores indicate evaluation task difficulty or poor guidelines
  - Large discrepancies between human and LLM rankings suggest systematic bias
  - High variance in Elo ratings indicates unstable evaluation
  - LLM consistently giving high scores suggests optimism bias
- First 3 experiments:
  1. Run pilot pairwise evaluations on a small subset of prompts to validate annotation guidelines and estimate human workload
  2. Test LLM evaluator consistency by duplicating 10% of pairwise comparisons with flipped options to measure position bias
  3. Conduct ablation study comparing agreement rates on cultural vs non-cultural prompts to quantify cultural nuance impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would LLM evaluators perform on culturally nuanced prompts in languages not included in this study?
- Basis in paper: [inferred] The paper found lower agreement between human and LLM evaluators on culturally nuanced prompts, particularly for Bengali and Odia, suggesting language-specific cultural understanding gaps.
- Why unresolved: The study only included 10 Indian languages, limiting generalizability to other languages and cultures.
- What evidence would resolve it: Evaluating LLM evaluators on culturally nuanced prompts in a broader range of languages would reveal if the observed trends hold across different linguistic and cultural contexts.

### Open Question 2
- Question: Can fine-tuning open-source models like Llama-3 70B with Indic language data improve their performance to match or exceed proprietary models?
- Basis in paper: [explicit] The paper suggests that fine-tuned versions of Llama-3 70B could be promising for future evaluation.
- Why unresolved: The study evaluated base Llama-3 models, not their fine-tuned counterparts, leaving the potential impact of fine-tuning unexplored.
- What evidence would resolve it: Fine-tuning Llama-3 70B and similar models with Indic language data and evaluating their performance against proprietary models would provide a definitive answer.

### Open Question 3
- Question: What are the specific factors contributing to the LLM evaluator's tendency to be overly optimistic and fail to detect hallucinations?
- Basis in paper: [explicit] The paper observed that LLM evaluators gave higher scores and were less likely to detect hallucinations compared to human evaluators.
- Why unresolved: The study identified the issue but did not investigate the underlying causes, such as differences in training data or evaluation methodology.
- What evidence would resolve it: Analyzing the LLM evaluator's training data, evaluation process, and comparing it to human evaluation methods could reveal the factors driving the observed biases.

## Limitations
- The study relies on a single LLM evaluator (GPT-4-32K) which may not generalize to other evaluator models
- Evaluation was conducted primarily by native speakers from India, potentially missing global perspectives on cultural nuances
- The study does not account for potential temporal variations in model performance or evaluator consistency

## Confidence
- Core finding on agreement variation: High confidence
- Self-bias identification: Medium confidence
- Language-specific performance variations: Medium confidence
- Open-source model ranking: Medium confidence

## Next Checks
1. Replicate self-bias analysis using multiple LLM evaluators (GPT-4, Claude, Llama-3) to determine if the observed bias is specific to GPT models or a general LLM phenomenon
2. Conduct cross-cultural validation by having evaluators from different cultural backgrounds (non-Indian native speakers) evaluate the same prompts to test the robustness of cultural nuance detection
3. Perform temporal stability analysis by re-running evaluations on a subset of prompts with the same models after 3-6 months to assess whether model rankings and human-LLM agreement patterns remain consistent over time