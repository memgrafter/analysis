---
ver: rpa2
title: Label Propagation Training Schemes for Physics-Informed Neural Networks and
  Gaussian Processes
arxiv_id: '2404.05817'
source_url: https://arxiv.org/abs/2404.05817
tags:
- points
- pinn
- training
- pigp
- co-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces semi-supervised learning methods for physics-informed
  machine learning, focusing on self-training and co-training for Physics-Informed
  Neural Networks (PINNs) and Physics-Informed Gaussian Processes (PIGPs). The key
  idea is to progressively label unlabeled collocation points during training based
  on prediction fidelity, either using the model itself (self-training) or another
  model (co-training).
---

# Label Propagation Training Schemes for Physics-Informed Neural Networks and Gaussian Processes

## Quick Facts
- arXiv ID: 2404.05817
- Source URL: https://arxiv.org/abs/2404.05817
- Reference count: 4
- Primary result: Introduces semi-supervised learning methods (self-training and co-training) for PINNs and PIGPs to improve accuracy and convergence by propagating information from boundary/initial conditions into the domain.

## Executive Summary
This paper proposes semi-supervised learning approaches to improve the training of physics-informed neural networks (PINNs) and physics-informed Gaussian processes (PIGPs). The core idea is to iteratively label unlabeled collocation points based on prediction fidelity, using either the model itself (self-training) or another model (co-training). For PINNs, fidelity is assessed via proximity to labeled data and PDE residue; for PIGPs, it is based on posterior variance. These methods address the common failure mode in PINNs where information from boundary and initial conditions fails to propagate into the domain. Numerical experiments on viscous Burgers, Allen-Cahn, and Helmholtz equations demonstrate significant improvements in accuracy and convergence.

## Method Summary
The paper introduces self-training and co-training for physics-informed machine learning. Self-training involves progressively labeling unlabeled collocation points during PINN training based on proximity to labeled data and PDE residue, or for PIGPs, based on posterior variance. Co-training alternates between PINN and PIGP models, using each other's predictions on collocation points to generate pseudo-labels. This approach leverages the strengths of both PINNs (flexibility) and PIGPs (uncertainty quantification via closed-form posterior variance) to guide the selection of reliable pseudo-labels. The methods aim to reduce reliance on dense collocation sampling by concentrating training on well-behaved regions.

## Key Results
- Self-training and co-training significantly improve PINN accuracy and convergence on viscous Burgers, Allen-Cahn, and Helmholtz equations.
- PIGP's closed-form posterior variance provides a natural fidelity metric for self-training, avoiding heuristic thresholds.
- Hybrid PINN-PIGP co-training enables uncertainty quantification in PINN predictions.
- The methods reduce the imbalance between data-fitting and residual losses, a common training challenge for PINNs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semi-supervised label propagation improves PINN training by reducing reliance on dense collocation sampling.
- Mechanism: Points with low PDE residual and high proximity to labeled data are progressively added to the training set, concentrating gradient descent effort on well-behaved regions.
- Core assumption: The PDE solution is smooth enough that points near known values and with small residuals are likely to be close to the true solution.
- Evidence anchors:
  - [abstract]: "self-training of physics-informed neural networks...based on prediction fidelity...proximity to labeled data and PDE residue"
  - [section]: "The difficulty of the training process...is caused by the fact that the collocation points within the domain are not associated with a direct (or noisy) function value"
- Break condition: If the PDE has discontinuities or sharp gradients, proximity and residual thresholds may admit points far from the true solution, causing error amplification.

### Mechanism 2
- Claim: Co-training leverages mutual agreement between PINN and PIGP to guide pseudo-label selection.
- Mechanism: PINN predicts on test points; PIGP accepts those with low residual and low posterior variance as pseudo-labels, and vice versa. Disagreement signals uncertainty.
- Core assumption: PINN and PIGP, though different approximators, share a common solution manifold for the same PDE.
- Evidence anchors:
  - [abstract]: "integration of the two via co-training...one make reliable predictions on the unlabeled points used by the other"
  - [section]: "Co-training has the added benefit of having the PIGP provide reliable uncertainty quantification metrics for the predictions made by the co-trained PINN"
- Break condition: If PINN and PIGP bias toward different solution regions (e.g., low vs high frequency), their agreement can be spurious.

### Mechanism 3
- Claim: PIGP's closed-form posterior variance naturally quantifies fidelity for self-training.
- Mechanism: Points with low predictive variance are deemed reliable and added to training; this avoids heuristic residual thresholds.
- Core assumption: PIGP's Gaussian assumption yields a valid posterior variance that correlates with prediction error.
- Evidence anchors:
  - [abstract]: "For PIGPs, it is based on posterior variance"
  - [section]: "In the case of PIGPs, the criterion of fidelity is readily available, consisting of the variance of the posterior distribution"
- Break condition: If the true solution is non-Gaussian or heteroscedastic, posterior variance may not track error.

## Foundational Learning

- Concept: Semi-supervised learning and pseudo-labeling
  - Why needed here: The method treats collocation points as unlabeled data and iteratively labels them based on model confidence.
  - Quick check question: What is the main difference between self-training and co-training in semi-supervised learning?

- Concept: Physics-informed loss formulation
  - Why needed here: Understanding how PINNs combine data-fitting and residual terms is critical to grasp why label propagation helps.
  - Quick check question: In a PINN loss, what roles do collocation points and boundary points play?

- Concept: Gaussian process regression and posterior variance
  - Why needed here: PIGP uses GP posterior variance as a natural fidelity metric for label propagation.
  - Quick check question: How does a GP posterior variance relate to prediction uncertainty?

## Architecture Onboarding

- Component map: Training loop -> Loss computation (data + residual) -> Pseudo-label selection (proximity/residual/variance) -> Update training set -> Next epoch
- Critical path: 1) Compute PDE residual on test points. 2) Select points below thresholds. 3) Add to labeled set. 4) Retrain model. Loop until convergence or max iterations.
- Design tradeoffs: Self-training is simpler but relies on a single model's confidence; co-training is more robust but doubles computational cost. PIGP offers closed-form variance but may be slower to train than PINN for large datasets.
- Failure signatures: (a) Pseudo-labels drift from true solution if residual threshold too lax. (b) Training stalls if thresholds too strict. (c) Co-training diverges if PINN and PIGP disagree strongly.
- First 3 experiments:
  1. Implement PINN self-training on a 1D heat equation with fixed thresholds; monitor L2 error vs iteration.
  2. Replace self-training with co-training using a simple GP; compare convergence speed.
  3. For Allen-Cahn, enable the co-training mode and check if high-frequency components improve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of kernel in PIGPs affect the convergence and accuracy of semi-supervised learning, particularly for stiff PDEs?
- Basis in paper: [explicit] The paper mentions that the kernel choice is critical for PIGP performance, especially for stiff PDEs like the viscous Burgers equation, and that a suitable kernel can significantly impact the results.
- Why unresolved: The paper uses specific kernels for different experiments but does not explore the broader impact of kernel selection or provide a systematic study of how different kernels affect convergence and accuracy.
- What evidence would resolve it: A comparative study of different kernel functions (e.g., RBF, Mat√©rn, periodic) across various PDE types and stiffness levels, showing their impact on convergence rates and prediction accuracy in self-training and co-training scenarios.

### Open Question 2
- Question: What are the theoretical bounds on the improvement in accuracy when using self-training or co-training compared to standard PINN or PIGP training?
- Basis in paper: [inferred] The paper demonstrates improved accuracy with self-training and co-training but does not provide theoretical guarantees or bounds on the extent of improvement.
- Why unresolved: While empirical results show gains, the paper lacks a theoretical framework to quantify the expected improvement or convergence guarantees for these semi-supervised methods.
- What evidence would resolve it: Mathematical proofs or error bounds showing how self-training and co-training reduce the generalization error compared to baseline PINN/PIGP, possibly leveraging PAC-Bayes or Rademacher complexity analysis.

### Open Question 3
- Question: How does the selection criterion for pseudo-labeled points (e.g., residual threshold, proximity, posterior variance) impact the stability and performance of the semi-supervised training process?
- Basis in paper: [explicit] The paper uses different fidelity criteria for PINNs (residual and proximity) and PIGPs (posterior variance), but does not analyze how varying these thresholds affects training dynamics or final accuracy.
- Why unresolved: The experiments use fixed thresholds without exploring sensitivity to these hyperparameters or the consequences of overly aggressive or conservative labeling.
- What evidence would resolve it: A sensitivity analysis varying the fidelity thresholds and measuring their effect on convergence speed, final accuracy, and the risk of propagating incorrect labels, possibly including adaptive threshold strategies.

### Open Question 4
- Question: Can the semi-supervised label propagation approach be extended to inverse problems or uncertainty quantification in PDE solutions beyond what is demonstrated?
- Basis in paper: [explicit] The paper mentions that the co-trained PIGP provides uncertainty quantification for PINN predictions, but does not explore inverse problems or more advanced UQ applications.
- Why unresolved: The focus is on forward PDE solving, and the potential for applying these methods to parameter estimation, inverse design, or robust prediction under uncertainty is not investigated.
- What evidence would resolve it: Experiments applying self-training and co-training to inverse problems (e.g., inferring PDE coefficients or initial conditions) and comparing UQ performance with standard methods, possibly in the context of Bayesian inference or robust optimization.

## Limitations

- The exact implementation details of fidelity thresholds for pseudo-label selection are not fully specified, which could affect reproducibility.
- The computational cost of co-training and its scalability for larger problems is not discussed in depth.
- The assumption that PINN and PIGP solutions lie on a common manifold may not hold for all PDEs, particularly those with sharp gradients or discontinuities.

## Confidence

- High: The core mechanism of using posterior variance for PIGP self-training and the hybrid PINN-PIGP co-training approach.
- Medium: The effectiveness of the proposed methods across all tested PDEs, particularly the Helmholtz equation where no prior PINN results are available.
- Low: The claim that co-training provides robust uncertainty quantification, as this depends heavily on the agreement between PINN and PIGP models.

## Next Checks

1. Test the sensitivity of self-training to different residual and proximity thresholds on a simple PDE to determine optimal values.
2. Compare co-training performance against traditional PINN training on problems with known solutions to quantify improvement.
3. Evaluate the scalability of co-training by applying it to a 2D PDE problem and measuring computational overhead.