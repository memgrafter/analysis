---
ver: rpa2
title: 'From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling
  Autonomous Navigation with Reinforcement Learning and no Libraries'
arxiv_id: '2403.18219'
source_url: https://arxiv.org/abs/2403.18219
tags:
- action
- self
- state
- learning
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines reinforcement learning agent performance in
  2D and 3D environments, using a Q-learning algorithm implemented without pre-made
  libraries. The research compares agent adaptation and learning trajectories in both
  spatial settings.
---

# From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries

## Quick Facts
- arXiv ID: 2403.18219
- Source URL: https://arxiv.org/abs/2403.18219
- Reference count: 0
- Primary result: Q-learning agent required ~22x more episodes to stabilize learning when moving from 2D (65 episodes) to 3D (1450 episodes) environments

## Executive Summary
This study compares reinforcement learning agent performance in 2D and 3D grid environments using a Q-learning algorithm implemented without pre-made libraries. The research examines agent adaptation and learning trajectories across spatial dimensions, revealing that scaling from 2D to 3D environments significantly increases computational requirements. The agent demonstrated improved performance over training in both environments, achieving goals with decreasing step counts, but the transition to 3D demanded substantially more episodes for learning stabilization.

## Method Summary
The study implements a tabular Q-learning agent with epsilon-greedy action selection to navigate grid-based environments. The 2D environment uses a 50x50 grid with four actions (up, down, left, right), while the 3D environment uses a 50x50x50 grid with six actions (adding forward and backward). Both environments use identical hyperparameters: learning rate 0.5, discount factor 0.5, and epsilon 0.2 for exploration. Agents are trained for 500 episodes in 2D and 5000 episodes in 3D, with maximum 20,000 steps per episode, tracking rewards and steps to evaluate learning progress.

## Key Results
- In the 2D environment (50x50 grid), the agent stabilized learning after approximately 65 episodes, achieving goals in about 107 steps
- In the 3D environment (50x50x50 grid), stabilization required roughly 1450 episodes and 163 steps
- The transition from 2D to 3D demanded approximately 22 times more episodes for learning stabilization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Q-learning scales suboptimally when adding spatial dimensions due to exponential growth in state space
- Mechanism: In 2D, state space = width x height. In 3D, state space = width x height x depth. Each added dimension multiplies the number of possible states, increasing the number of Q-table entries needed. More entries mean more memory and more updates per episode, slowing learning convergence.
- Core assumption: The agent uses a tabular Q-learning approach without function approximation or generalization
- Evidence anchors:
  - [abstract] "In the 3D environment (50x50x50 grid), stabilization required roughly 1450 episodes and 163 steps."
  - [abstract] "The transition from 2D to 3D demanded approximately 22 times more episodes for learning stabilization."

### Mechanism 2
- Claim: Action space growth with dimensions adds to computational burden, but is less severe than state space growth
- Mechanism: 2D has 4 actions; 3D has 6 actions. More actions increase the inner loop iterations when selecting and updating Q-values. However, this effect is smaller than state space growth because action space size increases linearly, not exponentially.
- Core assumption: Action selection and update loops iterate over all possible actions each step
- Evidence anchors:
  - [abstract] "The agent's capacity to attain the goal with remarkable efficiency underscore the efficacy of RL algorithms in navigating complex, multi-dimensional spaces."
  - [section] "num_actions=4" for 2D and "num_actions=6" for 3D.

### Mechanism 3
- Claim: Learning rate and discount factor settings that work in 2D may be suboptimal in 3D, causing slower convergence
- Mechanism: The same hyperparameters (learning_rate=0.5, discount_factor=0.5) are used in both dimensions. In higher-dimensional spaces, the effective horizon for credit assignment lengthens, potentially requiring lower learning rates or higher discount factors to propagate rewards effectively.
- Core assumption: Hyperparameters tuned for 2D transfer directly to 3D without adjustment
- Evidence anchors:
  - [section] "Both the learning_rate and discount_factor parameters control the learning behavior, both set to 0.5, while epsilon defines the exploration rate of the agent's policy, set to 0.2, maintaining a balance between exploration and exploitation strategies."
  - [abstract] "Stabilization required roughly 1450 episodes and 163 steps" in 3D vs "stabilized learning after approximately 65 episodes, achieving goals in about 107 steps" in 2D.

## Foundational Learning

- Concept: Tabular Q-learning update rule
  - Why needed here: The agent stores Q-values for each state-action pair in a dictionary; understanding the update formula is essential to see why scaling is costly
  - Quick check question: What are the components of the Q-learning update equation used in the code?

- Concept: Epsilon-greedy action selection
  - Why needed here: Balancing exploration vs. exploitation is key to learning efficiency; changing epsilon affects convergence speed
  - Quick check question: How does the epsilon value influence the agent's behavior in early vs. late training episodes?

- Concept: State and action space definitions
  - Why needed here: Knowing how many states and actions exist in 2D vs. 3D explains why computation increases
  - Quick check question: How many total states exist in the 50x50x50 3D environment used in the study?

## Architecture Onboarding

- Component map:
  - QLearningAgent -> Environment/Environment3D -> train_agent

- Critical path:
  1. Initialize agent and environment
  2. For each episode: reset state, loop until goal or max steps
  3. Choose action → execute → observe reward → update Q
  4. Record metrics, proceed to next episode

- Design tradeoffs:
  - Tabular Q-learning is simple but memory-intensive; function approximation would reduce memory but add complexity
  - Fixed hyperparameters simplify setup but may be suboptimal for higher dimensions
  - No libraries means full control but more implementation effort

- Failure signatures:
  - Agent never reaches goal: likely exploration rate too low or reward shaping incorrect
  - Convergence extremely slow: possible state space too large for tabular approach
  - Q-table grows unbounded: missing state representation compression or discretization

- First 3 experiments:
  1. Run the 2D environment with default params; verify learning stabilizes around episode 65
  2. Run the 3D environment with default params; observe if stabilization occurs near episode 1450
  3. Reduce epsilon from 0.2 to 0.1 and compare convergence speed in both environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the learning trajectory change when extending beyond 3D to higher-dimensional spaces (e.g., 4D or 5D environments)?
- Basis in paper: [explicit] The paper notes that transitioning from 2D to 3D demanded approximately 22 times more episodes for learning stabilization, prompting further inquiry into the dynamics of learning in higher-dimensional spaces
- Why unresolved: The paper only tested 2D and 3D environments, leaving the scaling behavior in even higher dimensions unexplored
- What evidence would resolve it: Systematic experiments comparing learning curves across 4D, 5D, and higher-dimensional environments with consistent parameters

### Open Question 2
- Question: What specific architectural or algorithmic modifications could reduce the computational burden when scaling Q-learning to higher dimensions?
- Basis in paper: [inferred] The paper highlights the substantial increase in computational effort (22-fold more episodes) when moving from 2D to 3D, suggesting this as a promising avenue for future research
- Why unresolved: The study used a basic Q-learning implementation without exploring optimization techniques like function approximation, experience replay, or hierarchical approaches
- What evidence would resolve it: Comparative studies testing various optimization strategies (e.g., neural network function approximation, sparse representations) across different dimensionalities

### Open Question 3
- Question: How do different exploration strategies (beyond epsilon-greedy) affect learning efficiency in higher-dimensional spaces?
- Basis in paper: [explicit] The study used a fixed epsilon-greedy policy with epsilon=0.2, but reflections suggest examining the dynamics of learning in higher-dimensional environments
- Why unresolved: The paper employed a single exploration strategy without investigating alternatives like softmax, upper confidence bounds, or curiosity-driven exploration
- What evidence would resolve it: Controlled experiments comparing multiple exploration strategies across 2D, 3D, and higher-dimensional environments measuring convergence speed and final performance

## Limitations
- The study uses a basic tabular Q-learning approach without exploring function approximation or other optimization techniques that could mitigate the state space explosion
- Hyperparameters are fixed across dimensions without tuning, potentially masking dimension-specific optimal settings
- No specification of random seed makes it difficult to assess result reproducibility and variance

## Confidence

- Confidence in core finding that tabular Q-learning scales poorly with dimensions: **High**
- Confidence in specific performance metrics (steps, episodes): **Medium**
- Confidence in claim that same hyperparameters work across dimensions: **Low**

## Next Checks
1. Run multiple trials with different random seeds to establish confidence intervals for convergence episodes and steps
2. Test learning with reduced epsilon (0.1) and adjusted learning rate (0.3) to determine if hyperparameter tuning improves 3D performance
3. Implement a simple function approximation (e.g., linear tile coding) to compare tabular vs. approximate Q-learning scaling behavior