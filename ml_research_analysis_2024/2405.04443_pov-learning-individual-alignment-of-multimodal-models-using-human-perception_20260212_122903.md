---
ver: rpa2
title: 'POV Learning: Individual Alignment of Multimodal Models using Human Perception'
arxiv_id: '2405.04443'
source_url: https://arxiv.org/abs/2405.04443
tags:
- individual
- learning
- alignment
- task
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses individual alignment of multimodal models\
  \ by leveraging human visual perception. It introduces a novel task\u2014Perception-guided\
  \ Crossmodal Entence (PCE)\u2014where models predict how a person evaluates whether\
  \ image captions mention central visual objects, using their eye-tracking fixation\
  \ sequences as individual alignment signals."
---

# POV Learning: Individual Alignment of Multimodal Models using Human Perception

## Quick Facts
- arXiv ID: 2405.04443
- Source URL: https://arxiv.org/abs/2405.04443
- Reference count: 28
- Primary result: Perception-guided models achieve 77.61% accuracy vs 67.15% for naive models on subjective crossmodal entailment task

## Executive Summary
This paper introduces Perception-guided Crossmodal Entailment (PCE), a novel task for individual alignment of multimodal models using human perception signals. The approach leverages eye-tracking fixation sequences as individual alignment signals to predict how people evaluate whether image captions mention central visual objects. A new benchmark dataset with 109 participants was collected, capturing multimodal stimuli, fixation sequences, and subjective assessments. The Perception-Guided Multimodal Transformer (PGMT) incorporates fixation-based transition matrices into attention weights, showing significant improvements over content-only models and standard multimodal LLMs.

## Method Summary
The method involves collecting eye-tracking data from participants viewing image-caption pairs and recording their subjective assessments of whether central objects are mentioned. Fixation sequences are converted to symbolic Area of Interest (AOI) sequences and transition matrices. A baseline LSTM model processes these sequences, while a Transformer model uses pre-trained visual and textual features. The PGMT extends the Transformer by incorporating scaled transition matrices into attention weights. Ensemble methods combine perception and content signals. All models are trained with AdamW optimizer and cross-entropy loss.

## Key Results
- PGMT achieves 77.61% accuracy (vs 67.15% naive) and 52.94% F1 score on 3-class PCE task
- Ensemble methods combining perception and content features reach 87.42% accuracy
- Leading multimodal LLMs (GPT-4, Pixtral) underperform, indicating individual alignment gap
- Accuracy increases when perception signals are incorporated into attention mechanisms

## Why This Works (Mechanism)

### Mechanism 1
Individual perception traces capture cognitive processes that predict subjective evaluations. Eye-tracking fixation sequences reflect individual attentional patterns during multimodal interpretation. Encoding these as transition matrices and incorporating them into attention weights biases the model toward user-specific perceptual priorities. Performance degrades when fixation sequences are randomized, confirming their predictive value.

### Mechanism 2
Standard multimodal transformers optimize for population-level alignment through post-training, missing individual variation. Without explicit conditioning on individual perception signals (xALIGN), these models default to consensus-based predictions that don't capture subjective differences. Fine-tuning with individual perception signals should improve performance toward PGMT levels.

### Mechanism 3
Perception and content signals provide complementary information for subjective prediction. Perception signals capture individual cognitive patterns while content signals provide semantic grounding. Ensemble averaging reduces bias from either signal alone. If perception and content signals become highly correlated, ensemble gains diminish.

## Foundational Learning

- Crossmodal Entailment: Required for judging whether captions mention central visual objects, distinguishing PCE from standard visual entailment by focusing on subjective individual assessments rather than objective entailment.
- Transformer Attention Mechanisms: Essential for understanding how PGMT modifies attention weights using transition matrices derived from eye-tracking data, differing from standard multi-head attention by adding perceptual bias.
- Eye-tracking Data Processing: Critical for converting raw fixation sequences to symbolic AOI sequences and transition matrices, with continuous-to-discrete conversion potentially losing temporal and spatial precision information.

## Architecture Onboarding

- Component map: Stimulus encoder (BERT + ResNet) → Cross-modal classification token → Perception-guided attention (transition matrices) → Classification head (3-class output)
- Critical path: Raw multimodal stimulus → Feature extraction → Individual alignment signal integration → Prediction
- Design tradeoffs: PGMT reduces parameters vs. ensemble approach but requires careful λ scaling; perception signals improve individual alignment but add data collection complexity
- Failure signatures: Accuracy below 70% indicates ineffective perception signal integration; performance drop without participant embeddings suggests individual conditioning is critical
- First 3 experiments:
  1. Implement naive baseline predicting most frequent class
  2. Train content-only transformer with participant embeddings
  3. Implement PGMT with transition matrices and tune λ scaling parameter

## Open Questions the Paper Calls Out

- What specific features of eye-tracking data beyond fixation sequences contribute most to individual alignment performance? The paper focused on symbolic fixation sequences but acknowledges other features like dwell time and pupil dilation could be valuable.
- Can multimodal LLMs achieve competitive POV task performance through post-training fine-tuning rather than few-shot learning? The paper tested only few-shot performance and suggests post-training approaches need investigation.
- What are the generalization limits of POV learning across different subjective assessments and multimodal stimuli? The study focused on one task and dataset, leaving open questions about applicability to other domains.

## Limitations

- Dataset generalizability is limited by reliance on single dataset (153 images, 109 participants) potentially specific to stimulus set and population
- Major corpus evidence gaps exist with low FMR scores (0.46 average) indicating limited scholarly precedent for eye-tracking-based individual alignment
- Critical model architecture details underspecified including exact pre-trained models and precise transition matrix integration mechanism

## Confidence

- High Confidence: Standard multimodal LLMs underperform on individual-level subjective alignment, aligning with established population-level vs. individual-level behavior understanding
- Medium Confidence: Perception-guided attention improves accuracy (67.15% → 77.61%), though corpus evidence for this specific approach is weak
- Low Confidence: Broader claim that fixation sequences serve as reliable individual alignment signals lacks sufficient corpus validation and may be PCE-task specific

## Next Checks

1. Test PGMT on different multimodal dataset with eye-tracking data to assess generalizability of perception-guided alignment approach
2. Systematically vary λ scaling parameter and test alternative transition matrix integration methods to determine optimal strategy
3. Fine-tune GPT-4 or Pixtral with perception signals and compare performance against PGMT to isolate benefits of architectural vs. training methodology choices