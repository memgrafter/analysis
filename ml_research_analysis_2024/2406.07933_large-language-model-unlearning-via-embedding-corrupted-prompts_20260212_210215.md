---
ver: rpa2
title: Large Language Model Unlearning via Embedding-Corrupted Prompts
arxiv_id: '2406.07933'
source_url: https://arxiv.org/abs/2406.07933
tags:
- arxiv
- unlearning
- prompt
- original
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently unlearning specific
  knowledge from large language models (LLMs) while minimizing side effects. The authors
  propose Embedding-COrrupted (ECO) Prompts, a lightweight framework that enforces
  an unlearned state during inference by corrupting prompt embeddings via a learned
  corruption function.
---

# Large Language Model Unlearning via Embedding-Corrupted Prompts

## Quick Facts
- arXiv ID: 2406.07933
- Source URL: https://arxiv.org/abs/2406.07933
- Authors: Chris Yuhao Liu; Yaxuan Wang; Jeffrey Flanigan; Yang Liu
- Reference count: 40
- Key outcome: Achieves near-zero side effects while maintaining model utility for unlearning specific knowledge

## Executive Summary
This paper introduces ECO Prompts, a lightweight framework for unlearning specific knowledge from large language models by corrupting prompt embeddings rather than model weights. The method uses a prompt classifier to identify unlearning targets and applies zeroth-order optimization to learn corruption parameters offline. Experiments on entity leaking, hazardous knowledge, and copyrighted content extraction tasks demonstrate that ECO achieves comparable or superior unlearning effectiveness to baseline methods while maintaining model utility and scaling efficiently to 100 models up to 236B parameters.

## Method Summary
ECO Prompts operates in two phases: first, a prompt classifier is trained to distinguish prompts that should be forgotten from those that should be retained; second, zeroth-order optimization learns a corruption parameter offline that modifies prompt embeddings to approximate outputs from a model that never saw the forget data. During inference, flagged prompts have their embeddings corrupted before being processed by the LLM. The method requires no additional computational cost during inference and can be applied to any pre-trained model without fine-tuning.

## Key Results
- ECO achieves near-zero side effects on model utility while effectively unlearning targeted knowledge
- Scales efficiently to 100 LLMs ranging from 0.5B to 236B parameters
- Outperforms baseline methods including fine-tuning, gradient ascent/difference, and preference optimization across multiple unlearning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding-corrupted prompts produce outputs indistinguishable from those of a model that never saw the data
- Mechanism: The corruption function modifies prompt embeddings in embedding space, steering the LLM's output distribution toward that of a model trained without forget data
- Core assumption: Small perturbation in embedding space is sufficient to alter output distribution without harming utility
- Evidence anchors: [abstract] "... embedding-corrupted prompts ... closely approximate the output from a model that has never been trained on the data intended for forgetting"; [section] "We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline..."
- Break condition: If perturbation magnitude needed to obscure knowledge is too large, it will degrade utility; if too small, it won't obscure knowledge

### Mechanism 2
- Claim: The prompt classifier accurately identifies prompts that should be corrupted to enforce forgetting
- Mechanism: A classifier trained to distinguish forget vs retain prompts flags inputs for embedding corruption; thresholding or conformal prediction controls false positives/negatives
- Core assumption: Distribution of forget prompts is separable from retain prompts in embedding space
- Evidence anchors: [abstract] "we employ a prompt classifier to identify and safeguard prompts to forget"; [section] "To identify the unlearning target, we use a prompt classifier that is trained to explicitly model the prompt distribution..."
- Break condition: If boundary between forget and retain is fuzzy or adversarial prompts bypass classifier, forgetting will fail

### Mechanism 3
- Claim: Zeroth-order optimization learns corruption parameters offline, enabling scalable unlearning without per-prompt compute
- Mechanism: Offline learning of scalar corruption strength via finite differences avoids gradient-based updates to model weights
- Core assumption: Distance metric between unlearned and retained model outputs can be minimized effectively with scalar parameter
- Evidence anchors: [abstract] "... learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline..."; [section] "We leverage corruptions learned efficiently via zeroth-order optimization ... and apply them to the prompt's embedding space during inference."
- Break condition: If distance metric is too coarse or perturbation space too constrained, learned corruption won't approximate retained model

## Foundational Learning

- Concept: Embedding space manipulation and its effect on LLM outputs
  - Why needed here: Method relies on perturbing embeddings rather than model weights; understanding how embeddings map to token probabilities is critical
  - Quick check question: What happens to output distribution when you add Gaussian noise to token embeddings?

- Concept: Zeroth-order (gradient-free) optimization
  - Why needed here: Method uses finite differences to learn corruption parameters offline; knowing how perturbation size and smoothing affect convergence is essential
  - Quick check question: How does choice of perturbation size μ affect accuracy of finite difference gradient estimate?

- Concept: Conformal prediction and thresholding for classification
  - Why needed here: Classifier can use simple thresholding or conformal prediction to control false positives/negatives; understanding both is important for deployment
  - Quick check question: What is effect of increasing significance level α in conformal prediction on prediction set size?

## Architecture Onboarding

- Component map: Prompt classifier (RoBERTa or Llama-3.1-1B-Instruct) -> Corruption function (random noise, zero-out, sign-flip) -> Zeroth-order optimizer (offline learning of scalar σ) -> LLM inference pipeline (embedding corruption applied conditionally) -> Evaluation metrics (forget quality, model utility, ASG, etc.)

- Critical path: 1) Pre-train prompt classifier on forget vs retain prompts 2) Pre-learn corruption parameter σ offline via zeroth-order optimization on surrogate metric 3) At inference: classify prompt → if flagged, corrupt embeddings with learned σ → feed to LLM

- Design tradeoffs: Classifier complexity vs false positive rate; corruption strength vs output quality; number of corrupted embedding dimensions vs stealth of corruption; offline compute for σ learning vs inference speed

- Failure signatures: High false positives → utility loss on retain set; high false negatives → knowledge leakage on forget set; too weak corruption → outputs still reflect forget data; too strong corruption → outputs nonsensical or off-topic

- First 3 experiments: 1) Train prompt classifier on TOFU; measure FPR/FNR on general set 2) Learn σ on synthetic WMDP subset; test accuracy on real WMDP 3) Apply ECO on Llama-2-7B-Chat with BBC News corruption; measure ASG and perplexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ECO method perform on tasks requiring more nuanced understanding of context and longer-term dependencies, such as complex reasoning or multi-turn dialogue?
- Basis in paper: [explicit] Paper primarily focuses on unlearning in entity leaking, hazardous knowledge, and copyrighted content extraction tasks; does not explicitly address effectiveness on more complex reasoning or dialogue-based tasks
- Why unresolved: Paper does not provide experimental results or analysis on tasks that require understanding of context and longer-term dependencies; effectiveness in these scenarios remains open question
- What evidence would resolve it: Experiments evaluating ECO on tasks involving complex reasoning, multi-turn dialogue, or other scenarios requiring nuanced understanding of context and longer-term dependencies

### Open Question 2
- Question: How robust is ECO method against sophisticated adversarial attacks that specifically target prompt classifier or corruption function?
- Basis in paper: [explicit] Paper mentions prompt classifier could be vulnerable to adversarial attacks and suggests training it adversarially to enhance robustness; however, does not provide experimental results on effectiveness against such attacks
- Why unresolved: Paper acknowledges potential vulnerability but does not explore or evaluate robustness against adversarial attacks; effectiveness in face of sophisticated adversarial attempts remains unclear
- What evidence would resolve it: Experiments evaluating robustness of ECO against various adversarial attacks targeting prompt classifier or corruption function

### Open Question 3
- Question: How does choice of corruption function and its strength impact effectiveness of unlearning and potential side effects on model utility?
- Basis in paper: [explicit] Paper mentions choice of corruption function and its strength can impact effectiveness of unlearning; provides ablation studies on different corruption functions but does not thoroughly investigate impact of corruption strength on unlearning effectiveness and model utility
- Why unresolved: While paper demonstrates different corruption functions can achieve similar results, does not provide comprehensive analysis of how corruption strength affects trade-off between unlearning effectiveness and model utility; optimal choice for different tasks remains open question
- What evidence would resolve it: Systematic study on impact of corruption strength on unlearning effectiveness and model utility across various tasks and scenarios

## Limitations
- Method relies on prompt classifiers that can only unlearn knowledge identifiable through prompt features
- Zeroth-order optimization may converge to local minima that don't achieve optimal forgetting
- Claim of "no additional computational cost during inference" is technically accurate but potentially misleading due to offline learning requirements

## Confidence
- High Confidence: Core mechanism of embedding corruption and scalability to large models is well-supported by experimental results
- Medium Confidence: Assertion that ECO approximates model that never saw data is supported by similarity metrics but relies on assumptions about output distribution similarity
- Low Confidence: Claim of "no additional computational cost during inference" and generalizability to all knowledge domains remains unproven

## Next Checks
1. Systematically evaluate ECO's robustness against prompts specifically designed to bypass the classifier, measuring false negative rates under these conditions
2. After applying ECO, attempt to reconstruct the forgotten knowledge through alternative query strategies or by analyzing intermediate activation patterns
3. Evaluate whether the unlearned state degrades over time with continued model usage, particularly for knowledge that may be reinforced through incidental exposure to related content during normal operation