---
ver: rpa2
title: 'CompAct: Compressed Activations for Memory-Efficient LLM Training'
arxiv_id: '2410.15352'
source_url: https://arxiv.org/abs/2410.15352
tags:
- memory
- compact
- training
- projection
- galore
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CompAct reduces peak GPU memory by 25-30% for pretraining and\
  \ 50% for fine-tuning large language models. It compresses linear layer activations\
  \ using random projections, lowering the memory cost of the computation graph\u2014\
  the largest component of training memory."
---

# CompAct: Compressed Activations for Memory-Efficient LLM Training

## Quick Facts
- arXiv ID: 2410.15352
- Source URL: https://arxiv.org/abs/2410.15352
- Reference count: 18
- Key outcome: CompAct reduces peak GPU memory by 25-30% for pretraining and 50% for fine-tuning large language models through compressed activations.

## Executive Summary
CompAct introduces a novel approach to reduce peak GPU memory during LLM training by compressing linear layer activations using random projections. Unlike previous methods that only reduce optimizer overheads or parameter counts, CompAct directly targets the computation graph - the largest memory component during training. By storing low-rank, compressed activations during the forward pass and using them in the backward pass, CompAct achieves significant memory savings without requiring SVD computations or additional memory overhead.

## Method Summary
CompAct compresses activations in linear layers during the forward pass using random Gaussian projection matrices, storing the compressed activations instead of full activation tensors. During the backward pass, gradients are computed in the reduced subspace and projected back to full rank for weight updates. The method maintains optimizer states (first and second moment estimates) in the compressed space, requiring modifications to Adam's update rule. CompAct is applied to all attention and MLP blocks except the output projection in attention, using ranks of n/2, n/4, or n/8 where n is the input dimension.

## Key Results
- Reduces peak GPU memory by 25-30% for pretraining and 50% for fine-tuning
- For LLaMA-350M, cuts peak memory from ~40 GB to ~34.7 GB
- For RoBERTa-base, reduces peak memory in half
- Avoids SVD overhead while achieving superior memory savings compared to GaLore

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CompAct reduces peak GPU memory by compressing the largest component of allocated memory during training—the model's compute graph—rather than just optimizer states or parameters.
- Mechanism: CompAct stores low-rank, compressed activations during the forward pass instead of full activation tensors. This compression propagates through the backward pass, reducing the size of gradients and optimizer states as well, since gradients computed from compressed activations are also low-rank.
- Core assumption: The compressed activations contain sufficient information to compute gradients that, when projected back to full rank, yield weight updates comparable to training with uncompressed activations.
- Evidence anchors:
  - [abstract]: "By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters."
  - [section]: "By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters."
  - [corpus]: Weak evidence—no corpus papers explicitly discuss compression of the compute graph itself as the primary memory reduction strategy.
- Break condition: If the projection matrix fails to preserve essential information, weight updates become inaccurate, leading to poor convergence or degraded model performance.

### Mechanism 2
- Claim: Random projections maintain approximation quality while avoiding SVD overhead.
- Mechanism: CompAct uses Gaussian random matrices sampled on-the-fly during the forward pass, eliminating the need to store or compute optimal projection matrices. This reduces memory overhead while still preserving the top singular values of the activation matrices with high probability.
- Core assumption: Random Gaussian matrices preserve the norm and essential structure of the original activations sufficiently for effective gradient computation and model convergence.
- Evidence anchors:
  - [abstract]: "Our compression uses random projection matrices, thus avoiding additional memory overheads."
  - [section]: "Using a random matrix from a Gaussian distribution ensures that the projected subspace maintains the norm of the original vectors with high probability (Dasgupta and Gupta, 2003; Indyk and Motwani, 1998), which is critical for preserving information (Hao et al., 2024)."
  - [corpus]: Weak evidence—corpus papers focus on random projections in other contexts but not specifically for activation compression in LLM training.
- Break condition: If the random projection fails to adequately preserve activation norms or structure, the resulting gradients will be poor approximations, harming convergence.

### Mechanism 3
- Claim: CompAct's memory savings scale with batch size because activations grow with batch size while optimizer states do not.
- Mechanism: CompAct compresses activations (which depend on batch size) rather than just optimizer states (which are independent of batch size). Therefore, the memory reduction becomes more significant as batch size increases.
- Core assumption: The activation memory is the dominant memory component that scales with batch size in LLM training.
- Evidence anchors:
  - [abstract]: "Our compression uses random projection matrices, thus avoiding additional memory overheads. Comparisons with previous techniques for either pretraining or fine-tuning show that CompAct substantially improves existing compute-performance tradeoffs."
  - [section]: "CompAct significantly reduces peak memory usage, and its benefit scales with batch size, whereas GaLore provides a fixed reduction in peak memory relative to the baseline."
  - [corpus]: Weak evidence—corpus papers don't explicitly discuss batch-size-dependent memory scaling for activation compression methods.
- Break condition: If other memory components (like optimizer states) become the dominant factor at larger batch sizes, the relative benefit of CompAct would diminish.

## Foundational Learning

- Concept: Low-rank matrix approximation and random projection theory
  - Why needed here: CompAct relies on projecting high-dimensional activation matrices into lower-dimensional subspaces while preserving essential information for gradient computation.
  - Quick check question: What property of Gaussian random matrices ensures they preserve norms of vectors they project?

- Concept: Backpropagation and gradient computation
  - Why needed here: Understanding how gradients flow through compressed activations and how weight updates are computed from these gradients is critical for implementing CompAct correctly.
  - Quick check question: How does compressing activations during the forward pass affect the computation of gradients with respect to weights?

- Concept: Optimizer state management (Adam optimizer)
  - Why needed here: CompAct maintains optimizer states (first and second moment estimates) in the compressed subspace, requiring understanding of how Adam operates on reduced-rank gradients.
  - Quick check question: What modifications are needed to Adam's update rule when working with compressed gradients?

## Architecture Onboarding

- Component map:
  - Forward pass: Input tensor → Linear layer → Random projection → Compressed activation stored
  - Backward pass: Compressed activation + output gradient → Compressed gradient computed → Projected back to full rank → Full gradient used for weight update
  - Optimizer: Maintains moments in compressed space → Updates performed in compressed space → Projection back to full space for weight update

- Critical path: Forward pass (compression) → Backward pass (gradient computation from compressed activations) → Optimizer update (in compressed space) → Weight update (projection back to full space)

- Design tradeoffs:
  - Rank selection: Lower rank provides greater memory savings but may degrade model performance; higher rank preserves performance but offers less memory reduction
  - Projection type: Gaussian random matrices are computationally efficient but may introduce some approximation error; structured projections might offer better approximation at higher computational cost
  - Update frequency: More frequent projection updates may improve approximation quality but increase computational overhead

- Failure signatures:
  - Training instability or divergence: Likely indicates projection matrix is not preserving sufficient information
  - Degraded model performance: May indicate rank is too low or projection matrix quality is insufficient
  - Minimal memory savings: Could indicate incorrect implementation of compression or activation checkpointing interfering with compression

- First 3 experiments:
  1. Implement CompAct with rank = n/2 on a small LLaMA model (60M parameters) and verify memory reduction vs baseline
  2. Compare perplexity and training stability across different ranks (n/2, n/4, n/8) on the same small model
  3. Measure throughput impact of CompAct vs baseline and vs GaLore with and without activation checkpointing on the small model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical justification for using random projections over learned projections in CompAct?
- Basis in paper: [explicit] The paper references Theorem 2 from Meier and Nakatsukasa (2024), which shows that random sketching preserves top singular values with high probability.
- Why unresolved: While the theorem provides probabilistic guarantees, it doesn't directly address the practical trade-offs between random and learned projections in the specific context of LLM training with CompAct's architecture.
- What evidence would resolve it: Controlled experiments comparing CompAct with learned projections (similar to GaLore's approach) against the random projections used in CompAct, measuring both memory efficiency and model performance.

### Open Question 2
- Question: How does CompAct's performance scale with model size beyond LLaMA-65B?
- Basis in paper: [inferred] The authors explicitly state "We expect CompAct's savings to scale even higher for larger models" and show increasing memory savings with model size from 60M to 65B parameters.
- Why unresolved: The paper only tests up to LLaMA-65B and extrapolates from that data. Without empirical validation on larger models, the scalability claim remains theoretical.
- What evidence would resolve it: Empirical results showing memory savings and performance metrics for models larger than 65B parameters, ideally approaching or exceeding 100B parameters.

### Open Question 3
- Question: What is the impact of applying CompAct to non-linear activation layers (beyond linear layers)?
- Basis in paper: [explicit] The authors mention that Yang et al. (2024) focus on "saving activation memory generated by nonlinear functions and normalization layers, whereas our work focuses on the activations generated by linear layers."
- Why unresolved: The paper explicitly states this as a limitation and potential area for improvement, noting that "Future work could explore compressing their activations directly within the computation graph."
- What evidence would resolve it: Experimental results showing the memory savings and performance impact when applying CompAct-like compression to non-linear activations, including proper handling of gradient computations for these layers.

## Limitations
- The paper lacks formal bounds on approximation quality when using random projections with rank reduction
- Performance claims are primarily validated on language modeling and GLUE benchmarks, not diverse LLM tasks
- Scaling behavior on truly large-scale models (10B+ parameters) remains unverified

## Confidence
- **High Confidence**: The core memory reduction mechanism (compressing activations via random projections) is technically sound and well-supported by both theory and empirical evidence
- **Medium Confidence**: The claim that CompAct is "particularly effective for larger models" is logically consistent but lacks empirical validation on large-scale models
- **Low Confidence**: The assertion that CompAct "achieves superior memory savings without significant performance loss" compared to GaLore is based on limited head-to-head comparisons

## Next Checks
1. **Rank Sensitivity Analysis**: Systematically evaluate model performance and training stability across a wider range of ranks (n/16, n/32, n/64) on the 350M parameter model to establish the minimum viable rank that maintains baseline performance, particularly for the fine-tuning scenario where gains are largest.

2. **Large-Scale Validation**: Implement CompAct on a 7B-13B parameter model and measure both memory reduction and any degradation in convergence speed or final model quality compared to baseline training. This would validate the claim about effectiveness on larger models.

3. **Task Diversity Testing**: Apply CompAct to fine-tuning on tasks beyond GLUE, such as summarization (CNN/DailyMail), code generation (HumanEval), or multilingual benchmarks, to verify the claim of general applicability without significant performance loss.