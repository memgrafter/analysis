---
ver: rpa2
title: Progressive Query Expansion for Retrieval Over Cost-constrained Data Sources
arxiv_id: '2406.07136'
source_url: https://arxiv.org/abs/2406.07136
tags:
- query
- retrieval
- expansion
- arxiv
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the problem of retrieval over cost-constrained
  data sources, where the corpus is accessed via APIs that charge per retrieved document.
  The authors propose ProQE, a progressive query expansion algorithm that combines
  classic pseudo-relevance feedback with modern LLM-based techniques.
---

# Progressive Query Expansion for Retrieval Over Cost-constrained Data Sources

## Quick Facts
- **arXiv ID**: 2406.07136
- **Source URL**: https://arxiv.org/abs/2406.07136
- **Reference count**: 40
- **Primary result**: ProQE achieves 37% improvement in MRR and R@1 metrics while being the most cost-effective solution for retrieval over paid APIs

## Executive Summary
This paper addresses the challenge of retrieving relevant documents from cost-constrained data sources where each document access incurs a fee. The authors propose ProQE, a progressive query expansion algorithm that iteratively retrieves documents one at a time, uses LLM-based relevance scoring to filter expansion terms, and combines classic pseudo-relevance feedback with modern LLM techniques. ProQE is designed to work with both sparse and dense retrieval systems while minimizing retrieval costs through progressive refinement.

## Method Summary
ProQE implements an iterative query expansion process where each iteration retrieves the top-1 document using the current query, extracts potential expansion terms via an LLM, and scores the document's relevance using the same LLM. Based on relevance feedback, ProQE updates term weights by increasing weights for relevant terms and decreasing weights for irrelevant ones. This process repeats for n iterations before appending LLM-generated context to form the final expanded query. The algorithm is compatible with both sparse retrieval (where term repetition boosts weights) and dense retrieval (where embeddings are combined with learned weights).

## Key Results
- ProQE outperforms state-of-the-art baselines by 37% on MRR and R@1 metrics
- Achieves the most cost-effective solution for retrieval over paid APIs
- Delivers an average gain of 8% for dense retrieval systems
- Demonstrates superior performance across four datasets (NQ, WQ, TREC DL19, DL20)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Iterative retrieval with LLM relevance scoring progressively filters noise from expansion terms
- **Mechanism**: ProQE retrieves one document at a time, uses LLM to judge relevance, and adjusts term weights based on this feedback. Relevant terms get their weights increased (by Î²), irrelevant terms get weights decreased (by Î³), so over multiple iterations the query focuses on truly relevant vocabulary
- **Core assumption**: Single-document retrieval per iteration reduces noise and cost compared to batch retrieval, and LLM relevance judgments are sufficiently accurate to guide weight updates
- **Evidence anchors**: [abstract] states: "we employ an LLM as a relevance judge for each returned result" and "evaluating each document ensures that terms from more relevant documents receive higher weights, allowing for progressive updates to the query terms based on LLM feedback." [section] explains: "By boosting and decreasing the weights of expansion terms based on feedback from the LLM and the retrieved passage, only relevant terms are appended to the query, thereby reducing noise." [corpus] shows this is a novel approach: "To the best of our knowledge, we are the first to consider the cost of retrieval as a constraint."
- **Break condition**: If LLM relevance judgments are inaccurate or inconsistent, noise could accumulate and hurt retrieval performance

### Mechanism 2
- **Claim**: Combining classic PRF with LLM-generated context leverages both vocabulary expansion and semantic enrichment
- **Mechanism**: ProQE uses PRF-style term extraction from retrieved documents, then at the end appends LLM-generated chain-of-thought context to the intermediate query. This merges term-based matching with semantic context
- **Core assumption**: Extracted terms address lexical mismatch, while LLM context adds semantic depth; their combination is more effective than either alone
- **Evidence anchors**: [abstract] states: "Our key contribution for improving retrieval accuracy is to combine classic pseudo-relevance feedback expansion techniques with modern LLM-based query expansion techniques." [section] explains: "we promptL using chain-of-thought instruction...and receive the output ðœƒð‘. The final query ð‘žâ€² is formulated as ð‘žâ€² = ð‘ð‘œð‘›ð‘ð‘Žð‘¡ (ð‘ž+, ðœƒð‘)." [corpus] supports: "recent studies have instead used Large Language Models (LLMs) to generate additional content to expand a query" and "hallucination [ 45] and thus can generate highly irrelevant content."
- **Break condition**: If LLM-generated context is too hallucinated or misaligned with the corpus, the final query could be misled

### Mechanism 3
- **Claim**: Plug-and-play compatibility with both sparse and dense retrievers maximizes applicability and cost efficiency
- **Mechanism**: For sparse retrieval, ProQE appends weighted terms to the query string; for dense retrieval, it adjusts query embeddings by combining original query, term embeddings, and CoT embedding with learned weights. This allows ProQE to work with any retrieval backend without retraining
- **Core assumption**: The retrieval system's scoring function can handle either the expanded sparse query or the combined dense embedding, and the cost model assumes retrieval API charges per unique document
- **Evidence anchors**: [abstract] states: "ProQE is compatible with both sparse and dense retrieval systems." [section] explains: "Appending a term multiple times does not boost its weight in a dense retrieval system as the whole semantic meaning is captured in an embedding. We use an encoder from a dense retriever model to create embeddings..." [corpus] notes: "we also assume that the query interface only charges for retrieving new unique documents."
- **Break condition**: If the retrieval system does not support dynamic query expansion or if the API's cost model differs, ProQE's cost advantages could be negated

## Foundational Learning

- **Concept**: Pseudo-Relevance Feedback (PRF) and vocabulary mismatch
  - **Why needed here**: ProQE uses PRF to extract expansion terms, so understanding how PRF works and its pitfalls (noisy early results) is essential
  - **Quick check question**: What problem does PRF solve in sparse retrieval, and why can it introduce noise?

- **Concept**: Dense vs. sparse retrieval embeddings
  - **Why needed here**: ProQE has separate update rules for sparse (term weighting) and dense (embedding weighting), so engineers must understand the difference in how retrievers process queries
  - **Quick check question**: How does appending a term multiple times affect sparse retrieval vs. dense retrieval?

- **Concept**: Large Language Model hallucination and relevance scoring
  - **Why needed here**: ProQE relies on LLM for both term extraction and relevance judgment, so understanding LLM limitations and how to prompt for accurate feedback is critical
  - **Quick check question**: Why might LLM-generated context be irrelevant, and how does ProQE mitigate that risk?

## Architecture Onboarding

- **Component map**: Original query -> Retrieval API -> LLM relevance scoring -> Term extractor -> Weight updater -> Intermediate query -> LLM CoT context -> Final expanded query
- **Critical path**:
  1. Retrieve top-1 document with current query
  2. Extract m expansion terms via LLM
  3. Score relevance of document via LLM
  4. Update term weights (increase if relevant, decrease if not)
  5. Build intermediate query from weighted terms
  6. Repeat n times
  7. Generate final CoT context via LLM
  8. Append to intermediate query for final output
- **Design tradeoffs**:
  - Single-document vs. batch retrieval: lower cost but slower convergence
  - Term repetition vs. embedding weighting: term repetition helps sparse but not dense
  - LLM call frequency: more calls improve accuracy but increase cost
  - Stopping criterion: fixed iterations (n) vs. early stopping on performance plateau
- **Failure signatures**:
  - Query drift: expanded terms pull retrieval away from original intent
  - Cost overrun: too many API calls or irrelevant LLM generations
  - Hallucination injection: CoT context introduces false facts
  - Weight imbalance: some terms dominate unfairly due to noisy relevance feedback
- **First 3 experiments**:
  1. Run ProQE with n=1 and m=3 on a small dev set; verify term extraction and weight updates
  2. Compare sparse vs. dense expansion outputs on same query to confirm plug-and-play compatibility
  3. Test cost-effectiveness by measuring API calls and retrieval accuracy against baseline PRF

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does ProQE's performance scale with different retrieval API cost structures, such as tiered pricing or per-document costs that vary by document size?
- **Basis in paper**: [explicit] The paper mentions that retrieval APIs charge a fee per retrieved document but assumes a fixed cost per document. It also notes that some APIs charge per page
- **Why unresolved**: The paper does not explore how ProQE would perform under different pricing models or how cost-effectiveness would be impacted by varying document sizes or tiered pricing structures
- **What evidence would resolve it**: Experiments comparing ProQE's performance and cost-effectiveness across different pricing models, including tiered pricing and per-page charges, would provide insights into its adaptability to various cost structures

### Open Question 2
- **Question**: Can ProQE be adapted to handle dynamic or streaming document collections where new documents are continuously added?
- **Basis in paper**: [inferred] The paper focuses on static document collections and does not address how ProQE would handle evolving corpora
- **Why unresolved**: The algorithm is designed for a fixed set of documents, and its effectiveness in a dynamic environment where documents are added or removed over time is unclear
- **What evidence would resolve it**: Testing ProQE on a simulated streaming dataset with documents added over time and evaluating its performance and cost-effectiveness in such a setting would demonstrate its adaptability to dynamic collections

### Open Question 3
- **Question**: How does ProQE's performance compare to other cost-aware methods like FrugalGPT and EcoRank when applied to retrieval tasks?
- **Basis in paper**: [explicit] The paper mentions that FrugalGPT and EcoRank consider LLM API costs for question-answering and text re-ranking tasks but does not compare ProQE to these methods for retrieval
- **Why unresolved**: The paper focuses on the cost of document retrieval but does not evaluate how ProQE stacks up against other cost-aware methods specifically designed for retrieval tasks
- **What evidence would resolve it**: Direct comparisons between ProQE and cost-aware retrieval methods like FrugalGPT and EcoRank on the same datasets and cost structures would provide insights into their relative effectiveness and cost-efficiency

## Limitations
- The cost model assumes simple per-document pricing that may not reflect real-world API pricing complexities
- Performance evaluation relies on ranking metrics without examining downstream task performance
- Exact LLM prompting templates are not fully specified, making precise replication challenging

## Confidence
- **High confidence**: The core iterative expansion mechanism combining PRF with LLM relevance scoring is clearly specified and experimentally validated
- **Medium confidence**: The plug-and-play compatibility claim for both sparse and dense retrievers is supported but lacks detailed implementation specifics for dense systems
- **Low confidence**: The cost-effectiveness claims rely on simplified cost assumptions that may not generalize to all real-world API pricing models

## Next Checks
1. Verify the exact LLM prompting templates for term extraction and relevance scoring through ablation studies
2. Test ProQE on additional datasets with different cost structures to validate generalizability
3. Conduct a thorough cost-benefit analysis comparing ProQE against baseline methods across varying budget constraints