---
ver: rpa2
title: Interacting Large Language Model Agents. Interpretable Models and Social Learning
arxiv_id: '2411.01271'
source_url: https://arxiv.org/abs/2411.01271
tags:
- llmas
- bayesian
- llma
- agents
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops interpretable models and stochastic control
  algorithms for interacting large language model agents (LLMAs) using methods from
  statistical signal processing and microeconomics. It constructs a Bayesian sensor
  model for a single LLMA and proves necessary and sufficient conditions for it to
  be a rationally inattentive Bayesian utility maximizer (RIBUM), providing algorithms
  to reconstruct its utility function.
---

# Interacting Large Language Model Agents. Interpretable Models and Social Learning

## Quick Facts
- arXiv ID: 2411.01271
- Source URL: https://arxiv.org/abs/2411.01271
- Reference count: 40
- This paper develops interpretable models and stochastic control algorithms for interacting large language model agents using methods from statistical signal processing and microeconomics.

## Executive Summary
This paper introduces a framework for analyzing large language model agents (LLMAs) as rationally inattentive Bayesian utility maximizers. The authors construct a Bayesian sensor model for single LLMAs and prove conditions for rational inattention, providing algorithms to reconstruct their utility functions. They then study sequential social learning among multiple LLMAs, demonstrating inevitable information cascades and herding behavior. To address these limitations, the paper formulates optimal stopping problems and proposes policy gradient algorithms to delay herding. The framework is validated on real-world datasets for hate speech classification and product quality assessment.

## Method Summary
The paper constructs LLMAs with Bayesian engines for sequential inference, applying Bayesian revealed preferences to reconstruct utility functions from action-state data. It simulates Bayesian social learning protocols with multiple agents and formulates optimal stopping problems with threshold policies. A policy gradient algorithm is used to estimate optimal policy parameters without knowledge of system parameters. The approach is validated on real datasets including Measuring Hate Speech corpus, Civil Comments dataset, and Amazon Reviews dataset for classification tasks.

## Key Results
- Proved necessary and sufficient conditions for LLMAs to be rationally inattentive Bayesian utility maximizers (RIBUM)
- Demonstrated inevitable information cascades and herding behavior in sequential social learning with LLMAs
- Developed policy gradient algorithm for optimal stopping control that delays herding behavior
- Validated framework on real datasets for hate speech classification and product quality assessment

## Why This Works (Mechanism)
The framework works by modeling LLMAs as Bayesian agents that balance information acquisition costs against utility maximization. The rationally inattentive framework captures the bounded rationality of LLMAs, where agents optimally choose which information to process given cognitive constraints. The social learning component shows how sequential interactions lead to information cascades when agents prioritize private information over public signals. The optimal stopping formulation provides a mechanism to interrupt this cascade by strategically controlling when agents observe public information.

## Foundational Learning
- Bayesian Revealed Preferences: Used to reconstruct utility functions from observed actions - needed to make LLMA behavior interpretable and comparable to economic models
- Information Cascades: Explain herding behavior in sequential decision-making - needed to understand limitations of social learning with LLMAs
- Policy Gradient Methods: Optimization technique for finding optimal control policies - needed to estimate threshold parameters for optimal stopping without full system knowledge
- Rational Inattention: Economic theory of bounded rationality - needed to model LLMA behavior as utility maximization under information constraints
- Bayesian Social Learning: Sequential learning framework where agents update beliefs based on private and public information - needed to study collective behavior of interacting LLMAs

## Architecture Onboarding

**Component Map:**
LLMA construction -> Bayesian revealed preferences -> Social learning simulation -> Optimal stopping formulation -> Policy gradient optimization

**Critical Path:**
The critical path involves constructing the LLMA with Bayesian engine, applying revealed preferences to obtain utility function, running social learning simulation to observe herding, and implementing optimal stopping control to delay herding.

**Design Tradeoffs:**
- Model complexity vs interpretability: More complex Bayesian models may better capture LLMA behavior but reduce interpretability
- Prior specification: Strong priors can prevent premature herding but may bias state estimation
- Observation frequency: More frequent observations improve state estimation but increase computational cost

**Failure Signatures:**
- Premature herding: Indicates poor prior specification or overly informative private observations
- Policy gradient divergence: Suggests inappropriate step size or network architecture
- Utility function reconstruction failure: Indicates insufficient action-state data or non-rational behavior

**3 First Experiments:**
1. Single LLMA utility reconstruction on Measuring Hate Speech dataset using Algorithm 2
2. Two-agent social learning simulation with varying prior strengths to observe herding onset
3. Policy gradient training on Civil Comments dataset with different neural network architectures

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption that LLMAs can be modeled as rationally inattentive Bayesian utility maximizers may not fully capture real LLM behavior
- The framework assumes full observability of prior beliefs and observation matrices, which may not hold in practice
- Empirical validation is limited to specific classification tasks and may not generalize to other domains

## Confidence
- **High**: Mathematical proofs for RIBUM characterization and herding behavior in social learning
- **Medium**: Policy gradient algorithm effectiveness for optimal stopping control
- **Low**: Generalizability of LLMA behavioral models to diverse real-world scenarios

## Next Checks
1. Test the RIBUM framework on multiple LLM architectures (e.g., transformer-based, recurrent models) to verify robustness across different model types
2. Conduct ablation studies on the policy gradient algorithm by varying neural network architectures and hyperparameter settings to identify optimal configurations
3. Apply the social learning framework to non-classification tasks (e.g., regression, reinforcement learning) to assess broader applicability