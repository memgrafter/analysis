---
ver: rpa2
title: 'ATLAS: Adapter-Based Multi-Modal Continual Learning with a Two-Stage Learning
  Strategy'
arxiv_id: '2410.10923'
source_url: https://arxiv.org/abs/2410.10923
tags:
- learning
- knowledge
- task
- tasks
- adapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ATLAS, an adapter-based two-stage learning
  paradigm for multi-modal continual learning that addresses catastrophic forgetting
  while improving generalization. The method consists of experience-based learning,
  which leverages prior task knowledge through weighted adapter combinations, and
  novel knowledge expansion, which compensates for knowledge beyond previously seen
  tasks.
---

# ATLAS: Adapter-Based Multi-Modal Continual Learning with a Two-Stage Learning Strategy

## Quick Facts
- arXiv ID: 2410.10923
- Source URL: https://arxiv.org/abs/2410.10923
- Authors: Hong Li; Zhiquan Tan; Xingyu Li; Weiran Huang
- Reference count: 40
- Key outcome: Introduces ATLAS, an adapter-based two-stage learning paradigm that reduces catastrophic forgetting while improving downstream generalization across uni-modal and multi-modal tasks

## Executive Summary
This paper addresses the challenge of catastrophic forgetting in multi-modal continual learning by introducing ATLAS, an adapter-based two-stage learning paradigm. The method decomposes new tasks into previously learned knowledge through experience-based learning using weighted adapter combinations, then compensates for novel knowledge through a second expansion stage. Experiments demonstrate significant improvements in downstream generalization, with experience-based learning accuracy increasing from 45.81 to 49.09 and low-shot learning accuracy improving from 41.39 to 44.10 across four downstream datasets.

## Method Summary
ATLAS employs a two-stage learning strategy that first leverages prior task knowledge through weighted adapter combinations (experience-based learning), then compensates for knowledge beyond previously seen tasks (novel knowledge expansion). The method uses knowledge vector learning with cosine similarity between task inputs and adapter keys to determine adapter involvement weights. Adapter modules are added for each task, with learnable reference vectors serving as abstract descriptions. An orthogonal constraint is applied to queries and keys to prevent interference between existing and new knowledge, enabling effective continual learning while maintaining parameter efficiency.

## Key Results
- Experience-based learning accuracy increases from 45.81 to 49.09 on downstream tasks
- Low-shot learning accuracy improves from 41.39 to 44.10
- Significant reduction in catastrophic forgetting across five upstream tasks
- Achieves performance close to fine-tuning while maintaining parameter efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage learning reduces catastrophic forgetting by decomposing new tasks into previously learned knowledge before adding novel components.
- Mechanism: Experience-based learning stage uses adapter weighting (via cosine similarity between task inputs and adapter keys) to combine existing adapter knowledge. Novel knowledge expansion stage then compensates for knowledge not covered by previous adapters.
- Core assumption: Task-specific knowledge can be decomposed into weighted combinations of previously learned adapters without significant loss of information.
- Evidence anchors: [abstract]: "experience-based learning, which leverages prior task knowledge through weighted adapter combinations, and novel knowledge expansion, which compensates for knowledge beyond previously seen tasks"

### Mechanism 2
- Claim: Knowledge vector learning enables efficient selection of relevant adapters through cosine similarity computation.
- Mechanism: For each adapter Φi, a learnable reference vector ki is assigned as its abstract description. The cosine similarity between sample-level descriptions di(xt) and reference vectors ki determines adapter involvement weights.
- Core assumption: The learned reference vectors ki can effectively capture the distinctive characteristics of each adapter's knowledge domain.
- Evidence anchors: [section 4.1.1]: "we propose to compute a sample-level description vector di(xt) for t-th task input xt with respect to the i-th adapter"

### Mechanism 3
- Claim: Orthogonal constraint on queries and keys prevents interference between different task knowledge.
- Mechanism: Lortho(X) = ∥X⊤X − I∥F regularization is applied to both task-specific queries qt and adapter-specific keys Kt to ensure orthogonality.
- Core assumption: Orthogonal vectors have less interference between existing and new knowledge, improving continual learning performance.
- Evidence anchors: [section 4.1.4]: "Empirical study [19] proposes that orthogonal vectors have less interference between existing and new knowledge"

## Foundational Learning

- Concept: Adapter-based parameter-efficient fine-tuning
  - Why needed here: ATLAS uses adapters instead of full model fine-tuning to avoid catastrophic forgetting while maintaining computational efficiency
  - Quick check question: What is the primary advantage of using adapters over full fine-tuning in continual learning scenarios?

- Concept: Multi-modal model continual learning
  - Why needed here: The method handles both uni-modal and multi-modal tasks, requiring understanding of how different modalities interact during sequential learning
  - Quick check question: How does the model handle tasks that require processing only single modalities when it's a multi-modal model?

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The core problem ATLAS addresses is preventing the model from forgetting previously learned tasks while learning new ones
  - Quick check question: What happens to the performance on previously learned tasks when a model learns a new task without any forgetting prevention mechanism?

## Architecture Onboarding

- Component map: Pre-trained vision-language model (M0) -> Adapter modules (Φt) -> Task-specific queries (qi t) -> Adapter reference vectors (ki) -> Experience-based learning stage -> Novel knowledge expansion stage -> Classifier

- Critical path: Input -> Cross-attention with queries -> Cosine similarity with keys -> Adapter weighting -> Adapter execution -> Classification

- Design tradeoffs:
  - Parameter efficiency vs. expressivity: Using adapters reduces parameters but may limit model capacity
  - Task decomposition vs. direct learning: Experience-based learning trades off some new task performance for reduced forgetting
  - Orthogonal constraints vs. flexibility: Preventing interference may restrict meaningful task relationships

- Failure signatures:
  - Degradation in experience-based learning accuracy indicates adapter decomposition is failing
  - Increasing forgetting across tasks suggests the two-stage approach isn't preventing interference
  - Low knowledge coefficients across all adapters indicate the similarity computation isn't working

- First 3 experiments:
  1. Test adapter weighting on a simple continual learning benchmark with known task relationships to verify cosine similarity selection
  2. Measure forgetting on a single task after learning multiple subsequent tasks to validate the two-stage approach
  3. Compare downstream generalization with and without orthogonal constraints to assess interference prevention effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the two-stage learning paradigm work equally well with different backbone architectures like CLIP, or is it specifically optimized for vision-language models like ViLT?
- Basis in paper: [explicit] The authors state "Future work includes applying our method to backbones like CLIP [16]" indicating this remains unexplored.

### Open Question 2
- Question: How does the orthogonal constraint between queries and adapter keys affect the final performance, and what happens if this constraint is removed?
- Basis in paper: [explicit] The authors mention adding orthogonal constraints "to avoid interference between existing and new knowledge" but don't explore the impact of removing this constraint.

### Open Question 3
- Question: Why does uni-modal task knowledge (like PIQA) sometimes improve performance on uni-modal tasks with different modalities (like Places365), despite the authors' intuition that "language task knowledge does not always translate to improvements in image tasks"?
- Basis in paper: [explicit] The authors observe this phenomenon and speculate "knowledge from a single-modal adapter contributes to the utilization of multi-modal models for single-modal tasks" but don't provide a definitive explanation.

## Limitations
- Weak theoretical grounding for why adapter decomposition reduces forgetting
- Missing comprehensive ablation studies on key components
- Limited task diversity with only five upstream tasks tested

## Confidence

| Assessment | Confidence Level |
|------------|------------------|
| Empirical results on tested task sequence | High |
| Mechanism claims about cosine similarity-based adapter selection | Medium |
| Generalization claims to different task modalities | Low |

## Next Checks

1. **Ablation study**: Remove the orthogonal constraint and knowledge vector learning components separately to quantify their individual impact on forgetting and downstream performance

2. **Cross-modal generalization**: Test ATLAS on downstream tasks that require different modality combinations than the upstream tasks to verify true multi-modal knowledge transfer

3. **Longer task sequences**: Evaluate the method on task sequences longer than five tasks to assess scalability and identify potential performance degradation points