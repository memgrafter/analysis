---
ver: rpa2
title: 'REAL: Response Embedding-based Alignment for LLMs'
arxiv_id: '2409.17169'
source_url: https://arxiv.org/abs/2409.17169
tags:
- response
- responses
- pairs
- dataset
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'REAL: Response Embedding-based Alignment for LLMs addresses the
  challenge of building high-quality preference datasets for LLM alignment. The paper
  proposes a method that selects less ambiguous response pairs for labeling by analyzing
  the similarity of response embeddings independently of prompts.'
---

# REAL: Response Embedding-based Alignment for LLMs

## Quick Facts
- **arXiv ID**: 2409.17169
- **Source URL**: https://arxiv.org/abs/2409.17169
- **Reference count**: 21
- **Primary result**: Response embedding-based selection improves LLM alignment by reducing annotation ambiguity and saving up to 65% of annotators' work

## Executive Summary
REAL proposes a method for improving LLM alignment by selecting less ambiguous response pairs for human preference labeling. The key insight is that pairs with low embedding similarity produce clearer preference judgments, reducing annotation errors and improving alignment efficiency. By analyzing response embeddings independently of prompts, the method identifies "easy" pairs that are semantically distinct, enabling more confident human judgments. Experiments on multiple datasets demonstrate that models trained on dissimilar pairs achieve higher margins, better loss values, and improved win rates compared to random or similar pairs.

## Method Summary
The method generates K responses per prompt, calculates embeddings for each response using mean pooling over last hidden states, and computes pairwise cosine similarities. Four selection strategies are employed: "easy" (most dissimilar pairs), "hard" (most similar pairs), "centroid" (cluster representatives), and "random" (uniform sampling). Human annotators then label preferred responses, and DPO training is performed on the selected subsets. The approach leverages the stability of response embeddings during training to enable off-policy selection, reducing computational overhead while maintaining data quality.

## Key Results
- Models trained on dissimilar pairs achieve higher margins and better loss values than those trained on random or similar pairs
- Centroid selection strategy demonstrates the best overall performance by balancing easy and random pair benefits
- The method saves up to 65% of annotators' work while improving alignment efficiency
- GPT-4 evaluation shows improved win rates for models trained with the proposed selection strategy

## Why This Works (Mechanism)

### Mechanism 1
Selecting response pairs with low embedding similarity reduces label noise in preference data. Human annotators make fewer errors when distinguishing between clearly different responses, as dissimilar pairs have larger embedding distances making preference judgment more obvious and less ambiguous. The core assumption is that the semantic embedding space captures human preference differences such that dissimilar responses are easier to rank correctly.

### Mechanism 2
Response embeddings remain stable during DPO training, enabling off-policy selection. The internal representation of responses changes minimally during fine-tuning because pre-training has already established robust representations, allowing selecting pairs before training without compromising their quality. The core assumption is that response embeddings are invariant to changes in the model's conditional probability distribution during DPO.

### Mechanism 3
Centroid selection balances the benefits of easy and random pairs while avoiding the pitfalls of hard pairs. By clustering responses and selecting representatives from each cluster, the method obtains pairs that are neither too similar (hard) nor too random, achieving better alignment performance. The core assumption is that the optimal pairs for alignment lie between the extremes of maximum and minimum similarity.

## Foundational Learning

- **Concept**: Direct Preference Optimization (DPO)
  - **Why needed here**: REAL builds upon DPO by improving the quality of the preference dataset used for training. Understanding DPO's mechanics is essential to grasp why dataset quality matters.
  - **Quick check question**: What is the key difference between DPO and traditional RLHF in terms of reward modeling?

- **Concept**: Sentence embeddings and cosine similarity
  - **Why needed here**: REAL uses embedding similarity to select response pairs. Understanding how embeddings capture semantic meaning and how cosine similarity measures this is crucial.
  - **Quick check question**: How does averaging token embeddings differ from using [CLS] tokens for sentence representation?

- **Concept**: Contrastive learning principles
  - **Why needed here**: REAL draws inspiration from contrastive learning, where similar examples help define decision boundaries. Understanding this helps explain why dissimilar pairs might be beneficial for alignment.
  - **Quick check question**: In contrastive learning, why are both similar and dissimilar pairs important for learning good representations?

## Architecture Onboarding

- **Component map**: Response generator (LLM) → Response embedding calculator → Similarity scorer → Pair selector (hard/easy/centroid/random) → Human annotator → DPO trainer
- **Critical path**: 1) Generate K responses per prompt 2) Calculate embeddings for each response 3) Compute pairwise cosine similarities 4) Select optimal pair based on strategy 5) Human labels preferred response 6) Train DPO model on labeled pairs
- **Design tradeoffs**: Off-policy vs on-policy selection saves computation but may miss adaptive improvements; K value affects selection quality vs computational cost; embedding method choice impacts similarity measurements
- **Failure signatures**: Poor alignment performance despite clean data selection; embeddings showing high variance during training; frequent human annotator disagreements even with "easy" pairs; centroid selection producing pairs similar to random selection
- **First 3 experiments**: 1) Compare alignment performance using easy vs random pairs on a small dataset 2) Test embedding stability by tracking similarity distributions across training checkpoints 3) Evaluate different selection strategies (hard, easy, centroid, random) on the same base model

## Open Questions the Paper Calls Out

### Open Question 1
How do response embeddings change when using different embedding methods (e.g., mean pooling vs. CLS token) for LLM alignment? The paper mentions using mean pooling over last hidden states but does not compare this to other methods. Experimental results comparing alignment performance using different embedding methods would resolve this.

### Open Question 2
Does the effectiveness of selecting dissimilar response pairs for alignment vary with model size (e.g., 1.3B vs. 70B parameters)? The paper conducted experiments on 1.3B to 7B parameter models but acknowledges results for larger models are unknown. Alignment experiments on models of varying sizes would provide clarity.

### Open Question 3
How does the proposed method perform when applied to online-DPO compared to the offline setting? The paper mentions the method can be extended to online-DPO but does not provide experimental results. Experimental results comparing offline and online-DPO settings would resolve this.

### Open Question 4
What is the impact of using different similarity metrics (e.g., cosine similarity vs. Euclidean distance) for selecting response pairs on alignment performance? The paper uses cosine similarity but does not explore other metrics. Experimental results comparing different similarity metrics would provide insights.

## Limitations

- Embedding stability assumption requires extensive validation across different model sizes and training durations
- Method validation primarily on SHP2 and HH-RLHF datasets may not generalize to diverse preference datasets
- Optimal selection strategy characteristics lack theoretical grounding for why centroid pairs outperform others

## Confidence

- **High confidence**: Core observation that dissimilar response pairs reduce annotation ambiguity and improve alignment efficiency is well-supported empirically
- **Medium confidence**: Embedding stability during training is plausible but requires more extensive validation
- **Medium confidence**: Centroid selection strategy's superiority is demonstrated empirically but lacks theoretical explanation

## Next Checks

1. **Embedding distribution tracking**: Monitor cosine similarity distributions across multiple training checkpoints for different model sizes to verify claimed stability of response embeddings during DPO training

2. **Cross-dataset generalization**: Apply REAL method to diverse preference datasets with different domains and response formats to test whether similarity-based selection maintains effectiveness

3. **Ablation on selection thresholds**: Systematically vary similarity thresholds used for "easy," "hard," and "centroid" pair selection to identify optimal ranges and understand sensitivity of alignment performance