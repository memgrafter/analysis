---
ver: rpa2
title: 'CNN2GNN: How to Bridge CNN with GNN'
arxiv_id: '2404.14822'
source_url: https://arxiv.org/abs/2404.14822
tags:
- graph
- neural
- networks
- learning
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of combining the strengths of
  CNNs and GNNs. CNNs excel at extracting intra-sample representations but require
  many layers and resources.
---

# CNN2GNN: How to Bridge CNN with GNN

## Quick Facts
- arXiv ID: 2404.14822
- Source URL: https://arxiv.org/abs/2404.14822
- Reference count: 10
- Primary result: Proposed CNN2GNN framework outperforms existing knowledge distillation methods and achieves higher accuracy than CNN teacher on CIFAR-100 and Mini-ImageNet

## Executive Summary
This paper addresses the challenge of combining the strengths of CNNs and GNNs by proposing a CNN2GNN framework that uses knowledge distillation. CNNs excel at extracting intra-sample representations but require many layers and resources, while GNNs efficiently learn topological relationships among graph data but cannot directly handle non-graph data and have high inference latency. The authors propose a novel approach that bridges these two heterogeneous networks by designing a differentiable sparse graph learning module that dynamically learns graph structures for inductive learning on non-graph data, and employs a response-based distillation strategy to transfer knowledge from CNN teacher to GNN student. The distilled two-layer GNN can simultaneously extract both intra-sample representations and topological relationships among instances.

## Method Summary
The CNN2GNN framework consists of a pretrained CNN teacher, a differentiable sparse graph learning head, and a two-layer GNN student. The graph head dynamically learns a graph structure by computing pairwise distances between samples, converting them to conditional probabilities with explicit sparsity control, and constructing a symmetric adjacency matrix. A response-based distillation strategy transfers knowledge from the CNN teacher to the GNN student using a combined loss of cross-entropy and KL-divergence between teacher and student logits. For inference, the framework uses a two-stage testing mechanism that approximates the graph structure using the most similar training samples for efficiency.

## Key Results
- CNN2GNN achieves higher accuracy than the CNN teacher, particularly on complex datasets like Mini-ImageNet
- The framework outperforms existing knowledge distillation methods on CIFAR-100 and Mini-ImageNet benchmarks
- The two-stage testing mechanism improves inference efficiency while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
The differentiable sparse graph head can inductively learn a graph structure for non-graph data by minimizing a distance-based probability formulation with sparsity constraints. It computes pairwise distances between samples using a learned function, converts them into conditional probabilities with explicit sparsity control (top-s similarities), and constructs a symmetric adjacency matrix that is differentiable and task-aware.

### Mechanism 2
The two-stage testing mechanism improves inference efficiency by approximating the graph structure using the most similar training samples. For a test batch, it first computes pairwise probabilities between test and training samples, selects the most probable training neighbors for each test sample, and builds an approximated sparse graph from these pairs.

### Mechanism 3
Response-based heterogeneous distillation transfers both the intra-sample representation and topological knowledge from CNN to GNN, boosting GNN performance beyond the CNN teacher. The CNN teacher's logits are used as soft targets in a KL-divergence loss combined with cross-entropy on the GNN student's predictions, allowing the student to mimic both the deep representation and relational patterns.

## Foundational Learning

- **Graph Neural Networks (GNN)**: GNNs are used as the student model to learn topological relationships among data points, complementing CNNs' intra-sample feature extraction. Quick check: What is the key difference between a GNN and a CNN in terms of the data they process and the information they capture?

- **Knowledge Distillation**: Distillation is the mechanism for transferring knowledge from the large CNN teacher to the small GNN student, enabling the student to benefit from the teacher's learned representations. Quick check: In knowledge distillation, what is the role of the temperature parameter τ in the soft target distribution?

- **Differentiable Sparse Graph Learning**: This allows the model to construct a graph structure directly from non-graph data in a way that is both sparse and task-aware, enabling GNNs to be applied to image datasets. Quick check: How does the sparsity parameter s in the graph learning head affect the number of edges in the induced graph?

## Architecture Onboarding

- **Component map**: CNN Teacher -> Differentiable Sparse Graph Head -> GNN Student -> Distillation Loss

- **Critical path**: 
  1. Pretrain CNN teacher on the dataset
  2. Initialize GNN student and graph head
  3. For each training batch: a) Compute graph adjacency using the graph head, b) Forward pass through GNN student, c) Compute combined loss (cross-entropy + distillation), d) Backpropagate and update student and graph head
  4. For inference: a) Use Mechanism 1 or 2 to construct graph for test samples, b) Forward pass through GNN student to get predictions

- **Design tradeoffs**: 
  - Graph sparsity vs. connectivity: Higher sparsity reduces computation but may lose important relationships
  - Distillation temperature: Higher τ produces softer targets, which can help with heterogeneous distillation but may slow convergence
  - Mechanism 1 vs. 2: Mechanism 1 is more accurate but slower; Mechanism 2 is faster but less precise

- **Failure signatures**:
  - Low accuracy on test set: Could indicate poor graph learning, ineffective distillation, or mismatch between teacher and student capacity
  - High variance in results: May suggest instability in graph construction or sensitivity to hyperparameters like sparsity or temperature
  - Slow training/inference: Could be due to dense graphs or inefficient implementation of the graph head

- **First 3 experiments**:
  1. Train the CNN teacher on CIFAR-100 and evaluate its baseline accuracy
  2. Train the GNN student with the graph head but without distillation (supervised only) to assess the standalone GNN performance
  3. Perform a sensitivity analysis on the sparsity parameter s and temperature τ to find a stable operating point before full distillation training

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CNN2GNN scale with increasing dataset complexity beyond Mini-ImageNet, such as on larger-scale image datasets? The paper demonstrates CNN2GNN's performance on CIFAR-100 and Mini-ImageNet but does not explore performance on datasets larger or more complex than Mini-ImageNet.

### Open Question 2
Can the differentiable sparse graph head be adapted for use with other types of neural networks beyond CNNs and GNNs, such as transformers or recurrent networks? The paper focuses on bridging CNNs and GNNs using a differentiable sparse graph head, suggesting potential for broader applicability.

### Open Question 3
What are the theoretical limits of the response-based heterogeneous distillation strategy in terms of the types of knowledge that can be effectively transferred between different neural network architectures? The paper introduces this strategy but does not delve into its theoretical underpinnings or limitations.

## Limitations
- The differentiable sparse graph learning head scales with the number of training samples (n neurons in the decision layer), creating computational overhead
- Mechanism 2 may produce poor graph approximations when test samples are dissimilar to training data
- The framework assumes the teacher's deep representations are meaningful for the student, which may not hold across very different architectures

## Confidence
- **High confidence**: CNNs and GNNs have complementary strengths - CNNs for intra-sample feature extraction and GNNs for topological relationships
- **Medium confidence**: The differentiable sparse graph learning head effectively learns meaningful graph structures
- **Low confidence**: The distilled GNN consistently outperforms the CNN teacher across all dataset complexities

## Next Checks
1. Evaluate the learned graph structures on CIFAR-100 using metrics like edge density, clustering coefficient, and node degree distribution to verify meaningful topologies
2. Perform a systematic hyperparameter sweep of τ (20-26) and s (10-90) to identify optimal operating point and assess stability
3. Compute similarity metrics (e.g., centered kernel alignment) between CNN teacher and GNN student embeddings to quantify how well distillation transfers complementary information