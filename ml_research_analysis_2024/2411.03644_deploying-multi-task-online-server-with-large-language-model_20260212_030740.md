---
ver: rpa2
title: Deploying Multi-task Online Server with Large Language Model
arxiv_id: '2411.03644'
source_url: https://arxiv.org/abs/2411.03644
tags:
- tasks
- task
- multi-task
- performance
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying multiple NLP tasks
  online efficiently using large language models. It proposes a three-stage multi-task
  learning framework that includes task filtering, fine-tuning on high-resource tasks,
  and fine-tuning on all tasks.
---

# Deploying Multi-task Online Server with Large Language Model

## Quick Facts
- arXiv ID: 2411.03644
- Source URL: https://arxiv.org/abs/2411.03644
- Reference count: 22
- Primary result: Achieves up to 90.9% reduction in serving costs compared to single-task methods while maintaining performance comparable to single-task models

## Executive Summary
This paper addresses the challenge of deploying multiple NLP tasks online efficiently using large language models. It proposes a three-stage multi-task learning framework that includes task filtering, fine-tuning on high-resource tasks, and fine-tuning on all tasks. The method aims to achieve performance comparable to single-task models while reducing deployment overhead. Experiments on CLUE and industry benchmarks show that the proposed approach achieves up to 90.9% reduction in serving costs compared to single-task methods, with the number of qualified tasks (achieving 99% of single-task performance) increasing significantly. The method demonstrates effectiveness across different task taxonomies and scales well to real-world applications with 17 tasks.

## Method Summary
The proposed method is a three-stage multi-task learning framework for deploying large language models online. First, tasks are filtered based on type to prevent negative transfer between heterogeneous tasks (generation vs. classification). Second, the model is fine-tuned on high-resource tasks using instance-balanced sampling to ensure proper convergence. Finally, all tasks are fine-tuned simultaneously using temperature-scaled sampling with artificial dataset size limits to balance the influence of tasks with imbalanced data distributions. The approach is evaluated on CLUE benchmark and industry customer service datasets, demonstrating significant reductions in deployment costs while maintaining performance comparable to single-task models.

## Key Results
- Achieves up to 90.9% reduction in serving costs compared to single-task methods
- Increases number of qualified tasks (achieving ≥99% of single-task performance) from 3 to 5 in experiments
- Demonstrates effectiveness across different task taxonomies with up to 3.0% improvement on CLUE benchmark
- Scales successfully to 17 domain-specific customer service tasks in industry applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task filtering prevents negative transfer between dissimilar tasks by removing incompatible task types from the training mixture.
- Mechanism: By categorizing tasks into generation vs. classification types and excluding incompatible pairs, the model avoids conflicting gradient directions during optimization, which would otherwise degrade performance on both task types.
- Core assumption: Generation tasks (flexible output) and classification tasks (fixed output) have fundamentally different optimization dynamics that create conflicting gradients when trained together.
- Evidence anchors:
  - [abstract] "task filtering strategy prevents the negative transfer between heterogeneous tasks"
  - [section] "We found that generation tasks and classification tasks would hinder each others' performance in multi-task training"
  - [corpus] Weak evidence - related papers focus on multi-task learning efficiency but don't specifically address task type filtering
- Break condition: If task similarity boundaries are too strict, potentially beneficial knowledge transfer between slightly different tasks may be lost.

### Mechanism 2
- Claim: Two-stage training with high-resource tasks first enables better learning dynamics by allowing different tasks to have different training epochs.
- Mechanism: By first fine-tuning on high-resource tasks (which need more epochs), then on all tasks with artificial dataset size limits, the model can achieve early stopping for low-resource tasks while ensuring high-resource tasks converge properly, preventing overfitting and underfitting simultaneously.
- Core assumption: Different tasks have different saturation points, and a single training duration cannot optimally train both high-resource and low-resource tasks simultaneously.
- Evidence anchors:
  - [abstract] "fine-tuning on high-resource tasks, and finally fine-tuning on all tasks"
  - [section] "we categorize tasks based on the training saturation steps in the single-task setting"
  - [corpus] Weak evidence - related papers discuss multi-task learning but don't specifically address staged training with different epochs
- Break condition: If task categorization is incorrect, some tasks may be overfit or underfit during their respective training stages.

### Mechanism 3
- Claim: Temperature-scaled sampling with dataset size limits balances the influence of tasks with imbalanced data distributions.
- Mechanism: By scaling sampling rates with temperature τ and capping dataset sizes at K, the method ensures that both high-resource and low-resource tasks contribute meaningfully to training without overwhelming the optimization process.
- Core assumption: Simple instance-balanced or class-balanced sampling cannot adequately address the extreme data imbalance present in real-world multi-task scenarios.
- Evidence anchors:
  - [abstract] "utilize temperature-scaled sampling and impose an artificial limit on dataset size"
  - [section] "We utilize temperature-scaled sampling and impose an artificial limit on dataset size to train all downstream tasks simultaneously"
  - [corpus] Moderate evidence - related papers discuss sampling strategies but the specific combination with dataset limits is novel
- Break condition: If temperature τ is poorly chosen, it may over- or under-weight certain tasks in the training mixture.

## Foundational Learning

- Concept: Multi-task learning optimization dynamics
  - Why needed here: Understanding how gradients from different tasks interact is crucial for implementing effective task filtering and preventing negative transfer
  - Quick check question: What happens to model performance when incompatible tasks (like generation and classification) are trained together without filtering?

- Concept: Data imbalance and sampling strategies
  - Why needed here: The temperature-scaled sampling with dataset limits directly addresses the imbalance problem, so understanding how different sampling strategies affect learning is essential
  - Quick check question: How does instance-balanced sampling differ from class-balanced sampling in handling tasks with vastly different dataset sizes?

- Concept: Task similarity and taxonomy
  - Why needed here: The paper demonstrates that more granular task categorization (binary vs ordinal vs multi-class) improves performance, so understanding task taxonomy is important for system design
  - Quick check question: Why might training binary and multi-class classification tasks together lead to worse performance than training them separately?

## Architecture Onboarding

- Component map: Task filtering module → High-resource task fine-tuning → Tasks mixture fine-tuning with temperature-scaled sampling → Model deployment
- Critical path: Task filtering → High-resource task fine-tuning → Tasks mixture fine-tuning with temperature-scaled sampling → Model deployment
- Design tradeoffs: The filtering approach improves performance but may reduce the number of deployable tasks; the two-stage training improves learning dynamics but adds complexity; temperature scaling requires hyperparameter tuning for τ and K.
- Failure signatures: Performance degradation when task filtering is too aggressive (missing beneficial transfer), incorrect task categorization leading to underfitting/overfitting, poor temperature parameter choices causing imbalanced learning.
- First 3 experiments:
  1. Implement task filtering and verify performance improvement on classification-only vs mixed task sets
  2. Test two-stage training vs single-stage training on a small set of high-resource and low-resource tasks
  3. Experiment with different temperature values (τ) and dataset size limits (K) to find optimal sampling parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal temperature (τ) value for temperature-scaled sampling across different task mixtures?
- Basis in paper: [explicit] The paper uses τ values of (1.43, 2, 3.33) and mentions that τ → ∞ approaches class-balanced sampling, but does not explore the optimal value for different task combinations.
- Why unresolved: The paper demonstrates effectiveness of temperature-scaled sampling but doesn't systematically explore how different τ values affect performance across various task mixtures.
- What evidence would resolve it: Systematic experiments varying τ values across different task combinations and measuring resulting performance metrics.

### Open Question 2
- Question: How does the proposed method scale to extremely large task sets (100+ tasks) with diverse characteristics?
- Basis in paper: [inferred] The paper shows effectiveness up to 17 tasks in industry applications, but doesn't test extreme scaling scenarios where task heterogeneity and data imbalance become more pronounced.
- Why unresolved: The paper demonstrates practical effectiveness but doesn't explore theoretical limits of the method's scalability.
- What evidence would resolve it: Empirical results showing performance degradation points and task characteristics that cause breakdown of the method.

### Open Question 3
- Question: What is the impact of different domain-specific continual pre-training strategies on multi-task performance?
- Basis in paper: [explicit] The paper mentions using domain-specific pre-training for customer service tasks but doesn't compare different pre-training strategies or their relative effectiveness.
- Why unresolved: The paper shows domain pre-training helps but doesn't explore how different pre-training approaches (e.g., different corpus sizes, training objectives) affect final performance.
- What evidence would resolve it: Comparative experiments testing various domain pre-training strategies and their impact on multi-task performance metrics.

### Open Question 4
- Question: How do different task filtering strategies (beyond the simple classification/generation split) affect multi-task performance?
- Basis in paper: [explicit] The paper uses a basic filtering strategy separating generation and classification tasks, but acknowledges more granular categorization is possible.
- Why unresolved: The paper demonstrates benefits of task filtering but doesn't explore optimal filtering strategies for complex task mixtures.
- What evidence would resolve it: Systematic evaluation of various filtering strategies and their impact on qualified task counts and performance metrics.

## Limitations
- Evaluation primarily focuses on Chinese NLP tasks, limiting generalizability to other languages
- Exact implementation details of temperature-scaled sampling mechanism are not fully specified
- Computational overhead of three-stage training process is not thoroughly analyzed

## Confidence

- **High confidence**: The core claim that task filtering prevents negative transfer between heterogeneous tasks is well-supported by experimental results
- **Medium confidence**: The effectiveness of the two-stage training approach is demonstrated, but task categorization criteria could benefit from more detailed analysis
- **Medium confidence**: The temperature-scaled sampling strategy shows promising results, but sensitivity to hyperparameter choices limits definitive conclusions

## Next Checks

1. **Cross-linguistic validation**: Test the three-stage framework on non-Chinese languages to verify generalizability across different linguistic structures and task distributions.

2. **Hyperparameter sensitivity analysis**: Conduct systematic study varying temperature τ and dataset size limits K across multiple task combinations to establish guidelines for optimal parameter selection.

3. **End-to-end deployment analysis**: Measure actual computational overhead and latency of the three-stage training process compared to single-task training, including both pre-training and fine-tuning phases.