---
ver: rpa2
title: 'ChartEye: A Deep Learning Framework for Chart Information Extraction'
arxiv_id: '2408.16123'
source_url: https://arxiv.org/abs/2408.16123
tags:
- text
- chart
- classification
- recognition
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes ChartEye, a deep learning framework for automated
  information extraction from chart images, addressing the complex multi-tasked nature
  of chart understanding due to style variations. The framework utilizes hierarchical
  vision transformers for chart-type and text-role classification, YOLOv7 for text
  detection, and ESRGAN for text resolution enhancement prior to recognition.
---

# ChartEye: A Deep Learning Framework for Chart Information Extraction

## Quick Facts
- arXiv ID: 2408.16123
- Source URL: https://arxiv.org/abs/2408.16123
- Authors: Osama Mustafa; Muhammad Khizer Ali; Momina Moetesum; Imran Siddiqi
- Reference count: 40
- Key outcome: F1-scores of 0.97 for chart-type classification, 0.91 for text-role classification, and mAP of 0.95 for text detection

## Executive Summary
ChartEye is a deep learning framework designed to automate the extraction of structured numerical data from chart images. The framework addresses the complex multi-tasked nature of chart understanding by employing a multi-stage pipeline that includes hierarchical vision transformers for chart-type and text-role classification, YOLOv7 for text detection, and ESRGAN for text resolution enhancement prior to recognition. The proposed approach demonstrates state-of-the-art performance on benchmark datasets, showcasing its effectiveness in handling various chart types and text elements.

## Method Summary
ChartEye employs a multi-stage deep learning pipeline to extract information from chart images. The process begins with chart-type classification using a Swin transformer, followed by text detection using YOLOv7. Detected text regions are then enhanced using ESRGAN to improve OCR accuracy, and finally, a text-role classification step using another Swin transformer assigns semantic roles to the recognized text. This hierarchical approach allows for robust handling of diverse chart styles and text elements.

## Key Results
- F1-score of 0.97 for chart-type classification
- F1-score of 0.91 for text-role classification
- Mean Average Precision (mAP) of 0.95 for text detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical vision transformers improve chart-type classification accuracy by capturing multi-scale features and positional dependencies.
- Mechanism: Swin transformer uses shifted window-based multi-head self-attention (SW-MSA) to aggregate features across different scales and capture long-range dependencies. This allows the model to better distinguish between visually similar chart types and handle variations in chart structure.
- Core assumption: Chart-type classification benefits more from attention-based contextual learning than from traditional CNN features.
- Evidence anchors:
  - [abstract] "Experimental results on a benchmark dataset show that our proposed framework achieves excellent performance at every stage with F1-scores of 0.97 for chart-type classification."
  - [section] "Due to contextual learning, it is more robust in the classification of chart types i.e. 15 in our case as shown in Table II."
  - [corpus] Weak evidence - corpus neighbors do not directly address Swin transformer performance on chart-type classification.
- Break condition: If the chart dataset contains many visually similar chart types or extreme style variations, the shifted window approach may fail to capture subtle distinctions.

### Mechanism 2
- Claim: YOLOv7's one-stage detection architecture provides faster and more precise text detection compared to two-stage detectors for chart images.
- Mechanism: YOLOv7 divides the input image into grids and performs object detection within each grid, allowing for real-time detection of text regions without the overhead of region proposal generation. This is particularly effective for the diverse text sizes and orientations found in charts.
- Core assumption: The computational efficiency and precision of YOLOv7 outweigh the potential accuracy benefits of more complex two-stage detectors in the chart domain.
- Evidence anchors:
  - [section] "Although recently, transformer architectures have been utilized, in addition to some convolutional ensembles for this task [2], however, YOLOv7 performs considerably in terms of precision and time-complexity."
  - [abstract] "YOLOv7 for text detection."
  - [corpus] No direct evidence in corpus neighbors about YOLOv7 performance on chart text detection.
- Break condition: If the chart contains densely packed or overlapping text elements, YOLOv7's grid-based approach may struggle with precise localization.

### Mechanism 3
- Claim: ESRGAN-based text resolution enhancement improves OCR accuracy by recovering high-frequency details lost in low-resolution chart text.
- Mechanism: ESRGAN uses a residual-in-residual dense block (RRDB) architecture without batch normalization to learn a mapping from low-resolution to high-resolution text images. This enhances character clarity and stroke definition, enabling more accurate recognition by the downstream OCR model.
- Core assumption: The improvement in text recognition accuracy from ESRGAN enhancement outweighs the computational cost and potential artifacts introduced by super-resolution.
- Evidence anchors:
  - [section] "In most chart samples, the resolution of the text is very low resulting in poor recognition even by a mature text recognizer. Thus, we propose an additional step utilizing enhanced super resolution generative adversarial network (ESRGAN) [35] to upscale the image without losing the pixel information as compared to conventional image upscaling techniques."
  - [abstract] "The detected text is then enhanced using Super Resolution Generative Adversarial Networks to improve the recognition output of the OCR."
  - [corpus] No direct evidence in corpus neighbors about ESRGAN performance on chart text.
- Break condition: If the upscaling factor is too high (e.g., 3.0x as mentioned in the paper), ESRGAN may introduce distortions that degrade OCR accuracy.

## Foundational Learning

- Concept: Vision Transformers and Self-Attention
  - Why needed here: Understanding how Swin transformers use shifted window self-attention to capture long-range dependencies is crucial for grasping why they outperform CNNs on chart-type classification.
  - Quick check question: How does the shifted window mechanism in Swin transformers differ from standard self-attention, and why is this beneficial for chart understanding?

- Concept: Object Detection Architectures (YOLO vs. RCNN)
  - Why needed here: Knowing the trade-offs between one-stage detectors like YOLOv7 and two-stage detectors like RCNN is essential for understanding the design choice for text detection in charts.
  - Quick check question: What are the key architectural differences between YOLO and RCNN, and how do these differences impact speed and accuracy?

- Concept: Super-Resolution GANs (ESRGAN)
  - Why needed here: Understanding how ESRGAN learns to enhance low-resolution images is important for appreciating its role in improving OCR accuracy on chart text.
  - Quick check question: How does ESRGAN's residual-in-residual dense block architecture contribute to its ability to recover high-frequency details in text images?

## Architecture Onboarding

- Component map: Input Chart Image -> Chart-Type Classification (Swin transformer) -> Text Detection (YOLOv7) -> Text Upscaling (ESRGAN) -> Text Recognition (TPS-ResNet-BiLSTM-Attn) -> Text-Role Classification (Swin transformer)

- Critical path: Chart image -> Chart-Type Classification -> Text Detection -> Text Upscaling -> Text Recognition -> Text-Role Classification

- Design tradeoffs:
  - Using Swin transformer for both chart-type and text-role classification provides consistency but may limit the ability to capture task-specific features.
  - ESRGAN enhancement improves OCR accuracy but adds computational overhead and potential for artifacts.
  - YOLOv7 offers speed and precision but may struggle with densely packed text compared to RCNN variants.

- Failure signatures:
  - Low chart-type classification accuracy: Indicates the Swin transformer is not capturing the distinguishing features between chart types.
  - Missed or incorrect text detection: Suggests YOLOv7 is not localizing text regions effectively, possibly due to extreme text size variations or overlapping elements.
  - Poor OCR results despite ESRGAN enhancement: May indicate the super-resolution model is introducing distortions or the OCR model is not robust to the enhanced text style.

- First 3 experiments:
  1. Evaluate chart-type classification accuracy on a held-out test set to verify the Swin transformer is learning the correct features.
  2. Visualize YOLOv7's text detection results on sample chart images to assess localization accuracy and identify failure modes.
  3. Compare OCR accuracy with and without ESRGAN enhancement on low-resolution text samples to quantify the benefit of the super-resolution step.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's robustness to chart variations in style, color schemes, and text formatting remains uncertain.
- The computational efficiency of the multi-stage pipeline, particularly the ESRGAN enhancement step, is not thoroughly evaluated for real-time applications.
- The study lacks validation on real-world, noisy chart images from diverse sources.

## Confidence

- Chart-type classification (F1 0.97): High confidence, supported by strong quantitative results and mechanism explanation
- Text detection (mAP 0.95): Medium confidence, though YOLOv7's performance on charts is not directly validated in the literature
- Text-role classification (F1 0.91): Medium confidence, limited discussion of how semantic roles are defined and validated
- ESRGAN enhancement benefit: Low confidence, no quantitative comparison of OCR accuracy with and without enhancement

## Next Checks
1. Evaluate framework performance on a diverse, real-world chart dataset (e.g., from academic papers, business reports) to assess robustness to style variations and noise.
2. Conduct ablation studies to quantify the individual contributions of each component (chart-type classification, text detection, ESRGAN enhancement) to overall pipeline performance.
3. Measure the computational efficiency of the complete pipeline, including inference time and memory usage, to determine its suitability for real-time applications.