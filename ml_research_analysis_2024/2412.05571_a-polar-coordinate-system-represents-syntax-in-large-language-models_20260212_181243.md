---
ver: rpa2
title: A polar coordinate system represents syntax in large language models
arxiv_id: '2412.05571'
source_url: https://arxiv.org/abs/2412.05571
tags:
- probe
- syntactic
- polar
- dependency
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of understanding how syntactic
  trees, originally formalized with symbolic representations, are represented in the
  activations of large language models (LLMs). The authors hypothesize that syntactic
  relations are coded by the relative direction between nearby embeddings, rather
  than just the distance between them as previously proposed.
---

# A polar coordinate system represents syntax in large language models

## Quick Facts
- arXiv ID: 2412.05571
- Source URL: https://arxiv.org/abs/2412.05571
- Reference count: 13
- The Polar Probe significantly outperforms the Structural Probe by nearly two-fold, achieving 95% AUC score and 70.2 Labeled Attachment Score on the English Web Treebank dataset.

## Executive Summary
This study addresses how syntactic trees, originally formalized with symbolic representations, are represented in the activations of large language models (LLMs). The authors introduce a "Polar Probe," a linear transformation trained to read syntactic relations from both the distance and the direction between word embeddings. The Polar Probe significantly outperforms the previous "Structural Probe" by nearly two-fold, achieving a 95% AUC score compared to the Structural Probe's 74%. It also achieves a 70.2 Labeled Attachment Score (LAS) on the English Web Treebank dataset, demonstrating its effectiveness in recovering both the existence and the type of syntactic relations. This work provides evidence that LLMs spontaneously learn a geometry of neural activations that explicitly represents the main symbolic structures of linguistic theory.

## Method Summary
The Polar Probe is a linear transformation trained to read syntactic relations from LLM activations by jointly optimizing structural and angular objectives. It operates on edge embeddings (differences between word embeddings) and minimizes a joint loss combining distance matching for dependency existence and contrastive loss for dependency type classification. The probe is trained using Adam optimizer with learning rate 0.005, batch size 200, and 30 epochs, with a hyperparameter λ=10.0 balancing the structural and angular objectives. Performance is evaluated on the English Web Treebank dataset using UUAS for structure, AUC/Balanced Accuracy for dependency types, and LAS for combined performance across all model layers.

## Key Results
- The Polar Probe achieves 95% AUC score for dependency type classification, nearly two-fold improvement over the Structural Probe's 74% AUC.
- It achieves 70.2 Labeled Attachment Score (LAS) on the English Web Treebank dataset.
- The probe reveals that a polar coordinate system exists in a low-dimensional subspace (optimal k''=128) of intermediate layers in many LLMs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Syntactic relations are encoded by the relative direction between nearby embeddings in LLMs.
- Mechanism: The Polar Probe learns a linear transformation that aligns edge embeddings of the same syntactic type along collinear directions while making embeddings of different types orthogonal.
- Core assumption: The intermediate layers of LLMs naturally organize syntactic relations into a polar coordinate system where distance = presence and direction = type.
- Evidence anchors:
  - [abstract] "we hypothesize that syntactic relations are, in fact, coded by the relative direction between nearby embeddings"
  - [section 2.3] "the Angular Probe is the linear transform which minimizes the contrastive loss: LA = 1/ΩA X s,s′∈ΩA (∡(BAs, BAs′) − 1[t(s) = t(s′)])2"
  - [corpus] Weak - only 1 of 8 neighbor papers mention syntax/probes directly; most focus on general probing or other modalities.
- Break condition: If the LLM activations do not naturally form distinguishable directional clusters for different syntactic types, the angular contrastive loss cannot be minimized and the probe fails to decode types.

### Mechanism 2
- Claim: A joint optimization of structural and angular objectives preserves both dependency existence and type information in a low-dimensional subspace.
- Mechanism: The Polar Probe optimizes both LS (distance matching) and LA (angular alignment) simultaneously, ensuring the transformed space retains the structural hierarchy (via distances) while encoding type information (via angles).
- Core assumption: The same subspace can encode both hierarchical distances and categorical angular distinctions without interference.
- Evidence anchors:
  - [abstract] "Our Polar Probe successfully recovers the type and direction of syntactic relations, and substantially outperforms the Structural Probe"
  - [section 2.4] "the Polar Probe minimizes the following loss function, with λ a hyper-parameter weighing the Angular objective: argminBP LS + λLA"
  - [corpus] Weak - neighbor papers focus on general probing methods but not specifically on dual-objective probes.
- Break condition: If λ is poorly tuned, the optimization may prioritize one objective over the other, degrading either structural or type recovery.

### Mechanism 3
- Claim: The optimal dimensionality for the Polar Probe is much smaller than the original embedding dimension, indicating a compact syntactic code.
- Mechanism: By varying k'' (dimensionality of probe space), the authors find a peak performance around k''=128, suggesting that syntactic trees can be represented efficiently in a low-dimensional polar coordinate system.
- Core assumption: Syntax can be compressed into a subspace without losing discriminative power for both existence and type.
- Evidence anchors:
  - [section 3] "We observe a peak around k′′ = 128... these results suggest that the space representing the complete syntactic tree needs not be unreasonably large."
  - [section 2.6] "We trained the Polar Probes with gradient descent... Hyperparameter λ is set to 10.0"
  - [corpus] Weak - no direct evidence from neighbors about dimensionality analysis.
- Break condition: If the dimensionality is too low, angular distinctions between types become ambiguous; if too high, the compactness advantage is lost and overfitting may occur.

## Foundational Learning

- Concept: Linear algebra - dot products, cosine similarity, orthogonal projections
  - Why needed here: The probe relies on measuring angles (cosine similarity) and distances (Euclidean norm) in vector spaces to decode syntax.
  - Quick check question: Given two vectors a and b, what is the cosine similarity formula and what does it measure?

- Concept: Contrastive learning - pulling same-class examples together, pushing different-class examples apart
  - Why needed here: The Angular Probe uses a contrastive loss to align same-type dependencies and orthogonalize different types.
  - Quick check question: In the Angular Probe loss, what happens to the cosine similarity between embeddings of the same type versus different types?

- Concept: Dependency grammar and tree structures
  - Why needed here: Understanding how syntactic trees are represented as labeled, directed graphs is essential to formulate the probe objectives.
  - Quick check question: In dependency grammar, what do the edges of the tree represent and how are they labeled?

## Architecture Onboarding

- Component map:
  - Input: LLM activations (contextualized word embeddings)
  - Probe: Linear transformation BP: R^k → R^{k''}
  - Edge embedding: s_ij = h_i - h_j
  - Type prototypes: V_c = average(BP(s) for s of type c)
  - Output: Predicted distance ||BP(s)||^2, predicted type argmax_c |cos(BP(s), V_c)|, predicted head based on sign of cos(BP(s), V_{predicted_type})

- Critical path:
  1. Extract word embeddings from LLM
  2. Compute edge embeddings (differences)
  3. Apply Polar Probe linear transform
  4. For type prediction: compare to prototypes via cosine similarity
  5. For direction prediction: check sign of alignment with predicted type vector
  6. For distance prediction: compute squared norm

- Design tradeoffs:
  - Dimensionality k'': Higher gives more capacity but risks overfitting and loses interpretability; lower is more compact but may lose discriminative power
  - λ weight: Balances structural vs angular objectives; too high on angular may distort distances, too high on structural may fail to separate types
  - Training data size: More sentences improve probe generalization but increase compute

- Failure signatures:
  - Low UUAS but high type accuracy: Probe captures types but loses structural hierarchy
  - High UUAS but low type accuracy: Probe preserves distances but fails to separate types
  - Performance near random: Angular objective not being minimized (types not separable)
  - Degradation on long/nested sentences: Probe not robust to sentence complexity

- First 3 experiments:
  1. Train Polar Probe on a small dataset (e.g., 1000 sentences) and evaluate LAS; compare to baseline Structural Probe
  2. Vary k'' (dimensionality) from 16 to 1024 and plot LAS/accuracy to find optimal dimensionality
  3. Test probe on controlled dataset with nested structures to verify consistency across sentence complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Polar Probe framework generalize to languages with grammatical structures significantly different from English?
- Basis in paper: [explicit] The authors note that human languages use different grammatical rules and may be structured according to different types of trees, and suggest this direction as an exciting possibility for future research.
- Why unresolved: The current study only investigates English, leaving the generalizability to other language families unexplored.
- What evidence would resolve it: Testing the Polar Probe on a diverse set of languages with varying grammatical structures (e.g., agglutinative languages, languages with free word order) and comparing performance across languages would demonstrate its generalizability.

### Open Question 2
- Question: Can the Polar Probe framework be extended to capture morphological structures beyond word-level syntax?
- Basis in paper: [explicit] The authors mention that syntax is not necessarily restricted to relations between words, and that morphology predicts words themselves may be represented as trees of morphemes.
- Why unresolved: The current work focuses on dependency structures between words, not on morphological composition.
- What evidence would resolve it: Adapting the Polar Probe to analyze morphological trees and comparing its ability to recover morpheme-level syntactic relations to its performance on word-level dependencies would address this question.

### Open Question 3
- Question: How would alternative geometric assumptions, such as hyperbolic representations, affect the performance of the Polar Probe?
- Basis in paper: [explicit] The authors discuss the potential of hyperbolic representations as a valuable avenue for extending the current work, citing their successful application in other NLP tasks.
- Why unresolved: The current implementation uses Euclidean probes, leaving the impact of alternative geometries unexplored.
- What evidence would resolve it: Implementing the Polar Probe using hyperbolic geometry and comparing its performance to the Euclidean version on the same datasets would provide evidence for or against the benefits of hyperbolic representations.

## Limitations
- Weak external validation from corpus analysis - only 1 of 8 neighboring papers directly address syntax probing.
- No conclusive evidence for why polar coordinate representation emerges during LLM training.
- Dimensionality analysis suggests 128 dimensions are optimal but lacks theoretical justification.

## Confidence
- High confidence: The Polar Probe significantly outperforms the Structural Probe on multiple metrics (UUAS, AUC, LAS) on the English Web Treebank dataset.
- Medium confidence: The claim that syntactic relations are encoded by relative direction rather than just distance is supported by the probe's superior performance, but alternative explanations cannot be ruled out.
- Low confidence: The assertion that a polar coordinate system "spontaneously" emerges in LLMs lacks strong theoretical grounding and direct empirical evidence beyond the probe's success.

## Next Checks
1. Test the Polar Probe on syntactically diverse languages (e.g., morphologically rich languages like Russian or Finnish) to assess cross-linguistic generalizability of the polar coordinate representation.
2. Conduct ablation studies by training separate probes for structural vs angular objectives to quantify the individual contributions of distance and direction to overall performance.
3. Perform interpretability analysis on the learned probe weights to visualize whether same-type dependencies truly cluster along collinear directions and different types are orthogonal in the transformed space.