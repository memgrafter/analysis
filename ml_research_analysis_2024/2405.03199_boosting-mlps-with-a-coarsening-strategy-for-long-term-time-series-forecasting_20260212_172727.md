---
ver: rpa2
title: Boosting MLPs with a Coarsening Strategy for Long-Term Time Series Forecasting
arxiv_id: '2405.03199'
source_url: https://arxiv.org/abs/2405.03199
tags:
- time
- series
- forecasting
- coarsening
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of long-term time series forecasting
  by proposing the Coarsened Perceptron Network (CP-Net). CP-Net features a two-stage
  coarsening strategy that enhances the efficiency of multi-layer perceptrons (MLPs)
  by forming information granules instead of solitary temporal points.
---

# Boosting MLPs with a Coarsening Strategy for Long-Term Time Series Forecasting

## Quick Facts
- arXiv ID: 2405.03199
- Source URL: https://arxiv.org/abs/2405.03199
- Reference count: 18
- Primary result: 4.1% improvement in MSE and 3.3% improvement in MAE over state-of-the-art on seven forecasting benchmarks

## Executive Summary
This paper addresses the challenge of long-term time series forecasting by proposing the Coarsened Perceptron Network (CP-Net), which enhances multi-layer perceptrons through a two-stage coarsening strategy. The method forms information granules instead of solitary temporal points, enabling MLPs to capture long-range dependencies more effectively. CP-Net demonstrates significant performance improvements while maintaining linear computational complexity and low runtime, even for very long input sequences where competing methods fail.

## Method Summary
CP-Net employs a two-stage coarsening framework to enhance MLP efficiency for long-term time series forecasting. The Token Projection Block forms semantic information granules using 1D convolution with kernel size equal to token length, while the Contextual Sampling Block creates contextual granules through dilated and equispaced convolutions. A multi-scale merging strategy integrates patterns of diverse granularities using multiple parallel branches with different token lengths and sampling rates, followed by 2D convolution-based fusion. The architecture is trained on seven benchmark datasets using Instance Normalization and evaluated on MSE and MAE metrics.

## Key Results
- 4.1% improvement in MSE compared to state-of-the-art method
- 3.3% improvement in MAE compared to state-of-the-art method
- Maintains linear computational complexity and low runtime even with look-back windows up to 2880 time steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Token Projection Block alleviates the MLP's deficiency in preserving contextual dependencies by forming semantic information granules from the input sequence before MLP projection.
- Mechanism: Uses standard 1D convolution with kernel size equal to token length to aggregate consecutive time points into overlapping coarse-grained tokens, capturing local semantic patterns that would be lost in point-wise MLP projection.
- Core assumption: Semantic patterns can be effectively captured through local token aggregation without requiring attention mechanisms.
- Evidence anchors: Abstract mentions "semantic and contextual patterns" and "preserves correlations over larger timespans"; Section 3.2 describes projecting coarse-grained tokens into preliminary future predictions.
- Break condition: If token length is too small (insufficient context) or too large (losing fine-grained patterns), semantic aggregation becomes ineffective.

### Mechanism 2
- Claim: The Contextual Sampling Block addresses the MLP's inadequate information bottleneck by forming contextual information granules through down-sampling, filtering out volatile noises while preserving long-range dependencies.
- Mechanism: Applies dilated convolution with dilation rate equal to sampling rate to capture periodic contextual patterns, then uses equispaced convolution (kernel size = stride = sampling rate) for down-sampling to extract longer-range temporal relationships.
- Core assumption: Temporal patterns of different scales can be captured through dilated and equispaced convolutions without complex attention mechanisms.
- Evidence anchors: Abstract mentions "filters out volatile noises"; Section 3.3 describes down-sampling preserving crucial longer-range temporal relationships while filtering redundant information.
- Break condition: If sampling rate is poorly chosen (too small to capture dependencies or too large causing excessive information loss), contextual patterns cannot be effectively extracted.

### Mechanism 3
- Claim: The multi-scale merging strategy enhances forecasting performance by integrating temporal patterns of diverse granularities, providing comprehensive prediction that captures both local and global temporal structures.
- Mechanism: Multiple parallel branches with different token length and sampling rate pairs process the same input, each capturing patterns at different scales, with outputs merged using 2D convolution with proper weighting.
- Core assumption: Real-world time series contain patterns at multiple temporal scales that can be effectively combined through weighted merging.
- Evidence anchors: Abstract mentions "patterns of diverse granularities are fused towards a comprehensive prediction"; Section 3.4 describes employing multi-scale strategy to integrate diverse granularities; Section 4.2 shows improvement with increasing number of branches.
- Break condition: If number of scales is insufficient to capture true temporal structure or merging weights cannot effectively balance contributions, comprehensive prediction becomes suboptimal.

## Foundational Learning

- Concept: Token aggregation and convolution-based feature extraction
  - Why needed here: Understanding how 1D convolutions with specific kernel sizes can transform raw time series into semantically meaningful tokens is fundamental to grasping the Token Projection Block's functionality
  - Quick check question: If a time series has length 96 and the Token Projection Block uses a kernel size of 12, how many tokens will be generated before applying the MLP?

- Concept: Dilated and equispaced convolutions for context capture and down-sampling
  - Why needed here: The Contextual Sampling Block relies on these specialized convolutions to extract long-range dependencies and filter information, which differs from standard CNN approaches
  - Quick check question: How does increasing the dilation rate in dilated convolution affect the receptive field and computational complexity?

- Concept: Multi-scale feature fusion and weighted combination
  - Why needed here: The merging strategy requires understanding how to effectively combine predictions from different scales without introducing instability or bias
  - Quick check question: What are potential issues when merging predictions from scales with vastly different signal-to-noise ratios?

## Architecture Onboarding

- Component map: Input normalization (Instance Normalization) -> Multiple parallel branches -> Token Projection Block (Conv1D + MLP) -> Contextual Sampling Block (DilatedConv1D + EquiConv1D) -> Predictor (2-layer MLP) -> 2D convolution merging layer -> Output truncation to desired prediction length

- Critical path: Raw input → Instance Normalization → Parallel Token Projection Blocks → Parallel Contextual Sampling Blocks → Separate Predictors → 2D Convolution Merging → Output Truncation

- Design tradeoffs:
  - Token length (TL) vs. computational cost: Larger TL captures more context but increases computation and may lose fine-grained patterns
  - Sampling rate (SR) vs. information retention: Higher SR captures longer-range dependencies but may lose local patterns through aggressive down-sampling
  - Number of scales vs. model complexity: More scales capture diverse patterns but increase computational cost and may lead to overfitting

- Failure signatures:
  - Performance degradation when TL is too small: Model behaves like standard MLP, losing contextual dependencies
  - Over-smoothing when SR is too large: Predictions become too smooth and miss important local variations
  - Training instability with too many scales: Model becomes difficult to train and may overfit to training data

- First 3 experiments:
  1. Ablation study with only Token Projection Block (remove Contextual Sampling Block) to verify semantic aggregation contribution
  2. Ablation study with only Contextual Sampling Block (remove Token Projection Block) to verify contextual sampling contribution
  3. Single-scale variant (TL=SR=1) comparison to verify if multi-scale strategy provides benefits over simple MLP baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CP-Net's performance scale with increasingly long input sequences beyond 2880 time steps, and what are the computational limitations in such scenarios?
- Basis in paper: The paper mentions that PatchTST runs out of memory when the look-back window I ≥ 2880, while CP-Net maintains linear computational complexity and low runtime.
- Why unresolved: The paper does not explore CP-Net's performance and scalability beyond 2880 time steps, leaving uncertainty about its behavior with extremely long sequences.
- What evidence would resolve it: Conducting experiments with input sequences longer than 2880 time steps to evaluate CP-Net's performance, memory usage, and computational efficiency would provide insights into its scalability limits.

### Open Question 2
- Question: What is the impact of varying the number of token lengths and sampling rates on the model's performance, and is there an optimal configuration?
- Basis in paper: The paper conducts an ablation study on the impact of the number of branches (token lengths and sampling rates) on the Electricity dataset, finding improvements up to 4 branches but less pronounced beyond that.
- Why unresolved: The study only considers a specific range of branches and does not explore a wider range of configurations or different datasets, leaving uncertainty about the optimal setup.
- What evidence would resolve it: Performing a comprehensive grid search over a broader range of token lengths and sampling rates across multiple datasets would help identify the optimal configuration for different scenarios.

### Open Question 3
- Question: How does the CP-Net handle non-stationary time series data, and what are its limitations in capturing trends and seasonality?
- Basis in paper: The paper focuses on long-term time series forecasting and mentions capturing long-term global dependencies, but does not explicitly address non-stationary data or the model's ability to handle trends and seasonality.
- Why unresolved: The paper does not provide specific insights into how the CP-Net deals with non-stationary data, which is a common characteristic of real-world time series.
- What evidence would resolve it: Testing the CP-Net on non-stationary time series datasets and analyzing its performance in capturing trends and seasonality would provide insights into its limitations and potential improvements.

## Limitations

- Evaluation relies entirely on relative performance gains reported against a single baseline without comprehensive ablation studies showing individual component contributions.
- Computational complexity analysis focuses only on FLOPs without addressing memory requirements or practical deployment considerations for different hardware configurations.
- The "low runtime" claim lacks quantitative benchmarks or comparisons to alternative methods' inference speeds.

## Confidence

- **High confidence**: The CP-Net architecture and its two-stage coarsening framework are well-defined and technically sound, with clear implementation details provided for the core components.
- **Medium confidence**: The claimed performance improvements (4.1% MSE, 3.3% MAE) are supported by benchmark results, though the absence of comprehensive ablation studies limits confidence in attributing gains to specific architectural innovations.
- **Low confidence**: The assertion that CP-Net maintains "linear computational complexity" while achieving superior performance is difficult to verify without detailed runtime measurements across varying sequence lengths and hardware configurations.

## Next Checks

1. **Ablation study implementation**: Implement and evaluate CP-Net variants with only Token Projection Block, only Contextual Sampling Block, and the baseline MLP to quantify individual component contributions to the reported performance gains.

2. **Hyperparameter sensitivity analysis**: Conduct systematic experiments across all seven datasets to evaluate CP-Net performance sensitivity to token length and sampling rate choices, identifying whether the current configurations are optimal or dataset-dependent.

3. **Computational overhead measurement**: Measure actual training and inference times across different sequence lengths and hardware setups, comparing CP-Net's practical runtime against both the baseline method and other competitive approaches to validate the "low runtime" claim.