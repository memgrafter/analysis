---
ver: rpa2
title: Why Fine-grained Labels in Pretraining Benefit Generalization?
arxiv_id: '2410.23129'
source_url: https://arxiv.org/abs/2410.23129
tags:
- log5
- learning
- training
- probability
- poly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical explanation for why pretraining
  with fine-grained labels improves generalization compared to coarse-grained pretraining.
  The authors introduce a "hierarchical multi-view" data model where input samples
  consist of patches containing either common features (shared across subclasses)
  or subclass-specific fine-grained features.
---

# Why Fine-grained Labels in Pretraining Benefit Generalization?

## Quick Facts
- arXiv ID: 2410.23129
- Source URL: https://arxiv.org/abs/2410.23129
- Authors: Guan Zhe Hong; Yin Cui; Ariel Fuxman; Stanley Chan; Enming Luo
- Reference count: 40
- This paper provides a theoretical explanation for why pretraining with fine-grained labels improves generalization compared to coarse-grained pretraining.

## Executive Summary
This paper addresses the question of why fine-grained pretraining improves generalization compared to coarse-grained pretraining. The authors introduce a "hierarchical multi-view" data model where input samples consist of patches containing either common features (shared across subclasses) or subclass-specific fine-grained features. Under this framework, they prove that coarse-grained pretraining only allows the network to learn common features well, leading to poor performance on hard test samples that require subclass-specific features. In contrast, fine-grained pretraining enables the network to learn both common and subclass-specific features, resulting in good performance on both easy and hard test samples. The theoretical results are supported by experiments on ImageNet21kâ†’ImageNet1k transfer and iNaturalist 2021 datasets.

## Method Summary
The authors propose a theoretical framework where image data consists of patches with either common features shared across subclasses or subclass-specific fine-grained features. They prove that pretraining with coarse labels only learns common features effectively, while fine-grained pretraining enables learning both common and subclass-specific features. The theory is validated through transfer learning experiments where models are pretrained on ImageNet21k with varying label granularities (different levels of WordNet hierarchy) and then fine-tuned on ImageNet1k. A Vision Transformer (ViT-B/16) is used, pretrained for 90 epochs on each hierarchy level, followed by 8 epochs of fine-tuning on the target dataset. Additional experiments on iNaturalist 2021 with manual biological hierarchy support the findings.

## Key Results
- Fine-grained pretraining on ImageNet21k consistently outperforms coarse-grained pretraining when transferring to ImageNet1k
- The advantage of fine-grained pretraining is more pronounced for harder test samples that require subclass-specific features
- Experiments on iNaturalist 2021 with manually constructed biological hierarchy confirm the theoretical predictions
- Overly fine-grained pretraining (near leaf level) can degrade performance due to sample starvation

## Why This Works (Mechanism)
The mechanism relies on the hierarchical multi-view data model where images contain patches with either common features shared across subclasses or subclass-specific fine-grained features. During coarse-grained pretraining, the network optimizes for distinguishing only between broad categories, effectively ignoring subclass-specific features since they don't contribute to the coarse classification task. This leads to learning only common features well. Fine-grained pretraining, however, forces the network to utilize subclass-specific features for accurate classification, enabling it to learn both common and fine-grained features. When transferred to a downstream task, the fine-grained pretrained model can better handle samples requiring subclass-specific features (hard samples), while the coarse-grained pretrained model struggles on these samples despite performing well on easy samples that only require common features.

## Foundational Learning
- **Hierarchical classification**: Understanding how labels at different levels of a hierarchy relate to each other and how they affect feature learning. Why needed: The paper's core argument depends on understanding the relationship between coarse and fine labels in a hierarchy. Quick check: Can you explain how WordNet hierarchy maps to image classification at different granularity levels?
- **Multi-view data models**: Grasping the concept of data consisting of multiple views or components with different characteristics. Why needed: The theoretical framework assumes data can be decomposed into common and subclass-specific components. Quick check: Can you describe how the "hierarchical multi-view" model differs from standard single-view classification?
- **Transfer learning theory**: Understanding how pretraining on one task affects performance on a related downstream task. Why needed: The experiments test whether fine-grained pretraining provides advantages in transfer scenarios. Quick check: Can you explain why transfer learning might benefit more from fine-grained pretraining than from coarse-grained pretraining?
- **Vision Transformer architecture**: Familiarity with ViT components, attention mechanisms, and how they process image patches. Why needed: The experiments use ViT-B/16 as the base architecture. Quick check: Can you describe how ViT processes image patches and why this matters for feature learning?

## Architecture Onboarding
- **Component map**: ViT-B/16 (base architecture) -> Pretraining on ImageNet21k with hierarchical labels -> Fine-tuning on ImageNet1k -> Validation accuracy measurement
- **Critical path**: The theoretical argument depends on the pretraining phase correctly learning either common-only features (coarse) or both common and fine-grained features (fine-grained), which then manifests as performance differences during fine-tuning
- **Design tradeoffs**: Using Vision Transformer allows for strong transfer performance but increases computational cost compared to CNNs; hierarchical pretraining adds complexity but provides theoretical insights into feature learning
- **Failure signatures**: If fine-grained pretraining doesn't outperform coarse-grained pretraining, potential causes include insufficient sample size per subclass, misalignment between pretraining and target label spaces, or the target task not requiring subclass-specific features
- **First experiments**: 1) Verify that pretraining with root-level labels performs worse than with leaf-level labels on ImageNet1k; 2) Test the effect of pretraining granularity on easy vs hard test samples separately; 3) Replicate the iNaturalist 2021 experiments with manual hierarchy to confirm findings

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the model's performance change when using different hierarchical levels of fine-grained labels during pretraining, and is there an optimal granularity that maximizes downstream accuracy?
- Basis in paper: [explicit] The authors mention that using overly fine-grained labels (e.g., assigning a unique label to every sample) can be detrimental to generalization, as the model might focus on frivolous details rather than discriminative features.
- Why unresolved: The paper does not provide a systematic study on how the granularity of fine-grained labels affects the model's performance, and whether there is an optimal level that balances learning discriminative features without overfitting to noise.
- What evidence would resolve it: Conducting experiments with various levels of label granularity during pretraining and measuring the resulting downstream accuracy would provide insights into the optimal granularity.

### Open Question 2
- Question: How does the model's performance change when using noisy fine-grained labels during pretraining, and is there a threshold for label noise that degrades the benefits of fine-grained pretraining?
- Basis in paper: [explicit] The authors mention that using noisy fine-grained labels during pretraining can be detrimental to generalization, as the model might learn to rely on spurious correlations rather than true discriminative features.
- Why unresolved: The paper does not provide a systematic study on how the noise level in fine-grained labels affects the model's performance, and whether there is a threshold for label noise that negates the benefits of fine-grained pretraining.
- What evidence would resolve it: Conducting experiments with varying levels of label noise in fine-grained pretraining and measuring the resulting downstream accuracy would provide insights into the threshold for label noise.

### Open Question 3
- Question: How does the model's performance change when using misaligned fine-grained labels during pretraining, where the fine-grained features considered discriminative by the pretraining label function do not align well with those valued by the target label function?
- Basis in paper: [explicit] The authors mention that for fine-grained pretraining to be effective, the features that the pretraining label function considers discriminative must align well with those valued by the label function of the target problem.
- Why unresolved: The paper does not provide a systematic study on how the alignment between fine-grained and target label spaces affects the model's performance, and whether there is a threshold for misalignment that degrades the benefits of fine-grained pretraining.
- What evidence would resolve it: Conducting experiments with varying degrees of alignment between fine-grained and target label spaces and measuring the resulting downstream accuracy would provide insights into the threshold for misalignment.

## Limitations
- The hierarchical multi-view model assumes independent patch contributions and binary feature presence/absence, which may not capture the complexity of real image data where features interact and have varying importance
- The theory focuses on binary classification for tractability, limiting direct applicability to the multi-class experiments conducted
- The experimental results show fine-grained pretraining consistently outperforms coarse-grained pretraining on transfer tasks, but the margin varies significantly across datasets and is modest in some cases (e.g., 1-2% improvements on ImageNet1k)

## Confidence
- **Theoretical framework validity**: **Medium** - The proofs are mathematically sound within their assumptions, but these assumptions may not reflect realistic data distributions
- **Experimental demonstration**: **High** - The transfer learning results are reproducible and show consistent trends, though effect sizes vary
- **Causal explanation**: **Low** - While the theory provides a plausible mechanism, the experiments don't definitively prove this mechanism operates as described

## Next Checks
1. Test the hierarchical multi-view model on datasets with known ground-truth hierarchical structure (like CIFAR-100) to verify the theoretical predictions match empirical behavior
2. Conduct controlled experiments ablating subclass-specific features during fine-tuning to directly measure their contribution to downstream performance
3. Evaluate whether the fine-to-coarse transfer advantage persists when training from scratch on target tasks with limited data, to distinguish pretraining effects from architectural advantages