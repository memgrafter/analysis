---
ver: rpa2
title: Core Knowledge Deficits in Multi-Modal Language Models
arxiv_id: '2410.10855'
source_url: https://arxiv.org/abs/2410.10855
tags:
- core
- knowledge
- arxiv
- abilities
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Multi-modal Large Language Models (MLLMs) excel at high-level\
  \ reasoning but struggle with tasks intuitive to humans, such as counting, spatial\
  \ reasoning, and perspective-taking\u2014a phenomenon known as Moravec\u2019s Paradox.\
  \ To investigate this gap, researchers introduced CoreCognition, a benchmark grounded\
  \ in developmental cognitive science, comprising 1,503 questions across 12 core\
  \ cognitive abilities from sensorimotor to formal operational stages."
---

# Core Knowledge Deficits in Multi-Modal Language Models

## Quick Facts
- **arXiv ID**: 2410.10855
- **Source URL**: https://arxiv.org/abs/2410.10855
- **Reference count**: 40
- **Primary result**: MLLMs perform significantly worse on low-level core cognitive abilities than high-level ones, with limited scaling improvements.

## Executive Summary
Multi-modal Large Language Models (MLLMs) exhibit a paradox: they excel at abstract reasoning yet struggle with basic cognitive tasks like counting and spatial reasoning—core abilities humans master early in development. Researchers address this gap by introducing CoreCognition, a benchmark grounded in developmental cognitive science, comprising 1,503 questions across 12 core abilities. Evaluating 230 MLLMs with 11 prompting strategies, they find that models perform significantly worse on low-level abilities than high-level ones, with weak cross-stage correlations indicating misaligned dependencies. Scaling laws do not uniformly apply: performance on low-level abilities shows limited or negative improvement with model size, while reasoning enhancements provide minimal gains. A novel controlled method, Concept Hacking, reveals that larger models rely more on shortcut learning rather than genuine conceptual understanding. These findings suggest that current MLLMs lack fundamental cognitive grounding, limiting robustness and generalization, and highlight the need for training strategies that explicitly instill core knowledge before large-scale pretraining.

## Method Summary
The study evaluates core knowledge deficits in 230 MLLMs using the CoreCognition benchmark, which includes 1,503 multiple-choice questions across 12 core cognitive abilities spanning sensorimotor to formal operational stages. Models are tested with 11 prompting strategies, and a hybrid matching pipeline (template matching with LLM-as-a-judge fallback) extracts answers. Circular evaluation mitigates option-position bias. The Concept Hacking method introduces controlled manipulations to test for shortcut learning. Performance is normalized by chance level, and statistical analyses (paired t-tests, regression) assess scaling effects and cross-stage correlations. Models with >20% fail rate are filtered out.

## Key Results
- MLLMs perform significantly worse on low-level core abilities (sensorimotor, preoperational) than high-level ones (concrete, formal operational).
- Scaling laws do not uniformly apply: low-level abilities show limited or negative improvement with model size.
- Concept Hacking reveals larger models rely more on shortcut learning rather than genuine conceptual understanding.
- Reasoning-augmented models (e.g., chain-of-thought) show negligible gains on low-level tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Core knowledge deficits arise because MLLMs lack foundational cognitive abilities humans acquire early, such as object permanence, spatial reasoning, and perspective-taking.
- Mechanism: Without these core abilities, MLLMs cannot build robust representations for higher-level reasoning and instead rely on pattern matching or spurious correlations that fail under distribution shift.
- Core assumption: Core cognitive abilities form the scaffolding for more complex reasoning; their absence leads to brittle performance.
- Evidence anchors:
  - [abstract] "models perform significantly worse on low-level core abilities than high-level ones, with weak cross-stage correlations indicating misaligned dependencies"
  - [section] "MLLMs consistently perform worse on low-level abilities compared to high-level abilities (Sec. 4.1)"
- Break condition: If scaling or pretraining on sufficiently diverse data could implicitly induce these core abilities, the deficit explanation would weaken.

### Mechanism 2
- Claim: Current MLLMs do not scale uniformly across cognitive abilities; low-level abilities show little or negative scaling with model size.
- Mechanism: Scaling laws that benefit abstract reasoning do not apply to foundational perceptual and spatial skills, suggesting these require different learning dynamics or architectural priors.
- Core assumption: Scaling laws for language and reasoning do not transfer to core perceptual and spatial cognition.
- Evidence anchors:
  - [abstract] "performance on low-level abilities showed limited or negative improvement with model size"
  - [section] "MLLMs exhibit less, or even no, scalability on low-level abilities compared to high-level abilities"
- Break condition: If future architectures or training methods demonstrate that scaling does improve low-level abilities, the non-scaling claim would break.

### Mechanism 3
- Claim: MLLMs rely on shortcut learning rather than genuine conceptual understanding, as shown by Concept Hacking experiments where models fail when familiar patterns are paired with inverted labels.
- Mechanism: Models exploit spurious correlations learned during training, so when ground truth is manipulated but context is preserved, their performance collapses—indicating lack of core knowledge.
- Core assumption: Performance on standard tasks can be achieved via memorization or shortcut learning without true understanding.
- Evidence anchors:
  - [abstract] "Concept Hacking... reveals MLLMs fail to progress toward genuine core knowledge understanding, but instead rely on shortcut learning as they scale"
  - [section] "Models increasing in size exhibit core deficits and shortcut-taking behaviors rather than progressing toward conceptual understanding"
- Break condition: If models could maintain performance on manipulated tasks without relying on shortcuts, the shortcut-learning claim would break.

## Foundational Learning

- Concept: Core cognitive abilities (object permanence, spatial reasoning, perspective-taking)
  - Why needed here: These form the developmental scaffolding that supports higher-level reasoning; without them, MLLMs cannot generalize robustly.
  - Quick check question: Can a model correctly infer that an occluded object still exists, or does it assume disappearance?

- Concept: Scaling laws and their limitations
  - Why needed here: Understanding why increasing model size does not uniformly improve all cognitive abilities is central to diagnosing core knowledge deficits.
  - Quick check question: Does doubling model size improve performance equally on spatial reasoning and abstract reasoning tasks?

- Concept: Shortcut learning vs. genuine understanding
  - Why needed here: Distinguishing whether performance stems from pattern matching or true conceptual grasp is key to interpreting MLLM capabilities.
  - Quick check question: If a model’s training data is subtly altered (e.g., ground truth inverted), does its performance degrade?

## Architecture Onboarding

- Component map: Vision encoder (e.g., CLIP, SigLip) → LLM backbone (e.g., transformer) → Alignment module → Prompt processor → Output mapper. Core knowledge evaluation sits downstream of perception and before reasoning.
- Critical path: Input image/video → Feature extraction → Multi-modal fusion → Prompt conditioning → Reasoning/QA → Answer selection/matching.
- Design tradeoffs: Dense vs. MoE backbones (parameter efficiency vs. complexity); frozen vs. trainable vision encoders (speed vs. adaptation); single-image vs. video support (scope vs. resource demand).
- Failure signatures: High FAIL rates in answer matching (indicates output formatting issues); poor scaling on low-level tasks (indicates missing perceptual grounding); success on manipulated Concept Hacking tasks (indicates shortcut reliance).
- First 3 experiments:
  1. Run CoreCognition benchmark on a small dense model (e.g., 7B) to establish baseline low-level vs. high-level performance gap.
  2. Apply Concept Hacking to the same model to test shortcut reliance.
  3. Incrementally scale model size (e.g., 13B, 33B) and compare scaling curves for low-level vs. high-level abilities to confirm non-uniform scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do scaling laws apply to low-level core abilities if pretraining explicitly includes structured grounding in core knowledge?
- Basis in paper: [inferred] The paper finds that scaling models does not improve low-level abilities, suggesting that pretraining strategies should explicitly target foundational capacities before large-scale pretraining.
- Why unresolved: The study shows that increasing model size does not inherently improve core knowledge, but does not test whether explicitly grounding models in core knowledge during pretraining changes this scaling dynamic.
- What evidence would resolve it: Experiments comparing models pretrained with explicit core knowledge scaffolding to standard models across a range of sizes, measuring performance on low-level abilities.

### Open Question 2
- Question: Can models be trained to develop robust core knowledge representations without relying on shortcut learning?
- Basis in paper: [explicit] The paper introduces Concept Hacking to reveal that larger models rely more on shortcut learning rather than genuine conceptual understanding.
- Why unresolved: While Concept Hacking identifies shortcut reliance, it does not explore training methods that might encourage models to develop robust, generalizable core knowledge instead.
- What evidence would resolve it: Training studies that apply techniques like adversarial training, meta-learning, or curriculum design specifically aimed at discouraging shortcuts and fostering conceptual understanding, then re-evaluating with Concept Hacking.

### Open Question 3
- Question: How do core knowledge deficits in MLLMs affect their ability to perform tasks requiring mental simulation or counterfactual reasoning?
- Basis in paper: [inferred] The paper finds poor performance on perspective-taking and mechanical reasoning, tasks that require mental simulation and model-based reasoning, and notes that reasoning-augmented models show negligible gains on such tasks.
- Why unresolved: The paper identifies poor performance on perspective-taking and mechanical reasoning but does not directly test whether this stems from core knowledge deficits or other architectural limitations.
- What evidence would resolve it: Targeted experiments comparing models with and without core knowledge grounding on tasks requiring mental simulation, counterfactual reasoning, and model-based planning, using established benchmarks and Concept Hacking variations.

## Limitations
- Reliance on a single benchmark (CoreCognition) for evaluating core knowledge deficits, with unclear cross-cultural validity.
- Limited transparency regarding model-specific configurations and exact implementation details of the hybrid matching strategy, affecting reproducibility.
- Concept Hacking methodology tests only a limited set of manipulations (45 samples) and may not capture the full spectrum of shortcut learning behaviors.

## Confidence
- **High Confidence**: The finding that MLLMs perform significantly worse on low-level core abilities than high-level ones, supported by strong statistical evidence and consistent across multiple prompting strategies.
- **Medium Confidence**: The claim about non-uniform scaling laws across cognitive abilities, as this requires extensive model coverage and may be influenced by specific architectural choices.
- **Medium Confidence**: The shortcut learning mechanism, as Concept Hacking provides suggestive evidence but may not definitively prove the absence of conceptual understanding.

## Next Checks
1. **Cross-Cultural Validation**: Replicate the CoreCognition benchmark evaluation with cognitive tasks grounded in non-Western developmental frameworks to test the universality of identified core knowledge deficits.
2. **Architectural Ablation Study**: Systematically vary vision encoder architectures (CLIP vs. SigLip vs. custom) and fusion mechanisms across a controlled set of MLLMs to isolate which components most influence performance on low-level vs. high-level abilities.
3. **Dynamic Core Knowledge Testing**: Develop and evaluate models on continuous spatial reasoning tasks and dynamic object interaction scenarios that go beyond static multiple-choice questions to better assess genuine core knowledge understanding.