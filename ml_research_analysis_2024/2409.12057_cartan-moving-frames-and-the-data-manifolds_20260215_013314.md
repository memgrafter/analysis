---
ver: rpa2
title: Cartan moving frames and the data manifolds
arxiv_id: '2409.12057'
source_url: https://arxiv.org/abs/2409.12057
tags:
- data
- matrix
- network
- curvature
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses Cartan moving frames to study the geometry of data
  manifolds in neural networks, linking the network's output changes to the curvature
  of these manifolds. The authors define a data information matrix (DIM) analogous
  to the Fisher information matrix and show that it generates a foliation structure
  in the data space.
---

# Cartan moving frames and the data manifolds

## Quick Facts
- arXiv ID: 2409.12057
- Source URL: https://arxiv.org/abs/2409.12057
- Reference count: 33
- Primary result: Uses Cartan moving frames to study data manifold geometry in neural networks, linking output changes to curvature and revealing which classes are easily reachable from input points.

## Executive Summary
This paper applies Cartan moving frames to analyze the geometry of data manifolds in neural networks. The authors define a data information matrix (DIM) analogous to the Fisher information matrix and use it to compute connection forms and curvature on the data manifold. Experiments on MNIST and CIFAR10 demonstrate that the DIM reveals which classes are easily reachable from a given input point, providing geometric explanations for neural network behavior.

## Method Summary
The method involves training neural networks with softmax outputs and ReLU activations, then computing the data information matrix (DIM) from probability gradients with respect to inputs. The DIM serves as a metric tensor on the data manifold, enabling computation of Cartan frames, connection forms, and curvature. The restricted DIM in the probability-based frame shows which input directions correspond to easy changes in output probabilities, providing interpretability for classification decisions.

## Key Results
- DIM computed on MNIST and CIFAR10 datasets reveals class reachability patterns
- Cartan frame vectors (partial derivatives of class probabilities) form basis for data leaves
- Curvature calculations show relationship between input changes and classification decisions
- Method provides geometric explanations for neural network behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The data information matrix (DIM) provides a Riemannian metric on the data manifold that enables geometric interpretation of neural network behavior.
- Mechanism: DIM is defined as expectation of gradients of log probabilities with respect to input coordinates, creating positive semidefinite matrix that serves as metric tensor.
- Core assumption: Data manifold has smooth Riemannian structure and DIM is non-degenerate on tangent space of leaves.
- Evidence anchors: Abstract mentions DIM as analog of Fisher information matrix; section shows DIM is positive semidefinite with specific kernel properties.
- Break condition: When some pk(x) approach zero causing numerical instability.

### Mechanism 2
- Claim: Cartan moving frame defined by partial derivatives of class probabilities reveals which classes are easily reachable from a given input point.
- Mechanism: Frame vectors ek = ∑i∂ipk(x)∂i form basis for tangent space of data leaves; restricted DIM shows directions corresponding to easy output probability changes.
- Core assumption: Distribution D defined by frame vectors is integrable, creating foliation structure on data space.
- Evidence anchors: Abstract mentions explanations through class reachability; section defines distribution D with specific span properties.
- Break condition: When distribution D fails to be integrable or has constant rank violated.

### Mechanism 3
- Claim: Computing curvature forms in Cartan frame provides quantitative measures of how input changes affect classification decisions.
- Mechanism: Connection forms ωi_j and curvature forms Ωi_j computed using DIM and its derivatives in probability-based frame to characterize local geometry.
- Core assumption: Neural network satisfies H1 (softmax activation) and H2 (second derivatives of score functions vanish almost everywhere).
- Evidence anchors: Abstract links partial derivatives to curvature; section references specific theorem for curvature computation.
- Break condition: When H2 is violated by activation functions producing non-vanishing second derivatives.

## Foundational Learning

- Concept: Differential geometry and Riemannian manifolds
  - Why needed here: Method relies on manifold structure to define metrics, connections, and curvature for analyzing neural network behavior
  - Quick check question: Can you explain what a Riemannian metric tensor is and how it differs from a general metric?

- Concept: Information geometry and Fisher information
  - Why needed here: DIM is defined analogously to Fisher information matrix, using probability gradients instead of parameter gradients
  - Quick check question: How does the Fisher information matrix relate to the geometry of statistical models?

- Concept: Cartan moving frames and exterior calculus
  - Why needed here: Method uses Cartan's approach to define frames, compute connection forms, and calculate curvature on data manifold
  - Quick check question: What is the relationship between connection forms and the Levi-Civita connection?

## Architecture Onboarding

- Component map:
  Input preprocessing -> Neural network model -> Automatic differentiation -> DIM computation -> Frame construction -> Curvature computation -> Visualization

- Critical path:
  1. Load and preprocess dataset
  2. Forward pass through trained network to get probabilities
  3. Compute ∂p/∂x using automatic differentiation
  4. Calculate DIM = (J(p)J(p)^T) ⊙ P^(-1) where ⊙ is element-wise multiplication
  5. Build Cartan frame from probability gradients
  6. Compute connection and curvature forms
  7. Visualize results for interpretation

- Design tradeoffs:
  - Computational cost vs interpretability: Computing all partial derivatives for C classes is expensive but provides complete geometric information
  - Numerical stability: Handling cases where probabilities approach zero requires careful implementation
  - Model specificity: Method assumes ReLU activations and softmax output, limiting applicability to other architectures

- Failure signatures:
  - Numerical instability when pk(x) → 0 causing division by zero in DIM
  - Non-invertible DIM when rank is less than C-1
  - Inconsistent results when H2 assumption is violated
  - High computational cost for networks with many output classes

- First 3 experiments:
  1. Implement DIM computation on simple MLP trained on MNIST and verify positive semidefinite property
  2. Visualize restricted DIM matrices for correctly classified vs misclassified examples
  3. Compare interpretability of DIM-based explanations with SHAP or LIME on same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does curvature of data manifold relate to adversarial robustness in neural networks?
- Basis in paper: Paper discusses curvature of data manifolds in relation to neural network behavior and class reachability
- Why unresolved: Paper focuses on explaining decisions through geometric properties but doesn't explore connection between curvature and adversarial attacks
- What evidence would resolve it: Experiments showing higher curvature regions correlate with increased vulnerability to adversarial perturbations

### Open Question 2
- Question: Can data information matrix (DIM) be used to identify and mitigate bias in neural network predictions?
- Basis in paper: Paper mentions DIM reveals which classes are easily reachable from a given input point
- Why unresolved: Paper demonstrates DIM's ability to explain local geometry but doesn't investigate potential for bias detection or mitigation
- What evidence would resolve it: Analysis of DIM matrices across different demographic groups to identify disparities in class reachability

### Open Question 3
- Question: How does choice of activation function in neural networks affect geometry of data manifold?
- Basis in paper: Paper mentions popular neural networks with ReLU non-linearity satisfy assumptions for distribution D to be integrable
- Why unresolved: Paper assumes certain conditions for data manifold geometry but doesn't explore how different activation functions might alter this geometry
- What evidence would resolve it: Comparative experiments using different activation functions to analyze their impact on DIM and resulting data manifold structure

### Open Question 4
- Question: Can Cartan moving frame approach be extended to understand geometry of latent spaces in autoencoders?
- Basis in paper: Paper's focus on data manifolds and geometric properties suggests potential applications to other dimensionality reduction techniques
- Why unresolved: Paper specifically addresses neural network classifiers but doesn't explore applications to generative models or autoencoders
- What evidence would resolve it: Application of DIM and curvature analysis to latent spaces of trained autoencoders to uncover geometric structures

## Limitations
- Restricted to networks with softmax outputs and ReLU activations due to H2 assumption requirements
- Numerical instability when class probabilities approach zero can lead to unreliable curvature calculations
- Experimental validation limited to simple datasets and basic MLP architectures

## Confidence
- Theoretical framework well-established: High
- Experimental validation limited to simple cases: Medium
- Practical applicability claims: Medium

## Next Checks
1. Test method on more complex architectures (ResNet, Vision Transformers) and datasets (ImageNet) to evaluate scalability and robustness.
2. Conduct ablation studies to quantify impact of H2 assumption violation on curvature calculations and interpretability.
3. Compare DIM-based explanations with established interpretability methods (SHAP, LIME) on identical examples to assess practical utility.