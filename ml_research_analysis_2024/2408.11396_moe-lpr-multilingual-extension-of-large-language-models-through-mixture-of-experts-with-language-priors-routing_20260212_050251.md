---
ver: rpa2
title: 'MoE-LPR: Multilingual Extension of Large Language Models through Mixture-of-Experts
  with Language Priors Routing'
arxiv_id: '2408.11396'
source_url: https://arxiv.org/abs/2408.11396
tags:
- languages
- language
- original
- moe-lpr
- expanded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MoE-LPR tackles catastrophic forgetting in multilingual LLM post-training
  by employing a two-stage training strategy: first upcycling to a Mixture-of-Experts
  (MoE) architecture and post-pretraining on expanded languages with frozen original
  parameters, then reviewing with language priors routing (LPR) using less than 1%
  replay data to recover original language performance. Evaluations across multiple
  benchmarks show MoE-LPR improves expanded language performance by 2.7 points and
  maintains original language proficiency within 96.6% of baseline, outperforming
  existing methods including LoRA, LLaMA-Pro, and LoRAMoE.'
---

# MoE-LPR: Multilingual Extension of Large Language Models through Mixture-of-Experts with Language Priors Routing

## Quick Facts
- **arXiv ID**: 2408.11396
- **Source URL**: https://arxiv.org/abs/2408.11396
- **Authors**: Hao Zhou; Zhijun Wang; Shujian Huang; Xin Huang; Xue Han; Junlan Feng; Chao Deng; Weihua Luo; Jiajun Chen
- **Reference count**: 17
- **Primary result**: MoE-LPR improves expanded language performance by 2.7 points while maintaining original language proficiency within 96.6% of baseline

## Executive Summary
MoE-LPR addresses catastrophic forgetting in multilingual LLM post-training by employing a two-stage training strategy: first upcycling to a Mixture-of-Experts (MoE) architecture and post-pretraining on expanded languages with frozen original parameters, then reviewing with language priors routing (LPR) using less than 1% replay data to recover original language performance. Evaluations across multiple benchmarks show MoE-LPR improves expanded language performance by 2.7 points and maintains original language proficiency within 96.6% of baseline, outperforming existing methods including LoRA, LLaMA-Pro, and LoRAMoE. The approach scales efficiently, adding model parameters without increasing inference overhead, and generalizes well to unseen original languages.

## Method Summary
MoE-LPR employs a two-stage training approach to extend multilingual LLMs to new languages while preventing catastrophic forgetting. The first stage upcycles a dense model to MoE architecture by copying FFN parameters and adding a router matrix, then post-prettains on expanded languages while freezing original parameters. The second stage applies Language Priors Routing (LPR) using less than 1% replay data to retrain the router, with a loss function that encourages routing original language tokens to the frozen expert. This enables the model to selectively activate a subset of experts for each input token, maintaining the same inference overhead while increasing total model parameters.

## Key Results
- Improves expanded language performance by 2.7 points over baseline
- Maintains original language proficiency within 96.6% of baseline performance
- Outperforms existing methods including LoRA, LLaMA-Pro, and LoRAMoE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing original parameters prevents catastrophic forgetting of original languages during post-pretraining.
- Mechanism: By keeping the original model weights frozen, the learned representations and knowledge for original languages remain intact, preventing the model from overwriting them during training on expanded languages.
- Core assumption: The original model's knowledge for original languages is sufficiently robust to remain effective even when the model is exposed to significantly different data distributions from expanded languages.
- Evidence anchors:
  - [abstract] "Freezing original parameters preserves original language knowledge while adding new experts preserves the learning ability."
  - [section] "This ensures that the original capabilities of the model are preserved while expanding its proficiency in additional languages."
  - [corpus] No direct evidence found - weak corpus support for this mechanism.

### Mechanism 2
- Claim: Adding new experts through MoE architecture enables learning of expanded languages without increasing inference overhead.
- Mechanism: The MoE architecture allows the model to selectively activate a subset of experts for each input token, enabling the addition of parameters for expanded languages while keeping the number of active parameters per token constant.
- Core assumption: The routing mechanism can effectively select appropriate experts for different languages without introducing significant computational overhead.
- Evidence anchors:
  - [abstract] "Additionally, the MoE architecture maintains the same inference overhead while increasing total model parameters."
  - [section] "This enables the model to selectively activate a subset of experts for each input token, enabling the addition of parameters for expanded languages while keeping the number of active parameters per token constant."
  - [corpus] No direct evidence found - weak corpus support for this mechanism.

### Mechanism 3
- Claim: Language Priors Routing (LPR) with small replay data effectively recovers original language performance.
- Mechanism: By training the router with a small amount of replay data from original languages and using a loss function that encourages routing original language tokens to the frozen expert, the model learns to properly utilize the expanded experts for expanded languages while preserving the original expert for original languages.
- Core assumption: A small amount of replay data is sufficient to retrain the router to properly handle both original and expanded languages.
- Evidence anchors:
  - [abstract] "Reviewing with language priors routing (LPR) using less than 1% replay data to recover original language performance."
  - [section] "We design a LPR loss to be a Cross-Entropy loss for the tokens from the original languages, forcing the top-1 selection of these tokens to be expert 0, making the model work exactly the same as before the expansion."
  - [corpus] No direct evidence found - weak corpus support for this mechanism.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - Why needed here: Understanding why LLMs lose performance on original languages when trained on new languages is crucial for designing methods to prevent this phenomenon.
  - Quick check question: What is catastrophic forgetting and why does it occur in neural networks during sequential training?

- **Concept: Mixture-of-Experts (MoE)**
  - Why needed here: The MoE architecture is central to MoE-LPR's approach, allowing the model to add parameters for new languages without increasing inference overhead.
  - Quick check question: How does the MoE architecture enable efficient scaling of model parameters while maintaining constant inference overhead?

- **Concept: Load Balancing Loss**
  - Why needed here: The load balancing loss is used during post-pretraining to ensure that all experts are utilized effectively and to prevent routing collapse.
  - Quick check question: What is the purpose of the load balancing loss in MoE models and how does it prevent routing collapse?

## Architecture Onboarding

- **Component map**: Dense model (frozen) -> Router network -> New experts (trainable) -> LPR loss function -> Load balancing loss function

- **Critical path**: 
  1. Upcycle dense model to MoE architecture
  2. Post-pretrain new experts on expanded languages with load balancing loss
  3. Review with LPR on replay data to retrain router

- **Design tradeoffs**: 
  - Freezing original parameters prevents forgetting but may limit adaptation to new languages
  - Adding experts increases model capacity but requires effective routing
  - Small replay data is efficient but may not fully recover original language performance

- **Failure signatures**: 
  - Original language performance drops significantly after post-pretraining
  - Expanded language performance plateaus or decreases during training
  - Router becomes biased towards certain experts, leading to routing collapse

- **First 3 experiments**:
  1. Verify that freezing original parameters prevents catastrophic forgetting by comparing performance with and without freezing
  2. Test the effectiveness of the load balancing loss by training with and without it and observing expert utilization
  3. Evaluate the impact of replay data size on original language recovery by varying the amount of replay data used in the review stage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MoE-LPR's performance compare to continual learning approaches that dynamically expand experts over time rather than upscaling at once?
- Basis in paper: [inferred] The paper mentions Lifelong-MoE as related work but notes it uses different freezing methods and a different training framework. MoE-LPR performs expert expansion in a single upcycling step rather than dynamically.
- Why unresolved: The paper only evaluates static expert expansion through upcycling and doesn't compare against dynamic lifelong learning approaches that might offer better parameter efficiency.
- What evidence would resolve it: Experiments comparing MoE-LPR against dynamic expert expansion methods using the same multilingual datasets and evaluation protocols.

### Open Question 2
- Question: What is the impact of different routing strategies (e.g., top-1 vs top-2 vs top-K) on both expanded language performance and catastrophic forgetting prevention?
- Basis in paper: [explicit] The paper mentions using a top-2 strategy for expert selection, stating it achieves a trade-off between inference overhead and learning capabilities, but doesn't explore other routing configurations.
- Why unresolved: The paper commits to a top-2 strategy without empirical justification for why this is optimal or how it compares to alternatives.
- What evidence would resolve it: Systematic ablation studies varying K in the top-K routing across multiple benchmarks to measure trade-offs in performance and forgetting.

### Open Question 3
- Question: How does MoE-LPR's catastrophic forgetting prevention generalize to low-resource languages not seen during post-pretraining or review?
- Basis in paper: [explicit] The paper demonstrates generalization to French and Portuguese (high-resource languages not in the review stage) but only tests this on high-resource languages.
- Why unresolved: The evaluation only covers languages where the base model has relatively strong performance, leaving unclear whether the LPR mechanism works for truly low-resource languages with minimal pre-training data.
- What evidence would resolve it: Evaluations on genuinely low-resource languages (e.g., from the FLORES-101 dataset) not present in either the post-pretraining or review stages.

### Open Question 4
- Question: What is the relationship between the amount of replay data used in the review stage and the model's ability to recover original language capabilities?
- Basis in paper: [explicit] The paper reports using 50K documents per language (less than 1% of post-pretraining data) but only explores a limited range of data sizes (0K to 150K).
- Why unresolved: The paper doesn't explore the full spectrum of possible replay data amounts or establish clear scaling laws for how much data is truly necessary.
- What evidence would resolve it: Experiments testing multiple orders of magnitude variation in replay data amounts (e.g., 0.1%, 1%, 10%, 25%) to establish precise data-efficiency curves.

## Limitations

- Experimental scope limited to single base model (Qwen-1.5-1.8B) and small set of languages
- Evaluation focuses primarily on classification benchmarks, leaving gaps in understanding generative task performance
- Absolute performance levels for expanded languages remain modest despite improvements over baseline

## Confidence

**High Confidence**: The mechanism of freezing original parameters to prevent catastrophic forgetting is well-supported by experimental results showing maintained performance on original languages while expanding to new ones.

**Medium Confidence**: The effectiveness of Language Priors Routing with less than 1% replay data is supported by results, but the small sample size and limited language set make generalization difficult.

**Low Confidence**: Claims about efficient scaling to larger models or many more languages are speculative, as experiments only test on a small model and three additional languages.

## Next Checks

1. **Test Robustness to Base Model Size**: Evaluate MoE-LPR on larger base models (e.g., 7B or 13B parameters) to verify that the approach scales effectively and maintains claimed efficiency benefits.

2. **Expand Language Family Coverage**: Test the approach on languages from different families (e.g., Slavic, Semitic) to assess whether the routing mechanism generalizes beyond Indo-European languages used in the study.

3. **Validate on Generative Tasks**: Evaluate MoE-LPR on generative benchmarks (e.g., translation, summarization) to determine if the approach maintains effectiveness beyond classification tasks.