---
ver: rpa2
title: 'ConDo: Continual Domain Expansion for Absolute Pose Regression'
arxiv_id: '2412.13452'
source_url: https://arxiv.org/abs/2412.13452
tags:
- condo
- data
- training
- scan
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConDo addresses the problem of Absolute Pose Regression (APR) failing
  catastrophically on novel poses or scene conditions seen during inference. The core
  idea is to continually update the deployed APR model using unlabeled inference data
  by distilling knowledge from scene-agnostic localization methods, which are more
  robust to such changes.
---

# ConDo: Continual Domain Expansion for Absolute Pose Regression

## Quick Facts
- arXiv ID: 2412.13452
- Source URL: https://arxiv.org/abs/2412.13452
- Reference count: 36
- Primary result: Reduces localization error by >7x (14.8m vs 1.7m) on challenging scenes

## Executive Summary
ConDo addresses the fundamental limitation of Absolute Pose Regression (APR) models that fail catastrophically when encountering novel poses or scene conditions during inference. The core innovation is a continual learning framework that updates deployed APR models using unlabeled inference data by distilling knowledge from more robust scene-agnostic localization methods. By uniformly sampling from both historical and newly collected data, ConDo expands APR's generalization domain without requiring expensive full retraining, achieving similar performance up to 25x faster.

## Method Summary
ConDo is a continual learning framework for APR that operates during inference by collecting unlabeled data and using it to update the deployed model. The method distills knowledge from scene-agnostic localization methods (like HLoc) that are more robust to novel conditions, while maintaining performance on historical data through uniform sampling. Limited computation is assigned to each update round to ensure practical efficiency. The framework addresses the key challenge that standard APR training on additional data with pose variations can actually degrade performance on original scenes.

## Key Results
- Achieves 7.4x improvement in localization accuracy (14.8m to 1.7m median position error) on challenging scenes
- Outperforms baseline APRs across various architectures, scene types, and data changes
- Achieves similar performance to model retraining up to 25x faster while using unlabeled data
- Demonstrates robustness against different compute budgets, replay buffer sizes, and teacher prediction noise

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ConDo effectively expands APR's generalization domain by distilling knowledge from scene-agnostic localization methods.
- **Mechanism:** ConDo uses unlabeled inference data to update the deployed APR model by generating supervision signals through knowledge distillation from scene-independent localization methods like HLoc, which are more robust to scene and pose changes.
- **Core assumption:** Scene-agnostic methods provide reliable pose estimates for unlabeled data that can be used to train APR models without catastrophic forgetting.
- **Evidence anchors:**
  - [abstract] "ConDo effectively learns from unlabeled data by distilling knowledge from scene-agnostic localization methods."
  - [section 3.2] "Inspired by the fact that scene-independent methods (Arandjelovic et al. 2016; Sarlin et al. 2019; V on Stumberg and Cremers 2022), though slower and more memory consuming during inference, are much more robust than APR to novel poses and scene conditions, we distill the knowledge from these methods to APR using ∆."
  - [corpus] Weak evidence - no direct citations about scene-agnostic methods in related papers.
- **Break condition:** If the scene-agnostic teacher method becomes unreliable on novel data or produces noisy pose estimates that degrade APR performance.

### Mechanism 2
- **Claim:** Uniform sampling from historical and newly collected data prevents catastrophic forgetting while expanding the model's domain.
- **Mechanism:** During each ConDo update, data is uniformly sampled from both the original training set SΩ and newly collected unlabeled data ∆ to form training batches, ensuring the model maintains performance on historical data while adapting to new conditions.
- **Core assumption:** Uniform sampling from both old and new data provides sufficient diversity to prevent catastrophic forgetting while enabling domain expansion.
- **Evidence anchors:**
  - [abstract] "By sampling data uniformly from historical and newly collected data, ConDo can effectively expand the generalization domain of APR."
  - [section 3.2] "To expand the generalization domain of APR without forgetting, we uniformly sample images from SΩ S ∆ to form a training batch during the model update."
  - [corpus] Weak evidence - no direct citations about uniform sampling strategies in related papers.
- **Break condition:** If the distribution shift between historical and new data becomes too large, uniform sampling may dilute the effectiveness of adaptation.

### Mechanism 3
- **Claim:** ConDo achieves similar performance to full model retraining while using significantly less computation by limiting update budgets per round.
- **Mechanism:** ConDo assigns limited computation to each update round (N ∗ b/batch size iterations for N new images), ensuring total computation remains comparable to training one APR model from scratch while achieving similar accuracy.
- **Core assumption:** Limited computation per update round is sufficient to achieve meaningful domain adaptation without requiring full retraining.
- **Evidence anchors:**
  - [abstract] "Comparing to model re-training, ConDo achieves similar performance up to 25x faster."
  - [section 3.2] "The server continually expands the generalization domain of APR by updating it with the labeled training data (SΩ, PΩ), unlabeled data ∆ and a scene-independent teacher method fteacher for knowledge distillation. Limited computation is assigned to each round of model update to ensure practical efficiency."
  - [section 5.2] "ConDo reached a similar accuracy much faster than Re-train with GT, even without using GT. E.g., the performance of ConDo with just 12min of model updates was on-par with Re-train with GT for 5h — a 25x compute/time reduction."
- **Break condition:** If the computation budget per round is too restrictive, the model may not adapt sufficiently to novel data conditions.

## Foundational Learning

- **Concept:** Knowledge distillation in supervised learning
  - Why needed here: ConDo relies on transferring knowledge from scene-agnostic teacher models to APR student models using unlabeled data
  - Quick check question: How does knowledge distillation work in standard supervised learning, and what modifications are needed when the teacher and student have different robustness characteristics?

- **Concept:** Catastrophic forgetting in continual learning
  - Why needed here: ConDo must update the APR model with new data without degrading performance on previously seen data
  - Quick check question: What are the main strategies to prevent catastrophic forgetting in continual learning, and how does uniform sampling address this challenge?

- **Concept:** Domain adaptation without labels
  - Why needed here: ConDo operates in an unsupervised domain adaptation setting where only unlabeled data is available during inference
  - Quick check question: What are the key differences between supervised and unsupervised domain adaptation, and why do standard UDA methods fail for APR?

## Architecture Onboarding

- **Component map:** APR base model (PoseNet/PoseTransformer) -> Scene-agnostic teacher model (HLoc) -> Uniform sampling module -> Knowledge distillation training loop -> Updated APR model
- **Critical path:** Inference data → Teacher prediction → Distillation loss → APR update → Redeployment
- **Design tradeoffs:**
  - Teacher model accuracy vs. computation overhead (HLoc is more accurate but slower than retrieval-based methods)
  - Replay buffer size vs. storage constraints (reservoir sampling enables small buffers)
  - Update frequency vs. computation budget (more frequent updates adapt faster but consume more resources)
  - Multi-head vs. single-head architecture for multi-scene handling (more scalable but potentially less accurate)
- **Failure signatures:**
  - Performance degradation on historical data (catastrophic forgetting)
  - No improvement on novel data (insufficient adaptation)
  - High variance in pose predictions (teacher model noise)
  - Slow convergence or plateauing accuracy (inadequate computation budget)
- **First 3 experiments:**
  1. Validate teacher model accuracy on held-out inference data to ensure reliable supervision signals
  2. Test uniform sampling effectiveness by comparing performance with and without historical data inclusion
  3. Measure computation efficiency by varying update budgets and measuring accuracy gains per unit computation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental reason that APR models do not always benefit from seeing more data with pose changes, even when the additional data includes ground truth labels?
- Basis in paper: [explicit] The paper discusses that training on more data with strong pose changes interferes the performance on training scans, regardless of whether ConDo is applied.
- Why unresolved: The paper identifies this as a problem but does not provide a definitive explanation for why this interference occurs.
- What evidence would resolve it: Experiments showing how different APR architectures handle pose variations and whether certain architectural choices mitigate the negative impact of pose changes.

### Open Question 2
- Question: How does the choice of teacher model affect the performance of ConDo, and what are the trade-offs between different teacher models?
- Basis in paper: [explicit] The paper mentions that the accuracy of ConDo and the teacher is positively correlated, but different teachers provide reasonable improvements despite varying prediction noise.
- Why unresolved: The paper does not explore the specific characteristics of teacher models that lead to better performance or how to select the optimal teacher for different scenarios.
- What evidence would resolve it: Comparative studies of ConDo performance using different teacher models under various conditions and analyses of the characteristics that make some teachers more effective.

### Open Question 3
- Question: Can ConDo be effectively combined with stronger pre-trained backbones, and what are the potential synergies or conflicts between the two approaches?
- Basis in paper: [explicit] The paper shows that using a pre-trained Dino v2 backbone improves APR generalization but does not completely resolve the issue of scene condition and pose changes.
- Why unresolved: The paper does not explore the optimal way to combine pre-trained backbones with ConDo or whether there are conflicts in their training dynamics.
- What evidence would resolve it: Experiments testing various combinations of ConDo and different pre-trained backbones, analyzing their individual and combined effects on APR performance.

## Limitations

- Limited generalization validation on geographically diverse scenes beyond Pittsburgh area
- Potential performance degradation when historical and new data distributions have extreme domain discrepancy
- Reliance on scene-agnostic teacher methods that may have their own limitations in novel environments

## Confidence

- Core effectiveness claims: **High confidence** (7x improvement well-supported by quantitative results)
- Generalization claims: **Medium confidence** (primarily tested on scenes from same geographic area)
- Knowledge distillation mechanism: **Medium confidence** (limited ablation studies on teacher model choice)
- Catastrophic forgetting prevention: **High confidence** for tested scenarios

## Next Checks

1. Test ConDo on geographically diverse scenes (e.g., Tokyo or Berlin datasets) to verify cross-scene generalization claims
2. Conduct systematic ablation studies varying teacher model accuracy vs. inference speed to quantify the trade-off
3. Evaluate performance when historical and new data distributions have high domain discrepancy (e.g., day vs. night conditions)