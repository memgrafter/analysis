---
ver: rpa2
title: Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model Training
  Pipelines via Memoization-Awareness
arxiv_id: '2411.03731'
source_url: https://arxiv.org/abs/2411.03731
tags:
- eeipu
- cost
- pipeline
- stage
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the high cost of hyperparameter tuning for machine
  learning, vision, and language models, especially for large language models where
  a single training run can take days on a GPU. The authors propose EEIPU, a "memoization-aware"
  Bayesian Optimization algorithm that leverages pipeline caching to significantly
  reduce tuning costs.
---

# Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model Training Pipelines via Memoization-Awareness

## Quick Facts
- **arXiv ID**: 2411.03731
- **Source URL**: https://arxiv.org/abs/2411.03731
- **Reference count**: 40
- **Primary result**: EEIPU achieves 103% more hyperparameter candidates and 108% higher validation metric improvement compared to other BO algorithms

## Executive Summary
This paper addresses the high computational cost of hyperparameter tuning for machine learning, vision, and language models, particularly for large language models where training can take days on GPUs. The authors propose EEIPU, a memoization-aware Bayesian Optimization algorithm that leverages pipeline caching to significantly reduce tuning costs. EEIPU extends the Expected Improvement acquisition function by incorporating stage-wise cost models and memoization awareness, allowing it to cache intermediate pipeline stage outputs from top-performing configurations and restart from cached stages instead of the beginning. The algorithm uses a cost-cooling factor that encourages exploration of low-cost regions early on, gradually shifting to high-cost, high-quality regions as the budget depletes. Experiments show that EEIPU achieves an average of 103% more hyperparameter candidates and 108% higher validation metric improvement compared to other BO algorithms, effectively reducing hyperparameter tuning costs while maintaining or improving model quality.

## Method Summary
EEIPU is a memoization-aware Bayesian Optimization algorithm that extends Expected Improvement (EI) with stage-wise cost models and memoization awareness. The method incorporates pipeline caching by storing intermediate stage outputs from top-performing hyperparameter configurations, allowing future evaluations to reuse cached stages through prefix matching. EEIPU models stage-wise costs separately using Gaussian Processes for each pipeline stage, and uses a cost-cooling factor that starts at 1 and decays with remaining budget to balance exploration of low-cost regions early with exploitation of high-quality regions later. The algorithm runs on real ML, vision, and language model pipelines with specified optimization budgets, comparing results against baseline BO methods using metrics like iterations achieved and objective values.

## Key Results
- EEIPU achieves 103% more hyperparameter candidates within the same budget compared to other BO algorithms
- EEIPU shows 108% higher validation metric improvement on average compared to baselines
- The algorithm effectively reduces hyperparameter tuning costs while maintaining or improving model quality across ML, vision, and language model pipelines

## Why This Works (Mechanism)

### Mechanism 1
EEIPU reduces hyperparameter tuning costs by caching intermediate pipeline outputs and restarting from cached stages. When a hyperparameter configuration is evaluated, EEIPU caches the outputs of all pipeline stages except the final one. Future evaluations can reuse these cached stages by matching the prefix of the new hyperparameter configuration, thus avoiding recomputation of earlier stages. Core assumption: The cost savings from reusing cached stages outweigh the overhead of managing the cache and performing prefix matching. Evidence anchors: [abstract] "The authors propose EEIPU, a 'memoization-aware' Bayesian Optimization algorithm that leverages pipeline caching to significantly reduce tuning costs." [section] "Because pipelines are sequential – each stage's output is the input of the next one – we can memoize (cache) the outputs of intermediate stages." [corpus] "Weak corpus support: Only one related paper mentions caching, but not in the context of hyperparameter tuning." Break condition: If the overhead of cache management or prefix matching exceeds the cost savings, or if pipeline stages have non-deterministic outputs.

### Mechanism 2
EEIPU balances exploration and exploitation by incorporating a cost-cooling factor that shifts focus from low-cost regions to high-quality regions as the budget depletes. EEIPU uses a cost-cooling factor η that starts at 1 and decays with the remaining budget (η = remaining_budget / total_budget). This encourages exploration of low-cost regions early on, then gradually shifts to exploiting high-quality regions that may have higher costs. Core assumption: Early exploration of low-cost regions leads to discovering good hyperparameter prefixes that can be exploited later with memoization. Evidence anchors: [abstract] "It uses a cost-cooling factor that encourages exploration of low-cost regions early on, gradually shifting to high-cost, high-quality regions as the budget depletes." [section] "We adopt a budget-based cost-cooling mechanism to assign a high level of importance to low-cost regions and memoized candidate observations during earlier iterations, before gradually pushing EEIPU towards exploring higher-cost and entirely unexplored regions in the later iterations as the assigned budget decreases." [corpus] "Weak corpus support: Cost-cooling is not mentioned in related works, suggesting it may be a novel contribution." Break condition: If the cost-cooling factor is set too aggressively, EEIPU may prematurely shift to high-cost regions and exhaust the budget before finding good solutions.

### Mechanism 3
EEIPU models stage-wise costs separately to handle unknown and non-uniform evaluation costs, enabling cost-aware Bayesian optimization. EEIPU fits a separate Gaussian Process (GP) for each pipeline stage to model the log-costs of that stage. When evaluating a new hyperparameter configuration, it samples from these stage-wise cost GPs to estimate the total cost, with memoized stages having their costs set to a small epsilon value. Core assumption: The stage-wise costs are independent and can be modeled separately without significant loss of accuracy. Evidence anchors: [abstract] "The result is better-quality hyperparameters in the same amount of search time, or equivalently, reduced search time to reach the same hyperparameter quality." [section] "We incorporate memoization-awareness into EEIPU by training a set of K GP models on (X_k, ln(C_k)) for all k in {1, 2, ..., K} for modeling stage-wise cost surrogates." [corpus] "Weak corpus support: While cost-aware BO is mentioned, stage-wise cost modeling is not discussed in related works." Break condition: If the stage-wise costs are highly correlated or if the number of pipeline stages is very large, the independent modeling assumption may break down.

## Foundational Learning

- **Concept: Bayesian Optimization (BO)**
  - Why needed here: EEIPU is built upon BO principles, using surrogate models to guide the search for optimal hyperparameters.
  - Quick check question: What is the main advantage of BO over grid search or random search for hyperparameter tuning?

- **Concept: Gaussian Processes (GPs)**
  - Why needed here: GPs are used as surrogate models to predict both the objective function (model quality) and the stage-wise costs.
  - Quick check question: How does a GP model handle uncertainty in predictions, and why is this important for BO?

- **Concept: Memoization (Caching)**
  - Why needed here: Memoization allows EEIPU to reuse intermediate pipeline outputs, reducing the cost of evaluating new hyperparameter configurations.
  - Quick check question: What is the difference between memoization and standard caching, and how does it apply to multi-stage pipelines?

## Architecture Onboarding

- **Component map**: BO core -> GP models (objective + K stage-wise) -> Cache -> Cost-cooling -> Pipeline runner
- **Critical path**:
  1. Generate candidate hyperparameters
  2. Evaluate candidates using the acquisition function (EI × expected inverse cost^η)
  3. Run the pipeline with memoization
  4. Update the cache and GP models
  5. Repeat until budget is exhausted
- **Design tradeoffs**:
  - Cache size vs. overhead: Larger caches store more memoization opportunities but increase overhead
  - Number of GP models vs. accuracy: More stage-wise GPs capture cost variations better but increase computational cost
  - Cost-cooling schedule: Aggressive cooling finds high-quality solutions faster but may miss good low-cost regions
- **Failure signatures**:
  - Cache thrashing: If the cache is too small or the hyperparameter space is too large, memoization may not be effective
  - GP overfitting: If the GP models are not regularized or if the training data is noisy, predictions may be inaccurate
  - Premature convergence: If the cost-cooling factor is set too aggressively, EEIPU may get stuck in suboptimal regions
- **First 3 experiments**:
  1. Run EEIPU on a simple synthetic pipeline with known optimal hyperparameters to verify convergence
  2. Compare EEIPU's performance against a baseline BO algorithm (e.g., EI) on a real ML pipeline to demonstrate cost savings
  3. Test EEIPU's sensitivity to the cache size and cost-cooling factor on a synthetic pipeline to find good default values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EEIPU's performance scale with extremely large language models (e.g., 70B+ parameters) compared to smaller models?
- Basis in paper: [explicit] The paper mentions that the T5-large (770M parameters) experiment showed EEIPU's effectiveness, but does not test larger models.
- Why unresolved: The paper only tests up to T5-large (770M parameters). Larger models like GPT-3 (175B) or LLaMA (65B+) were not included due to computational constraints.
- What evidence would resolve it: Benchmarking EEIPU on 70B+ parameter models with similar pipeline structures and comparing iteration counts and objective values against baselines.

### Open Question 2
- Question: What is the optimal number of cached observations (Q) for EEIPU in different pipeline scenarios, and how sensitive is performance to this parameter?
- Basis in paper: [explicit] The paper uses Q=5 based on empirical testing but acknowledges that this choice impacts execution time and search performance.
- Why unresolved: The paper only tests Q=5 and does not systematically explore how different Q values affect performance across various pipeline lengths and complexities.
- What evidence would resolve it: Comprehensive ablation studies varying Q from 1 to 20 across different pipeline types (ML, vision, language) to determine optimal Q values and sensitivity.

### Open Question 3
- Question: How does EEIPU perform when the objective function is positively correlated with cost, where high-quality solutions require high-cost evaluations?
- Basis in paper: [inferred] The paper mentions this as a limitation in the Conclusion section, noting that EEIPU's initial focus on low-cost regions might delay discovery of high-quality, high-cost solutions.
- Why unresolved: The paper does not provide experimental evidence or solutions for scenarios where objective and cost are positively correlated.
- What evidence would resolve it: Testing EEIPU on benchmark functions where optimal solutions are known to be in high-cost regions, and comparing performance with and without warmup phases.

## Limitations

- The paper's claims about significant cost reduction rely heavily on synthetic benchmarks and limited real-world experiments with only three real pipelines, making generalizability uncertain.
- The effectiveness of memoization depends on pipeline stage structure and hyperparameter sensitivity, which varies across different ML tasks and may limit applicability.
- The comparison with only five baseline methods, none of which implement similar memoization techniques, limits the strength of the conclusions about EEIPU's superiority.

## Confidence

- **High Confidence**: The core mechanism of combining memoization with Bayesian optimization is technically sound and well-explained. The stage-wise cost modeling approach is a logical extension of existing cost-aware BO methods.
- **Medium Confidence**: The claimed performance improvements are supported by experimental results, but the limited scope of experiments (3 real pipelines) makes it difficult to assess generalizability. The choice of cost-cooling factor and cache size appears critical but is only validated through ablation studies.
- **Low Confidence**: The paper's claims about "dramatically reducing" costs and achieving "best of both worlds" are strong statements that may overstate the practical impact. The comparison with only 5 baseline methods, none of which implement similar memoization techniques, limits the strength of the conclusions.

## Next Checks

1. Test EEIPU on additional real-world ML pipelines beyond the three presented (Stacking, Segmentation, T5-small) to assess generalizability across different task types and pipeline structures.
2. Conduct a detailed analysis of the relationship between pipeline stage characteristics (e.g., computational cost, hyperparameter sensitivity) and memoization effectiveness to identify scenarios where the approach is most beneficial.
3. Implement a direct comparison between EEIPU and a naive BO baseline that uses standard caching without the memoization-aware acquisition function to isolate the contribution of the novel acquisition function from the caching mechanism.