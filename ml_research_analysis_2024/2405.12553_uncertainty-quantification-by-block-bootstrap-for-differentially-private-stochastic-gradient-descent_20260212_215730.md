---
ver: rpa2
title: Uncertainty quantification by block bootstrap for differentially private stochastic
  gradient descent
arxiv_id: '2405.12553'
source_url: https://arxiv.org/abs/2405.12553
tags:
- where
- bootstrap
- which
- privacy
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a block bootstrap method for uncertainty quantification
  in locally differentially private stochastic gradient descent (LDP-SGD). The proposed
  approach applies a multiplier block bootstrap to the mean of SGD iterates, providing
  a computationally tractable alternative to existing methods that require privacy
  budget splitting.
---

# Uncertainty quantification by block bootstrap for differentially private stochastic gradient descent

## Quick Facts
- arXiv ID: 2405.12553
- Source URL: https://arxiv.org/abs/2405.12553
- Authors: Holger Dette; Carina Graw
- Reference count: 40
- Key outcome: Block bootstrap method for uncertainty quantification in LDP-SGD that avoids privacy budget splitting

## Executive Summary
This paper develops a block bootstrap method for uncertainty quantification in locally differentially private stochastic gradient descent (LDP-SGD). The approach applies a multiplier block bootstrap to the mean of SGD iterates, providing a computationally tractable alternative to existing methods that require privacy budget splitting. The method is shown to be statistically valid under appropriate conditions on block length and learning rate, and avoids the need for LDP estimation of gradient matrices. Simulation studies on quantile estimation and quantile regression demonstrate that the proposed method achieves comparable or better coverage probabilities than existing approaches while producing shorter confidence intervals.

## Method Summary
The method implements block bootstrap for uncertainty quantification in LDP-SGD by running the LDP-SGD algorithm to generate iterates, partitioning these iterates into blocks of length l = ⌊n^0.75⌋, generating B=500 bootstrap samples with random multipliers, and computing confidence intervals from the empirical distribution of bootstrap estimates. The approach uses Laplace mechanism for gradient perturbation in quantile regression and Randomized Response for quantiles, with learning rate schedule ηi=i^-0.51 and privacy parameter ϵ=1. The key innovation is avoiding privacy budget splitting by applying multiplier block bootstrap directly to LDP-SGD iterates.

## Key Results
- Achieves comparable or better coverage probabilities than batch mean approach
- Produces shorter confidence intervals while maintaining nominal coverage
- Avoids privacy budget splitting required by existing LDP-SGD uncertainty quantification methods
- Valid under conditions on block length (l = ⌊n^0.75⌋) and learning rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The block bootstrap provides a valid approximation to the distribution of the LDP-SGD estimator without splitting privacy budget
- Mechanism: By resampling blocks of SGD iterates and multiplying by random weights, the method mimics the strong dependence structure in the SGD path while avoiding repeated queries to private data
- Core assumption: The error sequence in LDP-SGD is a martingale difference process and the learning rate and block length satisfy Theorem 3.2 conditions
- Evidence anchors: [abstract] "computationally tractable and does not require an adjustment of the privacy budget"; [section 3] "we propose to apply the multiplier block bootstrap principle to the iterations"

### Mechanism 2
- Claim: The bootstrap distribution converges weakly to the asymptotic normal distribution of the LDP-SGD estimator
- Mechanism: Conditional on the SGD iterates, the weighted block means of centered iterates approximate the normal limit distribution by Berry-Esseen arguments under bounded multipliers
- Core assumption: The martingale difference errors satisfy uniform integrability and moment bounds as in Assumption A.2
- Evidence anchors: [section 3] "consistency of the proposed multiplier block bootstrap for the LDP-SGD under appropriate conditions"; [supplement] Theorem B.1 assumes linear gradient

### Mechanism 3
- Claim: The LDP-SGD error term decomposes into a martingale part and a remainder that vanishes asymptotically, enabling bootstrap validity
- Mechanism: The decomposition ξi = ξi(0) + ζi(θLDP i−1) in Assumption A.1(4) allows separating the stochastic error from the estimation error, which is negligible for large blocks
- Core assumption: The remainder ζi(θLDP i−1) is small relative to the martingale term and the learning rate
- Evidence anchors: [section 2] "define indeed martingale difference processes"; [supplement] Assumption A.1(4) requires E[ζi(θLDP i−1)|Fi−1] → 0 a.s.

## Foundational Learning

- Concept: Stochastic Gradient Descent with martingale difference errors
  - Why needed here: The validity of the block bootstrap relies on the errors forming a martingale difference process so that central limit theorem arguments apply
  - Quick check question: What condition must the error ξi satisfy to be a martingale difference process with respect to the filtration Fi?

- Concept: Local Differential Privacy mechanisms (Laplace, Randomized Response)
  - Why needed here: LDP-SGD must add noise at each iteration without a trusted curator; understanding the privacy mechanisms is essential to analyze the error decomposition
  - Quick check question: How does the Laplace mechanism ensure ϵ-LDP for gradient updates?

- Concept: Multiplier block bootstrap for dependent data
  - Why needed here: Standard bootstrap fails for dependent SGD iterates; the block bootstrap with multipliers is designed to handle dependence without privacy budget splitting
  - Quick check question: What role does the block length l play in the validity of the multiplier block bootstrap?

## Architecture Onboarding

- Component map: LDP-SGD module -> Bootstrap module -> Confidence interval output
- Critical path: 1) Run LDP-SGD to generate iterates θLDP i; 2) Partition iterates into m blocks of length l; 3) Generate B bootstrap samples with multipliers ϵj; 4) Compute bootstrap estimates ¯θ⋆(b); 5) Calculate empirical quantiles for confidence interval
- Design tradeoffs: Larger block length l captures dependence better but reduces number of blocks m; smaller privacy budget ϵ increases noise but preserves utility; number of bootstrap samples B affects runtime but not validity asymptotically
- Failure signatures: Coverage far from nominal → martingale difference assumptions violated; bootstrap variance much larger than asymptotic variance → insufficient block length or too small m; runtime too long → B too large or blocks too small
- First 3 experiments: 1) Run LDP-SGD with Laplace mechanism on synthetic normal data; verify martingale difference property empirically; 2) Apply block bootstrap with l=⌊n^0.75⌋, B=500; compare coverage to batch mean method; 3) Vary privacy budget ϵ and block length l; measure effect on coverage and interval length

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of block length l and multiplier distribution affect the statistical efficiency and coverage probability of the block bootstrap method?
- Basis in paper: [explicit] The paper discusses choosing l = ⌊n^0.75⌋ and mentions Unif(-√3, √3) multipliers, but doesn't systematically explore the impact of these choices
- Why unresolved: The paper provides one example of parameter choices but doesn't investigate sensitivity to different block lengths or multiplier distributions
- What evidence would resolve it: Systematic simulation studies varying β and multiplier distribution, comparing coverage probabilities and interval lengths

### Open Question 2
- Question: Can the block bootstrap method be extended to handle non-stationary or time-varying optimization problems?
- Basis in paper: [inferred] The paper focuses on stationary SGD settings and doesn't address scenarios where the loss function changes over time
- Why unresolved: The theoretical analysis relies on stationarity assumptions that may not hold in dynamic optimization environments
- What evidence would resolve it: Theoretical extension of bootstrap validity proofs to non-stationary settings and simulation studies on time-varying problems

### Open Question 3
- Question: How does the block bootstrap method compare to other LDP-SGD uncertainty quantification approaches in terms of computational efficiency and privacy-utility trade-offs?
- Basis in paper: [explicit] The paper mentions existing methods require privacy budget splitting and compares to some alternatives, but doesn't provide comprehensive computational complexity analysis
- Why unresolved: While claiming computational tractability, the paper doesn't formally analyze time/space complexity or compare privacy budget utilization efficiency
- What evidence would resolve it: Detailed computational complexity analysis and empirical comparisons of runtime, memory usage, and accuracy across different LDP-SGD inference methods

## Limitations
- Relies on strong assumptions about martingale difference properties and uniform integrability that may not hold in practice
- Theoretical guarantees depend on careful calibration of block length l = ⌊n^0.75⌋ which may not be optimal for all settings
- Simulation studies focus on specific normal/truncated normal distributions that may not generalize to complex data structures

## Confidence

- **High confidence**: Computational tractability advantage over existing LDP-SGD uncertainty quantification methods (no privacy budget splitting required)
- **Medium confidence**: Theoretical validity under Assumption A.1 and Theorem 3.2 conditions, pending verification of uniform integrability and martingale difference properties in practice
- **Medium confidence**: Empirical performance gains in coverage and interval length compared to batch mean approaches, though this depends on specific experimental conditions

## Next Checks

1. **Assumption verification**: Empirically test whether the martingale difference property and uniform integrability conditions hold for various privacy budgets and learning rate schedules using residual analysis from LDP-SGD runs.

2. **Robustness to data distributions**: Evaluate coverage and interval performance on heavy-tailed distributions and non-normal data to assess generalizability beyond the normal/truncated normal settings used in simulations.

3. **Block length sensitivity analysis**: Systematically vary block length l around the theoretical choice l = ⌊n^0.75⌋ to quantify the method's sensitivity to this critical tuning parameter and identify optimal choices for different sample sizes.