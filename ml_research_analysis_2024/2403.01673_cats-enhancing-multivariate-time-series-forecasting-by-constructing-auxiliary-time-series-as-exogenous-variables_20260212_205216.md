---
ver: rpa2
title: 'CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary
  Time Series as Exogenous Variables'
arxiv_id: '2403.01673'
source_url: https://arxiv.org/abs/2403.01673
tags:
- series
- cats
- time
- forecasting
- multivariate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the under-performance of multivariate time
  series forecasting models compared to univariate ones, despite the latter ignoring
  inter-series relationships. The authors propose CATS (Constructing Auxiliary Time
  Series), which generates auxiliary time series from original data to capture inter-series
  relationships through a 2D attention-like mechanism.
---

# CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables

## Quick Facts
- arXiv ID: 2403.01673
- Source URL: https://arxiv.org/abs/2403.01673
- Authors: Jiecheng Lu; Xu Han; Yan Sun; Shihao Yang
- Reference count: 40
- Primary result: State-of-the-art performance across nine datasets using simple MLP predictor

## Executive Summary
This paper addresses the under-performance of multivariate time series forecasting models compared to univariate ones, despite the latter ignoring inter-series relationships. The authors propose CATS (Constructing Auxiliary Time Series), which generates auxiliary time series from original data to capture inter-series relationships through a 2D attention-like mechanism. CATS identifies three key principles for auxiliary series - continuity, sparsity, and variability - implemented through different modules. Even with a simple 2-layer MLP predictor, CATS achieves state-of-the-art performance across nine datasets, significantly reducing complexity and parameters compared to previous multivariate models. The method is particularly effective in scenarios with strong multivariate relationships, demonstrating adaptability across different dataset characteristics.

## Method Summary
CATS constructs auxiliary time series from original multivariate data to capture inter-series relationships that traditional multivariate models struggle to model effectively. The method employs a 2D attention-like mechanism to identify relationships between original and auxiliary series. Three construction principles guide the auxiliary series generation: continuity (maintaining temporal coherence), sparsity (capturing infrequent but important patterns), and variability (representing dynamic changes). These principles are implemented through dedicated modules that transform the original time series into auxiliary representations. The approach is paired with a simple 2-layer MLP predictor, demonstrating that the auxiliary construction mechanism itself drives performance rather than complex temporal modeling architectures.

## Key Results
- Achieves state-of-the-art performance across nine benchmark datasets
- Significantly reduces model complexity and parameters compared to previous multivariate models
- Outperforms complex deep learning architectures while using only a simple 2-layer MLP predictor
- Particularly effective in datasets with strong multivariate relationships
- Demonstrates adaptability across different dataset characteristics and domains

## Why This Works (Mechanism)
The paper addresses the fundamental challenge that multivariate time series models often underperform univariate models because they struggle to effectively capture inter-series relationships. CATS solves this by explicitly constructing auxiliary time series that encode these relationships through a 2D attention-like mechanism. The three construction principles work synergistically: continuity ensures temporal coherence is maintained, sparsity captures rare but important cross-series patterns, and variability represents dynamic changes that might indicate important relationships. By transforming the original data into these auxiliary representations, the model can leverage the strong predictive power of univariate methods while implicitly incorporating multivariate information.

## Foundational Learning
- **2D Attention-like Mechanism**: Used to identify relationships between original and auxiliary time series; needed because traditional attention mechanisms are designed for sequential data, not cross-series relationships; quick check: verify the mechanism can distinguish between relevant and irrelevant auxiliary series
- **Auxiliary Time Series Construction**: The process of generating new time series from original data to capture hidden relationships; needed because direct modeling of inter-series relationships is computationally expensive and often ineffective; quick check: ensure constructed series add information beyond the original data
- **Continuity Principle**: Ensures auxiliary series maintain temporal coherence with original data; needed to preserve meaningful temporal patterns while adding multivariate information; quick check: verify auxiliary series don't introduce temporal artifacts
- **Sparsity Principle**: Captures infrequent but important cross-series patterns; needed because not all relationships are constant or frequent; quick check: identify cases where sparse patterns are critical for accurate forecasting
- **Variability Principle**: Represents dynamic changes in relationships over time; needed because inter-series relationships often evolve; quick check: detect scenarios where relationship dynamics significantly impact forecast accuracy

## Architecture Onboarding

**Component Map**: Original Time Series -> Construction Modules (Continuity, Sparsity, Variability) -> Auxiliary Time Series -> 2D Attention-like Mechanism -> MLP Predictor -> Forecast

**Critical Path**: The core innovation lies in the auxiliary time series construction modules, which transform original data into representations that better capture inter-series relationships. The 2D attention-like mechanism then identifies which auxiliary series are most relevant for each prediction. The simple MLP predictor demonstrates that the construction mechanism itself drives performance.

**Design Tradeoffs**: Simplicity vs. complexity - CATS achieves superior performance with a simple MLP predictor, suggesting the auxiliary construction is more important than complex temporal modeling. The tradeoff is that the method requires careful design of construction principles to work effectively across domains.

**Failure Signatures**: The method may underperform when inter-series relationships are weak or non-existent, as the auxiliary construction could add noise rather than signal. It may also struggle with highly irregular or sparse data where the continuity principle becomes difficult to maintain.

**3 First Experiments**:
1. Test auxiliary series generation on a dataset with known strong multivariate relationships to verify the construction principles capture expected patterns
2. Evaluate the 2D attention-like mechanism's ability to identify relevant auxiliary series by masking different auxiliary series and measuring impact on forecast accuracy
3. Compare CATS performance with and without each construction principle (continuity, sparsity, variability) to isolate their individual contributions

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Performance may degrade on noisy, irregular, or sparse data common in healthcare or IoT applications, as the paper focuses on relatively clean, structured time series data
- Unclear how CATS would perform when paired with more complex architectures that might better capture temporal dynamics, as the paper relies on a simple 2-layer MLP predictor
- The 2D attention-like mechanism lacks explicit interpretability compared to more established attention mechanisms in sequence modeling

## Confidence
- **High**: Core claim that CATS outperforms baseline multivariate models on tested datasets, given rigorous experimental setup and significant performance improvements
- **Medium**: Claim about parameter reduction, as comparison is primarily against complex deep learning architectures and actual savings may vary by implementation
- **Low**: Generalizability of the three construction principles across diverse domains, as validation is primarily on energy and electricity datasets

## Next Checks
1. Test CATS on datasets with known irregular sampling patterns and high noise levels, such as medical time series or sensor data from industrial IoT applications, to assess robustness beyond clean energy data
2. Evaluate the method with more complex temporal modeling architectures (e.g., Transformer-based or recurrent networks) as the predictor component to determine if performance gains persist with stronger temporal encoders
3. Conduct ablation studies specifically isolating the contribution of each construction principle (continuity, sparsity, variability) to determine their individual importance and potential for domain-specific adaptation