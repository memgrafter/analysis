---
ver: rpa2
title: 'AgentGen: Enhancing Planning Abilities for Large Language Model based Agent
  via Environment and Task Generation'
arxiv_id: '2408.00764'
source_url: https://arxiv.org/abs/2408.00764
tags:
- arxiv
- planning
- agent
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AGENT GEN, a framework that automatically generates
  diverse environments and planning tasks to enhance the planning abilities of LLM-based
  agents through instruction tuning. The method leverages an inspiration corpus to
  generate environment specifications with LLMs, implements these environments in
  code, and employs a bidirectional evolution method (BI-EVOL) to create planning
  tasks with a gradual difficulty curve.
---

# AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation

## Quick Facts
- arXiv ID: 2408.00764
- Source URL: https://arxiv.org/abs/2408.00764
- Authors: Mengkang Hu; Pu Zhao; Can Xu; Qingfeng Sun; Jianguang Lou; Qingwei Lin; Ping Luo; Saravan Rajmohan
- Reference count: 40
- Key outcome: AGENT GEN-tuned Llama-3.1-8B achieves 33.3 progress rate on in-domain tasks vs 25.0 for GPT-3.5, representing >10x improvement over raw Llama-3.1-8B

## Executive Summary
This paper introduces AGENT GEN, a framework that enhances LLM-based agents' planning abilities by automatically generating diverse environments and planning tasks through instruction tuning. The method uses an inspiration corpus to generate environment specifications with LLMs, implements these in code, and employs bidirectional evolution (BI-EVOL) to create tasks with gradual difficulty curves. The approach demonstrates state-of-the-art performance, with AGENT GEN-tuned Llama-3.1-70B achieving the best results in planning tasks and Llama-3.1-8B outperforming GPT-3.5 by 33.3 vs 25.0 progress rate.

## Method Summary
AGENT GEN is a two-stage framework for enhancing LLM-based agent planning abilities. First, it generates diverse environments by using an inspiration corpus of domain-specific text segments to prompt LLMs for environment specifications, which are then implemented in code. Second, it creates planning tasks using bidirectional evolution (BI-EVOL) that evolves tasks in both easier (easy-evol) and harder (hard-evol) directions to create a smooth difficulty curve. The framework synthesizes 7,246 trajectories from 592 environments with 20 tasks each, which are used to instruction-tune LLMs. The method achieves state-of-the-art results, with AGENT GEN-tuned Llama-3.1-8B outperforming GPT-3.5 on in-domain tasks and Llama-3.1-70B achieving SOTA performance.

## Key Results
- AGENT GEN-tuned Llama-3.1-8B achieves 33.3 progress rate on in-domain tasks vs 25.0 for GPT-3.5
- More than tenfold improvement over raw Llama-3.1-8B (33.3 vs 3.0)
- AGENT GEN-tuned Llama-3.1-70B achieves state-of-the-art results in planning tasks
- Demonstrates effectiveness and generalization across different models and both in-domain and out-of-domain tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AGENT GEN improves LLM planning abilities by generating diverse environments and tasks through automated synthesis
- Mechanism: The framework uses LLMs to generate environment specifications from an inspiration corpus, implements them in code, and creates planning tasks with bidirectional evolution (BI-EVOL) to ensure difficulty diversity
- Core assumption: LLMs can effectively generate diverse and valid environment specifications when provided with varied contextual inspiration
- Evidence anchors: [abstract] mentions using inspiration corpus for environment generation; [section 3.1] details the corpus approach; related works show weak evidence for inspiration corpus effectiveness

### Mechanism 2
- Claim: Bidirectional evolution (BI-EVOL) creates a smoother learning curve for LLMs by evolving tasks in both easier and harder directions
- Mechanism: BI-EVOL starts with random planning tasks and applies constraints to simplify them (easy-evol) and make them more complex (hard-evol), creating a comprehensive task set
- Core assumption: LLMs can effectively generate tasks that can be meaningfully evolved in both directions to create appropriate difficulty progression
- Evidence anchors: [abstract] introduces BI-EVOL for bidirectional task evolution; [section 3.2] describes the two-directional approach; related works show weak evidence for bidirectional evolution effectiveness

### Mechanism 3
- Claim: The combination of diverse environments and difficulty-varied tasks leads to better generalization across both in-domain and out-of-domain planning tasks
- Mechanism: AGENT GEN creates 592 diverse environments with 20 tasks each, generating 7,246 trajectories that train LLMs to handle various planning scenarios
- Core assumption: A larger, more diverse dataset leads to better generalization performance
- Evidence anchors: [abstract] reports improved generalization performance; [section 4.2] shows AGENT GEN-tuned model outperforms baselines; related works show moderate evidence for dataset diversity benefits

## Foundational Learning

- Concept: Planning problems as tuples (T, E) with environments and tasks
  - Why needed here: Understanding how AGENT GEN structures environments (E) and tasks (T) is crucial for grasping the framework's approach
  - Quick check question: What are the components of a planning problem tuple (T, E)?

- Concept: PDDL (Planning Domain Definition Language) for representing planning problems
  - Why needed here: The paper uses PDDL to implement environments and tasks, so understanding this representation is essential
  - Quick check question: How does PDDL represent environments versus tasks in planning problems?

- Concept: Instruction tuning for agent training
  - Why needed here: AGENT GEN uses instruction tuning with synthesized trajectories, so understanding this training approach is important
  - Quick check question: What is the difference between instruction tuning and traditional fine-tuning?

## Architecture Onboarding

- Component map: Inspiration corpus → LLM environment specification generation → Code implementation → Environment library → LLM task generation → BI-EVOL (bidirectional evolution) → Task set with difficulty curve → Expert planner (FastDownward) → Trajectory synthesis → Instruction tuning

- Critical path: Environment specification generation → Code implementation → Task generation with BI-EVOL → Trajectory synthesis → Model training

- Design tradeoffs:
  - Environment diversity vs. implementation complexity: Using an inspiration corpus increases diversity but requires more sophisticated LLM prompting
  - Task difficulty range vs. task quality: BI-EVOL creates more varied difficulty but may generate some poor-quality tasks
  - Dataset size vs. training efficiency: 7,246 trajectories provide good coverage but increase training time

- Failure signatures:
  - If generated environments fail validation: Likely issues with LLM prompting or inspiration corpus quality
  - If tasks don't show difficulty progression: BI-EVOL may not be effectively evolving tasks
  - If training doesn't improve performance: Possible issues with trajectory quality or instruction tuning approach

- First 3 experiments:
  1. Test environment generation with a small inspiration corpus subset to verify LLM can generate valid specifications
  2. Implement BI-EVOL on a single seed task to verify bidirectional evolution works as expected
  3. Train a small model on a subset of generated trajectories to verify the training pipeline works before full-scale implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size and composition of the inspiration corpus for generating diverse environments?
- Basis in paper: Explicit
- Why unresolved: The paper mentions using LIMA as the inspiration corpus but doesn't explore how different corpus sizes, compositions, or sources might affect environment diversity and quality.
- What evidence would resolve it: Comparative experiments testing different inspiration corpus types (e.g., domain-specific vs. general-purpose, various sizes) and their impact on environment diversity metrics and agent performance.

### Open Question 2
- Question: How does the bidirectional evolution method (BI-EVOL) compare to alternative task difficulty progression methods?
- Basis in paper: Explicit
- Why unresolved: While BI-EVOL is proposed as an improvement over one-directional evolution, the paper doesn't compare it to other potential methods like curriculum learning, adaptive difficulty scaling, or human-designed difficulty curves.
- What evidence would resolve it: Systematic comparison of BI-EVOL against alternative difficulty progression methods using the same environment generation framework and evaluating their impact on agent learning efficiency and final performance.

### Open Question 3
- Question: What is the relationship between trajectory quality metrics and downstream agent performance?
- Basis in paper: Inferred
- Why unresolved: The paper uses a domain-independent planner to generate high-quality trajectories but doesn't explore how different trajectory quality metrics (e.g., optimality, diversity, complexity) correlate with agent performance improvements.
- What evidence would resolve it: Correlation analysis between trajectory quality metrics (optimality scores, diversity indices, step counts) and agent performance across multiple tasks and models, potentially informing trajectory selection criteria.

## Limitations

- The scalability and robustness of BI-EVOL for more complex planning domains remains uncertain
- Generated task quality depends heavily on LLM's ability to maintain logical consistency during evolution
- The specific impact of different inspiration corpus compositions on environment diversity is not thoroughly explored

## Confidence

**High Confidence:** The core architecture of AGENT GEN (environment generation + task generation + instruction tuning) is well-specified and the experimental methodology is sound. The comparison results against GPT-3.5 and Llama-3.1 baselines are reliable given the clear experimental setup.

**Medium Confidence:** The claims about BI-EVOL's effectiveness in creating smooth difficulty curves are supported by experimental results, but the mechanism's generalizability to more complex domains remains uncertain. The paper demonstrates success in the tested domains but doesn't provide extensive ablation studies on the evolution mechanism itself.

**Low Confidence:** The specific impact of different inspiration corpus compositions on environment diversity is not thoroughly explored. While the paper claims diversity is important, it doesn't systematically investigate how corpus composition affects final performance or what minimal corpus size is required.

## Next Checks

1. **Cross-domain robustness test:** Evaluate AGENT GEN on planning tasks from domains not represented in the original inspiration corpus to assess generalization beyond the training distribution.

2. **BI-EVOL quality analysis:** Conduct a systematic analysis of evolved tasks to measure how often evolution produces logically inconsistent or nonsensical tasks, particularly in hard-evol direction.

3. **Resource efficiency evaluation:** Measure the computational resources required for environment generation, task evolution, and training compared to alternative approaches to assess practical deployment feasibility.