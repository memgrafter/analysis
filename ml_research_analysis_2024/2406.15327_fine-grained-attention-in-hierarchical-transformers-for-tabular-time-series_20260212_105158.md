---
ver: rpa2
title: Fine-grained Attention in Hierarchical Transformers for Tabular Time-series
arxiv_id: '2406.15327'
source_url: https://arxiv.org/abs/2406.15327
tags:
- tabular
- time-series
- attention
- data
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose Fieldy, a hierarchical transformer architecture
  for tabular time-series data that enables fine-grained attention across all fields.
  Unlike existing approaches that restrict attention to either rows or columns in
  the first stage, Fieldy combines row-wise and column-wise transformers to learn
  contextualized field representations, which are then processed by a final transformer.
---

# Fine-grained Attention in Hierarchical Transformers for Tabular Time-series

## Quick Facts
- arXiv ID: 2406.15327
- Source URL: https://arxiv.org/abs/2406.15327
- Authors: Raphael Azorin; Zied Ben Houidi; Massimo Gallo; Alessandro Finamore; Pietro Michiardi
- Reference count: 25
- One-line primary result: Fieldy achieves lower RMSE and higher average precision than state-of-the-art hierarchical and single-stage transformer models on tabular time-series tasks

## Executive Summary
This paper proposes Fieldy, a hierarchical transformer architecture that enables fine-grained attention across all fields in tabular time-series data. Unlike existing approaches that restrict attention to either rows or columns in the first stage, Fieldy combines row-wise and column-wise transformers to learn contextualized field representations, which are then processed by a final transformer. This design allows the model to capture interactions between fields across separate rows and columns. Evaluated on pollution prediction and loan default classification tasks, Fieldy achieves lower RMSE and higher average precision compared to state-of-the-art hierarchical and single-stage transformer models, as well as non-deep learning baselines.

## Method Summary
Fieldy uses a two-stage hierarchical transformer architecture for tabular time-series data. In the first stage, two separate transformers process the data row-wise and column-wise, respectively, to generate contextualized field representations. These representations are then combined and passed through a final transformer in the second stage to capture relationships across all fields. The model incorporates both row position and column index embeddings to provide structural information. Fieldy is trained using BERT-like pre-training with token masking followed by fine-tuning. The architecture is evaluated on two public datasets (pollution prediction and loan default classification) against row-based and column-based TabBERT models, as well as single-stage transformer baselines.

## Key Results
- Fieldy achieves lower RMSE on pollution prediction tasks compared to hierarchical and single-stage transformer models
- Fieldy obtains higher average precision on loan default classification compared to row-based, column-based, and single-stage baselines
- The performance improvements demonstrate that combining row-wise and column-wise attention enhances results without increasing model size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained attention across all fields improves performance by capturing interactions that row- or column-wise attention alone misses.
- Mechanism: Fieldy's two-stage architecture first contextualizes each field both row-wise and column-wise, then relates all contextualized fields to each other in the second stage, enabling cross-field attention that spans rows and columns simultaneously.
- Core assumption: Capturing field-level relationships across separate rows and columns is crucial for tabular time-series tasks.
- Evidence anchors:
  - [abstract] "Unlike existing approaches that restrict attention to either rows or columns in the first stage, Fieldy combines row-wise and column-wise transformers to learn contextualized field representations, which are then processed by a final transformer."
  - [section] "While hierarchical architectures capture all table dimensions, they don't do that simultaneously, hence limiting visibility on more subtle cross-field relationships important for the downstream task."
- Break condition: If field-level relationships are not important for the specific task, the additional computational cost may not be justified.

### Mechanism 2
- Claim: Incorporating both row position and column index embeddings provides essential structural information for field-based attention.
- Mechanism: Fieldy adds row position and column index embeddings to each contextualized field before passing them to the final transformer, compensating for the lack of inherent table structure in the field-wise attention approach.
- Core assumption: The final transformer needs explicit positional information to understand the original table structure when processing a flattened sequence of fields.
- Evidence anchors:
  - [section] "Therefore, we incorporate row and column positional embeddings [4]. Before being passed to the Final transformer, each contextualized field is augmented (by means of element-wise addition) with two embeddings: one carrying its original row position and one carrying its original column index."
- Break condition: If the final transformer can infer table structure from the data itself, these embeddings may be unnecessary.

### Mechanism 3
- Claim: Balancing model capacity between the first and second stages affects performance, with more capacity in the first stage being beneficial.
- Mechanism: Fieldy allocates more layers to the first-stage Field transformers, which learn contextualized field representations, rather than the second-stage Final transformer.
- Core assumption: The quality of field representations learned in the first stage is more important than the second stage's ability to relate these representations.
- Evidence anchors:
  - [section] "We observe that hierarchical architectures perform better when favoring the first stage. This is particularly evident for Fieldy with up to +16% performance improvement for the Pollution data set."
- Break condition: If the second stage is crucial for capturing complex relationships between fields, reducing its capacity may harm performance.

## Foundational Learning

- Concept: Hierarchical transformer architectures
  - Why needed here: Understanding the two-stage approach where fields are first contextualized then related to each other.
  - Quick check question: What are the two stages in Fieldy's architecture and what does each accomplish?

- Concept: Attention mechanisms in transformers
  - Why needed here: Fieldy relies on self-attention to relate fields across rows and columns.
  - Quick check question: How does self-attention allow a transformer to relate different elements in a sequence?

- Concept: Positional encodings
  - Why needed here: Fieldy uses row position and column index embeddings to provide structural information.
  - Quick check question: Why does Fieldy need to add positional information when other hierarchical models don't?

## Architecture Onboarding

- Component map: Input → Field transformers (row-wise and column-wise) → Combine → Final transformer → Output
- Critical path: Input → Field transformers → Fully connected layer → Final transformer → Task-specific prediction
- Design tradeoffs:
  - Fieldy trades computational cost for fine-grained attention across all fields
  - Requires more careful tuning of positional encodings compared to row- or column-based approaches
  - Model capacity distribution between stages affects performance
- Failure signatures:
  - Poor performance on tasks where field-level cross-row/column relationships are not important
  - Overfitting on small datasets due to increased model complexity
  - Incorrect results if positional embeddings are not properly configured
- First 3 experiments:
  1. Replace Fieldy's two Field transformers with a single transformer that processes flattened input, comparing performance to assess the value of separate row-wise and column-wise processing.
  2. Remove positional embeddings from Fieldy and compare performance to understand their importance.
  3. Reallocate layers between Field and Final transformers (e.g., more layers in Final transformer) to test the hypothesis about capacity distribution.

## Open Questions the Paper Calls Out

- Question: Does Fieldy's performance advantage persist on larger tabular time-series datasets with longer sequences?
  - Basis in paper: [explicit] The authors mention that Fieldy's computational complexity is higher than row-based/column-based models and suggest that future work should evaluate on larger datasets like click-through rate data which has sequential nature and large volume.
  - Why unresolved: The current evaluation only uses two relatively small public datasets (67K samples for pollution, 5K for loan default). The authors acknowledge this limitation and explicitly call out the need for evaluation on larger datasets.
  - What evidence would resolve it: Testing Fieldy on large-scale tabular time-series datasets (e.g., click-through rate data with millions of samples) and comparing its performance and computational efficiency against hierarchical and single-stage transformer baselines.

- Question: How does Fieldy perform when applied to multivariate time-series data beyond tabular formats?
  - Basis in paper: [explicit] The authors mention in the discussion that "the wider domain of multivariate time-series might also benefit from field-based hierarchical architectures, combining length and channel signals in a first stage."
  - Why unresolved: The paper only evaluates Fieldy on tabular time-series data where both rows and columns have explicit semantics. The authors hypothesize it could work for general multivariate time-series but don't test this.
  - What evidence would resolve it: Implementing Fieldy for standard multivariate time-series (where dimensions represent different measured variables rather than table columns) and comparing its performance against specialized time-series architectures on benchmarks like UCR/UEA datasets.

- Question: How sensitive is Fieldy's performance to the choice of numerical feature embedding techniques?
  - Basis in paper: [explicit] The authors note that their work "does not include sophisticated tabular data embeddings techniques such as numerical features embeddings from [6, 11]" and suggest this as a future direction.
  - Why unresolved: Fieldy uses simple 50-quantile discretization for numerical features, but the authors acknowledge that more sophisticated embedding techniques could improve performance and could be combined with their architecture.
  - What evidence would resolve it: Evaluating Fieldy with different numerical feature embedding methods (e.g., continuous embeddings from [6], target-aware embeddings from [14]) on the same datasets to measure performance improvements and determine which embedding techniques work best with field-wise attention.

## Limitations
- Fieldy's computational complexity is higher than row-based or column-based models due to its field-wise attention mechanism
- The evaluation only uses two relatively small public datasets, limiting generalizability to larger-scale problems
- The paper does not include sophisticated tabular data embeddings techniques such as advanced numerical feature embeddings

## Confidence

- **High confidence**: The general architecture design of Fieldy combining row-wise and column-wise attention is clearly specified and reproducible. The performance improvements over baselines are well-documented with appropriate metrics and statistical significance.
- **Medium confidence**: The mechanism claims about why fine-grained attention works (cross-field relationships across rows/columns) are logically sound but lack direct empirical validation through ablation studies.
- **Low confidence**: The specific implementation details required for exact reproduction, particularly feature tokenization and embedding strategies, are not fully specified in the paper.

## Next Checks

1. **Ablation study on attention mechanisms**: Test whether combining row-wise and column-wise attention provides benefits over using only one mechanism by comparing Fieldy variants with single attention types.
2. **Positional embedding sensitivity analysis**: Evaluate Fieldy's performance with different positional encoding strategies (removing them entirely, using only row positions, only column indices) to quantify their importance.
3. **Dataset size sensitivity test**: Train Fieldy on subsets of the training data (e.g., 25%, 50%, 75%) to assess how performance scales with dataset size and identify potential overfitting issues.