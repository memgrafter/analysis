---
ver: rpa2
title: Preserving Silent Features for Domain Generalization
arxiv_id: '2401.03170'
source_url: https://arxiv.org/abs/2401.03170
tags:
- features
- domain
- silent
- learning
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of domain generalization (DG),
  where the goal is to improve a model's ability to generalize to unseen test domains
  using only training data from known source domains. The authors identify that self-supervised
  contrastive pre-trained models, despite their success in improving robustness on
  downstream tasks, do not outperform supervised pre-trained models in the DG setting.
---

# Preserving Silent Features for Domain Generalization

## Quick Facts
- arXiv ID: 2401.03170
- Source URL: https://arxiv.org/abs/2401.03170
- Authors: Chujie Zhao; Tianren Zhang; Feng Chen
- Reference count: 40
- Primary result: STEP achieves state-of-the-art performance on standard DG benchmarks by preserving silent features through LP-FT and SWAD

## Executive Summary
This paper addresses the problem of domain generalization (DG), where the goal is to improve a model's ability to generalize to unseen test domains using only training data from known source domains. The authors identify that self-supervised contrastive pre-trained models, despite their success in improving robustness on downstream tasks, do not outperform supervised pre-trained models in the DG setting. They argue that this is due to the suppression of "silent features" - richer intra-class discriminative features extracted by self-supervised contrastive learning - during supervised fine-tuning. The authors theoretically prove that preserving silent features can achieve lower expected test domain risk under certain conditions. To address this, they propose a method called STEP (Silent Feature Preservation), which consists of two main components: (1) linear probing followed by full fine-tuning (LP-FT) to prevent the suppression of initial features during downstream fine-tuning, and (2) Stochastic Weight Averaging Densely (SWAD) to find flat minima and improve convergence stability. Experimental results on standard DG benchmarks show that STEP achieves state-of-the-art performance, demonstrating the effectiveness of preserving silent features for improving DG performance.

## Method Summary
STEP (Silent Feature Preservation) is a two-stage learning strategy that preserves "silent features" extracted by self-supervised contrastive learning to improve domain generalization performance. The method uses linear probing followed by full fine-tuning (LP-FT) to prevent suppression of these features during supervised fine-tuning, combined with Stochastic Weight Averaging Densely (SWAD) to improve convergence stability and find flatter minima. The approach is evaluated on five standard DG benchmarks (VLCS, PACS, Office-Home, TerraIncognita, and DomainNet) using ResNet-50 backbones pre-trained on ImageNet via SwAV self-supervised contrastive learning.

## Key Results
- STEP achieves state-of-the-art performance on standard DG benchmarks, demonstrating the effectiveness of preserving silent features
- Linear probing followed by full fine-tuning (LP-FT) significantly improves DG performance compared to direct fine-tuning
- SWAD improves convergence stability and generalization in the presence of silent features
- STEP works better with larger distribution shifts and higher diversity between source and target domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised contrastive learning extracts richer intra-class discriminative features ("silent features") that are suppressed during supervised fine-tuning, leading to poor DG performance.
- Mechanism: The contrastive loss encourages the model to pull together positive pairs (augmented views of the same instance) and push apart negative pairs (different instances), which captures subtle within-class variations. Supervised fine-tuning with cross-entropy loss focuses on maximizing inter-class separability, causing the model to discard these subtle features that could be more generalizable across domains.
- Core assumption: Silent features contain information that is more discriminative on unseen test domains than on training domains, especially when there are significant distribution shifts.
- Evidence anchors:
  - [abstract] "self-supervised models do not exhibit better generalization performance than supervised models pre-trained on the same dataset in the DG setting... this is owing to the fact that the richer intra-class discriminative features extracted by self-supervised contrastive learning... are suppressed during supervised fine-tuning."
  - [section 1] "we find that the direct combination of self-supervised contrastive learning models pre-trained on ImageNet and ERM does not yield better performance, and sometimes even performs worse than supervised learning pre-trained models. We conjecture that this is caused by the downstream ERM methodâ€™s supervised fine-tuning phase, which suppresses the intra-class features extracted by the self-supervised contrastive learning algorithm."
- Break condition: If silent features are not actually more discriminative on test domains (e.g., when distribution shift is small), or if the test domain distribution is adversarial relative to the silent features.

### Mechanism 2
- Claim: Preserving silent features through linear probing followed by full fine-tuning (LP-FT) improves OOD generalization by preventing their suppression during supervised fine-tuning.
- Mechanism: Linear probing freezes the backbone and only trains a linear classifier, allowing the model to adapt to the downstream dataset without modifying the pre-trained features. This preserves the silent features extracted by self-supervised learning. Full fine-tuning then fine-tunes all parameters to optimize for the specific task, but starting from a representation that retains the silent features.
- Core assumption: The initial features from self-supervised pre-training contain valuable silent features that can be preserved through a two-stage training process.
- Evidence anchors:
  - [section 5.2.1] "we introduce a two-stage learning strategy of linear probing followed by overall fine-tuning... This stage will directly inherit the pre-trained features, allowing the model to initially adapt to the downstream datasets and optimize the loss to compress the subsequent parameter search space."
  - [section 5.1] "we empirically find that directly employing the model pretrained on ImageNet via self-supervised contrastive learning in the DG setting does not obtain higher generalization performance than the conventional supervised learning pretrained model... We propose a simple yet effective method called Silent Feature Preservation (STEP) to alleviate the suppression of silent features caused by supervised fine-tuning."
- Break condition: If the linear probing stage overfits to the training domains or if the full fine-tuning stage still suppresses the silent features despite the initial preservation.

### Mechanism 3
- Claim: Stochastic Weight Averaging Densely (SWAD) improves convergence stability and generalization by finding flatter minima in the presence of silent features.
- Mechanism: SWAD averages model weights over training iterations, encouraging the model to converge to flatter minima which are more robust to perturbations and have better generalization. The presence of silent features can make the loss landscape more complex, and SWAD helps navigate this complexity.
- Core assumption: Flat minima lead to better generalization in the DG setting, and the presence of silent features makes the optimization landscape more challenging.
- Evidence anchors:
  - [section 5.2.2] "we also observed significant fluctuations in the test error after introducing the self-supervised contrastive learning pre-trained model during the actual training process... incorporating more diverse silent features prevents the model from readily converging to a solution with better generalization performance."
  - [section 5.2.2] "SWAD is an ensemble learning method... to help the model converge to flatter minima. As flat minima increase the robustness of the model... and have a smaller generalization gap under the DG scenario... we adopt the SWAD technique in the second stage (fine-tuning) to improve the convergence stability and generalization performance of the pre-trained backbones with self-supervised contrastive learning."
- Break condition: If SWAD does not actually find flatter minima in this specific setting, or if the computational cost outweighs the benefits.

## Foundational Learning

- Concept: Self-supervised contrastive learning
  - Why needed here: Understanding how contrastive learning extracts features is crucial to grasping why silent features are formed and how they differ from features learned through supervised pre-training.
  - Quick check question: What is the key difference between the loss function used in self-supervised contrastive learning and the loss function used in supervised learning?

- Concept: Domain generalization (DG)
  - Why needed here: DG is the target problem, and understanding its unique challenges (e.g., no access to test domains during training) is essential to appreciate the proposed solution.
  - Quick check question: How does domain generalization differ from domain adaptation, and why is it considered more challenging?

- Concept: Feature suppression
  - Why needed here: The core problem being addressed is the suppression of silent features during supervised fine-tuning, so understanding what feature suppression means in this context is crucial.
  - Quick check question: What is meant by "feature suppression" in the context of this paper, and how does it relate to the difference between dominant and silent features?

## Architecture Onboarding

- Component map: Pre-trained backbone (ResNet-50, SwAV) -> Linear probing stage (frozen backbone, trained linear head) -> Fine-tuning stage (full model, SWAD) -> Downstream DG datasets (VLCS, PACS, Office-Home, Terra Incognita, DomainNet)

- Critical path:
  1. Load pre-trained SwAV model
  2. Linear probing on downstream DG dataset
  3. Fine-tuning with SWAD on downstream DG dataset
  4. Evaluate on unseen test domains

- Design tradeoffs:
  - Linear probing vs. direct fine-tuning: Linear probing preserves silent features but may require more training time and careful hyperparameter tuning.
  - SWAD vs. standard fine-tuning: SWAD improves stability and generalization but adds computational overhead.
  - Choice of pre-trained model: SwAV was chosen for its strong performance, but other self-supervised contrastive learning methods might yield different results.

- Failure signatures:
  - Poor performance on DG benchmarks: Could indicate that silent features are not actually beneficial for the specific datasets or that the LP-FT and SWAD components are not properly implemented.
  - Large gap between training and test performance: Could indicate overfitting during linear probing or instability during fine-tuning.

- First 3 experiments:
  1. Implement linear probing only on a small DG dataset and compare to direct fine-tuning.
  2. Implement SWAD only on a small DG dataset and compare to standard fine-tuning.
  3. Implement the full STEP pipeline (LP-FT + SWAD) on a small DG dataset and compare to ERM with supervised pre-training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we define "silent features" more precisely and measure their relative importance in self-supervised vs. supervised models?
- Basis in paper: [inferred] The paper discusses "silent features" as intra-class discriminative features extracted by self-supervised contrastive learning, but does not provide a formal definition or quantitative measure.
- Why unresolved: Without a precise definition and measurement, it's difficult to validate the theory or develop more targeted methods for preserving these features.
- What evidence would resolve it: A method to quantify the relative weight of silent features in different models, possibly using techniques like mutual information estimation or feature importance analysis.

### Open Question 2
- Question: Can STEP be extended to preserve only the most beneficial subset of silent features for domain generalization, rather than all of them?
- Basis in paper: [explicit] The paper mentions that STEP might cause feature redundancy issues since it's unclear which part of the silent features generalizes well across unseen domains.
- Why unresolved: The current method preserves all silent features, which may include irrelevant or even harmful features for certain target domains.
- What evidence would resolve it: An adaptive method that identifies and preserves only the most beneficial silent features for a given target domain, potentially through domain adaptation techniques or meta-learning.

### Open Question 3
- Question: How does the performance of STEP vary with different self-supervised contrastive learning algorithms and their hyperparameters?
- Basis in paper: [explicit] The paper briefly mentions that different self-supervised contrastive learning algorithms provide various weights of silent features, but does not extensively explore this.
- Why unresolved: The choice of pre-training algorithm and its hyperparameters can significantly impact the quality and quantity of silent features, which may affect STEP's performance.
- What evidence would resolve it: A comprehensive study comparing STEP's performance across multiple self-supervised contrastive learning algorithms (e.g., MoCo, SimCLR) and their hyperparameters on various domain generalization benchmarks.

## Limitations
- The theoretical analysis relies on a simplified model where the relationship between silent features and domain generalization is assumed but not empirically validated across diverse dataset pairs.
- The ablation studies focus on specific components (LP-FT and SWAD) but do not explore alternative methods for preserving silent features or different pre-training approaches.
- The evaluation is limited to ResNet-50 architectures, leaving uncertainty about scalability to larger models.

## Confidence

- Mechanism 1 (Silent feature suppression): High confidence - well-supported by both theoretical analysis and experimental results showing LP-FT's effectiveness
- Mechanism 2 (LP-FT preservation): High confidence - strong ablation results and consistent performance improvements across benchmarks
- Mechanism 3 (SWAD stability): Medium confidence - while SWAD improves results, the specific benefits for silent feature preservation versus general optimization benefits are not clearly differentiated

## Next Checks

1. Conduct feature importance analysis to empirically verify that preserved silent features are indeed more discriminative on test domains versus training domains
2. Test STEP with different self-supervised pre-training methods (MoCo, SimCLR) to determine if the approach generalizes beyond SwAV
3. Perform an ablation study comparing STEP against alternative silent feature preservation methods like feature distillation or attention-based approaches