---
ver: rpa2
title: 'Toward In-Context Teaching: Adapting Examples to Students'' Misconceptions'
arxiv_id: '2405.04495'
source_url: https://arxiv.org/abs/2405.04495
tags:
- student
- teaching
- students
- examples
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ADAPT, a framework for evaluating adaptive
  teaching methods that tailor examples to students'' misconceptions. The framework
  includes simulated and human student evaluations across three domains: fraction
  arithmetic, English verb conjugation, and function learning.'
---

# Toward In-Context Teaching: Adapting Examples to Students' Misconceptions

## Quick Facts
- arXiv ID: 2405.04495
- Source URL: https://arxiv.org/abs/2405.04495
- Authors: Alexis Ross; Jacob Andreas
- Reference count: 40
- Primary result: ATOM outperforms LLM-based (GPT-4) and standard Bayesian teaching models in simulated experiments, while both ATOM and GPT-4 outperform random selection in human experiments

## Executive Summary
This paper introduces ADAPT, a framework for evaluating adaptive teaching methods that tailor examples to students' misconceptions. The framework includes simulated and human student evaluations across three domains: fraction arithmetic, English verb conjugation, and function learning. The authors propose ATOM, a probabilistic model that jointly infers student priors and selects informative examples. In simulated experiments, ATOM outperforms LLM-based (GPT-4) and standard Bayesian teaching models. In human experiments, both ATOM and GPT-4 outperform non-adaptive random example selection, demonstrating the potential of adaptive teaching methods.

## Method Summary
The authors propose ADAPT, a framework for adaptive teaching that selects examples to correct student misconceptions. They introduce ATOM, a probabilistic model using online maximum a posteriori (MAP) estimation to infer student priors while selecting informative examples. The framework evaluates teaching methods across three domains using both simulated students (for controlled experiments) and human participants (for ecological validity). Teaching effectiveness is measured by how quickly students learn target concepts, with area under learning curves serving as the primary metric.

## Key Results
- ATOM outperforms both GPT-4 and non-adaptive Bayesian teaching models in simulated experiments across all three tasks
- In human experiments, both ATOM and GPT-4 significantly improve learning compared to random example selection (p < 0.05)
- ATOM achieves higher accuracy in inferring student misconceptions (84.48%) compared to GPT-4 (59.72%) in simulated function learning
- GPT-4 shows pedagogical reasoning by selecting critical examples early when it knows the student type

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ATOM's online MAP estimation of student priors enables more efficient teaching than fixed-prior methods
- Mechanism: ATOM iteratively updates its belief about the student's prior parameters after each prediction, allowing it to select examples that are increasingly targeted to the student's actual misconceptions
- Core assumption: Students' priors can be inferred from their sequence of predictions on examples
- Evidence anchors:
  - [abstract]: "ATOM, a new probabilistic model for adaptive teaching that jointly infers students' past beliefs and optimizes for the correctness of future beliefs"
  - [section 4.1]: "ATOM assumes that students are Bayesian reasoners and chooses examples to maximize the posterior probability that the student assigns to the target concept h*. Because the student's prior is unknown, however, this process involves two steps: (1) Maximum a posteriori estimation of student priors"
  - [corpus]: Weak - the corpus mentions "LLM-based Cognitive Models of Students with Misconceptions" but doesn't provide direct evidence for ATOM's specific MAP approach
- Break condition: If student predictions are too noisy to provide reliable signals about priors, or if the concept space is too large for tractable inference

### Mechanism 2
- Claim: GPT-4 shows some pedagogical reasoning by selecting critical examples early when it knows the student type
- Mechanism: When GPT-4 has access to the true student type, it concentrates on examples that distinguish the target concept from the student's misconception early in teaching
- Core assumption: GPT-4 can reason about what examples would be most informative for students with specific misconceptions
- Evidence anchors:
  - [section 5.4]: "GPT-4-K NOWN also shows a concentration of critical examples early, though they are more spread out for some concepts... GPT-4 thus exhibits some pedagogical reasoning, focusing on examples that will target the f-learner's misconceptions when it knows that the student type is an f-learner"
  - [section 5.3]: "When GPT-4 does not have access to the true student type, we observe that critical examples are still more concentrated at the start than for RANDOM, suggesting some degree of adaptivity"
  - [corpus]: Missing - no direct corpus evidence about GPT-4's pedagogical reasoning capabilities
- Break condition: If GPT-4's pedagogical reasoning is inconsistent across different concepts or student types, or if it fails to generalize beyond the specific prompts used

### Mechanism 3
- Claim: Human students learn more efficiently from both ATOM and GPT-4 compared to random example selection
- Mechanism: Both adaptive methods select examples that are more informative for correcting specific misconceptions, leading to faster learning
- Core assumption: The examples selected by adaptive methods are indeed more informative for students with misconceptions than random examples
- Evidence anchors:
  - [abstract]: "In human experiments, both ATOM and LLMs outperform non-adaptive random example selection"
  - [section 6.2]: "As shown in Figure 6, we find that both GPT-4 and ATOM improve significantly over the RANDOM baseline (p < 0.05 using a paired t-test)"
  - [corpus]: Weak - the corpus mentions "A Benchmark for Math Misconceptions" but doesn't provide direct evidence for the effectiveness of adaptive teaching methods
- Break condition: If the improvement over random selection is not consistent across different student types or learning domains

## Foundational Learning

- Concept: Bayesian inference and posterior updating
  - Why needed here: The entire framework relies on Bayesian models of student learning and teaching, where teachers maintain beliefs about student concepts and update them based on observations
  - Quick check question: Given a student who has seen examples (1,2) and (3,6) and predicts 4 for input 2, what would be the posterior probability of the student believing in the "addition" concept versus the "multiplication" concept?

- Concept: Concept spaces and program representations
  - Why needed here: Students' knowledge is represented as distributions over possible concepts/programs, and teaching effectiveness depends on understanding this representation
  - Quick check question: For the fraction task, if the concept space consists of programs for fraction arithmetic, what would be the difference between a "multiplication learner" and an "addition learner" in terms of their prior distributions over these programs?

- Concept: Online learning and active experimentation
  - Why needed here: Teaching is framed as an interactive process where the teacher must select examples one at a time while inferring student knowledge, requiring understanding of sequential decision-making
  - Quick check question: If a teacher is trying to distinguish between two student types and has selected three examples so far, what factors should they consider when choosing the fourth example to maximize information gain about the student's type?

## Architecture Onboarding

- Component map:
  - Student models: Bayesian models maintaining distributions over concept spaces (fractions, verbs, functions)
  - Teacher models: ATOM (online MAP estimation + example selection), GPT-4 (prompt-based), Random (baseline), Non-adaptive (OT with fixed prior guess)
  - Evaluation framework: Simulated students (offline evaluation), Human experiments (function task only)
  - Data generation: Ground truth concepts, student types with specific misconceptions, teaching examples

- Critical path:
  1. Define target concept and student types with misconceptions
  2. Generate ground truth data for evaluation
  3. Implement student models with appropriate concept spaces
  4. Implement teacher models (ATOM, GPT-4, baselines)
  5. Run simulated experiments to compare teaching methods
  6. If adding new task, extend concept space and student type definitions
  7. For human experiments, implement chat interface and evaluation metrics

- Design tradeoffs:
  - Simulated vs human students: Simulated students allow controlled experiments but may not capture all aspects of human learning; human experiments are more ecologically valid but harder to control
  - Concept space size: Larger spaces allow more nuanced student types but increase computational complexity
  - Teacher model complexity: More sophisticated models (like ATOM) may perform better but are harder to implement and debug

- Failure signatures:
  - Student models not learning: Check concept space definition, prior distributions, and noise parameters
  - Teacher models selecting poor examples: Verify student model predictions, example selection criteria, and whether true student type is being correctly inferred
  - Human experiments showing no improvement: Check task instructions, interface usability, and whether the teaching methods are actually being executed as intended

- First 3 experiments:
  1. Verify student model learning: Run a single teaching interaction with a known student type and plot the student's belief in the target concept over time
  2. Compare teaching methods on simulated students: Run all teacher models on the same student types and compare area under learning curves
  3. Test GPT-4 integration: Verify that GPT-4 can be prompted correctly to generate teaching examples and that the parsing of outputs works as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively can GPT-4 infer student misconceptions in real-time during teaching interactions?
- Basis in paper: [explicit] The paper analyzes GPT-4's inference accuracy about student types, finding 66.67% for fractions, 53.47% for functions, and 100% for verbs. It also notes GPT-4 exhibits recency bias, with accuracy decreasing from 64.2% to 53.47% over 40 steps in function learning.
- Why unresolved: While the paper provides accuracy rates, it doesn't explore whether GPT-4's inference errors are systematic (e.g., consistently confusing certain misconception types) or random. It also doesn't examine how these inference errors impact teaching effectiveness.
- What evidence would resolve it: Detailed error analysis showing patterns in GPT-4's incorrect inferences, and controlled experiments measuring teaching outcomes when GPT-4's inferences are correct versus incorrect.

### Open Question 2
- Question: Does combining structured Bayesian models like ATOM with LLMs improve teaching effectiveness compared to either approach alone?
- Basis in paper: [explicit] The authors attempt a preliminary combination of GPT-4 and ATOM by using ATOM's student type inferences to guide GPT-4's example selection, but find no improvement over either method individually.
- Why unresolved: The preliminary experiment may have used suboptimal integration methods. Different ways of combining the approaches (e.g., using LLM-generated examples to update ATOM's priors, or using ATOM's uncertainty estimates to guide LLM prompting) might yield better results.
- What evidence would resolve it: Systematic comparison of multiple integration strategies, including hybrid models that dynamically switch between LLM and structured approaches based on confidence metrics.

### Open Question 3
- Question: How do human students' actual learning trajectories compare to the simulated student models used in ADAPT?
- Basis in paper: [inferred] The paper validates ATOM against human students in function learning, finding that both ATOM and GPT-4 significantly outperform random selection. However, it doesn't directly compare human learning patterns to the simulated student models' predictions.
- Why unresolved: The paper shows that Bayesian models can effectively teach humans, but doesn't verify whether these models accurately capture human learning dynamics, including error patterns, learning plateaus, and the influence of prior knowledge.
- What evidence would resolve it: Detailed comparison of human students' prediction accuracy curves, error types, and response times against those of the simulated students across all three ADAPT tasks.

## Limitations
- Human experiments are limited to the function learning task only, leaving uncertainty about whether findings generalize to fractions and verb tasks
- GPT-4 prompting strategy is not fully specified, making it difficult to reproduce or extend the LLM-based teaching approach
- Concept space sizes are relatively small (5-8 programs), which may not capture the complexity of real student misconceptions

## Confidence

**High confidence**: The core finding that adaptive methods (ATOM and GPT-4) outperform random example selection in human experiments is well-supported by statistical tests (p < 0.05). The simulated experiments showing ATOM's superiority over other methods are also robust across multiple student types and tasks.

**Medium confidence**: The claim that ATOM's online MAP estimation specifically drives its performance, while intuitively sound, is not directly tested against variants of ATOM with different inference approaches. The mechanism by which GPT-4 selects examples (whether through genuine pedagogical reasoning or prompt engineering) remains unclear.

**Low confidence**: The generalizability of these results to more complex educational domains or to students with more nuanced or overlapping misconceptions is uncertain, given the limited scope of the concept spaces and the artificial nature of the misconceptions.

## Next Checks
1. **Replicate human experiment results**: Conduct a new human study with a larger sample size and include all three tasks (fractions, verbs, functions) to verify the consistency of adaptive teaching benefits across domains.

2. **Ablation study for ATOM**: Test ATOM variants where the MAP estimation is replaced with other inference methods (e.g., fixed prior, random prior sampling) to isolate the contribution of the online estimation mechanism to overall performance.

3. **GPT-4 prompting analysis**: Systematically vary the GPT-4 prompts and analyze how different prompt formulations affect the selection of critical examples, to better understand the pedagogical reasoning capabilities of the model.