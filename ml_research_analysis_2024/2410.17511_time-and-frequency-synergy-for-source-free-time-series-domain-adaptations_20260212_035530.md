---
ver: rpa2
title: Time and Frequency Synergy for Source-Free Time-Series Domain Adaptations
arxiv_id: '2410.17511'
source_url: https://arxiv.org/abs/2410.17511
tags:
- domain
- frequency
- time
- learning
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Time Frequency Domain Adaptation (TFDA), a
  method to address the problem of source-free time-series domain adaptation where
  labeled source data is unavailable due to privacy constraints. Unlike existing approaches
  that rely solely on time-domain features, TFDA utilizes both time and frequency
  components through a dual-branch network structure consisting of separate time and
  frequency encoders, each with its own classifier.
---

# Time and Frequency Synergy for Source-Free Time-Series Domain Adaptations

## Quick Facts
- **arXiv ID:** 2410.17511
- **Source URL:** https://arxiv.org/abs/2410.17511
- **Reference count:** 39
- **Primary result:** TFDA achieves F1 scores of 90.09%, 62.9%, and 92.77% on UCIHAR, SSC, and MFD datasets respectively

## Executive Summary
This paper addresses the challenge of source-free time-series domain adaptation, where labeled source data is unavailable due to privacy constraints. The proposed Time Frequency Domain Adaptation (TFDA) method leverages both time and frequency domain information through a dual-branch network architecture, outperforming existing approaches that rely solely on time-domain features. The method employs multiple complementary strategies including neighborhood-based pseudo-labeling, contrastive learning in both domains, time-frequency consistency, uncertainty reduction, and curriculum learning.

The experimental results demonstrate TFDA's superiority across three benchmark datasets with 15 cross-domain scenarios, showing significant improvements over state-of-the-art methods. The ablation study reveals that contrastive learning contributes the most to performance gains, with a 35% improvement when included. While the empirical results are strong, the theoretical understanding of why time-frequency synergy specifically benefits domain adaptation remains somewhat limited.

## Method Summary
TFDA introduces a dual-branch network structure with separate time and frequency encoders, each equipped with its own classifier. The method processes time-series data through both time-domain and frequency-domain branches, allowing the model to capture complementary features from both representations. Key components include neighborhood-based pseudo-labeling that aggregates predictions from similar samples, contrastive learning with negative pair exclusion strategy applied in both time and frequency domains, and self-distillation for time-frequency consistency. The approach also incorporates uncertainty reduction to handle domain shift and curriculum learning to prevent memorization of noisy pseudo-labels. During training, the model generates pseudo-labels for target domain data and progressively refines them while maintaining consistency between time and frequency representations.

## Key Results
- TFDA achieves average F1 scores of 90.09%, 62.9%, and 92.77% on UCIHAR, SSC, and MFD datasets respectively
- Contrastive learning shows the largest impact with 35% improvement when included in the ablation study
- TFDA outperforms state-of-the-art source-free domain adaptation methods across all 15 cross-domain scenarios tested
- The dual-branch architecture demonstrates clear advantages over single-branch time-domain only approaches

## Why This Works (Mechanism)
The effectiveness of TFDA stems from its comprehensive exploitation of time-series data characteristics. By processing data through both time and frequency domain branches, the model captures complementary temporal patterns and spectral features that may be missed when using only time-domain information. The contrastive learning component enhances feature discriminability in both domains while the negative pair exclusion strategy prevents misleading comparisons. The self-distillation mechanism ensures consistency between time and frequency representations, creating a regularization effect that improves generalization. Curriculum learning gradually increases the complexity of pseudo-label refinement, preventing the model from overfitting to early noisy predictions. The uncertainty reduction component specifically addresses domain shift by focusing on high-confidence samples during adaptation.

## Foundational Learning

**Time-Frequency Domain Analysis** - Why needed: Time-series data contains both temporal patterns and frequency components that can be complementary. Quick check: Verify that Fourier transform correctly captures frequency domain representation of time-series signals.

**Contrastive Learning** - Why needed: Enhances feature discriminability by learning to distinguish between similar and dissimilar samples. Quick check: Ensure positive pairs are sufficiently similar and negative pairs are properly excluded.

**Pseudo-Labeling** - Why needed: Enables training on unlabeled target data by generating synthetic labels. Quick check: Monitor pseudo-label quality and confidence scores throughout training.

**Curriculum Learning** - Why needed: Prevents overfitting to noisy early pseudo-labels by gradually increasing learning complexity. Quick check: Verify that training starts with high-confidence samples and progressively includes more challenging ones.

**Self-Distillation** - Why needed: Enforces consistency between different representations (time and frequency) to improve generalization. Quick check: Ensure temperature scaling in distillation loss appropriately balances teacher-student knowledge transfer.

## Architecture Onboarding

**Component Map:** Raw Time-Series -> Time Encoder -> Time Classifier + Time Contrastive Loss; Raw Time-Series -> FFT -> Frequency Encoder -> Frequency Classifier + Frequency Contrastive Loss; Time & Frequency Outputs -> Consistency Loss -> Final Predictions

**Critical Path:** Raw data flows through both time and frequency branches in parallel, with contrastive learning and self-distillation applied within each branch before combining outputs through consistency regularization.

**Design Tradeoffs:** The dual-branch architecture increases model capacity and computational cost but captures richer feature representations. The neighborhood-based pseudo-labeling provides more robust labels but requires additional computational overhead for similarity calculations.

**Failure Signatures:** Performance degradation when time and frequency representations are inconsistent, poor pseudo-label quality due to domain shift, or collapse of contrastive learning objectives indicating feature collapse.

**First Experiments:** 1) Verify individual time and frequency branch performance separately, 2) Test contrastive learning effectiveness with and without negative pair exclusion, 3) Validate time-frequency consistency by measuring divergence between branch outputs.

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided content. However, implicit questions include the theoretical foundations for why time-frequency synergy specifically benefits domain adaptation, the scalability of the approach to larger datasets, and the generalizability across different types of time-series data beyond the tested domains.

## Limitations

- Theoretical foundation for time-frequency synergy benefits in domain adaptation is not fully established
- Computational complexity of dual-branch architecture may limit scalability to larger datasets
- All experiments conducted on three specific datasets, limiting generalizability claims

## Confidence

- **Performance claims:** Medium - Strong empirical results but limited dataset diversity
- **Theoretical contributions:** Low - Limited theoretical analysis of time-frequency synergy mechanisms
- **Scalability:** Low - No detailed complexity analysis or runtime comparisons provided
- **Generalizability:** Medium - Results show promise but tested on specific dataset types

## Next Checks

1) Test TFDA on additional time-series datasets with different characteristics (e.g., longer sequences, different sampling rates) to verify robustness across diverse scenarios

2) Conduct ablation studies specifically isolating the contribution of time-frequency consistency versus individual domain contrastive learning to better understand their relative importance

3) Evaluate performance on streaming data scenarios where domain shift occurs gradually over time rather than as discrete shifts between datasets