---
ver: rpa2
title: All Roads Lead to Rome? Exploring Representational Similarities Between Latent
  Spaces of Generative Image Models
arxiv_id: '2407.13449'
source_url: https://arxiv.org/abs/2407.13449
tags:
- latent
- space
- image
- spaces
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates representational similarities between latent
  spaces of different generative image models, including VAEs, GANs, Normalizing Flows,
  and Diffusion Models. The authors propose a method that involves training linear
  maps between frozen latent spaces of these models to "stitch" arbitrary pairs of
  encoders and decoders, then measuring output-based and probe-based metrics on the
  resulting stitched models.
---

# All Roads Lead to Rome? Exploring Representational Similarities Between Latent Spaces of Generative Image Models

## Quick Facts
- arXiv ID: 2407.13449
- Source URL: https://arxiv.org/abs/2407.13449
- Reference count: 8
- Primary result: Linear maps between latent spaces of performant generative models preserve most visual information even when latent sizes differ

## Executive Summary
This paper investigates representational similarities between latent spaces of different generative image models, including VAEs, GANs, Normalizing Flows, and Diffusion Models. The authors propose a method that involves training linear maps between frozen latent spaces of these models to "stitch" arbitrary pairs of encoders and decoders, then measuring output-based and probe-based metrics on the resulting stitched models. The study finds that linear maps between latent spaces of performant models preserve most visual information, with gender-related attributes showing the highest similarity across different models' latent spaces. The research also demonstrates that latent space representations converge early in training, with probe accuracy plateauing after only 20% of training epochs.

## Method Summary
The authors investigate representational similarities by training linear maps between frozen latent spaces of different generative image models. They use the CelebA dataset and pre-trained models including VAE, VQVAE, Normalizing Flow (NF), and Diffusion Model (DM) architectures. For each pair of models, they train a linear regression model to map between their latent spaces using 9000 training images. They then evaluate the stitched models using reconstruction-based metrics (latent MSE, pixel-space RMSE, LPIPS, FID) and probe-based metrics (attribute detection accuracy, probe prediction consistency). The methodology allows them to quantify how well information is preserved when transforming between different models' latent representations.

## Key Results
- Linear maps between latent spaces of performant models preserve most visual information even when latent sizes differ
- On CelebA, gender-related attributes (particularly "Male" and "Heavy Makeup") are the most similarly represented across different models' latent spaces
- Latent space representations converge early in training, with probe accuracy plateauing after only 20% of training epochs on a Normalizing Flow model
- NF and VQVAE encoders can be stitched to other models' decoders to produce high-quality reconstructions, while VAE and DM encoders perform worse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear maps between latent spaces of performant generative models preserve most visual information.
- Mechanism: The linear mapping finds a projection that aligns the latent representations across models, effectively creating a shared representational space. Since the models are trained to capture similar visual concepts (faces in this case), their latent spaces contain overlapping information that can be linearly related.
- Core assumption: The latent spaces of performant models contain sufficient linear correlation to allow information preservation through simple linear transformations.
- Evidence anchors:
  - [abstract] "linear maps between latent spaces of performant models preserve most visual information even when latent sizes differ"
  - [section] "On CelebA we find that powerful models can be stitched via their latent spaces to reconstruct an image with little loss in quality"
  - [corpus] "The Universality Hypothesis in large language models (LLMs) claims that different models converge towards similar concept representations in their latent spaces"
- Break condition: If models learn fundamentally different representations or if the visual information is distributed in highly non-linear patterns that cannot be captured by linear maps.

### Mechanism 2
- Claim: Gender-related attributes are most similarly represented across different models' latent spaces.
- Mechanism: Gender-related attributes like "Male" and "Heavy Makeup" represent fundamental variations in facial structure and appearance that all models must learn to distinguish, leading to convergent representations across different architectures.
- Core assumption: Certain visual attributes contain more variance and are more universally important for image generation, leading to similar representations across models.
- Evidence anchors:
  - [abstract] "On the CelebA dataset, gender-related attributes (particularly 'Male' and 'Heavy Makeup') are the most similarly represented across different models' latent spaces"
  - [section] "Attributes correlated with gender are represented similarly by nearly every model"
  - [corpus] "The Platonic Representation Hypothesis claims that recent foundation models are converging to a shared representation space as a function of their downstream task performance"
- Break condition: If training datasets have different distributions or if models are trained on very different tasks that don't require similar gender representations.

### Mechanism 3
- Claim: Latent space representations converge early in training, with probe accuracy plateauing after only 20% of training epochs.
- Mechanism: Models quickly discover the most salient features and structures in the data that are necessary for basic reconstruction and discrimination, and subsequent training mainly refines these representations rather than fundamentally changing them.
- Core assumption: The "natural" or optimal representation for the data emerges early in training and remains relatively stable.
- Evidence anchors:
  - [abstract] "The study also shows that latent space representations converge early in training, with probe accuracy plateauing after only 20% of training epochs on a Normalizing Flow model"
  - [section] "Probe accuracy on most attributes seems to plateau after epoch 6" and "Attributes correlated with gender as well as Smiling... change minimally in their representations after epoch 1"
  - [corpus] "Structure-preserving contrastive learning for spatial time series" (suggesting early structural learning)
- Break condition: If training data is highly complex or if models require very deep architectures that need more epochs to discover meaningful representations.

## Foundational Learning

- Concept: Linear algebra and linear transformations
  - Why needed here: The entire methodology relies on training linear maps between latent spaces, requiring understanding of how linear transformations preserve or distort information
  - Quick check question: If you have two latent spaces of dimensions 512 and 768, what is the shape of the linear map matrix that would transform from the first to the second?

- Concept: Representation learning and latent spaces
  - Why needed here: Understanding how generative models encode visual information in compressed latent spaces is fundamental to interpreting the results
  - Quick check question: Why might a VAE's latent space be more compressed than a Normalizing Flow's latent space, and what implications does this have for information preservation?

- Concept: Probe-based metrics and supervised learning
  - Why needed here: The paper uses linear probes to detect specific attributes in latent spaces, requiring understanding of how supervised learning can reveal semantic structure
  - Quick check question: If a linear probe achieves 95% accuracy on an attribute in one model's latent space but only 60% in another's, what might this tell you about the latent space structure?

## Architecture Onboarding

- Component map: Data pipeline -> Model zoo -> Stitching engine -> Evaluation suite -> Visualization tools
- Critical path: Load pre-trained models → Freeze encoder/decoder weights → Train linear maps on training set → Evaluate on test set → Generate reconstructions → Train probes → Analyze attribute consistency
- Design tradeoffs: Using frozen models ensures the evaluation measures only the linear map's effectiveness, but prevents end-to-end optimization. The choice of linear maps keeps the method simple but may miss non-linear relationships.
- Failure signatures: Poor reconstruction quality indicates the linear map fails to preserve information. Low probe accuracy suggests the latent space lacks semantic structure or the attribute is not well-represented. High variance in attribute consistency across models suggests non-convergent representations.
- First 3 experiments:
  1. Train and evaluate a linear map between two VAE latent spaces of the same dimension to establish baseline performance
  2. Train a linear map between a VAE and Normalizing Flow (different dimensions) to test cross-architecture stitching
  3. Train probes on each model's latent space for a simple attribute like "Smiling" to verify semantic structure exists

## Open Questions the Paper Calls Out
1. How do latent space representations of generative models trained on multi-class datasets compare to those trained on single-class datasets like CelebA?
2. How do the latent space representations of non-binary attributes compare to those of binary attributes in terms of similarity across different generative models?
3. How does the choice of latent space representation (e.g., style space for GANs) affect the similarity of representations across different generative models?

## Limitations
- The study relies on linear mappings between latent spaces, which may not capture non-linear relationships in representation geometry
- The analysis is limited to face datasets (CelebA) and may not generalize to other domains
- The frozen-model approach prevents end-to-end optimization that might yield better stitching performance

## Confidence
- High confidence: Linear maps preserve visual information in performant models
- Medium confidence: Gender attributes show highest cross-model similarity
- Medium confidence: Early convergence of latent representations

## Next Checks
1. Test stitching performance on non-face datasets (e.g., LSUN bedrooms, CIFAR-10) to assess domain generality
2. Compare linear mapping approach against non-linear alternatives (e.g., small MLPs) for challenging model pairs
3. Extend training convergence analysis to VAEs and DMs to verify early plateauing is not architecture-specific