---
ver: rpa2
title: Dynamic Graph Information Bottleneck
arxiv_id: '2402.06716'
source_url: https://arxiv.org/abs/2402.06716
tags:
- dgib
- graph
- dynamic
- information
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dynamic Graph Information Bottleneck (DGIB),
  a framework for learning robust representations on dynamic graphs using the information
  bottleneck principle. DGIB addresses the challenge of learning minimal, sufficient,
  and consensual representations by decomposing the overall objective into DGIBM and
  DGIBC channels.
---

# Dynamic Graph Information Bottleneck

## Quick Facts
- arXiv ID: 2402.06716
- Source URL: https://arxiv.org/abs/2402.06716
- Authors: Haonan Yuan; Qingyun Sun; Xingcheng Fu; Cheng Ji; Jianxin Li
- Reference count: 40
- Primary result: DGIB outperforms state-of-the-art baselines in future link prediction under adversarial attacks

## Executive Summary
This paper introduces Dynamic Graph Information Bottleneck (DGIB), a framework for learning robust representations on dynamic graphs using the information bottleneck principle. DGIB addresses the challenge of learning minimal, sufficient, and consensual representations by decomposing the overall objective into DGIB_M and DGIB_C channels. It introduces a spatio-temporal local dependence assumption and variational bounds for tractable optimization. Experiments on real-world datasets show DGIB outperforms state-of-the-art baselines in future link prediction tasks under both non-targeted and targeted adversarial attacks, demonstrating superior robustness.

## Method Summary
DGIB is a framework for dynamic graph representation learning that decomposes the information bottleneck objective into two complementary channels: DGIB_MS for learning minimal and sufficient representations, and DGIB_C for ensuring predictive consensus among temporal representations. The method employs a spatio-temporal local dependence assumption to simplify complex dependencies, making variational bounds tractable for optimization. Using a GAT backbone with structure and feature refinement, DGIB processes dynamic graph snapshots to produce robust representations for future link prediction. The framework is trained end-to-end with weighted combination of both channels, demonstrating superior performance and robustness compared to baselines under various adversarial attack scenarios.

## Key Results
- DGIB-Bern and DGIB-Cat achieve consistently higher AUC scores than baselines on clean data
- Under adversarial attacks, DGIB shows superior robustness with significant performance gaps
- The framework demonstrates effectiveness across three real-world dynamic graph datasets (COLLAB, Yelp, ACT)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing the IB objective into DGIB_M and DGIB_C channels enables satisfaction of the Minimal-Sufficient-Consensual (MSC) Condition
- Mechanism: DGIB_M learns minimal and sufficient representations from input data, while DGIB_C ensures consensus among temporal representations through a consensual constraint. This joint optimization balances expressiveness and robustness.
- Core assumption: The MSC Condition captures the optimal representation requirements for dynamic graphs, and decomposing it into two complementary channels improves optimization feasibility.
- Evidence anchors:
  - [abstract]: "we decompose the overall IB objectives into DGIB_MS and DGIB_C, in which the DGIB_MS channel aims to learn the minimal and sufficient representations, with the DGIB_C channel guarantees the predictive consensus"
  - [section 4.2]: "We decompose the overall DGIB into DGIB_MS and DGIB_C channels, both sharing the same IB structure as Eq. (1)"
  - [corpus]: Weak - No direct corpus evidence found for this specific decomposition mechanism
- Break condition: If the consensual constraint is too weak, DGIB_C fails to enforce consensus; if too strong, it may over-constrain and harm expressiveness

### Mechanism 2
- Claim: Spatio-temporal local dependence assumption enables tractable IB optimization on dynamic graphs
- Mechanism: By assuming nodes depend only on their spatio-temporal k-hop neighbors, the complex dependencies in dynamic graphs are simplified, making variational bounds estimable and optimization feasible.
- Core assumption: Dynamic graphs exhibit local dependence patterns where distant nodes are conditionally independent given their local neighborhoods.
- Evidence anchors:
  - [section 4.2.1]: "we propose the Spatio-Temporal Local Dependence Assumption following [34]"
  - [section 4.2.1]: "Based on the Markov property D ⊥ ⊥ Z_T+1 | {{Â_t}_t ∈ I_A ∪ {Z_t}_t ∈ I_Z}, for any Q(Â_t) and Q(Z_t)"
  - [corpus]: Weak - No direct corpus evidence found for this specific local dependence assumption in IB context
- Break condition: If the local dependence assumption is violated (e.g., long-range dependencies are crucial), the approximation becomes invalid

### Mechanism 3
- Claim: Structure-involved variational bounds improve robustness compared to non-structure approaches
- Mechanism: By explicitly modeling distributions over graph structures (Â_t) and node features (Z_t) using variational bounds, the framework can compress and refine both structural and feature information, enhancing robustness against adversarial attacks.
- Core assumption: Directly involving graph structures in the IB optimization process provides more effective compression than two-step approaches that separate structure and feature processing.
- Evidence anchors:
  - [abstract]: "we decompose the overall IB objectives into DGIB_MS and DGIB_C, in which the DGIB_MS channel aims to learn the minimal and sufficient representations, with the DGIB_C channel guarantees the predictive consensus"
  - [section 4.2.1]: "Variational Bounds of DGIB_MS. We respectively introduce the lower bound of I(Y_T+1; Z_T+1) with respect to [31], and the upper bound of I(D; Z_T+1) inspired by [46]"
  - [corpus]: Weak - No direct corpus evidence found for this specific structure-involved bound approach
- Break condition: If the variational bounds are poorly estimated or the assumed distributions are incorrect, the optimization may not converge to optimal representations

## Foundational Learning

- Concept: Information Bottleneck (IB) principle
  - Why needed here: IB provides the theoretical foundation for learning minimal yet sufficient representations by balancing mutual information between representations, targets, and inputs
  - Quick check question: What are the two terms in the IB objective function and what do they represent?

- Concept: Dynamic Graph Neural Networks (DGNNs)
  - Why needed here: DGNNs provide the backbone architecture for processing dynamic graphs and learning representations that capture spatio-temporal patterns
  - Quick check question: What are the two main categories of DGNNs and how do they differ in handling temporal information?

- Concept: Variational Inference and bounds
  - Why needed here: Variational bounds make the intractable IB objectives computationally feasible by providing tractable approximations for mutual information terms
  - Quick check question: What is the relationship between the true mutual information and its variational lower/upper bounds?

## Architecture Onboarding

- Component map:
  - Dynamic graph snapshots {G_t}_t=1^T and next-step node features X_T+1 -> DGIB_MS channel and DGIB_C channel -> Robust representation Z_T+1 for future link prediction

- Critical path:
  1. Apply spatio-temporal sampling to obtain k-hop neighbors
  2. Process each graph snapshot through GAT backbone with structure and feature refinement
  3. Estimate variational bounds for DGIB_MS and DGIB_C
  4. Jointly optimize both channels with weighted combination
  5. Predict links at T+1 using learned representation

- Design tradeoffs:
  - Local vs global dependence: Local dependence assumption simplifies optimization but may miss long-range dependencies
  - Structure vs feature emphasis: Tradeoff between refining graph structures vs node features in the compression process
  - Computation vs robustness: More complex bounds and sampling increase robustness but also computational cost

- Failure signatures:
  - Poor performance on clean data: May indicate over-regularization or insufficient expressiveness
  - Sensitivity to hyperparameter choices: May indicate optimization instability or inappropriate bounds
  - Degradation under specific attack types: May indicate blind spots in the robustness mechanism

- First 3 experiments:
  1. Verify the MSC Condition satisfaction by comparing representations learned with/without DGIB_C
  2. Test the impact of spatio-temporal sampling range k on performance and robustness
  3. Compare structure-involved vs non-structure variational bounds on clean vs adversarial data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MSC Condition generalize to different types of dynamic graph prediction tasks beyond link prediction, such as node classification or graph-level prediction?
- Basis in paper: [inferred] The paper focuses on link prediction but discusses the theoretical foundations of the MSC Condition in a way that could be applied to other prediction tasks.
- Why unresolved: The experiments only validate DGIB on link prediction, leaving open whether the MSC Condition and DGIB framework perform similarly well on other tasks.
- What evidence would resolve it: Experiments applying DGIB to node classification and graph-level prediction tasks on dynamic graphs, comparing performance against state-of-the-art methods for those tasks.

### Open Question 2
- Question: What is the impact of different spatio-temporal neighborhood sizes (k) on the performance and robustness of DGIB across various dynamic graph datasets?
- Basis in paper: [explicit] The paper mentions using a spatio-temporal k-hop neighbor sampling mechanism but does not extensively analyze how different values of k affect results.
- Why unresolved: While the paper states the use of k-hop neighbors, it doesn't provide a sensitivity analysis or discussion on how varying k influences the model's effectiveness or robustness.
- What evidence would resolve it: Systematic experiments varying the k parameter across multiple datasets, showing how performance and robustness metrics change with different neighborhood sizes.

### Open Question 3
- Question: How does DGIB perform in real-world scenarios with streaming data, where graph snapshots arrive continuously and the model needs to update in an online fashion?
- Basis in paper: [inferred] The paper evaluates DGIB on static splits of dynamic graphs but doesn't address the challenge of online learning with streaming graph data.
- Why unresolved: The experimental setup uses pre-defined train/validation/test splits, which doesn't reflect the continuous nature of real-world dynamic graphs where new data arrives over time.
- What evidence would resolve it: Experiments implementing an online version of DGIB that updates its model as new graph snapshots arrive, comparing performance against online learning baselines on streaming graph datasets.

## Limitations

- The framework relies on a strong spatio-temporal local dependence assumption that may not hold for all dynamic graph structures
- Variational bounds are approximate and their quality is not thoroughly evaluated
- Results are shown only on three real-world datasets with specific adversarial attack scenarios

## Confidence

- Core performance claims: Medium
- Theoretical decomposition: Medium
- Local dependence assumption: Low
- Variational bounds quality: Low

## Next Checks

1. Perform ablation studies removing the DGIB_C channel to empirically verify that the consensus constraint is essential for achieving MSC Condition satisfaction and robustness gains
2. Test the framework on dynamic graphs with known long-range dependencies to assess whether the local dependence assumption breaks down and impacts performance
3. Evaluate the quality of the variational bounds by comparing the estimated mutual information values with ground-truth or alternative estimation methods on synthetic data