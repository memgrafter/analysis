---
ver: rpa2
title: Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt
  it Like One
arxiv_id: '2402.12150'
source_url: https://arxiv.org/abs/2402.12150
tags:
- women
- gender
- your
- more
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FAIRTHINKING improves LLM fairness by automatically generating
  diverse roles and using multi-agent debates to incorporate minority viewpoints,
  achieving 11.14 reasons per answer (vs 2.73 baseline) and reducing jury rejection
  rate from 71.33% to 34.00% on GPT-3.5-Turbo. The method addresses LLMs' tendency
  to express majority perspectives by prompting with rich role descriptions and structured
  debates, leading to fairer conclusions that consider diverse viewpoints.
---

# Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One

## Quick Facts
- arXiv ID: 2402.12150
- Source URL: https://arxiv.org/abs/2402.12150
- Reference count: 40
- FAIRTHINKING achieves 11.14 reasons per answer (vs 2.73 baseline) and reduces jury rejection rate from 71.33% to 34.00% on GPT-3.5-Turbo

## Executive Summary
FAIRTHINKING addresses LLMs' tendency to express majority perspectives by automatically generating diverse roles and using multi-agent debates to incorporate minority viewpoints. The method prompts LLMs with detailed role descriptions and orchestrates structured debates between multiple agents, with jury evaluation ensuring fairness. Experiments show FAIRTHINKING significantly improves fairness metrics across GPT-3.5, GPT-4, Llama2, and Mistral, demonstrating that LLMs can be guided to consider diverse perspectives through thoughtful prompting.

## Method Summary
FAIRTHINKING is a prompting framework that enhances LLM fairness through three phases: Automated Roles Generation, Debate, and Fairness Evaluation. The system generates detailed role descriptions covering identity, personality, admired celebrity, concept, slogan, growth experience, and social status for given topics. Multiple role-playing agents then engage in structured debate with alternating turns, while a clerk agent summarizes perspectives to form balanced conclusions. Finally, LLM-generated jurors with various backgrounds evaluate the fairness of conclusions, accepting them only if more than five-sixths vote in favor.

## Key Results
- Jury Rejection Rate (JRR) reduced from 71.33% to 34.00% on GPT-3.5-Turbo
- Number of Reasons (NR) increased from 2.73 to 11.14 across all models
- Biased Answer Rate (BR) decreased from 14.80% to 5.50% on GPT-3.5-Turbo
- FAIRTHINKING maintains consistent improvements across GPT-3.5, GPT-4, Llama2, and Mistral

## Why This Works (Mechanism)

### Mechanism 1
Role prompting enables LLMs to express diverse perspectives by assigning them specific identities and viewpoints. By prompting LLMs with detailed role descriptions, the model shifts from expressing majority perspectives to adopting the assigned role's viewpoint. Core assumption: LLMs have a default human personality representing majority training data, which can be overridden by specific role prompts.

### Mechanism 2
Multi-agent debate structures enable deeper perspective integration and fairer conclusions. Multiple role-playing agents engage in structured debate with alternating turns, while a clerk agent impartially summarizes perspectives to form balanced conclusions. Core assumption: Structured communication between multiple agents can surface and reconcile diverse viewpoints more effectively than single-agent generation.

### Mechanism 3
Jury evaluation provides objective assessment of answer fairness by measuring viewpoint diversity. Multiple juror agents with different backgrounds evaluate debate conclusions, with acceptance determined by majority vote threshold. Core assumption: LLM-generated jurors can reliably assess fairness in a way that correlates with human judgment.

## Foundational Learning

- Concept: Chain of Thought reasoning
  - Why needed here: Enables step-by-step construction of complex role descriptions and debate progression
  - Quick check question: How does chain-of-thought prompting help in generating detailed role descriptions compared to single-shot prompts?

- Concept: Multi-agent communication protocols
  - Why needed here: Structured debate requires agents to take turns and build on each other's arguments
  - Quick check question: What happens if agents don't properly reference previous arguments in the debate structure?

- Concept: Fairness metrics and evaluation
  - Why needed here: Need to measure whether generated answers actually improve fairness across different perspectives
  - Quick check question: How do you distinguish between surface-level diversity and genuine consideration of minority viewpoints?

## Architecture Onboarding

- Component map: Role Generator -> Debater Pool -> Debate Manager -> Clerk Agent -> Juror Pool -> Fairness Evaluator
- Critical path: 1. Generate relevant roles for given topic 2. Assign roles to debaters and jurors 3. Conduct structured debate with clerk facilitation 4. Jurors evaluate final conclusion 5. Return accepted/rejected verdict with reasoning
- Design tradeoffs: Number of debate rounds vs. computational cost; Role detail depth vs. prompt length limits; Juror diversity vs. consensus difficulty; Debate structure rigidity vs. natural conversation flow
- Failure signatures: Debaters repeating same points without building on each other; Jurors consistently rejecting all conclusions (overly strict); Role descriptions failing to trigger expected perspectives; Debate conclusions dominated by majority viewpoint despite role diversity
- First 3 experiments: 1. Test role prompting alone without debate - measure if single roles can express minority perspectives 2. Test debate with simplified roles (identity only) - measure impact of role detail on perspective diversity 3. Test jury evaluation with known biased vs. fair answers - measure correlation with human judgment

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of FAIRTHINKING vary when using different LLMs as the backbone model, particularly comparing GPT-3.5, GPT-4, Llama2, and Mistral? While the paper shows that FAIRTHINKING improves fairness across all four models, it doesn't analyze which model performs best or how their inherent capabilities affect the outcome.

### Open Question 2
What is the optimal number of debate rounds (M) and the ratio of debaters to jurors (Nd:Nj) for achieving the best fairness outcomes in FAIRTHINKING? The paper uses default settings of 3 debate rounds, 4 debaters, and 6 jurors, but does not explore how varying these parameters affects fairness.

### Open Question 3
How does the quality and fairness of FAIRTHINKING's conclusions compare to those generated by human experts on the same fairness-related topics? While the paper shows that LLM jurors align with human evaluation in a limited study, it does not directly compare the quality and fairness of FAIRTHINKING's conclusions to those produced by human experts.

## Limitations
- Weak empirical foundation for role-based personality switching in LLMs
- Jury evaluation system reliability depends on untested LLM-generated juror assessment
- Implementation details of communication strategy and acceptance threshold are not fully specified

## Confidence

**Medium Confidence**: Core claim that FAIRTHINKING improves fairness metrics is supported by experimental results
**Low Confidence**: Assumption that LLMs have default majority personality that can be overridden by role prompts
**Medium Confidence**: Multi-agent debate structure showing improved perspective integration is plausible but lacks comparative studies

## Next Checks

1. **Role Prompt Efficacy Test**: Conduct controlled experiments comparing single-role prompts with and without detailed descriptions (identity only vs. full template) to measure the impact of role detail depth on perspective diversity.

2. **Juror Reliability Validation**: Test the jury evaluation system using known biased and fair answers to measure correlation with human judgment and assess whether the five-sixths threshold appropriately balances strictness with practical utility.

3. **Communication Strategy Analysis**: Implement and test the "one-by-one" communication strategy with different turn structures to identify optimal debate flow that prevents repetitive arguments while maintaining perspective integration.