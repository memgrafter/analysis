---
ver: rpa2
title: 'Propulsion: Steering LLM with Tiny Fine-Tuning'
arxiv_id: '2409.10927'
source_url: https://arxiv.org/abs/2409.10927
tags:
- propulsion
- fine-tuning
- lora
- rank
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Propulsion, a parameter-efficient fine-tuning
  (PEFT) method for large language models (LLMs). The method applies element-wise
  multiplicative updates to the outputs of frozen model layers using a small set of
  trainable parameters.
---

# Propulsion: Steering LLM with Tiny Fine-Tuning

## Quick Facts
- arXiv ID: 2409.10927
- Source URL: https://arxiv.org/abs/2409.10927
- Authors: Md Kowsher; Nusrat Jahan Prottasha; Prakash Bhat
- Reference count: 40
- Parameter reduction: 355.3M to 0.086M parameters (12x fewer than AdaLoRA)

## Executive Summary
This paper introduces Propulsion, a parameter-efficient fine-tuning method that applies element-wise multiplicative updates to frozen LLM layers using a small set of trainable parameters. The method achieves competitive performance across multiple benchmarks while dramatically reducing trainable parameters. Theoretical analysis based on Neural Tangent Kernel theory suggests Propulsion approximates full fine-tuning with high probability. Experimental results demonstrate superior performance on GLUE tasks with 12x fewer parameters than AdaLoRA and faster convergence compared to existing PEFT methods.

## Method Summary
Propulsion modifies the output of frozen model layers through element-wise multiplicative updates using a small set of trainable parameters. Unlike traditional PEFT methods that introduce additional parameters, Propulsion operates by scaling the existing layer outputs directly. The method is theoretically grounded in Neural Tangent Kernel analysis, which shows that these multiplicative updates can approximate the behavior of full fine-tuning. During training, only the multiplicative parameters are updated while the base model remains frozen, enabling significant parameter efficiency. The approach is applied across multiple LLM architectures and evaluated on diverse downstream tasks including GLUE, question answering, text summarization, and reasoning.

## Key Results
- GLUE benchmark improvements of 2.48% to 8.92% over state-of-the-art PEFT methods
- Parameter reduction from 355.3 million to 0.086 million (12x fewer than AdaLoRA)
- Faster convergence and lower memory usage compared to baselines
- Competitive or superior performance across question answering, text summarization, and reasoning tasks

## Why This Works (Mechanism)
Propulsion works by applying element-wise multiplicative updates to the outputs of frozen model layers, effectively learning scaling factors that adjust the model's behavior for specific downstream tasks. This approach leverages the pre-trained model's representations while fine-tuning only a small set of parameters that control the magnitude of these representations. The multiplicative nature allows for both amplification and suppression of feature activations, providing fine-grained control over the model's output without modifying the underlying learned weights. By keeping the base model frozen, Propulsion maintains the general knowledge while adapting the specific task behavior through these scaling parameters.

## Foundational Learning

**Neural Tangent Kernel (NTK)**: Why needed - Provides theoretical foundation for understanding how small parameter updates affect model behavior during training. Quick check - Verify NTK-based convergence guarantees through empirical validation on different model architectures.

**Parameter-efficient Fine-tuning (PEFT)**: Why needed - Addresses computational and memory constraints of fine-tuning large language models. Quick check - Compare parameter counts and memory usage across different PEFT methods under identical conditions.

**Element-wise Multiplicative Updates**: Why needed - Enables fine-grained control over model outputs without introducing new parameters. Quick check - Analyze the distribution and magnitude of learned scaling factors across different layers and tasks.

## Architecture Onboarding

**Component Map**: Input -> Frozen Base Model -> Element-wise Multiplication -> Output
                    â†“
                Trainable Scaling Parameters

**Critical Path**: The critical path involves computing layer outputs, applying element-wise multiplication with trainable scaling parameters, and calculating gradients only for these parameters. The frozen base model ensures stable representations while the multiplicative updates provide task-specific adaptation.

**Design Tradeoffs**: Propulsion trades some potential fine-tuning flexibility for dramatic parameter efficiency. While full fine-tuning allows arbitrary weight modifications, Propulsion's multiplicative approach constrains updates to scaling operations. This limitation enables the 12x parameter reduction but may miss certain fine-tuning opportunities that require more complex weight modifications.

**Failure Signatures**: Performance degradation may occur when tasks require substantial modifications to the base model's internal representations rather than simple scaling adjustments. Tasks with highly specialized vocabulary or structural requirements may not benefit as much from multiplicative updates alone.

**First Experiments**:
1. Compare GLUE performance across different layer selection strategies for applying multiplicative updates
2. Measure convergence speed and final performance on a subset of GLUE tasks with varying training set sizes
3. Analyze the learned scaling parameter distributions across different model depths and tasks

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the theoretical foundations and practical limitations of Propulsion. The connection between NTK theory and empirical performance requires more rigorous validation through ablation studies and theoretical error bounds. The method's behavior under different initialization schemes and data distributions remains unexplored. Additionally, the computational efficiency claims would benefit from detailed wall-clock time comparisons and energy consumption metrics across different hardware configurations.

## Limitations

The theoretical analysis connecting NTK theory to Propulsion's performance lacks rigorous empirical validation through ablation studies or theoretical error bounds. The relative efficiency gains are not consistently maintained across all tested tasks, with modest improvements observed in reasoning benchmarks compared to GLUE tasks. The computational efficiency claims rely primarily on parameter count reduction without providing detailed wall-clock time comparisons or energy consumption metrics. The method's performance under domain shift scenarios with out-of-distribution data remains unverified.

## Confidence

**High confidence** in parameter efficiency claims (measurable reductions from 355.3M to 0.086M parameters are verifiable)
**Medium confidence** in performance claims on GLUE and standard benchmarks (results are reported but lack extensive cross-validation)
**Low confidence** in theoretical NTK-based analysis and reasoning task improvements (limited empirical support provided)

## Next Checks

1. Conduct ablation studies varying initialization schemes and layer selection strategies to verify robustness of NTK-based theoretical claims
2. Perform wall-clock time and memory usage benchmarking across different hardware configurations to validate computational efficiency claims
3. Test performance degradation under domain shift scenarios with out-of-distribution data to assess generalization boundaries