---
ver: rpa2
title: 'SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated Responses'
arxiv_id: '2404.04298'
source_url: https://arxiv.org/abs/2404.04298
tags:
- discrimination
- generation
- arxiv
- https
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether LLMs are better at discriminating
  among their own generated responses compared to generating initial responses. The
  authors propose a unified framework to evaluate both generative and discriminative
  capabilities of LLMs on the same footing.
---

# SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated Responses

## Quick Facts
- arXiv ID: 2404.04298
- Source URL: https://arxiv.org/abs/2404.04298
- Authors: Dongwei Jiang; Jingyu Zhang; Orion Weller; Nathaniel Weir; Benjamin Van Durme; Daniel Khashabi
- Reference count: 40
- Primary result: LLMs are not reliably better at discriminating among their own generated responses compared to generating initial responses, with 54 out of 56 experiments failing to reject this hypothesis.

## Executive Summary
This paper investigates whether large language models (LLMs) are better at discriminating among previously generated responses compared to generating initial responses. The authors propose a unified framework to evaluate both generative and discriminative capabilities of LLMs on the same footing. Through experiments across multiple tasks (GSM8K, TriviaQA, MT-Bench, TruthfulQA) and various models (Phi-3, LLaMA series, Mixtral, GPT series), they find that LLMs are not reliably better at discrimination than generation. The average difference between discrimination and generation performance (DG-DIFF) is generally small or negative, with only the strongest models showing slightly better discrimination, but the gap remains minimal.

## Method Summary
The paper introduces a two-phase methodology to compare LLM generation and discrimination capabilities. In the generation phase, an LLM samples n candidate answers using a higher temperature setting for diversity. One answer is randomly selected for generation accuracy measurement. In the discrimination phase, the same LLM evaluates all n candidates using a lower temperature setting for consistency, selecting the best answer. The performance difference (DG-DIFF = Sdisc - Sgen) is then statistically tested to determine if discrimination significantly exceeds generation performance using appropriate tests (McNemar's for binary metrics, Wilcoxon signed-rank for categorical metrics).

## Key Results
- 54 out of 56 experiments fail to reject the SELF-[IN]CORRECT hypothesis
- DG-DIFF is generally small or negative across both pre-trained models and aligned models
- Non-autoregressive models (Flan-T5-XXL, Flan-UL2) show significantly higher DG-DIFF compared to autoregressive models
- When incorrect answers are replaced with obviously wrong ones, discrimination performance improves significantly

## Why This Works (Mechanism)

### Mechanism 1
LLMs cannot reliably distinguish their own better outputs from worse ones when both are self-generated. The self-generated alternatives share similar generation patterns and confidence levels, making discrimination difficult even when one answer is objectively better. Core assumption: Self-generated responses are sufficiently similar in quality that discrimination requires distinguishing subtle differences.

### Mechanism 2
Pre-training objectives influence discrimination ability. Autoregressive models are optimized for generation, not discrimination, creating an inherent bias toward generation performance. Core assumption: The training objective shapes the model's learned capabilities in ways that favor generation over discrimination.

### Mechanism 3
Self-improvement appears successful when evaluated with different metrics than used during training. Metrics used during self-improvement evaluation may not capture the same quality dimensions as the model's internal discrimination process. Core assumption: There's a mismatch between how models internally evaluate options versus how external metrics assess quality.

## Foundational Learning

- Concept: Statistical hypothesis testing (one-sided tests, p-values, significance levels)
  - Why needed here: The paper uses hypothesis testing to determine whether discrimination performance exceeds generation performance
  - Quick check question: What does it mean when a p-value exceeds 0.05 in the context of rejecting the SELF-[IN]CORRECT hypothesis?

- Concept: Temperature scaling in language model sampling
  - Why needed here: Different temperature settings are used for generation (higher temperature for diversity) versus discrimination (lower temperature for consistency)
  - Quick check question: Why would you use temperature=0 during the discrimination phase but temperature=0.7 during generation?

- Concept: Few-shot prompting and in-context learning
  - Why needed here: The discrimination phase relies on few-shot examples to guide the model's selection behavior
  - Quick check question: How might increasing the number of in-context examples affect the discrimination performance according to the ablation study?

## Architecture Onboarding

- Component map: Input → Generation → Sampling → Discrimination → Selection → Evaluation → Statistical comparison
- Critical path: Input → Generation → Sampling → Discrimination → Selection → Evaluation → Statistical comparison
- Design tradeoffs:
  - Using same LLM for both phases ensures fair comparison but may introduce shared biases
  - Fixed number of candidates (n) provides consistency but may not reflect all real-world scenarios
  - Binary metrics simplify testing but lose granularity compared to continuous scores
- Failure signatures:
  - Invalid discrimination outputs (>5% as noted in the paper) indicate prompt interpretation issues
  - Small or negative DG-DIFF values suggest discrimination isn't improving over generation
  - Inconsistent performance across similar tasks may indicate task-specific limitations
- First 3 experiments:
  1. Replicate GSM8K experiment with LLaMA-2 13B to observe negative DG-DIFF
  2. Test non-autoregressive model (Flan-T5-XXL) on TriviaQA to see if SELF-[IN]CORRECT is rejected
  3. Run simplified discrimination setup on TriviaQA by replacing incorrect answers with obviously wrong ones from other questions

## Open Questions the Paper Calls Out

- Can autoregressive pre-training objectives fully explain the observed SELF-[IN]CORRECT phenomenon?
- What is the minimum level of discrimination capability required for effective self-improvement in LLMs?
- How do different instruction-tuning datasets and methodologies affect discrimination vs. generation capabilities?

## Limitations
- Heavy dependence on prompt engineering and model-specific behaviors
- Scope limited to self-generated candidates rather than external references
- Hypothesis testing framework assumes independence between generation and discrimination phases

## Confidence
- **High Confidence**: The experimental results showing 54 out of 56 experiments failing to reject the SELF-[IN]CORRECT hypothesis
- **Medium Confidence**: The claim that non-autoregressive models show better discrimination than autoregressive models
- **Low Confidence**: The inference that self-improvement frameworks appear successful only when evaluated with different metrics than used during training

## Next Checks
1. Systematically vary the discrimination prompt structure across all four tasks to determine whether the SELF-[IN]CORRECT pattern persists or breaks under different prompt formulations
2. Modify the experimental framework to include discrimination between self-generated responses and external reference answers to test whether models discriminate better when comparing against non-self-generated alternatives
3. Implement a version of the self-refine evaluation where the same metric used for final assessment is also used during the internal discrimination scoring, to test whether the observed metric mismatch is responsible for the self-improvement illusion