---
ver: rpa2
title: 'Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter
  Efficient'
arxiv_id: '2410.08893'
source_url: https://arxiv.org/abs/2410.08893
tags:
- sequence
- world
- learning
- conference
- mamba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the sample efficiency challenge in model-based
  reinforcement learning (MBRL) by introducing Drama, a world model that leverages
  Mamba-2 as its sequence model backbone. Drama achieves linear computational complexity
  O(n) while effectively capturing long-term dependencies, enabling efficient training
  with longer sequences on standard hardware.
---

# Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient

## Quick Facts
- arXiv ID: 2410.08893
- Source URL: https://arxiv.org/abs/2410.08893
- Authors: Wenlong Wang; Ivana Dusparic; Yucheng Shi; Ke Zhang; Vinny Cahill
- Reference count: 22
- Key outcome: Drama achieves 105% normalized score on Atari100k benchmark using only a 7M-parameter world model

## Executive Summary
This paper addresses the sample efficiency challenge in model-based reinforcement learning by introducing Drama, a world model that leverages Mamba-2 as its sequence model backbone. Drama achieves linear computational complexity O(n) while effectively capturing long-term dependencies, enabling efficient training with longer sequences on standard hardware. The authors also introduce dynamic frequency-based sampling (DFS) to mitigate suboptimality caused by inaccurate world models during early training stages. Drama achieves competitive performance with state-of-the-art MBRL algorithms while using significantly fewer parameters.

## Method Summary
Drama is a world model for MBRL that uses Mamba-2 as its sequence model backbone. The architecture consists of a discrete variational autoencoder for observation compression, a Mamba-2 sequence model for predicting latent states and rewards, and a behavior policy trained via imagination rollouts. The method introduces Dynamic Frequency-Based Sampling (DFS) to balance training between the world model and behavior policy. The entire system is trained end-to-end with the world model learning to predict future latent states, rewards, and terminal flags based on observation-action sequences.

## Key Results
- Achieves 105% normalized score on Atari100k benchmark
- Uses only 7 million parameters for the world model
- Demonstrates linear computational complexity O(n) for sequence modeling
- Shows Mamba-2 outperforms Mamba in Atari games for sequence modeling

## Why This Works (Mechanism)

### Mechanism 1
Mamba-2's semi-separable matrix structure enables linear complexity O(n) while maintaining strong long-term dependency capture. Mamba-2 reformulates state-space equations as a single matrix multiplication using a specially designed semi-separable lower triangular matrix. This matrix can be decomposed into q × q blocks, where causal attention over short ranges and hidden state transformations are handled by specialized blocks. The key is that this structure allows GPU parallelism without sequential dependencies, unlike recurrent approaches.

### Mechanism 2
Dynamic Frequency-Based Sampling (DFS) mitigates suboptimality from inaccurate world models during early training stages. DFS maintains two vectors tracking how often transitions are sampled for world model training versus behavior policy training. It computes sampling probabilities that favor transitions where the world model has been trained more frequently than the behavior policy. This ensures that 'imagination' sampling favors transitions learned by the world model while avoiding excessive determinism.

### Mechanism 3
Separating the sequence model from the autoencoder allows more efficient training with longer sequences. Unlike DreamerV3 where zt+1 depends on both Ot+1 and dt, Drama's sequence model predicts ˆzt+1 based solely on dt, eliminating sequential dependence. This architectural choice enables Mamba's efficiency with long sequences and accelerates the 'imagination' process.

## Foundational Learning

- **Concept**: State Space Models (SSMs) and their linear complexity
  - **Why needed here**: Understanding SSMs is crucial because Drama leverages Mamba-2, an advanced SSM architecture that provides O(n) complexity for sequence modeling
  - **Quick check question**: What is the computational complexity of standard transformers for sequence modeling, and how does it compare to SSMs?

- **Concept**: Partially Observable Markov Decision Processes (POMDPs)
  - **Why needed here**: The paper frames the reinforcement learning problem as a POMDP where the agent observes high-dimensional images rather than true states, which is fundamental to understanding world model approaches
  - **Quick check question**: In a POMDP, how does the agent's observation differ from the true state, and why is this distinction important for world model learning?

- **Concept**: Variational Autoencoders (VAEs) for latent space representation
  - **Why needed here**: Drama uses a discrete VAE to compress observations into latent variables, which the sequence model then processes, making understanding VAEs essential for grasping the overall architecture
  - **Quick check question**: What is the purpose of discretizing the latent embeddings in Drama's VAE, and how does this differ from standard continuous VAE approaches?

## Architecture Onboarding

- **Component map**: Raw frames → VAE encoder → Mamba-2 sequence model → latent predictions → behavior policy training → action selection
- **Critical path**: Raw frames → VAE encoder → Mamba-2 sequence model → latent predictions → behavior policy training → action selection
- **Design tradeoffs**:
  - Mamba-2 vs. Transformers: Linear vs. quadratic complexity, but potentially less expressive power
  - Discrete vs. Continuous VAE: Better compression but introduces discretization artifacts
  - World model accuracy vs. behavior policy optimality: Early in training, inaccurate world models can lead to suboptimal policies
- **Failure signatures**:
  - Poor reconstruction quality indicates VAE issues
  - Policy collapse suggests world model inaccuracies
  - Training instability may indicate Mamba-2 parameter tuning problems
- **First 3 experiments**:
  1. **Baseline comparison**: Run Drama with Mamba-2 vs. standard RNN-based world model on a simple Atari game to verify computational efficiency gains
  2. **DFS ablation**: Compare DFS vs. uniform sampling on Atari100k to measure sample efficiency improvements
  3. **Sequence length scaling**: Test Drama's performance with varying sequence lengths (8, 32, 128) to identify optimal configuration for different game types

## Open Questions the Paper Calls Out

### Open Question 1
Does the Mamba-2 architecture provide consistent performance improvements over Mamba across diverse MBRL tasks beyond the Atari benchmark? The paper states "Mamba-2 achieves superior results as a sequence model in the Atari100k benchmarks" but only compares them on a subset of Atari games. The paper only provides comparative results on a limited set of Atari games and a custom grid world environment, which may not be representative of all MBRL scenarios. Comprehensive comparative studies across multiple MBRL benchmarks (MuJoCo, DeepMind Control Suite, etc.) showing consistent Mamba-2 performance advantages over Mamba would resolve this.

### Open Question 2
What is the precise relationship between autoencoder robustness and task performance in visually complex environments? The paper suggests that the XS model's poor performance in Breakout is due to missing ball reconstruction, and the S model improves when doubling filters, but doesn't establish a systematic relationship. The paper provides anecdotal evidence from two models but doesn't conduct controlled experiments varying autoencoder capacity or analyze the quantitative relationship between reconstruction quality and downstream performance. Controlled ablation studies varying autoencoder capacity parameters while keeping other components fixed, with quantitative analysis of reconstruction error versus task performance would resolve this.

### Open Question 3
How does the Dynamic Frequency-Based Sampling (DFS) method perform in environments with non-stationary reward distributions or sparse rewards? The paper notes DFS "performs less effectively in games like Breakout and KungFuMaster, likely because the critical game dynamics are accessible early in the gameplay" and mentions it works well in some games but struggles in sparse reward games. The paper only evaluates DFS on Atari games with relatively dense rewards and doesn't test it in environments specifically designed to have changing reward distributions or extremely sparse rewards. Experiments on benchmark environments with known non-stationary dynamics (changing reward functions) or artificially designed sparse reward environments comparing DFS against alternative sampling strategies would resolve this.

## Limitations
- Limited evaluation scope - only tested on Atari100k benchmark without continuous control tasks
- Insufficient ablation studies to isolate contributions of Mamba-2 vs. DFS vs. architectural choices
- Claims about Mamba-2 superiority over Mamba lack comprehensive comparative analysis

## Confidence
- **High confidence**: The computational complexity claims for Mamba-2 (O(n) vs O(n²)) are well-established in the literature and the architectural design choices are clearly specified
- **Medium confidence**: The performance improvements on Atari100k are demonstrated but the sample efficiency gains could be influenced by implementation details not fully specified in the paper
- **Low confidence**: The claim that Mamba-2 outperforms Mamba in Atari games lacks sufficient comparative analysis and the DFS mechanism's contribution to overall performance is not thoroughly isolated

## Next Checks
1. **Ablation on sequence modeling backbone**: Replace Mamba-2 with RWKV and transformers of similar parameter counts to isolate the contribution of Mamba-2's specific architecture to performance gains
2. **Extended benchmark evaluation**: Test Drama on continuous control tasks (DMC Suite) and procedurally generated environments (Procgen) to assess generalizability beyond Atari games
3. **DFS mechanism isolation**: Run experiments with uniform sampling versus DFS while keeping all other components constant to quantify the exact contribution of dynamic frequency-based sampling to sample efficiency improvements