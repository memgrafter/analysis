---
ver: rpa2
title: Evolution Guided Generative Flow Networks
arxiv_id: '2402.02186'
source_url: https://arxiv.org/abs/2402.02186
tags:
- gflownets
- egfn
- training
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes EGFN, a method that augments GFlowNet training
  with evolutionary algorithms to address challenges in long trajectories and sparse
  rewards. EGFN evolves a population of agents to generate high-reward trajectories
  stored in a replay buffer, which are then combined with online samples to train
  the main GFlowNet agent.
---

# Evolution Guided Generative Flow Networks

## Quick Facts
- arXiv ID: 2402.02186
- Source URL: https://arxiv.org/abs/2402.02186
- Authors: Zarif Ikram; Ling Pan; Dianbo Liu
- Reference count: 33
- Primary result: EGFN improves GFlowNet training in long-trajectory, sparse-reward settings by combining evolutionary algorithms with prioritized replay buffers

## Executive Summary
This paper introduces Evolution Guided Generative Flow Networks (EGFN), a method that augments GFlowNet training with evolutionary algorithms to address challenges in long trajectories and sparse rewards. EGFN evolves a population of agents to generate high-reward trajectories stored in a replay buffer, which are then combined with online samples to train the main GFlowNet agent. The method is evaluated across synthetic hypergrid tasks and real-world molecular design problems, showing improved mode discovery and distribution fitting compared to GFlowNets and other baselines, with performance gains increasing for longer trajectories and sparser rewards.

## Method Summary
EGFN combines evolutionary algorithms with GFlowNet training to address the challenges of long trajectories and sparse rewards. The method maintains a population of GFlowNet agents that are evolved using selection, crossover, and mutation operations. These agents generate trajectories that are evaluated based on their terminal rewards and stored in a prioritized replay buffer. During GFlowNet training, the main agent (star agent) is trained using a combination of online trajectories sampled from its current policy and offline trajectories retrieved from the replay buffer. This approach provides richer gradient signals and promotes exploration, particularly in challenging environments where standard GFlowNet training struggles.

## Key Results
- EGFN demonstrates improved mode discovery and distribution fitting compared to standard GFlowNets across synthetic and real-world tasks
- Performance gains increase significantly for longer trajectories and sparser reward settings
- Ablation studies show that the combination of mutation and prioritized replay buffer is crucial for EGFN's effectiveness

## Why This Works (Mechanism)

### Mechanism 1
Evolutionary algorithms bias the population toward high-reward regions, providing better gradient signals for GFlowNet training. EA's selection operation evaluates fitness across entire trajectories, naturally favoring regions with high expected returns. This is especially valuable when rewards are sparse and trajectories are long, because standard GFlowNet training struggles with credit assignment in these settings. The core assumption is that the fitness metric (mean reward over E trajectories) is a reliable proxy for the quality of the agent's policy.

### Mechanism 2
Mutation in EA promotes natural exploration, which is crucial for GFlowNets training and mode-finding. Mutation introduces Gaussian perturbations to the agent weights, ensuring stochasticity and preventing premature convergence to suboptimal regions. This diversity is essential for GFlowNets, which rely on diverse samples for better training and mode discovery. The core assumption is that the mutation strength (γ) is appropriately tuned to balance exploration and exploitation.

### Mechanism 3
The prioritized replay buffer (PRB) stores high-reward trajectories, providing sample redundancy and improving gradient signals for stable GFlowNet training. EA agents generate trajectories, which are stored in the PRB based on their terminal rewards. During GFlowNet training, these high-reward trajectories are combined with online samples, providing a richer and more stable gradient signal than online samples alone. The core assumption is that the PRB effectively prioritizes and samples trajectories that are beneficial for training the star agent.

## Foundational Learning

- **Concept: Generative Flow Networks (GFlowNets)**
  - Why needed here: EGFN builds directly on GFlowNets, so understanding their training objectives (FM, DB, TB) and challenges (long trajectories, sparse rewards) is essential
  - Quick check question: What is the key difference between GFlowNets and standard RL in terms of their objective?

- **Concept: Evolutionary Algorithms (EAs)**
  - Why needed here: EGFN uses EAs to evolve a population of GFlowNet agents, so understanding selection, crossover, and mutation is crucial
  - Quick check question: How does the selection operation in EAs differ from the gradient-based update in GFlowNets?

- **Concept: Prioritized Replay Buffers (PRBs)**
  - Why needed here: EGFN uses a PRB to store and sample high-reward trajectories, so understanding how PRBs prioritize and sample experiences is important
  - Quick check question: How does a PRB differ from a standard replay buffer in terms of sampling strategy?

## Architecture Onboarding

- **Component map:** Star agent -> EA GFlowNet agents -> Prioritized replay buffer -> Fitness evaluation -> Selection, crossover, mutation
- **Critical path:** 1) Initialize EA population with random weights. 2) Evaluate fitness of each agent by sampling trajectories and calculating mean reward. 3) Select elite agents, perform crossover and mutation to generate new population. 4) Store evaluated trajectories in PRB. 5) Sample online trajectories from star agent and offline trajectories from PRB. 6) Train star agent using combined samples and a GFlowNets loss function (FM, DB, or TB).
- **Design tradeoffs:** Population size (k): larger populations provide more diversity but increase computational cost. Elite ratio (ϵ): higher ratios preserve good solutions but reduce exploration. Mutation strength (γ): higher values increase exploration but may destabilize training. PRB size and priority percentile: larger