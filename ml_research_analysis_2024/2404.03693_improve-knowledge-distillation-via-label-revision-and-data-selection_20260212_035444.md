---
ver: rpa2
title: Improve Knowledge Distillation via Label Revision and Data Selection
arxiv_id: '2404.03693'
source_url: https://arxiv.org/abs/2404.03693
tags:
- teacher
- student
- distillation
- knowledge
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of unreliable supervision in
  knowledge distillation, where incorrect predictions from the teacher model can mislead
  the student model''s training. The authors propose a two-pronged approach: Label
  Revision (LR) to rectify the teacher''s inaccurate predictions using ground truth
  labels, and Data Selection (DS) to select appropriate samples for distillation to
  reduce the impact of erroneous supervision.'
---

# Improve Knowledge Distillation via Label Revision and Data Selection

## Quick Facts
- arXiv ID: 2404.03693
- Source URL: https://arxiv.org/abs/2404.03693
- Reference count: 40
- Improves knowledge distillation accuracy by 1-2% on CIFAR-100 and ImageNet

## Executive Summary
This paper addresses unreliable supervision in knowledge distillation by proposing two complementary techniques: Label Revision (LR) to correct teacher predictions using ground truth labels, and Data Selection (DS) to select informative samples for distillation based on influence scores. The approach significantly improves student model performance on CIFAR-100 and ImageNet datasets, outperforming vanilla knowledge distillation and maintaining compatibility with other state-of-the-art methods.

## Method Summary
The authors propose a two-pronged approach to improve knowledge distillation: Label Revision (LR) corrects teacher predictions by blending soft labels with ground truth one-hot labels using a weighted formula, while Data Selection (DS) identifies and selects samples with high influence scores for teacher supervision. The method splits training data into high-influence samples (Dt) that receive revised teacher supervision and low-influence samples (Ds) that use only ground truth labels. This combination reduces the impact of erroneous teacher predictions while preserving useful generalization cues.

## Key Results
- Achieves 1-2% accuracy improvements on CIFAR-100 and ImageNet compared to vanilla KD
- Outperforms using either LR or DS alone, demonstrating their complementary effects
- Maintains compatibility with other state-of-the-art distillation approaches

## Why This Works (Mechanism)

### Mechanism 1
Label Revision (LR) corrects teacher soft labels by combining them with ground truth, improving student supervision quality. The formula `p = βpt + (1 − β)y` blends teacher confidence with one-hot correctness. This assumes ground truth labels are correct and the teacher's relative class probabilities still contain useful generalization cues. If ground truth labels are noisy, the revision can inject more error rather than correct it.

### Mechanism 2
Data Selection (DS) reduces exposure to unreliable teacher supervision by selecting only informative samples for distillation. Influence scores estimate each sample's effect on model parameters. High-influence samples are more "difficult" and benefit more from teacher guidance. The dataset is split into Dt (high-influence, teacher-supervised) and Ds (low-influence, ground-truth-only). If influence scores correlate poorly with actual learning difficulty, DS may discard useful samples or keep noisy ones.

### Mechanism 3
Combining LR and DS yields multiplicative improvements over using either alone. LR corrects supervision on high-influence samples; DS ensures only these samples receive teacher guidance, reducing overall noise. Together they improve both accuracy and training efficiency. If the teacher model is highly accurate, DS may remove useful samples and LR may overcorrect, hurting performance.

## Foundational Learning

- Concept: Influence function estimation
  - Why needed here: Used to rank samples by their impact on model parameters, guiding DS
  - Quick check question: What does a high influence score indicate about a training sample?

- Concept: Softmax temperature scaling
  - Why needed here: Softens teacher logits to preserve inter-class relationships in LR
  - Quick check question: How does increasing temperature affect the sharpness of the softmax output?

- Concept: KL divergence vs. MSE in distillation
  - Why needed here: Determines how student predictions are compared to teacher outputs
  - Quick check question: In what scenario would MSE be preferable to KL divergence for distillation loss?

## Architecture Onboarding

- Component map: Teacher model (pre-trained, fixed) → Influence scoring (per-sample) → Data split (Dt, Ds) → Label Revision (for Dt) → Student training (loss = cross-entropy + distillation + revised distillation)
- Critical path: Score → Split → Revise → Train student
- Design tradeoffs: Higher DS percentage improves accuracy but increases teacher supervision cost; higher LR strength improves label quality but may distort teacher knowledge
- Failure signatures: Student accuracy drops when ground truth is noisy; performance stalls if influence scores misrank samples; training diverges if LR over-corrects
- First 3 experiments:
  1. Run baseline KD on CIFAR-100 with ResNet teacher/student; record accuracy
  2. Apply only DS (80% high-influence to teacher); compare accuracy and training time
  3. Apply only LR (fix η=0.8); compare accuracy and observe label changes on mispredicted samples

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed Label Revision (LR) method perform when applied to knowledge distillation tasks with noisy labels in the training data? The paper assumes correctly labeled training samples for LR to rectify wrong predictions, but acknowledges that ground truth labels can be incomplete or missing in real-world applications. Experiments comparing LR's effectiveness on datasets with varying levels of label noise would provide insights into its robustness and limitations.

### Open Question 2
Can alternative methods to the influence function, such as Data Shapley or TracIn, be used for Data Selection (DS) to potentially improve sample selection and distillation performance? The paper uses the influence function for DS but mentions that other estimation methods are worth investigating in the future, as they may help select more appropriate samples. Comparative experiments using different sample selection methods for DS on various datasets and model architectures would reveal the most effective approach.

### Open Question 3
How does the proposed method perform when applied to knowledge distillation tasks beyond image classification, such as natural language processing or speech recognition? The paper focuses on image classification tasks and mentions that the proposed method can be combined with other distillation approaches, but does not explore its applicability to other domains. Applying the proposed method to knowledge distillation tasks in NLP, speech recognition, or other domains and comparing its performance with existing methods would demonstrate its versatility and effectiveness across different applications.

## Limitations

- Effectiveness depends critically on ground truth label accuracy - noisy labels could amplify errors rather than correct them
- Data Selection's influence function computation is computationally expensive with no runtime comparisons provided
- Performance on non-image domains or with different teacher-student capacity gaps remains untested

## Confidence

- High confidence in the mathematical formulation of LR and DS mechanisms
- Medium confidence in empirical results due to limited ablation studies on hyperparameter sensitivity
- Medium confidence in generalization claims since results are only shown on CIFAR-100 and ImageNet

## Next Checks

1. Test Label Revision on a dataset with known label noise (e.g., CIFAR-10 with 10-20% random label corruption) to verify the method doesn't amplify label errors
2. Measure and compare the wall-clock training time with and without Data Selection to quantify the computational overhead
3. Run experiments with a weaker teacher (e.g., ResNet18 → ResNet34) to verify performance gains persist when teacher accuracy is closer to student accuracy