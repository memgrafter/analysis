---
ver: rpa2
title: Data curation via joint example selection further accelerates multimodal learning
arxiv_id: '2406.17711'
source_url: https://arxiv.org/abs/2406.17711
tags:
- training
- jest
- data
- learning
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that joint batch selection is more effective
  for multimodal contrastive learning than individual example selection. The proposed
  JEST (Joint Example Selection) algorithm efficiently selects highly learnable batches
  from larger super-batches by scoring them according to their joint learnability
  - the difference between learner and reference model losses.
---

# Data curation via joint example selection further accelerates multimodal learning

## Quick Facts
- arXiv ID: 2406.17711
- Source URL: https://arxiv.org/abs/2406.17711
- Authors: Talfan Evans; Nikhil Parthasarathy; Hamza Merzic; Olivier J. Henaff
- Reference count: 40
- This paper demonstrates that joint batch selection is more effective for multimodal contrastive learning than individual example selection, achieving state-of-the-art performance with up to 10× fewer FLOPs and 13× fewer training iterations.

## Executive Summary
This paper introduces JEST (Joint Example Selection), an algorithm that accelerates multimodal contrastive learning by jointly selecting highly learnable batches from larger super-batches. Unlike previous methods that select examples independently, JEST scores entire batches according to their joint learnability - the difference between learner and reference model losses. The method is particularly effective at "data quality bootstrapping," where a reference model trained on a small curated dataset can guide the selection of much larger uncurated data. When combined with efficient multi-resolution training and model approximation, JEST++ achieves state-of-the-art performance on ImageNet and COCO benchmarks with dramatically reduced computational requirements.

## Method Summary
The JEST algorithm accelerates multimodal learning by jointly selecting batches based on learnability scores that combine both learner and reference model losses. The method works by first scoring large super-batches using a pretrained reference model, then applying the JEST algorithm to select the most learnable sub-batches for training. The learnability score prioritizes examples that are easy for the reference model (indicating quality) but hard for the learner (indicating they still have information to provide). The paper also introduces JEST++ which adds multi-resolution training using FlexiViT for model approximation, allowing efficient scoring of large super-batches at lower resolutions. This approach enables strong data quality bootstrapping where small curated datasets can effectively guide the selection of much larger uncurated datasets.

## Key Results
- JEST achieves state-of-the-art performance on ImageNet (0-Shot/10-Shot) and COCO benchmarks with up to 10× fewer FLOPs and 13× fewer training iterations
- Joint batch selection outperforms independent example selection by identifying mutually informative example combinations
- Data quality bootstrapping enables reference models trained on small curated datasets to effectively guide selection of much larger uncurated datasets
- Multi-resolution training with Flexi-JEST maintains performance while significantly reducing computational costs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Joint batch selection yields more learnable data combinations than independent example selection.
- **Mechanism**: The learnability of a batch is highly structured and non-diagonal, meaning joint selection can identify combinations of examples that are mutually informative. By scoring entire batches and sampling in proportion to their joint learnability, the method selects sub-batches with high conditional learnability.
- **Core assumption**: The learnability matrix is non-diagonal and structured such that some combinations of examples are more informative together than individually.
- **Evidence anchors**: Multimodal contrastive objectives expose dependencies between data; joint learnability decomposes into conditional learnabilities.
- **Break condition**: If the learnability matrix is diagonal or random, joint selection provides no advantage over independent selection.

### Mechanism 2
- **Claim**: Learnability scoring combines hard negatives and positives to accelerate learning.
- **Mechanism**: The learnability score is the difference between learner and reference model losses, prioritizing examples that are easy for the reference model but hard for the learner.
- **Core assumption**: A pretrained reference model can accurately assess which examples are high-quality and easy to learn.
- **Evidence anchors**: Learnability combines shard(B|θ) and seasy(B|θ∗) = ℓ(B|θ) − ℓ(B|θ∗).
- **Break condition**: If the reference model is poorly trained or data distribution shifts significantly, learnability scores become unreliable.

### Mechanism 3
- **Claim**: Data quality bootstrapping allows small curated datasets to guide learning on much larger uncurated data.
- **Mechanism**: A reference model trained on a small, well-curated dataset can steer curation toward high-quality data within larger uncurated datasets.
- **Core assumption**: The distribution of high-quality data in curated datasets is representative enough to guide selection in larger datasets.
- **Evidence anchors**: Reference models trained on curated datasets can effectively guide curation of much larger datasets.
- **Break condition**: If the curated dataset is too small or unrepresentative, it cannot effectively guide selection.

## Foundational Learning

- **Concept**: Multimodal contrastive learning objectives
  - Why needed here: The paper's joint selection method relies on contrastive losses that expose dependencies between data points in a batch.
  - Quick check question: What is the difference between softmax-contrastive and sigmoid-contrastive losses, and why does the paper prefer sigmoid?

- **Concept**: Importance sampling and its connection to learnability
  - Why needed here: The learnability scoring can be interpreted as an importance sampling scheme where data points contributing most to gradient expectation are prioritized.
  - Quick check question: How does the learnability score relate to the gradient magnitude of each example?

- **Concept**: Model approximation techniques for efficient inference
  - Why needed here: The paper uses FlexiViT to reduce the cost of scoring large super-batches by lowering image resolution.
  - Quick check question: What are the trade-offs between different model approximation strategies (patch dropping vs. resolution reduction)?

## Architecture Onboarding

- **Component map**: Learner model -> Reference model -> Scoring mechanism -> Batch selection algorithm -> Multi-resolution training
- **Critical path**: 1) Score large super-batches using reference model (with approximation), 2) Apply JEST algorithm to select most learnable sub-batches, 3) Train learner on selected sub-batches using multi-resolution training, 4) Cache reference model scores
- **Design tradeoffs**: Higher filtering ratios increase computational overhead but improve performance; full-resolution scoring is more accurate but expensive vs. low-resolution; training batch size vs. effective batch size
- **Failure signatures**: Training instabilities with aggressive filtering ratios; performance degradation with poorly trained reference models; reduced gains with diagonal learnability matrices
- **First 3 experiments**: 1) Implement JEST with small super-batch size and visualize learnability matrix, 2) Compare JEST against independent example selection, 3) Test different filtering ratios for optimal trade-off

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the research presented, several natural questions emerge from the work:

## Limitations
- Effectiveness depends critically on reference model quality and data distribution alignment
- Computational overhead of scoring large super-batches may limit practical applicability in resource-constrained settings
- Assumes learnability matrix is non-diagonal and structured, which may not hold for all datasets or learning objectives

## Confidence
- High confidence: Joint batch selection yields more learnable data combinations than independent example selection
- Medium confidence: Learnability scoring combines hard negatives and positives to accelerate learning
- Medium confidence: Data quality bootstrapping allows small curated datasets to guide learning on much larger uncurated data

## Next Checks
1. Test JEST performance on a dataset with a known diagonal learnability matrix to confirm that joint selection provides no advantage in this case.
2. Evaluate the impact of reference model quality on JEST performance by training reference models with varying levels of supervision and data quality.
3. Measure the computational overhead of JEST at different filtering ratios and super-batch sizes to determine the optimal trade-off between performance gains and resource requirements.