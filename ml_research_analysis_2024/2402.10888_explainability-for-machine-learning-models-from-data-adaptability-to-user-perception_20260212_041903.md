---
ver: rpa2
title: 'Explainability for Machine Learning Models: From Data Adaptability to User
  Perception'
arxiv_id: '2402.10888'
source_url: https://arxiv.org/abs/2402.10888
tags:
- explanation
- explanations
- methods
- users
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis addresses the challenge of generating meaningful explanations
  for machine learning models, focusing on balancing fidelity to the model and comprehensibility
  for users. The work is structured in two parts: the first enhances rule-based explanations
  and introduces methods for evaluating linear explanations'' suitability, while also
  comparing counterfactual explanation techniques.'
---

# Explainability for Machine Learning Models: From Data Adaptability to User Perception

## Quick Facts
- arXiv ID: 2402.10888
- Source URL: https://arxiv.org/abs/2402.10888
- Authors: julien Delaunay
- Reference count: 0
- One-line primary result: Enhances rule-based explanations and evaluates linear explanation suitability while investigating user perception through experiments.

## Executive Summary
This thesis addresses the challenge of generating meaningful explanations for machine learning models by balancing fidelity and comprehensibility. The work is structured in two parts: the first enhances rule-based explanations through improved discretization and introduces methods for evaluating linear explanations' suitability, while also comparing counterfactual explanation techniques. The second part investigates how different explanation methods and representations affect user trust and understanding through user experiments. Key contributions include improvements to Anchors, the APE framework for linear explanation suitability, and a comparative study of transparent versus opaque counterfactual methods.

## Method Summary
The thesis employs a multi-faceted approach to explainable AI, combining technical improvements to explanation algorithms with user-centered evaluation. Methodologically, it enhances rule-based explanations through MDLP discretization and pertinent negatives for textual data, develops the APE framework to test linear explanation suitability via unimodality and separability analysis, and compares transparent counterfactual methods against opaque latent-space approaches. User studies are conducted to evaluate the impact of different explanation techniques and their graphical versus textual representations on trust, understanding, and satisfaction across healthcare and legal domains.

## Key Results
- MDLP discretization yields the highest F1 score and shortest anchors among tested methods
- APE Oracle successfully predicts when linear explanations are suitable versus when rule-based approaches are needed
- Growing Net outperforms opaque counterfactual methods in terms of outlierness and sparsity for NLP tasks
- Rule-based explanations with textual representation achieve highest precision and self-reported understanding
- Graphical representations enhance user trust more than textual representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Improving discretization and neighborhood generation directly enhances the precision, coverage, and length of anchor-based explanations.
- Mechanism: MDLP discretization minimizes the number of intervals needed to separate classes, yielding shorter rules with higher coverage. Pertinent negatives extend the rule language to include absent words, increasing expressiveness and F1 score.
- Core assumption: The quality of rule-based explanations depends on both the discretization method for tabular data and the completeness of the neighborhood generation for textual data.
- Evidence anchors:
  - [abstract] "The work is structured in two parts: the first enhances rule-based explanations..."
  - [section] "We observe that overall, MDLP achieves the best F1 followed by quartile and entropy... MDPL yields on average the shortest anchors."
  - [corpus] Weak - corpus focuses on user-centered approaches rather than discretization improvements.
- Break condition: If discretization or neighborhood generation does not improve coverage/precision/length metrics, the improvement claim fails.

### Mechanism 2
- Claim: The suitability of linear explanations can be predicted by characterizing the classifier's decision boundary via counterfactuals.
- Mechanism: APE Oracle tests unimodality and linear separability of friend/enemy clusters; if both pass, a linear surrogate is suitable, otherwise rule-based is recommended.
- Core assumption: A single faithful linear surrogate exists only if decision boundary clusters are unimodal and linearly separable.
- Evidence anchors:
  - [abstract] "...introduces a novel approach for evaluating the suitability of linear explanations to approximate a model."
  - [section] "APE Oracle determines that a linear explanation is suitable to approximate the model in the locality of x1... APE Oracle determines that the region around x2 is not linearly separable..."
  - [corpus] Weak - corpus neighbors focus on user-centered explainability, not linear suitability testing.
- Break condition: If unimodality or separability tests fail to predict actual adherence/fidelity of linear explanations, the mechanism fails.

### Mechanism 3
- Claim: Simpler transparent counterfactual methods can outperform complex opaque latent-space methods for NLP tasks.
- Mechanism: Growing Net/ Language perturb text directly in interpretable space using WordNet or embeddings, producing sparse, fluent counterfactuals faster than VAE/GAN-based opaque methods.
- Core assumption: Latent perturbations do not guarantee sparse, fluent counterfactuals and add unnecessary complexity.
- Evidence anchors:
  - [abstract] "...comparing counterfactual explanation techniques... the benefits of one over the other."
  - [section] "Our findings reveal that methods positioned in the middle-ground, particularly Growing Net, performed favorably compared to opaque approaches... Growing Net and Growing Language outperformed counterfactualGAN and Polyjuice in terms of outlierness..."
  - [corpus] Weak - corpus does not include direct comparisons of transparent vs. opaque NLP counterfactual methods.
- Break condition: If transparent methods fail to match or exceed opaque methods on minimality, outlierness, or plausibility, the mechanism fails.

## Foundational Learning

- Concept: Anchors as rule-based explanations
  - Why needed here: Anchors are the baseline method improved in Chapter 2 and referenced throughout; understanding their mechanics is essential.
  - Quick check question: How does Anchors ensure a rule's precision threshold is met during search?

- Concept: Counterfactuals as contrastive explanations
  - Why needed here: Counterfactuals are central to Chapters 3 and 4; they define the "closest enemy" and decision boundary.
  - Quick check question: What distinguishes a valid counterfactual from an invalid one in terms of classifier prediction?

- Concept: Linear surrogate fidelity vs. complexity
  - Why needed here: Chapter 3 evaluates when linear explanations are suitable; understanding fidelity trade-offs is key.
  - Quick check question: How does adherence differ from fidelity when assessing a surrogate's quality?

## Architecture Onboarding

- Component map: Data preprocessing → Discretization (MDLP, k-means) → Rule mining (Anchors) OR Counterfactual generation (Growing Fields/Language/Net) → Suitability testing (APE Oracle) → Surrogate selection (linear vs. rule-based) → User evaluation (trust/understanding metrics)
- Critical path: Data → Explanation generation → Suitability testing → Output explanation
- Design tradeoffs:
  - Discretization granularity vs. rule length/coverage
  - Search space breadth vs. computational cost in Anchors
  - Linear surrogate simplicity vs. fidelity vs. rule-based completeness
  - Transparent perturbation vs. opaque latent space complexity
- Failure signatures:
  - Discretization yields too many/few intervals → poor rule quality
  - Neighborhood generation fails to find enemies → APE Oracle cannot run
  - Linear separability test passes but adherence low → false positive
  - Counterfactuals are not sparse or fluent → low user trust
- First 3 experiments:
  1. Apply MDLP discretization to tabular dataset; compare anchor F1/length vs. entropy
  2. Run APE Oracle on synthetic binary dataset; verify linear vs. rule-based selection matches ground truth
  3. Generate counterfactuals for text dataset using Growing Net vs. counterfactualGAN; measure minimality and plausibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of explanation technique (feature-attribution, rule-based, or counterfactual) impact user comprehension across different domains (e.g., healthcare, legal, finance)?
- Basis in paper: [explicit] The paper found that rule-based explanations with textual representation were most effective for comprehension in the healthcare domain, but no significant difference was found in the legal domain.
- Why unresolved: The study only evaluated two domains (healthcare and legal). The impact of explanation techniques on user comprehension may vary across other domains with different levels of complexity and user familiarity.
- What evidence would resolve it: Conduct user studies across multiple domains (e.g., healthcare, legal, finance, education) to compare the effectiveness of different explanation techniques on user comprehension. Analyze the results to identify patterns and determine which techniques are most effective in each domain.

### Open Question 2
- Question: How does the level of user trust in AI systems influence their perception of explanation quality and their willingness to follow AI recommendations?
- Basis in paper: [inferred] The paper found that users with higher trust in AI systems were more likely to follow AI recommendations, but the relationship between trust and explanation quality perception was not explicitly explored.
- Why unresolved: The study did not directly measure the impact of user trust on their perception of explanation quality. It is unclear whether users with higher trust in AI systems are more likely to perceive explanations as high-quality, or if explanation quality influences trust.
- What evidence would resolve it: Conduct user studies that measure both user trust in AI systems and their perception of explanation quality. Analyze the relationship between these two factors to determine if there is a correlation and, if so, the direction of the relationship.

### Open Question 3
- Question: How does the complexity of the AI model being explained affect the effectiveness of different explanation techniques and representations?
- Basis in paper: [inferred] The paper focused on relatively simple AI models (MLP classifiers) and did not explore the impact of model complexity on explanation effectiveness. It is possible that more complex models may require different explanation techniques or representations to be effective.
- Why unresolved: The study did not evaluate the impact of model complexity on explanation effectiveness. It is unclear whether the same explanation techniques and representations that were effective for simple models would be equally effective for more complex models.
- What evidence would resolve it: Conduct user studies that compare the effectiveness of different explanation techniques and representations across AI models with varying levels of complexity (e.g., simple linear models, complex neural networks). Analyze the results to identify which techniques and representations are most effective for each level of model complexity.

## Limitations

- Discretization improvements primarily evaluated on tabular datasets; performance on high-dimensional or mixed-type data untested
- APE framework's unimodality and separability assumptions may not hold for complex decision boundaries in real-world models
- Counterfactual comparison limited to specific transparent vs. opaque methods; generalization to other approaches uncertain
- User study findings based on specific experimental conditions may not generalize across different user populations or task contexts

## Confidence

- High: The fundamental mechanisms linking discretization quality to anchor performance (Mechanism 1) and the APE Oracle's theoretical basis for linear suitability testing (Mechanism 2)
- Medium: The comparative performance of transparent vs. opaque counterfactual methods (Mechanism 3), as results may depend heavily on specific implementation details and datasets
- Low: The generalizability of user study findings to broader contexts, given limited sample sizes and specific experimental designs

## Next Checks

1. Cross-dataset validation: Apply MDLP discretization improvements to diverse tabular datasets (including high-dimensional and mixed-type data) to verify robustness of anchor quality improvements
2. APE Oracle stress test: Evaluate APE Oracle predictions on synthetic datasets with known non-linear, multi-modal decision boundaries to quantify false positive/negative rates
3. Counterfactual ablation study: Conduct controlled experiments isolating specific components (e.g., perturbation strategy, search algorithm) of transparent vs. opaque methods to identify which design choices drive performance differences