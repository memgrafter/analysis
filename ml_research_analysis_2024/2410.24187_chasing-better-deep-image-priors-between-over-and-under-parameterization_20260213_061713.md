---
ver: rpa2
title: Chasing Better Deep Image Priors between Over- and Under-parameterization
arxiv_id: '2410.24187'
source_url: https://arxiv.org/abs/2410.24187
tags:
- uni00000013
- uni00000048
- uni00000011
- uni00000003
- uni00000033
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores a middle ground between over- and under-parameterized
  image priors by leveraging sparsity and the lottery ticket hypothesis (LTH). It
  proposes the lottery image prior (LIP), which identifies sparse subnetworks from
  over-parameterized deep image priors (DIPs) that match or exceed the performance
  of dense models while being more compact and efficient.
---

# Chasing Better Deep Image Priors between Over- and Under-parameterization

## Quick Facts
- **arXiv ID**: 2410.24187
- **Source URL**: https://arxiv.org/abs/2410.24187
- **Reference count**: 40
- **Key outcome**: This paper explores a middle ground between over- and under-parameterized image priors by leveraging sparsity and the lottery ticket hypothesis (LTH), proposing the lottery image prior (LIP) that identifies sparse subnetworks from over-parameterized deep image priors (DIPs) that match or exceed the performance of dense models while being more compact and efficient.

## Executive Summary
This paper introduces the Lottery Image Prior (LIP), a method that identifies sparse, high-performing subnetworks within over-parameterized deep image priors (DIPs) using iterative magnitude pruning (IMP). By exploiting the lottery ticket hypothesis, LIP subnetworks achieve comparable or superior performance to dense models while being significantly more parameter-efficient. The method is validated across multiple image restoration tasks and extended to compressive sensing with pre-trained GANs, demonstrating broad applicability. LIP subnetworks are shown to be transferable across images and tasks, offering a promising direction for efficient, high-performance image restoration.

## Method Summary
The method involves applying iterative magnitude pruning (IMP) to over-parameterized DIP models to identify sparse subnetworks (LIP) that retain or exceed the performance of the original dense models. Multi-image IMP with weight sharing is used to enhance transferability across tasks. The approach is further extended to pre-trained GAN generators for compressive sensing applications, validating its broader applicability.

## Key Results
- LIP subnetworks outperform under-parameterized deep decoders at comparable parameter counts.
- LIP subnetworks exhibit strong transferability across images and restoration tasks.
- LIP extends to compressive sensing with pre-trained GANs, confirming its validity in this setting.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-parameterized DIP models contain sparse subnetworks that can match or exceed dense performance when used as image priors.
- Mechanism: Iterative magnitude pruning (IMP) identifies sparse subnetworks by iteratively removing small-magnitude weights while maintaining image restoration capability.
- Core assumption: The original over-parameterized DIP contains redundant parameters that can be pruned without losing essential image prior properties.
- Evidence anchors:
  - [abstract] "lottery image prior (LIP), which identifies sparse subnetworks from over-parameterized deep image priors (DIPs) that match or exceed the performance of dense models"
  - [section] "we successfully locate the 'matching subnetworks' from over-parameterized DIP models by iterative magnitude pruning"
  - [corpus] Weak - no direct matches found in corpus, though related concepts exist in "Deep Diffusion Image Prior for Efficient OOD Adaptation in 3D Inverse Problems"
- Break condition: If IMP removes weights critical for capturing natural image statistics, performance degrades below deep decoder baseline.

### Mechanism 2
- Claim: LIP subnetworks exhibit strong transferability across images and restoration tasks while maintaining compact parameterization.
- Mechanism: Multi-image IMP with weight sharing creates subnetworks that capture generalizable features rather than overfitting to single images.
- Core assumption: Sparse subnetworks preserve low-level features (color, texture, shape) that are more transferable than high-level semantic features.
- Evidence anchors:
  - [abstract] "Those LIP subnetworks significantly outperform deep decoders under comparably compact model sizes" and "possess high transferability across different images as well as restoration task types"
  - [section] "LIP subnetworks can achieve better performances than dense DIP models and randomly pruned ones in the cross-domain setting"
  - [corpus] Weak - no direct matches found in corpus, though "SILO: Solving Inverse Problems with Latent Operators" suggests similar transferability concepts
- Break condition: If transferred to high-level tasks like classification where mid-to-high level features dominate, LIP subnetworks fail to transfer.

### Mechanism 3
- Claim: LIP extends beyond untrained priors to pre-trained GAN generators for compressive sensing applications.
- Mechanism: IMP applied to pre-trained GAN generators preserves essential generative capacity while reducing parameter count.
- Core assumption: Lottery ticket phenomenon applies to both randomly initialized and pre-trained network priors.
- Evidence anchors:
  - [abstract] "extend LIP to compressive sensing image reconstruction, where a pre-trained GAN generator is used as the prior" and "confirm its validity in this setting too"
  - [section] "we apply the IMP algorithm... evaluate the tickets on the compressive sensing task following the setting in Jalal et al. (2020)"
  - [corpus] Weak - no direct matches found in corpus, though "GLIP: Electromagnetic Field Exposure Map Completion by Deep Generative Networks" suggests related applications
- Break condition: If IMP removes weights critical for maintaining GAN's output distribution approximation, reconstruction quality degrades.

## Foundational Learning

- Concept: Lottery Ticket Hypothesis (LTH)
  - Why needed here: Provides theoretical foundation that sparse subnetworks exist within over-parameterized networks and can be identified through pruning
  - Quick check question: What distinguishes a "matching subnetwork" from a randomly pruned subnetwork according to LTH?

- Concept: Iterative Magnitude Pruning (IMP)
  - Why needed here: The practical algorithm used to identify LIP subnetworks by iteratively removing smallest-magnitude weights
  - Quick check question: How does IMP differ from one-shot pruning in terms of preserving network performance?

- Concept: Deep Image Prior (DIP)
  - Why needed here: The target architecture being sparsified - untrained networks that regularize image inverse problems through architecture alone
  - Quick check question: Why does DIP rely on early stopping, and how does this relate to the need for pruning?

## Architecture Onboarding

- Component map: Over-parameterized DIP → IMP algorithm → Sparse LIP subnetwork → Evaluation on image restoration tasks
- Critical path: Initial over-parameterized DIP → IMP iterations (prune 20% weights each round) → Validation of matching performance → Transferability testing
- Design tradeoffs: Over-parameterization vs efficiency (LIP wins), deep decoder compactness vs performance (LIP wins), single-image vs multi-image finding (multi-image improves transferability)
- Failure signatures: Performance degradation below deep decoder baseline, loss of transferability across tasks, inability to match dense model at extreme sparsities
- First 3 experiments:
  1. Apply IMP to hourglass DIP architecture on denoising task with single image, measure PSNR at various sparsity levels
  2. Compare LIP subnetwork performance against random pruning and SNIP baselines using same sparsity levels
  3. Test transferability of LIP subnetwork from denoising to inpainting task on held-out images

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Lottery Image Prior (LIP) hold for other under-parameterized architectures beyond the deep decoder, such as autoencoders or sparse coding models?
- Basis in paper: [inferred] The paper compares LIP with the deep decoder and discusses under-parameterization, but does not explore other under-parameterized priors.
- Why unresolved: The study focuses on the deep decoder as a representative under-parameterized prior, leaving the generality of LIP to other under-parameterized models untested.
- What evidence would resolve it: Experimental validation of LIP performance on a range of under-parameterized architectures (e.g., autoencoders, sparse coding models) across various image restoration tasks.

### Open Question 2
- Question: Can the Lottery Image Prior (LIP) be extended to handle real-time applications or dynamic scenes, such as video restoration?
- Basis in paper: [inferred] The paper discusses static image restoration tasks and mentions efficiency, but does not explore dynamic or real-time applications.
- Why unresolved: The current focus is on static images, and extending LIP to dynamic scenes would require addressing additional challenges like temporal coherence and computational efficiency.
- What evidence would resolve it: Demonstrations of LIP's effectiveness on video restoration tasks or other real-time applications, showing improved efficiency and performance compared to existing methods.

### Open Question 3
- Question: How does the Lottery Image Prior (LIP) perform in the presence of adversarial attacks or other forms of corruption beyond noise?
- Basis in paper: [inferred] The paper mentions robustness to noise and mentions adversarial robustness in related literature, but does not directly test LIP under adversarial conditions.
- Why unresolved: The robustness of LIP to adversarial attacks or other corruption types is not explored, leaving its security and reliability under such conditions uncertain.
- What evidence would resolve it: Empirical studies evaluating LIP's performance under adversarial attacks or other corruption types, comparing its robustness to other image restoration methods.

## Limitations
- LTH application is demonstrated empirically but lacks theoretical justification for why lottery tickets exist in untrained DIP architectures specifically
- Transferability claims are limited to low-level image features (color, texture, shape) and may not extend to higher-level semantic tasks
- Multi-image IMP approach is computationally expensive and may not scale to very large datasets

## Confidence
- High confidence: LIP subnetworks outperform deep decoders at comparable parameter counts (empirical validation on Set5/Set14)
- Medium confidence: LIP subnetworks transfer across different restoration tasks (evidence from denoising→inpainting transfer)
- Medium confidence: LTH applies to both untrained DIPs and pre-trained GANs (preliminary GAN results but less extensive validation)

## Next Checks
1. Test LIP transferability on high-level semantic tasks (e.g., classification) to verify the limits of feature transferability
2. Compare LIP with alternative sparse priors like NASNet-derived architectures to benchmark the effectiveness of IMP
3. Analyze the computational overhead of multi-image IMP versus single-image approaches to establish practical scalability limits