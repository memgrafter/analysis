---
ver: rpa2
title: Hierarchical Conditional Tabular GAN for Multi-Tabular Synthetic Data Generation
arxiv_id: '2411.07009'
source_url: https://arxiv.org/abs/2411.07009
tags:
- data
- synthetic
- table
- hctgan
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HCTGAN, a GAN-based method for generating
  synthetic data from multi-tabular relational datasets. The key idea is to condition
  the child table generators on both the categorical conditioning vector (as in CTGAN)
  and on Gaussian noise vectors derived from the parent tables, allowing hierarchical
  information transfer while preserving privacy since no real parent data is exposed
  during child generation.
---

# Hierarchical Conditional Tabular GAN for Multi-Tabular Synthetic Data Generation

## Quick Facts
- arXiv ID: 2411.07009
- Source URL: https://arxiv.org/abs/2411.07009
- Authors: Wilhelm Ågren; Victorcio Úbeda Sosa
- Reference count: 40
- Primary result: HCTGAN generates multi-tabular synthetic data with comparable quality to HMA1 but faster and with guaranteed referential integrity

## Executive Summary
This paper introduces HCTGAN, a GAN-based method for generating synthetic data from multi-tabular relational datasets. The key innovation is conditioning child table generators on both categorical conditioning vectors (from CTGAN) and Gaussian noise vectors derived from parent tables, enabling hierarchical information transfer while preserving privacy. The method maintains referential integrity through a specialized sampling algorithm that uses heuristics for child row counts and consistent foreign key generation. Evaluated on three small datasets against the HMA1 probabilistic model, HCTGAN achieves comparable data quality metrics while generating data faster and always maintaining referential integrity.

## Method Summary
HCTGAN extends CTGAN to multi-tababular data by introducing a hierarchical generation process. The method uses separate GAN models for each table in the relational schema, with child table generators conditioned on both categorical features (as in CTGAN) and Gaussian noise vectors derived from parent tables. The sampling algorithm ensures referential integrity by generating parent tables first, then using heuristics to determine child row counts and maintaining consistent foreign keys. The architecture allows for any number of tables and depth, with the generator and discriminator components structured similarly to CTGAN but with added conditioning on parent-derived noise vectors.

## Key Results
- HCTGAN achieves comparable data quality to HMA1 with CS scores around 0.7-0.8 and CPT scores around 0.5-0.7
- HCTGAN generates data significantly faster than HMA1 while maintaining 100% referential integrity
- HCTGAN produces more novel rows and better range coverage compared to HMA1
- Recommended for large or deeply connected datasets; HMA1 preferred for small datasets where data quality is paramount

## Why This Works (Mechanism)
The method works by leveraging hierarchical conditioning to transfer information from parent to child tables without exposing real parent data, thus preserving privacy while maintaining relational structure. The Gaussian noise vectors derived from parent tables provide sufficient statistical information for child generators to produce contextually appropriate data, while the sampling algorithm ensures that foreign key relationships remain consistent. This approach combines the efficiency of GAN-based generation with the structural awareness needed for relational databases.

## Foundational Learning
- **GAN fundamentals**: Conditional GANs generate data conditioned on auxiliary information, essential for generating contextually appropriate child table data
- **Why needed**: Without conditioning, child generators would produce data unrelated to parent context
- **Quick check**: Verify generator produces different outputs when given different conditioning vectors

- **Referential integrity**: Maintaining consistent foreign key relationships across tables
- **Why needed**: Ensures synthetic data remains valid for relational database operations
- **Quick check**: Verify all generated foreign keys have corresponding primary keys in parent tables

- **Hierarchical data generation**: Sequential generation where parent tables are generated before children
- **Why needed**: Children depend on parent data for context and foreign key relationships
- **Quick check**: Confirm parent tables are always generated before any dependent child tables

## Architecture Onboarding

**Component Map**: Data -> CTGAN-style GANs (per table) -> Conditional Noise Vectors (parent→child) -> Sampling Algorithm -> Synthetic Data

**Critical Path**: Parent Table Generation → Noise Vector Extraction → Child Table Conditioning → Referential Integrity Sampling → Final Synthetic Dataset

**Design Tradeoffs**: Uses heuristic-based sampling for speed and simplicity vs. exact probabilistic modeling; trades some data fidelity for faster generation and guaranteed integrity

**Failure Signatures**: 
- Inconsistent foreign keys indicate sampling algorithm failure
- Poor child data quality suggests inadequate conditioning information transfer
- Excessive novel rows may indicate generator overfitting to noise

**First 3 Experiments**:
1. Generate single-table synthetic data and compare to CTGAN baseline
2. Generate two-table parent-child data and verify referential integrity
3. Test with varying parent-child ratios to evaluate sampling heuristic robustness

## Open Questions the Paper Calls Out
The paper identifies several open questions including how to handle circular dependencies in relational schemas, how to optimize the sampling algorithm for specific parent-child relationship distributions, and how to extend the method to handle time-series data in relational contexts.

## Limitations
- Experimental datasets are relatively small (max 2,200 rows), limiting scalability validation
- Evaluation relies primarily on statistical similarity measures rather than task-specific downstream utility
- The heuristic for child row count determination (1.5× parent rows) is arbitrary and may not generalize

## Confidence

**High confidence**: HCTGAN's ability to maintain referential integrity during synthetic data generation; speed advantage over HMA1; generation of novel rows

**Medium confidence**: Data quality metrics (CS and CPT scores) being comparable to HMA1; improved range coverage claims

**Low confidence**: Real-world applicability to large-scale enterprise databases; effectiveness of synthetic data for specific downstream ML tasks not tested

## Next Checks

1. Test HCTGAN on larger datasets (10,000+ rows) with more complex relational structures to validate scalability claims

2. Conduct task-specific downstream evaluation (e.g., classification, regression) using synthetic data versus real data to measure practical utility beyond statistical similarity

3. Implement cross-validation by training HCTGAN on one subset of data and testing on held-out parent-child relationships to verify generalization capability