---
ver: rpa2
title: Multimodal Information Bottleneck for Deep Reinforcement Learning with Multiple
  Sensors
arxiv_id: '2410.17551'
source_url: https://arxiv.org/abs/2410.17551
tags:
- information
- learning
- representations
- multimodal
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MIB, a multimodal information bottleneck
  method for deep reinforcement learning using egocentric images and proprioception.
  The core idea is to learn compressed joint representations by minimizing mutual
  information between the joint representation and raw observations while preserving
  predictive information for future states.
---

# Multimodal Information Bottleneck for Deep Reinforcement Learning with Multiple Sensors

## Quick Facts
- arXiv ID: 2410.17551
- Source URL: https://arxiv.org/abs/2410.17551
- Authors: Bang You; Huaping Liu
- Reference count: 24
- Primary result: MIB achieves 711 reward on Hurdle Cheetah Run compared to 642 for Vanilla SAC

## Executive Summary
This paper introduces MIB, a multimodal information bottleneck method for deep reinforcement learning using egocentric images and proprioception. The core idea is to learn compressed joint representations by minimizing mutual information between the joint representation and raw observations while preserving predictive information for future states. MIB outperforms leading baselines on challenging locomotion tasks in terms of sample efficiency and zero-shot robustness to white noise and natural backgrounds. For example, MIB achieves 711 reward on Hurdle Cheetah Run compared to 642 for Vanilla SAC.

## Method Summary
The method learns continuous control policies from multimodal observations (egocentric images + proprioception) using a modified SAC algorithm with information bottleneck regularization. The approach combines image and proprioception encoders through a fusion model, applies stochastic encoding, and uses contrastive learning to preserve predictive information between consecutive state representations. The objective balances compression (minimizing mutual information with raw observations) against predictive relevance (maximizing information between consecutive representations given actions).

## Key Results
- MIB achieves 711 reward on Hurdle Cheetah Run compared to 642 for Vanilla SAC
- MIB demonstrates better sample efficiency than baseline methods on locomotion tasks
- MIB shows improved zero-shot robustness to white noise and natural backgrounds compared to baselines
- Using multiple modalities (images + proprioception) improves policy learning compared to single modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MIB improves sample efficiency by learning compressed joint representations that retain only task-relevant predictive information.
- Mechanism: The MIB objective minimizes mutual information between joint representation and raw observations while maximizing predictive information between consecutive representations. This forces the learned representation to filter out irrelevant details like noise or background changes while preserving information useful for predicting future states given actions.
- Core assumption: Task-relevant information for control is primarily in the predictive relationships between states, not in raw sensory details.
- Evidence anchors:
  - [abstract]: "learns compressed joint representations by minimizing mutual information between the joint representation and raw observations while preserving predictive information for future states"
  - [section 4.1]: "minimizes the mutual information between the latent joint representation and raw multimodal sensory inputs, while maximizing the predictive information between two successive joint representations given the corresponding action"
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If task-relevant information is not primarily predictive in nature (e.g., requires memorizing specific visual patterns), compression could discard useful information.

### Mechanism 2
- Claim: Using multiple modalities (images + proprioception) provides better performance than single modalities by capturing complementary information.
- Mechanism: The fusion model combines egocentric visual information (obstacles, environment layout) with proprioceptive feedback (internal state of robot) into a single joint representation that captures both external and internal state information needed for effective control.
- Core assumption: Locomotion tasks require both external perception (obstacles, terrain) and internal state awareness (joint positions, forces) for optimal control.
- Evidence anchors:
  - [abstract]: "leveraging information from egocentric images and proprioception is more helpful for learning policies on locomotion tasks than solely using one single modality"
  - [section 1]: "a locomotion robot needs to observe obstacles in front of its body via its egocentric camera and sense its own motors by using force and torque sensors"
  - [section 5.2]: Table 3 shows MIB outperforms Non-Img and Non-Prop ablations
- Break condition: If one modality becomes unreliable or noisy, the other may not compensate adequately if the fusion mechanism is not robust.

### Mechanism 3
- Claim: The InfoNCE lower bound on predictive information provides a tractable surrogate for the intractable mutual information objective.
- Mechanism: By using contrastive learning with positive samples (consecutive state representations) and negative samples (random state representations), the InfoNCE bound encourages representations to capture temporal consistency and predictive relationships while remaining computationally feasible.
- Core assumption: Temporal consistency in the latent space is important for policy learning and can be effectively captured through contrastive estimation.
- Evidence anchors:
  - [section 4.3]: "To effectively estimate the predictive information I(zt, at; zt+1), we employ the InfoNCE lower bound"
  - [section 4.2]: "The prediction head and the projection head both induce nonlinear transformation on zt and at, which is used to estimate the lower bound on I(zt, at; zt+1)"
  - [corpus]: Weak - no direct corpus evidence supporting this specific contrastive mechanism
- Break condition: If the contrastive objective becomes too easy (e.g., representations become too distinct), it may not provide meaningful gradients for learning.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The problem is formulated as an MDP where the agent observes multimodal states and takes actions to maximize cumulative reward
  - Quick check question: What are the key components of an MDP and how do they relate to this multimodal RL problem?

- Concept: Information Bottleneck Principle
  - Why needed here: MIB is built on the information bottleneck principle to compress raw observations while preserving task-relevant information
  - Quick check question: How does the information bottleneck principle balance compression and relevance in representation learning?

- Concept: Variational Inference
  - Why needed here: Used to derive tractable upper bounds on the mutual information terms in the MIB objective
  - Quick check question: What is the relationship between variational inference and the KL divergence bounds used in MIB?

## Architecture Onboarding

- Component map: Image encoder → Proprioception encoder → Fusion model → Stochastic encoder → Policy/Value networks. Prediction head and projection head for contrastive learning.
- Critical path: Raw observations → Encoders → Fusion → Stochastic sampling → Policy/Value networks. MIB loss computed in parallel.
- Design tradeoffs: Compression vs. information retention (controlled by α), contrastive learning vs. reconstruction-based approaches, stochastic vs. deterministic encoding.
- Failure signatures: Poor performance on single-modality tasks (suggests fusion is detrimental), sensitivity to hyperparameter α, instability in contrastive learning.
- First 3 experiments:
  1. Test ablation without compression term (Non-KL) to verify importance of compression
  2. Test with single modality only to confirm multimodal advantage
  3. Test with different α values to find optimal compression-relevance balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MIB's performance scale with more than two sensor modalities (e.g., adding tactile or audio feedback)?
- Basis in paper: [explicit] The authors mention in the conclusion that future work could extend MIB to tasks with different sensory inputs like tactile and audio feedback.
- Why unresolved: The paper only evaluates MIB on two modalities (egocentric images and proprioception), leaving the performance on more complex multimodal scenarios untested.
- What evidence would resolve it: Experimental results showing MIB's performance on tasks with three or more sensor modalities, compared to baseline methods.

### Open Question 2
- Question: What is the optimal balance between compression and predictive information preservation in MIB, and how does it vary across different tasks?
- Basis in paper: [explicit] The MIB objective includes a hyperparameter α that balances compression and relevance, and the authors mention performing hyperparameter tuning on one task.
- Why unresolved: The paper only reports results for one fixed α value across all tasks, without exploring how optimal α values might differ across environments or affect performance.
- What evidence would resolve it: A systematic study varying α across different locomotion tasks, showing how performance changes with different compression-relevance trade-offs.

### Open Question 3
- Question: How does MIB's zero-shot robustness to natural backgrounds compare to fine-tuning approaches?
- Basis in paper: [explicit] The authors evaluate zero-shot robustness to natural backgrounds but note that MIB degrades in their presence while remaining more robust than baselines.
- Why unresolved: The paper only tests zero-shot performance without comparing to methods that could be fine-tuned on the new backgrounds, leaving open whether MIB's advantage is significant.
- What evidence would resolve it: Experiments comparing MIB's zero-shot performance to baseline methods after fine-tuning on tasks with natural backgrounds.

## Limitations
- Evaluation limited to a narrow set of DeepMind Control Suite tasks, limiting generalizability to real-world robotic systems
- Lack of direct analysis of what information is being preserved versus discarded in the compressed representations
- Contrastive learning mechanism lacks corpus support and empirical validation against simpler alternatives

## Confidence
- High confidence in Mechanisms 2 and 3 - the multimodal advantage is well-supported by direct comparisons in Table 3, and the InfoNCE framework is established in contrastive learning literature
- Medium confidence in Mechanism 1 - while the theoretical motivation is sound, the ablation results are less definitive, and the claim that compression improves sample efficiency needs more direct evidence

## Next Checks
1. Add a reconstruction-based baseline (e.g., VAE) to isolate whether the contrastive predictive objective is superior to reconstruction for capturing temporal dependencies
2. Use canonical correlation analysis (CCA) or similar techniques to quantify how much information from each modality is preserved in the joint representation
3. Test MIB on a non-locomotion task (e.g., manipulation) to verify that the multimodal information bottleneck approach transfers beyond the current domain