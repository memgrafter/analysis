---
ver: rpa2
title: 'Evading Toxicity Detection with ASCII-art: A Benchmark of Spatial Attacks
  on Moderation Systems'
arxiv_id: '2409.18708'
source_url: https://arxiv.org/abs/2409.18708
tags:
- ascii
- attacks
- fonts
- tokens
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a new class of adversarial attacks that exploit language
  models' inability to interpret spatially structured ASCII art. Using the ToxASCII
  benchmark and two custom fonts (special-token-based and text-filled), we achieve
  a perfect 1.0 Attack Success Rate across ten models including GPT-4o, LLaMA 3.1,
  and Phi-3.5-mini.
---

# Evading Toxicity Detection with ASCII-art: A Benchmark of Spatial Attacks on Moderation Systems

## Quick Facts
- arXiv ID: 2409.18708
- Source URL: https://arxiv.org/abs/2409.18708
- Reference count: 15
- We present a new class of adversarial attacks that exploit language models' inability to interpret spatially structured ASCII art, achieving perfect 1.0 Attack Success Rate across ten models including GPT-4o, LLaMA 3.1, and Phi-3.5-mini.

## Executive Summary
This paper introduces a novel class of adversarial attacks that exploit language models' inability to interpret spatially structured ASCII art. Using the ToxASCII benchmark and two custom fonts (special-token-based and text-filled), the authors achieve a perfect 1.0 Attack Success Rate across ten state-of-the-art toxicity detection models. The attacks effectively bypass moderation systems by leveraging the models' lack of spatial reasoning capabilities. The paper proposes defenses including adversarial training and OCR-based detection, showing partial mitigation of standard ASCII attacks but limited effectiveness against special-token variants.

## Method Summary
The authors create the ToxASCII benchmark by applying 269 ASCII fonts to 26 toxic phrases (one per alphabet letter). They also develop two custom font types: special-token-based fonts using model-specific special tokens like <|EOS|>, and text-filled fonts where the model processes only filler text while ignoring larger letter structures. The attacks are tested on ten models including GPT-4o, LLaMA 3.1, and Phi-3.5-mini. Defenses include adversarial training on LLaMA 3.1-70B with palindromic phrases and OCR-based detection for text-filled fonts using Tesseract and EasyOCR.

## Key Results
- Perfect 1.0 Attack Success Rate across all ten tested models
- Special-token-based ASCII art achieves highest evasion rates
- Adversarial training shows partial effectiveness against standard ASCII attacks
- OCR-based defense struggles with complex text-filled fonts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Language models fail to interpret spatially structured ASCII art, allowing toxic content to bypass toxicity detection.
- **Mechanism**: ASCII art represents text visually through character arrangement rather than linear tokenization. LLMs trained on standard text corpora lack the spatial reasoning capabilities to decode these visual representations, treating them as random character sequences instead.
- **Core assumption**: Language models primarily process text through semantic and syntactic patterns, not spatial/visual pattern recognition.
- **Evidence anchors**:
  - [abstract] "exploit language models' inability to interpret spatially structured ASCII art"
  - [section] "Our research shows that ASCII art fonts...can be used to bypass modern toxicity detectors"
  - [corpus] "Language models fail to interpret spatially structured ASCII art" (weak evidence, indirect support)
- **Break condition**: If models incorporate spatial reasoning modules or are trained on ASCII art datasets, they could learn to decode the visual representations.

### Mechanism 2
- **Claim**: Special tokens embedded within ASCII art disrupt the model's tokenization and interpretation process.
- **Mechanism**: Special tokens like <|EOS|> and <|SEP|> are unexpected elements that break the model's expected input patterns. When these tokens are used to construct ASCII art, they create sequences that the model cannot properly parse or interpret, effectively hiding the underlying message.
- **Core assumption**: Language models have fixed tokenization schemes that cannot handle unexpected special token arrangements.
- **Evidence anchors**:
  - [abstract] "embedding special tokens (e.g., <EOS>) can render toxic text virtually invisible"
  - [section] "We identified and exploited a vulnerability in the tokenization process...by constructing ASCII art fonts entirely from special tokens"
  - [section] "Any ASCII art structure composed of special tokens was way harder to detect for LLMs"
- **Break condition**: If preprocessing strategies split special tokens or if models are trained to recognize special token patterns, the attack becomes less effective.

### Mechanism 3
- **Claim**: Text-filled ASCII fonts exploit the model's inability to reconcile visual structure with semantic content.
- **Mechanism**: In text-filled fonts, the model reads only the filler text while ignoring the larger letter structure visible to humans. This creates a disconnect where the semantic content (filler text) differs from the intended message (large letter shapes), allowing toxic content to bypass detection.
- **Core assumption**: Language models process text linearly and cannot simultaneously recognize both the visual structure and the embedded semantic content.
- **Evidence anchors**:
  - [abstract] "one filled with text" and "models only processed the filler text, ignoring larger letter structure"
  - [section] "The objective of this approach is to design text that is readable to humans when interpreted as individual large letters, while the model reads and processes only the filler text"
  - [section] "models only processed the filler text, ignoring larger letter structure"
- **Break condition**: If OCR-based preprocessing or spatial reasoning is applied before text processing, the model could recognize the visual structure and intended message.

## Foundational Learning

- **Concept**: Tokenization and special tokens in language models
  - **Why needed here**: Understanding how special tokens like <|EOS|> and <|SEP|> function is crucial for grasping how the special token attack works and why it's effective.
  - **Quick check question**: What is the purpose of special tokens in language model tokenization, and how might their unexpected arrangement affect model interpretation?

- **Concept**: Spatial reasoning and visual pattern recognition
  - **Why needed here**: The core vulnerability exploited by ASCII art attacks is the model's lack of spatial reasoning capabilities. Understanding how humans process visual patterns versus how models process text is fundamental to this research.
  - **Quick check question**: How does human visual pattern recognition differ from language model text processing, and why does this difference create a vulnerability?

- **Concept**: Adversarial training and model robustness
  - **Why needed here**: The paper discusses defense mechanisms, including adversarial training. Understanding how adversarial examples can improve model robustness is important for evaluating the effectiveness of proposed defenses.
  - **Quick check question**: How does adversarial training work, and why might it be effective against ASCII art-based attacks?

## Architecture Onboarding

- **Component map**: Art library -> ASCII art generation -> Tokenization -> Model inference -> ASR calculation -> Defense implementation
- **Critical path**: Generate ASCII art using custom fonts → Process through model tokenization → Run inference on toxicity detection models → Analyze results for attack success rate → Implement and test defense mechanisms
- **Design tradeoffs**:
  - Using special tokens vs. regular characters: Special tokens are more effective at evading detection but may be easier to filter with preprocessing
  - Text-filled vs. regular ASCII: Text-filled fonts are harder to detect visually but require more complex generation
  - Adversarial training vs. preprocessing: Training improves robustness but requires significant resources, while preprocessing is more lightweight but may be circumvented
- **Failure signatures**:
  - Models correctly identifying ASCII art as toxic: Defense mechanisms are working
  - Models failing to detect ASCII art but also failing to recognize the underlying message: Attack is successful but model is confused
  - OCR preprocessing failing on text-filled fonts: Defense mechanism needs refinement
- **First 3 experiments**:
  1. Test regular ASCII fonts on a simple model (e.g., Gemma 2-9b) to verify basic attack functionality
  2. Implement special token attack on the same model to compare effectiveness
  3. Apply adversarial training on LLaMA 3.1-8B with a small dataset to observe initial defense effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which special tokens like <|SEP|> and <|endoftext|> disrupt the spatial recognition capabilities of LLMs when embedded in ASCII art?
- Basis in paper: [explicit] The paper demonstrates that ASCII art composed of special tokens is harder for LLMs to detect, but does not fully explain the underlying mechanism of this disruption.
- Why unresolved: The paper provides examples showing the disruption but lacks a detailed explanation of how the tokenization process interacts with spatial recognition in LLMs.
- What evidence would resolve it: A detailed analysis of the tokenization process and its impact on the spatial structure of ASCII art, possibly through visualization of token embeddings or attention maps, would clarify this mechanism.

### Open Question 2
- Question: How can adversarial training be optimized to improve the generalization of LLMs across diverse phrases and complex ASCII art variations?
- Basis in paper: [explicit] The paper notes that adversarial training improved model performance but struggled with generalization across different phrases.
- Why unresolved: While adversarial training showed promise, the paper does not explore advanced techniques or architectures that could enhance generalization beyond specific phrases and fonts.
- What evidence would resolve it: Experiments with different training strategies, such as curriculum learning or meta-learning, could provide insights into optimizing adversarial training for better generalization.

### Open Question 3
- Question: What are the limitations of OCR-based defenses against text-filled ASCII art attacks, and how can they be overcome?
- Basis in paper: [explicit] The paper highlights challenges in achieving consistent OCR results across different fonts and phrases, suggesting the need for fine-tuning.
- Why unresolved: The paper does not explore alternative OCR techniques or preprocessing methods that could enhance the robustness of text-filled ASCII art detection.
- What evidence would resolve it: Comparative studies of various OCR models and preprocessing techniques, along with their effectiveness against different ASCII art configurations, would address these limitations.

## Limitations

- The reported 1.0 Attack Success Rate may not hold for models updated since August 2024
- OCR-based defense lacks comprehensive evaluation of computational overhead and false positive rates
- Findings focus exclusively on English language toxic content, leaving cross-linguistic generalizability unclear

## Confidence

- **High confidence**: The fundamental observation that language models struggle with spatially structured ASCII art due to their text-based tokenization approach
- **Medium confidence**: The effectiveness of special-token-based attacks, as these depend on model-specific tokenization schemes
- **Medium confidence**: The adversarial training defense mechanism's partial effectiveness, as the evaluation was limited to specific datasets
- **Low confidence**: The generalizability of findings to production systems that likely employ multi-layered defense strategies

## Next Checks

1. **Temporal robustness test**: Evaluate the same ASCII art attacks against current versions of the tested models (as of December 2024) to determine if the 1.0 ASR still holds, particularly for GPT-4o and LLaMA 3.1 which are frequently updated

2. **Cross-linguistic validation**: Test the attack effectiveness using toxic phrases translated into multiple languages (e.g., Spanish, Arabic, Chinese) to determine if the vulnerability exists across different linguistic structures and tokenization approaches

3. **Real-world deployment simulation**: Implement a comprehensive defense pipeline combining adversarial training, OCR preprocessing, and character-level analysis on a production-like toxicity detection system, measuring both attack success rates and false positive rates on legitimate ASCII art content