---
ver: rpa2
title: Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch
arxiv_id: '2410.18693'
source_url: https://arxiv.org/abs/2410.18693
tags:
- question
- arxiv
- data
- math
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ScaleQuest introduces a two-stage question-tuning process to enable
  lightweight 7B models to generate high-quality mathematical reasoning data from
  scratch. By first fine-tuning on existing problems (QFT) and then optimizing for
  solvability and difficulty (QPO), the method produces 1 million diverse problem-solution
  pairs without relying on powerful proprietary models or seed data.
---

# Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch

## Quick Facts
- arXiv ID: 2410.18693
- Source URL: https://arxiv.org/abs/2410.18693
- Reference count: 40
- Key outcome: ScaleQuest enables lightweight 7B models to generate 1M+ high-quality mathematical reasoning problems, achieving 29.2%-46.4% gains over baselines

## Executive Summary
ScaleQuest introduces a two-stage question-tuning process that enables lightweight 7B models to generate high-quality mathematical reasoning data from scratch. The method combines question fine-tuning (QFT) to activate question-generation capabilities with question preference optimization (QPO) to enhance solvability and difficulty. Models trained on this data achieve significant performance gains—ranging from 29.2% to 46.4% over baselines—and outperform prior open-source datasets on both in-domain and out-of-domain benchmarks. The approach scales well with data size and generalizes to code reasoning tasks, offering a cost-effective solution for enhancing reasoning capabilities in the open-source community.

## Method Summary
ScaleQuest employs a two-stage fine-tuning process where lightweight 7B models first learn to generate mathematical questions through QFT on existing problems (GSM8K and MATH), then optimize for solvability and difficulty via QPO using preference optimization. The method generates questions from scratch without relying on powerful proprietary models or seed data, filters them through reward-based selection and difficulty-aware sampling, and produces high-quality problem-solution pairs. The resulting datasets are used to train models that outperform those trained on traditional datasets across multiple benchmarks.

## Key Results
- ScaleQuest achieves 55.4% pass@1 on GSM8K and 16.1% on MATH, outperforming prior open-source datasets
- The mixed-generator approach (using both Qwen2-Math-QGen and DeepSeekMath-QGen) shows 2.2% and 4.5% improvements over single-generator datasets
- ScaleQuest scales effectively, with performance improvements from 100K to 1M QA pairs on both GSM8K and MATH
- The method generalizes to code reasoning, improving Codeforces and CodeContests performance by 3.2% and 3.8% respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage fine-tuning (QFT + QPO) activates and improves the lightweight model's ability to generate diverse, high-quality mathematical questions from scratch.
- Mechanism: QFT first primes the model with a small set of existing problems to unlock question-generation capability; QPO then refines these questions to enhance solvability and difficulty using a preference optimization framework.
- Core assumption: The model has latent question-generation potential that can be activated through task-specific fine-tuning.
- Evidence anchors: QFT trains the problem-solving model on GSM8K and MATH problems to activate question-generation capabilities (Section 2.2); QPO optimizes solvability and difficulty using Direct Preference Optimization (DPO) on 10K questions (Section 2.3); experiments show that QFT+QPO outperforms the raw model in solvability and difficulty metrics (Figure 5).

### Mechanism 2
- Claim: Reward-based filtering and difficulty-aware sampling improve response quality and dataset balance.
- Mechanism: For each generated question, five responses are sampled and scored using a reward model (InternLM2-7B-Reward); the highest-scoring response is selected, and overly simple questions are filtered out based on predicted difficulty scores.
- Core assumption: A well-trained reward model can effectively distinguish high-quality reasoning responses from lower-quality ones.
- Evidence anchors: InternLM2-7B-Reward is used to evaluate and select responses with the highest scores (Section 2.5); difficulty scoring is based on fail rates computed using DeepSeekMath-7B-RL to estimate problem hardness (Section 2.4); reward filtering leads to improved instruction-tuning performance over the raw QFT+QPO model (Figure 5).

### Mechanism 3
- Claim: Using multiple question generators increases dataset diversity and improves downstream performance.
- Mechanism: Two models (Qwen2-Math-QGen and DeepSeekMath-QGen) are trained and used to generate questions; the combined dataset yields better results than either generator alone.
- Core assumption: Different generators produce complementary question styles, and diversity is beneficial for model generalization.
- Evidence anchors: Section 3.3 compares datasets generated by a single model vs. a mix; the mixed dataset shows the largest improvement; DeepSeekMath-QGen tends to produce simpler, real-world questions, while Qwen2-Math-QGen produces more challenging, theory-driven ones; Table 3 shows the mixed data outperforms single-generator data in instruction-tuning results.

## Foundational Learning

- Concept: Preference optimization (DPO) and reward modeling
  - Why needed here: QPO and response selection rely on comparing and ranking model outputs, which is exactly what DPO and reward models are designed for.
  - Quick check question: How does DPO use pairwise comparisons to train a model to prefer certain outputs over others?

- Concept: Difficulty estimation via fail rates
  - Why needed here: Difficulty filtering ensures the synthetic dataset contains a balanced mix of easy and hard problems, preventing over-representation of simple questions.
  - Quick check question: How is the fail rate calculated, and why does it correlate with problem difficulty?

- Concept: Fine-tuning on instruction-tuning data
  - Why needed here: The final instruction-tuning step adapts the base model to the synthetic question-answer format, enabling it to generalize to unseen problems.
  - Quick check question: What hyperparameters are critical for effective instruction-tuning on a large synthetic dataset?

## Architecture Onboarding

- Component map:
  QFT model (Qwen2-Math-7B-Instruct or DeepSeekMath-7B-RL) → generates raw questions
  QPO model (Llama3.1-70B-Instruct or GPT-4o-mini) → optimizes solvability/difficulty
  Response generator (Qwen2-Math-7B-Instruct) → produces solutions
  Reward model (InternLM2-7B-Reward) → scores responses
  Difficulty scorer (DeepSeekMath-7B-Base) → estimates problem hardness
  Filter pipeline → removes non-English, unsolvable, and overly simple questions

- Critical path:
  1. QFT fine-tuning → 2. QPO fine-tuning → 3. Generate 2M questions → 4. Apply filters → 5. Generate responses → 6. Apply reward filtering → 7. Final dataset (1M QA pairs)

- Design tradeoffs:
  - QFT data size vs. overfitting: Using 15K problems balances activation of question generation without memorization.
  - QPO optimization direction: Randomly selecting solvability or difficulty per sample prevents simultaneous optimization conflicts.
  - Reward model choice: InternLM2-7B-Reward is used for its strong performance on reasoning tasks, but may be replaced by stronger models.

- Failure signatures:
  - Low solvability ratio → QPO optimization or response generation is ineffective.
  - Unbalanced difficulty distribution → Difficulty sampling or filtering is misconfigured.
  - Low diversity → Only one question generator is used, or generators produce similar outputs.

- First 3 experiments:
  1. Train QFT model on GSM8K+MATH and generate 100 questions; check if solvability and diversity improve over raw model.
  2. Apply QPO with GPT-4o-mini on 1K questions; evaluate solvability/difficulty changes.
  3. Generate 10K questions with QFT+QPO, filter them, and train a small base model; compare performance to baseline dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ScaleQuest performance scale with larger models such as Qwen2.5-Math-72B-Instruct and Llama3.3-70B-Instruct?
- Basis in paper: [explicit] "The effectiveness on larger and more powerful models, such as Qwen2.5-Math-72B-Instruct and Llama3.3-70B-Instruct, remains uncertain."
- Why unresolved: The paper acknowledges this as a limitation but does not conduct experiments with these larger models.
- What evidence would resolve it: Comparative experiments training ScaleQuest datasets on these larger models versus traditional datasets.

### Open Question 2
- Question: Can the QFT and QPO processes be further optimized to improve question quality beyond the current 20K training example convergence point?
- Basis in paper: [inferred] The paper notes that both solvable rate and difficulty improve with training data volume, converging around 20K examples, suggesting potential for further optimization.
- Why unresolved: The paper stops optimization at 20K examples without exploring whether more data or different optimization strategies could yield better results.
- What evidence would resolve it: Experiments varying QFT and QPO training data volumes beyond 20K and testing different optimization objectives or architectures.

### Open Question 3
- Question: What is the optimal balance between diversity and quality when using multiple question generators versus a single generator?
- Basis in paper: [explicit] "We find that the mixed data outperforms the data generated by either single generator" and discusses the complementary nature of DSMath-QGen (simpler, real-world) versus Qwen2-Math-QGen (challenging, theory-driven).
- Why unresolved: While the paper shows mixed data performs better, it doesn't systematically explore the optimal ratio or selection strategy for combining multiple generators.
- What evidence would resolve it: Controlled experiments varying the proportion of questions from different generators and measuring the resulting model performance.

## Limitations

- The approach relies on proprietary models for QPO (GPT-4o-mini) and reward scoring (InternLM2-7B-Reward), limiting full open-source implementation.
- Difficulty estimation using fail rates assumes DeepSeekMath-7B-RL predictions align well with human judgment, which may not generalize across all mathematical domains.
- Scalability claims and generalization to code reasoning are based on limited experimental validation and require more extensive testing.

## Confidence

- High Confidence: The core claim that two-stage fine-tuning (QFT + QPO) improves question generation quality is well-supported by experimental evidence.
- Medium Confidence: The claim that mixed-generator datasets outperform single-generator datasets is supported by the data, but the diversity benefit may be context-dependent.
- Low Confidence: The scalability claims and the assertion that this approach generalizes to code reasoning tasks are based on limited experimental validation.

## Next Checks

1. **Reward Model Alignment Validation**: Conduct human evaluation studies comparing reward model selections against human preferences for response quality, particularly focusing on whether the InternLM2-7B-Reward model's rankings align with human judgment across different difficulty levels and mathematical domains.

2. **Cross-Domain Generalization Test**: Apply the ScaleQuest methodology to a non-mathematical reasoning domain (such as logical reasoning or commonsense reasoning) and evaluate whether the two-stage fine-tuning approach transfers effectively, or if domain-specific modifications are required.

3. **Robustness to Model Architecture Variations**: Test whether the QFT+QPO approach works equally well when starting from different base model architectures (e.g., decoder-only vs. encoder-decoder models) and whether the optimal hyperparameters for QFT and QPO training remain consistent across architectures.