---
ver: rpa2
title: Extracting Heuristics from Large Language Models for Reward Shaping in Reinforcement
  Learning
arxiv_id: '2405.15194'
source_url: https://arxiv.org/abs/2405.15194
tags:
- reward
- action
- environment
- shaping
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using large language models (LLMs) to generate
  heuristics for reward shaping in reinforcement learning. The authors propose abstracting
  Markov decision processes into deterministic or hierarchical forms and using LLMs
  (with or without verifiers) to generate plans in these abstractions.
---

# Extracting Heuristics from Large Language Models for Reward Shaping in Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.15194
- Source URL: https://arxiv.org/abs/2405.15194
- Authors: Siddhant Bhambri; Amrita Bhattacharjee; Durgesh Kalwar; Lin Guan; Huan Liu; Subbarao Kambhampati
- Reference count: 40
- Primary result: LLM-generated plans significantly improve RL sample efficiency through reward shaping

## Executive Summary
This paper proposes using large language models to generate heuristics for reward shaping in reinforcement learning, addressing the challenge of sparse rewards in complex environments. The authors abstract Markov decision processes into deterministic or hierarchical forms and use LLMs (with optional verifiers) to generate plans in these abstractions. These plans are then converted into reward shaping functions that provide intrinsic rewards to RL agents, improving their sample efficiency. Experiments across multiple domains including BabyAI, Household, Mario, and Minecraft demonstrate significant performance gains for PPO, A2C, and Q-learning algorithms.

## Method Summary
The method involves abstracting the original MDP into either deterministic or hierarchical forms, then querying LLMs to generate plans in these abstractions. For deterministic abstraction, LLMs generate text-based plans describing action sequences from text descriptions of the environment. For hierarchical abstraction, PDDL-style representations are used to generate symbolic plans with sub-goals. A verifier component can optionally check and refine LLM-generated plans for validity. The resulting plans are used to construct potential-based reward shaping functions that add intrinsic rewards to the RL agent's training loop, guiding it toward more efficient learning without changing the optimal policy.

## Key Results
- LLM-generated plans provide significant improvements in sample efficiency for PPO, A2C, and Q-learning algorithms
- Partial plans in high-level (symbolic) action spaces are more effective for reward shaping than partial plans in low-level action spaces
- Verifier-augmented LLMs generate more valid (goal-reaching) plans, improving heuristic quality
- Results demonstrate the effectiveness of partial plans as heuristics for reward shaping in sparse reward domains

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated plans provide a heuristic for reward shaping that improves sample efficiency. The LLM generates a plan in either deterministic or hierarchical abstraction, which is used to construct a reward shaping function. This function adds intrinsic rewards to the RL agent's training, guiding it toward more efficient learning. The core assumption is that the LLM can generate a plan that captures useful structure of the underlying MDP, even if incomplete.

### Mechanism 2
Verifier-augmented LLMs generate valid (goal-reaching) plans, improving heuristic quality. A model-based verifier checks each action suggested by the LLM for validity in the current state. If invalid, the verifier provides feedback and prompts the LLM again, iteratively refining the plan until a valid one is found. This assumes the verifier has accurate knowledge of the environment's domain model and can efficiently check action validity.

### Mechanism 3
Partial plans in high-level (symbolic) action space are more effective than partial plans in low-level action space for reward shaping. Hierarchical abstraction allows LLMs to generate plans consisting of sub-goals (symbolic actions). These sub-goals can be used to provide intrinsic rewards to states where those sub-goal fluents are true, guiding the RL agent toward completing sub-tasks efficiently. This assumes sub-goal fluents in the hierarchical abstraction are meaningful and align with the underlying MDP's structure.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The entire framework operates on MDP abstractions (deterministic and hierarchical). Understanding state, action, reward, and transition functions is essential for designing and interpreting the abstractions.
  - Quick check question: What is the difference between a deterministic and a stochastic transition function in an MDP?

- Concept: Potential-based Reward Shaping
  - Why needed here: The reward shaping function is constructed using the Potential-based Reward Shaping (PBRS) approach, which guarantees policy invariance. Understanding this ensures the intrinsic rewards do not alter the optimal policy.
  - Quick check question: What is the mathematical form of the reward shaping function in PBRS, and why does it preserve policy invariance?

- Concept: PDDL (Planning Domain Definition Language)
  - Why needed here: The hierarchical abstraction uses PDDL-style representations for the abstract MDP. Understanding PDDL syntax and semantics is crucial for constructing the abstract MDP and interpreting LLM-generated plans.
  - Quick check question: What are the key components of a PDDL planning problem, and how do they relate to the MDP components?

## Architecture Onboarding

- Component map: LLM -> Verifier -> Reward Shaping Function -> RL Agent -> Environment

- Critical path:
  1. Construct abstract MDP (deterministic or hierarchical)
  2. Query LLM for plan in abstract MDP
  3. (Optional) Use verifier to refine plan
  4. Construct reward shaping function from plan
  5. Train RL agent with shaped rewards
  6. Evaluate sample efficiency improvement

- Design tradeoffs:
  - Deterministic vs. Hierarchical Abstraction: Deterministic preserves low-level action space but may be harder for LLMs; hierarchical simplifies LLM task but provides less granular rewards
  - With vs. Without Verifier: Verifier improves plan quality but adds complexity and requires domain model knowledge
  - Plan Completeness: Complete plans provide better guidance but may be harder to generate; partial plans are easier but may be less effective

- Failure signatures:
  - LLM consistently fails to generate any valid plan
  - Verifier incorrectly identifies valid actions as invalid (or vice versa)
  - Reward shaping function provides misleading signals, harming RL performance
  - RL agent does not converge or converges to suboptimal policy despite shaped rewards

- First 3 experiments:
  1. Run LLM on DoorKey environment without verifier; measure plan length and reward
  2. Run LLM+verifier on Empty-Random environment; compare plan quality to no-verifier case
  3. Train PPO agent on LavaGap with reward shaping from LLM plan; compare sample efficiency to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM architectures (e.g., transformer vs. non-transformer) perform in generating heuristics for reward shaping?
- Basis in paper: [inferred] The paper tests multiple LLM models (GPT-3.5, GPT-4o, Claude Haiku, Llama 3 8B) but does not analyze architectural differences
- Why unresolved: The experiments focus on model performance rather than architectural analysis, and do not compare transformer-based models against non-transformer alternatives
- What evidence would resolve it: Comparative experiments testing various LLM architectures (including non-transformer models) on the same tasks with systematic performance metrics

### Open Question 2
- Question: What is the optimal frequency for applying reward shaping updates during RL training?
- Basis in paper: [explicit] The paper mentions updating the replay buffer with shaped rewards but does not specify timing or frequency of these updates
- Why unresolved: The algorithm description shows a general framework but lacks specific implementation details about when and how often to apply the shaped rewards
- What evidence would resolve it: Ablation studies testing different frequencies of reward shaping updates (e.g., every episode, every N steps, dynamically based on performance) and their impact on sample efficiency

### Open Question 3
- Question: How does the performance scale with environment complexity beyond the tested domains?
- Basis in paper: [inferred] Experiments are limited to specific BabyAI, Household, Mario, and Minecraft environments without exploring more complex or diverse domains
- Why unresolved: The paper does not test environments with significantly different characteristics (e.g., continuous state spaces, larger action spaces, or more complex dynamics)
- What evidence would resolve it: Experiments on a broader range of environments with varying complexity metrics (state dimensionality, action space size, temporal horizon length) to establish scaling relationships

## Limitations
- Effectiveness critically depends on the quality of LLM-generated plans and correctness of abstract MDP representation
- Verifier component adds overhead and requires domain-specific knowledge, limiting generalization across environments
- Results are limited to relatively small, well-defined domains - scalability to complex real-world tasks remains unclear

## Confidence
- High: LLM-generated plans can provide useful reward shaping signals
- Medium: Verifier-augmented LLMs consistently produce valid plans
- Low: Claims about hierarchical abstractions being universally superior

## Next Checks
1. Test LLM + reward shaping on a novel environment with different state/action complexity than BabyAI/Minecraft to assess generalization limits
2. Measure the impact of plan incompleteness by systematically varying the fraction of generated plan steps available to the RL agent
3. Benchmark against domain-specific heuristics rather than just RL baselines to establish the value proposition of LLM-generated guidance