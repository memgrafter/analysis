---
ver: rpa2
title: 'WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge
  Work Tasks'
arxiv_id: '2407.05291'
source_url: https://arxiv.org/abs/2407.05291
tags:
- task
- tasks
- agent
- page
- workarena
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces WorkArena++, a benchmark designed to evaluate
  the performance of autonomous web agents in enterprise settings. It expands upon
  the original WorkArena benchmark with 682 tasks that simulate realistic workflows
  routinely performed by knowledge workers, requiring complex skills like planning,
  reasoning, and information retrieval.
---

# WorkArena++

## Quick Facts
- arXiv ID: 2407.05291
- Source URL: https://arxiv.org/abs/2407.05291
- Reference count: 40
- Key outcome: Introduces WorkArena++ benchmark with 682 tasks to evaluate autonomous web agents in enterprise settings, revealing current agents struggle significantly while humans achieve high success rates

## Executive Summary
WorkArena++ is a comprehensive benchmark designed to evaluate the planning, reasoning, and contextual understanding abilities of autonomous web agents in enterprise settings. The benchmark expands upon the original WorkArena with 682 tasks that simulate realistic workflows performed by knowledge workers, introducing two new difficulty levels (L2 and L3) and five skill categories. A key contribution is the framework for generating thousands of ground-truth observation-action traces, enabling fine-tuning of existing models. Empirical studies with state-of-the-art language models and human workers reveal that current agents struggle significantly with WorkArena++ tasks, while humans achieve high success rates.

## Method Summary
The benchmark evaluates autonomous web agents using BrowserGym environment for interaction with ServiceNow UI components. Agents use chain-of-thought prompting to produce actions based on current observations, with a maximum of 50 time-steps per task. The evaluation includes state-of-the-art language models (GPT-3.5, GPT-4o, GPT-4o-v, Llama3-70b, Mixtral-8x22b) and vision-language models, compared against human workers in a study with 15 participants. Task instances are generated using a standardized evaluation curriculum with seeds 0-9 for evaluation.

## Key Results
- Current state-of-the-art language models and vision-language models struggle significantly with WorkArena++ tasks
- Human workers achieve high success rates on the same task subset, establishing a strong baseline
- Most agent failures stem from challenges in understanding task goals and hallucinating actions, rather than ServiceNow-specific issues
- Agents show particular difficulty with L3 tasks requiring information retrieval from knowledge bases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** WorkArena++ tasks require agents to combine planning, problem-solving, and contextual understanding in enterprise settings.
- **Mechanism:** Tasks are composed of atomic building blocks chained together to form realistic workflows that mimic daily knowledge work.
- **Core assumption:** Agents can successfully navigate and interact with ServiceNow UI components while maintaining task context across multiple steps.
- **Evidence anchors:** [abstract] "WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents." [section 3.1] "Each task consists of a logical combination of simpler atomic tasks, chained together to form a realistic workflow."

### Mechanism 2
- **Claim:** L3 difficulty level increases task complexity by requiring agents to extract goals from tickets and knowledge bases rather than explicit instructions.
- **Mechanism:** Agents must parse natural language tickets, locate relevant knowledge base articles, extract instructions, and apply them to complete tasks.
- **Core assumption:** Agents can effectively perform information retrieval and memorization across multiple sources to understand task requirements.
- **Evidence anchors:** [section 3.1] "The goal is provided to the agent in a manner that mimics how human agents receive work assignments as knowledge workers - through a ticket assigned to them."

### Mechanism 3
- **Claim:** The benchmark's visual diversity through 10 fictitious company themes improves agent generalization.
- **Mechanism:** Random company branding changes UI colors and logos, forcing agents to rely on structural understanding rather than visual memorization.
- **Core assumption:** Agents can generalize across different visual presentations of the same underlying interface structure.
- **Evidence anchors:** [section 3.3] "To better assess agents' ability to generalize across different enterprise settings, we introduce 10 fictitious brands, each with distinct styles of the ServiceNow interface."

## Foundational Learning

- **Concept:** Compositional task planning
  - Why needed here: WorkArena++ tasks are built by composing atomic tasks, requiring agents to understand how to break down complex goals into manageable subtasks.
  - Quick check question: How would you decompose "onboard new employee" into atomic ServiceNow tasks?

- **Concept:** Information retrieval from knowledge bases
  - Why needed here: L3 tasks require agents to locate and extract relevant information from company knowledge bases to understand task requirements.
  - Quick check question: What steps would an agent take to find instructions for expense management in a knowledge base?

- **Concept:** Context window management
  - Why needed here: Tasks require maintaining context across multiple steps and pages, necessitating effective use of context windows for storing and retrieving relevant information.
  - Quick check question: How would you structure a prompt to maintain task context across 10+ steps?

## Architecture Onboarding

- **Component map:** BrowserGym environment -> chat-based interaction -> multimodal observations (HTML, AXTree, screenshots) -> high-level action space -> WorkArena++ task execution
- **Critical path:** Observation → Reasoning → Action selection → Execution → Validation → Reward
- **Design tradeoffs:** Visual diversity vs. visual consistency, task complexity vs. solvability, explicit vs. implicit task instructions, atomic vs. compositional task design
- **Failure signatures:** Stuck in exploration loops, hallucinating non-existent UI elements, failing to retrieve relevant information, inconsistent thought-action patterns, inability to identify infeasible tasks
- **First 3 experiments:**
  1. Run baseline agents on L1 tasks to establish performance floor
  2. Test agent ability to complete single-step atomic tasks
  3. Evaluate agent performance on simple 2-3 step compositional tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the error patterns observed in WorkArena++ translate to other enterprise software platforms beyond ServiceNow?
- Basis in paper: [explicit] The paper states that "most failures are not tied to ServiceNow-specific issues. Instead, they stem from challenges like understanding the task goal, hallucinating actions, etc."
- Why unresolved: While the paper claims that errors are platform-agnostic, no empirical evidence is provided by testing agents on other enterprise platforms.

### Open Question 2
- Question: What is the relationship between context length and performance on WorkArena++ tasks, particularly for the more complex L2 and L3 tasks?
- Basis in paper: [explicit] The paper mentions an ablation study on WorkArena-L1 showing similar performance across different context lengths, but notes "similar performances across different context lengths should not be expected on benchmarks with more complex tasks."
- Why unresolved: The ablation study was only conducted on L1 tasks, which are simpler than L2 and L3.

### Open Question 3
- Question: What specific architectural or training modifications could enable agents to overcome the "low-level understanding in L3 tasks" failure mode identified in the error analysis?
- Basis in paper: [explicit] The error analysis identifies "Low-Level Understanding in L3 Tasks" as a failure mode where "the models fail to comprehend the necessary subtasks fully"
- Why unresolved: The paper identifies this as a failure mode but does not propose specific solutions or test whether architectural modifications could address it.

## Limitations
- Relies heavily on simulated enterprise tasks without real-world deployment validation
- Agent performance tested only within ServiceNow UI, limiting generalizability claims
- Human baseline comparison uses a small sample size (15 participants)
- Benchmark focuses on English-only tasks, limiting applicability to multilingual enterprise environments

## Confidence
- **High confidence** in the benchmark design and task composition methodology - the architectural framework is well-specified and the task generation process is clearly documented
- **Medium confidence** in the reported agent performance metrics - while the evaluation methodology is sound, the results are limited to specific model versions and configurations
- **Low confidence** in generalizability claims - the benchmark's focus on ServiceNow UI and English tasks restricts broader applicability to diverse enterprise environments

## Next Checks
1. Conduct cross-platform validation by testing agents on alternative enterprise systems (Salesforce, SAP) to assess UI-agnostic performance
2. Expand human baseline evaluation to include 50+ participants across different experience levels and cultural backgrounds
3. Implement ablation studies to quantify the impact of visual diversity features on agent generalization capabilities