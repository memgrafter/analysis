---
ver: rpa2
title: The Empirical Impact of Forgetting and Transfer in Continual Visual Odometry
arxiv_id: '2406.01797'
source_url: https://arxiv.org/abs/2406.01797
tags:
- action
- learning
- train
- loss
- replay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates catastrophic forgetting and knowledge transfer
  in neural networks trained continuously for visual odometry in embodied settings.
  Using a dataset of indoor trajectories from Habitat simulator, the research examines
  how a model learns to estimate agent displacement across different environments
  over time.
---

# The Empirical Impact of Forgetting and Transfer in Continual Visual Odometry

## Quick Facts
- arXiv ID: 2406.01797
- Source URL: https://arxiv.org/abs/2406.01797
- Reference count: 40
- One-line primary result: Neural networks trained continuously for visual odometry initially show high forward transfer and good performance, but eventually specialize on current environments at the expense of generalization to past and future scenes, with regularization and larger models proving ineffective.

## Executive Summary
This study investigates catastrophic forgetting and knowledge transfer in neural networks trained continuously for visual odometry in embodied settings. Using a dataset of indoor trajectories from Habitat simulator, the research examines how a model learns to estimate agent displacement across different environments over time. The findings reveal initial high forward transfer and satisfactory performance across environments, followed by a specialization phase where the model improves on current environments at the expense of generalization to past and future scenes. Regularization techniques and increased model capacity prove ineffective in mitigating this behavior, while rehearsal strategies offer mild benefits at substantial memory costs. Providing action information to the model improves convergence but exacerbates specialization, making the model overly reliant on motion expectations rather than visual cues.

## Method Summary
The study uses Habitat simulator to generate indoor trajectories with RGB-D images and ground truth displacement (Δz, Δx, Δθ) for 72 apartments. Models (ResNet-18/50) are trained continually across environments with MSE loss, measuring backward transfer (BWT), forgetting ratio (FR), and forward transfer (FWT) after each environment. The experiments test naive training, action information incorporation, regularization techniques (EWC), rehearsal strategies, and model scaling effects on the forgetting and transfer dynamics.

## Key Results
- Initial phase shows high forward transfer and satisfactory cross-environment performance
- Specialization phase emerges where model improves on current environments but loses generalization to past/future scenes
- Regularization and larger models (ResNet-50) fail to mitigate specialization behavior
- Rehearsal strategies provide mild benefits but require substantial memory costs
- Action information accelerates convergence but increases model reliance on motion expectations over visual cues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early forward transfer occurs because the model first learns general motion patterns that apply across environments, before overfitting to scene-specific cues.
- Mechanism: During initial continual training, the visual odometry model is exposed to diverse indoor scenes. The CNN backbone captures motion primitives (forward, rotate left/right) that are consistent across environments. These general features dominate early performance gains, leading to high BWT and FWT.
- Core assumption: Early experiences present sufficient variation in layout, lighting, and texture to force the model to extract invariant motion cues.
- Evidence anchors:
  - [abstract]: "initial satisfactory performance with high transferability between environments"
  - [section]: "The first few experiences provide a general improvement on the overall task"
  - [corpus]: Weak. Corpus neighbors discuss transfer and forgetting but do not specifically analyze the temporal dynamics of initial generalization.
- Break condition: If early experiences are too similar (e.g., all same apartment style), the model may overfit immediately and never exhibit high initial forward transfer.

### Mechanism 2
- Claim: Action information accelerates convergence but locks the model into motion-prior expectations, reducing reliance on visual context.
- Mechanism: Providing the discrete action as an input channel allows the model to map each action to a fixed expected displacement. The CNN only needs to compensate for noise, so training converges faster. However, when actions fail (e.g., collision), the model's rigid expectation causes large prediction errors because it underweights visual cues.
- Evidence anchors:
  - [abstract]: "Incorporating action information...facilitates quicker convergence but exacerbates specialization"
  - [section]: "the model can confidently predict values close to the expected motion, using the image only to slightly compensate for noise"
  - [corpus]: Weak. Neighbors mention action and embodiment but do not analyze the trade-off between convergence speed and visual cue reliance.
- Break condition: If the action space is continuous or noisy, the model cannot rely on fixed expectations and must depend more on vision, negating the specialization effect.

### Mechanism 3
- Claim: Rehearsal with small buffers cannot fully recover past performance because the buffer undersamples the diversity of each environment.
- Mechanism: When replaying past samples, the model revisits only a fraction of each experience's trajectories. These samples do not represent the full distribution of visual states, especially collisions or rare viewpoints. Thus, the model improves on buffer-covered states but still forgets unseen states from past experiences.
- Evidence anchors:
  - [abstract]: "Rehearsal is instead mildly beneficial but with the addition of a substantial memory cost"
  - [section]: "even when the buffer contains as many samples as the current experience, the improvement is very limited"
  - [corpus]: Weak. Neighbors discuss rehearsal and memory, but not the relationship between buffer size and distributional coverage in embodied VO.
- Break condition: If buffer sampling is done via episodic or generative replay that approximates full experience distributions, the limitation may be mitigated.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The study's central phenomenon is the model's progressive loss of generalization to past scenes after training on new ones.
  - Quick check question: What happens to a model's performance on dataset A after fine-tuning on dataset B without any regularization?

- Concept: Transfer learning and backward/forward transfer metrics
  - Why needed here: The paper quantifies how much knowledge from past or future experiences helps the current model, which is key to interpreting adaptation vs. retention.
  - Quick check question: If a model trained on task X achieves 90% accuracy on task Y without Y-specific training, what does that say about FWT?

- Concept: Continual learning scenarios (domain-incremental, task-incremental, class-incremental)
  - Why needed here: The paper situates visual odometry in the domain-incremental scenario, where task stays constant but data distribution shifts with new scenes.
  - Quick check question: In a domain-incremental setup, does the output space change across experiences?

## Architecture Onboarding

- Component map: RGB-D frames (341x192x8) + optional action channel (1) -> ResNet-18/50 backbone -> Displacement vector (Δz, Δx, Δθ)
- Critical path: Load current and next frame pair -> Concatenate with action channel if enabled -> Forward through backbone -> Compute pose prediction -> MSE loss against ground truth -> Backprop and update
- Design tradeoffs:
  - Model capacity vs. forgetting: Larger models (ResNet-50) did not reduce forgetting, suggesting capacity alone is insufficient.
  - Action info vs. visual reliance: Action channels speed convergence but increase specialization; removing them forces visual learning but slows training.
  - Buffer size vs. training time: Larger replay buffers improve performance but linearly increase epoch time.
- Failure signatures:
  - Persistent high validation loss on past experiences despite low current loss → specialization phase.
  - Training loss decreases but validation loss on future scenes plateaus → lack of forward transfer.
  - Loss spikes when encountering unseen scenes or collision samples → poor generalization.
- First 3 experiments:
  1. Train naive model on scene sequence; plot BWT, FWT, FR to confirm forgetting.
  2. Add action channel; compare convergence speed and test loss vs. no-action baseline.
  3. Apply EWC regularization; check if metrics improve over naive baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the presence of environmental noise and unexpected obstacles affect the model's performance in visual odometry tasks?
- Basis in paper: [explicit] The paper mentions that noise is modeled as a combination of Gaussian noise and sharper disturbances due to collisions with obstacles.
- Why unresolved: The paper does not provide a detailed analysis of how these noise factors specifically impact the model's performance.
- What evidence would resolve it: Conducting experiments that systematically vary the level of environmental noise and obstacle presence, and measuring the resulting impact on model performance.

### Open Question 2
- Question: What are the long-term effects of using rehearsal strategies on model performance and memory usage in continual learning scenarios?
- Basis in paper: [explicit] The paper discusses the use of rehearsal strategies and their impact on mitigating forgetting, but notes the trade-off with increased memory usage and training time.
- Why unresolved: The paper does not explore the long-term sustainability and scalability of rehearsal strategies in more complex or resource-constrained environments.
- What evidence would resolve it: Long-term studies comparing different rehearsal strategies across various scenarios and resource constraints, assessing both performance and memory usage over time.

### Open Question 3
- Question: How does the incorporation of action information influence the model's ability to generalize across different environments?
- Basis in paper: [explicit] The paper shows that providing action information improves convergence but exacerbates specialization, making the model overly reliant on motion expectations.
- Why unresolved: The paper does not explore whether alternative methods of incorporating action information could mitigate this specialization without losing the benefits of improved convergence.
- What evidence would resolve it: Experiments testing different methods of integrating action information, such as adaptive weighting or context-aware integration, to balance specialization and generalization.

### Open Question 4
- Question: What is the impact of increasing model scale on the performance and efficiency of continual learning in visual odometry?
- Basis in paper: [explicit] The paper investigates the impact of model scale using a larger ResNet50 model and finds no significant improvement in mitigating forgetting or improving transfer.
- Why unresolved: The paper does not explore whether other architectural changes or hybrid models could provide better results.
- What evidence would resolve it: Comparative studies of various model architectures and scales, including hybrid models, to identify configurations that optimize both performance and efficiency in continual learning tasks.

## Limitations

- The study's conclusions about regularization and model scaling ineffectiveness are based on limited architectural variations (ResNet-18 vs ResNet-50) and may not generalize to alternative network designs.
- The rehearsal strategy's "mild benefits" are qualified by memory cost trade-offs, but the paper doesn't quantify practical feasibility for real-world robotics applications.
- The analysis doesn't explore alternative regularization techniques like synaptic intelligence or dynamic architecture adjustment that could potentially yield better mitigation of specialization.

## Confidence

- High confidence: The observation of specialization phases following initial forward transfer is well-supported by the BWT/FWT/FR metrics and consistent with established catastrophic forgetting literature.
- Medium confidence: The claim that action information exacerbates specialization is supported by convergence speed metrics, but the underlying mechanism (rigid motion expectations vs. visual learning) requires further empirical validation through ablation studies.
- Medium confidence: The conclusion that larger models and regularization fail to prevent forgetting is based on specific architectural choices and may not generalize to alternative network designs or regularization schemes.

## Next Checks

1. **Cross-environment validation**: Test the specialization hypothesis by evaluating models on held-out scenes from the same environment category to distinguish between true forgetting and adaptation to specific visual features.

2. **Alternative rehearsal strategies**: Implement episodic memory sampling with weighted importance sampling or generative replay to test whether distributional coverage, not just buffer size, determines rehearsal effectiveness.

3. **Continuous action space experiments**: Replace discrete actions with noisy or continuous action representations to empirically verify whether the specialization effect diminishes when rigid motion expectations cannot be formed.