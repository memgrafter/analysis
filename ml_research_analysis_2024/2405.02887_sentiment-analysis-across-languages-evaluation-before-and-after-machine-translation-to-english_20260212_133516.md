---
ver: rpa2
title: 'Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation
  to English'
arxiv_id: '2405.02887'
source_url: https://arxiv.org/abs/2405.02887
tags:
- sentiment
- translation
- languages
- machine
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the performance of transformer models for sentiment
  analysis across multilingual datasets and machine-translated text. BERT and XLM-RoBERTa
  models were fine-tuned on 50,000 samples from French, German, Spanish, Japanese,
  and Chinese Amazon reviews.
---

# Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English

## Quick Facts
- arXiv ID: 2405.02887
- Source URL: https://arxiv.org/abs/2405.02887
- Reference count: 21
- Primary result: XLM-RoBERTa outperforms BERT across languages; machine translation preserves performance for European languages but degrades it for Asian languages

## Executive Summary
This study compares transformer models (BERT and XLM-RoBERTa) for sentiment analysis across five languages (French, German, Spanish, Japanese, Chinese) using Amazon reviews. Models were fine-tuned on 50,000 samples per language and evaluated both on original text and machine-translated English versions (20,000 samples per language). XLM-RoBERTa consistently outperformed BERT across all languages. Machine translation preserved performance for European languages but significantly degraded results for Asian languages. No model reached the English baseline F1 score of 0.91, suggesting limitations in translation quality or fine-tuning data size.

## Method Summary
The study fine-tuned BERT and XLM-RoBERTa models on 50,000 Amazon review samples from each of five languages. An additional 20,000 samples per language were machine-translated to English using OPUS-MT. Models were evaluated on both original and translated text using F1-score for binary sentiment classification and multi-class star rating prediction. The multilingual corpus was sourced from the "hungnm/multilingual-amazon-review-sentiment-processed" dataset.

## Key Results
- XLM-RoBERTa slightly outperformed BERT across all languages
- Machine translation did not significantly affect performance for European languages (Spanish, German, French)
- Machine translation significantly degraded performance for Asian languages (Chinese, Japanese)
- No model reached the English baseline F1 score of 0.91

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning XLM-RoBERTa on multilingual Amazon reviews enables better cross-lingual transfer than monolingual BERT models through its shared vocabulary architecture. XLM-RoBERTa uses masked language modeling during pre-training on large multilingual corpora, creating language-agnostic representations that can be applied across languages with less domain drift. This works because European languages share structural similarities that facilitate transfer.

### Mechanism 2
Machine translation preserves sentiment for European languages because they share similar syntax (SVO order) and vocabulary overlap with English. The OPUS-MT transformer-based model translates these sentences while maintaining sentiment-carrying phrases that English-trained models can interpret accurately. This mechanism breaks when translation introduces cultural or syntactic distortion that loses sentiment meaning.

## Foundational Learning

**Multilingual Pre-training**: Why needed - creates language-agnostic representations for cross-lingual transfer. Quick check - verify shared vocabulary size across languages.

**Cross-lingual Embeddings**: Why needed - enables semantic meaning transfer between languages. Quick check - measure embedding alignment scores between language pairs.

**Sentiment Preservation**: Why needed - ensures translated text maintains original sentiment polarity. Quick check - compare sentiment scores pre/post-translation for sample sentences.

**Machine Translation Quality**: Why needed - poor translation directly impacts downstream sentiment analysis performance. Quick check - calculate BLEU scores for translated text.

## Architecture Onboarding

**Component Map**: Multilingual Reviews Corpus -> Fine-tuning (BERT/XLM-RoBERTa) -> Sentiment Classification -> F1 Evaluation

**Critical Path**: Translation Quality -> Fine-tuning Data Size -> Cross-lingual Transfer Capability -> Final Sentiment Performance

**Design Tradeoffs**: Multilingual models sacrifice some language-specific precision for broader coverage; machine translation adds preprocessing overhead but enables use of English-trained models.

**Failure Signatures**: Significant performance drop after translation indicates either translation quality issues or fundamental semantic misalignment between languages.

**First Experiments**:
1. Fine-tune XLM-RoBERTa on French reviews and evaluate on German reviews to test cross-lingual transfer
2. Translate Spanish reviews to English and compare performance with original Spanish reviews
3. Vary fine-tuning dataset size (10k, 50k, 100k) to determine impact on performance

## Open Questions the Paper Calls Out

**Open Question 1**: How do different fine-tuning dataset sizes affect sentiment analysis performance across languages? The paper suggests larger datasets may be needed to reach English baseline performance but only tested with 50,000 samples.

**Open Question 2**: What specific linguistic features in Asian languages cause performance degradation after machine translation? The study observed performance drops but didn't analyze which features (word order, morphology, context) contribute to degradation.

**Open Question 3**: How do domain-specific knowledge graphs impact sentiment analysis performance across languages? The paper mentions ConceptNet as a potential improvement but didn't incorporate external knowledge resources in the study.

## Limitations

- No explicit translation quality metrics or evaluation of translation artifacts
- Absence of detailed hyperparameter settings limits reproducibility
- Study didn't explore whether larger fine-tuning datasets could bridge performance gaps
- No analysis of specific linguistic features causing Asian language degradation

## Confidence

**High confidence**: XLM-RoBERTa outperforms BERT across languages due to shared vocabulary mechanism and multilingual pre-training literature.

**Medium confidence**: Machine translation doesn't significantly impact European languages, but degradation for Asian languages lacks analysis of underlying causes.

## Next Checks

1. Conduct human evaluation of translated Chinese and Japanese samples to identify specific translation errors causing sentiment loss; compare translation quality metrics between European and Asian language pairs.

2. Systematically vary fine-tuning sample sizes (10k, 25k, 50k, 100k) for each language to determine if performance approaches English baseline, particularly for Asian languages.

3. Use embedding alignment techniques (like VecMap) to quantify how well sentiment-bearing words from non-English languages align with English translations, explaining differential transfer performance.