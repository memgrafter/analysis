---
ver: rpa2
title: 'Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression'
arxiv_id: '2407.04965'
source_url: https://arxiv.org/abs/2407.04965
tags:
- unstructured
- wanda
- gblm
- compression
- semistructured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates how model compression techniques
  impact the safety of large language models (LLMs) across multiple dimensions, including
  degeneration harm (toxicity and bias in generation), representational harm (bias
  in discriminative tasks), dialect bias, and language modeling performance. The authors
  evaluate a wide range of compression methods, including unstructured pruning, semi-structured
  pruning, and quantization, across various compression rates.
---

# Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression

## Quick Facts
- arXiv ID: 2407.04965
- Source URL: https://arxiv.org/abs/2407.04965
- Authors: Zhichao Xu; Ashim Gupta; Tao Li; Oliver Bentham; Vivek Srikumar
- Reference count: 40
- Primary result: Model compression methods affect LLM safety differently across dimensions - quantization preserves bias better than pruning at moderate rates

## Executive Summary
This paper systematically investigates how model compression techniques impact LLM safety across multiple dimensions including degeneration harm (toxicity and bias in generation), representational harm (bias in discriminative tasks), dialect bias, and language modeling performance. The authors evaluate unstructured pruning, semi-structured pruning, and quantization across various compression rates. Key findings reveal that while compression may reduce degeneration harm due to generation quality degradation, it can exacerbate representational harm. Quantization methods generally preserve bias and toxicity better than pruning at moderate compression rates, while pruning degrades quickly and exhibits divergent impacts on different protected groups.

## Method Summary
The study applies compression methods (unstructured pruning, semi-structured pruning, quantization) to base models LLAMA-2 and TÜLU-2 (7B and 13B variants). Models are evaluated across multiple safety dimensions using datasets including REALTOXICITY PROMPTS, TOXIGEN, ADVPROMPTSET, BOLD, HOLISTICBIASR, BBQ, UNQOVER, and TRUTHFULQA. Performance metrics include toxicity rates, bias scores, perplexity, downstream task accuracy, and ROUGE scores. The evaluation pipeline assesses degeneration harm (toxicity in generation), representational harm (bias in discriminative tasks), dialect bias (AAE vs standard English), and overall performance.

## Key Results
- Quantization methods preserve bias and toxicity better than pruning at moderate compression rates (e.g., 50%)
- Higher compression rates lead to divergent impacts on different protected groups with no clear pattern
- Generation quality degradation masks toxicity in compressed models through disfluent outputs
- Supervised fine-tuning reduces degeneration harm but not representational harm

## Why This Works (Mechanism)

### Mechanism 1
Quantization preserves bias and toxicity better than pruning at moderate compression rates because quantization scales weights rather than removing them, maintaining the relative importance of different parameter groups. This preserves learned representations that encode bias patterns, while pruning removes entire parameter groups which can disrupt these representations. Core assumption: bias patterns are distributed across parameters in a way that quantization preserves but pruning disrupts.

### Mechanism 2
Generation quality degradation masks toxicity in compressed models as compression increases, models produce more disfluent outputs that are classified as non-toxic by model-based toxicity classifiers, even though the underlying bias patterns remain. Core assumption: model-based toxicity classifiers rely on coherent language structure to identify toxic content.

### Mechanism 3
Supervised fine-tuning reduces degeneration harm but not representational harm because SFT specifically targets generation behaviors through instruction-following training, reducing toxic outputs, but doesn't address the underlying representational biases in the model's understanding of different groups. Core assumption: SFT primarily modifies generation patterns rather than fundamental representations.

## Foundational Learning

- Concept: Perplexity as training loss proxy
  - Why needed here: Compression methods traditionally optimize for perplexity, but this paper shows it's insufficient for safety evaluation
  - Quick check question: What does perplexity measure and why might it fail to capture safety-related model behaviors?

- Concept: Bias vs toxicity distinction
  - Why needed here: The paper evaluates two distinct types of harm - degeneration (toxicity in generation) and representational (bias in discriminative tasks)
  - Quick check question: How do degeneration and representational harms differ in their manifestation and evaluation?

- Concept: Structured vs unstructured pruning
  - Why needed here: Different pruning approaches have different impacts on model safety
  - Quick check question: What are the key differences between structured and unstructured pruning, and how might these affect bias preservation?

## Architecture Onboarding

- Component map: Base LLM → Compression method → Safety evaluation pipeline → Performance metrics
- Critical path: Compression → Safety evaluation (degeneration, representational, dialect) → Performance evaluation (perplexity, downstream tasks)
- Design tradeoffs: Safety preservation vs compression ratio, bias preservation vs generation quality, dialect bias vs general performance
- Failure signatures:
  - Sharp performance drop with minimal safety improvement
  - Preserved perplexity but degraded safety metrics
  - Inconsistent bias patterns across protected groups
- First 3 experiments:
  1. Compare perplexity and safety metrics across compression ratios for quantization vs pruning
  2. Evaluate bias patterns for individual protected groups vs aggregated metrics
  3. Test dialect bias preservation across compression methods using AAE vs standard English datasets

## Open Questions the Paper Calls Out

### Open Question 1
How does the order of applying pruning and supervised fine-tuning (SFT) affect representational harm in compressed LLMs? The paper observes that Prune→SFT models have lower degeneration harm but higher representational harm compared to SFT→Prune models, with no clear explanation for this pattern.

### Open Question 2
Do compressed LLMs exhibit dialect bias differently across language varieties beyond African American English (AAE) versus "standard" English? The paper finds that even heavily compressed models maintain lower perplexity on "standard" English compared to AAE, but only examines these two dialect pairs.

### Open Question 3
How do larger LLM sizes (30B, 70B) respond to compression in terms of safety metrics compared to 7B and 13B models? The authors explicitly state they limited their evaluation to 7B and 13B models due to computational constraints.

### Open Question 4
Can model-based evaluation metrics reliably detect degeneration harm in compressed LLMs when generation quality degrades? The paper observes that as pruning increases, generations become disfluent, and model-based toxicity classifiers may mark these as non-toxic simply because they're not coherent language.

## Limitations
- Limited scope of bias evaluation focusing primarily on gender and racial biases
- Model-specific findings limited to LLAMA-2 and TÜLU-2 architectures
- Evaluation methodology constraints with model-based toxicity classifiers sensitive to generation quality
- Calibration criteria ambiguity without specific implementation details

## Confidence

- High Confidence: Claims about quantization preserving bias better than pruning at moderate compression rates (50%)
- Medium Confidence: Claims about generation quality masking toxicity through disfluent outputs
- Medium Confidence: Claims about SFT reducing degeneration harm but not representational harm

## Next Checks

1. Cross-Model Validation: Repeat compression experiments on different LLM architecture (e.g., Mistral or BLOOM) to verify generalizability of safety patterns beyond LLAMA-2 and TÜLU-2.

2. Alternative Toxicity Detection: Implement human evaluation or use multiple independent toxicity classifiers to verify that generation quality degradation isn't masking true toxicity levels.

3. Expanded Bias Evaluation: Add evaluation of additional bias dimensions (religious, socioeconomic, intersectional) and test on diverse cultural datasets to determine if compression impacts are consistent across different types of representational harm.