---
ver: rpa2
title: 'Cherry on the Cake: Fairness is NOT an Optimization Problem'
arxiv_id: '2406.16606'
source_url: https://arxiv.org/abs/2406.16606
tags:
- fairness
- such
- theorem
- problem
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that optimizing group fairness metrics\
  \ in machine learning can inherently lead to cherry-picking\u2014where models intentionally\
  \ make errors within the same demographic group to superficially meet fairness constraints.\
  \ Using fair cake-cutting theory, the authors show that optimal solutions to fairness\
  \ problems may be forced to cherry-pick, undermining the intended purpose of fairness\
  \ measures."
---

# Cherry on the Cake: Fairness is NOT an Optimization Problem

## Quick Facts
- arXiv ID: 2406.16606
- Source URL: https://arxiv.org/abs/2406.16606
- Reference count: 40
- Primary result: Optimizing group fairness metrics can lead to cherry-picking behavior that undermines fairness goals

## Executive Summary
This paper demonstrates that optimizing group fairness metrics in machine learning can inherently lead to cherry-picking—where models intentionally make errors within the same demographic group to superficially meet fairness constraints. Using fair cake-cutting theory, the authors show that optimal solutions to fairness problems may be forced to cherry-pick, undermining the intended purpose of fairness measures. They prove that certain fairness measures always allow for non-cherry-picking solutions, while others, like Predictive Parity, can lead to cherry-picking. The study highlights the importance of considering fairness beyond mathematical optimization and advocates for a post-processing approach to fairness.

## Method Summary
The authors employ fair cake-cutting theory as a theoretical framework to analyze the relationship between fairness optimization and cherry-picking behavior. They develop mathematical proofs showing when certain fairness metrics can lead to cherry-picking and when they cannot. The paper distinguishes between different group fairness measures and their susceptibility to cherry-picking, particularly focusing on the relationship between group fairness and individual fairness within demographic groups.

## Key Results
- Optimizing group fairness metrics can force models to make intentional errors within the same demographic group to meet fairness constraints
- Certain fairness measures (like Demographic Parity) always allow for non-cherry-picking solutions, while others (like Predictive Parity) can lead to cherry-picking
- The paper advocates for a post-processing approach to fairness rather than optimization-based methods

## Why This Works (Mechanism)
The mechanism works because group fairness optimization creates incentives for models to manipulate their error distribution within groups to meet aggregate metrics, even if this means making suboptimal decisions for individual members of those groups. This is analogous to fair cake-cutting problems where the most "fair" division from a group perspective may require cherry-picking certain portions.

## Foundational Learning
- Fair cake-cutting theory: Understanding the mathematical foundations of fair division problems and their application to fairness in ML
  - Why needed: Provides the theoretical framework for analyzing when fairness optimization can lead to undesirable behaviors
  - Quick check: Can you explain the difference between envy-freeness and proportionality in cake-cutting?

- Group vs individual fairness: The tension between ensuring fairness across demographic groups versus treating similar individuals similarly
  - Why needed: Critical for understanding why optimizing group metrics can harm individual fairness
  - Quick check: Can you articulate when group fairness and individual fairness are aligned or in conflict?

- Fairness metric properties: Understanding the mathematical properties of different fairness measures (Demographic Parity, Equal Opportunity, Predictive Parity)
  - Why needed: Different metrics have different vulnerabilities to cherry-picking behavior
  - Quick check: Can you explain which fairness metrics are susceptible to cherry-picking and why?

## Architecture Onboarding
- Component map: Fairness metric selection -> Optimization method -> Error distribution analysis -> Post-processing application
- Critical path: Define fairness metric → Apply optimization → Analyze error patterns → Apply post-processing
- Design tradeoffs: Optimizing for group fairness vs. avoiding cherry-picking, computational cost of post-processing
- Failure signatures: Models that appear fair by group metrics but show suspicious error patterns within groups
- First experiments: 1) Test different fairness metrics on a simple classification problem 2) Analyze error distribution within groups 3) Apply post-processing and measure changes

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims rely on abstract fair cake-cutting models that may not fully capture real-world ML deployment complexity
- Proof of cherry-picking vulnerability depends on assumptions about group definitions and error patterns that may not hold in practice
- Proposed post-processing approach lacks empirical validation on real datasets

## Confidence
- High: Mathematical proofs about cake-cutting analogies and their relationship to fairness metrics
- Medium: Claim that certain fairness measures inherently enable cherry-picking behavior
- Low: Effectiveness of proposed post-processing solution in real-world applications

## Next Checks
1. Empirical validation of cherry-picking behavior on real-world ML datasets across multiple domains
2. Comparison of post-processing approach against state-of-the-art fairness optimization techniques on benchmark fairness datasets
3. User studies to assess whether stakeholders can detect and measure cherry-picking behavior in deployed systems