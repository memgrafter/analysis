---
ver: rpa2
title: '$i$REPO: $i$mplicit Reward Pairwise Difference based Empirical Preference
  Optimization'
arxiv_id: '2405.15230'
source_url: https://arxiv.org/abs/2405.15230
tags:
- irepo
- human
- preference
- responses
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: iREPO introduces an iterative preference optimization framework
  for aligning large language models with human preferences. It avoids explicit reward
  modeling by directly regressing the pairwise implicit reward difference to the logit
  of empirical preferences obtained from LLM or human annotators.
---

# $i$REPO: $i$mplicit Reward Pairwise Difference based Empirical Preference Optimization

## Quick Facts
- arXiv ID: 2405.15230
- Source URL: https://arxiv.org/abs/2405.15230
- Authors: Long Tan Le; Han Shu; Tung-Anh Nguyen; Choong Seon Hong; Nguyen H. Tran
- Reference count: 40
- Primary result: Achieves superior alignment performance over iterative DPO and IPO on Phi-2 and Mistral-7B models across multiple benchmarks

## Executive Summary
$i$REPO introduces an iterative preference optimization framework that aligns large language models with human preferences without requiring explicit reward modeling. The method directly regresses the pairwise implicit reward difference to the logit of empirical preferences obtained from LLM or human annotators. By employing Zermelo rankings to select the strongest and weakest responses in each batch, iREPO captures the largest reward gaps and updates the policy through a novel regression-based loss function. The approach demonstrates superior performance on established benchmarks while offering theoretical guarantees of zero distribution gap under ideal conditions.

## Method Summary
$i$REPO operates through an iterative self-generation process where the current policy generates response pairs for given prompts, which are then evaluated by AI annotators to obtain pairwise preferences. Zermelo's ranking algorithm identifies the strongest and weakest responses from each batch, and a regression-based loss function minimizes the difference between the implicit reward pairwise difference and the logit of empirical preferences. This process repeats across multiple iterations, progressively refining the aligned policy. The method avoids explicit reward modeling by directly optimizing for the empirical preference distribution, theoretically achieving zero distribution gap when the self-generated data distribution matches the target human preference distribution.

## Key Results
- Outperforms iterative DPO and IPO on Language Model Evaluation Harness benchmarks (ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, GSM8K)
- Demonstrates superior performance on Multi-turn Benchmark (MT-Bench) for both Phi-2 and Mistral-7B models
- Ablation studies show the logit term is crucial, with omission significantly degrading performance
- Shows robustness with 9 annotators performing slightly better than 5 or 21, indicating optimal annotator count

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method achieves alignment by regressing the implicit reward pairwise difference directly to the logit of empirical preferences, avoiding explicit reward modeling.
- Mechanism: By using Zermelo rankings to select the strongest and weakest responses, the method captures the largest reward gap without needing to model the reward function explicitly. This regression-based loss function aligns the policy with empirical human preferences.
- Core assumption: The pairwise preference follows a Bradley-Terry model where the probability of preference can be expressed through exponential of reward differences.
- Evidence anchors: [abstract], [section 4.2], [corpus]

### Mechanism 2
- Claim: The iterative self-generation of data with AI annotators closes the distribution gap between the model's outputs and the target human preference distribution.
- Mechanism: By generating new responses with the current policy and having them evaluated by AI annotators, the training data becomes more representative of the target distribution over time, reducing the need for pre-collected static datasets.
- Core assumption: The self-generated data distribution converges to the target distribution as the policy improves through iterations.
- Evidence anchors: [abstract], [section 4.2], [corpus]

### Mechanism 3
- Claim: The logit of empirical human preference captures the nuanced quality gap between responses better than binary preferences.
- Mechanism: By using the logit transformation of the empirical preference probability, the method quantifies the degree of preference difference rather than just the order, leading to more precise alignment.
- Core assumption: The logit transformation effectively represents the strength of preference differences between responses.
- Evidence anchors: [section 4.2], [section 5.3], [corpus]

## Foundational Learning

- Concept: Bradley-Terry model for pairwise comparisons
  - Why needed here: The method relies on modeling pairwise preferences as a function of the underlying reward differences, which is exactly what the Bradley-Terry model provides.
  - Quick check question: In a Bradley-Terry model, if response A is preferred over response B with probability p, and B over C with probability q, what is the probability of A being preferred over C?

- Concept: Maximum likelihood estimation for parameter estimation
  - Why needed here: The method uses MLE to estimate the strengths of responses from pairwise comparison data, which is essential for the Zermelo ranking process.
  - Quick check question: When maximizing the log-likelihood for pairwise comparisons, what is the update rule for the strength parameter of an item that wins more often than it loses?

- Concept: Zermelo's algorithm for ranking from pairwise comparisons
  - Why needed here: The method uses an accelerated version of Zermelo's algorithm to efficiently rank responses based on pairwise comparison data and select the strongest/weakest responses for training.
  - Quick check question: What is the key difference between the traditional Zermelo algorithm and Newman's accelerated version in terms of computational efficiency?

## Architecture Onboarding

- Component map: Data generation pipeline -> AI annotator interface -> Zermelo ranking module -> Loss computation -> Policy update
- Critical path: Data generation → AI annotation → Zermelo ranking → Loss computation → Policy update → Next iteration
- Design tradeoffs:
  - Using AI annotators vs human annotators: Cost vs quality
  - Number of responses per prompt: More responses give better rankings but increase computation
  - Number of annotators: More annotators improve reliability but increase cost
- Failure signatures:
  - Slow convergence: May indicate poor AI annotator quality or insufficient response diversity
  - Degraded performance on benchmarks: Could suggest the model is overfitting to the annotator preferences
  - High variance in rankings: Might indicate inconsistent AI annotator behavior
- First 3 experiments:
  1. Baseline comparison: Run SFT, DPO, and IPO on the same dataset to establish performance baselines
  2. Single iteration test: Run iREPO for one iteration with a small dataset to verify the ranking and loss computation
  3. Full iterative run: Execute the complete iREPO pipeline on Phi-2 or Mistral-7B and evaluate on LM-Eval-Harness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of LLM annotators required to achieve maximum performance in iREPO?
- Basis in paper: [explicit] The paper mentions that increasing the number of annotators from 5 to 21 has minimal impact on performance, with 9 annotators slightly outperforming others.
- Why unresolved: The study only tested a limited range of annotator numbers (5, 9, 15, 21) and found minimal differences. It's unclear if there's an upper limit beyond which additional annotators provide no benefit, or if the optimal number varies with different model sizes or task complexities.
- What evidence would resolve it: Systematic testing of iREPO performance with a broader range of annotator numbers (e.g., 3, 7, 11, 13, 17, 25) across multiple model sizes and task types, measuring performance gains and computational costs.

### Open Question 2
- Question: How does the performance of iREPO compare to traditional preference optimization methods when applied to real-world, noisy human feedback data?
- Basis in paper: [inferred] The paper demonstrates iREPO's superiority over baselines using LLM-generated responses and AI annotators, but doesn't test it with actual human feedback data that may contain inconsistencies or biases.
- Why unresolved: The study uses AI annotators and synthetic data, which may not fully capture the complexities and noise present in real human feedback. The robustness of iREPO in handling inconsistent or contradictory human preferences remains untested.
- What evidence would resolve it: Comparative experiments using iREPO and baseline methods on large-scale datasets of actual human feedback, measuring performance metrics and analyzing how each method handles contradictory or noisy annotations.

### Open Question 3
- Question: What is the impact of response diversity on iREPO's performance, and is there an optimal diversity level for generating training data?
- Basis in paper: [inferred] The paper mentions generating responses with different sampling parameters but doesn't explore how response diversity affects the model's learning and alignment quality.
- Why unresolved: While the paper uses varied sampling parameters to generate responses, it doesn't investigate how the diversity of these responses influences the model's ability to learn from empirical preferences. Too little diversity might limit learning, while too much could introduce noise.
- What evidence would resolve it: Controlled experiments varying the diversity of generated responses (e.g., using different temperature settings, top_p values) and measuring iREPO's performance across iterations, identifying if there's a sweet spot for diversity that maximizes learning efficiency and alignment quality.

## Limitations

- The method assumes pairwise preferences follow a Bradley-Terry model, which may not hold for real-world human preferences that can be context-dependent or cyclic
- Empirical validation relies on AI-generated preference data rather than human annotations, raising questions about generalization to actual human preferences
- Computational cost increases with the number of responses per prompt and number of annotators, potentially limiting scalability

## Confidence

- Theoretical guarantees: Medium - relies on ideal assumptions that may not hold in practice
- Empirical effectiveness: Medium - demonstrated on AI-generated data, pending human preference validation
- Reproducibility: Medium - implementation details of Zermelo ranking and AI annotator configuration are not fully specified

## Next Checks

1. Conduct human preference studies comparing iREPO-aligned models against baselines using the same prompts to verify that AI-annotator alignment translates to human preference
2. Test the robustness of iREPO under different pairwise preference distributions that violate the Bradley-Terry assumptions, including scenarios with cyclic preferences or context-dependent preferences
3. Perform ablation studies on the number of responses per prompt and number of annotators to quantify the impact on both performance and computational cost