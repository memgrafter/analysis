---
ver: rpa2
title: More Compute Is What You Need
arxiv_id: '2404.19484'
source_url: https://arxiv.org/abs/2404.19484
tags:
- zhang
- training
- scaling
- compute
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new scaling law suggesting that large language
  model performance scales primarily with the total compute used for training, rather
  than the specific allocation between model size and dataset size. By plotting compression
  scores against compute across multiple open-source models, the authors find a linear
  relationship in log scale, challenging the traditional Chinchilla-optimal scaling
  approach.
---

# More Compute Is What You Need

## Quick Facts
- **arXiv ID**: 2404.19484
- **Source URL**: https://arxiv.org/abs/2404.19484
- **Reference count**: 23
- **Primary result**: Proposes a unified scaling law where model performance scales primarily with total compute (N × D) rather than the specific allocation between model size and dataset size.

## Executive Summary
This paper challenges the traditional Chinchilla-optimal scaling approach by proposing a new unified scaling law where large language model performance depends primarily on total compute used for training rather than the specific allocation between model size and dataset size. Through analysis of compression scores (bits per character) across multiple open-source models, the authors demonstrate a linear relationship with compute on a log scale, suggesting that the product of model parameters and training tokens is the dominant factor. The findings suggest that for inference efficiency, training should prioritize smaller models with larger datasets, and that once high-quality web datasets are exhausted, scaling model size may be the only path to further improvements.

## Method Summary
The paper collects compression scores (BPC) for various open-source models including Falcon, Llama families, Qwen, DeepSeek, and Yi, then calculates compute as the product of model parameters and training tokens. By plotting these compression scores against log-scale compute and fitting a linear regression, the authors establish a unified scaling law that challenges the traditional approach of optimizing the token-per-parameter ratio. The method validates predictions against known model performances and explores implications for inference efficiency and future scaling directions.

## Key Results
- Demonstrates a linear relationship between compression scores (BPC) and log-scale compute (model parameters × training tokens) across open-source models
- Shows that for inference efficiency, smaller models trained with larger datasets perform comparably to larger models with smaller datasets
- Predicts that once high-quality web datasets are exhausted, scaling model size may be the only remaining path for performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model performance depends primarily on total compute rather than the specific allocation between model size and dataset size
- Mechanism: When plotting compression scores against compute on a log scale, the relationship becomes linear, suggesting that the product of model size and training tokens (N × D) is the dominant factor
- Core assumption: Compression efficiency (BPC) linearly correlates with model performance across different models and tokenizers
- Evidence anchors:
  - [abstract] "Using this unified scaling law, we predict that (a) for inference efficiency, training should prioritize smaller model sizes and larger training datasets..."
  - [section] "Using this unified scaling law, we predict that (a) for inference efficiency, training should prioritize smaller model sizes and larger training datasets..."
  - [corpus] Weak corpus evidence; neighboring papers discuss scaling laws but don't directly support the linear BPC-compute relationship
- Break condition: When data quality becomes a limiting factor or when the token-per-parameter ratio falls outside a practical range

### Mechanism 2
- Claim: For inference efficiency, training should prioritize smaller models with larger datasets
- Mechanism: Since performance scales with compute, smaller models require fewer FLOPs per inference, making them more efficient for deployment while maintaining comparable performance
- Core assumption: The relationship between compute and performance holds across different model sizes
- Evidence anchors:
  - [abstract] "Using this unified scaling law, we predict that (a) for inference efficiency, training should prioritize smaller model sizes and larger training datasets..."
  - [section] "Our findings suggest that: 1. For inference efficiency, training should prioritize smaller model sizes and larger training datasets."
  - [corpus] No direct corpus evidence supporting this inference efficiency claim
- Break condition: When dataset size becomes impractical to scale further due to data quality constraints

### Mechanism 3
- Claim: Once high-quality web datasets are exhausted, scaling model size may be the only path to further improvements
- Mechanism: The linear relationship between BPC and compute suggests that without additional high-quality data, increasing compute by scaling model size is the remaining option
- Core assumption: The exhaustion of high-quality web datasets is imminent or has already occurred
- Evidence anchors:
  - [abstract] "(b) assuming the exhaustion of available web datasets, scaling the model size might be the only way to further improve model performance."
  - [section] "Assuming the exhaustion of available web datasets [17], scaling the model size might be the only way to further improve model performance."
  - [corpus] Neighboring papers discuss data limitations but don't specifically address dataset exhaustion
- Break condition: When synthetic data or new data sources become viable alternatives

## Foundational Learning

- Concept: Scaling laws in deep learning
  - Why needed here: Understanding how model performance scales with compute, model size, and dataset size is crucial to grasp the paper's hypothesis
  - Quick check question: What is the traditional Chinchilla-optimal scaling law and how does it allocate compute between model size and dataset size?

- Concept: Bits per character (BPC) as a performance metric
  - Why needed here: The paper uses BPC as a proxy for model performance, claiming it linearly correlates with model capabilities
  - Quick check question: How does BPC relate to model compression efficiency and why might it be a better metric than cross-entropy loss?

- Concept: Compute-optimal training
  - Why needed here: The paper challenges the concept of compute-optimal training by suggesting that total compute matters more than the specific allocation
  - Quick check question: What is the difference between compute-optimal training and the proposed unified scaling law approach?

## Architecture Onboarding

- Component map: Data collection pipeline -> Model architecture (transformer) -> Training framework with compute tracking -> BPC evaluation system -> Analysis tools for plotting

- Critical path:
  1. Collect data on model parameters, training tokens, and BPC for various open-source models
  2. Calculate compute (N × D) for each model
  3. Plot BPC against log-scale compute
  4. Fit a linear regression to establish the relationship
  5. Validate predictions against known model performances

- Design tradeoffs:
  - Prioritizing smaller models vs. larger models for a given compute budget
  - Balancing dataset size with data quality
  - Choosing between real-world data and synthetic data for training

- Failure signatures:
  - Non-linear relationship between BPC and compute when plotted
  - High variance in BPC for models with similar compute budgets
  - Poor correlation between BPC and other performance metrics

- First 3 experiments:
  1. Replicate the linear relationship between BPC and compute using publicly available model data
  2. Test inference efficiency of smaller models trained with larger datasets versus larger models with smaller datasets
  3. Analyze the impact of dataset quality on the optimal token-per-parameter ratio

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between model size and dataset size for optimal compute efficiency, given different data quality levels?
- Basis in paper: [inferred] The paper acknowledges that data quality affects the optimal token-per-parameter ratio, with higher-quality data requiring fewer tokens per parameter, but doesn't specify exact scaling relationships
- Why unresolved: The paper mentions this as a limitation but doesn't provide quantitative analysis or propose methods to determine optimal ratios for different data quality levels
- What evidence would resolve it: Empirical studies measuring model performance across different combinations of model sizes, dataset sizes, and data quality levels, potentially through controlled experiments or detailed analysis of existing models

### Open Question 2
- Question: How does the proposed scaling law relate to other evaluation metrics beyond compression scores?
- Basis in paper: [explicit] The paper explicitly states that while compression scores correlate with model performance, they may not fully capture all aspects of model capabilities, and future research is needed to explore relationships with other metrics
- Why unresolved: The analysis only considers compression scores, and there's no empirical validation of how well the compute-based scaling law predicts performance on task-specific benchmarks or human evaluations
- What evidence would resolve it: Comparative studies measuring multiple evaluation metrics (like benchmark scores, human evaluations, task-specific performance) against compute across various models to validate if the linear relationship holds

### Open Question 3
- Question: What is the valid range of the proposed scaling law, and at what points do extreme model size-to-dataset ratios become impractical?
- Basis in paper: [explicit] The paper mentions that extreme ratios (like quadrillion parameters with million tokens or vice versa) seem impractical, but doesn't define specific boundaries
- Why unresolved: The paper acknowledges potential limitations in the applicable range but doesn't provide quantitative analysis of where the log-linear relationship breaks down
- What evidence would resolve it: Empirical studies testing the scaling law across different orders of magnitude in model size and dataset size to identify specific thresholds where the linear relationship fails or becomes impractical

## Limitations
- The analysis relies on a relatively small dataset of open-source models, limiting generalizability
- The paper assumes uniform tokenizer impact across all models, which may not hold in practice
- Claims about dataset exhaustion lack strong empirical support and may not account for synthetic data approaches

## Confidence

- **High confidence**: The mathematical relationship between BPC and compute is correctly calculated for the models analyzed
- **Medium confidence**: The linear relationship between BPC and log-scale compute holds across the tested model range
- **Low confidence**: Claims about dataset exhaustion and the universal applicability of the scaling law to all model families

## Next Checks

1. Test the scaling law's predictions against held-out models not included in the original analysis, particularly those with extreme token-per-parameter ratios
2. Verify whether the BPC-compute relationship holds when controlling for tokenizer differences by normalizing scores across models
3. Conduct ablation studies varying dataset quality while holding compute constant to identify when the linear relationship breaks down