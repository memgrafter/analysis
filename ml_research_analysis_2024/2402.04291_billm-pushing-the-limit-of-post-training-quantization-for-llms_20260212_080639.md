---
ver: rpa2
title: 'BiLLM: Pushing the Limit of Post-Training Quantization for LLMs'
arxiv_id: '2402.04291'
source_url: https://arxiv.org/abs/2402.04291
tags:
- billm
- quantization
- weights
- llms
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents BiLLM, a post-training binarization method
  for large language models (LLMs) that achieves high-accuracy inference with only
  1.08-bit weights on LLaMA2-70B (8.41 perplexity). The method leverages Hessian-based
  weight importance analysis to identify and binarize salient weights via residual
  approximation, while non-salient weights are grouped and binarized using an optimal
  splitting strategy based on their bell-shaped distribution.
---

# BiLLM: Pushing the Limit of Post-Training Quantization for LLMs

## Quick Facts
- arXiv ID: 2402.04291
- Source URL: https://arxiv.org/abs/2402.04291
- Reference count: 40
- One-line primary result: Achieves 8.41 perplexity on LLaMA2-70B with only 1.08-bit weights

## Executive Summary
BiLLM presents a post-training binarization method for large language models that achieves ultra-low bit-width quantization while maintaining high accuracy. The method leverages Hessian-based weight importance analysis to identify salient weights, applies residual approximation for these weights, and uses an optimal splitting strategy for non-salient weights based on their bell-shaped distribution. BiLLM significantly outperforms state-of-the-art quantization methods and enables efficient binarization of 7-billion parameter LLMs on a single GPU within 0.5 hours.

## Method Summary
BiLLM is a post-training quantization framework that binarizes large language models by exploiting the distribution characteristics of weights and their Hessian sensitivity. The method identifies salient weights through Hessian analysis, applies residual approximation to minimize binarization error for these important weights, and groups non-salient weights using an optimal splitting strategy based on their bell-shaped distribution. The approach achieves 1.08-bit weights on LLaMA2-70B with 8.41 perplexity while maintaining computational efficiency.

## Key Results
- Achieves 8.41 perplexity on LLaMA2-70B with only 1.08-bit weights
- Outperforms state-of-the-art quantization methods by significant margins
- Enables binarization of 7-billion parameter LLMs within 0.5 hours on a single GPU

## Why This Works (Mechanism)

### Mechanism 1
Hessian-based weight importance analysis enables accurate identification of salient weights in LLMs. The second-order Hessian matrix of weights exhibits a long-tail distribution, where a small fraction of weights possess significantly high Hessian values and substantially influence layer outputs, while most Hessian values cluster around zero. This distribution allows for structured selection of important weights.

### Mechanism 2
Optimal splitting strategy for non-salient weights minimizes binarization errors by exploiting bell-shaped distribution. Non-salient weights follow a bell-shaped distribution that resembles Gaussian or Laplace distributions. The optimal splitting strategy identifies a break-point that divides these weights into sparse and concentrated areas, allowing separate binarization to minimize errors.

### Mechanism 3
Residual approximation for salient weights reduces quantization error while maintaining ultra-low bit-width. Instead of directly binarizing salient weights, BiLLM applies a two-stage residual approximation where weights are first binarized, then the residuals are binarized again. This approach minimizes quantization error compared to direct binarization or retaining weights at higher precision.

## Foundational Learning

- **Concept**: Hessian matrix and second-order optimization
  - Why needed here: Understanding how the Hessian matrix captures parameter sensitivity is crucial for grasping why BiLLM uses it to identify salient weights.
  - Quick check question: How does the Hessian matrix differ from gradient-based methods in measuring parameter importance?

- **Concept**: Quantization error and mean squared quantization error (MSQE)
  - Why needed here: The paper's core optimization involves minimizing quantization error, particularly through the optimal splitting strategy for non-salient weights.
  - Quick check question: What is the mathematical relationship between quantization step size and mean squared quantization error in uniform quantization?

- **Concept**: Long-tail and bell-shaped distributions
  - Why needed here: The paper's approach relies on specific distribution patterns (long-tail for Hessian, bell-shaped for weights) to guide the binarization process.
  - Quick check question: How do long-tail distributions differ from normal distributions, and why might they be advantageous for identifying important weights?

## Architecture Onboarding

- **Component map**: Hessian computation → Salient weight identification → Residual approximation → Non-salient weight splitting → Binarization → Error compensation

- **Critical path**: Hessian computation → Salient weight identification → Residual approximation → Non-salient weight splitting → Binarization → Error compensation

- **Design tradeoffs**:
  - Structured vs unstructured salient weight selection (structured reduces bitmap overhead but may miss some important weights)
  - Single vs multiple break-points for non-salient weight splitting (single is faster but may be less optimal)
  - Residual approximation vs direct binarization (residual reduces error but adds computation)
  - Block size selection (affects both accuracy and efficiency)

- **Failure signatures**:
  - Significant performance degradation when Hessian distribution patterns change across LLM architectures
  - Suboptimal break-point selection leading to increased quantization error
  - Residual approximation failing to converge or introducing instability
  - Block-wise compensation not adequately addressing quantization errors

- **First 3 experiments**:
  1. Verify Hessian distribution patterns on a small LLM (e.g., OPT-1.3B) to confirm long-tail characteristics
  2. Test optimal splitting strategy on non-salient weights with controlled distributions (Gaussian, Laplace) to validate MSQE minimization
  3. Implement residual approximation on a single layer and compare quantization error against direct binarization and 8-bit retention methods

## Open Questions the Paper Calls Out

### Open Question 1
How does the optimal breakpoint in the non-salient weight distribution vary across different layers and model sizes? The paper discusses the optimal splitting search for non-salient weights but does not provide a comprehensive analysis of how this breakpoint varies across different layers and model sizes.

### Open Question 2
What is the impact of different block sizes on the quantization performance of BiLLM? The paper mentions that a block size of 128 is used in the experiments but does not provide a comprehensive analysis of the impact of different block sizes on quantization performance.

### Open Question 3
How does the residual approximation strategy for salient weights affect the quantization performance compared to other methods? The paper introduces the residual approximation strategy for salient weights but does not provide a detailed comparison of its effectiveness compared to other methods.

## Limitations

- Hessian computation scalability concerns for trillion-parameter models due to O(n²) complexity
- Distribution stability uncertainty across different LLM architectures beyond OPT and LLaMA families
- Calibration data dependency with unspecified requirements for amount, diversity, and preprocessing

## Confidence

**High Confidence (8/10)**:
- The overall framework design and mathematical formulation of the BiLLM approach
- The general effectiveness of Hessian-based weight importance analysis (supported by extensive prior work)
- The fundamental principle that structured quantization with error compensation improves accuracy

**Medium Confidence (6/10)**:
- The specific claim that BiLLM achieves 8.41 perplexity on LLaMA2-70B with 1.08-bit weights
- The effectiveness of the optimal splitting strategy for non-salient weights
- The residual approximation approach for salient weights

**Low Confidence (4/10)**:
- The generalizability of BiLLM to non-Transformer architectures
- The computational efficiency claims for very large models (>70B parameters)
- The robustness of the method to different calibration datasets and preprocessing

## Next Checks

1. **Distribution Pattern Verification**: Replicate the Hessian distribution analysis on a small-scale model (OPT-1.3B or LLaMA-7B) with controlled weight initialization to verify the long-tail pattern persists across different training regimes and whether it holds for models with different architectural choices (e.g., GPT-style vs. OPT-style).

2. **Optimal Splitting Strategy Validation**: Implement the optimal splitting search independently and test it on synthetic weight distributions with known optimal break-points (Gaussian, Laplace, and bimodal distributions) to verify the MSQE minimization claims and quantify the gap between single and multiple break-point approaches.

3. **Residual Approximation Stability**: Conduct ablation studies comparing direct binarization, 8-bit retention, and the proposed residual approximation on individual transformer layers, measuring both quantization error and computational overhead to verify that the residual approach provides net benefits across different weight magnitude distributions.