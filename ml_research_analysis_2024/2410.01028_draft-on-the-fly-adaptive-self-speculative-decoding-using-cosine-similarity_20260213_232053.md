---
ver: rpa2
title: 'Draft on the Fly: Adaptive Self-Speculative Decoding using Cosine Similarity'
arxiv_id: '2410.01028'
source_url: https://arxiv.org/abs/2410.01028
tags:
- draft
- layers
- decoding
- inference
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple method for accelerating large language
  model (LLM) inference using adaptive self-speculative decoding. The approach removes
  attention layers on the fly based on cosine similarity of hidden states, combined
  with deterministic layer removal rules.
---

# Draft on the Fly: Adaptive Self-Speculative Decoding using Cosine Similarity

## Quick Facts
- **arXiv ID**: 2410.01028
- **Source URL**: https://arxiv.org/abs/2410.01028
- **Reference count**: 6
- **Primary result**: Achieves 1.44× speedup on CNN/DM summarization with Llama-2-13B

## Executive Summary
This paper proposes Adaptive Self-Speculative Decoding (ASD), a method for accelerating large language model inference by removing attention layers on the fly based on cosine similarity of hidden states. The approach combines cosine similarity thresholding with deterministic layer removal rules, avoiding the need for fine-tuning or black-box optimization required by prior methods. Experiments show the method achieves comparable inference speedup to state-of-the-art self-speculative decoding techniques while being truly plug-and-play.

## Method Summary
The method computes average cosine similarity (ACS) between hidden states before and after each attention layer during inference. Layers with ACS above threshold α are removed, assuming high similarity indicates redundancy. Two deterministic rules are also applied: removing every mth attention and MLP layer, and preserving the last n layers. This creates a draft model D on-the-fly from the original model M, which is then used in a speculative decoding framework where D generates tokens that are verified against M's predictions in parallel.

## Key Results
- Achieves 1.44× speedup on CNN/DM summarization with Llama-2-13B
- Comparable performance to state-of-the-art self-speculative decoding methods
- Truly plug-and-play implementation avoiding fine-tuning or optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing attention layers based on cosine similarity threshold preserves accuracy while reducing inference time
- Mechanism: High ACS values indicate minimal information gain, making layers redundant
- Core assumption: Attention layers with high cosine similarity between input and output hidden states are redundant
- Evidence: Higher mean and median ACS for attention layers compared to MLP layers
- Break condition: If ACS values are uniformly low across all layers

### Mechanism 2
- Claim: Deterministic rules improve speedup when combined with cosine similarity thresholding
- Mechanism: Remove every mth layer and preserve last n layers based on layer redundancy and importance
- Core assumption: Neighboring layers contain similar information; last layers are critical
- Evidence: Observations from related work on layer redundancy and importance of final layers
- Break condition: If model has minimal layer redundancy or specialized architecture

### Mechanism 3
- Claim: Self-speculative decoding avoids challenges of obtaining separate draft models
- Mechanism: Generate draft model D on-the-fly by removing layers based on input context
- Core assumption: Input context provides sufficient information for layer removal decisions
- Evidence: Avoids fine-tuning and optimization required by other methods
- Break condition: If layer interactions are too complex for simple removal rules

## Foundational Learning

- **Speculative decoding mechanism**: Understanding draft model generation and verification is essential for grasping acceleration benefits
  - Quick check: What makes speculative decoding faster than vanilla autoregressive generation?

- **Cosine similarity interpretation**: Method relies on cosine similarity to determine layer importance
  - Quick check: What does cosine similarity close to 1 between hidden states indicate about a layer's contribution?

- **Transformer architecture and layer functions**: Method removes attention layers based on assumptions about their redundancy
  - Quick check: How do attention layers contribute to capturing dependencies in input sequences?

## Architecture Onboarding

- **Component map**: Input context → ACS computation → Layer removal decision → Draft model generation → Token drafting → Parallel verification → Output generation
- **Critical path**: Input context → ACS computation → Layer removal decision → Draft model generation → Token drafting → Parallel verification → Output generation
- **Design tradeoffs**: Speed vs. accuracy (higher α removes more layers but may reduce accuracy); computation overhead from ACS calculation; plug-and-play vs. optimization
- **Failure signatures**: Excessive layer removal causing accuracy degradation; insufficient speedup despite layer removal; high computation overhead; sensitivity to hyperparameters
- **First 3 experiments**: 1) Baseline vanilla autoregressive generation; 2) CS thresholding only; 3) Full ADMG with all three rules

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold value α for cosine similarity across different LLM architectures and tasks?
- Basis: Paper uses α=0.985 but CodeLlama-13B performed better with different values
- Why unresolved: Only one α value tested; relationship between α, architecture, task, and speedup unexplored
- Evidence needed: Systematic study varying α across architectures and tasks to identify patterns

### Open Question 2
- Question: How does the method perform when input context distribution differs from calibration data?
- Basis: Paper suggests ADMG might be more robust to distribution shifts than Draft & Verify
- Why unresolved: Experiments use test sets similar to training data; hypothesis not empirically tested
- Evidence needed: Experiments on out-of-distribution inputs or adversarial examples

### Open Question 3
- Question: What is the theoretical relationship between removable attention layers and achieved inference speedup?
- Basis: Paper observes correlation but doesn't provide theoretical framework
- Why unresolved: No mathematical model explaining how layer redundancy, cosine similarity, and computational complexity interact
- Evidence needed: Mathematical model predicting speedup as function of these parameters

### Open Question 4
- Question: How does the method scale to much larger LLMs (70B+ parameters)?
- Basis: Experiments limited to 13B parameter models
- Why unresolved: Scaling behavior and whether same principles apply to larger models unexplored
- Evidence needed: Experiments across range of model sizes to identify scaling patterns

## Limitations

- Effectiveness depends on assumption that high cosine similarity indicates redundant layers, which may not hold for all architectures
- Computational overhead from cosine similarity calculations not fully quantified
- Hyperparameter sensitivity (α, m, n) across different models and datasets may limit plug-and-play claims
- Limited testing on model architectures beyond Llama-2 family

## Confidence

- **Layer Removal Based on Cosine Similarity**: High confidence - theoretically sound with empirical justification
- **Effectiveness of Deterministic Rules**: Medium confidence - motivated by related work but needs more validation
- **Plug-and-Play Nature**: Medium confidence - avoids fine-tuning but still requires hyperparameter tuning

## Next Checks

1. **Ablation Study on Rule Effectiveness**: Isolate individual contributions of deterministic rules to quantify their impact on speedup and accuracy

2. **Cross-Architecture Generalization**: Test method on diverse model architectures beyond Llama-2 to validate plug-and-play claims

3. **Computational Overhead Analysis**: Measure exact computational overhead from cosine similarity calculations and compare against achieved speedup to determine practical efficiency gains