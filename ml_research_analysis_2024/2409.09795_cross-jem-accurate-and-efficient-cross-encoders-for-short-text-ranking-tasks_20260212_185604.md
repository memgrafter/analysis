---
ver: rpa2
title: 'CROSS-JEM: Accurate and Efficient Cross-encoders for Short-text Ranking Tasks'
arxiv_id: '2409.09795'
source_url: https://arxiv.org/abs/2409.09795
tags:
- latexit
- ranking
- cross-jem
- sha1
- base64
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CROSS-JEM introduces a joint ranking architecture that scores multiple
  short-text items per query in a single pass by leveraging token overlap and interactions
  across items. It uses a transformer encoder to process the query and union of item
  tokens together, followed by selective pooling and a shared classifier.
---

# CROSS-JEM: Accurate and Efficient Cross-encoders for Short-text Ranking Tasks

## Quick Facts
- arXiv ID: 2409.09795
- Source URL: https://arxiv.org/abs/2409.09795
- Reference count: 40
- CROSS-JEM achieves state-of-the-art accuracy and over 4x lower ranking latency over standard cross-encoders

## Executive Summary
CROSS-JEM introduces a joint ranking architecture that scores multiple short-text items per query in a single pass by leveraging token overlap and interactions across items. It uses a transformer encoder to process the query and union of item tokens together, followed by selective pooling and a shared classifier. The model is trained with a novel Ranking Probability Loss that explicitly optimizes for listwise ranking probabilities. Experiments on public benchmarks and large-scale search logs show 3-13% accuracy improvements over state-of-the-art methods, with 4-6x lower latency enabling real-time deployment. Online A/B tests confirm 1.8% reduction in quick-back-rate, indicating better ad relevance.

## Method Summary
CROSS-JEM is designed for short-text ranking tasks where a query needs to be matched with multiple candidate items. The core innovation is joint scoring of all items per query in a single forward pass, achieved by concatenating the query tokens with a union of all item tokens. This shared representation is processed by a transformer encoder, followed by selective pooling to extract item-specific embeddings. A shared classifier then computes scores for all items simultaneously. The model is trained using a novel Ranking Probability Loss (RPL) that models the probability distribution over item rankings, encouraging correct ordering of relevant items.

## Key Results
- 3-13% accuracy improvements over state-of-the-art methods on public benchmarks
- 4-6x lower ranking latency compared to standard cross-encoders
- 1.8% reduction in quick-back-rate in online A/B tests, indicating better ad relevance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jointly scoring multiple items per query in a single forward pass reduces inference latency.
- Mechanism: CROSS-JEM concatenates query tokens with a union of all item tokens, allowing the encoder to process them together. The selective pooling layer then extracts item-specific representations from this shared context, and the shared classifier computes all scores simultaneously.
- Core assumption: Item tokens have significant overlap, so the union set is much smaller than the sum of all individual item tokens.
- Evidence anchors:
  - [abstract]: "CROSS-JEM achieves state-of-the-art accuracy and over 4x lower ranking latency over standard cross-encoders."
  - [section]: "CROSS-JEM exploits this redundancy to sidestep processing long sequences, thereby keeping the inference latency small."
  - [corpus]: Weak/no direct evidence on overlap statistics in corpus.
- Break condition: If item token overlap is low, the union set size approaches the sum of individual item tokens, negating latency gains.

### Mechanism 2
- Claim: The Ranking Probability Loss (RPL) explicitly optimizes for the probability of ranking items in the correct order.
- Mechanism: RPL is a cross-entropy loss between a predicted ranking probability distribution (computed from model logits) and a ground-truth ranking distribution (computed from target scores). This encourages the model to assign higher scores to items that should be ranked higher.
- Core assumption: The ranking probability distribution can be approximated by summing logits of items ranked lower than the target item.
- Evidence anchors:
  - [abstract]: "a novel training objective that models ranking probabilities."
  - [section]: "minimizing the distance between P and P* is equivalent to minimizing the KL divergence between the predicted, and ground truth ranking distributions."
  - [corpus]: Weak/no direct evidence on ranking probability distribution approximation.
- Break condition: If the approximation of ranking probabilities by summing logits is poor, RPL may not effectively optimize for correct rankings.

### Mechanism 3
- Claim: Selective pooling enables efficient extraction of item-specific representations from the shared encoder context.
- Mechanism: For each item, CROSS-JEM pools the contextual embeddings of the query tokens and the tokens in the item union set that are present in the item. This creates a compact representation for each item without needing separate encoder passes.
- Core assumption: Mean pooling of relevant token embeddings is sufficient to capture the item's relevance to the query.
- Evidence anchors:
  - [section]: "a pooled representation for pair (ð’’, ð’Œ ð‘— ) is computed as the mean of the contextual embeddings for all tokens in Tð’’ and those tokens in Tð‘ˆð’’ which are present in Tð’Œ ð‘—."
  - [corpus]: Weak/no direct evidence on effectiveness of mean pooling for relevance capture.
- Break condition: If mean pooling fails to capture important interactions between query and item tokens, the extracted representations may be insufficient for accurate scoring.

## Foundational Learning

- Concept: Transformer encoder architecture
  - Why needed here: CROSS-JEM relies on a transformer encoder to process the concatenated query and item union tokens and generate contextual embeddings.
  - Quick check question: What is the role of self-attention in a transformer encoder, and how does it enable the model to capture relationships between query and item tokens?

- Concept: Listwise ranking vs. pointwise ranking
  - Why needed here: CROSS-JEM is designed for listwise ranking, where the model scores all items for a query simultaneously, considering their interactions. This is in contrast to pointwise ranking, where items are scored independently.
  - Quick check question: How does listwise ranking differ from pointwise ranking in terms of the information used for scoring and the potential for capturing item-item interactions?

- Concept: Cross-entropy loss and ranking probability distributions
  - Why needed here: CROSS-JEM uses a modified version of the ListNet loss, which is a cross-entropy loss defined over ranking probability distributions. Understanding the relationship between cross-entropy loss and ranking probability distributions is crucial for interpreting the RPL.
  - Quick check question: How does the cross-entropy loss encourage the model to assign higher probabilities to items that should be ranked higher in the ground-truth ranking?

## Architecture Onboarding

- Component map: Encoder -> Selective Pooling Layer -> Shared Classifier -> Ranking Probability Loss
- Critical path: Encoder â†’ Selective Pooling Layer â†’ Shared Classifier â†’ RPL
  - The encoder processes the input tokens, the selective pooling layer extracts item representations, the shared classifier computes scores, and the RPL trains the model to optimize for correct rankings.
- Design tradeoffs:
  - Accuracy vs. Latency: CROSS-JEM achieves higher accuracy than pointwise methods but requires more computation than simple dual encoders.
  - Complexity vs. Simplicity: The selective pooling layer and RPL add complexity compared to standard cross-encoders, but they enable joint modeling and listwise ranking.
- Failure signatures:
  - High latency: If item token overlap is low, the union set size may be large, negating latency gains.
  - Poor accuracy: If the selective pooling layer fails to capture relevant interactions or if the RPL approximation is poor, the model may not achieve high accuracy.
- First 3 experiments:
  1. Vary the number of items scored per query (N) and measure the impact on latency and accuracy.
  2. Compare the performance of CROSS-JEM with and without the selective pooling layer to assess its contribution to accuracy.
  3. Evaluate the impact of different ranking probability loss functions (e.g., standard ListNet vs. RPL) on CROSS-JEM's performance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond mentioning future work on adapting CROSS-JEM for long-text ranking tasks using positional encodings.

## Limitations
- Token Overlap Assumption: The latency advantage critically depends on high token overlap among items, but no empirical validation of overlap statistics is provided.
- Ranking Probability Loss Approximation: The RPL formulation uses an approximation for ranking probabilities that lacks rigorous justification and may not hold for all ranking distributions.
- Selective Pooling Efficacy: The mean pooling operation may not capture complex interactions between query and item tokens, especially for longer or more semantically diverse items.

## Confidence
- High Confidence: The architectural framework is clearly specified, and the reported accuracy improvements over baselines are substantial and consistent across multiple datasets.
- Medium Confidence: The RPL formulation and its relationship to standard ranking losses is well-explained, but the approximation used for ranking probabilities introduces uncertainty about its general applicability.
- Low Confidence: The critical assumption about token overlap in real-world search scenarios is not empirically validated, and the effectiveness of mean pooling for capturing item-query relevance is not rigorously tested.

## Next Checks
1. **Token Overlap Validation**: Compute the ratio of union token set size to total individual item token count on your specific dataset to verify the latency advantage claim holds.
2. **Pooling Strategy Ablation**: Compare CROSS-JEM's performance with alternative pooling methods (max pooling, attention-based pooling) to assess whether mean pooling is optimal for your data.
3. **RPL Sensitivity Analysis**: Test CROSS-JEM with standard ListNet loss and Binary Cross-Entropy to determine whether the ranking probability approximation provides measurable benefits over established alternatives.