---
ver: rpa2
title: Solution for Temporal Sound Localisation Task of ECCV Second Perception Test
  Challenge 2024
arxiv_id: '2409.19595'
source_url: https://arxiv.org/abs/2409.19595
tags:
- sound
- audio
- features
- video
- yang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the Temporal Sound Localisation (TSL) task,
  which requires locating and classifying sound events in videos using predefined
  sound classes. The approach improves upon last year's champion solution by emphasizing
  audio modality over video, hypothesizing that sound localization benefits more from
  audio features.
---

# Solution for Temporal Sound Localisation Task of ECCV Second Perception Test Challenge 2024

## Quick Facts
- arXiv ID: 2409.19595
- Source URL: https://arxiv.org/abs/2409.19595
- Authors: Haowei Gu; Weihao Zhu; Yang Yang
- Reference count: 22
- One-line primary result: Achieved 0.4925 mAP on test set, ranking first in ECCV Second Perception Test Challenge 2024

## Executive Summary
This work addresses the Temporal Sound Localisation (TSL) task by proposing a multimodal approach that prioritizes audio features over video features. The method employs multiple audio models (InterVideo, CaVMAE, and VideoMAE) to extract complementary audio representations, which are concatenated with video features from UMT and VideoMAE models. An Actionformer-based encoder-decoder architecture processes the fused multimodal features for temporal sound localization, with post-processing via Weighted Boxes Fusion (WBF) to refine predictions. The approach achieves state-of-the-art performance with 0.4925 mAP on the test set.

## Method Summary
The method leverages a multimodal fusion approach using Actionformer as the base model for temporal sound localization. Video features are extracted using UMT-Large and VideoMAE-Large models, while audio features come from BEATS, CAV-MAE(AudioSet), and CAV-MAE(VGGSound) models. These features are concatenated to form a comprehensive 2304-dimensional audio representation. The fused multimodal features are processed through a multi-scale Transformer encoder and lightweight decoder architecture. Post-processing via Weighted Boxes Fusion combines predictions from models trained at different epochs (20, 25, and 30) to improve localization precision.

## Key Results
- Achieved 0.4925 mAP on test set, ranking first in ECCV Second Perception Test Challenge 2024
- Demonstrated superiority of audio features over video features for sound localization task
- Successfully combined multiple audio models to create richer feature representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio modality carries more discriminative power for sound event localization than video modality.
- Mechanism: Prioritizing audio feature extraction and fusion increases localization accuracy by aligning model capacity with the task's core objective (detecting sound events).
- Core assumption: Sound events are primarily defined by their acoustic characteristics, not visual appearance.
- Evidence anchors:
  - [abstract] "Considering the TSL task aims to localize sound events, we conduct relevant experiments that demonstrated the superiority of sound features"
  - [section] "Since TSL task aims to localize sound events, we propose that the audio play a more important role in it"
- Break condition: If visual cues are critical for disambiguating sound sources or if the audio signal is noisy/corrupted, this assumption may fail.

### Mechanism 2
- Claim: Using multiple complementary audio models (InterVideo, CaVMAE, VideoMAE) provides richer audio representations than any single model.
- Mechanism: Each model captures different aspects of audio features; concatenation aggregates these complementary views into a 2304-dimensional representation.
- Core assumption: Different pre-training objectives and datasets lead to non-redundant feature spaces.
- Evidence anchors:
  - [section] "To enhance audio modality and achieve a more comprehensive acoustic representation, we leverage three models: the BEATS model and two variants of the CA V-MAE model fine-tuned on AudioSet and VGGSound, respectively"
  - [section] "We then concatenate the three feature vectors, yielding a 2304-dimensional representation that encapsulates a wide range of acoustic characteristics"
- Break condition: If the models extract highly correlated features or if one model dominates the concatenated representation.

### Mechanism 3
- Claim: Weighted Boxes Fusion (WBF) improves localization precision by combining predictions from models trained at different epochs.
- Mechanism: WBF aggregates bounding box predictions using weighted averaging based on confidence scores, reducing false positives and refining temporal boundaries.
- Core assumption: Models trained at different epochs capture different aspects of the solution space and make complementary errors.
- Evidence anchors:
  - [section] "Weighted Boxes Fusion (WBF) [8] is commonly employed to combine predictions from object detection models. We have made modifications to the WBF method to ensure its applicability to the TSL task"
  - [section] "We generate results from models trained for 20, 25, and 30 epochs for each method. Subsequently, these results were input into the WBF module, with identical fusion weights assigned to each model's results"
- Break condition: If models are highly correlated in their predictions or if the ensemble introduces significant computational overhead without performance gain.

## Foundational Learning

- Concept: Multimodal fusion strategies
  - Why needed here: The paper combines video and audio features for sound localization; understanding different fusion approaches (early, late, hybrid) is critical for effective integration.
  - Quick check question: What's the difference between early fusion (concatenation) and late fusion (separate predictions then combine) in multimodal learning?

- Concept: Transformer-based encoder-decoder architectures
  - Why needed here: Actionformer is used as the backbone; understanding how Transformers handle temporal sequences and attention mechanisms is essential.
  - Quick check question: How does the self-attention mechanism in Transformers help capture long-range temporal dependencies in video sequences?

- Concept: Audio feature extraction from spectrograms
  - Why needed here: Audio models process mel-spectrograms; understanding this representation is crucial for interpreting model inputs and outputs.
  - Quick check question: What information is preserved and what is lost when converting raw audio to mel-spectrograms?

## Architecture Onboarding

- Component map: Untrimmed audio A → Mel-spectrogram → BEATS/CaVMAE/VideoMAE → 2304D audio features → Concatenation with video features → Actionformer encoder-decoder → Sound event predictions

- Critical path: Audio feature extraction → Feature fusion → Actionformer → WBF → final predictions

- Design tradeoffs:
  - Audio focus vs balanced modality: Prioritizing audio improves localization but may miss visual cues
  - Model ensemble vs single model: Multiple audio models provide richer features but increase computational cost
  - WBF vs single model: Ensembling improves robustness but adds post-processing complexity

- Failure signatures:
  - Poor localization when audio signal is noisy or contains overlapping sound events
  - Degradation when video contains critical visual cues for sound source identification
  - Computational bottlenecks during feature extraction or WBF post-processing

- First 3 experiments:
  1. Baseline ablation: Train with only audio features vs only video features to validate the audio superiority hypothesis
  2. Single vs multi-model audio: Compare CaVMAE-only vs CaVMAE + BEATS + VideoMAE concatenation to test complementary feature hypothesis
  3. WBF sensitivity: Test different model combinations and fusion weights in WBF to optimize post-processing parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the temporal sound localization task change when using only video features versus only audio features, and what does this reveal about the relative importance of each modality?
- Basis in paper: [explicit] The paper mentions that experiments demonstrated the superiority of sound features and that the audio plays a more important role in the TSL task, but does not provide detailed comparative results of using only video features versus only audio features.
- Why unresolved: The paper does not provide a detailed comparison of the performance when using only video features versus only audio features, which would help in understanding the relative importance of each modality.
- What evidence would resolve it: Conducting experiments that compare the performance of the model using only video features versus only audio features, and analyzing the results to determine which modality contributes more significantly to the task.

### Open Question 2
- Question: How does the fusion strategy of audio and video features affect the performance of the temporal sound localization task, and are there alternative fusion strategies that could be explored?
- Basis in paper: [explicit] The paper mentions using an early fusion strategy by concatenating video and audio features along the channel dimension, but does not explore alternative fusion strategies.
- Why unresolved: The paper does not explore or compare different fusion strategies, such as late fusion or attention-based fusion, which could potentially improve the performance of the model.
- What evidence would resolve it: Experimenting with different fusion strategies, such as late fusion or attention-based fusion, and comparing their performance with the early fusion strategy used in the paper.

### Open Question 3
- Question: What is the impact of using different audio feature extraction models (e.g., InterVideo, CaVMAE, VideoMAE) on the overall performance of the temporal sound localization task?
- Basis in paper: [explicit] The paper mentions using multiple models (InterVideo, CaVMAE, and VideoMAE) to extract audio features and concatenating them to form a comprehensive audio representation, but does not provide a detailed analysis of the impact of each model.
- Why unresolved: The paper does not provide a detailed analysis of how each audio feature extraction model contributes to the overall performance, nor does it explore the impact of using different combinations of these models.
- What evidence would resolve it: Conducting experiments that analyze the performance of the model when using different combinations of audio feature extraction models, and comparing the results to determine the impact of each model on the overall performance.

## Limitations
- The audio superiority hypothesis lacks rigorous ablation validation through controlled experiments comparing audio-only versus video-only performance.
- The claim about complementary audio features from different models is unsupported by quantitative correlation analysis or ablation tests.
- WBF post-processing uses uniform fusion weights without optimization, suggesting potential performance gains remain unexplored.

## Confidence
- **High Confidence**: The architectural implementation of multimodal feature concatenation and Actionformer-based temporal localization is technically sound and follows established practices in the field.
- **Medium Confidence**: The hypothesis that audio modality is more important than video for sound localization is plausible but lacks rigorous ablation validation.
- **Low Confidence**: The assertion that WBF with uniform weights optimally combines predictions across different training epochs lacks supporting analysis.

## Next Checks
1. **Ablation study validation**: Conduct controlled experiments comparing audio-only, video-only, and multimodal approaches on the same dataset to empirically validate the audio superiority hypothesis and quantify the contribution of each modality.

2. **Feature complementarity analysis**: Perform correlation analysis between features extracted from BEATS, CaVMAE(AudioSet), and CaVMAE(VGGSound) to quantitatively demonstrate whether these models provide complementary information or redundant representations.

3. **WBF optimization evaluation**: Systematically test different fusion weight configurations and model epoch combinations in the WBF post-processing to determine whether uniform weights are optimal or whether learned weighting schemes could improve performance.