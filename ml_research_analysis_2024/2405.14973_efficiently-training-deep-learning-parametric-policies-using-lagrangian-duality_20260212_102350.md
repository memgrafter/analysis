---
ver: rpa2
title: Efficiently Training Deep-Learning Parametric Policies using Lagrangian Duality
arxiv_id: '2405.14973'
source_url: https://arxiv.org/abs/2405.14973
tags:
- policy
- ts-ddr
- constraints
- learning
- sddp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently training parametric
  policies for Constrained Markov Decision Processes (CMDPs) with hard nonlinear constraints,
  which is critical in high-stakes domains like power systems, finance, and robotics
  where constraint violations are costly. The proposed Two-Stage Deep Decision Rules
  (TS-DDR) method trains deep neural network policies using a self-supervised learning
  approach that combines stochastic gradient descent with Lagrangian duality.
---

# Efficiently Training Deep-Learning Parametric Policies using Lagrangian Duality

## Quick Facts
- **arXiv ID**: 2405.14973
- **Source URL**: https://arxiv.org/abs/2405.14973
- **Reference count**: 40
- **Primary result**: TS-DDR reduces training and inference times by several orders of magnitude while achieving near-optimal solutions for CMDPs with hard nonlinear constraints.

## Executive Summary
This paper addresses the challenge of efficiently training parametric policies for Constrained Markov Decision Processes (CMDPs) with hard nonlinear constraints, which is critical in high-stakes domains like power systems, finance, and robotics where constraint violations are costly. The proposed Two-Stage Deep Decision Rules (TS-DDR) method trains deep neural network policies using a self-supervised learning approach that combines stochastic gradient descent with Lagrangian duality. In the forward pass, TS-DDR solves deterministic optimization problems to ensure feasibility and near-optimality, while the backward pass leverages duality theory to obtain closed-form gradients for policy updates. Applied to the Long-Term Hydrothermal Dispatch problem using Bolivian power system data, TS-DDR demonstrates significant improvements in solution quality and computational efficiency compared to state-of-the-art methods.

## Method Summary
TS-DDR trains deep neural network policies for CMDPs by combining stochastic gradient descent with Lagrangian duality. The method operates in two stages: during the forward pass, it solves deterministic multi-stage optimization problems to find feasible control actions that satisfy constraints and minimize costs; during the backward pass, it uses duality theory to obtain exact gradients for policy updates without requiring implicit function differentiation. The approach is trained self-supervised on historical scenarios and at inference time relies on fast evaluation of the trained neural network. Applied to Long-Term Hydrothermal Dispatch in the Bolivian power system, TS-DDR demonstrates significant improvements in both solution quality and computational efficiency compared to traditional methods.

## Key Results
- TS-DDR achieves near-optimal solutions while reducing training and inference times by several orders of magnitude compared to state-of-the-art methods.
- On Bolivian power system data, TS-DDR outperforms both traditional stochastic programming approaches and model-free reinforcement learning methods in solution quality and computational efficiency.
- The method successfully handles the Long-Term Hydrothermal Dispatch problem with AC, SOC, and DCLL power flow formulations, demonstrating flexibility across different modeling approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TS-DDR achieves feasibility and near-optimality by solving deterministic multi-stage optimization problems in the forward pass.
- Mechanism: During training, TS-DDR predicts target states using a deep neural network policy and then solves a deterministic optimization problem to find control actions that satisfy constraints and minimize costs. This ensures that the policy decisions are always feasible.
- Core assumption: The second-stage optimization problem is convex or can be efficiently solved.
- Evidence anchors:
  - [abstract]: "its forward passes solve deterministic optimization problems to find feasible policies"
  - [section]: "During training, its forward pass solves a multi-step deterministic optimization problem ensuring feasibility and near-optimality"
  - [corpus]: "Found 25 related papers... Average neighbor FMR=0.563" (weak evidence, not directly related to this mechanism)
- Break condition: If the second-stage optimization problem becomes non-convex and intractable, the forward pass may fail to find feasible solutions efficiently.

### Mechanism 2
- Claim: TS-DDR obtains exact gradients for policy updates through duality theory in the backward pass.
- Mechanism: By representing the policy in terms of only the realized uncertainties, TS-DDR avoids implicit function differentiation of KKT conditions. Instead, it uses duality theory to obtain the subgradient of the subproblem objective with respect to the target state, which is given by the dual variable of the associated constraint.
- Core assumption: The optimization problem has a well-defined dual and the policy is differentiable with respect to its parameters.
- Evidence anchors:
  - [abstract]: "its backward passes leverage duality theory to train the parametric policy with closed-form gradients"
  - [section]: "Since the target ˆxt = πt({wj}j=2..t, x0; θt) is a right-hand-side (rhs) parameter of the inner problem Q, by duality theory, the subgradient of a subproblem objective with respect to ˆxt is given by the dual variable of its associated constraint."
  - [corpus]: No direct evidence found in corpus (weak evidence)
- Break condition: If the optimization problem does not have a well-defined dual or the policy is not differentiable, the backward pass may fail to obtain exact gradients.

### Mechanism 3
- Claim: TS-DDR inherits the computational efficiency of deep learning while maintaining the benefits of stochastic optimization.
- Mechanism: TS-DDR is trained using stochastic gradient descent (SGD), which leverages the computational performance of deep learning. At inference time, it relies on the evaluation of a deep learning network, which is extremely fast. This combination allows TS-DDR to achieve significant improvements in training and inference times compared to traditional methods.
- Core assumption: Deep learning networks can be efficiently evaluated and SGD converges to good solutions for the policy parameters.
- Evidence anchors:
  - [abstract]: "TS-DDR inherits the flexibility and computational performance of deep learning methodologies to solve CMDP problems"
  - [section]: "TS-DDR is trained in a self-supervised manner using stochastic gradient descent (SGD)... At inference time, TS-DDR relies on the evaluation of a deep learning network."
  - [corpus]: No direct evidence found in corpus (weak evidence)
- Break condition: If the deep learning network becomes too complex or SGD fails to converge, the computational efficiency benefits may be lost.

## Foundational Learning

- **Concept**: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: CMDPs are the problem class that TS-DDR is designed to solve. Understanding CMDPs is crucial for grasping the challenges that TS-DDR addresses.
  - Quick check question: What is the main difference between MDPs and CMDPs?

- **Concept**: Lagrangian Duality
  - Why needed here: Lagrangian duality is the key theoretical tool that enables TS-DDR to obtain exact gradients for policy updates without requiring implicit function differentiation.
  - Quick check question: How does Lagrangian duality help in converting constrained optimization problems into unconstrained ones?

- **Concept**: Stochastic Gradient Descent (SGD)
  - Why needed here: SGD is the optimization algorithm used to train the TS-DDR policy parameters. Understanding SGD is essential for comprehending the training process.
  - Quick check question: What is the main advantage of using SGD over other optimization algorithms for training deep neural networks?

## Architecture Onboarding

- **Component map**: Historical scenarios → Policy Network → Target States → Deterministic Optimization → Feasible Actions → Dual Variables → Gradients → Updated Policy Parameters

- **Critical path**:
  1. Sample a scenario from the stochastic process.
  2. Compute target states using the policy network.
  3. Solve the second-stage optimization problem to find feasible control actions.
  4. Obtain dual variables from the optimization problem.
  5. Compute gradients using the dual variables and the policy network's gradients.
  6. Update the policy parameters using SGD.

- **Design tradeoffs**:
  - Complexity vs. Efficiency: Using a deep neural network for the policy allows for complex decision rules but may require more computational resources.
  - Exactness vs. Approximation: Solving the deterministic optimization problem exactly ensures feasibility but may be computationally expensive for large-scale problems.
  - Sample Efficiency vs. Convergence: Using SGD with a small batch size improves sample efficiency but may lead to slower convergence.

- **Failure signatures**:
  - Training divergence: If the policy parameters become unstable during training, it may indicate issues with the backward pass or the optimization problem.
  - Poor feasibility: If the policy consistently produces infeasible solutions, it may suggest problems with the forward pass or the optimization problem.
  - Slow convergence: If the training process takes too long to converge, it may indicate issues with the SGD algorithm or the policy network architecture.

- **First 3 experiments**:
  1. Train TS-DDR on a simple CMDP with a known optimal policy and evaluate its performance.
  2. Compare the training time and solution quality of TS-DDR with a baseline method (e.g., SDDP) on a small-scale problem.
  3. Analyze the sensitivity of TS-DDR to the choice of hyperparameters (e.g., learning rate, network architecture) on a medium-scale problem.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does TS-DDR scale to extremely large-scale instances where even SDDP becomes computationally prohibitive?
  - Basis in paper: [inferred] The paper mentions this as a limitation, noting that "the scalability of TS-DDR on extremely large instances needs to be investigated" and that "Evaluating TS-DDR for applications outside the reach of SDDP is an important direction."
  - Why unresolved: The current evaluation focuses on instances where SDDP is still applicable, so the performance boundaries of TS-DDR remain untested.
  - What evidence would resolve it: Empirical results on power system instances significantly larger than the Bolivian 28-bus case (e.g., continental-scale systems with thousands of buses) comparing training/inference times and solution quality against both SDDP and other methods.

- **Open Question 2**: How sensitive is TS-DDR's performance to the choice of neural network architecture and hyperparameters?
  - Basis in paper: [inferred] The paper briefly mentions using Flux.jl with specific configurations (Adam optimizer, latent space size 64) but doesn't systematically explore architectural variations or conduct sensitivity analysis.
  - Why unresolved: The current implementation uses a fixed architecture without ablation studies or exploration of how different designs affect performance across problem types.
  - What evidence would resolve it: Systematic experiments varying network depth, width, activation functions, and hyperparameters across multiple CMDP problem classes, showing how these choices impact solution quality and computational efficiency.

- **Open Question 3**: Can TS-DDR effectively handle discrete decision variables commonly found in CMDP applications?
  - Basis in paper: [explicit] The paper explicitly lists this as a limitation: "addressing the challenges posed by discrete variables" is mentioned as an opportunity for future work.
  - Why unresolved: The current formulation and experiments focus on continuous variables, with no discussion of how the approach would extend to problems with integer or binary decisions.
  - What evidence would resolve it: Demonstrations of TS-DDR on CMDP problems with discrete components (e.g., unit commitment in power systems) showing successful handling of mixed-integer formulations.

## Limitations
- **Scaling uncertainty**: The paper's computational efficiency claims rely on assumptions about forward pass optimization tractability at scale, which remains untested for extremely large instances.
- **Architecture sensitivity**: Performance sensitivity to neural network architecture and hyperparameter choices has not been systematically explored across different problem classes.
- **Discrete variables**: The approach currently focuses on continuous variables and lacks demonstration of handling discrete decision variables common in CMDP applications.

## Confidence
- **High confidence**: The mechanism of using duality theory to obtain closed-form gradients (Mechanism 2) is well-established in optimization theory and the paper provides sufficient theoretical justification.
- **Medium confidence**: The computational efficiency claims (Mechanism 3) are supported by the experimental results but lack broader validation across different problem classes and scales.
- **Low confidence**: The assumption that the forward pass optimization remains tractable for complex non-convex problems (Mechanism 1) is not fully validated, particularly for AC power flow formulations.

## Next Checks
1. **Scaling Analysis**: Test TS-DDR on progressively larger power systems (e.g., 100+ buses) to empirically validate the claimed computational efficiency improvements and identify the scalability limits.
2. **Robustness Testing**: Evaluate TS-DDR's performance when the underlying optimization problems have ill-conditioned duals or when the policy network architecture is pushed beyond the specified configurations.
3. **Alternative Formulations**: Apply TS-DDR to CMDPs with different constraint structures (e.g., probabilistic constraints, multi-agent settings) to assess the generalizability of the approach beyond the hydrothermal dispatch problem.