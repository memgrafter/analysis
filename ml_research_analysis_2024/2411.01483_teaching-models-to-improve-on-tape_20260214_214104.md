---
ver: rpa2
title: Teaching Models to Improve on Tape
arxiv_id: '2411.01483'
source_url: https://arxiv.org/abs/2411.01483
tags:
- tasks
- feedback
- critique
- task
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CORGI, a reinforcement learning framework
  that trains language models to effectively utilize iterative corrective feedback
  during controlled text generation. The method involves simulating interaction sessions
  where the model receives textual feedback and rewards based on the best critique
  score obtained across multiple attempts.
---

# Teaching Models to Improve on Tape

## Quick Facts
- arXiv ID: 2411.01483
- Source URL: https://arxiv.org/abs/2411.01483
- Authors: Liat Bezalel; Eyal Orgad; Amir Globerson
- Reference count: 19
- Primary result: CORGI framework trains LLMs to effectively use iterative corrective feedback, achieving 2-5% improvement on source tasks and demonstrating strong meta-learning capabilities

## Executive Summary
This paper introduces CORGI, a reinforcement learning framework that trains language models to effectively utilize iterative corrective feedback during controlled text generation. The method involves simulating interaction sessions where the model receives textual feedback and rewards based on the best critique score obtained across multiple attempts. Evaluated on 11 controlled generation tasks using unlabeled data, CORGI consistently outperforms baseline RL methods that do not incorporate conversational feedback, showing 2-5% improvement on source tasks and demonstrating strong meta-learning capabilities with 4-1.2% gains on unseen target tasks. The results indicate that models trained with CORGI develop a generalizable skill of using corrective feedback to refine outputs across diverse generation tasks.

## Method Summary
CORGI employs reinforcement learning with PPO to train LLMs to improve outputs based on textual feedback across multiple attempts. The framework simulates interaction sessions where the model generates text, receives critique scores and feedback, and iteratively refines its output up to K attempts (4 during training, 10-20 during evaluation). The reward is based on the maximum critique score achieved across attempts. The method is evaluated in both single-task and multi-task settings across 11 diverse controlled generation tasks, with results showing consistent improvements over baseline RL approaches that don't incorporate feedback.

## Key Results
- CORGI achieves 2-5% improvement on source tasks compared to baseline RL methods
- Models trained with CORGI demonstrate meta-learning capabilities with 4-1.2% gains on unseen target tasks
- The framework shows consistent performance improvements across all 11 evaluated controlled generation tasks
- Multi-task training enables better generalization to new tasks with similar feedback structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CORGI improves controlled generation by training models to iteratively refine outputs based on corrective feedback
- Mechanism: The model learns to treat feedback as actionable signals, updating its generation strategy across multiple attempts to maximize the best critique score obtained in each session
- Core assumption: The model can learn to interpret textual feedback as optimization signals and apply them across attempts
- Evidence anchors:
  - [abstract]: "models trained with CORGI develop a generalizable skill of using corrective feedback to refine outputs across diverse generation tasks"
  - [section]: "We implement an RL based approach for training LLMs to improve their corrective skills"

### Mechanism 2
- Claim: Multi-task training with CORGI enables meta-learning and generalization to unseen tasks
- Mechanism: By training on multiple source tasks with varying constraints, the model develops transferable skills for interpreting and applying feedback across different generation contexts
- Core assumption: Skills learned from feedback interpretation on source tasks transfer to new tasks with similar feedback structures
- Evidence anchors:
  - [abstract]: "CORGI's interactive framework enables meta-learning, allowing the LLM to generalize better to guided interaction in new tasks"
  - [section]: "We observe that these trained models also improve when using feedback in tasks they were not trained on"

### Mechanism 3
- Claim: Reinforcement learning with PPO optimizes the model's ability to use feedback iteratively
- Mechanism: The PPO algorithm maximizes expected reward based on the best critique score across multiple attempts, encouraging the model to learn effective refinement strategies
- Core assumption: The PPO framework with its reward structure effectively captures the optimization goal of improving output quality through feedback
- Evidence anchors:
  - [section]: "We utilize reinforcement learning... PPO (Schulman et al. 2017), a method proven effective in Reinforcement Learning from Human Feedback (RLHF)"

## Foundational Learning

- Concept: Reinforcement Learning and PPO optimization
  - Why needed here: CORGI uses PPO to optimize the model's ability to improve outputs based on feedback across multiple attempts
  - Quick check question: What is the key difference between standard RL and RLHF, and why is PPO appropriate for this feedback-based optimization?

- Concept: Iterative refinement and multi-attempt generation
  - Why needed here: The framework relies on multiple generation attempts with feedback, requiring understanding of how iterative processes can improve output quality
  - Quick check question: How does limiting attempts to K and rewarding the best score influence the model's generation strategy?

- Concept: Meta-learning and transfer learning
  - Why needed here: CORGI aims to teach generalizable skills that transfer to unseen tasks, requiring understanding of how models learn transferable capabilities
  - Quick check question: What conditions must be met for feedback interpretation skills learned on source tasks to successfully transfer to target tasks?

## Architecture Onboarding

- Component map:
  - Generator model (LLM) that produces text outputs
  - Critique environment that evaluates outputs and provides scores + textual feedback
  - PPO trainer that optimizes the generator based on rewards
  - Data pipeline that samples prompts from multiple tasks
  - Mask system that differentiates action tokens from non-action tokens

- Critical path:
  1. Sample task prompt from dataset
  2. Generator produces output with greedy decoding
  3. Critique evaluates output, returns score and feedback
  4. Store session trajectory with all attempts and critiques
  5. After K attempts or perfect score, compute reward as max score
  6. PPO updates model parameters based on trajectories
  7. Repeat for multiple epochs across all tasks

- Design tradeoffs:
  - Number of attempts (K): More attempts allow better refinement but increase computational cost
  - KL control coefficient: Higher values ensure stability but may slow learning
  - Task diversity in multi-task training: More tasks improve generalization but may dilute task-specific performance
  - Feedback detail level: Detailed feedback provides more learning signal but may be harder to interpret

- Failure signatures:
  - Model shows no improvement on source tasks after training
  - Model fails to generalize to target tasks despite multi-task training
  - Training becomes unstable with high KL divergence
  - Model generates repetitive or irrelevant text instead of refining based on feedback

- First 3 experiments:
  1. Single-task training on numerical planning task: Verify CORGI improves performance compared to Vanilla-Llama on a simple constraint satisfaction task
  2. Ablation with binary feedback only: Test whether detailed feedback is essential by comparing full feedback vs. binary "incorrect" messages
  3. Transfer test on unseen task: Train on source tasks, then evaluate on clustering task to verify meta-learning capability

## Open Questions the Paper Calls Out
- How does the model learn to translate textual feedback into actionable improvements in generation?
- Can the textual feedback itself be optimized rather than using human-designed feedback?
- How well would CORGI generalize to creative writing tasks where critiques are likely to be suboptimal?

## Limitations
- CORGI requires curated feedback systems and multiple-task training setups, limiting practical deployment in resource-constrained environments
- The method's effectiveness depends on the quality and specificity of textual feedback, which may not generalize well to subjective tasks like creative writing
- Computational overhead of multiple generation attempts increases wall-clock time compared to single-attempt approaches

## Confidence
- Source task improvements (2-5% gains): Medium
- Meta-learning capabilities (4-1.2% gains on unseen tasks): Medium
- Scalability to real-world applications: Low

## Next Checks
1. Test CORGI's performance with progressively less detailed feedback (binary vs. detailed) to quantify the value of rich textual critiques
2. Evaluate the computational overhead and wall-clock time differences between single-attempt and multi-attempt generation across different task types
3. Assess CORGI's robustness to noisy or contradictory feedback by introducing controlled perturbations in the critique signals during training