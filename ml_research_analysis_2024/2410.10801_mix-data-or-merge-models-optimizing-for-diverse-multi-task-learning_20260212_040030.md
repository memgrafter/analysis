---
ver: rpa2
title: Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning
arxiv_id: '2410.10801'
source_url: https://arxiv.org/abs/2410.10801
tags:
- safety
- merging
- performance
- general
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares two approaches\u2014data mixing and model\
  \ merging\u2014for balancing safety and general performance in multilingual language\
  \ models. The authors train separate models for safety and general-purpose tasks,\
  \ then merge them using four techniques (Linear, SLERP, TIES, DARE-TIES) and compare\
  \ against traditional data mixing."
---

# Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning

## Quick Facts
- arXiv ID: 2410.10801
- Source URL: https://arxiv.org/abs/2410.10801
- Reference count: 23
- Primary result: Model merging outperforms data mixing for balancing safety and general performance in multilingual models, with SLERP achieving 7% gains in general performance and 3.1% reductions in harmful generations.

## Executive Summary
This paper compares two approaches—data mixing and model merging—for balancing safety and general performance in multilingual language models. The authors train separate models for safety and general-purpose tasks, then merge them using four techniques (Linear, SLERP, TIES, DARE-TIES) and compare against traditional data mixing. They find that model merging outperforms data mixing, with SLERP achieving 7% gains in general performance and 3.1% reductions in harmful generations over the baseline. The study also reveals that merging DPO checkpoints is more effective than SFT checkpoints, and that merging monolingual models across languages improves performance compared to multilingual training. Overall, merging proves more effective than data mixing for building safe, multilingual models.

## Method Summary
The authors train separate models for safety and general tasks on Aya 23 8B, then apply four merging techniques (Linear, SLERP, TIES, DARE-TIES) to combine these models. They compare merged models against a baseline of 15% safety data mixing. The study evaluates both objective-based merging (safety vs. general) and language-based merging (monolingual models across languages). Performance is measured on Aya Red-teaming (safety) and Multilingual Dolly-200 (general performance) using LLM-as-a-judge with GPT-4. The experiments test different merging sequences and compare DPO versus SFT checkpoints.

## Key Results
- Model merging outperforms data mixing, with SLERP achieving 7% gains in general performance and 3.1% reductions in harmful generations
- DPO checkpoints merge more effectively than SFT checkpoints, with average gains of 2.8% (general) and 2.2% (safety) over the base model
- Language-based merging shows mixed results, improving performance for related languages (EN, FR, SP) but degrading for some distant language pairs
- Merging models before preference training yields better outcomes than merging after preference training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model merging outperforms data mixing for balancing safety and general performance.
- Mechanism: Merging allows separate optimization for distinct objectives (safety vs. general) before combining, avoiding the interference and trade-offs inherent in mixed training.
- Core assumption: Separate optimization followed by merging preserves task-specific capabilities better than simultaneous mixed training.
- Evidence anchors:
  - [abstract] "objective-based merging is more effective than mixing data, with improvements of up to 8% and 10% in general performance and safety respectively."
  - [section 3.1] "Merging almost always benefits general performance, with all techniques but one (TIES) outperforming the 15% Safety Mix baseline"
  - [corpus] Weak evidence - corpus papers discuss merging but not specifically safety-general trade-offs
- Break condition: If merged models experience catastrophic interference or if task-specific parameters cannot be preserved during merging

### Mechanism 2
- Claim: DPO checkpoints merge more effectively than SFT checkpoints for safety alignment.
- Mechanism: DPO training creates more separable reward signals that can be preserved during merging, while SFT may create more overlapping parameter changes.
- Core assumption: DPO produces cleaner, more disentangled parameter updates for safety objectives compared to SFT.
- Evidence anchors:
  - [section 3.2] "Our experiments show larger consistent improvements when merging DPO checkpoints, with average gains of 2.8% and 2.2% over the base model across the four merging methods assessed for general performance and safety respectively."
  - [section 2.6] "continually preference-tuning the models after performing the merge yields better outcomes"
  - [corpus] Weak evidence - corpus discusses merging but not DPO vs SFT differences
- Break condition: If DPO training creates unstable checkpoints or if the preference signal degrades during merging

### Mechanism 3
- Claim: Language-based merging (merging monolingual models) can outperform multilingual training for some language combinations.
- Mechanism: Monolingual models can develop language-specific optimizations without interference, and merging preserves these optimizations better than multilingual training.
- Core assumption: Language-specific training followed by merging preserves language-specific capabilities better than training on mixed multilingual data.
- Evidence anchors:
  - [section 3.4] "when compared to the base model, we successfully increase general performance and reduce harm generations across all variants" and "merging 3 models ("[EN,FR,SP]") yields better performance on both tasks compared to merging 6 models"
  - [section 3.3] "merging leads to performance degradation in some languages compared to data mixing" but still maintains "an absolute win-rate above 50% for all languages"
  - [corpus] Weak evidence - corpus discusses merging but not language-specific effects
- Break condition: If cross-lingual interference during merging outweighs the benefits of language-specific optimization

## Foundational Learning

- Concept: Parameter space interpolation
  - Why needed here: Model merging techniques like SLERP and Linear interpolation rely on understanding how to combine parameter vectors in high-dimensional space
  - Quick check question: What's the difference between SLERP and Linear interpolation in parameter space?

- Concept: Multi-task learning trade-offs
  - Why needed here: Understanding how optimizing for multiple objectives simultaneously can lead to interference and degraded performance
  - Quick check question: Why might training on mixed safety and general data lead to worse performance than separate training followed by merging?

- Concept: Language family relationships
  - Why needed here: The paper shows that merging models from closely related languages (EN,FR,SP) performs better than merging from distantly related languages
  - Quick check question: How might the linguistic similarity between languages affect the success of language-based merging?

## Architecture Onboarding

- Component map:
  - Aya 23 8B base models
  - Training data pipelines (safety datasets, general datasets, data mixtures)
  - Merging algorithms (Linear, SLERP, TIES, DARE-TIES)
  - Evaluation framework (Aya Red-teaming, Multilingual Dolly-200)
  - LLM-as-judge system (GPT-4)

- Critical path:
  1. Train base models on data mixtures
  2. Select best checkpoints for merging
  3. Apply merging algorithms
  4. Evaluate merged models
  5. Compare against data mixing baseline

- Design tradeoffs:
  - Safety vs general performance (inherent tension)
  - Number of languages to merge (3 vs 6 shows different performance)
  - Merging stage (before vs after preference training)
  - Algorithm choice (Linear, SLERP, TIES, DARE-TIES have different characteristics)

- Failure signatures:
  - Catastrophic interference during merging
  - Degraded performance on specific languages
  - Inconsistent improvements across merging algorithms
  - Increased harmful generations after merging

- First 3 experiments:
  1. Linear merge of 0% Safety and 100% Safety SFT checkpoints with varying weights
  2. SLERP merge of DPO checkpoints optimized for English and French
  3. TIES merge of monolingual models for Spanish, Arabic, and Russian

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of model merging vary across different language families and linguistic structures?
- Basis in paper: [explicit] The paper mentions that "each language introduces unique and varied learning challenges across tasks" and observes "uneven gains across languages" in their results.
- Why unresolved: The paper only evaluates six languages from five different families (English, Hindi, French, Spanish, Arabic, Russian) and shows varying performance across them, but doesn't deeply analyze why certain languages benefit more from merging than others.
- What evidence would resolve it: A systematic study comparing merging effectiveness across multiple language families with different typological features (e.g., agglutinative vs. fusional languages, languages with different word order) would help understand the relationship between linguistic structure and merging performance.

### Open Question 2
- Question: What is the optimal sequence of operations (SFT, merging, DPO) for achieving the best balance between safety and general performance?
- Basis in paper: [explicit] The paper investigates this in section 3.6, finding that "continually preference-tuning the models after performing the merge yields better outcomes" but only compares two specific sequences.
- Why unresolved: The paper only tests two sequences (SFT→merge→DPO vs SFT→DPO→merge) and shows the former is better, but doesn't explore other possible sequences or combinations of these operations.
- What evidence would resolve it: A comprehensive ablation study testing various sequences and timing of SFT, merging, and DPO operations would reveal optimal training pipelines for different objectives.

### Open Question 3
- Question: How does model merging scale when applied to models with significantly different architectures or training histories?
- Basis in paper: [inferred] The paper focuses on merging models that share the same pre-trained base model and discusses merging as an efficient alternative to training from scratch, implying some limitations on model compatibility.
- Why unresolved: The paper only tests merging on models fine-tuned from the same base model and doesn't explore merging across different architectures, sizes, or training paradigms.
- What evidence would resolve it: Experiments merging models with different architectures (e.g., different number of layers, attention mechanisms), sizes (small vs. large models), or training histories (different base models) would reveal the limits and potential of model merging.

## Limitations
- Experiments limited to 6 languages from similar language families, limiting cross-linguistic generalization
- Evaluation relies on LLM-as-a-judge (GPT-4) which may not capture nuanced human judgments
- Results may not generalize to smaller or larger model architectures beyond the 8B parameter models tested

## Confidence
- Model merging effectiveness (High): Consistent improvements across multiple merging algorithms and evaluation metrics
- DPO vs SFT checkpoint merging (Medium): Limited sample size with only one round of preference training per objective
- Language-based merging (Medium): Mixed results with performance degradation for some distant language pairs

## Next Checks
1. Test merging stability across different model scales (1B, 3B, 70B) to verify that the merging benefits scale proportionally and identify potential breaking points
2. Conduct human evaluation studies to validate LLM-as-a-judge assessments, particularly for safety metrics where automated red-teaming may miss context-dependent harmful content
3. Experiment with more diverse language families (e.g., mixing Indo-European with Uralic or Sino-Tibetan languages) to determine the boundaries of effective language-based merging and identify linguistic factors that predict merging success