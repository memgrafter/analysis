---
ver: rpa2
title: What is Left After Distillation? How Knowledge Transfer Impacts Fairness and
  Bias
arxiv_id: '2410.08407'
source_url: https://arxiv.org/abs/2410.08407
tags:
- student
- distillation
- fairness
- teacher
- distilled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Knowledge Distillation (KD) is widely used for model compression,
  but its impact on fairness and class-wise accuracy remains underexplored. This study
  systematically evaluates how KD temperature influences bias and fairness across
  multiple image datasets (CIFAR-10/100, SVHN, Tiny ImageNet, ImageNet) and fairness-sensitive
  datasets (CelebA, Trifeature, HateXplain).
---

# What is Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias

## Quick Facts
- **arXiv ID**: 2410.08407
- **Source URL**: https://arxiv.org/abs/2410.08407
- **Reference count**: 18
- **Primary result**: KD temperature significantly affects class-wise accuracy and fairness metrics, with optimal temperatures improving fairness while extreme temperatures degrade both accuracy and fairness.

## Executive Summary
This study systematically investigates how knowledge distillation (KD) temperature impacts fairness and class-wise accuracy across multiple image datasets and fairness-sensitive benchmarks. The research finds that increasing KD temperature can improve demographic parity and equalized odds metrics, sometimes surpassing the teacher model's fairness. However, extreme temperatures (≥20) degrade both accuracy and fairness due to overly uniform teacher outputs. For balanced image datasets, up to 41% of classes show statistically significant accuracy changes due to KD, with the number of affected classes increasing with temperature. The study highlights KD's nuanced effects on bias and fairness, urging caution in sensitive applications.

## Method Summary
The study evaluates KD across multiple image datasets (CIFAR-10/100, SVHN, Tiny ImageNet, ImageNet) and fairness-sensitive datasets (CelebA, Trifeature, HateXplain). Teacher models (ResNet-56, ResNet-50, DenseNet, ViT) are trained first, then distilled to student models (ResNet-20, ResNet-18) at temperatures T=2-10 and extreme temperatures T=20,30,40. Baseline student models are trained from scratch for comparison. The analysis uses Welch's t-test to determine statistically significant class-wise accuracy changes, and measures fairness via Demographic Parity Difference, Equalized Odds Difference, and individual fairness metrics. Data augmentation and standard training procedures are employed across experiments.

## Key Results
- Up to 41% of classes show statistically significant accuracy changes due to KD temperature variations
- Increasing KD temperature improves demographic parity and equalized odds metrics for fairness-sensitive datasets
- Extreme temperatures (≥20) degrade both accuracy and fairness due to overly uniform softmax outputs
- Higher temperatures improve individual fairness by encouraging consistent predictions for similar inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Higher distillation temperatures improve fairness by smoothing teacher output distributions.
- Mechanism: Increasing temperature softens the teacher's softmax outputs, producing richer, less peaked probability distributions that transfer more nuanced class relationships to the student. This smoothness reduces the student's sensitivity to imbalanced or noisy training signals.
- Core assumption: The teacher model has learned fair representations that can be effectively communicated through softened outputs.
- Evidence anchors:
  - [abstract] "increasing the distillation temperature improves the distilled student model’s fairness"
  - [section] "by increasing the value of the temperature, a distilled student model’s fairness improves"
  - [corpus] Weak - no direct evidence in neighbor papers about temperature-fairness link
- Break condition: When temperature becomes too high (≥20 in experiments), softmax outputs become nearly uniform, losing discriminative information and degrading both accuracy and fairness.

### Mechanism 2
- Claim: Distillation selectively amplifies or suppresses class-level biases present in the teacher model.
- Mechanism: The student model learns to mimic the teacher's class probability patterns. If the teacher is biased toward certain classes, distillation reinforces those biases; if the teacher has learned to mitigate bias, distillation transfers that mitigation.
- Core assumption: Class-wise accuracy changes between teacher and distilled student are statistically significant and non-random.
- Evidence anchors:
  - [abstract] "as many as 41% of the classes are statistically significantly affected by distillation"
  - [section] "a small subset of classes is disproportionately affected by distillation"
  - [corpus] Weak - neighbor papers focus on different aspects of KD, not class-wise bias transfer
- Break condition: When the student model capacity is too limited to capture the teacher's nuanced class patterns, or when temperature is too low to transfer meaningful information.

### Mechanism 3
- Claim: Distillation improves individual fairness by encouraging consistent predictions for similar inputs.
- Mechanism: Higher temperatures produce smoother probability distributions that better capture semantic similarities between inputs. The student learns to give similar predictions to similar inputs, improving Lipschitz continuity.
- Core assumption: Individual fairness can be measured and improved through distributional smoothness in teacher outputs.
- Evidence anchors:
  - [abstract] "higher temperatures also improve the distilled student model’s individual fairness"
  - [section] "ensuring similar individuals receive similar predictions" and "quantifies fairness as the degree of variation in predictions with respect to input similarity"
  - [corpus] Weak - neighbor papers don't address individual fairness in KD context
- Break condition: When the temperature is too high, the nearly uniform softmax outputs provide no discriminative information, preventing the student from learning meaningful similarity relationships.

## Foundational Learning

- **Concept**: Statistical significance testing (Welch's t-test)
  - Why needed here: To determine whether observed changes in class-wise accuracy are meaningful or just random noise from stochastic training
  - Quick check question: What p-value threshold was used to determine statistical significance of class-wise accuracy changes?

- **Concept**: Fairness metrics (Demographic Parity Difference and Equalized Odds Difference)
  - Why needed here: To quantify bias across demographic groups and measure improvements in fairness from distillation
  - Quick check question: How do DPD and EOD differ in what aspects of fairness they capture?

- **Concept**: Temperature scaling in softmax
  - Why needed here: To understand how temperature affects the probability distributions used for knowledge transfer
  - Quick check question: What happens to the softmax output distribution as temperature approaches infinity?

## Architecture Onboarding

- **Component map**: Teacher model → Softmax with temperature → Probability distribution → Distillation loss → Student model training
- **Critical path**: Teacher pre-training → Temperature selection → Distillation training → Fairness evaluation
- **Design tradeoffs**: Higher temperature improves fairness but risks accuracy degradation; lower temperature preserves accuracy but may not transfer fairness improvements
- **Failure signatures**: Uniform softmax outputs at high temperatures, class-wise accuracy degradation, fairness metrics worsening despite temperature increase
- **First 3 experiments**:
  1. Train teacher model on balanced dataset, measure baseline class-wise accuracy and fairness metrics
  2. Perform distillation at varying temperatures (T=2,5,10), measure changes in class-wise accuracy significance and fairness metrics
  3. Test very high temperatures (T=20,30,40) to observe degradation thresholds for both accuracy and fairness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the impact of knowledge distillation on fairness and bias vary across different model architectures beyond those tested (ResNet, DenseNet, ViT, BERT)?
- Basis in paper: [inferred] The paper tests distillation effects on multiple architectures but acknowledges results may not generalize to all possible architectures, especially newer or domain-specific ones.
- Why unresolved: The study focuses on common architectures; less common or emerging architectures might exhibit different bias/fairness behaviors during distillation.
- What evidence would resolve it: Systematic experiments applying the same distillation analysis across a broader range of model architectures, including newer transformer variants and domain-specific models.

### Open Question 2
- Question: Does the observed improvement in fairness through distillation hold when using real-world datasets with inherent class imbalance rather than balanced benchmarks?
- Basis in paper: [explicit] The paper notes that their datasets are balanced and lack the long-tailed distributions common in real-world data, and explicitly states caution is needed when interpreting results in imbalanced settings.
- Why unresolved: All tested datasets are balanced, so the impact of distillation on fairness in imbalanced scenarios remains unknown.
- What evidence would resolve it: Replicating the distillation and fairness analysis on real-world imbalanced datasets like medical imaging, surveillance, or natural language datasets with skewed class distributions.

### Open Question 3
- Question: What is the long-term generalization effect of distillation on fairness metrics when models are continuously updated or fine-tuned after initial distillation?
- Basis in paper: [inferred] The study only evaluates fairness at a fixed point after distillation training, with no exploration of how fairness evolves during further training or deployment.
- Why unresolved: Continuous learning or fine-tuning could potentially erode or enhance fairness gains achieved through initial distillation, but this dynamic is not explored.
- What evidence would resolve it: Longitudinal studies tracking fairness metrics during ongoing model updates, fine-tuning, or adaptation to new data distributions post-distillation.

## Limitations

- The study doesn't verify the teacher model's fairness baseline before distillation, making it unclear whether improvements come from KD or pre-existing teacher characteristics
- Experiments use relatively small vision models (ResNet-56 to ResNet-20), limiting generalizability to larger architectures where KD dynamics may differ
- All tested datasets are balanced, so the impact of distillation on fairness in imbalanced scenarios remains unknown

## Confidence

- **High Confidence**: The temperature-degradation relationship (T≥20 causing uniform outputs and accuracy/fairness loss) is well-supported by theoretical softmax behavior and experimental evidence
- **Medium Confidence**: Claims about class-wise accuracy changes being "statistically significant" are supported by Welch's t-test methodology, but the practical significance of affecting 41% of classes needs further context about which specific classes are impacted
- **Low Confidence**: The assertion that KD can "sometimes surpass" teacher fairness is concerning given the mechanism described (KD transfers teacher characteristics). Without evidence that teachers were initially biased, this claim requires additional validation

## Next Checks

1. **Teacher Fairness Baseline Verification**: Measure and report fairness metrics of teacher models before distillation to establish whether observed improvements are truly KD-driven or reflect teacher characteristics
2. **Cross-Architecture Generalization Test**: Repeat key experiments with larger models (e.g., ResNet-50 to ResNet-18, or Vision Transformers) to validate that temperature-fairness relationships hold across scales
3. **Class-Specific Impact Analysis**: Identify which specific classes show significant accuracy changes and analyze whether these correlate with dataset bias or represent random variation