---
ver: rpa2
title: An Application of Large Language Models to Coding Negotiation Transcripts
arxiv_id: '2407.21037'
source_url: https://arxiv.org/abs/2407.21037
tags:
- coding
- code
- transcripts
- sentences
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Vanderbilt AI Negotiation Lab applied Large Language Models
  to automatically code negotiation transcripts, replacing costly and time-consuming
  human coding. Starting with 770 ideal example sentences across 19 codes, they tested
  zero-shot, fine-tuning, and in-context learning approaches.
---

# An Application of Large Language Models to Coding Negotiation Transcripts

## Quick Facts
- arXiv ID: 2407.21037
- Source URL: https://arxiv.org/abs/2407.21037
- Reference count: 18
- Primary result: LLM-based coding of negotiation transcripts achieved 73-91% accuracy

## Executive Summary
The Vanderbilt AI Negotiation Lab applied Large Language Models to automatically code negotiation transcripts, replacing costly and time-consuming human coding. Starting with 770 ideal example sentences across 19 codes, they tested zero-shot, fine-tuning, and in-context learning approaches. In-context learning using Claude 3 achieved 96% accuracy on ideal sentences, but only 30% on real transcripts. Switching to training on actual human-coded transcripts improved performance to 73-91% accuracy depending on validation method. The model also handled Chinese text. This demonstrates LLMs can effectively code negotiation transcripts, though optimal performance requires training on real transcripts rather than ideal examples. The approach provides a template for applying LLMs to other coding tasks in social science research.

## Method Summary
The researchers developed an LLM-based system to automatically code negotiation transcripts using in-context learning with Claude 3 Opus. They started with 770 ideal example sentences across 19 codes from the Jäckel coding scheme, then tested various approaches including zero-shot, fine-tuning, and in-context learning. When initial attempts using ideal sentences showed low accuracy on real transcripts (30%), they switched to training the model on actual human-coded transcripts. The system required consistency across five runs, with at least three agreeing on the same code for each sentence. The approach incorporated context about speakers and prior dialogue, and was tested on both English and Chinese transcripts.

## Key Results
- In-context learning with Claude 3 achieved 96% accuracy on ideal example sentences
- Accuracy on real transcripts improved from 30% to 73-91% when training on human-coded transcripts instead of ideal examples
- The model successfully coded Chinese transcripts with 89% accuracy
- Consistency measure (3/5 runs agreeing) improved reliability of outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning with Claude 3 achieved high accuracy on ideal sentences because the prompt included both code definitions and multiple example sentences, allowing the model to learn the specialized negotiation language.
- Mechanism: The large context window (100k+ tokens) allowed the model to process extensive training material within a single prompt, making the coding scheme "top-of-mind" during analysis.
- Core assumption: The model can effectively learn from examples provided in the prompt without requiring parameter updates.
- Evidence anchors:
  - [abstract]: "In-context learning using Claude 3 achieved 96% accuracy on ideal sentences"
  - [section]: "In-context learning refers to providing the base model with one set of instructions (a prompt) that combines the training material with the analysis instructions"
  - [corpus]: Weak - the corpus contains no directly relevant papers on this specific in-context learning mechanism
- Break condition: If the prompt exceeds context window limits or if the model cannot maintain consistency across runs.

### Mechanism 2
- Claim: Training on real human-coded transcripts improved model performance on real conversations compared to ideal sentences because it exposed the model to natural language patterns and contextual cues.
- Mechanism: The model learned to recognize negotiation-specific language patterns and contextual dependencies by analyzing actual coded transcripts rather than artificially constructed examples.
- Core assumption: Real transcripts contain sufficient examples of each code for the model to learn effectively.
- Evidence anchors:
  - [abstract]: "Switching to training on actual human-coded transcripts improved performance to 73-91% accuracy"
  - [section]: "What became clear is that we had taught the model with ideal sentences that were clear examples of codes, while real-life conversations are more halting, broken, incomplete, and ambiguous"
  - [corpus]: Weak - corpus lacks papers specifically addressing this transcript-based training approach
- Break condition: If training transcripts don't adequately represent the diversity of negotiation language or if certain codes appear too infrequently.

### Mechanism 3
- Claim: The consistency measure (requiring 3/5 runs to agree) improved reliability by filtering out model uncertainty and ensuring stable outputs.
- Mechanism: By running the model five times and requiring majority agreement, the approach reduces the impact of stochastic variations in model responses.
- Core assumption: The model's uncertainty is detectable through output variation across runs.
- Evidence anchors:
  - [section]: "To test how consistently the model coded a transcript's sentences, we decided to repeat the coding process five times, producing five codes for each sentence"
  - [section]: "Some variation across five runs might be acceptable, but before we would allow the model to report a code to users, the model had to give the same code at least three out of five times"
  - [corpus]: Weak - no corpus evidence on this specific consistency validation approach
- Break condition: If the model consistently produces different outputs even for clear cases, or if the consistency threshold is too strict and leaves too many sentences uncoded.

## Foundational Learning

- Concept: Context window limitations
  - Why needed here: Understanding context window constraints was critical for choosing in-context learning over fine-tuning and for managing prompt length
  - Quick check question: What is the approximate word length limit for Claude 2's context window?

- Concept: Tokenization
  - Why needed here: The model processes input in tokens, not words, which affects how much content can fit in prompts and how to count input length
  - Quick check question: How many tokens approximately equal 400 words for BERT?

- Concept: Stochastic generation in LLMs
  - Why needed here: Understanding that LLMs can produce different outputs for the same input explains why consistency measures were needed
  - Quick check question: Why might an LLM produce different outputs when given the same input multiple times?

## Architecture Onboarding

- Component map: Claude 3 Opus (base model) → Prompt (code definitions + training transcripts + analysis instructions) → API call → Five runs → Consistency check → Output
- Critical path: Prompt construction → API call with temperature control → Five parallel runs → Majority voting → Output generation
- Design tradeoffs: In-context learning trades off fine-tuning's permanent knowledge updates for flexibility and access to more powerful base models
- Failure signatures: Inconsistent outputs across runs, skipped sentences (model "laziness"), format deviations, context window overflow
- First 3 experiments:
  1. Zero-shot testing with BERT and BART using only code names as categories
  2. Fine-tuning BERT with domain-specific data to create BERT-NegCodingJäckel
  3. In-context learning with Claude 1 using code definitions and sample sentences in the prompt

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of training transcripts needed to achieve high accuracy across different coding schemes and negotiation contexts?
- Basis in paper: [inferred] The paper mentions testing different numbers of training transcripts and finding that more than five degraded performance, but suggests this may vary by coding scheme complexity and transcript length
- Why unresolved: The study only tested one coding scheme with a limited number of transcript combinations. Different coding schemes with varying numbers of codes, or negotiations with different complexity levels, may require different training set sizes
- What evidence would resolve it: Systematic testing across multiple coding schemes with varying numbers of codes, different negotiation contexts (simple vs complex), and varying transcript lengths to identify optimal training set sizes for each scenario

### Open Question 2
- Question: How well do LLM-based coding systems generalize across different negotiation contexts beyond the Sweet Shop simulation?
- Basis in paper: [explicit] The paper notes that accuracy dropped from 73% to 65% when testing on different simulations (Cartoon and Les Flores), but only tested 12 additional transcripts
- Why unresolved: The study only tested two additional negotiation contexts with a small sample size. It's unclear how well the approach would work with negotiations from entirely different domains (business vs international vs personal)
- What evidence would resolve it: Testing the model across a wide range of negotiation contexts including business negotiations, international diplomacy, labor disputes, and personal disputes, with sufficient sample sizes to establish reliability

### Open Question 3
- Question: What is the minimum level of human-model agreement that can be considered acceptable for research purposes?
- Basis in paper: [explicit] The paper states "What level of match is an acceptable is an unresolved issue" and discusses mismatch analysis as a potential approach
- Why unresolved: There are no established standards for acceptable agreement levels between human and AI coders, and the paper suggests that even human-human agreement may not be perfect
- What evidence would resolve it: Empirical studies comparing research findings using different thresholds of human-model agreement, along with assessments of how different agreement levels impact research conclusions and validity

### Open Question 4
- Question: How does the performance of LLM-based coding systems vary across different languages and cultural contexts?
- Basis in paper: [explicit] The paper tested Chinese translation of 27 speaking units and found 89% accuracy, but this was a limited test
- Why unresolved: The study only tested one non-English language with a small sample size. It's unclear how well the approach would work with languages that have very different structures or in cultural contexts with different communication norms
- What evidence would resolve it: Systematic testing across multiple languages representing different language families, along with testing on negotiations from different cultural contexts to identify any systematic biases or limitations

## Limitations

- The proprietary nature of Claude 3 and lack of public access to exact prompts and training materials limits reproducibility of specific accuracy figures
- The transition from 96% accuracy on ideal sentences to only 30% on real transcripts highlights challenges in applying LLMs to real-world messy data
- The consistency measure (requiring 3/5 runs to agree) suggests inherent stochasticity in model outputs that cannot be fully eliminated

## Confidence

- **High Confidence**: The general methodology of using in-context learning with Claude 3 for coding negotiation transcripts is sound and well-documented. The improvement from training on real transcripts versus ideal sentences is clearly demonstrated.
- **Medium Confidence**: The specific accuracy figures (73-91%) are based on the authors' implementation and validation approach, but exact reproduction would require access to proprietary tools and potentially similar training data. The consistency measure adds reliability but may not be universally applicable.
- **Low Confidence**: The generalizability of these results to other coding tasks or negotiation contexts is uncertain, as the paper focuses on a specific coding scheme and language context.

## Next Checks

1. Test the consistency measure by running the same transcripts through the model multiple times and analyzing the distribution of agreement rates across different code categories.
2. Evaluate the model's performance on transcripts from different negotiation contexts (beyond the Sweet Shop, Cartoon, and Les Flores scenarios) to assess generalizability.
3. Compare the in-context learning approach against fine-tuning alternatives using publicly available models to quantify the tradeoff between flexibility and performance.