---
ver: rpa2
title: Match, Compare, or Select? An Investigation of Large Language Models for Entity
  Matching
arxiv_id: '2405.16884'
source_url: https://arxiv.org/abs/2405.16884
tags:
- matching
- entity
- llms
- strategy
- record
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Entity matching based on large language models (LLMs) has shown
  great promise, but current approaches typically follow a binary matching paradigm
  that ignores global consistency among record relationships. This paper investigates
  three strategies for LLM-based entity matching: matching, comparing, and selecting.'
---

# Match, Compare, or Select? An Investigation of Large Language Models for Entity Matching

## Quick Facts
- arXiv ID: 2405.16884
- Source URL: https://arxiv.org/abs/2405.16884
- Reference count: 23
- Current LLM approaches for entity matching ignore global consistency among record relationships

## Executive Summary
This paper investigates three strategies for LLM-based entity matching: matching (binary classification), comparing (relative ranking), and selecting (direct selection from list). The selecting strategy, which incorporates global record interactions, achieves an average 13.39% improvement in F1 over the matching strategy. To address challenges like position bias and long context requirements, the authors propose COMEM, a compositional framework that combines multiple strategies and LLMs. COMEM further improves F1 by up to 8.46% while reducing costs through filtering with smaller LLMs.

## Method Summary
The paper proposes three LLM-based entity matching strategies and evaluates them on 8 ER datasets. The matching strategy uses binary classification for pairwise comparison, the comparing strategy ranks candidates using relative scores, and the selecting strategy directly identifies matches from all candidates in context. The COMEM framework combines these approaches by first filtering candidates with a medium-sized LLM using the matching strategy, then applying the selecting strategy with a larger LLM to the top candidates. The study tests 10 different LLMs including ChatGPT, Llama2, Mistral, and others.

## Key Results
- Selecting strategy improves average F1 by 13.39% over matching strategy
- COMEM framework improves average F1 of selecting strategy by up to 8.46% while reducing costs
- Matching strategy outperforms comparing strategy for ranking/filtering tasks despite comparing offering finer-grained similarity scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting strategy with global record interactions improves F1 over pairwise matching by 13.39% on average.
- Mechanism: By feeding all candidate records in the same context to the LLM, it can use inter-record distinctions (e.g., different model names) to disambiguate matches.
- Core assumption: LLMs can handle longer contexts and understand subtle distinctions among multiple similar records.
- Evidence anchors:
  - [abstract]: "the global selecting strategy is often the most effective"
  - [section 4.3]: "Feeding LLMs all candidate matches in the same context at a time allows LLMs to make better decisions by considering interactions between candidate records"
- Break condition: If the LLM's context window is exceeded or if the candidates are too similar to distinguish.

### Mechanism 2
- Claim: COMEM framework reduces cost by filtering candidates with a smaller LLM before fine-grained selection with a larger LLM.
- Mechanism: Preliminary ranking and filtering narrows down to top-k candidates, reducing expensive LLM invocations in the selecting stage.
- Core assumption: A medium-sized LLM can perform effective ranking and filtering, preserving the best candidates for the expensive selecting step.
- Evidence anchors:
  - [section 4.4]: "We first utilize a medium-sized LLM to rank and filter the candidate matches... We then utilize an LLM to identify the match... with the selecting strategy"
  - [abstract]: "COMEM further improves the average F1 of the selecting strategy by up to 8.46% while reducing costs"
- Break condition: If the filtering step incorrectly removes the true match or if the medium-sized LLM cannot distinguish relevant candidates.

### Mechanism 3
- Claim: Matching strategy is better for ranking/filtering than comparing strategy, even though comparing offers finer-grained similarity scores.
- Mechanism: Matching uses simpler pairwise labels that align with training tasks LLMs have seen, making it more robust for sorting large candidate lists.
- Core assumption: LLMs trained on pairwise tasks (NLI, QA) generalize better to pairwise matching than to triplewise comparison.
- Evidence anchors:
  - [section 5.3]: "the matching strategy outperforms the comparing strategy under different model parameter sizes"
  - [section 5.3]: "This may be due to the fact that these models are trained on many pairwise tasks, such as natural language inference and question answering, but few triplewise tasks"
- Break condition: If the LLM has been fine-tuned on tasks requiring comparison of multiple items or if candidate lists are very short.

## Foundational Learning

- Concept: Entity Resolution (ER) and Entity Matching (EM)
  - Why needed here: Understanding that EM is about identifying matching records from candidate sets, often under constraints like 1-1 matching.
  - Quick check question: What is the difference between entity resolution and entity matching?

- Concept: Large Language Model prompting and in-context learning
  - Why needed here: Strategies depend on how well the LLM can be prompted to match, compare, or select from records.
  - Quick check question: How does few-shot prompting differ from zero-shot prompting in this context?

- Concept: Trade-offs between context length, number of candidates, and LLM accuracy
  - Why needed here: Selecting strategy suffers from position bias as more candidates are added; filtering mitigates this.
  - Quick check question: What happens to selection accuracy as the number of candidates increases beyond the LLM's context capacity?

## Architecture Onboarding

- Component map: Anchor record → blocking → pre-filtering (matching) → fine identification (selecting) → output
- Critical path: Anchor record → blocking → pre-filtering (matching) → fine identification (selecting) → output
- Design tradeoffs:
  - Cost vs. accuracy: Using smaller LLM for filtering saves cost but risks missing matches
  - Ranking quality vs. context length: More candidates improve recall but hurt selection accuracy
  - Strategy choice: Matching for filtering, selecting for identification; comparing is less effective
- Failure signatures:
  - High false positives: Likely in matching step with chat-tuned LLMs
  - Missed matches in top-k: Filtering step too aggressive or LLM poorly tuned
  - Low precision in selecting: Too many candidates passed to selecting step
- First 3 experiments:
  1. Run each strategy (matching, comparing, selecting) on a small dataset and compare F1, precision, recall.
  2. Test different k values (number of candidates passed to selecting) and observe F1/precision trade-off.
  3. Swap the filtering LLM with different model sizes and measure impact on final accuracy and cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt engineering approaches affect the performance of the three strategies across various datasets?
- Basis in paper: [inferred] The paper mentions that they only attempted few-shot prompting on the matching strategy and left exploration of better prompt engineering with different strategies to future work.
- Why unresolved: The authors explicitly state that they did not explore prompt engineering for the comparing and selecting strategies, despite acknowledging its importance for LLM-based applications.
- What evidence would resolve it: Systematic experiments comparing different prompt engineering techniques (few-shot, chain-of-thought, zero-shot) across all three strategies and multiple datasets, with ablation studies on prompt components.

### Open Question 2
- Question: What is the optimal number of candidate records (k) for the COMEM framework's filtering stage across different dataset characteristics?
- Basis in paper: [explicit] The ablation study shows k=4 was the sweet spot for ChatGPT but mentions this may vary by dataset and that dynamic adjustment could be explored.
- Why unresolved: The paper only tested a fixed k=4 value and acknowledged that the optimal value likely depends on dataset characteristics and LLM capabilities.
- What evidence would resolve it: Comparative analysis of COMEM performance across multiple k values (1-10) for each dataset, examining the relationship between dataset properties (record count, attribute count, match density) and optimal k values.

### Open Question 3
- Question: How does the performance of COMEM scale with larger candidate lists beyond the tested datasets?
- Basis in paper: [inferred] The paper tested datasets with up to 10 candidate matches per record, but notes that blocking steps often generate more candidates than LLMs can handle effectively.
- Why unresolved: The experimental setup was limited to datasets with relatively small candidate lists (10 max), while real-world blocking often produces hundreds of candidates.
- What evidence would resolve it: Experiments scaling the number of candidate matches to 50-200 per record, measuring performance degradation and the effectiveness of different filtering strategies at scale.

## Limitations
- Position bias in selecting strategy becomes increasingly problematic as candidate lists grow
- Study limited to 8 ER datasets with fixed blocking parameters (10 candidates per record)
- Cost reductions demonstrated but not thoroughly analyzed across different dataset characteristics

## Confidence
- **High**: The 13.39% F1 improvement of selecting over matching strategy (well-supported by experimental results across multiple datasets)
- **Medium**: The effectiveness of COMEM framework (demonstrates improvements but limited exploration of edge cases and varying blocking quality)
- **Medium**: The claim that matching strategy outperforms comparing for filtering (supported by results but lacks theoretical explanation for why this occurs)

## Next Checks
1. **Context Length Analysis**: Systematically test the selecting strategy's performance as candidate list size increases beyond 10, measuring the exact point where position bias degrades accuracy significantly.

2. **Blocking Quality Impact**: Evaluate COMEM's performance across different blocking quality levels (tight vs. loose blocking) to understand how filtering errors propagate through the pipeline.

3. **Strategy Transferability**: Test the matching/comparing/selecting strategies on datasets with different characteristics (e.g., longer text fields, different entity types) to validate generalizability beyond the 8 ER datasets used.