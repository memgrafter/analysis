---
ver: rpa2
title: Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting
  in AIoT
arxiv_id: '2407.11041'
source_url: https://arxiv.org/abs/2407.11041
tags:
- quantized
- quantization
- transformer
- precision
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying Transformer models
  for on-device time-series forecasting in AIoT systems by designing an integer-only
  quantized hardware accelerator optimized for embedded FPGAs. The authors integrate
  Quantization-Aware Training with custom VHDL templates to realize 6-bit and 4-bit
  quantized Transformer models, maintaining precision comparable to 8-bit baselines.
---

# Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT

## Quick Facts
- arXiv ID: 2407.11041
- Source URL: https://arxiv.org/abs/2407.11041
- Authors: Tianheng Ling; Chao Qian; Gregor Schiele
- Reference count: 20
- One-line primary result: 4-bit quantized Transformer achieves 132.33× faster inference and 48.19× less energy consumption on Xilinx Spartan-7 XC7S15 FPGA while increasing RMSE by only 0.63% vs 8-bit baseline

## Executive Summary
This paper addresses the challenge of deploying Transformer models for on-device time-series forecasting in AIoT systems by designing an integer-only quantized hardware accelerator optimized for embedded FPGAs. The authors integrate Quantization-Aware Training with custom VHDL templates to realize 6-bit and 4-bit quantized Transformer models, maintaining precision comparable to 8-bit baselines. Using a Xilinx Spartan-7 XC7S15 FPGA, they evaluate model configurations across two datasets, demonstrating that a 4-bit quantized model increases test loss by only 0.63% versus an 8-bit reference while achieving 132.33× faster inference and consuming 48.19× less energy.

## Method Summary
The approach combines integer-only quantization with Quantization-Aware Training (QAT) in PyTorch to reduce numerical bitwidth from 8-bit to 4-bit while preserving model precision. Trained quantized models are translated into FPGA-friendly hardware accelerators using reusable VHDL templates for operations like matrix multiplication, Softmax, and batch normalization. The methodology was validated on a Xilinx Spartan-7 XC7S15 FPGA, evaluating performance across PeMS and AirU time-series datasets with comprehensive metrics including RMSE, resource utilization (LUTs, BRAM, DSPs), timing, power, and energy consumption.

## Key Results
- 4-bit quantized model increases RMSE by only 0.63% compared to 8-bit baseline
- Achieves 132.33× faster inference and 48.19× less energy consumption on embedded FPGA
- Reducing bitwidth does not consistently improve latency or energy, highlighting need for systematic optimization

## Why This Works (Mechanism)

### Mechanism 1
Integer-only quantization reduces FPGA resource usage without degrading model precision beyond acceptable limits by mapping continuous tensors to discrete integer values using scale factors and zero points. During training, QAT adapts these parameters to the tensor's statistical distribution, enabling 4-bit precision with minimal RMSE increase. The core assumption is that QAT can dynamically learn quantization parameters that preserve model performance at low bitwidths.

### Mechanism 2
Hardware co-design using VHDL templates enables efficient deployment of quantized models on resource-constrained FPGAs by translating PyTorch-trained quantized models into VHDL components via reusable templates. Optimized operations are implemented with integer-only arithmetic, reducing logic and memory overhead. The core assumption is that the VHDL templates can accurately represent quantized operations while maintaining timing and resource efficiency.

### Mechanism 3
Lower bitwidth does not always translate to improved latency or energy efficiency due to resource utilization and clock frequency trade-offs. While reducing bitwidth simplifies arithmetic, it can also reduce DSP slice usage, increasing logic delay and lowering clock frequency. The overall inference time and energy consumption depend on the balance between cycle count and frequency. The core assumption is that the FPGA's timing and resource constraints create a non-linear relationship between bitwidth and performance.

## Foundational Learning

- **Quantization-Aware Training (QAT)**: Why needed here: QAT adapts quantization parameters during training, preserving model precision at low bitwidths, which is critical for accurate time-series forecasting on embedded devices. Quick check question: What is the primary advantage of QAT over post-training quantization for integer-only deployment?

- **FPGA hardware resource mapping (LUTs, BRAM, DSPs)**: Why needed here: Understanding how quantized operations map to FPGA resources is essential for optimizing model configurations and ensuring deployment feasibility on resource-constrained devices. Quick check question: Which FPGA resource type is most heavily utilized by matrix multiplication operations in quantized models?

- **Integer-only arithmetic and scaling**: Why needed here: Integer-only operations avoid floating-point units, reducing resource usage and power consumption, but require careful scaling to maintain numerical accuracy. Quick check question: How does the scaling factor S in quantization relate to the original tensor's dynamic range?

## Architecture Onboarding

- **Component map**: The FPGA accelerator consists of quantized linear layers, multi-head self-attention (MHA) with scaled dot-product attention, batch normalization, and global average pooling, all implemented with integer-only arithmetic and connected via VHDL templates.

- **Critical path**: The critical path includes the Softmax computation (with lookup tables and division), matrix multiplication in MHA, and the final linear layer, as these operations dominate latency and resource usage.

- **Design tradeoffs**: Balancing model precision (d_model, n) against FPGA resource utilization (LUTs, BRAM, DSPs) and timing (clock frequency, inference time) is essential; reducing bitwidth can improve some metrics but may degrade others.

- **Failure signatures**: Excessive RMSE increase (>0.63% vs 8-bit baseline), synthesis failures due to resource overflow, or inference times that do not improve with lower bitwidths indicate suboptimal configurations.

- **First 3 experiments**:
  1. Deploy the 4-bit quantized model (n=12, d_model=32) on the Spartan-7 XC7S15 FPGA and measure RMSE, resource utilization, timing, and power.
  2. Compare the 6-bit quantized model (n=12, d_model=64) to the 4-bit model on the same FPGA to analyze trade-offs in precision and resource usage.
  3. Sweep n and d_model values for 4-bit quantization to identify the configuration that best balances RMSE and resource utilization for the target FPGA.

## Open Questions the Paper Calls Out

### Open Question 1
How does the optimal quantization bitwidth vary across different time-series datasets with varying characteristics (e.g., univariate vs multivariate, stationary vs non-stationary)? The study only compares two datasets; broader empirical validation across diverse time-series domains is needed.

### Open Question 2
Can mixed-precision quantization schemes further improve the precision-resource trade-off compared to uniform bitwidth quantization? The authors state they plan to extend their approach to mixed-precision quantization in future work, suggesting it remains unexplored.

### Open Question 3
What is the impact of different FPGA architectures (e.g., newer Xilinx UltraScale+ vs Spartan-7) on the scalability and performance of quantized Transformer accelerators? The study is limited to the resource-constrained Spartan-7 XC7S15 FPGA, which constrained model size and forced compromises between precision and resource utilization.

## Limitations

- The complete architectural specifications and source code are not fully available in the public repository, potentially hindering exact replication of the experimental setup.
- Results are highly dependent on the specific FPGA platform (Xilinx Spartan-7 XC7S15) and may not generalize to other embedded devices or larger-scale deployments.
- The study focuses on two specific datasets and model configurations, limiting validation breadth across diverse time-series forecasting tasks.

## Confidence

**High Confidence**: The core claims regarding integer-only quantization and QAT preserving model precision at low bitwidths are well-supported by the experimental results, showing only a 0.63% increase in RMSE for the 4-bit model versus the 8-bit baseline.

**Medium Confidence**: The assertion that reducing bitwidth does not always translate to improved latency or energy efficiency is plausible, given the reported counter-example of the 6-bit model exhibiting higher power consumption than the 8-bit model.

**Low Confidence**: The reusability and scalability of the VHDL templates for different Transformer architectures or larger models are not fully demonstrated, with limited discussion on extensibility to more complex tasks or larger datasets.

## Next Checks

1. **Reproduce Core Results**: Implement the 4-bit quantized Transformer model on a Xilinx Spartan-7 XC7S15 FPGA and verify that the RMSE, latency, and energy consumption match the reported values (RMSE increase ≤0.63%, latency 132.33× faster, energy 48.19× less).

2. **Generalize Bitwidth Trade-offs**: Experiment with additional bitwidths (e.g., 3-bit, 5-bit) and model configurations (varying n and d_model) to assess the consistency of the observed non-linear relationship between bitwidth, latency, and energy efficiency across different setups.

3. **Test Template Reusability**: Adapt the VHDL templates to a different Transformer architecture (e.g., ViT or BERT) or a larger model (e.g., increasing d_model or n) and evaluate whether the templates can be efficiently reused without significant manual intervention or performance degradation.