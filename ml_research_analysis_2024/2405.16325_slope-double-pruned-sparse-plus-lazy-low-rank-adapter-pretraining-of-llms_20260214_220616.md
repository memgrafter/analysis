---
ver: rpa2
title: 'SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs'
arxiv_id: '2405.16325'
source_url: https://arxiv.org/abs/2405.16325
tags: []
core_contribution: This paper presents SLoPe, a method for double-pruned sparse plus
  lazy low-rank adapter pretraining of large language models (LLMs). The key idea
  is to improve the accuracy of sparse LLMs while accelerating their pretraining and
  inference, and reducing their memory footprint.
---

# SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs

## Quick Facts
- arXiv ID: 2405.16325
- Source URL: https://arxiv.org/abs/2405.16325
- Reference count: 40
- Primary result: Accelerates training and inference of billion-parameter LLMs by up to 1.25x and 1.54x respectively while reducing memory usage by up to 0.63x (training) and 0.61x (inference)

## Executive Summary
SLoPe presents a method for improving sparse large language models through double-pruned backward pass formulation and strategic low-rank adapter integration. The approach targets the dual challenges of maintaining accuracy in sparse models while accelerating pretraining and reducing memory footprint. By implementing N:M structured sparsity in the backward pass and adding adapters only in the final 1% of pretraining, SLoPe achieves significant computational and memory efficiency gains without sacrificing model performance.

## Method Summary
SLoPe introduces a double-pruned backward pass that applies N:M sparsity patterns to transposed weight matrices during gradient computation, enabling accelerated sparse operations. The method incorporates low-rank adapters specifically during the final 1% of pretraining iterations, adding minimal overhead while potentially capturing fine-grained task-specific adaptations. This combination allows for substantial acceleration of both training and inference phases while maintaining or improving model accuracy compared to dense baselines.

## Key Results
- Training acceleration of up to 1.25x for OPT-33B and OPT-66B models
- Inference acceleration of up to 1.54x for the same model scales
- Memory reduction of up to 0.63x during training and 0.61x during inference
- Low-rank adapters added only in final 1% of pretraining without significant overhead

## Why This Works (Mechanism)
The double-pruned approach works by exploiting structured sparsity patterns (N:M) in both forward and backward passes, reducing the computational complexity of matrix operations. By applying sparsity to transposed weight matrices during backpropagation, SLoPe minimizes the number of non-zero gradient computations required. The lazy integration of low-rank adapters in the final pretraining phase allows the model to first learn general representations before fine-tuning with specialized adapters, capturing task-specific adaptations without disrupting the core pretraining process.

## Foundational Learning
- **N:M Sparsity**: Structured sparsity pattern where N out of every M elements are non-zero; needed for hardware-efficient sparse operations; quick check: verify hardware supports target sparsity pattern
- **Low-Rank Adapters**: Parameter-efficient modules added to transformer layers; needed to inject task-specific knowledge without full fine-tuning; quick check: confirm adapter rank is appropriate for target task
- **Double-Pruned Backward Pass**: Application of sparsity to both forward and transposed weight matrices during backpropagation; needed to maximize computational savings; quick check: ensure gradient sparsity matches forward sparsity pattern

## Architecture Onboarding
- **Component Map**: Input -> Dense Layers -> Sparse Projection (N:M) -> Low-Rank Adapters (final 1%) -> Output
- **Critical Path**: The sparse matrix multiplications in both forward and backward passes represent the computational bottleneck that SLoPe optimizes
- **Design Tradeoffs**: Balancing sparsity ratio against accuracy retention, timing of adapter insertion against training efficiency, and hardware-specific optimizations
- **Failure Signatures**: Degradation in accuracy when sparsity exceeds hardware-optimized ratios, memory inefficiencies when adapters are added too early, and training instability from improper gradient pruning
- **First Experiments**: 1) Test N:M sparsity patterns at different ratios on small models, 2) Profile memory usage with varying adapter insertion timings, 3) Benchmark training speed with different hardware sparsity acceleration capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability beyond tested model sizes (OPT-33B/66B) remains unproven
- Memory and speed improvements may vary significantly with different sparsity ratios and hardware configurations
- Early adapter integration timing (beyond final 1%) has not been explored for potential additional benefits

## Confidence
- **High confidence**: The core technical approach (double-pruned backward pass with N:M sparsity) is well-defined and implementable
- **Medium confidence**: The training acceleration and memory reduction results are reproducible within the tested model range
- **Low confidence**: Generalization to different model sizes, sparsity patterns, and hardware platforms

## Next Checks
1. Evaluate SLoPe on a broader range of model sizes (both smaller and larger than OPT-33B/66B) to verify that the 1.25x/1.54x improvements hold across different scales
2. Test the method with different N:M sparsity ratios (e.g., 2:4, 4:8) and across multiple hardware platforms (GPUs with different sparsity acceleration capabilities) to quantify the impact on the claimed memory and speed improvements
3. Conduct ablation studies on when to introduce low-rank adapters during pretraining (varying the 1% final iterations parameter) to determine if earlier or more gradual adapter integration provides additional benefits