---
ver: rpa2
title: Offline Reinforcement Learning for LLM Multi-Step Reasoning
arxiv_id: '2412.16145'
source_url: https://arxiv.org/abs/2412.16145
tags:
- arxiv
- reasoning
- preprint
- value
- oreo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OREO (Offline Reasoning Optimization), an
  offline reinforcement learning method designed to enhance large language models'
  multi-step reasoning capabilities. The method addresses limitations of Direct Preference
  Optimization (DPO) for reasoning tasks by jointly learning a policy and value function
  through the soft Bellman Equation, enabling better credit assignment without requiring
  paired preference data.
---

# Offline Reinforcement Learning for LLM Multi-Step Reasoning

## Quick Facts
- arXiv ID: 2412.16145
- Source URL: https://arxiv.org/abs/2412.16145
- Authors: Huaijie Wang; Shibo Hao; Hanze Dong; Shenao Zhang; Yilin Bao; Ziran Yang; Yi Wu
- Reference count: 27
- Key outcome: OREO achieves 52.5% accuracy on MATH dataset with 1.5B model, outperforming baselines across mathematical reasoning and embodied agent control tasks

## Executive Summary
This paper introduces OREO (Offline Reasoning Optimization), an offline reinforcement learning method that enhances large language models' multi-step reasoning capabilities by jointly learning policy and value functions through the soft Bellman Equation. Unlike Direct Preference Optimization (DPO), OREO eliminates the need for paired preference data and provides better credit assignment for multi-step reasoning tasks with sparse rewards. The method demonstrates consistent performance improvements across mathematical reasoning tasks (GSM8K, MATH) and embodied agent control (ALFWorld), achieving state-of-the-art results with smaller models and enabling test-time performance gains through value-guided tree search.

## Method Summary
OREO addresses the limitations of Direct Preference Optimization for multi-step reasoning by framing reasoning as a Markov Decision Process where tokens are actions and reasoning trajectories are state sequences. The method jointly learns a policy network and value network by optimizing the soft Bellman Equation, which provides better credit assignment than response-level methods. Unlike DPO, OREO works with unpaired data and sparse terminal rewards, making it more suitable for reasoning tasks. The learned value function can be leveraged during inference to guide beam search, providing additional performance gains without additional training.

## Key Results
- OREO achieves 52.5% accuracy on MATH dataset with 1.5B model, significantly outperforming baseline methods
- On GSM8K, OREO achieves 75.4% accuracy with 1.5B model and 83.8% with 7B model
- Test-time value-guided beam search provides 11.4% relative improvement on GSM8K and 17.9% on MATH
- OREO shows steady improvement across iterative training rounds while rejection sampling plateaus

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: OREO enables fine-grained credit assignment in multi-step reasoning tasks by jointly learning policy and value functions through the soft Bellman Equation.
- **Mechanism**: The method optimizes the soft Bellman Equation at each time step, allowing the value function to capture the contribution of individual tokens to the final reward. This contrasts with response-level methods that treat all tokens uniformly.
- **Core assumption**: The soft Bellman Equation holds at each timestep and can be enforced through joint optimization of policy and value functions.
- **Evidence anchors**: 
  - [abstract]: "Building on insights from previous works of maximum entropy reinforcement learning, it jointly learns a policy model and value function by optimizing the soft Bellman Equation."
  - [section 3.3]: "We observed that the magnitude of Aθ is generally smaller than that of Aϕ... indicating its weaker ability to distinguish correct and incorrect reasoning steps."

### Mechanism 2
- **Claim**: OREO eliminates the need for paired preference data by leveraging unpaired data with sparse rewards.
- **Mechanism**: Instead of requiring pairwise comparisons as in DPO, OREO uses the soft Bellman Equation to learn from unpaired trajectories where only terminal states have non-zero rewards.
- **Core assumption**: Unpaired trajectories with sparse terminal rewards contain sufficient information for learning effective policies and value functions.
- **Evidence anchors**:
  - [abstract]: "It reduces the need to collect pairwise data and enables better credit assignment."
  - [section 2.1]: "DPO relies on paired preference data, which is not readily available for multi-step reasoning tasks."

### Mechanism 3
- **Claim**: The learned value function enables test-time performance improvements through guided tree search.
- **Mechanism**: The value function trained during OREO can be used at inference time to guide beam search or select best-of-K actions, effectively providing process-level supervision without additional training.
- **Core assumption**: The value function learned during training accurately estimates the expected future reward for reasoning steps.
- **Evidence anchors**:
  - [abstract]: "The learned value function can be leveraged to guide the tree search for free, which can further boost the performance during test time."
  - [section 5.4]: "Beam search with B = 7 provides a 11.4% relative improvement in GSM8K and a 17.9% relative improvement in MATH."

## Foundational Learning

- **Concept: Markov Decision Process (MDP) for reasoning tasks**
  - Why needed here: The paper frames LLM reasoning as an MDP where tokens are actions and reasoning trajectories are state sequences. Understanding this formulation is crucial for grasping how OREO applies reinforcement learning to reasoning.
  - Quick check question: In the reasoning MDP formulation, what constitutes the state representation and what are the actions?

- **Concept: Soft Bellman Equation and maximum entropy RL**
  - Why needed here: OREO is built on the foundation of maximum entropy RL, where the soft Bellman Equation relates the optimal policy and value function. This is the theoretical basis for the method's credit assignment capability.
  - Quick check question: How does the soft Bellman Equation differ from the standard Bellman equation, and why is this difference important for reasoning tasks?

- **Concept: KL regularization in RLHF**
  - Why needed here: The paper incorporates KL regularization to keep the learned policy close to the reference policy. Understanding this component is essential for grasping the full objective function.
  - Quick check question: What role does the KL regularization term play in the RLHF objective, and how does it affect the learned policy?

## Architecture Onboarding

- **Component map**: Policy network πθ -> Value network Vϕ -> KL regularization with reference policy πref
- **Critical path**: 1) Collect trajectories using current policy πθ, 2) Compute rewards (sparse, terminal only), 3) Update value network Vϕ to minimize Bellman inconsistency, 4) Update policy network πθ using the learned value function, 5) Apply KL regularization to maintain alignment with πref.
- **Design tradeoffs**: The method trades computational complexity (joint training of two networks) for better credit assignment and the ability to learn from unpaired data. The choice between token-level, step-level, and response-level objectives involves a tradeoff between granularity of credit assignment and training stability.
- **Failure signatures**: Poor performance may manifest as: 1) Value function collapse (all values converge to similar values), 2) Policy collapse (policy becomes too deterministic or too random), 3) Distribution shift (value function becomes inaccurate for new types of reasoning steps), 4) Slow convergence (Bellman inconsistency remains high).
- **First 3 experiments**:
  1. Implement the basic OREO objective with token-level optimization on a small reasoning dataset (e.g., a subset of GSM8K) to verify the training pipeline works.
  2. Compare token-level vs step-level vs response-level objectives on the same dataset to understand the impact of credit assignment granularity.
  3. Implement the value-guided beam search at inference time and measure the performance gain over greedy decoding on a held-out test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental limitation that prevents Direct Preference Optimization (DPO) from effectively handling multi-step reasoning tasks compared to OREO?
- Basis in paper: [explicit] The paper identifies two main issues with DPO for reasoning tasks: (1) it requires paired preference data which is not readily available for multi-step reasoning, and (2) it treats all tokens uniformly, making it ineffective for credit assignment in tasks with sparse rewards.
- Why unresolved: While the paper demonstrates OREO's superiority empirically, it doesn't provide a complete theoretical explanation for why DPO fails specifically in multi-step reasoning contexts where the reward structure is known.
- What evidence would resolve it: A formal theoretical analysis comparing the information-theoretic properties of DPO versus OREO's soft Bellman Equation approach, specifically showing how the relaxation from step-level to trajectory-level credit assignment degrades performance.

### Open Question 2
- Question: How does the choice between implicit (policy-based) and explicit (separate value network) value functions impact long-horizon reasoning performance?
- Basis in paper: [explicit] The paper presents case studies showing that the explicit value function Vϕ provides more accurate advantage estimates than the implicit value function derived from πθ, particularly in distinguishing correct from incorrect reasoning steps.
- Why unresolved: While the paper demonstrates the superiority of explicit value functions empirically, it doesn't fully explain the underlying mechanisms that cause the softmax bottleneck in policy networks to reduce their effectiveness as value functions.
- What evidence would resolve it: Systematic experiments varying reasoning task complexity and horizon length, combined with ablation studies on the architecture of the value network versus the policy network, to identify which aspects of the value function's representation capacity are most critical.

### Open Question 3
- Question: What is the optimal balance between exploration and exploitation in the iterative OREO framework for multi-step reasoning?
- Basis in paper: [inferred] The paper shows that OREO improves steadily across iterations while rejection sampling saturates, suggesting that OREO better exploits failed trajectories. However, it doesn't investigate how the sampling strategy for generating new data affects learning efficiency.
- Why unresolved: The paper uses a simple data collection approach where the updated policy generates responses, but doesn't explore whether alternative exploration strategies (e.g., curiosity-driven exploration, uncertainty-based sampling) could accelerate learning.
- What evidence would resolve it: Comparative experiments testing different data collection strategies in the iterative framework, measuring not just final performance but learning curves and sample efficiency across reasoning tasks of varying complexity.

## Limitations

- Performance gains show diminishing returns with larger models, suggesting potential scaling limitations
- Reliance on sparse terminal rewards may limit applicability to domains where reward signals are difficult to define
- Computational overhead of joint policy-value training and test-time tree search may constrain practical deployment

## Confidence

**High Confidence Claims:**
- OREO outperforms baseline methods (rejection sampling, DPO, KTO) on GSM8K, MATH, and ALFWorld tasks
- The value function learned during training can improve test-time performance through guided tree search
- Token-level optimization provides better credit assignment than step-level or response-level approaches

**Medium Confidence Claims:**
- OREO enables effective learning from unpaired data without requiring pairwise preferences
- The soft Bellman Equation formulation provides superior credit assignment for multi-step reasoning
- Iterative training rounds provide consistent improvements

**Low Confidence Claims:**
- OREO's effectiveness generalizes to reasoning tasks beyond mathematical problem-solving and embodied control
- The computational overhead of OREO is justified by the performance gains in real-world deployment scenarios
- The method scales effectively to much larger models (e.g., 70B+ parameters)

## Next Checks

1. **Cross-domain generalization test**: Evaluate OREO on commonsense reasoning benchmarks (e.g., StrategyQA, OpenBookQA) and logical reasoning tasks to assess generalizability beyond mathematical reasoning and embodied control.

2. **Scaling behavior analysis**: Systematically evaluate OREO's performance across a wider range of model sizes (1.5B, 7B, 13B, 34B, 70B) to identify potential scaling limitations and diminishing returns.

3. **Computational overhead benchmark**: Measure wall-clock training time, inference latency, and memory requirements for OREO compared to baselines across different model sizes to quantify the practical deployment costs.