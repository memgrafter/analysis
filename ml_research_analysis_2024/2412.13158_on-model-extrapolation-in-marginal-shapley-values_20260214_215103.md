---
ver: rpa2
title: On Model Extrapolation in Marginal Shapley Values
arxiv_id: '2412.13158'
source_url: https://arxiv.org/abs/2412.13158
tags:
- values
- shapley
- extrapolation
- features
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of model extrapolation in marginal
  Shapley values when features are correlated, which can lead to unintuitive and model-dependent
  attributions. It demonstrates this issue using a simple linear spline model, where
  marginal averaging forces extrapolation into regions with no data, producing undesirable
  effects.
---

# On Model Extrapolation in Marginal Shapley Values

## Quick Facts
- arXiv ID: 2412.13158
- Source URL: https://arxiv.org/abs/2412.13158
- Authors: Ilya Rozenfeld
- Reference count: 19
- Primary result: Proposes a stratified approach to marginal Shapley values that avoids model extrapolation when features are correlated, producing intuitive attributions consistent with causal Shapley values

## Executive Summary
This paper addresses a fundamental problem in model interpretability: marginal Shapley values can lead to model extrapolation when features are correlated, producing unintuitive and model-dependent attributions. The author demonstrates this issue using a simple linear spline model where marginal averaging forces predictions into regions with no data. To solve this, the paper proposes a "stratified" approach that calculates Shapley values separately for each feature stratum, avoiding extrapolation. By incorporating causal information, the method redistributes reference values to ensure consistency across strata, producing intuitive attributions. When applied to real-world French motor insurance data, the stratified approach revealed strong interactions between driver age and bonus-malus score that were not apparent using standard Shapley values.

## Method Summary
The paper proposes a stratified approach to calculating marginal Shapley values that avoids model extrapolation. Instead of averaging over the full marginal distribution, Shapley values are computed separately for each feature stratum (disjoint regions where the model is well-defined). Causal information is then used to redistribute reference values across strata, ensuring consistency and recovering causal Shapley values without conditional averaging. The method is validated on both a simple linear spline model and real-world French motor insurance data using Histogram-Based Gradient Boosting models.

## Key Results
- Standard marginal Shapley values can force model extrapolation into impossible feature regions when features are correlated
- The stratified approach avoids extrapolation while preserving marginal averaging benefits
- Incorporating causal information allows the method to replicate causal Shapley values without conditional averaging
- Applied to French motor insurance data, the approach revealed strong interactions between driver age and bonus-malus score not visible with standard Shapley values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Marginal averaging can force extrapolation into feature regions where the model is not well defined, producing unintuitive Shapley values.
- Mechanism: When features are correlated, marginal averaging samples from the marginal distribution, which can generate feature combinations that do not exist in the data. The model is then asked to make predictions in these impossible regions, leading to extrapolation artifacts.
- Core assumption: The model's functional form allows extrapolation beyond the support of the training data.
- Evidence anchors:
  - [abstract] "it is a well-known fact that marginal approach to calculating Shapley values leads to model extrapolation where it might not be well defined."
  - [section 2] Demonstrates this with a linear spline model where features (X1, X2) are correlated; marginal averaging forces predictions into impossible regions (X1 < 0, X2 = 1) and (X1 > 0, X2 = 0).
  - [corpus] Found 25 related papers; average FMR=0.481. No direct evidence of extrapolation issues, but related work on Shapley values and missing data suggests awareness of sampling-related problems.
- Break condition: The model is well-defined and bounded over the entire feature space, or features are independent so marginal averaging does not create impossible combinations.

### Mechanism 2
- Claim: Stratifying Shapley value calculation by feature strata avoids extrapolation while preserving marginal averaging benefits.
- Mechanism: Instead of averaging over the full marginal distribution, compute Shapley values separately for each stratum (e.g., each combination of feature values that can occur). This confines the computation to regions where the model is well-defined.
- Core assumption: The feature space can be partitioned into disjoint strata where the model is well-behaved and extrapolation is not needed.
- Evidence anchors:
  - [section 3] Proposes calculating Shapley values separately for feature regions (X1 > 0, X2 = 1) and (X1 < 0, X2 = 0), avoiding extrapolation into impossible regions.
  - [abstract] "proposes an approach which while using marginal averaging avoids model extrapolation."
  - [corpus] Weak evidence; no direct mention of stratification, but papers on "Shapley Marginal Surplus for Strong Models" suggest exploration of alternative Shapley computation methods.
- Break condition: The stratification does not capture all relevant interactions, or the number of strata becomes too large to compute efficiently.

### Mechanism 3
- Claim: Incorporating causal information allows redistribution of reference values to ensure consistency across strata and recover causal Shapley values.
- Mechanism: After stratifying, each stratum has its own reference value. Causal information (e.g., which feature causes which) is used to redistribute portions of these reference values, creating a common baseline and attributing effects appropriately.
- Core assumption: The causal structure among features is known or can be inferred.
- Evidence anchors:
  - [section 3] Introduces causal relationships (X1 ‚Üí X2 or X2 ‚Üí X1) and redistributes reference values to obtain consistent Shapley values across strata.
  - [abstract] "with addition of causal information replicates causal Shapley values."
  - [corpus] Direct evidence in related paper "Causal Analysis of Shapley Values: Conditional vs. Marginal" with FMR=0.546, suggesting active research in causal Shapley values.
- Break condition: The assumed causal structure is incorrect, leading to misattribution of effects.

## Foundational Learning

- Concept: Shapley values and their axioms (Efficiency, Dummy, Symmetry, Additivity)
  - Why needed here: The paper builds on Shapley values as the foundation for model explainability and modifies their computation to address extrapolation issues.
  - Quick check question: What are the four axioms that uniquely determine Shapley values, and why is the Additivity axiom relevant when combining games?

- Concept: Marginal vs. conditional averaging in Shapley value computation
  - Why needed here: The paper contrasts these two approaches and argues for marginal averaging with modifications to avoid its pitfalls.
  - Quick check question: How do marginal and conditional averaging differ when features are correlated, and what are the known issues with each?

- Concept: Causal inference and do-calculus
  - Why needed here: Causal information is used to redistribute reference values and recover causal Shapley values without conditional averaging.
  - Quick check question: How does Pearl's do-operator relate to marginal averaging, and why might causal assumptions be necessary for consistent attributions?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model training -> Stratification engine -> Causal inference module -> Reference redistribution -> Shapley computation

- Critical path:
  1. Detect correlated features and define strata.
  2. For each stratum, compute marginal Shapley values using background samples from that stratum.
  3. Use causal information to redistribute reference values and ensure consistency.
  4. Output final attributions with common baseline.

- Design tradeoffs:
  - Stratification granularity vs. computational cost: Finer strata reduce extrapolation but increase computation.
  - Causal information accuracy vs. attribution quality: Incorrect causal assumptions can lead to misattribution.
  - Background sample size per stratum vs. variance in estimates: Small strata may require more samples for stable estimates.

- Failure signatures:
  - Large differences between standard and stratified Shapley values indicate strong correlations and potential extrapolation issues.
  - Inconsistent attributions across strata suggest incorrect causal assumptions or insufficient background samples.
  - High variance in Shapley values for small strata indicates need for more background data.

- First 3 experiments:
  1. Reproduce the linear spline example: Implement marginal, stratified, and causal Shapley value calculations; verify that stratified avoids extrapolation and matches causal values.
  2. Apply to correlated features in real data: Use the French motor insurance dataset; compare standard vs. stratified Shapley values for Bonus Malus and Driver Age; check for interaction detection.
  3. Test causal assumption sensitivity: Vary assumed causal directions (X1‚ÜíX2 vs. X2‚ÜíX1); observe impact on attributions and reference redistribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the stratified approach to Shapley values perform on datasets with complex non-linear interactions between features?
- Basis in paper: [explicit] The paper proposes a stratified approach and demonstrates it on a linear spline model and real-world motor insurance data, but does not explore complex non-linear interactions
- Why unresolved: The current paper only tests the stratified approach on a simple linear model and real data with mostly linear relationships. The performance on datasets with complex non-linear feature interactions remains unknown
- What evidence would resolve it: Empirical studies applying the stratified approach to benchmark datasets known for complex non-linear interactions (like Friedman's benchmark problems) would provide evidence of its performance in these scenarios

### Open Question 2
- Question: What are the computational implications of implementing stratified Shapley values for high-dimensional feature spaces?
- Basis in paper: [inferred] The paper demonstrates the method on datasets with 9 features but doesn't discuss scalability or computational costs for high-dimensional problems
- Why unresolved: While the method is shown to work on moderate-dimensional data, the paper doesn't analyze how computational costs scale with the number of features or discuss potential optimization strategies
- What evidence would resolve it: Computational complexity analysis and runtime comparisons between standard and stratified approaches across datasets with varying feature dimensions would quantify the scalability

### Open Question 3
- Question: Can the stratified approach be extended to handle cases where causal direction is uncertain or ambiguous?
- Basis in paper: [explicit] The paper assumes known causal relationships (ùëãùëã1 ‚Üí ùëãùëã2 or ùëãùëã2 ‚Üí ùëãùëã1) but acknowledges that causal information may not always be available
- Why unresolved: The current stratified method requires explicit causal assumptions, but in practice, causal directions are often uncertain. The paper doesn't address how to handle ambiguity in causal structure
- What evidence would resolve it: Development and testing of methods that can handle uncertain or multiple causal hypotheses, perhaps through ensemble approaches or sensitivity analysis across different causal assumptions, would address this limitation

## Limitations
- The approach requires accurate knowledge of causal relationships among features, which may be difficult to establish in practice
- Computational cost increases with the number of strata, potentially limiting scalability to high-dimensional feature spaces
- The redistribution of reference values based on causal information needs careful validation to avoid introducing new biases

## Confidence
- High confidence in the problem identification: Model extrapolation in marginal Shapley values when features are correlated is a well-documented issue with clear examples.
- Medium confidence in the stratification solution: The approach logically addresses extrapolation, but its effectiveness depends on proper stratification and sufficient background data per stratum.
- Medium confidence in causal redistribution: The mechanism is sound, but requires accurate causal knowledge which may not always be available.

## Next Checks
1. Conduct sensitivity analysis on assumed causal directions: Systematically vary the assumed causal structure among features and quantify the impact on Shapley value attributions to assess robustness.
2. Evaluate performance on high-dimensional correlated features: Test the stratified approach on datasets with many correlated features to identify scalability limits and potential breakdown conditions.
3. Compare with alternative extrapolation mitigation strategies: Benchmark against other methods for handling correlated features in Shapley values (e.g., conditional sampling with adjustments) to establish relative performance.