---
ver: rpa2
title: 'Prompting ChatGPT for Translation: A Comparative Analysis of Translation Brief
  and Persona Prompts'
arxiv_id: '2403.00127'
source_url: https://arxiv.org/abs/2403.00127
tags:
- translation
- chatgpt
- prompt
- prompts
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explored the use of translation briefs and persona prompts
  to improve translation quality in ChatGPT. It compared outputs from four prompts
  (basic, translation brief, author, translator) using an English-to-Chinese scientific
  article.
---

# Prompting ChatGPT for Translation: A Comparative Analysis of Translation Brief and Persona Prompts

## Quick Facts
- arXiv ID: 2403.00127
- Source URL: https://arxiv.org/abs/2403.00127
- Authors: Sui He
- Reference count: 7
- Key outcome: Translator persona prompts yielded highest quality translations, while translation brief prompts performed worst; human evaluators noted fluency issues in AI translations versus accuracy trade-offs in human translations

## Executive Summary
This study investigates whether classical translation briefs and persona prompts can improve machine translation quality in ChatGPT. Using an English-to-Chinese scientific article translation task, the research compares four prompt types: basic, translation brief, author persona, and translator persona. The findings reveal that assigning ChatGPT the role of a translator produces the highest quality outputs, while translation brief prompts perform worst. Human evaluation highlights fluency and naturalness issues in AI translations, contrasting with human translations that show accuracy trade-offs due to creative adaptation. The study suggests that traditional translation brief elements designed for human workflows may not effectively transfer to LLM contexts, while persona-based prompting shows promise for improving machine translation quality.

## Method Summary
The study used GPT-4 accessed via ChatGPT interface with temperature=0.5 to translate an English scientific article from Discover Magazine (1253 words, Dec 2021) into Chinese. Four prompt variants were tested: basic, translation brief, author persona, and translator persona, each generated three times for consistency verification. Translations were aligned with a published Chinese reference using SDL Trados Studio 2022 and evaluated using BLEU and COMET-22 metrics. Human evaluation involved four UK-based university lecturers rating 10 text segments without knowing which were machine-generated. The fourth output from each prompt was selected for final analysis based on consistency checks.

## Key Results
- Translator persona prompt achieved highest translation quality rankings in human evaluation
- Translation brief prompt performed worst among all prompt types tested
- Human evaluators noted fluency and naturalness issues in AI translations
- Human translations showed accuracy trade-offs due to creative adaptation
- BLEU and COMET-22 scores showed mixed alignment with human evaluation rankings

## Why This Works (Mechanism)

### Mechanism 1
Translation brief prompts underperform because ChatGPT lacks structured comprehension of translation task metadata. Translation briefs in human workflows guide strategic decisions (tone, audience, function), but ChatGPT interprets these as free-text instructions rather than procedural constraints. Translation briefs are designed for human cognitive scaffolding, not LLM prompt parsing. The classical settings of a translation brief, aiming at human-only workflow, might not work as well as one would expect in a human-machine workflow.

### Mechanism 2
Persona assignment (translator) improves output quality by providing consistent stylistic and lexical framing. Assigning "translator" persona triggers internal style-guide and domain-consistent vocabulary selection, reducing randomness in word choice. LLMs can simulate persona-consistent behavior through role-based prompt priming. Assigning ChatGPT with the role of a translator appears to have a better result than assigning the role of an author.

### Mechanism 3
Human evaluation reveals fluency and naturalness deficits in AI translations, while human translations trade accuracy for creativity. AI translations follow statistical patterns but lack pragmatic adaptation; human translators balance fidelity with reader engagement. Translation quality encompasses both semantic accuracy and pragmatic fluency, with different weightings across audiences. Comments on the issues of fluency and naturalness suggest problems associated with syntax, collocation, and lack of creativity in rendering expressions.

## Foundational Learning

- Concept: Translation brief
  - Why needed here: Understands why traditional translation briefs fail with LLMs; reveals limits of human-centered translation concepts in machine contexts.
  - Quick check question: What key elements of a translation brief are lost when provided as free-text to an LLM?

- Concept: Persona prompting
  - Why needed here: Explains how role-based priming affects model behavior and output consistency; clarifies differences between translator and author personas.
  - Quick check question: How might "translator" and "author" personas differently influence word choice and sentence structure?

- Concept: BLEU and COMET metrics
  - Why needed here: Interprets automated quality scores; understands why they may misalign with human judgments in this study.
  - Quick check question: Why might BLEU underrepresent fluency issues that human evaluators notice?

## Architecture Onboarding

- Component map:
  - Prompt templates (basic, brief, author, translator) -> Text preprocessing (markdown formatting, delimiters) -> Model interface (ChatGPT API/UI with temperature=0.5) -> Evaluation pipeline (BLEU, COMET-22, human grading form) -> Reference translation (published Chinese version)

- Critical path:
  1. Define prompt variants
  2. Generate translations (3 runs per prompt)
  3. Align source-target segments
  4. Compute BLEU/COMET scores
  5. Distribute human grading forms
  6. Aggregate segment-level averages
  7. Compare rankings across metrics

- Design tradeoffs:
  - BLEU favors n-gram overlap but ignores fluency; COMET-22 uses embeddings but may not capture discourse coherence.
  - Human grading avoids fixed rubrics but introduces subjectivity and evaluator fatigue.
  - Using a published translation as reference may bias metrics against creative adaptations.

- Failure signatures:
  - High BLEU/COMET but low human scores → fluency issues not captured by metrics.
  - Inconsistent segment rankings across evaluators → ambiguous quality signals.
  - All prompts yield similar scores → prompt design ineffective or evaluation insensitive.

- First 3 experiments:
  1. Run consistency check: generate 5 outputs per prompt, compute inter-run BLEU variance.
  2. Structured brief test: convert brief prompt into JSON metadata, compare performance.
  3. Persona ablation: test "translator" vs "interpreter" vs no persona to isolate persona effect.

## Open Questions the Paper Calls Out

### Open Question 1
How can translation briefs be redesigned or modified to be more effective for improving ChatGPT's translation performance? The paper found that translation briefs, which are effective in human-to-human translation, did not improve ChatGPT's translation quality. This remains unresolved because the study only tested one type of translation brief format and did not explore alternative designs or elements that could be more effective. Testing various redesigned translation briefs with different elements or formats would resolve this question.

### Open Question 2
What specific aspects of persona assignment (translator vs. author) contribute to improved translation quality in ChatGPT? The study found that assigning the role of a translator led to better performance than assigning the role of an author or using a basic prompt. This remains unresolved because the study did not analyze the specific elements of the translator persona that influenced the results. Conducting experiments that isolate and test different elements of the translator persona would resolve this question.

### Open Question 3
How do human evaluation metrics need to be adapted to better assess machine-generated translations in real-world contexts? The study noted that traditional evaluation scales may oversimplify the nuanced nature of translation and that a reader-oriented scale would be beneficial. This remains unresolved because current evaluation metrics are primarily designed for technical aspects of translation rather than real-world reader reception. Developing and testing new evaluation metrics that incorporate reader reception and real-world usage scenarios would resolve this question.

## Limitations

- Human evaluation introduces potential subjectivity due to lack of fixed grading rubric and shared institutional biases among evaluators
- Reference translation used for automatic metrics may have taken creative liberties, disadvantaging both AI and human translations
- Single-genre, single-article scope limits generalizability to other domains or languages
- Brief prompt poor performance might reflect prompt engineering limitations rather than fundamental flaws in translation brief concepts

## Confidence

- High confidence: Translator persona improves output quality over basic prompts - supported by consistent human evaluation rankings and multiple evaluation methods.
- Medium confidence: Translation brief prompts underperform due to LLM processing differences - mechanism plausible but not definitively proven; structured brief variants not tested.
- Medium confidence: Human translations show accuracy trade-offs for creativity - human evaluation indicates this, but reference translation's creative adaptations complicate attribution.
- Low confidence: Classical translation brief concepts fundamentally don't transfer to LLM workflows - study suggests this but doesn't systematically test alternative brief formulations.

## Next Checks

1. **Structured Brief Test**: Reformulate the translation brief prompt as structured JSON metadata (e.g., {"audience": "general readers", "tone": "neutral", "purpose": "inform"}) and compare performance against the original brief prompt to isolate prompt format effects.

2. **Cross-Genre Replication**: Repeat the experiment with multiple genres (legal, literary, technical) to determine if translator persona benefits generalize beyond scientific articles and if brief prompt performance varies by domain.

3. **Inter-Evaluator Reliability Analysis**: Calculate inter-rater agreement scores (Cohen's kappa) for the human evaluation data to quantify subjectivity and determine if evaluator consensus supports the reported quality rankings.