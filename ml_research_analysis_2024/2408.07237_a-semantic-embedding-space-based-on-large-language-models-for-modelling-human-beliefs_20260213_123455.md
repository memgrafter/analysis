---
ver: rpa2
title: A semantic embedding space based on large language models for modelling human
  beliefs
arxiv_id: '2408.07237'
source_url: https://arxiv.org/abs/2408.07237
tags:
- belief
- beliefs
- user
- space
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel framework for modeling human beliefs
  using large language models (LLMs) fine-tuned on online debate data. It constructs
  a high-dimensional semantic embedding space where beliefs are mapped as vectors,
  capturing their interrelationships and polarization patterns.
---

# A semantic embedding space based on large language models for modelling human beliefs

## Quick Facts
- arXiv ID: 2408.07237
- Source URL: https://arxiv.org/abs/2408.07237
- Reference count: 0
- Key outcome: Novel framework using LLMs to model human beliefs as vectors, capturing polarization and predicting stances with relative dissonance metric.

## Executive Summary
This paper proposes a framework for modeling human beliefs using large language models fine-tuned on online debate data. The approach constructs a high-dimensional semantic embedding space where beliefs are mapped as vectors, capturing their interrelationships and polarization patterns. By leveraging user voting records from Debate.org and fine-tuning Sentence-BERT with contrastive learning, the method effectively represents individuals as vectors and predicts their stance on new debates based on existing beliefs. A key contribution is the introduction of a "relative dissonance" metric that quantifies the decision-making process.

## Method Summary
The method fine-tunes a pre-trained Sentence-BERT model using contrastive learning on belief triplets sampled from Debate.org voting records. Belief statements are created from PRO/CON votes using templates, and triplets are formed based on co-voting patterns where positive pairs share voting context and negative pairs do not. The fine-tuned model produces 768-dimensional belief embeddings that capture semantic relationships. Users are represented as average vectors of their expressed beliefs, enabling stance prediction on unseen debates through distance-based classification.

## Key Results
- Triplet evaluation accuracy: ~0.95 train, ~0.67 test
- Semantic similarity preservation: Spearman r = 0.718 on GLUE-STSB
- Prediction accuracy: F1-score ~0.59 for user stances on unseen debates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Belief embeddings trained via triplet loss align spatially with semantic and contextual relationships.
- Mechanism: Fine-tuning Sentence-BERT on user voting records creates triplets where anchor-positive pairs share voting context and anchor-negative pairs do not. Triplet loss pulls similar beliefs closer in embedding space.
- Core assumption: Co-voting patterns reflect genuine belief similarity, not just topical overlap.
- Evidence anchors: [abstract] "leveraging an online user debate data and mapping beliefs onto a neural embedding space constructed using a fine-tuned large language model (LLM)"
- Break condition: If co-voting patterns are driven by popularity or surface features rather than belief similarity, embeddings lose semantic validity.

### Mechanism 2
- Claim: Individual belief systems can be represented as vectors by averaging belief embeddings.
- Mechanism: User vector = average of their expressed belief vectors. This enables reasoning about user positions and predicting new beliefs.
- Core assumption: Averaging captures the "center" of a user's belief space without distortion.
- Evidence anchors: [section] "We represent each user by their average belief vector, defined as ⃗ u=PNu i=1 ⃗bu i /Nu"
- Break condition: If user beliefs are highly heterogeneous or multimodal, simple averaging fails to represent the user accurately.

### Mechanism 3
- Claim: Relative dissonance d* linearly predicts the probability of choosing the closer belief.
- Mechanism: Decision modeled as minimizing cognitive dissonance; d* = (dmax - dmin)/dmin captures relative discomfort. Prediction accuracy rises linearly with d*.
- Core assumption: Users prefer beliefs that reduce dissonance proportionally to the relative distance.
- Evidence anchors: [section] "The likelihood of a user choosing a closer belief linearly increases with relative dissonance d*"
- Break condition: If users sometimes choose dissonant beliefs for identity or other reasons, the linear model breaks.

## Foundational Learning

- Concept: Triplet loss in contrastive learning
  - Why needed here: Enables fine-tuning of embeddings so that semantically related beliefs are closer and unrelated ones are farther apart.
  - Quick check question: If anchor-positive distance is 2 and anchor-negative is 5, with margin 1, what is the loss?

- Concept: Principal Component Analysis (PCA)
  - Why needed here: Reveals structure in high-dimensional belief space and identifies axes of polarization.
  - Quick check question: What does it mean if two clusters are separated along PC1 but overlap on PC2?

- Concept: Effective radius of belief distribution
  - Why needed here: Quantifies how dispersed a user's beliefs are, affecting predictability of new beliefs.
  - Quick check question: If a user's beliefs are tightly clustered, is their effective radius large or small?

## Architecture Onboarding

- Component map: Debate.org voting records → Belief statements via templates → Fine-tuned Sentence-BERT → 768-dim belief embeddings → PCA/UMAP analysis → User vectors (belief averages) → Distance-based prediction → Evaluation

- Critical path: Data → Triplet sampling → Fine-tuning → Embedding space → User representation → Prediction → Evaluation

- Design tradeoffs:
  - Fine-tuning vs. zero-shot: Fine-tuning captures dataset-specific semantics but may overfit; zero-shot is more general but less precise.
  - Averaging vs. weighted user vectors: Averaging is simple but ignores belief importance; weighting could improve accuracy but requires additional assumptions.
  - Distance vs. learned similarity: Distance is interpretable but may miss complex relationships; learned similarity could be more accurate but less transparent.

- Failure signatures:
  - Low triplet evaluation accuracy → embeddings don't capture belief relationships
  - Random baseline beats model → user vectors not informative
  - No correlation between d* and accuracy → dissonance model invalid
  - PCA shows no clustering → embedding space lacks structure

- First 3 experiments:
  1. Run triplet evaluation on a small sampled dataset; confirm model separates positive/negative pairs.
  2. Generate user vectors for 100 users; plot PCA and check for visible clustering by self-reported ideology.
  3. Pick 10 new debates; predict user stances using distance rule; compare to actual votes; compute accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the belief embedding framework perform when applied to non-US-centric cultural contexts, and what structural differences in belief polarization might emerge across different societies?
- Basis in paper: [explicit] The authors note their dataset is primarily US-centric and acknowledge findings may differ in other cultural contexts.
- Why unresolved: The study relies entirely on US-based Debate.org data without cross-cultural validation.
- What evidence would resolve it: Comparative analysis using debate data from other countries with different political and cultural backgrounds to test if belief polarization patterns and embedding structures remain consistent or show significant variations.

### Open Question 2
- Question: What are the temporal dynamics of belief space evolution, and how do belief relationships and polarization patterns change over time as new social issues emerge?
- Basis in paper: [explicit] Authors acknowledge they did not investigate temporal properties and assume stability of belief space, while noting beliefs can continuously change in reality.
- Why unresolved: The study uses static snapshot data without analyzing how belief positions, clusters, or relationships evolve across different time periods.
- What evidence would resolve it: Longitudinal analysis tracking belief embeddings and user positions across multiple time periods to identify how belief space structure transforms as new debates emerge and social issues evolve.

### Open Question 3
- Question: How do inherent biases in pre-trained LLMs affect the representation of human beliefs, particularly regarding underrepresented cultural perspectives and potential reinforcement of existing stereotypes?
- Basis in paper: [explicit] Authors acknowledge concerns about inherent biases in pre-trained LLMs and note they may reflect Western-centric viewpoints.
- Why unresolved: The study uses standard pre-trained models without investigating how these biases manifest in belief representations or affect downstream predictions.
- What evidence would resolve it: Comparative analysis using LLMs trained on more diverse, multilingual, and culturally balanced datasets to evaluate how different training data affect belief embedding quality and bias patterns.

## Limitations

- Dataset bias: Reliance on Debate.org data may not represent general populations due to platform's attraction of users with strong opinions.
- Simplified user representation: Averaging approach may not capture complex belief systems where users hold contradictory positions across different domains.
- Linear dissonance assumption: The model assumes linear relationships that may oversimplify human decision-making, particularly for identity-protective cognition.

## Confidence

**High Confidence**: The core embedding methodology using triplet loss on co-voting patterns, the basic prediction framework using distance metrics, and the correlation between relative dissonance and prediction accuracy.

**Medium Confidence**: The semantic validity of the embedding space beyond the specific Debate.org domain, and the generalizability of the relative dissonance model to other contexts or belief formation processes.

**Low Confidence**: Claims about the model capturing the full complexity of human belief systems, particularly regarding how it handles conflicting beliefs, belief strength, or contextual factors in real-world belief formation.

## Next Checks

1. **Out-of-Domain Testing**: Apply the fine-tuned belief embeddings to a different debate dataset (e.g., political surveys or other online forums) and evaluate whether semantic relationships and prediction accuracy transfer.

2. **Ablation on User Representation**: Compare the simple averaging approach against weighted averaging and against clustering-based representations to reveal whether the averaging assumption is a significant limitation.

3. **Dissonance Model Validation**: Design experiments where users are explicitly asked to choose between beliefs with controlled dissonance levels, comparing actual choices against model predictions to validate the linear relationship.