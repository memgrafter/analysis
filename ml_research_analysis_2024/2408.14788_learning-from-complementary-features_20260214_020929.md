---
ver: rpa2
title: Learning from Complementary Features
arxiv_id: '2408.14788'
source_url: https://arxiv.org/abs/2408.14788
tags:
- values
- proposed
- exact
- learning
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a learning problem where some qualitative
  input features are only available as complementary information (CFs) indicating
  "what it is not" rather than precise values. The authors propose a new learning
  scenario called Complementary Feature Learning (CFL) and derive an information-theoretic
  objective function to estimate the exact values of CFs and predict output labels
  based on these estimations.
---

# Learning from Complementary Features

## Quick Facts
- arXiv ID: 2408.14788
- Source URL: https://arxiv.org/abs/2408.14788
- Reference count: 26
- One-line primary result: Proposed method effectively estimates CFs' exact values and significantly improves prediction performance compared to baselines

## Executive Summary
This paper introduces Complementary Feature Learning (CFL), a novel learning scenario where qualitative input features are only available as complementary information indicating "what it is not" rather than precise values. The authors propose a two-step approach: first estimating the exact values of CFs using graph-based iterative confidence propagation, then using these estimates in ordinary supervised learning to predict output labels. The method is validated on real-world datasets and demonstrates significant improvements over existing approaches, particularly for CFs with fewer possible values.

## Method Summary
The proposed method addresses the CFL problem through a two-step approach. First, it estimates the exact values of CFs by constructing a similarity graph and iteratively propagating confidences using weighted averaging. To handle computational complexity, the method uses marginal confidences instead of joint confidences based on conditional independence assumptions. Second, the estimated CF values are used as input to train a supervised model for label prediction. The method is guided by an information-theoretic objective function that serves as an upper bound on the standard supervised learning objective, ensuring the approach is theoretically grounded.

## Key Results
- The proposed method effectively estimates CFs' exact values and significantly improves prediction performance compared to baselines that use CFs directly or existing partial label learning methods
- The method is particularly effective for CFs with fewer possible values, where estimation accuracy is higher
- Computational complexity is reduced from O(T n²|X_c|) to O(T n²∑|X_c_j|) by using marginal confidences instead of joint confidences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complementary Feature Learning (CFL) allows effective supervised learning when some features are only available as "what it is not" information.
- Mechanism: The method estimates the exact values of CFs using a graph-based iterative confidence propagation algorithm, then uses these estimates in ordinary supervised learning to predict output labels.
- Core assumption: The smoothness assumption holds - points similar in the input space are also similar in the output space (X_o similarity corresponds to X_c probability distribution similarity).
- Evidence anchors:
  - [abstract]: "propose a new learning scenario called Complementary Feature Learning (CFL) and derive an information-theoretic objective function to estimate the exact values of CFs"
  - [section]: "we apply the smoothness assumption in SSL [1], which assumes that points that are similar in the input space are also similar in the output space"
  - [corpus]: Weak - no direct evidence found in corpus neighbors about CFL or complementary features
- Break condition: The smoothness assumption fails, meaning similar X_o points do not have similar X_c distributions. This would invalidate the weight optimization approximation.

### Mechanism 2
- Claim: The proposed method reduces computational complexity from O(T n²|X_c|) to O(T n²∑|X_c_j|) by using marginal confidences instead of joint confidences.
- Mechanism: Instead of propagating joint confidence distributions over all CFs simultaneously, the method propagates marginal confidence distributions for each CF separately, then combines them.
- Core assumption: The conditional independence assumption X_c1 ⊥ ... ⊥ X_cF_c | X_o holds, allowing marginal confidences to be combined without losing information.
- Evidence anchors:
  - [section]: "Because the marginal confidence for each CF is represented by an |X_c_j|-dimensional confidence vector, the computational complexity of confidence propagation for each CF is O(T n²|X_c_j|)"
  - [section]: "Therefore, by substituting the propagation of marginal confidence for the propagation of joint confidence, the computational complexity reduces from O(T n²∏|X_c_j|) to O(T n²∑|X_c_j|)"
  - [corpus]: Weak - no direct evidence found in corpus neighbors about marginal confidence propagation
- Break condition: The conditional independence assumption is violated. If CFs are strongly dependent given X_o, the marginal approach loses critical joint information.

### Mechanism 3
- Claim: The objective function JKL + JMI serves as an upper bound on the standard supervised learning objective, justifying the two-step approach of estimating CFs then predicting labels.
- Mechanism: The method derives an information-theoretic bound showing that the standard supervised learning loss is bounded by the sum of label prediction loss using estimated CFs and the information loss from CF estimation.
- Core assumption: The derived information-theoretic bound holds and is tight enough to guide effective learning.
- Evidence anchors:
  - [abstract]: "we derive an objective function from an information-theoretic perspective to estimate the OF values corresponding to CFs and to predict output labels based on these estimations"
  - [section]: "we consider JKL + JMI as the objective function for CFL, which is an upper bound on the objective function defined by Eq. (1) for ordinary supervised learning"
  - [section]: Theorem 1 proves the bound relationship mathematically
  - [corpus]: Weak - no direct evidence found in corpus neighbors about information-theoretic bounds in CFL
- Break condition: The bound becomes too loose in practice, making the optimization ineffective for improving prediction accuracy.

## Foundational Learning

- Concept: Information Theory and Kullback-Leibler Divergence
  - Why needed here: The entire method relies on KL divergence to measure information loss and construct the objective function
  - Quick check question: What does KL divergence measure between two probability distributions, and why is it appropriate for measuring information loss in this context?

- Concept: Graph-based Confidence Propagation
  - Why needed here: The core estimation algorithm propagates confidence over a similarity graph to estimate CF exact values
  - Quick check question: How does confidence propagation work in semi-supervised learning, and what modifications are needed for the CFL setting?

- Concept: Conditional Independence and Marginalization
  - Why needed here: The method uses marginal confidences instead of joint confidences based on conditional independence assumptions
  - Quick check question: Under what conditions can marginal distributions be safely combined without losing information about the joint distribution?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Convert categorical features to CFs, scale features for distance calculations
  - Weight matrix optimization: Solve Eq. (18) to determine instance similarity weights
  - Confidence propagation: Iterate Eq. (20) to estimate CF exact values
  - Label prediction: Train supervised model using estimated CF values
  - Post-processing: Apply Eq. (23) to enforce consistency with observed CF values

- Critical path: Weight matrix optimization → Confidence propagation → Label prediction
- Design tradeoffs:
  - Computational efficiency vs. estimation accuracy: Using marginal confidences reduces complexity but may lose joint information
  - Number of iterations T vs. convergence: More iterations may improve estimates but risk propagating errors
  - Hyperparameter k vs. smoothness assumption: Larger k considers more neighbors but may violate local similarity assumptions

- Failure signatures:
  - High CE (cross-entropy) but low Acc indicates confident but incorrect estimates
  - SE (Shannon entropy) not decreasing suggests the method isn't reducing uncertainty
  - Poor prediction performance despite good CF estimation indicates a mismatch between CF estimation and label prediction

- First 3 experiments:
  1. Test the weight matrix optimization with synthetic data where the smoothness assumption is known to hold
  2. Evaluate convergence behavior with different values of T and k on a small dataset
  3. Compare joint vs. marginal confidence propagation on a dataset with known CF dependencies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of unique values for complementary features affect the estimation accuracy of their exact values?
- Basis in paper: [explicit] The paper states that the proposed method tends to yield more accurate estimations for CFs with a smaller number of unique values, and that as the number of unique values increases, satisfying the smoothness assumption becomes more challenging, potentially hindering estimation accuracy.
- Why unresolved: While the paper demonstrates that the proposed method is more effective for CFs with fewer unique values, it doesn't provide a detailed analysis of the relationship between the number of unique values and estimation accuracy. The paper doesn't explore the specific threshold at which the number of unique values starts to significantly impact accuracy, or how different ranges of unique values affect the method's performance.
- What evidence would resolve it: Detailed experiments varying the number of unique values for CFs across a wide range, analyzing the correlation between unique values and estimation accuracy, and potentially proposing strategies for handling CFs with a high number of unique values.

### Open Question 2
- Question: How does the proposed method perform in comparison to existing methods when applied to real-world datasets with a large number of complementary features?
- Basis in paper: [inferred] The paper mentions that the computational complexity of the proposed method increases exponentially with the number of CFs, and addresses this by using marginal confidences instead of joint confidences. However, it doesn't provide a direct comparison of the proposed method's performance against existing methods when dealing with a large number of CFs in real-world datasets.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the proposed method for datasets with a moderate number of CFs (5 and 7 CFs in the experiments). It doesn't explore how the method scales when applied to datasets with a significantly larger number of CFs, which is a common scenario in real-world applications.
- What evidence would resolve it: Experiments applying the proposed method to real-world datasets with a large number of CFs (e.g., 20 or more), comparing its performance against existing methods in terms of estimation accuracy and computational efficiency.

### Open Question 3
- Question: How does the proposed method handle cases where the exact values of complementary features are not independent, given the observed values?
- Basis in paper: [explicit] The paper assumes that the exact values of CFs are independent given the observed values (X c 1 ⊥ ⊥ · · · ⊥ ⊥X c F c |X o). However, this assumption might not hold in real-world scenarios where the exact values of CFs could be correlated.
- Why unresolved: The paper doesn't explore the impact of relaxing the independence assumption on the estimation accuracy of CFs' exact values. It also doesn't provide insights into how the proposed method could be adapted to handle cases where the exact values of CFs are not independent.
- What evidence would resolve it: Experiments evaluating the proposed method's performance when the independence assumption is violated, comparing it against methods that can handle dependent CFs, and potentially proposing modifications to the proposed method to account for CF dependencies.

## Limitations

- The method's effectiveness depends on the smoothness assumption holding true, which may not always be the case in real-world datasets
- The assumption of conditional independence between CFs given observed values may not hold in practice, potentially limiting the method's applicability
- The method's performance may degrade when CFs have a large number of unique values, making estimation more challenging

## Confidence

- Method validity (Medium confidence): The method's validity critically depends on the smoothness assumption
- Computational complexity reduction (Medium confidence): The reduction is well-justified theoretically but assumes conditional independence that may not hold in practice
- Information-theoretic bound (High confidence in proof, Medium confidence in practical tightness): Provides theoretical grounding but the paper doesn't extensively test how loose this bound becomes in real-world scenarios

## Next Checks

1. Test the method on datasets where CFs are known to be conditionally dependent to evaluate the impact of violating the marginal confidence assumption
2. Analyze the smoothness assumption by measuring how similar X_o points relate to X_c distribution similarity across different datasets
3. Evaluate the information-theoretic bound tightness by comparing the actual supervised learning loss to the proposed upper bound on held-out test data