---
ver: rpa2
title: Robust Multivariate Time Series Forecasting against Intra- and Inter-Series
  Transitional Shift
arxiv_id: '2407.13194'
source_url: https://arxiv.org/abs/2407.13194
tags:
- series
- time
- forecasting
- distribution
- jointpgm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of non-stationary multivariate time
  series forecasting, where the data distribution shifts over time. To address this,
  the authors propose a novel probabilistic graphical model (PGM) called JointPGM
  that jointly captures intra- and inter-series correlations while modeling time-variant
  transitional distributions.
---

# Robust Multivariate Time Series Forecasting against Intra- and Inter-Series Transitional Shift

## Quick Facts
- arXiv ID: 2407.13194
- Source URL: https://arxiv.org/abs/2407.13194
- Reference count: 40
- Primary result: JointPGM achieves state-of-the-art forecasting performance with 15.3% MAE and 37.9% MSE improvements over baselines on six non-stationary MTS datasets

## Executive Summary
This paper addresses the challenge of non-stationary multivariate time series forecasting where data distribution shifts over time. The authors propose JointPGM, a novel probabilistic graphical model that captures both intra- and inter-series correlations while modeling time-variant transitional distributions. By decomposing the problem into intra-series temporal dynamics and inter-series spatial relationships, JointPGM uses a dual-encoder architecture with dynamic time factors learned via Fourier basis functions. Extensive experiments demonstrate significant improvements over state-of-the-art baselines.

## Method Summary
JointPGM is a probabilistic graphical model designed for non-stationary multivariate time series forecasting. It employs a dual-encoder architecture consisting of a Time Factor Encoder (TFE) that learns dynamic time factors using Fourier basis functions, and an Independence-based Series Encoder (ISE) with separate intra-series and inter-series learners. The intra-series learner captures temporal dynamics through temporal gates adjusted by dynamic time factors, while the inter-series learner models spatial dynamics via multi-hop propagation with Gumbel-softmax sampling. The learned dynamics are fused into a latent variable for prediction and reconstruction, with training performed using variational inference and ELBO optimization.

## Key Results
- Achieves 15.3% average improvement in MAE over all baselines
- Achieves 37.9% average improvement in MSE over all baselines
- Outperforms state-of-the-art methods including FEDformer, Autoformer, and N-HiTS on six highly non-stationary MTS datasets

## Why This Works (Mechanism)

### Mechanism 1
JointPGM's dual-encoder architecture with intra-series and inter-series learners jointly captures time-variant dynamics at finer granularity than normalization-based or coarse time-variant models. The intra-series learner uses temporal gates adjusted by dynamic time factors to capture time-variant transitional distributions within each series, while the inter-series learner models spatial dynamics through multi-hop propagation with Gumbel-softmax sampling. These are fused into a latent variable for prediction. The core assumption is that distribution shifts can be decomposed into intra-series and inter-series transitional shifts, and these can be effectively modeled separately then combined.

### Mechanism 2
Dynamic time factors learned via Fourier basis functions provide sensitive environmental change indicators that improve time-variant modeling. Multiple Fourier basis functions with diverse scale parameters capture high-frequency temporal patterns, which are processed through feedforward networks to learn dynamic time factors M(1)t that reflect environmental changes and regulate temporal gates. The core assumption is that time order information spanning both lookback and horizon windows provides better predictive power than simple positional encodings.

### Mechanism 3
Independence-based series encoding allows JointPGM to focus on temporal dynamics within each series before modeling inter-series relationships, improving representation quality. Each series is processed independently through a linear layer to h(i)t−L:t, then temporal gates are applied. This prevents early mixing of information across series, allowing cleaner temporal modeling. Inter-series relationships are then modeled through self-attention on these temporally-processed representations. The core assumption is that modeling intra-series temporal dynamics first provides a better foundation for subsequent inter-series correlation modeling.

## Foundational Learning

- **Concept: Distribution shift in time series**
  - Why needed here: Understanding distribution shift is fundamental to JointPGM's design - it's the problem being addressed. The paper distinguishes between normalization-based approaches (addressing temporal mean/covariance shifts) and time-variant approaches (modeling time-variant transitional distributions).
  - Quick check question: What are the two types of distribution shifts JointPGM decomposes the problem into, and how do they differ from traditional normalization approaches?

- **Concept: Probabilistic graphical models (PGMs)**
  - Why needed here: JointPGM is built on a PGM framework that represents probability distributions and dependency relationships among variables. This provides the theoretical foundation for decomposing the transitional distribution.
  - Quick check question: How does the PGM framework in JointPGM enable decomposition of time-variant transitional distribution into intra- and inter-series components?

- **Concept: Variational inference and ELBO**
  - Why needed here: JointPGM uses variational inference with KL divergence decomposition (Eq. 29) and ELBO for training. Understanding these concepts is crucial for grasping the loss function and optimization objectives.
  - Quick check question: What are the three terms in the KL divergence decomposition, and what aspect of JointPGM's learning objective does each term address?

## Architecture Onboarding

- **Component map**: TFE (Time Factor Encoder) → ISE (Independence-based Series Encoder with Intra-series and Inter-series learners) → DI (Dynamic Inference) → Decoder

- **Critical path**: TFE output M(1)t → Temporal gates G(i) applied to h(i)t−L:t → Intra-series latent variables ˆz(i)t−L:t → Self-attention to compute Wt → Gumbel-softmax sampling to get At → Multi-hop propagation to get ˜z(i)t−L:t → Fusion into Zt−L:t → DI to infer ˆZt → Decoder for predictions

- **Design tradeoffs**: Series independence vs. joint processing (potential information loss vs. cleaner temporal modeling), Fourier basis functions vs. positional encodings (better high-frequency capture vs. simpler implementation), multi-hop propagation depth vs. computational cost and oversmoothing

- **Failure signatures**: Poor performance on datasets with strong inter-series correlations (suggests independence assumption too strong), sensitivity to lookback window length (suggests temporal gate mechanism issues), degradation with longer horizons (suggests time factor learning inadequacy)

- **First 3 experiments**:
  1. Compare JointPGM with w/o ISE (A) and w/o ISE (F) variants on a small dataset to verify the independence-based series encoder provides benefits over standard Autoformer/FEDformer backbones
  2. Test different Fourier feature sizes and scale parameters (σs values) to find optimal time factor representation
  3. Evaluate the impact of the temporal gate by comparing JointPGM with and without TG on datasets with known distribution shifts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model performance change when using timestamp-based time factors instead of order-based time factors?
- Basis in paper: The paper mentions that they replaced order-based time factors with timestamp-based time factors in an ablation study and found it difficult to yield satisfactory results, suggesting that the order-based time factor encoder may have successfully learned advantageous high-frequency patterns.
- Why unresolved: The paper does not provide quantitative results comparing the performance of the model with order-based and timestamp-based time factors.
- What evidence would resolve it: A direct comparison of the model's performance metrics (MAE, MSE) using order-based and timestamp-based time factors on the same dataset.

### Open Question 2
- Question: How does the model's performance vary with different latent dimension sizes?
- Basis in paper: The paper mentions that they carefully tuned the latent dimension size d in {64, ...,512} and selected the optimal size of 128 to avoid inferior performance due to possible overfitting.
- Why unresolved: The paper does not provide results showing how the model's performance changes with different latent dimension sizes.
- What evidence would resolve it: A graph or table showing the model's performance metrics (MAE, MSE) for different latent dimension sizes on a specific dataset.

### Open Question 3
- Question: How does the model's performance compare to other state-of-the-art methods on datasets with different levels of non-stationarity?
- Basis in paper: The paper compares the model's performance to other state-of-the-art methods on six highly non-stationary MTS datasets, but does not provide a comparison across datasets with different levels of non-stationarity.
- Why unresolved: The paper does not provide a comparison of the model's performance across datasets with different levels of non-stationarity, as measured by the ADF test statistic.
- What evidence would resolve it: A comparison of the model's performance metrics (MAE, MSE) on datasets with different levels of non-stationarity, as measured by the ADF test statistic.

## Limitations

- The paper's performance improvements rely on evaluation across six datasets but lack statistical significance testing
- The independence-based series encoding assumption may not hold for datasets with strong inherent cross-series dependencies
- Key implementation details like Fourier basis function parameters and Gumbel-softmax temperature are not fully specified

## Confidence

- Dual-encoder architecture effectiveness: **Medium** - supported by ablation studies but lacks statistical significance testing
- Time factor learning via Fourier basis: **Low-Medium** - mechanism is theoretically sound but no ablation on alternative time encoding methods
- Independence-based series encoding benefits: **Medium** - ablation variants show improvement but no comparison to other series processing approaches
- State-of-the-art performance claims: **Medium** - strong quantitative results but limited baseline diversity and no statistical validation

## Next Checks

1. **Statistical significance testing**: Perform paired t-tests on the 6 datasets comparing JointPGM against top-3 baselines to establish whether reported improvements are statistically significant at α=0.05.

2. **Robustness to independence assumption**: Create a synthetic dataset with known inter-series correlations and test JointPGM against variants that use joint series processing (like standard Autoformer) to quantify the cost of the independence assumption.

3. **Time factor representation ablation**: Compare JointPGM with Fourier basis functions against versions using learned positional encodings and timestamp features on the ETTm2 dataset to isolate the contribution of the Fourier approach to overall performance.