---
ver: rpa2
title: Fact or Fiction? Improving Fact Verification with Knowledge Graphs through
  Simplified Subgraph Retrievals
arxiv_id: '2408.07453'
source_url: https://arxiv.org/abs/2408.07453
tags:
- claims
- bert
- subgraphs
- arxiv
- fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fact verification using structured
  knowledge graphs by simplifying the evidence retrieval process. The authors propose
  using direct subgraph retrieval methods, such as direct, contextual, and single-step
  approaches, to improve efficiency and accuracy compared to complex trained approaches.
---

# Fact or Fiction? Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retrievals

## Quick Facts
- **arXiv ID**: 2408.07453
- **Source URL**: https://arxiv.org/abs/2408.07453
- **Reference count**: 9
- **Primary result**: Simplified subgraph retrieval methods achieved 93.49% accuracy on fact verification, outperforming previous benchmarks by 15.84 percentage points

## Executive Summary
This paper addresses the challenge of fact verification using knowledge graphs by proposing simplified subgraph retrieval methods that significantly improve both accuracy and efficiency compared to complex trained approaches. The authors demonstrate that direct subgraph retrieval (direct, contextual, and single-step methods) outperforms more sophisticated retrieval mechanisms while reducing computational requirements. Three model architectures were evaluated: fine-tuned BERT, QA-GNN (a hybrid graph-language model), and ChatGPT prompting, with BERT using single-step subgraphs achieving the highest performance at 93.49% accuracy. The work establishes that simpler, more efficient approaches can deliver superior results in fact verification tasks.

## Method Summary
The paper proposes simplifying evidence retrieval in knowledge graph-based fact verification by using direct subgraph retrieval methods rather than complex trained approaches. Three retrieval strategies were implemented: direct retrieval (finding shortest paths), contextual retrieval (incorporating node context), and single-step retrieval (limiting to immediate neighbors). These methods were evaluated across three model architectures: a fine-tuned BERT model, a QA-GNN hybrid graph-language model with fixed embeddings, and a ChatGPT prompting approach. The evaluation used the FEVEROUS dataset of English simple Wikipedia facts, comparing performance against a benchmark that used trained subgraph retrieval with GEAR models requiring 2-3 days of training time.

## Key Results
- Fine-tuned BERT with single-step subgraphs achieved 93.49% test accuracy, significantly outperforming the previous benchmark of 77.65%
- QA-GNN trained in 1.5 hours compared to 2-3 days for the previous GEAR-based model, demonstrating substantial efficiency gains
- Simplified retrieval methods proved more effective than complex trained approaches for evidence retrieval in fact verification tasks
- BERT outperformed both QA-GNN and ChatGPT prompting approaches on the FEVEROUS dataset

## Why This Works (Mechanism)
The paper demonstrates that simplified subgraph retrieval methods improve fact verification by reducing the complexity of evidence selection while maintaining comprehensive coverage of relevant information. By focusing on direct, contextual, or single-step subgraphs rather than training complex retrieval models, the approach minimizes noise and computational overhead while ensuring that verification models receive appropriately focused evidence. The success of the BERT model with single-step subgraphs suggests that simpler evidence retrieval, when combined with strong language models, can achieve superior performance by providing cleaner, more relevant input for the verification task.

## Foundational Learning

**Knowledge Graphs**: Structured representations of facts as nodes and relationships - needed to understand the evidence structure for fact verification; quick check: can identify entities and their relationships in a sample knowledge graph.

**Subgraph Retrieval**: Methods for extracting relevant portions of knowledge graphs for specific verification tasks - needed to comprehend how evidence is selected; quick check: can explain the difference between direct, contextual, and single-step retrieval approaches.

**Fine-tuning vs. Fixed Embeddings**: Training paradigms where models either adapt to task data or use pretrained representations - needed to understand architectural choices; quick check: can distinguish between BERT fine-tuning and QA-GNN's fixed embedding approach.

**Fact Verification Benchmarks**: Standardized datasets like FEVEROUS used to evaluate performance - needed to contextualize results; quick check: can describe the FEVEROUS dataset structure and evaluation metrics.

**Computational Efficiency Metrics**: Measurements of training and inference time, memory usage - needed to evaluate practical deployment; quick check: can calculate and compare training time efficiency across different models.

## Architecture Onboarding

**Component Map**: Knowledge Graph -> Subgraph Retrieval -> Verification Model -> Fact Classification (BERT fine-tuning or QA-GNN)

**Critical Path**: Subgraph retrieval (single-step) -> BERT fine-tuning -> Fact verification classification

**Design Tradeoffs**: Simple retrieval methods vs. complex trained approaches - simpler methods provide faster training and better accuracy, while complex methods may offer more nuanced evidence selection but at significant computational cost.

**Failure Signatures**: Poor retrieval leading to missing critical evidence, overly complex subgraphs introducing noise, or mismatched model architecture for the retrieval method used.

**3 First Experiments**:
1. Compare direct, contextual, and single-step subgraph retrieval methods on a small subset of the FEVEROUS dataset
2. Evaluate BERT performance with varying subgraph sizes (1-hop to 3-hop) to identify optimal evidence scope
3. Test QA-GNN with different embedding pretraining approaches to optimize training efficiency

## Open Questions the Paper Calls Out
None

## Limitations

The evaluation is limited to English simple Wikipedia facts, which may not generalize to more complex fact verification scenarios or different knowledge domains. The computational efficiency claims lack detailed analysis of inference-time performance across different hardware configurations. The comparison between BERT and QA-GNN models uses different training paradigms, making direct architectural comparisons somewhat confounded by training methodology differences.

## Confidence

**High Confidence**: Simplified subgraph retrieval methods outperform complex trained approaches in both accuracy (93.49% vs 77.65%) and efficiency.

**Medium Confidence**: QA-GNN achieves faster training times (1.5 hours vs 2-3 days), though broader computational requirements need more comprehensive benchmarking.

**Medium Confidence**: Future work should explore larger subgraphs and LLM-knowledge graph combinations, though this remains untested in the current study.

## Next Checks

1. **Cross-Domain Generalization**: Test the simplified subgraph retrieval approach on fact verification datasets from different domains (scientific claims, news articles, or domain-specific knowledge bases) to assess generalizability beyond Wikipedia facts.

2. **Inference-Time Benchmarking**: Conduct comprehensive performance evaluation measuring memory usage, latency, and throughput during inference across different hardware configurations, particularly comparing BERT and QA-GNN approaches under identical conditions.

3. **Ablation Study on Subgraph Size**: Systematically evaluate fact verification performance across varying subgraph sizes (beyond the single-step approach) to identify the optimal trade-off between retrieval complexity and verification accuracy.