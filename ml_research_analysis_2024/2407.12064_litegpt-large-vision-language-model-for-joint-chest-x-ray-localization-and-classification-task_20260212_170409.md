---
ver: rpa2
title: 'LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization and
  Classification Task'
arxiv_id: '2407.12064'
source_url: https://arxiv.org/abs/2407.12064
tags:
- image
- medical
- visual
- images
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LiteGPT, a unified vision-language framework
  for joint localization and classification of critical findings in chest X-ray images.
  The method integrates multiple pre-trained visual encoders (BiomedCLIP and PubMedCLIP)
  to extract rich medical image representations, which are then projected into the
  language space of Llama 2 to generate detailed textual descriptions of both global
  diagnoses and localized findings with bounding boxes.
---

# LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization and Classification Task

## Quick Facts
- arXiv ID: 2407.12064
- Source URL: https://arxiv.org/abs/2407.12064
- Reference count: 40
- Primary result: Achieves F1-score of 60% for classification and sets new baselines for VLM localization (accuracy@0.3: 12.54%, accuracy@0.5: 8.12%) on VinDr-CXR dataset

## Executive Summary
LiteGPT introduces a unified vision-language framework that jointly performs localization and classification of critical findings in chest X-ray images. The method leverages multiple pre-trained visual encoders (BiomedCLIP and PubMedCLIP) to extract rich medical image representations, which are projected into the language space of Llama 2 to generate detailed textual descriptions including bounding boxes. Trained in two stages on the VinDr-CXR dataset, LiteGPT achieves state-of-the-art classification performance and establishes new localization baselines for vision-language models in medical imaging.

## Method Summary
LiteGPT employs a two-stage training approach on chest X-ray images. First, grounded critical findings learning uses BiomedCLIP and PubMedCLIP encoders (frozen) to extract features that are projected into Llama 2's language space via linear layers with GELU activation. The [identify] token is used for localization tasks. Second, diagnosis learning stage adapts the model for classification. The model generates textual outputs containing both global diagnoses and localized findings with bounding boxes, trained using instruction-tuned prompts.

## Key Results
- Achieves F1-score of 60% for image classification, significantly outperforming existing baselines
- Sets new VLM baselines for localization with accuracy@0.3 of 12.54% and accuracy@0.5 of 8.12%
- Demonstrates strong text validity scores across multiple NLP metrics (ROUGE, BLEU, METEOR, CIDEr)
- Shows substantial improvements over prior methods in both classification and localization tasks

## Why This Works (Mechanism)

### Mechanism 1
Multiple pre-trained visual encoders (BiomedCLIP + PubMedCLIP) improve image representation quality over a single encoder by concatenating features from two encoders trained on different medical datasets, increasing diversity and richness of visual representations. Core assumption: different encoders capture complementary information from medical images. Evidence: authors explicitly state leveraging multiple encoders enhances richness of image representation. Break condition: if concatenated features are redundant or one encoder dominates.

### Mechanism 2
Using the [identify] token instead of [grounding] improves localization accuracy because it is trained on a broader grounding dataset (RefCOCO, RefCOCO+, RefCOCOg) that teaches spatial relationships, whereas [grounding] is trained only on GRIT-20M in a single stage. Core assumption: spatial relationship learning from diverse grounding datasets translates to better localization in medical images. Evidence: paper attributes improved localization to [identify] token's broader training dataset. Break condition: if medical domain differs too much from grounding datasets.

### Mechanism 3
Freezing visual encoders during training preserves quality of extracted features while adapting language model to medical domain by avoiding catastrophic forgetting of learned representations. Core assumption: pre-trained visual encoders already provide high-quality medical image features, and fine-tuning them is unnecessary or harmful. Evidence: authors initially set visual encoders frozen during training stage. Break condition: if dataset distribution differs significantly from pre-training data.

## Foundational Learning

- Concept: Vision-Language Model (VLM) architecture
  - Why needed here: Understanding how visual features are projected into language space and fused with text tokens is critical to adapting model to medical tasks
  - Quick check question: Can you explain the role of visual projection layer and why concatenating five adjacent visual tokens is done?

- Concept: Instruction tuning and token-based task specification
  - Why needed here: Model uses special tokens ([identify], [vqa]) to switch between localization and classification tasks; understanding this is essential for correct data formatting
  - Quick check question: What is the difference between input templates for grounded critical findings learning stage and diagnosis learning stage?

- Concept: Evaluation metrics for medical image tasks
  - Why needed here: Knowing how precision, recall, F1-score, and IoU are used for classification and localization helps interpret results and debug failures
  - Quick check question: Why does paper use Accuracy@threshold instead of mAP for localization evaluation?

## Architecture Onboarding

- Component map: DICOM images (224x224 RGB) -> BiomedCLIP + PubMedCLIP (frozen) -> feature concatenation -> visual projection (linear layers + GELU) -> concatenate 5 adjacent visual tokens -> Llama 2-Chat (7B) with MiniGPT-v2 weights -> task tokens ([identify], [vqa]) + instruction text -> text output

- Critical path:
  1. Load and preprocess DICOM images (convert to RGB, normalize)
  2. Pass through both visual encoders → concatenate features
  3. Concatenate every five visual tokens → project into LLM space
  4. Add task tokens and instruction text → feed to LLM
  5. Generate text output containing findings and bounding boxes

- Design tradeoffs:
  - Freezing visual encoders saves compute and preserves feature quality but limits domain adaptation
  - Using multiple encoders increases compute and memory but improves representation
  - LoRA fine-tuning reduces parameter count but may limit adaptation capacity

- Failure signatures:
  - Poor localization: Low Accuracy@threshold, especially at higher IoU thresholds
  - Imbalanced classification: High recall for "No finding" but low for "Finding" due to dataset imbalance
  - Text validity issues: Low ROUGE/BLEU scores indicating poor text generation

- First 3 experiments:
  1. Train with only BiomedCLIP (single encoder) and compare to multi-encoder baseline
  2. Swap [identify] token for [grounding] token and measure localization accuracy change
  3. Unfreeze visual encoders and train end-to-end; compare performance and compute cost

## Open Questions the Paper Calls Out

### Open Question 1
What is the impact of using different pre-trained vision encoders beyond BiomedCLIP and PubMedCLIP on LiteGPT's performance? Basis: authors mention integrating multiple visual encoders but only explore BiomedCLIP and PubMedCLIP combination. Unresolved because paper doesn't investigate benefits of other pre-trained models. Evidence needed: comparative experiments using LiteGPT with different combinations of pre-trained vision encoders on VinDr-CXR dataset.

### Open Question 2
How does LiteGPT perform on other medical imaging datasets beyond VinDr-CXR? Basis: authors evaluate LiteGPT only on VinDr-CXR dataset, suggesting generalizability to other medical imaging tasks is unknown. Unresolved because paper focuses solely on VinDr-CXR dataset. Evidence needed: experiments applying LiteGPT to other well-established medical imaging datasets like ChestX-ray14 or MIMIC-CXR.

### Open Question 3
What is the optimal training strategy for LiteGPT, considering different combinations of grounded critical findings learning and diagnosis learning stages? Basis: authors describe two-stage training process but don't explore alternative training strategies or impact of different stage combinations. Unresolved because paper presents specific training approach without investigating alternative strategies. Evidence needed: ablation studies comparing LiteGPT's performance using different training strategies like single-stage training or varying order and duration of two stages.

## Limitations
- All experiments conducted on single dataset (VinDr-CXR), limiting generalizability claims
- No ablation study validates whether multi-encoder combination is superior to individual encoders
- Key hyperparameter sensitivity (LoRA rank, learning rate) not explored
- Token choice mechanism not rigorously tested through ablation

## Confidence

- **High Confidence**: Claims about overall performance improvements on VinDr-CXR (F1-score 60%, accuracy@0.3 12.54%, accuracy@0.5 8.12%)
- **Medium Confidence**: Claims about multi-encoder benefits and token choice advantages are plausible but lack direct ablation studies
- **Low Confidence**: Claims about generalizability to other medical imaging tasks or datasets are unsupported due to single-dataset evaluation

## Next Checks

1. **Ablation Study on Visual Encoders**: Train LiteGPT using only BiomedCLIP and only PubMedCLIP separately, then compare their performance to multi-encoder baseline to validate whether feature complementarity truly drives performance gains.

2. **Token Swap Experiment**: Replace [identify] token with [grounding] token in input prompt and retrain model. Measure change in localization accuracy to confirm token choice is causal factor for improved performance.

3. **Cross-Dataset Generalization**: Evaluate LiteGPT on different chest X-ray dataset (e.g., ChestX-ray14 or MIMIC-CXR) to test whether model's performance transfers beyond training distribution, validating claims about broad applicability.