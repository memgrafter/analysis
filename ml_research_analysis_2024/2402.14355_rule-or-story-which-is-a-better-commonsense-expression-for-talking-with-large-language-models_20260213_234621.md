---
ver: rpa2
title: Rule or Story, Which is a Better Commonsense Expression for Talking with Large
  Language Models?
arxiv_id: '2402.14355'
source_url: https://arxiv.org/abs/2402.14355
tags:
- commonsense
- stories
- rules
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether stories or rules are a better commonsense
  expression for talking with large language models (LLMs). The authors systematically
  compare stories and rules for retrieving and leveraging commonsense in LLMs using
  28 commonsense QA datasets.
---

# Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?

## Quick Facts
- arXiv ID: 2402.14355
- Source URL: https://arxiv.org/abs/2402.14355
- Reference count: 40
- Key outcome: Stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy

## Executive Summary
This paper systematically compares stories and rules as expressions for retrieving and leveraging commonsense knowledge in large language models (LLMs). Using 28 commonsense QA datasets and three LLMs (ChatGPT, Vicuna, Alpaca), the authors find that stories generally outperform rules in generating confident and accurate commonsense knowledge. The study reveals that stories are more effective for daily event questions while rules excel at scientific questions, aligning with the reporting bias in text corpora. The research also proposes iterative self-supervised fine-tuning to further improve the quality of generated commonsense stories.

## Method Summary
The authors systematically compare stories and rules for commonsense retrieval in LLMs using 28 commonsense QA datasets. They generate stories and rules using ChatGPT, Vicuna, and Alpaca models, then evaluate their confidence (via perplexity reduction) and commonsense accuracy (via ChatGPT evaluation). The effectiveness of stories versus rules is measured through QA accuracy on the datasets, with experiments testing each format alone and in combination. The study also implements iterative self-supervised fine-tuning using LoRA to improve story generation quality.

## Key Results
- Stories outperform rules in generating confident commonsense knowledge (higher perplexity reduction) across all tested LLMs
- Stories achieve higher commonsense accuracy than rules in generation tasks
- Stories are more effective for daily event questions while rules are better for scientific questions
- Combining stories and rules as context improves QA accuracy on 10-13 datasets compared to using either format alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stories are more effective than rules for retrieving commonsense from LLMs due to the reporting bias in training corpora.
- Mechanism: Human-written text corpora predominantly convey commonsense through stories rather than explicit rules. LLMs trained on these corpora learn to retrieve commonsense more effectively when expressed as stories because they have encountered more story-based commonsense examples during training.
- Core assumption: The training corpora of LLMs contains more story-based commonsense examples than rule-based ones.
- Evidence anchors:
  - [abstract] "This aligns with the reporting bias of commonsense in text corpora."
  - [section 4.2] "Commonsense in our daily life events...is subject to more pronounced reporting bias than other forms of commonsense"
  - [corpus] Found 25 related papers with average neighbor FMR=0.475, suggesting moderate relatedness to commonsense expression topics.
- Break condition: If the training corpora were to shift significantly towards rule-based commonsense expression, this mechanism would weaken.

### Mechanism 2
- Claim: LLMs can more confidently generate stories than rules due to exposure bias in reasoning.
- Mechanism: LLMs are exposed to more story-based reasoning patterns in training data. When generating commonsense, they have more examples to draw from for stories than for explicit rules, leading to higher confidence (measured by perplexity reduction) in story generation.
- Core assumption: The training data contains more examples of story-based reasoning than explicit rule-based reasoning.
- Evidence anchors:
  - [section 3.1] "Figure 2 shows that stories have significantly higher PR than rules for both Vicuna and Alpaca models"
  - [section 4.1] "LLMs are more confident in commonsense reasoning based on stories than on rules"
- Break condition: If LLMs were fine-tuned specifically on rule-based reasoning examples, this mechanism could reverse.

### Mechanism 3
- Claim: Stories and rules can complement each other to improve commonsense QA accuracy.
- Mechanism: Stories provide contextual information and narrative structure that help LLMs understand daily events, while rules provide concise, structured knowledge that helps with scientific concepts. Combining both allows LLMs to leverage the strengths of each format.
- Core assumption: LLMs can effectively integrate information from multiple formats (stories and rules) when provided together.
- Evidence anchors:
  - [section 4.2] "when employing both stories and rules as contextual inputs, LLMs can achieve higher QA accuracy on 10, 12, and 13 datasets compared to using either stories or rules alone"
  - [table 2] Shows accuracy improvements when using both stories and rules across multiple datasets
- Break condition: If LLMs struggle to integrate multiple formats or if one format consistently dominates, this complementary effect could diminish.

## Foundational Learning

- Concept: Perplexity as a confidence measure
  - Why needed here: The paper uses perplexity reduction (PR) to measure the confidence of LLM generation and reasoning
  - Quick check question: If an LLM generates text with lower perplexity, does that indicate higher or lower confidence in the generation?

- Concept: Reporting bias in text corpora
  - Why needed here: The paper argues that reporting bias affects how LLMs learn to express and retrieve commonsense knowledge
  - Quick check question: If reporting bias causes certain types of commonsense knowledge to be underrepresented in text corpora, how might this affect an LLM's ability to generate that knowledge?

- Concept: Zero-shot learning
  - Why needed here: The paper evaluates commonsense QA performance in zero-shot settings, where models answer questions without task-specific fine-tuning
  - Quick check question: In zero-shot learning, what distinguishes the model's performance from few-shot or fine-tuned approaches?

## Architecture Onboarding

- Component map: Question → Story/Rule Generation → Context Creation → Answer Generation → Evaluation (accuracy and confidence metrics)
- Critical path: Question → Story/Rule Generation → Context Creation → Answer Generation → Evaluation (accuracy and confidence metrics)
- Design tradeoffs: Using stories provides better performance on daily events but worse on scientific questions compared to rules; combining both formats improves overall performance but increases computational cost and complexity
- Failure signatures: Low commonsense accuracy in generated stories/rules indicates hallucination issues; low BERT similarity scores indicate semantic drifting; poor QA performance with stories on scientific questions suggests format mismatch
- First 3 experiments:
  1. Generate 5 stories and 5 rules for 100 randomly selected questions from each dataset using Vicuna and Alpaca, measuring perplexity reduction to compare generation confidence
  2. Use ChatGPT to evaluate the commonsense accuracy of generated stories and rules, comparing the two formats
  3. Answer questions using stories, rules, and both as context, measuring accuracy differences across datasets to identify format effectiveness for different commonsense types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of large language models on commonsense reasoning tasks vary across different model sizes and architectures?
- Basis in paper: Inferred from the paper's discussion of using different models (ChatGPT, Vicuna, and Alpaca) and the potential for future research to examine other models like GPT-4, Mistral, and Google's Bard.
- Why unresolved: The paper focuses on a specific set of models and does not explore the impact of model size or architecture on commonsense reasoning performance.
- What evidence would resolve it: Comparative studies evaluating the performance of various LLMs with different sizes and architectures on a standardized set of commonsense reasoning tasks.

### Open Question 2
- Question: What are the potential biases and limitations of using model-based scoring methods (like Vera and BERT similarity) to assess the quality of generated stories and filter training data?
- Basis in paper: Inferred from the paper's acknowledgment of potential biases in the Vera model and BERT similarity score, as well as the need for further investigation into these biases.
- Why unresolved: The paper uses these scoring methods but does not conduct a detailed analysis of their potential biases or limitations.
- What evidence would resolve it: A comprehensive analysis of the biases and limitations of various model-based scoring methods, including their impact on the quality and diversity of generated stories.

### Open Question 3
- Question: How can large language models be effectively trained to handle negations in commonsense reasoning tasks?
- Basis in paper: Explicitly discussed in the paper's analysis of datasets related to negation, where it was found that LLMs struggle with handling negations in commonsense questions.
- Why unresolved: The paper identifies the challenge but does not provide a solution or explore potential methods for improving LLM performance on negation tasks.
- What evidence would resolve it: Development and evaluation of new training techniques or model architectures specifically designed to improve LLM performance on negation tasks in commonsense reasoning.

## Limitations
- The study focuses on three specific LLMs (ChatGPT, Vicuna, Alpaca) which may not represent the broader landscape of language models
- Evaluation relies on automated methods (perplexity reduction, model-based scoring) that may not fully capture the complexity of commonsense reasoning
- The 28 commonsense QA datasets, while diverse, may not fully represent real-world commonsense reasoning scenarios

## Confidence
- High confidence: Stories generally outperform rules for daily event commonsense retrieval
- Medium confidence: Stories are more effective for daily events while rules are better for scientific questions
- Medium confidence: The mechanism linking reporting bias to LLM performance differences
- Low confidence: Generalizability of iterative self-supervised fine-tuning results

## Next Checks
1. Replicate the core experiments using a fundamentally different LLM architecture (e.g., BERT-based models or smaller language models) to validate whether the story/rule effectiveness pattern holds across diverse model families.

2. Conduct a systematic analysis of the 28 datasets to quantify the distribution of daily event versus scientific questions, ensuring that the observed format preferences aren't artifacts of dataset imbalance rather than genuine format effectiveness differences.

3. Compare ChatGPT's commonsense accuracy evaluations against human annotator judgments on a subset of generated stories and rules to validate whether the automated evaluation method aligns with human commonsense reasoning judgments.