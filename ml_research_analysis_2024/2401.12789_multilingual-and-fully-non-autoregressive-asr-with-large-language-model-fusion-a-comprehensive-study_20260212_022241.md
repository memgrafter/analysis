---
ver: rpa2
title: 'Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion:
  A Comprehensive Study'
arxiv_id: '2401.12789'
source_url: https://arxiv.org/abs/2401.12789
tags:
- scoring
- language
- size
- fusion
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a non-autoregressive LM-fused ASR system to
  address latency issues in autoregressive decoding for large-scale models. The approach
  combines the Universal Speech Model (USM) and the PaLM 2 language model in per-segment
  scoring mode, achieving an average relative WER improvement of 10.8% on FLEURS and
  3.6% on YouTube captioning.
---

# Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study

## Quick Facts
- arXiv ID: 2401.12789
- Source URL: https://arxiv.org/abs/2401.12789
- Reference count: 0
- The paper proposes a non-autoregressive LM-fused ASR system achieving 10.8% relative WER improvement on FLEURS and 3.6% on YouTube captioning

## Executive Summary
This paper addresses latency issues in autoregressive decoding for large-scale ASR models by proposing a non-autoregressive LM-fused approach. The system combines the Universal Speech Model (USM) with PaLM 2 language model in per-segment scoring mode, achieving significant WER improvements while enabling streaming inference. The comprehensive study analyzes key parameters including LLM size, context length, vocabulary size, and fusion methodology, providing insights into the factors influencing large-scale LM-fused speech recognition systems.

## Method Summary
The approach uses USM (2B Conformer) with CTC decoder for non-autoregressive hypothesis generation, then scores these hypotheses using PaLM 2 LLM in per-segment mode. Audio is segmented into 8-second chunks, with hypotheses scored using the LLM and context from previous segments. The system achieves streaming capability while improving accuracy through LLM fusion, with the LLM size and fusion weight being critical parameters for performance.

## Key Results
- Average relative WER improvement of 10.8% on FLEURS and 3.6% on YouTube captioning
- Larger LLMs (up to 340B parameters) reduce sensitivity to fusion weight tuning
- Optimal context length determined to be 32 seconds (4 segments of 8 seconds each)
- 32k vocabulary size achieves comparable performance to 256k while reducing computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-autoregressive LM-fused ASR improves accuracy by parallelizing decoding and leveraging LLM contextual scoring
- Mechanism: Combines CTC-decoded USM model with non-autoregressive PaLM 2 scoring step, allowing simultaneous processing of audio chunks and hypothesis scoring
- Core assumption: LLM scoring can effectively correct CTC-decoded hypotheses without full autoregressive decoding
- Evidence anchors: 10.8% WER improvement on FLEURS, 3.6% on YouTube; hypotheses scored non-autoregressively in teacher forcing mode
- Break condition: If LLM cannot correct CTC errors effectively, accuracy gains diminish and may be offset by computational overhead

### Mechanism 2
- Claim: Per-segment scoring with fixed prefix from prior segments improves long-form ASR performance
- Mechanism: Segments audio into 8-second chunks, scores current segment hypotheses using LLM, uses concatenated top hypothesis from previous two segments as LM prefix
- Core assumption: ~32 seconds of decoded text provides sufficient context for accurate scoring
- Evidence anchors: Iterative process updates every 8 seconds; 32 seconds (4 context segments) found optimal
- Break condition: If context beyond 32 seconds significantly improves accuracy, current segmentation strategy may be suboptimal

### Mechanism 3
- Claim: Larger LLMs provide more robust scoring with reduced sensitivity to fusion weight tuning
- Mechanism: As LLM size increases, model's ability to correctly score hypotheses improves, allowing higher fusion weights
- Core assumption: Larger LLMs have emergent properties making them more effective at ASR hypothesis scoring
- Evidence anchors: WER improves with larger models but gains might not offset inference costs; optimal LM weight increases from 0.25 (128M) to 0.45 (340B)
- Break condition: If inference costs outweigh accuracy gains, system may become impractical despite improved performance

## Foundational Learning

- Concept: Non-autoregressive decoding
  - Why needed here: Enables parallel processing of audio segments, reducing latency compared to autoregressive models
  - Quick check question: What is the main computational advantage of non-autoregressive decoding over autoregressive decoding?

- Concept: CTC (Connectionist Temporal Classification)
  - Why needed here: Provides way to generate non-autoregressive hypotheses from USM model without requiring frame-level alignment
  - Quick check question: How does CTC enable sequence prediction without explicit alignment between input frames and output tokens?

- Concept: Shallow fusion vs. per-segment scoring
  - Why needed here: Understanding tradeoff between computational cost and accuracy when fusing LLM scores with ASR hypotheses
  - Quick check question: Why might per-segment scoring be preferred over per-frame scoring in streaming ASR system?

## Architecture Onboarding

- Component map: Audio input -> USM encoding -> CTC decoding -> confusion network lattice -> N-best selection -> LLM scoring with prefix -> Score fusion -> final hypothesis output

- Critical path:
  1. Audio input → USM encoding → CTC decoding → confusion network lattice
  2. Lattice segmentation → N-best hypothesis selection → LLM scoring with prefix
  3. Score fusion → final hypothesis output

- Design tradeoffs:
  - Segment length: 8 seconds balances latency and context preservation
  - N-best list size: 16 hypotheses provides good accuracy without excessive computation
  - Context segments: 2 prior segments offer optimal context for scoring
  - Vocabulary size: 256k vs. 32k tokens tradeoff between accuracy and computational cost

- Failure signatures:
  - High WER with LM fusion: Check if LLM correctly processing hypotheses or if fusion weight misconfigured
  - Increased latency: Verify if segmentation or LLM scoring causing bottlenecks
  - Language-specific degradation: Examine if LLM prefix context insufficient for certain languages

- First 3 experiments:
  1. Verify non-autoregressive CTC decoding by comparing WER with and without LLM scoring
  2. Test impact of segment length (4s, 8s, 16s) on accuracy and latency
  3. Evaluate effect of context length (1-4 segments) on WER for multilingual test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different vocabulary sizes for ASR models impact effectiveness of LLM fusion?
- Basis in paper: [explicit] Study explores impact of vocabulary size by testing reduced 32k token vocabulary for PaLM 2, finding minimal performance degradation
- Why unresolved: Only compares two vocabulary sizes (256k and 32k) for LLM; impact of different vocabulary sizes for ASR model itself on LLM fusion effectiveness not explored
- What evidence would resolve it: Comprehensive study comparing various vocabulary sizes for both ASR and LLM models, measuring impact on WER and computational efficiency

### Open Question 2
- Question: How does choice of fusion methodology (per-segment vs. per-frame scoring) affect scalability of LLM-fused ASR systems for real-world applications?
- Basis in paper: [explicit] Compares per-segment and per-frame scoring, finding per-frame slightly outperforms per-segment but at higher computational cost
- Why unresolved: Study focuses on specific scenario with matched vocabularies; impact of fusion methodology on scalability in diverse real-world applications with varying vocabularies and latency requirements not fully explored
- What evidence would resolve it: Large-scale evaluation of different fusion methodologies across diverse ASR tasks and LLM sizes, measuring impact on WER, computational efficiency, and latency in real-world applications

### Open Question 3
- Question: How does size of LLM impact sensitivity of WER to fusion weight changes in multilingual ASR?
- Basis in paper: [explicit] Finds larger LLMs show decreased WER sensitivity to LM weight changes, suggesting smaller models require more cautious weighting
- Why unresolved: Study only examines limited range of LLM sizes (128M to 340B parameters); relationship between LLM size and WER sensitivity to fusion weight changes across wider range of sizes not fully understood
- What evidence would resolve it: Comprehensive study testing wider range of LLM sizes (e.g., 10M to 1T parameters) and measuring impact on WER sensitivity to fusion weight changes in multilingual ASR tasks

## Limitations

- Computational Cost Tradeoffs: Paper acknowledges but doesn't fully quantify practical tradeoff between accuracy gains and inference costs; 340B parameter LLM's computational overhead may render approach impractical for real-world deployment
- Language Generalization: Evaluation focuses on FLEURS (100+ languages with potential data quality issues) and YouTube captions (limited to 77 videos across 10 languages); claimed multilingual robustness not thoroughly validated across truly low-resource languages
- Streaming Latency Quantification: Paper emphasizes streaming capability but doesn't provide concrete latency measurements comparing non-autoregressive approach to autoregressive baselines

## Confidence

**High Confidence**: Core mechanism of combining USM with LLM scoring through per-segment fusion is well-demonstrated, with clear WER improvements (10.8% on FLEURS, 3.6% on YouTube); architectural design choices and their individual impacts are supported by ablation studies

**Medium Confidence**: Claims about LLM size effects and context length optimization are supported by experiments but could benefit from more extensive cross-validation across diverse domains; assertion that larger models reduce sensitivity to fusion weight tuning needs more rigorous statistical validation

**Low Confidence**: Practical deployment viability given computational costs remains speculative; paper doesn't provide cost-per-word estimates or real-time factor measurements necessary for production decisions

## Next Checks

1. **End-to-end Latency Measurement**: Implement comprehensive timing analysis measuring real-time factor and total processing latency for 8-second segments across different LLM sizes (1B, 8B, 70B, 340B) on representative hardware; include both inference time and any preprocessing/postprocessing overhead to validate streaming claims quantitatively

2. **Low-resource Language Stress Test**: Evaluate system on carefully curated low-resource language subset (e.g., languages with <100 hours of training data in FLEURS) to assess whether 32k vocabulary and 2B USM model provide sufficient capacity; compare against 256k vocabulary variant to measure accuracy-cost tradeoff for challenging languages

3. **Cost-Benefit Analysis Across Domains**: Conduct systematic study measuring WER improvement per unit of computation (e.g., WER reduction per GFLOP) across YouTube and FLEURS datasets; include ablation studies varying segment length (4s, 8s, 16s), n-best list size (8, 16, 32), and context segments (1-4) to identify optimal accuracy-efficiency operating point for different use cases