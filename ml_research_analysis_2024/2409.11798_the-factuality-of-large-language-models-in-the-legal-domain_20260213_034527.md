---
ver: rpa2
title: The Factuality of Large Language Models in the Legal Domain
arxiv_id: '2409.11798'
source_url: https://arxiv.org/abs/2409.11798
tags:
- language
- legal
- llms
- knowledge
- abstain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) as knowledge
  bases in the legal domain, focusing on factual accuracy. The authors design a dataset
  of legal questions from Wikidata and Wikipedia, then assess several open-source
  LLMs using exact, alias, and fuzzy matching evaluation methods.
---

# The Factuality of Large Language Models in the Legal Domain

## Quick Facts
- arXiv ID: 2409.11798
- Source URL: https://arxiv.org/abs/2409.11798
- Reference count: 27
- Key outcome: SaulLM achieves 81% precision on legal questions after legal pre-training, compared to 63% for Mistral-7B, with performance significantly improving under alias and fuzzy matching evaluation methods.

## Executive Summary
This paper evaluates large language models (LLMs) as knowledge bases in the legal domain, focusing on factual accuracy. The authors design a dataset of legal questions from Wikidata and Wikipedia, then assess several open-source LLMs using exact, alias, and fuzzy matching evaluation methods. They find that performance significantly improves under alias and fuzzy matching, with SaulLM achieving 81% precision after legal pre-training, compared to 63% for Mistral-7B. The study also shows that allowing models to abstain from answering and using in-context examples both enhance precision. Training on legal documents substantially improves factuality, particularly precision, though recall decreases. The results suggest LLMs can be reliable knowledge bases in legal applications when properly evaluated and instructed.

## Method Summary
The authors construct a dataset of 8,920 legal question-answer pairs from Wikidata, covering relations like "applies to jurisdiction" and "majority opinion by." They evaluate multiple open-source LLMs (Gemma, Llama, Mistral, Phi, SaulLM) using zero-shot and few-shot prompting strategies with and without abstain instructions. Responses are assessed using exact, alias, and fuzzy matching evaluation methods to calculate precision and recall metrics. The study compares models with and without legal pre-training and explores the impact of in-context examples on factuality.

## Key Results
- SaulLM (Mistral-7B with legal pre-training) achieves 81% precision compared to 63% for base Mistral-7B on legal questions
- Performance improves significantly under alias and fuzzy matching evaluation methods compared to exact matching
- Instructing models to abstain from uncertain answers and using in-context examples both enhance precision
- Legal pre-training substantially improves factuality, particularly precision, though recall decreases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using alias and fuzzy matching evaluation methods significantly improves the measured performance of LLMs compared to exact matching.
- Mechanism: Exact matching requires the model's answer to perfectly match the ground truth label, which is overly strict because LLMs often produce correct answers in different surface forms (e.g., using aliases or more verbose phrasing). Alias matching allows answers that match any known alias of the correct entity, while fuzzy matching accepts answers that contain the correct label or alias, even within a larger response. This relaxes the evaluation criteria to capture valid answer variations that exact matching misses.
- Core assumption: The relaxed matching methods (alias and fuzzy) do not introduce excessive false positives that would undermine the validity of the improved scores.
- Evidence anchors:
  - [abstract] "Our results show that the performance improves significantly under the alias and fuzzy matching methods."
  - [section] "These observations highlight the limitations of exact matching-based evaluation, which can unfairly penalize models that tend to generate more contextual, rich, and verbose answers, like SaulLM and Mistral-7B."
  - [corpus] Weak evidence. Related papers focus on factuality alignment and evaluation but do not directly validate the specific claim about alias/fuzzy matching improving LLM-as-KB evaluation metrics.
- Break condition: If the relaxed matching methods allow too many incorrect answers to be counted as correct (high false positive rate), the improved scores would not reflect genuine knowledge retrieval ability.

### Mechanism 2
- Claim: Instructing LLMs to abstain from answering when uncertain significantly increases precision by reducing the number of incorrect answers.
- Mechanism: By adding an instruction to respond with "I don't know" when unsure, the model learns to recognize and decline to answer questions beyond its knowledge. This abstention reduces the total number of answers provided, but the subset of answers given is more likely to be correct, thus increasing precision (correct answers / total answers).
- Core assumption: The model can accurately self-assess its confidence and distinguish between answerable and unanswerable questions, and that abstaining is better than providing incorrect answers in high-stakes domains.
- Evidence anchors:
  - [abstract] "Further, we explore the impact of abstaining and in-context examples, finding that both strategies enhance precision."
  - [section] "Including an abstain instruction generally improves precision across different models... This behavior is particularly important for LLMs accessible only through APIs that do not provide probability scores."
  - [corpus] Weak evidence. Related papers discuss factuality alignment and evaluation but do not directly validate the specific claim about abstention improving precision in LLM-as-KB settings.
- Break condition: If the model abstains too frequently on answerable questions, recall would drop significantly, making the system less useful despite higher precision.

### Mechanism 3
- Claim: Providing in-context examples (few-shot prompting) significantly improves the factuality of LLMs by teaching them the expected answer format and correcting erroneous patterns from pretraining.
- Mechanism: In-context examples demonstrate the correct type and format of answers expected for specific relations and subject classes. This helps the model learn the mapping between question types and answer types, and overrides incorrect patterns learned during pretraining (e.g., reversing plaintiff/defendant roles in legal cases). This leads to more accurate and appropriately formatted answers.
- Core assumption: The model can effectively learn from a small number of in-context examples without fine-tuning, and that the examples provided are representative and correct.
- Evidence anchors:
  - [abstract] "Further, we explore the impact of abstaining and in-context examples, finding that both strategies enhance precision."
  - [section] "Our observations indicate that in-context examples improve precision for 6 out of 8 models... revealing two main benefits: 1. In-context examples help the model learn the expected type and format of answers... 2. In-context examples correct wrong patterns learned during pre-training."
  - [corpus] Weak evidence. Related papers focus on factuality alignment and evaluation but do not directly validate the specific claim about few-shot prompting improving LLM-as-KB performance.
- Break condition: If the in-context examples are not representative or contain errors, the model may learn incorrect patterns, reducing factuality.

## Foundational Learning

- Concept: Knowledge Base (KB) concepts (precision, recall, exact/alias/fuzzy matching)
  - Why needed here: The paper evaluates LLMs as knowledge bases, using precision and recall metrics, and compares different evaluation methods (exact, alias, fuzzy matching). Understanding these concepts is crucial for interpreting the results and methodology.
  - Quick check question: What is the difference between precision and recall in the context of evaluating a knowledge base, and how do exact, alias, and fuzzy matching differ in their evaluation criteria?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: The paper uses few-shot prompting with in-context examples to improve LLM performance. Understanding how to construct effective prompts and leverage few-shot learning is essential for replicating and extending the experiments.
  - Quick check question: How do in-context examples in few-shot prompting help an LLM learn the expected answer format and correct erroneous patterns from pretraining?

- Concept: Domain-specific training and fine-tuning
  - Why needed here: The paper demonstrates that pre-training an LLM on legal documents (SaulLM) significantly improves its factuality in the legal domain. Understanding the impact and methods of domain-specific training is important for applying LLMs to specialized knowledge bases.
  - Quick check question: Why does training an LLM on legal documents improve its factuality in the legal domain, and how does this compare to the benefits of few-shot prompting?

## Architecture Onboarding

- Component map: Dataset of legal questions (from Wikidata/Wikipedia) -> Open-source LLMs (Gemma, Llama, Mistral, Phi, SaulLM) -> Prompt strategies (zero-shot, few-shot with/without abstain instruction) -> Evaluation methods (exact, alias, fuzzy matching) -> Precision and recall metrics

- Critical path: 1) Construct legal question dataset from Wikidata/Wikipedia. 2) Select and load open-source LLM models. 3) Design prompt strategies (zero-shot and few-shot with abstain instruction). 4) Query LLMs with the dataset using different prompts. 5) Evaluate responses using exact, alias, and fuzzy matching methods. 6) Calculate precision and recall metrics for each model/prompt/evaluation combination.

- Design tradeoffs: Exact matching is strict but may underestimate LLM performance by missing valid answer variations. Alias and fuzzy matching are more lenient but risk introducing false positives. Allowing abstention increases precision but reduces recall. Few-shot prompting improves performance but requires constructing representative in-context examples. Domain-specific pre-training improves factuality but requires additional resources and data.

- Failure signatures: If precision is high but recall is very low, the model may be abstaining too frequently. If performance doesn't improve with alias/fuzzy matching, the evaluation method may not be capturing valid answer variations. If few-shot prompting doesn't help, the in-context examples may be poorly constructed or unrepresentative.

- First 3 experiments:
  1. Evaluate a base LLM (e.g., Mistral-7B) on the legal dataset using exact matching with zero-shot prompting to establish a baseline.
  2. Repeat the evaluation using alias matching to see if performance improves, indicating the model provides valid answer variations not captured by exact matching.
  3. Add an abstain instruction to the prompt and re-evaluate using fuzzy matching to measure the combined effect on precision and recall.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs in the legal domain compare to specialized legal databases in terms of precision and recall?
- Basis in paper: [inferred] The paper focuses on LLMs as knowledge bases but doesn't directly compare their performance to existing legal databases.
- Why unresolved: The study only evaluates LLMs against a dataset derived from Wikidata and Wikipedia, without benchmarking against established legal databases.
- What evidence would resolve it: A comparative study evaluating the same legal questions using both LLMs and specialized legal databases, measuring precision and recall for each.

### Open Question 2
- Question: What is the impact of model size on the factuality of LLMs in the legal domain?
- Basis in paper: [inferred] The study uses models under 8B parameters, but doesn't explore the relationship between model size and factuality.
- Why unresolved: The paper only tests a limited range of model sizes, leaving the question of whether larger models would perform significantly better.
- What evidence would resolve it: A systematic evaluation of LLMs with varying parameter counts, testing their factuality on the same legal dataset.

### Open Question 3
- Question: How do different fine-tuning strategies for legal domain adaptation affect LLM factuality?
- Basis in paper: [explicit] The paper mentions that SaulLM, which is Mistral-7B with additional legal pre-training, performs better, but doesn't explore other fine-tuning strategies.
- Why unresolved: The study only compares one model with legal pre-training (SaulLM) to its base model, without exploring alternative fine-tuning approaches.
- What evidence would resolve it: A comparative study of various legal fine-tuning strategies (e.g., supervised fine-tuning on legal datasets, reinforcement learning from legal feedback) and their impact on factuality.

## Limitations
- The evaluation relies on a dataset derived from Wikidata and Wikipedia, which may not fully represent the complexity of real-world legal databases
- The study only tests models under 8B parameters, leaving the question of whether larger models would perform significantly better
- The optimal balance between precision and recall when using abstention remains unclear, particularly for different legal question types

## Confidence
- Mechanism 1 (Alias/Fuzzy Matching): Medium - The performance improvements are well-documented, but the validity of relaxed matching in high-stakes legal contexts needs further validation
- Mechanism 2 (Abstain Instructions): Medium - Precision improvements are demonstrated, but the trade-off with recall and optimal abstention thresholds remain unclear
- Mechanism 3 (In-Context Examples): High - The benefits of few-shot prompting are consistently observed across multiple models and well-supported by the data

## Next Checks
1. Conduct human evaluation of fuzzy matching results to establish false positive rates and determine acceptable thresholds for legal applications
2. Test abstention strategies with different confidence thresholds to optimize the precision-recall trade-off for various legal use cases
3. Expand the evaluation to include cross-jurisdictional legal questions and more complex multi-hop reasoning tasks to assess scalability