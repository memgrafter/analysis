---
ver: rpa2
title: 'From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in
  Text Watermarking for Large Language Models'
arxiv_id: '2406.11106'
source_url: https://arxiv.org/abs/2406.11106
tags:
- text
- watermarking
- arxiv
- techniques
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of text watermarking
  techniques for large language models (LLMs). The authors construct a taxonomy based
  on application-driven intentions, evaluation data sources, watermark addition methods,
  and adversarial attacks.
---

# From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in Text Watermarking for Large Language Models

## Quick Facts
- arXiv ID: 2406.11106
- Source URL: https://arxiv.org/abs/2406.11106
- Reference count: 20
- Primary result: Comprehensive survey constructing taxonomy of text watermarking techniques for LLMs based on application intentions, evaluation data sources, watermark addition methods, and adversarial attacks

## Executive Summary
This paper provides a comprehensive survey of text watermarking techniques for large language models (LLMs), constructing a taxonomy based on application-driven intentions, evaluation data sources, watermark addition methods, and adversarial attacks. The authors identify three main intentions for watermarking: text quality preservation, similar output distribution, and model ownership verification. They highlight key challenges including the need for more rigorous testing against diverse de-watermarking attacks, standardized benchmarks for fair comparison, and understanding how watermarking impacts LLM factuality. The survey covers 50+ research papers and provides detailed descriptions of various watermarking techniques including rule-based substitutions, embedding-level additions, and ad-hoc methods.

## Method Summary
The authors conducted a comprehensive literature review of 50+ research papers on text watermarking techniques for LLMs. They constructed a taxonomy by analyzing papers based on specific intentions behind different watermarking techniques, evaluation datasets used, watermarking addition methods, and adversarial attacks. The survey synthesizes existing research without presenting new empirical results, focusing instead on categorizing and organizing the current state of the field into a cohesive framework that identifies open challenges and gaps in current research efforts.

## Key Results
- Identified three main intentions for text watermarking in LLMs: text quality preservation, output distribution similarity, and model ownership verification
- Highlighted critical challenges including lack of rigorous testing against diverse de-watermarking attacks and need for standardized benchmarks
- Noted the absence of analysis on how watermarking techniques affect LLM factuality and hallucination rates
- Covered 50+ research papers detailing various watermarking techniques including rule-based substitutions, embedding-level additions, and ad-hoc methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text watermarking techniques can preserve text quality while embedding ownership information.
- Mechanism: Watermarking methods like green-red list partitioning adjust token probabilities to minimize perplexity impact and maintain semantic relatedness.
- Core assumption: The model can embed watermarks without significantly altering the statistical properties of the generated text.
- Evidence anchors:
  - [abstract] The authors analyze watermarking techniques based on text quality preservation, output distribution similarity, and model ownership verification.
  - [section] "One example is the use of green-red list rules...to train LLMs to produce only green words and are used to minimize perplexity impact."
  - [corpus] Weak corpus evidence; no direct mention of green-red lists or perplexity preservation.
- Break condition: If the watermarking process significantly increases perplexity or degrades semantic relatedness, the text quality will be noticeably affected.

### Mechanism 2
- Claim: Watermarking can be achieved by modifying output logits to maintain similar word distributions.
- Mechanism: Adjusting the output logits of LLMs to inject watermarks while keeping the overall word distribution consistent with the original text.
- Core assumption: The model can subtly alter output logits without changing the natural flow and distribution of words.
- Evidence anchors:
  - [abstract] The survey covers techniques including rule-based substitutions, embedding-level additions, and ad-hoc methods.
  - [section] "These strategies involve adjusting (re-weighting) the probabilities of select words during text generation to ensure the overall distribution of words remains consistent with the original."
  - [corpus] Weak corpus evidence; no direct mention of output logits modification for watermarking.
- Break condition: If the modified logits lead to unnatural word distributions or noticeable changes in text fluency, the watermarking will be ineffective.

### Mechanism 3
- Claim: Trigger set-based methods can enhance model ownership verification by reducing false positives.
- Mechanism: Using specific inputs or conditions that activate embedded watermarks, ensuring reliable detection under certain triggers.
- Core assumption: The trigger sets can be designed to reliably activate watermarks without being easily bypassed by attackers.
- Evidence anchors:
  - [abstract] The authors categorize watermarking techniques into three types: Text Quality, Output Distribution, and Model Ownership Verification.
  - [section] "Trigger set-based methods reduce the amount of false positives. Trigger sets are specific inputs designed to activate watermarks embedded within a model or dataset."
  - [corpus] Weak corpus evidence; no direct mention of trigger sets for ownership verification.
- Break condition: If attackers can predict or manipulate the trigger conditions, the watermarking method becomes vulnerable to bypass.

## Foundational Learning

- Concept: Text watermarking in LLMs
  - Why needed here: Understanding the basics of how watermarks are embedded in text generated by large language models is crucial for developing and evaluating watermarking techniques.
  - Quick check question: What are the primary goals of text watermarking in the context of LLMs?

- Concept: Perplexity and its role in text quality assessment
  - Why needed here: Perplexity measures the model's confidence in its generations, and maintaining low perplexity is essential for preserving text quality during watermarking.
  - Quick check question: How does perplexity relate to the quality of text generated by LLMs, and why is it important in watermarking?

- Concept: Adversarial attacks on watermarking techniques
  - Why needed here: Understanding potential attacks helps in designing robust watermarking methods that can withstand attempts to remove or bypass watermarks.
  - Quick check question: What are some common adversarial attacks on text watermarking, and how can watermarking techniques be made more resilient?

## Architecture Onboarding

- Component map:
  Text watermarking techniques (Rule-based substitutions, embedding-level additions, ad-hoc additions) -> Evaluation methods (Downstream NLP tasks, post-watermark text similarity analysis) -> Adversarial attacks (Text insertion, deletion, and substitution attacks)

- Critical path:
  1. Identify the watermarking technique based on the desired intention (text quality, output distribution, model ownership).
  2. Apply the watermarking method to the text.
  3. Evaluate the effectiveness of the watermarking using appropriate benchmarks and downstream tasks.
  4. Test the robustness of the watermark against potential adversarial attacks.

- Design tradeoffs:
  - Balancing watermark strength with text quality preservation.
  - Ensuring robustness against adversarial attacks while maintaining efficiency.
  - Compatibility with various NLP downstream tasks.

- Failure signatures:
  - Increased perplexity or degraded semantic relatedness.
  - Noticeable changes in text fluency or natural flow.
  - Vulnerability to adversarial attacks leading to watermark removal.

- First 3 experiments:
  1. Implement a green-red list watermarking technique and measure its impact on text perplexity and semantic relatedness.
  2. Evaluate the robustness of the watermarking method against text insertion and deletion attacks.
  3. Test the compatibility of the watermarking technique with various NLP downstream tasks, such as text classification and summarization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different watermarking techniques impact the factuality and hallucination rates of LLM-generated text?
- Basis in paper: [explicit] The paper states: "Despite this potential trade-off, there is a lack of analysis on how watermarking techniques affect the output inaccuracies or hallucinations."
- Why unresolved: While the paper acknowledges this potential trade-off, it does not provide empirical evidence or detailed analysis on how various watermarking methods specifically affect LLM factuality and hallucination rates.
- What evidence would resolve it: Comparative studies measuring hallucination rates and factuality scores across multiple watermarking techniques, using standardized evaluation datasets and factuality metrics.

### Open Question 2
- Question: What are the most effective de-watermarking attacks against current text watermarking techniques, and how can watermarking methods be made more resilient?
- Basis in paper: [explicit] The paper highlights: "One of the critical challenges in the field is the lack of comprehensive evaluation against a diverse range of de-watermarking attacks."
- Why unresolved: While the paper outlines potential attack types (text insertion, deletion, substitution), it does not provide systematic testing of these attacks against various watermarking techniques or propose specific countermeasures.
- What evidence would resolve it: Comprehensive red-teaming studies applying multiple attack types to various watermarking techniques, along with development and testing of robust countermeasures.

### Open Question 3
- Question: How can watermarking techniques be standardized to enable fair and consistent comparison across different methods and applications?
- Basis in paper: [explicit] The paper states: "There is a need for standardized benchmarks and evaluation metrics to ensure fair and consistent comparison between different watermarking techniques."
- Why unresolved: The paper observes that different works use varied evaluation datasets and metrics, making it difficult to compare effectiveness across techniques.
- What evidence would resolve it: Development of a standardized evaluation framework with common datasets, metrics, and testing protocols for watermarking techniques across various NLP tasks and applications.

## Limitations
- The survey lacks empirical validation or quantitative metrics, limiting assessment of practical effectiveness of different watermarking techniques
- Potential incompleteness of the taxonomy due to rapidly evolving nature of LLM watermarking research
- Effectiveness of proposed solutions against sophisticated adversarial attacks remains largely theoretical without extensive real-world testing

## Confidence

- High confidence: The classification framework and taxonomy structure based on application-driven intentions is well-supported by the literature review
- Medium confidence: The characterization of key challenges (need for rigorous testing against diverse de-watermarking attacks, standardized benchmarks, and understanding watermarking's impact on LLM factuality) is logically derived from the literature
- Low confidence: Specific effectiveness claims about individual watermarking techniques cannot be verified without access to the original experimental results from cited papers

## Next Checks

1. Conduct controlled experiments comparing multiple watermarking techniques across the three identified intentions using standardized benchmarks and diverse adversarial attack scenarios to quantify effectiveness and robustness.

2. Test selected watermarking methods across multiple datasets and model architectures to evaluate whether the taxonomy's categorizations hold consistently across different LLM families and text domains.

3. Design experiments specifically measuring how different watermarking intensities affect LLM factuality and hallucination rates, addressing the identified gap in understanding this relationship.