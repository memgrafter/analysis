---
ver: rpa2
title: 'Towards certifiable AI in aviation: landscape, challenges, and opportunities'
arxiv_id: '2409.08666'
source_url: https://arxiv.org/abs/2409.08666
tags:
- learning
- data
- certification
- neural
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of certifying AI systems for
  use in aviation, a critical domain with stringent safety requirements. It presents
  a comprehensive roadmap for AI certification in avionics, based on the European
  Union Aviation Safety Agency's (EASA) guidelines for Level 1 & 2 AI.
---

# Towards certifiable AI in aviation: landscape, challenges, and opportunities

## Quick Facts
- arXiv ID: 2409.08666
- Source URL: https://arxiv.org/abs/2409.08666
- Reference count: 40
- One-line primary result: Presents a comprehensive roadmap for AI certification in avionics based on EASA guidelines, highlighting the limitations of current AI development practices and the need for certification beyond performance metrics.

## Executive Summary
This paper addresses the critical challenge of certifying AI systems for aviation applications, where safety requirements are stringent and non-negotiable. The authors present a structured roadmap for AI certification in avionics based on European Union Aviation Safety Agency (EASA) guidelines for Level 1 & 2 AI. The roadmap breaks down the certification process into four main blocks: Trustworthiness Analysis, AI Assurance, Human Factors for AI, and AI Safety Risk Mitigation. Through a case study using YOLOv8, the paper demonstrates that classical AI development practices lack essential certification steps, emphasizing the need for generalization of certification methods, proper operational design domain description, and attention to ethical and human factors.

## Method Summary
The paper synthesizes EASA guidelines and current research to create a structured roadmap for AI certification in aviation. The methodology involves reviewing state-of-the-art methods for each of the four certification blocks, analyzing the gaps between classical AI development practices and certification requirements, and demonstrating these gaps through a case study of YOLOv8. The authors examine existing AI development cycles, identify missing certification elements, and propose a systematic approach to address these shortcomings. The work combines literature review with practical analysis of a real-world computer vision model to illustrate the certification challenges.

## Key Results
- Classical AI development stops at performance metrics without incorporating formal trustworthiness analysis, AI assurance, human factors, or safety risk mitigation.
- Incomplete operational design domain (ODD) description undermines dataset completeness and representativeness, weakening generalization.
- The lack of human-AI teaming certification blocks leaves human-automation interaction unsafe in critical domains.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Breaking the certification loop at performance metrics alone leads to unreliable AI systems in safety-critical domains.
- Mechanism: The classical AI development cycle stops at test-time performance evaluation without incorporating formal trustworthiness analysis, AI assurance, human factors, or safety risk mitigation. This omission allows models like YOLOv8 to achieve high accuracy but remain uncertified for avionics use.
- Core assumption: Performance metrics (e.g., mAP, FPS) are insufficient for safety-critical deployment.
- Evidence anchors:
  - [abstract] states the paper highlights "limitations of current AI development practices" and emphasizes the need for qualification "beyond performance metrics."
  - [section 7] explicitly shows YOLOv8 examples where trustworthiness analysis, safety risk assessment, and human factors are missing or marked as incomplete.
  - [corpus] includes related works discussing the need for certification adaptation and risk mitigation in aviation AI.
- Break condition: If performance-only evaluation becomes legally acceptable in regulated domains, the certification framework becomes unnecessary and this mechanism collapses.

### Mechanism 2
- Claim: Incomplete operational design domain (ODD) description undermines dataset completeness and representativeness, weakening generalization.
- Mechanism: Without precise ODD definition, datasets cannot be guaranteed to cover all required operating conditions, leading to gaps in training data. This causes poor generalization, undetected edge cases, and unstable system behavior in unseen scenarios.
- Core assumption: ODD completeness is a prerequisite for dataset representativeness.
- Evidence anchors:
  - [section 2.3] defines ODD as "a set of constraints and requirements for a specific purpose" and notes that precise ODD definition is "a prerequisite for the quality, completeness, and representativeness of the datasets."
  - [section 8] lists "Operational design domain description (ODD): The lack of OD and ODD description in the DNN development process greatly affects the completeness and representativeness of the dataset selection."
  - [corpus] includes works on defining operational conditions from data, reinforcing ODD importance.
- Break condition: If automated methods can infer ODD from data without explicit specification, this mechanism loses relevance.

### Mechanism 3
- Claim: The lack of human-AI teaming certification blocks leaves human-automation interaction unsafe.
- Mechanism: Current AI development ignores human factors such as operational explainability, interaction modality, error and failure management, and CRM integration. This creates systems that users cannot trust or safely collaborate with, especially under stress or abnormal conditions.
- Core assumption: Human-AI teaming is essential for safety-critical system certification.
- Evidence anchors:
  - [section 5] outlines five certification requirements for human factors, including "AI operational explainability," "Human-AI teaming," "Interaction Modality," "Error Management," and "Failure Management."
  - [section 7] shows YOLOv8 examples with missing or incomplete human factor certification entries.
  - [corpus] includes works on human-AI collaboration and error management in avionics testing.
- Break condition: If AI systems are deployed without human-in-the-loop requirements, this mechanism becomes irrelevant.

## Foundational Learning

- Concept: Operational Design Domain (ODD)
  - Why needed here: ODD defines the exact scope of operating conditions under which the AI must function safely. It is the foundation for dataset representativeness and generalization bounds.
  - Quick check question: What constraints and requirements define the intended application's ODD for collision avoidance in avionics?
- Concept: Trustworthiness Analysis (TA)
  - Why needed here: TA ensures ethical, safety, and security compliance beyond performance. It is required for certification in critical domains.
  - Quick check question: Which ethical and safety assessments are mandatory for Level 1 and Level 2 AI according to EASA?
- Concept: AI Assurance (AIA) and the W-shape model
  - Why needed here: AIA provides a structured cycle for learning assurance and explainability, ensuring the model is built right and built correctly.
  - Quick check question: What are the two main blocks of AI assurance and their key objectives in the certification cycle?

## Architecture Onboarding

- Component map: Certification process → Trustworthiness Analysis → AI Assurance (W-shape: Data Management, Learning Management, Model Training, Learning Verification, Model Implementation, Inference Verification, Data & Learning Verification) → Human Factors for AI → AI Safety Risk Mitigation
- Critical path: ODD definition → Dataset completeness/representativeness → Model training with generalization bounds → Learning verification (stability & robustness) → Human-AI teaming certification → Continuous safety risk monitoring
- Design tradeoffs: General-purpose models vs. task-specific models (certification complexity), performance vs. interpretability (trustworthiness), dataset size vs. ODD coverage (representativeness)
- Failure signatures: High performance on validation/test data but poor robustness to out-of-distribution data, missing ethical assessment, lack of human factors integration, incomplete ODD specification
- First 3 experiments:
  1. Define ODD for a specific avionics task (e.g., collision avoidance) and map required operating conditions
  2. Audit a YOLOv8-like model for missing trustworthiness analysis, AI assurance, and human factors entries using the provided tables
  3. Simulate out-of-distribution data to test model stability and robustness, documenting failure modes for certification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generalization bounds for deep learning models be effectively quantified to ensure performance on unseen data and define uncertainty bounds for out-of-distribution data?
- Basis in paper: [explicit] The paper discusses the need for clear and accurate generalization bounds for deep learning models to ensure performance on unseen data and help define uncertainty bounds for out-of-distribution data.
- Why unresolved: The paper mentions that while there are methods to increase generalizability, effective methods need to be developed to quantify levels of generalization assurance throughout the learning assurance cycle.
- What evidence would resolve it: Development and validation of methods that can effectively quantify generalization bounds for deep learning models, with empirical evidence showing improved performance on unseen data and out-of-distribution data.

### Open Question 2
- Question: How can the completeness and representativeness of datasets be effectively quantified to ensure compliance with operational design domain (ODD) requirements in AI certification?
- Basis in paper: [explicit] The paper emphasizes the importance of completeness and representativeness of datasets in determining compliance with ODD requirements and mentions the need for a trade-off between these two objectives.
- Why unresolved: The paper highlights that real data is high-dimensional and faces challenges such as missing values, outliers, noise, and labeling errors, making it difficult to quantify the trade-off between completeness and representativeness in real conditions.
- What evidence would resolve it: Development and validation of techniques that can effectively quantify the completeness and representativeness of datasets, with empirical evidence showing improved compliance with ODD requirements.

### Open Question 3
- Question: How can explainability and uncertainty assurance be effectively integrated into the AI certification process for critical domains such as avionics?
- Basis in paper: [explicit] The paper discusses the need for explainability and uncertainty assurance in critical domains like avionics, mentioning the importance of quantifying the uncertainty of explanations and explaining the sources of uncertainty to achieve trustworthy AI.
- Why unresolved: The paper acknowledges that deep neural networks are increasing in size and complexity, making it challenging to understand why new methods perform best and integrate explainability into the certification process.
- What evidence would resolve it: Development and validation of techniques that can effectively integrate explainability and uncertainty assurance into the AI certification process, with empirical evidence showing improved trustworthiness of AI systems in critical domains.

## Limitations

- The certification framework relies heavily on EASA guidelines that are still evolving, creating uncertainty about final regulatory requirements
- The YOLOv8 case study demonstrates gaps but uses a single model, which may not generalize to all computer vision applications in aviation
- Human factors section acknowledges significant research gaps in measuring human-AI collaboration effectiveness, particularly for error management and CRM integration
- Safety risk mitigation strategies are outlined conceptually but lack specific implementation details and quantitative risk assessment methods

## Confidence

**High Confidence**: The need for certification beyond performance metrics in safety-critical domains; the importance of operational design domain definition for dataset representativeness; the fundamental gap between classical AI development and certification requirements

**Medium Confidence**: The effectiveness of the proposed four-block certification roadmap structure; the specific prioritization of challenges across different AI application levels; the generalizability of YOLOv8 findings to other computer vision tasks

**Low Confidence**: The exact metrics and assessment methods for ethical compliance and human-AI collaboration; the quantitative impact of missing certification steps on actual safety outcomes; the implementation details for AI safety risk mitigation strategies

## Next Checks

1. **Cross-model validation**: Test the certification framework on multiple AI models beyond YOLOv8 (e.g., different object detection architectures, classification models) to assess generalizability across computer vision applications in aviation

2. **Human factors measurement development**: Design and pilot test specific metrics for evaluating human-AI teaming effectiveness, operational explainability, and error management in simulated aviation scenarios

3. **ODD specification validation**: Develop a systematic method for defining and validating operational design domains for different aviation AI applications, then test its effectiveness in identifying dataset coverage gaps and generalization limitations