---
ver: rpa2
title: 'COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8
  Training'
arxiv_id: '2410.19313'
source_url: https://arxiv.org/abs/2410.19313
tags:
- training
- quantization
- coat
- optimizer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of memory inefficiency in FP8
  training of large language models by proposing COAT, a framework that compresses
  both optimizer states and activations to FP8 precision. The key innovations include
  Dynamic Range Expansion to better align optimizer state distributions with FP8 representation
  range, reducing quantization error, and Mixed-Granularity Activation Quantization
  that applies per-tensor quantization to linear layers and fine-grained quantization
  to non-linear layers.
---

# COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training

## Quick Facts
- arXiv ID: 2410.19313
- Source URL: https://arxiv.org/abs/2410.19313
- Reference count: 40
- Primary result: Achieves 1.54x memory reduction and 1.43x training speedup compared to BF16 while maintaining nearly lossless performance

## Executive Summary
This paper addresses the challenge of memory inefficiency in FP8 training of large language models by proposing COAT, a framework that compresses both optimizer states and activations to FP8 precision. The key innovations include Dynamic Range Expansion to better align optimizer state distributions with FP8 representation range, reducing quantization error, and Mixed-Granularity Activation Quantization that applies per-tensor quantization to linear layers and fine-grained quantization to non-linear layers. COAT achieves 1.54x memory reduction and 1.43x training speedup compared to BF16, while maintaining nearly lossless performance across various tasks including LLM pretraining, fine-tuning, and VLM training.

## Method Summary
COAT introduces a comprehensive framework for memory-efficient FP8 training by addressing two major sources of memory consumption: optimizer states and activations. The Dynamic Range Expansion technique applies a power function to optimizer states within quantization groups to better utilize FP8's representation range, reducing quantization error. The Mixed-Granularity Activation Quantization approach applies per-tensor quantization to linear layers for efficiency and fine-grained quantization to non-linear layers for accuracy. The FP8 precision flow eliminates intermediate quantization overhead by quantizing activations before linear/non-linear layers and saving only quantized tensors. The framework integrates custom Triton kernels for FP8 matrix multiplication and fused CUDA kernels for optimizer state updates.

## Key Results
- Achieves 1.54x memory reduction compared to BF16 training
- Provides 1.43x training speedup over BF16 baselines
- Maintains nearly lossless performance across LLM pretraining, fine-tuning, and VLM training tasks
- Enables full-parameter training of large models on fewer GPUs and doubles batch sizes in distributed settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic Range Expansion aligns optimizer state distributions with FP8's representation range, reducing quantization error.
- Mechanism: For each quantization group, the method applies a power function f(x) = sign(x)|x|^k to expand the dynamic range so that min and max values fully utilize FP8's available range. The exponent k is calculated per group and per step as k = log(RX / RE4M3), where RX is the group's dynamic range and RE4M3 is FP8's ideal dynamic range.
- Core assumption: Optimizer states have low dynamic ranges within groups (max/min ratio far below FP8's capacity), and expanding these ranges preserves the statistical properties needed for training convergence.
- Evidence anchors: [abstract] "Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error"; [section 4.2] "For a quantization group X, after applying the expand function f to X, the dynamic range becomes Rf(X) = (RX)^k"

### Mechanism 2
- Claim: Mixed-Granularity Activation Quantization balances precision and efficiency by applying different quantization strategies to linear vs non-linear layers.
- Mechanism: Linear layers use per-tensor quantization for efficiency on TensorCores, while non-linear layers use per-group quantization for accuracy. The group scaling method reduces overhead by splitting max reduction into two stages: first computing per-block maxima, then reducing those intermediate results.
- Core assumption: Non-linear layers are more sensitive to quantization errors than linear layers, and TensorCores are optimized for per-tensor quantization patterns.
- Evidence anchors: [section 5.2] "We apply fine-grained quantization to non-linear layers and apply per-tensor quantization to linear layers"; [section 5.2] "We also propose Group Scaling, an efficient per-tensor scaling method"

### Mechanism 3
- Claim: FP8 precision flow eliminates intermediate quantization overhead by quantizing activations before linear/non-linear layers and saving only quantized tensors.
- Mechanism: Inputs and outputs of all linear and non-linear layers are maintained in FP8 format throughout the forward pass. Only the quantized tensors are saved for backward computation, eliminating the need for separate quantization operations during the forward pass.
- Core assumption: The quantization error introduced by early quantization is acceptable when combined with the appropriate granularity strategy for different layer types.
- Evidence anchors: [section 5.2] "FP8 precision flow requires the input and output of all linear and non-linear layers in FP8"; [section 5.2] "By directly saving the input tensor in FP8 format for the backward pass, we eliminate the need for an extra quantization operation"

## Foundational Learning

- **Concept**: FP8 quantization and representation formats (E4M3 vs E5M2)
  - Why needed here: Understanding the quantization process and format differences is essential for implementing both Dynamic Range Expansion and the mixed-granularity approach
  - Quick check question: What are the minimum and maximum representable values for E4M3 and E5M2 formats, and how do they differ in precision vs range trade-offs?

- **Concept**: Optimizer state quantization and AdamW update mechanics
  - Why needed here: The paper's approach specifically targets first-order (m) and second-order (v) momentum quantization, requiring understanding of how these terms contribute to weight updates
  - Quick check question: In the AdamW update rule, which terms are most sensitive to quantization error and why?

- **Concept**: Mixed-precision training and TensorCore optimization
  - Why needed here: The paper leverages FP8 for linear layers on TensorCores while maintaining higher precision for sensitive components, requiring knowledge of hardware-specific optimization
  - Quick check question: Why is per-tensor quantization preferred for linear layers when using TensorCores?

## Architecture Onboarding

- **Component map**: Optimizer state quantization module with Dynamic Range Expansion -> Mixed-granularity activation quantization controller -> FP8 precision flow manager for linear and non-linear layers -> Group scaling implementation for efficient per-tensor quantization -> Custom Triton kernels for FP8 matrix multiplication -> Fused CUDA kernels for optimizer state updates

- **Critical path**: 
  1. Forward pass: Quantize activations → apply linear/non-linear layers in FP8 → save quantized activations
  2. Backward pass: Load quantized activations → compute gradients in BF16 → update weights
  3. Optimizer step: Dequantize optimizer states → update in FP32 → re-quantize with Dynamic Range Expansion

- **Design tradeoffs**: 
  - Per-group vs per-tensor quantization: Accuracy vs efficiency
  - Early quantization vs late quantization: Memory savings vs potential error accumulation
  - Dynamic vs static exponent calculation: Precision vs computational overhead

- **Failure signatures**: 
  - Training divergence: Likely indicates quantization error accumulation
  - Memory usage higher than expected: May indicate quantization not being applied correctly
  - Speed slower than baseline: Could indicate inefficient kernel implementation or unnecessary precision conversions

- **First 3 experiments**:
  1. Verify Dynamic Range Expansion works by measuring MSE reduction on synthetic optimizer state distributions with known low dynamic ranges
  2. Test mixed-granularity approach on a single transformer layer by comparing accuracy degradation between per-tensor and per-group quantization on non-linear layers
  3. Validate FP8 precision flow by running a small model with and without activation quantization, measuring both memory savings and accuracy impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal group size for quantization in COAT that balances memory efficiency and accuracy across different model architectures?
- Basis in paper: [explicit] The paper mentions group sizes of 1×128 for optimizer states and 1×16 for non-linear layer activations, but also explores different group sizes in Table 10.
- Why unresolved: The paper only reports results for specific group sizes and doesn't systematically explore the trade-off across various architectures and tasks.
- What evidence would resolve it: A comprehensive study testing multiple group sizes (e.g., 4, 8, 16, 32, 64, 128) across diverse model architectures (CNNs, transformers, vision transformers) and tasks (pretraining, fine-tuning, generation) with ablation studies on memory usage, speed, and accuracy.

### Open Question 2
- Question: How does COAT's Dynamic Range Expansion technique perform when applied to other data types beyond FP8, such as INT8 or BF16?
- Basis in paper: [explicit] The paper mentions compatibility with DE8 (8-bit dynamic quantization) in Table 9, but doesn't explore other data types.
- Why unresolved: The paper only tests Dynamic Range Expansion on FP8 and DE8 formats, leaving its effectiveness on other data types unexplored.
- What evidence would resolve it: Experiments applying Dynamic Range Expansion to INT8, BF16, and other low-precision formats, comparing quantization error, memory usage, and training stability across these formats.

### Open Question 3
- Question: What is the impact of COAT on model convergence speed and final performance when training extremely large models (e.g., trillion-parameter models)?
- Basis in paper: [inferred] The paper demonstrates effectiveness on models up to 30B parameters, but doesn't address scaling to trillion-parameter scales.
- Why unresolved: The paper focuses on models up to 30B parameters and doesn't investigate how COAT scales to much larger models where memory constraints become more severe.
- What evidence would resolve it: Training experiments on trillion-parameter models or models with extremely long sequences, measuring convergence speed, final performance, and memory usage compared to BF16 baselines.

## Limitations

- **Dynamic Range Expansion Mechanism**: The claim that optimizer states have consistently low dynamic ranges within groups that benefit from power-law expansion remains largely unverified. While the paper shows improved MSE metrics, there's limited ablation showing what happens when this assumption is violated or when simpler scaling approaches are used.

- **Mixed-Granularity Justification**: The paper asserts non-linear layers are more sensitive to quantization errors without providing quantitative sensitivity analysis comparing different layer types. The choice of per-tensor for linear layers assumes TensorCore optimization without benchmarking against other approaches.

- **FP8 Precision Flow**: The approach of quantizing activations early in the forward pass and saving only quantized tensors is presented as beneficial, but the paper doesn't adequately address potential error accumulation through multiple layers or provide error propagation analysis.

## Confidence

- **High Confidence**: Memory reduction claims (1.54x) and speedup metrics (1.43x) - these are measurable hardware-dependent results with clear baselines
- **Medium Confidence**: Training accuracy preservation - while results show minimal degradation, the paper lacks extensive ablation studies across diverse model architectures and tasks
- **Low Confidence**: The specific mechanism claims (Dynamic Range Expansion, mixed-granularity benefits) - these are supported by internal metrics but lack comparison with simpler alternatives or comprehensive sensitivity analysis

## Next Checks

1. **Dynamic Range Sensitivity Test**: Run ablation studies on synthetic optimizer state distributions with varying dynamic range characteristics to quantify when Dynamic Range Expansion provides benefits versus when it introduces distortion.

2. **Layer Sensitivity Analysis**: Systematically vary the quantization granularity across all layer types (not just linear vs non-linear) and measure accuracy degradation to validate the claim that non-linear layers are uniquely sensitive to quantization errors.

3. **Error Propagation Study**: Implement a forward-only pass through multiple quantized layers with synthetic activations to measure how quantization errors accumulate and compare this against the claimed benefits of the FP8 precision flow approach.