---
ver: rpa2
title: Multi-Grained Preference Enhanced Transformer for Multi-Behavior Sequential
  Recommendation
arxiv_id: '2411.12179'
source_url: https://arxiv.org/abs/2411.12179
tags:
- sequential
- multi-behavior
- recommendation
- preference
- m-gpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes M-GPT, a multi-grained preference enhanced
  transformer framework for multi-behavior sequential recommendation. The model addresses
  two key challenges: modeling interaction-level multi-behavior dependencies and capturing
  dynamic behavior-aware multi-grained preference.'
---

# Multi-Grained Preference Enhanced Transformer for Multi-Behavior Sequential Recommendation

## Quick Facts
- arXiv ID: 2411.12179
- Source URL: https://arxiv.org/abs/2411.12179
- Authors: Chuan He; Yongchao Liu; Qiang Li; Weiqiang Wang; Xin Fu; Xinyi Fu; Chuntao Hong; Xinwei Yao
- Reference count: 40
- Key outcome: M-GPT achieves up to 12.2% improvement in HR@5 and 11.1% in NDCG@5 compared to best baselines

## Executive Summary
This paper proposes M-GPT, a novel transformer-based framework for multi-behavior sequential recommendation that addresses two key challenges: modeling interaction-level multi-behavior dependencies and capturing dynamic behavior-aware multi-grained preference. The model introduces an interaction-level dependency extractor that constructs graphs considering both item-level and behavior-level dependencies, then performs graph convolution to learn complex correlations. Additionally, a multifaceted sequential pattern generator employs a multi-scale transformer architecture with multi-grained self-attention to encode temporal behavior-aware preference at different time scales. Experiments on three real-world datasets demonstrate that M-GPT consistently outperforms state-of-the-art methods.

## Method Summary
M-GPT is a multi-grained preference enhanced transformer framework that processes user interaction sequences containing both item IDs and behavior types. The method consists of two core components: an Interaction-Level Dependency Extractor (IDE) that constructs interaction-level graphs and applies graph convolution to capture multi-behavior dependencies, and a Multifaceted Sequential Pattern Generator (MSPG) that uses linear self-attention and multi-grained multi-head self-attention with session division to encode temporal patterns. The model is trained using a Cloze task with masking and evaluated using leave-one-out strategy with standard recommendation metrics including HR@5, NDCG@5, HR@10, NDCG@10, and MRR.

## Key Results
- M-GPT achieves improvements of up to 12.2% in HR@5 and 11.1% in NDCG@5 compared to best baselines
- Consistently outperforms state-of-the-art methods across three real-world datasets (Taobao, IJCAI, Retailrocket)
- Demonstrates effectiveness in capturing both interaction-level dependencies and multi-grained temporal preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The interaction-level dependency extractor captures complex multi-behavior dependencies by modeling both item-level and behavior-level semantics simultaneously through a specially constructed graph.
- Mechanism: The model constructs an interaction-level graph where each node represents a user-item interaction with both item-specific and behavior-specific embeddings. The incidence matrix A is computed as the product of item-level dependency representation (ğ¸ğ‘–,ğ‘— = ğ‘’ğ‘– âŠ™ ğ‘’ğ‘—) and behavior-level dependency representation (ğµğ‘–,ğ‘— = ğ‘ğ‘– âŠ™ ğ‘ğ‘–). This integrated approach allows the model to capture dependencies between interactions that involve different items and different behaviors simultaneously.
- Core assumption: Item-level and behavior-level dependencies can be effectively combined through inner product to capture interaction-level dependencies that reflect real-world user behavior patterns.
- Evidence anchors:
  - [abstract] "First, M-GPT constructs an interaction-level graph of historical cross-typed interactions in a sequence. Then graph convolution is performed to derive interaction-level multi-behavior dependency representation repeatedly"
  - [section 3.3.2] "To learn interaction-level multi-behavior dependency, we introduce the incidence matrix A âˆˆ R|ğ‘ ğ‘¢| Ã— |ğ‘ ğ‘¢| taking both item-level and behavior-level semantics into consideration"
  - [corpus] Weak - while related papers discuss behavior-level and item-level modeling, they don't specifically address the simultaneous modeling of both through graph construction as described here
- Break condition: If the inner product of item-level and behavior-level representations fails to capture meaningful interaction-level dependencies, or if the graph convolution over these interactions doesn't improve recommendation performance.

### Mechanism 2
- Claim: The multifaceted sequential pattern generator captures dynamic behavior-aware multi-grained preference by encoding temporal patterns at different time scales using multi-grained self-attention.
- Mechanism: The model divides interaction sequences into sessions at different time scales (ğ‘¡1 and ğ‘¡2), then applies multi-grained multi-head self-attention to capture user preference within each session. By grouping the last items with different lengths in a session and using them as queries with varying granularity, the model captures both coarse-grained and fine-grained preference patterns that reflect how user interests evolve over time.
- Core assumption: User preferences exhibit multi-scale temporal patterns that can be effectively captured by dividing sequences into sessions and applying attention mechanisms with different granularities.
- Evidence anchors:
  - [abstract] "a novel multi-scale transformer architecture equipped with multi-grained user preference extraction is proposed to encode the interaction-aware sequential pattern enhanced by capturing temporal behavior-aware multi-grained preference"
  - [section 3.4.3] "The multifaceted sequential pattern generator aims to capture the sequential interaction pattern enhanced by extracting behavior-aware multi-grained preference in different time scales"
  - [corpus] Moderate - several related papers discuss multi-scale or multi-granularity approaches, but the specific combination of session division with multi-grained attention for preference extraction appears novel
- Break condition: If the multi-grained attention mechanism fails to capture meaningful preference patterns at different time scales, or if the session division strategy doesn't align with actual user behavior patterns.

### Mechanism 3
- Claim: The linear attention mechanism with positional embeddings efficiently encodes global sequential patterns while maintaining computational feasibility for long sequences.
- Mechanism: Instead of standard dot-product attention, the model uses linear attention by computing ğ‘²Tğ‘½ first, then applying row-wise and column-wise L2 normalization to the query and key matrices respectively. This reduces the computational complexity from O(NÂ²d) to O(NdÂ²) while preserving the ability to capture long-range dependencies in the interaction sequence.
- Core assumption: Linear attention with proper normalization can approximate the behavior of standard attention while significantly reducing computational costs, making it suitable for processing long interaction sequences.
- Evidence anchors:
  - [section 3.4.2] "To alleviate the high computational and memory cost of dot-product for long term sequence, we utilize a linear self-attention layer to encode the global sequential pattern"
  - [section 3.4.2] "we first calculate ğ‘²Tğ‘½ rather than ğ‘„ğ¾T, which reduce the model complexity from O(NÂ²d) to O(NdÂ²)"
  - [corpus] Moderate - linear attention mechanisms are discussed in related literature, but the specific implementation with ELU activation and dual normalization appears to be a novel contribution
- Break condition: If the linear attention approximation significantly degrades the quality of sequential pattern encoding compared to standard attention, or if the computational savings don't justify any potential performance trade-offs.

## Foundational Learning

- Graph Neural Networks
  - Why needed here: The interaction-level dependency extractor relies on graph convolution to propagate information across the interaction-level graph, capturing complex dependencies between different types of user behaviors across items.
  - Quick check question: How does graph convolution differ from standard convolution, and why is it particularly suited for modeling relationships in user-item interaction graphs?

- Attention Mechanisms
  - Why needed here: Both the linear self-attention for global sequential patterns and the multi-grained multi-head self-attention for local preference patterns are core components that enable the model to focus on relevant parts of the interaction sequence.
  - Quick check question: What is the key difference between standard multi-head attention and the multi-grained multi-head attention proposed here, and how does this difference help capture user preference patterns?

- Transformer Architecture
  - Why needed here: The multifaceted sequential pattern generator is built on transformer principles, using self-attention layers, positional embeddings, and feed-forward networks to encode sequential information effectively.
  - Quick check question: How do positional embeddings contribute to the transformer's ability to understand sequential order, and why are they particularly important in sequential recommendation scenarios?

## Architecture Onboarding

- Component map:
  - Input Layer: Item embeddings (ğ‘’ğ‘–) and behavior embeddings (ğ‘ğ‘–) for each interaction
  - Interaction-Level Dependency Extractor: Constructs interaction-level graph, computes incidence matrix, applies graph convolution for multi-order dependency learning
  - Multifaceted Sequential Pattern Generator:
    - Sequential Information Injection: Adds positional embeddings
    - Global Sequential Pattern Encoding: Applies linear self-attention
    - Temporal Multi-Grained Preference Encoding: Divides into sessions, applies multi-grained self-attention
    - Multifaceted Pattern Fusion: Combines global and local patterns
    - Multifaceted Transformer Layer: Applies non-linearity and residual connections
  - Prediction Layer: Uses max pooling across dependency orders to generate final predictions

- Critical path: Item/behavior embeddings â†’ Interaction-level graph construction â†’ Graph convolution (multi-order) â†’ Positional embedding injection â†’ Linear self-attention â†’ Multi-grained self-attention â†’ Pattern fusion â†’ Prediction

- Design tradeoffs:
  - Graph construction complexity vs. dependency modeling capability: More complex graph structures could capture richer dependencies but increase computational cost
  - Number of dependency orders: More orders capture more complex patterns but increase model complexity and training time
  - Session granularity: Finer granularity captures more detailed preference patterns but may overfit to noise

- Failure signatures:
  - Poor performance on datasets with very short sequences (graph construction becomes less meaningful)
  - Overfitting on datasets with high interaction frequency (too many sessions capture noise rather than signal)
  - Computational inefficiency on very long sequences despite linear attention (graph convolution still scales with sequence length)

- First 3 experiments:
  1. Ablation study: Remove the interaction-level dependency extractor and compare performance to baseline to validate its contribution
  2. Hyperparameter sensitivity: Test different values for the number of dependency orders (ğ‘™) to find optimal complexity
  3. Scalability test: Measure training and inference time on sequences of varying lengths to validate the computational efficiency claims of linear attention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of M-GPT vary with different numbers of dependency orders beyond the tested range of 1-4?
- Basis in paper: [explicit] The paper states "We search the number of interaction-level multi-behavior dependency from 1,2,3,4" but does not explore higher orders.
- Why unresolved: The experimental evaluation only tested up to 4th order dependencies, leaving uncertainty about whether higher orders would further improve performance or potentially overfit.
- What evidence would resolve it: Comprehensive experiments testing dependency orders 5-8 or using adaptive order selection methods would clarify the optimal range for different datasets and scenarios.

### Open Question 2
- Question: How robust is M-GPT's performance when the sequence length varies significantly beyond the fixed maximum length of 200 used in experiments?
- Basis in paper: [explicit] The paper states "we set the max sequence length ğ‘ to 200 for all of the models" without exploring how performance scales with longer or shorter sequences.
- Why unresolved: The experiments use a fixed sequence length, but real-world datasets may have much longer or shorter interaction sequences, which could affect the model's ability to capture meaningful patterns.
- What evidence would resolve it: Experiments varying the maximum sequence length from 50 to 1000, along with analysis of how different sequence lengths impact performance metrics and computational efficiency.

### Open Question 3
- Question: How does M-GPT handle cold-start scenarios where users have very few interactions or new items have no interaction history?
- Basis in paper: [inferred] While the paper mentions "cold-start issues" in the introduction, it does not explicitly test or discuss M-GPT's performance in cold-start scenarios.
- Why unresolved: The experiments use existing user-item interaction data without addressing how the model performs when interactions are sparse or entirely absent for new users/items.
- What evidence would resolve it: Experiments specifically designed to evaluate M-GPT on datasets with varying levels of sparsity, including synthetic cold-start scenarios and ablation studies on components that might be particularly helpful for sparse data.

## Limitations
- Limited ablation studies on individual components to quantify their specific contributions to overall performance
- Fixed sequence length of 200 used across all experiments without exploring how performance scales with different sequence lengths
- No explicit evaluation of cold-start scenarios where users have very few interactions or new items lack interaction history

## Confidence
- High confidence in the overall framework effectiveness and the improvement claims (up to 12.2% HR@5, 11.1% NDCG@5)
- Medium confidence in the specific mechanisms of interaction-level dependency extraction and multi-grained preference encoding due to limited component-level analysis
- Medium confidence in the computational efficiency claims, as the linear attention benefits are stated but not thoroughly benchmarked

## Next Checks
1. Perform detailed ablation studies removing each major component (IDE, MSPG, linear attention) to quantify their individual contributions to overall performance
2. Conduct hyperparameter sensitivity analysis for critical parameters including dependency order, session granularity, and preference extraction scales
3. Benchmark computational efficiency by measuring training and inference times across varying sequence lengths and comparing against alternative attention mechanisms