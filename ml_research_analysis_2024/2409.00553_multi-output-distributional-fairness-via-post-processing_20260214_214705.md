---
ver: rpa2
title: Multi-Output Distributional Fairness via Post-Processing
arxiv_id: '2409.00553'
source_url: https://arxiv.org/abs/2409.00553
tags:
- learning
- fairness
- classification
- post-processing
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of ensuring fairness in multi-output
  machine learning models, which is challenging due to the complex joint distribution
  of outputs across different groups. The authors propose a post-processing method
  that leverages optimal transport mappings to move model outputs across different
  groups towards their empirical Wasserstein barycenter, achieving distributional
  parity.
---

# Multi-Output Distributional Fairness via Post-Processing

## Quick Facts
- arXiv ID: 2409.00553
- Source URL: https://arxiv.org/abs/2409.00553
- Authors: Gang Li; Qihang Lin; Ayush Ghosh; Tianbao Yang
- Reference count: 40
- This paper addresses ensuring fairness in multi-output ML models via post-processing using optimal transport mappings to Wasserstein barycenters.

## Executive Summary
This paper tackles the challenging problem of fairness in multi-output machine learning models where outputs have complex joint distributions across different groups. The authors propose a novel post-processing approach that leverages optimal transport theory to achieve distributional parity. By moving model outputs across groups toward their empirical Wasserstein barycenter, the method effectively reduces disparities while maintaining predictive accuracy.

The key innovation lies in approximating the Wasserstein barycenter and using kernel regression to extend the fairness adjustment to out-of-sample data, significantly reducing computational complexity compared to exact methods. Experimental results on multi-label/multi-class classification and representation learning tasks demonstrate that this approach achieves better fairness-accuracy trade-offs compared to existing baselines.

## Method Summary
The authors propose a post-processing method for achieving distributional fairness in multi-output ML models. The core idea is to use optimal transport mappings to move model outputs from different groups toward their empirical Wasserstein barycenter, thereby achieving distributional parity. To address computational challenges, they approximate the barycenter and employ kernel regression to extend the fairness adjustment to new, unseen data points. This approach is designed to work with various multi-output scenarios including multi-label classification and representation learning tasks.

## Key Results
- Proposed method achieves better fairness-accuracy trade-offs compared to existing baselines
- Effective performance demonstrated on multi-label/multi-class classification tasks
- Successful application to representation learning tasks
- Computational efficiency improved through barycenter approximation and kernel regression

## Why This Works (Mechanism)
The method works by leveraging optimal transport theory to redistribute model outputs across different demographic groups toward a common distribution. By calculating the Wasserstein barycenter of output distributions from different groups, the approach identifies a target distribution that represents a fair compromise. The optimal transport mapping then transforms outputs from each group to align with this barycenter, reducing distributional disparities while preserving the essential structure of the predictions.

## Foundational Learning

**Optimal Transport Theory**
- Why needed: Provides mathematical framework for measuring and transforming distributions
- Quick check: Understand Wasserstein distance as earth mover's distance between distributions

**Wasserstein Barycenter**
- Why needed: Represents a central/fair distribution that minimizes transport cost to all group distributions
- Quick check: Verify barycenter calculation minimizes sum of Wasserstein distances

**Kernel Regression**
- Why needed: Enables extension of fairness adjustment to out-of-sample data
- Quick check: Confirm kernel choice and bandwidth affect approximation quality

## Architecture Onboarding

Component Map: Model Outputs -> Optimal Transport Mapping -> Wasserstein Barycenter -> Kernel Regression -> Fair Outputs

Critical Path: The fairness transformation flows from original model outputs through optimal transport to the barycenter approximation, then to out-of-sample fairness via kernel regression.

Design Tradeoffs:
- Exact vs. approximate barycenter computation (accuracy vs. efficiency)
- Choice of kernel function and bandwidth (approximation quality vs. generalization)
- Post-processing vs. in-processing fairness methods (flexibility vs. training-time constraints)

Failure Signatures:
- Poor fairness improvement indicates suboptimal barycenter approximation
- Degradation in accuracy suggests excessive transport or inappropriate kernel choice
- Computational bottlenecks likely in high-dimensional output spaces

First Experiments:
1. Verify Wasserstein distance calculations between synthetic distributions
2. Test barycenter approximation accuracy against exact computation
3. Validate kernel regression performance on out-of-sample data

## Open Questions the Paper Calls Out

None

## Limitations

- Computational complexity of approximating Wasserstein barycenters in high-dimensional multi-output spaces
- Limited empirical validation to specific multi-label/multi-class classification and representation learning tasks
- Focus on post-processing approaches may not suit scenarios requiring fairness constraints during training

## Confidence

- Theoretical framework and methodology: High confidence
- Computational efficiency of the approximation method: Medium confidence
- Experimental results and fairness-accuracy trade-offs: Medium confidence

## Next Checks

1. Conduct scalability tests on high-dimensional multi-output spaces to verify computational efficiency claims
2. Evaluate the method across diverse real-world datasets beyond the presented classification tasks
3. Perform ablation studies to quantify the contribution of each component (optimal transport, barycenter approximation, kernel regression) to the overall performance