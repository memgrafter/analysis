---
ver: rpa2
title: Correcting Negative Bias in Large Language Models through Negative Attention
  Score Alignment
arxiv_id: '2408.00137'
source_url: https://arxiv.org/abs/2408.00137
tags:
- negative
- attention
- nasa
- answer
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work identifies a negative bias in large language models\
  \ (LLMs) for binary decision tasks requiring complex reasoning, where models exhibit\
  \ low recall despite high precision, suggesting overly cautious behavior towards\
  \ positive responses. To address this, the authors propose a negative attention\
  \ score (NAS) that quantifies the model\u2019s bias by measuring attention allocated\
  \ to negative answer candidates (\u201CNo\u201D) versus positive ones (\u201CYes\u201D\
  )."
---

# Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment

## Quick Facts
- arXiv ID: 2408.00137
- Source URL: https://arxiv.org/abs/2408.00137
- Authors: Sangwon Yu; Jongyoon Song; Bongkyu Hwang; Hoyoung Kang; Sooah Cho; Junhwa Choi; Seongho Joe; Taehee Lee; Youngjune L. Gwon; Sungroh Yoon
- Reference count: 28
- Large language models show negative bias in binary decision tasks, exhibiting low recall despite high precision

## Executive Summary
This paper addresses a critical negative bias in large language models when performing binary decision tasks requiring complex reasoning. The authors observe that models exhibit low recall despite high precision, indicating overly cautious behavior toward positive responses. To quantify and correct this bias, they introduce a Negative Attention Score (NAS) metric that measures attention allocated to negative versus positive answer candidates. Using this metric, they develop a parameter-efficient fine-tuning method called Negative Attention Score Alignment (NASA) that iteratively tunes identified negative attention heads while preventing overcorrection.

## Method Summary
The authors propose a framework to detect and mitigate negative bias in LLMs for binary decision tasks. They first introduce NAS, a metric that quantifies negative bias by measuring the attention difference between negative and positive answer tokens. NAS is calculated using attention weights from all heads, with higher values indicating stronger negative bias. The method identifies negative attention heads that consistently allocate more attention to "No" than "Yes" regardless of prompt content. NASA then performs parameter-efficient fine-tuning by sequentially updating only the query and key projection weights of these identified heads, using early stopping and update cancellation to prevent overcorrection. The approach is tested across four LLM families (LLaMA3, Mistral, Gemma, Qwen2) and multiple reasoning datasets.

## Key Results
- NASA significantly reduces the precision-recall gap in binary decision tasks across all tested LLMs
- The method improves F1 scores while maintaining or improving general reasoning performance on short-answer QA datasets
- NAS values are effectively reduced to appropriate levels, and model calibration is improved through better alignment between prediction confidence and accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative attention heads exist that preferentially attend to negative answer tokens ("No") over positive ones ("Yes"), causing models to output negative responses too frequently in binary decision tasks.
- Mechanism: During the binary decision-making process, models attend to answer candidate tokens provided in instructions. If attention weights for "No" consistently exceed those for "Yes", the model is biased toward negative responses. The proposed NAS metric quantifies this bias by summing attention weights for both tokens weighted by their log ratio, identifying heads that systematically favor "No".
- Core assumption: Attention weights directly influence output token selection in binary decision tasks, and these weights can be systematically biased toward negative tokens regardless of prompt content.
- Evidence anchors:
  - [abstract]: "Based on our observations and the rationale about attention-based model dynamics, we propose a negative attention score (NAS) to systematically and quantitatively formulate negative bias."
  - [section 3.2.1]: "The negative bias of the LLM can be seen as originating from assigning greater attention weight to negative answer candidate tokens during the reasoning process."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.468. Top related titles include "A Multifaceted Analysis of Negative Bias in Large Language Models" and "Debiasing LLMs by Masking Unfairness-Driving Attention Heads" - suggests this is an active research area but specific NAS mechanism not previously established.

### Mechanism 2
- Claim: Parameter-efficient fine-tuning of identified negative attention heads can reduce bias while preserving general reasoning ability.
- Mechanism: NASA method performs head-wise incremental tuning, updating only query and key projection weights of the most biased attention heads. Early stopping and update cancellation prevent overcorrection and positive bias induction. This targeted approach reduces NAS values and improves precision-recall balance.
- Core assumption: Fine-tuning specific attention heads can correct bias without degrading overall model performance, and the bias is localized to these heads rather than being distributed throughout the model.
- Evidence anchors:
  - [abstract]: "A parameter-efficient fine-tuning method, Negative Attention Score Alignment (NASA), is then introduced to iteratively tune these identified heads while monitoring NAS to prevent overcorrection."
  - [section 4.3.2]: "We propose to further fine-tune each attention head sequentially in order of decreasing NAS... We only train the query and key projection modules of the target attention head."
  - [section 5.2]: "Experimental results demonstrate that our method significantly reduces negative bias while maintaining general reasoning performance."
  - [corpus]: Related work includes "Debiasing LLMs by Masking Unfairness-Driving Attention Heads" - suggests attention head modification is a viable debiasing approach.

### Mechanism 3
- Claim: NAS provides an effective quantitative measure of negative bias that correlates with model output behavior.
- Mechanism: NAS combines attention weights for both answer tokens with their log ratio, creating a metric that captures both the magnitude of attention and the preference direction. High NAS values indicate strong negative bias, and reducing NAS through NASA correlates with improved model calibration and performance.
- Core assumption: The defined NAS metric accurately captures negative bias and correlates with observable model behavior across different datasets and model types.
- Evidence anchors:
  - [section 3.2.2]: "We measure the correlation between NAS and the negative confidence observed in model responses... In all cases, we confirm that NAS indeed has a positive correlation with negative confidence."
  - [section 5.2]: "Regarding the NAS metric, we observe that our method effectively reduces NAS to an appropriate level."
  - [section 6.4]: "NASA consistently demonstrates improved calibration... better alignment between prediction confidence and accuracy."

## Foundational Learning

- Concept: Attention mechanisms in transformer models
  - Why needed here: Understanding how attention weights influence output token selection is crucial for grasping why negative attention heads cause biased responses and how NASA addresses this.
  - Quick check question: How do attention weights in transformer models typically influence which tokens are output in the final prediction?

- Concept: Binary decision tasks and evaluation metrics
  - Why needed here: The work focuses on yes-no question answering, requiring understanding of precision, recall, F1 scores, and their relationships to model bias.
  - Quick check question: In a binary classification task, if a model has high precision but low recall, what does this indicate about its decision-making behavior?

- Concept: Parameter-efficient fine-tuning techniques
  - Why needed here: NASA uses selective fine-tuning of specific attention head components, requiring understanding of how targeted parameter updates can modify model behavior.
  - Quick check question: What are the advantages of updating only query and key projection weights versus full attention head fine-tuning?

## Architecture Onboarding

- Component map: Input prompt -> attention weight extraction -> NAS calculation -> negative head identification -> head-wise fine-tuning with early stopping -> evaluation of bias reduction and performance preservation
- Critical path: Input prompt → attention weight extraction → NAS calculation → negative head identification → head-wise fine-tuning with early stopping → evaluation of bias reduction and performance preservation
- Design tradeoffs: Fine-tuning only query and key weights (parameter-efficient but may be less effective) vs. full attention head updates (more parameters but potentially better bias correction); sequential vs. parallel head tuning (better control but slower); fixed instruction format vs. generalized instructions (easier implementation but less robust)
- Failure signatures: If NAS doesn't decrease during fine-tuning, if precision-recall gap doesn't improve, if general reasoning performance degrades, or if model calibration worsens after NASA application
- First 3 experiments:
  1. Calculate NAS values across all attention heads on a small dataset subset to verify negative heads exist and correlate with negative bias
  2. Implement NASA on a single attention head and measure changes in NAS, precision, and recall to validate the fine-tuning approach
  3. Test NASA with random attention head selection vs. identified negative heads to demonstrate the importance of targeted head selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact mechanisms and causes of negative bias in large language models for binary decision tasks?
- Basis in paper: [explicit] The paper identifies negative bias but acknowledges that further research is needed to understand its causes and mechanisms.
- Why unresolved: While the paper proposes a framework to detect and mitigate negative bias, it does not delve into the root causes of why LLMs exhibit this bias in the first place.
- What evidence would resolve it: Detailed analysis of model parameters and attention mechanisms during training and inference, along with comparisons across different model architectures and training data.

### Open Question 2
- Question: How does negative bias in LLMs affect their performance on other types of reasoning tasks beyond binary decision tasks?
- Basis in paper: [inferred] The paper focuses on binary decision tasks and mentions that negative bias is a task-specific factor, but it does not explore its impact on other reasoning tasks.
- Why unresolved: The study is limited to binary decision tasks, leaving the generalizability of the findings to other reasoning tasks unexplored.
- What evidence would resolve it: Experiments evaluating LLM performance on various reasoning tasks with and without negative bias mitigation techniques.

### Open Question 3
- Question: Can the Negative Attention Score Alignment (NASA) method be extended to address other types of biases in LLMs beyond negative bias?
- Basis in paper: [inferred] The paper introduces NASA as a method to address negative bias, but does not discuss its potential application to other biases.
- Why unresolved: The study focuses solely on negative bias, leaving the broader applicability of the NASA framework unexplored.
- What evidence would resolve it: Adaptation of the NASA method to detect and mitigate other types of biases, followed by experiments to evaluate its effectiveness.

## Limitations

- The NAS metric may not capture all forms of negative bias, particularly those arising from prompt formatting or instruction tuning effects rather than attention patterns
- The claim that negative bias is "query-agnostic" relies on probing set analysis but doesn't fully explore whether bias varies with different binary task formulations
- While NASA improves precision-recall balance, the paper doesn't thoroughly investigate whether this comes at the cost of calibration degradation in non-binary tasks

## Confidence

- **High confidence**: The existence of negative bias in binary decision tasks (evidenced by consistent precision-recall gaps across multiple models and datasets); the effectiveness of NASA in reducing NAS values and improving F1 scores (supported by experimental results across four different LLMs)
- **Medium confidence**: The NAS metric's validity as a quantitative measure of negative bias (supported by correlation analysis but limited cross-validation); the claim that attention heads are the primary source of negative bias (plausible given the mechanism but not definitively proven)
- **Low confidence**: The assertion that NASA maintains general reasoning performance across all task types (limited to short-answer QA datasets; longer-form reasoning tasks not thoroughly tested); the claim that NASA doesn't induce positive bias (early stopping helps but threshold determination is unclear)

## Next Checks

1. **Probe NAS sensitivity to prompt format variations**: Test whether NAS values change significantly when the binary decision instructions are rephrased or when the answer tokens ("Yes"/"No") are presented in different positions or formats, to validate the claim that negative bias is attention-head driven rather than instruction-format dependent.

2. **Evaluate NASA on longer-form reasoning tasks**: Apply NASA to models and test on datasets requiring multi-step reasoning or open-ended answers (not just binary decisions) to verify the claim of maintained general reasoning performance beyond short-answer QA.

3. **Test NAS correlation with human judgment**: Compare NAS values with human evaluations of model response bias on the same binary decision tasks to validate whether the metric accurately reflects perceived negative bias and not just a mathematical artifact of attention patterns.