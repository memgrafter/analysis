---
ver: rpa2
title: 'Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders'
arxiv_id: '2407.06851'
source_url: https://arxiv.org/abs/2407.06851
tags:
- prompts
- sentence
- unsafe
- knowledge
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of sentence encoders to distinguish
  safe from unsafe prompts and classify unsafe prompts according to a safety taxonomy.
  It introduces new pairwise datasets (Safety-Challenging and Safety-Contrast) and
  the Categorical Purity (CP) metric to measure this capability.
---

# Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders
## Quick Facts
- arXiv ID: 2407.06851
- Source URL: https://arxiv.org/abs/2407.06851
- Reference count: 15
- Primary result: Sentence encoders effectively distinguish safe from unsafe prompts for stereotypes and privacy, but struggle with context understanding; LLM2vec-Mistral shows strongest safety knowledge.

## Executive Summary
This paper investigates whether sentence encoders can distinguish safe from unsafe prompts and classify unsafe prompts according to a safety taxonomy. The authors introduce new pairwise datasets (Safety-Challenging and Safety-Contrast) and propose the Categorical Purity (CP) metric to measure this capability. Through extensive experiments with multiple sentence encoder models, they find that while models perform well on stereotype and privacy-related categories, they struggle with understanding various contexts. The LLM-based encoder (LLM2vec-Mistral) demonstrates the most robust Safety-Challenging knowledge, and CP is shown to be an effective metric for measuring Safety-Taxonomy knowledge.

## Method Summary
The study employs existing sentence encoder models including SBERT variants, SimCSE, ST5 family, LLM2vec, and OpenAI's text-embedding-3-large. The authors create new pairwise datasets to evaluate Safety-Challenging knowledge (distinguishing safe/unsafe prompts) and Safety-Taxonomy knowledge (categorizing unsafe prompts). They measure Safety-Challenging knowledge using normalized cosine similarity between safe and unsafe prompts, and introduce the Categorical Purity (CP) metric to assess how well sentence encoders recognize common characteristics of unsafe prompts. The evaluation pipeline involves generating or loading prompt pairs, encoding them with baseline models, computing normalized cosine similarities, calculating CP scores, and visualizing results with t-SNE.

## Key Results
- Sentence encoders effectively handle stereotypes and privacy-related topics but struggle with understanding various contexts
- LLM2vec-Mistral demonstrates the most robust Safety-Challenging knowledge among all tested models
- CP is shown to be a reasonable metric for measuring Safety-Taxonomy knowledge, with higher CP scores indicating better categorization of unsafe prompts

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Sentence embeddings that capture semantic nuance can separate safe from unsafe prompts even when they are lexically similar.
- Mechanism: Contrastive learning trains the embedding space so that prompts with different safety implications are pushed apart, while prompts sharing safety features are pulled together.
- Core assumption: The safety distinction is primarily semantic and can be learned from contrastive pairs without explicit labeling of all unsafe categories.
- Evidence anchors:
  - [abstract] "similarity search-based techniques that identify specific features of unsafe prompts provide a more robust and effective solution"
  - [section] "The greater the distance between embeddings of an unsafe prompt and a similar but safe prompt, the higher Safety-Challenging knowledge it has"
  - [corpus] Weak - related papers focus on gradient analysis or RL-based safety filters, not directly on semantic embedding separation
- Break condition: If unsafe prompts require knowledge beyond semantics (e.g., cultural context, real-world plausibility) the embedding separation fails.

### Mechanism 2
- Claim: Categorical Purity (CP) effectively measures how well embeddings group prompts by shared safety characteristics.
- Mechanism: CP uses k-nearest neighbor search in embedding space to count how many neighbors share the same safety category; higher CP means better clustering by safety.
- Core assumption: The embedding space is isotropic enough that cosine similarity meaningfully reflects category membership without heavy normalization artifacts.
- Evidence anchors:
  - [section] "we propose a new perspective on purity, Categorical Purity (CP) from the standpoint of categories by using the similarity search methodology"
  - [section] "Categories with high CP, such as Privacy Leak (Personal) and Stereotype, show a clear tendency to group together"
  - [corpus] Weak - no direct corpus support; related works use different clustering or detection metrics
- Break condition: If the embedding space is highly anisotropic or safety categories are too broad/overlapping, CP scores become unreliable.

### Mechanism 3
- Claim: Larger encoder models and LLM-based encoders capture more Safety-Critical knowledge than smaller or purely supervised models.
- Mechanism: Model capacity and unsupervised pre-training allow richer semantic representations that encode subtle safety distinctions; supervised fine-tuning on NLI data alone does not add this knowledge.
- Core assumption: Safety knowledge is a natural byproduct of rich semantic representation learning, not something that requires targeted safety supervision.
- Evidence anchors:
  - [section] "LLM2vec-Mistral records the lowest safety-boundary similarity compared to all other sentence encoders"
  - [section] "the larger the model, the higher the CP score, indicating that a larger model possesses more Safety-Taxonomy knowledge"
  - [corpus] Weak - related papers discuss guardrails or safety fine-tuning, not size vs. unsupervised benefits
- Break condition: If safety distinctions require explicit adversarial training or specialized safety datasets, model size alone will not suffice.

## Foundational Learning
- Concept: Cosine similarity and embedding space geometry
  - Why needed here: The whole evaluation relies on comparing distances between prompt embeddings to judge safety distinction
  - Quick check question: If two prompts have cosine similarity 0.9, are they necessarily semantically similar in safety context?
- Concept: Contrastive learning objectives
  - Why needed here: The paper builds new datasets to test whether embeddings can be trained to separate safe/unsafe pairs
  - Quick check question: In a Siamese network, what loss encourages the embedding of dissimilar pairs to be far apart?
- Concept: Clustering purity metrics
  - Why needed here: CP is a variant of cluster purity adapted to safety category evaluation
  - Quick check question: How does CP differ from traditional cluster purity when evaluating imbalanced safety categories?

## Architecture Onboarding
- Component map: Dataset builders (XSTest, Safety-Contrast, Do-Not-Answer) -> Embedding models (SBERT variants, SimCSE, ST5 family, LLM2vec, OpenAI text-embedding-3-large) -> Evaluation pipeline (normalization, CP calculation, t-SNE visualization)
- Critical path: 1. Generate or load prompt pairs 2. Encode prompts with baseline model 3. Compute normalized cosine similarities 4. Calculate CP or safety-boundary metrics 5. Visualize with t-SNE for sanity check
- Design tradeoffs:
  - Normalization vs. raw similarity: Normalization removes anisotropy bias but may obscure absolute differences
  - k in CP: Larger k smooths noise but may dilute category boundaries
  - Model size vs. speed: Larger models give better CP but slower inference
- Failure signatures:
  - High CP but poor qualitative separation in t-SNE → metric overfits to dataset quirks
  - Similar embeddings for obviously different safety prompts → model lacks Safety-Challenging knowledge
  - CP varies wildly with k → embedding space too noisy for reliable clustering
- First 3 experiments:
  1. Run CP with k=5,10,15 on a small subset to observe stability
  2. Visualize t-SNE for high vs. low CP categories to confirm clustering
  3. Compare normalized vs. raw similarities on a known safe/unsafe pair to check anisotropy impact

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do Safety-Challenging and Safety-Taxonomy knowledge interact in real-world safety detection scenarios?
- Basis in paper: [explicit] The paper separately measures Safety-Challenging knowledge (distinguishing safe/unsafe prompts) and Safety-Taxonomy knowledge (categorizing unsafe prompts), but doesn't explore their interaction.
- Why unresolved: The paper presents these as separate dimensions of safety-critical knowledge without examining how they might complement or conflict with each other in practical applications.
- What evidence would resolve it: Empirical studies testing integrated models that leverage both types of knowledge simultaneously, measuring performance improvements or trade-offs in real-world safety detection tasks.

### Open Question 2
- Question: Would the proposed Categorical Purity metric remain effective when applied to more granular safety taxonomies or different safety classification systems?
- Basis in paper: [explicit] The paper applies CP to a specific 12-category taxonomy from the Do-Not-Answer dataset and shows promising results, but doesn't test its generalizability.
- Why unresolved: The metric's effectiveness across different taxonomies, safety frameworks, or more/less granular categorization schemes remains unknown.
- What evidence would resolve it: Testing CP across multiple safety taxonomies, comparing results with existing metrics, and evaluating its performance in cross-domain safety classification tasks.

### Open Question 3
- Question: How do different training methodologies (supervised vs unsupervised) impact the development of Safety-Critical knowledge in sentence encoders?
- Basis in paper: [explicit] The paper observes that supervised models like LLM2vec-Llama3 and Sup-SimCSE don't necessarily outperform unsupervised models like LLM2vec-Mistral and Unsup-SimCSE in safety-critical tasks.
- Why unresolved: The paper identifies this discrepancy but doesn't explore the underlying reasons or optimal training approaches for safety-critical knowledge development.
- What evidence would resolve it: Controlled experiments comparing different training methodologies, ablation studies isolating safety-specific components, and analysis of how safety-critical knowledge emerges during training.

## Limitations
- Dataset Composition Uncertainty: The creation process for the Safety-Challenging dataset includes 25 manually augmented prompts, but the exact methodology for this manual augmentation is not specified.
- Metric Generalizability: CP shows promise within the Do-Not-Answer dataset but its performance on other safety taxonomies remains unverified.
- Model Comparison Baseline: The paper doesn't include safety-specialized models, making it unclear whether observed differences are due to general semantic representation or safety-specific training.

## Confidence
- High Confidence: Sentence encoders can distinguish between safe and unsafe prompts for stereotype and privacy-related topics.
- Medium Confidence: Larger models and LLM-based encoders capture more Safety-Critical knowledge than smaller or purely supervised models.
- Low Confidence: The claim that CP is a reasonable metric for measuring Safety-Taxonomy knowledge across all safety domains.

## Next Checks
1. Create a new set of manually verified safe/unsafe pairs outside the original Safety-Challenging dataset and measure whether the same CP patterns hold, particularly for categories that showed weaker performance.
2. Apply the CP metric to a different safety taxonomy dataset (such as RealToxicityPrompts or Jigsaw's toxicity dataset) to verify whether high CP scores consistently indicate better safety categorization across different safety domains.
3. Include at least one model specifically fine-tuned for safety detection in the comparison to determine whether general semantic representation or targeted safety training drives the observed differences in Safety-Challenging and CP performance.