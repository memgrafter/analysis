---
ver: rpa2
title: Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation
arxiv_id: '2402.02857'
source_url: https://arxiv.org/abs/2402.02857
tags:
- gradient
- bias
- convergence
- stochastic
- adagrad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes convergence of SGD with biased gradient estimates
  and adaptive steps for non-convex smooth functions. Existing theory mostly assumes
  unbiased gradients, but in applications like VAEs, reinforcement learning, and zeroth-order
  optimization, gradients are often biased.
---

# Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation

## Quick Facts
- arXiv ID: 2402.02857
- Source URL: https://arxiv.org/abs/2402.02857
- Reference count: 40
- Primary result: SGD with biased gradient estimates and adaptive steps converges to critical points at rate O(log n/√n + bn) for non-convex smooth functions

## Executive Summary
This paper provides the first non-asymptotic convergence analysis of adaptive stochastic gradient methods (Adagrad, RMSProp, AMSGRAD) when gradient estimates are biased. While most theoretical work assumes unbiased gradients, many practical applications like VAEs, reinforcement learning, and zeroth-order optimization inherently involve biased gradient estimators. The authors establish convergence rates for both non-convex and convex functions, showing that adaptive methods can compensate for bias through time-dependent bias reduction techniques. Experimental results on VAEs demonstrate that appropriate hyperparameter tuning can significantly reduce the effect of bias and accelerate convergence.

## Method Summary
The paper analyzes Stochastic Approximation algorithms with biased gradient estimates and adaptive step sizes. The method incorporates time-dependent bias that decreases as O(n^-α) at iteration n, using techniques like increasing the number of samples for gradient estimation. The authors establish convergence rates for Adagrad, RMSProp, and AMSGRAD to critical points for smooth non-convex functions, showing that the convergence rate is similar to the unbiased case when the bias is properly controlled. For convex functions, they provide explicit rates for finding an ε-optimal solution. The analysis uses Randomized Stochastic Approximation, where a random variable R selects a parameter estimate from the sequence, enabling convergence analysis in non-convex settings.

## Key Results
- Convergence rate of O(log n/√n + bn) for non-convex functions, where bn is the bias term
- Convergence to ε-optimal solution at rate O(log²n/ε²) for convex functions
- Time-dependent bias reduction technique achieves convergence rates similar to unbiased case
- Experimental validation on VAEs shows bias reduction through hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adaptive steps compensate for bias in gradient estimates
- **Mechanism**: Adagrad, RMSProp, and AMSGRAD adjust step sizes based on accumulated gradient history, allowing the algorithm to compensate for biased gradients. By increasing samples for gradient estimation at each iteration (n^α samples at iteration n), the bias decreases as O(n^-α), achieving convergence rates similar to the unbiased case.
- **Core assumption**: Bias and MSE of gradient estimator can be bounded and controlled over time
- **Evidence anchors**:
  - [abstract]: "The authors establish that Adagrad, RMSProp, and AMSGRAD with biased gradients converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case."
  - [section]: "Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias of the gradient estimator."
- **Break condition**: Constant bias that doesn't decrease over time

### Mechanism 2
- **Claim**: Randomized Stochastic Approximation enables convergence analysis in non-convex settings
- **Mechanism**: A random variable R selects a parameter estimate from the sequence {θn}, allowing convergence in expectation to a critical point. This approach is particularly useful when no global minimum is guaranteed in non-convex optimization.
- **Core assumption**: The sequence of parameter estimates converges when averaged or sampled
- **Evidence anchors**:
  - [abstract]: "Finally, we provide experimental results using Variational Autoenconders (VAE) that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning."
  - [section]: "In classical SA, the update (3) is performed a fixed number of times n, and the quantity of interest is the last parameter θn. On the other hand, in Randomized Stochastic Approximation, we introduce a random variable R which takes its values in {1, . . . , n} and the quantity of interest is θR."
- **Break condition**: Improper definition of random variable R or non-convergent sequence of parameter estimates

### Mechanism 3
- **Claim**: Biased gradient estimators can be effectively used in deep learning applications
- **Mechanism**: The paper demonstrates that biased gradient estimators in VAEs, such as the Importance Weighted Autoencoder (IWAE), can achieve good performance by controlling bias through the number of samples used for gradient estimation. Increasing samples at each iteration decreases bias, leading to improved convergence rates.
- **Core assumption**: Bias in gradient estimator can be reduced by increasing number of samples
- **Evidence anchors**:
  - [abstract]: "Finally, we provide experimental results using Variational Autoenconders (VAE) that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning."
  - [section]: "To illustrate our results, we choose to incorporate a time-dependent bias that decreases by choosing a bias of order O(n^{-α}) at iteration n."
- **Break condition**: Insufficient increase in samples fails to reduce bias adequately

## Foundational Learning

- **Concept**: Stochastic Approximation (SA) algorithms
  - **Why needed here**: SA algorithms form the foundation for understanding SGD with biased gradients. The paper builds on SA theory to analyze convergence when gradient estimates are biased.
  - **Quick check question**: What is the difference between vanilla SGD and Stochastic Approximation?

- **Concept**: Adaptive learning rates
  - **Why needed here**: Adaptive methods like Adagrad and RMSProp are crucial for handling biased gradients by adjusting step sizes based on past gradient information.
  - **Quick check question**: How do Adagrad and RMSProp adjust the learning rate based on past gradients?

- **Concept**: Convex and non-convex optimization
  - **Why needed here**: The paper provides convergence rates for both convex and non-convex functions, which is important for understanding applicability to different ML problems.
  - **Quick check question**: What is the difference between convex and non-convex optimization, and why does it matter for convergence rates?

## Architecture Onboarding

- **Component map**: Gradient estimator -> Adaptive step size mechanism (Adagrad/RMSProp/AMSGRAD) -> Bias control strategy -> Parameter update
- **Critical path**: Gradient estimator provides information for parameter updates, adaptive step size ensures appropriate scaling, and bias control strategy reduces estimation error over time
- **Design tradeoffs**: Computational cost of increasing samples for gradient estimation versus benefit of bias reduction
- **Failure signatures**: Failure to converge or requiring excessive iterations when bias is not controlled; divergence or slow convergence when adaptive step size is not properly tuned
- **First 3 experiments**:
  1. Implement algorithm with simple biased gradient estimator and test convergence on a convex function
  2. Vary number of samples used for gradient estimation and observe effect on bias and convergence rate
  3. Test algorithm on non-convex function and compare convergence rate with and without bias control

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the eigenvalues control conditions (Assumption 4) be satisfied in practical settings such as Stochastic Newton or Gauss Newton methods?
- **Basis in paper**: Explicit - Mentioned in the discussion section that the authors intend to investigate this
- **Why unresolved**: The paper does not provide specific details on how to verify or enforce these conditions in practice
- **What evidence would resolve it**: A rigorous analysis demonstrating how to satisfy the eigenvalue conditions for these specific algorithms, potentially with empirical validation

### Open Question 2
- **Question**: Can the theoretical results be extended to the Adam optimizer, which incorporates momentum?
- **Basis in paper**: Explicit - Discussed as a possible extension in the conclusion
- **Why unresolved**: The current analysis focuses on Adagrad and RMSProp, and the incorporation of momentum in Adam introduces additional complexity
- **What evidence would resolve it**: A comprehensive convergence analysis of Adam with biased gradients, ideally with both theoretical guarantees and experimental validation

### Open Question 3
- **Question**: What is the optimal value of the hyperparameter α for the time-dependent bias reduction technique in practice?
- **Basis in paper**: Explicit - The paper discusses the trade-off between faster convergence and computational cost, suggesting that α = 1/4 might be optimal in some cases
- **Why unresolved**: The optimal value of α likely depends on the specific problem, model, and computational resources, making it difficult to provide a universal answer
- **What evidence would resolve it**: Extensive empirical studies across a range of problems and models, identifying general guidelines for choosing α based on problem characteristics and available computational resources

## Limitations
- Assumes bounded gradient norms and smoothness conditions that may not hold for all deep learning architectures
- Convergence rates depend critically on the bias decay rate α, but the paper does not provide systematic guidance for selecting optimal α values in practice
- Theoretical framework assumes i.i.d. sampling and bounded stochastic gradients, which may be violated in practical implementations with mini-batches

## Confidence
- Non-convex convergence rate (O(log n/√n + bn)): High - well-supported by theoretical analysis
- Bias reduction through sample increase: Medium - theoretical but requires empirical validation
- VAE experimental results: Low-Medium - limited experimental details provided

## Next Checks
1. Test convergence with varying bias decay rates α on synthetic non-convex functions to map the relationship between α and convergence speed
2. Implement the same algorithms with exact unbiased gradients to benchmark the performance degradation from bias
3. Evaluate the algorithms on larger-scale VAE experiments with different architectures to assess practical applicability beyond the CIFAR-10 example