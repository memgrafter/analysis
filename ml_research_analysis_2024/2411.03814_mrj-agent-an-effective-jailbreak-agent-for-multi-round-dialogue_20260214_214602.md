---
ver: rpa2
title: 'MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue'
arxiv_id: '2411.03814'
source_url: https://arxiv.org/abs/2411.03814
tags:
- attack
- arxiv
- harmful
- multi-round
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MRJ-Agent, a novel multi-round dialogue jailbreaking
  agent for Large Language Models (LLMs). The method addresses the limitation of existing
  single-round jailbreak attacks by introducing a risk decomposition strategy that
  distributes risks across multiple rounds of queries and incorporates psychological
  tactics to enhance attack strength.
---

# MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue

## Quick Facts
- arXiv ID: 2411.03814
- Source URL: https://arxiv.org/abs/2411.03814
- Reference count: 26
- Key outcome: Novel multi-round dialogue jailbreaking agent achieving state-of-the-art attack success rates on both open-source and closed-source LLMs

## Executive Summary
This paper introduces MRJ-Agent, a red-team agent designed to perform effective jailbreak attacks on Large Language Models through multi-round dialogue. The key innovation is a risk decomposition strategy that breaks harmful queries into multiple semantically similar but less risky sub-queries across multiple rounds, combined with psychological strategies to enhance attack effectiveness. The method addresses the limitation of single-round attacks and demonstrates superior performance compared to existing methods, achieving high attack success rates across various model families while showing strong generalization capabilities.

## Method Summary
MRJ-Agent employs a risk decomposition strategy (Td) that transforms harmful queries into multiple sub-queries while maintaining semantic similarity, and psychological refinement (Tp) that applies 13 different psychological strategies to enhance persuasiveness. A red-team agent trained via Direct Preference Optimization (DPO) on successful attack data dynamically generates effective queries based on real-time feedback. The system operates through multi-round dialogue where each round presents a sub-query that gradually guides the target model toward producing harmful content, with a judge model evaluating whether harmful content was successfully elicited.

## Key Results
- Achieves 89.66% attack success rate on Llama2-7B and 87.50% on Mistral-7B
- Outperforms existing single-round methods by significant margins across all tested models
- Successfully generalizes to unseen harmful queries and target models not in the training set
- Demonstrates effectiveness across different categories of harmful queries, with highest success on violence-related content (92.85%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Risk decomposition strategy distributes the risk of harmful queries across multiple rounds of interaction.
- Mechanism: The method breaks down a harmful query into multiple sub-queries that maintain semantic similarity to the original query while reducing immediate risk. This allows the model to be gradually guided toward harmful content without triggering safety mechanisms in any single round.
- Core assumption: Decomposing a harmful query into semantically related but less risky sub-queries will bypass safety mechanisms while maintaining the overall attack trajectory.
- Evidence anchors:
  - [abstract]: "We propose a risk decomposition strategy that distributes risks across multiple rounds of queries and utilizes psychological strategies to enhance attack strength."
  - [section]: "We propose a heuristic framework that gradually decomposes risky intentions across multiple rounds of inquiry."
  - [corpus]: The corpus contains multiple papers on multi-round dialogue jailbreaking and safety alignment for LLMs, supporting the relevance of this approach.
- Break condition: If the semantic similarity threshold (τ) is set too low, sub-queries may lose connection to the original harmful intent, breaking the attack chain.

### Mechanism 2
- Claim: Psychological strategies enhance the effectiveness of decomposed queries by making them more persuasive to the target model.
- Mechanism: The method applies psychological tactics (such as "Multi-faceted evidence support" or "Social Influence") to the sub-queries to increase the likelihood of the target model providing harmful responses. These strategies exploit the model's training biases and response patterns.
- Core assumption: Large language models respond differently to queries framed with specific psychological tactics, making them more likely to bypass safety filters.
- Evidence anchors:
  - [abstract]: "utilizes psychological strategies to enhance attack strength"
  - [section]: "we incorporate psychological strategies to enhance this series of sub-queries"
  - [corpus]: The corpus includes papers on jailbreak attacks using various psychological and persuasive techniques, supporting this mechanism.
- Break condition: If the psychological strategy is incompatible with the query context, it may actually reduce attack success rather than enhance it.

### Mechanism 3
- Claim: A dedicated red-team agent trained through preference optimization can dynamically generate effective queries based on model responses.
- Mechanism: The red-team model (MRJ-Agent) is trained using Direct Preference Optimization (DPO) on successful attack data, allowing it to learn which query patterns and psychological strategies work best against specific models.
- Core assumption: A model trained on successful attack patterns can generalize to new attack scenarios by dynamically adjusting queries based on real-time feedback.
- Evidence anchors:
  - [abstract]: "We further trained a red team agent, which is more capable of learning the two aforementioned transformations effectively."
  - [section]: "To achieve this, we devise two strategies: Information-based control strategy... Psychology induction strategy"
  - [corpus]: The corpus includes papers on preference optimization and red-teaming for LLMs, supporting this training approach.
- Break condition: If the training data is too narrow or biased toward specific model types, the agent may fail to generalize to new models or attack scenarios.

## Foundational Learning

- Concept: Semantic similarity measurement using sentence embeddings
  - Why needed here: To ensure decomposed sub-queries maintain connection to the original harmful intent while appearing less risky
  - Quick check question: What is the minimum semantic similarity threshold (τ) used to filter sub-queries in this method?

- Concept: Multi-round dialogue attack strategies
  - Why needed here: To understand how the attack unfolds across multiple conversational turns rather than a single prompt
  - Quick check question: How does the risk decomposition strategy differ from simply asking the same harmful question multiple times?

- Concept: Preference optimization for adversarial purposes
  - Why needed here: To train the red-team agent to generate effective attack queries based on successful examples
  - Quick check question: What optimization method is used to train the red-team model on attack data?

## Architecture Onboarding

- Component map:
  - Risk decomposition module (Td) -> Psychological refinement module (Tp) -> Red-team agent (πred) -> Target model (T) -> Judge model (J) -> Loop back if needed

- Critical path: Harmful query → Risk decomposition → Psychological refinement → Red-team agent → Target model → Judge evaluation → Loop back if needed

- Design tradeoffs:
  - Query similarity vs. stealth: Higher similarity maintains attack intent but may trigger safety mechanisms
  - Number of rounds vs. success rate: More rounds generally increase success but also increase detection risk
  - Model size vs. efficiency: Larger red-team models perform better but require more computational resources

- Failure signatures:
  - Low semantic similarity scores between sub-queries and original query
  - Target model consistently rejecting queries before reaching harmful content
  - Judge model failing to identify harmful content even when present

- First 3 experiments:
  1. Test risk decomposition with a simple harmful query on a single target model to verify the Td transformation works
  2. Apply psychological strategies to decomposed queries and measure success rate improvement
  3. Train the red-team agent on successful attacks from steps 1-2 and evaluate against both trained and new target models

## Open Questions the Paper Calls Out

1. How does the effectiveness of MRJ-Agent vary across different categories of harmful queries (e.g., physical harm vs. disinformation)?
[explicit] - The paper mentions that Sexual Content was the most difficult category to attack with only 71.42% success rate.

2. What is the relationship between the number of dialogue rounds and attack success rate?
[inferred] - The paper shows that increasing rounds improves success rates, but doesn't specify the optimal number of rounds.

3. How do different psychological strategies compare in effectiveness across different types of harmful queries?
[inferred] - The paper shows "Multi-faceted evidence support" was most effective overall, but doesn't compare effectiveness across query types.

4. What is the computational overhead of using larger red-teaming models (e.g., 13B vs 7B) in terms of both training and inference time?
[inferred] - The paper mentions training times for different model sizes but doesn't discuss inference efficiency or practical deployment considerations.

5. How does MRJ-Agent perform against more sophisticated defense mechanisms beyond simple prompt detection and system prompts?
[inferred] - The paper only tests against basic defenses, suggesting potential vulnerabilities to more advanced detection methods.

## Limitations

- The specific implementation details of the 13 psychological strategies and their templates are not fully specified, making it difficult to assess whether the claimed improvements are due to these strategies or other factors in the pipeline.
- The evaluation methodology relies on GPT-4 as a judge, which introduces potential bias since GPT-4 may share similar safety alignment with the target models.
- The paper does not provide ablation studies showing how each of the 13 psychological strategies contributes individually or which combinations work best.

## Confidence

- **High Confidence**: The core methodology of decomposing harmful queries across multiple rounds to evade safety detection is technically sound and well-grounded in prior research on multi-turn dialogue attacks. The overall architecture and training pipeline using DPO is standard and reproducible.
- **Medium Confidence**: The claim that MRJ-Agent achieves state-of-the-art performance is supported by experimental results, but the evaluation methodology relies on GPT-4 as a judge, which introduces potential bias since GPT-4 may share similar safety alignment with the target models. The generalization claims across different model families need independent verification.
- **Low Confidence**: The specific implementation details of the 13 psychological strategies and their templates are not fully specified, making it difficult to assess whether the claimed improvements are due to these strategies or other factors in the pipeline.

## Next Checks

1. **Ablation study of psychological strategies**: Systematically disable each of the 13 psychological strategies and measure the impact on attack success rate to identify which strategies contribute most significantly to performance gains.

2. **Cross-model robustness testing**: Evaluate MRJ-Agent against models with fundamentally different architectures (e.g., Claude, Gemini) and safety training approaches to verify the claimed generalization capabilities beyond the tested models.

3. **Human evaluation of attack detection**: Replace the GPT-4 judge with human evaluators to assess whether the attacks successfully produce harmful content while maintaining conversational coherence, addressing potential bias in automated evaluation.