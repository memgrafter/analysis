---
ver: rpa2
title: 'LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding'
arxiv_id: '2407.15754'
source_url: https://arxiv.org/abs/2407.15754
tags:
- video
- videos
- long
- lmms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LONG VIDEO BENCH, a benchmark for long-context
  interleaved video-language understanding. It addresses the lack of public benchmarks
  for evaluating large multimodal models on hour-long subtitled videos.
---

# LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding

## Quick Facts
- **arXiv ID**: 2407.15754
- **Source URL**: https://arxiv.org/abs/2407.15754
- **Reference count**: 40
- **Key outcome**: LongVideoBench shows even advanced proprietary models struggle with long-context interleaved video-language understanding, while open-source models perform significantly worse.

## Executive Summary
LongVideoBench introduces a novel benchmark for evaluating large multimodal models on hour-long subtitled videos through a "referring reasoning" task. The benchmark contains 6,678 human-annotated multiple-choice questions across 17 fine-grained categories on 3,763 diverse videos, requiring models to reason over detailed multimodal information from long video sequences. Evaluations demonstrate that advanced proprietary models (GPT-4o, Gemini-1.5-Pro) struggle significantly with this benchmark, while open-source models show even larger performance gaps. Model performance improves only when processing more frames, validating LongVideoBench as a valuable benchmark for future long-context LMMs.

## Method Summary
The LongVideoBench benchmark evaluates long-context interleaved video-language understanding through a novel "referring reasoning" task. The dataset consists of 3,763 web-collected videos with synchronized subtitles, processed at 1fps to create interleaved visual-textual inputs. 6,678 human-annotated multiple-choice questions are organized into 17 categories across 4 duration groups (8-15s, 15-60s, 3-10min, 15-60min). Questions are structured at two levels: Perception (recognizing objects, attributes, events in individual scenes) and Relation (associating multiple scenes across temporal contexts). The benchmark is evaluated zero-shot on both proprietary models (GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo, Gemini-1.5-Flash) and open-source models (Idefics2, Phi-3-Vision-Instruct, Mantis-Idefics2, Mantis-BakLLaV A).

## Key Results
- Even advanced proprietary models (GPT-4o, Gemini-1.5-Pro) struggle significantly on LongVideoBench, with performance gaps across duration groups
- Open-source models show substantially worse performance than proprietary models on the benchmark
- Model performance improves only when capable of processing more frames, validating the benchmark's effectiveness
- Video frames (visual modality) are crucial components, as removing them leads to much worse results for all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark effectively measures long-context multimodal understanding by requiring models to process hundreds of frames per video to achieve optimal performance.
- Mechanism: By introducing "referring reasoning" questions that explicitly reference specific video contexts (referred context), the benchmark forces models to demonstrate both retrieval of detailed multimodal information and reasoning over contextual relations across long video sequences.
- Core assumption: Models cannot achieve high performance on this benchmark by processing only a few key frames or by relying on summarization-level understanding.
- Evidence anchors: Abstract shows proprietary models struggle and performance improves with more frames; Section 5.1 demonstrates significant improvements with increased input length; Average neighbor FMR=0.456 indicates weak correlation with this mechanism.
- Break condition: If models develop techniques to effectively summarize long videos without processing all frames, or if the benchmark questions can be answered with key-frame extraction rather than full-context understanding.

### Mechanism 2
- Claim: The interleaved video-subtitle format is crucial for evaluating true multimodal understanding rather than visual-only processing.
- Mechanism: By requiring models to process both visual frames and synchronized subtitles together, the benchmark tests whether models can integrate text and visual information meaningfully, not just recognize visual patterns.
- Core assumption: Models that process only visual information or only text information will perform significantly worse than those that integrate both modalities.
- Evidence anchors: Abstract notes challenge of understanding long-context interleaved multimodal inputs; Section 5.2 shows removing video frames leads to much worse results; Average neighbor FMR=0.456 indicates weak correlation with this mechanism.
- Break condition: If models develop superior visual-only understanding that makes subtitles redundant, or if the subtitle integration becomes a trivial mapping task.

### Mechanism 3
- Claim: The two-level question structure (Perception and Relation) effectively discriminates between basic retrieval capabilities and complex reasoning abilities.
- Mechanism: Level 1 (Perception) questions test whether models can locate and understand specific visual concepts in single moments, while Level 2 (Relation) questions require tracking objects across scenes and understanding temporal/spatial relationships, creating a graduated difficulty scale.
- Core assumption: Models that perform well on Level 1 but poorly on Level 2 demonstrate incomplete multimodal understanding that lacks temporal reasoning capabilities.
- Evidence anchors: Abstract describes perception questions requiring recognition of objects in individual scenes vs relation questions requiring association of multiple scenes; Section 5.2 shows disparity between L1 and L2 performance indicates insufficient understanding of temporal dynamics; Average neighbor FMR=0.456 indicates weak correlation with this mechanism.
- Break condition: If models develop shortcuts to answer relation questions without actually tracking objects across scenes, or if the distinction between perception and relation becomes artificial.

## Foundational Learning

- Concept: Needle-in-a-haystack evaluation
  - Why needed here: This concept provides the theoretical foundation for understanding why retrieving specific details from long contexts is challenging and how to measure it effectively
  - Quick check question: What is the key difference between standard QA benchmarks and needle-in-a-haystack evaluation when measuring long-context understanding?

- Concept: Multimodal fusion techniques
  - Why needed here: Understanding how to effectively combine visual and textual information is crucial for models to perform well on interleaved inputs
  - Quick check question: What are the main architectural approaches for fusing visual and textual embeddings in multimodal models?

- Concept: Temporal reasoning in video understanding
  - Why needed here: The benchmark heavily relies on models understanding temporal relationships between different video segments
  - Quick check question: How does temporal reasoning differ from spatial reasoning in video understanding tasks?

## Architecture Onboarding

- Component map: Video preprocessing pipeline -> Subtitle alignment system -> Context length management -> Multimodal encoder -> Reasoning module -> Output formatter
- Critical path: Frame extraction → Subtitle alignment → Multimodal encoding → Context window filling → Question answering → Answer selection
- Design tradeoffs: Higher frame rates improve detail capture but exponentially increase computational cost; longer context windows enable better temporal reasoning but reduce per-token attention capacity
- Failure signatures: Performance degradation when referred context is temporally distant from the question; inability to track objects across scene changes; confusion between visually similar objects in different contexts
- First 3 experiments:
  1. Measure performance baseline with minimal frames (1-4) vs. full context to establish frame requirement curve
  2. Test visual-only vs. text-only vs. interleaved inputs to quantify modality contribution
  3. Compare performance on perception vs. relation questions to identify reasoning bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the LongVideoBench performance gap between proprietary and open-source models persist when proprietary models' context windows are artificially limited to match open-source models?
- Basis in paper: [inferred] The paper shows open-source models fail to improve with more frames while proprietary models improve significantly. This suggests comparing models under identical context constraints.
- Why unresolved: The paper only varies max_frames for open-source models up to their context limits, not by artificially constraining proprietary models.
- What evidence would resolve it: Experiments showing performance when proprietary models are limited to 16-32 frames like open-source models.

### Open Question 2
- Question: How does LongVideoBench performance vary with video compression levels and frame rate reductions?
- Basis in paper: [explicit] The paper notes videos are sampled to different sizes and discusses frame processing, but doesn't systematically evaluate compression/quality effects.
- Why unresolved: The paper focuses on frame count rather than quality/compression trade-offs in video input.
- What evidence would resolve it: Performance measurements across multiple compression/quality settings with fixed frame counts.

### Open Question 3
- Question: What is the minimum context length required for open-source models to begin showing improvement on LongVideoBench?
- Basis in paper: [explicit] Open-source models show severe degradation at 64 frames rather than improvement, suggesting a threshold effect.
- Why unresolved: The paper doesn't test intermediate frame counts between 16 and 64 to identify the inflection point.
- What evidence would resolve it: Performance measurements at 24, 32, 40, and 48 frames to identify where degradation begins.

## Limitations

- Dataset Generalization: The benchmark relies on web-collected videos from 119 channels, which may introduce domain bias and limit generalizability to other video types.
- Context Window Validation: The paper demonstrates performance improves with more frames but doesn't establish optimal frame rate or context window size for each model type.
- Question Quality Control: While questions are human-annotated, the paper doesn't provide detailed quality metrics, inter-annotator agreement scores, or systematic validation of question ambiguity.

## Confidence

**High Confidence**: The claim that LongVideoBench presents significant challenges for state-of-the-art models is well-supported by empirical results showing performance gaps across multiple duration groups and model types.

**Medium Confidence**: The assertion that interleaved video-subtitle format is crucial for true multimodal understanding has strong evidence but could be more precisely quantified through detailed modality contribution analysis.

**Low Confidence**: The claim about the two-level question structure effectively discriminating between basic retrieval and complex reasoning could be more rigorously validated with inter-annotator agreement metrics and systematic analysis of question difficulty factors.

## Next Checks

1. **Frame Rate Sensitivity Analysis**: Conduct experiments varying the frame extraction rate (0.5fps, 1fps, 2fps, 5fps) to identify the optimal trade-off between computational cost and performance for different video types and model architectures.

2. **Cross-Domain Generalization Test**: Evaluate model performance on LongVideoBench using videos from domains not represented in the training data (e.g., medical procedures, technical demonstrations, sports) to assess whether the benchmark's difficulty stems from inherent multimodal understanding challenges or domain-specific familiarity.

3. **Annotation Quality Validation**: Implement a second round of human evaluation where independent annotators answer the same questions without model assistance, measuring inter-annotator agreement and identifying potentially ambiguous questions that might inflate the benchmark's apparent difficulty.