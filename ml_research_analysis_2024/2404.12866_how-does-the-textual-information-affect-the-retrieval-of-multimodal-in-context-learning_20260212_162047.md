---
ver: rpa2
title: How Does the Textual Information Affect the Retrieval of Multimodal In-Context
  Learning?
arxiv_id: '2404.12866'
source_url: https://arxiv.org/abs/2404.12866
tags:
- in-context
- examples
- msier
- multimodal
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of textual information in multimodal
  in-context learning (M-ICL) retrieval. Current methods focus on visual data, overlooking
  textual cues.
---

# How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?

## Quick Facts
- arXiv ID: 2404.12866
- Source URL: https://arxiv.org/abs/2404.12866
- Reference count: 8
- Primary result: MSIER retriever improves M-ICL performance by up to 5.52 CIDEr points over unsupervised methods

## Executive Summary
This paper investigates how textual information influences retrieval for multimodal in-context learning (M-ICL). The authors introduce MSIER, a supervised retriever trained using MLLM confidence scores that selects relevant multimodal in-context examples. Through extensive experiments on image captioning, VQA, and rank classification tasks, they demonstrate that incorporating textual data during both training and evaluation significantly improves retrieval performance compared to visual-only approaches. MSIER shows strong transferability, maintaining effectiveness when applied to larger models, making it a cost-efficient solution for large-scale M-ICL applications.

## Method Summary
MSIER is a supervised retriever that leverages MLLM confidence scores to train multimodal retrieval for in-context learning. The method uses a CLIP-based encoder to extract joint image-text features, while an MLLM scorer (OpenFlamingo-3B) evaluates candidate prompts by computing NLL loss during in-context learning. During training, Top-N candidates are first selected using unsupervised retrieval, then scored by the MLLM to identify positive and negative examples. Contrastive learning aligns the retriever to prefer examples that yield better MLLM predictions. The trained MSIER is then used to retrieve in-context examples for downstream M-ICL tasks. The approach is evaluated on MS COCO, OK-VQA, and HatefulMemes datasets.

## Key Results
- MSIER improves image captioning CIDEr score by up to 5.52 points over unsupervised retrieval methods
- Incorporating textual information during both training and evaluation significantly enhances retrieval performance
- MSIER trained on OpenFlamingo-3B transfers effectively to OpenFlamingo-9B, demonstrating strong scalability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MSIER improves M-ICL by using MLLM confidence scores to supervise retriever training
- Mechanism: MLLM scorer evaluates retrieved candidates' in-context learning performance, labels them as positive/negative based on NLL loss, and contrastive learning aligns the retriever to prefer examples that yield better MLLM predictions
- Core assumption: MLLM confidence scores (NLL loss) are a reliable proxy for in-context example quality
- Evidence anchors: [abstract] "MSIER, a supervised retriever trained on MLLM confidence scores, which selects relevant multimodal in-context examples." [section 3.3] "We initially devise effective prompts... The MLLM scorer is furnished with a specific image or image-text prompt... These predictions are subsequently leveraged to compute the NLL loss, which serves as the performance metric for the given in-context instance."
- Break condition: If MLLM NLL loss does not correlate well with actual downstream performance, supervised training becomes ineffective

### Mechanism 2
- Claim: Incorporating both visual and textual modalities during training and evaluation leads to better retrieval performance
- Mechanism: The retriever uses joint image-text feature similarity (cosine similarity of multimodal embeddings) to match queries with examples, enabling richer semantic matching
- Core assumption: Textual information provides complementary cues that enhance semantic similarity matching beyond visual features alone
- Evidence anchors: [section 3.2] "we performed a comprehensive comparison... the Q-I-M-IT setting exerts a positive impact on the M-ICL performance to varying degrees, which verifies the substantial influence of textual content on M-ICL performance." [abstract] "incorporating textual data during both training and evaluation significantly improves retrieval performance."
- Break condition: If textual content is noisy or irrelevant to the task, it may degrade retrieval quality

### Mechanism 3
- Claim: MSIER's training on smaller MLLMs transfers effectively to larger MLLMs
- Mechanism: The retriever learns general multimodal relevance patterns during training on a smaller MLLM scorer, which remain applicable when evaluating with larger MLLMs without retraining
- Core assumption: Multimodal feature relevance and semantic similarity are task-agnostic and model-size-independent
- Evidence anchors: [abstract] "MSIER demonstrates strong transferability, maintaining effectiveness when applied to larger models, making it a cost-efficient solution for large-scale M-ICL applications." [section 4.4] "The outcomes in Table 4 demonstrate that the MSIER approach, when utilizing the 3B model as a scorer, manifests superior transferability and outperforms the MUIER-9B method."
- Break condition: If larger MLLMs have fundamentally different feature representations, transferability breaks down

## Foundational Learning

- Concept: Contrastive learning for multimodal retrieval
  - Why needed here: MSIER uses contrastive learning to align positive and negative example embeddings, enabling the retriever to distinguish relevant from irrelevant in-context examples
  - Quick check question: In contrastive learning for retrieval, what defines a positive example pair and a negative example pair?

- Concept: In-context learning (ICL) in multimodal settings
  - Why needed here: MSIER's goal is to select in-context examples that improve MLLM's ICL performance; understanding ICL is essential to designing the retriever
  - Quick check question: How does multimodal ICL differ from text-only ICL in terms of input format and expected output?

- Concept: Multimodal feature extraction and fusion
  - Why needed here: The retriever must extract and compare joint image-text features; understanding how CLIP or similar models encode multimodal data is critical
  - Quick check question: What is the role of the text encoder and image encoder in CLIP, and how are their outputs combined for similarity computation?

## Architecture Onboarding

- Component map: Query encoder (CLIP) -> Context encoder (CLIP) -> MLLM scorer (OpenFlamingo) -> Top-N candidate selector -> Contrastive loss trainer

- Critical path:
  1. Given a test query, retrieve Top-N candidates using MUIER (multimodal unsupervised retrieval)
  2. Score candidates with MLLM scorer using in-context learning setup
  3. Select top-K positive and bottom-K negative examples
  4. Train MSIER using contrastive loss to align query and context encoders
  5. Use trained MSIER to retrieve in-context examples for downstream M-ICL

- Design tradeoffs:
  - Using MLLM scorer for supervision ensures relevance but adds computational cost during training
  - Joint image-text similarity enables richer matching but requires careful feature normalization
  - Freezing one encoder (text vs image) can control overfitting and improve efficiency

- Failure signatures:
  - Low improvement over MUIER suggests scorer does not effectively rank examples
  - Degraded performance on larger MLLMs indicates poor transferability
  - High variance across runs may indicate instability in contrastive training

- First 3 experiments:
  1. Verify multimodal retrieval improves over visual-only retrieval on a held-out validation set
  2. Test MSIER trained on smaller MLLM transfers to larger MLLM without retraining
  3. Ablation: freeze text encoder vs image encoder to see which has larger impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MSIER change when using larger MLLMs as scorers, such as transitioning from OpenFlamingo-3B to OpenFlamingo-9B or even larger models?
- Basis in paper: [explicit] The paper mentions that MSIER trained on OpenFlamingo-3B as a scorer is applied to OpenFlamingo-9B, showing good transferability. However, the full potential of scaling the scorer is not explored.
- Why unresolved: The study only tests transferability from a 3B to a 9B model, leaving questions about performance with even larger models.
- What evidence would resolve it: Conducting experiments with MSIER trained on progressively larger MLLM scorers (e.g., 30B, 70B) and evaluating their performance on downstream tasks would provide insights into the scalability of the method.

### Open Question 2
- Question: Can MSIER be effectively extended to handle more than two modalities, such as incorporating audio or video alongside text and images?
- Basis in paper: [inferred] The paper focuses on text and image modalities, but acknowledges the growing use of additional modalities in M-ICL. This suggests a potential gap in multimodal integration.
- Why unresolved: The current framework is limited to text and images, and there is no exploration of how additional modalities might enhance retrieval performance.
- What evidence would resolve it: Implementing MSIER with additional modality encoders (e.g., audio or video) and evaluating its performance on tasks requiring these modalities would demonstrate its adaptability and effectiveness.

### Open Question 3
- Question: How does the diversity of retrieved in-context examples impact the performance of MSIER, and can a balance between diversity and relevance be optimized?
- Basis in paper: [explicit] The paper mentions that both diversity and relevance are critical in selecting in-context examples for M-ICL, but does not explore how to optimize this balance.
- Why unresolved: While MSIER focuses on relevance, the study does not investigate the trade-off between diverse and relevant examples, which could affect learning outcomes.
- What evidence would resolve it: Designing experiments that vary the diversity of retrieved examples while maintaining relevance, and measuring the impact on M-ICL performance, would provide insights into optimizing example selection.

## Limitations

- The method relies on MLLM confidence scores as a proxy for in-context example quality, which may not generalize across all task types or dataset distributions
- Transferability claims are limited to scaling within the same architecture family (OpenFlamingo), leaving uncertainty about performance on different MLLM architectures
- The computational overhead of using an MLLM scorer during training creates a practical barrier for resource-constrained applications

## Confidence

**High Confidence**: The experimental results demonstrating MSIER's superior performance over unsupervised methods (up to 5.52 CIDEr points) are well-supported by the ablation studies and cross-task comparisons presented in the evaluation section.

**Medium Confidence**: The claim about strong transferability to larger models is supported by the specific experiment comparing 3B to 9B parameter models, but the evidence base is limited to a single scaling direction within one architecture family.

**Low Confidence**: The assertion that textual information universally improves retrieval performance assumes that all multimodal tasks benefit equally from text inclusion, which lacks comprehensive testing across diverse domain contexts.

## Next Checks

1. **Cross-Architecture Transfer Validation**: Test MSIER's transferability by training the retriever on a 3B parameter model from one architecture family and evaluating its performance on a 9B or larger parameter model from a completely different architecture.

2. **Textual Information Robustness Testing**: Systematically vary the quality and relevance of textual information in the retrieval pipeline by introducing controlled noise or irrelevant text to evaluate whether the retriever's performance degrades gracefully or catastrophically.

3. **Computational Efficiency Analysis**: Conduct a detailed cost-benefit analysis comparing the training time and computational resources required for MSIER (including MLLM scorer supervision) against the performance gains achieved.