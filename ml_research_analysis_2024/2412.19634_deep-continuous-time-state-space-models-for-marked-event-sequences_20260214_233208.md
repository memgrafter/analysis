---
ver: rpa2
title: Deep Continuous-Time State-Space Models for Marked Event Sequences
arxiv_id: '2412.19634'
source_url: https://arxiv.org/abs/2412.19634
tags:
- intensity
- time
- dlhp
- event
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deep linear Hawkes processes (DLHP) combine linear Hawkes processes
  with deep state-space models (SSMs) to model marked temporal point processes. DLHP
  leverages parallel scans for efficient computation, achieving linear complexity
  and sublinear scaling while retaining expressivity.
---

# Deep Continuous-Time State-Space Models for Marked Event Sequences

## Quick Facts
- arXiv ID: 2412.19634
- Source URL: https://arxiv.org/abs/2412.19634
- Reference count: 40
- Deep linear Hawkes processes (DLHP) achieve 33% average improvement over existing approaches

## Executive Summary
Deep Linear Hawkes Processes (DLHP) introduces a novel approach for modeling marked temporal point processes by combining linear Hawkes processes with deep state-space models (SSMs). The framework leverages parallel scans for efficient computation, achieving linear complexity with sublinear scaling while maintaining expressivity. DLHP uses stochastic jump differential equations interleaved with nonlinearities to capture continuous-time event sequences, demonstrating state-of-the-art predictive likelihoods across eight real-world datasets.

## Method Summary
DLHP extends traditional Hawkes processes by integrating them with deep continuous-time state-space models. The core innovation lies in using parallel scans to compute the intensity function efficiently, enabling linear computational complexity. The model employs stochastic jump differential equations that capture both the continuous-time dynamics and discrete event occurrences. Non-linear transformations are applied between state transitions to enhance modeling flexibility. This architecture allows DLHP to handle marked events (events with associated attributes) while maintaining computational tractability for long sequences.

## Key Results
- Achieves state-of-the-art predictive likelihoods across eight real-world datasets
- Delivers an average improvement of 33% over the best existing approaches
- Maintains linear computational complexity with sublinear scaling for long sequences

## Why This Works (Mechanism)
DLHP works by decomposing the temporal point process modeling task into two complementary components: a linear Hawkes process that captures the self-exciting temporal dependencies, and a deep state-space model that provides rich non-linear representations. The parallel scan implementation enables efficient computation of the intensity function across the entire sequence, avoiding the sequential bottlenecks of traditional approaches. The stochastic jump differential equations naturally model the discontinuity introduced by event occurrences while maintaining continuous-time dynamics between events. This combination allows DLHP to capture complex temporal patterns while remaining computationally tractable.

## Foundational Learning
- **Linear Hawkes processes**: Self-exciting temporal point processes where past events increase the likelihood of future events. Needed to model temporal dependencies; quick check: verify that intensity function properly captures excitation effects.
- **State-space models**: Framework for modeling systems with hidden states and noisy observations. Needed to provide rich representations; quick check: ensure state transitions properly capture temporal dynamics.
- **Parallel scan algorithms**: Technique for computing prefix sums in logarithmic depth. Needed for computational efficiency; quick check: verify parallel implementation matches sequential computation.
- **Stochastic differential equations**: Mathematical framework for systems with random fluctuations. Needed to model continuous-time dynamics with noise; quick check: validate numerical stability of SDE solver.
- **Temporal point processes**: Mathematical framework for modeling event sequences in continuous time. Needed as the underlying modeling paradigm; quick check: ensure proper handling of event times and marks.
- **Marked events**: Events with associated attributes or marks. Needed for real-world applications; quick check: verify mark modeling captures relevant attributes.

## Architecture Onboarding

**Component map**: Input sequence -> State initialization -> Parallel scan computation -> Intensity function calculation -> Event prediction

**Critical path**: Event times → State initialization → Parallel scan → Intensity function → Likelihood computation

**Design tradeoffs**: The paper prioritizes computational efficiency through parallel scans over sequential approaches, accepting the complexity of stochastic differential equations for better modeling fidelity. The use of deep SSMs provides expressivity but increases parameter count and potential overfitting risk.

**Failure signatures**: Poor performance on datasets with weak temporal dependencies, numerical instability for high event density sequences, degraded accuracy for very long sequences where approximation errors accumulate, and sensitivity to initialization in deep state components.

**First experiments**:
1. Test DLHP on synthetic Hawkes process data with known parameters to verify recovery accuracy
2. Compare training time and memory usage against baseline methods on sequences of increasing length
3. Evaluate performance degradation when mark information is removed to assess mark modeling contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Computational efficiency claims require more rigorous empirical validation across diverse dataset characteristics
- Stochastic jump differential equation formulation may affect numerical stability for high event density datasets
- Empirical evaluation focuses primarily on predictive likelihood metrics without thorough examination of calibration or uncertainty quantification

## Confidence
- **High confidence**: The mathematical formulation of DLHP and its connection to linear Hawkes processes is well-established and theoretically sound
- **Medium confidence**: The computational complexity analysis and empirical performance claims, pending further validation on additional datasets and edge cases
- **Low confidence**: The practical implications of the stochastic jump formulation and its impact on model behavior in real-world deployment scenarios

## Next Checks
1. Conduct ablation studies isolating the contributions of the deep SSM component versus the Hawkes process component, and test performance across datasets with varying degrees of temporal dependency structure
2. Evaluate DLHP's behavior on extremely long sequences (>10,000 events) to empirically verify the sublinear scaling claims and identify any computational bottlenecks
3. Test the model's robustness by evaluating performance when training and test data have different temporal patterns or mark distributions, and assess uncertainty calibration using proper scoring rules