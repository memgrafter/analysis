---
ver: rpa2
title: How Feature Learning Can Improve Neural Scaling Laws
arxiv_id: '2409.17858'
source_url: https://arxiv.org/abs/2409.17858
tags:
- scaling
- learning
- neural
- feature
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a solvable model of neural scaling laws that
  allows for feature learning beyond the lazy kernel regime. The model consists of
  a two-layer linear network where the hidden layer weights evolve via projected gradient
  descent while the output weights are updated by standard SGD.
---

# How Feature Learning Can Improve Neural Scaling Laws

## Quick Facts
- arXiv ID: 2409.17858
- Source URL: https://arxiv.org/abs/2409.17858
- Reference count: 40
- Primary result: Feature learning can nearly double the time scaling exponent for hard tasks compared to lazy training

## Executive Summary
This paper develops a solvable model of neural scaling laws that incorporates feature learning beyond the lazy kernel regime. The authors analyze a two-layer linear network where hidden layer weights evolve via projected gradient descent while output weights are updated by standard SGD. Using dynamical mean-field theory, they derive predictions for how loss scales with training time, model size, and dataset size, identifying three distinct regimes based on task difficulty measured by the source exponent β.

The key finding is that for hard tasks (β < 1) outside the initial reproducing kernel Hilbert space, feature learning can nearly double the time scaling exponent compared to lazy training. This leads to fundamentally different compute-optimal scaling laws where training time and model size must be balanced differently than in the lazy regime. The theory is validated through experiments on MLPs fitting functions on the circle and CNNs on MNIST/CIFAR, showing good agreement for hard tasks.

## Method Summary
The authors propose a two-layer linear network model with random features ψ∞(x) ∈ R^M, hidden weights A(t) ∈ R^(N×M), and output weights w(t) ∈ R^N. The model is trained using online SGD for w(t) and projected SGD for A(t), with the projection term A(0)⊤A(0)/N ensuring finite parameter bottlenecks. They derive a dynamical mean-field theory that predicts the loss scaling as L(t) ~ t^(-χ) where χ depends on the task difficulty exponent β. The theory identifies three regimes: hard (β < 1), easy (1 < β < 2 - 1/α), and super-easy (β > 2 - 1/α), with different scaling behaviors in each regime.

## Key Results
- For hard tasks (β < 1), feature learning improves the time scaling exponent from χ = β to χ = 2β/(1+β), nearly doubling the improvement
- The model bottleneck N^(-α min{2,β}) and SGD noise effects (1/B · t^(-(2-1/α))) create different scaling behaviors across the three regimes
- Experiments on MLPs fitting circle functions and CNNs on MNIST/CIFAR validate the theoretical predictions for hard tasks
- Compute-optimal scaling laws differ significantly between lazy and feature learning regimes, requiring different balances of training time and model size

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Feature learning improves scaling exponents specifically for hard tasks (β < 1) by changing the optimization trajectory from the lazy kernel regime.
- **Mechanism**: For hard tasks outside the initial RKHS, the model's effective kernel K(t) diverges as K(t) ∼ t^(1-χ) when β < 1, causing the model to learn progressively higher-order modes k*(t) ∼ t^((2-χ)/α), leading to improved loss scaling L(t) ∼ t^(-χ) where χ = 2β/(1+β) > β.
- **Core assumption**: The finite-width bottleneck A(0)⊤A(0)/N prevents trivial fitting and forces the model to reweigh existing features rather than access infinite-width features directly.
- **Evidence anchors**: [abstract] demonstrates both analytically and empirically that feature learning can improve scaling for hard tasks; [section 4] shows the exponent increases to χ = 2β/(1+β) when β < 1.

### Mechanism 2
- **Claim**: The model's projected gradient descent dynamics with unbalanced learning rates (parameter γ) create a smooth transition between lazy and rich regimes.
- **Mechanism**: The hyperparameter γ controls the speed of feature evolution, transitioning from lazy learning (γ → 0) to feature learning as γ increases. The dynamics balance gradient flow with feature adaptation through the term γw(t)v0(t)⊤ in the A(t) update.
- **Core assumption**: The projection term ensures features can only access the initial N-dimensional features A(0)ψ∞ rather than infinite-width features.
- **Evidence anchors**: [section 2] explains the projection mechanism; [section 3] describes how γ sets the rate of feature evolution.

### Mechanism 3
- **Claim**: SGD noise effects are suppressed for hard tasks due to the dominance of feature learning dynamics over stochastic fluctuations.
- **Mechanism**: For hard tasks (β < 1), feature learning dynamics cause the model to learn progressively higher modes, making SGD noise transient 1/B · t^(-(2-1/α)) subleading compared to the gradient flow term t^(-2β/(1+β)).
- **Core assumption**: Hard tasks are those with β < 1 where the RKHS norm of the target function is infinite, making the model focus on learning the most relevant features.
- **Evidence anchors**: [section 4] shows SGD noise does not significantly alter scaling behavior for hard tasks; [section 6] suggests SGD transients are suppressed for realistic tasks.

## Foundational Learning

- **Concept**: Neural Tangent Kernel (NTK) and its role in distinguishing lazy vs feature learning regimes
  - **Why needed here**: The central insight depends on understanding when the NTK changes during training versus when it remains constant. The source exponent β determines whether a task lies within or outside the initial NTK's RKHS.
  - **Quick check question**: For a target function with Fourier spectrum w_k ~ k^(-q) and an initial kernel with eigenvalues λ_k ~ k^(-2q_ϕ), what condition on q and q_ϕ determines whether the task is "hard" (β < 1) or "easy" (β > 1)?

- **Concept**: Mean-field theory and dynamical mean-field theory (DMFT) for analyzing neural network dynamics
  - **Why needed here**: The paper uses DMFT to derive closed-form predictions for the loss scaling by averaging over random initializations and data samples. This theoretical framework is essential for understanding how the model's predictions emerge from the underlying stochastic dynamics.
  - **Quick check question**: In the DMFT framework, what are the key correlation functions that need to be tracked to compute the test loss, and how do they evolve during training?

- **Concept**: Power-law spectra and source/capacity conditions in kernel regression
  - **Why needed here**: The analysis relies on power-law eigenvalue spectra for both the kernel (λ_k ~ k^(-α)) and the target function (w*_k ~ k^(-β)). These conditions determine the scaling behavior and identify the three regimes based on the relative decay rates.
  - **Quick check question**: Given kernel eigenvalues λ_k ~ k^(-α) and target coefficients w*_k ~ k^(-β), what is the mathematical condition that determines whether the target function has finite RKHS norm?

## Architecture Onboarding

- **Component map**: Random features ψ∞(x) ∈ R^M → Hidden weights A(t) ∈ R^(N×M) → Output weights w(t) ∈ R^N → Loss L(t)

- **Critical path**: Initialize A(0) → Compute initial loss → Update w(t) via SGD → Update A(t) via projected SGD → Compute new loss → Repeat until convergence or compute budget exhausted

- **Design tradeoffs**:
  - Width N vs training time t: Larger N provides better approximation of infinite-width features but increases computational cost; longer t allows more feature learning but may lead to overfitting
  - Learning rate ratio γ: Small γ approaches lazy learning (no feature adaptation); large γ accelerates feature learning but may cause instability
  - Batch size B: Larger B reduces SGD noise but increases per-iteration computational cost; smaller B introduces noise that can help escape local minima

- **Failure signatures**:
  - No improvement in scaling exponent despite increasing γ: Likely β ≥ 1, meaning the task is within the initial RKHS and feature learning provides no benefit
  - Unstable training with large γ: Feature evolution is too aggressive relative to gradient flow, causing oscillations or divergence
  - Poor performance despite good scaling: Model may be underparameterized (N too small) or task may be too difficult (β too small)

- **First 3 experiments**:
  1. **Validate lazy vs feature learning transition**: Train the model with varying γ values on a task with known β < 1. Measure loss scaling exponents and verify transition from t^(-β) to t^(-2β/(1+β)) as γ increases.
  2. **Test regime boundaries**: Train on tasks with β = 0.5, 1.0, and 1.5. Verify that only β < 1 shows improved scaling while β > 1 maintains lazy learning exponents.
  3. **Analyze bottleneck effects**: Train with varying N and B on a hard task (β < 1). Measure how the model bottleneck N^(-α min{2,β}) and SGD noise B^(-1) terms affect the overall scaling law.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Under what precise conditions do adaptive optimizers (like Adam) alter the scaling law exponents compared to vanilla SGD?
- **Basis in paper**: [inferred] The paper mentions that most modern models use cross-entropy loss with adaptive optimizers, but their theory only covers MSE loss with SGD.
- **Why unresolved**: The theoretical framework focuses on SGD dynamics and derives scaling laws specifically for this case. No analysis of adaptive methods is provided.
- **What evidence would resolve it**: Experiments comparing scaling laws for the same architectures and tasks using SGD versus adaptive optimizers, with systematic variation of learning rate schedules and weight decay.

### Open Question 2
- **Question**: How do finite-width effects beyond the linear combination assumption (A(t)ψ∞) affect the predicted scaling laws in practice?
- **Basis in paper**: [explicit] The paper states "our model treats the learned features as linear combinations of the initial features, an assumption which may be violated in finite width neural networks."
- **Why unresolved**: The theoretical model assumes infinite-dimensional features can be perfectly approximated by finite linear combinations, but real networks have finite width.
- **What evidence would resolve it**: Systematic experiments varying network width while keeping other factors constant, measuring deviations from predicted scaling laws, and analyzing the alignment between learned features and the span of initial features.

### Open Question 3
- **Question**: What determines the transition point where CIFAR-5M networks begin to outperform the predicted scaling laws after sufficient training?
- **Basis in paper**: [explicit] The paper observes that "CNN training on CIFAR-5M is initially well described by our new exponent, but eventually achieves a better power law" than predicted.
- **Why unresolved**: The paper identifies this phenomenon but does not explain the mechanism behind the improved performance or characterize when/how this transition occurs.
- **What evidence would resolve it**: Detailed analysis of kernel evolution dynamics during extended training, measurement of feature alignment metrics over time, and identification of specific architectural or data properties that enable this improved scaling.

## Limitations
- The theory relies on the infinite-width limit, but experiments use finite-width models, creating potential discrepancies between theory and practice.
- The projection mechanism in A(t)'s dynamics is mathematically elegant but its practical implications for real neural networks remain unclear.
- SGD noise suppression for hard tasks is predicted theoretically but the conditions under which this holds in practice need further validation.

## Confidence
- **High confidence**: The three-regime classification (hard, easy, super-easy) based on β and α is well-supported by both theory and experiments.
- **Medium confidence**: The mechanism of feature learning improving scaling exponents for hard tasks is theoretically sound but requires more extensive empirical validation across diverse architectures.
- **Low confidence**: The practical implications of the unbalanced learning rates (γ parameter) for controlling the lazy-to-rich transition need more experimental verification.

## Next Checks
1. **Finite-width validation**: Systematically vary model width N in the MLP experiments and verify that the predicted scaling exponents converge to theoretical values as N increases.
2. **Architecture generalization**: Test the predictions on deeper networks (3+ layers) and residual architectures to determine if the two-layer theory generalizes.
3. **Real-world task validation**: Apply the framework to realistic computer vision tasks (e.g., ImageNet classification) and verify whether the hard task scaling improvements hold beyond the controlled synthetic experiments.