---
ver: rpa2
title: A Survey on Transformer Compression
arxiv_id: '2402.05964'
source_url: https://arxiv.org/abs/2402.05964
tags:
- arxiv
- vision
- language
- preprint
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews compression methods for Transformer-based
  models, categorizing them into pruning, quantization, knowledge distillation, and
  efficient architecture design. It highlights that Transformer models' unique architecture,
  featuring alternating attention and feedforward neural network (FFN) modules, requires
  specific compression techniques.
---

# A Survey on Transformer Compression

## Quick Facts
- arXiv ID: 2402.05964
- Source URL: https://arxiv.org/abs/2402.05964
- Authors: Yehui Tang; Yunhe Wang; Jianyuan Guo; Zhijun Tu; Kai Han; Hailin Hu; Dacheng Tao
- Reference count: 40
- Key outcome: Comprehensive survey categorizing transformer compression methods into pruning, quantization, knowledge distillation, and efficient architecture design, highlighting domain-specific requirements and future research directions

## Executive Summary
This survey provides a systematic overview of compression techniques for transformer-based models, which have become essential for deploying large language and vision models on practical devices. The paper categorizes compression methods into four main approaches—pruning, quantization, knowledge distillation, and efficient architecture design—while emphasizing the unique architectural requirements of transformers with their alternating attention and feedforward neural network modules. The survey demonstrates that despite the differences between NLP and CV applications, compression methods share common underlying principles and can be adapted across domains. A key insight is that retraining large models is often impractical, making efficient compression methods crucial for real-world deployment.

## Method Summary
The survey methodology involves systematically categorizing existing transformer compression techniques based on their underlying principles and application domains. Rather than implementing specific compression methods, the authors review and analyze published research across pruning (structured/unstructured, weight-based/activity-based), quantization (post-training/quantization-aware training, symmetric/asymmetric), knowledge distillation (logits-based/hint-based), and efficient architecture design (hybrid architectures, alternative attention mechanisms). The paper examines how these methods apply to both NLP and CV transformers, noting that while the applications differ, the compression techniques often share similar implementation strategies due to the fundamental architectural similarities between transformer variants.

## Key Results
- Transformer compression methods are categorized into four main approaches: pruning, quantization, knowledge distillation, and efficient architecture design
- Despite NLP and CV being treated as different domains, compression methods share common underlying principles due to similar transformer architectures
- Efficient compression is paramount as retraining large models on entire training datasets is usually impractical

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer compression must preserve architectural integrity while reducing redundancy.
- Mechanism: Compression techniques are categorized into pruning, quantization, knowledge distillation, and efficient architecture design, each targeting different aspects of redundancy without breaking the model's alternating attention and FFN module structure.
- Core assumption: The alternating attention and FFN module design is critical to transformer performance and cannot be altered without significant accuracy loss.
- Evidence anchors:
  - [abstract] "Given the unique architecture of Transformer, featuring alternative attention and Feedforward Neural Network (FFN) modules, specific compression techniques are usually required."
  - [section] "Different from other architectures like CNN or RNN, the transformer features a unique design with alternative attention and FFN modules."
  - [corpus] Weak evidence - only 2 of 8 neighbor papers explicitly mention architecture-specific compression needs.
- Break Condition: If a compression method significantly alters the attention-FFN alternation or introduces non-trainable components that disrupt gradient flow.

### Mechanism 2
- Claim: Training efficiency is a critical constraint for large transformer compression.
- Mechanism: Post-training quantization and pruning methods are preferred over quantization-aware training or full fine-tuning due to the high computational cost of retraining large models.
- Core assumption: The original training data is often unavailable or retraining is prohibitively expensive for large models.
- Evidence anchors:
  - [abstract] "The efficiency of these compression methods is also paramount, as retraining large models on the entire training dataset is usually impractical."
  - [section] "Due to the high computational cost of large model, it is usually unaffordable to retrain the whole model on the original training set."
  - [corpus] Moderate evidence - several neighbor papers mention training efficiency as a key consideration.
- Break Condition: If a compression method requires access to original training data or involves retraining that exceeds available computational budget.

### Mechanism 3
- Claim: Compression methods share common underlying principles across NLP and CV domains.
- Mechanism: Despite different applications, both NLP and CV transformers use similar attention mechanisms and FFN structures, allowing compression techniques to be adapted across domains.
- Core assumption: The fundamental operations of attention and FFN are similar enough between NLP and CV transformers that compression techniques can transfer.
- Evidence anchors:
  - [abstract] "For NLP and CV tasks, the compression methods share common underlying principles."
  - [section] "Though NLP and CV are usually treated as very different domains, we observe that their models compression methods actually share the similar principles."
  - [corpus] Strong evidence - all neighbor papers discuss compression across both domains.
- Break Condition: If domain-specific architectural features (e.g., image patches vs. text tokens) require fundamentally different compression approaches.

## Foundational Learning

- Concept: Attention Mechanism
  - Why needed here: Understanding how self-attention works is crucial for grasping why certain compression methods (like pruning attention heads) are effective.
  - Quick check question: What is the computational complexity of standard self-attention, and why does this make it a target for compression?

- Concept: Quantization Fundamentals
  - Why needed here: Quantization is a primary compression method, and understanding the difference between post-training and quantization-aware training is essential for choosing the right approach.
  - Quick check question: What is the key difference between symmetric and asymmetric quantization, and when is each typically used?

- Concept: Knowledge Distillation
  - Why needed here: Knowledge distillation is a key method for transferring knowledge from large models to smaller ones, and understanding the different types (logits-based, hint-based) is important for implementation.
  - Quick check question: What is the main difference between logits-based and hint-based knowledge distillation, and when might one be preferred over the other?

## Architecture Onboarding

- Component map: Input tokens -> Multi-head Attention -> Add & Norm -> Feedforward Neural Network -> Add & Norm -> Output
- Critical path: The self-attention mechanism with quadratic complexity relative to sequence length is the primary bottleneck for transformer inference
- Design tradeoffs: Choosing between different compression methods involves tradeoffs between accuracy, compression ratio, and computational cost; post-training quantization is faster but less accurate than quantization-aware training
- Failure signatures: Common failure modes include significant accuracy degradation when compressing to extremely low bit-widths, and performance issues when pruning too aggressively or without proper recovery techniques
- First 3 experiments:
  1. Implement post-training quantization on a small transformer model (e.g., BERT-base) and measure accuracy loss at different bit-widths
  2. Apply structured pruning to a vision transformer (e.g., ViT) by removing attention heads and evaluate the impact on accuracy and inference speed
  3. Use knowledge distillation to compress a large language model (e.g., GPT-2) into a smaller one, comparing different distillation strategies (logits vs. intermediate features)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal combination of compression methods for achieving extreme compression rates in large transformer models while maintaining performance?
- Basis in paper: [explicit] The paper discusses how different compression methods can be used together to obtain an extremely efficient architecture, mentioning the combination of architecture design, pruning, quantization, and distillation.
- Why unresolved: While the paper acknowledges the potential of combining compression methods, it notes that finding the proper combination strategy via joint search is challenging due to the complexity of transformer architectures and their high computational cost.
- What evidence would resolve it: A systematic study comparing different combinations of compression methods on various transformer models, evaluating both compression rate and performance retention, would provide evidence for the optimal combination strategy.

### Open Question 2
- Question: How can we efficiently prune large transformer models while preserving their multi-task nature and minimizing retraining costs?
- Basis in paper: [explicit] The paper discusses the challenge of pruning large language models, emphasizing the need for training-efficient methods due to the high computational cost of retraining. It mentions the importance of preserving the multi-task nature of pre-trained models.
- Why unresolved: The paper identifies the difficulty of estimating parameter importance with limited data and the need for efficient retraining strategies, but does not provide a definitive solution.
- What evidence would resolve it: Developing and evaluating pruning methods that accurately estimate parameter importance using minimal data and require minimal retraining, while maintaining the model's performance across multiple tasks, would address this open question.

### Open Question 3
- Question: What are the most efficient architectures beyond transformers for handling long sequences in language and vision tasks?
- Basis in paper: [explicit] The paper discusses the quadratic computational complexity of the self-attention mechanism in transformers for long sequences and mentions emerging architectures like RWKV and RetNet as promising alternatives.
- Why unresolved: While the paper highlights the potential of these alternative architectures, it does not provide a comprehensive comparison of their efficiency and performance against transformers across various tasks.
- What evidence would resolve it: Conducting extensive experiments comparing the efficiency, generalization, and scaling ability of transformer-based models with alternative architectures like RWKV, RetNet, and pure MLP models on a wide range of language and vision tasks would provide evidence for the most efficient architectures beyond transformers.

## Limitations
- Limited empirical validation of compression effectiveness across domains
- No quantitative comparison of compression methods' performance trade-offs
- Lack of discussion on domain-specific architectural considerations that might limit method transferability

## Confidence

- **High confidence**: The categorization of compression methods (pruning, quantization, knowledge distillation, efficient architecture design) and their basic mechanisms
- **Medium confidence**: The claim that compression methods share common principles across NLP and CV, based on architectural similarities
- **Low confidence**: The assertion that compression efficiency is paramount without quantitative evidence of practical constraints

## Next Checks

1. Conduct a controlled experiment comparing the same compression method (e.g., structured pruning) on both NLP and CV transformers to quantify performance transferability and identify domain-specific limitations
2. Implement and benchmark post-training quantization versus quantization-aware training on a large transformer model to empirically validate the efficiency claims and accuracy trade-offs
3. Analyze the impact of compressing specific transformer components (attention vs. FFN modules) to determine which architectural elements are most critical for maintaining performance across different applications