---
ver: rpa2
title: 'BayesAdapter: enhanced uncertainty estimation in CLIP few-shot adaptation'
arxiv_id: '2412.09718'
source_url: https://arxiv.org/abs/2412.09718
tags:
- confidence
- cvpr
- clapcvpr
- tipaeccv
- tipa-f-eccv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BayesAdapter, a Bayesian approach to improve
  uncertainty estimation in CLIP adapters for few-shot classification. While existing
  adapters focus on accuracy, BayesAdapter uses variational inference to model a posterior
  distribution over parameters instead of a single point estimate, enabling better
  calibration and selective classification.
---

# BayesAdapter: enhanced uncertainty estimation in CLIP few-shot adaptation

## Quick Facts
- **arXiv ID**: 2412.09718
- **Source URL**: https://arxiv.org/abs/2412.09718
- **Reference count**: 40
- **Primary result**: BayesAdapter achieves significantly better Expected Calibration Error (ECE) and higher test set coverage at high confidence levels while maintaining competitive accuracy in CLIP few-shot classification.

## Executive Summary
BayesAdapter introduces a Bayesian approach to improve uncertainty estimation in CLIP adapters for few-shot classification. Unlike existing adapters that produce single point estimates, BayesAdapter uses variational inference to maintain a posterior distribution over adapter parameters, enabling better calibration and selective classification capabilities. Experiments across 11 diverse datasets demonstrate that BayesAdapter achieves significantly better Expected Calibration Error (ECE) and higher test set coverage at high confidence levels (e.g., 10.53% vs. 0% at 99% confidence) while maintaining competitive accuracy. The method shows improved reliability and diversity in high-confidence predictions compared to state-of-the-art adapters.

## Method Summary
BayesAdapter extends CLIP adapter-based transfer learning by replacing point estimates with Bayesian inference over adapter parameters. The method uses variational inference with a Gaussian posterior distribution to capture parameter uncertainty, trained via the evidence lower bound (ELBO) objective. During inference, Monte Carlo sampling approximates the posterior predictive distribution, enabling uncertainty-aware predictions. The approach maintains a block-diagonal covariance structure for computational tractability while preserving class-wise uncertainty information. BayesAdapter is evaluated on 11 diverse datasets using standard few-shot protocols (1-32 shots per class) with ResNet-50 and ViT-B/16 backbones.

## Key Results
- BayesAdapter achieves significantly better Expected Calibration Error (ECE) than state-of-the-art adapters across 11 datasets (e.g., 0.0047 vs 0.0262 average ECE)
- At 99% confidence threshold, BayesAdapter maintains 10.53% test set coverage versus 0% for deterministic adapters
- BayesAdapter provides more diverse high-confidence predictions compared to deterministic baselines while maintaining competitive accuracy
- The method shows consistent improvements in calibration metrics across all tested shot levels (1-32 shots per class)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian inference over adapter parameters improves uncertainty quantification in CLIP few-shot adaptation.
- Mechanism: Instead of estimating a single point value for adapter weights W, BayesAdapter maintains a posterior distribution q(W) over parameters. This distribution captures the variability and uncertainty inherent in the parameter space, leading to better calibrated predictions and selective classification capabilities.
- Core assumption: The posterior distribution over adapter parameters can be effectively approximated using variational inference with a Gaussian posterior, and this uncertainty in parameters translates to meaningful uncertainty in predictions.
- Evidence anchors:
  - [abstract] "By estimating a probability distribution instead of a point estimate, we can better handle uncertainty quantification in the parameter space. Interestingly, we will show that this leads to enhanced uncertainty estimation in the predictions."
  - [section 3.2] "By performing MAP inference, CLAP estimates a single W. However, this does not capture the uncertainty encoded in the posterior distribution p(W|X, Y). Therefore, we propose to use Bayesian inference to leverage a probability distribution instead of a single value [42, 54]."

### Mechanism 2
- Claim: The variational Bayes approach with block-diagonal covariance enables tractable training while capturing class-wise uncertainty.
- Mechanism: BayesAdapter uses a Gaussian posterior with block-diagonal covariance matrix, where each class has its own variance parameter. This structure allows for efficient computation of the ELBO and enables MC sampling through the reparameterization trick while capturing different uncertainty levels across classes.
- Core assumption: The block-diagonal structure with class-wise variances is sufficient to capture meaningful uncertainty patterns, and the number of MC samples (sMC=3) is adequate for stable training.
- Evidence anchors:
  - [section 3.2] "In our case, we consider a Gaussian posterior distribution with a block-diagonal covariance matrix, similar to the prior. Namely, we consider q(W) = N (W|Ω, Σ), where Ω ∈ RD×C and Σ = diag( σ21,D ···, σ21, ··· , σ2C,D ···, σ2C) are the parameters α to be learned."
  - [section 4.1] "The number of MC samples is set to sMC = 3 in all the experiments. Other values are analyzed in Table 4."

### Mechanism 3
- Claim: The Gaussian prior centered at zero-shot prototypes provides useful regularization that improves adaptation with limited data.
- Mechanism: BayesAdapter initializes the prior mean T with zero-shot prototypes and sets a small prior standard deviation (0.01). This prior acts as regularization that prevents overfitting to limited training data while allowing the model to adapt meaningfully from the zero-shot initialization.
- Core assumption: The zero-shot prototypes contain useful information that should be preserved during adaptation, and the small prior variance provides appropriate regularization without being too restrictive.
- Evidence anchors:
  - [section 3.2] "Regarding the initializations and hyperparameters that are specific to BayesAdapter, the prior mean T is set to the ZS prototypes, since we are inspired by the same CLAP rationale of leveraging the prior knowledge in the textual encoder [47]."
  - [section 4.1] "Regarding the prior variance Λ, the standard deviation for all classes is set to 0.01. We choose this value based on the average empirical performance of the ZS classification, and fix it throughout the experimentation."

## Foundational Learning

**Variational Inference**: A Bayesian method that approximates intractable posterior distributions with simpler parametric families, optimizing the Evidence Lower Bound (ELBO) to find the best approximation. Why needed: Enables tractable Bayesian inference over high-dimensional adapter parameters that would be computationally prohibitive to compute exactly.

**Evidence Lower Bound (ELBO)**: The objective function for variational inference that balances data fit (expected log-likelihood) against model complexity (KL divergence to prior). Why needed: Provides a tractable optimization target that ensures the learned posterior doesn't deviate too far from the prior while fitting the data.

**Monte Carlo Integration**: A technique for approximating expectations under probability distributions by averaging samples. Why needed: Allows computation of posterior predictive distributions and softmax probabilities when the exact integration is intractable.

**Expected Calibration Error (ECE)**: A metric measuring the discrepancy between predicted confidence scores and actual accuracy. Why needed: Quantifies how well predicted probabilities reflect true uncertainty, which is crucial for reliable few-shot classification.

**KL Divergence Annealing**: A training technique that gradually increases the weight of the KL divergence term in the ELBO to stabilize training. Why needed: Prevents posterior collapse to the prior in early training stages when the model hasn't learned meaningful patterns yet.

**Selective Classification**: The practice of only making predictions when confidence exceeds a threshold, trading coverage for reliability. Why needed: Enables safe deployment of few-shot models by avoiding predictions in uncertain regions.

## Architecture Onboarding

**Component Map**: CLIP Vision Encoder -> CLIP Text Encoder -> Adapter Parameters (W) -> BayesAdapter Posterior (q(W)) -> Predictive Distribution -> Classification

**Critical Path**: Input images → CLIP feature extraction → Adapter transformation → Monte Carlo sampling → Posterior predictive → Softmax → Classification

**Design Tradeoffs**: The block-diagonal covariance structure trades off between computational efficiency and the ability to capture full parameter correlations. The choice of 3 MC samples balances training stability against computational cost.

**Failure Signatures**: Poor calibration indicates posterior collapse or inadequate training. Very low coverage at moderate confidence thresholds suggests over-conservative uncertainty estimates. Performance degradation with very few shots indicates insufficient data to learn meaningful posteriors.

**First Experiments**:
1. Compare BayesAdapter ECE vs deterministic adapters on a single dataset (e.g., Caltech101) with varying shot counts
2. Analyze posterior variance evolution during training to verify learning meaningful uncertainty
3. Evaluate coverage-accuracy trade-off at different confidence thresholds to assess selective classification capabilities

## Open Questions the Paper Calls Out
The paper identifies that the main limitation of the proposed approach is that it struggles to fully exploit its richness in the presence of very few shots per-class, e.g., 1-shot. The authors suggest that the amount of training data is known before adaptation, which could be a criterion to decide whether to use the deterministic or probabilistic formulation.

## Limitations
- Performance degrades with extremely limited data (1-shot per class) where the Bayesian framework cannot fully exploit its advantages
- Computational overhead is higher than deterministic adapters due to Monte Carlo sampling and maintaining posterior distributions
- The block-diagonal covariance structure may be too restrictive to capture important correlations between parameters

## Confidence
- **High confidence** in accuracy claims: The reported accuracy improvements are modest and align with typical adapter performance ranges
- **Medium confidence** in calibration improvements: The ECE improvements are substantial but calibration metrics can be sensitive to implementation details
- **Medium confidence** in coverage claims: The dramatic differences in test set coverage are compelling but depend heavily on confidence calibration quality

## Next Checks
1. Perform prior sensitivity analysis by varying prior standard deviation (0.001, 0.01, 0.1, 1.0) to determine robustness to this hyperparameter
2. Implement BayesAdapter on a different VLM (e.g., BLIP-2 or LLaVA) using the same datasets to assess generalizability
3. Analyze shot-level performance breakdown (1, 2, 4, 8, 16, 32 shots) to identify optimal ranges for BayesAdapter's benefits