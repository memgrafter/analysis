---
ver: rpa2
title: Application-Driven Innovation in Machine Learning
arxiv_id: '2403.17381'
source_url: https://arxiv.org/abs/2403.17381
tags:
- learning
- research
- machine
- adml
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the under-valuation of application-driven machine
  learning (ADML) research within the ML community. It contrasts ADML with the dominant
  methods-driven research paradigm, highlighting how ADML focuses on real-world problems,
  application-specific evaluation metrics, and domain knowledge integration.
---

# Application-Driven Innovation in Machine Learning

## Quick Facts
- arXiv ID: 2403.17381
- Source URL: https://arxiv.org/abs/2403.17381
- Authors: David Rolnick; Alan Aspuru-Guzik; Sara Beery; Bistra Dilkina; Priya L. Donti; Marzyeh Ghassemi; Hannah Kerner; Claire Monteleoni; Esther Rolf; Milind Tambe; Adam White
- Reference count: 20
- Primary result: ADML research addresses real-world problems, fosters innovation, and requires better recognition and support within the ML community

## Executive Summary
This paper argues that application-driven machine learning (ADML) is under-valued within the ML community despite its significant contributions to innovation and practical impact. ADML focuses on real-world problems, application-specific evaluation metrics, and domain knowledge integration, contrasting with the dominant methods-driven research paradigm that prioritizes standardized benchmarks and generalizable algorithms. The authors identify systemic barriers in reviewing, hiring, and teaching practices that disadvantage ADML work and propose solutions including fairer evaluation criteria, interdisciplinary hiring practices, and education reforms.

## Method Summary
The paper presents a conceptual framework contrasting ADML with methods-driven research, identifying three key mechanisms by which ADML drives innovation: generalizability of solutions across domains, novel evaluation metric development, and diversification of research directions. It analyzes systemic barriers in academic and industrial settings that hinder ADML recognition and proposes solutions for each barrier. The methodology involves literature review, analysis of current practices, and identification of gaps in support structures for ADML research.

## Key Results
- ADML innovations can generalize across seemingly unrelated domains due to shared underlying structure in real-world problems
- ADML creates novel evaluation metrics and benchmarks that better capture real-world performance requirements
- ADML research diversifies ML innovation by preventing the community from falling into monolithic research patterns
- Systemic barriers in reviewing, hiring, and teaching practices disadvantage ADML work despite its practical importance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADML innovations can generalize across seemingly unrelated domains because real-world problems share more underlying structure than standardized benchmarks.
- Mechanism: When algorithms are designed to solve specific real-world challenges with domain constraints, they often capture universal principles that apply broadly.
- Core assumption: Real-world tasks have structural commonalities that transcend their surface differences.
- Evidence anchors:
  - [abstract] "Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself."
  - [section] "Methods originally tailored for a specific problem have often proven useful to a variety of seemingly dissimilar problems."
  - [corpus] Found 25 related papers; however, no direct citations or h-index data available, suggesting this claim needs further empirical validation.
- Break condition: If domain-specific constraints turn out to be too specialized to extract generalizable principles.

### Mechanism 2
- Claim: ADML research creates novel evaluation metrics and benchmarks that better capture real-world performance requirements.
- Mechanism: By working directly with stakeholders, ADML researchers identify and formalize success criteria that methods-driven research overlooks.
- Core assumption: Standardized benchmarks inadequately represent real-world success conditions.
- Evidence anchors:
  - [section] "One of the key reasons that methods-driven ML fails to be impactful in some practical applications is that the set of appropriate evaluation metrics varies greatly between real-world use cases."
  - [section] "An increasing number of application-specific benchmarks... have been created, often with considerable time and effort."
  - [corpus] Weak evidence - corpus lacks citations on benchmark development, suggesting this mechanism may be under-researched.
- Break condition: If application-specific metrics cannot be generalized or standardized for broader ML community adoption.

### Mechanism 3
- Claim: ADML research diversifies ML innovation by preventing the community from falling into monolithic patterns of research.
- Mechanism: ADML introduces problems and constraints that challenge existing assumptions and create new research directions.
- Core assumption: ML research trends are self-reinforcing and benefit from external challenges.
- Evidence anchors:
  - [section] "ADML has the potential to diversify research directions. Machine learning has often been steered by trends and hype cycles..."
  - [section] "The use cases raised in ADML can also tie in to broad challenges considered in methods-driven research, or may raise new challenges that can be abstracted away from the task at hand."
  - [corpus] No direct evidence in corpus - this claim relies entirely on internal paper arguments.
- Break condition: If ADML research becomes so niche that it fails to influence broader ML community practices.

## Foundational Learning

- Concept: Real-world data characteristics vs. benchmark data
  - Why needed here: ADML researchers must understand why standard benchmarks fail to capture real-world complexity
  - Quick check question: What are three key differences between real-world datasets and standardized benchmarks?

- Concept: Stakeholder collaboration and translation
  - Why needed here: ADML requires effective communication between ML researchers and domain experts
  - Quick check question: What are the key steps in translating stakeholder needs into ML research objectives?

- Concept: Evaluation metric design for specific applications
  - Why needed here: Different applications require different success criteria beyond accuracy
  - Quick check question: How would you design evaluation metrics for a medical imaging application versus a satellite imagery application?

## Architecture Onboarding

- Component map: Problem framing -> Data acquisition -> Algorithm design with domain constraints -> Stakeholder validation -> Real-world deployment
- Critical path: Identify real-world problem → Understand domain constraints → Design appropriate metrics → Collect/prepare domain-specific data → Develop algorithms → Validate with stakeholders → Iterate
- Design tradeoffs: ADML trades off generalizability for specificity, accepting that methods may not perform well on standard benchmarks but excel in target domains
- Failure signatures: Methods fail when they ignore domain constraints, use inappropriate evaluation metrics, or cannot be validated by stakeholders
- First 3 experiments:
  1. Reimplement a standard algorithm on a real-world dataset from a target domain and document performance gaps
  2. Interview stakeholders to identify key success criteria and translate them into measurable metrics
  3. Design and test a domain-constrained variant of an existing algorithm on both standard benchmarks and real-world data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective mechanisms to incentivize and reward application-driven ML research within traditional ML publication venues?
- Basis in paper: [explicit] The paper discusses systemic barriers in reviewing practices where ADML papers face common criticisms like "unfamiliar benchmarks" and "too simple," and suggests reviewer guidelines and diverse reviewer pools as potential solutions
- Why unresolved: While the paper proposes solutions like special tracks and reviewer guidelines, it doesn't provide empirical evidence of which mechanisms most effectively change reviewer behavior or publication outcomes
- What evidence would resolve it: Comparative studies of publication acceptance rates before and after implementing specific ADML-focused review mechanisms, or controlled experiments testing different reviewer training approaches

### Open Question 2
- Question: How can machine learning education be restructured to effectively balance methods-driven and application-driven approaches without sacrificing depth in either area?
- Basis in paper: [explicit] The paper identifies that current teaching practices predominantly emphasize methods-driven work, creating a self-perpetuating cycle, and suggests exposing students to both paradigms
- Why unresolved: The paper proposes curriculum changes but doesn't provide evidence on optimal balance points, integration strategies, or how to measure educational outcomes across both paradigms
- What evidence would resolve it: Longitudinal studies tracking career outcomes and research contributions of students from programs with different ADML integration levels, or controlled experiments testing various pedagogical approaches

### Open Question 3
- Question: What organizational structures and support systems most effectively enable interdisciplinary collaboration between ML researchers and domain experts?
- Basis in paper: [explicit] The paper discusses the importance of collaboration in ADML and suggests building frameworks for interdisciplinary work and providing support for data engineering teams
- Why unresolved: While the paper recommends certain support structures, it doesn't empirically evaluate which organizational models (e.g., embedded researchers, dedicated liaison roles, shared funding mechanisms) prove most effective
- What evidence would resolve it: Comparative studies of different institutional collaboration models and their impact on research output quality, speed, and real-world deployment success rates

## Limitations

- Evidence base relies heavily on internal reasoning rather than empirical validation, with key claims lacking citation support
- No directly relevant citations found in corpus search, suggesting either literature gaps or search methodology limitations
- Mechanisms proposed for why ADML works are plausible but not rigorously tested
- Limited evidence provided on the actual impact of systemic barriers on innovation rates or researcher careers

## Confidence

- Medium: The characterization of ADML vs methods-driven paradigms
- Low: Claims about ADML's ability to diversify ML research and create generalizable innovations
- Low: Evidence that current systemic barriers significantly impede ADML research

## Next Checks

1. Conduct citation analysis to identify concrete examples of ADML innovations that successfully generalized to other domains
2. Survey ML researchers and practitioners to quantify the impact of systemic barriers on ADML work adoption and career progression
3. Implement pilot programs for the proposed solutions (e.g., modified evaluation criteria at conferences) and measure their effect on ADML paper acceptance rates and researcher participation