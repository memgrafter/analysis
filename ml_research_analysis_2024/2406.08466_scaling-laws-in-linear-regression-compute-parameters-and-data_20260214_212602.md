---
ver: rpa2
title: 'Scaling Laws in Linear Regression: Compute, Parameters, and Data'
arxiv_id: '2406.08466'
source_url: https://arxiv.org/abs/2406.08466
tags:
- neff
- lemma
- have
- proof
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the theoretical foundation of neural scaling
  laws, which describe how test error improves polynomially with model and data size.
  The authors address the puzzle of why variance error, which increases with model
  size in standard statistical learning theory, does not appear in empirical neural
  scaling laws.
---

# Scaling Laws in Linear Regression: Compute, Parameters, and Data

## Quick Facts
- arXiv ID: 2406.08466
- Source URL: https://arxiv.org/abs/2406.08466
- Reference count: 0
- The paper provides theoretical foundation for neural scaling laws, showing that variance error is suppressed by implicit regularization in one-pass SGD, making it disappear from empirical scaling law observations.

## Executive Summary
This paper provides a theoretical foundation for neural scaling laws by analyzing infinite-dimensional linear regression with sketched covariates. The key insight is that while standard statistical learning theory predicts variance should increase with model size, one-pass SGD with geometric decaying stepsizes introduces implicit regularization that suppresses this variance term. This explains why empirical neural scaling laws appear to ignore variance error despite its theoretical presence. The analysis shows that under power-law spectrum assumptions, the reducible part of test error follows the scaling law Θ(M^-(a-1) + N^-(a-1)/a), matching empirical observations in deep learning.

## Method Summary
The paper analyzes infinite-dimensional linear regression where M-dimensional sketched covariates are learned using one-pass SGD with N data points. The setup assumes a Gaussian prior on the optimal parameter and a power-law spectrum for the data covariance matrix. The key innovation is analyzing how implicit regularization from one-pass SGD with geometrically decaying stepsizes affects the variance error term in the risk decomposition. The analysis focuses on how the reducible part of test error scales with model size M and dataset size N, showing that variance error is dominated by approximation and bias errors under these conditions.

## Key Results
- The reducible part of test error follows scaling law Θ(M^-(a-1) + N^-(a-1)/a) under power-law spectrum assumptions
- Variance error, which typically increases with model size in standard theory, is suppressed by implicit regularization from one-pass SGD
- The variance term is of strictly higher order (min{M, (Nγ)^1/a}/N) compared to dominant terms, making it nearly unobservable empirically
- When tasks are relatively easy (source condition with b > a), optimal stepsize selection can maintain clean scaling laws

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The variance error term that typically increases with model size in standard learning theory is suppressed by implicit regularization from one-pass SGD, making it disappear from the scaling law.
- **Mechanism:** One-pass SGD with geometrically decaying stepsizes introduces implicit regularization that controls the variance. The variance error is of strictly higher order compared to the dominant terms (M^(-(a-1)) + N^(-(a-1)/a)), making it nearly unobservable when fitting scaling laws empirically.
- **Core assumption:** The optimal parameter follows a Gaussian prior and the data covariance has a power-law spectrum with degree a > 1.
- **Break condition:** If the data spectrum deviates significantly from power-law assumptions, or if multi-pass SGD is used, or if explicit regularization is removed.

### Mechanism 2
- **Claim:** The reducible part of test error follows a scaling law Θ(M^-(a-1) + N^-(a-1)/a) under power-law spectrum assumptions.
- **Mechanism:** Under a power-law spectrum with degree a > 1, the eigenvalues of the data covariance matrix follow λ_i ≂ i^(-a). The approximation error scales as M^(1-a), the bias error scales as (Nγ)^(1/a-1), and the variance error is suppressed.
- **Core assumption:** The eigenvalues of the data covariance matrix satisfy a power-law of degree a > 1.
- **Break condition:** If the power-law spectrum assumption is violated, or if a ≤ 1, or if the model is under-specified relative to data complexity.

### Mechanism 3
- **Claim:** The variance error can be controlled through optimal stepsize selection, allowing clean scaling laws even in easier tasks (source condition with b > a).
- **Mechanism:** When the optimal parameter satisfies an anisotropic prior (source condition with exponent b), the variance error can be suppressed by appropriately tuning the stepsize γ. For tasks with b > a, optimal stepsize selection can make the variance term negligible compared to approximation and bias errors.
- **Core assumption:** The optimal parameter satisfies a source condition with 1 < b < a + 1.
- **Break condition:** If b ≥ a + 1, where gaps appear between upper and lower bounds, or if stepsize tuning is not possible.

## Foundational Learning

- **Concept:** Power-law spectrum of data covariance
  - **Why needed here:** The paper's main theoretical results depend on the eigenvalues of the data covariance matrix following a power-law decay λ_i ≂ i^(-a).
  - **Quick check question:** What happens to the scaling law if the eigenvalue decay is exponential instead of power-law?

- **Concept:** Implicit regularization through SGD
  - **Why needed here:** The paper attributes the disappearance of variance error from the scaling law to implicit regularization effects of one-pass SGD.
  - **Quick check question:** How would the scaling law change if we used batch gradient descent instead of one-pass SGD?

- **Concept:** Bias-variance decomposition in high-dimensional settings
  - **Why needed here:** The paper shows that while standard theory predicts variance should increase with model size, in this setup the variance is dominated by other terms.
  - **Quick check question:** Under what conditions does variance dominate approximation error in high-dimensional linear regression?

## Architecture Onboarding

- **Component map:** Data generation -> Sketched linear regression model -> One-pass SGD trainer -> Risk computation -> Scaling law fitter
- **Critical path:** 1. Generate synthetic data satisfying Assumptions 1 and 2 2. Train sketched linear model using one-pass SGD 3. Compute risk components and verify scaling relationships 4. Compare empirical results with theoretical predictions
- **Design tradeoffs:** Using Gaussian sketches vs. other sketching methods (simplicity vs. generality), one-pass vs. multi-pass SGD (variance control vs. convergence speed), geometric decaying vs. fixed stepsizes (implicit regularization strength vs. simplicity)
- **Failure signatures:** Risk doesn't follow power-law when plotted on log-log scale (spectrum assumption violated), variance term becomes significant when increasing model size (implicit regularization insufficient), bias dominates even with large data (model under-specified or stepsize too large)
- **First 3 experiments:** 1. Verify that risk follows power-law in M when N is fixed and spectrum is power-law 2. Test that variance term becomes negligible when using one-pass SGD vs multi-pass 3. Demonstrate that optimal stepsize selection can suppress variance in source condition cases

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do neural scaling laws behave when the optimal model parameters follow an anisotropic prior distribution?
- **Basis in paper:** Explicit - The paper analyzes anisotropic priors in Theorem 4.2 and shows they lead to different scaling exponents (b) compared to isotropic priors.
- **Why unresolved:** The paper only analyzes anisotropic priors for specific values of b relative to a, but doesn't fully characterize the regime where b ≥ a+1.
- **What evidence would resolve it:** Complete theoretical analysis of the bias-variance decomposition for all values of b ≥ a+1, with matching upper and lower bounds on the excess risk.

### Open Question 2
- **Question:** How does the choice of sketching method (beyond Gaussian) affect neural scaling laws?
- **Basis in paper:** Explicit - The paper states their results can be extended to other sketching methods but only proves results for Gaussian sketching.
- **Why unresolved:** The paper only proves theoretical results for Gaussian sketches, leaving open how other sketching methods (e.g., subsampled randomized Hadamard transform, count-sketch) affect the scaling laws.
- **What evidence would resolve it:** Theoretical analysis of neural scaling laws under different sketching methods, showing how the sketching technique affects the exponents in the risk bound.

### Open Question 3
- **Question:** How does multiple-pass SGD training affect the neural scaling laws?
- **Basis in paper:** Explicit - The paper mentions that Muennighoff et al. (2023) found clean scaling laws no longer hold with multiple passes, but doesn't analyze this theoretically.
- **Why unresolved:** The paper only analyzes one-pass SGD, leaving open how the number of passes affects the variance error and overall scaling behavior.
- **What evidence would resolve it:** Theoretical analysis of neural scaling laws for SGD with k passes (k > 1), showing how the variance error changes with the number of passes and how this affects the overall risk bound.

## Limitations
- Strong dependence on power-law spectrum assumption for data covariance eigenvalues, which may not hold for real-world data
- Analysis is limited to infinite-dimensional linear regression, which may not capture all complexities of finite-dimensional neural networks
- The implicit regularization effect of one-pass SGD may vary with different optimization schedules or architectures

## Confidence
- **High confidence:** The decomposition of risk into approximation, bias, and variance terms follows standard statistical learning theory
- **Medium confidence:** The specific scaling law exponents Θ(M^-(a-1) + N^-(a-1)/a) are correctly derived under the stated assumptions
- **Medium confidence:** The claim that variance error disappears from scaling laws due to implicit regularization, though empirical verification across diverse datasets would strengthen this

## Next Checks
1. Test the scaling law predictions on real-world datasets with varying power-law characteristics to validate the theoretical assumptions empirically
2. Compare one-pass SGD scaling behavior with multi-pass SGD to isolate the implicit regularization effect on variance suppression
3. Extend the analysis to non-Gaussian data distributions and alternative sketching methods to assess the robustness of the scaling law predictions