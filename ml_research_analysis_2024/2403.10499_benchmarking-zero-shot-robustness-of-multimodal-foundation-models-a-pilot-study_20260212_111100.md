---
ver: rpa2
title: 'Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot
  Study'
arxiv_id: '2403.10499'
source_url: https://arxiv.org/abs/2403.10499
tags:
- label
- clip
- robustness
- distribution
- imagenet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark for evaluating the
  zero-shot robustness of multimodal foundation models, focusing on CLIP as a pilot
  study. The benchmark covers 7 natural, 3 synthetic distribution shifts, and 11 adversarial
  attacks.
---

# Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study

## Quick Facts
- **arXiv ID**: 2403.10499
- **Source URL**: https://arxiv.org/abs/2403.10499
- **Reference count**: 40
- **One-line primary result**: CLIP significantly underperforms compared to supervised models on synthetic distribution shifts and adversarial attacks, with an average accuracy drop of 11.8%.

## Executive Summary
This paper presents a comprehensive benchmark for evaluating the zero-shot robustness of multimodal foundation models, focusing on CLIP as a pilot study. The benchmark covers 7 natural distribution shifts, 3 synthetic distribution shifts, and 11 adversarial attacks. Results show that CLIP significantly underperforms compared to supervised ImageNet models on synthetic distribution shifts and adversarial attacks. Notably, CLIP exhibits a 34.7% performance drop on new typographic attack datasets. Data overlap analysis suggests that CLIP's reported robustness on natural distribution shifts may be partly attributed to data contamination, highlighting the need for comprehensive robustness evaluation and improvement of zero-shot multimodal models.

## Method Summary
The study benchmarks CLIP's zero-shot robustness using a comprehensive evaluation framework that includes 7 natural distribution shifts (ImageNetV2, ImageNet-R, ObjectNet, ImageNet-Sketch, ImageNet-A, Youtube-BB, ImageNet-Vid), 3 synthetic distribution shifts (ImageNet-C, ImageNet-P, Stylized ImageNet), and 11 adversarial attacks (FGSM, DeepFool, BIM, MIM, DIM, NES, SPSA). The evaluation compares CLIP against supervised ImageNet models using manual prompts and an automated prompt generation method called CLIP-Auto. The paper introduces new typographic attack datasets (ImageNet-T, CIFAR-10-T) and measures effective robustness and relative robustness metrics to assess model performance across different shift types.

## Key Results
- CLIP exhibits a 34.7% performance drop on new typographic attack datasets, significantly worse than supervised models
- Data overlap analysis reveals that CLIP's robustness on natural distribution shifts may be partly attributed to pre-training data contamination
- CLIP-Auto, which uses automated prompt generation, does not significantly improve robustness over CLIP across any test sets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Zero-shot multimodal models like CLIP rely heavily on pre-training data overlap with test sets for perceived robustness on natural distribution shifts.
- **Mechanism**: CLIP's pre-training on massive web data unintentionally includes images similar to test datasets (e.g., ImageNetV2), leading to inflated robustness metrics under natural distribution shifts.
- **Core assumption**: CLIP's pre-training dataset (YFCC100M subset) contains images similar to robustness test sets, causing data contamination.
- **Evidence anchors**:
  - [abstract]: "data overlap analysis suggests that CLIP's reported robustness on natural distribution shifts may be partly attributed to data contamination."
  - [section]: "While performance drops on ImageNetV2, we see an accuracy improvement on Stylized ImageNet. This indicates that the natural distribution shift benefits from the data overlap as the pre-training set contains similar images."
  - [corpus]: Weak evidence. Corpus papers focus on CLIP robustness but do not address data contamination in detail.
- **Break condition**: If CLIP is retrained on cleaned data without overlap, robustness on natural distribution shifts decreases significantly.

### Mechanism 2
- **Claim**: CLIP's vulnerability to typographic attacks stems from multimodal neurons responding to both images and text.
- **Mechanism**: CLIP learns to associate visual concepts with textual descriptions, making it susceptible to misclassification when adversarial text is added to images.
- **Core assumption**: CLIP's architecture inherently links text and image representations, causing misclassification under typographic attacks.
- **Evidence anchors**:
  - [abstract]: "CLIP exhibits a 34.7% performance drop on new typographic attack datasets."
  - [section]: "Different from standard models, multimodal CLIP learns to respond to both images and text given a concept. Adding adversarial text to images can fool the CLIP models."
  - [corpus]: Weak evidence. Corpus papers do not discuss typographic attacks specifically.
- **Break condition**: If CLIP is modified to ignore textual inputs during image classification, vulnerability to typographic attacks decreases.

### Mechanism 3
- **Claim**: Prompt engineering has limited impact on CLIP's robustness compared to pre-trained image representations.
- **Mechanism**: CLIP's robustness is primarily determined by the quality of pre-trained image features, while learned prompts only synthesize classifiers on top of these features.
- **Core assumption**: Image representations learned during pre-training are the key factor in CLIP's robustness, not the prompts used for classification.
- **Evidence anchors**:
  - [abstract]: "Data overlap analysis suggests that CLIP's reported robustness on natural distribution shifts may be partly attributed to data contamination."
  - [section]: "We find that the performance of CLIP and CLIP-Auto are comparable, again suggesting that the impact of CLIP models on the robustness is limited."
  - [corpus]: Weak evidence. Corpus papers focus on CLIP's zero-shot capabilities but do not compare robustness with and without prompt engineering.
- **Break condition**: If CLIP is evaluated with and without prompt engineering, robustness remains similar, confirming limited impact of prompts.

## Foundational Learning

- **Concept**: Distribution shifts in image classification.
  - **Why needed here**: Understanding different types of distribution shifts (natural vs. synthetic) is crucial for evaluating CLIP's robustness.
  - **Quick check question**: What is the difference between natural and synthetic distribution shifts in image classification?

- **Concept**: Adversarial attacks and their impact on model robustness.
  - **Why needed here**: Evaluating CLIP's vulnerability to adversarial attacks is essential for assessing its real-world applicability.
  - **Quick check question**: How do adversarial attacks differ from natural distribution shifts in terms of their impact on model robustness?

- **Concept**: Data contamination and its effects on model evaluation.
  - **Why needed here**: Recognizing data contamination is vital for accurately interpreting robustness results and avoiding inflated performance metrics.
  - **Quick check question**: What is data contamination, and how can it affect the evaluation of a model's robustness?

## Architecture Onboarding

- **Component map**: CLIP consists of an image encoder (ResNet or Vision Transformer) and a text encoder (Transformer). The image encoder extracts features from images, while the text encoder generates classifiers based on textual descriptions.
- **Critical path**: Pre-training on large-scale image-text pairs → Feature extraction via image encoder → Classifier synthesis via text encoder → Zero-shot classification on downstream tasks.
- **Design tradeoffs**: CLIP trades off task-specific fine-tuning for zero-shot generalization, relying on pre-trained representations and learned prompts.
- **Failure signatures**: Significant performance drops on synthetic distribution shifts and adversarial attacks indicate limitations in CLIP's robustness.
- **First 3 experiments**:
  1. Evaluate CLIP's performance on natural distribution shifts (e.g., ImageNetV2) to confirm data overlap effects.
  2. Test CLIP's vulnerability to typographic attacks by adding adversarial text to images and measuring misclassification rates.
  3. Compare CLIP's robustness with and without prompt engineering to assess the impact of learned prompts on performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does data overlap explain the robustness gap between zero-shot multimodal models and supervised models on natural distribution shifts?
- **Basis in paper**: [explicit] The authors find that CLIP's robustness on natural distribution shifts may be partly attributed to data overlap, as similar or identical images from ImageNetV2 were found in the pre-training data via Google Images searches.
- **Why unresolved**: The authors note that the entire pre-training dataset has not been released, so the data overlap analysis is based on a subset (YFCC100M). The extent of data overlap in the full pre-training set remains unknown.
- **What evidence would resolve it**: Releasing the full pre-training dataset would allow a rigorous measurement of data overlap between the pre-training data and the robustness test sets. This would determine if data overlap explains CLIP's robustness on natural distribution shifts.

### Open Question 2
- **Question**: Can automated prompt generation improve the robustness of zero-shot multimodal models like CLIP?
- **Basis in paper**: [explicit] The authors find that CLIP-Auto, which uses automated prompt generation, does not significantly improve robustness over CLIP on any of the test sets, including natural distribution shifts, synthetic distribution shifts, and adversarial attacks.
- **Why unresolved**: The authors suggest that the key to CLIP's performance is the pre-trained image representations, while the impact of language prompts is limited. However, it remains unclear if more advanced prompt generation techniques could improve robustness.
- **What evidence would resolve it**: Testing CLIP-Auto and other prompt generation methods on a wider range of robustness test sets, including new types of adversarial attacks, could determine if automated prompts can improve zero-shot robustness.

### Open Question 3
- **Question**: How can the robustness of zero-shot multimodal models be improved on synthetic distribution shifts and adversarial attacks?
- **Basis in paper**: [explicit] The authors find that CLIP significantly underperforms compared to supervised models on synthetic distribution shifts and adversarial attacks, with an average accuracy drop of 11.8%. CLIP is particularly vulnerable to typographic attacks, with a 34.7% performance drop.
- **Why unresolved**: The authors suggest that the pre-training stage does not include synthetic distribution shifts or adversarial examples, and is not robust to these types of attacks. However, it remains unclear what techniques could be used to improve zero-shot robustness on these test sets.
- **What evidence would resolve it**: Experimenting with different pre-training objectives, data augmentation techniques, or adversarial training methods could determine how to improve zero-shot robustness on synthetic distribution shifts and adversarial attacks.

## Limitations

- The study focuses exclusively on CLIP as a representative multimodal foundation model, limiting generalizability to other models like ALIGN or Flamingo.
- Data contamination analysis relies on indirect evidence rather than direct dataset comparison, leaving uncertainty about the actual extent of overlap.
- New typographic attack datasets lack extensive validation against human perception baselines, raising questions about their ecological validity.

## Confidence

- **High Confidence**: CLIP significantly underperforms on synthetic distribution shifts and adversarial attacks compared to supervised models.
- **Medium Confidence**: Data overlap contributes to CLIP's perceived robustness on natural distribution shifts.
- **Low Confidence**: CLIP's typographic attack vulnerability is primarily due to multimodal neurons responding to both images and text.

## Next Checks

1. **Cross-model validation**: Evaluate ALIGN, Flamingo, or other multimodal models on the same benchmark to determine if CLIP's performance patterns are representative of the broader class of zero-shot multimodal models.

2. **Direct data overlap analysis**: Conduct a rigorous comparison between CLIP's pre-training dataset and the robustness test sets to quantify actual image overlap, rather than relying on performance pattern correlations.

3. **Typographic attack mechanism isolation**: Design experiments that systematically disable CLIP's text processing capabilities (e.g., using only the image encoder) to confirm whether multimodal text-image interaction is the primary cause of typographic attack vulnerability.