---
ver: rpa2
title: 'TrajDeleter: Enabling Trajectory Forgetting in Offline Reinforcement Learning
  Agents'
arxiv_id: '2404.12530'
source_url: https://arxiv.org/abs/2404.12530
tags:
- unlearning
- uni00000013
- agent
- agents
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TRAJDELETER, the first practical trajectory-level
  unlearning method designed specifically for offline reinforcement learning (RL)
  agents. The core idea is to guide the agent to demonstrate deteriorating performance
  when encountering states associated with unlearning trajectories while maintaining
  original performance for remaining trajectories.
---

# TrajDeleter: Enabling Trajectory Forgetting in Offline Reinforcement Learning Agents

## Quick Facts
- **arXiv ID**: 2404.12530
- **Source URL**: https://arxiv.org/abs/2404.12530
- **Reference count**: 40
- **Key result**: Introduces first practical trajectory-level unlearning method for offline RL, achieving 94.8% unlearning effectiveness with only 1.5% of retraining time

## Executive Summary
This paper introduces TRAJDELETER, the first practical trajectory-level unlearning method specifically designed for offline reinforcement learning agents. The method addresses the critical need to remove specific trajectories from trained RL models without requiring complete retraining. TRAJDELETER operates in two phases: a "forgetting" phase that minimizes value function for unlearning samples while maximizing it for remaining samples, followed by a "convergence training" phase that stabilizes the unlearned agent. To evaluate effectiveness, the paper introduces TRAJAUDITOR, a simple yet efficient auditing method that uses shadow agents and state perturbations to verify unlearning success. Experiments across six offline RL algorithms on three tasks demonstrate that TRAJDELETER requires only 1.5% of retraining time while effectively unlearning an average of 94.8% of targeted trajectories.

## Method Summary
TRAJDELETER is a two-phase trajectory unlearning framework for offline RL agents. In the "forgetting" phase, it minimizes the value function Qπ′ for unlearning samples (Df) while maximizing it for remaining samples (Dm), causing the agent to make suboptimal decisions on unlearning trajectories. The "convergence training" phase then fine-tunes the unlearned agent to ensure policy stability by minimizing discrepancies between the unlearned and original agents' value functions on remaining trajectories. TRAJAUDITOR evaluates unlearning success by creating shadow agents through fine-tuning, using state perturbations to generate diverse auditing bases, and comparing value distributions between unlearned and shadow agents. The method achieves effective unlearning with minimal computational overhead compared to full retraining.

## Key Results
- Achieves 94.8% average unlearning effectiveness across six offline RL algorithms
- Requires only 1.5% of the time needed for complete model retraining
- Successfully defends against trajectory poisoning attacks by degrading performance on poisoned trajectories
- Maintains strong performance on remaining trajectories after unlearning
- Demonstrates effectiveness across multiple benchmark tasks including Hopper, HalfCheetah, and Walker2d

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unlearning works by degrading value function for unlearning trajectories while preserving it for remaining ones.
- Mechanism: The "forgetting" phase minimizes the value function Qπ′ for unlearning samples (Df) while maximizing it for remaining samples (Dm). This causes the agent to make suboptimal decisions when encountering states associated with unlearning trajectories.
- Core assumption: Value function Q represents the expected cumulative reward and directly correlates with agent performance on trajectories.
- Evidence anchors:
  - [abstract]: "guide the agent to demonstrate deteriorating performance when it encounters states associated with unlearning trajectories"
  - [section V-B]: "minimizes the value function Q for the unlearning samples" and "maximizes Q on remaining samples simultaneously"
- Break condition: If value function doesn't correlate with actual performance, or if the balance between minimizing and maximizing is incorrect.

### Mechanism 2
- Claim: Convergence training ensures stability after forgetting by aligning value functions between unlearned and original agents.
- Mechanism: The "convergence training" phase minimizes the difference between value functions of the unlearned agent (π′) and the original agent (π) on remaining trajectories (Dm). This fine-tuning ensures the unlearned agent converges to a stable policy.
- Core assumption: Reducing value function differences between original and unlearned agents will stabilize the unlearned agent's policy.
- Evidence anchors:
  - [section V-B]: "minimizes the discrepancies in cumulative rewards obtained by following the original and unlearned agents"
  - [section V-D]: "minimizing the difference between the value functions of the unlearned policy π′ and the fixed original policy π"
- Break condition: If value function alignment doesn't translate to policy stability, or if convergence training introduces new instability.

### Mechanism 3
- Claim: TRAJAUDITOR effectively evaluates unlearning by comparing value distributions between unlearned and shadow agents.
- Mechanism: TRAJAUDITOR creates shadow agents by fine-tuning the original agent, then uses state perturbations to generate diverse auditing bases. It compares cumulative rewards (value vectors) from unlearning trajectories between unlearned and shadow agents to determine if unlearning was successful.
- Core assumption: Value vectors of trajectories serve as unique identifiers that can distinguish whether trajectories were part of training data.
- Evidence anchors:
  - [section IV-A]: "cumulative rewards can serve as a unique identifier" and "Qπ measures the expected cumulative reward"
  - [section IV-B]: "We call our method TRAJAUDITOR" with description of shadow model preparation and value collections
- Break condition: If value vectors don't uniquely identify trajectories, or if shadow agent generation fails to accurately simulate training with unlearning data.

## Foundational Learning

- Concept: Value function in reinforcement learning (Q(s,a))
  - Why needed here: The entire unlearning mechanism relies on manipulating value functions to degrade performance on specific trajectories while preserving it on others.
  - Quick check question: What does Q(s,a) represent in reinforcement learning, and how does it relate to agent decision-making?

- Concept: Policy gradient theorem and advantage functions
  - Why needed here: The unlearning algorithm uses policy gradient updates based on advantage functions to adjust the agent's policy in the direction of forgetting specific trajectories.
  - Quick check question: How does the advantage function A(s,a) = Q(s,a) - V(s) help determine which actions to modify during unlearning?

- Concept: Wasserstein distance for distribution comparison
  - Why needed here: TRAJAUDITOR uses Wasserstein distance to compare value distributions between unlearned agents and shadow agents when evaluating unlearning success.
  - Quick check question: What property of Wasserstein distance makes it suitable for comparing value vector distributions in trajectory auditing?

## Architecture Onboarding

- Component map:
  Original trained agent (π) with value function Qπ -> Unlearning dataset (Df) -> Unlearned agent (π′) with value function Qπ′ -> TRAJAUDITOR with shadow agents for evaluation

- Critical path:
  1. Initialize unlearned agent as copy of original agent
  2. Execute "forgetting" phase: minimize Q for Df, maximize Q for Dm
  3. Execute "convergence training" phase: align Qπ′ with Qπ on Dm
  4. Use TRAJAUDITOR to verify unlearning success
  5. Deploy unlearned agent if evaluation passes

- Design tradeoffs:
  - Forgetting steps (K) vs. convergence training steps (H): More forgetting steps improve unlearning but may require more convergence training
  - Balancing factor (λ): Controls trade-off between forgetting and maintaining performance; too high causes instability, too low prevents effective unlearning
  - Number of shadow agents (N) and perturbation rounds (M) in TRAJAUDITOR: More agents and perturbations improve evaluation accuracy but increase computational cost

- Failure signatures:
  - High PPR (Percentage of Positive Predictions) from TRAJAUDITOR indicates unlearning failure
  - Significant performance degradation in remaining trajectories indicates over-unlearning
  - Inconsistent results across different random seeds indicates instability
  - Low F1-score from TRAJAUDITOR indicates poor evaluation capability

- First 3 experiments:
  1. Test TRAJDELETER on a single trajectory from a simple environment (e.g., CartPole) to verify basic unlearning mechanism works
  2. Evaluate sensitivity to balancing factor λ by running with values {0.25, 0.5, 0.75, 1.0, 1.5} on a medium-difficulty task
  3. Compare TRAJDELETER against baseline methods (fine-tuning, random-reward) on a standard benchmark task (e.g., Hopper) to establish efficacy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness of TRAJDELETER be evaluated against real-world data poisoning attacks that go beyond simple reward manipulation?
- Basis in paper: [explicit] The paper mentions defending against trajectory poisoning attacks but only tests a simple reward manipulation scenario where action values are scaled by 1.5 times their mean.
- Why unresolved: Real-world attacks could be more sophisticated, involving complex state manipulations or temporal dependencies that aren't captured by simple reward scaling.
- What evidence would resolve it: Experiments showing TRAJDELETER's performance against various types of poisoning attacks including state manipulation, temporal attacks, and adversarial trajectory generation.

### Open Question 2
- Question: What is the impact of repeated unlearning on long-term model performance and stability across different offline RL algorithms?
- Basis in paper: [explicit] The paper briefly mentions that repeated unlearning and relearning of the same trajectories shows no significant performance degradation, but only tests up to 10 iterations.
- Why unresolved: The long-term effects of multiple unlearning cycles on model stability, convergence, and generalization capabilities remain unexplored, particularly for different offline RL algorithms.
- What evidence would resolve it: Extended experiments testing hundreds or thousands of unlearning iterations across various offline RL algorithms to measure performance degradation, convergence stability, and potential catastrophic forgetting.

### Open Question 3
- Question: How can TRAJDELETER be extended to handle partial unlearning requests where only specific portions of trajectories need to be forgotten?
- Basis in paper: [inferred] The current TRAJDELETER framework operates at the trajectory level, but real-world scenarios might require more granular control over which parts of trajectories should be unlearned.
- Why unresolved: The paper doesn't address the technical challenges of partial trajectory unlearning, such as maintaining temporal consistency and ensuring proper credit assignment across different segments of trajectories.
- What evidence would resolve it: A modified version of TRAJDELETER that can handle partial trajectory unlearning requests, along with experimental validation showing its effectiveness and efficiency compared to the current trajectory-level approach.

## Limitations

- Limited evaluation on non-locomotion tasks beyond the D4RL benchmark, raising questions about generalizability to diverse offline RL domains
- Scalability concerns with large datasets as computational complexity increases with dataset size during both forgetting and auditing phases
- TRAJAUDITOR's reliance on value vectors as trajectory fingerprints may miss unlearning failures that manifest in state-action space rather than value space

## Confidence

**High Confidence**: The core mechanism of value function manipulation for trajectory forgetting (Mechanism 1) is well-grounded in RL theory. The mathematical formulation in Equations 1-5 is internally consistent and leverages established concepts from policy gradient methods.

**Medium Confidence**: The convergence training phase (Mechanism 2) shows promise in stabilizing unlearned agents, but the paper provides limited analysis of how different convergence training durations affect final policy quality. The balancing act between forgetting and preservation relies on empirical tuning rather than theoretical guarantees.

**Low Confidence**: The TRAJAUDITOR evaluation method (Mechanism 3) represents the paper's most novel contribution but also its weakest point. While the approach is computationally efficient compared to baseline auditing methods, its ability to detect subtle unlearning failures remains unproven.

## Next Checks

1. **Cross-Domain Validation**: Test TRAJDELETER on at least two qualitatively different offline RL tasks (e.g., robotic manipulation and Atari games) to assess generalizability beyond locomotion tasks. Measure unlearning effectiveness, retraining time savings, and policy stability across domains.

2. **Scalability Benchmark**: Evaluate TRAJDELETER on datasets spanning three orders of magnitude in size (10², 10⁴, and 10⁶ trajectories) while maintaining consistent unlearning targets. Track computational costs and effectiveness degradation patterns to establish scaling laws.

3. **Failure Mode Analysis**: Systematically test TRAJDELETER under adversarial conditions where unlearning targets are selected to maximize interference with remaining trajectories. Use both TRAJAUDITOR and direct environment evaluation to compare detection capabilities and identify gaps in the auditing methodology.