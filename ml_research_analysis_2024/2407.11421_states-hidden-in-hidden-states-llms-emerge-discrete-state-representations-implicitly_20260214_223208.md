---
ver: rpa2
title: 'States Hidden in Hidden States: LLMs Emerge Discrete State Representations
  Implicitly'
arxiv_id: '2407.11421'
source_url: https://arxiv.org/abs/2407.11421
tags:
- uni00000003
- uni00000013
- uni0000004c
- uni00000048
- uni00000046
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper uncovers a novel emergent ability in Large Language
  Models (LLMs): the intrinsic ability to perform extended sequences of arithmetic
  calculations without relying on step-by-step solutions. The authors hypothesize
  that models form Implicit Discrete State Representations (IDSRs) within their hidden
  states to perform symbolic calculations internally.'
---

# States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly

## Quick Facts
- arXiv ID: 2407.11421
- Source URL: https://arxiv.org/abs/2407.11421
- Reference count: 13
- Key outcome: LLMs can perform extended arithmetic calculations by forming Implicit Discrete State Representations (IDSRs) in hidden states, though these representations are lossy and limit accuracy.

## Executive Summary
This paper reveals a novel emergent ability in Large Language Models: the capacity to perform extended sequences of arithmetic calculations without step-by-step solutions by forming Implicit Discrete State Representations (IDSRs) in their hidden states. The authors construct a synthetic dataset of consecutive addition problems and employ probing methods to examine these representations across various LLMs. Their experiments confirm the existence of IDSRs, reveal their digit-wise sequential formation pattern, and demonstrate layer-wise specialization where early layers directly compute while later layers integrate task context. However, they find that these state representations are far from lossless in current models, leading to inaccuracies in final performance.

## Method Summary
The researchers created a synthetic dataset of consecutive addition and subtraction problems with 2-14 addends, each 1-3 digits long. They probed hidden states from LLMs using MLPs of varying sizes (829,400, 81,920, and 40,960 hidden units) to predict either the full sum or individual digits. The probes were trained on 80% of the data and evaluated on held-out test sets. They tested different prompt formats and examined probing accuracy at various tokens (between additions and at the equals sign) and across different transformer layers to understand IDSR formation and propagation.

## Key Results
- IDSRs exist in LLM hidden states and can be decoded with linear probes to recover intermediate arithmetic results
- Digit prediction accuracy follows an ascending digit order, mirroring human digit-by-digit calculation
- Early transformer layers (first ~10) directly compute arithmetic results, while later layers incorporate task context and rebuild representations, causing resolution loss
- Current IDSRs are lossy, with probing accuracy decreasing as sequence length increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs form IDSRs in hidden states to track intermediate results during arithmetic
- Mechanism: The model uses internal states at each step to store the running sum, allowing direct output of final results
- Core assumption: Hidden states encode arithmetic results that can be decoded with a linear probe
- Evidence anchors: Abstract and section 3 hypothesis statements
- Break condition: If probing accuracy drops to random chance or linear probes fail

### Mechanism 2
- Claim: IDSRs are formed digit-wise and sequentially, starting from lowest-order digit
- Mechanism: The model builds up the result one digit at a time, storing each intermediate sum in hidden states
- Core assumption: Digit-wise probing can recover the intermediate sum from hidden states
- Evidence anchors: Section 6.1's digit prediction accuracy pattern
- Break condition: If digits don't appear sequentially or accuracy is random

### Mechanism 3
- Claim: Shallow semantic layers directly compute arithmetic while later layers integrate task context
- Mechanism: Early layers focus on pure numeric computation; later layers adjust based on prompt semantics, causing resolution loss
- Core assumption: Probing results differ between shallow and later layers, and prompts affect later layers more
- Evidence anchors: Section 6.3's layer-wise analysis and prompt disruption experiments
- Break condition: If layer-wise probing shows no mechanism shift, or prompts don't differentially affect layers

## Foundational Learning

- Concept: Hidden state probing and classification
  - Why needed here: To detect and interpret IDSRs in transformer hidden states
  - Quick check question: Can you describe how a linear probe maps hidden states to target labels and why softmax is used?

- Concept: Digit-wise representation in neural networks
  - Why needed here: Understanding how models can represent multi-digit numbers separately in hidden states
  - Quick check question: Why would a model store digits of a sum independently in hidden states, and how could you verify this?

- Concept: Layer-wise function specialization
  - Why needed here: To reason about how different transformer layers contribute differently to arithmetic reasoning
  - Quick check question: What evidence would you need to show that early layers perform arithmetic while later layers integrate context?

## Architecture Onboarding

- Component map: Input → Tokenizer → Transformer layers → Output logits. Probing happens by extracting hidden states from specified layers/tokens and feeding them into a small MLP classifier.
- Critical path: Forward pass through model → Extract hidden states at `+` and `=` tokens → Apply probing MLP → Evaluate accuracy
- Design tradeoffs: Using larger probes increases accuracy but risks overfitting; probing at `+` vs `=` tokens may yield different resolution
- Failure signatures: Random or near-random probing accuracy; probing accuracy that doesn't improve with model size; accuracy that drops sharply after a few tokens
- First 3 experiments:
  1. Probe hidden states at `+` tokens for 2-digit sums and measure per-digit accuracy
  2. Repeat probing at `=` tokens and compare resolution to `+` token probing
  3. Vary the probe model size (linear vs small MLP vs larger MLP) and observe effect on accuracy

## Open Questions the Paper Calls Out

- What specific training data or mechanisms enable LLMs to develop the ability to perform implicit consecutive addition?
- How can the resolution and accuracy of IDSRs be improved in current LLMs?
- To what extent do IDSRs generalize beyond arithmetic tasks to other forms of multi-step reasoning?

## Limitations

- Findings are based entirely on synthetic arithmetic datasets with fixed prompt formats, limiting generalizability to natural language problems
- Critical probe architecture details (initialization, activation functions, training procedures) are not specified, making exact reproduction difficult
- Experiments focus on a narrow range of models and task types, not exploring other mathematical operations or reasoning domains

## Confidence

- High confidence: IDSR existence and basic properties (digit-wise formation, layer-wise specialization) are well-supported by probing experiments
- Medium confidence: The claim about shallow semantic layers directly computing while later layers rebuild representations based on task context
- Low confidence: The broader claim that IDSRs are a general mechanism for symbolic reasoning in LLMs beyond the tested arithmetic tasks

## Next Checks

1. Systematically vary probe model complexity (linear vs MLP vs larger networks) and training duration to establish whether current accuracy results are probe-limited rather than IDSR-limited

2. Apply the same probing methodology to multiplication, division, and algebraic reasoning tasks to determine if IDSRs are task-specific or a general mechanism

3. Test IDSR formation and accuracy when arithmetic problems are embedded in natural language contexts (e.g., word problems) to assess robustness to semantic noise