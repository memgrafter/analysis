---
ver: rpa2
title: 'CPRM: A LLM-based Continual Pre-training Framework for Relevance Modeling
  in Commercial Search'
arxiv_id: '2412.01269'
source_url: https://arxiv.org/abs/2412.01269
tags:
- llms
- pre-training
- relevance
- queries
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CPRM, a continual pre-training framework for
  relevance modeling in commercial search. The method enhances LLM domain knowledge
  through joint pre-training on queries and multi-field items, in-context pre-training
  on semantically related query/item collections, and reading comprehension distillation
  to generate domain knowledge.
---

# CPRM: A LLM-based Continual Pre-training Framework for Relevance Modeling in Commercial Search

## Quick Facts
- arXiv ID: 2412.01269
- Source URL: https://arxiv.org/abs/2412.01269
- Reference count: 7
- Key outcome: CPRM achieves 93.49% AUC and 94.42% F1 score, improving over GLM-2B baseline by 2.45% AUC and showing 0.32% valid PVCTR increase in production

## Executive Summary
This paper introduces CPRM, a continual pre-training framework designed to enhance large language models (LLMs) for relevance modeling in commercial search engines. The framework addresses the challenge of bridging semantic gaps between short, colloquial queries and long, formal item descriptions through three complementary mechanisms: joint pre-training on queries and multi-field items, in-context pre-training on semantically related query/item collections, and reading comprehension distillation using a larger teacher LLM. Evaluated on real-world mini-app search data, CPRM demonstrates significant performance improvements over strong baselines including DSSM, BERT variants, and other LLMs, with successful deployment in production for over nine months.

## Method Summary
CPRM employs a three-module approach to continual pre-training of LLMs for search relevance tasks. The Domain Knowledge Enhancement (DKE) module jointly pre-trains on queries and structured item data using both token-level and segment-level masked language modeling to bridge semantic differences. The In-Context Pre-training (ICP) module constructs collections of semantically related queries or items from click logs and pre-trains the model on these relationships to enhance in-context learning capabilities. The Reading Comprehension Distillation (RCD) module uses a larger teacher LLM to generate domain-specific knowledge, summaries, and related queries from item data, which are then used as additional pre-training data. The framework is built on a GLM-2B architecture and includes supervised fine-tuning on labeled relevance data.

## Key Results
- CPRM achieves 93.49% AUC and 94.42% F1 score on the test set, improving over baseline GLM-2B by 2.45% AUC
- The framework handles both short and long queries effectively, with performance improvements across all query length intervals
- Online A/B testing shows a statistically significant 0.32% increase in valid PVCTR, with CPRM deployed in production for over nine months
- CPRM outperforms multiple strong baselines including DSSM, BERT variants, and other LLM-based approaches across AUC, F1, and accuracy metrics

## Why This Works (Mechanism)

### Mechanism 1: Joint Pre-training on Queries and Multi-field Items
- Claim: Joint pre-training bridges the semantic gap between short, colloquial queries and long, formal item descriptions
- Mechanism: By concatenating queries and items with segment embeddings and applying both token-level and segment-level MLM, the model learns explicit semantic relationships between the two domains during pre-training
- Core assumption: Queries and items belong to different domains with substantial semantic differences that can be bridged through joint representation learning
- Evidence anchors: [abstract]: "Firstly, employing both queries and multi-field item to jointly pre-train for enhancing domain knowledge"; [section]: "Different from conventional pre-training methods, we jointly pre-train the structured item data with multiple queries... allowing LLMs to explicitly model the relationships between them"

### Mechanism 2: In-context Pre-training on Semantically Related Sets
- Claim: Constructing and pre-training on collections of semantically similar queries or items improves the model's in-context learning ability for relevance tasks
- Mechanism: Using click logs to create mappings between queries and items, then filtering semantically unrelated pairs using semantic similarity models, and finally constructing ICP instances by concatenating queries with semantically similar items (or vice versa)
- Core assumption: Historical click behavior reflects semantic relevance, and semantically related sets provide meaningful context for the model to learn relevance patterns
- Evidence anchors: [abstract]: "Secondly, applying in-context pre-training, a novel approach where LLMs are pre-trained on a sequence of related queries or items"; [section]: "The overall idea is to build collections of semantically similar queries and items as pre-training data to further stimulate in-context learning capabilities of LLMs"

### Mechanism 3: Reading Comprehension Distillation for Domain Knowledge
- Claim: Using a larger teacher LLM to generate domain-specific knowledge and queries from structured item data enriches the pre-training corpus with relevant background information
- Mechanism: Employing the teacher LLM to summarize and paraphrase items, generate related queries, and provide explanations for these queries, then using these outputs as additional pre-training data
- Core assumption: A larger teacher LLM can effectively extract and generate relevant domain knowledge from structured item data that a smaller model cannot learn from raw data alone
- Evidence anchors: [abstract]: "Thirdly, conducting reading comprehension on items to produce associated domain knowledge and background information (e.g., generating summaries and corresponding queries) to further strengthen LLMs"; [section]: "We employ the teacher LLM for reading comprehension on items... facilitating the generation of relevant domain knowledge for pre-training"

## Foundational Learning

- Concept: Semantic gap between queries and items
  - Why needed here: Understanding why joint pre-training is necessary to bridge the difference between short, colloquial queries and long, formal item descriptions
  - Quick check question: What are the typical characteristics of queries vs. items in commercial search scenarios?

- Concept: In-context learning capabilities of LLMs
  - Why needed here: Understanding how constructing semantically related sets can improve the model's ability to make predictions based on context
  - Quick check question: How does pre-training on related query/item collections differ from standard pre-training?

- Concept: Knowledge distillation and transfer
  - Why needed here: Understanding how a larger teacher LLM can generate domain-specific knowledge that can be transferred to smaller models through pre-training
  - Quick check question: What is the role of the teacher LLM in generating additional pre-training data?

## Architecture Onboarding

- Component map: CPRM framework consists of three main components - Domain Knowledge Enhancement (DKE), In-Context Pre-training (ICP), and Reading Comprehension Distillation (RCD). The model uses a GLM-2B architecture with 36 layers, hidden size of 2048, FFN size of 8192, and 32 attention heads.
- Critical path: The critical path involves constructing pre-training data (DKE, ICP, RCD), pre-training the GLM-2B model on this data, then fine-tuning on the supervised dataset, and finally evaluating on the test set.
- Design tradeoffs: Larger teacher LLMs (like Qwen2-72B) provide better knowledge generation but increase computational cost. Joint pre-training requires more memory than single-domain pre-training. ICP requires constructing and filtering large datasets from click logs.
- Failure signatures: Poor performance on long queries suggests ICP/RCD methods aren't capturing relevant patterns. Significant performance drop at certain training steps indicates domain adaptation issues. No improvement over baseline suggests pre-training data construction is ineffective.
- First 3 experiments:
  1. Run DKE pre-training only on GLM-2B and evaluate on validation set to verify joint pre-training improves over baseline
  2. Implement semantic filtering using Contriever and construct ICP instances, then pre-train and evaluate
  3. Use Qwen2-72B as teacher LLM to generate RCD data, then pre-train GLM-2B and evaluate performance gain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CPRM scale with different model sizes beyond the tested GLM-2B, and what are the optimal trade-offs between model size, computational cost, and relevance performance?
- Basis in paper: [explicit] The paper mentions that performance improves with model size but gains diminish, and larger models like GLM-10B only show marginal improvement over GLM-5B. It also notes that large models have low inference efficiency affecting deployment.
- Why unresolved: The paper only tested up to GLM-10B and focused on GLM-2B for CPRM. The relationship between model size and performance across a broader range of sizes remains unexplored.
- What evidence would resolve it: Comprehensive experiments comparing CPRM performance across multiple model sizes (e.g., GLM-1B, GLM-3B, GLM-7B, GLM-20B) with detailed analysis of computational costs and latency metrics.

### Open Question 2
- Question: How does CPRM's performance generalize to other domains beyond mini-app search, such as e-commerce, document retrieval, or academic search?
- Basis in paper: [inferred] The paper demonstrates CPRM on mini-app search data but does not test it on other search domains. The methods (DKE, ICP, RCD) are described as general approaches for relevance modeling.
- Why unresolved: The paper's experiments are confined to one specific domain, leaving open whether the techniques would transfer effectively to different types of search tasks with varying query-item characteristics.
- What evidence would resolve it: Cross-domain experiments applying CPRM to diverse search scenarios (e-commerce, academic search, document retrieval) with performance comparisons to domain-specific baselines.

### Open Question 3
- Question: What is the impact of different types of background knowledge generated through Reading Comprehension Distillation on CPRM's performance, and can this process be automated or optimized?
- Basis in paper: [explicit] The paper describes RCD as generating summaries, paraphrases, and related queries, but does not systematically analyze which types of generated knowledge contribute most to performance improvements.
- Why unresolved: The paper treats all RCD outputs equally without investigating which specific knowledge types (summaries vs. queries vs. explanations) drive the performance gains, nor whether the prompt design can be optimized.
- What evidence would resolve it: Ablation studies testing CPRM with different RCD output types (summaries only, queries only, explanations only) and automated prompt optimization experiments to identify the most effective knowledge generation strategies.

## Limitations

- Data scale and representativeness concerns due to reliance on proprietary mini-app search dataset that cannot be publicly verified
- Knowledge distillation quality dependency where poor teacher LLM outputs could introduce harmful noise into the pre-training process
- Computational overhead trade-offs where the 0.32% valid PVCTR improvement may not justify increased complexity for all deployment scenarios

## Confidence

- High confidence claims: CPRM framework architecture and methodology are clearly specified; Offline evaluation results (AUC, F1, accuracy) are reproducible given the datasets; Online A/B testing methodology is sound
- Medium confidence claims: The three mechanisms (DKE, ICP, RCD) individually contribute to performance gains; Performance improvements generalize across query lengths; The 0.32% valid PVCTR improvement represents meaningful business impact
- Low confidence claims: CPRM's effectiveness in domains beyond mini-app search; Long-term stability of performance improvements in production; Generalization to languages other than Chinese (implied by dataset)

## Next Checks

1. **Cross-domain validation study:** Evaluate CPRM on publicly available commercial search datasets (e.g., Amazon product search, MSN click logs) to verify that performance gains generalize beyond the proprietary mini-app dataset.

2. **Ablation study with controlled noise injection:** Systematically vary the quality of teacher LLM-generated data in the RCD module by introducing controlled noise levels, then measure impact on final performance.

3. **Computational cost-benefit analysis:** Measure wall-clock training time, memory usage, and inference latency for CPRM versus baseline models across different hardware configurations, comparing these costs against performance improvements.