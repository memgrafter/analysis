---
ver: rpa2
title: 'TIMA: Text-Image Mutual Awareness for Balancing Zero-Shot Adversarial Robustness
  and Generalization Ability'
arxiv_id: '2405.17678'
source_url: https://arxiv.org/abs/2405.17678
tags:
- adversarial
- text
- zero-shot
- image
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of achieving zero-shot adversarial
  robustness while preserving zero-shot generalization in large-scale foundation models,
  specifically CLIP. The authors propose a novel Text-Image Mutual Awareness (TIMA)
  method that increases inter-class distances in both text and image embeddings to
  improve robustness against adversarial perturbations of various magnitudes.
---

# TIMA: Text-Image Mutual Awareness for Balancing Zero-Shot Adversarial Robustness and Generalization Ability

## Quick Facts
- arXiv ID: 2405.17678
- Source URL: https://arxiv.org/abs/2405.17678
- Authors: Fengji Ma; Li Liu; Hei Victor Cheng
- Reference count: 40
- One-line primary result: TIMA achieves state-of-the-art zero-shot adversarial robustness while preserving zero-shot generalization in CLIP models

## Executive Summary
This paper addresses the challenge of achieving zero-shot adversarial robustness while preserving zero-shot generalization in large-scale foundation models, specifically CLIP. The authors propose a novel Text-Image Mutual Awareness (TIMA) method that increases inter-class distances in both text and image embeddings to improve robustness against adversarial perturbations of various magnitudes. The method consists of two key components: Image-Aware Text (IAT) tuning, which uses Minimum Hyperspherical Energy (MHE) to increase inter-class distances in text embeddings while preserving semantic information through knowledge distillation, and Text-Aware Image (TAI) tuning, which increases inter-class distances in image embeddings using text-distance based adaptive margins. Extensive experiments on multiple datasets demonstrate that TIMA achieves state-of-the-art zero-shot adversarial robustness under both small and large perturbations while maintaining strong zero-shot generalization performance.

## Method Summary
TIMA is a two-stage adversarial fine-tuning method that balances zero-shot adversarial robustness and generalization ability in CLIP models. The first stage (IAT) applies MHE loss to text embeddings to increase inter-class distances, while using image-to-text knowledge distillation to preserve semantic relationships. The second stage (TAI) adds text-distance based adaptive margins to image embeddings and uses text-to-image knowledge distillation. The combined approach improves robustness against adversarial perturbations of varying magnitudes while maintaining strong clean accuracy across multiple datasets.

## Key Results
- TIMA achieves state-of-the-art zero-shot adversarial robustness under both small and large perturbations
- Significant improvements over baseline methods in both clean accuracy and robust accuracy metrics
- Maintains strong zero-shot generalization performance while improving adversarial robustness
- Demonstrates effectiveness across multiple datasets including ImageNet, Tiny-ImageNet, CIFAR10, CIFAR100, STL10, OxfordPets, Food101, SUN397, DTD, and EuroSAT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing inter-class distances in text embeddings using MHE improves zero-shot adversarial robustness.
- Mechanism: The MHE loss encourages text embeddings from different classes to be distributed uniformly over a hypersphere, maximizing their inter-class distances and making it harder for adversarial perturbations to cause misclassification.
- Core assumption: Larger inter-class distances in embedding space provide more robust decision boundaries that are less susceptible to adversarial attacks.
- Evidence anchors:
  - [abstract]: "We propose an Image-Aware Text (IAT) tuning mechanism that increases the inter-class distance of text embeddings by incorporating the Minimum Hyperspherical Energy (MHE)."
  - [section]: "Inspired by it, we aim to strengthen zero-shot adversarial robustness under the large perturbation radius by increasing the inter-class distance of text embeddings. The idea of MHE... is applied to fine-tune text embeddings, enforcing embeddings from different classes to be distributed uniformly over the hypersphere representing the text embedding space, thus maximizing their inter-class distances."
  - [corpus]: Weak - no direct citations in the corpus support this specific mechanism, though related work on MHE exists.
- Break condition: If the MHE-tuned text embeddings lose semantic relationships between classes or if the increased distances don't translate to improved decision boundaries under attack.

### Mechanism 2
- Claim: Preserving semantic consistency through cross-modal knowledge distillation maintains zero-shot generalization while increasing robustness.
- Mechanism: Image-Aware Knowledge Distillation (IAKD) uses fixed pre-trained image embeddings as auxiliary supervision to maintain similarity between MHE-tuned and original text embeddings, preserving semantic information between classes.
- Core assumption: The original semantic relationships encoded in pre-trained CLIP can be preserved during adversarial fine-tuning through distillation.
- Evidence anchors:
  - [abstract]: "Simultaneously, fixed pre-trained image embeddings are used as cross-modal auxiliary supervision to maintain the similarity between the MHE-tuned and original text embeddings by the knowledge distillation, preserving semantic information between different classes."
  - [section]: "we utilize the (fixed) pre-trained image embeddings as auxiliary supervisory information for cross-modal supervision distillation... This approach applies the Minimum Hyperspherical Energy (MHE) principle to increase the distances between classes in text embeddings, meanwhile ensuring the preservation of the semantic information and coherent relationship between text and image embeddings inherent in the original pre-trained CLIP model using an image-to-text distillation module."
  - [corpus]: Weak - no direct citations in the corpus support this specific distillation mechanism, though knowledge distillation is mentioned in related works.
- Break condition: If the distillation process fails to capture the semantic relationships or if the fixed image embeddings don't provide useful supervision for text embeddings.

### Mechanism 3
- Claim: Increasing inter-class distances in image embeddings using text-distance based adaptive margins improves zero-shot adversarial robustness.
- Mechanism: Text-Aware Image (TAI) tuning introduces text-distance based adaptive margins to different classes during training, increasing the inter-class distance of image embeddings based on the semantic proximity between classes.
- Core assumption: Semantic relationships captured in pre-trained text embeddings can be used to inform appropriate margin adjustments for image embeddings.
- Evidence anchors:
  - [abstract]: "we introduce a Text-Aware Image (TAI) tuning mechanism, which increases inter-class distance between image embeddings during the training stage by Text-distance based Adaptive Margin (TAM)."
  - [section]: "we propose the text-distance based adaptive margin. When the cosine similarity between the image embedding and the corresponding text embedding falls below a threshold, indicating a significant semantic difference, then no additional margin is added. Conversely, when the similarity exceeds the threshold, indicating semantic proximity, an additional margin is added to these classes."
  - [corpus]: Weak - no direct citations in the corpus support this specific adaptive margin mechanism, though related work on margin-based approaches exists.
- Break condition: If the adaptive margin mechanism doesn't effectively increase inter-class distances or if it disrupts the semantic relationships in image embeddings.

## Foundational Learning

- Concept: Hyperspherical embeddings and Minimum Hyperspherical Energy (MHE)
  - Why needed here: Understanding how MHE encourages uniform distribution on a hypersphere is crucial for grasping why increasing inter-class distances improves robustness.
  - Quick check question: What property of the hypersphere does MHE exploit to maximize inter-class distances?

- Concept: Knowledge distillation in multimodal learning
  - Why needed here: The cross-modal distillation mechanism is key to preserving semantic relationships while increasing robustness.
  - Quick check question: How does using fixed pre-trained image embeddings as supervision help maintain semantic consistency in text embeddings during adversarial fine-tuning?

- Concept: Adaptive margins in metric learning
  - Why needed here: Understanding how text-distance based adaptive margins work is essential for implementing the TAI mechanism.
  - Quick check question: Why does the TAI mechanism use different margin values based on the semantic proximity between classes?

## Architecture Onboarding

- Component map:
  - CLIP image encoder (Fθ)
  - CLIP text encoder (Fφ)
  - MHE loss module for text embeddings
  - Image-Aware Knowledge Distillation (IAKD) module
  - Text-distance Adaptive Margin (TAM) module for image embeddings
  - Text-Aware Knowledge Distillation (TAKD) module
  - Adversarial fine-tuning pipeline with PGD attacks

- Critical path:
  1. Input image and class text prompt
  2. Generate image and text embeddings
  3. Apply MHE loss to increase inter-class distances in text embeddings
  4. Apply IAKD to preserve semantic relationships in text embeddings
  5. Apply TAM to increase inter-class distances in image embeddings
  6. Apply TAKD to preserve semantic relationships in image embeddings
  7. Compute total loss and perform gradient update

- Design tradeoffs:
  - MHE vs. other inter-class distance maximization techniques (MHE provides uniform distribution but may be computationally expensive)
  - Fixed vs. learnable distillation targets (fixed targets provide stability but may not adapt to fine-tuning dynamics)
  - Static vs. adaptive margins (adaptive margins are more sophisticated but require careful hyperparameter tuning)

- Failure signatures:
  - Decreased clean accuracy despite improved adversarial robustness (indicates loss of semantic relationships)
  - Adversarial robustness that doesn't scale with perturbation radius (indicates insufficient inter-class distance increases)
  - Training instability or slow convergence (may indicate hyperparameter issues with MHE or margin values)

- First 3 experiments:
  1. Implement MHE loss on text embeddings alone and evaluate inter-class distance increases and clean accuracy
  2. Add IAKD to preserve semantic relationships and evaluate trade-off between robustness and generalization
  3. Add TAM to image embeddings and evaluate combined effect on both small and large perturbations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the TIMA method perform when applied to other large-scale foundation models beyond CLIP, such as those in natural language processing or multi-modal models like GPT-4?
- Basis in paper: [inferred] The paper focuses on CLIP but mentions large-scale foundation models broadly, suggesting potential applicability to other models.
- Why unresolved: The study is limited to CLIP, and no experiments or comparisons with other foundation models are provided.
- What evidence would resolve it: Empirical results demonstrating TIMA's effectiveness on other foundation models, such as BERT or GPT-4, would clarify its broader applicability.

### Open Question 2
- Question: What is the impact of TIMA on the computational efficiency of adversarial training, particularly in terms of training time and resource usage?
- Basis in paper: [inferred] The paper discusses the effectiveness of TIMA but does not address its computational efficiency or resource requirements.
- Why unresolved: The focus is on performance metrics like accuracy and robustness, with no mention of computational costs or scalability.
- What evidence would resolve it: Detailed analysis of training time, memory usage, and computational overhead when implementing TIMA compared to baseline methods.

### Open Question 3
- Question: How does TIMA handle domain-specific adversarial attacks, such as those targeting specialized datasets or real-world applications like autonomous driving or medical imaging?
- Basis in paper: [explicit] The paper mentions EuroSAT for domain-specialized tasks but does not explore adversarial attacks specific to such domains.
- Why unresolved: The experiments focus on general and fine-grained classification tasks, without addressing domain-specific vulnerabilities or attack scenarios.
- What evidence would resolve it: Testing TIMA on domain-specific datasets under targeted adversarial attacks would reveal its robustness in specialized applications.

## Limitations
- The exact implementation details of MHE loss and text-distance based adaptive margin are not fully specified
- Computational overhead and memory requirements during fine-tuning are not discussed
- Limited evaluation to CLIP architecture with unclear generalizability to other vision-language models
- No ablation studies to isolate individual contributions of MHE, IAKD, TAM, and TAKD components

## Confidence
- **High confidence**: The paper demonstrates improved zero-shot adversarial robustness across multiple datasets and perturbation radii, with significant improvements over baseline methods.
- **Medium confidence**: The mechanisms for increasing inter-class distances and preserving semantic relationships are theoretically sound, but the exact implementation details needed for faithful reproduction are not fully specified.
- **Low confidence**: The paper doesn't provide ablation studies to isolate the individual contributions of MHE, IAKD, TAM, and TAKD components, making it difficult to assess their relative importance.

## Next Checks
1. **Inter-class distance analysis**: Measure the actual inter-class distances in text and image embeddings before and after applying TIMA to verify that the proposed mechanisms are effectively increasing these distances as claimed.

2. **Ablation study**: Implement and evaluate each component of TIMA (MHE, IAKD, TAM, TAKD) separately to quantify their individual contributions to the overall improvement in robustness and generalization.

3. **Computational overhead evaluation**: Measure the additional training time, memory usage, and inference latency introduced by TIMA compared to standard CLIP fine-tuning to assess its practical applicability for real-world deployment.