---
ver: rpa2
title: 'CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs'' Mathematical
  Reasoning Capabilities'
arxiv_id: '2401.06961'
source_url: https://arxiv.org/abs/2401.06961
tags:
- problem
- solution
- answer
- concepts
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CHAMP, a dataset of 270 high school math
  competition problems with annotations of relevant concepts and hints. The dataset
  enables evaluation of LLMs' ability to solve challenging problems, make use of additional
  contextual information, and verify solutions.
---

# CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities

## Quick Facts
- arXiv ID: 2401.06961
- Source URL: https://arxiv.org/abs/2401.06961
- Authors: Yujun Mao; Yoon Kim; Yilun Zhou
- Reference count: 22
- Primary result: Even best models achieve only 67% accuracy on competition-level math problems, with full solution correctness much lower

## Executive Summary
This paper introduces CHAMP, a dataset of 270 high school math competition problems with detailed annotations of relevant concepts and hints. The dataset enables evaluation of LLMs' ability to solve challenging problems, make use of additional contextual information, and verify solutions. Experiments with 10 models (including GPT-4, Llama 3, and Mixtral) reveal that current LLMs struggle with mathematical reasoning, often arriving at correct answers through incorrect reasoning steps. The study highlights the limitations of current evaluation methods and the need for more fine-grained benchmarks to assess LLMs' mathematical reasoning capabilities.

## Method Summary
The CHAMP dataset contains 270 competition-level math problems from Engel (2008) with step-wise solutions and annotations of relevant concepts and hints. The evaluation involves generating model solutions using 10 different LLMs with 17 prompt variants, then manually annotating first wrong steps (FWS) in incorrect solutions. Models are assessed on final answer accuracy, full solution correctness, and solution verification ability. The study compares zero-shot and few-shot prompting approaches while varying how concepts and hints are provided to the models.

## Key Results
- Best models achieve only 67% accuracy on final answers, with full solution correctness much lower
- Models often arrive at correct final answers through incorrect reasoning steps
- Providing concepts and hints sometimes improves performance, but results vary by model and prompting method
- All models struggle with solution verification, often failing to identify errors in generated solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing additional contextual information (concepts and hints) can improve LLM performance on math problems.
- Mechanism: External concepts and hints provide relevant mathematical knowledge and problem-specific strategies that help guide the LLM's reasoning process.
- Core assumption: The LLM has some capacity to understand and utilize the provided concepts and hints, even if it doesn't fully comprehend the problem initially.
- Evidence anchors:
  - [abstract] "With concepts and hints, performance sometimes improves, indicating that some models can make use of such side information."
  - [section 4.2] "Providing hints helps, the performance increase is correlated with the model's 'base capability': GPT-4, 4 Turbo, Llama 3 70B and Mixtral 8x22B, which have the best zero-shot accuracy, are able to score 10% higher on average with hints."

### Mechanism 2
- Claim: LLMs can sometimes arrive at correct final answers through incorrect reasoning steps.
- Mechanism: The LLM uses heuristic shortcuts or pattern matching to arrive at the correct final answer, even if the intermediate reasoning steps are flawed.
- Core assumption: The LLM has been exposed to enough training data where the correct final answers appear frequently, allowing it to memorize or pattern-match these answers.
- Evidence anchors:
  - [abstract] "Using this corpus, we find that models often arrive at the correct final answer through wrong reasoning steps."
  - [section 4.3] "However, focusing on final answer alone could inflate the models' performance as incorrect reasoning could lead to correct final answers, especially for questions with yes/no answers."

### Mechanism 3
- Claim: LLMs struggle with solution verification, often failing to identify errors in generated solutions.
- Mechanism: The LLM lacks the ability to critically analyze and evaluate the logical correctness of a given solution, even if it can generate a solution itself.
- Core assumption: Solution verification requires a higher level of reasoning and understanding than solution generation, which the LLM may not possess.
- Evidence anchors:
  - [abstract] "In addition, we test whether models are able to verify these solutions, and find that most models struggle."
  - [section 5.2] "These results collectively indicate that solution verification is beyond the current capabilities of many LLMs."

## Foundational Learning

- Concept: Mathematical reasoning and problem-solving strategies
  - Why needed here: The LLM needs to understand and apply mathematical concepts and problem-solving techniques to solve competition-level math problems.
  - Quick check question: Can the LLM correctly solve a variety of math problems without external guidance or hints?

- Concept: Context understanding and utilization
  - Why needed here: The LLM needs to effectively understand and utilize the provided concepts and hints to improve its problem-solving performance.
  - Quick check question: Does the LLM's performance improve when provided with relevant concepts and hints compared to solving problems without any additional information?

- Concept: Solution verification and error identification
  - Why needed here: The LLM needs to be able to critically analyze and evaluate the logical correctness of a given solution, identifying any errors or flaws in the reasoning.
  - Quick check question: Can the LLM accurately identify errors in a given solution, even if it is able to generate a correct solution itself?

## Architecture Onboarding

- Component map: Dataset (CHAMP) -> Evaluation pipeline -> Prompt design -> Result analysis
- Critical path: Collect and annotate dataset (CHAMP) -> Generate model solutions for evaluation -> Manually annotate first wrong steps in solutions -> Design and implement evaluation prompts -> Analyze and interpret results
- Design tradeoffs:
  - Dataset size vs. annotation quality: Smaller dataset with high-quality annotations vs. larger dataset with potentially noisier annotations
  - Prompt complexity vs. evaluation comprehensiveness: More complex prompts that incorporate various aspects of the problem vs. simpler prompts that focus on specific aspects
  - Automated evaluation vs. manual grading: Faster, more scalable evaluation using automated methods vs. more accurate evaluation using manual grading
- Failure signatures:
  - Low final answer accuracy: Indicates the LLM struggles with the math problems or fails to utilize the provided concepts and hints effectively
  - Large discrepancy between final answer and full solution accuracy: Suggests the LLM relies on heuristic shortcuts or pattern matching rather than true mathematical reasoning
  - Poor solution verification performance: Indicates the LLM lacks the ability to critically analyze and evaluate the logical correctness of a given solution
- First 3 experiments:
  1. Evaluate LLM performance on CHAMP dataset using standard prompts (problem statement only)
  2. Assess the impact of providing relevant concepts and hints on LLM performance using various prompt designs
  3. Analyze the discrepancy between final answer and full solution accuracy to understand the LLM's reasoning process

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs truly understand the mathematical concepts they use, or do they merely recognize patterns and apply memorized procedures?
- Basis in paper: [explicit] The paper highlights that models often arrive at correct answers through incorrect reasoning steps, suggesting they may rely on shortcut heuristics rather than genuine understanding.
- Why unresolved: Distinguishing between pattern recognition and true understanding requires more nuanced evaluation methods beyond final answer correctness.
- What evidence would resolve it: Designing problems that specifically test conceptual understanding, such as requiring explanations of why a certain approach works or asking models to identify and correct errors in flawed solutions.

### Open Question 2
- Question: How can we effectively leverage external information like concepts and hints to improve LLMs' mathematical reasoning capabilities?
- Basis in paper: [explicit] The paper explores various ways to provide concepts and hints to models, but the results show inconsistent performance improvements, indicating that the optimal method for incorporating this information remains unclear.
- Why unresolved: The impact of external information may depend on the model's architecture, training data, and the specific problem at hand. Further research is needed to identify the most effective strategies.
- What evidence would resolve it: Systematic experiments comparing different methods of providing external information, such as direct injection, retrieval-based approaches, or prompting strategies, and analyzing their impact on model performance across various problem types.

### Open Question 3
- Question: What are the limitations of current evaluation methods for assessing LLMs' mathematical reasoning abilities, and how can we develop more fine-grained benchmarks?
- Basis in paper: [explicit] The paper emphasizes the need for more fine-grained evaluation methods, as focusing solely on final answer accuracy can overestimate models' true reasoning capabilities. The introduction of the CHAMP dataset and FWS annotations aims to address this limitation.
- Why unresolved: Developing comprehensive benchmarks that capture various aspects of mathematical reasoning, such as problem-solving strategies, solution verification, and conceptual understanding, is a challenging task that requires ongoing research and collaboration.
- What evidence would resolve it: Creating and validating new datasets with diverse problem types, annotation schemes, and evaluation metrics that provide a more holistic assessment of LLMs' mathematical reasoning abilities.

## Limitations

- The dataset contains only 270 problems, which may not provide sufficient statistical power for fine-grained analyses
- The manual annotation process for first wrong steps is labor-intensive and may have consistency issues
- The evaluation focuses primarily on zero-shot and few-shot prompting without exploring chain-of-thought or more sophisticated prompting strategies
- The study does not investigate whether models can improve through fine-tuning on the CHAMP dataset

## Confidence

**High Confidence**: The claim that current LLMs struggle with competition-level mathematical reasoning is well-supported by the empirical results showing low accuracy rates (67% for best models) and the observation that models frequently arrive at correct answers through incorrect reasoning.

**Medium Confidence**: The claim that providing concepts and hints sometimes improves performance is supported by the data, but the inconsistent effects across models and provision methods suggest this is not a reliable strategy.

**Low Confidence**: The paper's claims about the dataset enabling fine-grained analyses are limited by the relatively small size and the labor-intensive nature of the annotations.

## Next Checks

1. **Annotation Reliability Test**: Conduct inter-rater reliability analysis on a subset of problems by having multiple annotators independently identify first wrong steps and compare their annotations. This would quantify the consistency of the labor-intensive FWS annotation process.

2. **Prompt Strategy Comparison**: Systematically compare the 17 prompt variants against more sophisticated prompting strategies like chain-of-thought reasoning, self-consistency sampling, and active prompting approaches. This would determine whether the limited effectiveness of concept/hint provision is due to the prompting methodology.

3. **Dataset Size Sensitivity Analysis**: Perform statistical power analysis to determine how the confidence intervals of the reported accuracy rates would change with different dataset sizes. This would help quantify whether the current 270-problem dataset is sufficient for the fine-grained analyses claimed.