---
ver: rpa2
title: 'ColBERT''s [MASK]-based Query Augmentation: Effects of Quadrupling the Query
  Input Length'
arxiv_id: '2408.13672'
source_url: https://arxiv.org/abs/2408.13672
tags:
- mask
- query
- tokens
- token
- colbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how ColBERT\u2019s use of [MASK] tokens\
  \ affects retrieval performance, specifically focusing on term weighting and scalability.\
  \ Through experiments, the authors demonstrate that [MASK] tokens primarily serve\
  \ to weight existing query terms rather than introduce new ones."
---

# ColBERT's [MASK]-based Query Augmentation: Effects of Quadrupling the Query Input Length

## Quick Facts
- **arXiv ID**: 2408.13672
- **Source URL**: https://arxiv.org/abs/2408.13672
- **Reference count**: 19
- **Primary result**: Performance improves when [MASK] tokens match training query length (~32), degrades with too few, and plateaus at 128 tokens.

## Executive Summary
This paper investigates how ColBERT's use of [MASK] tokens affects retrieval performance, focusing on term weighting and scalability. Through systematic experiments, the authors demonstrate that [MASK] tokens primarily serve to weight existing query terms rather than introduce new ones. They find that performance improves significantly when the number of [MASK] tokens is increased to match the average query length used during training (around 32 tokens), and remains stable even when the query length is quadrupled to 128 tokens. Notably, using too few [MASK] tokens (e.g., 8 or less) actually degrades performance compared to using none at all. These findings provide insights into ColBERT's query augmentation mechanism and its robustness to extended query lengths.

## Method Summary
The authors use a ColBERTv2 checkpoint trained with a maximum query length of 32 tokens and systematically vary the number of [MASK] tokens from 0 to 96 in steps of two. They evaluate retrieval effectiveness on three datasets: MS MARCO passage retrieval dev set, TREC 2019-2020 deep passage retrieval task, and TREC COVID dataset. Performance is measured using nDCG@10, nDCG@1000, MAP, and MRR under three conditions: set retrieval only, reranking only, and both phases combined.

## Key Results
- Performance improves significantly when [MASK] tokens match the training query length (~32 tokens)
- Using too few [MASK] tokens (≤8) degrades performance compared to using none
- Performance plateaus and remains stable when query length is quadrupled to 128 tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: [MASK] tokens in ColBERT primarily serve as term weighting signals rather than introducing new terms.
- Mechanism: During BERT contextualization, [MASK] tokens cannot attend to other tokens (attention scores are zeroed out), so their representations depend only on the non-[MASK] tokens. These [MASK] embeddings end up clustering near existing query terms, effectively reweighting them in the MaxSim scoring.
- Core assumption: The model treats [MASK] embeddings as auxiliary signals that modify the contribution of existing terms rather than generate new semantic content.
- Evidence anchors:
  - [abstract]: "Prior work shows [MASK] tokens weighting non-[MASK] query terms, emphasizing certain tokens over others, rather than introducing whole new terms as initially proposed."
  - [section]: "Wang et al. [15] considered whether [MASK]s in ColBERT actually add new terms to the query... They found that it did not, and presented an IDF-based approach for adding new terms to the query."
  - [corpus]: Weak evidence; no direct citations in neighbor papers to this specific claim.

### Mechanism 2
- Claim: Performance scales with the number of [MASK] tokens up to the training query length, then plateaus.
- Mechanism: Initially, adding [MASK] tokens increases term weighting granularity, improving retrieval. Once the number of [MASK] tokens matches the average query length used during training (32 tokens), additional [MASK] tokens produce diminishing returns because they follow a repeating cosine similarity pattern rather than adding new weighting capacity.
- Core assumption: ColBERT's training process conditions the model to expect a certain number of [MASK] tokens for optimal weighting; beyond that, extra tokens are redundant.
- Evidence anchors:
  - [abstract]: "performance improves significantly when the number of [MASK] tokens is increased to match the average query length used during training (around 32 tokens), and remains stable even when the query length is quadrupled to 128 tokens."
  - [section]: "From 4 to ~24 [MASK]s, however, we see a sharp increase in nDCG@10/@1000 and MAP(rel≥2). This peak coincides with the point where on average, queries have an overall length of 32 (i.e., the input size used for training)."
  - [corpus]: Weak evidence; neighbor papers do not address this specific scaling behavior.

### Mechanism 3
- Claim: Using too few [MASK] tokens (≤8) degrades performance compared to using none.
- Mechanism: With very few [MASK] tokens, the weighting mechanism is incomplete or unbalanced, leading to suboptimal term emphasis. This partial weighting is worse than no weighting at all because it misguides the scoring without sufficient granularity.
- Core assumption: The model's weighting mechanism requires a minimum number of [MASK] tokens to function effectively; below this threshold, the signal becomes misleading.
- Evidence anchors:
  - [abstract]: "using too few [MASK] tokens (e.g., 8 or less) actually degrades performance compared to using none at all."
  - [section]: "For most of the metrics, moving from 0 to 4 [MASK]s appears to actually have a detrimental affect, indicating only using a couple [MASK] tokens is worse than none at all."
  - [corpus]: No direct evidence; this is a specific finding from the paper.

## Foundational Learning

- **Concept**: MaxSim scoring in ColBERT
  - Why needed here: Understanding how ColBERT scores documents by matching each query token embedding to its most similar document token embedding is crucial to grasping how [MASK] tokens influence retrieval.
  - Quick check question: In MaxSim scoring, what happens to the contribution of a document token if it is not the closest match to any query token?

- **Concept**: BERT attention masking
  - Why needed here: Knowing that [MASK] tokens cannot attend to other tokens during self-attention explains why their embeddings behave differently and why they primarily weight existing terms.
  - Quick check question: During BERT contextualization, can a [MASK] token's representation be influenced by other tokens in the sequence?

- **Concept**: Term weighting vs. term expansion
  - Why needed here: Distinguishing between these two mechanisms is key to understanding why [MASK] tokens are more effective at emphasizing existing terms rather than introducing new ones.
  - Quick check question: What is the primary difference between term weighting and term expansion in the context of query processing?

## Architecture Onboarding

- **Component map**: BERT encoder -> [MASK] token padding -> MaxSim scoring -> Initial retrieval and reranking
- **Critical path**: 
  1. Tokenize query and pad with [MASK] tokens
  2. Generate contextualized embeddings
  3. Compute MaxSim scores against document embeddings
  4. Retrieve and rerank documents
- **Design tradeoffs**: 
  - Using [MASK] tokens for term weighting vs. using explicit IDF-based weights
  - Fixed query length (32 tokens) vs. variable length queries
  - Memory usage vs. retrieval effectiveness with more [MASK] tokens
- **Failure signatures**: 
  - Performance degradation with too few [MASK] tokens
  - Plateau in effectiveness with excessive [MASK] tokens
  - Inconsistent results across different datasets
- **First 3 experiments**: 
  1. Vary the number of [MASK] tokens from 0 to 96 and measure retrieval performance on TREC 2019-2020
  2. Compare baseline performance to performance when query length is extended to 128 tokens
  3. Test the effect of remapping [MASK] tokens to their nearest non-[MASK] embeddings on retrieval metrics

## Open Questions the Paper Calls Out

- **Open Question 1**: How does ColBERT's performance change when using query augmentation with terms beyond [MASK] tokens, such as pseudo-relevance feedback or other query expansion techniques?
- **Open Question 2**: Does the repeating cosine similarity pattern observed in [MASK] tokens (as query length increases beyond 32 tokens) hold across different BERT-based models or architectures?
- **Open Question 3**: What is the impact of varying the number of [MASK] tokens on ColBERT's performance for queries with significantly different lengths or structures (e.g., very short vs. very long queries)?

## Limitations
- All experiments are conducted on three specific datasets (MS MARCO, TREC 2019-2020, TREC COVID), which may limit generalizability
- The analysis assumes the ColBERTv2 checkpoint maintains zeroed-out attention scores for [MASK] tokens as in the original ColBERT
- Implementation details of the "set retrieval" and "reranking" phases are not fully specified, relying on PyTerrier bindings

## Confidence
- **High Confidence**: The core finding that [MASK] tokens primarily serve as term weighting signals rather than introducing new terms is supported by experimental results and the mechanism described (zeroed-out attention scores during BERT contextualization).
- **Medium Confidence**: The scaling behavior of performance with increasing [MASK] tokens depends on the specific training setup of the ColBERTv2 checkpoint and may not be universally applicable.
- **Low Confidence**: The claim that performance remains stable at 128 tokens is based on a single experimental condition and may not hold across all datasets or retrieval scenarios.

## Next Checks
1. Retrain ColBERTv2 with variable [MASK] token counts: Train a new ColBERTv2 checkpoint with a maximum query length of 64 or 128 tokens (instead of 32) and evaluate whether the optimal number of [MASK] tokens scales accordingly.
2. Analyze attention patterns for [MASK] tokens: Conduct a detailed analysis of the attention scores and embeddings for [MASK] tokens across different query lengths to verify clustering behavior and cosine similarity patterns.
3. Test on additional datasets and query types: Evaluate the performance of ColBERT with varying [MASK] token counts on a broader range of datasets, including those with longer or shorter average query lengths.