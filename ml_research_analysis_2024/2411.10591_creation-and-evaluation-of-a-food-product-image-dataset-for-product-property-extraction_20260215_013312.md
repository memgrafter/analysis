---
ver: rpa2
title: Creation and Evaluation of a Food Product Image Dataset for Product Property
  Extraction
arxiv_id: '2411.10591'
source_url: https://arxiv.org/abs/2411.10591
tags:
- product
- images
- image
- dataset
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a dataset of 1,034 high-quality food product
  images with detailed annotations for product recognition and classification tasks.
  The dataset includes 250 products with 30 object detection labels and 5 image classification
  labels based on GS1 standards.
---

# Creation and Evaluation of a Food Product Image Dataset for Product Property Extraction

## Quick Facts
- arXiv ID: 2411.10591
- Source URL: https://arxiv.org/abs/2411.10591
- Authors: Christoph Brosch; Alexander Bouwens; Sebastian Bast; Swen Haab; Rolf Krieger
- Reference count: 4
- One-line primary result: A dataset of 1,034 high-quality food product images with detailed annotations for product recognition and classification tasks, achieving mAP@.5 of 0.698 and recall of 0.496 for object detection using YOLOv5

## Executive Summary
This paper presents a novel dataset of 1,034 high-quality food product images designed to advance product property extraction in retail AI applications. The dataset includes 250 products with 30 object detection labels and 5 image classification labels based on GS1 standards. The authors demonstrate baseline performance using YOLOv5 for object detection (mAP@.5: 0.698, recall: 0.496) and ResNet50 for image classification (F1-scores: 0.73-0.88). The dataset addresses the critical need for training data in retail AI by providing standardized, professionally captured images with comprehensive annotations.

## Method Summary
The authors created a dataset of 1,034 high-quality food product images through professional studio photography, following GS1 standards for image specifications and product categorization. They used LabelImg and Label Studio for multi-format annotation, creating 30 object detection labels and 5 image classification labels per image. Baseline models were trained using pre-trained YOLOv5 for object detection with an 80/20 product-level split, and ResNet50 for image classification through fine-tuning. The dataset and evaluation code are publicly available in a GitLab repository.

## Key Results
- Baseline YOLOv5 model achieved mAP@.5 of 0.698 and recall of 0.496 for object detection
- ResNet50 image classification models achieved F1-scores of 0.73-0.88 depending on label type
- The dataset contains 1,034 images across 250 products with 30 object detection labels and 5 image classification labels
- Professional studio photography with optimal lighting and neutral backgrounds ensured consistent image quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality standardized images enable accurate object detection and classification
- Mechanism: The dataset uses professional studio photography with optimal lighting and neutral backgrounds, reducing visual noise and enabling consistent feature extraction by machine learning models
- Core assumption: Visual clarity and standardization are more important than variety in real-world conditions for initial model training
- Evidence anchors:
  - [abstract] "taken under studio conditions" with "optimal lighting, neutral backgrounds"
  - [section 3.3] "We used this part of the specification to determine, how to create product images" following GS1 standards
  - [corpus] Weak evidence - related papers focus on different approaches (LLMs, knowledge graphs) rather than image quality
- Break condition: If models trained on this dataset fail to generalize to real-world retail images with varied lighting and backgrounds

### Mechanism 2
- Claim: GS1 standard alignment enables dataset extensibility and cross-industry adoption
- Mechanism: By basing category selection, packaging types, and image specifications on GS1 standards, the dataset becomes interoperable with existing retail systems and can be extended by other researchers following the same guidelines
- Core assumption: Standardization reduces friction for adoption and extension by third parties
- Evidence anchors:
  - [section 3.1] "we provide researchers with a guideline on how to extend this dataset to cater to their economic or scientific needs"
  - [section 4.1] "All images were shot in JPG format, using a 1:1 aspect ratio, as described in the GS1 product image specification"
  - [section 4.2] "Where possible, we applied the naming convention provided by the GS1 Web Vocabulary"
- Break condition: If extension attempts by other researchers fail due to unclear or incomplete standard documentation

### Mechanism 3
- Claim: Comprehensive multi-label annotations enable both object detection and image classification tasks
- Mechanism: The dataset provides 30 object detection labels and 5 image classification labels per image, allowing researchers to train models for different levels of product property extraction (local features vs. global attributes)
- Core assumption: Multi-task annotations increase dataset utility and enable transfer learning between related computer vision tasks
- Evidence anchors:
  - [abstract] "annotated with 5 class labels and 30 object detection labels"
  - [section 4.2] "we created a list of 30 labels that can be used to label different relevant areas on retail products"
  - [section 4.3] "we categorised each product and annotated the images" with multiple classification attributes
- Break condition: If the annotation complexity overwhelms the dataset's practical utility or if models cannot effectively leverage the multi-label structure

## Foundational Learning

- Concept: Pascal VOC annotation format
  - Why needed here: The dataset uses XML files in Pascal VOC format for object detection labels, which is a standard format for computer vision datasets
  - Quick check question: What are the key components of a Pascal VOC XML annotation file for object detection?

- Concept: Intersection over Union (IoU) metric
  - Why needed here: The baseline model evaluation uses mAP@.5 and mAP@.5:.95 metrics which depend on IoU thresholds to determine correct object detection
  - Quick check question: How is IoU calculated and why is it important for evaluating object detection performance?

- Concept: Transfer learning with pre-trained models
  - Why needed here: The baseline models use pre-trained ResNet50 and YOLOv5 models, fine-tuning them on this specific dataset rather than training from scratch
  - Quick check question: What are the advantages of using pre-trained models for computer vision tasks on domain-specific datasets?

## Architecture Onboarding

- Component map: Image acquisition pipeline → Annotation system (LabelImg + Label Studio) → CSV metadata management → Model training (ResNet50 for classification, YOLOv5 for detection) → Evaluation framework
- Critical path: Product selection → Professional photography → Multi-format annotation → Metadata creation → Model training → Performance evaluation
- Design tradeoffs: Studio quality vs. real-world variety, comprehensive annotation vs. annotation effort, GS1 standardization vs. flexibility for novel use cases
- Failure signatures: Poor model generalization to real retail images, annotation inconsistencies across products, metadata misalignment between CSV files and image labels
- First 3 experiments:
  1. Train YOLOv5 on subset of high-confidence labels (barcode, nutritionTable) to establish baseline performance
  2. Fine-tune ResNet50 on material classification task to verify classification pipeline works
  3. Cross-validate object detection annotations by having a second annotator label 10% of images to measure inter-annotator agreement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance scale with increasing training dataset size, particularly for labels with high inner-class variation and low distinctness?
- Basis in paper: [explicit] The authors state they plan to analyze the influence of training dataset size on model performance, noting that labels with high inner-class variation and low distinctness (e.g., detailedProductName, ingredientStatement) likely require more training data than labels with low inner-class variation and high distinctness (e.g., barcode, nutritionTable).
- Why unresolved: The authors have not yet conducted experiments to empirically validate their hypothesis about the relationship between dataset size and model performance for different label types.
- What evidence would resolve it: Experimental results showing performance metrics (precision, recall, mAP) for object detection models trained on varying dataset sizes, particularly comparing performance trends between different label types.

### Open Question 2
- Question: Can synthetic product image generation effectively augment real training data to improve model generalization?
- Basis in paper: [explicit] The authors mention developing a prototype for synthetic product image generation and note that initial results with YOLOv5 showed a lack of generalization when trained on synthetic data, suggesting this as a future research direction.
- Why unresolved: The synthetic data generation approach is still in early development, and the authors have not yet demonstrated whether combining synthetic and real data improves model performance.
- What evidence would resolve it: Comparative studies showing model performance when trained on: 1) only real data, 2) only synthetic data, and 3) a combination of synthetic and real data, with ablation studies on different synthetic data generation parameters.

### Open Question 3
- Question: How effective are large language models like GPT-4 for extracting product properties from images compared to traditional computer vision approaches?
- Basis in paper: [explicit] The authors plan to evaluate large language models such as GPT-4 as an alternative approach for extracting information from product images.
- Why unresolved: This represents a planned future work item that has not yet been implemented or tested against the existing baseline models.
- What evidence would resolve it: Performance comparisons between GPT-4 and the baseline YOLOv5 models for extracting product properties, including accuracy metrics for different label types and computational efficiency analysis.

## Limitations
- The dataset focuses on high-quality studio images rather than real-world retail conditions, potentially limiting model generalization
- The relatively small size (1,034 images across 250 products) may affect model robustness for rare product categories
- Significant performance variation between object detection (mAP@.5: 0.698) and classification tasks (F1: 0.73-0.88) suggests potential architectural or annotation quality issues

## Confidence
- High confidence: Dataset creation methodology and GS1 standard alignment mechanisms
- Medium confidence: Baseline model performance claims and evaluation metrics
- Low confidence: Generalization capabilities to real-world retail environments and synthetic data generation effectiveness

## Next Checks
1. Test model performance on real retail images with varying lighting and backgrounds to assess generalization capability
2. Conduct inter-annotator agreement study on 10% of images to validate annotation consistency and quality
3. Perform ablation study removing GS1 standard constraints to determine if standardization limits model performance or extensibility