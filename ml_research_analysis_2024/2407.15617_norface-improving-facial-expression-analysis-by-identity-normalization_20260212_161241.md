---
ver: rpa2
title: 'Norface: Improving Facial Expression Analysis by Identity Normalization'
arxiv_id: '2407.15617'
source_url: https://arxiv.org/abs/2407.15617
tags:
- facial
- expression
- recognition
- ieee
- identity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Norface, a unified framework for facial expression
  analysis that addresses the problem of task-irrelevant noise like identity, pose,
  and background. The core idea is identity normalization, which transforms all input
  faces to a common identity with consistent pose and background while preserving
  facial expressions.
---

# Norface: Improving Facial Expression Analysis by Identity Normalization

## Quick Facts
- arXiv ID: 2407.15617
- Source URL: https://arxiv.org/abs/2407.15617
- Reference count: 40
- Primary result: Norface achieves up to 8.3% improvement in F1 score for AU detection and 9.34% improvement in accuracy for FER by using identity normalization to remove task-irrelevant noise.

## Executive Summary
This paper introduces Norface, a unified framework for facial expression analysis that addresses the problem of task-irrelevant noise like identity, pose, and background. The core innovation is identity normalization, which transforms all input faces to a common identity with consistent pose and background while preserving facial expressions. The framework consists of two stages: a normalization network using Masked AutoEncoder and Expression Merging Module to perform identity normalization, and a classification network incorporating Mixture of Experts to refine facial representations and handle multiple AU or emotion labels. Extensive experiments demonstrate that Norface outperforms state-of-the-art methods on multiple facial expression analysis tasks including AU detection, AU intensity estimation, and FER, as well as cross-dataset tasks.

## Method Summary
Norface is a two-stage framework consisting of a normalization network and a classification network. The normalization network uses a pre-trained Masked AutoEncoder to extract facial features and an Expression Merging Module to adaptively merge expression features from the original face into a target face while maintaining identity consistency through multiple loss functions. The classification network takes normalized images as input and uses a Mixture of Experts architecture to refine facial representations and produce predictions for multiple AU or emotion labels. The framework is trained end-to-end with carefully designed loss functions including adversarial, reconstruction, perceptual, identity, landmark, expression, and eyebrow losses for normalization, plus cross-entropy/L2, importance, and global/local losses for classification.

## Key Results
- Achieves up to 8.3% improvement in F1 score for AU detection compared to state-of-the-art methods
- Demonstrates 9.34% improvement in accuracy for FER tasks
- Shows strong cross-dataset generalization performance, reducing identity bias across different facial expression datasets
- Outperforms existing methods on multiple benchmarks including BP4D, BP4D+, DISFA, AffectNet, and RAF-DB

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Identity normalization reduces task-irrelevant noise by transforming all input faces to a common identity with consistent pose and background while preserving facial expressions.
- Mechanism: The normalization network uses a Masked AutoEncoder (MAE) to extract facial features, then an Expression Merging Module (EMM) adaptively merges expression features from the original face into the target face while maintaining identity consistency through multiple loss functions (identity, landmark, expression, and eyebrow losses).
- Core assumption: The normalization process can successfully disentangle facial expressions from identity, pose, and background factors while maintaining expression fidelity.
- Evidence anchors:
  - [abstract] "Norface consists of a normalization network and a classification network. First, the carefully designed normalization network struggles to directly remove the above task-irrelevant noise, by maintaining facial expression consistency but normalizing all original images to a common identity with consistent pose, and background."
  - [section] "Specifically, we employ a pre-trained Masked AutoEncoder (MAE) to extract facial features, effectively capturing both expression and other attributes. Then, we develop an Expression Merging Module (EMM) to adaptively merge expression features from the original faces into the target face."
  - [corpus] Weak - corpus contains related works on identity preservation and facial animation transfer, but none specifically address the identity normalization mechanism described in Norface.
- Break condition: If the normalization network cannot successfully disentangle expressions from identity/pose/background, or if expression consistency is compromised during normalization.

### Mechanism 2
- Claim: The Mixture of Experts (MoE) architecture in the classification network improves facial expression analysis by dynamically activating task-specific experts for different AUs or emotions.
- Mechanism: The classification network uses Input MoE to refine latent facial representations and Output MoE to facilitate detection, estimation, or recognition of multiple AU or emotion labels. Each MoE block contains multiple expert MLPs with a routing network that assigns weights based on input.
- Core assumption: Different facial expression analysis tasks benefit from specialized expert networks that can focus on specific patterns while maintaining overall task coherence.
- Evidence anchors:
  - [abstract] "Additionally, the classification network incorporates a Mixture of Experts to refine the latent representation, including handling the input of facial representations and the output of multiple (AU or emotion) labels."
  - [section] "MoE aims to learn several expert sub-branches which are automatically trained to dynamically activate task-specific experts. This work proposes Input and Output MoE modules, where the input MoE module can refine latent facial representation, and the output MoE module can facilitate the detection, estimation, or recognition of multiple AU or emotion labels."
  - [corpus] Moderate - corpus contains related works on mixture-of-experts models, but none specifically validate the dual-input/output MoE architecture for facial expression analysis.
- Break condition: If the MoE architecture fails to provide meaningful specialization or if the routing mechanism becomes unstable during training.

### Mechanism 3
- Claim: Using normalized images instead of expression features provides better performance because images contain more structured details at the pixel level that MAE can refine.
- Mechanism: The classification network takes normalized images as input rather than abstract expression features, allowing the pre-trained MAE to refine facial representations from these normalized images based on large-scale training.
- Core assumption: Pixel-level information in normalized images provides richer semantic context than abstract expression features for facial expression classification tasks.
- Evidence anchors:
  - [section] "Compared to highly abstract expression features, images encompass more structured details at pixel level. Furthermore, the used MAE can refine the facial representation from those normalized images due to its training on large-scale datasets. These factors contribute to the superior performance of normalized images over expression features."
  - [corpus] Weak - corpus contains related works on facial expression recognition and feature extraction, but none specifically compare normalized images versus expression features for this type of classification task.
- Break condition: If the additional pixel-level information in normalized images does not provide meaningful improvement over more abstract feature representations.

## Foundational Learning

- Concept: Masked AutoEncoder (MAE) architecture
  - Why needed here: MAE is used as the core feature extractor in the normalization network to capture both expression and identity attributes from facial images. Understanding MAE is crucial for implementing and modifying the normalization component.
  - Quick check question: How does the masking strategy in MAE force the model to learn topological and semantic information of masked image patches?

- Concept: Mixture of Experts (MoE) routing mechanism
  - Why needed here: MoE is the key architectural component in the classification network that enables dynamic expert activation. Understanding the Top-k routing strategy and importance loss is essential for proper implementation.
  - Quick check question: What is the purpose of the noisy Top-k routing strategy and how does it prevent expert collapse?

- Concept: Expression consistency loss functions
  - Why needed here: Multiple loss functions (expression, eyebrow, identity, landmark) are used to ensure the normalization network maintains expression fidelity while transforming identities. Understanding these losses is critical for proper training.
  - Quick check question: Why are both expression loss and eyebrow loss needed, and how do they complement each other in maintaining expression consistency?

## Architecture Onboarding

- Component map:
  - Normalization Network: MAE-based encoder + EMM + convolutional decoder
  - Classification Network: Face encoder + Input MoE + Output MoE + Linear classifier
  - Losses: Adversarial, reconstruction, perceptual, identity, landmark, expression, eyebrow (normalization); cross-entropy/L2, importance, global/local (classification)

- Critical path:
  1. Original face → MAE encoder → patch embeddings
  2. Target face → MAE encoder → patch embeddings
  3. EMM merges expression from original with attributes from target
  4. Decoder generates normalized face
  5. Normalized face + original face → classification network
  6. Input MoE refines representations → Output MoE produces final predictions

- Design tradeoffs:
  - Using normalized images vs. expression features (more pixel detail vs. computational efficiency)
  - Multiple expert networks vs. single unified network (specialization vs. simplicity)
  - Extensive loss functions vs. simpler training objectives (better consistency vs. training complexity)

- Failure signatures:
  - Identity normalization fails: expressions not preserved, artifacts in normalized images
  - MoE routing fails: all experts get similar weights, no specialization observed
  - Training instability: gradient explosion, loss oscillation, or convergence to poor local minima

- First 3 experiments:
  1. Validate identity normalization: Generate normalized faces for several identities and verify expression consistency visually and quantitatively
  2. Test MoE routing: Monitor expert weight distributions during training to ensure proper specialization
  3. Ablation study: Compare performance with and without identity normalization and MoE components to quantify their contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of target identity impact the performance of identity normalization across different facial expression analysis tasks?
- Basis in paper: [explicit] The paper discusses that different target identities contribute differently to performance enhancement and some identities may be unsuitable as target faces due to expression confusion.
- Why unresolved: The paper only mentions that certain identities are unsuitable but doesn't provide a systematic analysis of how different target identities affect performance across tasks or a method for selecting optimal target identities.
- What evidence would resolve it: Empirical results showing performance comparisons across multiple target identities for each task, and ideally a method for automatically selecting the most suitable target identity based on dataset characteristics.

### Open Question 2
- Question: Can the identity normalization framework be extended to handle video sequences rather than just single-frame images?
- Basis in paper: [inferred] The paper focuses on single-frame datasets and explicitly states that most expression reenactment methods rely on consecutive frames, but the normalization network is not designed for temporal information.
- Why unresolved: The paper does not explore temporal consistency or the application of identity normalization to video sequences, which would be valuable for real-world applications involving dynamic facial expressions.
- What evidence would resolve it: Experimental results demonstrating identity normalization applied to video sequences, with metrics evaluating both spatial normalization quality and temporal consistency across frames.

### Open Question 3
- Question: What is the impact of identity normalization on the model's ability to generalize to completely unseen identities not present in the training data?
- Basis in paper: [explicit] The paper states that identity normalization mitigates identity bias and helps the model generalize to unseen identities, but doesn't provide quantitative analysis of this generalization capability.
- Why unresolved: While the paper claims improved generalization, it doesn't provide controlled experiments measuring performance specifically on identities not seen during training or validation.
- What evidence would resolve it: Controlled experiments comparing model performance on held-out identities (completely unseen during training) versus seen identities, with and without identity normalization, to quantify the actual improvement in generalization.

## Limitations

- The framework's performance depends heavily on the quality of the pre-trained MAE and careful balancing of multiple loss functions, making it sensitive to hyperparameter tuning
- Using a single fixed target identity for normalization may limit generalizability across diverse facial structures and may not be optimal for all tasks
- The computational overhead of generating normalized images for each input adds processing time compared to direct feature-based approaches

## Confidence

- Identity normalization mechanism: Medium - Strong empirical results but limited ablation studies on individual components
- MoE architecture benefits: Medium - Performance gains demonstrated but routing mechanism stability not thoroughly validated
- Normalized images vs. features: Low - Claim supported by results but lacks direct comparative analysis

## Next Checks

1. Conduct ablation studies to isolate the contribution of identity normalization from other framework components by testing with identity-preserving transformations
2. Evaluate the framework's performance with multiple target identities to assess sensitivity to identity normalization choices
3. Test the approach on out-of-distribution datasets with significantly different demographic characteristics to validate robustness to identity variation