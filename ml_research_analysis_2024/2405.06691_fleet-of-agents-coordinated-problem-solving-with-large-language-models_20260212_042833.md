---
ver: rpa2
title: 'Fleet of Agents: Coordinated Problem Solving with Large Language Models'
arxiv_id: '2405.06691'
source_url: https://arxiv.org/abs/2405.06691
tags:
- agents
- cost
- language
- resampling
- fleet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fleet of Agents (FoA) is a novel framework that uses genetic-type
  particle filtering to coordinate multiple LLM agents in dynamic tree searches. It
  spawns n agents to explore the search space autonomously, then resamples them based
  on a heuristic value function to balance exploration and exploitation.
---

# Fleet of Agents: Coordinated Problem Solving with Large Language Models

## Quick Facts
- arXiv ID: 2405.06691
- Source URL: https://arxiv.org/abs/2405.06691
- Authors: Lars Klein; Nearchos Potamitis; Roland Aydin; Robert West; Caglar Gulcehre; Akhil Arora
- Reference count: 40
- Key outcome: Fleet of Agents achieves ~5% quality improvement while requiring only ~40% of the cost compared to previous state-of-the-art methods

## Executive Summary
Fleet of Agents (FoA) introduces a novel framework that coordinates multiple LLM agents using genetic-type particle filtering to solve complex reasoning tasks. The approach spawns multiple agents that explore search spaces autonomously, then resamples them based on a heuristic value function to optimize exploration-exploitation balance. Remarkably, FoA + LLaMA3.2-11B surpasses the larger LLaMA3.2-90B model, demonstrating that coordinated search can bridge reasoning gaps between different model sizes.

## Method Summary
FoA implements genetic-type particle filtering by spawning n agents that explore the search space autonomously for k steps each, followed by a selection phase where agents are resampled with replacement based on their heuristic value function scores. The framework offers precise control over tree width (number of agents) and depth (steps per agent), leading to predictable latency and cost. Agents use a common cache to store intermediate states and batch LLM requests to reduce overhead. The method was tested on three benchmark tasks: Game of 24 (1362 subtasks), Mini Crosswords (156 subtasks), and WebShop (12,087 subtasks).

## Key Results
- FoA achieves the best cost-quality trade-off among all benchmarked methods, with ~5% quality improvement at ~40% of the cost
- FoA + LLaMA3.2-11B surpasses the larger Llama3.2-90B model, demonstrating bridging of reasoning gaps between model sizes
- On average across all tasks and models, FoA achieves the best cost-quality trade-off

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FOA achieves better exploration-exploitation balance by resampling agents based on value function estimates.
- Mechanism: The genetic-type particle filtering in FOA mutates each agent's state independently, then resamples agents proportional to their heuristic value, concentrating search effort on promising regions while preserving diversity.
- Core assumption: The heuristic value function accurately estimates the utility of states and guides meaningful resampling.
- Evidence anchors:
  - [abstract] "FOA spawns a multitude of agents, each exploring the search space autonomously, followed by a selection phase where resampling based on a heuristic value function optimizes the balance between exploration and exploitation."
  - [section] "FOA spawns a fleet of n agents that collectively search for a solution. The genetic filtering search has a mutation phase during which each agent explores the search space autonomously... During the selection phase, we resample, with replacement, the population of agents."
  - [corpus] Weak evidence: related works focus on single-agent or static coordination, not dynamic particle filtering.
- Break condition: If the value function is inaccurate or noisy, resampling may concentrate agents on misleading states, reducing overall search effectiveness.

### Mechanism 2
- Claim: FOA enables smaller models to match or exceed larger models by compensating reasoning gaps through coordinated search.
- Mechanism: The fleet structure allows multiple smaller agents to explore different reasoning paths in parallel, with resampling ensuring high-value paths are pursued, effectively pooling reasoning capacity.
- Core assumption: Parallel exploration by multiple smaller agents can discover reasoning paths that single larger agents miss.
- Evidence anchors:
  - [abstract] "Notably, our analyses reveal that (1) FOA achieves the best cost-quality trade-off among all benchmarked methods and (2) FOA + LLaMA3.2-11B surpasses the Llama3.2-90B model."
  - [section] "Notably, FOA + LLaMA3.2-11B surpasses the larger Llama3.2-90B model, demonstrating that FOA can bridge reasoning gaps between different model sizes."
  - [corpus] Weak evidence: no direct corpus examples of smaller models outperforming larger ones through coordination.
- Break condition: If the smaller model's reasoning capability is fundamentally too weak, even coordinated search may not compensate for the capability gap.

### Mechanism 3
- Claim: FOA's precise control over tree width and depth leads to predictable latency and cost.
- Mechanism: By explicitly setting the number of agents (tree width) and steps per agent (tree depth), FOA avoids exponential growth in search space that occurs in uncontrolled tree-search methods.
- Core assumption: Controlling agent count and steps directly translates to predictable computational resource usage.
- Evidence anchors:
  - [section] "FOA offers precise control over the tree's width (n agents) and depth (t steps), leading to predictable latency and cost. In particular, it is possible to tune the size n of the fleet to the available resources."
  - [section] "FOA achieves the best cost-quality trade-off among all benchmarked methods... FOA obtains a quality improvement of ≃ 5% while requiring only ≃ 40% of the cost of previous SOTA methods."
  - [corpus] Weak evidence: related works mention cost but not precise control mechanisms.
- Break condition: If value function evaluations are expensive or unpredictable, the cost predictability benefit diminishes.

## Foundational Learning

- Concept: Genetic particle filtering
  - Why needed here: FOA uses genetic-type particle filtering to coordinate multiple LLM agents in dynamic tree searches, enabling exploration-exploitation balance.
  - Quick check question: What are the two phases of genetic particle filtering and how do they relate to agent coordination?

- Concept: Heuristic value function
  - Why needed here: FOA relies on a heuristic value function to guide resampling of agents, determining which states to explore further.
  - Quick check question: How does the value function estimate state utility and what happens if it's inaccurate?

- Concept: Dynamic tree search
  - Why needed here: FOA performs dynamic tree searches where agents explore and branch based on discovered solutions, unlike static tree structures.
  - Quick check question: How does FOA's dynamic branching differ from traditional tree search approaches?

## Architecture Onboarding

- Component map:
  - Initialize n agents with starting states
  - Run mutation phase for k steps per agent
  - Evaluate all states with value function
  - Resample agents based on value weights
  - Check for solution or timeout
  - Repeat until termination

- Critical path:
  1. Initialize n agents with starting states
  2. Run mutation phase for k steps
  3. Evaluate all states with value function
  4. Resample agents based on value weights
  5. Check for solution or timeout
  6. Repeat until termination

- Design tradeoffs:
  - Agent count vs. cost: More agents improve exploration but increase cost
  - Steps per agent vs. depth: More steps allow deeper reasoning but risk wasted computation
  - Resampling frequency vs. stability: Frequent resampling adapts quickly but may oscillate
  - Backtracking discount vs. exploration: Higher discount encourages revisiting but risks stale states

- Failure signatures:
  - Low success rate with moderate cost: Value function may be inaccurate
  - High cost with no improvement: Too many agents or steps relative to task complexity
  - Inconsistent results across runs: Resampling mechanism may be too stochastic

- First 3 experiments:
  1. Vary agent count (1, 3, 5, 9) on Game of 24 to find optimal exploration-exploitation balance
  2. Test different resampling frequencies (k=0, 1, 3, 6) to observe adaptation speed vs. stability
  3. Compare linear vs. exponential vs. greedy resampling methods on Mini Crosswords to evaluate selection strategy impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal adaptive fleet size strategy for varying task difficulties, and how would it impact cost-quality trade-offs?
- Basis in paper: [explicit] The paper mentions "Currently, we assign a fixed number n of agents to each task. However, it may be advantageous to allocate more agents to more difficult tasks to enhance sample efficiency."
- Why unresolved: The paper only discusses a fixed fleet size and does not explore adaptive sizing based on task difficulty.
- What evidence would resolve it: Experimental results comparing fixed vs. adaptive fleet sizes across tasks of varying difficulty, showing cost-quality trade-offs.

### Open Question 2
- Question: How do different resampling mechanisms (beyond linear, greedy, and exponential weighting) affect exploration-exploitation balance and performance?
- Basis in paper: [inferred] The paper mentions "This framework can capture many resampling schemes" but only evaluates a few, suggesting potential for other mechanisms.
- Why unresolved: The paper only tests three resampling schemes and does not explore more sophisticated or learned resampling strategies.
- What evidence would resolve it: Systematic comparison of various resampling mechanisms (e.g., learned, Bayesian, or entropy-based) across multiple tasks.

### Open Question 3
- Question: How does hierarchical fleet organization with nested particle filtering improve performance compared to the current flat structure?
- Basis in paper: [explicit] The paper states "Currently, we consider a homogenous fleet of identical agents. In the future, we would like to introduce further coordination between the individual agents with a hierarchical organizational structure."
- Why unresolved: The paper only implements a flat fleet structure and does not explore hierarchical or nested coordination.
- What evidence would resolve it: Experimental results comparing flat vs. hierarchical fleet structures on complex tasks requiring multi-level reasoning.

### Open Question 4
- Question: What is the impact of caching on solution quality consistency across multiple runs, and how can potential inconsistencies be mitigated?
- Basis in paper: [explicit] The ablation analysis shows "disabling the cache leads FOA to incur a higher cost, but interestingly, it also leads to a lower success rate" with a hypothesis about inconsistencies.
- Why unresolved: The paper only observes the correlation between caching and quality but does not investigate the underlying mechanisms causing inconsistencies.
- What evidence would resolve it: Detailed analysis of solution consistency with and without caching, and experiments testing methods to mitigate potential inconsistencies.

## Limitations
- The heuristic value function implementation details are not fully specified, making exact reproduction challenging
- The claim that FOA can "bridge reasoning gaps between different model sizes" is based primarily on one comparison rather than systematic testing across multiple model size pairs
- The cost comparison is made against previous state-of-the-art methods, but doesn't explore how FOA performs against simpler baselines like greedy search or beam search

## Confidence
- **High confidence**: The claim that FOA achieves better cost-quality trade-off than previous methods (5% quality improvement at 40% of the cost) is well-supported by the experimental results across all three tasks.
- **Medium confidence**: The claim that FOA can enable smaller models to match or exceed larger models is supported by the LLaMA3.2-11B vs LLaMA3.2-90B comparison, but would benefit from more systematic testing across different model size combinations.
- **Medium confidence**: The mechanism of genetic particle filtering enabling exploration-exploitation balance is theoretically sound and demonstrated empirically, though the exact impact of the value function accuracy on performance remains somewhat unclear.

## Next Checks
1. Test FOA across more diverse model size pairs: Systematically compare FOA performance with different combinations of small, medium, and large models (e.g., GPT-3.5 vs GPT-4, LLaMA3.2-8B vs LLaMA3.2-70B) to validate whether the size-bridging capability is generalizable or task-specific.

2. Benchmark against simpler baselines: Compare FOA performance against greedy search and beam search approaches with similar computational budgets to quantify the actual benefit of the genetic particle filtering mechanism versus simpler search strategies.

3. Analyze value function sensitivity: Conduct ablation studies varying the heuristic value function implementation (e.g., using random values, simple heuristics, and learned value functions) to quantify how sensitive FOA's performance is to value function quality and determine if simpler alternatives could achieve similar results.