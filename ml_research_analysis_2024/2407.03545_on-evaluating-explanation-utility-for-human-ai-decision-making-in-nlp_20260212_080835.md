---
ver: rpa2
title: On Evaluating Explanation Utility for Human-AI Decision Making in NLP
arxiv_id: '2407.03545'
source_url: https://arxiv.org/abs/2407.03545
tags:
- hazard
- information
- task
- human
- application
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the lack of application-grounded human evaluations
  for explainable NLP models. It establishes criteria for selecting suitable datasets
  and tasks, finding only 4 out of 53 datasets appropriate for studying explanation
  usefulness in human-AI decision making.
---

# On Evaluating Explanation Utility for Human-AI Decision Making in NLP
## Quick Facts
- arXiv ID: 2407.03545
- Source URL: https://arxiv.org/abs/2407.03545
- Reference count: 40
- Key outcome: Only 4 out of 53 datasets suitable for studying explanation utility in human-AI decision making; explanations don't improve team performance in legal claim verification

## Executive Summary
This paper addresses the critical gap in application-grounded human evaluations for explainable NLP models. The authors establish rigorous criteria for selecting datasets and tasks suitable for studying explanation utility in human-AI collaboration, finding that only 4 out of 53 NLP datasets meet these standards. Through two exemplar studies using the ContractNLI dataset for legal claim verification, they demonstrate that providing explanations to human experts does not significantly improve human-AI team performance. The research reveals that the main challenge lies in getting experts to invest sufficient time in instances before leveraging AI assistance, suggesting that automatic deferral might be a more promising approach for effective human-AI collaboration.

## Method Summary
The authors developed a comprehensive framework for selecting appropriate datasets for human-AI evaluation studies. They established four key criteria: (1) presence of a human-machine team setting with time constraints, (2) human decision-maker responsibility for final decisions, (3) high-stakes real-world consequences, and (4) a suitable NLP task. After applying these criteria to 53 NLP datasets, only 4 were deemed appropriate. The two exemplar studies focused on legal claim verification using the ContractNLI dataset, where participants acted as legal experts verifying contract claims. Participants were divided into control and treatment groups, with the treatment group receiving AI explanations alongside predictions.

## Key Results
- Only 4 out of 53 NLP datasets meet the criteria for application-grounded human-AI evaluation studies
- Explanations did not significantly improve human-AI team performance in legal claim verification tasks
- The primary challenge identified is getting experts to spend sufficient time on instances before using AI assistance

## Why This Works (Mechanism)
The findings suggest that human-AI collaboration in high-stakes NLP tasks is more complex than simply providing explanations. When experts have limited time and bear full responsibility for decisions, they may not effectively integrate AI explanations into their decision-making process. The lack of improvement from explanations indicates that the cognitive load of processing both the task and explanations, combined with time pressure, may overwhelm experts rather than assist them. This suggests that effective human-AI collaboration requires addressing fundamental human factors like time allocation and cognitive load, rather than focusing solely on explanation quality.

## Foundational Learning
- **Dataset Selection Criteria**: Why needed - To ensure studies reflect real-world human-AI collaboration; Quick check - Can the task be completed within time constraints while maintaining high stakes?
- **Application-Grounded Evaluation**: Why needed - To bridge the gap between theoretical explanations and practical utility; Quick check - Does the evaluation setting mirror actual human-AI team environments?
- **Human-AI Team Dynamics**: Why needed - To understand how humans actually interact with AI assistance; Quick check - Are experts making final decisions and bearing responsibility?
- **Time Pressure Effects**: Why needed - To simulate real-world constraints on expert decision-making; Quick check - Does the task design incorporate realistic time limitations?
- **Explanation Utility Measurement**: Why needed - To quantify whether explanations actually improve collaboration; Quick check - Are performance metrics aligned with real-world outcomes?

## Architecture Onboarding
**Component Map**: Dataset selection -> Human subjects recruitment -> Task design -> Explanation implementation -> Performance measurement
**Critical Path**: Expert receives AI prediction → Expert decides whether to view explanation → Expert makes final decision → Performance is measured
**Design Tradeoffs**: Time constraints vs. explanation comprehension vs. decision accuracy
**Failure Signatures**: When experts don't engage with explanations, when time pressure leads to rushed decisions, when explanation complexity overwhelms cognitive capacity
**3 First Experiments**:
1. Test different explanation formats (text vs. visual) under identical time constraints
2. Vary the timing of explanation availability (before vs. after initial prediction)
3. Implement automatic deferral thresholds and measure impact on overall team performance

## Open Questions the Paper Calls Out
The paper explicitly calls out the need to explore automatic deferral as a more promising approach for human-AI collaboration. It questions whether the current paradigm of providing explanations to time-constrained experts is fundamentally flawed, and whether alternative approaches that minimize human cognitive load while maintaining decision quality should be prioritized. The authors also highlight the need for more diverse high-stakes domains to validate whether the legal claim verification findings generalize to other contexts.

## Limitations
- Narrow dataset selection resulting in only 4 suitable datasets out of 53 examined
- Findings may not generalize beyond legal claim verification to other high-stakes domains
- The specific implementation of explanations or task structure could influence the negative results
- Limited exploration of alternative explanation formats or delivery mechanisms

## Confidence
**High**: Methodology for dataset selection and experimental design using ContractNLI
**Medium**: Generalizability of findings regarding explanation utility across domains
**Medium**: Proposed shift toward automatic deferral as a solution for human-AI collaboration

## Next Checks
1. Replicate studies across diverse domains (medical diagnosis, financial auditing) to test generalizability of findings that explanations don't improve human-AI team performance
2. Conduct ablation studies varying explanation types, presentation formats, and timing of delivery to isolate factors affecting human-AI collaboration
3. Implement and evaluate automatic deferral systems in controlled experiments to determine if they address the expert time allocation problem identified in the paper