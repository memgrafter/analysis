---
ver: rpa2
title: 'CHIQ: Contextual History Enhancement for Improving Query Rewriting in Conversational
  Search'
arxiv_id: '2406.05013'
source_url: https://arxiv.org/abs/2406.05013
tags:
- query
- history
- conversational
- search
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes CHIQ, a method that enhances conversational\
  \ history quality using open-source LLMs before query rewriting in conversational\
  \ search. It addresses the challenge of ambiguous queries by applying five NLP techniques\u2014\
  question disambiguation, response expansion, pseudo response generation, topic switching\
  \ detection, and history summarization\u2014to improve context clarity."
---

# CHIQ: Contextual History Enhancement for Improving Query Rewriting in Conversational Search

## Quick Facts
- arXiv ID: 2406.05013
- Source URL: https://arxiv.org/abs/2406.05013
- Authors: Fengran Mo; Abbas Ghaddar; Kelong Mao; Mehdi Rezagholizadeh; Boxing Chen; Qun Liu; Jian-Yun Nie
- Reference count: 24
- One-line primary result: Open-source LLM-based history enhancement improves conversational query rewriting, matching closed-source model performance

## Executive Summary
CHIQ addresses the challenge of ambiguous queries in conversational search by enhancing conversation history quality before query rewriting. The method leverages open-source LLMs to apply five NLP techniques - question disambiguation, response expansion, pseudo response generation, topic switching detection, and history summarization - to clarify search intent. Experimental results on five benchmarks demonstrate that CHIQ outperforms state-of-the-art methods and rivals systems using closed-source LLMs while maintaining competitive performance.

## Method Summary
CHIQ is a two-step method that enhances conversational history using open-source LLMs before query rewriting. The approach applies five NLP techniques to resolve ambiguities in conversation history, then generates search queries using both ad-hoc rewriting and search-oriented fine-tuned models. The method includes a fusion step that combines results from different approaches, capturing complementary information that neither method captures alone. The process uses open-source LLMs (LLaMA-2-7B, Mistral-2-7B) to enhance history and generate pseudo-supervision signals for fine-tuning smaller models.

## Key Results
- CHIQ outperforms state-of-the-art methods across five benchmarks in both dense and sparse retrieval tasks
- The approach achieves significant improvements in MRR, NDCG@3, and Recall@10 metrics
- CHIQ rivals systems using closed-source LLMs while maintaining competitive performance with open-source models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enhancing conversation history before query rewriting improves search effectiveness.
- **Mechanism:** Five NLP techniques (question disambiguation, response expansion, pseudo response generation, topic switching detection, and history summarization) reduce ambiguity and noise in conversation history, leading to clearer search intent representation.
- **Core assumption:** Open-source LLMs possess sufficient basic NLP capabilities to effectively perform these enhancement tasks.
- **Evidence anchors:** Abstract states the method "leverages the capabilities of LLMs to resolve ambiguities in the conversation history"; section mentions "basic NLP task-solving capabilities, which even small-scale open-source LLMs (e.g., 7B) possess"; corpus shows weak evidence from related papers.
- **Break condition:** Method fails if open-source LLMs cannot adequately resolve coreference relations, expand responses, or detect topic switches.

### Mechanism 2
- **Claim:** Search-oriented fine-tuning with pseudo supervision signals improves query rewriting model performance.
- **Mechanism:** Generates multiple pseudo-queries from enhanced history and selects the one with highest retrieval score as supervision, creating better training signals than human-rewritten queries.
- **Core assumption:** Pseudo-queries generated by LLMs can serve as effective supervision signals for fine-tuning smaller models.
- **Evidence anchors:** Section describes extending approach to generate pseudo-supervision signals; section explains selecting query with highest retrieval score as supervision signal; corpus shows weak evidence from related papers.
- **Break condition:** Method fails if generated pseudo-queries consistently underperform human-rewritten queries or if retrieval-based selection doesn't capture true relevance.

### Mechanism 3
- **Claim:** Combining ad-hoc query rewriting and search-oriented fine-tuned models through fusion yields superior performance.
- **Mechanism:** Aggregates rank lists from two different query generation methods, capturing complementary information that neither method captures alone.
- **Core assumption:** Different query generation approaches produce complementary retrieval results that can be effectively combined.
- **Evidence anchors:** Section describes fusing rank lists using result-level fusion technique; section notes CHIQ-FT generates complementary content not captured by CHIQ-AD; corpus shows weak evidence from related papers.
- **Break condition:** Method fails if two approaches generate highly redundant or conflicting queries that don't complement each other.

## Foundational Learning

- **Concept:** Conversational search and query rewriting
  - Why needed here: Understanding the context-dependent nature of conversational queries and importance of rewriting them for effective search
  - Quick check question: What are the main challenges in conversational search compared to traditional ad-hoc search?

- **Concept:** Large language models and their capabilities
  - Why needed here: Understanding basic NLP capabilities of LLMs and how they can be leveraged for conversational search tasks
  - Quick check question: What are the key differences between open-source and closed-source LLMs in terms of capabilities and accessibility?

- **Concept:** Dense and sparse retrieval methods
  - Why needed here: Understanding different retrieval approaches and their performance characteristics in conversational search
  - Quick check question: What are the main differences between dense and sparse retrieval, and when might one be preferred over the other?

## Architecture Onboarding

- **Component map:** Conversational history (H) and new user question (un+1) -> Enhancement module (5 NLP techniques) -> Query generation (ad-hoc rewriting and fine-tuned models) -> Fusion module (combines results) -> Ranked list of relevant passages

- **Critical path:** 1) Receive conversational history and new question, 2) Apply enhancement techniques to improve history quality, 3) Generate queries using ad-hoc rewriting and fine-tuned models, 4) Perform retrieval using both query sets, 5) Fuse retrieval results to produce final ranked list

- **Design tradeoffs:** Open-source vs. closed-source LLMs (cost and accessibility vs. potentially superior performance), single vs. multiple enhancement techniques (simplicity vs. comprehensive ambiguity resolution), separate vs. fused approaches (modularity vs. potentially better performance)

- **Failure signatures:** Poor performance on topic-switching datasets (indicates issues with topic switch detection), inconsistent results across different LLMs (suggests model-specific limitations), no improvement over baseline methods (indicates problems with enhancement techniques or query generation)

- **First 3 experiments:** 1) Compare performance of each individual enhancement technique to understand their individual contributions, 2) Evaluate impact of using different open-source LLMs (e.g., LLaMA vs. Mistral) on overall performance, 3) Test fusion approach with different balance factors to optimize result combination

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does CHIQ perform with larger open-source LLMs (e.g., Mixtral 7B, LLaMA 3B/70B) compared to Mistral-2-7B and LLaMA-2-7B?
- **Basis in paper:** The paper states it did not experiment with larger open-source LLMs due to computation limitations.
- **Why unresolved:** The paper did not explore performance of CHIQ with larger open-source models like Mixtral or LLaMA 3B/70B, which could potentially enhance its effectiveness.
- **What evidence would resolve it:** Experiments comparing CHIQ performance using different sizes of open-source LLMs, such as Mixtral 7B and LLaMA 3B/70B, on the same benchmarks.

### Open Question 2
- **Question:** How would CHIQ perform with newer closed-source models like GPT-4 or Claude, and what would be the impact of history enhancement?
- **Basis in paper:** The paper mentions not incorporating more closed-source models like GPT-4 or Claude due to financial constraints.
- **Why unresolved:** The paper only tested with ChatGPT-3.5, so performance impact of using newer models like GPT-4 or Claude with CHIQ is unknown.
- **What evidence would resolve it:** Experiments comparing CHIQ performance using different closed-source models (GPT-4, Claude) with history enhancement, showing their effectiveness relative to each other and open-source models.

### Open Question 3
- **Question:** What would be the effect of adding a backoff filtering strategy to detect noisy LLM outputs in CHIQ?
- **Basis in paper:** The paper mentions potential design choices for further analysis, including adding a backoff filtering strategy to detect noisy LLM outputs.
- **Why unresolved:** The paper did not implement or test a backoff filtering strategy, so its impact on improving CHIQ's performance is unknown.
- **What evidence would resolve it:** Experiments comparing CHIQ with and without a backoff filtering strategy, measuring reduction in noisy outputs and overall performance improvement.

### Open Question 4
- **Question:** How does CHIQ handle complex conversational scenarios involving multiple topic switches, and what are its limitations?
- **Basis in paper:** The paper discusses importance of detecting topic switches in conversation history for improving query rewriting.
- **Why unresolved:** The paper did not extensively explore CHIQ's performance in complex conversational scenarios with multiple topic switches, nor did it detail its limitations in such cases.
- **What evidence would resolve it:** Experiments testing CHIQ on datasets with complex conversational scenarios involving multiple topic switches, analyzing its performance and identifying specific limitations or failure cases.

## Limitations
- Limited evaluation of larger open-source LLMs (13B, 33B) to understand performance scaling
- Missing ablation studies to isolate individual contributions of each enhancement technique
- Unclear exact prompt templates and instructions for the five NLP enhancement techniques

## Confidence
- **High confidence** in overall approach's effectiveness (consistent improvements across multiple benchmarks)
- **Medium confidence** in specific mechanism claims (theoretical justification but limited ablation studies)
- **Medium confidence** in generalizability claims (zero-shot results show promise but limited to one transfer scenario)

## Next Checks
1. Conduct ablation studies to isolate contribution of each enhancement technique (question disambiguation, response expansion, etc.) to identify which are most critical for performance gains
2. Test approach with different open-source LLM sizes (13B, 33B) to understand performance scaling and determine if smaller models can achieve comparable results
3. Evaluate robustness of method across different retrieval models beyond specific dense and sparse retrievers used in current experiments