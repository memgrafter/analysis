---
ver: rpa2
title: Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter
  Merging
arxiv_id: '2410.01610'
source_url: https://arxiv.org/abs/2410.01610
tags:
- upit
- expert
- experts
- upcycling
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UpIT, a data-efficient approach for transforming
  dense pre-trained models into MoE instruction models by leveraging intermediate
  checkpoints during instruction tuning. The method uses genetic algorithms and parameter
  merging to expand experts flexibly while pre-optimizing routing vectors to ensure
  each expert performs optimally.
---

# Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter Merging

## Quick Facts
- arXiv ID: 2410.01610
- Source URL: https://arxiv.org/abs/2410.01610
- Authors: Tingfeng Hui; Zhenyu Zhang; Shuohuan Wang; Yu Sun; Hua Wu; Sen Su
- Reference count: 20
- Primary result: Upcycling dense models to MoE achieves up to 2.76% better average performance than baselines across 9 benchmarks

## Executive Summary
This paper introduces UpIT, a data-efficient approach for transforming dense pre-trained models into MoE instruction models. The method leverages intermediate checkpoints during instruction tuning, using genetic algorithms and parameter merging to expand experts flexibly while pre-optimizing routing vectors. Experiments demonstrate that UpIT consistently outperforms existing upcycling methods across various data scales and settings, achieving superior scalability and data efficiency compared to traditional dense instruction tuning approaches.

## Method Summary
UpIT transforms dense pre-trained models into MoE instruction models through four stages: Expert Preparation (fine-tune dense model and save checkpoints), Expert Expansion (genetic algorithm with parameter merging to create experts), Router Initialization (pre-optimize routing vectors with auxiliary loss using expert-specific data), and Model Upcycling (merge experts and routers). The approach uses intermediate checkpoints as naturally specialized experts, expands them using genetic algorithms with DARE mutation, and initializes routing vectors using 1% seed data selected by perplexity to ensure optimal token-expert matching.

## Key Results
- UpIT consistently outperforms existing upcycling methods across various data scales
- Achieves up to 2.76% better average performance than baselines on 9 benchmarks
- Demonstrates superior scalability, maintaining stable improvements when scaling expert numbers or training data
- Shows continued performance gains beyond dense baseline plateaus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intermediate checkpoints during instruction tuning serve as naturally specialized experts for different domains.
- Mechanism: Checkpoints saved at different epochs exhibit distinct performance strengths across benchmarks (e.g., epoch 2 excels in HellaSwag, epoch 0.25 excels in MMLU), suggesting domain-specific expertise.
- Core assumption: The divergence in checkpoint performance across benchmarks reflects genuine specialization rather than random variation.
- Evidence anchors:
  - [abstract] "different checkpoints saved at different epochs excel in different benchmarks"
  - [section 2.2] "models with different training steps demonstrate varying expertise in handling distinct domains"
  - [corpus] "Intermediate checkpoints during instruction tuning serve as naturally specialized experts for different domains"
- Break condition: If checkpoint performance differences are due to noise or initialization variance rather than learned specialization.

### Mechanism 2
- Claim: Genetic algorithm-based expert expansion maintains expert diversity while scaling the number of experts.
- Mechanism: Selects two most dissimilar experts, merges them with weighted averaging and mutation via DARE, producing new diverse experts without additional training.
- Core assumption: Parameter merging with mutation can generate functionally distinct experts from existing ones.
- Evidence anchors:
  - [section 2.2] "we select two experts with the greatest differences... apply DARE to introduce mutations into the newly created expert"
  - [section 3.7] "merging two randomly selected expert models... results in a performance decline, again validating the importance of maintaining expert diversity"
  - [corpus] "Genetic algorithm-based expert expansion maintains expert diversity while scaling the number of experts"
- Break condition: If merged experts become too similar or lose their specialized capabilities.

### Mechanism 3
- Claim: Router initialization with expert-specific data ensures each expert activates on appropriate tokens.
- Mechanism: Pre-optimizes routing vectors using small seed data where each expert excels, introducing auxiliary binary classification loss to maximize correct token-expert matching.
- Core assumption: Expert-specific data selection based on perplexity can identify tokens each expert handles best.
- Evidence anchors:
  - [section 2.2] "we introduce an auxiliary binary classification loss to pre-optimize the corresponding routing vector"
  - [section 3.5] "UpIT accurately allocates tokens from different domains to specific experts, highlighting the significant differentiation among routers and experts"
  - [corpus] "Router initialization with expert-specific data ensures each expert activates on appropriate tokens"
- Break condition: If routing vectors overfit to seed data or fail to generalize to unseen tokens.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: The paper builds upon MoE to create data-efficient upcycling methods
  - Quick check question: How does MoE achieve scalability without proportional computational cost?

- Concept: Genetic algorithms
  - Why needed here: Used for expert expansion to maintain diversity when scaling expert numbers
  - Quick check question: What selection criterion ensures generated experts maintain diversity?

- Concept: Parameter merging techniques
  - Why needed here: Core to both expert expansion and model upcycling processes
  - Quick check question: How does weighted averaging of parameters preserve or enhance capabilities?

## Architecture Onboarding

- Component map:
  Dense pre-trained model → Expert preparation (checkpoint saving) → Expert expansion (genetic algorithm + DARE) → Router initialization (seed data selection + auxiliary loss) → Model upcycling (parameter merging) → Post-training

- Critical path:
  1. Fine-tune dense model and save intermediate checkpoints
  2. Expand experts using genetic algorithm until target number reached
  3. Select seed data and pre-optimize routing vectors
  4. Merge parameters into final MoE model
  5. Post-train on full dataset

- Design tradeoffs:
  - Checkpoint frequency vs. expert diversity quality
  - Seed data size (1% used) vs. router initialization effectiveness
  - Number of experts vs. data distribution and activation patterns

- Failure signatures:
  - Uniform token distribution across experts (indicates router failure)
  - Performance drops when scaling expert count (indicates insufficient data per expert)
  - Inconsistent benchmark performance (indicates lack of true specialization)

- First 3 experiments:
  1. Verify checkpoint performance differences across benchmarks
  2. Test genetic algorithm expert expansion with random vs. similarity-based selection
  3. Compare router initialization with/without expert-specific data selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does UpIT's performance scale when using checkpoints from different dense model architectures (e.g., different pre-training objectives or model sizes) for expert preparation?
- Basis in paper: [inferred] The paper mentions using intermediate checkpoints from dense models for expert preparation, but only evaluates this with Llama 2 7B and Sheared Llama 2.7B.
- Why unresolved: The paper only tests UpIT with two specific dense model variants, leaving open whether the method generalizes to other architectures.
- What evidence would resolve it: Experiments comparing UpIT's performance when using checkpoints from diverse dense model architectures (e.g., OPT, BLOOM, different Llama versions) would demonstrate the method's architectural robustness.

### Open Question 2
- Question: What is the theoretical upper bound of UpIT's performance gain compared to traditional dense instruction tuning when unlimited data and computational resources are available?
- Basis in paper: [explicit] The paper shows UpIT's performance improves with more training data and epochs, but notes baselines plateau while UpIT continues improving.
- Why unresolved: The experiments only extend to 8 epochs total (4 for preparation, 4 for post-training), leaving the asymptotic performance gap unknown.
- What evidence would resolve it: Scaling experiments with 10x more data and training epochs would reveal whether UpIT continues to outperform dense baselines indefinitely or eventually plateaus.

### Open Question 3
- Question: How does the quality of expert specialization vary when using different seed data selection strategies beyond the proposed PPL-based method?
- Basis in paper: [explicit] The paper compares PPL-based data selection to random selection and finds significant performance differences, but doesn't explore alternative selection criteria.
- Why unresolved: The analysis only compares two extremes (PPL-based vs. random), without testing other potentially informative selection strategies.
- What evidence would resolve it: Comparative experiments using different seed data selection methods (e.g., entropy-based, nearest-neighbor in embedding space, or human-curated subsets) would reveal whether PPL-based selection is optimal or if better strategies exist.

## Limitations

- Effectiveness heavily depends on availability of high-quality intermediate checkpoints exhibiting genuine specialization
- Risk of expert collapse where merged experts become too similar or lose specialized capabilities
- Potential bottleneck in router initialization with limited 1% seed data that may overfit

## Confidence

**High Confidence:** The core premise that intermediate checkpoints during instruction tuning can serve as specialized experts for different domains, supported by clear empirical evidence showing checkpoint performance divergence across benchmarks. The mechanism of using parameter merging for model upcycling is well-established and the experimental setup is sufficiently detailed for reproduction.

**Medium Confidence:** The effectiveness of the genetic algorithm for expert expansion and the router initialization approach. While the paper provides theoretical justification and some empirical support, the specific implementation details (similarity metrics, mutation rates, auxiliary loss hyperparameters) significantly impact performance and are not fully specified. The scalability claims require more extensive testing across diverse model sizes and domains.

**Low Confidence:** The claim of data efficiency, particularly the assertion that 1% seed data is sufficient for effective router initialization. The paper doesn't adequately address how performance scales with different seed data ratios or what happens when expert numbers scale beyond the tested range. The long-term stability of the upcycling approach under continued training or deployment scenarios remains unexplored.

## Next Checks

1. **Checkpoint Specialization Validation**: Conduct ablation studies comparing expert performance when using randomly selected checkpoints versus performance-based checkpoint selection. Measure expert similarity metrics and domain coverage to verify that checkpoint differences reflect genuine specialization rather than noise or initialization variance.

2. **Router Generalization Test**: Evaluate routing vector performance when initialized with varying seed data ratios (0.1%, 1%, 10%, 50%) and test generalization to out-of-distribution data. Monitor router activation patterns during both seed data training and full-dataset post-training to identify potential overfitting.

3. **Expert Diversity Quantification**: Implement quantitative measures of expert diversity (e.g., pairwise similarity metrics, activation overlap statistics) across different expert counts and expansion rounds. Test the limits of expert expansion by attempting to scale beyond the paper's tested range and measuring performance degradation points.