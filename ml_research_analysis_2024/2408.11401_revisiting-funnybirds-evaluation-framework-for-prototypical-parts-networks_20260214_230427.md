---
ver: rpa2
title: Revisiting FunnyBirds evaluation framework for prototypical parts networks
arxiv_id: '2408.11401'
source_url: https://arxiv.org/abs/2408.11401
tags:
- parts
- protopnet
- funnybirds
- similarity
- maps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses a critical issue in evaluating prototypical
  parts networks like ProtoPNet within the FunnyBirds benchmark framework. The original
  evaluation used bounding boxes to approximate similarity maps, which can lead to
  inaccurate metric scores and misinterpretation of the model's interpretability.
---

# Revisiting FunnyBirds evaluation framework for prototypical parts networks

## Quick Facts
- arXiv ID: 2408.11401
- Source URL: https://arxiv.org/abs/2408.11401
- Reference count: 27
- One-line primary result: Summed similarity maps (SSM) significantly improve the correctness of prototypical parts network evaluation in the FunnyBirds benchmark, increasing Single Deletion scores from 0.24 to 0.73.

## Executive Summary
This study addresses a critical issue in evaluating prototypical parts networks like ProtoPNet within the FunnyBirds benchmark framework. The original evaluation used bounding boxes to approximate similarity maps, which can lead to inaccurate metric scores and misinterpretation of the model's interpretability. We propose using summed similarity maps (SSM) instead, which more precisely capture the importance of image regions for each prototypical part. Our comprehensive analysis demonstrates that SSM-based explanations significantly improve the correctness of evaluation metrics (Single Deletion score increases from 0.24 to 0.73) and provide more reliable completeness scores. However, we observe a trade-off where SSM reduces certain completeness metrics (CSDC, PC, DC) due to less overidentification of parts as important. These findings advocate for adopting SSM-based visualizations in future explainable AI benchmarks to ensure fair and accurate comparisons between prototypical parts methods and other attribution-based techniques.

## Method Summary
The study evaluates prototypical parts networks using the FunnyBirds benchmark, comparing traditional bounding box-based visualizations with summed similarity map (SSM)-based visualizations. The authors implemented ProtoPNet with three backbone architectures (ResNet50, VGG19, DenseNet169) on the FunnyBirds dataset (50 bird classes, 50,000 training images, 5,000 test images). They modified the FunnyBirds interface functions to use SSM instead of bounding boxes for calculating importance scores and important parts. The evaluation framework metrics (Completeness, Correctness, Contrastivity) including Single Deletion, Target Sensitivity, Controlled Synthetic Data Check, Preservation Check, Deletion Check, and Distractibility were computed for both visualization methods to compare their performance.

## Key Results
- SSM-based explanations increase Single Deletion scores from 0.24 to 0.73, demonstrating significantly improved correctness.
- SSM provides more reliable completeness scores by reducing overidentification of parts as important, leading to lower CSDC, PC, and DC metrics.
- The study shows that ProtoPNet's interpretability is much better than previously reported when using SSM-based evaluation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Summed similarity maps (SSM) more accurately reflect the importance of image regions for prototypical parts than bounding boxes.
- Mechanism: SSM preserves the spatial precision of similarity values, whereas bounding boxes collapse these values to a maximum within a region, losing fine-grained information.
- Core assumption: The distribution of similarity scores within a bounding box contains meaningful information about part importance.
- Evidence anchors:
  - [abstract] "we propose using summed similarity maps (SSM) instead, which more precisely capture the importance of image regions for each prototypical part"
  - [section 3.2] "we propose an alternative definition of the interface functions based on the similarity maps, which are more precise than bounding boxes"
  - [corpus] Weak evidence; no direct citations of SSM accuracy from external sources.
- Break condition: If the similarity maps are too noisy or the weights are unstable, the summed values may not reliably indicate part importance.

### Mechanism 2
- Claim: SSM improves correctness metrics by aligning the evaluation with ProtoPNet's design intuition.
- Mechanism: By using SSM, the importance scores computed for each part match better with the actual similarity values learned by the network, leading to more accurate Single Deletion (SD) scores.
- Core assumption: The ProtoPNet's decision-making process relies on weighted similarity values, not just the presence of a part within a region.
- Evidence anchors:
  - [abstract] "SSM-based explanations significantly improve the correctness of evaluation metrics (Single Deletion score increases from 0.24 to 0.73)"
  - [section 5.1] "a more precise SSM attribution map demonstrates that ProtoPNet is much more correct than reported in [9]"
  - [corpus] No direct evidence from related papers supporting this mechanism.
- Break condition: If the weights between prototypical parts and classes are not properly calibrated, SSM may not reflect true importance.

### Mechanism 3
- Claim: SSM reduces overidentification of parts, leading to more reliable completeness scores.
- Mechanism: SSM's precise attribution avoids assigning importance to parts that do not significantly contribute to the decision, thus reducing false positives in completeness metrics like CSDC, PC, and DC.
- Core assumption: Overidentification by bounding boxes inflates completeness scores by incorrectly marking non-contributing parts as important.
- Evidence anchors:
  - [abstract] "we observe a trade-off where SSM reduces certain completeness metrics (CSDC, PC, DC) due to less overidentification of parts as important"
  - [section 5.1] "the original BB approach tends to overidentify parts as important, which results in an incorrectly high completeness score"
  - [corpus] No external evidence supporting the claim about overidentification.
- Break condition: If the threshold for determining important parts is too strict, SSM might miss genuinely important parts, lowering completeness unfairly.

## Foundational Learning

- Concept: Understanding of prototypical parts networks (ProtoPNet)
  - Why needed here: The paper's analysis revolves around evaluating ProtoPNet explanations, so understanding how ProtoPNet works is essential.
  - Quick check question: What is the key difference between ProtoPNet and post-hoc explanation methods like GradCAM?

- Concept: Familiarity with evaluation metrics for interpretability
  - Why needed here: The study uses various metrics (SD, TS, CSDC, PC, DC, D) to assess explanation quality, requiring knowledge of what each measures.
  - Quick check question: How does the Single Deletion (SD) metric assess the correctness of an explanation?

- Concept: Knowledge of attribution maps and similarity maps
  - Why needed here: The paper compares bounding boxes and similarity maps as visualization techniques, so understanding these concepts is crucial.
  - Quick check question: What is the main difference between an attribution map based on bounding boxes and one based on similarity maps?

## Architecture Onboarding

- Component map:
  Input -> FunnyBirds Dataset -> ProtoPNet Models (ResNet50, VGG19, DenseNet169) -> SSM/BB Visualization Methods -> FunnyBirds Evaluation Framework -> SD, TS, CSDC, PC, DC, D Metrics

- Critical path:
  1. Train ProtoPNet models on FunnyBirds dataset
  2. Generate explanations using SSM and BB
  3. Compute interface functions (PI and P) for each explanation method
  4. Calculate evaluation metrics
  5. Compare results between SSM and BB

- Design tradeoffs:
  - SSM vs. BB: SSM provides more precise attribution but is computationally more intensive.
  - Backbone choice: Different backbones affect interpretability (metrics) and accuracy.

- Failure signatures:
  - Low SD scores: Indicates that the explanations do not align well with the model's decisions.
  - High completeness scores with BB: Suggests overidentification of parts as important.

- First 3 experiments:
  1. Train a ProtoPNet with ResNet50 backbone and compare SD scores using SSM vs. BB.
  2. Vary the threshold t in the interface function P and observe its effect on completeness metrics.
  3. Test the effect of different backbone architectures on the trade-off between accuracy and interpretability.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided. However, based on the analysis, several implicit open questions emerge:

1. How do SSM-based explanations perform on real-world datasets compared to synthetic datasets like FunnyBirds?
2. What is the optimal threshold t for the P(Â·) function that balances completeness and correctness in SSM-based explanations?
3. How does the interpretability-accuracy trade-off manifest when using SSM-based explanations across different model architectures?

## Limitations
- The findings are primarily based on a single ProtoPNet model and one synthetic dataset, limiting generalizability.
- The trade-off between correctness and completeness metrics requires careful consideration in practical applications.
- Computational overhead of SSM-based explanations is not explicitly discussed, which could be limiting for real-time applications.

## Confidence

- **High Confidence**: The mechanism by which SSM more accurately reflects the importance of image regions for prototypical parts (Mechanism 1) is well-supported by the evidence provided in the paper and the logical reasoning presented.
- **Medium Confidence**: The claim that SSM improves correctness metrics (Mechanism 2) is supported by the reported SD score increase, but the lack of external evidence from related papers introduces some uncertainty.
- **Medium Confidence**: The assertion that SSM reduces overidentification of parts (Mechanism 3) is supported by the observed trade-off in completeness metrics, but the lack of external validation limits the confidence in this claim.

## Next Checks

1. Test the SSM-based evaluation on a different dataset with prototypical parts to verify if the reported improvements in correctness and completeness metrics hold.
2. Evaluate the impact of SSM on other prototypical parts networks (e.g., ProtoPNeXt) to assess the generalizability of the findings.
3. Measure the computational overhead of SSM-based explanations compared to bounding boxes to determine if the increased accuracy justifies the additional computational cost.