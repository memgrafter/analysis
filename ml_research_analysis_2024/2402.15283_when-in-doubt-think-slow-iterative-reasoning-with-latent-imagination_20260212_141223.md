---
ver: rpa2
title: 'When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination'
arxiv_id: '2402.15283'
source_url: https://arxiv.org/abs/2402.15283
tags:
- agent
- performance
- inference
- iterative
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a training-free approach to improve model-based
  reinforcement learning agents through iterative inference at decision-time. The
  method fine-tunes inferred agent states based on the coherence of future state representations
  using latent imagination.
---

# When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination

## Quick Facts
- **arXiv ID**: 2402.15283
- **Source URL**: https://arxiv.org/abs/2402.15283
- **Reference count**: 31
- **Primary result**: Training-free approach that improves model-based RL agents through iterative inference at decision-time using latent imagination

## Executive Summary
This paper introduces a training-free method to improve model-based reinforcement learning agents through iterative inference at decision-time. The approach fine-tunes inferred agent states based on the coherence of future state representations using latent imagination. Applied to visual 3D navigation tasks, it achieves consistent improvements in both reconstruction accuracy and task performance. The method shows greater benefits in partially-observable environments compared to fully-observable ones, with agents having less pre-training benefiting most.

## Method Summary
The method applies iterative inference at decision-time to fine-tune the agent's latent state representations using an inference objective based on expected future model evidence. It works by applying gradient steps to refine latent states using coherence with imagined future states, optimizing for alignment between current and predicted future states without changing world model parameters. The approach is tested on DreamerV3 agents across three DeepMind Lab tasks, one Atari task, and a custom MiniWorld task.

## Key Results
- Consistent improvements in both reconstruction accuracy (PSNR, SSIM, MSE) and task performance (episode scores, length)
- Greater benefits observed in partially-observable environments compared to fully-observable ones
- Agents with less pre-training benefit most from the iterative inference approach
- Optimal rollout lengths vary by environment type: longer lengths for PO environments, shorter for FO environments

## Why This Works (Mechanism)

### Mechanism 1
Iterative inference improves state representations by minimizing future state uncertainty. The method applies gradient steps to refine latent states using an inference objective based on expected future model evidence. By optimizing for coherence with imagined future states, the agent's current state representation becomes more aligned with what the world model predicts.

### Mechanism 2
The approach reduces the amortisation gap in variational inference. By iteratively refining the latent state representation at decision-time using gradients of the inference objective, the method brings the approximate posterior closer to the true posterior without changing world model parameters.

### Mechanism 3
The method is particularly effective in partially-observable environments because it compensates for missing information. In PO environments, state representations must integrate information over time, and the iterative refinement using future predictions helps fill in gaps where observations are sparse or missing.

## Foundational Learning

- **Variational inference and ELBO**: Understanding the evidence lower bound framework is crucial as the method optimizes an inference objective related to the ELBO. *Quick check: What are the two main terms in the ELBO decomposition, and what do they represent?*
- **Amortized inference and amortisation gap**: The method specifically addresses the amortisation gap by applying iterative refinement at decision-time, so understanding this concept is essential. *Quick check: What causes the amortisation gap in variational autoencoders, and why does it occur?*
- **Model-based RL with world models**: The approach builds on DreamerV3, a model-based RL agent with a learned world model, so understanding how these components work together is important. *Quick check: How does DreamerV3 use its world model for planning and learning?*

## Architecture Onboarding

- **Component map**: DreamerV3 world model (encoder, sequence model, dynamics predictor, reward predictor, continue predictor, decoder) -> Actor-critic for action selection -> Iterative inference module (applies gradient updates to model state) -> Inference objectives (State Information Gain, Parameter Information Gain, Entropy)
- **Critical path**: 1) Receive observation xt and current recurrent state ht 2) Generate initial latent state z0_t using encoder 3) Sample trajectories using world model and actor 4) Compute inference objective over imagined rollouts 5) Backpropagate gradients to update model state 6) Return updated state and action to DreamerV3
- **Design tradeoffs**: Training-free approach vs. potential benefits of fine-tuning world model parameters; computational cost of iterative refinement vs. performance gains; choice of inference objective and rollout length
- **Failure signatures**: Performance degradation when step size α is too large; limited improvement in fully-observable environments; increased variance in metrics with longer rollout lengths
- **First 3 experiments**: 1) Implement iterative inference with State Information Gain objective and rollout length λ=1 on DMLab Collect Good Objects task 2) Compare performance with different rollout lengths (λ=1, 3, 8, 16) using the same objective 3) Test Parameter Information Gain objective on the same task to compare effectiveness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but suggests several areas for future work including: introducing mechanisms to dynamically identify states requiring iterative inference at decision-time; investigating the impact of the number of gradient steps on performance; exploring alternative inference objectives; and extending the approach to other model-based RL architectures beyond DreamerV3.

## Limitations
- Limited ablation studies on inference objectives, making it unclear which components contribute most to performance gains
- No systematic investigation of how the number of gradient steps affects performance
- Lack of direct empirical validation for the theoretical mechanisms explaining why the method works

## Confidence
- **High confidence**: The mathematical formulation of the iterative inference method and its integration with DreamerV3's architecture is well-specified and reproducible
- **Medium confidence**: The empirical results showing performance improvements across multiple tasks are compelling, though the ablation studies could be more comprehensive
- **Low confidence**: The theoretical explanations for why the method works (particularly the three mechanisms) lack direct experimental validation and rely heavily on intuition

## Next Checks
1. **Ablation study on inference objectives**: Systematically compare State Information Gain, Parameter Information Gain, and Entropy objectives across all tasks to determine which contributes most to performance improvements and under what conditions
2. **Amortisation gap quantification**: Design an experiment that measures the initial amortisation gap in the baseline DreamerV3 agent and tracks how much it reduces after iterative refinement, ideally with a visualization of the latent state trajectories
3. **Generalization stress test**: Apply the iterative inference method to a wider range of environments, particularly those with varying degrees of partial observability and different visual complexity, to better understand the boundaries of its effectiveness