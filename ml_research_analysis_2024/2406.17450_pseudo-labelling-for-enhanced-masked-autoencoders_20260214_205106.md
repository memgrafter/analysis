---
ver: rpa2
title: Pseudo Labelling for Enhanced Masked Autoencoders
arxiv_id: '2406.17450'
source_url: https://arxiv.org/abs/2406.17450
tags:
- pseudo
- teacher
- labelling
- vit-b
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PseudoMAE enhances Masked Autoencoders (MAE) by integrating pseudo\
  \ labelling for both class and data tokens, replacing pixel-level reconstruction\
  \ with token-level reconstruction. The method uses two disentangled teacher networks\u2014\
  one for pseudo labelling and one for reconstruction\u2014each updated with different\
  \ Exponential Moving Average (EMA) schedules."
---

# Pseudo Labelling for Enhanced Masked Autoencoders

## Quick Facts
- arXiv ID: 2406.17450
- Source URL: https://arxiv.org/abs/2406.17450
- Reference count: 35
- Primary result: PseudoMAE achieves 84.1% top-1 accuracy on ImageNet-1K, outperforming MAE and other self-supervised learning methods with less pretraining time.

## Executive Summary
PseudoMAE enhances Masked Autoencoders (MAE) by integrating pseudo labelling for both class and data tokens, replacing pixel-level reconstruction with token-level reconstruction. The method uses two disentangled teacher networks—one for pseudo labelling and one for reconstruction—each updated with different Exponential Moving Average (EMA) schedules. This approach promotes instance-level discrimination and captures local context through discrete token generation. PseudoMAE achieves a 84.1% top-1 accuracy on ImageNet-1K, outperforming MAE and other self-supervised learning methods with less pretraining time. It also excels in downstream tasks such as classification, semantic segmentation, and object detection.

## Method Summary
PseudoMAE introduces a novel self-supervised learning framework that enhances Masked Autoencoders by incorporating pseudo labelling for both class and data tokens. The method replaces pixel-level reconstruction with token-level reconstruction, utilizing two separate teacher networks: one for generating pseudo labels and another for reconstruction. Each teacher network is updated with different EMA schedules, allowing for instance-level discrimination and capturing local context through discrete token generation. This dual-teacher approach aims to improve the representation learning capabilities of MAE while reducing pretraining time.

## Key Results
- Achieves 84.1% top-1 accuracy on ImageNet-1K, outperforming MAE and other self-supervised learning methods.
- Demonstrates superior performance in downstream tasks such as classification, semantic segmentation, and object detection.
- Reduces pretraining time compared to traditional MAE approaches.

## Why This Works (Mechanism)
The mechanism behind PseudoMAE's success lies in its dual-teacher architecture and token-level reconstruction. By using two separate teacher networks with different EMA schedules, the method can effectively capture both global and local features of the data. The pseudo labelling teacher focuses on instance-level discrimination, while the reconstruction teacher ensures accurate token-level reconstruction. This combination allows PseudoMAE to learn more discriminative and context-aware representations compared to traditional pixel-level reconstruction methods.

## Foundational Learning

### Masked Autoencoders (MAE)
- **Why needed**: MAE is a self-supervised learning method that masks a portion of input data and reconstructs the missing parts, enabling the model to learn rich representations.
- **Quick check**: MAE has been shown to achieve competitive results on various vision tasks, but it primarily focuses on pixel-level reconstruction, which may limit its ability to capture high-level semantic information.

### Pseudo Labelling
- **Why needed**: Pseudo labelling involves generating labels for unlabeled data using a trained model, which can then be used to improve the model's performance.
- **Quick check**: Pseudo labelling has been successfully applied in semi-supervised learning and can enhance model performance by providing additional supervisory signals.

### Exponential Moving Average (EMA)
- **Why needed**: EMA is a technique used to smooth model parameters over time, which can help stabilize training and improve generalization.
- **Quick check**: EMA is commonly used in self-supervised learning to maintain a consistent teacher model, but the dual EMA schedule in PseudoMAE is a novel approach.

## Architecture Onboarding

### Component Map
Input Image -> Mask Generation -> Encoder -> Masked Tokens -> Two Teacher Networks (Pseudo Labelling + Reconstruction) -> EMA Updates -> Token-Level Reconstruction -> Output

### Critical Path
The critical path involves the encoder processing the masked tokens, followed by the two teacher networks generating pseudo labels and reconstructions. The EMA updates ensure that the teacher networks evolve smoothly over time, leading to improved token-level reconstruction.

### Design Tradeoffs
- **Pros**: Improved representation learning, reduced pretraining time, and better performance on downstream tasks.
- **Cons**: Increased computational overhead due to the dual-teacher architecture and the need for discrete token generation.

### Failure Signatures
Potential failure modes include instability in the EMA updates, which could lead to poor convergence, and the discrete token generation process, which may introduce quantization errors.

### First Experiments
1. Evaluate the impact of different masking ratios on PseudoMAE's performance.
2. Compare the computational overhead of PseudoMAE with standard MAE.
3. Assess the robustness of PseudoMAE to varying dataset sizes and domains.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability and generalizability of PseudoMAE to other datasets and domains remain unclear.
- The dual-teacher EMA schedule strategy lacks detailed ablation studies to isolate the contribution of each component.
- The computational overhead introduced by maintaining two separate teacher networks and the discrete token generation process is not thoroughly analyzed.

## Confidence

### Major Claim Clusters Confidence:
- **Performance Improvements on ImageNet-1K**: High - The 84.1% top-1 accuracy is well-documented and compared against established baselines.
- **Token-Level Reconstruction Advantage**: Medium - While the theoretical motivation is clear, empirical evidence for its superiority over pixel-level reconstruction is limited.
- **Two-Teacher EMA Strategy**: Medium - The design is innovative, but the lack of ablation studies makes it difficult to assess the necessity of both teachers.

## Next Checks
1. Evaluate PseudoMAE on diverse datasets (e.g., COCO, CIFAR-10, and medical imaging datasets) to assess generalizability.
2. Conduct ablation studies to quantify the individual contributions of the pseudo-labelling teacher, reconstruction teacher, and discrete token generation.
3. Analyze the computational overhead and memory requirements of the two-teacher architecture compared to standard MAE.