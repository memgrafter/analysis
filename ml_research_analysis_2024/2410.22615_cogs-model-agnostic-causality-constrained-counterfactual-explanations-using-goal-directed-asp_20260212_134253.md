---
ver: rpa2
title: 'CoGS: Model Agnostic Causality Constrained Counterfactual Explanations using
  goal-directed ASP'
arxiv_id: '2410.22615'
source_url: https://arxiv.org/abs/2410.22615
tags:
- state
- cogs
- counterfactual
- rules
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents CoGS, a model-agnostic framework for generating
  causally consistent counterfactual explanations for classification models. CoGS
  uses rule-based machine learning (RBML) algorithms, particularly FOLD-SE, to extract
  the underlying logic of statistical models and employs Answer Set Programming (ASP)
  to compute realistic, causally consistent modifications to feature values.
---

# CoGS: Model Agnostic Causality Constrained Counterfactual Explanations using goal-directed ASP

## Quick Facts
- arXiv ID: 2410.22615
- Source URL: https://arxiv.org/abs/2410.22615
- Authors: Sopam Dasgupta; Joaquín Arias; Elmer Salazar; Gopal Gupta
- Reference count: 3
- Generates causally consistent counterfactual explanations for classification models using FOLD-SE and s(CASP)

## Executive Summary
CoGS addresses a critical limitation in counterfactual explanation methods by generating causally consistent modifications to feature values. Unlike existing approaches that assume feature independence, CoGS uses rule-based machine learning (RBML) and Answer Set Programming (ASP) to account for causal dependencies between features. The framework extracts decision rules from classification models using FOLD-SE and employs s(CASP) to compute realistic, step-by-step paths from undesired to desired outcomes. By modeling causal relationships explicitly, CoGS produces counterfactuals that are both interpretable and actionable, making it applicable to any classification model regardless of its underlying complexity.

## Method Summary
The CoGS framework operates through a two-stage process: first, it uses FOLD-SE to extract the underlying logic of statistical models, creating interpretable decision rules that capture both the model's behavior and causal dependencies among features. Second, it employs the s(CASP) system to compute realistic modifications to feature values, ensuring each intervention respects the causal structure encoded in the rules. The framework traces step-by-step paths through intermediate states, avoiding the unrealistic assumption that all features can be changed simultaneously. This approach enables the generation of actionable counterfactual explanations that respect real-world causal constraints, making the framework model-agnostic and applicable to various classification algorithms including DNNs, GBCs, RFs, and LRs.

## Key Results
- Successfully learns underlying logic of various classification models (DNN, GBC, RF, LR) with good fidelity metrics across multiple datasets
- Generates causally consistent counterfactual paths while accounting for feature dependencies through ASP reasoning
- Demonstrates scalability in path generation, though computation time increases with larger feature value spaces
- Provides step-by-step actionable explanations rather than simultaneous interventions, improving interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoGS generates causally consistent counterfactuals by modeling feature dependencies through Answer Set Programming
- Mechanism: The framework uses s(CASP) to compute a step-by-step path from an undesired outcome to a desired one, ensuring each intervention respects causal dependencies between features
- Core assumption: Causal dependencies between features can be captured through rule-based representations that s(CASP) can reason about
- Evidence anchors:
  - [abstract] "CoGS leverages the goal-directed Answer Set Programming system s(CASP) to compute realistic and causally consistent modifications to feature values, accounting for causal dependencies between them"
  - [section] "CoGS models various scenarios (or worlds): the current initial state i, represents a negative outcome, while the goal state, g, represents a positive outcome"
  - [corpus] Weak evidence - only neighboring papers discuss causality but not the specific ASP implementation
- Break condition: If causal dependencies cannot be accurately captured by the rule-based system or if s(CASP) cannot find a valid path through the state space

### Mechanism 2
- Claim: FOLD-SE algorithm extracts the underlying logic of statistical models to create interpretable decision rules
- Mechanism: The framework uses rule-based machine learning (RBML) algorithms such as FOLD-SE to extract the underlying logic of a statistical model, creating decision rules that can be used for counterfactual generation
- Core assumption: The FOLD-SE algorithm can accurately approximate the behavior of complex models like DNNs, GBCs, RFs, and LRs with sufficient fidelity
- Evidence anchors:
  - [abstract] "By using rule-based machine learning algorithms (RBML), notably the FOLD-SE algorithm, CoGS extracts the underlying logic of a statistical model to generate counterfactual solutions"
  - [section] "FOLD-SE can also be used for learning rules capturing causal dependencies among features in a dataset"
  - [corpus] Weak evidence - neighboring papers discuss FOLD-SE but not the specific extraction methodology
- Break condition: If FOLD-SE cannot achieve sufficient fidelity (as measured in Table 1) or if the extracted rules do not accurately represent the original model's behavior

### Mechanism 3
- Claim: CoGS provides step-by-step actionable explanations rather than simultaneous interventions
- Mechanism: Unlike approaches that apply all interventions simultaneously, CoGS traces a path through intermediate states, showing how one action leads to the next while respecting causal dependencies
- Core assumption: Sequential interventions are more realistic and achievable in real-world scenarios than simultaneous changes
- Evidence anchors:
  - [abstract] "By tracing a step-by-step path from an undesired outcome to a desired one, CoGS offers interpretable and actionable explanations of the changes required to achieve the desired outcome"
  - [section] "CoGS takes causal dependencies among features into account when computing a step-by-step path to obtaining these counterfactuals"
  - [corpus] Weak evidence - neighboring papers discuss sequential vs simultaneous interventions but don't provide specific evidence for CoGS's approach
- Break condition: If the state space becomes too large to efficiently search for paths, or if the step-by-step approach leads to impractical solutions

## Foundational Learning

- Concept: Answer Set Programming (ASP) and s(CASP) system
  - Why needed here: ASP provides the computational framework for modeling causal dependencies and finding paths through the state space
  - Quick check question: What is the key difference between standard ASP and s(CASP) that makes the latter suitable for counterfactual reasoning?

- Concept: Rule-based Machine Learning (RBML) and FOLD-SE algorithm
  - Why needed here: FOLD-SE extracts interpretable decision rules from complex models, enabling the generation of counterfactuals
  - Quick check question: How does FOLD-SE maintain scalability while learning a relatively small number of rules?

- Concept: Causal modeling and Structural Causal Models (SCMs)
  - Why needed here: Understanding causal dependencies between features is essential for generating realistic counterfactuals
  - Quick check question: What is the difference between correlation and causation, and why does this distinction matter for counterfactual explanations?

## Architecture Onboarding

- Component map: Input data and model -> FOLD-SE module -> Decision rules -> s(CASP) engine -> Causal rules module -> Step-by-step counterfactual paths
- Critical path: Model → FOLD-SE extraction → Decision rules → s(CASP) planning → Counterfactual path
- Design tradeoffs: Interpretability vs. accuracy (FOLD-SE may sacrifice some model performance for explainability), computational efficiency vs. completeness (searching the state space)
- Failure signatures: Low fidelity scores (Table 1), inability to find a path through the state space, counterfactual solutions that don't respect causal dependencies
- First 3 experiments:
  1. Test FOLD-SE's fidelity on a simple dataset with a known decision boundary
  2. Verify that s(CASP) can find paths in a small state space with known causal dependencies
  3. Test the end-to-end system on a binary classification problem with a few features and known causal relationships

## Open Questions the Paper Calls Out
- How to extend CoGS to non-tabular data such as image classification tasks
- How to handle continuous feature spaces more effectively
- How to automatically validate causal dependencies discovered by FOLD-SE

## Limitations
- Assumes causal dependencies can be accurately captured through rule-based representations, which may not reflect true causal mechanisms
- Computationally intensive as it searches through an ever-increasing solution space, limiting scalability
- Limited evaluation on datasets with continuous features and mixed data types

## Confidence

**High confidence** in the technical feasibility of the ASP-based pathfinding mechanism and the general approach of using RBML for model extraction. The methodology is well-grounded in established AI techniques (ASP, rule learning).

**Medium confidence** in the causal consistency claims, as the paper demonstrates that causal dependencies can be encoded but doesn't validate whether the generated counterfactuals are truly causally compliant or simply respect the encoded rules.

**Medium confidence** in the practical utility, given that the evaluation shows good performance metrics but doesn't include user studies or real-world deployment scenarios to assess whether the step-by-step explanations are actually actionable and interpretable for end users.

## Next Checks

1. Validate causal compliance by comparing CoGS-generated counterfactuals against known ground truth causal graphs on synthetic datasets where true causal relationships are known.

2. Test the framework's performance on datasets with continuous features by discretizing them and measuring how the fidelity and computation time scale with the granularity of discretization.

3. Conduct a user study with domain experts to evaluate whether the step-by-step counterfactual explanations are interpretable and actionable in practice, particularly for scenarios where causal relationships are well-understood.