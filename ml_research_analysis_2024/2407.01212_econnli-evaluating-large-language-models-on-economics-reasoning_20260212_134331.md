---
ver: rpa2
title: 'EconNLI: Evaluating Large Language Models on Economics Reasoning'
arxiv_id: '2407.01212'
source_url: https://arxiv.org/abs/2407.01212
tags:
- economic
- premise
- llms
- hypothesis
- events
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study introduces EconNLI, a new dataset designed to evaluate\
  \ large language models (LLMs) on their ability to reason about economic events.\
  \ EconNLI contains sentence pairs of premises and hypotheses, both representing\
  \ economic events, and is used to assess LLMs\u2019 performance in classification\
  \ (determining causal relationships) and generation (inferring resulting events)\
  \ tasks."
---

# EconNLI: Evaluating Large Language Models on Economics Reasoning

## Quick Facts
- arXiv ID: 2407.01212
- Source URL: https://arxiv.org/abs/2407.01212
- Authors: Yue Guo; Yi Yang
- Reference count: 19
- Key outcome: LLMs struggle with economic reasoning, achieving only 0.835 accuracy on classification tasks and generating economically incorrect or hallucinated events

## Executive Summary
This paper introduces EconNLI, a new dataset designed to evaluate large language models on their ability to reason about economic events. The dataset contains premise-hypothesis pairs representing economic events and is used to assess LLMs' performance in classification (determining causal relationships) and generation (inferring resulting events) tasks. Experiments show that LLMs, including both open-source and commercial models, struggle with economic reasoning, often generating incorrect or hallucinated answers. The study highlights the limitations of LLMs in critical decision-making involving economic analysis and emphasizes the need for further research to improve their reasoning capabilities in the financial domain.

## Method Summary
The EconNLI dataset was constructed by collecting economic and financial Wikipedia pages, extracting sentences with causal linking phrases, and using a supervised fine-tuned LLAMA2 model for event extraction. Premise-hypothesis candidates were generated based on syntax patterns and labeled using ChatGPT and GPT-4 for the training set, with human annotation for the test set. Various LLMs including BERT, RoBERTa, FinBERT, FLANG-BERT, FLANG-ELECTRA, LLAMA2-chat, Alpaca, FINMA, ChatGPT, and GPT-4 were evaluated using supervised fine-tuning for encoder-only models and LLAMA2-chat, and zero-shot, in-context learning (ICL), and chain-of-thought (CoT) prompting for decoder-only models. Evaluation metrics included classification accuracy (precision, recall, F1) and generation quality assessed by entailment, contradiction, or irrelevance of generated events compared to reference hypotheses.

## Key Results
- GPT-4 achieved only 0.835 accuracy on the classification task
- LLAMA2-chat (7B) had a 8.72% contradiction/entailment rate in generating consequent events
- LLMs struggle with economic reasoning and may generate wrong or hallucinated answers
- Supervised fine-tuning on economic event pairs significantly improved model performance compared to zero-shot prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models struggle with economic reasoning because it requires domain-specific knowledge and causal inference beyond surface linguistic patterns.
- Mechanism: The dataset isolates economic reasoning by providing premise-hypothesis pairs grounded in economic theory, requiring models to infer causal relationships rather than relying on linguistic entailment cues.
- Core assumption: Traditional NLI datasets rely on semantic similarity or common sense, whereas EconNLI requires understanding of economic theories to correctly infer relationships.
- Evidence anchors:
  - [abstract] "both of which are economic events... it requires understanding the economic theory to conduct the inference."
  - [section] "Inference on the example... is based on the quantity theory of money."
- Break condition: If models can generalize from training data to unseen economic scenarios without explicit economic theory grounding, this mechanism breaks down.

### Mechanism 2
- Claim: Supervised fine-tuning on economic event pairs significantly improves model performance compared to zero-shot prompting.
- Mechanism: By training on labeled premise-hypothesis pairs, models learn the mapping between economic events and their causal relationships, capturing patterns not present in general pre-training data.
- Core assumption: Economic reasoning patterns can be learned from examples rather than requiring explicit economic knowledge encoding.
- Evidence anchors:
  - [section] "LLAMA2-chat models... significantly improved the results from encoder-only models, as the 7B and 13B LLAMA2 models achieved similar results with around 0.87 F1 score."
- Break condition: If zero-shot prompting with appropriate economic context could achieve similar performance, the need for supervised fine-tuning would be questioned.

### Mechanism 3
- Claim: Generation tasks reveal model hallucinations more clearly than classification tasks in economic reasoning.
- Mechanism: When asked to generate consequent events, models may produce plausible-sounding but economically incorrect outputs, exposing reasoning limitations that classification might mask.
- Core assumption: Classification tasks can be gamed through pattern matching, while generation requires genuine understanding of causal relationships.
- Evidence anchors:
  - [abstract] "Our experiments reveal that LLMs are not sophisticated in economic reasoning and may generate wrong or hallucinated answers."
- Break condition: If classification tasks could be designed to be equally sensitive to reasoning errors, this mechanism would lose its distinguishing value.

## Foundational Learning

- Concept: Causal inference in economic systems
  - Why needed here: Models must understand that economic events have causal relationships governed by theories, not just linguistic associations
  - Quick check question: Given "minimum wage increases" and "unemployment rises," can you explain the economic mechanism linking these events?

- Concept: Domain-specific knowledge representation
  - Why needed here: Economic reasoning requires understanding concepts like supply/demand, monetary theory, and market mechanisms that aren't captured in general language models
  - Quick check question: What economic principle explains why "higher volatility increases option premiums"?

- Concept: Evaluation methodology for reasoning tasks
  - Why needed here: Standard metrics like BLEU are insufficient for reasoning tasks; we need methods to detect entailment, contradiction, and irrelevance in generated economic events
  - Quick check question: How would you distinguish between a correct but different answer and an incorrect answer in economic event generation?

## Architecture Onboarding

- Component map: Data pipeline → Model training/inference → Evaluation → Analysis
  - Corpus preparation (Wikipedia economics pages)
  - Event extraction using fine-tuned LLAMA2
  - Label assignment (ChatGPT/GPT-4 for training, human annotation for testing)
  - Model evaluation (classification and generation tasks)
  - Analysis and interpretation of results

- Critical path: Corpus → Event Extraction → Label Assignment → Model Training → Evaluation → Analysis
  - Each step depends on the previous one; failure at any point compromises the entire pipeline

- Design tradeoffs:
  - Using LLMs for label assignment vs. human annotation (scale vs. quality)
  - Focusing on classification vs. generation tasks (precision vs. insight into reasoning)
  - Wikipedia-based corpus vs. financial reports (coverage vs. domain specificity)

- Failure signatures:
  - High Cohen's kappa between ChatGPT and GPT-4 labels but poor human performance indicates dataset quality issues
  - Models performing at random guess level suggest task difficulty exceeds current model capabilities
  - High contradiction/entailment rates in generation indicate reasoning failures rather than fluency issues

- First 3 experiments:
  1. Run classification on small subset with human-annotated labels to verify dataset quality before full-scale training
  2. Compare zero-shot vs. few-shot performance on validation set to determine optimal prompting strategy
  3. Test generation quality using simple entailment check before implementing full GPT-4 evaluation pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of EconNLI change if the dataset were expanded to include more diverse economic scenarios, such as those from emerging markets or specific industry sectors?
- Basis in paper: [inferred] The paper mentions the dataset was constructed from Wikipedia articles and focuses on general economic topics, but does not explore specific economic contexts or emerging markets.
- Why unresolved: The paper does not provide evidence or experiments on how the dataset's performance might vary with different economic contexts or sectors.
- What evidence would resolve it: Experiments comparing EconNLI's performance across different economic contexts, such as emerging markets or specific industries, would provide insights into its generalizability.

### Open Question 2
- Question: What are the potential impacts of using domain-specific pre-training on LLMs' performance in economic reasoning tasks?
- Basis in paper: [explicit] The paper discusses the use of domain-specific models like FinBERT and FLANG-BERT but does not explore the impact of domain-specific pre-training on economic reasoning.
- Why unresolved: The paper evaluates existing models but does not investigate how pre-training on economic data specifically might improve reasoning capabilities.
- What evidence would resolve it: Training LLMs on economic data before evaluating them on EconNLI would demonstrate the potential benefits of domain-specific pre-training.

### Open Question 3
- Question: How does the performance of LLMs on EconNLI correlate with their performance on other reasoning tasks, such as logical or common-sense reasoning?
- Basis in paper: [inferred] The paper focuses on economic reasoning but does not compare it with other types of reasoning tasks.
- Why unresolved: The study does not provide a comparative analysis of LLMs' performance across different reasoning domains.
- What evidence would resolve it: Conducting experiments to evaluate LLMs on both EconNLI and other reasoning benchmarks would reveal correlations and differences in performance.

### Open Question 4
- Question: What are the limitations of using Wikipedia as the primary source for constructing EconNLI, and how might these limitations affect the dataset's representativeness?
- Basis in paper: [explicit] The paper acknowledges that Wikipedia may not fully represent the complexity of real financial reports and reasoning scenarios.
- Why unresolved: The paper does not explore alternative sources or methods to address the limitations of using Wikipedia.
- What evidence would resolve it: Creating and evaluating EconNLI using alternative sources, such as real financial reports or academic papers, would highlight the limitations and potential improvements.

### Open Question 5
- Question: How does the performance of LLMs on EconNLI vary with different prompt strategies, and which strategy is most effective for economic reasoning?
- Basis in paper: [explicit] The paper experiments with different prompt strategies (zero-shot, ICL, CoT) but does not provide a comprehensive analysis of their effectiveness for economic reasoning.
- Why unresolved: The study does not systematically compare the impact of different prompt strategies on economic reasoning tasks.
- What evidence would resolve it: A detailed analysis comparing the effectiveness of various prompt strategies on EconNLI would identify the most suitable approach for economic reasoning.

## Limitations
- Dataset construction relies heavily on LLMs for initial label assignment, which may introduce systematic biases
- Human annotation agreement shows only moderate inter-annotator agreement (0.61), suggesting potential subjectivity in the economic reasoning task
- The dataset size (1,052 samples) is relatively small for training large models

## Confidence
- High Confidence: The core finding that LLMs struggle with economic reasoning is well-supported by consistent performance across multiple models and task types
- Medium Confidence: The dataset construction methodology is sound, but the reliance on LLM-generated labels introduces uncertainty about potential biases
- Low Confidence: The generation task evaluation methodology, particularly the use of GPT-4 as a judge for determining entailment, contradiction, or irrelevance, may not be reliable given the high error rates observed in model outputs

## Next Checks
1. Conduct a comprehensive human evaluation of a stratified sample of generated outputs across all models to verify the GPT-4-based classification results, particularly focusing on the contradiction/entailment cases that showed high error rates

2. Analyze the correlation between model performance and specific economic domains or theory types in the dataset to identify whether certain areas of economic reasoning are particularly challenging for LLMs

3. Implement an alternative evaluation approach for generation tasks using a different methodology (such as pairwise comparison or multiple human annotators) to cross-validate the GPT-4-based classification results and establish more robust metrics for generation quality