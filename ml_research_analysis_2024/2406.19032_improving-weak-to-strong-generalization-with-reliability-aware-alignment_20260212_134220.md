---
ver: rpa2
title: Improving Weak-to-Strong Generalization with Reliability-Aware Alignment
arxiv_id: '2406.19032'
source_url: https://arxiv.org/abs/2406.19032
tags:
- weak
- reliability
- labels
- weak-to-strong
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of aligning super-human language
  models with human knowledge, known as the "super-alignment" problem, which requires
  improving weak-to-strong generalization from imperfect supervision. The authors
  propose an unsupervised method that queries the weak supervisor for multiple answers,
  estimates answer reliability using entropy-based uncertainty and probability-based
  metrics, and enhances alignment by filtering uncertain data or re-weighting reliable
  data.
---

# Improving Weak-to-Strong Generalization with Reliability-Aware Alignment

## Quick Facts
- arXiv ID: 2406.19032
- Source URL: https://arxiv.org/abs/2406.19032
- Reference count: 24
- Primary result: Reliability-aware alignment methods improve weak-to-strong generalization with accuracy gains of 0.1-0.2 and PGR increases of 0.3-0.6

## Executive Summary
This paper addresses the challenge of aligning super-human language models with human knowledge by improving weak-to-strong generalization from imperfect supervision. The authors propose unsupervised reliability estimation methods that query weak supervisors for multiple answers and use entropy-based uncertainty and probability-based metrics to identify high-quality data. Through experiments on four diverse datasets, they demonstrate that their approach significantly outperforms naive baselines, effectively filtering unreliable weak labels and re-weighting reliable data to improve alignment performance.

## Method Summary
The proposed method involves generating N prompt variations for each question, obtaining multiple answers from a weak supervisor model, and computing reliability scores using entropy-based uncertainty metrics and probability-based frequency metrics. Two alignment approaches are evaluated: uncertainty filtering that selects examples with low entropy scores, and reliability re-weighting that adjusts training weights based on answer frequency. The strong model is fine-tuned using supervised fine-tuning with LoRA adapters, with the reliability estimation performed entirely unsupervised without requiring ground truth labels.

## Key Results
- Entropy scores correlate inversely with weak label accuracy, effectively identifying unreliable data
- Reliability-aware alignment methods achieve accuracy improvements of 0.1-0.2 over naive baselines
- Performance gap recovery increases by 0.3-0.6 across different model settings (Llama2-7B→Llama2-13B, Llama2-7B→Mistral-7B, Llama2-7B→Llama3-8B, Mistral-7B→Llama3-8B)
- Both uncertainty filtering and reliability re-weighting significantly improve weak-to-strong generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy-based uncertainty filtering identifies high-quality weak labels by measuring answer consistency across prompt variations
- Mechanism: When weak model provides consistent answers across multiple prompt variations, entropy is low indicating high reliability. High entropy indicates inconsistent responses suggesting unreliable labels
- Core assumption: Consistency across prompt variations correlates with label correctness
- Evidence anchors:
  - [abstract]: "entropy scores correlating inversely with weak label accuracy"
  - [section 4.2]: "Examples with lower entropy indicate that the weak model is more certain about its answers"
  - [corpus]: Weak evidence - related works focus on consistency-based reliability but don't specifically validate entropy-accuracy correlation
- Break condition: If weak model produces systematically inconsistent responses due to prompt sensitivity rather than uncertainty

### Mechanism 2
- Claim: Reliability re-weighting improves alignment by giving higher training weights to answers that appear frequently across prompt variations
- Mechanism: Compute empirical probability of each answer across N prompt variations. Answers appearing more frequently receive higher reliability scores and larger training weights in loss function
- Core assumption: Frequent answers across prompt variations indicate higher confidence and correctness
- Evidence anchors:
  - [abstract]: "probability-based metrics" and "re-weighting reliable data"
  - [section 4.3]: "A larger reliability score indicates that answer prediction ai appears more frequently in the weak model's answer set"
  - [corpus]: Weak evidence - related works use consistency checks but don't validate frequency-based re-weighting for alignment
- Break condition: If weak model has systematic bias toward certain answers regardless of correctness

### Mechanism 3
- Claim: Unsupervised reliability estimation enables alignment improvement without requiring ground truth labels
- Mechanism: Use entropy and frequency metrics calculated solely from weak model's responses to multiple prompt variations, eliminating need for ground truth supervision
- Core assumption: Weak model's internal consistency metrics can substitute for ground truth supervision in reliability estimation
- Evidence anchors:
  - [abstract]: "Our reliability estimation methods are unsupervised and do not rely on model-specific characteristics"
  - [section 4]: "Our methods address the core challenge of weak-to-strong alignment: the inherent noise in weak labels and the inaccessibility of the ground truth"
  - [corpus]: Moderate evidence - multiple papers cite unsupervised approaches to weak-to-strong generalization
- Break condition: If weak model's responses are too noisy or if prompt variations don't capture meaningful uncertainty

## Foundational Learning

- Concept: Supervised fine-tuning (SFT) with LoRA adapters
  - Why needed here: Enables efficient fine-tuning of strong models on weak supervision data without full parameter updates
  - Quick check question: What are the key hyperparameters for LoRA configuration in this work?

- Concept: Entropy as uncertainty metric
  - Why needed here: Provides quantitative measure of answer consistency across prompt variations to filter unreliable data
  - Quick check question: How