---
ver: rpa2
title: Are LLMs Aware that Some Questions are not Open-ended?
arxiv_id: '2410.00423'
source_url: https://arxiv.org/abs/2410.00423
tags:
- question
- awareness
- llms
- questions
- temperature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) possess\
  \ question awareness\u2014the ability to recognize when answers should be deterministic\
  \ versus creative. The authors find that while LLMs show some awareness, they often\
  \ lack it in domains like factual knowledge, leading to hallucinations."
---

# Are LLMs Aware that Some Questions are not Open-ended?

## Quick Facts
- arXiv ID: 2410.00423
- Source URL: https://arxiv.org/abs/2410.00423
- Reference count: 5
- LLMs show limited question awareness, leading to hallucinations in factual domains

## Executive Summary
This paper investigates whether large language models (LLMs) possess question awarenessâ€”the ability to recognize when answers should be deterministic versus creative. The authors find that while LLMs show some awareness, they often lack it in domains like factual knowledge, leading to hallucinations. To address this, they propose Question Awareness Temperature Sampling (QuATS), a method that adaptively adjusts output distributions using a learned temperature strategy based on question features. Experiments on models like LLaMA 2 and Falcon show that QuATS consistently improves performance across benchmarks, particularly for non-open-ended questions, by enhancing the models' question awareness.

## Method Summary
The method involves training a DetBlock (a tiny neural network) to predict the determinacy score of questions. During inference, the DetBlock's output is used to adjust the temperature of the Softmax function dynamically, enhancing the model's question awareness. The approach is evaluated on LLaMA 2-Chat and Falcon-instruct models across various benchmarks, including TruthfulQA, GSM8K, and RefGPT-Fact, as well as custom datasets for open-ended questions.

## Key Results
- QuATS consistently improves performance on non-open-ended questions across multiple benchmarks
- The method effectively reduces hallucinations in factual knowledge domains
- Ablation studies show that temperature adjustment is crucial for the method's effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adjusting the temperature in the softmax function directly controls the steepness of the output distribution, which reflects the model's determinacy in answering questions.
- Mechanism: The temperature parameter in the softmax function modulates the probability distribution of the next token. A lower temperature results in a steeper distribution, where the model is more confident and deterministic in selecting the next token. Conversely, a higher temperature flattens the distribution, allowing for more diverse and less deterministic choices.
- Core assumption: The steepness of the output distribution is a valid proxy for the model's determinacy and question awareness.
- Evidence anchors:
  - [abstract] "We utilize the temperature of the Softmax function (Bridle, 1989) to adjust the steepness to externally change the question awareness."
  - [section 2.1] "We can consider the original Softmax function without T as the Softmax function with a temperature of 1. As shown in Figure 1, if we sample the next token with a lower temperature, the output distribution will get steeper thus more likely sampling the token with a large probability."
- Break condition: If the relationship between temperature and output distribution steepness does not hold consistently across different models or datasets, the mechanism may break down.

### Mechanism 2
- Claim: The kurtosis of the output distribution serves as a metric for measuring the steepness and, consequently, the determinacy of the model's answers.
- Mechanism: Kurtosis quantifies the "tailedness" of a probability distribution. A higher kurtosis indicates a distribution with heavier tails and a sharper peak, which corresponds to a more deterministic output. By calculating the average kurtosis of the output distributions, the model's overall determinacy can be assessed.
- Core assumption: Kurtosis is a reliable measure of the steepness of the output distribution and correlates with the model's determinacy.
- Evidence anchors:
  - [section 2.2] "To measure the steepness, we introduce kurtosis as the metric. If the distribution is steeper, the model is more deterministic on this generated token and the kurtosis gets larger."
  - [section 2.2] "We use the average kurtosis of the distributions of all answer tokens to reflect the general determinacy of the answer."
- Break condition: If kurtosis does not accurately reflect the steepness of the output distribution or fails to correlate with the model's determinacy, the mechanism may break down.

### Mechanism 3
- Claim: The DetBlock network predicts the required determinacy for answering a given question, enabling the model to adaptively adjust its temperature and improve question awareness.
- Mechanism: The DetBlock is a small neural network that takes the hidden states of the input question as input and predicts a determinacy score. This score is then used to adjust the temperature dynamically during inference, allowing the model to be more or less deterministic based on the question type.
- Core assumption: The DetBlock can accurately predict the required determinacy for different types of questions.
- Evidence anchors:
  - [section 3.1] "We introduce a tiny network called DetBlock to predict the determinacy and leverage it to find the optimal temperature for sampling."
  - [section 3.1] "We use the questions as the input and determinacy scores as training labels."
- Break condition: If the DetBlock fails to accurately predict the required determinacy for different types of questions, the mechanism may break down.

## Foundational Learning

- Concept: Understanding the relationship between temperature, output distribution steepness, and model determinacy.
  - Why needed here: The core mechanism of the proposed method relies on adjusting the temperature to control the output distribution steepness, which in turn affects the model's determinacy in answering questions.
  - Quick check question: How does changing the temperature in the softmax function affect the output distribution and the model's determinacy?

- Concept: Familiarity with kurtosis as a statistical measure and its application in evaluating the steepness of probability distributions.
  - Why needed here: Kurtosis is used as a metric to quantify the steepness of the output distribution, which is crucial for assessing the model's determinacy and question awareness.
  - Quick check question: How does kurtosis relate to the shape of a probability distribution, and why is it suitable for measuring the steepness of the output distribution?

- Concept: Knowledge of neural network architectures and training processes, particularly the design and training of small networks like the DetBlock.
  - Why needed here: The DetBlock is a key component of the proposed method, and understanding its architecture and training process is essential for implementing and fine-tuning the model.
  - Quick check question: What are the key considerations in designing and training a small neural network like the DetBlock to predict the required determinacy for different types of questions?

## Architecture Onboarding

- Component map:
  LLM (e.g., LLaMA 2, Falcon) -> DetBlock -> Temperature Adjustment Module -> Output Distribution Analysis

- Critical path:
  1. Input question is fed into the LLM to generate hidden states.
  2. Hidden states are passed to the DetBlock to predict the determinacy score.
  3. Determinacy score is used to adjust the temperature dynamically during inference.
  4. Adjusted temperature influences the output distribution, affecting the model's determinacy in answering the question.

- Design tradeoffs:
  - Model size vs. performance: Larger models tend to be more deterministic but may require more computational resources.
  - Determinacy vs. creativity: Higher determinacy leads to more focused answers but may reduce the model's ability to generate creative responses.
  - Training complexity vs. effectiveness: The DetBlock adds complexity to the training process but can improve the model's question awareness.

- Failure signatures:
  - Inconsistent determinacy scores from the DetBlock across similar questions.
  - Overcorrection in temperature adjustment, leading to overly deterministic or creative responses.
  - Failure to improve question awareness or reduce hallucinations in the model's answers.

- First 3 experiments:
  1. Evaluate the relationship between temperature and output distribution steepness across different models and datasets.
  2. Assess the effectiveness of the DetBlock in predicting the required determinacy for different types of questions.
  3. Compare the performance of the model with and without the QuATS method on various benchmarks, focusing on question awareness and hallucination reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can intrinsic question awareness in LLMs be enhanced through architectural modifications rather than external adjustments like QuATS?
- Basis in paper: Explicit - The paper states that QuATS only improves question awareness externally through the DetBlock and does not enhance the model itself.
- Why unresolved: The paper does not explore architectural changes to improve intrinsic question awareness, focusing instead on external adjustments.
- What evidence would resolve it: Experimental results comparing models with intrinsic architectural modifications to those using QuATS would clarify if intrinsic enhancements are more effective.

### Open Question 2
- Question: How does the effectiveness of QuATS vary across different domains beyond factual knowledge and open-ended questions?
- Basis in paper: Explicit - The paper mentions that LLMs lack awareness in some domains like factual knowledge but does not extensively test QuATS across varied domains.
- Why unresolved: The evaluation focuses on specific types of questions (non-open-ended vs. open-ended) without a comprehensive analysis across diverse domains.
- What evidence would resolve it: Conducting experiments across a broader range of domains (e.g., technical, creative, conversational) with QuATS would provide insights into its effectiveness.

### Open Question 3
- Question: What is the long-term impact of using QuATS on model performance and adaptability in dynamic conversational settings?
- Basis in paper: Inferred - While the paper demonstrates immediate performance improvements, it does not address long-term adaptability or performance in evolving contexts.
- Why unresolved: The study focuses on static benchmarks and does not evaluate QuATS in dynamic or long-term conversational scenarios.
- What evidence would resolve it: Longitudinal studies assessing model performance and adaptability in real-time, dynamic conversations over extended periods would clarify the long-term impact of QuATS.

## Limitations

- The method relies on external adjustments rather than improving the model's intrinsic question awareness
- The effectiveness of kurtosis as a determinacy metric lacks direct experimental validation
- The manual labeling process for determinacy scores and its potential biases are not fully addressed

## Confidence

- High Confidence: Experimental results showing QuATS improving performance across multiple benchmarks are well-supported with quantitative data.
- Medium Confidence: The theoretical framework connecting temperature, output distribution steepness, and determinacy is logically sound but relies on assumptions that need more empirical validation.
- Low Confidence: The generalization of the method across different model architectures and domains beyond the tested benchmarks is uncertain.

## Next Checks

1. **Cross-Model Validation**: Test the QuATS method on additional model architectures beyond LLaMA 2 and Falcon to verify its generalizability and identify any model-specific limitations.

2. **Kurtosis Correlation Study**: Conduct experiments to explicitly measure the correlation between output distribution kurtosis and actual determinacy in answers, using human evaluation to validate the metric.

3. **DetBlock Robustness Analysis**: Systematically vary the training data composition and labeling criteria to assess the DetBlock's sensitivity to different determinacy score distributions and identify potential failure modes.