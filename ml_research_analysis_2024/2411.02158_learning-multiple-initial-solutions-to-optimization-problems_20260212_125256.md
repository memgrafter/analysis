---
ver: rpa2
title: Learning Multiple Initial Solutions to Optimization Problems
arxiv_id: '2411.02158'
source_url: https://arxiv.org/abs/2411.02158
tags:
- initial
- optimization
- solutions
- control
- multiple
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MISO (Learning Multiple Initial Solutions) trains a single neural
  network to predict multiple diverse initial solutions for optimization problems,
  enabling local optimizers to better handle non-convex, multimodal landscapes. The
  method offers two strategies: a single-optimizer approach that selects the most
  promising solution, and a multiple-optimizers approach that runs several optimizers
  in parallel with different initializations.'
---

# Learning Multiple Initial Solutions to Optimization Problems

## Quick Facts
- arXiv ID: 2411.02158
- Source URL: https://arxiv.org/abs/2411.02158
- Reference count: 35
- One-line primary result: MISO trains a single neural network to predict multiple diverse initial solutions for optimization problems, enabling local optimizers to better handle non-convex, multimodal landscapes.

## Executive Summary
MISO (Learning Multiple Initial Solutions) is a method that trains a single neural network to predict multiple diverse initial solutions for optimization problems, improving the performance of local optimizers on non-convex, multimodal landscapes. The method offers two strategies: a single-optimizer approach that selects the most promising solution, and a multiple-optimizers approach that runs several optimizers in parallel with different initializations. MISO guarantees performance at least as good as traditional warm-start heuristics by design. Tested across three tasks—cart-pole, reacher, and autonomous driving—using optimizers DDP, MPPI, and iLQR, MISO significantly outperformed baselines including warm-start, regression, and ensembles. In sequential optimization, MISO winner-takes-all and MISO mix reduced mean costs by up to 90% compared to warm-start. The method scales efficiently with the number of initial solutions, making it highly effective for real-time, safety-critical applications.

## Method Summary
MISO learns to predict multiple diverse initial solutions for optimization problems by training a Transformer-based neural network. The network takes problem instance parameters as input and outputs K candidate initial solutions. These candidates are then used to initialize either a single optimizer or multiple optimizers in parallel. The training process employs either a pairwise distance loss, winner-takes-all loss, or mixture loss to promote diversity among the predicted solutions. By including a default (warm-start) initialization among the candidates, MISO guarantees that the final solution is at least as good as using the default alone. The method was evaluated on three robot control tasks—cart-pole, reacher, and autonomous driving—using different optimizers (DDP, MPPI, iLQR) and showed significant improvements over baselines in both one-off and sequential optimization settings.

## Key Results
- MISO significantly outperformed baselines including warm-start, regression, and ensembles in three robot control tasks.
- In sequential optimization, MISO winner-takes-all and MISO mix reduced mean costs by up to 90% compared to warm-start.
- The method scales efficiently with the number of initial solutions, making it highly effective for real-time, safety-critical applications.
- MISO guarantees performance at least as good as traditional warm-start heuristics by design.

## Why This Works (Mechanism)

### Mechanism 1
The multi-output network can generate diverse candidate solutions that span different modes of the optimization landscape. By training with a winner-takes-all loss, the model is incentivized to produce multiple predictions, each targeting a different region of the solution space. Only the best prediction is penalized, allowing others to explore alternative modes without interference. This mechanism assumes the underlying optimization problem has multiple distinct basins of attraction (local/global optima), and diverse initializations improve the chance of reaching the global optimum.

### Mechanism 2
Including a default (warm-start) initialization among the candidates guarantees that the final solution is at least as good as using the default alone. The selection function can always choose the default initialization if none of the learned candidates are better. This ensures a performance floor equal to the baseline, assuming the default initialization is feasible and provides a reasonable baseline performance.

### Mechanism 3
Training with a pairwise distance loss term promotes dispersion among the predicted solutions, preventing mode collapse. The loss function penalizes similarity between any pair of outputs, encouraging the network to produce solutions that are spread across the solution space rather than clustering in one region. This mechanism assumes the optimization landscape contains multiple modes, and spreading predictions across them increases the likelihood of capturing a good initialization.

## Foundational Learning

- Concept: Supervised learning with regression loss
  - Why needed here: The network must learn to map problem parameters to initial solutions close to the optimal, requiring a regression objective on the solution space.
  - Quick check question: Does the loss function measure distance between predicted and optimal solutions in the control or state space?

- Concept: Multimodal output generation
  - Why needed here: Single-output models tend to predict the mean of the data distribution, which may fall in suboptimal regions. Multiple outputs allow capturing different modes.
  - Quick check question: How does the winner-takes-all loss encourage the model to produce diverse predictions rather than collapsing to one mode?

- Concept: Selection functions and post-processing
  - Why needed here: After generating multiple candidates, a mechanism is required to choose the most promising one for the optimizer, based on domain-specific criteria.
  - Quick check question: What criteria beyond the objective function could be used to select the best initial solution in a safety-critical application?

## Architecture Onboarding

- Component map: Input parameters -> Transformer network -> K candidate initial solutions -> Selection function -> Chosen candidate -> Local optimizer -> Final solution
- Critical path: 1. Forward pass through Transformer to generate K candidates 2. Apply selection function to choose the best candidate 3. Feed chosen candidate into local optimizer 4. Execute optimizer and evaluate final solution cost
- Design tradeoffs:
  - K vs. inference time: Larger K improves coverage but increases computation; multi-output models scale better than ensembles
  - Loss formulation: Winner-takes-all promotes diversity but may underfit; pairwise distance ensures spread but requires tuning
  - State vs. control loss: Including state loss mitigates compounding errors but requires differentiable dynamics
- Failure signatures:
  - All K candidates produce similar trajectories → mode collapse, likely due to insufficient dispersion loss or poor training data
  - Selected candidate leads to optimizer failure → selection function may not align with optimizer requirements
  - No improvement over warm-start → candidates may be too far from optimal regions or optimization landscape is too smooth
- First 3 experiments:
  1. Train with K=1 and compare to warm-start baseline to verify basic learning capability
  2. Increase K and evaluate whether diversity improves optimizer performance on multimodal tasks
  3. Test different selection functions (e.g., constraint satisfaction vs. objective minimization) on a safety-critical scenario

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance guarantees of MISO hold when the initial solution provided by MISO is worse than the default initialization?
- Basis in paper: The paper states that MISO is guaranteed to be equal or better than the default initialization by design.
- Why unresolved: The paper does not provide evidence or analysis of scenarios where MISO's initial solutions might be worse than the default.
- What evidence would resolve it: Experimental results comparing the cost of MISO's initial solutions to the default initialization across various problem instances.

### Open Question 2
- Question: How does the choice of the selection function Λ affect the performance of MISO in different optimization tasks?
- Basis in paper: The paper discusses using the objective function J as the selection function but mentions alternative choices.
- Why unresolved: The paper does not explore or compare the impact of different selection functions on MISO's performance.
- What evidence would resolve it: Comparative analysis of MISO's performance using different selection functions across various tasks.

### Open Question 3
- Question: How does MISO perform when applied to optimization problems with high-dimensional and complex solution structures?
- Basis in paper: The paper mentions that in highly complex optimization problems, learning initial solution candidates can become challenging.
- Why unresolved: The paper does not provide empirical results or analysis for high-dimensional optimization problems.
- What evidence would resolve it: Experimental results evaluating MISO on optimization problems with high-dimensional solution spaces.

## Limitations
- Limited empirical validation of performance guarantees and diversity mechanisms.
- Lack of ablation studies on the selection function and comparisons to other multi-solution generation methods.
- Absence of analysis for high-dimensional optimization problems and real-world safety-critical applications.

## Confidence
- Performance guarantee via default initialization: Medium
- Diversity promotion through loss functions: Medium
- Scalability and efficiency claims: Medium
- Safety and constraint satisfaction in real-world applications: Low

## Next Checks
1. Run ablation studies removing the default initialization to quantify its contribution to the performance floor.
2. Measure and report the diversity of generated solutions (e.g., pairwise distances, mode coverage) on a known multimodal optimization benchmark.
3. Compare MISO against ensemble methods in terms of both solution quality and computational efficiency to validate the claimed scaling advantages.