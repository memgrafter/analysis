---
ver: rpa2
title: Fuzzy Logic Function as a Post-hoc Explanator of the Nonlinear Classifier
arxiv_id: '2401.14417'
source_url: https://arxiv.org/abs/2401.14417
tags:
- classifier
- features
- feature
- truth
- black
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using fuzzy logic functions as post-hoc
  explainers for deep neural network classifiers. The approach maps feature importance
  measures to truth values for input to a fuzzy logic function, aiming to match the
  decisions of the black box classifier.
---

# Fuzzy Logic Function as a Post-hoc Explanator of the Nonlinear Classifier

## Quick Facts
- arXiv ID: 2401.14417
- Source URL: https://arxiv.org/abs/2401.14417
- Authors: Martin Klimo; Lubomir Kralik
- Reference count: 13
- Key outcome: DeconvNet achieved perfect matching of classification decisions between the explainable and black box classifier on MNIST and FashionMNIST datasets

## Executive Summary
This paper investigates using fuzzy logic functions as post-hoc explainers for deep neural network classifiers. The approach maps feature importance measures to truth values for input to a fuzzy logic function, aiming to match the decisions of the black box classifier. Four feature importance methods were compared: raw features, Saliency Maps, DeconvNet, Guided Backpropagation, and Layer-wise Relevance Propagation (LRP). On MNIST and FashionMNIST datasets, DeconvNet achieved perfect matching of classification decisions between the explainable and black box classifier, while other methods showed lower performance. The results suggest DeconvNet is the optimal transformation of feature values to truth values for this fuzzy logic-based post-hoc explanation approach.

## Method Summary
The method trains a LeNet-5 CNN on MNIST and FashionMNIST datasets, then uses four feature importance methods (raw features, Saliency Maps, DeconvNet, Guided Backpropagation, LRP) to generate truth values for a fuzzy logic function. The fuzzy logic function with Zadeh logic uses these truth values to make classification decisions that are compared against the black box classifier. The approach filters out irrelevant features using an irrelevance threshold and evaluates success by measuring the matching rate between explainable and black box classifier decisions.

## Key Results
- DeconvNet achieved perfect matching of classification decisions between the explainable and black box classifier
- Other methods (Saliency Maps, Guided Backpropagation, LRP, raw features) showed lower matching performance
- The irrelevance threshold mechanism effectively filtered out features that were indifferent to the classification outcome

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeconvNet produces truth values for features that best align the fuzzy logic classifier's decisions with the black box classifier.
- Mechanism: DeconvNet propagates only positive gradients during backward pass, effectively filtering out negative contributions and focusing on features most relevant for the current classification decision.
- Core assumption: Features with positive gradient contributions are more critical for explaining the classification decision than features with negative or mixed contributions.
- Evidence anchors:
  - [abstract]: "DeconvNet achieved perfect matching of classification decisions between the explainable and black box classifier"
  - [section]: "DeconvNet [8] slightly modifies the previous method in terms of changing the computation of the gradient. In this case, only positive gradient values are propagated, and any negative ones are simply set to zero."
  - [corpus]: Weak evidence - corpus neighbors do not discuss DeconvNet specifically, though they cover fuzzy systems and explainability

### Mechanism 2
- Claim: The fuzzy logic function with Zadeh logic can perfectly mimic black box decisions when provided with optimal truth values.
- Mechanism: The fuzzy logic function operates on truth values derived from feature importance measures, where features are classified as positively relevant (1), negatively relevant (0), or irrelevant (X), and uses modus ponens IF-THEN rules in disjunctive normal form.
- Core assumption: Classification decisions can be expressed as logical combinations of feature relevances, and Zadeh fuzzy logic can capture the necessary logical relationships.
- Evidence anchors:
  - [abstract]: "fuzzy logic function forms the classifier and DeconvNet importance gives the truth values"
  - [section]: "The logical inference is based on modus ponens IF-THEN rules forming a logic function in a disjunctive normal form. Due to the vagueness of input data, fuzzy logic was applied."
  - [corpus]: Weak evidence - corpus neighbors discuss fuzzy logic systems but not specifically Zadeh logic for post-hoc explanation

### Mechanism 3
- Claim: Filtering out irrelevant features (those with truth values near 0.5) improves the alignment between explainable and black box classifiers.
- Mechanism: Features with normalized importance measures near 0.5 are considered irrelevant and excluded from the fuzzy logic computation, preventing them from diluting the decision signal.
- Core assumption: Features with importance measures near the midpoint of the normalized range do not significantly contribute to the classification decision and their inclusion adds noise rather than signal.
- Evidence anchors:
  - [section]: "We must avoid the influence of features that are indifferent to the outcome of the decision (a truth value of about one-half). We label such features irrelevant and do not use them as input to an explainable classifier."
  - [section]: "Rounding ùëêùëñ= { 1, ùë¶ÃÉ·µ¢> 1 2+Œî ùëã, 1 2‚àíŒî‚â§ùë¶ÃÉ·µ¢‚â§ 1 2+Œî 0, ùë¶ÃÉ·µ¢< 1 2‚àíŒî" (shows the irrelevance threshold mechanism)
  - [corpus]: No direct evidence - corpus neighbors do not discuss feature relevance filtering

## Foundational Learning

- Concept: Zadeh fuzzy logic and membership functions
  - Why needed here: The paper uses Zadeh fuzzy logic to handle the vagueness of input data and assign truth values to features for the explainable classifier.
  - Quick check question: What is the difference between classical Boolean logic and Zadeh fuzzy logic in terms of truth values?

- Concept: Gradient-based feature importance methods
  - Why needed here: The paper compares four gradient-based methods (Saliency Maps, DeconvNet, Guided Backpropagation, LRP) to determine which best transforms features into truth values for the fuzzy logic function.
  - Quick check question: How does DeconvNet differ from standard gradient-based methods in terms of how it handles negative gradients?

- Concept: Post-hoc explainability and fidelity
  - Why needed here: The paper's approach is to create an explainable classifier that matches the decisions of a black box classifier, measuring success by fidelity (matching decisions).
  - Quick check question: What is the difference between model-agnostic and model-specific explainability methods?

## Architecture Onboarding

- Component map:
  - Black box classifier (LeNet-5 CNN) -> Feature extraction layer -> Feature importance computation (Saliency Maps, DeconvNet, Guided Backpropagation, or LRP) -> Normalization and relevance categorization -> Fuzzy logic function with Zadeh logic -> Decision matching evaluation

- Critical path: Image ‚Üí Feature extraction ‚Üí Feature importance computation ‚Üí Normalization ‚Üí Relevance categorization ‚Üí Fuzzy logic function ‚Üí Decision ‚Üí Compare with black box decision

- Design tradeoffs:
  - Accuracy vs. interpretability: The approach prioritizes matching black box decisions over creating truly interpretable rules
  - Computational cost: Gradient-based importance methods add computation time compared to using raw features
  - Generalization: The approach is tested only on MNIST and FashionMNIST with LeNet-5 architecture

- Failure signatures:
  - Low matching rate between explainable and black box classifiers
  - The irrelevance threshold filters out too many relevant features
  - The fuzzy logic function cannot capture complex decision boundaries

- First 3 experiments:
  1. Replace DeconvNet with raw normalized features as truth values and measure matching rate
  2. Vary the irrelevance threshold Œî and observe its effect on matching rate and explanation quality
  3. Test the approach on a different dataset (e.g., CIFAR-10) with a different architecture to assess generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal irrelevance range Œî for filtering out irrelevant features in the fuzzy logic function?
- Basis in paper: [explicit] The paper mentions that setting the truth value to one-half is not sufficient to exclude irrelevant features, and an irrelevance range Œî is needed. It states that "no general rule for the irrelevance range finding was not obtained yet."
- Why unresolved: The paper does not provide a specific method or formula for determining the optimal Œî value. It only mentions that increasing Œî reduces the number of negatively relevant features and makes the decision easier to interpret.
- What evidence would resolve it: Experimental results showing the performance of the fuzzy logic function with different Œî values on various datasets and recognition systems would help determine the optimal irrelevance range.

### Open Question 2
- Question: Can the post-hoc explainable classifier based on Zadeh's fuzzy logic generalize to other datasets and recognition systems beyond MNIST and FashionMNIST?
- Basis in paper: [inferred] The paper only tests the approach on MNIST and FashionMNIST datasets with LeNet-5 architecture. It states that "to conclude, the main findings are: The post-hoc explainer of the black box classifier based on Zadeh fuzzy logic gives the perfect fitting on MNIST and FashionMNIST databases."
- Why unresolved: The paper does not provide evidence of the approach's performance on other datasets or recognition systems. The generalizability of the method is unknown.
- What evidence would resolve it: Experimental results demonstrating the performance of the post-hoc explainable classifier on various datasets (e.g., CIFAR-10, ImageNet) and recognition systems (e.g., ResNet, VGG) would show the generalizability of the approach.

### Open Question 3
- Question: How can the explainable classifier detect misclassification made by the black box classifier?
- Basis in paper: [explicit] The paper states that "the main goal is a class prediction fitting the explainable and black box classifier. Therefore the explainable classifier cannot detect misclassification. The evaluation of confidence and truth degree is left for further research."
- Why unresolved: The paper does not provide a method for the explainable classifier to identify when the black box classifier has made an incorrect prediction. The evaluation of confidence and truth degree is mentioned as a topic for future research.
- What evidence would resolve it: A proposed method or algorithm that allows the explainable classifier to detect misclassification, along with experimental results demonstrating its effectiveness, would resolve this open question.

## Limitations

- The evaluation is limited to MNIST and FashionMNIST datasets with LeNet-5 architecture, leaving uncertainty about performance on more complex image datasets or different neural network architectures.
- The paper provides weak evidence from corpus neighbors for the effectiveness of DeconvNet and fuzzy logic systems in this specific application.
- The approach prioritizes matching black box decisions over creating truly interpretable rules, which may limit practical interpretability.

## Confidence

- Medium confidence in the claim that DeconvNet achieves perfect matching on MNIST and FashionMNIST datasets
- Low confidence in the generalizability of results to other datasets and architectures
- Medium confidence in the mechanism explanation, though underlying theory could benefit from additional analysis

## Next Checks

1. Test the approach on CIFAR-10 or ImageNet datasets with deeper CNN architectures to assess scalability and robustness to more complex visual features.

2. Conduct ablation studies varying the irrelevance threshold Œî to determine optimal values and understand sensitivity to this hyperparameter.

3. Compare DeconvNet's performance against other post-hoc explanation methods like SHAP or LIME on the same datasets to establish relative effectiveness in matching black box decisions.