---
ver: rpa2
title: Evaluation of data inconsistency for multi-modal sentiment analysis
arxiv_id: '2406.03004'
source_url: https://arxiv.org/abs/2406.03004
tags:
- sentiment
- conflicting
- data
- multimodal
- multi-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark called DiffEmo to evaluate
  multi-modal sentiment analysis models on data with conflicting emotional expressions
  across text, audio, and visual modalities. The authors construct a test set from
  the CH-SIMS v2.0 dataset by selecting samples where uni-modal sentiment annotations
  differ by more than 1.0 in absolute value.
---

# Evaluation of data inconsistency for multi-modal sentiment analysis

## Quick Facts
- arXiv ID: 2406.03004
- Source URL: https://arxiv.org/abs/2406.03004
- Reference count: 0
- Multi-modal models show significant performance degradation when processing conflicting emotional expressions across text, audio, and visual modalities

## Executive Summary
This paper introduces DiffEmo, a benchmark designed to evaluate multi-modal sentiment analysis models on data with conflicting emotional expressions across different modalities. The authors construct a test set from CH-SIMS v2.0 by selecting samples where uni-modal sentiment annotations differ by more than 1.0 in absolute value. They evaluate both traditional multi-modal fusion models and multi-modal large language models across three settings: Mixed, Conflicting, and Aligned data. Results demonstrate substantial performance drops when handling conflicting data, with accuracy decreases of 10-15% compared to aligned samples.

## Method Summary
The authors create the DiffEmo benchmark by selecting samples from CH-SIMS v2.0 where uni-modal sentiment annotations differ by more than 1.0 in absolute value. They evaluate traditional multi-modal fusion models (MulT, MFN) and multi-modal large language models (MLLMs) including Video-LLaMA across three experimental settings: Mixed (containing both aligned and conflicting samples), Conflicting (only conflicting samples), and Aligned (only consistent samples). The evaluation measures accuracy and qualitative response quality, with ablation studies examining model behavior on different data types.

## Key Results
- Performance degradation of 10-15% accuracy across all models when processing conflicting vs. aligned data
- Traditional models (MulT, MFN) show relatively better performance on conflicting data compared to MLLMs
- Video-LLaMA produces irrelevant responses on conflicting samples and struggles to distinguish conflicting from aligned data

## Why This Works (Mechanism)
Models trained on aligned multi-modal data learn to fuse consistent emotional cues across modalities. When presented with conflicting information, these learned fusion patterns break down because the model cannot reconcile contradictory signals. Traditional models with explicit fusion mechanisms can sometimes average or weight modalities differently, providing some robustness. MLLMs, which rely on end-to-end reasoning, lack explicit mechanisms to handle modality conflicts and default to simplistic strategies like classifying most samples as conflicting.

## Foundational Learning
1. **Cross-modal sentiment fusion** - Models learn weighted combinations of text, audio, and visual sentiment signals; needed to understand how models combine information, quick check: examine attention weights across modalities
2. **Conflict detection mechanisms** - Models must identify when modalities disagree; needed to explain why some models perform better on conflicting data, quick check: measure confidence scores on conflicting vs. aligned samples
3. **Multi-modal large language model reasoning** - MLLMs process multiple modalities through end-to-end architectures; needed to understand limitations in handling semantic inconsistencies, quick check: analyze MLLM output quality on conflicting samples

## Architecture Onboarding

**Component Map:** Raw Data -> Feature Extraction (Text/Audio/Visual) -> Sentiment Scoring (Uni-modal) -> Conflict Detection -> Multi-modal Fusion -> Final Sentiment Prediction

**Critical Path:** Feature extraction from all three modalities → Uni-modal sentiment scoring → Multi-modal fusion module → Final classification decision

**Design Tradeoffs:** Traditional models use explicit fusion mechanisms (attention, tensor fusion) that can be tuned for conflict handling vs. MLLMs use end-to-end reasoning that lacks explicit conflict resolution but offers more flexible representations

**Failure Signatures:** Performance collapse on conflicting data, tendency to default to majority class predictions, generation of irrelevant or generic responses when modalities disagree

**First Experiments:**
1. Evaluate model confidence scores on conflicting vs. aligned samples to assess detection capability
2. Perform ablation by removing individual modalities to identify which contributes most to confusion
3. Test threshold sensitivity by varying the inconsistency cutoff from 0.5 to 1.5

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on sentiment classification without exploring nuanced emotional states or intensity levels
- DiffEmo benchmark may not generalize to other types of cross-modal inconsistencies beyond sentiment
- Arbitrary selection threshold (> 1.0 difference) could influence results and may not capture all meaningful inconsistencies

## Confidence
- High confidence: Performance degradation on conflicting vs. aligned data is consistently observed across all tested models
- Medium confidence: Relative ranking of model performances (traditional vs. MLLM) is reliable within specific experimental conditions
- Low confidence: Generalizability of findings to other MLLMs beyond Video-LLaMA or to other multi-modal tasks

## Next Checks
1. Test the same models on DiffEmo using different inconsistency thresholds (e.g., > 0.5 and > 1.5) to assess sensitivity to selection criteria
2. Evaluate additional MLLM architectures beyond Video-LLaMA to determine if observed weaknesses are model-specific or represent a broader trend
3. Implement and test confidence calibration mechanisms to measure whether models can reliably detect when processing conflicting vs. aligned data