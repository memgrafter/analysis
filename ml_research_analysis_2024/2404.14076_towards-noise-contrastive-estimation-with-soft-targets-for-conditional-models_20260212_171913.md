---
ver: rpa2
title: Towards noise contrastive estimation with soft targets for conditional models
arxiv_id: '2404.14076'
source_url: https://arxiv.org/abs/2404.14076
tags:
- soft
- infonce
- target
- loss
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of combining soft targets with
  noise contrastive estimation (NCE) for supervised classification. The authors propose
  a novel loss function, soft target InfoNCE, which integrates soft target probabilities
  into the NCE framework by creating weighted linear combinations of label embeddings.
---

# Towards noise contrastive estimation with soft targets for conditional models

## Quick Facts
- arXiv ID: 2404.14076
- Source URL: https://arxiv.org/abs/2404.14076
- Reference count: 22
- Primary result: Proposes soft target InfoNCE loss that integrates soft target probabilities into NCE framework, achieving competitive accuracy and improved calibration on image classification tasks

## Executive Summary
This paper addresses the challenge of combining soft targets with noise contrastive estimation (NCE) for supervised classification. The authors propose soft target InfoNCE, a novel loss function that integrates soft target probabilities into the NCE framework by creating weighted linear combinations of label embeddings. This approach is motivated through noise contrastive estimation and leverages the continuous categorical distribution. Experiments on ImageNet, Tiny ImageNet, CIFAR-100, and CellTypeGraph demonstrate that soft target InfoNCE performs on par with strong soft target cross-entropy baselines and outperforms hard target NLL and InfoNCE losses, while showing improved calibration properties.

## Method Summary
The paper proposes soft target InfoNCE, a loss function that extends the InfoNCE framework to handle soft target probabilities. The method creates weighted linear combinations of label embeddings based on soft target probabilities, allowing the model to learn from uncertain or smoothed labels. The approach is motivated through noise contrastive estimation and leverages the continuous categorical distribution. The implementation is compatible with standard classification models trained with cross-entropy, requiring only minor modifications to accept soft target probabilities as input.

## Key Results
- Soft target InfoNCE achieves competitive classification accuracy compared to strong soft target cross-entropy baselines
- The method outperforms hard target NLL and InfoNCE losses on multiple benchmarks including ImageNet, Tiny ImageNet, CIFAR-100, and CellTypeGraph
- Models trained with soft target InfoNCE show improved calibration properties with lower expected calibration error (ECE) compared to soft target cross-entropy models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft target InfoNCE integrates soft target probabilities into the InfoNCE framework by creating weighted linear combinations of label embeddings.
- Mechanism: The method uses soft targets to weight the scoring function between data embeddings and label embeddings, effectively pulling embeddings closer based on target probability. This replaces the hard one-hot encoding with probabilistic weighting, allowing the model to handle uncertainty in labels.
- Core assumption: The continuous categorical distribution can approximate the true conditional density when combined with soft targets.
- Evidence anchors:
  - [abstract] "Our new soft target InfoNCE loss is conceptually simple, efficient to compute, and can be motivated through the framework of noise contrastive estimation."
  - [section 3.2] "Our new soft target InfoNCE loss is conceptually simple, efficient to compute, and can be motivated through the framework of noise contrastive estimation by leveraging the continuous categorical distribution in combination with a Bayesian argument."
- Break condition: When the number of classes is extremely large (thousands) and sampling from the continuous categorical distribution becomes computationally intractable.

### Mechanism 2
- Claim: Soft target InfoNCE improves parameter estimation in cases of label uncertainty compared to standard NLL.
- Mechanism: By using soft targets instead of hard one-hot encodings, the method creates a smoother loss landscape that better captures the uncertainty in label assignments. This allows the model to make more accurate parameter estimates when the true conditional distribution is ambiguous.
- Core assumption: The true conditional distribution is more complex than a degenerate one-hot distribution in many real-world scenarios.
- Evidence anchors:
  - [section 3.1] "The results, visualized in Figure 1, indicate that increased levels of label uncertainty lead to worse parameter estimates using NLL when compared to InfoNCE."
  - [section 3.1] "Our experiment demonstrates the advantage of implicit sampling from a dataset generated by the true data distribution as opposed to using an explicit model distribution as target."
- Break condition: When label uncertainty is minimal and the true conditional distribution is nearly degenerate, where hard targets may perform comparably.

### Mechanism 3
- Claim: Soft target InfoNCE provides improved calibration properties compared to standard soft target cross-entropy.
- Mechanism: The contrastive nature of the loss creates a different energy landscape that encourages better separation between classes while maintaining appropriate confidence levels. The soft targets modulate the strength of attraction between data embeddings and label embeddings.
- Core assumption: Model calibration is important for practical deployment and affects decision-making quality.
- Evidence anchors:
  - [abstract] "Experiments on ImageNet, Tiny ImageNet, CIFAR-100, and CellTypeGraph demonstrate that soft target InfoNCE performs on par with strong soft target cross-entropy baselines and outperforms hard target NLL and InfoNCE losses."
  - [section 4.2] "Notably, models trained with soft target InfoNCE were better calibrated in these two cases. Specifically, we obtained an expected calibration error (ECE) of 3.9% on Tiny ImageNet and 2.9% on CIFAR-100 which was lower than the respective ECE of the soft target cross-entropy models (7% Tiny ImageNet, 15.8% CIFAR-100)."
- Break condition: When calibration is not a priority and pure accuracy is the only metric of interest.

## Foundational Learning

- Concept: Noise Contrastive Estimation (NCE)
  - Why needed here: NCE forms the theoretical foundation for InfoNCE and allows estimation of unnormalized probability distributions through negative sampling.
  - Quick check question: How does NCE differ from maximum likelihood estimation in terms of computational complexity when dealing with large label spaces?

- Concept: Continuous Categorical Distribution
  - Why needed here: This distribution generalizes the discrete categorical distribution and allows for efficient sampling of probability vectors that sum to one, which is crucial for the soft target integration.
  - Quick check question: What is the key difference between the continuous categorical distribution and its discrete counterpart in terms of the support space?

- Concept: Label Smoothing
  - Why needed here: Label smoothing is a regularization technique that prevents overconfidence by distributing small probability mass across non-target classes, which is directly integrated into the soft target InfoNCE framework.
  - Quick check question: How does label smoothing affect the entropy of the target distribution, and why might this be beneficial for model generalization?

## Architecture Onboarding

- Component map: Data → Model (produces logits) → Soft target InfoNCE (computes weighted scores) → Loss → Backpropagation (updates both embedding and classification head weights)
- Critical path: Data → Model (produces logits) → Soft target InfoNCE (computes weighted scores) → Loss → Backpropagation (updates both embedding and classification head weights)
- Design tradeoffs: Using a 3-layer MLP classification head instead of a linear layer provides better performance but increases model complexity. The loss requires careful temperature scaling and can be sensitive to batch size due to the number of available negative samples.
- Failure signatures: Poor performance with small batch sizes (insufficient negative samples), degraded accuracy with linear classification heads compared to MLP heads, and potential instability when temperature is not properly tuned.
- First 3 experiments:
  1. Compare soft target InfoNCE vs. standard cross-entropy on a small image classification dataset with label smoothing enabled.
  2. Test the effect of varying label smoothing parameters (ϵ) on model calibration and accuracy.
  3. Evaluate the impact of batch size on performance, comparing coupling vs. separating negative samples from the batch.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does soft target InfoNCE compare to other contrastive learning methods that use soft targets, such as Debiased Contrastive Learning (Chuang et al., 2020) or Hard Negative Mixing (Kalantidis et al., 2020)?
- Basis in paper: [inferred] The paper compares soft target InfoNCE to other methods that use soft targets, such as soft target cross-entropy and label smoothing. However, it does not compare to other contrastive learning methods that use soft targets.
- Why unresolved: The paper focuses on comparing soft target InfoNCE to other loss functions, but does not explore how it compares to other contrastive learning methods that use soft targets.
- What evidence would resolve it: Experiments comparing soft target InfoNCE to other contrastive learning methods that use soft targets on the same benchmarks used in the paper.

### Open Question 2
- Question: How does the choice of noise distribution in soft target InfoNCE affect the learned representations and classification performance?
- Basis in paper: [explicit] The paper mentions that the noise distribution can be chosen as a uniform distribution or a label smoothing distribution, but does not explore how different choices of noise distribution affect the learned representations and classification performance.
- Why unresolved: The paper does not investigate the impact of different noise distributions on the learned representations and classification performance.
- What evidence would resolve it: Experiments comparing soft target InfoNCE with different choices of noise distribution on the same benchmarks used in the paper.

### Open Question 3
- Question: How does soft target InfoNCE perform in other problem settings, such as knowledge distillation or semi-supervised learning?
- Basis in paper: [explicit] The paper mentions that soft targets are also used in knowledge distillation, but does not explore how soft target InfoNCE performs in this setting.
- Why unresolved: The paper focuses on supervised classification tasks and does not investigate the performance of soft target InfoNCE in other problem settings.
- What evidence would resolve it: Experiments evaluating the performance of soft target InfoNCE in knowledge distillation or semi-supervised learning tasks.

## Limitations
- Scalability concerns with extremely large label spaces (10K+ classes) due to computational overhead of sampling from continuous categorical distribution
- Performance improvements may be partially attributed to specific combination of augmentations (label smoothing, MixUp, CutMix) rather than the loss function alone
- Calibration improvements observed on specific datasets may not generalize to all model architectures or diverse datasets

## Confidence

- **High confidence**: The theoretical derivation connecting soft targets to the continuous categorical distribution and InfoNCE framework is mathematically sound and well-established in the literature.
- **Medium confidence**: The empirical results showing competitive accuracy with soft target cross-entropy baselines and improved calibration are convincing for the tested datasets, but may not generalize universally.
- **Low confidence**: The scalability claims to extremely large label spaces and the robustness of calibration improvements across diverse model architectures and datasets.

## Next Checks

1. **Scalability Test**: Evaluate soft target InfoNCE on a dataset with 10,000+ classes (e.g., JFT-300M or OpenImages) to assess computational overhead and performance degradation.

2. **Architecture Ablation**: Test the method with different backbone architectures (CNNs, MLPs, smaller transformers) and classification head designs to verify the robustness of calibration improvements.

3. **Noise Robustness**: Evaluate the method's performance under varying levels of label noise and compare against standard label smoothing and other robust loss functions to isolate the contribution of the contrastive framework.