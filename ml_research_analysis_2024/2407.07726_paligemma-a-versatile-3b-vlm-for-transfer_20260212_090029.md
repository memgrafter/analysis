---
ver: rpa2
title: 'PaliGemma: A versatile 3B VLM for transfer'
arxiv_id: '2407.07726'
source_url: https://arxiv.org/abs/2407.07726
tags:
- image
- paligemma
- tasks
- transfer
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PaliGemma is a 3B-parameter open Vision-Language Model combining\
  \ a SigLIP-So400m vision encoder with a Gemma-2B language model. Trained with 1B\
  \ multimodal examples at 224px resolution and further fine-tuned at higher resolutions\
  \ (448px, 896px), it achieves strong transfer performance across 40+ diverse tasks\u2014\
  including standard VQA, captioning, segmentation, remote sensing, and OCR\u2014\
  without instruction tuning."
---

# PaliGemma: A versatile 3B VLM for transfer

## Quick Facts
- arXiv ID: 2407.07726
- Source URL: https://arxiv.org/abs/2407.07726
- Reference count: 40
- PaliGemma achieves strong transfer performance across 40+ diverse tasks, demonstrating that smaller VLMs can match or exceed much larger models when trained for broad transfer.

## Executive Summary
PaliGemma is a 3B-parameter open Vision-Language Model combining a SigLIP-So400m vision encoder with a Gemma-2B language model. Trained with 1B multimodal examples at 224px resolution and further fine-tuned at higher resolutions (448px, 896px), it achieves strong transfer performance across 40+ diverse tasks—including standard VQA, captioning, segmentation, remote sensing, and OCR—without instruction tuning. The base model consistently reaches state-of-the-art results in its size class, demonstrating that smaller VLMs can match or exceed much larger models when trained for broad transfer. Hyperparameter sensitivity is low, and strong results are attainable with few examples (e.g., 256–4k), confirming its effectiveness as a versatile foundation for vision-language tasks.

## Method Summary
PaliGemma follows a three-stage training procedure: Stage0 (unimodal pretraining of SigLIP-So400m vision encoder and Gemma-2B language model), Stage1 (multimodal pretraining at 224px resolution with 1 billion examples), and Stage2 (resolution increase to 448px and 896px with task-specific fine-tuning). The model uses prefix-LM masking and task-specific prefixes to improve spatial understanding and transfer performance. No components are frozen during pretraining, and the model is evaluated on a diverse set of 40+ tasks, including standard VLM benchmarks and specialized tasks like remote sensing and segmentation.

## Key Results
- PaliGemma achieves state-of-the-art results in its size class on 40+ diverse tasks without instruction tuning.
- Strong performance is attainable with few examples (256–4k), demonstrating low hyperparameter sensitivity.
- The three-stage training procedure, including resolution increase, significantly improves performance on tasks benefiting from higher resolution.

## Why This Works (Mechanism)
The paper does not explicitly discuss the mechanism behind PaliGemma's effectiveness. However, the three-stage training procedure, including resolution increase, likely contributes to the model's strong transfer performance by allowing it to learn rich multimodal representations at different scales.

## Foundational Learning
- Multimodal pretraining: Why needed - to learn rich representations across vision and language modalities. Quick check - evaluate the model's performance on a diverse set of downstream tasks.
- Resolution scaling: Why needed - to improve performance on tasks requiring high-resolution inputs. Quick check - compare the model's performance at different resolutions on resolution-sensitive tasks.
- Prefix-LM masking: Why needed - to improve spatial understanding and transfer performance. Quick check - evaluate the impact of prefix-LM masking on the model's pretraining perplexity and transfer performance.

## Architecture Onboarding
- Component map: SigLIP-So400m vision encoder -> linear projection -> Gemma-2B language model
- Critical path: Multimodal pretraining (Stage1) -> Resolution increase (Stage2) -> Task-specific fine-tuning
- Design tradeoffs: Smaller model size (3B parameters) vs. strong transfer performance; no instruction tuning vs. versatility across tasks
- Failure signatures: Poor transfer performance due to suboptimal hyperparameter selection; instability during training or transfer due to model's small size and task complexity
- First experiments:
  1. Pretrain the model on a publicly available task mixture and evaluate performance on a diverse set of downstream tasks.
  2. Investigate the impact of the three-stage training procedure on the final performance by comparing against a model trained with a different procedure.
  3. Compare the performance of PaliGemma against larger VLMs on a subset of tasks to validate the claim that it can match or exceed much larger models.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does PaliGemma's performance compare to other models when transferred with limited examples, especially for tasks requiring spatial understanding?
- Basis in paper: [explicit] The paper shows PaliGemma can reach within 10% of full-data score with 4k examples and 20% with 256 examples, but doesn't provide direct comparisons with other models on the same tasks with limited data.
- Why unresolved: While the paper demonstrates PaliGemma's effectiveness with limited examples, it doesn't benchmark this against other VLMs or large language models in similar few-shot settings.
- What evidence would resolve it: A direct comparison of PaliGemma's few-shot transfer performance with other VLMs (e.g., LLaVA, BLIP-2) on a common set of tasks using the same limited number of examples.

### Open Question 2
- Question: What is the impact of task-specific prefixes during pretraining on zero-shot performance versus transfer performance?
- Basis in paper: [explicit] The paper shows that task prefixes don't noticeably affect transfer performance but significantly improve pretraining validation perplexity for tasks requiring spatial understanding.
- Why unresolved: The paper only evaluates the impact on transfer performance and pretraining perplexity, not on zero-shot generalization to unseen tasks.
- What evidence would resolve it: A comparison of zero-shot performance on a diverse set of tasks between PaliGemma trained with and without task prefixes, using the same pretraining duration and evaluation methodology.

### Open Question 3
- Question: How does the choice of image resolution and sequence length interact with the model's ability to generalize to different image distributions, such as remote sensing data?
- Basis in paper: [inferred] The paper shows that increasing resolution improves performance on tasks benefiting from higher resolution, but remote sensing tasks (e.g., RSVQA) have significantly different image distributions and show less improvement with resolution increase.
- Why unresolved: The paper doesn't investigate the interaction between resolution, sequence length, and generalization to out-of-distribution data.
- What evidence would resolve it: An ablation study varying both resolution and sequence length during pretraining and transfer for remote sensing tasks, comparing performance to standard image tasks.

## Limitations
- The exact composition of the pretraining task mixture and the specific datasets used are not fully specified in the paper.
- The paper does not provide the optimal hyperparameters for each transfer task, only a general range.
- The paper does not report the computational resources required for pretraining and transfer, which may be a barrier for some researchers.

## Confidence
- High: Effectiveness of the three-stage training procedure and the use of prefix-LM masking and task-prefixes
- Medium: PaliGemma achieves state-of-the-art performance in its size class
- Low: PaliGemma can match or exceed much larger models

## Next Checks
1. Replicate the pretraining procedure using a publicly available task mixture and evaluate the performance of the pretrained model on a diverse set of downstream tasks.
2. Investigate the impact of the three-stage training procedure on the final performance of the model by comparing against a model trained with a different procedure.
3. Compare the performance of PaliGemma against larger VLMs on a subset of tasks to validate the claim that it can match or exceed much larger models.